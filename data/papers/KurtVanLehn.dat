Cognitive and Metacognitive Educational Systems:
Papers from the AAAI Fall Symposium (FS-09-02)

Invited Speaker Abstracts
Stephen Grossberg, Kurt VanLehn, Cristina Conati, Arthur C. Graesser, John C. Cherniavsky

How the Brain Learns in Health and Disease:
Towards Understanding the Many Factors that
Inﬂuence Effective Learning

Moos, 2007; Biswas, Leelawong and Schwartz, 2005) and
the proper use of a tutor’s hints (for example, Roll, Aleven,
McLaren and Koedinger, 2007). Unfortunately, many students stop using these beneﬁcial learning practices as soon
as the metatutoring ceases. Apparently, the metatutors were
nagging rather than convincing. This talk will present a
study of Pyrenees, a metatutor that coaches students to focus on learning domain principles rather than solutions to
examples. It was convincing, in that students who were
taught probability with Pyrenees used principle-based problem solving on post-test more so than students taught by Andes, which did not focus students on principles. Moreover,
when all students were transferred to Andes for learning
of physics, those who were metatutored used the principlefocused strategies even without the nagging of a metatutor.
On a variety of measures, the effect sizes for both probability
and physics were 1.0 or larger.

Presented by Stephen Grossberg, Department of Cognitive
and Neural Systems, Center for Adaptive Systems, and Center of Excellence for Learning in Education, Science, and
Technology, Boston University, Boston, MA 02215
A deep and rational understanding of the factors that inﬂuence effective education and learning technologies depends
on a corresponding understanding of how the brain in health
and disease controls learned behaviors. There has been a
revolution in discovering new computational paradigms, organizational principles, mechanisms, and models of how
learning processes enable brains to give rise to minds. This
talk will begin with a review of what this revolution is,
what some of the new paradigms are, and what modeling
methods can effective link mind to brain on multiple levels. It will then survey, in the time available, a variety of
the brain’s learning mechanisms and modulators that can
inﬂuence whether learning is effective or not, and indeed
whether it becomes imbalanced in ways that are characteristic of mental disorders. Examples will be taken from
cognitive working memory and learned planning, cognitiveemotional interactions, recognition learning, reinforcement
learning, spatial attention and visual search, adaptive timing
of behavior, and adaptive sensory-motor control. How such
processes break down in mental disorders such as autism,
schizophrenia, and amnesia will also be noted, and used to
illustrate how learning processes that are needed for effective learning can break down. See cns.bu.edu/ steve for models of these various processes.

Modeling Student Metacognitive Skills and
Affective States in Education Systems
Presented by Cristina Conati, Computer Science Department and Laboratory for Computational Intelligence University of British Columbia
Student modeling has played a key role in the success of
computer-based tutors known as intelligent tutoring systems
(ITS), by allowing these systems to dynamically adapt to
a student’s knowledge and problem solving behavior. In
this talk, I will discuss how the scope and effectiveness of
ITS can be further increased by enabling these systems to
capture a student’s domain independent metacognitive skills
and affective states. In particular, I will illustrate how we are
applying this research to novel forms of pedagogical interactions designed to provide student-adaptive support to studying examples, exploring interactive simulations and playing
educational games.

Supported in part by the NSF Science of Learning Center
program (SBE-0354378), and the DARPA SyNAPSE program
(HR0011-09-3-0001, HR0011-09-C-0011).

How Can a Tutor get Students to Keep Using a
Metacognitive Strategy After the Tutoring Stops?

Computer Agents that Improve Metaknowledge in
Tutorial Dialogue

Presented by Kurt VanLehn, Department of Computer Science and Engineering, Arizona State University
metatutors have successfully coached students to use
domain-general strategies such as self-explanation (for example, Chi, de Leeuw, Chiu and LaVancher, 1994), selfregulatory strategies (for example, Azevedo, Greene and

Presented by Arthur C. Graesser, Department of Psychology
and Institute for Intelligent Systems, University of Memphis
Metaknowledge is knowledge that the student or tutor has
about cognition, emotions, communication, and pedagogy.
Metaknowledge is necessary for learners who want to take
command of their learning experiences in a self-regulated
ix

Galaxy Zoo project engages amateurs in the classiﬁcation of
astronomical objects to enhance science. In addition, with
the development of the web (again with NSF involvement
through the funding of Mosaic), access to education materials is pervasive — ranging from the desktop to cell phones.
Finally new capabilities made possible through Moore’s law
and the miniaturization of components give new opportunities for providing education resources — for example, GPS
capabilities in inexpensive hand-held computers.
The NSF has responded to these changes by encouraging transformative research in learning technologies through
programs of research support. During this presentation,
I will discuss research support opportunities for learning
technologies from the Computer and Information Science
and Engineering Directorate, the Education and Human Resources Directorate, the Engineering Directorate, and the
Ofﬁce of Cyber-infrastructure at the NSF.

manner and for tutors who want to optimize the students’
learning and motivation. It is well documented, however,
that metaknowledge is quite limited for most students and
tutors. This presentation describes ﬁve illusions that illustrate such limitations. Therefore, it is worthwhile to consider computer-based training environments for improving
metaknowledge proﬁciency. One class of learning environments uses animated conversational agents that hold conversations with students in natural language. This presentation
discusses some agent-based environments that help students
learn at deeper levels and that also improve metaknowledge.
For example, AutoTutor helps students build explanations
while answering difﬁcult questions on topics in science and
technology. AutoTutor takes an indirect approach to promoting metaknowledge by tracking of the student model
that is inferred from the learner’s language and to intelligently respond to the student with a wide range of discourse
moves, including those that attempt to improve metaknowledge. This ﬁne-grained adaptivity considers the emotional
states of the learner in addition to the cognitive states and the
discourse states of the exchange. A second approach is to directly train students on the fundamentals of metaknowledge,
which is an approach taken by iSTART, MetaTutor, SEEK
Web Tutor, Operation ARIES!, and some other agent-based
environments.

Research in Learning Technologies at NSF
Presented by John C. Cherniavsky, Division of Research
on Learning in Formal and Informal Settings, Education
and Human Resources (EHR) Directorate, National Science
Foundation
It has been almost 50 years since Donald Bitzer developed
PLATO 1 — with funding from the Army, Navy, and Air
Force — arguably this is the ﬁrst computer-based learning
technology. The National Science Foundation participated
in the full development of PLATO through the support of the
Computer-based Education Research Laboratory at the University of Illinois starting in 1967 and was involved in other
Learning Technology efforts at Stanford University and elsewhere. The National Science Foundation (NSF), along with
the Ofﬁce of Naval Research and the Advanced Research
Projects Agency, funded early AI and cognitive science research that led to the development of other intelligent tutoring systems — particularly the Cognitive Tutors now marketed by Carnegie Learning, Inc. Thus there is a long history of NSF involvement in AI research, education research,
and research in the behavioral and cognitive science research
underpinning Learning Technologies.
With the advent of transformative cyber-infrastructure
(for example, high speed networks, large data repositories,
sensor networks, and high speed computers that transform
research and education), there is a new need for research on
how this infrastructure can transform education at all levels.
That it is transforming education already can be seen in the
transformation of education and research in astronomy. It is
no longer necessary, or even desirable, for astronomers to go
to telescopes for observations. Observations are automated
and automatically stored in data repositories for analysis.
Nor is analysis restricted to professional astronomers — the
x

When Is Tutorial Dialogue More Eﬀective
Than Step-Based Tutoring?
Min Chi1 , Pamela Jordan2, and Kurt VanLehn3
1
2

3

Computer Science Department, North Carolina State University,Raleigh NC USA
mchi@ncsu.edu
Learning Research and Development Center, University of Pittsburgh, Pittsburgh,
PA USA
pjordan@pitt.edu
School of Computing, Informatics and Decision Science Engineering, Arizona State
University, AZ USA
Kurt.Vanlehn@asu.edu

Abstract. It is often assumed that one-on-one dialogue with a tutor,
which involves micro-steps, is more eﬀective than conventional step-based
tutoring. Although earlier research often has not supported this hypothesis, it may be because tutors often are not good at making micro-step
decisions. In this paper, we compare a micro-step based NL-tutoring
system that employs induced pedagogical policies, Cordillera, to a wellevaluated step-based ITS, Andes. Our overall conclusion is that the pairing of eﬀective policies with a micro-step based system does signiﬁcantly
outperform a step-based system; however, there is no signiﬁcant diﬀerence in the absence of eﬀective policies. Moreover, while micro-step tutoring is more time-consuming, the ﬁndings still hold for ﬁve out of six
learning performance measures when time on task is factored out.
Keywords: Natural Language tutoring systems, Step-based tutoring
systems, Reinforcement Learning, Pedagogical Strategy.

1

Introduction

In domains like math and science solving a problem requires producing an argument, proof or derivation consisting of one or more inference steps, and each
step is the result of applying a domain principle, operator or rule. For instance,
solving an algebraic equation, 2x+5=21 can be done via two steps: 1) subtract
the same term 5 from both sides of the equation; and 2) divide both sides by 2.
A typical Intelligent Tutoring System (ITS) is step-based [13] while human
tutors or Natural Language (NL) tutoring systems, by contrast, often scaﬀold
students via a series of micro-steps leading to the full step. A step corresponds to
(writing) actions that are ordinarily done even without the tutor, whereas microsteps correspond roughly to dialogue acts in tutorial dialogues. For example, in
order to solve a physics problem, the student needs to apply several domain
principles, some of which may need to be applied multiple times. Each principle
S. Trausan-Matu et al. (Eds.): ITS 2014, LNCS 8474, pp. 210–219, 2014.
c Springer International Publishing Switzerland 2014


When Is Tutorial Dialogue More Eﬀective Than Step-Based Tutoring?

211

1. T: So let’s start with determining the value of KE0 .
2. T: Which principle will help you calculate the rock’s kinetic energy at T0?
Please provide the name of the principle, not an equation. {ELICIT}
3. S: Deﬁnition of kinetic energy
4. T: Yes, I agree. Now I will write the equation for applying the deﬁnition of
kinetic energy to the rock at T0: KE0 = 1/2*m*v0ˆ2 {TELL}
Fig. 1. Illustrations of Two Micro-Steps

application can be seen as a step in the ITS. In a physics tutor, for example,
applying the deﬁnition of Kinetic Energy (KE = 12 mv 2 ) to solve for the kinetic
energy of a falling rock at T0 is a step; once a student enters a step, then the tutor
gives feedback and/or hints. Human tutors, by contrast, often scaﬀold students
via a series of micro-steps leading to the full step. In the step mentioned above,
for instance, a human tutor can take the following micro-level steps: selecting
the principle to apply; writing the corresponding equation; solving the equation;
and engaging in some qualitative discussion about the principle.
Fig. 1 illustrates two micro-steps and each numbered line represents a dialogue
turn. The labels T and S designate tutor and student turns respectively. In
this example, the tutor and the student ﬁrst select a principle (lines 2 & 3)
and then write the corresponding equation (line 4). Some of the tutor turns in
Fig. 1 are labeled {ELICIT} or {TELL}. This label designates a tutorial
decision step wherein the tutor has to make a tutorial decision whether to ask
the student for the requisite information or to tell it to the student. For example,
in line 2, the tutor chooses to elicit the answer by asking, “Which principle will
help you calculate the rock’s kinetic energy at T0? Please provide the name of
the principle, not an equation.” If the tutor elects to tell, however, then he or
she would state, “To calculate the rock’s kinetic energy at T0, let’s apply the
deﬁnition of Kinetic Energy.”
One common hypothesis as to the eﬀectiveness of human one-on-one tutoring
comes from the detailed management of “micro-steps” in tutorial dialogue[6,7]
and thus suggests that micro-step based tutors are more eﬀective than step-based
tutors. In several tests of this hypothesis, however, neither human tutors nor NL
tutors designed to mimic human tutors, outperformed step-based tutors once
content was controlled to be the same across all conditions [5,12]. All three types
of tutors were more eﬀective than no instruction (e.g., students reading material
and/or solving problems without feedback or hints). One possible conclusion is
that tutoring is eﬀective, but the micro-steps of human tutors and NL tutoring
systems provide no additional value beyond conventional step-based tutors[13].
Alternatively, we argue that the lack of diﬀerence between micro-step and
step-based tutors is because neither the human tutors nor the NL tutoring systems involved in those studies were good at making micro-step decisions and
several studies provide some support for this claim[3,11,2]. Previously, we investigated the impact of pedagogical policies on student learning by comparing
diﬀerent versions of a micro-step based NL tutoring system called Cordillera [2].

212

M. Chi, P. Jordan, and K. VanLehn

We applied a general data-driven methodology, Reinforcement Learning (RL),
to induce pedagogical policies directly from student interactivity logs and found
that Cordillera with eﬀective pedagogical policies, RL-induced Cordillera significantly out-performed other versions of Cordillera. However, it is still unclear
whether the former is signiﬁcantly better than a step-based ITS.
In this paper, we directly compare RL-induced Cordillera with a well-evaluated
step-based conventional ITS, Andes [14]. Our main research question is: Can a
NL tutoring system with machine-learned pedagogical policies be more eﬀective
than a step-based ITS? Overall, we ﬁnd that RL-induced Cordillera signiﬁcantly
outperforms Andes. In order to investigate whether this result is indeed caused
by eﬀective RL-induced policies, we also compare Andes to two other versions
of Cordillera: Hybrid-RL and Random. In the following, we will brieﬂy describe
the two types of tutoring systems and the pedagogical policies employed in them
and then describe our study and ﬁnally present our results.

2

Two Types of ITSs

The Micro-Step Based Cordillera: NL Tutorial Dialogue System
The Cordillera tutorial dialogue system tutors students in both quantitative and
qualitative physics in the work-energy domain and was implemented using the
TuTalk tutorial dialogue system toolkit toolkit [8]. TuTalk supports dialogues in
which a tutor tries to elicit the main line of reasoning from a student by a series
of coherent questions. This style of dialogue was inspired by CIRCSIM-Tutor’s
directed lines of reasoning [5]. The Cordillera style of dialogue is system-initiative
in that the system always chooses the topics discussed.
Figure 2 illustrates a sample student dialogue with Cordillera. The upper top
right pane of the ﬁgure shows the problem that the student is attempting to

Fig. 2. An example of the Cordillera interface

When Is Tutorial Dialogue More Eﬀective Than Step-Based Tutoring?

213

solve. The top left pane shows a portion of the dialogue history, and illustrates
a few questions and student responses, as well as a number of system informs;
the pending tutor question is shown in the input pane at the bottom followed by
the response the student is entering. Finally, the variables in the bottom right
pane and the equations (hidden) were entered either by the student using a form
interface (not shown) or provided by the tutor. When the tutor asks the student
to compute the value for a variable, the student must transform the equation
to a solvable form with the known values substituted and then the tutor will
do the ﬁnal calculation. In order to avoid confounds due to imperfect NL understanding, a human wizard replaced the NL understanding module. During
tutoring, the wizard matched students’ answers to one of the available responses
but made no tutorial decisions.
The Step-Based Andes Tutoring System
Andes provides a multi-paned screen that consists of a problem-statement window, a variable window, an equation window, and a dialogue window. An example of the Andes interface, as the student would see it, is shown in Figure 3. On
Andes, students construct and manipulate a solution. The interaction is openended, event-driven and student-initiated. Students can enter an equation that
is the algebraic combination of several principle applications and Andes provides
immediate feedback on each entry. Andes can also algebraically manipulate equations to calculate the value for a variable. It considers an entry correct if it is
true, regardless of whether it is useful for solving the problem. When an entry
is incorrect, students can either ﬁx it independently, or ask for what’s-wrong
help. When they do not know what to do next, they can ask for next-step help.
Both next-step and what’s-wrong helps are provided via a sequence of hints

Fig. 3. An example of the Andes interface

214

M. Chi, P. Jordan, and K. VanLehn

that gradually increase in speciﬁcity. The last hint in the sequence, called the
bottom-out hint, tells the student exactly what to do.
Andes provides conceptual and procedural help that is designed to encourage
students to think on their own. Students can always enter any correct step and
Andes does not attempt to determine their problem-solving plans. If necessary
for giving a hint, it asks students what principle they are working on. If students
indicate a principle that is part of a solution to the problem, Andes hints an
uncompleted step from the principle application. If no acceptable principle is
chosen, Andes picks an unapplied principle from the solution that they are most
likely to be working on.

3

Decision Policies within Cordillera and Andes

In many tutoring systems, the system’s behaviors can be viewed as a sequential
decision process wherein, at each discrete step, the system is responsible for selecting the next action to take. Pedagogical strategies are deﬁned as policies to
decide the next system action when multiple are available. Each of these system decisions aﬀects the user’s successive actions and performance. Its impact
on student learning cannot often be observed immediately and the eﬀectiveness of one decision also depends on the eﬀectiveness of subsequent decisions.
Ideally, an eﬀective tutor should craft and adapt its decisions to users’ needs
[1,10]. However, there is no existing well-established theory on how to make
these system decisions eﬀectively. In this work, diﬀerent versions of micro-step
based Cordillera employed diﬀerent pedagogical policies. The step-based Andes
employs hand-coded rules.
Three versions of Cordillera - Random, Hybrid-RL, and RL-induced - were involved. The only diﬀerence among the three is the policy used. Random Cordillera
made tutorial decisions randomly. Hybrid-RL Cordillera used expert-guided datadriven induced rules. These rules were induced by using 18 features and a greedylike procedure to prune the features to meet eﬃciency and training constraints[4].
Both the initial features and pruning procedure were suggested by human experts
and the ﬁnal induced policies were also checked and approved by human experts.
But no signiﬁcant diﬀerence was found on overall learning performance between
the Hybrid-RL and random policies. For RL-induced Cordillera, the data-driven
approach was greatly improved. More speciﬁcally, the RL approach involved a
much larger feature set (50 features), and more advanced domain-general feature
selection approaches. Human experts were not involved in directing the policy
generation. As reported earlier[2], these RL-induced policies indeed helped students learn more and in a deeper way than either Hybrid-RL or random policies.
Andes, on the other hand, like most existing ITSs employs hand-coded pedagogical policies. For example, help in Andes is provided upon request because
it is assumed that students know when they need help and will only process
help when they desire it. A student deciding to request help can be seen as a
human-like decision policy for whether to skip or not skip content.

When Is Tutorial Dialogue More Eﬀective Than Step-Based Tutoring?

4

215

Methods

Participants: A total of 163 participants used either Andes or one of the three
versions of Cordillera: the Andes group comprised 33 students; the Random
Cordillera Group comprised 64 students so that we could collect enough data
for RL policy induction; the Hybrid-RL Cordillera Group comprised 37 students;
and the RL-induced Cordillera group comprised 29 students. All participants
were recruited in the same way but in diﬀerent years.
Domain and Procedure: The training covered the ﬁrst-year college physics
work-energy domain. All participants experienced identical procedures: 1) a
background survey; 2) read a textbook covering the target domain knowledge;
3) took a pretest; 4) solved the same seven training problems in the same order on either Andes or Cordillera; and 5) ﬁnally took a posttest. The pretest
and posttest were identical and contained 16 quantitative items and 16 qualitative items. Both quantitative and qualitative items include multiple choice and
open-ended problems.
Students’ learning outcomes were measured by using three types of scores:
quantitative, qualitative and overall. All tests were graded in a double-blind
manner by experienced graders. In a double-blind manner, neither the students
nor the graders know who belongs to which group. For comparison purpose all
test scores were normalized to fall in the range of [0,1].
Except for following the policies (Random, Hybrid-RL, or RL-induced), the
remaining components of Cordillera, including the interface, the training problems, and the tutorial scripts, were identical for all students. However, there are
some noticeable diﬀerences for the Andes training compared to Cordillera.
Diﬀerences in the Training: The Cordillera dialogues guided students through
the training problems by hinting at the next problem solving step to be completed, or telling them what it is. Hints took the form of short answer questions.
In addition to guiding the student through problem solving, Cordillera also attempted to help the student increase his/her conceptual understanding of the
domain by asking for justiﬁcations for the most important problem solving steps.
The decision for when to ask for a justiﬁcation was determined by a set of pedagogical policies. For an example of a justiﬁcation requested during problem
solving, see the current tutor turn in the bottom left input pane in Figure 2.
There was also a post-problem discussion for each problem which sought to
increase the student’s conceptual qualitative understanding.
We implemented the same seven training problems in Andes and because
Cordillera provided drawings and pre-deﬁned some variables for each problem,
we set-up Andes to provide the same. We added a post-problem discussion to
Andes by collecting all the post-problem discussion for Cordillera into a static
text document so that the content coverage for post-problem discussion was
about the same. The post-problem discussion was delivered in a series of web
pages after the experimenter veriﬁed that the student had completed the Andes
problem.

216

M. Chi, P. Jordan, and K. VanLehn

Note that we did not attempt to provide identical content for the problem
solving help since it reﬂects two diﬀerent tutoring systems, but what is available
is similar. For example, while the Cordillera system’s micro-steps will always
present the content illustrated in Fig. 1, Andes will show the following series of
hints for this same step after the student makes four consecutive help requests:
1) Why don’t you continue with the solution by working on the deﬁnition of
kinetic energy. 2) What is the kinetic energy of the rock at T0? 3) The kinetic
energy of an object is deﬁned as one half its mass times its velocity squared.
That is, 0.5 ∗ m ∗ v 2 . 4) Write the equation KE0 = 0.5 ∗ m ∗ v02 . So for this
illustration asking for all hints on the Andes step is equivalent to a decision to
tell for all the related micro-steps in Cordillera.
While the problem solving help content is similar, there is also some conceptual qualitative discussion during Cordillera’s problem solving that Andes does
not oﬀer. It is up to the student to consider the concepts involved on their own.
However, as has been pointed out, novice students have a tendency to simply
manipulate equations to isolate the unknown and seldom consider the conceptual
knowledge involved during problem solving [9].

5

Results

Overall Training Time
A one-way ANOVA showed signiﬁcant diﬀerences among the four groups on
overall training time: F (3, 154) = 53.90, p < 0.001. The Andes group spent
signiﬁcantly less time1 than the other three groups but there were no signiﬁcant diﬀerences in time on task among the three Cordillera groups. The average
training time (in minutes) across the seven training problems, was M = 115.94,
SD = 42.03 for Andes, M = 280.38, SD = 66.88 for Random, M = 294.33,
SD = 87.51 for Hybrid-RL, and M = 259.99, SD = 59.22 for RL-induced.
Learning Performance
Although students were recruited during diﬀerent time periods, they appear
balanced on incoming competence across the conditions. A one-way ANOVA
showed that there were no signiﬁcant diﬀerences in pretest scores among the
four groups on either quantitative: F (3, 159) = 1.18, p = .32, or qualitative:
F (3, 159) = 0.06, p = .98 , or overall questions F (3, 159) = 0.46, p = .71.
A repeated measures analysis using test (pretest vs. posttest) as a factor and
test score as the dependent measure showed that there was a main eﬀect for
test. All four groups of students scored signiﬁcantly higher on the posttest than
the pretest, F (1, 32) = 19.87, p < 0.001 for Andes, F (1, 63) = 78.37, p < 0.001
for Random, F (1, 36) = 48.36, p < 0.001 for Hybrid-RL, and F (1, 28) = 238.58,
p < 0.001 for RL-induced.
The same results were found from pretest to posttest on both quantitative
and qualitative questions as well. More speciﬁcally, on quantitative questions,
1

Some reading times for the last problem were lost so we used the minimum average
reading time for all other easier problems.

When Is Tutorial Dialogue More Eﬀective Than Step-Based Tutoring?

217

Table 1. RL-induced Cordillera vs. Andes on Various Test Scores
Test Item Test
Set
Score
quant
Pre
Post
Adj Post
NLG
qual
Pre
Post
Adj Post
NLG
Overall Pre
Post
Adj Post
NLG

RL-induced Andes
Cordillera
0.35 (0.25) 0.28 (0.26)
0.64 (0.22) 0.41 (0.30)
0.61 (.18) 0.44 (.17)
0.49 (0.28) 0.16 (0.38)
0.46(0.12) 0.45(0.14)
0.65 (0.14) 0.54 (0.18)
0.65 (.14) 0.54 (.14)
0.36 (0.24) 0.14 (0.34)
0.42 (0.15) 0.39 (0.16)
0.65 (0.15) 0.50 (0.21)
0.64 (.11) 0.51 (.12)
0.42 (0.19) 0.17 (0.28)

Stat

cohen
d
t(60) = 1.01, p = .28
0.27
t(60) = 3.29, p = 0.002
0.87 ∗ ∗
F (1, 59) = 13.793, p < .0001 0.97 ∗ ∗
F (1, 59) = 14.442, p < 0.0001 0.99 ∗ ∗
t(60) = 0.40, p = .688
0.08
t(60) = 2.68, p = 0.010
0.68 ∗ ∗
F (1, 59) = 7.74, p = .007
0.79 ∗ ∗
F (1, 59) = 8.86, p = 0.004
0.75 ∗ ∗
t(60) = 0.87, p = .39
0.19
t(60) = 3.35, p = 0.001
0.82 ∗ ∗
F (1, 59) = 16.50, p < .0001 1.13 ∗ ∗
F (1, 59) = 15.97, p < 0.0001 1.04 ∗ ∗

F (1, 32) = 15.83, p < 0.001 for Andes, F (1, 63) = 33.55, p < 0.001 for Random,
F (1, 36) = 58.01, p < 0.001 for Hybrid-RL, and F (1, 28) = 95.79, p < 0.001
for RL-induced. On qualitative questions, F (1, 32) = 7.68, p = 0.009 for Andes, F (1, 63) = 40.62, p < 0.001 for Random, F (1, 36) = 17.20, p < 0.001 for
Hybrid-RL, and F (1, 28) = 89.56, p < 0.001 for RL-induced. Therefore all four
conditions made signiﬁcant gains from pre-test to post-test across all three sets of
questions: quantitative, qualitative and overall questions. In order to investigate
whether micro-step based tutors can be more eﬀective than step-based tutors,
we ﬁrst investigated whether the most eﬀective version of Cordillera would outperform Andes.
RL-Induced Cordillera vs. Andes
Table 1 compares the pre-test, post-test, adjusted post-test, and NLG scores
between the RL-induced Cordillera and Andes conditions by question type. The
adjusted Post-test scores were compared between the two conditions via an ANCOVA with the corresponding pre-test score as a covariate. NLG measures students’ gain irrespective of their incoming competence: N LG = posttest−pretest
.
1−pretest
Here 1 is the maximum score. The third and fourth columns in Table 1 list the
means and SDs of the two groups’ corresponding scores. The ﬁfth column lists
the statistical comparison and the last column lists the eﬀect size of the comparison using Cohen’s d2 . Table 1 shows that there was no signiﬁcant diﬀerence
between the two conditions on pre-test scores. However, there were signiﬁcant
diﬀerences between them on the post-test, adjusted post-test, and NLG scores
for all three question types.
We then compared the two groups’ performance on six types of learning measures: {Quantitative, Qualitative, Overall} × {Posttest, NLG} using both pre-test
and total training time as the covariates. On one measure, quantitative posttest,
2

Which is deﬁned as the mean learning gain of the experimental group minus the
mean of the control group, divided by the groups’ pooled standard deviation.

218

M. Chi, P. Jordan, and K. VanLehn

there was no signiﬁcant diﬀerence between the two groups: F (1, 58) = 2.34, p =
0.132. But on the remaining ﬁve measures, RL-induced Cordillera signiﬁcantly outperformed Andes: F (1, 58) = 7.27, p = 0.009 for qualitative posttest, F (1, 58) =
5.94, p = 0.018 for overall posttest, F (1, 59) = 4.72, p = 0.034 for quantitative
NLG, F (1, 59) = 7.34, p = 0.009 for qualitative NLG and F (1, 58) = 9.71, p =
0.003 for overall NLG respectively.
In sum, our results showed that micro-step based tutors can indeed be more
eﬀective than step-based tutors as RL-induced Cordillera signiﬁcantly outperformed Andes on all types of test questions. Even when time on task is factored
out, the same results hold for ﬁve out of six learning measures. Next, we compared Random and Hybrid-RL Cordillera with Andes to investigate whether the
micro-step tutor would still be more eﬀective than the step-based tutor without
eﬀective pedagogical policies.
Random vs. Andes and Hybrid-RL Cordillera vs. Andes: There were
no signiﬁcant diﬀerences between the Random-Cordillera and Andes groups on
any of the learning outcome measures. Since Andes students spent signiﬁcantly
less time than Cordillera students, we compared the two conditions’ posttest
scores using both pre-test score and total training time as covariates and their
NLG scores using total training time as the covariate. To our surprise, we still
found no signiﬁcant diﬀerences between the two groups. We had expected the
eﬃciency of the Andes group to have some impact.
Similar results were found when we compared Hybrid-RL Cordillera and Andes on all types of learning outcome measures either when time on task is factored
in or out. Since Hybrid-RL Cordillera employed human-inﬂuenced pedagogical
rules, these results again indicate that expert tutors’ pedagogical rules may not
always be eﬀective. Again, this study suggests that ﬁne-grained interactions at
micro-steps are a potential source of pedagogical power, but human tutors may
not be particularly skilled at choosing the right micro-steps.

6

Conclusions and Future Work

Although it is often believed that micro-step based NL tutoring systems should
be more eﬀective than conventional step-based ITSs, little evidence was previously found to support this. Our hypothesis is that it is because the existing micro-step based NL tutoring systems do not employ eﬀective pedagogical
strategies. Previous work applied a general data-driven RL approach to induce
eﬀective pedagogical policies directly from student logs and found them to be
more eﬀective than either random or Hybrid-RL policies. However, it was still
not clear whether these RL-induced policies would make micro-step based NL
tutoring systems more eﬀective than step-based ITSs.
In this paper, we found that RL-induced Cordillera signiﬁcantly outperforms
Andes while neither Hybrid-RL Cordillera nor Random Cordillera were signiﬁcantly diﬀerent from step-based Andes. Our overall conclusion is that a microstep based system with eﬀective RL-induced policies can signiﬁcantly outperform
a step-based ITS with hand-coded policies; however, there is no signiﬁcant diﬀerence between micro-step based and step-based tutoring systems in the absence of

When Is Tutorial Dialogue More Eﬀective Than Step-Based Tutoring?

219

eﬀective policies. Note that micro-step based Cordillera is more time-consuming
than Andes. However, even when time on task is factored out, the micro-step
based tutoring system with eﬀective RL-induced policies is still signiﬁcantly better than the step-based tutoring systems with hand-coded policies on ﬁve out of
six learning performance measures.
Future work that remains is to explore policy-induction for Andes and to conduct a comparison of step-based tutoring to micro-step tutoring when both have
eﬀective RL-induced pedagogical policies. This may improve our understanding
of the grain-size (step vs. micro-step) issue.
Acknowledgments. This work was supported by NSF Award #0325054.

References
1. Anderson, J.R., Corbett, A.T., Koedinger, K.R., Pelletier, R.: Cognitive tutors:
Lessons learned. The Journal of the Learning Sciences 4(2), 167–207 (1995)
2. Chi, M., VanLehn, K., Litman, D.J., Jordan, P.W.: Empirically evaluating the
application of reinforcement learning to the induction of eﬀective and adaptive
pedagogical strategies. User Model. User-Adapt. Interact. 21(1-2), 137–180 (2011)
3. Chi, M.T.H., Siler, S., Jeong, H.: Can tutors monitor students’ understanding
accurately? Cognition and Instruction 22(3), 363–387 (2004)
4. Chi, M., Jordan, P.W., VanLehn, K., Litman, D.J.: To elicit or to tell: Does it
matter? In: Dimitrova, V., Mizoguchi, R., du Boulay, B., Graesser, A.C. (eds.)
AIED, pp. 197–204. IOS Press (2009)
5. Evens, M., Michael, J.: One-on-one Tutoring By Humans and Machines. Erlbaum,
Mahwah (2006)
6. Graesser, A.C., Person, N., Magliano, J.: Collaborative dialog patterns in naturalistic one-on-one tutoring. Applied Cognitive Psychology 9, 359–387 (1995)
7. Graesser, A.C., VanLehn, K., Rosé, C.P., Jordan, P.W., Harter, D.: Intelligent
tutoring systems with conversational dialogue. AI Magazine 22(4), 39–52 (2001)
8. Jordan, P.W., Hall, B., Ringenberg, M., Cui, Y., Rosé, C.: Tools for authoring a
dialogue agent that participates in learning studies. In: Proceedings of AIED 2007,
pp. 43–50 (2007)
9. Leonard, W., Dufresne, R., Mestre, J.: Using qualitative problem-solving strategies to highlight the role of conceptual knowledge in solving problems. American
Journal of Physics 64(12) (1996)
10. Phobun, P., Vicheanpanya, J.: Adaptive intelligent tutoring systems for e-learning
systems. Procedia - Social and Behavioral Sciences 2(2), 4064–4069 (2010), innovation and Creativity in Education
11. Putnam, R.T.: Structuring and adjusting content for students: A study of live and
simulated tutoring of addition. Amer. Edu. Res. Journal 24(1), 13–48 (1987)
12. VanLehn, K., Graesser, A.C., Jackson, G.T., Jordan, P., Olney, A., Rosé, C.P.:
When are tutorial dialogues more eﬀective than reading? Cog. Sci. 31(1), 3–62
(2007)
13. VanLehn, K.: The relative eﬀectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems. Educational Psychologist 46(4), 197–221 (2011)
14. VanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., Shelby, R., Taylor, L., Treacy,
D., Weinstein, A., Wintersgill, M.: The andes physics tutoring system: Lessons
learned. IJAIED 15(3), 147–204 (2005)

A System Architecture for Affective Meta Intelligent
Tutoring Systems
Javier Gonzalez-Sanchez1, Maria Elena Chavez-Echeagaray1, Kurt VanLehn1,
Winslow Burleson1, Sylvie Girard2, Yoalli Hidalgo-Pontet1, and Lishan Zhang1
1

Arizona State University, Tempe, AZ, USA
{javiergs,mchaveze,kurt.vanlehn,winslow.burleson,
yoalli.hidalgopontet,lishan.zhang}@asu.edu
2
University of Birmingham, Birmingham, UK
s.a.girard@bham.ac.uk

Abstract. Intelligent Tutoring Systems (ITSs) constitute an alternative to expert
human tutors, providing direct customized instruction and feedback to students.
ITSs could positively impact education if adopted on a large scale, but doing
that requires tools to enable their mass production. This circumstance is the key
motivation for this work. We present a component-based approach for a system
architecture for ITSs equipped with meta-tutoring and affective capabilities. We
elicited the requirements that those systems might address and created a system
architecture that models their structure and behavior to drive development
efforts. Our experience applying the architecture in the incremental
implementation of a four-year project is discussed.
Keywords: architecture, component-based, tutoring, meta-tutoring, affect.

1

Introduction

Intelligent Tutoring Systems (ITSs) seem capable of becoming untiring and
economical alternatives to expert human tutors. This possibility has proven difficult to
achieve, but significant progress has been made. The use of ITSs has become more
common, and there is significant work about their pedagogical and instructional
design but not about their technological implementation. ITSs are software products
and, as for any other software product, their implementation on a massive scale relies
on the principle of assembly instead of crafting them as one-of-a-kind systems.
Component-based software engineering [1] is an appropriate approach for handling
mass production. Component-based software engineering addresses the development
of systems as an assembly of parts (components), with the development of these parts
as reusable entities and with the maintenance and upgrading of systems through
customizing and replacing such parts.
Following a component-based approach, we have defined a system architecture to
drive the development of ITSs equipped with affective and meta-tutoring capabilities,
called affective meta intelligent tutoring systems (AMTs). Defining a system architecture
S. Trausan-Matu et al. (Eds.): ITS 2014, LNCS 8474, pp. 529–534, 2014.
© Springer International Publishing Switzerland 2014

530

J. Gonzalez-Sanchez et al.

is the first step in creating a component-based software framework to implement AMTlike applications. This system architecture takes advantage of previous experiences with
ITS implementations; most of that previous experience was extracted from the analysis
made on existing ITS behavior described in [2], as well as from previous experience in
the development of real-time affective companions, mainly by the work described in [3].
This paper is organized as follows: Section 2 provides the terminology and
background for system architectures, ITS behavior, and affect recognition; Section 3
describes the AMT system architecture; Section 4 describes the implementation of an
application following the AMT system architecture and discusses its software metrics;
and Section 5 provides a conclusion.

2

Terminology and Background

The following terminology and background summary contextualizes the work
described in this paper:
System Architecture. A system is a group of interacting, interrelated or independent
modules forming a complex whole. Modules are self-contained entities that carry out
a specific function; they are implemented as a set of parts called components. A
system architecture is a conceptual model that describes the modules and components
of a system and how they interconnect with each other; it becomes a software design
model by mapping each component to a set of classes following software engineering
methodologies. The system architecture is essential for realizing the system's quality
attributes [4].
ITS Behavioral Description. ITSs are typically used to assign tasks to students; tasks
are composed of steps that the student must accomplish. The structure of this kind of
ITSs, called step-based, is described in [2] and can be summarized as follows: (1) the
group of tasks known by the ITS conforms its Knowledge Base; (2) a Task Selector
chooses from the Knowledge Base the Task that the student must solve by considering
the student’s previous performance reported by an Assessor; (3) a User Interface (a
tool or an environment) provides the space in which the interaction between the tutor
and the student occurs; (4) a Step Analyzer methodically examines the student’s steps
and determines whether they are correct or incorrect and then reports that information
to a Pedagogical Module and to an Assessor; (5) a Pedagogical Module provides
support (hints and feedback); the provided support depends on current steps and the
student’s previous performance; and (6) an Assessor measures the performance of the
student (requested hints, time used to go from one step to another, etc.).
Affect Recognition Strategies. Research shows that learning is enhanced when
affective support is present [5]. To provide that support, ITSs need to recognize
students’ affect. Diverse strategies exist for affect recognition; the one we are
considering for this work uses sensing devices to read students’ physiological responses;
this strategy uses, among others, brain-computer interfaces, eye-tracking systems, facebased emotion recognition systems, and diverse sensors to measure skin conductance
(arousal), posture, and finger pressure [6].

A System Architecture for Affective Meta Intelligent Tutoring Systems

3

531

System Architecture

The system architecture was engineered [7] on the principles of encapsulation, low
coupling, centralized shared data, and layering. Functionality is encapsulated in
simple components; components that are complex and/or serve diverse purposes are
split into several collaborative components. Components are low-coupled to facilitate
replacement, i.e., to increase modifiability. A centralized data-sharing mechanism is
used to pass data among modules to reduce latency. Components are organized in a
three-layer structure in which the bottom layer encodes utility services for data
management and communication responsibilities; the middle layer encodes the
business logic; and the topmost layer encloses the user interface, which handles the
interaction with the user. Since the user interface is particular to a specific system, it
is not described here. Fig. 1 shows modules (boxes), components (gray boxes), and
their relationships (arrows) as follows:
Tutor Module. It encapsulates the ITS behavior. Its components and relationships are
summarized in Section 2.
Meta Tutor Module. It encapsulates the logic for providing meta-tutoring
recommendations and promoting meta-skills in the student. The Meta Tutor module
has two components: (1) an Inspector that reads Tutor events (populated in the Shared
Repository) and filters those that suggest an intervention is needed; and (2) an Engine
that provides intelligence to the Meta Tutor; the Engine is notified by the Inspector of
compelling events and it infers the type of intervention that must be done.
Interventions consist of showing a message or disabling channels of user interaction.
The Engine implements the policies about how and when interventions must be done.
It communicates the interventions to the User Interface for its execution.
Affective Companion Module. It encapsulates the logic for generating affective
interventions. The Affective Companion has two components: (1) an Event Selector
reads the data for Tutor events and affective states (populated in the Shared
Repository) and filters combinations that suggest an intervention is needed; and (2) an
Affective Engine that implements the affective intelligence; the Affective Engine is
notified by the Event Selector of compelling combinations of Tutor events and
affective state data and infers the type of intervention that must be done. Interventions
consist of motivational messages. The Affective Engine implements the policies about
how and when interventions must be done. It communicates the interventions to the
User Interface for its execution.
Shared Repository. It is a centralized means for passing data among the other
modules, which are running concurrently. The Shared Repository module follows a
blackboard architectural model, in which a common data repository, “the blackboard,”
is updated by some modules and read by others. The Tutor posts events to the
blackboard and the Emotion Recognition Subsystem posts affective state reports.
The Meta Tutor and Affective Companion observe the blackboard, looking for data that
triggers an action on their side.

532

J. Gonzalez-Sanchez et al.

Emotion Recognition Subsystem. It is a facade that provides a simplified interface
to a source of affective state data, such as a third-party system, framework, or library.
The system architecture prioritizes the quality attributes of modifiability, extensibility,
and integrability. Modifiability refers to the ease with which a component can be
modified for use in applications or environments other than those for which it was
specifically designed; affective and cognitive intelligence require this quality since they
are implemented in different ways. Extensibility refers to being prepared for extension
into unforeseen contexts since not all application requirements can be determined in
advance; our system architecture required this quality to make feasible the addition of
new tutoring, meta-tutoring, or affective support capabilities. Integrability is the process
of combining software subsystems to assemble an overall system; AMT system
architecture requires the integration of a third-party system or code (1) for affect
recognition to support the functionality of the Affective Companion module and (2)
for decision-making (machine-learning algorithm implementation) to support the
functionality of the Affective Companion and Meta Tutor modules.

Fig. 1. AMT System Architecture

4

Usage and Discussion

The AMT system architecture has been used as a reference during a four-year project
focused on developing an AMT application [8]. The AMT application was
implemented in Java with Swing components. The final version is composed of 16
packages, 120 classes, 1507 methods, 1810 attributes, and 37,374 lines of code. A
production-rule system and a third-party implementation of emotion recognition
algorithms were used to support the application development. A detailed description
of moving from AMT system architecture to software design is outside the scope of

A System Architecture for Affective Meta Intelligent Tutoring Systems

533

this paper due to space limitations; nevertheless, a description of mapping the ITS
module to a software design can be found in [9]. The four-year implementation
process was managed using a revision control system and comprises 1,643 revisions
and 8 released versions. Differences between released versions include, among others,
changes in requirements, enhancements of decision-making strategies, and bug fixing.
A total of 15 developers were involved in the different stages of the project, and a
team of at least four developers was working concurrently in every stage.
The results of applying the system architecture were measured indirectly by
evaluating the structural software quality of the systems developed under its influence
using software metrics. Due to space limitations we report the evaluation of four AMT
application releases, one from each development year, as follows: (1) Release 742
implemented the first deployed Tutor; it was focused on the User Interface (a tool) and
had limited tutoring capabilities; coding the skeleton of the system was the primary goal
during this year. (2) Release 1277 refashioned the User Interface and implemented an
enhanced Tutor. (3) Release 1545 included a Meta Tutor, continued refashioning the
User Interface, and enhanced the Tutor module. (4) Release 1643 added the Affective
Companion capabilities, enhanced the Meta Tutor, and refactored the User Interface
and Tutor. The metrication of structural qualities, shown in Table 1, includes measures
for size, complexity, and coupling as follows: number of packages (P), number of
classes (F), number of functions (Fn), number of lines of code (LoC), number of
comments (LoCm), average cyclomatic complexity (AvC), maximum afferent coupling
(MaxAC), and maximum efferent coupling (MaxEC) [10].
Table 1. Comparison of software metrics for modules in diverse AMT application releases
Release
742
1277
1545
1643

Release
1545
1643

Release
1643

Date
07/2010
07/2011
07/2012
07/2013

P
5
9
11
14

F
24
42
55
62

Fn
347
650
885
936

Tutor
LoC
10656
20839
24542
25189

LoCm
2861
4127
4654
4816

AvgC
3.11
3.61
3.03
2.96

MaxAC
4
8
9
12

MaxEC
5
9
9
10

Date
07/2012
07/2013

P
1
1

F
22
22

Fn
202
248

Meta Tutor
LoC
3346
4210

LoCm
437
458

AvgC
2.68
3.05

MaxAC
4
5

MaxEC
4
7

Date
07/2013

P
3

F
36

Affective Companion
Fn
LoC
LoCm
323
7975
1403

AvgC
2.59

MaxAC
9

MaxEC
6

Even though we had a high turnover in the development team, the size, complexity,
and coupling remained at acceptable values. Size measurements (P, F, Fn, LoC, and
LoCm) show a correspondence of the requirements implemented in each release and
the size of the application, as well as a balance in its granularity. The average
complexity (AvgC) at the module level remains within acceptable ranges (below
five); at a fine-grain level (classes), not shown in the table, those values are not
always acceptable. The decrease in average complexity in the latest versions of Tutor
shows the refactoring outcome (functionality was fixed and developers focused on
code improvements). Lower values in coupling measures (MaxAC and MaxEC) are

534

J. Gonzalez-Sanchez et al.

better since they are a sign of independence; the high values of coupling in Tutor can
be justified because they belong to the User Interface (highly connected); Meta Tutor
values are acceptable, but Affective Companion values suggest that a refactoring
would be required in the implementation of this module.

5

Conclusions

In this paper, we have presented the AMT system architecture, the first step for
creating a component-based software framework to implement AMT-like
applications. We have defined its requirements and qualities and have shown how the
AMT system architecture addresses them to support large-scale reuse. Software
metrics for different releases of one AMT application show how the system
architecture provided a flexible partition of the system that facilitates modifiability,
extensibility, and integrability. With this proposed system architecture, we aim to
share our experience, looking forward to making the development of AMT-like
systems an easier, faster, and standardized process.
Acknowledgments. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References
1. Crnkovic, I.: Component-Based Software Engineering - New Challenges in Software
Development. Software Focus 2(4), 127–133 (2001)
2. VanLehn, K.: The Behavior of Tutoring Systems. International Journal of Artificial
Intelligence in Education 16(3), 227–265 (2006)
3. Burleson, W.: Affective Learning Companions: Strategies for Empathetic Agents with
Real-Time Multimodal Affective Sensing to Foster Meta-Cognitive Approaches to
Learning, Motivation, and Perseverance. MIT PhD thesis (2006)
4. Bass, L., Clements, P., Kazman, R.: Software Architecture in Practice, 2nd edn. AddisonWesley, Boston (2003)
5. Lehman, B., D’Mello, S.K., Person, N.: All Alone with Your Emotions. In: 9th
International Conference on Intelligent Tutoring Systems. Springer (2008)
6. Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E., Atkinson, R., Burleson, W.: Multimodal
Detection of Affective States: A Roadmap Through Diverse Technologies. In: ACM
SIGCHI Conference on Human Factors in Computing Systems. ACM (2014)
7. Firesmith, D.G., Capell, P., Falkentha, D., Hammons, C.B., Latimer IV, D.T., Merendino,
T.: The Method Framework for Engineering System Architectures. CRC Press (2008)
8. VanLehn, K., Burleson, W., Girard, S., Chavez- Echeagaray, M.E., Gonzalez-Sanchez, J.,
Hidalgo-Pontet, Y., Zhang, L.: The Affective Meta-Tutoring project: Lessons Learned. In:
15th International Conference on Intelligent Tutoring Systems. Springer (2014)
9. Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E., VanLehn, K., Burleson, W.: From
Behavioral Description to A Pattern-Based Model for Intelligent Tutoring Systems. In:
18th International Conference on Pattern Languages of Programs. ACM (2011)
10. Pressman, R.S.: Software Engineering, 7th edn. McGraw-Hill, Boston (2009)

Student Modeling from Conventional Test Data:
A Bayesian Approach without Priors
Kurt VanLehn, Zhendong Niu, Stephanie Siler and Abigail S. Gertner
Learning Research and Development Center
University of Pittsburgh
Pittsburgh, PA 15260
VanLehn@cs.pitt.edu, niu+@pitt.edu, siler+@pitt.edu, gertner+@pitt.edu

Abstract. Although conventional tests are often used for determining a
student’s overall competence, they are seldom used for determining a finegrained model. However, this problem does arise occasionally, such as when a
conventional test is used to initialize the student model of an ITS. Existing
psychometric techniques for solving this problem are intractable. Straightforward Bayesian techniques are also inapplicable because they depend too
strongly on the priors, which are often not available. Our solution is to base the
assessment on the difference between the prior and posterior probabilities. If
the test data raise the posterior probability of mastery of a piece of knowledge
even slightly above its prior probability, then that is interpreted as evidence that
the student has mastered that piece of knowledge. Evaluation of this technique
with artificial students indicates that it can deliver highly accurate assessments.

Introduction
Diagnostic testing uses conventional test formats (e.g., items with multiple choice or
numerical answers) but assumes that a student’s competence can be characterized by a
set of several subskills or factors. For instance, competence in multi-digit addition
might be characterized with the subskills of carrying, processing columns without
carries, and other subskills as well.
In its simplest form, a diagnostic test scoring algorithm inputs binary answer data
(i.e., 1 if the question was answered correctly, 0 if answered incorrectly) and outputs a
level of mastery for each of the subskills. Existing algorithms (e.g., DiBello et al.,
1995; Samejima, 1995; Tatsuoka, 1990, 1995) enumerate all possible subsets of
subskills that have distinguishable patterns of answers. The algorithms examine each
subset and pick the one that best fits the answer data.
An unusual application of diagnostic testing arose during the development of the
Andes physics tutoring system (VanLehn, 1996; Conati, Gertner, VanLehn &
Druzdzel, 1997). Andes has a student modeler that uses Bayesian techniques to
calculate the probability of mastery of each of about 350 rules. Because it is a
Bayesian, it requires for each rule a prior probability, that is, the probability that a
randomly drawn student from the population will have already mastered that rule

B.P. Goettl et al. (Eds.): ITS '98, LNCS 1452, pp. 434-443, 1998.
© Springer-Verlag Berlin Heidelberg 1998

Student Modeling from Conventional Test Data

435

before using Andes. In order to find these prior probabilities, we planned to use
diagnostic testing. That is, we planned to treat each of the rules as a distinct
“subskill” or “factor,” then use diagnostic testing to find out which “subskills” each
student had mastered. By counting how many times each rule/subskill was mastered,
we could estimate the prior probability of that rule in the population.
The test, which was developed by the physics instructors associated with the
project, had 34 items, all of which had multiple choice or short-answer formats. We
determined which of the Andes rules were required for correctly answering each
problem. Overall, 66 rules were used at least once during the test.
Unfortunately, 66 “subskills”are much more than typically used in diagnostic
66
testing. The existing scoring techniques would have to examine as many as 2
subsets of rules in order to tell which subset best fit a given student’s answers. This
made existing scoring techniques inapplicable, so we had to develop a diagnostic test
scoring algorithm that would infer, given a student’s binary answer data, whether or
not the student had mastered each of the 66 rules.
If such an algorithm can be found, then it could be used for other purposes besides
determining priors for Andes student modeling. For instance, we could routinely give
a student who was about to use Andes a short multiple-choice test. The results of the
test could be used to initialize the student’s model.
The algorithm needs to be told which rules (or subskills, etc.) are required for
correctly solving each problem. If the problem can be solved with two or more
correct strategies, then it may be necessary to use a disjunction in such a specification.
For instance, if one strategy requires rules 1, 2 and 3, and the other strategy requires
rules 3, 4 , 5 and 6, then one would have to specific that correctly solving the problem
requires knowing rule 3 and either rules 1 and 2 or rules 4, 5 and 6.
However, items are usually written to have just one correct solution, although
students may always invent ones that the authors did not anticipate. Psychometricians
often assume that items have just one correct solution, which allows them to use a
simplified representation, called a Q-matrix (Tatsuoka, 1990), for the relationship
between knowledge and problems. In a Q-matrix, the columns are problems, the rows
are pieces of knowledge, and cells are 1 if the piece of knowledge is required by the
problem and 0 is if the knowledge is not required for solving that problem. Although
a Q-matrix cannot accurately represent problems with multiple correct solution
strategies, it has been found adequate for most tests, including the one we have used
for testing.
The general problem can be stated as follows: Given
• a test with N items;
• a knowledge base of M rules (we will use “rule” to stand for any kind of
unitization of knowledge);
• a M by N Q-matrix;
• an N-long binary vector indicating which test items the student answered
correctly,
output a M-long real-numbered vector indicating the probability of mastery of each
rule.

436

Kurt VanLehn et al.

Research approach/method
Our approach was empirical: Try several diagnostic test scoring methods and
evaluate them using artificial students. An artificial student is a function that, when
given a set of mastery levels for the rules, produces a binary answer data vector. To
use an artificial student for evaluation, the answer vector is fed into the test scoring
method, which then predicts the mastery levels for each rule. If the predictions match
the original mastery levels, then the test scoring method is accurate.
Although we would have preferred to evaluate the scoring methods with human
students, that would require knowing with certainty what their levels of mastery were
on each of the 66 rules.
In principle, artificial students should be based on real cognitive models. They
should model forgetting, priming, guessing, cheating and all the other things students
do during tests. However, we used a simpler framework that only modeled inadvertent
mistakes (slips) and guessing. In our artificial students:
• P(the problem’s answer is correct | all rules required for answering it are
mastered) = 1 – slip.
• P(the problem’s answer is correct | at least one rule required for answering it
is not mastered) = guess / number of possible answers for this problem.
That is, the model has two global parameters: the probability of a slip and the
probability of a guess. Once a person has decided to guess, whether they get the
problem correct is inversely proportional to how many possible answers that multiple
choice problem has. Since the equations above only give the probability of a
problem’s answer being correct, the probabilities are used as odds in a random
number generator to generate the actual binary answer vector.
For evaluating the proposed assessments, we needed a collection of artificial
students. We generated a collection of 201 artificial students with slip = .1 and guess
= .3 by first randomly generating a level of competence C, then randomly generating
rule masteries such that P(mastery of a rule) = C. That is, artificial students with high
C had many rules mastered and student with low C had few rules mastered.
After the artificial students have been assessed and predictions about their mastery
levels have been obtained, we need to measure the degree of match between the
predicted and actual mastery levels. We evaluated the match separately for each rule,
since we wanted to see if some rules were easier to assess than others. For each rule,
we counted the number of artificial students where (A) the rule was actually mastered,
(B) the rule was predicted to be mastered, and (C) the rule was both predicted to be
mastered and actually mastered. Then:
• Accuracy is C/B. It should be as close to 1 as possible.
• Coverage is C/A. This should also be as close to 1 as possible.
Originally, we sought to maximize accuracy alone. However, when we were
adjusting parameters (as described below), we found that accuracy reached a plateau
rather than a peak as we varied the parameter values. We needed some way to select
a parameter value among those which tied for producing the maximum accuracy.
Thus, we adopted coverage as a desirable but secondary feature. To make parameter
tuning easier, we define utility = coverage + 2*accuracy, which gives accuracy twice
as much weight as coverage. We later considered the effects of replacing the “2” with

Student Modeling from Conventional Test Data

Rule1

Rule2

Problem1

Rule3

Rule4

Problem2

437

Rule5

Problem3

Fig. 1. A Bayesian network used for assessment.

other values in order to see if varying the relative importance of coverage and
accuracy made any difference in our findings (see below).

Failures
We tried several obvious schemes but rejected them for various reasons. However,
the scheme we ultimately accepted is based on what we learned from our failures, so
we will discuss them first.

Noisy And
The first test scoring method we considered was based directly on the causality, as we
perceived it. Mastery of rules causes problems to be answered correctly or not. Thus,
we constructed a Bayesian network (Pearl, 1988) with nodes for problems and rules,
and links from the rules to the problems (the causal direction). That is, a problem’s
node’s parents were the nodes representing rules that were required for answering the
problem correctly. For instance, in Figure 1, rules 1, 2 and 3 are required by problem
1, so they are the parents of problem 1 in the network. The conditional probability
table of the problem’s node is:
• P(answer correct | all rules required for answering it are mastered) = 1 –
slip.
• P(answer correct | at least one rule required for answering it is not mastered)
= guess / number of possible answers to that problem.
This is similar to a noisy And (Pearl, 1988), hence the name. It is also the same
function as used in the artificial students, but the slip and guess parameters can be
changed in order to optimize the performance of the assessment function.
Unfortunately, because the rule nodes are roots of the network, this approach
requires assigning a prior for each rule node. In our application, the population priors
are not known. Bayesians often use 0.5 for a prior when they don’t know the actual
prior. However, we discovered that for most rules, the posterior depended strongly on
the choice of prior. For 46 of the 66 rules, when we tried the priors of .3, .5 and .8,

438

Kurt VanLehn et al.

the posterior was within ±0.05 of the prior. For the remaining 20 rules, the posteriors
were relatively insensitive to the priors.
What seems to be happening with the 46 rules that are overly sensitive to the prior
is the following:
• If a problem is incorrect, then any rule required by it could be unmastered.
Thus, the network passes out blame rather thinly.
• If a problem is correct, then either the student guessed or all the rules must
be mastered. Thus, even a little evidence of nonmastery of one of the rules
will cause the network to infer that the student is guessing, which attenuates
the credit the network passes out.
Although the above is just our interpretation of what happens, it is a fact that the
network passes out credit and blame rather thinly. For most rules, it seems to take a
lot of consistent evidence before the noisy-And network is willing to change the prior
probabilities much.

Noisy And with second order probabilities
Since the influence of the priors was so strong, we sought to unbias the network using
second order probabilities (Neopolitan, 1990), which are thought to be a way of
representing ignorance of priors. Each rule node was given a new parent with 5
values, 0 through 4, which were given a uniform prior probability distribution (each
value had a prior of 0.2).
The rule’s conditional probability table was
P(rule|parent=N) = 0.25*N, where N is one of the parent’s values. In other words,
this network is saying that it is equally likely that the rule node could have a “prior” of
either 0, 0.25, 0.5, 0.75 or 1.0
This made no difference. Assessments given to students were exactly the same as
those given by 0.5 priors in the preceding, simpler noisy-And network.
A noisy Or network.
To obviate the need for priors, we tried reversing the links in the Bayesian network.
Each rule became a noisy Or (Pearl, 1988), with problems as the parents. That is, if
any of the problems that required a rule was answered correctly, then the rule was
probably mastered.
However, this meant that the rules were conditionally independent given the
evidence, which just isn’t true. For example, suppose rules R1 and R2 are required by
problem P, and P is incorrect. Finding out that R1 is mastered should convince you
that R2 is not mastered. But because R1 is d-separated from R2 by P, they are
conditionally independent in the network. Thus, evidence about R1 will not influence
the probability of mastery of R2 the way it should.

Student Modeling from Conventional Test Data

439

Counting
In desperation, we tried an extremely simple solution. We kept a score for each rule.
We incremented the score by 1 when a problem requiring the rule is correct. We
decremented the score by 1/N when a problem was incorrect and required N rules to
solve it correctly. The intuition is that if the problem is correct, then the rule must be
mastered. If it is incorrect, then at least one of the N rules required by it is incorrect,
but since we don’t know if it is this one, we blame each rule equally.
Because rules participate in differing numbers of problems, the ranges of the scores
can vary widely. Each rule needs a threshold that is unique to that rule. If the rule’s
score is above the threshold, the rule is predicted to be mastered.
In order to find values for all 66 thresholds, we used the artificial students to
generate scores on rules. For each rule, we adjusted its threshold to maximize the
rule’s utility.
We discovered that the best threshold was almost always the minimal possible
score for that rule. That is, a rule was considered mastered if any of the problems it
participated in were correct. This means that the counting technique is working
exactly like the noisy Or, without the noise. Therefore, it also misrepresents the
dependencies.

A success
Of all the methods we tried, the noisy-And seemed the most plausible since it
approximated the causality and it represented the conditional dependencies correctly.
The only problem was that most of the rules were overly sensitive to their prior
probabilities. In fact, they had posterior probabilities that were within .05 of their
prior probabilities.
We examined the 20 rules where priors were not affecting the posteriors. In all
cases, there was at least one problem that required either only that rule, or that rule
and one other. That is, there was at least one problem where the rule was used in
isolation (or nearly in isolation). Conversely, all rules that were used in isolation (that
is, one of the problems they appeared in had only one or two required rules) were
relatively unaffected by the priors. In Figure 1, rules 3 and 5 are “used in isolation”
because they are used in problem 3, which has only two rules required for correctly
solving it.
Finding that rules used in isolation were insensitive to the priors suggested that it
was the test that was responsible for lack of sensitivity, and not the assessment
method itself. This renewed our faith in the noisy And method.
When the noisy And apportions credit and blame, it usually spreads them thinly
among the rules, so that it hardly moves the rules’ probabilities from their priors. This
suggests looking at the difference (change) in probability as a way to compensate for
the small amount of credit/blame being handed out.
To implement this key insight, we modified the assessment function by giving each
rule a threshold that was just above the prior. If the rule’s posterior is above the
threshold, then the rule is predicted to be mastered. The intuition is that in such cases,
the rule is receiving credit (albeit not much) for the student’s correct answers, and is

440

Kurt VanLehn et al.

2.500
2.000
1.500
1.000
0.500
0.000
0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 0.54
threshold
accuracy

coverage

utility

Fig. 2. Choosing a threshold to maximize utility
thus probably known. Because the thresholds are functions of the priors, it shouldn’t
make any difference what priors are used. Thus, the predictions should be
independent of the priors.
Setting the thresholds
For each rule, we tried varying the threshold slightly around the prior. Figure 2 shows
the result of varying the threshold on a typical rule (not one of the ones that appears in
isolation) from –0.04 to +0.04 in steps of 0.01 about the prior, which was 0.5 in this
case. As can be seen, the coverage varies from 1 to 0 while the accuracy varies from
0.5 to 1. Utility, which was defined as coverage + 2*accuracy, peaks at 0.51, so that
is the threshold chosen.
Notice that plateau in accuracy. If we tried to use accuracy alone in order to select
a threshold, all values for the threshold above 0.51 would be tied. However, by
adding in a little bit of coverage to the utility function, we can make it peak. For this
particular rule, if utility is defined as coverage + M*accuracy and M>1.8, then there is
a peak at 0.51. The peak is at 0.50 if 1.6<M<1.8. There is no peak in utility if
M<1.6. Instead, there is a plateau from 0 to 0.49. Since every rule’s curve was
similar to this one, values for M of 2 or more (i.e., accuracy is twice as important as
coverage) caused the peak in utility to be slightly above the prior. Values of M
between 1 and 2 would cause utility to peak at the prior itself, whereas values of M
less than 1 (i.e., coverage is more important than accuracy) yielded plateaus rather
than peaks. We defined utility with M=2 since that tended to maximize accuracy
without affecting coverage very much (see Figure 2).
The rules that appeared in isolation had flat curves for accuracy and coverage in the
vicinity of the prior, so we gave them thresholds that were the priors themselves.
Their accuracy and coverage were both close to 1.

Student Modeling from Conventional Test Data

441

Setting the prior, slip and guess parameters
The new noisy And has three global parameters, namely the prior, the slip and the
guess. To find values for these parameters, we searched the parameter space for
values that would maximize utility given an initial set of 201 artificial students.
The procedure was to select values for the 3 global parameters, run the noisy And,
collect posteriors on each rule for each student, select rule thresholds that maximized
utility for that rule, then sum the overall utility.
We found that prior = 0.5, guess = 0.3 and slip = 0.01 yield maximal values.
However, we also found that utility was relatively insensitive to all three values.
Varying the set of artificial students
However, we worried that these results could be idiosyncratic to the particular set of
201 artificial students used to produce them. Therefore, we generated new sets of
artificial students, and compared the utilities obtained by assessing them to the
utilities obtained from the original 201 students.
The first set was 400 artificial students generated with the same values for the
guess and slip parameters. Predicted levels of mastery were generated for each of the
400 student’s rules, and the prediction’s accuracy, coverage and utility were
calculated. The mean utility for the 400-student set was 2.465, and the mean utility for
the original 201-student set was 2.372. Although statistically reliable (T-test,
p<.0001) due to the large number of data points, the difference in means was only 4%.
The next set was 201 artificial students generated by raising the value of the slip
parameter to 0.1. Again, there was a small (1%) but reliable difference (p < .01).
Similarly, lowering guess to 0.1 and keeping slip at 0.1 generated a small (1%) but
reliable difference (p < .001).
Thus, the results seem relatively insensitive to the specifics of the set of artificial
students used for evaluation. Regardless of how the artificial students use to evaluate
the test scoring method were generated, the test scoring method yielded high accuracy
and moderate coverage.

Discussion
The problem discussed here is to infer the posterior probability of mastery of a set of
N rules given a test with M items, where each item has been scored as correct or
incorrect. We are given a Q-matrix, which indicates which rules are required for
correct answers to each problem.
Since we do not have prior probabilities of mastery, it seemed initially that we
could not use the Noisy-and approach. However, we were unable to find a method
that both represented the conditional dependencies that exist and did not require priors
until we noticed that the posterior probabilities of the noisy And did vary in response
to the data, but just did not move far from the priors. The network was passing out
credit and blame rather thinly. Thus, we used priors, but looked for small changes
away from the prior. If a rule’s posterior is above the prior by an amount specified by

442

Kurt VanLehn et al.

a rule-specific threshold, then the rule was interpreted as being mastered. Artificial
students were used to select thresholds that maximized accuracy and coverage.
We discovered that there were essentially two kinds of rules on the test. The rules
used in isolation were accurately assessed by virtually any combination of parameters.
For the other rules, the coverage and accuracy traded off, as shown in Figure 2.
Thresholds were picked so that the accuracy was always over 90% and often close to
100%, but the coverage was around 25%.
According to this evaluation, if the assessment says that the student has mastered a
rule, then it is probably right. On the other hand, if it says that the student has not
mastered a rule, then it will often be wrong. The latter should thus be interpreted as
“insufficient evidence to conclude mastery exists” instead of “non mastery.” We
experimented with Bayesian network-based methods for discriminating between lack
of evidence and evidence of non-mastery, but failed to find one that worked. This
would be a good topic for future work.
Bayesian methods have always had difficulty working with applications where
priors are not available. As textbook authors (e.g., Neopolitan, 1990) are fond of
pointing out, a prior of 0.5 on a true/false variable does not represent ignorance, but
instead represents certain knowledge that the variable in is true 50% of the time in the
population. Second order probabilities (Neopolitan, 1990), Dempster-Shafer (Pearl,
1988; Neopolitan, 1990) and other techniques have been used to represent ignorance,
but they have drawbacks of their own. As far as we know, using the difference
between priors and posteriors has never been tried before as a means of making
predictions insensitive to the priors. It only applies when the outcome nodes (the ones
whose posteriors are important) are the same as the nodes whose priors are unknown.
However, such networks are common in diagnostic applications such as student
modeling. Empirical testing with artificial data suggests that our technique is
successful for our application, but more work is needed to understand the properties
of this solution.
We embarked on this research in order to use a diagnostic test to provide priors for
Andes’ Bayesian network. Having evaluated the proposed diagnostic testing method
and finding that it produces accurate results with artificial students, we have analyzed
the test data from the 178 human students, found out which rules were mastered by
which students, and thus obtained the priors that Andes’ needs. Ironically, the method
that we proposed here could be used with Andes itself, thus removing the need for
priors. Our next step may be to try this and evaluate it with either artificial students.

Acknowledgements
This research is supported by ARPA’s Computer Aided Education and Training
Initiative under grant N660001-95-C-8367, by ONR’s Cognitive Science Division
under grant N00014-96-1-0260 and by AFOSR’s Artificial Intelligence Division
under grant F49620-96-1-0180. We would like to thank the rest of the Andes group
for their help.

Student Modeling from Conventional Test Data

443

References
Conati, C., Gertner, A., VanLehn, K. & Druzdzel, M. (1997). On-line student
modeling for coached problem solving using Bayesian networks. In A. Jameson,
C. Paris & C. Tasso, User Modeling: Proceedings of the Sixth International
conference, UM97. New York: Springer Wein.
DiBello, L. V., Stout, W.F. & Roussos, L. A. (1995) Unified cognitive/psychometric
diagnostic assessment likelihood-based classification techniques. In P. D.
Nichols, S. F. Chipman & R. L. Brennan (Eds.) Cognitively Diagnostic
Assessment. Hillsdale, NJ: Erlbaum.
Neopolitan, R. E. (1990). Probabilistic Reasoning in Expert Systems: Theory and
Algorithms. New York: Wiley.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo, CA: Morgan-Kaufman.
Samejima, F. (1995). A cognitive diagnosis method using latent trait models:
Competency space approach and its relationship with DiBello and Stout’s
unified cognitive-psychometric diagnosis model. In P. D. Nichols, S. F.
Chipman & R. L. Brennan (Eds.) Cognitively Diagnostic Assessment. Hillsdale,
NJ: Erlbaum.
Tatsuoka, K. K. (1990). Toward an integration of item-response theory and cognitive
error diagnoses. In N. Fredericksen, R. L. Glaser, A. M. Lesgold & M. G.
Shafto (Eds), Diagnostic Monitoring of Skill and Knowledge Acquisition.
Hillsdale, NJ: Erlbaum.
Tatsuoka, K. K. (1995). Architecture of knowledge structures and cognitive diagnosis:
A statistical pattern recognition and classification approach. In P. D. Nichols, S.
F. Chipman & R. L. Brennan (Eds.) Cognitively Diagnostic Assessment.
Hillsdale, NJ: Erlbaum.
VanLehn, K. (1996). Conceptual and meta learning during coached problem solving.
In C. Frasson, G. Gauthier and A. Lesgold (Eds.) ITS96: Proceeding of the Third
International Conference on Intelligent Tutoring Systems. New York: SpringerVerlag.

The Affective Meta-Tutoring Project: Lessons Learned
Kurt VanLehn1, Winslow Burleson1, Sylvie Girard2,
Maria Elena Chavez-Echeagaray1, Javier Gonzalez-Sanchez1,
Yoalli Hidalgo-Pontet1, and Lishan Zhang1
1
Arizona State University, Tempe, AZ, USA
{kurt.vanlehn,winslow.burleson,mchaveze,javiergs,
yoalli.hidalgopontet,Lishan.zhang}@asu.edu
2
University of Birmingham, Birmingham, UK
s.a.girard@bham.ac.uk

Abstract. The Affective Meta-Tutoring system is comprised of (1) a tutor that
teaches system dynamics modeling, (2) a meta-tutor that teaches good strategies
for learning how to model from the tutor, and (3) an affective learning
companion that encourages students to use the learning strategy that the metatutor teaches. The affective learning companion’s messages are selected by
using physiological sensors and log data to determine the student’s affective
state. Evaluations compared the learning gains of three conditions: the tutor
alone, the tutor plus meta-tutor and the tutor, meta-tutor and affective learning
companion.
Keywords: Tutoring, meta-tutoring, learning strategies, affective learning
companion, and affective physiological sensors.

1

Introduction

A learning strategy is a method used by a student for studying a task domain and
doing exercises; a good learning strategy tends to increase the learning of students
who follow it, whereas a poor learning strategy tends to decrease learning. A learning
strategy is a kind of meta-strategy or meta-cognition. That is, it is knowledge about
knowledge acquisition. For example, when studying a worked example, a good
learning strategy is to self-explain every line of the example [1]. When working on a
tutoring system that gives hints, a good learning strategy is to ask for hints when and
only when one is unsure about what to do [2].
A perennial problem is that after students have mastered a learning strategy, they
may still choose not to use it [3]. The AMT (Affective Meta-Tutoring) project tested
whether an affective learning companion (ALC) could persuade students who were
taught a learning strategy to continue using it after instruction in the learning strategy
had ceased. The project built a system composed of four modules:
• An editor, which was used by students to take the steps needed to solve problems.
• A tutor, which taught students a problem-solving skill by giving hints and
feedback on each step as the problem is being solved.
S. Trausan-Matu et al. (Eds.): ITS 2014, LNCS 8474, pp. 84–93, 2014.
© Springer International Publishing Switzerland 2014

The Affective Meta-Tutoring Project

85

• A meta-tutor, which taught a learning strategy by giving hints and feedback about
it as the students’ used the tutor.
• An affective learning companion, having the goal of persuading students to use the
learning strategy even after the meta-tutor is turned off.
The evaluation of the system focused on students’ learning gains. We
hypothesized that when ranked by learning gains, the three conditions we studied
would exhibit this pattern:
tutor < meta-tutor + tutor < ALC + meta-tutor + tutor
We also tested whether students instructed with the affective pedagogical agent
persisted in using the learning strategy when the meta-tutoring ceased.
This paper summarizes the AMT system and its evaluation, and concludes by
discussing similar work. Many details are suppressed in order to keep the paper short,
but can be found in the project publication referenced herein.

2

The Task Domain: System Dynamics Modeling

Recent standards for K-12 science and math education have emphasized the
importance of teaching students to engage in modeling [4, 5]. Although “modeling”
can mean many different things [6], we are interested in teaching students to construct
models of systems that change over time (dynamic systems) where the model is
expressed in a graphical language that is equivalent to sets of ordinary temporal
differential equations.
Stella (www.iseesystems.com), Vensim (vensim.com),
Powersim (hwww.powersim.com) and similar graphical model editors are now widely
used in education as well as industry and science. Much is known about students’
difficulties with “systems thinking” and how it improves when students learn how to
construct models [6]. The practical importance and strong research base motivated
our choice of task domain.
However, even with kid-friendly editors [7], students still require a long time (tens
of hours) to acquire even minimal competence in the task. Most science and math
classes cannot afford to dedicate this amount of time to learning a modeling tool, so
this path to deeper understanding of systems, too often, remains closed. One of the
long-term practical goals of this work is to reduce the time-to-mastery from tens of
hours to just an hour or two.

3

The AMT System

This section introduces the main parts of the AMT system: the editor, tutor, metatutor and ALC.
3.1

The Model Editor

The model editor had two tabs. One presented the problem to be solved, such as:

86

K. VanLehn et al.

A bottle initially holds 100 bacteria. They grow quickly. On average, 40% of
the bacteria reproduce each hour, each creating one new bacterium. Graph
the number of bacteria in
n the bottle each hour.
The second tab was for co
onstructing the model, which was done by creating noodes
(see Figure 1). Each node represented a quantity. A node was defined in 3 steeps,
each achieved by filling ou
ut a form. The first step in defining a node was selectinng a
quantity from a large menu
u that included both relevant and irrelevant quantities. T
The
second step (which was acctually split into two forms) required qualitative plannning
and consisted of deciding whether the quantity was a numerical constant (e.g., the
bacteria birthrate is 0.4), a quantity that was a function of other quantities (e.g., the
number of bacteria births per hour is a function of the bacteria birthrate and the
current bacteria population)), or a quantity that changed by a specified amount per uunit
time (e.g., bacteria populattion increased each hour by the number of bacteria birrths
per hour). The third step was
w stating a specific formula for calculating the quanntity
represented by the node.

Fig. 1. Model for the
t bacteria problem, and graph of node “population”

When students had finisshed constructing a model, they could click on a buttonn to
execute it. This added a grraph to each node, which students could see by clickingg on
the node (See Figure 1). Even
E
if the tutoring system was turned off, students w
were
given feedback on the corrrectness of their graphs. They could see both their grraph
and the correct graph. Thee little “g” circle inside the node icons (see Figure 1) w
was
green if the students’ graph
h matched the correct graph and red otherwise. Studeents
could go on to the next prob
blem only when all the nodes’ graphs were correct.
3.2

The Tutor

d on, students could get minimal feedback and bottom out
When the tutor was turned
hints. When they were filliing out forms, the student could click on a button labellled
“Check.” This would colo
or the student’s entries either green for correct or red for
incorrect. The color codin
ng comprised minimal feedback on correctness. Studeents
could also click on a button
n labelled “Solve it for me.” It would finish filling in the
form, but would also color the entries yellow. This comprised a bottom-out hint. In
order to discourage overusee of the Solve-it-for-me button, when the student finished

The Affective Meta-Tutoring Project

87

editing a node, the status of their work was visible as colors on the little circles inside
the node icons (“i” means input; “c” means calculation).
When the tutor was turned off, its feedback and hints were disabled. In particular,
the Solve-it-for-me button was always disabled, and the Check button was disabled on
all forms except the first one. The Check button was enabled during the first step
because the system needed to know which node the student was trying to define so
that it could associate a correct graph with the node.
3.3

The Meta-Tutor

When the meta-tutor was turned off, students tended to first define nodes for all the
numbers in the problem statement, even if the numbers were irrelevant. Next, they
tried to guess definitions of more nodes using keywords such as “initially,” “increase”
or “altogether.” Sometimes they used methodical guessing. Indeed, some students
seldom looked at the tab containing the text of the problem. These represent a few of
the practices called “shallow modeling” in the literature [6]. The purpose of the metatutor is to prevent shallow modelling and encourage deep, thoughtful modeling.
Inspired by the success of the Pyrenees meta-tutor [8], our meta-tutor explicitly
taught students a general goal decomposition method. For the students’ benefit, we
called it the Target Node Strategy and described it as follows:
1. Pick the quantity that the problem asks you to graph, create a node for it, and call it
the target node.
2. Define the target node completely. If the node needs some inputs that haven’t been
defined yet, create those nodes but don’t bother filling them in yet. Return to
working on the target node, and don’t stop until it’s finished.
3. When the target node is finished, if there are nodes that have been created but not
defined, then pick any of them as the new target node, and go to step 2. If every
node has been defined, then the model is complete and you can execute it.
When the meta-tutor was on, it required the student to follow the Target Node
Strategy. It also complained if the students overused the Solve-it-for-me or Check
buttons, just as other meta-tutors do [9]. The meta-tutor also advised students on how
to debug models (e.g., if several nodes have incorrect graphs, examine first those
whose input nodes have correct graphs). We use the term “meta-strategy” to refer to
this whole collection of strategic advice about how to use the tutor and the editor.
3.4

The Affective Learning Companion (ALC)

The main job of the ALC was to persuade students that the meta-strategy was worth
their time and effort, and thus they should use it frequently not only when the metatutor was nagging them, but also when the meta-tutor and the ALC were turned off.
To achieve this persuasion, we used both affect-based and motivation-based designs
for the agent and its behavior. These designs are discussed in the order in which they
were encountered by the student.

88

K. VanLehn et al.

Following Dweck and others [10], all students began their training by reading a
brief text introducing the “mind is a muscle” paradigm: the more you exercise your
mind, the more competent you become. The ALC often referred to this concept,
whereas the non-ALC interventions never mentioned it again.
After reading the “mind is a muscle” passage, students in the ALC condition first
encountered the agent. The agent’s appearance and initial behavior were designed to
help establish rapport with the student. Following Gulz [11], the agent was a cartoon
of a human. Following Arroyo et al. [12], its gender matched the gender of the
student. Given the mixed results of D’Mello and Graesser [13], the agent display a
fixed neutral expression. Following Gulz et al. [14], the agent introduced him/herself,
and engaged the student in light conversation about the student’s interests. The
agent’s dialogue turns were text, and the student’s turns were selected from menus.
The student’s next activity was to study a series of PowerPoint slides interwoven
with simple exercises. These taught the basics of modeling and the user interface.
This activity was the same for both the ALC intervention and the non-ALC
intervention, and the agent was absent during it.
When the student had finished the introduction and was about to begin problem
solving with the tutor, the ALC appeared and expressed enthusiasm about the
upcoming challenges. It also reminded the student that the “mind is a muscle.”
Once the student began solving a problem, the ALC “spoke” via a pop-up text
approximately once a minute. If the student was practicing deep modeling frequently,
then the agent remained silent.
When the agent “spoke,” its message was selected based on log data and
physiological sensor data that were interpreted by machine-learned models. The
sensors were a facial expression camera and a posture-sensing chair. The sensor data
were cleaned, synched and input to a regression model that predicted the student’s
emotional state. The emotional state and the output of the log data detectors drove a
decision tree that selected one of the following 7 categories, whose message was then
presented to the student:
1. Good Modeling: Students exhibit frequent deep modeling behaviors and low
variation among affective states. ALC: “You really nailed it efficiently! It seems
like you are using the strategy and that all your efforts are helping you to make
strong connections in your brain. Nice work!”
2. Engaged: Students make few errors and show high level of excitement and
confidence. ALC: “That’s it! By spending time and effort verifying your answers
and planning ahead as you use the strategy, your brain is creating more connections
that will help you in your future modeling.”
3. New Understanding: Students show some shallow behaviors without making too
many errors, and some may show some frustration. ALC: “You’re getting good at
this. Planning ahead is the way to go. I can almost see the connections forming in
your brain.”
4. Inconsistent: Students make many shallow behaviors and show high level of
frustration. ALC: “Remember to stay focused and use the strategy and your plan.
Your actions seem to be inconsistent with the plan you picked earlier. If you

The Affective Meta-Tutoring Project

89

planned on having a <fixed value> node, then why are you trying to create a
<function>? It’s OK; sometimes it can be confusing; just remember to always try
to do your best…”
5. Guessing: Students enter several answers before getting one correct, perform many
shallow behaviors, and show low level of excitement: “Sometimes one must guess.
But even if you’ve been guessing recently, try to figure out why the response that
got green was correct. That way you can get there faster next time without
guessing.”
6. Fluttering, Confused, Lost: Students make many errors. While the student
sometimes refers to instructions and the problem, the student only uses these
features when stuck, not when planning the modeling activity. ALC: “You seem a
little lost. Sometimes these activities can be confusing. Do you think you can go
back to the strategy and use it to make a plan about the best way to spend your
effort? This will probably help you make progress.”
7. Boredom: Students make some errors and show consistently low level of interest.
ALC: “If this activity seems boring, why not turn it into a game to make it more
fun? For instance, do you think you can finish a node while getting green on your
first try at every tab?”
The ALC messages quoted above were the ones presented initially. If the same
message needed to be presented later, one of 10 short versions was presented instead.
When students finished a problem, a rectangular bar appeared alongside the agent
in order to reify the student’s meta-cognitive performance, following [15, 16]. The
bar was divided into three segments that displayed the proportion of student actions
that were deep (green), shallow (red) or neutral (yellow). The modeling depth bar
was intended to shift students’ motivational focus from correctness (the red/green
coding of the tutor) to effort (the red/green coding of the bar). After the ALC
explained what the bar meant, it presented a message based on a 6-way categorization
that took into account the student’s behavior throughout the solving of the problem
[17]. The student was then prompted to begin the next problem.
When the training phase was completed, the ALC appeared for the last time and
encouraged the student to continue to use deep modeling practice in the forthcoming
transfer phase.
The ALC’s messages turned out to be mostly motivational and meta-cognitive.
The messages were designed “bottom up” by experienced human coders who were
familiar with the affect and motivation literature. The messages were tailored to fit
the student’s state as the coders interpreted it rather than to cleave precisely to one
affect/motivation theory or another.
However, the ALC did choose which message to present on the basis of the
student’s affective state, as detected by the sensors. As advocated by [18], some
messages probably work best if they were delivered only in some affective states.
For instance, criticizing the students’ effort when they are frustrated may cause
disengagement, but the same message delivered to a bored student might have a better
chance at re-engaging them.

90

4

K. VanLehn et al.

Evaluation

This section reports the outcomes (main results) of our experiments evaluating the
meta-tutor (studies 3, 4 and 5) and the ALC (studies 6 and 7). Studies 1 and 2 were
pilot studies that involved only the editor and the tutor, and will not be discussed here.
4.1

Methods

Procedure: All five experiments used the same procedure. There were two phases: A
75 minute training phase and a 30 minute transfer phase. During the training phase,
all students studied PowerPoint slides which introduced them to system dynamics
modeling, the model editor and the Target Node Strategy. They also engaged in a
series of training problems of increasing complexity. The Check and Solve-it-for-me
buttons were available to give them feedback and demonstrations, respectively, on
each step in constructing a model. During the transfer phase, the tutor, meta-tutor and
ALC were all turned off. Thus, the transfer phase allowed students to display both
competence at system dynamics modeling and the Target Node Strategy.
Design: Students were randomly assigned to treatment groups. The treatment
manipulation occurred only during the training phase and only while the students
were solving problems. There were three treatment conditions: tutor alone; tutor +
meta-tutor and tutor + meta-tutor + ALC.
Measures: The studies used basically the same measures, although there were
improvements as the studies progressed. There were three types of measures, which
were all calculated from log data:
• Efficiency: How much modeling were students able to complete in a fixed period
of time?
• Error rate: How many mistakes did students make when defining a node? How
often did they get green (correct) the first time they clicked the Check button?
• Modeling depth: Did students use deep or shallow modeling practices?
─ How frequently did students guess or otherwise “game the system?”
─ How frequently were their actions consistent with the Target Node Strategy?
─ How frequently did students refer to the problem statement?
─ How frequently did students refer back to the introductory PowerPoint slides?
─ How many irrelevant nodes did students create?
─ How many episodes were classified as deep by the log data detectors?
Participants: Because we aimed at evaluating affective interventions, we conducted
the studies (except 6) in a classroom context, namely ASU summer schools for high
school students. ASU summer school classes always had between 40 and 50 students
each. Background questionnaires indicated that students varied in their mathematical
preparation from Algebra I to Calculus. We attempted to deal with the high incoming
variance using co-variants (studies 3, 4 and 5) and stratified sampling (studies 6 and 7).

The Affective Meta-Tutoring Project

91

Nonetheless, the high variance in incoming attributes and the limited number of
participants resulted in our studies being underpowered, which partly explains why
several tests presented below turned out to be statistically unreliable.
4.2

Results of Comparing Meta-Tutor + Tutor to Tutor Alone

Studies 3, 4 and 5, which are fully described in [19], evaluate the impact of metatutoring using two treatment groups. The experimental group had both the meta-tutor
and tutor turned on, whereas the control group had the meta-tutor turned off leaving
only the tutor active. Our three main hypotheses and their evaluations follow.
During the training phase, meta-tutoring should improve students’ efficiency, error
rate and depth of modeling. In all three studies, on almost all measures, the results
were in the expected direction, but the differences were statistically reliable only
about half the time. The results for efficiency were weakest, probably because
guessing often took less time than thinking hard. On the whole, we conclude that
meta-tutoring probably did improve training phase performance.
During the transfer phase, efficiency and error rate should be better for the metatutored group because they should have acquired more skill in modeling during the
training phase. Although there were weak trends in the expected direction, only one
of the depth measures showed a statistically reliable difference. We conclude that
meta-tutoring did not improve transfer phase performance enough to be detectable.
During the transfer phase, the meta-tutored group should not use deep model
practices more frequently than the control group because the meta-tutor merely nags;
it is the job of the ALC to persuade students to keep using deep modeling practices.
This hypothesis predicts a null result, which was observed with all measures in all
experiments, but the low power prevents drawing any conclusion from the null
results.
4.3

Results of Adding the ALC to the Meta-Tutor + Tutor

Study 6 evaluated a preliminary version of the ALC that only intervened between
modeling tasks and was not driven by the physiological sensors. None of the Study 6
measures showed benefits for this preliminary ALC compared to using the system
without the ALC. Unlike the other studies, this was a lab study with university
students intended mostly to collect data for calibrating the physiological sensors.
Study 7 compared the complete system to the same system with the ALC turned
off. Our findings were:
• During the training phase, the ALC group was better than the non-ALC group on
all measures, although the differences were reliable on only half the measures.
• During the transfer phase, the two groups tied on all error rate and efficiency
measures, suggesting that they both learned the same amount during training.
• Also during the transfer phase, the ALC group was not different from the non-ALC
group in this use of deep modeling practices.
Our interpretation of the results of Study 7 is that the ALC probably acted like an
improved meta-tutor. That is, during training, it caused students to use deeper

92

K. VanLehn et al.

modeling strategies, which increased their efficiency and decreased their error rates,
but did not apparently affect their learning very much, because their advantage over
the comparison group did not persist into the transfer phase. Although the AMT
project has made many contributions, this finding is perhaps the main result of the
project.

5

Discussion

While our studies were being conducted, other related studies were being done. There
are now 12 studies in the literature besides our own where an ALC acted somewhat
like ours [20], and only 4 had reliable main effects. Of them, 3 studies used
memorization tasks, and the fourth study confounded instructional information with
the affective intervention. On the other hand, all 8 studies with null effects used
complex tasks, as did our studies. It is tempting to hypothesize that ALCs work best
with simple, short tasks perhaps because there are more frequent opportunities for
interacting with the ALC between tasks.
Overall, the good news is that we have discovered improvements to meta-tutoring
that increase the frequency of deep modeling practices when the meta-tutoring is
operating. This is important because modeling is becoming a more central part of the
math and science standards, and students have strong tendencies to use shallow
modeling practices. Unfortunately, we have not yet found a way to get this improved
performance to persist when the meta-tutoring is turned off.
Another piece of good news is that students were able to achieve adequate
competence in constructing system dynamics models with only 75 minutes of
training. This is nearly an order of magnitude faster than earlier work with high
school students [6].
In one key respect, the ALC’s intervention could be improved. Our hypothesis
was that using the affect sensors and detectors would allow the ALC’s messages to be
presented at emotionally optimal times. However, we did not actually vary the time
of the messages enough. This would be a good topic for future work.
Acknowledgements. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References
1. Fonseca, B., Chi, M.T.H.: The self-explanation effect: A constructive learning activity. In:
Mayer, R.E., Alexander, P. (eds.) The Handbook of Research on Learning and Instruction,
pp. 296–321. Routledge, New York (2011)
2. Aleven, V., et al.: Help seeking and help design in interactive learning environments.
Review of Educational Research 73(2), 277–320 (2003)
3. Hattie, J., Biggs, J., Purdie, N.: Effects of learning skills interventions on student learning:
A meta-analysis of findings. Review of Educational Research 66, 99–136 (1996)
4. National, R.C.: A Framework for K-12 Science Education: Practices, Crosscutting
concepts, and Core Ideas. National Academies Press, Washington (2012)

The Affective Meta-Tutoring Project

93

5. CCSSO, The Common Core State Standards for Mathematics (2011),
http://www.corestandards.org (October 31, 2011)
6. VanLehn, K.: Model construction as a learning activity: A design space and review.
Interactive Learning Environments 21(4), 371–413 (2013)
7. Metcalf, S.J., Krajcik, J., Soloway, E.: Model-It: A design retrospective. In: Jacobson,
M.J., Kozma, R.B. (eds.) Innovations in Science and Mathematics Education: Advanced
Designs for Technologies of Learning, pp. 77–115 (2000)
8. Chi, M., VanLehn, K.: Meta-cognitive strategy instruction in intelligent tutoring systems:
How, when and why. Journal of Educational Technology and Society 13(1), 25–39 (2010)
9. Roll, I., et al.: Improving students’ help-seeking skills using metacognitive feedback in an
intelligent tutoring system. Learning and Instruction, 267–280 (2011)
10. Dweck, C.S., Leggett, E.L.: A social-cognitive approach to motivation and personality.
Psychological Review 95(2), 256–273 (1988)
11. Gulz, A.: Benefits of virtual characters in computer-based learning environments: Claims
and evidence. International Journal of Artificial Intelligence and Education 14(3), 313–334
(2004)
12. Arroyo, I., et al.: The impact of animated pedagogical agents on girls’ and boys’ emotions,
attitudes, behaviors and learning. In: International Conference on Advanced Learning
Technologies (ICALT 2011), Athens, Georgia (2011)
13. D’Mello, S., Lehman, B., Sullins, J., Daigle, R., Combs, R., Vogt, K., Perkins, L.,
Graesser, A.: A time for emoting: When affect-sensitivity is and isn’t effective at
promoting deep learning. In: Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010, Part I.
LNCS, vol. 6094, pp. 245–254. Springer, Heidelberg (2010)
14. Gulz, A., Haake, M., Silvervarg, A.: Extending a teachable agent with a social
conversation module – Effects on student experiences and learning. In: Biswas, G., Bull,
S., Kay, J., Mitrovic, A. (eds.) AIED 2011. LNCS, vol. 6738, pp. 106–114. Springer,
Heidelberg (2011)
15. Arroyo, I., et al.: Repairing disengagement with non-invasive interventions. In: Luckin, R.,
Koedinger, K.R., Greer, J. (eds.) Artificial Intelligence in Education, pp. 195–202. IOS
Press, Amsterdam (2007)
16. Walonoski, J.A., Heffernan, N.T.: Prevention of off-task gaming behavior in intelligent
tutoring systems. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 722–724. Springer, Heidelberg (2006)
17. Girard, S., Chavez-Echeagaray, M.E., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., Zhang,
L., Burleson, W., VanLehn, K.: Defining the behavior of an affective learning companion
in the affective meta-tutor project. In: Lane, H.C., Yacef, K., Mostow, J., Pavlik, P. (eds.)
AIED 2013. LNCS, vol. 7926, pp. 21–30. Springer, Heidelberg (2013)
18. D’Mello, S.K., Graesser, A.C.: Dynamics of affective states during complex learning.
Learning and Instruction 22, 145–157 (2012)
19. Zhang, L., et al.: Evaluation of a meta-tutor for constructing models of dynamic systems.
Computers & Education (in press)
20. Girard, S., et al.: How can Affect be used to improve the Learning outcomes of Interactive
Instructional Systems? ( in prep.)

Machine Learning 4, 99-106, 1989
© 1989 Kluwer Academic Publishers--Manufactured in The Netherlands

Efficient Specialization of Relational Concepts
KURT VANLEHN

(VANLEHN@PSY,CMU.EDU)

Departments of Psychology and Computer Science, Carnegie-Mellon University, Pittsburgh, PA 15213
USA
Editor: Thomas Dietterich
Keywords: concept induction, specialization, version spaces.
Abstract. An algorithm is presented for a common induction problem, the specialization of overly
general concepts. A concept is too general when it matches a negative example. The particular case
addressed here assumes that concepts are represented as conjunctions of positiye literals, that specialization is performed by conjoining literals to the overly general concept, and that the resulting specializations are to be as general as possible. Although the problem is NP-hard, there exists an algorithm,
based on manipulation of bit vectors, .that has provided good performance in practice.

In the process of including a concept from examples, it is sometimes necessary
to make a concept less general because the concept matches a negative example.
However, one usually wants to reduce the generality as little as possible. Thus,
the so-called specialization problem is to find all specializations of a given concept
that are maximally general and yet do not match the given negative example. This
paper provides an algorithm for a restricted class of specialization problems.
This particular specialization problem was uncovered during the development of
Sierra, a program that learns procedures from examples (VanLehn, 1987). Part of
Sierra's job included inducing concepts, expressed in a restricted version of the
predicate calculus, from positive and negative examples. In this respect, it was
much like Winston's (1975) famous arch learner. Sierra used Mitchell's (1982)
version space technique. Although Mitchell's presentation of the candidate elimination algorithm used propositional representations, it was straight forward to
adapt them to the first-order representation used by Sierra. Unfortunately, the
algorithms were very slow. The slowest one was the specialization algorithm that
was used in the course of updating the G set. With the help of Johan de Kleer, a
much faster algorithm was found. It allowed Sierra to reduce its learning time from
30 hours to a few minutes in some cases. This paper describes the particular
specialization problem that the algorithm solves, then presents the algorithm itself,
and finishes with a brief comment on the application of the algorithm to discimination learners, such as ID3 (Quinlan, 1986) and Prism (Langley, 1987).
Most of the speed in the algorithm is due to the particular representation of
concepts it uses, so the problem will be defined in terms of it. A concept is represented by a conjunction of positive literals, where all variables are interpreted
existentially. An example of a concept is:

100

K. VANLEHN

Exists (X, I1) such that color(X, red) & on(X, I1) &
abuts(bottom(X), top(Y))
where variables are capaitalized and constants are not. This concept would be true
of an example where there is a red block on top of a green block and a blue block
on top of the table. Because the logical form is always the same--conjunctions of
literals embedded in existential quantifiers--it is convenient to drop the logical
symbols and represent concepts as sets of literals. The above concept would be
represented as:

{color(X, red), on(X, Y), abuts(bottom(X), top(Y)))
Examples are represented in the same way as concepts except that variables are
not allowed in examples (i.e., they are ground sentences of first-order logic).
A concept g is said to match another concept or example s if there exists a
substitution for the variables of g such that every literal in g is equal to some literal
in s. To put it more succinctly, g matches s if and only if there exists a substitution
that makes g a subset of s. A concept is said to be consistent with a set of positive
and negative examples if it matches each of the positive examples and none of the
negative examples. Given two concepts, g and s, if g matches s, then g is said to
be a generalization of concept s, and s is said to be a specialization of g.
In this version of the specialization problem, the only type of specialization
allowed is the one just defined. Thus, the following concept would be a specialization of the one above:

{color(X, red), on(X, b17), abuts(bottom(X), top(bl7)), color(Z, blue)}
because one positive literal (the last one) has been added and one of the variables
has been turned into a constant.
Lastly, and most importantly, variables are assumed to designate distinct objects.
That is, there is an implicit inequality between each pair of variables so that
{p(X, Y)} really means {p(X, Y), X ~ II}. This implies that {p(X, Y)} is not a
generalization of {p(V, V)}.
The assumption that distinct variables designate distinct objects implies that the
set of all generalizations of a concept s corresponds to the set of all subsets of the
literals of s. To see this, suppose that g is an arbitrary generalization of s. This
means that g matches s, so there is some mapping of the variables of g into the
variables of s such that the image of g under the mapping is a subset of s. This
mapping must be a one-to-one mapping, because distinct variables must designate
distinct objects. Thus, the mapping between g and its image in s is one-to-one and
onto, which means that g is an alphabetic variant (i.e., the names of its variables
have been systematically changed) of some subset of s. This shows that every
generalization of s is isomorphic to some member of the power set of s.

SPECIALIZATION OF RELATIONAL CONCEPTS

101

This completes the definition of the representation language for concepts and
examples. It is a rather ordinary representation language, similar in most important
respects to the semantic net representations used by Winston and others. Given
this representation language, the specialization problem can now be stated.
In Mitchell's version space approach to concept formation, the set, V, of all
concepts consistent with the examples given so far (i.e., the version space) is
represented by two subsets of V, called G and S. The G set is the set of all maximally
general elements of V (i.e., for each g ~ G, there does not exist any element of
V more general than g). The S set is the set of all maximally specific elements of
V (i.e., for each s E s, there is no element of V more specific than s). Moreover,
Mitchell (1978) proves that G and S form the boundaries of an interval, so that
for each g in G, there is some concept s in S such that g is a generalization of s,
and for each concept s in S there is a concept g in G such that s is a specialization
of g.
The specialization problem to be discussed occurs when the G set must be
modified in order to make its members consistent with a newly received negative
example. Suppose a concept g E G matches the negative example. This means it
is too general and must be specialized. Given the assumptions above, specializing
it means finding literals from some s E S that can be added to g. This leads to the
following specialization problem:
Given:
n, a negative example,
g, a member of G that matches n, and
s, a member of S that does not match n and is a specialization of g.
Find a set of concepts C such that:
no concept in C matches n,
every concept in C is a specialization of g,
every concept in C is a generalization of s, and
no concept in C is a generalization of any other concept in C.
The G and S set often have multiple members, so the above problem may have
to be solved for more than one g/s pair and the resulting C sets must be merged
to form the new G set. 1
A direct, but inefficient way to solve the problem is to use a brute-force search
that adds a single literal from s to the evolving g, then checks to see if the resulting
concept matches n. If it still does, then the search continues. If the concept does
not match n, then the search has found one candidate for C, so it backtracks to
find more. The branching factor of the search is Isl - Igl where Ixl is the number
of literals in x. At one time, Sierra used this technique for updating its version
spaces. Since Isl - Igl ranged between 30 and 150, Sierra often took 30 hours or
more just to handle a few negative examples. With the new algorithm, Sierra
completes the same calculations in a minute or two.

102

K. VANLEHN

The algorithm uses the same search as before, and thus has the same branching
factor, but the search step is made significantly faster. The slowest step in the search
is checking to see whether a newly built concept matches the negative example.
However, this check always occurs between n and a concept that is some subset
of s. The algorithm takes advantage of this by precomputing a bit-vector representation that allows the fast parallel bit-vector computation provided by most
machines to be used in place of the slow matching step. Thus, although the search
still has the same exponential combinatorics as before, it is sped up by such a large
constant factor that the overall speed becomes quite acceptable. Since the problem
is NP-hard, it doubtful that a polynomial solution will be found.:
There are two steps to the initialization that precede the search. The first step
is to enumerate all substitutions of objects in n for variables in s. For example, if
there are four variables in s and seven objects in n, then there are 7 × 6 × 5 ×
4 substitutions? Having enumerated the substitutions, each literal in s is assigned
a bit vector. The bit vector has one bit position for each substitution. If the literal
is in n under a given substitution, then the bit is zero. If the literal is not in n, then
the bit is one. If any literal has a bit vector that is all ones, then it is not in n under
any substitution, so it does not match n. The result of this step of the initialization
is a set, call it s-pairs, that consists of the literals of s paired with their bit vectors.
The second step in the initialization is to convert g into a subset of s by finding
a substitution of its variables for variables in s. If there is more than one such
substitution, then the search must be run once for each substitution. Let g' be a
version of g with variables from s substituted for its original variable. Let g'-pairs
be a set consisting of the literals of g' paired with their bit vectors.
Attaching bit vectors to literals converts the matching problem into a well-known
problem, the set covering problem: Given a target set and a collection of subsets
of it, find a cover for the target set, where a cover is a set of subsets such that the
union of those subsets equals the target set. In this case, the target set is represented
by a bit vector that is all ones, and the collection of subsets is represented by the
bit vectors of s-pairs. The goal is to find a set, call it c-pairs, which is a superset
of g'-pairs and a subset of s-pairs, such that the logical Or of the bit vectors of
c-pairs is all ones. When the logical Or (union) of the bit vectors of c-pairs is all
ones, then the conjunction of literals of c-pairs does not match n because those
literals are not a subset of the literals of n under any substitution. Hence, this
conjunction is almost the concept needed: it is a specialization of g because it is a
superset of the literals of g'; it is a generalization of s because it is a subset of the
literals of s; and it does not match n. The only criterion left to meet is that the
concepts be maximally general with respect to each other. The criterion can be
partially met by using an appropriate set covering algorithm, which is the topic we
turn to next.
There are several different versions of the set covering problem, depending on
the kind of cover desired. An irredundant cover is a cover that is not properly
included in any other cover (i.e., none of its subsets is redundant in that it can be
removed from the cover without affecting the cover's equality to the target set).

103

SPECIALIZATION OF RELATIONAL CONCEPTS

Table 1. Well's algorithm for finding irredundant covers.
(Defun FindCover (Cover Covered Duplicates Usable)
(And Usable
(Let
( (Candidate (Car Usable) )
(NewCover (Cons Candidate Cover))
(NewCovered (LogicalOr Covered (BitVector Candidate) ) )
(NewDuplicates
(LogicalOr Duplicates (LogicalAnd Covered (BitVector Candidate)))
(NewUsable (Cdr Usable) )
(Cond
((For X in NewCover thereis
(AllOnes
(LogicalOr
(LogicalNot (BitVector X) )
NewDuplicates) ) ) )
(FindCover Cover Covered Duplicates NewUsable))
( (AllOnes NewCovered)
(Cons NewCover
(FindCover Cover Covered Duplicates NewUsable))
(T

(Append
(FindCover NewCover NewCovered NewDuplicates NewUsable)
(FindCover Cover Covered Duplicates NewUsable) ) ) ) ) ) ) )

Let C' stand for all possible irredundant covers of the all-one bit vector using
subsets of s-pairs and supersets of g'-pairs. Then the concepts represented by C'
are maximally general. If it were otherwise, then there would be two elements of
C', call them a and b, such that the literals of a are a proper subset of the literals
of b. But then b would not be an irredundant cover, because some of its elements
could be removed and yet the logical Or of the remaining bit vectors would still
be all ones. So C' represents concepts that are maximally general with respect to
themselves, are specializations of g' and generalizations of s, and do not match n.4
As noted earlier, the G set and the S set might have multiple members, and
each g might have multiple g'. In order to get a new G set, all the C' generated
from specific s/g' pairs must be merged.
In order to generate irredundant covers, Sierra uses an algorithm from Wells
(1971, section 6.4.3), which is based on depth-first search (see table 1). The algorithm keeps the cover generated so far in the variable Cover and the possible
cover elements that have not been used yet in the variable Usable. It also maintains
in the variable Cover a bit vector representing the substitutions that have been
covered so far. The trick to Well's algorithm is to prune the search whenever adding
a new cover element to the cover would cause the cover to become redundant. In
order to detect this, it maintains in the variable Duplicates a bit vector of the
substitutions that have been covered by two or more cover elements. This piece
of search pruning is achieved in the first cond clause of table 1. In order to calculate
C', FindCover is called with the following argument values:
Cover = g'-pairs
Covered = the bit vector that is the logical Or of the bit vectors of

g'-pairs

104

K. VANLEHN

Table 2. Concepts and bit vectors used during FindCover
X=bl
Y=b2

X=bl
Y=t

X=b2
Y=bl

X=b2
Y---t

X=t
Y=bl

on(X,Y)

0

1

1

0

1

1

table(Y)

1

0

1

0

1

1

red(X)

0

0

1

1

1

1

red(X),on(X,Y)

0

1

1

1

1

1

Concept

I

J

I yX=;t2

red(X),on(X,Y),table(Y)

1

1

1

1

1

1

red(X) ,table(Y)

1

0

1

1

1

1

Duplicates = the bit vector that is the logical Or of the bit vectors of g'-pairs
Usable = s-pairs with g'-pairs removed
As an illustration of the algorithm's execution, consider the following problem:
n = {block(bl), red(bl), block(b2), green(b2), table(t), on(bl,b2), on (bt,t)}
g' = {red(X)}
s = {red(X), on(X,Y), table(Y)}
The concept g' matches the negative example, and it is a specialization of s with
the same variables as s. Table 2 shows several concepts with their associated bit
vectors. The columns correspond to bits in the bit vectors. Each column is labeled
with the substitu6on that its bit position represents. Since there are two variables
in s, and three constants in n, there are 3 x 2 substitutions. The first three rows
of the table correspond to the elements of s-pairs. Notice that a bit is one if the
literal does not match n under the substitution corresponding to that bit position.
The algorithm is initialized with Cover equal to g'-pairs, whose only element corresponds to the third row in the table. This means that Covered is the bit vector
shown in the third row. Because Covered is not all ones, the search takes the first
element of Usable (which is initialized to the first two rows of the table), and
conjoins it with Cover. This produces the concept shown in the fourth row of the
table. This concept's vector is not all ones, so the algorithm again adds an element
of Usable, resulting in the concept shown in the fifth row. This concept's bit vector
is all ones, so it is saved for later output. The search continues, eventually producing
the sixth row in the table. However, all these later search paths fail because Usable
empties before the search finds any new covers. Thus, FindCover returns a singleton
list consisting of the cover corresponding to the fifth row of the table. Although
this example illustrates the bit-vector representation and the algorithm's flow of
control, it does not exhibit the search pruning caused by Duplicates.
As mentioned earlier, this algorithm was used with success as part of a version
space maintenance module. It may also be useful in inductive concept learners that
do discrimination learning, such as ID3 (Quinlan, 1986) and PRISM (Langley,

SPECIALIZATION OF RELATIONAL CONCEPTS

]_05

1987). These programs were initially developed with propositional concept representations, just as the version space algorithm was. The set covering technique
could be adapted to let them use the first-order representations defined above. The
following paragraphs sketch this application.
Suppose for the sake of illustration that the original version of the discrimination
learning problem is to build a concept that discriminates positive from negative
examples (i.e., binary discrimination, rather than multi-ary) and that concepts are
represented as feature sets. Then the main step in the search is to take a concept
(e.g., tree node) that is not yet capable of discriminating the positive from the
negative examples, find a single feature that discriminates as many positive from
negative examples as possible, and extend the concept with that feature.
Now suppose that concepts are represented in the first-order language defined
above. The discrimination learner's search would now work with literals rather
than features. However, adding a literal to a set of literals does not have the same
easily calculated effect on a concept's extension as adding a feature to a set of
features. Thus, the search would have to recalculate the extension of a concept
after selecting a literal and adding it to the concept. It could do this by matching
the new concept against everynegative and positive example in the data. Clearly,
this would be quite expensive.
However, the same bit-vector representation used above should help here as
well. In this case, there are multiple negative examples to match, so each bit position
stands for the result of matching to one example with one substitution. This will
make the bit vectors longer in this case than in the candidate elimination algorithm,
which processes one example at a time. However, the extension of a concept can
still be calculated by taking the logical Or of the old concept's bit vector and the
literal's bit vector. Presumably, this would still be faster than matching even if the
bit vectors were long. Obviously, .specialized parallel hardware could increase the
speed even more.

Notes
1. Merging the C set consists of more than a simple union, however. Although the definition of C
specifies that the concepts in C are maximally general with respect to each other, when multiple C
sets are unioned in order to get the new version of the G set, the result must be filtered one last
time in order to remove concepts that are specialized by others in the set. (It would be interesting
to see if leaving these "almost" maximally general concepts in the G set speeds up the candidate
elimination algorithm.)
2. As shown later, this problem can be converted to a set covering problem. In theorem 10.2.9 of Aho,
Hopcroft and Ullman (1974), it is shown that set covering is NP-complete.
3. Sierra's concept representation enforces additional constraints that reduces the number of substitutions still further. This reduction is crucial, because Sierra's concepts usually had between 10 and
50 variables. In retrospect, the same reduction could be achieved more elegantly by assigning types
to variables and objects, then enumerating only substitutions that paired objects and variables of
the same type.

106

K. VANLEHN

4. For version space maintenance, the definition of the G set implies the covering algorithm desired
will be one that finds irredundant covers. For other applications of this technique, one might consider
an algorithm for calculating minimal covers, which are covers with the fewest number of elements.
This would cause the specialization algorithm to prefer maximally "simple" concepts instead of
maximally general ones.

References
Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). The design and analysis of computer algorithms.
Reading, MA: Addison-Wesley.
Langley, P. (1987). A general theory of discrimination learning. In D. Klahr, P. Langley, & R. Neches
(Eds.), Production system models of learning and development. Cambridge, MA: MIT Press.
Mitchell, T. M. (1978). Version spaces: An approach to concept learnbzg (Tech. Rep. STAN-CS-78711). Palo Alto, CA: Stanford University, Computer Science Department.
Mitchell, T. M. (1982). Generalization as search. Artificial Intelligence, 18, 203-226.
Quinlan, J. R. (1986). The effect of noise on concept learning. In R. S. Michalski, J. G. Carbonell, &
T. M. Mitchell (Eds.), Machine learning: An artificial intelligence approach. (Vol. 2). Los Altos, CA:
Morgan Kaufmann.
VanLehn, K. (1987). Learning one subprocedt~re per lesson. Artificial Intelligence, 31, 1-40.
Wells, M. B. (1971). Elements of combinatorial computing. New York: Pergamon Press.
Winston, P. H. (1975). Learning structural descriptions from examples. In P. H. Winston (Ed.), The
psychology of computer vision. New York: McGraw-Hill.

Late-Breaking Work: Collaborative Technologies

#chi4good, CHI 2016, San Jose, CA, USA

Electronic Posters to Support
Formative Assessment
Salman Cheema
Arizona State University
Tempe, AZ, USA
salman.cheema@asu.edu

Daniel Pead
University of Nottingham
Nottingham, UK
daniel.pead@nottingham.ac.uk

Kurt VanLehn
Arizona State University
Tempe, AZ 85281, USA
Kurt.VanLehn@asu.edu

Alan Schoenfeld
University of California
Berkeley, CA, USA
alans@berkeley.edu

Hugh Burkhardt
University of Nottingham
Nottingham, UK
hugh.burkhardt@nottingham.ac.uk

Abstract
Formative Assessment is difficult to apply in real-world
classrooms due to the requirement for extensive interaction between students and teachers. We have constructed
a distributed system called FACT for in-class use that facilitates the use of popular Classroom Challenges (CCs) developed by the Mathematics Assessment Project. FACT lets
students work on Android tablets equipped with styli, and
enables a teacher to manage the class and to orchestrate
the activities required by the CCs. In this work, we discuss
the design of our system and our rationale for choosing its
interaction metaphors.

Author Keywords
Formative Assessment; Pen-based Interaction; Computer
Supported Collaborative Learning

ACM Classification Keywords

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
CHI’16 Extended Abstracts, May 07-12, 2016, San Jose, CA, USA
ACM 978-1-4503-4082-3/16/05.

http://dx.doi.org/10.1145/2851581.2892417

H.5.2 [User Interfaces]: Interaction Styles; H.5.3 [Group and
Organizational Interfaces]: Computer Supported Cooperative Work

Introduction
A Formative Assessment requires teachers to observe students during the course of learning activities and to provide useful feedback that guides students toward a better understanding of the subject material. This is differ-

1159

Late-Breaking Work: Collaborative Technologies

ent from a Summative Assessment which requires testing and relies on scores/grades to ascertain the level of
student learning. The Mathematics Assessment Project
(http://map.mathshell.org/) has developed 100 Classroom
Challenges (CCs) for use in middle and high school mathematics classrooms. These lessons cover a range of mathematical topics and provide rich collaborative activities for
students that are designed to expose common misconceptions. The Classroom Challenges and associated materials are freely available and have seen over 3 million downloads.
In this work, we present the FACT system which provides
a digital equivalent of the Classroom Challenges for use in
classrooms. This system has been tested multiple times in
middle school classrooms in California and Britain. FACT
uses both pen- and touch-input to support a rich set of
tasks that enable students to reason about mathematical
concepts in a natural manner. We first describe the existing paper-based CCs, then the design and workflow of the
FACT system, followed by a discussion of lessons learned.

Paper-based Classroom Challenges

Figure 1: Graph and interpretation
card sets taken from the
‘Interpreting Distance-Time
Graphs’ lesson.

The Classroom Challenges (CCs) developed by the Mathematics Assessment Project are structured as teacher manuals, containing a suggested lesson plan and all required
student and projector materials. Each CC has 3 stages: a
pre-assessment, a collaborative small group-based main
activity and a post-assessment, all taking place on different
days. The activities can be broadly split into two groups:
card arrangement and handwriting-intensive problem solving problems. The lesson plans also describe common errors and guidelines for teachers on how to address them
using formative questions. An example of a card arrangement CC is the ‘Interpreting Distance-Time Graphs’ lesson.
In this lesson, students are given cutouts of graph cards

#chi4good, CHI 2016, San Jose, CA, USA

and corresponding interpretations (See Figure 1), along
with poster paper and glue. Working in small groups of 24 people, students match each graph card to its correct
interpretation. The end product is a poster, which can be
shared with the class to discuss the approach taken by the
students.
Similarly, an example of a handwriting-intensive problem
solving CC is the ‘Sampling and Estimating: Counting Trees’
lesson. This lesson requires students to estimate the number of different types of trees in a tree farm without counting
them (See Figure 2). Students must choose an appropriate
sampling strategy and use that to estimate the number of
each type of tree. Students are required to write down the
reasoning behind their solutions and may also draw on the
provided tree farm.
The Classroom Challenges promote several classroom
practices that have been shown to improve long-term learning outcomes:
• Collaborative Learning, discussion and reflection[5]
• Emphasis on rich problem solving tasks[4]
• Rich formative feedback, followed by re-engagement
with tasks[1]
Recently, an external evaluation [3] found measurable improvements from using the CCs in Kentucky classrooms,
reporting learning gains on state exams equivalent to an
additional 4.6 months of schooling.

The FACT System
The Classroom Challenges (CCs) in their original format
are entirely paper-based, allowing them to be used in any
classroom without the need for technology. However, the
teachers’ workload is high because they are required to

1160

Late-Breaking Work: Collaborative Technologies

analyze complex student work and plan a productive comment upon it, all within seconds of walking up to a student
or group. To address these issues, we have developed the
FACT (Formative Assessment using Computational Technologies) system. FACT is intended to help teachers by
analyzing student work and making suggestions to teachers about what to say to students. To accomplish this, FACT
lets students solve the assigned problems by writing and
interacting with the lesson materials on an android tablet,
working either alone or in small groups as dictated by the
teacher. Similarly, the teacher can use her tablet to select a
lesson1 , assign an appropriate activity, create and manage
groups of students, collect student work, view it, provide
feedback on it, share it with the whole class and return it to
students. FACT uses the Samsung Galaxy Tab 10 (2014
edition) as its work platform, because this was one of the
better options combining stylus and touch input at the time
of the project’s inception. A central server application handles wireless communication between student and teacher
tablets and acts as data storage. The FACT prototype has
the following goals:

Figure 2: Tree farm and
associated questions from the
‘Sampling and Estimating:
Counting Trees’ lesson.

1. Do No Harm: FACT should not cause any loss of
learning opportunities compared to the paper version.
2. Generality: FACT should be applicable to a range of
tasks and activities, not just a few selected ones.
3. Teacher Assistance: FACT should record and analyze student work, identifying learning opportunities
and points of intervention for the teacher to provide
formative assessment.
4. Unconstrained Input: FACT should not restrict students’ ability to express themselves, so that it can
understand their work easily.
1
FACT lessons are the digital equivalent to paper-based Classroom
Challenges.

#chi4good, CHI 2016, San Jose, CA, USA

The fundamental unit of student work in FACT is a student
poster. Every FACT lesson is modeled as a sequence of
classroom activities, in which students edit posters, working alone or in small groups, as dictated by the lesson plan.
The FACT system includes 9 lessons converted from paperbased Classroom Challenges. These 9 lessons comprise
sufficient diversity to represent all the CCs and are among
the most-downloaded materials on the Mathematics Assessment Project’s website. For each activity in our chosen CCs, we have created an equivalent electronic poster.
Although students can write on the poster itself, most of
the work is done by writing on and/or arranging cards on
the poster. Figure 3 shows the poster for the ‘Interpreting
Distance-Time Graphs’ lesson in FACT.

The FACT Student Interface
The FACT student application presents a working area with
a menu bar when students log in (Shown in Figure 3). Students can log in simply by providing a username. The work
area contains two side-by-side viewports, with a left viewport for viewing prior work and a right viewport for editing
the currently assigned or opened poster. The left side of
the menu bar displays the username for the student, the
name of their current group, and a SYNC button. When the
teacher assigns an activity to the whole class, the SYNC
button on each student tablet is highlighted in red. Tapping
it downloads the appropriate poster (unedited) from the
FACT server. The right side of the menu bar has options for
changing the size and color of the pen, choosing an eraser,
and for undoing/redoing student actions. Additionally, there
is a drop down menu for adding new cards. FACT allows
students to add four different types of cards: unlined, lined,
graph and table cards (See Figure 4). FACT implements
access control at the card level to prevent simultaneous
edits to the same card by group members. The first group

1161

Late-Breaking Work: Collaborative Technologies

#chi4good, CHI 2016, San Jose, CA, USA

Figure 3: A screenshot of the FACT student application.

member to start editing the card2 locks it. Locked cards are
shown with a padlock icon to other group members. The
lock is automatically released after 5 seconds of inactivity.

Figure 4: Different types of cards
supported by the FACT system.
Each card includes controls for
pin/unpin, changing color, and
control points for resizing.

Students edit posters by using a combination of pen and
touch input. When working alone, all student actions are
transferred wirelessly to the FACT server and are saved in
its database. When working in groups, all student actions
are also transmitted to all their group members. This gives
group members the illusion of simultaneously editing the
same underlying poster. FACT automatically persists all
student work, in order to speed recovery in case of hardware failure due to broken styli, and/or discharged batteries.

The FACT Teacher Interface
The FACT teacher application has more functionality that
the student application. The teacher’s user interface consists of four different views: Activity, Organize, Review, and
My Sheet (See Figure 5). The teacher can switch between
2

Edits to a card include moving, resizing and writing.

different views by selecting a pane from the menu bar. The
Activity screen allows teachers to select an available lesson, displaying its activities in a tabular format (See Figure 7). Teachers can select an activity by tapping a row.
This causes the SYNC button to be highlighted for all students and groups in the class. The My Sheet screen allows
teachers to create their own solutions to any of the posters,
which can be shared with students at the teacher’s discretion. The Review screen allows teachers to download student work from the FACT server. They can then take the
teacher tablet home in order to provide comments on the
students’ work.
The Organize pane allows teachers to manage the state of
the classroom. They can create classes3 , organizations4 ,
and create, edit, and delete groups of students. The menu
bar also provides options for projecting the teacher’s screen
via screen mirroring and the ability to pause the entire
3
4

A class is a set of student names.
An organization is a set of groups within a class.

1162

Late-Breaking Work: Collaborative Technologies

Figure 6: A screenshot of the
Organize screen with an alert
available for a group card.

Figure 5: A screenshot of the Organize screen in the FACT
teacher application.

Figure 7: A screenshot of the
Activity screen showing the
activities in a selected lesson.

Figure 8: An alert message shown
to the teacher while peeking. The
region of the poster relevant to the
alert is highlighted in a dotted red
rectangle.

class. Students and groups are represented as cards (See
Figure 5). The teacher can drag and drop student cards
onto group cards to group them. Alternatively, teachers can
enable ‘Open Enrollment’ from the menu bar, allowing students to self-group. Disconnected or absent students are
shown as gray cards. Students who have not synced with
the assigned classroom activity are highlighted in an orange color. At any time, the teacher can tap on a student
or group card to peek into their work. The FACT server
includes a simple analysis system for card arrangement
posters. This module works by monitoring interesting regions in all card arrangement posters. It can detect incorrect pairings or placement of cards, map the mistake to an
underlying misconception and send an alert to the teacher
suggesting appropriate feedback to be shared with the student. Alerts are shown on the Organize screen as numeric
highlights on student and group cards (Shown in Figure 6).
To view the message associated with an alert, teachers
must peek into the student/group’s poster by tapping on its

#chi4good, CHI 2016, San Jose, CA, USA

card. While peeking, FACT shows the most recent alert to
the teacher. If the student/group is still working in the same
region as the mistake, the alert is rendered in blue color, indicating that it is still relevant. However, if the student/group
has moved to a different region5 , the alert is rendered in
a gray color, indicating that a teacher intervention might
cause a shift in engagement from the student/group’s current task. The choice to intervene is left to the teacher, in
order to maintain flexibility. Teachers can tap the alert icon
(See Figure 8) to see the detailed message to be displayed
on the student/group poster. They can then choose to send
the message, delete it or leave it on top of the queue. While
peeking, teachers can also directly write on the poster and
add their own cards. Teacher cards and pen utilize a shade
of red color that students cannot access, thus making the
distinction clear.

Discussion and Future Work
The FACT system is not a traditional Intelligent Tutoring
System [6] because it does not give feedback and hints
directly to the student. Instead, FACT aims to be an unobtrusive helper in a classroom, helping teachers in crafting
effective learning experiences, and utilizing technology to
identify interesting points of intervention. We see this as a
new design space for classroom systems. There has been
some work in this domain, with notable systems like Group
Scribbles [2], but existing approaches usually do not include
analysis of student work to highlight opportunities for intervention. The current version of the FACT system includes
simple analysis for card arrangement posters. We are already extending the range of the analysis module by incorporating handwriting recognition and Natural Language
Processing.
5
This is detected by examining the positions of student inputs (pen
and touch) with respect to the predefined regions of interest in the poster.

1163

Late-Breaking Work: Collaborative Technologies

We have conducted seven classroom trials during 2015 in
middle school classrooms in California and Britain. These
have enabled us to refine the teacher and student interfaces to their current form. One of the major drawbacks
of the current prototype is its pre-class setup time. Before
every class, the FACT application must be set up on approximately 30 tablets, which must be taken to a school
along with a server machine, a wireless router6 and a display adapter to support screen mirroring to a projector. This
is a logistical problem which is acceptable for a research
prototype but must be overcome to increase the chances
for large scale adoption.
Our trials indicate that the initial class setup7 takes 10-15
minutes in a 50 minute lesson, which is too high. In successive lessons, this time cost is negligible as the information
from the first session is preserved. To overcome the high
initial setup time, we have given teachers the ability to allow
‘Open Enrollment’ (See Figure 5), which allows students
to quickly choose groups themselves. The ability to pause
the entire class has also proved extremely popular in our
classroom trials, serving as a convenient way for teacher to
get students’ attention. We have also found that students
usually discover the user interface on their own by experimenting for a few minutes, indicating that the interface is
easy to use. For teachers, the abilities to pause the class,
projecting their screen and not having to deal with paper
materials have proven popular. Lastly, our analysis system,
though an early stage prototype, is clearly helpful to the
teachers in providing feedback to the students. This is the
major avenue of future work for the FACT system.

#chi4good, CHI 2016, San Jose, CA, USA

Acknowledgments
The FACT project is supported by the Bill and Melinda
Gates Foundation under grant OPP10612881.

References
[1] Paul Black and Dylan Wiliam. 1998. Assessment and
classroom learning. Assessment in education 5, 1
(1998), 7–74.
[2] Yannis Dimitriadis, Juan Ignacio Asensio-Pérez,
Davinia Hernáez-Leo, Jeremy Roschelle, John Brecht,
Deborah Tatar, S Raj Chaudhury, Chris DiGiano, and
Charles M Patton. 2007. From socially-mediated to
technology-mediated coordination: A study of design
tensions using Group Scribbles. In Proceedings of
the 8th international conference on Computer supported collaborative learning. International Society of
the Learning Sciences, 184–186.
[3] Joan Herman, Scott Epstein, Seth Leon, Deborah
La Torre Matrundola, Sarah Reber, and Kilchan Choi.
2015. Implementation and Effects of LDC and MDC in
Kentucky Districts. (2015).
[4] Alan H Schoenfeld. 1992. Learning to think mathematically: Problem solving, metacognition, and sense
making in mathematics. Handbook of research on
mathematics teaching and learning (1992), 334–370.
[5] Malcolm Swan. 2006. Collaborative learning in mathematics. A Challenge to our Beliefs (2006).
[6] Kurt Vanlehn. 2006. The behavior of tutoring systems.
International journal of artificial intelligence in education 16, 3 (2006), 227–265.

6
We use our own router due to variance in school wireless facilities
and to avoid conflicts with school IT policies.
7
Initial setup requires a teacher to create a class, create an organization, and create groups of students.

1164

Journal of Automated Reasoning 32: 187–226, 2004.
© 2004 Kluwer Academic Publishers. Printed in the Netherlands.

187

Abductive Theorem Proving for Analyzing
Student Explanations to Guide Feedback in
Intelligent Tutoring Systems
MAXIM MAKATCHEV, PAMELA W. JORDAN, and KURT VANLEHN
Learning Research and Development Center, University of Pittsburgh, Pittsburgh, PA 15260, U.S.A.
e-mail: {maxim,pjordan,vanlehn}@pitt.edu
Abstract. The Why2-Atlas tutoring system presents students with qualitative physics questions and
encourages them to explain their answers through natural language. Although there are inexpensive
techniques for analyzing explanations, we claim that better understanding is necessary for use within
tutoring systems. In this paper we motivate and describe how the system creates and uses a deeper
proof-based representation of student essays in order to provide students with substantive feedback
on their explanations. We describe in detail the abductive reasoner, Tacitus-lite+, that we use within
the tutoring system. We also discuss evaluation results for an early version of the Why2-Atlas system
and a subsequent evaluation of the theorem-proving module. We conclude with the discussion of
work in progress and additional future work for deriving more benefits from a proof-based approach
for tutoring applications.
Key words: intelligent tutoring systems, abductive reasoning, qualitative physics.

1. Introduction
Whereas most natural language explanations are produced and adapted to benefit or
inform a hearer, a self-explanation is produced for the benefit of the speaker. If there
is a hearer, he often already knows all about the topic, as is the case in a tutoring
context. Self-explanation is a cognitively valuable pedagogical activity because
it leads students to construct knowledge (Chi et al., 1994), and it can expose deep
misconceptions (Slotta et al., 1995). But it is difficult to encourage self-explanation
without giving the students substantive feedback on what they generate (Aleven
and Koedinger, 2000; Chi et al., 2001). To give substantive feedback, the system
has to be able to understand student explanations to some degree.
To study the problem of how to encourage students to productively self-explain,
we built the Why2-Atlas intelligent tutoring system and selected qualitative physics
as its domain of instruction. Qualitative physics is a worthy pedagogical goal because it is well known that college physics students are often unable to construct
acceptable answers for even simple qualitative physics questions. Students with top
grades in their physics classes get low scores on standardized measures of qualitative understanding, such as the Force Concepts Inventory (Hestenes et al., 1992).

188

MAXIM MAKATCHEV ET AL.

Question: Suppose you are running in a straight line at constant speed. You throw a pumpkin
straight up. Where will it land? Explain.
Explanation: Once the pumpkin leaves my hand, the horizontal force that I am exerting on it
no longer exists, only a vertical force (caused by my throwing it). As it reaches it’s maximum
height, gravity (exerted vertically downward) will cause the pumpkin to fall. Since no horizontal force acted on the pumpkin from the time it left my hand, it will fall at the same place
where it left my hands.
Figure 1. The statement of the problem and a verbatim explanation from a student who
received no follow-up discussions on any problems.

Qualitative physics problems emphasize physics concepts more so than problemsolving tactics and applications of equations. Because little progress has been made
at successfully improving qualitative understanding by altering physics classroom
instruction and because training involving qualitative physics problems does help
repair some persistent physics misconceptions (Hake, 1998), continued pursuit of
qualitative understanding by tutoring systems is worthwhile.
The Why2-Atlas system covers seven qualitative problems on introductory mechanics that are adapted from (Hewitt, 1998). When the system presents one of
these questions, it asks that the student type an answer and explanation, and it
informs the student it will analyze and discuss the final response with the student.
After the discussion, the system asks that the student revise the explanation, and
the cycle of explanation, revision, and follow-up discussion continues until all the
flaws in the student’s response have been addressed. One such problem is shown
in Figure 1. The student response shown is from our corpus of students’ problemsolving sessions and illustrates how challenging these explanations are for a system
to analyze for flaws that are worth discussing with a student. The dialogue shown
in Figure 2 is a portion of a follow-up dialogue between a student and a human
tutor for a flaw similar to that in Figure 1, while the dialogue shown in Figure 3
is a dialogue that the Why2-Atlas system held with a different student who is also
exhibiting a similar flaw during an early evaluation of the system.
One difficulty in analyzing students’ explanations for qualitative physics problems is that they require more than a single-sentence response and take the form of
short essays. Two experienced physics instructors provided us with detailed explanations for 10 qualitative problems as well as minimal, but complete, essays that
they would accept from students in an introductory physics course. The physics
instructors arrived at their explanations and essays independently and then met
and agreed on the minimum essays required. These reconciled, minimal essays
contained 6.9 sentences on average. So the system must be capable of analyzing
the relationships between sentences.
Statistical text classification approaches, such as latent semantic analysis
(Landauer et al., 1998) and naive Bayes (McCallum and Nigam, 1998), have shown
promise for assessing multisentential student explanations (Graesser et al., 2000;
Rosé et al., 2002) and are particularly desirable because, for training, they require

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

189

T: After it is thrown what is the horizontal velocity of the pumpkin?
S: zero
T: If the velocity changes from whatever it was to zero, then there should be a force on it?
S: no
T: Does a change in velocity mean acceleration?
S: yes
T: What is required to produce an acceleration?
S: force
T: Right. So, if the pumpkins horizontal velocity was same as that of the runner and then it
becomes zero when released then has there been a deceleration in it?
S: yes
T: Right. So there must be force which causes it?
S: yes
T: Ok.What is that force which reduces the horizontal velocity of the pumpkin to zero?
S: gravity
T: Let us see. What is the direction of the gravity?
S: vertical
T: Thats right. Can a vertical force cause a horizontal acceleration?
S: no
T: Right. So gravity cannot change the horizontal velocity of the pumpkin. can you see any
horizontal force on the pumpkin?
S: no
T: Right. Then can its horizontal velocity component change after it is released?
S: no
Figure 2. A verbatim follow-up discussion between a human tutor and another student
exhibiting the impetus misconception.

nothing more than natural language text from a particular domain and the identification of short texts that represent prescriptively good and bad descriptions against
which a student text can be compared. For instance, a bad description that should
match Figure 1 is the often-observed impetus misconception:
If there is no force on a moving object, it slows down.
These types of classification approaches create a statistically derived form of
semantics for a text relative to the training data but do so by treating language
as an unordered bag of words in which the organization of the words imparts no
meaning. Because these techniques fail to capture this additional semantics, they
are insensitive to a number of language phenomena that help distinguish between
good and bad explanations. First, they would clearly be insensitive to argument
ordering variations making the difference between “x accelerates faster than y”
and “y accelerates faster than x” undetectable. Negation is another phenomenon to
which these techniques are insensitive. For example, a good explanation followed

190

MAXIM MAKATCHEV ET AL.

Explanation: The pumpkin moves at a constant velocity in the horizonatal direction which is
zero. The only force moving the pumpkin is gravitational force. . .
T: I see a topic we should explore. Let’s consider what forces act on objects after they have
been pushed or thrown. Suppose a person is floating in deep space. The person, who is wearing
a space suit of course, pushes a wrench away. While the person is pushing the wrench, what
forces are acting on it?
S: Gravity.
T: Uhhh. . . Let’s assume that the stars and planets are all so far away that gravitational forces
are negligible. So the only force acting on the wrench is the pushing force exerted by the
person on the wrench. After the person has pushed the wrench, it floats away from her. Now
what forces are acting on the wrench?
S: The force given.
T: Let’s look at this more closely. Are there any other forces on the wrench after it leaves the
person? If so, what are they?
S: No.
T: Fine. If there was one force acting on the wrench after the person has pushed it away, how
would that force affect the wrench’s velocity?
S: I don’t know.
T: Let’s consider this then. If you exert even a small force on a stationary billiard ball, it
accelerates and begins to move. If a stationary rocket ship turns on its thrusters, it accelerates
and begins to move a little even if the thruster is weak. So if there is even a small force on the
wrench, shouldn’t it also accelerate?
S: Yes.
T: Okay. So if there is a force on the wrench due to the person after the wrench has left the
person, then the wrench should accelerate. Is it really reasonable for the wrench to speed up
all by itself after leaving the person? Have you ever seen a billiard ball speed up after being
hit? Or a hockey puck speed up after being shot?
S: No.
T: Fine. Because the wrench really doesn’t speed up after leaving the person, there is no force
acting on it. The force of the person’s push ends as soon as the person stops touching the
wrench. In general, when do pushing forces, pulling forces and other forces due to the contact
of two objects end?
S: When contact is over.
T: Well. . . When one objects exerts a contact force on another object, the force stops when
the contact stops. Thus, when the wrench leaves the person’s hand, the contact ends, so the
pushing force ends. In general, after a person or some other agent has started an object moving
but is no longer touching the object, does it still exert a force on the object?
S: Yes.
T: Uhhh. . . No, it does not. Except for field forces, like gravitational or magnetic force, forces
only exist between two objects when they are touching each other. Your essay seemed to imply
otherwise to me.
Figure 3. A verbatim student explanation and discussion with the Why2-Atlas system during
an early evaluation that was completed by 26 students.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

191

by “But I don’t think that will happen” would be classified as good because “not”
is too general to have a high information value.
A third, even more complex, language phenomenon to which these classification techniques are insensitive is anaphoric expressions (e.g., determining the
referent of a pronoun such as “that” in the previous example, or “it”). Although
other statistical techniques for pronominal anaphora resolution are highly effective (Strube et al., 2002; Ng and Cardie, 2002), they cannot be directly combined
with superficial, statistically derived semantics approaches. As an example of the
problem, consider the last clause of the essay shown above in Figure 1:
it will fall at the same place where it left my hands.
This clause would tend to be misclassified as the correct answer “The pumpkin
will land in my hands.” The reason is that the words “fall,” “my,” and “hands” have
a high information value relative to the expected answer, while the temporal and
nominal anaphora involved in “will fall” and “at the same place” do not. Hence,
these anaphoric expressions will be overlooked although they change the meaning
significantly in this case.
Fourth, the inferences captured by statistical semantics approaches are too weak.
In Figure 1, the student has the extreme belief that the pumpkin has no horizontal
velocity. This would probably not be recognized as a case of “slowing down” by
this type of statistical analysis. Even more difficult is that no horizontal velocity is not explicit; there is a multistep chain of inference involved that statistical
approaches are not equipped to handle. The chain of inference can be informally
expressed as “pumpkin’s final horizontal position = pumpkin’s initial horizontal position” → “pumpkin’s horizontal displacement is zero” → “pumpkin’s
horizontal velocity is zero.”
Furthermore, these statistical techniques are often too insensitive to recognize
that student statements are true but vague in cases where a few content words are
missing. In these cases the tutor should acknowledge the correct statement and
elicit more precision rather than continuing as if the statement were wrong or
accepting it without requiring more precision from the student. For example, if
a student makes a correct statement about an axial component of the velocity of an
object but does not report it in terms of the horizontal and vertical components of
the velocity, the tutor should ask which was intended.
Although additional preprocessing of the language and postprocessing of the
classifications can be done to alleviate some of the problems involved (Rosé et al.,
2002), there is no clear workaround for the problem of weak inferencing. To both
capture what are subtle differences to statistical semantics classification and address the problem of weak inferencing, we need the precision possible so far only
with approaches that try for a deeper understanding of the student’s reasoning.
The Geometry Explanation Tutor is an operational prototype that does a deeper
semantics classification (Aleven et al., 2001b, 2001a) of student utterances. It parses
a student explanation into a propositional representation using a syntactic grammar
and lexical semantics and then uses LOOM, a terminological knowledge represen-

192

MAXIM MAKATCHEV ET AL.

tation tool, to classify these relative to prescriptive categories that typically express
one proposition. This approach looks promising (Aleven et al., 2002), but the system’s goal is to elicit a justification for a single step in a geometry proof; generally,
such a justification can be expressed with a single sentence that succinctly translates into a small number of propositions. It isn’t clear that this approach will work
well for the longer, more complex explanations that the Why2-Atlas system elicits,
since it will largely overlook the intersentential, or discourse-level, meaning of the
text.
Our approach to the problem of recognizing inferential relationships between
sentences is to create a proof based on the student’s natural language essay and
then check the proof. Why2-Atlas parses student utterances into propositional representations. It uses a syntactic grammar and lexical semantics to create a representation for each sentence (Rosé et al., 2002) and then resolves temporal and nominal anaphora (Jordan and VanLehn, 2002). But instead of classifying the resulting
propositions relative to a terminological representation of the domain knowledge,
the Why2-Atlas system constructs proofs by using abductive reasoning. Abduction
is a process of reasoning from an observation to possible explanations for that
observation. In this application the observations are the propositions that represent
the student’s essay, and the proof is the abductive reasoning steps that explain the
propositions.
A proof-based approach gives more insight into the line of reasoning the student
may be following across multiple sentences because proofs of the propositions
should share subproofs. For example, consider the last sentence and part of the first
sentence of the essay in Figure 1. The sentences have the informal proof shown
in Figure 4, where the first column is a reference number for the proof step; the
second column is a gloss of a proposition that is in the student’s explanation, or
is inferred, or is given; and the third column is the rule or justification for the
proposition. The proof for the second sentence is steps 3–7, and the proof for the
first sentence is steps 1–4, so that the first sentence is a subproof that supports the
second. Moreover, subtle misconceptions such as impetus (as in step 5) are revealed
when they must be used to prove a student-supplied proposition.
The proof-based approach also opens the possibility of implementing interactive proof generation through a dialogue with the student. This interaction can serve
the dual purpose of revealing the conjectured argumentation behind the student’s
statement and disambiguating the student’s intended meaning when there are multiple proofs. For example, if there are two equally good proofs of a student statement,
where one involves a misconception about the relationship between force and velocity and the other involves a misunderstanding of when a particular force is
negligible, then we can use the structure of the proof to identify a possible series
of disambiguation questions. This example will be further developed in Sections 2
and 6.
Although we could use deductive inference as an approach for building and
checking proofs of student explanations, abductive inference is a better choice

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

Reference
Number
1
2

3
4
5
6
7

193

Proposition

Justification

before the release, the man is holding the pumpkin
the man exerts a nonzero horizontal
force on the pumpkin

given

after the release, nothing is is touching the pumpkin
after the release, the horizontal force
is zero
the pumpkin’s horizontal velocity is
zero

given

the pumpkin’s horizontal displacement is zero
the pumpkin’s initial & final positions are equal

if zero velocity then zero displacement
if zero displacement then initial and
final positions are equal

*if body1 & body2 in contact then
body1 exerts a nonzero force on
body2

if no contact then contact force is
zero
*if zero force then zero velocity (impetus)

Figure 4. An informal proof of the excerpt “Once the pumpkin leaves my hand, the horizontal
force that I am exerting on it no longer exists. . . . Since no horizontal force acted on the
pumpkin from the time it left my hand, it will fall at the same place where it left my hands”
(from the essay in Figure 1). Buggy justifications are preceded by an asterisk.

because we are performing a diagnostic task, and we must robustly and efficiently
deal with the ambiguity and vagueness introduced by natural language, students’
incomplete proofs, and an incomplete knowledge base.
Although the reasoning system we use within Why2-Atlas has some similarities
with other qualitative physics reasoning systems (Weld and de Kleer, 1990) in its
ontology and rules, their tasks are different. Most existing systems do the student’s
task: given a physical system, the reasoner can predict or explain the system’s
behavior deductively. In our case, the student essay is viewed as a fragmentary,
incomplete, and possibly incorrect proof. Our task is to complete that proof insofar
as possible.
In this paper we motivate and describe an abductive reasoning system that creates proof-based representations of student essays for tutorial applications. First
we give an overview of the Why2-Atlas tutoring system architecture to clarify
the context in which the abductive reasoner operates. As we describe the tutoring
system, we explain the pedagogical considerations that motivate how we use a
proof-based representation of a student’s essay and provide an example of how
a proof is built and used. We then motivate our choice of weighted abduction for
 Abductive inference has a long history in plan recognition, text understanding, and discourse

processing (Appelt and Pollack, 1992; Charniak, 1986; Hobbs et al., 1993; McRoy and Hirst, 1995;
Lascarides and Asher, 1991; Rayner and Alshawi, 1992).

194

MAXIM MAKATCHEV ET AL.

Figure 5. Why2-Atlas tutoring system architecture.

building proofs and explain in detail our abductive inference engine, Tacitus-lite+.
Next we present evaluation results for an early version of the Why2-Atlas system
and the results of a subsequent evaluation of the abductive reasoner using a test
suite of 45 student essays. Finally, we describe current work in progress and some
of our future plans for deriving additional benefits from a proof-based approach for
tutoring applications.

2. Building and Using Abductive Proofs
Our discussion in this paper focuses on building proofs using an abductive reasoner
where the input is a propositional representation of the student’s essay. In this
section, we describe the architecture of the entire system as background, so that
it is clearer how the input for the proof building is provided and what is done as
a result of analyzing the proof. Except for the abductive inference engine module,
none of the other system modules described in this section will be addressed in this
paper.
 We are using an extended version of SRI’s Tacitus-lite weighted abductive inference engine

(Hobbs et al., 1993) as our main tool for building abductive proofs.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

195

2.1. THE WHY 2- ATLAS TUTORING SYSTEM
The architecture for the current version of the Why2-Atlas qualitative physics tutoring system is shown in Figure 5. The user interface for the system is a screen
area in which the physics question is displayed along with an essay entry window
and a dialogue window. As the student enters an answer and explanation for a
qualitative physics question, the sentence-level understanding module builds sets
of propositions to represent sentences as the student enters them. The user interface and the sentence-level understanding components are described in detail in
(VanLehn et al., 2002; Rosé et al., 2002).
The sets of propositions are passed by the discourse manager to the discourselevel understanding module. Each set of propositions represents one interpretation
of a sentence. The discourse-level understanding module resolves anaphoric expressions and other language dependencies within the sentence representation as
described in (Jordan and VanLehn, 2002). It then uses domain reasoning rules and
the Tacitus-lite+ abductive inference engine to create a set of proofs.
The proofs that are produced represent the student’s knowledge and beliefs
about physics with respect to the problem to which the student is responding. One
difficulty that must be addressed is uncertainty about the beliefs and knowledge
that should be attributed to a student. This uncertainty arises because some of
the knowledge and beliefs about the student are inferred based on observed student actions or utterances (Zukerman and Albrecht, 2001). Thus, as with decisiontheoretic approaches (Murray and VanLehn, 2000; Keeney and Raiffa, 1976), the
system needs to reason about the utility of separately attributing each of these mutually exclusive representations of varying plausibility to the student. Tacitus-lite+
tries to estimate this by associating costs with the proofs it creates by weighted
abduction. In weighted abduction, weights are assigned to propositions in the bodies of the Horn clauses in order to compute the cost of assuming a proposition
without proof. Assuming a proposition is further referred to as abducing, and such
a proposition is called an assumption. Weighted abduction is explained in more
detail in Section 4.
Even with a mechanism for ascertaining the plausibility of alternative proofs,
there can still be multiple proofs that are considered equally good representations.
Hence, once proofs have been built, the discourse-level understanding module updates the history with the results from Tacitus-lite+ and selects the best proofs
to send to the tutorial strategist. The tutorial strategist poses relevant communicative goals for itself by analyzing proofs. Acquiring and reasoning about student
beliefs and knowledge are central issues addressed by work in student modeling.
A student model is a type of user model, and in general a user model provides
information the system can use in adapting to the needs of its user (Wahlster and
Kobsa, 1989). The Why2-Atlas system uses the proofs derived from the student’s
essay to identify effective communicative strategies and goals that will (1) effectively help students realize and correct their errors and misconceptions and (2)

196

MAXIM MAKATCHEV ET AL.

enable students to realize what reasoning is necessary when generating a complete
explanation.
Currently there are four categories of communicative goals. Two of these, disambiguating terminology and clarifying the essay, are addressed through directives to modify the essay. The other two, remediating misconceptions and eliciting
more complete explanations, are addressed through dialogue. Misconceptions are
detected when the proof includes a rule that is incorrect or inapplicable. Incompleteness is detected under two conditions. First, there may be multiple proofs
that are significantly different and equally plausible. This condition indicates that
the student did not say enough in an explanation for the system to decide which
proof best represents what the student’s reasoning may be. Each possible line of
reasoning could point to different underlying problems with the student’s physics
knowledge. The second condition occurs when the student fails to explicitly state
a mandatory point, which is a proposition that domain instructors require of any
acceptably complete essay. Once the tutorial strategist has identified communicative goals, it ranks them according to curriculum constraints and sends them to the
discourse manager. The discourse manager selects the highest-priority goal after
taking dialogue coherency into account and sends the goal to either the dialogue
engine or the sentence-level realization module.
In an educational context it is generally more effective if students discover
their own errors and misconceptions rather than always simply being told of the
error and its correction. Therefore, the dialogue engine initiates and carries out a
dialogue plan that will either help the student recognize and repair a misconception
or elicit a more complete explanation from the student. The main mechanism for
addressing these goals are what we call a knowledge construction dialogue (KCD)
specification. A KCD specification is a hand-authored push-down network. Nodes
in the KCD network are either the system’s assertions and questions to students
or pushes and pops to other networks. The links exiting a node correspond to
anticipated responses to the question. Each assertion and question are a canned
string, ready for presentation to a student. The dialogue engine is described in
detail in (Rosé et al., 2001).
If the tutorial strategist’s analysis of the proofs that represented the student’s
essay reveals a misconception or error, then the dialogue engine will engage the
student in a knowledge construction dialogue (KCD) that works through an analogous, but simplified, problem and summarizes at the end with a generalization
of the reasoning that the student is expected to transfer to the current problem.
If incompleteness is revealed by the analysis of the proof, then the system will
engage the student in a KCD that leads the student to express the missing detail by
reminding the student of an appropriate rule of physics, and a fact that is relevant
to the premise or conclusion of the rule, and then asking the results of applying the
rule.
Working through an analogous problem is currently the only technique implemented in the system for leading a student to recognize an error or misconception.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

197

Another possibility is to step through the reasoning associated with the current
problem and ask the student to fill in any missing details. Having some of these
details wrong may have led the student to draw a wrong conclusion, and with the
corrected details the student may be able to easily see the error. Other techniques
for dialogue strategies to correct misconceptions, errors, and incompleteness may
be derivable from argumentation strategies used in argument generation as described in (Zukerman et al., 2000) (e.g., reductio ad absurdum, premise to goal,
and reasoning by cases).
The other communicative goals, disambiguating terminology and clarifying the
essay, are addressed by the discourse manager as directives for the student to
modify the essay. It passes propositions and a goal to the sentence-level realization module, which uses templates to build the deep syntactic structures required
by the RealPro realizer (Lavoie and Rambow, 1997) for generating a string that
communicates the goal.
While a dialogue is in progress, the discourse-level understanding and tutorial
strategist modules are currently bypassed until the essay is revised. Once the student revises the essay, it is reanalyzed, and the cycle repeats until no additional
communicative goals arise from the system’s analysis of the essay.
2.2. EXAMPLES OF BUILDING AND USING PROOFS
The Tacitus-lite+ abductive reasoner currently has 105 qualitative physics rules
available to use in building proofs, where propositional representations of a student’s sentences are input as observations that are to be explained. These rules
cover seven problems as well as parts of many other problems. Figures 6 and 7 are
examples of two simplified alternative abductive proofs for sentence (1).
The pumpkin slows down.

(1)

For these examples, we take as given the fact that the air resistance is 0 and
that the runner is not applying a horizontal force to the pumpkin after he throws
it. Since students often overlook relevant givens, proofs that ignore these givens
can be considered as well whenever the given is represented by a rule and a buggy
counterpart is also included (as described in Section 3.4).
Each level of downward arrows from the gloss of a proposition in the two alternative proofs shown in Figures 6 and 7 represent a domain rule that can be used
to prove that proposition. To simplify the example, we assume that the weights in
all the rules are evenly divided between the propositions in the body of each rule.
The number in parentheses at the end of each proposition represents the cost of
abducing the proposition.
In both proofs shown in Figures 6 and 7, one way to prove that the velocity of the
pumpkin is decreasing is to infer, through the rule Imprecision, that the horizontal
component of the velocity vector was meant to be decreasing. The system will also
build alternative proofs in which it tries to prove that the student means the vertical

198

MAXIM MAKATCHEV ET AL.

Figure 6. Example of one possible simplified abductive proof for “The pumpkin slows down.”
Rule names are in italics; arrows are in the direction of abductive inference. Total cost of the
proof is .5.

Figure 7. Example of an alternative simplified abductive proof for “The pumpkin slows
down.” Rule names are in italics; arrows are in the direction of abductive inference. Total
cost of the proof is .5.

velocity instead (especially because, during certain time intervals, this is true), but
for this example we will ignore these other proofs.
Next we consider two ways of proving that the horizontal component is decreasing. First, we consider the case of the proof in Figure 6. In this case Tacitus-lite+
has selected a buggy physics rule that is one manifestation of the impetus miscon-

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

199

ception; the student thinks that a force is necessary to maintain a constant velocity.
In this proof it is abduced that the student has this bug at a cost of .5, and no further
attempts are made to prove it. Alternatively, the system could try to gather more
evidence that this is true by asking the student diagnostic questions.
Next, Tacitus-lite+ proves that the total force on the pumpkin is zero by proving
that the possible addend forces are zero. Since it is a given that air resistance is
negligible, this proposition unifies with this given fact for zero cost. Likewise, since
we said that it was also given that the man is applying a horizontal force of 0 to
the pumpkin after he throws it, this proposition unifies with the given fact for zero
cost as well. Since the proof contains just one assumption, that the student has the
impetus bug, the total cost of the proof is .5.
Looking again at the alternative proof in Figure 7, we see that it attempts to
prove the horizontal component of the velocity is decreasing by first trying to
prove that the horizontal component of the acceleration is nonzero in the direction
opposite the velocity. To prove this, we must prove that the total horizontal force
on the pumpkin is nonzero in the same direction as the acceleration. One approach
is to prove that at least one of the addend forces is nonzero. The system can ignore
either of the two givens at this point in order to try to prove that there is at least
one nonzero force on the pumpkin. In this case it tries to prove that wind resistance
is not negligible; but since it cannot prove this, the result must be abduced at a
cost of .5. So the total cost of this alternative proof is .5 as well. In this example,
the system now has two plausible proofs with no means of choosing between them
without more information from the student.
If the student had instead supplied the sentence
The pumpkin slows down because there is no horizontal force on it.

(2)

which provides some justification, the proof in Figure 6 would be preferred. This
is because the proposition representing this justification conflicts with the inferred
proposition in Figure 7 that the total horizontal force is nonzero. This new proposition will not add an additional cost to the proof in Figure 6 because it unifies with
a proposition that has already been proven.
In the case of the proof in Figure 6 the tutorial strategist identifies a dialogue
goal to address the impetus misconception, since an impetus bug assumption is part
of the proof. In the case of the proof in Figure 7 it identifies a goal to address the
wrong assumption that air resistance is nonnegligible.
In addition to identifying errors and misconceptions, the system can also give
some direct, constructive feedback on an essay relative to the proof in response to
certain kinds of vagueness. For example, with the proofs in Figures 6 and 7, when
the system attempts to prove that the student means either the horizontal or vertical
velocity, it triggers a clarification question asking the student to clarify whether the
horizontal or vertical velocity is meant.

200

MAXIM MAKATCHEV ET AL.

Now that the context for building and using proofs is established, we will focus solely on the details of how the abductive reasoner creates the proofs given
propositional representations of a student’s essay as input.
3. Qualitative Reasoning in Mechanics
The development of qualitative mechanics has been driven largely by the needs of
automated reasoning about physical systems. The applications range from monitoring and engineering design to education. In education, qualitative physics has
been used in modeling and design environments (Forbus et al., 2001), an example
of which is Articulate Software (Forbus, 1997).
The subset of physics that Why2-Atlas addresses motivates the ontology for the
propositional representations on which the theorem prover operates. The goal to
model and check the correctness of the student’s reasoning motivates the types of
rules included.
3.1. THE STRUCTURE OF QUALITATIVE PROBLEMS
The qualitative problems that we have chosen are from a first-year college course in
mechanics and have several differences from the problems addressed in the analysis of dynamical systems, which is the common domain for previous automated
qualitative reasoning systems.
The problems in our domain, unlike design problems, usually have all the bodies
explicitly defined in the problem statement. In several situations the descriptions
of bodies (“you throw a pumpkin”) allow for more than one idealization of a
composite body (man and pumpkin versus man, hand and pumpkin).
The initial conditions are relatively unambiguously specified in the problem’s
description. This and other features of the problems we select ensures that the envisionment, as defined in (de Kleer, 1990), normally includes only one acceptable
sequence of events. However, similarly to the case of composite bodies, various
partitionings of the timeline into intervals are possible.
After the bodies and time intervals are identified, the next step in the solution
of the problem is to choose the sequence of time intervals (usually one or two) as
main time intervals at the beginning of which sufficient knowledge about physical
quantities (initial conditions) can be obtained from the givens, so that a sequence
of inferences will result in an appropriate conclusion about the sought physical
quantity (which is usually related to the time instant at the end of the sequence of
main time intervals).
In the case of the pumpkin problem (Fig. 1), the possible sets of main intervals
include, for example, the following two:
1. (a) Man is pushing the pumpkin up; (b) the pumpkin is flying.
2. The pumpkin is flying.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

201

The initial conditions corresponding to the first partitioning would include a
zero initial vertical velocity and a nonzero upward force from the man. From these
conditions we can infer (among other things) that
– from Newton’s first law, the pumpkin’s horizontal velocity is not changing
during the pushing stage,
– from continuity of the velocity at the instant of the release, the horizontal
velocity is still the same at the beginning of the second time interval (the
flight) as it was before.
Human tutors, however, normally also accept a solution based on the second
choice of the main interval. In this case the student could obtain the initial condition of the pumpkin having the same horizontal velocity as the runner at the
beginning of the flight directly by interpreting the given “You throw a pumpkin straight up.” Note that different choices of the main intervals correspond to
different idealizations of the problem.
After the idealization stage is complete, the student has to apply qualitative
inference rules to produce a solution. Thus, for the second choice of the main time
interval above, the following is an acceptable solution:
– Only the force due to gravity acts on the pumpkin during the flight so it has
no horizontal acceleration (an application of Newton’s first law).
– Zero horizontal acceleration implies that the horizontal velocity of the pumpkin is constant during the flight.
– The constant horizontal velocity during the flight is equal to its value at the
beginning of the flight, namely, to the velocity of the man (from the initial
conditions).
– Therefore, the pumpkin and man have the same horizontal velocity during the
flight, so the pumpkin will always be above the runner until it falls back into
his hands (two bodies with the same initial position and same velocity over a
time interval have the same positions over this time interval).
This set of inferences (an essay) can be viewed as a qualitative proof of the
answer to the problem (“it falls back into his hands”). Note that the student’s actual
natural language argument can be presented in reverse order (or even some other ordering) and can include irrelevant steps. In the case of a different presentation order,
the natural language used signals the underlying ordering of the steps involved so
that after the discourse-level understanding module resolves anaphoric expressions
and other language dependencies, the underlying order of the argument is revealed.
If the underlying ordering is incorrect, then it can be addressed by anticipating
typical incorrect orderings with buggy rules. Irrelevant steps are handled in one of
two ways: (1) typical irrelevant steps are anticipated with buggy rules, or (2) the
step is assumed without proof at a high cost (see Section 4.2) and can be presented
to the student.

202

MAXIM MAKATCHEV ET AL.

3.2. EFFECT ON THE IMPLEMENTATION
The goal of our reasoning engine is twofold. First, we’d like to know the logical
steps the student did not mention explicitly in his essay. Second, we want to reason
about the correctness of these hidden steps as well as of the statements in the essay.
Note that unlike many of the systems for automated reasoning in qualitative
physics, we do not solve the physics problem. Also, since the problems do not
deal with complex envisionments, we do not have to reason about envisioning. Our
target is mainly the idealization and the following stage.
Not having to represent envisionments consisting of multiple plausible scenarios of events allows us to largely avoid one of the major difficulties facing
the developers of qualitative physics problem solvers – implementation of a vast
amount of common-sense knowledge.
3.3. QUALITATIVE PHYSICS ONTOLOGY
The Why2-Atlas ontology is inspired by that used in previous qualitative physics
reasoning work. In particular, for both ontology and rules, we borrowed extensively
from (Ploetzner and VanLehn, 1997), making appropriate simplifications given the
subset of physics the system is addressing. The ontology is further adapted to
take advantage of the knowledge representation facilities of the Tacitus-lite+ abductive reasoning engine, such as ordered sorts, which are described in detail in
Section 3.5. Until then, less formally, a term is defined as either a variable or a
constant (there are no functions in Tacitus-lite+). Terms are assigned sorts from a
partially ordered set of sort symbols, such that every term has a unique least sort.
The Why2-Atlas ontology comprises bodies, physical quantities, states, times,
and relations, each of which we describe below in more detail.
3.3.1. Bodies
The physics problems in our scope deal only with solid bodies, with the possible
exception of air, which occurs only in the context of air resistance. We distinguish
bodies with respect to their contact properties: bodies that generally require contact
to exert a force are of sort Regular-body; the other bodies, such as planets, are
said to be of sort Special-body. For the sake of simplicity, all the forces in our
ontology have a corresponding pair of bodies; therefore, we treat air, in the context
of air resistance, as a special body.
Regular bodies usually have the semantics of point masses. The few exceptions
are handled with ad hoc axioms (see, for example, contact states in Section 3.3.3).
3.3.2. Physical Quantities
The constants of the sort Quantity1b that represent vector quantities attributed
to a single body are position, displacement, velocity, acceleration, and
total-force. The sort Quantity2b for vector quantities involving two bodies has

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

203

Table I. Slots of a vector quantity of sort Quantity1b.
Description

The Generic Sort of the Filler

Quantity
Identifier
Body (or two bodies in the case of force)
Axial component or not
Qualitative derivative of the magnitude
Quantitative derivative of the magnitude
Zero or nonzero magnitude
Quantitative magnitude
Sign for axial component
Quantitative direction
Qualitative derivative of the direction
Beginning of time interval
End of time interval

Quantity1b
Id
Body
Comp
D-mag
D-mag-num
Mag-zero
Mag-num
Dir
Dir-num
D-dir
Time
Time

–
–
–

–

–
–
–
–
–
–
–

Quantity1b = {position, displacement, velocity, . . . }
Id
Comp
• Axial = {horizontal, vertical}
• No-comp = {no-comp} (i. e. full vector)
D-mag
• Constant = {constant}
• Nonconstant = {increase, decrease, varying}
D-mag-num
Mag-zero = {zero, nonzero}
Mag-num
Dir = {pos, neg}
Dir-num
D-dir = {constant, nonconstant}
Time = Problem-specific constants

Figure 8. Fragment of the sort hierarchy.

a single member in our ontology, the constant force. The constants of the sort
Scalar are duration, mass, and distance.
Every vector quantity has slots and respective restrictions on the sort of a slot
filler as shown in Table I. The hierarchy of sorts from Table I (except for sort Body,
which was described before) is shown in Figure 8. The names of sorts begin with an
uppercase character; the names of constants begin with a lowercase character. Note
that sorts Id, D-mag-num, Mag-num, and Dir-num do not have subsorts or constants.
Variables of these sorts are used only for cross-referencing between atoms (see
Section 3.5).

204

MAXIM MAKATCHEV ET AL.

3.3.3. States
Individual bodies can be in the following states: vacuum or freefall. Being in one
of these states implies respective restrictions on the forces applied on the body.
A special state between two bodies is contact. The contact between two bodies
can be attached – the bodies can exert mutual forces and the positions of two
bodies are equal; detached – the bodies do not exert mutual forces (except for
possibly the forces due to gravity); or moving-contact – the bodies can exert
mutual force (no conclusion on the respective positions). The last type of contact is
introduced to account for the fact that we often want to treat bodies as point masses
capable of pushing or pulling each other for certain time intervals (a nonimpact
type of contact), for example the man pushing the pumpkin up.
3.3.4. Time
The current representation of time most closely resembles the method of temporal arguments (Haugh, 1987), with a limited number of arguments per predicate,
but our predicates take a mix of temporal and nontemporal arguments, similar to
(Bacchus et al., 1989).
We use time instants as basic primitives. A time interval is a pair (ti , tj ) of instants. This definition of time intervals is sufficient for implementing the semantics
of open time intervals in the context of the mechanics domain.
3.3.5. Relations
The multiplace relations are represented in Table II. The respective hierarchy of
sorts is shown in Figure 9. The relation non-equal can be used for any pair of
terms. There is no explicit relation for equating arbitrary terms. Instead, substitution is used to ensure that equal terms have the same names. The relation before
relates time instants in the obvious way. The relation rel-position provides the
means to represent the relative position of two bodies with respect to each other,
independently of the choice of a coordinate system – a common way to informally
compare positions in natural language. The relation compare provides the means to
represent the ratio and difference of the magnitudes of two quantities and, for quantities changing with time, the magnitudes of the derivatives of two quantities. The
ralation compare-dir represents the relative directions of two vector quantities.
3.4. RULES
A distinctive feature of the task of modeling the student’s reasoning is that it becomes necessary to account for erroneous facts and rules. False facts corresponding
to a wrong idealization are called buggy givens. Other false facts are typically
conclusions that students make by applying false domain rules and are modeled
by buggy domain rules and buggy metaknowledge rules.

205

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

Table II. Relations.
Relation

1st and 2nd Arguments

3rd Argument

4th Argument

non-equal
before
rel-position
compare

any terms
Time
Body
Mag-num or D-mag-num of any
scalar or vector quantity
Dir-num of any vector quantity

Rel-location
Ratio

Difference

compare-dir

–
–

–

–

Rel-dir

Rel-location = {at, nonequal, to-left-of, below, etc.}
Ratio
• Greater-than-one = {two, etc.}
• One = {one}
Difference
• Nonzero
• Zero = {zero}
Rel-dir
• Parallel
* Collinear = {codirected, opposite}
* Non-collinear = {non-collinear}
• Non-parallel = {orthogonal, non-orthogonal}

Figure 9. Sort hierarchy for arguments of relations.

3.4.1. Idealization
The canonical idealization of the problem is formalized as givens, or facts, for the
abductive prover. The facts that may be misunderstood by the student because of a
possibly wrong idealization are represented as pairs of correct and buggy givens.
In the context of a student’s reasoning about the problem, buggy givens are wrong
assumptions the student made during idealization.
For example, with the pumpkin problem, the facts
→ the force of air resistance on the pumpkin is zero
and
→ the force of air resistance on the pumpkin is nonzero
are a pair of correct and buggy givens, respectively.
The facts that we consider to be common knowledge that are shared by the
student (i.e., we do not account for possible misunderstandings of those facts)
are represented as givens. Thus, assuming that no misunderstanding about the
trajectory of the man is possible, we can define as a given
→ the vertical position of the man is constant at all times.

206

MAXIM MAKATCHEV ET AL.

This pairing of correct and buggy givens is subject to integrity constraints –
only one member of the pair can be in any given proof (see Section 4.5). The
intention is to reduce the search space during proof generation; it represents a risky
assumption that we have made: Students rarely believe both of the givens in an
inconsistent pairing within the same proof.
3.4.2. Metaknowledge
To account for any wrong rules that students may be applying when they come
up with wrong conclusions, we pair the good version of such a rule with its buggy
counterpart. This pairing of correct and false rules is subject to integrity constraints
that are analogous to those for pairings of correct and false givens. Again, although
this approach reduces the search space during proof generation, it assumes that
students do not believe both of the rules in an inconsistent pairing.
3.4.3. Qualitative Newtonian Mechanics
Currently we are focusing on the problems of kinetics of a rigid body in a plane
of noncircular motion. Thus, the domain axioms cover qualitative kinematics and
qualitative versions of Newton’s laws and their derivatives. Since some problems
are essentially two dimensional, we have also implemented a basic algebra for
vector components.
Currently, there are 24 idealization rules (excluding problem-specific givens
that are assumed to be shared knowledge), 24 metaknowledge rules, and 57 rules
of qualitative Newtonian mechanics.

3.5. KNOWLEDGE REPRESENTATION
The domain propositions described above are represented in the theorem prover
by using order-sorted first-order logic (FOL) (see, for example, (Schmidt-Schauß,
1989; Walther, 1987)).
As we mentioned earlier, a term for us is a variable or a constant. Tacitus-lite+
does not provide any built-in support for functions since function-free clauses are
the natural output from the Sentence-Level Understanding module (see Section 2).
Every term has a sort specification that maps it to a member of a partially ordered
set of sorts.
Tacitus-lite+ allows for the use of predicate variables, which can also be assigned sorts. This is encapsulated in the framework of FOL by grouping a predicate
name together with its arguments as arguments of a metapredicate Mi , where i is
the total number of resulting arguments. Since there is exactly one metapredicate
symbol for each arity, metapredicate symbols can be safely omitted. Every such
representation of an atom is augmented with a corresponding sort specification for
the argument terms. For example, “Horizontal velocity of the pumpkin is decreas-

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

207

ing” is represented as shown below (where constants are in the lower-case script,
sorts begin with an upper-case letter and variables begin with “?”):
((velocity v1 pumpkin horizontal decrease
?d-mag-num ?mag-zero ?mag-num ?dir ?dir-num ?d-dir ?t1 ?t2)
(Quantity1b Id Regular-body Axial Nonconstant
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))

Each atom is indexed with a unique identifier, a constant of sort Id, which is
used for cross-referencing. For example, “Force of gravity acting on the pumpkin
is constant and nonzero” has the following representation in which the identifiers
f1 and ph1 appear as arguments in the due-to predicate:
((force f1 ?body1 pumpkin ?comp constant
?d-mag-num nonzero ?mag-num ?dir ?dir-num ?d-dir ?t1 ?t2)
(Quantity2b Id Body Regular-body Comp Constant
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))
((due-to d1 f1 ph1) (Due-to Id Id Id)
((phenomenon ph1 gravity) (Phenomenon Id Field-interaction))

There is no explicit negation. Instead, a negative student statement is represented as a conjunction of atoms with appropriate arguments whenever possible.
Thus, the fact that “there is no force” is represented as the force being zero. This
simplification is intentional and is done to avoid the problem of finding the scope
of negation in natural language text. The version of the system currently under development extends the knowledge representation to cover disjunctions, conditional
statements, and certain types of negations (see Section 6).
Rules in Tacitus-lite+ are in the form of extended Horn clauses; namely, the
head of a rule can be a conjunction of atoms. For example, a rule that states “if the
velocity of a body is zero over a time interval then its initial position is equal to its
final position” is represented as follows:
((velocity v1 ?body ?comp ?d-mag
?d-mag-num zero ?mag-num ?dir ?dir-num ?d-dir ?t1 ?t2)
(Quantity1b Id Body Comp D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))

→
((position p1 ?body ?comp ?d-mag1
?d-mag-num1 ?mag-zero1 ?mag-num1 ?dir1 ?dir-num1 ?d-dir1 ?t1 ?t1)
(Quantity1b Id Body Comp D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))
((position p2 ?body ?comp ?d-mag1
?d-mag-num1 ?mag-zero1 ?mag-num1 ?dir1 ?dir-num1 ?d-dir1 ?t2 ?t2)
(Quantity1b Id Body Comp D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))

208

MAXIM MAKATCHEV ET AL.

4. Weighted Abduction and Tacitus-lite+
4.1. ABDUCTION
Abduction is a process of reasoning from an observation to possible explanations
for that observation. In the case of the Why2-Atlas system the observations are
what the student said, and the possible explanations for why the student said this
are the qualitative physics rules (both good and bad) and orderings of those rules
that support what the student said. In order to arrive at the explanation, some assumptions have to be made along the way, since all the inferences that underlie an
explanation will not be expressed.
Formally, an abductive framework can be defined as a triple T , A, I , where T
is a theory, A is the set of abducible literals, and I is a set of integrity constraints
(Kakas et al., 1998; Paul, 1993). Then an abductive task for a given sentence G
(observation), is to find a set  ⊆ A such that
T ∪   G,

(3)

T ∪  satisfies I.

(4)

In the case of an abductive logic programming framework, and in the context
of Tacitus-lite, T is the set of givens and rules of the logic program. Any literal
can be abduced in our implementation, and the semantics of satisfying the integrity
constraints I follows the consistency view as described in Section 4.5.
Naturally, more than one solution may exist for the abductive task. Often it
is required that the solution  be minimal, namely, that no proper subset  of 
have the property T ∪  G. For the purpose of modeling the student’s reasoning,
however, other factors that influence the choice of solution may be more relevant,
as elaborated on in the next section.
4.2. CRITERIA FOR SELECTING AN ABDUCTIVE EXPLANATION
Various approaches are possible to define a preferred explanation among all admissible ones. (Leake, 1995) distinguishes between plausibility criteria and goalbased criteria. The following categories of plausibility criteria are identified: structural minimality, proof-based criteria, probabilistic/cost-based criteria, and criteria based on analogy with the previous explanations. The goal-based criteria are
the factors that depend on the intended use of the explanation: as Leake notices,
“A good explanation in a humorous context may be one that is farfetched or obviously false” (Leake, 1995).
For our task of building a model of the student’s reasoning, a combination of a
number of these criteria is used. Informally, we formulate our preference as “the
less deep, the fewer incorrect rules, and the smaller total cost of assumptions.”
More formally we would like to maximize a certain function of measures of utility
and plausibility.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

209

The utility measure is a goal-based criterion that estimates the utility of the
choice of a particular proof for the tutoring application given a plausibility distribution on a set of alternative proofs.
The plausibility measure indicates which explanation is the most likely. It gives
preference to the shallow proofs, which reflects our assumption of cognitive economy: if a short proof and a long proof both explain the student’s utterance, and
all rules and assumptions in both proofs are equally likely, then the short proof
is the more likely interpretation. Of course, comparison of the depths of proofs is
complicated by the fact that the rules in the theorem prover are not all of equal
importance in the context of the solution. Thus, some steps of the formal proof
can be safely omitted in an actual solution provided by an expert. In the context
of using the proof as a student model, this preference makes the model optimistic
about the student’s skills. In the context of using the proof for guiding tutoring
feedback, a shallow proof has greater utility because according to our assumption,
it is the type of the proof the tutor would prefer to talk about. Another factor that
contributes to the utility is the preference for explanations that use good physics as
opposed to “buggy” physics.
Since an explicit estimation of utility requires the generation of multiple proofs
and is therefore computationally expensive, we deploy a number of proof search
heuristics in an attempt to optimize the combination of the two measures. Although
currently the parameters of these heuristics are fixed for the duration of the tutoring
session, our implementation allows for varying the parameters on the fly. This may
be useful for dynamic adjustment of the student model, for example when there is
an indication that the model should be more pessimistic about the student’s skills
(more on the heuristics in Section 4.6).
While the depth preference is neutral to the content of the explanation and
the correctness preference gives only binary output for each rule, the cost-based
criteria make it possible to take into account the relative plausibility of individual
hypotheses. Thus, cost-based abduction, as with the approach defined in (Charniak
and Shimony, 1990) and applied to natural language understanding applications,
assigns quantitative costs to the hypotheses and orders the explanations by the total
cost of their hypotheses. The cost of a hypothesis is fixed and therefore is not
sensitive to such factors as (a) the relative plausibility of the goals (observations)
to be explained, (b) the explanatory chain that generated this hypothesis, and (c)
the relative plausibility of the antecedents of a particular rule.
This motivated us to choose another approach, weighted abduction (Hobbs et al.,
1988), which attempts to avoid these limitations by defining the cost of a hypothesis
 It was also shown in (Charniak and Shimony, 1990) and in (Poole, 1993) that belief revision

in Bayesian networks can be accurately modeled by cost-based abduction. That is, when costs are
chosen appropriately for the conjuncts of the rules, and the proof graph produced by applying them
to explain an utterance inherits those costs as conditional probabilities, then the resulting network
is a Bayesian network and thus can produce mathematically sound posterior marginal probabilities
(Conati et al., 2002).

210

MAXIM MAKATCHEV ET AL.

as a function of the explanatory chain that led to it and the cost of the goal at the
head of the chain. The drawback of weighted abduction in comparison to costbased abduction, however, is the lack of a precisely defined semantics of weights.
We do not attempt to provide a formal definition of such semantics in this paper;
instead, we use ad hoc heuristics that are suitable for our particular application.

4.3. WEIGHTED ABDUCTION
Following the weighted abductive inference algorithm described in (Stickel, 1988),
Tacitus-lite is a collection of rules where each rule is expressed as a Horn clause.
Further, each conjunct pi has a weight wi associated with it:
p1w1 ∧ · · · ∧ pnwn → r.

(5)

The weight is used to calculate the cost of abducing pi instead of proving it,
where cost(pi ) = cost(r) · wi . The costs of the observations are supplied with the
observations as input to the prover.
Given a subgoal or observation atom to be proven, Tacitus-lite takes one of three
actions: (1) abduces the atom at the cost associated with it, (2) unifies it with an
atom that either is a fact or has already been proven or abduced (in the latter case
the cost of the resultant atom is counted once in the total cost of the proof, as the
minimum of the two costs), or (3) attempts to prove it with a rule. Tacitus-lite calls
the second action factoring.
All possible proofs could be generated. However, Tacitus-lite allows the applications builder to set depth bounds on the number of rules applied in proving an
observation and on the global number of proofs generated during search. Tacituslite maintains a queue of proofs where the initial proof reflects abducing all the
observations and each of the three above actions adds a new proof to the queue.
The proof generation can be stopped at any point, and the proofs with the lowest
cost can be selected as the most plausible proofs for the observations.
Tacitus-lite uses a best-first search guided by heuristics that select which proof
to expand, which observation or goal in that proof to act upon, which action to
apply, and which rule to use when that is the selected action. As we mentioned,
most of the heuristics in Why2-Atlas are specific to the domain and application.
SRI’s release of Tacitus-lite was subsequently extended as part of the research
project described in (Thomason et al., 1996) and was named Tacitus-lite+ at that
time. We are using two main extensions from that work: (1) proofs falling below
a user defined cost threshold halt the search and (2) a simple system of variable sorts reduces the number of rules written and the size of the search space
(Hobbs et al., 1988, p. 102).
Unlike the earlier applications of Tacitus-lite+, which used it solely for reasoning about language, Why2-Atlas also uses it for shallow qualitative physics reasoning. To support qualitative physics reasoning, we’ve made a number of general

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

211

inference engine extensions, such as improved consistency checking and allowing
the rule author to express both good and bad rules in the same rule set.
While computing the minimal explanation with respect to many kinds of prioritization is known to be NP-hard (Bylander et al., 1991; Charniak and Shimony,
1994; Eiter and Gottlob, 1993; Selman and Levesque, 1990), polynomial algorithms have been found for some useful classes of abductive problems (Eshghi,
1993), including cost-based abduction (Santos and Santos, 1996). To the best of
our knowledge no such promising complexity results exist for the problems specific
to weighted abduction. We are still searching for the best heuristics to use with our
domain and application.
4.4. ORDER - SORTED ABDUCTIVE INFERENCE
Let S be a set of sort symbols with a partial order . A sorted term is a pair (t, τ ),
denoted as t : τ , where t is a term (a constant or a variable in our case) and τ is a
sort symbol. A sorted atom is of the form p(x1 , . . . , xn ) : (τ1 , . . . , τn ), where the
term xi is of the sort τi .
An order-sorted abductive logic programming framework T , A, I  is an abductive logic programming framework with all atoms augmented with the sorts of
their argument terms (so that they are sorted atoms).
Order-sorted deduction has received extensive treatment on its own and as an
extension of unsorted logics (Walther, 1987; Cohn, 1989; Kaneiwa and Tojo, 2001;
Frisch, 1991). In terms of unsorted predicate logic, formula ∃xp(x) : (τ ) can
be written as ∃xp(x) ∧ τ (x). For our domain we restrict the sort hierarchy to a
tree structure that is naturally imposed by set semantics and that has the following
property:
∃x τi (x) ∧ τj (x) → (τi  τj ) ∨ (τj  τi ),

(6)

where τi  τj is equivalent to ∀xτi (x) → τj (x). Without loss of generality for
the rest of this section, we will use binary predicates and constant-free atoms. The
latter can be easily achieved in order-sorted logic by transforming an ordered atom
p(a) : (τ ) that includes a constant a into p(xa ) : (τa ) and creating a new variable
xa and a new sort symbol τa ∈ S such that τa  τ . We will also assume that the
rules of the order-sorted logic program T are nongeneralizing, that is, for any rule
of the form
p(x, y) : (τ1 , τ2 ) ← q(x, z) : (τ3 , τ4 ),

(7)

it holds that τ1  τ3 . If for rule (7) this condition doesn’t hold (and therefore,
according to (6), τ3  τ1 must hold), it can be transformed into the nongeneralizing
form by substituting the sorts for the terms in the head of the rule by the most
specific sorts (for the respective terms) found in the body of the rule. For example,
rule (7), where τ3  τ1 , can be transformed into the nongeneralizing rule
p(x, y) : (τ3 , τ2 ) ← q(x, z) : (τ3 , τ4 ).

(8)

212

MAXIM MAKATCHEV ET AL.

It is easy to see from the set-theoretic semantics of sorts that this transformation is
model preserving.
Given the constraint (6) on the sort hierarchy, modus ponens can be extended to
sorted deduction as follows:
q(x  , z ) : (τ5 , τ6 )
p(x, y) : (τ1 , τ2 ) ← q(x, z) : (τ3 , τ4 )
τ5  τ3 , τ6  τ4
p(x  , y  ) : (min(τ5 , τ1 ), τ2 )
Similar to (Kakas et al., 1998) our abductive reasoning procedure interleaves
consistency check and backchaining stages. Briefly, the procedure can be described
as follows:
1. Unify the goal with the head of the rule.
2. If unification succeeds, apply the unifier to the body of the rule and generate
the candidate new goals from the atoms in the body of the rule.
3. Check whether the candidate new goals satisfy the consistency constraints. If
the constraints are violated, mark the pair (goal, rule) as not applicable.
4. If the consistency constraints are satisfied, (a) add the candidate new goals to
the list of goals; (b) remove the goal from the list of goals; and (c) add the goal
and the rest of the atoms from the conjunction in the head of the rule (having
applied the unifier to them as well) to the list of provens (which is used in the
consistency check).
4.5. CONSISTENCY CONSTRAINTS
Our definition of what it means for a knowledge base T ∪  to satisfy an integrity
constraint φ ∈ I is most closely related to the consistency view; see, for example,
(Kakas et al., 1998):
T ∪

satisfies

φ

iff

T ∪   ¬φ.

The particular integrity constraint we wish to enforce is
¬[p ∧ p ∗ ]

(9)

for every atom p, where p ∗ means an opposite of p, following the approach described in (Kakas et al., 1998; Eshghi and Kowalski, 1989). The definition of
an opposite of an atom is domain specific, and for a given atom an opposite is
not necessarily unique. For example, in the domain of qualitative mechanics, one
of the opposites of “velocity of pumpkin is constant” is “velocity of pumpkin
is nonconstant”; another is “velocity of pumpkin is increasing.” More formally,
every predicate p has a distinguished subset of argument places, called functional
arguments, with the following property: There is a mapping (specific to p) from
groundings of functional arguments to groundings of the rest of the arguments

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

213

(although this mapping may be unknown). For example, if in binary predicate
p the first argument is functional (and the second is not), then for a given atom
p(x, y) : (τ1 , τ2 ), the set P ∗ of opposites would be as follows:
P ∗ = {p(x, y  ) : (τ1 , τ2 ) | τ2 is incomparable to τ2 }.
In terms of provability, the abductive explanation  is said to satisfy constraint
(9) if and only if for every atom p,
T ∪   p ∧ p∗,

∀p ∗ ∈ P ∗ .

(10)

For the sake of computational efficiency we do not implement the completeness
part of the semantics of negation as failure (NAF), which requires that one of
the following must hold: T ∪   p or T ∪   p ∗ . Neither do we do a full
implementation of constraint (10) because in this case each step of a proof must
be checked by testing whether each opposite of the atom is provable with no new
steps or with steps that cost less than the proof of the original atom. As suggested in
(Appelt and Pollack, 1992) in the case of weighted abduction one should settle for
incomplete consistency checking and focus on detecting the inconsistencies that
are most likely to arise in the application domain.
Instead of implementing (10), we prevent abductive inference on rules that
would immediately give rise to a new goal p ∗ ∈ P ∗ when the proof generated so
far has atom p. Namely, we guarantee that the following holds for every atom p:
/ T ∪  ∪ Proof,
p ∧ p∗ ∈

∀p ∗ ∈ P ∗ .

(11)

For example, the atoms corresponding to the pair of statements
velocity of pumpkin is increasing
and
velocity of pumpkin is nonconstant
are consistent (since constant increasing is of sort Nonconstant), while the atoms
corresponding to the statements
velocity of pumpkin is increasing
and
velocity of pumpkin is constant
are inconsistent (since constant increasing is of sort Nonconstant which is incomparable with sort Constant).
As an example of the above, consider a fragment of a proof tree starting from
the subgoal “horizontal velocity of pumpkin is constant” as shown in Figure 10.
First, assume that the fact “total vertical force on pumpkin is a nonzero constant,”

214

MAXIM MAKATCHEV ET AL.

‘‘horizontal velocity of pumpkin is constant’’
Rule 24: “The magnitude of a vector is constant →
the magnitude of every component of the vector is constant”
‘‘velocity of pumpkin is constant’’
Rule 13-int: “Acceleration of a body is zero →
velocity of the body is constant”
‘‘acceleration of pumpkin is 0’’
Rule 6: “Total force on a body is zero →
acceleration of the body is zero”
‘‘total force on pumpkin is 0’’
Rule 23iff: “The magnitude of every component of a vector is zero →
the magnitude of the vector is zero”
‘‘total horizontal force on pumpkin is 0’’
‘‘total vertical force on pumpkin is 0’’
Figure 10. Example of an inconsistent proof. One of the newly generated goals “Total horizontal force on pumpkin is 0” is inconsistent with the previously proven fact “Total vertical
force on pumpkin is a nonzero constant.”

which refers to the time the pumpkin is in free-fall, has been proven in another
branch of the proof tree. In this case, the application of rule 23iff should not be
allowed in the same proof because it results in the need to prove the contradictory
statement “total vertical force on pumpkin is 0.”
Another kind of inconsistency is related to metaknowledge reasoning, namely,
the rules that have buggy counterparts. For example, if a correct rule (in the sense
of a rule schema, e.g., p, q, and q ∗ have unbound variables)
p→q
has a buggy counterpart
p → q∗,
then both of them cannot be a part of theory T , which includes fact ∃x p(x),
provided we want to keep T consistent. The obvious workaround is to implement
such pairs of rules as
bug∗ ∧ p → q
and
bug ∧ p → q ∗ ,
where bug and bug∗ are mutually exclusive abducibles, because of constraint (11),
that do not appear anywhere else.
In the actual implementation we handle this constraint on the metalevel by
simply disallowing the appearance of pairs of these rules within the same proof.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

215

We restate here that while the consistency constraints we describe are natural in
theorem proving, from the viewpoint of student modeling they represent a risky
assumption: that the student does not simultaneously hold inconsistent beliefs.
There is, however, some justification of the assumption that the student doesn’t
hold directly contradicting beliefs, implemented as constraint (11), coming from
cognitive science: it has been shown that even young children are unlikely to make
mistakes in tasks involving taxonomic categories (Chi and Ceci, 1987).

4.6. SEARCH ISSUES
Our goal is to guarantee that the resultant proof will satisfy one of the following
criteria:
– Strong criterion: there is no cheaper proof within the given threshold on depth
of the proof. This is likely to require close to an exhaustive search.
– Medium criterion: there is no cheaper proof that is of the same depth or
shorter, and the proof search has met the threshold on depth or the threshold
on number of proofs. This criterion is likely to require exhaustion of the latter threshold by iterative deepening for a significantly large depth threshold.
Therefore a deeper, cheaper proof within the depth threshold would not be
found.
– Weak criterion: there is no cheaper proof that is of the same depth or shorter,
and one of the thresholds (depth, number of proofs, satisfactory proof cost) is
met by the proof search.
The cost threshold allows us to avoid iterative deepening and implement heuristics to help find a low-cost proof before we exhaust depth or number of proofs
thresholds. Thus, our current search strives to satisfy the weak criterion. Heuristics
are used to select the best potential proof for expansion, the best goals in the proof
to address, and the best possible rule to apply to prove the goals. A description of
the heuristics follows.
The best goals are those that have high assumption costs and have not been
expanded for the most number of steps of the proof. Rules that would not satisfy
the condition of consistency are eliminated from the list of potential candidates.
Then the remaining pool of consistent rules is divided into classes of rules of the
same cost. The costs of rules cause the search to try to apply first the most specific
correct rules, then the more general correct rules, and, only when these two fail,
the buggy rules. This approach reflects our subjective estimate of the probability of
a successful proof for each choice of rule.
The cheapest rule is chosen, with possible conflicts being resolved nondeterministically. If no applicable rules are found in the given class, the next cheapest
class is searched. If no class has an applicable rule, the goals are abduced, and their
cost is added to the cost of the proof accordingly.

216

MAXIM MAKATCHEV ET AL.

5. Evaluating the Tutoring System and the Theorem Prover
The Why2-Atlas system participated in an evaluation study in the spring of 2002
in order to acquire baseline measurements of student learning gains. The experimental setup focused on the question of whether similar content delivered through
dialogue or a static text had different effects on student learning. Each condition
selected the material it presented from a limited, well-defined set of prescribed
physics topics for each training problem. The static text condition presented all of
the prescribed topics, while the dialogue condition chose a subset of the topics that
it deemed necessary given a student’s essay responses and previous dialogue.
The population tested was undergraduate students who had recently completed
an introductory physics course. Although the ideal population is physics students
who are currently taking physics and who are just learning the content covered
by the experiment, it is difficult to recruit enough students from such a highly
constrained population to offset low experiment completion rates. But given that
previous studies indicated that even physics students who have done well in their
courses perform poorly on qualitative physics problems (Hake, 1998), we expected
that students who had completed an introductory course to be appropriate as well
for the experiment.
Why2-Atlas was one of several dialogue conditions for the experiment. Another
dialogue condition of interest here was human tutoring dialogue. Although these
human tutors and their students communicated through typing, the human tutoring
condition was hypothesized to be better than the static text condition. We expected
that the Why2-Atlas system would be at least as good as the static text condition
given that the system and knowledge sources were known to be incomplete.
Although the students in every condition showed significant learning gains,
a surprising result was that the gains for all of the conditions were statistically
similar. Because the human tutoring condition and the static text condition were
similar, and counter to previous experiments comparing human tutoring to simpler instruction, subsequent experiments focused on comparisons involving human
tutors and the static text.
In addition, as we had expected, our system’s accuracy for identifying misconceptions was poor. None of the misconceptions that were identified were justified
according to human judgments of a sampling of the essays from students who
completed the experiment for the Why2-Atlas system. We also confirmed that the
propositional inputs to the theorem prover only partially represented the content
of student essays, so the system evaluation provided no informative performance
measurement of the theorem prover. Because the theorem prover was getting sparse
representations of student’s essays and because the system evaluation results are
inconclusive, we cannot yet test for a correlation between student learning and the
system’s accuracy at selecting appropriate dialogue topics. We expect that, with
system improvements and improvements in the experimental design, an upcoming
repeat of the system evaluation will be more informative.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

217

In the interim, we created a test suite using essays collected during the baseline evaluation and subsequent experiments. In addition to the essays from the
baseline evaluation, we have since collected essays from students who have never
taken physics but who receive a short instructive text prior to testing and training.
To create the test suite, we randomly selected 45 essays, while balancing problems, subjects, subject backgrounds, and essay versions. The 45 essays cover seven
problems and were written by 21 students (11 with physics backgrounds and 10
without); 32 of these essays are written by students who had not previously had a
physics course, and the remainder by students who had completed a physics course.
Since each student may have revised an essay for a problem multiple times, we
randomly selected one problem essay per student. Once the essays for the test suite
were selected, we hand-corrected the logged inputs for the various system modules
for each student essay in the test suite and had human judges annotate each essay
with the physics principles it covered and the misconceptions it exhibited.
Two main test suite evaluations are of interest for the abductive theorem prover
relative to processing bounds and efficiency: (1) the accuracy of the misconceptions
revealed by the proofs and (2) the accuracy of the whole proofs as student models.
We have an evaluation of the accuracy of misconceptions relative to the test suite
but have just begun a preliminary evaluation of the accuracy of the whole proofs
generated.
5.1. ACCURACY OF THE MISCONCEPTIONS REVEALED
To assess the accuracy of the misconceptions that are identified as a result of
the proofs produced by the theorem prover, we can compare the misconceptions
selected with those that should have been identified according to the human judgments for the essays in the test suite.
Our goal here is to minimize the number of misconceptions missed by the system that a human judge identified as relevant. In the 45 essays of the test suite,
three essays have two misconceptions each, eight essays have one misconception
each, and the rest of the essays don’t have any misconceptions from the list of 54
misconceptions that could arise for the training problems according to our physics
experts.
To evaluate the accuracy of the theorem prover at revealing misconceptions, we
compare the theorem prover’s results for an essay relative to the misconceptions
possible for the problem (system identified (SI) versus system did not identify
(SDI)) to those of the human judgments annotated in the test suite relative to
the misconceptions possible (human identified (HI) versus human did not identify
(HDI)) to determine the number of
– true positives (TP), where TP = SI ∩ HI
– true negatives (TN), where TN = SDI ∩ HDI
– false positives (FP), where FP = SI ∩ HDI
– false negatives (FN), where FN = SDI ∩ HI

218

MAXIM MAKATCHEV ET AL.

If for problem 1, experienced physics instructors indicate that 5 misconceptions
are relevant, A–E, and for an essay instance on problem 1 the theorem prover output
reveals misconceptions A and B while a human judge identified as present B and
C instead, then for that essay instance, TP = 1, TN = 2, FP = 1, FN = 1. So
TP + FN is the number of misconceptions that the human identified as present, and
FP + TN is the number of relevant misconceptions that were not present according
to the human judge.
We build a confusion matrix with the cells TP, FP, TN, FN by summing the
values across each essay in the test suite. From this confusion matrix we can compute the following measures, which are frequently used in classification tasks for
information retrieval and machine learning:
– recall (R) = TP/(TP + FN)
– precision (P) = TP/(TP + FP)
– positive false alarm rate = FP/(FP + TN)
– negative false alarm rate = FN/(FP + TN)
In addition, we also recorded the theorem prover’s results at various proof cost
thresholds to see how the performance changes as we move closer toward building
a complete proof. For each threshold of interest, we create a separate confusion
matrix. However, it is possible that other thresholds (for example, the threshold on
number of possible proofs generated) are exceeded before a proof satisfying the
cost threshold is found. When this case arises, we add the results of the best proof
so far to the target threshold confusion matrix regardless of what the actual cost is.
As shown in Figure 11, the recall increases from 0 at a proof cost of 1 (where
everything is assumed without proof) to 62% at a proof cost threshold of 0.2. As
the recall increases, the precision degrades but then levels off. We expect that the
precision will also improve rather than degrade once the planned improvements to
the theorem prover are implemented (see Section 6). These results mean that the
theorem prover can help to reveal up to 62% of the misconceptions that a human
would recognize, but at the cost of identifying some misconceptions that are not
justified by the essays. We consider recall to be the more important measure for
misconceptions because it is important to find and address the misconceptions that
are expected to be obvious to a human tutor.
In order to get a sense of how difficult the task of finding misconceptions is,
it is useful to also examine the false alarm rates, as shown in Figure 12. The
negative false alarm rate is inversely related to recall in that as recall increases,
the negative false alarm rate declines and indicates how many misconceptions are
overlooked. Our goal is for this number to fall as close to 0 as possible because we
hypothesize that overlooking misconceptions is detrimental in tutorial applications.
We have observed that students can have a complete explanation and still conclude
the wrong answer from that explanation.
The positive false alarm rate is inversely related to precision and indicates how
many misconceptions the system incorrectly attributed to essays. While we’d prefer
that this number fall to zero as well, it is not so bad to cover more misconceptions

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

219

Figure 11. Recall and precision measures as proof cost threshold decreases.

Figure 12. Negative and positive false alarm rates as proof cost threshold decreases.

than are needed. One might take the approach, as with the static text condition, of
covering all the misconceptions that are expected to be possible for a problem, but
some hypothesized downsides of this approach are inadvertently strengthening the
reasoning associated with a misconception and a loss of interest and cooperation on
the part of the student; the student’s effort to explain during subsequent problems
may drop off if the student perceives the system is not usually giving appropriately
focused feedback on their essay.
As expected, the effort to find more complete proofs and improvements in recall
and negative false alarm rates require an increase in processing time, as shown by
Figure 13.
While the theorem prover’s negative false alarm rate is considered good, we
expect that additional testing and fine-tuning of the rules, inference procedures,
and proof search heuristics will further improve the results.

220

MAXIM MAKATCHEV ET AL.

Figure 13. Processing time in seconds as proof cost threshold decreases.

5.2. ACCURACY OF THE WHOLE PROOFS GENERATED
Comparing the misconceptions revealed with those that a human judge identifies
is only a coarse measure of the accuracy of the proof generated. To determine
the fitness of the theorem prover for assessing completeness of an explanation,
we must also consider the accuracy of the whole proofs generated. Assessing proof
accuracy is more difficult because the proofs must be hand verified. In addition, it is
difficult to create a reliable gold standard against which to evaluate the accuracy of
proofs for essays and the reasons for any inaccuracy. The reason is that, in general,
language in context gives rise to many inferences (Austin, 1962; Searle, 1975).
In this case, we will judge whether the proof is at least a plausible model for the
student essay. Such an accuracy evaluation is still in progress.
6. Future Work
A number of improvements to Tacitus-lite+ and to Why2-Atlas are in progress, and
we also plan to address a number of improvements in the future. The improvements
we have identified for Tacitus-lite+ are as follows:
– To integrate into the tutoring system a refinement where the factoring operation (as defined in Section 4.3) distinguishes between factoring with more
specific and with more general atoms (and charges the proof an appropriate
cost).
– To respond to explicit conflicts in students’ essays in the near future and
to work on interactive proof generation (i.e., asking questions of the student when ambiguities arise, rather than dealing with them after the proof
generation is complete).
– To explore a stronger consistency criterion than (11) in order to improve confidence in the consistency of an abductive explanation. On the other hand, a
consistent proof may not be appropriate as a model for all types of students.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

221

This trade-off can be accounted for by allowing a certain degree of flexibility
in the consistency criterion. The relationship between the consistency of the
explanation and its measures of utility and plausibility is not straightforward
and is currently being investigated.
– To address weaknesses in the current reasoning system that stem from a lack
of explicit negation, quantifiers and disjunction in its knowledge representation. For example, it could be beneficial for the sort hierarchy to distinguish
between lexical negation (decreasing) and classical negation (¬increasing),
as proposed in (Kaneiwa and Tojo, 2001).
Naturally, addition of any of these features will require more sophisticated reasoning procedures.
In the area of improvements to Why2-Atlas, we are currently extending the
system to cover a larger subset of physics and as a result more physics problems.
Our next most immediate goal is to improve the feedback to students relative to
the proofs produced by Tacitus-lite+. We need to address two situations. The first
is when the system generates multiple lowest-cost proofs, as in the earlier example
associated with Figures 6 and 7. The second is when it produces just one lowestcost proof in which the student has presented unambiguous correct information in
the essay that leads the system to infer a correct and complete explanation or the
student has provided information that is clearly incorrect.
Although work on natural language interpretation selection (Zukerman and
George, 2002) and natural language generation work on argumentation and explanation (e.g., Zukerman et al., 2000; Horacek, 1997; Zukerman and McConachy,
1993) will give us a starting point for deriving feedback to the student relative to a
proof, there are a number of new challenges to address. First, generation work on
argumentation and explanation has focused on generating text as opposed to dialogue and deals mainly with correct representations. Second, in the case of choosing between alternative representations, the work in (Zukerman and George, 2002)
doesn’t address the problem of choosing between representations that have the
same number of inferences, as is the case for the alternative proofs in Figures 6
and 7.
In the situation where there are multiple lowest-cost proofs, the system needs to
locate major differences between the alternative proofs and ask clarification questions that will help the system determine which proof is the best representation of
the student’s beliefs. For instance, in the earlier example with Figures 6 and 7, there
are a number of possibilities for doing this. First, the system can ask, “Why do you
think the pumpkin slows down?” If the student responds with “I don’t know,” then
the system can ask a more leading question such as “What is the total horizontal
force on the pumpkin?” since a proposition about total force is present in both
proofs but has different argument values.
The other situation is where the system finds one plausible proof either on its
own or because it asked questions that allowed it to narrow down the possibilities
to one proof as above. We are working on the case where the system has to infer

222

MAXIM MAKATCHEV ET AL.

T: How do you know that the pumpkin will land on the man?
U: I don’t know
T: What is the pumpkin’s displacement relative to the man’s?
U: it is equal to the man’s
T: Right. The pumpkin’s horizontal displacement will be equal to the man’s displacement at
all times. So what leads you to know this, given you already know that the man is carrying the
pumpkin and is running at a constant speed?
U: I don’t have any idea
T: What can you say about the velocity of the pumpkin before the runner throws it?
U: it is equal to the man’s
T: Right. The horizontal velocity of the pumpkin is equal to the horizontal velocity of the
runner. So what follows from knowing this ?
Figure 14. A prototype dialogue based on a simplified input proof in which the proposition
“The pumpkin lands on the man” is marked as covered by the user’s essay.

most of this one proof from just a few true student propositions (Jordan, 2004).
Our approach is to use low-level constraints defined in terms of the structural
relationship between a proof step of high assumption cost that is to be elicited and
a proof step of low assumption cost. The structural relationships are ancestor, sibling, and transitive. Next we define higher-level elicitation strategies by specifying
constraints on the distance between the steps in the three structural relationships,
the orientation between the high-cost target step and the low-cost step, and the
distance and orientation when choosing the next high-cost target step to elicit. In
the case of an ancestor relationship, in one dialogue turn, the constraints select a
step of low assumption cost that is at a distance N on a path from the step that is
to be elicited and ask for an elaboration relative to the low-cost step. For example,
if the student had said, “The pumpkin lands on me because the velocity of the
pumpkin is constant” and N is 1, the system can ask either an open-ended question
such as “What follows from knowing that the pumpkin’s velocity is constant?” or
a more focused question such as “What does the constant velocity tell us about the
pumpkin’s acceleration?”
We have implemented a proof of concept prototype using these three low-level
constraints and additional higher-level dialogue strategy constraints. The prototype
takes a simplified proof as input, where the proof is marked with which steps were
covered in an essay and which are givens and therefore of low assumption cost. An
excerpt of a dialogue produced by this prototype, where the initial essay is only
“The pumpkin lands on the man,” is shown in Figure 14.
We are just beginning to explore the case where the one plausible proof contains
a bug because the student made some incorrect statements (as in the essay in Figure 1). Our approach treats these incorrect student statements as being correct and
attempts to lead the student to a contradiction (Jordan, 2004), as with reductio ad
absurdum (Zukerman et al., 2000). For example, if the target incorrect statement is
“There was a horizontal force acting on the pumpkin before the throw,” then a con-

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

223

tradiction is sought for the givens “The horizontal velocity of the man is constant
before the throw” and “The man is carrying the pumpkin before the throw.”
7. Conclusions
In this paper we have presented weighted abductive proofs as a method for developing a deeper understanding of students’ explanations for qualitative physics
problems than can be afforded by superficial sentence-level semantics. We viewed
abductive proofs that are based on student essays as a way to model students’
beliefs and knowledge of physics. We described how feedback that is adapted to a
student’s particular needs can be generated based on these student models. To show
how we are able to acquire these student models, we presented a qualitative physics
ontology with sorts and a collection of correct and buggy rules that were designed
to cover a subset of Newtonian mechanics and the most common misconceptions.
We also described how we adapted a weighted-abduction reasoning framework
for the task of building proofs of student essays. A combination of heuristics was
developed to assist in choosing the best proof and hence the best model of the
student by having these heuristics approximate selection criteria that are based on
measures of utility and plausibility of a candidate model.
Acknowledgments
This research was supported by MURI grant N00014-00-1-0600 from ONR Cognitive Science and by NSF grant 9720359. We thank the entire NLT team for their
many contributions in creating and building the Why2-Atlas system. In particular,
we thank Michael Ringenberg and Roy Wilson for their work with Tacitus-lite+,
and Uma Pappuswamy and Michael Böttner for their work with the ontology and
the domain rules.
References
Aleven, V. and Koedinger, K. R. (2000) The need for tutorial dialog to support self-explanation, in
Building Dialogue System for Tutorial Applications, Papers of the 2000 AAAI Fall Symposium.
Aleven, V., Popescu, O. and Koedinger, K. (2002) Pilot-testing a tutorial dialogue system that supports self-explanation, in Proceedings of Intelligent Tutoring Systems Conference, LNCS 2363,
Springer, pp. 344–354.
Aleven, V., Popescu, O. and Koedinger, K. R. (2001a) Toward tutorial dialog to support selfexplanation: Adding natural language understanding to a cognitive tutor, in Proceedings of
10th International Conference on Artificial Intelligence in Education (AI-ED 2001), IOS Press,
Amsterdam, pp. 246–255.
Aleven, V., Popescu, O. and Koedinger, K. R. (2001b) A tutorial dialogue system with knowledgebased understanding and classification of student explanations, in Working Notes of 2nd IJCAI
Workshop on Knowledge and Reasoning in Practical Dialogue Systems.
Appelt, D. and Pollack, M. (1992) Weighted abduction for plan ascription, User Modeling and UserAdapted Interaction 2(1–2), 1–25.

224

MAXIM MAKATCHEV ET AL.

Austin, J. L. (1962) How to Do Things with Words, Oxford University Press, Oxford.
Bacchus, F., Tennenberg, J. and Koomen, J. (1989) A non-reified temporal logic, in J. F. Allen,
R. Fikes and E. Sandewall (eds.), KR’89: Principles of Knowledge Representation and Reasoning, San Mateo, California, Morgan Kaufmann, pp. 2–10.
Bylander, T., Allemang, D., Tanner, M. C. and Josephson, J. R. (1991) The computational complexity
of abduction, Artificial Intelligence 49(1–3), 25–60.
Charniak, E. (1986) A neat theory of marker passing, in Proceedings of the 5th National Conference
on Artificial Intelligence (AAAI’86), pp. 584–588.
Charniak, E. and Shimony, S. E. (1990) Probabilistic semantics for cost-based abduction, in
Proceedings of AAAI-90, pp. 106–111.
Charniak, E. and Shimony, S. E. (1994) Cost-based abduction and MAP explanation, Artificial
Intelligence 66, 345–374.
Chi, M. T. H. and Ceci, S. J. (1987) Content knowledge: Its role, representation and restructuring in
memory development, Advances in Child Development and Behavior 20, 91–142.
Chi, M. T. H., de Leeuw, N., Chiu, M.-H. and LaVancher, C. (1994) Eliciting self-explanations
improves understanding, Cognitive Science 18, 439–477.
Chi, M. T. H., Siler, S. A., Jeong, H., Yamauchi, T. and Hausmann, R. G. (2001) Learning from
human tutoring, Cognitive Science 25(4), 471–533.
Cohn, A. G. (1989) Taxonomic reasoning with many-sorted logics, Artificial Intelligence 3, 89–128.
Conati, C., Gertner, A. and VanLehn, K. (2002) Using Bayesian networks to manage uncertainty in
student modeling, J. User Modeling and User-Adapted Interaction 12(4).
de Kleer, J. (1990) Multiple representations of knowledge in a mechanics problem-solver, in D. S.
Weld and J. de Kleer (eds.), Readings in Qualitative Reasoning about Physical Systems, San
Mateo, California, Morgan Kaufmann, pp. 40–45.
Eiter, T. and Gottlob, G. (1993) The complexity of logic-based abduction, in Symposium on
Theoretical Aspects of Computer Science, pp. 70–79.
Eshghi, K. (1993) A tractable class of abduction problems, in Proceedings 13th International Joint
Conference on Artificial Intelligence, Chambery, France, pp. 3–8.
Eshghi, K. and Kowalski, R. A. (1989) Abduction compared with negation by failure, in Proceedings
of the 6th International Conference on Logic Programming (ICLP ’89), pp. 234–254.
Forbus, K., Carney, K., Harris, R. and Sherin, B. (2001) A qualitative modeling environment for
middle-school students: A progress report, in QR-01.
Forbus, K. D. (1997) Using qualitative physics to create articulate educational software, IEEE Expert,
pp. 32–41.
Frisch, A. M. (1991) The substitutional framework for sorted deduction: Fundamental results on
hybrid reasoning, Artificial Intelligence 49(1–3), 161–198.
Graesser, A. C., Wiemer-Hastings, P., Wiemer-Hastings, K., Harter, D., Person, N. and the TRG
(2000) Using latent semantic analysis to evaluate the contributions of students in AutoTutor,
Interactive Learning Environments 8, 129–148.
Hake, R. R. (1998) Interactive-engagement versus traditional methods: A six-thousand student survey of mechanics test data for introductory physics students, American Journal of Physics 66(4),
64–74.
Haugh, B. (1987) Non-standard semantics for the method of temporal arguments, in Proc. of
IJCAI’87, pp. 449–454.
Hestenes, D., Wells, M. and Swackhamer, G. (1992) Force concept inventory, The Physics Teacher
30, 141–158.
Hewitt, P. G. (1998) Conceptual Physics, 8th edn, Addison-Wesley.
Hobbs, J., Stickel, M., Appelt, D. and Martin, P. (1993) Interpretation as abduction, Artificial
Intelligence 63(1–2), 69–142.
Hobbs, J., Stickel, M., Martin, P. and Edwards, D. (1988) Interpretation as abduction, in Proc. 26th
Annual Meeting of the ACL, Association of Computational Linguistics, pp. 95–103.

ABDUCTIVE THEOREM PROVING FOR ANALYZING EXPLANATIONS

225

Horacek, H. (1997) A model for adapting explanations to users’ likely inferences, User Modeling
and User-Adapted Interaction 7(1), 1–55.
Jordan, P. and VanLehn, K. (2002) Discourse processing for explanatory essays in tutorial applications, in Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue.
Jordan, P. W. (2004) Using student explanations as models for adapting tutorial dialogues, in
Proceedings of 17th International FLAIRS Conference.
Kakas, A., Kowalski, R. A. and Toni, F. (1998) The role of abduction in logic programming, in D. M.
Gabbay, C. J. Hogger and J. A. Robinson (eds.), Handbook of Logic in Artificial Intelligence and
Logic Programming, Vol. 5, Oxford University Press, pp. 235–324.
Kaneiwa, K. and Tojo, S. (2001) An order-sorted resolution with implicitly negative sorts, in Proceedings of the 2001 International Conference on Logic Programming (ICLP’01), LNCS 2237,
Springer, pp. 300–314.
Keeney, R. and Raiffa, H. (1976) Decisions with Multiple Objectives, Wiley.
Landauer, T. K., Foltz, P. W. and Laham, D. (1998) An introduction to latent semantic analysis,
Discourse Processes 25, 259–284.
Lascarides, A. and Asher, N. (1991) Discourse relations and defeasible knowledge, in 29th Annual
Meeting of the Association for Computational Linguistics, pp. 55–62.
Lavoie, B. and Rambow, O. (1997) A fast and portable realizer for text generation systems, in
Proceedings of the Fifth Conference on Applied Natural Language Processing Chapter of the
Association for Computational Linguistics, Washington, DC, pp. 265–268.
Leake, D. (1995) Abduction, experience, and goals: A model of everyday abductive explanation,
J. Experimental and Theoretical Artificial Intelligence 7, 407–428.
McCallum, A. and Nigam, K. (1998) A comparison of event models for naive Bayes text classification, in Proceedings of AAAI/ICML-98 Workshop on Learning for Text Categorization, AAAI
Press.
McRoy, S. and Hirst, G. (1995) The repair of speech act misunderstandings by abductive inference,
Computational Linguistics 21(4), 435–478.
Murray, R. C. and VanLehn, K. (2000) DT tutor: A dynamic decision-theoretic approach for optimal
selection of tutorial actions, in Proceedings of Intelligent Tutoring Systems Conference, LNCS
1839, Springer, pp. 153–162.
Ng, V. and Cardie, C. (2002) Improving machine learning approaches to coreference resolution, in
Proceedings of Association for Computational Linguistics 2002.
Paul, G. (1993) Approaches to abductive reasoning – An overview, Artificial Intelligence Review
7(2), 109–152.
Ploetzner, R. and VanLehn, K. (1997) The acquisition of qualitative physics knowledge during
textbook-based physics training, Cognition and Instruction 15(2), 169–205.
Poole, D. (1993) Probabilistic Horn abduction and Bayesian networks, Artificial Intelligence 64(1),
81–129.
Rayner, M. and Alshawi, H. (1992) Deriving database queries from logical forms by abductive definition expansion, in Proceedings of the Third Conference of Applied Natural Language Processing,
Trento, Italy, pp. 1–8.
Rosé, C., Bhembe, D., Roque, A., Siler, S., Srivastava, R. and VanLehn, K. (2002) A hybrid understanding approach for robust selection of tutoring goals, in Proceedings of Intelligent Tutoring
Systems Conference, LNCS 2363, Springer, pp. 552–561.
Rosé, C., Jordan, P., Ringenberg, M., Siler, S., VanLehn, K. and Weinstein, A. (2001) Interactive
conceptual tutoring in Atlas-Andes, in Proceedings of AI in Education 2001 Conference.
Rosé, C., Roque, A., Bhembe, D. and VanLehn, K. (2002) An efficient incremental architecture for
robust interpretation, in Proceedings of Human Language Technology Conference, San Diego,
CA.
Santos, Jr., E. and Santos, E. S. (1996) Polynomial solvability of cost-based abduction, Artificial
Intelligence 86(1), 157–170.

226

MAXIM MAKATCHEV ET AL.

Schmidt-Schauß, M. (1989) Computational Aspects of an Order-Sorted Logic with Term Declarations, Springer.
Searle, J. R. (1975) Indirect speech acts, in P. Cole and J. Morgan (eds.), Syntax and Semantics 3: Speech Acts, Academic Press. Reprinted in S. Davis (ed.), Pragmatics. A Reader, Oxford
University Press, 1991.
Selman, B. and Levesque, H. J. (1990) Abductive and default reasoning: A computational core, in
Proceedings of AAAI-90, Boston, MA, pp. 343–348.
Slotta, J. D., Chi, M. T. and Joram, E. (1995) Assessing students’ misclassifications of physics
concepts: An ontological basis for conceptual change, Cognition and Instruction 13(3), 373–400.
Stickel, M. (1988) A Prolog-like inference system for computing minimum-cost abductive explanations in natural-language interpretation, Technical Report 451, SRI International, 333
Ravenswood Ave., Menlo Park, California.
Strube, M., Rapp, S. and Müller, C. (2002) The influence of minimum edit distance on reference
resolution, in Proceedings of Empirical Methods in Natural Language Processing Conference.
Thomason, R. H., Hobbs, J. and Moore, J. D. (1996) Communicative goals, in K. Jokinen, M. Maybury, M. Zock and I. Zukerman (eds.), Proceedings of the ECAI 96 Workshop Gaps and Bridges:
New Directions in Planning and Natural Language Generation.
VanLehn, K., Jordan, P., Rosé, C., Bhembe, D., Böttner, M., Gaydos, A., Makatchev, M., Pappuswamy, U., Ringenberg, M., Roque, A., Siler, S. and Srivastava, R. (2002) The architecture of
Why2-Atlas: A coach for qualitative physics essay writing, in Proceedings of Intelligent Tutoring
Systems Conference, LNCS 2363, Springer, pp. 158–167.
Wahlster, W. and Kobsa, A. (1989) User models in dialogue systems, in A. Kobsa and W. Wahlster
(eds.), User Models in Dialogue Systems, Springer Verlag, Berlin, pp. 4–34.
Walther, C. (1987) A Many-Sorted Calculus Based on Resolution and Paramodulation, Morgan
Kaufmann, Los Altos, California.
Weld, D. S. and de Kleer, J. (eds.) (1990) Readings in Qualitative Reasoning about Physical Systems,
Morgan Kaufmann, San Mateo, California.
Zukerman, I. and Albrecht, D. W. (2001) Predictive statistical models for user modeling, User
Modeling and User-Adpated Interaction 11(1–2), 5–18.
Zukerman, I. and George, S. (2002) A minimum message length approach for argument interpretation, in Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue.
Zukerman, I. and McConachy, R. (1993) Generating concise discourse that addresses a user’s inferences, in Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence,
Morgan Kaufmann Publishers, Inc., pp. 1202–1207.
Zukerman, I., McConachy, R. and Korb, K. B. (2000) Using argumentation strategies in automated
argument generation, in Proceedings of the 1st International Natural Language Generation
Conference, pp. 55–62.

NeuroImage 58 (2011) 675–686

Contents lists available at ScienceDirect

NeuroImage
j o u r n a l h o m e p a g e : w w w. e l s ev i e r. c o m / l o c a t e / y n i m g

The neural correlates of strategic reading comprehension: Cognitive control and
discourse comprehension
Jarrod Moss a,⁎, Christian D. Schunn b, Walter Schneider b, Danielle S. McNamara c, Kurt VanLehn d
a

Department of Psychology, Mississippi State University, USA
Learning Research and Development Center, University of Pittsburgh, USA
Department of Psychology, University of Memphis, USA
d
School of Computing, Arizona State University, USA
b
c

a r t i c l e

i n f o

Article history:
Received 5 October 2010
Revised 25 April 2011
Accepted 13 June 2011
Available online 29 June 2011
Keywords:
Reading comprehension
Reading strategies
fMRI

a b s t r a c t
Neuroimaging studies of text comprehension conducted thus far have shed little light on the brain
mechanisms underlying strategic learning from text. Thus, the present study was designed to answer the
question of what brain areas are active during performance of complex reading strategies. Reading
comprehension strategies are designed to improve a reader's comprehension of a text. For example, selfexplanation is a complex reading strategy that enhances existing comprehension processes. It was
hypothesized that reading strategies would involve areas of the brain that are normally involved in reading
comprehension along with areas that are involved in strategic control processes because the readers are
intentionally using a complex reading strategy. Subjects were asked to reread, paraphrase, and self-explain
three different texts in a block design fMRI study. Activation was found in both executive control and
comprehension areas, and furthermore, learning from text was associated with activation in the anterior
prefrontal cortex (aPFC). The authors speculate that the aPFC may play a role in coordinating the internal and
external modes of thought that are necessary for integrating new knowledge from texts with prior
knowledge.
© 2011 Elsevier Inc. All rights reserved.

Introduction
The importance and difﬁculty of comprehending expository text is
evident to anyone who has attempted to learn about a new ﬁeld of
science by reading a textbook. Comprehension is not a simple process
of accessing word meanings and then combining them. The process of
comprehension involves the construction of a mental representation
of a text, which is referred to as a situation model (e.g., Kintsch, 1998;
Zwaan and Radvansky, 1998). The construction of a situation model
requires lexical processes to access word meanings, memory retrieval
to elaborate on the text and form connections to prior knowledge, and
inference processes to help integrate the current sentence with prior
sentences and knowledge.
The complexity of text comprehension processes results in large
individual differences in the strategies that students utilize to
understand texts as well as what students learn from texts (e.g., Chi
et al., 1989; Just and Carpenter, 1992; McNamara, 2004). Although
there have been neuroimaging studies of text comprehension (e.g.,
Ferstl and von Cramon, 2001; Xu et al., 2005; Yarkoni et al., 2008a,
2008b), these studies have not examined the differences in brain
⁎ Corresponding author at: Department of Psychology, PO Box 6161, Mississippi
State, MS 39762, USA. Fax: + 1 662 325 7212.
E-mail address: jarrod.moss@msstate.edu (J. Moss).
1053-8119/$ – see front matter © 2011 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2011.06.034

activity associated with different reading strategies. Understanding
the neural correlates of different types of strategic reading comprehension processes should help us to better understand the brain
mechanisms underlying comprehension.
Strategic reading comprehension
There are a number of theoretical frameworks that describe the
cognitive processes underlying text comprehension (Kintsch, 1988,
1998; McNamara and Magliano, 2009; Zwaan et al., 1995). Many of
these theories propose that the reader constructs a situation model
that is a representation of text content that abstracts away from the
written form of the sentences composing the text and includes
knowledge not contained directly in the text. Constructing a coherent
situation model requires that the reader form a textbase on the basis
of the propositions contained directly in the text itself, and elaborate
on this information by using prior knowledge through inference
processes (Kintsch, 1988, 1998; Zwaan, 1999; Zwaan and Radvansky,
1998).
The quality of the situation model depends on how successful the
reader is at representing the propositions of the text, providing
information missing from the text from prior domain-general and
domain-speciﬁc knowledge, and forming coherent representations by
drawing inferences across phrases in the text (Kintsch, 1998;

676

J. Moss et al. / NeuroImage 58 (2011) 675–686

McNamara et al., 1996). Characteristics of both the reader and the text
inﬂuence success at forming a good situation model. For some readers,
construction of a situation model is more difﬁcult because they have
little or no prior knowledge about the content of the text (Voss and
Silﬁes, 1996). For example, low domain knowledge readers learn more
from highly cohesive texts while high domain knowledge readers
learn more from low cohesion text (McNamara and Kintsch, 1996;
McNamara et al., 1996). Low domain knowledge readers are
presumably unable to make the necessary inferences from low
cohesion texts, whereas the low cohesion text forces the high domain
knowledge readers to engage in inferencing processes resulting in a
good situation model.
Reading comprehension strategies improve readers' comprehension
of text, and while some readers use strategies naturally, others beneﬁt
from being provided with strategy instruction (McNamara, 2007). Selfexplanation is one reading strategy that has been shown to be highly
effective (Bielaczyc et al., 1995; Chi, 2000; Chi et al., 1989, 1994;
McNamara, 2004). The self-explanation strategy was developed by
observing what good students do naturally when studying worked
examples in physics texts (Chi et al., 1989). Later studies on selfexplanation found that training poor students to self-explain improved
their comprehension and problem solving (e.g., Bielaczyc et al., 1995;
Chi et al., 1994; McNamara, 2004).
Because instructing readers to self-explain most often beneﬁts
readers who are skilled self-explainers more than less skilled selfexplainers (Chi et al., 1994), McNamara (2004) developed SelfExplanation Reading Training (SERT) in which students are provided
with instruction and practice on using reading strategies while selfexplaining texts. This approach combined the technique of selfexplanation with ﬁve reading strategies with demonstrated effectiveness: comprehension monitoring, paraphrasing, elaboration,
bridging, and prediction. Comprehension monitoring is being aware
of whether the text is being successfully understood while reading.
Paraphrasing is putting the text into one's own words in order to help
activate relevant semantic knowledge in long-term memory and
prepare the reader to make further inferences. Inferences are
necessary in text comprehension situations because texts do not
state all relevant pieces of information explicitly (Kintsch, 1998).
Elaboration involves making inferences that aid in understanding the
text by using knowledge from memory. Bridging involves making
inferences across sentence boundaries to aid in understanding the
text. Prediction is making predictions at the end of a sentence or
paragraph about what information will be contained in the next
section of the text.
Collectively, these strategies help the reader to process challenging, unfamiliar material by scaffolding the comprehension process.
The process of self-explaining externalizes the comprehension
process by helping the reader to understand the text (i.e., using
paraphrasing and comprehension monitoring) and go beyond the text
by generating inferences (i.e., using elaboration, bridging, and
prediction). The study presented in this paper uses an intelligent
tutoring system, iSTART (McNamara et al., 2004), to teach the ﬁve
SERT strategies so that the neural correlates of reading comprehension strategies can be examined during comprehension of expository
texts.
Neuroimaging studies of reading comprehension
There have been a number of neuroimaging studies that have
investigated text comprehension (Ferstl and von Cramon, 2001, 2002;
Ferstl et al., 2005; Friese et al., 2008; Hasson et al., 2007; Maguire et al.,
1999; Mar, 2004; Siebörger et al., 2007; Xu et al., 2005; Yarkoni et al.,
2008b). In a recent meta-analysis of neuroimaging studies of text
processing, Ferstl et al. (2008) identiﬁed a set of areas common to
many studies of text processing including the anterior temporal lobe
(aTL), areas along the superior temporal sulcus, inferior temporal

gyrus (ITG), inferior frontal gyrus (IFG), inferior frontal sulcus, presupplementary motor area (pSMA), and the cerebellum. In addition,
they also identiﬁed a set of regions that are associated with coherence
building processes including aTL, posterior superior temporal sulcus,
middle temporal gyrus (MTG), IFG, dorsal and ventral medial
prefrontal cortex (dmPFC and vmPFC), and precuneus. These latter
set of areas as well as the angular gyrus and posterior cingulate cortex
(PCC) are active in studies examining coherence building processes
such as inferencing and linking text content with global themes and
other information in memory (Ferstl and von Cramon, 2001, 2002;
Kuperberg et al., 2006; Maguire et al., 1999; Mellet et al., 2002).
Other discourse comprehension studies have attempted to map
processes such as situation model construction and updating on to
brain regions (e.g., Yarkoni et al., 2008b). In particular, Yarkoni et al.
examined areas that showed a linear increase in activation during
reading that might be associated with maintaining and integrating
information into a situation model as the reader proceeds through the
text. These areas include bilateral aTL, bilateral IFG, bilateral ITG, left
precentral gyrus, bilateral posterior parietal cortex (PPC), left fusiform
gyrus, and right precuneus. In addition, they also found that bilateral
dmPFC was activated exclusively in the story condition. Yarkoni et al.
argue that this dmPFC activation may reﬂect processes of integrating
information into a coherent situation model or that activity in this
area may reﬂect perspective-taking or theory-of-mind processes
associated with the narrative rather than more general comprehension processes. Situation model construction and updating are exactly
the kind of processes that a reading strategy such as self-explanation
is thought to enhance. Thus, it is likely many of these areas would also
be active when self-explaining.
Areas such as dmPFC, the angular gyrus, and the precuneus that
are involved in discourse comprehension are also considered part of
the brain's default network that is active when people are not engaged
in an external task (Buckner et al., 2008; Gusnard et al., 2001; Raichle
et al., 2001). Some studies of discourse processing have noted this
partial overlap between the default network and areas active during
comprehension (Xu et al., 2005; Yarkoni et al., 2008b). The default
network has been associated with self-referential processing and the
mental generation of a coherent scene through the retrieval and
integration of information (Hassabis and Maguire, 2007). These
cognitive processes should be involved in both comprehension and
reading strategies as the goal is to form a coherent representation of
the text being studied, and therefore one hypothesis is that areas such
as dmPFC, the angular gyrus, and the precuneus will be active during
the use of self-explanation.
Expository texts are designed to communicate knowledge often
including technical ideas and terms with which the reader is
unfamiliar embedded in low coherence text (Graesser et al., 2003).
Due to these properties of expository text, implicit and explicit
inference processes are likely to be needed more when processing
expository text than when processing narrative text. The effectiveness
of reading strategies has mostly been examined using expository
texts. However, most neuroimaging studies of discourse processing
have used narrative texts. The comprehension of narrative texts is
thought to be similar to but also different from expository texts
(Graesser et al., 2003; Kintsch, 1998). In particular, it could be
expected that theory-of-mind processes play less of a role in
expository text comprehension while casual and elaborative inferences play a larger role. Examining the brain areas associated with
using reading strategies should provide more information about the
role these areas play in the coherence building processes that are
essential for expository text comprehension.
Current study
The current study examines the brain areas active during performance of reading comprehension strategies that vary in complexity and

J. Moss et al. / NeuroImage 58 (2011) 675–686

effectiveness. Participants were taught self-explanation using iSTART, an
intelligent tutoring system previously found to teach self-explanation
effectively using the SERT strategies (McNamara et al., 2007). Paraphrasing a text to put it into one's own words is another reading strategy that
could be used to aid comprehension, and it is one of the ﬁve SERT
included in iSTART self-explanation training (McNamara et al., 2009).
Finally, a commonly used reading strategy is to simply reread the
material. Rereading is known to be less useful than self-explanation and
is often used as a control condition to evaluate the effectiveness of selfexplanation training (e.g., Chi et al., 1994). Participants were asked to
reread, paraphrase, and self-explain three different expository texts on
biology topics in a block design fMRI study. The comparisons of interest
were between the relative activation of brain areas during performance
of these three reading strategies. Learning was also assessed via
improvement from pretest to posttest. Pre–post data allowed for
veriﬁcation of the expected effectiveness of the reading strategies as
well as an analysis of the brain areas that correlated with measurable
learning.
Because self-explanation is an intentional strategy that enhances a
reader's existing comprehension processes, then it can be expected to
involve areas of the brain that are normally involved in reading
comprehension along with areas that are involved in strategic control
processes. A network of brain areas including dorsolateral prefrontal
cortex (DLPFC), anterior cingulate cortex/pre-supplementary motor
area (ACC/pSMA), dorsal pre-motor cortex (dPMC), anterior insular
cortex (AIC), inferior frontal junction (IFJ), and PPC have been shown
to be active in a variety of tasks involving executive control (Brass et
al., 2005; Chein and Schneider, 2005; Cole and Schneider, 2007;
Dosenbach et al., 2006; Schneider and Chein, 2003; Wager et al.,
2004). These areas also show high functional connectivity (Cole and
Schneider, 2007), and the amount of controlled processing necessary
for a task is related to the degree of activation in these areas (Chein
and Schneider, 2005). To aid in localizing the executive control
network, a variant of the line orientation search task used by Cole and
Schneider (2007) was used as a functional localizer to deﬁne regions
of interest (ROIs) for each subject.
Because reading strategies such as self-explanation are effortful
and complex, we hypothesize that this executive control network will
be active during self-explanation. We also expect lower levels of
activation in this network for less complex reading strategies that do
not involve as much effort and management of complex information,
such as paraphrasing or rereading. It was also predicted that more
complex strategies would show more activation of areas that previous
studies have associated with discourse comprehension. It is an open
question whether strategy effectiveness is primarily a function of
more engagement (as measured by activation of the executive control
network) or primarily a function of speciﬁc text comprehension
processes beyond the executive control components.
Method
Participants
Twenty-two right-handed, native English speakers were recruited
from the University of Pittsburgh and Carnegie Mellon University
communities (14 female, M age = 20.7; SD = 2.4; range = 18–28).
None of the participants were biology majors. One participant was
excluded from analysis due to excessive head motion (more than
9 mm) during the scanning session.
Materials
Three biology texts that were matched on length (approximately
580 words) were selected along with a set of short-answer questions.
Text and question difﬁculty were equated using data from a pilot
study in which another group of participants answered the questions

677

before and after reading and self-explaining the texts. The three texts
discussed the following topics: the process of cell mitosis, the
structure and function of DNA, and the circulatory system's role in
heat transportation. The texts were from different topic areas to
minimize transfer between them. Approximately half of the questions
for each text were text-based, meaning that they could be answered
given information from one sentence in the text. The answers for the
other half of the questions required bridging information across
multiple sentences in the text. Each text was separated into 12
paragraphs, with each paragraph containing 2–4 sentences, so that
they could be presented one paragraph at a time during the study.
Design
Each participant performed all three reading strategies: rereading,
paraphrasing, and self-explaining. Each participant was instructed to
use a given reading strategy to read all of a given text. The assignment
of reading strategies to texts was counterbalanced across participants.
The order in which participants performed the reading strategies was
randomized.
Each text was broken up into three sections consisting of four
paragraphs each. Each of these four-paragraph sections was presented
in a single data acquisition run. Because strategies were assigned to
texts, participants were always performing a single strategy during
each acquisition run. One four-paragraph section of each of the three
texts was presented before the next four-paragraph section of each
text. For example, this organization implies that the ﬁrst (second) and
second (third) blocks of paragraphs from a particular text were
separated by a block of each of the other two texts (e.g., Text1–Block1,
Text2–Block1, Text3–Block1, Text1–Block2, …). The blocks were
presented in this fashion so that each reading strategy would be
performed once in each third of the acquisition session in order to
help control for potential confounding effects (e.g., fatigue).
Procedure
This study took place over two sessions, separated by 2–5 days,
with fMRI data collected only during the second session.
Session 1
During the ﬁrst session, participants were given up to 30 min to
complete a pretest including all of the questions for each of the three
texts. Participants then completed an iSTART session, which provided
instruction on how to self-explain using reading strategies.
iSTART, described in greater detail by McNamara et al. (2004, 2006,
2007), provides students with instruction and practice on how to selfexplain texts using the ﬁve SERT reading strategies: comprehension
monitoring, paraphrasing, elaboration, bridging, and prediction.
iSTART uses animated agents to introduce each of the ﬁve strategies
by having a student agent receive instruction on the strategy by a
teacher agent, and then the student agent uses the strategy. Following
this introduction, for each strategy, the system asks the participant a
set of questions about each strategy and has the participant identify
each strategy in a set of example self-explanations. The participant
then reads one expository text and practices each of the ﬁve strategies
by typing in self-explanations and receiving feedback from the iSTART
system on the content and quality of the self-explanations. iSTART
training took approximately 90 min.
After iSTART training, the participants were provided with task
practice in an MRI simulator. The MRI simulator was designed to
closely simulate the physical conditions of the MRI scanner and
included a magnetic tracking system to track and present feedback to
the participant regarding head movement. The simulator practice was
done to screen for claustrophobia, to train participants to perform the
experiment (especially talking) without excessive head motion, and
to provide them with practice on the experimental task using the

678

J. Moss et al. / NeuroImage 58 (2011) 675–686

same button response system they would use during the scanning
session. In the simulator, participants were presented with 14
paragraphs from two practice texts that were of a similar expository
nature but contained different content than the texts in the
experiment. Before each block of paragraphs, participants read
instructions on the screen indicating the reading strategy they were
to use for that block.
The title of the text was centered on the top of the screen with the
paragraph appearing on the center of the screen. Along the bottom of
the screen was a prompt reminding the participant of the current
strategy. Participants were instructed to read the paragraph aloud
once, and then to press a button on a response glove. Once they did so,
the color of the paragraph's text changed from black to blue which
served as a cue that they were to perform the given reading strategy
aloud. The participants then reread, paraphrased, or self-explained
the text and pressed a button to move to reading the next paragraph.
The paraphrasing and self-explanation strategies had been introduced within iSTART, and thus, participants were provided only brief
instructions on how to either paraphrase or self-explain out loud each
sentence in the text. In the paraphrase condition, participants were
told to put each sentence in the paragraph into their own words
without using any of the other SERT strategies. In the self-explanation
condition, participants were instructed to self-explain each paragraph
using the reading strategies covered in iSTART. For the rereading
condition, they were told to read and then reread each paragraph out
loud until the computer indicated it was time to move to the next
paragraph of text. A prompt, which ﬂashed at the bottom of the screen,
instructed the participant to stop rereading and move on to the next
paragraph. The rereading condition was designed this way in order to
roughly equate the amount of time spent rereading with the amount of
time spent paraphrasing and self-explaining. The amount of time
allotted for rereading was 45 s, which was determined from a pilot
study in which participants applied the three strategies to the same
texts. Paraphrasing and self-explanation were self-paced with the
constraint that the participant could take no longer than 60 s.
Participants were prompted to move on using the same ﬂashing
prompt if they reached 60 s.

Session 2
The second session occurred 2–5 days after the ﬁrst session in
order to reduce the chance that participants would read the passages
with the pretest questions in mind. This session began with an iSTART
practice session lasting at most 30 min, which gave the participants
additional practice self-explaining. This practice session was similar to
the ﬁnal part of the initial iSTART training in which participants read
and self-explained an expository text while receiving feedback on the
self-explanations from iSTART. fMRI data was collected for the
remainder of the session. All tasks were presented using E-Prime
(Schneider et al., 2002) on a Windows PC for task presentation and
response collection. To verify strategy use within each condition,
verbal responses were collected using an active noise canceling
microphone system (Psychology Software Tools, Inc., Pittsburgh, PA),
which almost entirely removed the scanner background noise.
Participants were reminded of the instructions for the experiment
before and after being placed in the scanner. The only difference from
the MRI simulator procedure was that a 30-second rest period was
placed before and after each block of four paragraphs. A ﬁxation cross
was presented in the middle of a white screen for the rest period.
Participants were told to relax and to try not to think about anything
during this time. The participants completed a total of 9 fMRI runs
with each run consisting of 4 paragraphs (3 runs while performing
each of the 3 strategies). Following these 9 learning runs, participants
were presented with a posttest for each text. Although the posttest
was collected in the scanner, we do not examine the posttest imaging
data in this paper.

After the posttest runs, participants were presented with a line
search task that served as a functional localizer to localize activity in
control areas (Saxe et al., 2006). A version of this task has been used in
prior research on executive control (Cole and Schneider, 2007).
Participants received instructions on how to complete this task just
before the start of fMRI data acquisition. The task involved detecting a
target line orientation of 65° by monitoring lines of differing
orientation in four locations on the screen (see Fig. 1). There were
three angles of distractor lines: 85°, 45°, and 155°. The line in each of
the four locations changed orientation every 2 s. Only one location
changed at a time, and the orientation changes proceeded in a
clockwise fashion every 500 ms. Targets appeared at least 2 s apart.
The participants' task was to press a button when the target was
present. A control task was also presented with almost identical visual
stimuli except that the participants' task was to press a button every
time the central ﬁxation cross blinked. The central ﬁxation cross
blinked the same number of times as there were targets in the line
search task while all other stimuli were static. Each participant
completed one to two runs of this task depending on time constraints,
and each run consisted of 4 blocks of each task with blocks alternating
between the line and control tasks. Each block of the tasks began with
6 s of encoding, followed by 30 s of the task (control or line search),
followed by a 6 s delay before the next block began.
In order to increase statistical power in the pretest/posttest
comparison across reading strategy conditions while constraining
the number of fMRI participants, a second group of 14 behavioral
participants was run using the same reading strategy paradigm. The
only differences between the groups were that the behavioral group
was run in front of a computer instead of in the scanner and did not
complete the line search functional localizer task.

Data acquisition and analysis
Structural and functional images were collected on a whole body
Siemens Trio 3 T scanner at the Magnetic Resonance Research Center
of the University of Pittsburgh Medical Center during a 2-hour
scanning session. The scanning session began with the acquisition of
structural images, which included scanner-speciﬁc localizers and
volume anatomical series. The volume anatomical scan was acquired
in a sagittal plane (1 mm 3) using the Siemens MP-RAGE sequence and
the functional data were co-registered to these images. The functional
runs were acquired as 39 oblique-axial slices parallel to the AC–PC
plane using a T2*-weighted echo-planar imaging (EPI) pulse sequence
(TE = 25 ms, TR = 2000 ms, FOV = 21, slice thickness = 3.5 mm with
no gap, ﬂip angle = 76, in-plane resolution = 3.28 mm 2).
The raw neuroimaging data were preprocessed and analyzed using
the AFNI software package (Cox, 1996). Preprocessing included slice
scan time correction, three-dimensional motion correction, and
spatial smoothing. All functional images were realigned to the ﬁrst
image of each run, which were aligned to the ﬁrst run of each subject.
The signal for each voxel was spatially smoothed (7 mm FWHM). Each
subject's MP-RAGE anatomical images were co-registered to their
functional images by applying a transformation to the anatomical
images. The structural and functional images were then transformed
into a canonical Talairach space (Talairach and Tournoux, 1988).
Analyses of the fMRI data used voxel-based statistical techniques.
Unless otherwise speciﬁed, all results were corrected for multiple
comparisons using family-wise error (FWE) cluster size thresholding to
an FWE corrected p-value of less than .05 (Forman et al., 1995). Cluster
sizes were determined using AFNI's Alphasim, which allows for
determination of cluster size using Monte Carlo simulations. At the
individual subject level, general linear models were ﬁt to the data using
a set of boxcar functions convolved with a standard hemodynamic
response function (Boynton et al., 1996). Separate regressors for
reading, rereading, paraphrasing, and self-explaining were included in

J. Moss et al. / NeuroImage 58 (2011) 675–686

679

Fig. 1. Line search task. The top row is the control condition, and the bottom row is the search condition.

the model. Each group-level analysis used a mixed effects model with
subjects as a random factor.
The line search task was used as a functional localizer to deﬁne
subject-speciﬁc ROIs corresponding to the six bilateral areas of the
executive control network (DLPFC, ACC/pSMA, dPMC, AIC, IFJ, PPC).
The line search fMRI data were not spatially smoothed for this
analysis. ROIs corresponding to the control net regions were deﬁned
on the basis of each subject's FWE-corrected statistical map for the
contrast of the line search and control conditions. A corrected p-value
of .05 was obtained by using the combination of a voxel-based p-value
of .01 with a cluster threshold of 6 contiguous voxels. Local peaks of
activation corresponding to the anatomical location of the control net
areas were used to identify each ROI for each subject, and then all
statistically signiﬁcant voxels within a sphere of radius 15 mm from
the peak were included in the ROI.
Results
Behavioral results
The proportions correct on the pretest and posttest were used to
calculate a learning gain score, where gain = (posttest − pretest) / (1−
pretest). This gain score adjusts for the fact that questions already
answered correctly on the pretest cannot be improved upon on the
posttest (Cohen et al., 1999). Due to technical difﬁculties, the recordings
from a portion of two participants' posttests were not available to be
scored. These missing scores corresponded to the paraphrase strategy
for one participant and the self-explanation strategy for another.
The gain scores for the behavioral and imaging participants did not
differ on any of the three conditions (for all comparisons, p N .3), so the
data for these two groups were combined for the analysis of the effect
of strategy on learning. Planned comparisons showed that rereading
gain (M = .41, SD = .26) did not differ from paraphrasing (M = .42,
SD = .22), t b 1. As expected, self-explanation led to greater learning
(M = .51, SD = .19) than paraphrasing, t(32) = 2.41, p = .02, Cohen's
d = 0.4, and rereading, t(33) = 2.03, p = .05, Cohen's d = 0.4.
All participants in the imaging portion of the study performed the
line search task well; d′ was greater than 2 for all participants.
Imaging results
Analysis of areas that were more active in the line search task than
in the control condition conﬁrmed that the task served well as a
functional localizer. As can be seen in Fig. 2, this task activated the
expected set of six bilateral ROIs consistent with prior work on a
domain-general control network (e.g., Chein and Schneider, 2005).

Average percent signal change in the control network ROIs for each of
the three reading strategies relative to the rest condition is presented
in Fig. 3. For each ROI, an ANOVA was run to test for differences
between the three strategies. Bonferroni corrections were used
because 12 separate ANOVAs were conducted. For ANOVAs indicating
a signiﬁcant difference, a series of planned comparisons was used to
determine whether certain strategies activated the control regions
more than other strategies in a particular ROI. The 12 ROIs fell into two
groups. One group did not show any differential activation for the
three strategies. This group included right AIC, right IFJ, and right
DLPFC. The second group, consisting of the remaining 9 control
network ROIs, all showed greater activation for the paraphrase and
self-explanation strategies relative to the reread strategy but no
difference in activation between the paraphrase and self-explanation
strategies. Overall, the results indicate that with the exception of 3
ROIs in the right hemisphere the control network was more active
during performance of paraphrasing and self-explanation, but
the control network did not differentially activate for these two
strategies.
In order to directly examine differences in activation between the
different strategies, a voxel-wise ANOVA with strategy (reread,
paraphrase, self-explain) as a within-subjects factor was conducted
followed by three planned contrasts (paraphrase–reread, selfexplain–reread, and self-explain–paraphrase). Contrasts were done
using the strategy participants had been instructed to perform as well
as using a self-explanation coding process to determine whether they
had indeed self-explained each paragraph. The self-explanation
strategy training consisted of ﬁve separate techniques: comprehension monitoring, paraphrasing, bridging, elaboration, and prediction.
The verbal protocols from both the behavioral and imaging participants were transcribed, and the self-explanation for each paragraph
was coded for whether it contained each of the ﬁve techniques
comprising self-explanation using a coding scheme based on prior
self-explanation research (McNamara, 2004). Inter-rater agreement
between two independent coders was good (89% agreement; Cohen's
kappa = .66). If the self-explanation for a paragraph did not contain
any self-explanation strategy other than paraphrasing, then that selfexplanation was classiﬁed as being in the paraphrase condition. This
reclassiﬁcation resulted in an average of 1.7 out of 12 selfexplanations per participant being reclassiﬁed as paraphrases. The
fMRI results were similar for both versions of this analysis with the
reclassiﬁed data generally showing slightly more signiﬁcant local
maxima, therefore only the reclassiﬁed analysis is reported.
For the contrasts between the reading strategies, activation in the
line search task was examined to identify clusters of activation that fell
both inside and outside of the control net. Tables 1 and 2 show for each

680

J. Moss et al. / NeuroImage 58 (2011) 675–686

Fig. 2. Statistical map for group analysis of areas active in line search functional localizer task projected on to cortical surface (p b .001, minsize = 490 mm3). Statistical maps projected
on to cortical surface. Corresponds to table of regions in supplementary materials. For all ﬁgures, left hemisphere lateral and medial views are on the left of the ﬁgure.

peak whether or not the peak fell within a control net region or not. The
areas more active for the paraphrase condition compared to rereading
are shown in Table 1. Areas outside of the control net included left
pSMA, left IFG, right lingual gyrus, right cerebellum, and bilateral areas
of the basal ganglia. The self-explanation–reread contrast yielded many
of the same regions as the paraphrase–reread contrast as can be seen in
Table 1 and Fig. 4 (see supplementary materials for an image of the
paraphrase–reread contrast). In addition to the areas outside of the
control net seen in the paraphrase–reread contrast, regions of
activation included left dmPFC, left superior frontal gyrus, left
precuneus, left MTG, and the thalamus. Given that many of the peaks
overlapped with the control network, these results are consistent with
the hypothesis that there is engagement of a domain-general control
network with the use of complex reading strategies.

However, the contrast between the self-explanation and paraphrase conditions shows a different pattern of results as seen in
Table 2 and Fig. 5. None of the regions are part of the control network,
and they include bilateral activations in prefrontal cortex, PCC,
precuneus, and the angular gyrus.
An additional analysis was conducted to examine whether the
contrasts between the learning strategies may be explained in part by
production processes that differ across the three reading strategies
rather than comprehension processes. Coh-Metrix (Graesser et al.,
2004) was used to examine the transcribed utterances produced by
participants. Coh-Metrix analyzes text and provides a number of
variables related to the content of the texts being analyzed including
syntactic variables. The variables that Coh-Metrix reported were
examined to see if they differed across the reading strategies.

Fig. 3. Mean signal change and standard error in each executive control network ROI for each reading strategy.

J. Moss et al. / NeuroImage 58 (2011) 675–686

681

Table 1
Local maxima of regions showing positive activation in paraphrase–reread and self-explanation–reread contrasts (p b .001, minsize = 490 mm3).
Regions

Control
net

BA

Frontal cortex
L dPMC
L ACC/pSMA
L ACC
R ACC
R pSMA
L IFJ
L inferior frontal g
L inferior frontal g
L superior frontal g
L superior frontal g
L insula
L inferior frontal g
R dPMC

Partial
Partial
Partial
Yes
Yes
Yes
No
No
No
No
Yes
No
Yes

Parietal cortex
L superior parietal
L precuneus
L parietal/occipital
L inferior parietal
R superior parietal

Self-explanation–reread

Paraphrase–reread

Cluster size (mm3)

x

Peak t

Cluster size (mm3)

x

6
6,32
32
32
6
6,9,44
44,45
13,47
6,8
8
13,45
10,46
6

57,956
–

− 33
− 10

6
13

52
52

11.25
10.61

19
10
13
13
29
33
49
26
36
−7

38
56
31
10
0
52
42
3
17
56

7.06
6.43
8.93
6.68
6.44
5.5
4.6
6
5.28
5.18

47,631
–
–
–
–
–
–
–

− 30
− 10
−7
3
3
− 39
− 49
− 39

0
13
23
19
10
6
16
23

56
52
35
38
56
35
17
−1

10.54
10.77
7.99
6.6
5.97
8.78
8.08
3.34

–
–
–
–
–
–
–
–
–
980

3
3
− 36
− 46
− 43
− 10
− 10
− 26
− 43
20

753
1394

− 33
26

33
0

13
55

5.04
5.43

Yes
No
Yes
Yes
Yes

7
7
7,19
40
7

9345
–
–

− 23
−3
− 26

− 69
− 66
− 66

49
42
31

8.41
7.24
6.9

9646

− 13

− 69

49

10.25

–
3052

− 33
26

− 43
− 69

35
38

6.4
7.4

Temporal cortex
L middle temporal g

No

21,37

2788

Occipital cortex
R lingual g
R middle occipital g

No
Yes

18
19

16
33

− 79
− 79

−8
10

4.86
4.51

Cerebellum/subcortical
R cerebellum
R cerebellum
R cerebellum
R cerebellum
L caudate
L globus pallidus
L midbrain
L thalamus
R globus pallidus
R caudate

No
Yes
No
No
No
No
No
No
No
No

23
39
39
16
− 16
− 13

− 59
− 59
− 46
− 72
10
−4

− 29
− 25
− 46
− 39
14
3

7.62
7.11
4.35
6.37
6.63
8.64

13

0

3

5.35

y

z

− 56

− 46

−4

6.92

980

13

− 82

−8

7.25

528
565

12,548
–
–

23
33
39

− 66
− 49
− 56

− 25
− 29
− 46

9.24
8.11
5.23

− 13
−3
−7
16
16

−4
− 23
− 17
−4
6

3
− 11
17
3
21

9.89
4.71
4.66
8.55
5.11

10,099
–
–
–
4521
–

9345
–
–
3617
–

641

y

z

Peak t

Note. All regions within a connected cluster are presented on consecutive lines. The ﬁrst row within a cluster contains the cluster size, and all regions within the same cluster contain
a ‘–’ for cluster size.

Verbalizations during rereading had fewer verbs and a lower
Flesch Reading Ease score than paraphrases and self-explanations.
Because rereading was just a repetition of the texts, these differences
indicate that verbalizations composed by participants did differ from
the original texts. Paraphrases also differed from rereadings on
variables related to cohesion including noun-stem overlap, temporal
cohesion, and incidence of intentional actions/participles. Selfexplanations had higher frequency words, more adverbs, more
connectives, and a higher proportion of causal participles to causal
verbs than did rereadings.
Self-explanations differed in a number of ways from paraphrasing.
Syntactic differences included more adverbs, a higher proportion of
function words, higher lexical diversity, lower syntactic similarity
across sentences, fewer modiﬁers per noun phrase, and nouns with
lower hypernym value for self-explanations than paraphrases.
Measures of cohesion that differed included lower noun-stem overlap
and more causal verbs/participles for self-explanations than paraphrases. Also, self-explanations had a lower incidence of intentional
actions/events but a higher ratio of intentional participles to
intentional actions/events indicating that intentional cohesion was
higher for self-explanations.
Any variable that differed signiﬁcantly across the strategies was
included as a covariate in a group analysis of the imaging data that
replicated the contrasts reported above. Inclusion of the covariates did

not alter the signiﬁcance or location of any of the peaks reported for
the strategy contrasts.
The previous contrasts examine areas that were more active when
participants were self-explaining. However, another approach to
examining self-explanation is to examine those times when it led to
measurable learning. Thus, a separate analysis was conducted to
examine whether there were brain regions that had activity parametrically modulated by successful learning. This analysis was conducted by
using an amplitude-modulated regressor in addition to the strategy
regressor for the self-explanation runs. The amplitude of this regressor
was based on the gain score for a particular paragraph. The gain score for
each paragraph was calculated by ﬁrst determining for each question on
the pre/posttests in which paragraph the information to answer the
question was presented. Some paragraphs may have mapped to
multiple questions. In this case, the average gain across all questions
mapping to that paragraph was calculated. The regressor for the analysis
was formed by convolving a boxcar function whose amplitude was
determined by the gain score with a hemodynamic response function.
The mean gain score for each subject was subtracted from the
amplitudes to yield a regressor that was used to identify brain areas
exhibiting a linear relation to gain scores (e.g., Buchel et al., 1998).
This learning analysis identiﬁed a set of bilateral prefrontal areas
that were positively associated with learning gains. These areas are
shown in Fig. 6 and Table 3. There were no areas negatively associated

682

J. Moss et al. / NeuroImage 58 (2011) 675–686

Table 2
Local maxima of regions showing positive activation in self-explain–paraphrase
contrast (p b .001, minsize = 490 mm3).
Regions
Frontal cortex
L orbital g
R orbital g
L superior frontal g
R superior frontal g
L anterior cingulate
Parietal Cortex
L posterior cingulate
L posterior cingulate
L precuneus
R posterior cingulate
R precuneus
L angular g
R angular/middle
temporal g

Cluster size
(mm3)

Control
net

BA

942
–
942
–
490

No
No
No
No
No

10
10
9,10
9,10
10,32

−3
3
−7
3
−7

59
49
59
56
49

7
−1
24
28
3

4.77
4.51
5.59
3.98
4.91

12,360
–
–
–
–
4333
2223

No
No
No
No
No
No
No

23,31
23,31
7,31
23,31
7,31
39
37,39

−7
−7
−3
3
7
− 49
46

− 33
− 49
− 69
− 49
− 66
− 66
− 66

35
28
28
28
24
24
10

7.62
6.76
5.32
6.53
5.36
6.1
5.95

− 16

− 43

− 18

5.03

Cerebellum/Subcortical
L cerebellum
565

No

x

y

z

Peak t

Note. All regions within a connected cluster are presented on consecutive lines. The ﬁrst
row within a cluster contains the cluster size, and all regions within the same cluster
contain a ‘–’ for cluster size.

with learning gain. In addition to the areas that were active during
self-explanation, these anterior prefrontal areas were more active
during self-explanation trials during which material was learned well
enough to be answered better on the posttest than the pretest.
Discussion
The results provide evidence that complex reading strategies
engage executive control regions, semantic/comprehension regions,
and bilateral aPFC. The behavioral learning results conﬁrmed that the
three reading strategies differed in effectiveness as hypothesized.
With a relatively short learning period for complex science materials
and a short delay between learning and test, these moderately-sized
learning differences were as expected. With longer delays, there

would likely be further differentiation of results between paraphrasing and rereading as well as between self-explanation and
paraphrasing.
Comparing the least complex strategy, rereading, with the next
most complex strategy, paraphrasing, showed that predominantly
areas known to be involved in executive control were more active for
the more complex strategy. This ﬁnding is consistent with our initial
hypothesis that more complex strategies would require more
engagement and cognitive control. In addition to the control network,
areas of activation identiﬁed in a recent meta-analysis of language
processing included left pSMA and left IFG (Ferstl et al., 2008). The
other active non-control regions included right lingual, right
cerebellum, and portions of the basal ganglia, which have previously
been seen in studies of word and sentence reading (e.g., Joubert et al.,
2004; Xu et al., 2005). Based on these results, it appears that
paraphrasing activates the control network and a portion of the
language processing network more than rereading does.
Self-explanation when contrasted with rereading activated the
same regions as paraphrasing as well as additional areas including left
superior frontal gyrus near the dorsal median wall, left precuneus, left
MTG, and the thalamus. Many of these areas including premotor
cortex and the thalamus are known to be active during word and
sentence processing (e.g., Xu et al., 2005). Areas such as dmPFC, the
precuneus, and MTG have been linked to coherence building
processes including inferencing (Ferstl and von Cramon, 2001,
2002; Ferstl et al., 2008; Friese et al., 2008). In particular, Maguire et
al. (1999) found the same area of the precuneus to be more active
during the second reading of a narrative passage, and they
hypothesized that this area might be associated with the processing
of episodic memories while further developing a mental model of the
text. The self-explanation strategy was designed to promote coherence building processes, and these results support the link between
these brain regions and coherence building cognitive processes.
The control network was not more active for self-explanation than
it was for paraphrasing. The beneﬁts of self-explanation over
paraphrasing were clear in the behavioral learning results, but were
associated with areas outside of the control network. Because these
areas are deﬁned as control areas by the fact that they show practicerelated decreases as more automatic processing occurs (Chein and
Schneider, 2005), then the activation of the control network may be

Fig. 4. Statistical map for group analysis of areas more active in self-explanation than reread projected on to cortical surface (p b .001, minsize = 490 mm3). Corresponds to list of
regions in Table 3. Activation map is very similar to paraphrase–reread contrast map (see supplementary materials).

J. Moss et al. / NeuroImage 58 (2011) 675–686

683

Fig. 5. Statistical map for group analysis of areas more active in self-explanation than paraphrase projected on to cortical surface (p b .001, minsize = 490 mm3). Corresponds to list of
regions in Table 2.

seen as an indication of the amount of controlled processing required.
The effectiveness of self-explanation was never expected to be solely
due to the controlled effort involved, but it is interesting that the more
effective complex reading strategy requires a similar amount of effort
as a less effective one.
The contrast of self-explanation with paraphrase yielded activation in bilateral vmPFC (anterior cingulate and orbital gyri), bilateral
dmPFC (superior frontal gyrus), bilateral precuneus, and left PCC
which were all identiﬁed in a meta-analysis of studies contrasting
coherent with incoherent text (Ferstl et al., 2008). Also, the bilateral
angular gyrus activation found in this contrast is close to the superior
temporal sulcus region found in the same meta-analysis. The overlap
between this contrast and the meta-analysis shows that the regions
more active in self-explanation than paraphrasing are the same
regions known to be involved in coherence building processes while
reading. Most of the studies included in the meta-analysis used
narrative texts or sentences, so this overlap also indicates that the
processing of expository text involves similar brain regions as the
coherence building processes that occur for narrative texts. The only
area identiﬁed by Ferstl et al. (2008) that was not seen in this contrast
is the aTL. The lack of aTL activation is also consistent with other
studies that have examined inferencing in discourse comprehension
(Kuperberg et al., 2006). Ferstl et al. (2008) hypothesize that this
region may be associated with producing a semantic propositional

representation, and it could be that this process is equally important
for rereading, paraphrasing, and self-explaining which is why it was
not seen in our results.
The angular gyrus, PCC, and precuneus have been associated with
relating text to prior knowledge and the use and manipulation of
mental models (Maguire et al., 1999; Mellet et al., 2002; Xu et al.,
2005). The areas active in the MTG in self-explanation are also similar
to areas that have been found when people draw inferences during
text comprehension (Virtue et al., 2006). These are exactly the kinds
of cognitive processes that a reading strategy such as self-explanation
is assumed to engage to support deep comprehension of the text.
An open question is whether there is a special role for right
hemisphere language processing regions in comprehending discourse. Some neuroimaging and neuropsychological studies have
found that the right hemisphere may be more important for discourse
comprehension and making inferences than the left hemisphere (e.g.,
Beeman and Chiarello, 1998; Jung-Beeman, 2005; Lehman-Blake and
Tompkins, 2001; Mason and Just, 2004; St George et al., 1999). The
evidence is mixed as some studies, including a recent meta-analysis,
have not found a differential level of activity in the right hemisphere
during discourse comprehension (e.g., Ferstl and von Cramon, 2001,
2002; Ferstl et al., 2008; Kuperberg et al., 2006). Visual inspection of
Figs. 4 and 5 also shows that if anything activity is left lateralized. In
many cases, regions are activated bilaterally, but the right hemisphere

Fig. 6. Statistical map for areas linearly related to measurable learning gains during self-explanation projected on to cortical surface (p b .01, minsize = 1496 mm3). Corresponds to
list of regions in Table 3.

684

J. Moss et al. / NeuroImage 58 (2011) 675–686

Table 3
Local maxima of regions showing activation for learning regressor (p b .01,
minsize = 1496 mm3).
Regions

Cluster Size (mm3)

BA

x

y

z

Peak t

R inferior frontal gyrus
R superior frontal gyrus
R superior orbital gyrus
R middle frontal gyrus
L superior frontal gyrus
L middle frontal gyrus

4560
–
–
–
2148
–

46
10
10
9
10
9,10

35
23
23
31
− 18
− 33

28
53
41
45
43
39

18
15
0
31
21
28

5.11
4.86
4.21
4.10
5.05
4.86

Note. All regions within a connected cluster are presented on consecutive lines. The ﬁrst
row within a cluster contains the cluster size, and all regions within the same cluster
contain a ‘–’ for cluster size.

was not differentially activated for self-explanation than for either of
the other two strategies even though self-explanation should lead to
more inferences than the other strategies. The evidence in the
literature for a special role of the right hemisphere in inferencing is
mixed, but our results are consistent with other work on inferencing
in discourse comprehension (e.g., Kuperberg et al., 2006) as well as
the meta-analysis by Ferstl et al. (2008) that do not show differential
right hemisphere activity.
While the activation shown while performing self-explanation
seems to be associated with coherence building processes as
expected, it is interesting to note that the contrast between selfexplanation and paraphrase is not a subset of the regions active for the
self-explanation–reread contrast. This pattern of results indicates that
activation of many of the regions in the self-explanation–paraphrase
contrast was similar to the reread condition. Many of the regions in
the self-explanation–paraphrase contrast are part of the default
network (Buckner et al., 2008; Raichle et al., 2001). There are a
number of possible interpretations for the highly consistent pattern of
activity that deﬁnes the default network, but many of these
explanations focus on an internal mode of thought that is stimulus
independent self-guided thought (Buckner et al., 2008). These
stimulus-independent thoughts have been associated with lapses in
attention (Weissman et al., 2006) and mind wandering (Christoff et
al., 2009), but this mode of thought is also thought to have adaptive
purposes (Bar, 2007; Hassabis and Maguire, 2007). One explanation
for our results is that during rereading participants were engaging this
same network for the purposes of self-directed thought or mind
wandering instead of processing text. Rereading is not a particularly
demanding task especially because our participants repeated the
same paragraph two or more times in a row so that they spent the
same amount of time rereading as self-explaining and paraphrasing.
This less demanding strategy could have left enough time and
attention free that mind-wandering occurred to some degree while
rereading.
In support of this explanation of increased mind wandering during
rereading, we have data from a recent fMRI study on mind wandering
during reading strategies using a similar methodology as the current
study (Moss et al., 2011). In this follow-up study, participants rated
the frequency of mind wandering while performing the reading
strategies after each short paragraph. Mind wandering ratings were
signiﬁcantly higher for rereading than for self-explanation (p b .05),
and marginally higher for rereading than for paraphrasing (p b .06).
This data suggests that some of the differences between the rereading
contrasts and the self-explanation–paraphrase contrast may be due to
mind wandering.
Rereading is also different from paraphrasing and self-explanation
because it does not require the generation and production of new
sentences as the other two strategies do. While the inclusion of
covariates related to syntactic complexity did not alter the results, the
covariate analysis does not completely rule out production planning
and other production-related differences in the contrasts between

rereading and the other strategies. In fact, the generation of new
sentences beyond those contained in the text is an inherent difference
between rereading and the more effective strategies. The design of
this study does not permit the separation of the reread contrast results
into comprehension versus production related regions. This limitation
provides the basis for future work on understanding the neural
correlates of strategic reading comprehension.
It has been found that the default network is anti-correlated with
attentional and executive control areas (Fox et al., 2005). Effective
reading strategies appear to strongly activate both executive control
areas as well as default mode areas. These default mode areas likely
perform similar functions during rest and during comprehension. One
possibility is that effective reading strategies are explicit strategies
that involve intentionally carrying out a sequence of actions, but that
these strategies intentionally involve functions like memory retrieval,
mental simulation, and information integration that are performed
during mind wandering and other forms of self-directed thought as
well.
The analysis of the areas that were correlated with the amount
learned during self-explanation mainly included bilateral aPFC. That
is, in addition to the activity in executive control and text
comprehension areas associated with self-explanation, the aPFC was
more active during self-explanation of paragraphs where measurable
learning took place. Maguire et al. (1999) also found that a similar
region of the left aPFC was associated with the number of idea units
recalled after reading a narrative, and it was also active while listening
to a second repetition of the story. They hypothesized that this area is
associated with retrieval success. Alternatively, a recent theory of
aPFC function refers to it as a router or gateway between modes of
thought (Burgess et al., 2005, 2007). One of these modes of thought is
one in which external representations (i.e., objects in the environment) drive thought, and the other mode is one in which internal
representations drive thought. This gateway hypothesis might help to
explain the correlation of the aPFC with learning in this study. The
aPFC might be helping to coordinate the reading and processing of the
text presented on the screen with the internal retrieval of memories
and construction of situation models. It may also reﬂect the
coordination of an explicit strategy with the types of internal thought
normally associated with the default network. Self-explanation may
be most effective in aiding learning when there is a good deal of
strategic processing of internal representations.

Conclusions
This initial exploration of the neural correlates of strategic reading
comprehension has shown that networks of areas associated with
executive control and the manipulation of internal representations
and memories underlie the effectiveness of these strategies. Selfexplanation produced greater learning gains than the other two
strategies, and performing self-explanation led to greater activation in
areas associated with executive control as well as discourse comprehension areas involved in the maintenance and manipulation of
internal representations to build coherent situation models. The
results show that the beneﬁts of self-explanation are not solely due
to increased engagement of the executive control network because
paraphrasing activated the control network to a similar degree.
Instead, co-activation of the control network and discourse comprehension areas distinguished self-explanation from the less effective
strategies. In addition, aPFC activation was associated with learning
gains while performing self-explanation. Future work should explore
the role of aPFC in reading strategies as well as whether these results
will generalize to other texts and other types of texts, such as
narratives.
Supplementary materials related to this article can be found online
at doi:10.1016/j.neuroimage.2011.06.034.

J. Moss et al. / NeuroImage 58 (2011) 675–686

Acknowledgments
This work was supported by The Defense Advanced Research
Projects Agency (NBCH090053). The views, opinions, and/or ﬁndings
contained in this article are those of the authors and should not be
interpreted as representing the ofﬁcial views or policies, either
expressed or implied, of the Defense Advanced Research Projects
Agency or the Department of Defense. The authors would like to thank
Melissa Thomas, Kevin Jarbo, and Adrienne McGrail for their
assistance with data collection.

References
Bar, M., 2007. The proactive brain: using analogies and associations to generate
predictions. Trends Cogn. Sci. 11, 280–289.
Beeman, M.J., Chiarello, C., 1998. Complementary right- and left-hemisphere language
comprehension. Curr. Dir. Psychol. Sci. 7, 2–8.
Bielaczyc, K., Pirolli, P.L., Brown, A.L., 1995. Training in self-explanation and selfregulation strategies: investigating the effects of knowledge acquisition activities
on problem solving. Cogn. Instr. 13, 221–252.
Boynton, G.M., Engel, S.A., Glover, G.H., Heeger, D.J., 1996. Linear systems analysis of
functional magnetic resonance imaging in human V1. J. Neurosci. 16, 4207–4221.
Brass, M., Derrfuss, J., Forstmann, B., Cramon, D.Y., 2005. The role of the inferior frontal
junction area in cognitive control. Trends Cogn. Sci. 9, 314–316.
Buchel, C., Holmes, A.P., Rees, G., Friston, K.J., 1998. Characterizing stimulus–response
functions using nonlinear regressors in parametric fMRI experiments. NeuroImage
8, 140–148.
Buckner, R.L., Andrews-Hanna, J.R., Schacter, D.L., 2008. The brain's default network:
anatomy, function, and relevance to disease. Ann. N. Y. Acad. Sci. 1124, 1–38.
Burgess, P.W., Simons, J.S., Dumontheil, I., Gilbert, S.J., 2005. The gateway hypothesis of
rostral PFC function. In: Duncan, J., Phillips, L., McLeod, P. (Eds.), Measuring the
Mind: Speed Control and Age. Oxford University Press, Oxford, pp. 215–246.
Burgess, P.W., Dumontheil, I., Gilbert, S.J., 2007. The gateway hypothesis of rostral
prefrontal cortex (area 10) function. Trends Cogn. Sci. 11, 290–298.
Chein, J.M., Schneider, W., 2005. Neuroimaging studies of practice-related change: fMRI
and meta-analytic evidence of a domain-general control network for learning.
Brain Res. Cogn. Brain Res. 25, 607–623.
Chi, M.T.H., 2000. Self-explaining: the dual processes of generating inference and
repairing mental models. In: Glaser, R. (Ed.), Advances in Instructional Psychology.
Lawrence Erlbaum Associates, Mahwah, NJ, pp. 161–238.
Chi, M.T.H., Bassok, M., Lewis, M.W., Reimann, P., Glaser, R., 1989. Self-explanations:
how students study and use examples in learning to solve problems. Cogn. Sci. 13,
145–182.
Chi, M.T.H., Deleeuw, N., Chiu, M.H., Lavancher, C., 1994. Eliciting self-explanations
improves understanding. Cogn. Sci. 18, 439–477.
Christoff, K., Gordon, A.M., Smallwood, J., Smith, R., Schooler, J.W., 2009. Experience
sampling during fMRI reveals default network and executive system contributions
to mind wandering. Proc. Natl. Acad. Sci. U. S. A. 106, 8719–8724.
Cohen, P., Cohen, J., Aiken, L.S., West, S.G., 1999. The problem of units and the
circumstance for POMP. Multivariate Behav. Res. 34, 315–346.
Cole, M.W., Schneider, W., 2007. The cognitive control network: integrated cortical
regions with dissociable functions. NeuroImage 37, 343–360.
Cox, R.W., 1996. AFNI: software for analysis and visualization of functional magnetic
resonance neuroimages. Comput. Biomed. Res. 29, 162–173.
Dosenbach, N.U., Visscher, K.M., Palmer, E.D., Miezin, F.M., Wenger, K.K., Kang, H.C.,
Burgund, E.D., Grimes, A.L., Schlaggar, B.L., Petersen, S.E., 2006. A core system for
the implementation of task sets. Neuron 50, 799–812.
Ferstl, E.C., von Cramon, D.Y., 2001. The role of coherence and cohesion in text
comprehension: an event-related fMRI study. Brain Res. Cogn. Brain Res. 11,
325–340.
Ferstl, E.C., von Cramon, D.Y., 2002. What does the frontomedian cortex contribute to
language processing: coherence or theory of mind? NeuroImage 17, 1599–1612.
Ferstl, E.C., Rinck, M., von Cramon, D.Y., 2005. Emotional and temporal aspects of
situation model processing during text comprehension: an event-related fMRI
study. J. Cogn. Neurosci. 17, 724–739.
Ferstl, E.C., Neumann, J., Bogler, C., von Cramon, D.Y., 2008. The extended language
network: a meta-analysis of neuroimaging studies on text comprehension. Hum.
Brain Mapp. 29, 581–593.
Forman, S.D., Cohen, J.D., Fitzgerald, M., Eddy, W.F., Mintun, M.A., Noll, D.C., 1995.
Improved assessment of signiﬁcant activation in functional Magnetic Resonance
Imaging (fMRI): use of a cluster-size threshold. Magn. Reson. Med. 33, 636–647.
Fox, M.D., Snyder, A.Z., Vincent, J.L., Corbetta, M., Van Essen, D.C., Raichle, M.E., 2005.
The human brain is intrinsically organized into dynamic, anticorrelated functional
networks. Proc. Natl. Acad. Sci. U. S. A. 102, 9673–9678.
Friese, U., Rutschmann, R., Raabe, M., Schmalhofer, F., 2008. Neural Indicators of
inference processes in text comprehension: an event-related functional magnetic
resonance imaging study. J. Cogn. Neurosci. 20, 2110–2124.
Graesser, A.C., McNamara, D.S., Louwerse, M.M., 2003. What do readers need to learn in
order to process coherence relations in narrative and expository text. In: Sweet, A.P.,
Snow, C.E. (Eds.), Rethinking Reading Comprehension. Guilford Publications, New
York, NY, pp. 82–98.

685

Graesser, A.C., McNamara, D.S., Louwerse, M.M., Cai, Z., 2004. Coh-Metrix: analysis of
text on cohesion and language. Behav. Res. Methods 36, 193–202.
Gusnard, D.A., Akbudak, E., Shulman, G.L., Raichle, M.E., 2001. Medial prefrontal cortex
and self-referential mental activity: relation to a default mode of brain function.
Proc. Natl. Acad. Sci. U. S. A. 98, 4259–4264.
Hassabis, D., Maguire, E.A., 2007. Deconstructing episodic memory with construction.
Trends Cogn. Sci. 11, 299–306.
Hasson, U., Nusbaum, H.C., Small, S.L., 2007. Brain networks subserving the extraction of
sentence information and its encoding to memory. Cereb. Cortex 17, 2899.
Joubert, S., Beauregard, M., Walter, N., Bourgouin, P., Beaudoin, G., Leroux, J., Karama, S.,
Lecours, A.R., 2004. Neural correlates of lexical and sublexical processes in reading.
Brain Lang. 89, 9–20.
Jung-Beeman, M., 2005. Bilateral brain processes for comprehending natural language.
Trends Cogn. Sci. 9, 512–518.
Just, M.A., Carpenter, P.A., 1992. A capacity theory of comprehension: individual
differences in working memory. Psychol. Rev. 99, 122–149.
Kintsch, W., 1988. The role of knowledge in discourse comprehension: a construction–
integration model. Psychol. Rev. 95, 163–182.
Kintsch, W., 1998. Comprehension: A Paradigm for Cognition. Cambridge University
Press, Cambridge.
Kuperberg, G.R., Lakshmanan, B.M., Caplan, D.N., Holcomb, P.J., 2006. Making sense of
discourse: an fMRI study of causal inferencing across sentences. NeuroImage 33,
343–361.
Lehman-Blake, M.T., Tompkins, C.A., 2001. Predictive inferencing in adults with right
hemisphere brain damage. J. Speech Lang. Hear. Res. 44, 639–654.
Maguire, E.A., Frith, C.D., Morris, R.G.M., 1999. The functional neuroanatomy of comprehension and memory: the importance of prior knowledge. Brain 122, 1839–1850.
Mar, R.A., 2004. The neuropsychology of narrative: story comprehension, story
production and their interrelation. Neuropsychologia 42, 1414–1434.
Mason, R.A., Just, M.A., 2004. How the brain processes causal inferences in text. Psychol.
Sci. 15, 1–7.
McNamara, D.S., 2004. SERT: self-explanation reading training. Discourse Process 38,
1–30.
McNamara, D.S., 2007. Reading Comprehension Strategies: Theory, Interventions, and
Technologies. Erlbaum, Mahwah, NJ.
McNamara, D.S., Kintsch, W., 1996. Learning from texts: effects of prior knowledge and
text coherence. Discourse Process 22, 247–288.
McNamara, D.S., Magliano, J., 2009. Towards a comprehensive model of comprehension. In: Ross, B.H. (Ed.), The Psychology of Learning and Motivation. Academic
Press, New York, pp. 297–384.
McNamara, D.S., Kintsch, E., Songer, N.B., Kintsch, W., 1996. Are good texts always
better? Interactions of text coherence, background knowledge, and levels of
understanding in learning from text. Cogn. Instr. 14, 1–43.
McNamara, D.S., Levinstein, I.B., Boonthum, C., 2004. iSTART: interactive strategy
training for active reading and thinking. Behav. Res. Methods Instrum. Comput. 36,
222–233.
McNamara, D.S., O'Reilly, T.P., Best, R.M., Ozuru, Y., 2006. Improving adolescent
students' reading comprehension with iSTART. J. Educ. Comput. Res. 34, 147–171.
McNamara, D.S., O'Reilly, T., Rowe, M., Boonthum, C., Levinstein, I., 2007. iSTART: a web-based
tutor that teaches self-explanation and metacognitive reading strategies. Reading
Comprehension Strategies: Theories, Interventions, and Technologies, pp. 397–421.
McNamara, D.S., Boonthum, C., Kurby, C.A., Magliano, J., Pillarisetti, S.P., Bellissens, C.,
2009. Interactive paraphrase training: the development and testing of an iSTART
module. Proceedings of the 2009 Conference on Artiﬁcial Intelligence in Education:
Building Learning Systems that Care: From Knowledge Representation to Affective
Modeling. IOS Press, pp. 181–188.
Mellet, E., Bricogne, S., Crivello, F., Mazoyer, B., Denis, M., Tzourio-Mazoyer, N., 2002.
Neural basis of mental scanning of a topographic representation built from a text.
Cereb. Cortex 12, 1322–1330.
Moss, J., Schunn, C.D., Schneider, W., McNamara, D.S., 2011. An fMRI study of zoning out
during strategic reading comprehension. Proceedings of the Thirty-third Annual
Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society.
Raichle, M.E., MacLeod, A.M., Snyder, A.Z., Powers, W.J., Gusnard, D.A., Shulman, G.L.,
2001. A default mode of brain function. Proc. Natl. Acad. Sci. U. S. A. 98, 676–682.
Saxe, R., Brett, M., Kanwisher, N., 2006. Divide and conquer: a defense of functional
localizers. NeuroImage 30, 1088–1096.
Schneider, W., Chein, J.M., 2003. Controlled & automatic processing: behavior, theory,
and biological mechanisms. Cogn. Sci. 27, 525–559.
Schneider, W., Eschman, A., Zuccolotto, A., 2002. E-Prime User's Guide. Psychology
Software Tools Inc., Pittsburgh, PA.
Siebörger, F.T., Ferstl, E.C., von Cramon, D.Y., 2007. Making sense of nonsense: an fMRI
study of task induced inference processes during discourse comprehension. Brain
Res. 1166, 77–91.
St George, M., Kutas, M., Martinez, A., Sereno, M.I., 1999. Semantic integration in
reading: engagement of the right hemisphere during discourse processing. Brain
122, 1317–1325.
Talairach, J., Tournoux, P., 1988. Co-planar Stereotaxic Atlas of the Human Brain.
Thieme, New York.
Virtue, S., Haberman, J., Clancy, Z., Parrish, T., Jung Beeman, M., 2006. Neural activity of
inferences during story comprehension. Brain Res. 1084, 104–114.
Voss, J.F., Silﬁes, L.N., 1996. Learning from history text: The interaction of knowledge
and comprehension skill with text structure. Cogn. Instr. 14, 45.
Wager, T.D., Jonides, J., Reading, S., 2004. Neuroimaging studies of shifting attention: a
meta-analysis. NeuroImage 22, 1679–1693.
Weissman, D.H., Roberts, K.C., Visscher, K.M., Woldorff, M.G., 2006. The neural bases of
momentary lapses in attention. Nat. Neurosci. 9, 971–978.

686

J. Moss et al. / NeuroImage 58 (2011) 675–686

Xu, J., Kemeny, S., Park, G., Frattali, C., Braun, A., 2005. Language in context: emergent
features of word, sentence, and narrative comprehension. NeuroImage 25, 1002–1015.
Yarkoni, T., Speer, N.K., Balota, D.A., McAvoy, M.P., Zacks, J.M., 2008a. Pictures of a
thousand words: investigating the neural mechanisms of reading with extremely
rapid event-related fMRI. NeuroImage 42, 973–987.
Yarkoni, T., Speer, N.K., Zacks, J.M., 2008b. Neural substrates of narrative comprehension and memory. NeuroImage 41, 1408–1425.

Zwaan, R.A., 1999. Situation models: the mental leap into imagined worlds. Curr. Dir.
Psychol. Sci. 8, 15–18.
Zwaan, R.A., Radvansky, G.A., 1998. Situation models in language comprehension and
memory. Psychol. Bull. 123, 162–185.
Zwaan, R.A., Langston, M.C., Graesser, A.C., 1995. The construction of situation
models in narrative comprehension: an event-indexing model. Psychol. Sci. 6,
292–297.

Shall We Explain? Augmenting Learning from
Intelligent Tutoring Systems and Peer Collaboration
Robert G.M. Hausmann, Brett van de Sande, and Kurt VanLehn
Pittsburgh Science of Learning Center, University of Pittsburgh,
3939 O’Hara Street, Pittsburgh, Pa, 15260-5179
{bobhaus,bvds}@pitt.edu, vanlehn@cs.pitt.edu

Abstract. Learning outcomes from intelligent tutoring systems (ITSs) tend to
be quite strong, usually in the neighborhood of one standard deviation. However, most ITS designers use the learning outcomes from expert human tutoring
as the gold standard (i.e., two standard deviations). What can be done, with the
current state of the art, to increase learning from an ITS? One method is to
modify the learning situation by asking students to use the ITS in pairs. To enhance performance, we drew upon the beneficial effects of structured peer collaboration. The results suggest that the intervention was successful. Pairs of
students solved more problems and requested fewer bottom-out hints than individuals. To test the possibility that the effect was due to the best partner in the
group directing the problem solving, a nominal groups analysis was conducted.
A nominal group is a statistical pairing of the non-interacting individuals’ performance. The results from the nominal groups replicated the same pattern of
results, but with a reduced magnitude. This suggests that the best member may
have contributed to some of the overall success of the pair, but does not completely explain their performance.
Keywords: Collaborative learning; explanation activities; studying examples.

1 Introduction
An often-heard suggestion is that students may learn more from an intelligent tutoring
system (ITS) if two students worked together on the system instead of working on it
alone. Early studies did not support this hypothesis, and instead suggested that having
pairs of students using an ITS produced the same learning gains as having students
work alone as they used it [early studies with Lisp & geometry tutors]. However, null
results are often open to many interpretations, such as a lack of statistical power. This
issue has been re-opened recently, and for good reasons. The simplest reason for reopening the pair/solo hypothesis is that, despite the designers’ best efforts, the hints
given by an ITS are sometimes more confusing than helpful. Perhaps having two
students interpret the hints may help alleviate this problem.
Another reason for studying pairs using an ITS is that learning gains can be impressive when the students in a pair actually collaborate, as opposed to one student dominating the problem solving. Collaborators can encourage each other’s reasoning, notice and
productively resolve conflicts in their thinking, confirm each other’s beliefs, externalize
B. Woolf et al. (Eds.): ITS 2008, LNCS 5091, pp. 636–645, 2008.
© Springer-Verlag Berlin Heidelberg 2008

Shall We Explain? Augmenting Learning from Intelligent Tutoring Systems

637

their thoughts, and so on. Collaborators could use an ITS to catch mistakes that the students manage to overlook, to provide hints when they have exhausted their mutual
knowledge, or to resolve conflicts that they cannot resolve themselves. In short, metacognitive strategies exist for using an ITS as an effective scaffolding for peer problem
solving.
Lastly, many observers have noticed that students working alone often abuse the
ITS by asking for more help or less help than they need [1]. In particular, many ITSs
give increasingly specific hints when asked, and the last “bottom-out” hint tells the
student exactly what step to enter. Some students ask for bottom-out hints on almost
every step. Conversely, some students will enter incorrect versions of a step dozens of
times without asking for a hint. Although it is possible that students do not know how
to use the help system effectively, experiments with a help-seeking tutor have shown
that explicitly teaching students effective help-seeking strategies did not change their
long-term behavior [2]. They went back to abusing the ITS as soon as the helpseeking tutor was replaced by the regular ITS. This suggests that students know how
to seek help effectively, but they sometimes choose otherwise.
One way to reduce the frequency of help misuse may be to have students work in
pairs. If both students know that rapidly pressing the hint button in order to get the
bottom-out hint is bad for learning, then such abuse would only occur if they both
simultaneously agree to be “bad.” Similarly, they would have to both agree to enter
mistaken entries repeatedly without asking for help, even though they both know this
could be a waste of time. So perhaps having students work in pairs on an ITS could
increase the frequency of proper help usage, compared to students working alone.
In short, there are at least three reasons why pairs of student should learn more than
individuals using an ITS: (1) pairs may be able to interpret the ITS’s hints more successfully, (2) the ITS may help student collaborate effectively, and (3) pairs of students are less likely to abuse the ITS than individuals.
Although the first and third hypotheses are somewhat novel, there has been a considerable amount of work on the second hypothesis [3, 4]. For example, Rummel and
Spada [5] contrasted learning and problem solving under four different conditions.
The first was a detailed, worked-out example of successful collaboration. Participants
were asked to study the example of smooth collaboration, and apply it to their own
dialog. A second group was provided with a collaboration script, in which case their
interactions were structured in a way that was hypothesize to promote successful
collaborative outcomes. Finally, there were two control conditions that did not structure the collaborative interactions. They found that the example and scripted conditions demonstrated better understanding of successful collaboration principles, as well
as a better understanding of the domain (i.e., therapy and diagnosis of medical and
psychological disorders) than the control conditions.
The laboratory study by Rummel and Spada suggests that successful collaborative
interactions can be instructed and scripted. Walker et al. [6] extended this finding by
developing a cognitive tutor that teaches students how to interact in the context of
peer tutoring. They developed a peer-tutoring script that assists students along three
dimensions. The first dimension prepares students for peer tutoring by providing
instruction on the domain content, as well as relevant pedagogical strategies. The
second dimension involves actual tutoring, whereby the tutor is taught to set goals for
the student, as well as monitor the student’s progress during problem solving. The last

638

R.G.M. Hausmann, B. van de Sande, and K. VanLehn

dimension introduces skills for effective interaction, such as providing elaborated
help. These three dimensions were used to construct a cognitive tutoring system for
peer collaboration, thus reifying the empirical results on effective peer tutoring and
collaboration.
Although the available evidence suggests that ITSs can have a positive impact on
collaborative learning (i.e., hypothesis 2), research addressing the collaborative use of
ITS hints has been relatively sparse (i.e., hypotheses 1 and 3). However, the empirical
work on collaboratively studying worked-out examples, which is reviewed in the next
section, may be of some relevance because requesting a string of bottom-out hints can
turn a problem into a worked-out example.
1.1 Collaborative Example Studying
So far, we have discussed the hypothesis that pairs would be more effective than individuals when they work on an ITS, but similar remarks apply to studying examples as
well. By example, we mean a problem plus a presentation of the multiple steps required
for its solution. When individuals study an example, they sometimes self-explain it by
filling in the gaps between steps, relating the inter-step reasoning to prior knowledge, etc.
[7]. Sometimes students do anticipatory self-explanation, where they try to generate the
next solution step themselves, then look at the example to see if they are right [8].
Prompting can increase the amount of self-explanation [9, 10]. However, even with
prompting, students can abuse an example just as they can abuse an ITS. They abuse an
example by simply reading it shallowly and not trying to self-explain much of it.
When discussing pairs using an ITS, we suggested that they may be more effective
than individuals for 3 reasons: (1) pairs may be able to interpret the ITS’s hints more
successfully, (2) the ITS may help student collaborate effectively, and (3) pairs of
students are less likely to abuse the ITS than individuals. Those same three reasons
apply to examples as well. (1) A pair may be more able to interpret an examples’
reasoning more successfully than an individual student. (2) An example scaffolds
collaborators by helping them extend their reasoning when they get stuck, resolve
conflicts productively, externalize their reasoning, confirm their beliefs, etc. (3) Pairs
are less likely to abuse an example than individuals.
Laboratory evidence, which suggests that pairs may be better suited for studying
examples than individuals, can be found in [11]. In their experiment, participants were
asked to study some instructional materials collaboratively, then solve LISP programming problems individually. The pairs’ performance on the LISP problems was
contrasted with individuals studying the instructional materials alone. They found that
the programming performance of the pairs was significantly better than the solo student performance; however, the authors note that the advantage for collaboration
diminished over time.
The present study is another step toward understanding if and when “two heads are
better than one” for learning. The study compares pairs vs. solo students who are both
studying examples and solving problems with an ITS. As they study examples, they
are prompted to self-explain. This study is preliminary in that we did not use pre-tests
and post-tests, and thus cannot measure students’ learning gains. However, we did
record their behavior during the training in order to determine if it was affected by the
solo/pair manipulation. Thus, this study counts as a manipulation check for a subsequent study of learning gains.

Shall We Explain? Augmenting Learning from Intelligent Tutoring Systems

639

Although verbal protocols were collected, they have not yet been fully analyzed, so
this paper reports only the analyses of log files generated by the tutoring system. Using them, we found evidence that pairs abused the help system less frequently than
solo students, as predicted. We also looked for but failed to find signs of collaborative
facilitation, in that pairs would make fewer errors due to collaboration than nominal
pairs. On the other hand, the pairs did no worse than the nominal pairs, so there was
no process loss [12]. Thus, all the current evidence is positive—two heads may indeed
be better than one for explaining examples and solving problems with an ITS.
1.2 Problem Solving and Example Studying in an ITS
The Andes physics tutor was initially developed as a replacement for paper and pencil
homework problems. The advantage for solving problems with Andes is the adaptive
support it provides to the student. One form of adaptive support is the on-demand
hints, which are provided in a graded fashion. Typically, the first hint points the student’s attention to a relevant feature of the problem (i.e., a Pointing Hint). The second
hint level presents general instructional principles that are relevant to the problem
(i.e., a Teaching Hint). Finally, at the terminal level, the bottom-out hint tells the
student exactly which action to take (i.e., a Bottom-out Hint).
In addition to the on-demand hints, Andes provides a series of videos that the students can watch in order to learn about various solution strategies, as well as how to
use elements of the Andes interface. For the purposes of the current study, we modified the available videos so that they were broken down into individual problemsolving steps. Typically, students are responsible for starting and stopping the videos,
but that generally leads to shallow cognitive processing of the video content. Instead,
at the juncture of each step, we prompted the students to engage in an explanatory
activity. Solo students and pairs were prompted to generate explanations while studying video-based, worked-out examples. The purpose of prompting students was to
increase their cognitive processing of the examples.
The use of examples in the present experiment slightly diverges from traditional
studies in the sense that students were prompted to engage in explanation while studying an isomorphic worked-out example after solving a related problem. We used this
design for two reasons. First, the ACT-R theory of learning suggests that students
only learn from the correct application of knowledge [13]. Second, the cognitive load
associated with problem solving can impede a deep encoding of the problem-solving
goals and operators [14].

2 Method
The following experiment was designed to test the effects of collaboration on problem
solving and example studying while using an ITS, primarily with an emphasis on the
collaborative use of hints.
2.1 Participants
Thirty-nine undergraduates, enrolled in a second semester physics course, were randomly assigned to one of two experimental conditions: solo students (n = 11) or pairs

640

R.G.M. Hausmann, B. van de Sande, and K. VanLehn

(n = 14). Volunteers were recruited from several sections of a second-semester physics course, which covered Electricity and Magnetism. Participants were recruited
during the third week of the semester, with the intention that the experimental materials would coincide with their introduction in the actual physics course. The participants were paid $10 per hour. To ensure that the participants’ motivation remained
high during the entire two-hour session, they were offered an incentive of an additional $10 for doing well on the tests, which they all received.
2.2 Materials
The materials developed for this experiment were adapted from an earlier experiment
[15]. The domain selected for this experiment was electro-dynamics with a focus on
the definition of the electric field, which is expressed by the vector equation: F = qE.
This particular topic is typically covered within the first few weeks of a secondsemester physics course. Thus, it is an important concept for students to learn because
it represents their first exposure to the idea that a field can exert a force on a body.
To instruct the participants, several materials were developed. Four electrodynamics problems were created. These problems are representative of typical problems found at the end of a chapter in a traditional physics textbook. The problems
covered a variety of topics, including the definition of the electric field, Newton’s first
and second law, the weight law, and several kinematics equations. Each of the four
problems was implemented in Andes. Andes was chosen because its design allowed
for both the presentation of video-based examples, as well as coached problem solving. The first problem served as a warm-up problem because none of the students had
any prior experience with the Andes user interface.
In addition to the problems, three examples were created in collaboration with two
physics instructors at the U.S. Naval Academy. The examples contained a voice-over
narration of an expert solving the problems, and they were structured such that they
were isomorphic to the immediately preceding problem.
2.3 Procedure
The procedure consisted of several activities. The first activity was to watch a short,
introductory video on the Andes user interface. Afterwards, the participants read instructions on how to produce explanations, including an example. Next, participants
were asked to use Andes to solve a warm-up problem. The experimenter was available to answer any user-interface questions. He was not, however, allowed to give
away any domain-specific information. During problem solving, the student had access to the flag feedback, the hint sequences, and an Equation Cheat Sheet. Once the
student submitted a final answer, she then watched and explained an example of an
expert solution of an isomorphic problem. The example solutions were broken down
into steps, and at the conclusion of each step the student was prompted to explain
(either individually or collaboratively). Once the explanation was complete, the participant clicked a button to go onto the next step. Only the cover story and given
values differed between the problem-solving and example problems. The students
alternated between solving problems and studying examples until all four problems
were solved and all three examples were studied, or until two hours elapsed.

Shall We Explain? Augmenting Learning from Intelligent Tutoring Systems

641

2.4 Measures
Several dependent measures, taken from the log files, were used to assess problemsolving performance, including: the number of entries, correct entries, solution rate,
and the number of bottom-out hint requests.

3 Results
The results are organized into two sections. The first reports performance differences
between the solo students and pairs at the problem level. The second section then
reports the same dependent measures using a “nominal group analysis," which is
considered the gold standard for collaborative research [16]. A nominal group is a
statistical pairing of the non-interacting individuals. To construct a nominal group,
individuals from the solo condition were randomly paired, and the best performance
from each individual was taken to represent the pair.
3.1 Performance Differences
Before delving into the problem-solving performance measures, we first analyzed the
solution rates (i.e., whether or not a final answer was found) for two reasons. First, the
students worked at their own pace; and second, the experiment was capped at two
hours. Thus, there was no guarantee that all of the students would finish all of the
problems in the allotted time. The pairs were much more likely to submit an answer to
the final problem than the solo students (χ2 = 4.81, p = .03).
An analysis of the mean number of entries and correct entries for the final problem
confirmed the solution rate results. The pairs (M = 34.29, SD = 6.72) demonstrated
more entries than the solos (M = 25.00, SD = 14.97), F(1, 23) = 4.32, p = .05, d = .87.
Moreover, the pairs (M = 23.29, SD = 5.06) demonstrated reliably more correct entries for the final problem than the solos (M = 15.64, SD = 9.85), F(1, 23) = 6.36, p <
.02, d = 1.06. Taken together, these results suggest that the pairs were more efficient
in solving the problems during the two-hour experiment.
To test if the participants abused the available help, bottom-out hint requests were
analyzed. Requesting multiple bottom-out hints is an indication that the student required more direct instruction, and this may have translated into gaming the system
behaviors. However, if the student requests a reasonable number of bottom-out hints,
then that is an indication that she is more interested in connecting the information
found in the instructional materials (i.e., examples) to the individual problem-solving
steps.
The bottom-out hint usage interacted with time, such that the difference between
conditions was apparent early in the experiment, but diminished over time. To correct
for Type II errors due to multiple statistical tests across the four problems, a repeated
measures ANOVA was used. The univariate tests, which contrasted the two conditions for each problem, indicated that the solos demonstrated marginally higher bottom-out hint requests for the warm-up problem (F(1, 21) = 3.98, p = .06), reliably
more hint requests for the first problem (F(1, 21) = 7.64, p = .01), and no reliable
differences for the final two problems (see Fig. 1).

642

R.G.M. Hausmann, B. van de Sande, and K. VanLehn

Fig. 1. The mean number of bottom-out hint requests per problem for solos and pairs

3.2 Nominal Group Analyses
One of the dangers of working in a collaborative setting is the threat of requiring more
time to complete a task than it would when working alone. To test if there was a loss
in efficiency, we compared real pairs to the nominal pairs by measuring the amount of
time taken to input each correct entry. The results suggest there were no time penalties for working in a group. In fact, there was a small amount of evidence to the contrary. On average, real pairs (M = 47.96, SD = 17.17) demonstrated faster times between correct entries for the first problem than the nominal pairs (M = 67.05, SD =
16.45), F(1, 23) = 7.89, p = .01, d = 1.18.
In addition, the dependent measures used to contrast problem-solving performance
between the two experimental conditions were repeated for the real pairs and the

Fig. 2. The mean number of bottom-out hint requests per problem, including nominal pairs

Shall We Explain? Augmenting Learning from Intelligent Tutoring Systems

643

nominal pairs. For the final problem, the nominal pairs (M = 33.54, SD = 9.15) submitted an equal number of entries for the final problem as the real pairs (M = 34.29,
SD = 6.72). However, the number of correct entries made for the final problem was
replicated. There was a marginal difference between the two groups in terms of the
correct number of entries made on the last problem, F(1, 23) = 3.16, p = .09, d = .75.
The real pairs (M = 23.29, SD = 5.06) entered marginally more correct entries for the
final problem than the nominal pairs (M = 18.73, SD = 7.73).
In terms of the bottom-out hint requests, the pattern of results was also consistent
with the solo results (see Fig. 2). The real pairs still requested fewer bottom-out hints
for the Problem 1 than the nominal pairs, F(1, 23) = 5.11, p = .03, d = .95. None of
the other contrasts reached traditional levels of statistical significance.

4 Discussion
The introduction to this paper proposed three hypotheses regarding collaboration
during solving problems with an ITS. The first hypothesis stated that pairs may be in
a better position to profit from an ITS’s hints than individuals because each student
may interpret the hint in a different way. Through the process of sharing and debugging their various interpretations, pairs of students can help each other make sense of
the hints. Evidence for this claim can be found in both the completion rate and the use
of bottom-out hints. The pairs progressed further into the problem set than the individuals, and they required fewer bottom-out hints to finish the problems.
The second hypothesis stated that a step-based ITS may help students collaborate
more effectively. Although, the present study did not directly test this hypothesis (i.e.
by contrasting the frequency of successful collaborative processes for step-based
tutoring with a more open learning environment), we indirectly tested the hypothesis
by conducting a nominal groups analysis. Nominal groups were formed by randomly
pairing individuals from the solo condition and taking the best performance from each
member. For example, if Solo Member A asked for 3 bottom-out hints, and Solo
Member B asked for 2, then the score for that nominal group on the bottom-out hint
measure was “2.” However, if Solo A correctly imputed 8 steps, and Solo B entered 5
correct steps, then the score for that nominal pair was “8.” Therefore, the source of
the nominal pair’s score could come from a different individual for the different
measures of problem-solving performance.
The results from the nominal-groups analysis replicated the set of results from the
solos. Although the magnitude of the differences between pairs and solos was reduced, the same trend of results was observed. This suggests that there was something
special about participating in a collaborative discussion while solving problems with
an ITS. That is, the tutoring system helped to scaffold the dialog between interacting
students above and beyond the performance of the non-interacting individuals.
Finally, the third hypothesis stated that pairs of students should be less likely to
abuse the ITS than individuals because students have a general sense of the proper use
of a learning environment. Stated differently, having a partner keeps the individuals
honest. Evidence for the third hypothesis was most directly demonstrated with the
bottom-out hint requests. Pairs of students requested an average of 67.6% fewer bottom-out hints across the entire 2-hour experiment. The difference in bottom-out hint

644

R.G.M. Hausmann, B. van de Sande, and K. VanLehn

requests between the pairs and solos was most pronounced after studying the first
example (i.e., Warm-up Problem = 67.9% vs. Problem 1 = 85.6%). This suggests that
the pairs may have also been less likely to abuse the examples. Instead of shallowly
processing the content, they may have better comprehended and later reused the information in the examples. In the future, we plan to test this hypothesis more directly
by analyzing the verbal protocols produced while studying the examples.
In summary, the results from each of the three hypotheses suggest that asking students to solve problems collaboratively, with a step-based tutoring system, is a productive way to enhance learning from an ITS. This study, which served as a positive
example of a manipulation check, suggests that future experiments continue to examine the boundary conditions under which collaboration is effective in an ITS. Additional measures of learning need to be used to evaluate the strength of the learning
that results from collaboration. For example, the present study does not indicate if
learning from collaboration will transfer to individual problem solving and to novel
domains. Additional research is needed to answer these and related questions.
Acknowledgements. This work was supported by the Pittsburgh Science of Learning
Center, which is funded by the National Science Foundation award number SBE0354420.

References
1. Baker, R.S., Corbett, A.T., Koedinger, K.R., Wagner, A.Z.: Off-Task behavior in the cognitive tutor classroom: When students game the system. In: Proceedings of ACM CHI
2004: Computer-Human Interaction, pp. 383–390 (2004)
2. Roll, I., Aleven, V., McLaren, B.M., Koedinger, K.R.: Can help seeking be tutored?
Searching for the secret sauce of metacognitive tutoring. In: Luckin, R., Koedinger, K.R.,
Greer, J. (eds.) Artificial intelligence in education: Building technology rich learning contexts that work, vol. 158, pp. 203–210. IOS Press, Amsterdam (2007)
3. Barros, B., Conejo, R., Guzman, E.: Measuring the effect of collaboration in an assessment
environment. In: Luckin, R., Koedinger, K.R., Greer, J. (eds.) Artificial intelligence in
education: Building technology rich learning contexts that work, vol. 158, pp. 375–382.
IOS Press, Amsterdam (2007)
4. Biswas, G., Leelawong, K., Schwartz, D.L., Vye, N.: The Teachable Agents Group at Vanderbilt: Learning by teaching: A new agent paradigm for educational software. Applied
Artificial Intelligence 19, 363–392 (2005)
5. Rummel, N., Spada, H.: Learning to collaborate: An instructional approach to promoting
collaborative problem solving in computer-mediated settings. Journal of the Learning Sciences 14, 201–241 (2005)
6. Walker, E., McLaren, B.M., Rummel, N., Koedinger, K.: Who says three’s a crowd? Using a cognitive tutor to support peer tutoring. In: Luckin, R., Koedinger, K.R., Greer, J.
(eds.) Artificial intelligence in education: Building technology rich learning contexts that
work, vol. 158, pp. 399–406. IOS Press, Amsterdam (2007)
7. Chi, M.T.H., Bassok, M.: Learning from examples via self-explanations. In: Resnick, L.B.
(ed.) Knowing, learning, and instruction: Essays in honor of Robert Glaser, pp. 251–282.
Lawrence Erlbaum Associates, Inc., Hillsdale (1989)

Shall We Explain? Augmenting Learning from Intelligent Tutoring Systems

645

8. Renkl, A.: Learning from worked-out examples: A study on individual differences. Cognitive Science 21, 1–29 (1997)
9. Aleven, V.A.W.M.M., Koedinger, K.R.: An effective metacognitive strategy: Learning by
doing and explain with a computer-based Cognitive Tutor. Cognitive Science 26, 147–179
(2002)
10. Chi, M.T.H., DeLeeuw, N., Chiu, M.-H., LaVancher, C.: Eliciting self-explanations improves understanding. Cognitive Science 18, 439–477 (1994)
11. Bielaczyc, K., Pirolli, P., Brown, A.L.: Collaborative explanations and metacognition:
Identifying successful learning activities in the acquisition of cognitive skills. In: Ram, A.,
Eiselt, K. (eds.) Proceedings of the Sixteenth Annual Cognitive Science Society Conference, pp. 39–44. Laurence Earlbaum Associates, Hillsdale (1994)
12. Steiner, I.D.: Group processes and productivity. Academic Press, New York (1972)
13. Anderson, J.R., Corbett, A.T., Koedinger, K., Pelletier, R.: Cognitive tutors: Lessons
learned. The Journal of the Learning Sciences 4, 167–207 (1995)
14. Sweller, J.: The worked example effect and human cognition. Learning and Instruction 16,
165–169 (2006)
15. Hausmann, R.G.M., VanLehn, K.: Explaining self-explaining: A contrast between content
and generation. In: Luckin, R., Koedinger, K.R., Greer, J. (eds.) Artificial intelligence in
education: Building technology rich learning contexts that work, vol. 158, pp. 417–424.
IOS Press, Amsterdam (2007)
16. Lorge, I., Fox, D., Davitz, J., Brenner, M.: A survey of studies contrasting the quality of
group performance and individual performance. Psychological Bulletin 55, 337–372
(1958)

37

Evaluation of a meta-tutor for constructing models of
dynamic systems
Lishan Zhang, Winslow Burleson, Maria Elena Chavez-Echeagaray, Sylvie Girard,
Javier Gonzalez-Sanchez, Yoalli Hidalgo-Pontet, Kurt VanLehn
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe,
AZ, 85281, U.S.A.
{lishan.zhang, winslow.burleson, mchaveze, sylvie.girard, javiergs, yhidalgo,
kurt.vanlehn}@asu.edu

Abstract. While modeling dynamic systems in an efficient manner is an important skill to acquire for a scientist, it is a difficult skill to acquire. A simple
step-based tutoring system, called AMT, was designed to help students learn
how to construct models of dynamic systems using deep modeling practices. In
order to increase the frequency of deep modeling and reduce the amount of
guessing/gaming, a meta-tutor coaching students to follow a deep modeling
strategy was added to the original modeling tool. This paper presents the results
of two experiments investigating the effectiveness of the meta-tutor when compared to the original software. The results indicate that students who studied
with the meta-tutor did indeed engage more in deep modeling practices.
Keywords: meta-tutor , intelligent tutoring systems, empirical evaluation

1

Introduction

Modeling is both an important cognitive skill [1] and a potentially powerful means of
learning many topics [5]. The AMT system teaches students how to construct system
dynamics models. Such models are widely used in professions, often taught in universities and sometimes taught in high schools.
1.1

The modeling language, development tool and tutoring system

In our modeling language, a model is a directed graph with one type of link. Each
node represents both a variable and the computation that determines the variable’s
value. Links represent inputs to the calculations. As in illustration, Figure 1 shows a
model for the following system:
The initial population of bacteria is 100. The number of bacteria born each
hour is 10% of the population. Thus, as the population increases, the number
of births increases, too. Model the system and graph the population over 20
hours.
Clicking on a node opens an editor with these tabs (and 2 others not described here):

38

x

Description: The student enters a
description of the quantity represented
by the node.
x Inputs: The student selects inputs to
the calculation of the node’s value.
x Calculation: The student enters a
formula for computing the node’s
value in terms of the inputs.
There are three types of nodes in models:
x A fixed value node represents a constant value that is directly specified in
Fig. 1. A simple model.
the problem. A fixed value node has a
diamond shape, never contains incoming links, and its calculation is just a single
number. For instance, “growth rate” has 0.1 as the calculation of its value.
x An accumulator node accumulates the values of its inputs. That is, its current value is the sum of its previous value plus or minus its inputs. An accumulator node
has a rectangular shape and always has at least one incoming link. For instance,
the calculation tab of “population” states that its initial value is 100 and its next
value is its current value + births.
x A function node’s value is an algebraic function of its inputs. A function node has
a circular shape and at least one incoming link. For instance, “births” has as its
calculation “population * growth rate.”

The students’ task is to develop a model that represents a system described by a
short text. They can create, edit and delete nodes using the node editor. When all the
nodes have calculations, students can click the Run Model button, which performs
calculations and draws graphs of each nodes’ values over time. The system described
so far is just a model development tool.
AMT has a simple tutoring capability. Each tab of the node editor has a Check
button which turns its fields red if they are incorrect and green if they are correct.
Each tab also has a Give up button that fills out the tab correctly. Thus, the system
described so far is just a simple step-based tutoring system with minimal feedback on
demand and only one kind of hint: a bottom-out hint.
1.2

The meta-tutor

Unfortunately, it is a rare for students to think semantically in terms of what the
nodes, inputs and calculations mean actually mean. Students prefer to think of model
elements syntactically, like puzzle pieces that need to be fit together. This shows up
in a variety of ways, including rapid guessing, nonsensical constructions and the use
of syntactic rather than semantic language to refer to model elements. The literature
on model construction (reviewed in [5]) sometimes refers to these two extremes as
Deep vs. Shallow modeling. The objective of the AMT system is to increase the relative frequency of Deep modeling.

39

A variety of methods for increasing the frequency of Deep modeling have been
tried [5]. For instance, nodes can bear pictures of the quantities they represent, or
students can be required to type explanations for their calculations. One of the most
promising methods is procedural scaffolding, wherein students are temporarily required to follow a procedure; the requirement is removed as they become competent.
This technique was used by Pyrenees [2], where it caused large effect sizes.
We adapted Pyrenees’ procedure to our modeling language and called it the Target
Node Strategy. The strategy requires students to focus on one node, called the target
node, and completely define it before working on any other node. This decomposes
the whole modeling problem into a series of atomic modeling problems, one per node.
The atomic modeling problem is this: Given a quantity, find a simple calculation that
will compute its values in terms of other quantities without worrying about how those
other quantities values will be calculated. This is a much smaller problem than the
overall challenge of seeing how the overall model can be constructed.
As an illustration, let us continue the bacteria population example and suppose that
the target node is “number of bacteria born per hour.” The ideal student might think:
“It says births are 10% of the population, so if I knew population, then I could figure out the number of births. In fact, I could define a node to hold the 10%, and
then the calculation would multiply it and population. But do I need initial population or current population? Oh. The number of bacteria born is increasing, so I
must need current population, because it is also increasing.”
This is one form of deep modeling. By requiring students to finish one node before
working on another, the Target Variable Strategy encourages students to examine the
system description closely because it is the only resource that provides relevant information. When they are allowed to work on any tab on any node, then they jump
around trying to find a tab that can be easily filled in. This is a common form of shallow modeling, and the Target Node Strategy discourages it.
In addition to requiring the students to follow the Target Node Strategy, the metatutor nags students to avoid guessing and abuse of the Give Up button, just as the
Help-Tutor [3] did. Because neither the strategy nor the advice on help seeking are
specific to the domain (e.g., population dynamics), we consider them to be metacognitive instruction.

2

Evaluation

2.1

Experiment Design

The experiment was designed as a between-subject single treatment experiment with a
control condition, where the meta-tutor was off, and an experiment condition, where
the meta-tutor was on. The difference between the conditions occurred only during a
training phase where students learned how to solve model construction problems. In
order to assess how much students learned, a transfer phase followed the training
phase. During the transfer phase, all students solved model construction problems
with almost no help: the meta-tutor, the Check button and the Give-up button were all
turned off, except in the Description tab where the Check button remained enabled to

40

facilitate grounding. Because system dynamics is rarely taught in high school, no pretest was included in the procedure. We conducted two experiments with 44 students
participating in the first experiment and 34 students in the second experiment.
2.2

Hypotheses and Measures

Hypothesis 1 is that the meta-tutored students will use deep modeling more frequently than the control students during the transfer phase. We used the three measures
below to assess it.
x The number of the Run Model button presses per problem.
x The number of extra nodes created, where extra nodes are defined as the nodes that
can be legally created for the problem but are not required for solving the problem.
x The number of problems completed during the 30 minute transfer period.
Hypothesis 2 is that meta-tutored students will use deep modeling more frequently
than the control group students during the training phase. The three dependent
measures used to evaluate this hypothesis are described below:
x

x
x

Help button usage: was calculated as (nwc+3ngu)/nrn, where nwc is the number of
Check button presses that yielded red, ngu is the number of Give-up button presses, and nrn is the number of nodes required by the problem.
The percentage of times the first Check was correct.
Training efficiency: was calculated as 3ncn – ngu where ncn is the number of
nodes the student completed correctly (3ncn is the number of tabs), and ngu is the
number of Give-up buttons presses.

Hypothesis 3 is that the experimental group students, who were required to follow the
Target Node Strategy during training, would seldom use it during the transfer phase.
To evaluate this hypothesis, we calculated the proportion of student steps consistent
with the target node strategy.
2.3

Results

Table 1 summarizes the results of experiment 1 and experiment 2.

3

Conclusion and future work

Although we achieved some success in encouraging students to engage in deep modeling, there is much room for improvement. If the meta-tutor had been a complete
success at teaching deep modeling, we would expect to see students supported by the
meta-tutor working faster than the control students. The stage is now set for the last
phase of our project, where we add an affective agent to the system [4], in order to
encourage engagement and more frequent deep modeling.

41

Measure (predicted dir.)

Experiment 1 (N=44)

Experiment 2 (N=33)

Transfer phase (Hypothesis 1)
Run model button usage (E<C)

E<C (p=0.31, d=0.32)

E§C (p=0.98, d=-0.0093)

Extra nodes (E<C)
Probs completed (E>C)

E<C (p=0.02, d=0.80)
E§C (p=0.65, d=0.04)

E<C (p=0.47, d=0.26)
E<C (p=0.09, d=í0.57)

Training phase (Hypothesis 2)
E<C (p=0.04, d=0.68)
E<C (p=0.02, d=0.89)
Missing data
E>C (p=0.015, d=0.98)
E>C (p=0.59, d=0.19)
E<C (p=0.05, d=0.70)
Transfer phase use of Target Node Strategy (Hypothesis 3)
Usage (E=C)
Missing data
E§C (p=0.59, d=í0.19).
Help button usage (E<C)
Correct on 1st Check (E>C)
Efficiency (E>C)

Table 1. Results of Experiment 1 and 2: E stands for the meta-tutor group, and C stands for
the control group. Reliable results are bold.

Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 0910221.

References:
1. CCSSO.: The Common Core State Standards for Mathematics, Downloaded from
www.corestandards.org on October 31 (2011)
2. Chi, Min, & VanLehn, K.: Meta-cognitive strategy instruction in intelligent tutoring systems: How, when and why. Journal of Educational Technology and Society, 13(1). 25-39
(2010)
3. Roll, I., Aleven, V., McLaren, Bruce, Ryu, Eunjeong, Baker, R.S.J.d., & Koedinger, K. R.:
The Help Tutor: Does metacognitive feedback improve student's help-seeking actions,
skills and learning. In M. Ikeda, K. Ashley & T.-W. Chan (Eds.), Intelligent Tutoring Systems: 8th International Conference, pp. 360-369. Berlin: Springer (2006)
4. Girard, S., Chavez-Echeagaray, M. E., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., Zhange,
L., Burleson, W. & VanLehn, K.: Defining the behavior of an affective learning companion in the Affective Meta-Tutor project. In Proceedings of AI in Education (2013)
5. Treagust, David F., Chittleborough, Gail, & Mamiala, Thapelo.: Students' understanding of
the role of scientific models in learning science. International Journal of Science Education, 24(4), 357-368 (2002)
6. VanLehn, K. (in press). Model construction as a learning activity: A design space and review. Interactive Learning Environments.

Journal of Automated Reasoning 32: 3–33, 2004.
© 2004 Kluwer Academic Publishers. Printed in the Netherlands.

3

GRAMY: A Geometry Theorem Prover Capable
of Construction
NOBORU MATSUDA and KURT VANLEHN
Intelligent Systems Program, University of Pittsburgh.
e-mail: mazda@pitt.edu, vanlehn@cs.pitt.edu
Abstract. This study investigates a procedure for proving arithmetic-free Euclidean geometry theorems that involve construction. “Construction” means drawing additional geometric elements in
the problem figure. Some geometry theorems require construction as a part of the proof. The basic
idea of our construction procedure is to add only elements required for applying a postulate that
has a consequence that unifies with a goal to be proven. In other words, construction is made only
if it supports backward application of a postulate. Our major finding is that our proof procedure is
semi-complete and useful in practice. In particular, an empirical evaluation showed that our theorem
prover, GRAMY, solves all arithmetic-free construction problems from a sample of school textbooks
and 86% of the arithmetic-free construction problems solved by preceding studies of automated
geometry theorem proving.
Key words: automated geometry theorem proving, construction, search control, constraint satisfaction problem, intelligent tutoring system.

1. Introduction
Geometry theorem proving has been a challenging problem for automated reasoning systems. Indeed, some of the earliest work in automated reasoning used
geometry theorem proving as the task domain (Gelernter, 1959; Gelernter et al.,
1963; Reiter, 1972; Wong, 1972), and work has continued to the present time
(see, for example, Chou et al., 2000). A particularly challenging issue is to prove
theorems that require constructions, namely, to find proofs with additional lines,
points, or arcs constructed by a compass and a straightedge. No robust and efficient method for geometry theorem proving with construction is known so far.
Finding a construction is a hard task even for human problem solvers. Since one
can draw many segments and arcs at any point of a proof, the search space is
enormous.
Our long-term goal is to build an intelligent tutoring system for elementary
geometry. Any proofs and constructions found by our automated geometry theorem
prover must be stated with the common ontology of Euclidean geometry – the
axiomatized geometry system taught in schools. Thus, the algebraic techniques
 This research was supported by NSF grant number 9720359 to CIRCLE, Center for Interdisci-

plinary Research on Constructive Learning Environment.

4

NOBORU MATSUDA AND KURT VANLEHN

used in some earlier studies on geometry theorems (e.g., the area method discussed
in Chou et al., 1996) are not suitable for our purpose. The simplicity of the proof
procedure and the readability of the proofs are also important design issues.
The desired intelligent tutoring system will be built with a standard technique
called model tracing (Anderson et al., 1990) that requires a “complete” model
of reasoning just as an instructor is supposed to be “omniscient.” If the student
enters a step that is acceptable to instructors but the tutoring system does not
have it in its model’s reasoning, then the tutoring system will tell the student that
the step is unacceptable, which could have devastating consequences for learning.
Thus the desired geometry theorem prover must not only be able to find a single
comprehensible proof, it should also be able to find all proofs that are considered
acceptable to instructors. Accordingly, ad hoc heuristics that reduce the search
space are unacceptable because they may exclude proofs that the tutoring system
needs.
We discovered a construction procedure that is simple and involves no ad hoc
heuristics. We incorporated it into an automated geometry theorem prover called
GRAMY. In addition to presenting the construction procedure and GRAMY, this
paper addresses the following research questions:
– Is the construction procedure complete?
– Is the construction procedure efficient enough to run on a personal computer?
In the following sections, we first review the basic issues of geometry theorem
proving and give a brief history of the study. We then introduce our proof and construction procedure in Section 3. GRAMY’s performance is evaluated in Section 4,
followed by discussion and future work in Section 5.
2. Overview of Geometry Theorem Proving with Construction
2.1. THE PROBLEM AND DEFINITIONS
The target domain is elementary Euclidean geometry. In this study, we deal only
with proofs of equality and congruence that do not involve arithmetic operations
(i.e., sums and multiplications). Typical elements are points, segments, angles,
triangles, and their quantitative properties (e.g., length of a segment, degree of an
angle). Typical relations among elements are equivalence, congruence, perpendicularity, parallelism, coincident (X and Y intersect at Z), membership (X is a part
of Y ), and their negations.
A problem consists of a set of given propositions, a proposition to be proved,
and a diagram called the problem figure. GRAMY represents propositions as formulae in restricted first-order logic. Diagrams both for problem figures and for
other purposes are represented by the Cartesian coordinate system. A proposition is
 This restriction implies that the proofs of inequalities, ratios, and coincident intersections (i.e.,

to prove that three or more segments are intersecting at one point) are also excluded from the present
study.

GRAMY

5

called quantitatively satisfied if the relation stated in the proposition is consistent
with the measurements of the corresponding geometric elements in the problem
figure. For example, a proposition AB = CD is quantitatively satisfied if two
segments AB and CD in the diagram are approximately the same length.
In this article, the term “postulates” refers to the true statements given to the
theorem prover such as definitions, axioms, and theorems that have been shown to
be true. A postulate consists of premises and a consequence that are represented as
propositions. Each postulate is associated with a generic diagram that represents
topological information that is not explicitly represented in the premises and the
conclusions.
The output from GRAMY is proofs, each of which is a sequence of postulate
applications with or without construction. Construction is represented by a set of
geometric elements that have been added to the problem figure. The prover must
ensure that the construction can be drawn with a compass and a straightedge. An
example of invalid construction is to add lines that divide a given angle into three
equal angles, as trisection of an angle is known not to be possible with a compass
and a straightedge.
GRAMY finds proofs and constructions that hold within a given problem figure.
In general, one must find a universal proof that, in some case, must be conditional, thus requiring different problem figures that are consistent with the given
propositions. In most cases, however, classroom instruction requires students to
find a proof only for a particular problem figure and does not ask them to make
conditional proofs, so that is all GRAMY does.
Geometry theorem proving in general can be viewed as a state-space search.
Major components of a state are a problem figure, a set of propositions either
given or derived, and goals to prove. There are two kinds of operators to search
through the problem space: the operators for deduction and for construction. An
application of a deduction operator corresponds to an application of geometric
postulates, which changes either the set of propositions by forward chaining or
the set of goals by backward chaining. An application of a construction operator
changes the problem figure in a way that corresponds to construction by a compass
and a straightedge.
Since this study does not deal with arithmetic operators, applying a postulate
introduces no new geometric elements; hence, a proof has finitely many geometric
elements without construction. Furthermore, since there are finitely many relations,
there are finitely many possible relationships among the elements and hence finitely
many true propositions. The implication is that one can apply postulates at most
only finitely many times with no repetitive deductions. Therefore, regardless of
whether a proof is found or not, there is a state where no new propositions can
be derived. We call this state a quiescence state. The propositions in a quiescence
state are the deductive closure of the given propositions with respect to the given
postulates. In this paper, forward chaining from an arbitrary state to a quiescence
state is called exhaustive forward chaining. If a proof exists without construction,

6

NOBORU MATSUDA AND KURT VANLEHN

then there must be a proposition in the quiescence state that unifies with the goal
to prove.

2.2. BRIEF HISTORY OF STUDIES OF AUTOMATED THEOREM PROVING IN
GEOMETRY

Although different types of geometry theorem provers have been developed so far,
this section surveys only those studies that involve construction.
2.2.1. Proof Method: Axiomatic vs. Algebraic
The previous provers fall into one of two categories. The axiomatic approach uses
a set of axioms and inference rules to formulate a sequence of deductions as a
proof. Also, it typically uses heuristics to prune or guide the search. The algebraic
approach translates a set of postulates as well as a theorem to prove into equations
in such a way that solving the equations corresponds to finding a proof for the
theorem.
The algebraic approach is known to be powerful in that it can find proofs for
very hard problems. Examples of practical provers based on the algebraic approach
include the characteristic set method (Wu, 2000), the elimination method (Wang,
1995), the Gröbner basis method (Kapur, 1986), and the Clifford algebra approach
(Li, 2000). A drawback to these efficient approaches is that they are seldom comprehensible for students learning Euclidean geometry. As an exception, the area
method (Chou et al., 1996) is axiomatized by so-called area axioms that are intuitive enough for students to understand. As a result, the theorem prover can output
a “readable” proof as a sequence of transitions via axiom applications. A drawback
is that the students must be taught the area axioms instead of the standard Euclidean
axioms. Hence, a theorem prover based on the area method is hardly an appropriate
application for geometry education in current school systems.
One interesting aspect of the area method is that for a certain class of geometry,
the area method proves problems that require construction. This class is characterized as constructive geometry (Chou and Gao, 1989). However, there are problems
that do not fall into the constructive geometry class that can be proved by Euclidean
axioms with construction (for example, problem P108 shown in Appendix A).
Most automated theorem provers that deal with construction use the axiomatic
paradigm. They all add geometric elements to the problem figure so that a desired postulate will apply. The challenge is to focus only on a limited number of
constructions. Most provers utilize ad hoc heuristics to avoid unproductive constructions. One of the educational defects of those approaches is that they force
students to learn many ad hoc heuristics as well. Another problem is that it is
hard to show the completeness for theorem provers that use ad hoc heuristics. The
next few sections review construction heuristics and other aspects of the axiomatic
approach.

GRAMY

7

2.2.2. Heuristics of Construction
Heuristics of construction can be divided into three types: (1) constructing new
segments by only connecting existing points, (2) drawing segments and points
that are sufficient to make desired postulates applicable, and (3) applying ad hoc
construction heuristics.
Connecting Existing Points. The simplest heuristic is to draw a new line by connecting any existing two points (Anzai et al., 1979; Elcock, 1977; Gelernter, 1959;
Greeno et al., 1979). This heuristic does not introduce new points except the intersections of new segments and existing ones. Hence a construction procedure that
applies only this heuristic is not complete.
Making a Desired Postulate Applicable. In many construction problems, a particular postulate has a consequence that unifies with the goal to prove but its entire
configuration does not match with the problem figure. Hence, one heuristic is to
add elements to the figure that will enable the postulate to apply. Most of the construction procedures in the literature fall into this category. Two major issues here
are (1) selecting a desired postulate to apply and (2) ensuring that the constructed
elements can be drawn with a compass and a straightedge. Most previous work
resolves both issues by considering construction only for a particular postulate.
Nevins (1975) implemented construction only for the right-angle triangle axiom.
Elcock (1977) designed a procedure to construct a new point as an intersection
of two existing segments. Greeno et al. (1979) implemented a construction for
congruent triangles sharing a segment. Coelho and Pereira (1986) implemented
construction only for the quadrilateral axiom.
Ad Hoc Heuristics. Performance of a theorem prover could be improved by adding
ad hoc heuristics for construction. For example, Wong (1972) proposed a heuristic,
called midpoint reflection, that says, “If there exists a segment AB such that one
of its end points B is a midpoint of segment XY, then construct a new point by
reflecting A around XY.” This heuristic leads Wong’s prover to the construction
for Problem P103 shown in Appendix A. Our proof procedure identified that this
problem requires three constructions for three different postulates, but because of
a technical issue in its implementation, GRAMY could not solve this problem.
Although Wong’s heuristic makes much sense, because of the excessive generality we doubt it would work in a practical situation without a search explosion
(indeed, Wong’s heuristic has never been implemented). Another example of ad
hoc heuristics can be seen in AUXIL (Suwa and Motoda, 1989). AUXIL requires a
human problem solver to input proofs with construction. It then learns how to make
the construction by generalizing the operations used in the proofs. The acquired
construction operators and heuristics can be so complicated that they are hardly
 A counterexample of this observation is the construction with a transitive substitution, described

in Section 3.2.5.

8

NOBORU MATSUDA AND KURT VANLEHN

teachable. Hence we doubt that this type of prover can be a building block of a
tutoring system, even though it is computationally tractable (see Suwa and Motoda
(1989) for more about their so-called frustration-based search control technique).
An interesting study was conducted by Chou et al. (2000) in which they built a
powerful theorem prover capable of construction that can solve very hard problems. The construction procedure is implemented as a set of rules that specify
how to construct a new point in a certain situation. Unfortunately, because their
construction rules are not goal oriented, and the prover has the potential to make
many unsuccessful constructions. Furthermore, their proof procedure works only
for the class of constructive geometry, and therefore it has the same limitation as
the area method mentioned in the previous section.
2.3. WHY IS CONSTRUCTION SO HARD ?
To understand why theorem proving with construction is so hard, consider the
following brute-force proof procedure:
1. Apply exhaustive forward chaining without construction.
2. If a proof is found, namely, if there exists a derived proposition that unifies with
the goal, then output the proof and quit.
3. Apply every applicable primitive construction operator once to the problem
figure, and make a new state with the modified figure. A primitive construction
operator is either (1) drawing a line between two existing points or (2) drawing
an arc of a specified radius about an existing point.
4. Go to step 1.
If a proof with construction does exist, then the necessary construction should
be made with a finite number of applications of the primitive construction operators, which takes place during the third step mentioned above. Thus, the above
procedure will eventually find the proof. However, there are usually a large number
of applicable primitive construction operators at step 3, which greatly expands the
problem figure. This means that exhaustive forward chaining may produce a very
large number of propositions when it next applies at step 1. In short, this proof
procedure is complete but not practical.
We need to restrict the application of construction operators so that useless
construction will never be made. One approach to making construction more efficient is to combine several primitive operators into a macro operator that does
only “meaningful” construction and uses only such macro operators. To drop a
perpendicular line from a given point to a given line is an example of a macro
operator. Furthermore, the search can be made more efficient if macro operators
contain more information than just a sequence of primitive operators. For example,
consider again the construction of a perpendicular line from a given point to a given
segment. As shown in Figure 1, this construction is done by a macro operator that
comprises a sequence of seven primitive operators. This sequence does not hold

GRAMY

9

Figure 1. Construction for a perpendicular by primitive operators.

enough information to prove that  PXB is a right angle, which requires drawing
auxiliary segments SQ, QT, TR, and RS. Instead of forcing the theorem prover
to find this additional construction, we can augment the macro operator to assert
the right angles as true propositions. This augmentation not only improves the
efficiency but also provides a rationale for the construction.
However, replacing primitive construction operators with macro operators does
not settle the explosion of the search. We tested the brute-force procedure described
above with five macro operators: copying a distance onto a line, dropping a perpendicular from a point to a line, drawing a perpendicular to a point on a line,
drawing a parallel line, and plotting a midpoint. On Problem P123 in Appendix A,
for instance, the average number of macro operators applicable at each state was 78.
These macro operators did not constrain the search enough.
In sum, a brute-force search for construction would not be computationally
tractable even with macro operators. We need to add more conditions that tell
us when to apply the operators. The next section describes the approach used in
GRAMY.
3. Proof and Construction Procedures in GRAMY
This section describes the whole proof procedure with a construction technique
implemented in GRAMY. A discussion on its completeness follows.
3.1. KNOWLEDGE REPRESENTATION
In GRAMY, postulates that share the same topological configuration of points and
lines are composed into a single knowledge piece, called a diagrammatic schema
(DS). A DS consists of a diagram representing the topological configuration of the
geometric elements involved in a DS, a set of propositions representing geometric
relations (equal, parallel, perpendicular, etc.) that refer to the elements in the diagram, and deductive statements in the form “if a set of (possibly zero) premises
holds, then a set of consequences holds” where the premises and the consequences
refer to the geometric propositions. Figure 2 shows an example of a DS that rep-

10

NOBORU MATSUDA AND KURT VANLEHN

Figure 2. Example of a diagrammatic schema (DS).

resents the postulates related to triangle congruence. Seven relations are shown
under “Proposition” and four deductive statements under “Deduction.” Of those
four deductive statements, the first three statements show the conditions for two
triangles to be congruent. The last statement represents a postulate that says, “If
two triangles are congruent, then corresponding segments and angles are equal.”
The topological configuration associated with each DS is called a DS diagram.
We use the knowledge representation technique developed in Perdix (Greeno et al.,
1979) to represent the DS diagram as a semantic network. The nodes correspond to
geometric elements in the DS diagram (points, segments, rays, lines, angles, triangles, quadrangles, etc.) and the links correspond to relations between two elements
(e.g., a point is an end point of a segment). For example, associated with the DS
for the triangle-congruent postulate are 6 points, 6 segments, 12 rays, 6 angles, and
2 triangles. The problem figure is represented the same way.
Geometric propositions are represented with first-order predicates. The arguments of a predicate are geometric elements in a problem figure or a DS diagram.
For example, if two segments AB and CD are represented as the nodes s1 and s2 in
the semantic network, then the proposition AB = CD is represented as eq(s1 ,s2 ).
Although many provers use only points as arguments, GRAMY’s representation
reduces the combinatorics that would otherwise be caused by unifying coreferring
expressions such that AB = CD and BA = DC.
3.2. CONSTRUCTION PROCEDURE GUIDED BY DIAGRAMS
A proof is a sequence of postulate applications. Hence, if construction is made for
a proof, it is made so that some postulate, which otherwise is not applicable, can be
applied. In other words, the construction involves adding segments that are not part
of the original problem figure but are necessarily involved in postulate applications
in the proof.
The above observation implies that knowing which postulates are used in a
proof is sufficient to determine the target elements of construction. Three issues
arise: (1) how to find a set of postulate applications that constitutes a proof, (2) how
to identify the segments required making those postulates applicable, and (3) how
to construct (i.e., how to calculate coordinates of the end points of) those missing
segments. These issues are discussed in the next three sections.

GRAMY

11

Figure 3. A dependency network representing a proof with construction.

3.2.1. Identifying Postulates in a Proof
A postulate is called useful if its consequence unifies with the goal to prove and all
premises that match with the problem figure are quantitatively satisfied, whereas
some premises might not match with the problem figure. The basic idea on controlling a search for construction is that GRAMY attempts construction only when
required for the backward application of a “useful” postulate. This idea is best
explained with an example, depicted in Figure 3. The figure shows a dependency
network produced by exhaustive forward chaining on a geometry problem that has
five given propositions shown as the gray circles at the bottom row. A small square
shows a postulate application. The links coming into a square from the bottom
correspond to premises of a postulate application, and the links going out from the
top correspond to its consequences. White circles are derived propositions. If the
problem is solvable, then one of the derived propositions must unify with the goal
to be proven. Let P be a proposition that unifies with the goal. Thus, the heavier
links, both solid and dotted, show a proof for the proposition P . Hence exhaustive
forward chaining would eventually find a proof for the problem. The lighter links
indicate the extra work done by exhaustive forward chaining, namely, postulate
applications that do not appear in the proof.
Now we extend the above model by introducing construction. Let F be the
problem figure required to prove P , namely, all geometric elements appearing in
the proof of P are in F . Suppose that we remove a few segments from F , taking
care that they can be added back by construction. Some of the propositions can no

12

NOBORU MATSUDA AND KURT VANLEHN

longer hold in the resulting figure (e.g., if segment AB is removed, then AB = CD
does not hold). Assume that those propositions are indicated by the dotted circles in
Figure 3. Consequently, postulate applications that involve dotted circles, indicated
by the dotted squares, cannot apply either.
To find a proof, that is, to find postulate applications in the heavier links both
solid and dotted, one must construct the segments that have been removed. To avoid
search explosion, we would like the theorem prover to perform construction only
for the postulate applications specified by heavier dotted links. More precisely, the
theorem prover must first identify that postulate applications 13, 8, 6, and 2 are part
of the proof and construct only those segments such that propositions X, Y , and Z
hold in the resulting figure.
One approach is to first apply exhaustive forward chaining without construction,
then backward chain once with a useful postulate. We then apply construction
operators so that the diagram associated with the useful postulate matches the
problem figure completely. Now we have a new state with a new problem figure
and new goals. Since the problem figure has changed, forward chaining may assert
new propositions. So, we apply exhaustive forward chaining followed by backward
chaining with construction again, and we keep repeating this cycle until a proof is
found.
In Figure 3, postulate applications 1, 3, 4, 5, and 9 occur during initial forward
chaining. Assume there is no postulate applicable at this point. The prover then
makes a construction so that backward chaining for a useful postulate application 13 would succeed. If this construction eventually makes propositions X, Y ,
and Z appear in the problem figure, then succeeding exhaustive forward chaining
causes postulate applications 2, 6, and 8, which completes a proof.
It is not necessarily the case that a desired construction can be made by a useful
postulate relative to the top-level goal, namely, postulate application 13 for P in
the above example. Consider a problem that has the same five givens as shown at
the bottom of Figure 3 but has Q as a goal to prove instead of P . In this case, the
prover must do postulate application 15 backwards to create P as a subgoal of Q.
In general, a theorem prover might need to apply backward chaining multiple times
before carrying out construction.
Once a useful postulate is determined, there arise two technical issues to make
use of the above idea: (1) matching the DS diagram to the problem figure partially,
which in turn identifies DS segments that need to be constructed, and (2) discovering a sequence of construction operators that constructs these segments.
3.2.2. Identifying Missing Segments to Be Constructed
Intuitively, overlapping a DS diagram associated with a selected useful postulate
with the problem figure reveals missing segments necessary to apply the useful
postulate. This overlapping must be constrained so that the consequence of the
useful postulate overlaps the goal to be proven. For example, if the goal is to prove
AB = DE, then the triangle-congruent postulate uvw ≡ xyz → uv = xy is

GRAMY

13

useful. In this case, one may wish to overlap two triangles, uvw and xyz, onto
the problem figure so that the segment uv overlaps AB and xy overlaps DE. The
segments in the DS diagram that cannot be overlapped indicate which segments
must be added to the problem figure.
As mentioned in Section 3.1, DS diagrams are represented as semantic networks
in GRAMY. Overlapping is equivalent to finding a partial match between a semantic network representing a problem figure and a semantic network representing
the DS diagram of a useful postulate. A pair of an element in the DS diagram
and an element of the problem figure is called a binding. A partial match is a set
of bindings, which we hereafter call a binding list. An element in a DS diagram
involved in a binding list is said to be bound. For the sake of explanation, we need
to discriminate between the elements in the problem figure and the ones in the
DS diagram; for example, a problem point refers to a point in the problem figure,
whereas a DS point refers to a point in the DS diagram.
Although finding a partial match for two directed graphs is NP-hard, we have
achieved adequate performance by formalizing it as a constraint satisfaction problem and solving it with a forward-checking backtracking algorithm (Haralick and
Elliott, 1980) as described below.
Assume that semantic network SN DS represents the DS diagram associated with
a useful postulate and that semantic network SN P represents a problem figure. A
partial match binds each node in SN DS with a node in SN P so that the links in SN DS
and SN P are consistent with the bindings. We let the variables vi (i = 1, . . . , n)
of the constraint satisfaction problem represent the n nodes of SN DS . The domain
of a variable vi consists of a subset of nodes in SN P that are the same type as vi .
For example, the domain of a variable that represents a DS point is a set of all
problem points. A partial match occurs when some DS elements are not bound
to any problem elements. This is implemented by binding the value NIL to the
variables representing such DS elements. The value NIL is explicitly added into
the domain of each variable so that the constraint-satisfaction algorithm need not
be modified.
The constraints model relations among geometric elements. There are two
classes of constraint:
(1) Constraints on bound variables: These constraints check the consistency
among the relations between the bound geometric elements. For example, if
v1 and v2 are parallel lines in SN DS and they are bound to lines x1 and x2 in
SN P , then x1  x2 must be quantitatively satisfied. The initial domain values
that are inconsistent with the variable to be bound are also filtered out, prior
to the search.
(2) Constraints for partial matching: These constraints check whether a variable
can be bound to NIL by testing topological feasibility with the bound variables. For example, assume that v1 and v2 are two rays intersecting at point
v3 . If both v1 and v2 are bound to non-NIL values x1 and x2 , then v3 cannot
be bound to NIL (indeed, it must be bound to the problem point that is an

14

NOBORU MATSUDA AND KURT VANLEHN

intersection of problem rays x1 and x2 ). As another example, if all three segments of a triangle are bound to non-NIL values, then the triangle itself must
be bound to a non-NIL value as well.
3.2.3. Construction Operators to Construct a Missing End Point
Given a partial match, we now describe a technique for finding a sequence of
construction operators to create missing segments in the problem figure.
There exist only three types of partial match for a DS segment with respect
to bindings for its end points: both end points are bound, only one of the end
points is bound, or none of the end points is bound. One of these cases can be
eliminated by noticing that no postulate has an isolated DS segment. That is, all
the segments in a postulate have at least one end point shared with other segments.
Thus, repeatedly constructing a problem segment for a DS segment with a single
end point bound would eventually make all DS segments have both of their end
points bound. For example, consider a useful postulate with a rectangle abcd that
gets bindings {a/NIL, b/X, c/Y, d/NIL}. In this case, we have a DS segment
(bc) with both end points bound, two DS segments (ab and cd) with a single end
point bound, and a DS segment (ad) with no end points bound. If we successfully
construct a problem segment that corresponds with DS segment ab, then the DS
point a is bound to a new problem point. Now, the DS segment ad has a single end
point bound. Furthermore, constructing one of the remaining two DS segments
makes the last DS segment have both end points bound.
The above observation allows us to safely focus only on the construction of
segments with one or two bound end points. The basic idea follows:
1. For each unbound DS segment with both end points bound, construct a corresponding problem segment by simply connecting the two problem points
corresponding to the two bound DS end points. Update the partial match. If
a perfect match is found, then halt (i.e., construction is completed).
2. Nondeterministically choose an unbound DS segment that has exactly one of
its end points, say p, unbound.
3. Try to construct a problem point for the DS point p using the techniques described below. If the construction fails (i.e., there is no construction with a
compass and a straightedge), back up to step 2 and choose another unbound DS
segment. Otherwise update the partial match.
4. Go to step 1. Notice that the unbound DS segment chosen in step 2 now has both
end points bound, so step 1 will construct its problem segment at this time.
Let us call the unbound DS segment chosen in step 2 the target DS segment.
Also, let us call the unbound DS end point of the target DS segment the target DS
end point. Let us call the problem segment that we will construct for the target
 In this article hereafter, the small letters are used to represent the elements in a DS diagram and

the capital letters to represent the elements in a problem figure.

GRAMY

15

segment the target problem segment, and also call the to-be-constructed end point
the target problem end point.
For the target problem segment to be constructed, it is sufficient to construct
the target problem end point. Determining the coordinate of the target problem
end point is equivalent to determining the direction and length of the target problem segment relative to its end point that is bound. This motivates the following
definitions regarding the measurement of those quantities:
– The length of the target problem segment is determined if the useful postulate
states that the target DS segment is congruent to some DS segment that is
bound. For instance, suppose the target DS segment is xy. If a premise of a
useful postulate requires xy = uv and if uv is bound to problem segment AB,
then the length of the target problem segment must be equal to the length
of AB.
– The direction of the target problem segment is determined if one of the following four conditions holds. Suppose rt is a DS ray on the target DS segment
where the end point of rt is the same as the bound end point of the target DS
segment (see Figure 4 where lt is the target DS segment):
1. Ray rt is bound; hence the problem ray bound to rt gives the direction.
2. Ray rt contains a bound DS point p; hence the problem points bound to p
and to the end point of rt give the direction.
3. A premise of the useful postulate specifies that rt is parallel to a bound DS
ray rt	 .
4. A premise of the useful postulate specifies that rt is perpendicular to a
bound DS ray rt	 .
For each of these four cases, there exist a compass procedure and a straightedge procedure for drawing a problem ray from A in Figure 4 (e.g., for the last
case, we extend rt	 and then draw a perpendicular to rt	 through A, as shown in
Figure 1).
Given these definitions, we now examine different cases to construct a target
problem end point depending on whether the length and/or direction are determined. As shown in Figure 4, assume that pt is the target DS end point and pb is
the bound end point of the target DS segment. Also assume that A is the problem
point to which pb is bound, and B is the target problem end point that we wish to
construct.
We consider the easiest case first. If the length and the direction of the target
problem segment are both determined, then GRAMY constructs point B as follows.
It extends a ray from A in the determined direction. GRAMY then plots B on the
ray such that |AB| is equal to the length of the problem segment that determines the
length of the target problem segment. The second operation corresponds to drawing
an arc about A with the radius copied by a compass. This construction procedure
must always succeed because the ray intersects the arc exactly once.

16

NOBORU MATSUDA AND KURT VANLEHN

Figure 4. Construction of a target problem end point with the third point.

Next, we consider the hard case where either the length or the direction of the
target problem segment is determined, but not both. GRAMY nondeterministically
chooses a bound DS point p0 not equal to pb or pt (see Figure 4). Suppose the
problem point that is bound to p0 is C. There are several cases to consider. First,
if the length of the problem segment BC is determined and the length of the target
problem segment AB is determined, then GRAMY can draw arcs of determined
radii about A and C. Second, if the direction of BC is determined and the length
of AB is determined, then we can draw a ray from C in the determined direction
and an arc of determined radius around A. Third, if the directions of both AB and
BC are determined, then GRAMY can draw rays from A and C in the determined
directions. There is a fourth case, wherein the length of BC is determined and the
direction of AB is determined, but this is already covered by the second case, given
that one switches p0 and pb . To avoid duplication, GRAMY does not generate a
construction for this case.
It turns out that there exists a certain kind of problem that cannot be solved
by the above construction technique. This kind of problem requires a constructed
segment to hold a certain relation with a problem segment that is not involved in
a partial match for the useful postulate. In order to handle such cases, GRAMY
uses a novel technique described in the next section. The technique also solves
the problems that require construction where the existence of constructed points is
dubious (e.g., ray grazing a segment; see the footnote).
3.2.4. Construction with a Reference to a Free Segment
The problem shown in Figure 5 cannot be solved with the construction techniques
described in the preceding section. The useful postulate is the triangle-congruent
axiom with a partial match involving {pb /D, p0 /A, r0 /rayAC}, using the same
 The arcs about A and the ray from C may have either one or two intersections. If the construction

apparently has only one intersection (i.e., the arc just grazes the ray), a teacher would either reject a
student’s proof that uses such a construction or require a proof of the existence of such an intersection point. GRAMY simply does not generate such a construction and picks up other postulates or
bindings. A future extension could perhaps generate a construction and add a goal to the search state
to prove that the point or points exist.

GRAMY

17

Figure 5. An example of construction referring to a free segment.

configuration elements in Figure 4. Since the useful postulate states that the target
problem segment is equal to ED, GRAMY can draw the target problem point (X)
as an intersection of ray AC and an arc about D with |DE| as the radius, and assert
DX = DE as a true proposition. However, such construction does not lead to a
proof. Instead, the segment DX must be constructed to be parallel to AB.
To deal with this kind of problem, GRAMY tries to convert the first two hard
cases mentioned in the previous section into construction without drawing an arc.
Specifically, if drawing an arc successfully finds construction, then GRAMY also
tries to construct the same point via a ray instead of an arc. To do so, GRAMY
searches for a problem segment, even though it is not involved in the partial match,
that is approximately parallel or perpendicular to the target problem segment found.
It then treats the target segment as having a direction determined by the parallel or
perpendicular segment.
In the example shown in Figure 5, once the coordinate of problem point X is
determined as an intersection of ray AC and the arc from D, it can be seen that AB
is approximately parallel to DX. This observation is possible by calculating slopes
of AB and DX based on the xy-coordinates of A, B, D, and X. As a result, GRAMY
constructs X as an intersection of AC and a line from C parallel to AB, and asserts
DX  AB as a true proposition. This technique is also effective even when the circle
about D just grazes the line AC.
We call this technique construction with a reference to a free segment. This
construction procedure can produce different constructions by using different reference segments in the problem figure that are parallel or perpendicular to the
target segment. GRAMY finds all reference segments, and it outputs a different
construction for each.
3.2.5. Construction with Transitive Substitution
Like many theorem provers, GRAMY treats equality specially. It maintains a set
of equivalence classes. When it derives the equality of two elements (e.g., two
segments are equal, or two triangles are congruent), it puts them into the same
equivalence class and adds this class to the set of equivalence classes unless the
class is there already. GRAMY’s pattern matcher for propositions is modified so

18

NOBORU MATSUDA AND KURT VANLEHN

that when it tests whether a proposition holds, it succeeds not only if the two
constants are known to be equal but also if they are in the same equivalence class.
Theorem provers that do not treat equality in this way must have transitivity axioms
of the form “If X = Y and Y = Z, then X = Z,” which not only increase the
combinatorics but clutter the proof with “trivial” inferences.
Since GRAMY treats equality specially when doing its regular reasoning, it
must also treat equality specially when doing construction. For example, to prove
a goal AB = CD, it can consider finding a third segment, say, PQ, that is equal
to AB, and then can try to prove PQ = CD. Assuming that segment PQ does not
appear in the original problem figure, the triangle-congruent postulate involving
uvw ≡ xyz might be useful with a partial match that involves {uv/AB, xy/NIL}.
Once a partial match is found, we can apply the same procedure as described in the
previous sections. If construction eventually draws PQ, and there exists a proof
for uvw ≡ xyz (hence AB = PQ), then one may find a successful proof for
AB = CD by a transitive substitution provided that PQ = CD can also be proven.
GRAMY considers doing such construction not only for segment congruence but
also for all types of relationships that are transitive. Six problems in our problem
corpus require this construction technique (see Section 4.1).
3.3. PROOF PROCEDURE WITH CONSTRUCTION
This section summarizes the procedure to search for a proof with construction.
The states in the search space consist of a problem figure, a set of true propositions
(i.e., given and derived facts), and an ordered list of goals called the goal queue.
The search maintains a state queue for breadth-first search. The following is the
main search procedure:
1. Let S be the initial state, and let its goal be the top-level goal. Initialize the state
queue to hold just S.
2. (Exhaustive forward chaining) If the state queue is empty, then quit. Otherwise
remove state S from the front of the state queue. Apply exhaustive forward
chaining to S. S now holds the deductive closure of the given propositions and
the problem figure under the given postulates.
3. (Goal test) Remove all the goals in the goal queue of S that unify with the
propositions in S. If S has no goals left in its goal queue, then one proof has
been found. Put S aside, and go to step 2.
4. (Backward chaining) For each postulate p such that its DS diagram perfectly
matches with the problem figure in S, all p’s premises are quantitatively satisfied, and p’s consequence unifies with the first goal in the goal queue of S,
make a state Sp that is a copy of S with all the premises of p put at the front of
the goal queue. Put Sp onto the end of the state queue. Notice that Sp has the
same problem figure as S.
5. (Construction) For each useful postulate for the first goal in the goal queue of S,
find all constructions for each partial match with the construction procedure

GRAMY

19

below. For each successful construction, create a new state with a new problem
figure. Put these states on the end of the state queue. Notice that the new state
has the same goal queue as in S.
6. Go to step 2.
The construction procedure takes a useful postulate and a partial match as input,
and outputs all possible constructions. This is also done by state-space search.
Major components of a state include a problem figure, a list of missing segments,
and a binding list. A single application of a construction operator adds one missing
segment into the problem figure. Thus, each state has exactly one missing segment
less than its ancestor state. Since there are only a finite number of missing segments
to be constructed, depth-first search works fine. The following is the construction
procedure:
1. Make an initial state with the original problem figure and the partial match.
Initialize state queue with it.
2. (Goal test) If the state queue is empty, then halt. Otherwise remove a state C
from the front of the state queue. If C has no missing segments left, then
construction is done. So put C aside and repeat this step.
3. (Construction by connecting bound end points) For each unbound DS segment
with both end points bound, add the corresponding problem segment to the
problem figure by simply connecting the two problem points corresponding to
the bound DS end points. Update the binding list with the newly constructed
segments.
4. (Construction operator for missing segment with length and direction determined) Nondeterministically choose an unbound DS segment that has exactly
one of its end points unbound and has both its length and direction determined.
Calculate the coordinates of the target problem end point as an intersection of
an arc with the determined length as its radius and a ray with the determined
direction. Create a new state with the new problem figure. Assert appropriate
facts to the new state (e.g., that the target segment is congruent to the segment
that determined its length). Update the binding list in the new state so that the
target DS end point is bound to the newly added intersection.
5. (Construction operator for a missing segment with either length or direction
determined) Nondeterministically choose an unbound DS segment that has exactly one of its end points, pt , unbound and has either its length or direction (not
both) determined (see Figure 4 where lt is the unbound DS segment chosen).
5.1. Nondeterministically choose a bound DS point p0 such that the segment
between p0 and pt has either its length or direction determined.
5.2. Calculate the coordinate of an intersection(s) of the appropriate arcs and
rays (depending on whether lengths or directions are determined) for the
problem point that is supposed to be bound to pt .

20

NOBORU MATSUDA AND KURT VANLEHN

5.3. For each intersection, create a new state with the new problem figure
by adding the intersection. Assert appropriate facts (those made true by
construction) to the new state. Update the binding list in the new sate so
that the target DS end point is bound to the newly added intersection.
6. (Construction operator for a construction with a reference to a free segment)
Perform the same step specified in step 5 but not step 5.3. Nondeterministically
choose a reference segment in the problem figure that is parallel or perpendicular to the target problem segment determined by its bound end point and the
calculated intersection. For each reference segment, create a new state with
the new problem figure by adding the intersection. Assert appropriate facts
including that the new segment is parallel or perpendicular to the reference
segment. Update the binding list in the new state. Go to step 2.
At several steps in this procedure, failure can occur. For instance, after drawing
the relevant arcs and rays, there may be no intersection points. In this case, no
construction is produced, and that nondeterministic choice dies. If all such choices
fail, then this branch of the search also fails, which in turn abandons the whole
attempt to apply the selected useful postulate with the selected bindings at step 5
in the main search procedure.
3.4. SEMICOMPLETENESS OF THE PROOF PROCEDURE
Now we examine the completeness of the proof procedure. Since the ability of our
construction procedure to produce a particular construction thoroughly depends on
a set of postulates available, we emphasize that we discuss the completeness of the
search procedure, not the completeness of the set of postulates given to GRAMY.
In other words, we claim that GRAMY can find all proofs that consist only of the
postulates given.
In the proof procedure above, the first list of steps defines the main procedure,
and the second set defines the subprocedure for generating constructions that provide a perfect match for the useful postulate. We have not yet succeeded in proving
that the subprocedure is complete. In particular, the restriction that step 5.1 choose
a bound DS point p0 is necessary to prevent combinatorial explosion. Arcs or
rays that are drawn via p0 are actually drawn from the problem point that p0 is
bound to. But it may prevent the subprocedure from generating constructions that
both make the useful postulate match and lead ultimately to a successful proof.
In principle, one could draw arcs or rays from any problem points that are not
involved in the partial match. If the resulting construction makes the figure match
the useful postulate, then this branch of the search might survive to become a
successful proof. For example, assume that a goal AB = CD is proved with the
triangle congruent postulate ABP ≡ CDQ where the points P and Q are not
 We also claim that GRAMY’s procedure is sound, which means that if no proof exists, it will

not find one. This claim follows from the soundness of both forward and backward chaining.

GRAMY

21

in the original problem configuration and hence needed to be constructed. Those
points could have been constructed as an intersection of two arcs about some points
in the problem figure that are not involved in the partial match for this postulate.
We have yet to see a problem that requires such a branch in the search, but we are
also unable to show that such branches always fail.
Assuming that the subprocedure is complete, it can be shown that the main
procedure is complete. Thus, we claim that GRAMY is “semi-complete.” The proof
follows. Let P be a problem to be proven. Let F0 be an original problem figure
for P , and Fi be the problem figure after the i-th construction has been made. Let
 = {a1 , . . . , an } be a set of postulates given to the theorem prover sufficient to
prove P . Let PA(, Fi ) be a set of all possible ground instantiations of postulate
applications for all elements in  with respect to Fi . Since there are finitely many
elements in a problem figure, there are only finitely many instantiations for each
postulate; hence PA(, Fi ) is finite. If P has a proof Pr(P ) with k constructions,
then it must be a sequence of some ground instantiations in PA(, Fk ). Let Pr(P ) =
p1 , . . . , pn (pi ∈ PA(, Fk )) where the consequence of p1 unifies with the to-beproven goal specified in P , and the sequence is made by traversing a proof tree
depth-first.
In the case that P does not require construction at all, Pr(P ) must be found
because exhaustive forward chaining will eventually produce all the ground instantiations for the postulates in PA(, F0 ).
Assume that the proof procedure can find the proofs with k − 1 constructions
(k ≥ 1). We now prove that the proof procedure can also find the proofs with k
constructions. Let Pr(P ) = p1 , . . . , pn (pi ∈ PA(, Fk )) be a proof with k constructions. There must be at least one ground instantiation of a postulate application
in the proof that does not have a perfect match with F0 . Let pj (1 ≤ j ≤ n) be
the first postulate application that satisfies this property, namely, pm has a perfect
match with F0 for ∀m such that 1 ≤ m < j . Since the top-level goal of the problem
P refers to the geometric elements in F0 and backward chaining is always done for
the elements in the problem figure, it is easy to see that the consequence of pj also
refers to the elements in F0 . Hence, a partial match for a postulate application pj ,
which involves bindings for geometric elements referred in the consequence of pj ,
must be found by the search step 5 (construction) of the main procedure. If the
subprocedure is complete, it must produce constructions that generates F1 . Now
the problem becomes one that requires k − 1 construction, which can be solved,
according to our inductive hypothesis. Hence the proof procedure must find a proof
of P with k constructions. This completes the proof for the semi-completeness of
the proof procedure.

4. Evaluation of GRAMY
This section shows the performance of GRAMY on the proofs with construction.
To compare GRAMY with other theorem provers, we collected 23 construction

22

NOBORU MATSUDA AND KURT VANLEHN

problems from the ten literature studies cited earlier. Few construction problems
have been addressed in the literature, possibly because of the difficult nature of
automated construction. Construction is also difficult for humans, so we found only
17 construction problems in two American and three Japanese textbooks (Aref
and Wernick, 1968; Coxford and Usiskin, 1971; Kyogaku-Kenkyusha, undated;
Matsushita, 1993; Shinshindo, 1993). Of those 17 problems from the textbooks, 8
were also used in the preceding studies. As a result, the problem corpus contains
32 construction problems.
Table I shows the performance of the theorem provers on our problem corpus. In
the table, a row corresponds to a problem, and a column corresponds to a textbook
or a study (including the current one denoted as GRAMY04). A label in a cell
shows a problem number specified in the textbook or the literature, or it shows
the page number where the problem is first mentioned. An italicized, label means
that the theorem prover failed to find a proof. A nonitalicized, bold label means
the prover did find a proof. The sixth column, GRAMY04, lists all 32 construction
problems used in the current study.
In Wong (1972), 50 problems are used. However, we have picked only 18 problems from his study because those are the only problems that do not deal with
sums, inequality, ratio, and coincident intersections. The same filtering policy was
applied to selection from the textbooks. All the other literature has the same number
of problems shown in Table I.
The problems are classified into three categories according to when constructions should be made. (1) Twenty-six out of 32 problems can be solved by applying
a single construction in the first quiescence state, which is produced by exhaustive
forward chaining from the initial state. That is, the construction was made for the
top-level goal. (2) Five out of 32 problems, which are all italicized in Table I,
required multiple constructions applied for different subgoals. (3) Only one problem required backward chaining to replace the top-level goal with its immediate
subgoal before construction. The next few sections discuss each type of problem.

4.1. SINGLE CONSTRUCTION PROBLEMS
Table II shows results of the search for proofs on the problems that could be solved
by applying single construction to the top-level goal without transitive substitution.
As mentioned earlier, if GRAMY is to be used for tutoring, it must be able to generate all correct proofs. Hence the table consists of two parts: the search complexity
to find the first proof, and the search complexity to find all proofs. For the search
for the first proof, the table shows the depth of search (i.e., the number of cycles of
the main search procedure mentioned in Section 3.3 taken before the first proof is
found), the length of the proof (i.e., the minimum number of postulate applications
for a proof for the problem figure modified by the first successful construction), the
number of states expanded before the proof is found, the number of true propositions asserted by the time the proof is found (i.e., the number of statements in the

23

GRAMY

Table I. Problems used for the evaluation

5

1.11
1.9

1.1

1.22
p. 265

1.7
1.18

12.4.2
28

1.3
12.3.1

Gelernter63

Gelernter59

Wong72

Reiter72

Goldstein73

Nevins75

Ullman75

Elcock77

Suwa89

Chou00

GRAMY04

Theorem Provers

Kyogaku

Matsushita93

Shinshindo93

Coxford71

Aref68

Textbooks

P132
Fig. 5
P111Ex 6.4
p. 26Ex 2
p. 7 Ex 7
Appendix 2
P123
p. 34
P116
Problem 1
P124
Problem 2
P105
Ex 1
P101
Ex 2
P102
Ex 3
P103
Ex 4
P108
Ex 8
P109
Ex 9
P112
Ex 13
P110
Ex 14
P115
Ex 22
P125
Ex 30
P142
Ex 31
P143
Ex 34
P144
Ex 35
P135
Ex 40
P117
Ex 1
Ex 41
P139
Ex 45
P152
Ex 48
P154
Ex 50
p. 14 P126
p. 15 P127
P128
Pat 6
P129
Pat 15
P130
5
P131
P156
p. 26 P158
P159

24

NOBORU MATSUDA AND KURT VANLEHN

Table II. Search complexity for proofs with single construction to the top-level goal
Problem

First Proof

All Proofs

Depth

Length

State

Prop.

Time

All

Suc.

State

Prop.

P132

4

3

4

12

11

Const

Const

40

P123

5

4

15

38

35

5

P109

5

5

198

91

322

P127

5

3

P101

5

5

296

70

313

72

P117

6

9

48

P116

6

6

130

Time

40

40

1342

76

1

9

200

35

76

3

149

2414

512

1529

101

7

189

12252

2381

1764

109

8

205

13075

2573

178

54

17

1

33

1642

205

79

109

76

1

151

3303

392

P112

6

8

23

79

151

181

14

348

21584

3908

P108

6

14

112

92

185

49

1

97

2137

432

P115

7

8

26

65

61

55

1

109

2785

329

P129

7

10

13

349

278

36

1

71

15095

5268

P128

7

11

93

103

770

146

2

290

20255

2951

P111

8

7

80

146

544

135

4

256

18229

3967

P131

8

6

492

63

2156

122

16

228

12880

2027

P142

9

10

13

127

84

95

7

183

16251

3356

P144

9

19

85

152

146

61

6

112

2691

472

deductive closure), and the CPU time in seconds to find the proof. For the search
for all proofs, the table shows the number of constructions found regardless of
whether they led to a proof, the number of constructions that led to a correct proof,
the total number of states expanded, the total number of true propositions asserted,
and the total CPU time in seconds. The last three numbers aggregated searches for
all constructions found regardless of their correctness. As an example, Appendix B
shows successful construction for the problem P111 (an example of unsuccessful,
but reasonable, construction for P111 is shown in Figure 6).
As mentioned in Section 3.2.5, some problems require construction with transitive substitution. The current version of GRAMY first applies the construction
procedure without transitive substitution; only when no proof is found does it apply
construction with transitive substitution. Since the number of possible constructions with transitive substitution could be large, this trial-and-error approach could
be very costly. Table III shows search complexities on those problems. The number
in parentheses shows the number of successful constructions. In general, a partial
match for construction with transitive substitution has more variables assigned to
NIL. Hence the number of possible constructions made by transitive substitution is
bigger than the one by normal construction. As shown in the table, the ratio of suc The current version of GRAMY is written with Common LISP running on an Intel Pentium-III

600 MHz processor.

25

GRAMY

Figure 6. A “reasonable” but unsuccessful construction for P111.
Table III. Search complexity for proofs with single construction by transitive substitution
Normal Construction

Construction with Transitive Substitution

Problem Length #Const #State #Prop. Time Length

#Const

3 285

#State #Prop.

Time

P126

0

2

5

24

9.39

(5)

5

79 270.56

P139

0

0

3

55

1.37

P130

0

1

6

89 28.56

4 510

(1)

36

175 800.65

5 389

(2)

8

91 552.22

P110

0

2

7

42 15.98

6 293

(4)

9

94 349.77

P102

0

0

3

467

1.48

7

(6)

9

1087 314.01

P105

0

2

5

12

2.97

13 184 (184)

52

23

49

78.27

cessful to unsuccessful constructions is generally quite low. Nonetheless, GRAMY
found proofs for those problems in reasonable time. To find all proofs, however,
GRAMY should try transitive substitution on all problems, even those where other
proofs are found without transitive substitution. We have not yet experimented with
this extension.

4.2. MULTIPLE CONSTRUCTION PROBLEMS
Five problems in the corpus require multiple constructions. GRAMY could solve
those problems given unlimited resources (i.e., memory space and time). However, because of inefficient memory management in the current implementation,
GRAMY could not find a proof for these and only these problems. For example,
problem P103 needs three constructions for three consecutive subgoals. For the first
goal, 153 constructions were found. For those constructions, GRAMY expanded a
total of 3,687 immediate states. GRAMY crashed when searching for constructions for the third subgoal from the top-level goal. We believe that this is a fixable
technical problem, not a fatal theoretical problem.

26

NOBORU MATSUDA AND KURT VANLEHN

Table IV. Search complexity for proofs with single construction by transitive substitution
Problem
P143

Length
30

#Count
78

#Prop.
220

Time
1320

4.3. CONSTRUCTION FOLLOWING BACKWARD CHAINING
Problem P143 requires backward chaining prior to a construction. Table IV shows
the search complexity on this problem. Construction was carried out immediately
following one backward deduction applied at the first quiescence state. There are
four disjunctive subgoals for this backward deduction, and 20, 20, 38, and 0 constructions were found for each of the subgoals (hence, 78 total constructions as in
the table).
Since P143 is the only problem in our corpus that requires backward chaining prior to construction, it is hard to evaluate the effectiveness of GRAMY on
this type of problem. One would ask whether, since backward chaining may be
applied multiple times, the search might explode. In such cases, GRAMY can
iteratively increase the depth at which construction is carried out. For each iteration
GRAMY can apply depth-first iterative-deepening search to find a proof. Thus, we
believe that GRAMY is effective for constructions that follow multiple backward
chaining.

4.4. COMPLEXITY OF DIAGRAM MATCHING
Matching a DS diagram to a problem figure is complex. Suppose a problem figure
has P points, S segments, A angles, R rays, T triangles, and L lines. For a postulate
that has a DS diagram with p points, s segments, a angles, r rays, t triangles,
and l lines, there are P p × S s × Aa × R r × T t × Ll possible assignments. This
number tends to run into astronomical figures. For example, the triangle-congruent
postulate has 6 points, 6 segments, 6 angles, 2 triangles, and 12 rays. For P111,
which has 8 points, 21 segments, 30 angles, 11 triangles, and 26 rays, the number
of possible assignments with the triangle-congruent postulate is 1041 .
As mentioned earlier, GRAMY uses a constraint satisfaction algorithm to search
such huge spaces effectively. Table V shows the number of partial matches found
for each DS diagram with problems regarding the equality of segments. The first
six columns are the numbers of elements in the problem figure (Points, Segments,
Angles, Triangles, Rays, and Lines). The remaining columns show the number of
matches per DS diagram. As can be seen in the table, the diagram-matching module
works effectively.

27

GRAMY

Table V. The number of matches for problems regarding equality of segments
No.

P S A T R L

TRI- RIGHT- SEGMENT- RECTANGLE- RIGHT- RHOMBUS
CONG

TRI-

SUM

DIAGONAL

TRI-

CONG

TRIMIDPOINT-

MEDIAN

LAW

P132 4 4

4 0 8 0

252

34

0

0

0

0

0

P144 4 5

8 2 10 0

276

56

12

0

0

0

0

P109 4 6

8 3 10 1

252

87

12

1

2

6

0

P131 6 11 19

4 18 2

582

106

12

2

3

4

0

P108 6 12 15

4 16 4

228

46

34

0

0

0

0

P112 7 16 20

8 20 5

552

68

34

70

6

18

36

P101 7 18 30 12 24 4

678

68

12

2

3

4

0

P127 7 18 30 12 24 4

654

68

12

2

3

4

0

P111 8 21 30 11 26 6

546

56

34

46

5

18

40

5. Conclusion
We have implemented an efficient and semi-complete procedure for proofs with
construction. Goldstein (1973) and Anzai et al. (1979) appear to have used the
same main procedure as GRAMY, namely, alternating forward chaining with a
selection of a useful postulate as the subject of construction. Their work may have
been based on Wong’s work (1972) that proposed a search for a desired postulate in
both forward and backward chaining. These authors appear not to have noticed that
the main procedure is complete, assuming the completeness of the subprocedure.
No previous provers have been proven to be complete for theorem proving with
construction. GRAMY is not complete either, but we have succeeded in proving its
semi-completeness, and Section 3.4 localizes the lack of completeness to a single
subprocedure, thus setting the stage for further work. Moreover, the results from an
empirical evaluation show that GRAMY works on most of the problems gathered
from textbooks and the literature. This section summarizes the lessons learned from
GRAMY and discusses future issues.
5.1. WHY GRAMY WORKS
We attribute the success of GRAMY to several features, discussed below.
5.1.1. Use of Exhaustive Forward Chaining
Because it does not cost much to apply exhaustive forward chaining to calculate the
deductive closure, finding a proof within a fixed problem figure is not costly either.
Thus, most of the computation is spent finding possible constructions by depth-first
search. We note, however, that the proof procedure guarantees exhaustive forward
chaining to stop only because arithmetic operations are excluded. Introducing arith-

28

NOBORU MATSUDA AND KURT VANLEHN

metic operations no longer guarantees reaching a quiescence state (e.g., one can
divide a segment infinitely many times).
5.1.2. Heuristics for Construction
GRAMY differs from other provers in the way missing segments are constructed
without relying on ad hoc heuristics. Thus, GRAMY has the power to find more
constructions. For example, Goldstein (1973) claims that only two constructions
for P111 are based on the heuristics applied. GRAMY found 135 successful and
unsuccessful constructions, including the one shown in Figure 6. Although this
construction does not lead to a proof, it is just as reasonable as the other four
successful constructions shown in Appendix B at the time it was made. This kind
of deficit in completeness of a construction procedure affects the completeness of
the whole theorem prover as well.
5.1.3. Use of Diagrams for Geometry Theorem Proving
Beside the use of diagrams as search control for the backward inference, as done
by Gelernter (1959) and many other provers, another advantage of using diagrams
is that it localizes necessary information into a single knowledge piece. Larkin and
Simon (1987) argue that due to the dense information embedded in a diagram, using diagrammatic representation requires less effort to draw information necessary
to make inference.
The diagram configuration model developed by Koedinger and Anderson (1990)
is based on this principle. In this model, a set of geometric postulates that shares
the same geometric configuration is represented as a single piece of knowledge
called a diagram configuration schema. A diagram configuration schema consists
of a diagram and a set of propositions to be held in the diagram. A proposition that
is a sufficient condition of all other propositions is called a whole-statement. The
propositions that are not whole-statements are called part-statements. A diagram
configuration schema also has all possible combinations of the part-statements representing the conditions of the whole-statement to be a true proposition. When any
of the conditions is satisfied, then the whole-statement and all other part-statements
in the schema are asserted as the true propositions. This collective deduction plays
a central role to improve the efficiency of search.
GRAMY uses the same kind of inference procedure and knowledge representation implemented as the diagrammatic schema (DS). A difference is that
GRAMY’s knowledge representation schema has more accumulative power than
the one developed by Koedinger and Anderson. This is because GRAMY’s diagrammatic schema does not discriminate between the whole-statement and the
part-statements; that is, GRAMY’s knowledge representation does not require any
single proposition to be the sufficient condition of all other propositions. Instead, all
possible deductions relating to a single diagram are combined into a single schema

GRAMY

29

whether they share a single condition or not. Consider, for example, the midpoint
law that does not have a single proposition corresponding to a whole-statement.
Besides the expressive power of the diagrammatic schema used in GRAMY,
a major difference is in the way they are used; GRAMY uses a partial match of
the diagram to carry out construction. Although we agree with Koedinger and Anderson’s opinion that the diagram configuration model is “particularly well-suited
for adding a construction capability” (1990, p. 532), we further claim that finding
plausible construction requires an intensive search, and hence implementing an
effective search control is essential for successful construction.
5.1.4. Construction with Reference to a Free Segment
We realized that construction with a reference to a free segment was neglected by
preceding studies. All of them discuss only how to construct a missing element by
considering the geometric structure of the desired postulate per se. The empirical
evaluation revealed that construction with reference to a free segment is essential
for some problems because there does not seem to be a proof other than the ones
produced by GRAMY with a construction with reference to a free segment.
5.2. GRAIN SIZE OF THE POSTULATES
Discussions in the preceding sections depend on a set of postulates given to the
theorem prover. For example, suppose a theorem prover eventually found a proof
with multiple constructions. By adding the problem’s premises, the goal, and the
final problem figure to the prover as a postulate, the prover can find the same proof
with single construction applied at the first quiescent state. For example, GRAMY
could solve P125 by single construction if it were given the incenter theorem (“all
three bisectors of angles in a triangle meet at a single point”) as a DS where the
three perpendicular segments from the incenter appear in the DS diagram.
Although adding proven problems as the postulates is quite natural in axiomatized systems like Euclidean geometry, we must make a good balance among the
grain size of postulates and a load for students to learn those postulates. The experiments showed that the postulates currently implemented in GRAMY are sufficient
to prove many problems used in textbooks. Hence we apparently need to add more
specific postulates only to prove those problems that require multiple constructions.
5.3. IMPLICATIONS FOR GEOMETRY EDUCATION
We conclude that GRAMY is adequate as a building block for a tutoring system on
theorem proving for several reasons:
 The midpoint law could be implemented as a single diagrammatic configuration schema with

a geometric statement, say, the-midpoint-law-is-held-in-ABC as a whole-statement. However, that
kind of statement would never appear in a proof.

30

NOBORU MATSUDA AND KURT VANLEHN

(1) GRAMY is skillful enough to solve all arithmetic-free construction problems
that appear in some school textbooks.
(2) The deductions used in its proofs employ the Euclidean axiom system that is
widely used in current school curricula.
(3) As a result, the proofs output by GRAMY are natural for students who are
learning Euclidean geometry.
Since GRAMY can generate most of the solutions for a given problem, one can
build a model tracing tutor that can recognize a student’s proof steps and provide
appropriate feedback. For each problem, a model tracing tutor has ideal solutions
required to find a solution. It compares the student’s problem-solving steps to the
steps taken in ideal solutions. For geometry theorem proving, the ideal solutions
contain all proofs that instructors would accept, which GRAMY can create in a
reasonable amount of time.
Even though GRAMY draws inferences that are familiar to students, the proof
procedure of GRAMY may be too complex for students to carry out despite the fact
that it can be simply described (cf. the description of Section 3.3). For instance,
students can hardly be expected to keep a queue of search states. Nonetheless,
students could explore the search space generated by GRAMY and receive feedback whenever they start to head down a dead-end path. This pedagogical tactic is
used by Andes (VanLehn et al., 2002), a successful intelligent tutoring system for
physics based on a semi-complete physics theorem prover.
5.4. FUTURE WORK
One major task is to determine the completeness of the subprocedure that generates constructions for a useful postulate. Another task is to find some kind of
search control that will allow GRAMY to solve the multiple construction problems
efficiently without harming its completeness. It would also be an interesting issue
to make GRAMY capable of proving that existence of the points constructed. We
would like to extend GRAMY to solve geometry problems that involve arithmetic
operations. Lastly, an intelligent tutoring system for geometry theorem proving
must be built with GRAMY, and its educational effectiveness must be examined
with appropriate students.

31

GRAMY

Appendix A. Construction Problems Used in the Evaluation
This appendix shows only the selected construction problems that are mentioned
in this article.
Table VI. Selected construction problems
P103

Given:
Goal:

P108

Given:
Goal:

P111

Given:
Goal:

P123

Given:
Goal:

P125

Given:
Goal:

P143

Given:
Goal:

AB = CD
AE = CE
BF = DF
 AYE =  CXE

AB = AC
DE = EF
BD = CF

DQ = BQ
AP = CP
CDAB
AM = DM
AB = CD

 ABC =  DCB
 BAD =  CDA
 ABE =  CBE
 BCF =  ACF
 BAQ =  CAQ

Parallelogram ABCD
BNAC
DN ⊥ AC
PQ = BP

32

NOBORU MATSUDA AND KURT VANLEHN

Appendix B. Example of Construction
Table VII. Successful construction for Problem P111
Construction:
1. Connect D and P
2. Extend DP to AB and plot X
3. Connect M and X
Useful Postulate:
Triangle congruent theorem
(AXM ≡ DXM)
Construction:
1. Connect A and Q
2. Extend DC
3. Extend AQ and plot X
Useful Postulate:
The midpoint-connector theorem
(AXC)
Construction:
1. Connect D and P
2. Extend DP to AB and plot X
Useful Postulate:
The midpoint-connector theorem
(DAX)
Construction:
1. Connect D and P
2. Extend DP to AB and plot X
3. Connect X and M
4. Extend CD
5. Extend XM to the extension of CD and plot Y
6. Connect A and Y
Useful Postulate:
The rhombus theorem
(Rhombus YAXD)

References
Anderson, J. R., Boyle, C. F., Corbett, A. T. and Lewis, M. W. (1990) Cognitive modeling and
intelligent tutoring, Artificial Intelligence 42(1), 7–49.
Anzai, Y., Ishibashi, N., Mitsuya, Y. and Ura, S. (1979) Knowledge-based problem solving by a
labelled production system, IJCAI, pp. 22–24.
Aref, M. N. and Wernick, W. (1968) Problems & Solutions in Euclidean Geometry, Dover
Publications, New York.
Chou, S.-C. and Gao, X.-S. (1989) A class of geometry statements of constructive type and geometry
theorem proving, Technical Report TR-89-37, Department of Computer Science, University of
Texas at Austin.
Chou, S.-C., Gao, X.-S. and Zhang, J.-Z. (1996) Automated generation of readable proofs with
geometric invariants, I. Multiple and shortest proof generation, J. Automated Reasoning 17,
325–347.

GRAMY

33

Chou, S.-C., Gao, X.-S. and Zhang, J.-Z. (2000) A deductive database approach to automated
geometry theorem proving and discovering, J. Automated Reasoning 25(3), 219–246.
Coelho, H. and Pereira, L. M. (1986) Automated reasoning in geometry theorem proving with Prolog,
J. Automated Reasoning 2(4), 329–390.
Coxford, A. F. and Usiskin, Z. P. (1971) Geometry: A Transformation Approach, Laidlay Brothers,
Palo Alto, CA.
Elcock, E. W. (1977) Representation of knowledge in geometry machine, in E. W. Elcock and D.
Michie (eds.), Machine Intelligence, Vol. 8, Wiley, pp. 11–29.
Gelernter, H. (1959) Realization of a geometry-theorem proving machine, in Proceedings of the
International Conference on Information Processing, pp. 273–282.
Gelernter, H., Hansen, J. R. and Loveland, D. W. (1963) Empirical explorations of the geometrytheorem proving machine, in E. A. Feigenbaum and J. Feldman (eds.), Computers and Thought,
McGraw-Hill, pp. 153–163.
Goldstein, I. (1973) Elementary geometry theorem proving, Technical Report AI Memo No. 280,
Massachusetts Institute of Technology.
Greeno, J. G., Magone, M. E. and Chaiklin, S. (1979) Theory of constructions and set in problem
solving, Memory and Cognition 7(6), 445–461.
Haralick, R. M. and Elliott, G. L. (1980) Increasing tree search efficiency for constraint satisfaction
problems, Artificial Intelligence 14(3), 263–313.
Kapur, D. (1986) Using Gröbner bases to reason about geometry problems, J. Symbolic Comput.
2(4), 399–408.
Koedinger, K. R. and Anderson, J. R. (1990) Abstract planning and perceptual chunks: Elements of
expertise in geometry, Cognitive Science 14(4), 511–550.
Kyogaku-Kenkyusha (n.d.) Congruence and Similarity, Kyogaku Kenkyusha, Osaka (in Japanese).
Larkin, J. H. and Simon, H. A. (1987) Why a diagram is (sometimes) worth ten thousand words,
Cognitive Science 11(1), 65–100.
Li, H. (2000) Clifford algebra approaches to mechanical geometry theorem proving, in X.-S. Gao
and D. Wang (eds.), Mathematics Mechanization and Applications, Academic Press, San Diego,
CA, pp. 205–299.
Matsushita, I. (1993) APT: A Textbook for Advanced High School Entrance Examinations, Shin-shindo, Tokyo (in Japanese).
Nevins, A. J. (1975) Plane geometry theorem proving using forward chaining, Artificial Intelligence
6(1), 1–23.
Reiter, R. (1972) The use of models in automatic theorem-proving, Technical Report Computer
Science 72-09, University of British Columbia.
Shinshindo (1993) High-Level Geometry Problems, Shin-shin-do, Tokyo (in Japanese).
Suwa, M. and Motoda, H. (1989) Acquisition of associative knowledge by the frustration-based
learning method in an auxiliary-line problem, Knowledge Acquisition 1(1), 113–137.
VanLehn, K., Lynch, C., Taylor, L., Weinstein, A., Shelby, R., Schulze, K., Treacy, D. and Wintersgill, M. (2002) Minimally invasive tutoring of complex physics problem solving, in S. A.
Cerri, G. Gouarderes and F. Paraguacu (eds.), Proceedings of the 6th International Conference
on Intelligent Tutoring Systems, pp. 367–376.
Wang, D. (1995) Reasoning about geometric problems using an elimination method, in J. Pfalzgraf
and D. Wang (eds.), Automated Practical Reasoning, Springer, New York, pp. 147–185.
Wong, R. (1972) Construction heuristics for geometry and a vector algebra representation of
geometry, Technical Report Project MAC 28, Massachusetts Institute of Technology.
Wu, W.-T. (2000) The characteristic set method and its application, in X.-S. Gao and D. Wang (eds.),
Mathematics Mechanization and Applications, Academic Press, San Diego, CA, pp. 3–41.

From	
  Behavioral	
  Description	
  to	
  	
  
A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems	
   	
  
JAVIER	
  GONZALEZ-­‐SANCHEZ,	
  Arizona	
  State	
  University	
  	
  
MARIA	
  ELENA	
  CHAVEZ-­‐ECHEAGARAY,	
  Arizona	
  State	
  University	
  	
  
KURT	
  VANLEHN,	
  Arizona	
  State	
  University	
  	
  
WINSLOW	
  BURLESON,	
  Arizona	
  State	
  University	
  	
  

Intelligent	
  Tutoring	
  Systems	
  are	
  software	
  applications	
  capable	
  of	
  complementing	
  and	
  enhancing	
  the	
  learning	
  process	
  by	
  providing	
  direct	
  
customized	
  instruction	
  and	
  feedback	
  to	
  students	
  in	
  various	
  disciplines.	
  Although	
  Intelligent	
  Tutoring	
  Systems	
  could	
  differ	
  widely	
  in	
  their	
  
attached	
   knowledge	
   bases	
   and	
   user	
   interfaces	
   (including	
   interaction	
   mechanisms),	
   their	
   behaviors	
   are	
   quite	
   similar.	
   Therefore,	
   it	
   must	
   be	
  
possible	
   to	
   establish	
   a	
   common	
   software	
   model	
   for	
   them.	
   A	
   common	
   software	
   model	
   is	
   a	
   step	
   forward	
   to	
   move	
   these	
   systems	
   from	
   proof-­‐
of-­‐concepts	
  and	
  academic	
  research	
  tools	
  to	
  widely	
  available	
  tools	
  in	
  schools	
  and	
  homes.	
  	
  The	
  work	
  reported	
  here	
  addresses:	
  (1)	
  the	
  use	
  of	
  
Design	
   Patterns	
   to	
   create	
   an	
   object-­‐oriented	
   software	
   model	
   for	
   Intelligent	
   Tutoring	
   Systems;	
   (2)	
   our	
   experience	
   using	
   this	
   model	
   in	
   a	
  
three-­‐year	
   development	
   project	
   and	
   its	
   impact	
   on	
   facets	
   such	
   as	
   creating	
   a	
   common	
   language	
   among	
   stakeholders,	
   supporting	
   an	
  
incremental	
  development,	
  and	
  adjustment	
  to	
  a	
  highly	
  shifting	
  development	
  team;	
  and	
  (3)	
  the	
  qualities	
  achieved	
  and	
  trade-­‐offs	
  made.	
  	
  
Categories	
   and	
   Subject	
   Descriptors:	
   D.2.10	
   [Software	
   Engineering]:	
   Design;	
   D.2.11	
   [Software	
   Engineering]:	
   Software	
   Architecture;	
  
D.2.13	
  [Software	
  Engineering]:	
  Reusable	
  Software.	
  
General	
  Terms:	
  Design.	
  
Additional	
  Key	
  Words	
  and	
  Phrases:	
  Design	
  patterns,	
  component	
  model,	
  intelligent	
  tutoring	
  systems,	
  behavioral	
  description.	
  
ACM	
  Reference	
  Format:	
  	
  
Gonzalez-­‐Sanchez,	
  J.,	
  Chavez-­‐Echeagaray,	
  M.E.,	
  VanLehn,	
  K.	
  and	
  Burleson,	
  W.	
  2011.	
  From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  
for	
   Intelligent	
   Tutoring	
   Systems.	
   18th	
   Conference	
   on	
   Pattern	
   Languages	
   of	
   Programs	
   (PLoP),	
   Portland,	
   Oregon,	
   USA	
   (October	
   2011),	
   12	
  
pages.	
  
	
  

1. INTRODUCTION	
  
Intelligent	
   Tutoring	
   Systems	
   (ITS)	
   complements	
   and	
   enhances	
   the	
   learning	
   process	
   by	
   offering	
   support	
   for	
  
students	
   and	
   complementing	
   the	
   teacher’s	
   labor.	
   There	
   are	
   many	
   discussions	
   about	
   ITS	
   pedagogical	
   and	
  
instructional	
   design,	
   such	
   as	
   (Anderson	
   et	
   al.	
   1995,	
   Baker	
   et	
   al.	
   2009,	
   and	
   Nelson	
   2007),	
   but	
   not	
   about	
   their	
  
technical	
  implementation.	
  	
  
	
  
The	
   work	
   described	
   here	
   is	
   part	
   of	
   the	
   technical	
   implementation	
   of	
   an	
   ITS,	
   named	
   Affective	
   Meta	
   Tutor	
  
(AMT).	
   The	
   AMT	
   project	
   aims	
   to	
   use	
   an	
   affective	
   learning	
   companion	
   to	
   motivate	
   students	
   to	
   permanently	
  
adopt	
   effective	
   meta-­‐cognitive	
   strategies	
   (AMT	
   2012).	
   The	
   AMT	
   project	
   looks	
   to	
   improve	
   ITS	
   not	
   only	
   by	
  
adding	
   those	
   new	
   elements	
   (meta-­‐tutoring	
   strategies	
   and	
   affective	
   learning	
   companions),	
   but	
   also	
   by	
   taking	
  
advantage	
   of	
   previous	
   experiences	
   with	
   ITS	
   implementations	
   from	
   the	
   ITS	
   community	
   to	
   provide	
   a	
   software	
  
framework	
   for	
   designing	
   ITS	
   based	
   on	
   a	
   behavioral	
   description;	
   modeling	
   meta-­‐tutoring	
   capabilities	
   and	
  
learning	
  companions	
  modules	
  is	
  not	
  part	
  of	
  this	
  paper	
  and	
  is	
  left	
  as	
  future	
  work.	
  
	
  
An	
  analysis	
  and	
  comparison	
  of	
  existing	
  ITS	
  was	
  made	
  in	
  (VanLehn	
  2006).	
  The	
  analysis	
  included	
  a	
  diverse	
  
set	
   of	
   ITS	
   such	
   as	
   an	
   ITS	
   for	
   Algebra	
   in	
   High	
   School	
   (Anderson	
   et	
   al.	
   1995);	
   a	
   tutor	
   for	
   physics	
   in	
   College	
  
We	
   are	
   grateful	
   to	
   Hironori	
   Washizaki	
   for	
   his	
   support	
   during	
   the	
   writing	
   process	
   of	
   this	
   paper.	
   This	
   work	
   is	
   supported	
   by	
   the	
   National	
  
Science	
  Foundation,	
  including	
  the	
  following	
  grants:	
  (1)	
  IIS/HCC	
  Affective	
  Learning	
  Companions:	
  Modeling	
  and	
  supporting	
  emotion	
   during	
  
learning	
  (#0705883);	
  and	
  (2)	
  Deeper	
  Modeling	
  via	
  Affective	
  Meta-­‐tutoring	
  (DRL-­‐	
  0910221).	
  
Author's	
  address:	
  Javier	
  Gonzalez-­‐Sanchez,	
  University	
  Drive	
  and	
  Mill	
  Avenue,	
  Tempe	
  AZ	
  85287;	
  email:	
  javiergs@asu.edu;	
  Author’s	
  address:	
  
Maria-­‐Elena	
  Chavez-­‐Echeagaray,	
  University	
  Drive	
  and	
  Mill	
  Avenue,	
  Tempe	
  AZ	
  85287;	
  email:	
  helenchavez@asu.edu;	
  Author's	
  address:	
  Kurt	
  
VanLehn,	
   University	
   Drive	
   and	
   Mill	
   Avenue,	
   Tempe	
   AZ	
   85287;	
   email:	
   kurthvanlen@asu.edu;	
   Author’s	
   address:	
   Winslow	
   Burleson,	
  
University	
  Drive	
  and	
  Mill	
  Avenue,	
  Tempe	
  AZ	
  85287;	
  email:	
  winslow.burleson@asu.edu	
  	
  
Permission	
  to	
  make	
  digital	
  or	
  hard	
  copies	
  of	
  all	
  or	
  part	
  of	
  this	
  work	
  for	
  personal	
  or	
  classroom	
  use	
  is	
  granted	
  without	
  fee	
  provided	
  that	
  
copies	
  are	
  not	
  made	
  or	
  distributed	
  for	
  profit	
  or	
  commercial	
  advantage	
  and	
  that	
  copies	
  bear	
  this	
  notice	
  and	
  the	
  full	
  citation	
  on	
  the	
  first	
  page.	
  
To	
  copy	
  otherwise,	
  to	
  republish,	
  to	
  post	
  on	
  servers	
  or	
  to	
  redistribute	
  to	
  lists,	
  requires	
  prior	
  specific	
  permission.	
  A	
  preliminary	
  version	
  of	
  
this	
  paper	
  was	
  presented	
  in	
  a	
  writers'	
  workshop	
  at	
  the	
  18th	
  Conference	
  on	
  Pattern	
  Languages	
  of	
  Programs	
  (PLoP).	
  PLoP'11,	
  October	
  21-­‐
23,	
  Portland,	
  Oregon,	
  USA.	
  Copyright	
  2011	
  is	
  held	
  by	
  the	
  author(s).	
  ACM	
  978-­‐1-­‐4503-­‐1283-­‐7

(VanLehn	
   et	
   al.	
   2005);	
   a	
   tutor	
   for	
   qualitative	
   reasoning	
   in	
   natural	
   language	
   (Graesser	
   et	
   al.	
   2004);	
   a	
   simulated-­‐
based	
   tutor	
   for	
   repairing	
   avionic	
   electronic	
   equipment	
   (Katz	
   et	
   al.	
   1998);	
   and	
   an	
   ITS	
   to	
   teach	
   SQL	
   language	
  
(Mitrovic	
   2003).	
   The	
   analysis	
   and	
   comparison	
   concluded	
   that	
   only	
   a	
   few	
   pedagogical	
   features	
   have	
   been	
  
invented	
   and	
   that	
   existing	
   ITS	
   offered	
   different	
   combinations	
   of	
   those	
   features.	
   	
   It	
   also	
   claimed	
   that	
   ITS	
  
behaviors	
  are	
  similar	
  but	
  the	
  ITS	
  differ	
  widely	
  in	
  their	
  software	
  implementation.	
  
	
  
From	
  a	
  software	
  engineering	
  perspective,	
  this	
  variety	
  in	
  software	
  implementation	
  shows	
  a	
  lack	
  of	
  the	
  use	
  of	
  
software	
   engineering	
   techniques	
   and	
   methodologies	
   in	
   the	
   development	
   of	
   this	
   kind	
   of	
   systems,	
   because	
   the	
  
same	
  specifications	
  are	
  creating	
  different	
  products.	
  Subsequently,	
  it	
  will	
  be	
  valuable	
  to	
  establish	
  a	
  model	
  that	
  
moves	
  from	
  the	
  ITS	
  behavior	
  description	
  to	
  the	
  system	
  implementation.	
  An	
  optimal	
  model	
  should	
  be	
  capable	
  of	
  
satisfying	
  the	
  requirements	
  of	
  ITS	
  and	
  providing	
  desired	
  software	
  qualities.	
  This	
  paper	
  describes	
  our	
  approach	
  
to	
  address	
  the	
  design	
  of	
  this	
  model	
  within	
  a	
  context	
  driven	
  by	
  three	
  key	
  elements:	
  
	
  
• Incremental	
   requirements.	
   AMT	
   required	
   incremental	
   evolution	
   along	
   three-­‐years;	
   developing	
   an	
   ITS	
  
the	
  first	
  year,	
  adding	
  meta-­‐tutoring	
  support	
  the	
  second	
  year,	
  and	
  including	
  affective	
  learning	
  companions	
  
the	
  third	
  year.	
  Research	
  results	
  and	
  user	
  experience	
  reports	
  drove	
  new	
  requirements.	
  	
  	
  	
  
• Changing	
   requirements.	
   Several	
   and	
   diverse	
   research	
   approaches	
   were	
   tested	
   as	
   part	
   of	
   the	
   project	
  
implementation.	
   For	
   each	
   approach	
   a	
   solid	
   system	
   was	
   released	
   and	
   tested	
   with	
   students.	
   Research	
  
findings	
  were	
  translated	
  into	
  changes	
  in	
  the	
  system.	
  
• Shifting	
   development	
   team.	
   The	
   development	
   team,	
   composed	
   of	
   undergraduate	
   students,	
   shifted	
  
constantly	
  -­‐	
  every	
  4	
  to	
  6	
  months.	
  
	
  
In	
   this	
   context,	
   we	
   chose	
   to	
   incorporate	
   design	
   patterns	
   to	
   standardize	
   an	
   object-­‐oriented	
   model	
   for	
   ITS	
  
functionality	
   that	
  drives	
  the	
  way	
  in	
  which	
  software	
  is	
  developed.	
  We	
  mapped	
  the	
  functional	
  description	
  of	
  ITS	
  
behavior	
   given	
   in	
   (VanLehn	
   2006)	
   into	
   a	
   software	
   model	
   using	
   some	
   of	
   the	
   “Gang	
   of	
   Four”	
   (GoF)	
   design	
  
patterns	
   (Gamma	
   et	
   al.	
   1995);	
   GoF	
   design	
   patterns	
   were	
   chosen	
   because	
   they	
   are	
   classic	
   patterns	
   often	
  
considered	
  the	
  foundation	
  for	
  all	
  other	
  patterns.	
  Using	
  GoF	
  design	
  patterns	
  we	
  addressed	
  the	
  creation	
  of	
  the	
  
model	
  and	
  sought	
  to	
  incorporate	
  on	
  it	
  non-­‐functional	
  requirements	
  (i.e.	
  software	
  quality	
  factors)	
  particularly	
  
reusability,	
   extensibility,	
   and	
   adaptability	
   (IEEE	
   1999).	
   These	
   qualities	
   help	
   us	
   to	
   address	
   the	
   contextual	
  
elements	
   mentioned	
   above:	
   incremental	
   requirements,	
   changing	
   requirements,	
   and	
   a	
   shifting	
   development	
  
team.	
   With	
   this	
   approach	
   we	
   seek	
   to	
   contribute	
   moving	
   ITS	
   construction	
   from	
   software	
   development	
   as	
   a	
   one-­‐
of-­‐a-­‐kind	
   endeavor	
   to	
   software	
   development	
   as	
   a	
   system	
   of	
   components	
   that	
   are	
   widely	
   used	
   and	
   highly	
  
adaptable	
  (Jacobson	
  1997).	
  	
  
	
  
This	
   paper	
   is	
   organized	
   as	
   follows:	
   Section	
   2	
   provides	
   some	
   terminology	
   and	
   background	
   about	
   ITS,	
  
patterns,	
   and	
   software	
   qualities;	
   Section	
   3	
   explores	
   ITS	
   functional	
   specification	
   and	
   the	
   design	
   process	
   using	
  
patterns	
   to	
   model	
   ITS	
   software	
   components;	
   Section	
   4	
   describes	
   our	
   experience	
   using	
   design	
   patterns	
   into	
   the	
  
AMT	
  project	
  and	
  evaluates	
  pros	
  and	
  cons;	
  finally,	
  Section	
  5	
  concludes	
  the	
  paper	
  and	
  describes	
  ongoing	
  work.	
  
We	
  expect	
  developers	
  in	
  ITS	
  and	
  education	
  technology	
  communities	
  to	
  find	
  this	
  paper	
  useful	
  as	
  a	
  reference	
  and	
  
as	
   an	
   example	
   of	
   the	
   use	
   and	
   advantages	
   of	
   design	
   patterns	
   for	
   developing	
   software	
   systems;	
   for	
   software	
  
design	
   community,	
   this	
   is	
   an	
   experience	
   report	
   of	
   a	
   research	
   group	
   using	
   design	
   patterns	
   to	
   improve	
   its	
  
software	
  process.	
  
	
  
2. BACKGROUND	
  
This	
  section	
  provides	
  background	
  about	
  ITS	
  structure	
  and	
  clarifies	
  some	
  related	
  terminology	
  used	
  within	
  this	
  
paper.	
   It	
   also	
   provides	
   background	
   information	
   about	
   design	
   patterns,	
   and	
   the	
   definition	
   of	
   the	
   software	
  
qualities	
  expected	
  for	
  the	
  proposed	
  model.	
  	
  
	
  
2.1 ITS	
  Structure	
  
ITS	
  refers	
  to	
  a	
  computer	
  system	
  that	
  acts	
  as	
  a	
  tutor	
  showing	
  an	
  intelligent	
  way	
  to	
  provide	
  feedback	
  and	
  hints	
  to	
  
support	
   student	
   achievement	
   while	
   solving	
   tasks.	
   A	
   task	
   refers	
   to	
   a	
   multi-­‐minute	
   activity	
   assigned	
   to	
   the	
  
student	
  by	
  the	
  ITS.	
  Tasks	
  can	
  be	
  skipped	
  or	
  interchanged	
  with	
  other	
  tasks.	
  Each	
  of	
  the	
  actions	
  taken	
  to	
  achieve	
  
From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  2	
  

a	
  task	
  is	
  called	
  a	
  step.	
  Each	
  task	
  consists	
  of	
  multiple	
  steps	
  and	
  each	
  step	
  involves	
  events	
  with	
  a	
  User	
  Interface	
  
(either	
  through	
  a	
  tool	
  or	
  an	
  environment).	
  	
  Each	
  task	
  requires	
  the	
  use	
  of	
  one	
  or	
  more	
  knowledge	
  components	
  
to	
  be	
  accomplished.	
  Knowledge	
  components	
  are	
  fragments	
  of	
  persistent	
  domain-­‐specific	
  information	
  that	
  the	
  
student	
  is	
  expected	
  to	
  learn.	
  Knowledge	
  components	
  are	
  contained	
  in	
  a	
  knowledge	
  base.	
  
	
  
ITS	
   structure	
  is	
  a	
   three-­‐layer	
  model,	
   as	
   shown	
   in	
   Figure	
   1,	
   that	
   decouples	
  the	
   Knowledge	
   Base	
   and	
   the	
   User	
  
Interfaces	
  from	
  the	
  Core	
  of	
  the	
  ITS.	
  The	
  description	
  of	
  each	
  layer	
  is	
  as	
  follow:	
  
	
  
• Knowledge	
  Base	
  (KB)	
  includes	
  data	
  structures	
  and	
  databases	
  for	
  storing	
  and	
  organizing	
  the	
  information	
  
instructed	
   by	
   the	
   ITS.	
   The	
   process	
   of	
   putting	
   data	
   into	
   the	
   KB	
   is	
   called	
   authoring.	
   Authoring	
   involves	
   a	
  
human	
   expert	
   interacting	
   with	
   an	
   authoring	
   tool	
   to	
   provide	
   this	
   data.	
   Occasionally,	
   machine-­‐learning	
  
algorithms	
  have	
  been	
  used	
  to	
  create	
  this	
  expertise.	
  Authoring	
  and	
  KB	
  representation	
  are	
  topics	
  outside	
  of	
  
this	
  paper.	
  
• User	
   Interfaces	
   (UI)	
   include	
   graphical	
   interfaces	
   (windows,	
   buttons,	
   text,	
   and	
   so	
   on)	
   and	
   interaction	
  
mechanisms	
   (from	
   single	
   keyboard	
   events	
   to	
   more	
   complex	
   interfaces	
   such	
   as	
   motion	
   capture,	
   voice	
  
recognition,	
  brain-­‐computer	
  interface,	
  and	
  so	
  on).	
  
• Core	
   implements	
   the	
   ITS	
   behavior.	
   While	
   Knowledge	
   Base	
   and	
   User	
   Interfaces	
   are	
   highly	
   different	
   from	
  
one	
   ITS	
   to	
   another,	
   the	
   behavior	
   of	
   all	
   of	
   them	
   is	
   quite	
   similar.	
   The	
   Core	
   is	
   composed	
   of:	
   (1)	
   Task	
   Selector,	
  
which	
  provides	
  a	
  Task	
  (problem	
  or	
  activity)	
  that	
  the	
  student	
  must	
  solve;	
  (2)	
  Tool	
  or	
  Environment,	
  which	
  
models	
   and	
   presents	
   the	
   information	
   that	
   the	
   student	
   must	
   know	
   to	
   complete	
   the	
  Task;	
   (3)	
   Step	
   Analyzer,	
  
which	
  methodically	
  examines	
  and	
  measures	
  the	
  performance	
  of	
  the	
  student	
  and	
  provides	
  that	
  information	
  
to	
  the	
  Assessor	
  and	
  the	
  Pedagogical	
  Module;	
  (4)	
  Pedagogical	
  Module,	
  which	
  provides	
  support	
  (hints	
  and	
  
feedback)	
  to	
  make	
  the	
  student	
  successfully	
  complete	
  the	
  Task;	
  support	
  is	
  related	
  with	
  the	
  performance	
  of	
  
the	
  student	
  in	
  the	
  current	
  Step	
  and	
  the	
  information	
  from	
  the	
  student’s	
  Learner	
  Model;	
  and	
  (5)	
  Assessor,	
  
which	
  learns	
  from	
  the	
  student	
  (how	
  many	
  hints	
  he	
  needed,	
  how	
  skilled	
  he	
  was	
  in	
  the	
  topic,	
  how	
  much	
  time	
  
he	
  used	
  to	
  go	
  from	
  one	
  step	
  to	
  another	
  in	
  order	
  to	
  solve	
  the	
  task,	
  etc.)	
  and	
  then	
  stores	
  this	
  information	
  in	
  
what	
   is	
   known	
   as	
   a	
   Learner	
   Model.	
   The	
   connections	
   between	
   the	
   Core’s	
   components	
   are	
   described	
   in	
  
Section	
  3.	
  
	
  

	
  

Fig.	
  1.	
  ITS	
  layered	
  structure:	
  User	
  Interface	
  (and	
  interaction	
  mechanisms),	
  Functionality	
  (Core),	
  and	
  Data	
  (Knowledge	
  Base)	
  are	
  decoupled.	
  

	
  

From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  3	
  

The	
  rest	
  of	
  this	
  paper	
  is	
  devoted	
  to	
  modeling	
  the	
  Core,	
  the	
  layer	
  that	
  implements	
  the	
  behavioral	
  response	
  of	
  
the	
  ITS.	
  Modeling	
  Knowledge	
  Base,	
  User	
  Interfaces,	
  and	
  the	
  connection	
  between	
  them	
  and	
  the	
  Core	
  is	
  out	
  of	
  the	
  
scope	
   of	
   this	
   paper.	
   However,	
   the	
   connection	
   between	
   User	
   Interfaces	
   and	
   Core	
   can	
   be	
   easily	
   figured	
   out	
   as	
   an	
  
MVC	
   pattern	
   application	
   (Buschmann	
   et	
   al.	
   1996),	
   where	
   Core	
   acts	
   as	
   the	
   Model	
   part.	
   The	
   connection	
   between	
  
Core	
  and	
  Knowledge	
  Base	
  can	
  be	
  realized	
  using	
  diverse	
  data	
  access	
  approaches.	
  
	
  

2.2

Why	
  Design	
  Patterns?	
  

Software	
  design	
  patterns	
  are	
  used	
  as	
  a	
  general	
  reusable	
  solution	
  to	
  a	
  commonly	
  occurring	
  problem	
  in	
  software	
  
design,	
   to	
   show	
   relationships	
   and	
   interactions	
   between	
   components	
   and	
   provide	
   a	
   skeleton	
   for	
   the	
  
implementation	
   (Gamma	
   et	
   al.	
   1995).	
   Even	
   though	
   the	
   concept	
   of	
   patterns	
   has	
   received	
   relatively	
   little	
  
attention	
  in	
  the	
  field	
  of	
  ITS,	
  Devedzic	
  and	
  Harrer	
  (2005)	
  mention	
  that	
  many	
  ITS	
  designers	
  and	
  developers	
  use	
  
their	
   own	
   solutions	
   when	
   faced	
   with	
   design	
   problems	
   that	
   are	
   common	
   to	
   different	
   systems,	
   models,	
   and	
  
paradigms;	
   even	
   when	
   a	
   closer	
   look	
   into	
   that	
   solutions	
   and	
   their	
   comparison	
   often	
   shows	
   that	
   different	
  
solutions	
  and	
  the	
  contexts	
  in	
  which	
  they	
  are	
  applied	
  have	
  much	
  in	
  common.	
  
	
  
In	
  that	
  context,	
  our	
  choice	
  about	
  using	
  design	
  patterns	
  in	
  this	
  project	
  was	
  driven	
  by	
  our	
  interest	
  in:	
  
	
  
• Communication.	
   Since	
   patterns’	
   names	
   closely	
   match	
   their	
   objective	
   and	
   the	
   problem	
   they	
   solve,	
   we	
   used	
  
them	
  as	
  a	
  common	
  vocabulary	
  among	
  diverse	
  stakeholders	
  aiming	
  to	
  improve	
  the	
  communication	
  process.	
  
Patterns	
   provide	
   us	
   with	
   a	
   standard	
   vocabulary	
   to	
   describe	
   the	
   topology	
   of	
   the	
   system,	
   the	
   structural	
  
hierarchy	
  of	
  the	
  subsystems,	
  and	
  their	
  interfaces	
  and	
  connections.	
  
• Collaboration.	
   Patterns	
   support	
   the	
   sharing	
   of	
   constructions	
   between	
   developers	
   or	
   either	
   use	
   other’s	
  
constructions	
  to	
  enhance	
  our	
  own.	
  No	
  matter	
  what	
  is	
  been	
  built	
  or	
  what	
  others	
  built,	
  it	
  is	
  always	
  known	
  
which	
  are	
  going	
  to	
  be	
  the	
  relations	
  (connections)	
  among	
  different	
  constructions.	
  	
  
• Productivity.	
   Patterns	
   help	
   to	
   create	
   components,	
   and	
   components	
   support	
   the	
   creation	
   of	
   families	
   of	
  
products	
  and/or	
  several	
  versions	
  of	
  the	
  same	
  product	
  to	
  prototype	
  and	
  test	
  new	
  options	
  of	
  functionality.	
  
• Abstraction.	
   Patterns	
   are	
   more	
   abstract	
   than	
   just	
   a	
   technical	
   model,	
   but	
   more	
   technical	
   than	
   a	
   conceptual	
  
model.	
  Patterns	
  make	
  it	
  possible	
  to	
  provide	
  a	
  “controlled”	
  freedom	
  to	
  the	
  programmers	
  because	
  they	
  can	
  
develop	
  functionality	
  in	
  their	
  own	
  creative	
  way,	
  but	
  they	
  follow	
  and	
  preserve	
  the	
  guidelines	
  of	
  a	
  defined	
  
design.	
  	
  
	
  
These	
   benefits	
   of	
   using	
   patterns	
   (communication,	
   collaboration,	
   productivity,	
   and	
   abstraction)	
   help	
   us	
   to	
  
overcome	
   the	
   challenging	
   contextual	
   elements	
   of	
   the	
   project	
   (incremental	
   requirements,	
   changing	
  
requirements,	
  and	
  a	
  shifting	
  development	
  team).	
  
	
  
2.3

ITS	
  Qualities	
  

ITS	
  are	
  pieces	
  of	
  software,	
  hence	
  they	
  are	
  expected	
  to	
  meet	
  some	
  software	
  quality	
  criteria.	
  Therefore,	
  modeling	
  
ITS	
   behavior	
   is	
   also	
   about	
   accomplishing	
   quality	
   considerations	
   that	
   drive	
   their	
   design.	
   Software	
   quality	
  
criteria	
   are	
   specified	
   as	
   non-­‐functional	
   requirements.	
   Accomplishing	
   non-­‐functional	
   requirements	
   is	
   one	
  
additional	
   reason	
   to	
   use	
   design	
   patterns.	
   Design	
   patterns	
   let	
   us	
   take	
   advantage	
   of	
   previous	
   experiences	
   to	
  
implement	
   non-­‐functional	
   requirements	
   and	
   to	
   avoid,	
   when	
   properly	
   used,	
   accidental	
   complexity.	
   The	
   non-­‐
functional	
  requirements	
  addressed	
  in	
  the	
  project	
  were:	
  
	
  
• Reusability.	
   Reusability	
   refers	
   to	
   the	
   degree	
   to	
   which	
   a	
   software	
   module	
   or	
   other	
   work	
   product	
   can	
   be	
  
used	
  in	
  more	
  than	
  one	
  computer	
  program	
  or	
  software	
  system	
  (IEEE	
  1999).	
  ITS	
  components	
  must	
  be	
  able	
  
to	
  be	
  used	
  again	
  with	
  slight	
  or	
  no	
  modification	
  for	
  the	
  implementation	
  of	
  other	
  products	
  or	
  versions	
  of	
  the	
  
same	
  project.	
  	
  
• Extensibility.	
   Extensibility	
   is	
   the	
   degree	
   to	
   which	
   a	
   system	
   or	
   component	
   can	
   be	
   easily	
   modified	
   to	
  
increase	
   its	
   storage	
   or	
   functional	
   capacity	
   (IEEE	
   1999).	
   ITS	
   components	
   in	
   the	
   model	
   must	
   be	
   able	
   to	
  
incorporate	
  new	
  functionalities	
  or	
  modify	
  existing	
  functionalities	
  (e.g.,	
  assessment	
  strategies,	
  task-­‐creation	
  
strategies,	
  learning	
  algorithms	
  to	
  mining	
  learner	
  model,	
  etc.).	
  

From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  4	
  

•
•

•

Adaptability.	
   Adaptability	
   refers	
   to	
   the	
   ease	
   with	
   which	
   a	
   system	
   or	
   component	
   can	
   be	
   modified	
   for	
   using	
  
it	
  in	
  applications	
  or	
  environments	
  other	
  than	
  those	
  for	
  which	
  it	
  was	
  specifically	
  designed	
  (IEEE	
  1999).	
  	
  
Robustness.	
   Robustness	
   is	
   the	
   degree	
   to	
   which	
   a	
   system	
   or	
   component	
   can	
   function	
   correctly	
   in	
   the	
  
presence	
   of	
   invalid	
   inputs	
   or	
   stressful	
   environmental	
   conditions	
   (IEEE	
   1999).	
   Students	
   expect	
   to	
   get	
  
effective	
   and	
   efficient	
   support	
   from	
   the	
   ITS,	
   as	
   if	
   it	
   was	
   a	
   human	
   tutor;	
   interruptions	
   in	
   the	
   teaching-­‐
learning	
  process	
  due	
  to	
  software	
  failures	
  are	
  highly	
  undesirable.	
  	
  
Performance.	
   Performance	
   refers	
   to	
   the	
   degree	
   to	
   which	
   a	
   system	
   or	
   component	
   accomplishes	
   its	
  
designated	
  functions	
  within	
  given	
  constraints,	
  such	
  as	
  speed,	
  accuracy,	
  or	
  memory	
  usage	
  (IEEE	
  1999).	
  The	
  
ITS	
  must	
  emulate	
  real-­‐time	
  responses	
  from	
  a	
  human	
  tutor;	
  delays	
  must	
  be	
  avoided	
  and	
  latency	
  reduced.	
  

	
  
The	
  use	
  of	
  patterns	
  becomes	
  the	
  keystone	
  to	
  satisfy	
  the	
  first	
  three	
  qualities	
  enumerated	
  above.	
  Satisfaction	
  of	
  
the	
  last	
  two	
  requirements	
  (robustness	
  and	
  performance)	
  is	
  related	
  to	
  the	
  implementation	
  of	
  the	
  model	
  and	
  not	
  
with	
  the	
  model	
  per	
  se.	
  However,	
  in	
  our	
  experience	
  communication,	
  collaboration,	
  productivity,	
  and	
  abstraction	
  
impact	
  performance	
  and	
  robustness.	
  
	
  
2.4 Unified	
  Modeling	
  Language	
  
Unified	
   Modelling	
   Language	
   (UML)	
   notation	
   is	
   used	
   as	
   modelling	
   language,	
   specifically	
   UML	
   class	
   diagrams.	
   To	
  
facilitate	
  the	
  understanding	
  of	
  the	
  diagrams	
  offered	
  in	
  this	
  paper	
  (see	
  Figures	
  2,	
  3,	
  and	
  4)	
  this	
  section	
  provides	
  
a	
  brief	
  description	
  of	
  the	
  elements	
  (boxes	
  and	
  arrows)	
  within	
  a	
  UML	
  diagram.	
  
	
  
• Boxes	
  represent	
  components	
  or	
  classes.	
  
• Arrows	
   represent	
   association	
   relationships;	
   the	
   arrows	
   go	
   from	
   the	
   component	
   that	
   requests	
   functionality	
  
to	
  the	
  component	
  that	
  provides	
  that	
  functionality.	
  	
  
• Arrows	
   with	
   dashed	
   lines	
   represent	
   dependency	
   relationships;	
   in	
   the	
   model	
   we	
   are	
   showing	
   the	
  
dependency	
  between	
  functional	
  components	
  and	
  the	
  data	
  component	
  they	
  require	
  to	
  access	
  them.	
  
• Arrows	
  with	
  a	
  triangular	
  shape	
  in	
  the	
  arrowhead	
  represent	
  inheritance	
  relationships.	
  
• Arrows	
   starting	
   with	
   a	
   diamond	
   shape	
   represent	
   composition	
   relationships;	
   they	
   are	
   used	
   to	
   represent	
  
that	
  a	
  component	
  is	
  formed	
  by	
  a	
  conjunction	
  of	
  other	
  components.	
  	
  	
  	
  
	
  
3. MODELING	
  THE	
  ITS	
  BEHAVIOR	
  
This	
   section	
   uses	
   the	
   ITS	
   behavior	
   described	
   in	
   (VanLehn	
   2006)	
   to	
   create	
   a	
   conceptual	
   model	
   for	
   the	
   Core	
  
layer.	
   The	
   ITS	
   behavior,	
   stated	
   in	
   (VanLehn	
   2006),	
   is	
   summarized	
   in	
   a	
   list	
   of	
   statements	
   that	
   identifies	
   the	
  
involved	
  components,	
  responsibilities	
  for	
  each	
  component,	
  and	
  relationships	
  between	
  components.	
  In	
  the	
  list,	
  
components’	
   names	
   were	
   marked	
   in	
   bold	
   and	
   relationships	
   between	
   components	
   are	
   explained.	
   Complex	
  
components	
   were	
   split	
   into	
   simple	
   ones,	
   identifying	
   specific	
   responsibilities	
   and	
   assigning	
   them	
   to	
   new	
  
components.	
  The	
  list	
  of	
  statements	
  is	
  as	
  follows:	
  
	
  
• Students	
  interact	
   with	
   the	
   ITS	
   through	
   the	
  User	
   Interface.	
   The	
   events	
   triggered	
   by	
   the	
   User	
   Interface	
  are	
  
handled	
   by	
   the	
   Tool	
   component.	
   The	
   Tool	
   is	
   the	
   component	
   responsible	
   for	
   creating	
   and	
   managing	
   the	
  
environment	
  in	
  which	
  the	
  student	
  works.	
  	
  
• The	
  ITS	
  provides	
  students	
  with	
  tasks	
  to	
  be	
  solved	
  and	
  helps	
  them	
  in	
  the	
  process.	
  A	
  Task	
  is	
  a	
  set	
  of	
  steps.	
  	
  
For	
  simplicity,	
  this	
  report	
  assumes	
  that	
  the	
  set	
  of	
  steps	
  is	
  static	
  and	
  pre-­‐enumerated.	
  	
  In	
  principle,	
  a	
  Task	
  
could	
   be	
   a	
   step	
   generator,	
   which	
   means	
   steps	
   could	
   be	
   dynamically	
   generated.	
   	
   The	
   ITS	
   literature	
   refers	
   to	
  
tutors	
   using	
   a	
   static,	
   pre-­‐enumerated	
   set	
   of	
   steps	
   as	
   “example	
   tracing	
   tutors”	
   whereas	
   those	
   using	
   steps	
  
generators	
  are	
  called	
  “model	
  tracing	
  tutors”	
  (VanLehn,	
  2006).	
  	
  	
  Each	
  Step	
  is	
  related	
  to	
  events	
  in	
  the	
  User	
  
Interface.	
   Steps	
   include	
   Assessment	
   and	
   Help.	
   Help	
   could	
   be	
   Hints	
   before	
   completing	
   the	
   Step	
   or	
  
Feedback	
  after	
  completing	
  the	
  Step.	
  Each	
  Task	
  is	
  related	
  to	
  a	
  set	
  of	
  Knowledge	
  Components	
  that	
  are	
  the	
  
information	
   and	
   skills	
   that	
   a	
   student	
   needs	
   to	
   apply	
   in	
   order	
   to	
   solve	
   the	
   Task	
   successfully.	
   The	
  
Knowledge	
  Base	
  is	
  a	
  set	
  of	
  Tasks	
  and	
  Knowledge	
  Components	
  and	
  the	
  mapping	
  between	
  them.	
  
• The	
   ITS	
   behavior	
   starts	
   when	
   the	
   Task	
   Selector	
   selects	
   the	
   next	
   Task	
   that	
   the	
   student	
   must	
   solve	
   and	
  
places	
   the	
   Task	
   into	
   the	
   Tool	
   in	
   order	
   to	
   be	
   solved	
   by	
   the	
   student.	
   The	
   four	
   basic	
   methods	
   to	
   do	
   “task	
  
From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  5	
  

•

•

selection”	
   are	
   described	
   in	
   (VanLehn	
   2006).	
   The	
   Task	
   Selector	
   needs	
   to	
   have	
   access	
   to	
   a	
   source	
   of	
  Tasks.	
  
The	
   Task	
   Factory	
   is	
   the	
   source	
   of	
   Tasks.	
   It	
   either	
   reads	
   Tasks	
   stored	
   in	
  the	
   Knowledge	
   Base	
   (reading	
  
previously	
  human-­‐authored	
  tasks)	
  or	
  creates	
  tasks	
  in	
  real-­‐time.	
  
The	
   Step	
   Analyzer	
   compares	
   the	
   student’s	
   UI	
   behavior	
   to	
   the	
   correct	
   Steps	
   of	
   the	
   Task	
   and	
   provides	
   that	
  
information	
   to	
   the	
   Assessor	
   and	
   the	
   Pedagogical	
   Module.	
   The	
   Step	
   Analyzer	
   typically	
   determines	
  
whether	
  the	
  student’s	
  step	
  is	
  correct	
  or	
  incorrect,	
  and	
  which	
  of	
  the	
  task’s	
  steps	
  most	
  closely	
  matches	
  the	
  
student’s	
  step.	
  	
   The	
  Assessor	
  updates	
  the	
  Learner	
  Model.	
  The	
  Pedagogical	
  Module	
  provides	
  Help	
  using	
  
different	
   strategies	
   such	
   as	
   providing	
   immediate	
   or	
   delayed	
   help	
   or	
   providing	
   requested	
   or	
   unsolicited	
  
help.	
  
The	
   Learner	
   Model	
   represents	
   the	
   knowledge,	
   difficulties,	
   and	
   misconceptions	
   of	
   the	
   student.	
   The	
  
Learner	
   Model	
   lists	
   Tasks	
   assigned	
   to	
   the	
   student,	
   measures	
   of	
   the	
   time	
   spent	
   to	
   complete	
   the	
   Task,	
   and	
  
the	
  status	
  of	
  the	
  Task.	
  For	
  each	
  Step	
  in	
  the	
  Task	
  a	
  counter	
  of	
  the	
  Hints	
  requested	
  and	
  Feedback	
  (errors	
  
made)	
  is	
  kept.	
  	
  Most	
  importantly,	
  for	
  each	
  Knowledge	
  Component,	
  a	
  mastery	
  measure	
  is	
  also	
  kept.	
  	
  The	
  
Task	
  Selector	
  relies	
  on	
  the	
  Learner	
  Model’s	
  measures	
  of	
  Knowledge	
  Component	
  mastery	
  to	
  choose	
  a	
  
Task	
   that	
   is	
   neither	
   too	
   hard	
   (too	
   many	
   unmastered	
   knowledge	
   components)	
   nor	
   too	
   easy	
   (too	
   many	
  
mastered	
  knowledge	
  components).	
  

	
  
Figure	
  2	
  extends	
  the	
  ITS	
  structure	
  shown	
  in	
  Figure	
  1	
  in	
  order	
  to	
  identify	
  components	
  (functional	
  and	
  data)	
  and	
  
their	
  relationships.	
  UML	
  notation	
  is	
  used	
  inside	
  the	
  Core	
  block	
  to	
  create	
  a	
  first	
  attempt	
  of	
  the	
  object-­‐oriented	
  
model.	
  The	
  next	
  section	
  shows	
  the	
  details	
  of	
  this	
  model	
  using	
  a	
  pattern-­‐based	
  approach.	
  	
  
	
  
	
  

Fig.	
  2.	
  ITS	
  model.	
  White	
  boxes	
  represent	
  functional	
  components	
  and	
  gray	
  boxes	
  represent	
  data	
  components.	
  Relationships	
  of	
  association,	
  
dependency,	
  composition,	
  and	
  inheritance	
  are	
  shown	
  using	
  UML	
  notation.	
  

From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  6	
  

3.1 Pattern-­‐Based	
  Modeling	
  
The	
   model	
   in	
   Figure	
   2	
   represents	
   a	
   conceptual	
   description	
   of	
   who	
   is	
   doing	
   what	
   and	
   corresponds	
   to	
   an	
  
abstraction	
   of	
   the	
   expected	
   functionality	
   of	
   each	
   internal	
   component	
   in	
   Core.	
   Even	
   though	
   the	
   conceptual	
  
model	
   can	
   be	
   a	
   starting	
   point	
   to	
   implement	
   ITS	
   functionality,	
   it	
   is	
   still	
   too	
   abstract	
   to	
   be	
   a	
   software	
   design	
   and	
  
therefore	
  there	
  are	
  diverse	
  options	
  to	
  implement	
  it.	
  The	
  next	
  step	
  in	
  our	
  process	
  was	
  to	
  evolve	
  this	
  model	
  by	
  
defining	
   more	
   specific	
   relationships	
   between	
   components	
   using	
   design	
   patterns;	
   this	
   provided	
   us	
   with	
   a	
  
template	
   for	
   the	
   software	
   design	
   and	
   therefore	
   for	
   the	
   implementation.	
   Table	
   1	
   shows	
   the	
   relationship	
  
between	
  components	
  previously	
  defined	
  matched	
  with	
  a	
  pattern	
  name	
  with	
  a	
  description	
  of	
  the	
  meaning	
  of	
  the	
  
relationship.	
  
	
  
Table	
  1.	
  Relationships	
  between	
  ITS	
  components	
  and	
  design	
  patterns	
  
Components	
  
	
  
Tool	
  

Pattern
FACADE

TaskSelector	
  

STRATEGY

TaskSelector	
  
and	
  
Assessor	
  

OBSERVER

TaskFactory	
  

ABSTRACT FACTORY

TaskFactory	
  

STRATEGY

StepAnalyzer	
  

CHAIN OF
RESPONSIBILITIES

Assessor	
  

STRATEGY

Pedagogical	
  
Module	
  

STRATEGY

Step	
  

COMPOSITE

Assessor	
  
and	
  
StepAnalyzer	
  

OBSERVER

Pedagogical	
  
Module	
  
and	
  
StepAnalyzer	
  

OBSERVER

Description	
  
	
  
Tool	
  is	
  a	
  high-­‐level	
  interface	
  for	
  the	
  set	
  of	
  ITS	
  subsystems.	
  	
  
	
  
TaskSelector	
   is	
   implemented	
   using	
   STRATEGY	
   pattern	
   to	
   deal	
   with	
   the	
   fact	
   that	
  
selecting	
   the	
   next	
   Task	
   for	
   the	
   student	
   is	
   done	
   with	
   different	
   algorithms	
  
(methodologies)	
  described	
  in	
  (VanLehn	
  2006).	
  
	
  
The	
  relationship	
  between	
  TaskSelector	
  and	
  Assessor	
  can	
  be	
  described	
  by	
   OBSERVER	
  
pattern.	
   TaskSelector	
   needs	
   information	
   about	
   changes	
   in	
   the	
   LearnerModel	
  
(performance	
  of	
  the	
  student)	
  maintained	
  by	
  Assessor,	
  in	
  order	
  to	
  adjust	
  the	
  level	
  of	
  the	
  
next	
  Task.	
  
	
  
TaskFactory	
   creates	
   Task	
   objects.	
   The	
   relationship	
   between	
   TaskFactory	
   and	
   Task	
  
corresponds	
   to	
   the	
   relationship	
   between	
   a	
   factory	
   and	
   a	
   product	
   in	
   ABSTRACT	
  
FACTORY	
  pattern.	
  	
  
	
  
TaskFactory	
   implements	
   STRATEGY	
   to	
   create	
   Tasks,	
   due	
   to	
   the	
   fact	
   that	
   ITS	
   could	
  
implement	
   either	
   particular	
   algorithms	
   to	
   create	
   Tasks	
   in	
   real-­‐time	
   or	
   create	
   Tasks	
  
recovering	
  them	
  from	
  a	
  data	
  repository.	
  
	
  
CHAIN	
   OF RESPONSIBILITIES	
   is	
   a	
   design	
   pattern	
   that	
   avoids	
   coupling	
   the	
  
sender	
  of	
  a	
  request	
  to	
  its	
  receiver	
  by	
  giving	
  more	
  than	
  one	
  object	
  a	
  chance	
  to	
  handle	
  the	
  
request.	
   StepAnalyzer	
   chains	
   the	
   receiving	
   objects,	
   which	
   are	
   the	
   task’s	
   steps,	
   and	
  
passes	
  the	
  request	
  along	
  the	
  chain	
  until	
  one	
  object	
  handles	
  it.	
  Handling	
  a	
  request	
  means	
  
recognizing	
  a	
  student’s	
  UI	
  event	
  as	
  a	
  step	
  that	
  is	
  either	
  correct	
  or	
  incorrect.	
  	
  	
  
	
  
Assessor	
   implements	
   STRATEGY	
   to	
   maintain	
   the	
   LearnerModel.	
   Diverse	
   strategies	
  
could	
   be	
   tried	
   to	
   store	
   and	
   recover	
   the	
   LearnerModel	
   information.	
   	
   A	
   typical	
   strategy	
  
consists	
   of	
   associating	
   each	
   step	
   with	
   a	
   set	
   of	
   knowledge	
   components	
   that	
   represents	
  
what	
   the	
   student	
   would	
   need	
   to	
   know	
   in	
   order	
   to	
   get	
   that	
   step	
   correct.	
   	
   When	
   the	
  
Assessor	
  is	
  informed	
  that	
  a	
  particular	
  Step	
  is	
  correct,	
  it	
  increments	
  the	
  mastery	
  of	
  the	
  
associated	
   knowledge	
   components.	
   On	
   the	
   other	
   hand,	
   if	
   the	
   student	
   got	
   the	
   Step	
  
wrong,	
  then	
  Assessor	
  reduces	
  the	
  mastery	
  of	
  the	
  associated	
  knowledge	
  components.	
  	
  
	
  
PedagogicalModule	
   implements	
   STRATEGY	
   to	
   provide	
   support	
   to	
   the	
   student	
   in	
  
solving	
  the	
  current	
  Step.	
  Options	
  to	
  provide	
  Help	
  go	
  from	
  pressing	
  a	
  button	
  asking	
  for	
  a	
  
Hint	
   to	
   the	
   implementation	
   of	
   intelligent	
   algorithms	
   that	
   provide	
   support	
   to	
   maintain	
  
the	
   student	
   in	
   the	
   "zone	
   of	
   proximal	
   development"	
   (Vygotsky	
   1978),	
   where	
   tasks	
   are	
  
neither	
  boringly	
  easy	
  nor	
  frustratingly	
  difficult,	
  but	
  instead	
  these	
  tasks	
  afford	
  maximal	
  
learning	
  and	
  motivating	
  challenges.	
  
	
  
COMPOSITE	
   pattern	
   allows	
   us	
   to	
   compose	
   Steps	
   into	
   tree	
   structures	
   to	
   represent	
  
part-­‐whole	
   hierarchies.	
   COMPOSITE	
   pattern	
   lets	
   us	
   treat	
   individual	
   Steps	
   and	
  
hierarchies	
  of	
  Steps	
  (and	
  sub-­‐Steps)	
  uniformly.	
  
	
  
The	
   relationship	
   between	
   Assessor	
   and	
   StepAnalyzer	
   is	
   described	
   by	
   OBSERVER	
  
pattern.	
   Assessor	
   needs	
   information	
   about	
   student	
   performance	
   in	
   each	
   Step.	
   That	
  
information	
  is	
  obtained	
  from	
  StepAnalyzer.	
  
	
  
The	
   relationship	
   between	
   PedagogicalModule	
   and	
   StepAnalyzer	
   can	
   be	
   described	
   by	
  
OBSERVER	
   pattern.	
   PedagogicalModule	
   needs	
   information	
   about	
   student	
  
performance	
  in	
  each	
  Step.	
  That	
  information	
  is	
  obtained	
  from	
  StepAnalyzer.	
  

From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  7	
  

Finding	
   the	
   appropriate	
   pattern	
   to	
   be	
   applied	
   to	
   each	
   component	
   and	
   relationship	
   was	
   a	
   process	
   based	
   on	
  
experience	
   and	
   literature	
   research	
   (Buschmann	
   et	
   al.	
   1996,	
   Gamma	
   et	
   al.	
   1995).	
   	
   There	
   is	
   no	
   set	
   of	
   rules	
   on	
  
how	
  to	
  choose	
  a	
  pattern;	
  instead,	
  a	
  firm	
  knowledge	
  of	
  existing	
  patterns	
  as	
  well	
  as	
  the	
  problems	
  they	
  solve	
  is	
  
required	
   in	
   order	
   to	
   effectively	
   use	
   patterns	
   to	
   describe	
   what	
   happens	
   within	
   a	
   given	
   system.	
   Our	
   approach	
  
consists	
  of	
  using	
  the	
  pattern	
  that	
  most	
  closely	
  matches	
  the	
  semantic	
  description	
  of	
  the	
  requirement	
  or	
  group	
  of	
  
requirements.	
   From	
   the	
   “GoF”	
   design	
   patterns	
   documented	
   in	
   (Buschmann	
   et	
   al.	
   1996)	
   and	
   (Gamma	
   et	
   al.	
  
1995),	
   we	
   took	
   the	
   keywords	
   observer,	
   abstract	
   factory,	
   builder,	
   chain	
   of	
   responsibilities,	
   strategy,	
  
communicator,	
  facade,	
  composite,	
  and	
  singleton;	
  each	
  pattern	
  is	
  fairly	
  close	
  to	
  implementing	
  the	
  task	
  that	
  its	
  
name	
   means	
   and	
   what	
   each	
   component	
   is	
   supposed	
   to	
   do;	
   for	
   example,	
   TaskFactory	
   is	
   an	
   ABSTRACT
FACTORY	
  of	
  Tasks.	
  	
  
	
  
3.2 Putting	
  All	
  The	
  Patterns	
  Together	
  	
  
With	
   the	
   relationships	
   expressed	
   as	
   pattern	
   equivalences,	
   as	
   listed	
   in	
   the	
   previous	
   section,	
   creating	
   a	
   software	
  
design	
  is	
  fairly	
  straightforward.	
  Each	
  pattern	
  has	
  a	
  unique	
  equivalence	
  in	
  UML	
  (as	
  a	
  class	
  diagram).	
  Then,	
  our	
  
development	
  team	
  would	
  be	
  able	
  to	
  focus	
  on	
  the	
  detailed	
  implementation	
  of	
  the	
  desired	
  functionality,	
  filling	
  in	
  
specific	
  places	
  inside	
  of	
  specific	
  files,	
  methods,	
  and	
  attributes	
  (Booch	
  et	
  al.	
  2007).	
  The	
  UML	
  class	
  diagram	
  for	
  
the	
  Core	
  layer	
  is	
  shown	
  in	
  Figure	
  3.	
  
	
  

	
  
Fig.	
  3.	
  UML	
  class	
  diagram	
  showing	
  the	
  pattern-­‐based	
  model	
  for	
  the	
  “Core”	
  layer.	
  

It	
  is	
  important	
  to	
  note	
  the	
  following	
  relationships	
  in	
  the	
  diagram	
  shown	
  in	
  Figure	
  3:	
  
	
  
• The	
   implementation	
   of	
   Step	
   as	
   a	
   COMPOSITE	
   is	
   highly	
   useful;	
   it	
   provides	
   the	
   capability	
   of	
   managing	
   Steps	
  
as	
  one	
  or	
  as	
  a	
  hierarchy	
  of	
  several	
  hierarchical	
  Steps.	
  
From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  8	
  

•

•

TaskSelector,	
   TaskFactory,	
   Assessor,	
   and	
   PedagogicalModule	
   implement	
   the	
   STRATEGY	
   pattern	
   that	
  
permit	
   us	
   to	
   define	
   algorithms,	
   encapsulate	
   them,	
   and	
   make	
   them	
   interchangeable.	
   	
   STRATEGY	
   pattern	
  
lets	
  the	
  algorithm	
  vary	
  independently	
  of	
  the	
  classes	
  that	
  use	
  it.	
  Implementing	
  a	
  new	
  way	
  to	
  select	
  a	
  Task,	
  
create	
  a	
  Task,	
  manage	
  the	
  LearnerModel,	
  or	
  provide	
  Help	
  to	
  the	
  student	
  can	
  be	
  done	
  by	
  one	
  developer	
  who	
  
needs	
   no	
   knowledge	
   about	
   the	
   project	
   at	
   all;	
   the	
   developer	
   just	
   needs	
   to	
   follow	
   the	
   pattern	
   to:	
   (1)	
   create	
   a	
  
new	
  class	
  that	
  implements	
  the	
  corresponding	
  interface;	
  (2)	
  implement	
  at	
  least	
  the	
  algorithm	
  method;	
  and	
  
(3)	
  create	
  as	
  many	
  additional	
  methods	
  and/or	
  attributes	
  as	
  needed.	
  
TaskSelector	
  obtains	
  information	
  from	
  Assessor,	
  which	
  as	
  well	
  as	
  PedagogicalModule	
  obtains	
  information	
  
from	
   StepAnalyzer.	
   The	
   concept	
   of	
   “observing”	
   describes	
   the	
   relationship	
   and	
   clearly	
   identifies	
   how	
   the	
  
structure	
   of	
   communication	
   must	
   be	
   implemented	
   (methods	
   and	
   attributes).	
   	
   It	
   is	
   easy	
   to	
   notice	
   which	
  
component	
  needs	
  information	
  from	
  which	
  other	
  component.	
  

	
  
3.3.	
  Implementation	
  
Including	
   structure	
   and	
   functionality	
   the	
   current	
   system,	
   developed	
   in	
   Java,	
   is	
   formed	
   by:	
   10	
   packages;	
   62	
  
classes;	
   746	
   methods;	
   738	
   attributes;	
   22,434	
   lines	
   of	
   code;	
   1,150	
   revisions	
   maintained	
   in	
   a	
   revision	
   control	
  
system	
  (SVN)	
  created	
  between	
  July	
  2009	
  and	
  July	
  2011	
  with	
  a	
  shifting	
  development	
  team	
  of	
  nine	
  programmers	
  
(maintaining	
   a	
   team	
   of	
   two	
   programmers	
   at	
   a	
   time,	
   with	
   an	
   average	
   of	
   six	
   months	
   of	
   permanency)	
   and	
   two	
  
resident	
  software	
  engineers;	
  8	
  versions	
  released	
  to	
  clients;	
  and	
  140	
  users	
  working	
  with	
  the	
  system,	
  who	
  have	
  
been	
   high	
   school	
   students	
   and	
   undergraduate	
   students	
   participating	
   in	
   four	
   summer-­‐camp	
   courses	
   and	
   two	
  
university	
  courses	
  at	
  Arizona	
  State	
  University.	
  
	
  
The	
  implementation	
  of	
  the	
  Strategy	
  classes	
  included:	
  
	
  
• StrategyTaskSelector	
  interface	
  implemented	
  in	
  SequencialTaskSelection	
  class.	
  	
  SequencialTaskSelection	
  
class	
  defines	
  a	
  strategy	
  that	
  presents	
  Tasks	
  to	
  the	
  student	
  in	
  a	
  predefined	
  sequential	
  order.	
  
• StrategyTaskFactory	
  interface	
  implemented	
  in	
  TaskFromRepository	
  class.	
  TaskFromRepository	
  class	
  
defines	
  a	
  strategy	
  that	
  recovers	
  Tasks	
  from	
  text	
  files.	
  
• No	
  strategy	
  implemented	
  for	
  Assessor,	
  this	
  component	
  is	
  still	
  an	
  ongoing	
  part	
  of	
  the	
  project.	
  
• StrategyPedagogicalModule	
  interface	
  implemented	
  in	
  ConditionalPedagogicalStrategy	
  class.	
  
ConditionalPedagogicalStrategy	
  class	
  defines	
  a	
  strategy	
  in	
  which,	
  conditionally,	
  the	
  presence	
  of	
  certain	
  
events	
  or	
  actions	
  from	
  the	
  student	
  launches	
  pre-­‐established	
  responses.	
  	
  
	
  
These	
  classes,	
  which	
  implement	
  Strategy	
  interfaces,	
  are	
  not	
  shown	
  in	
  Figure	
  3	
  due	
  to	
  space	
  limitations.	
  
	
  
Regarding	
   the	
   Tool	
   component,	
   Tool	
   is	
   a	
   facade	
   for	
   an	
   environment	
   in	
   which	
   the	
   student	
   is	
   able	
   to	
   learn	
  
about	
  systems	
  dynamic	
  modeling,	
  using	
  a	
  graphical	
  representation.	
  Each	
  model	
  is	
  a	
  directed	
  graph	
  formed	
  by	
  
nodes	
   and	
   edges.	
   The	
   edges	
   indicate	
   flow	
   of	
   numeric	
   information	
   between	
   nodes	
   and	
   the	
   nodes	
   represent	
  
variables.	
  A	
  node	
  encapsulates	
  a	
  variable’s	
  value	
  as	
  an	
  algebraic	
  combination	
  of	
  the	
  numbers	
  coming	
  into	
  or	
  
going	
   out	
   of	
   it	
   via	
   edges.	
   Students	
   read	
   text	
   describing	
   the	
   problem,	
   and	
   then	
   define	
   nodes	
   and	
   edges,	
   enter	
  
values	
  or	
  equations	
  in	
  each	
  node,	
  run	
  the	
  model	
  and	
  compare	
  its	
  predictions	
  to	
  given	
  facts.	
  If	
  any	
  of	
  the	
  model’s	
  
predictions	
  is	
  false	
  (represented	
  with	
  red	
  colors	
  as	
  feedback),	
  students	
  must	
  debug	
  the	
  model.	
  Students	
  also	
  
can	
  ask	
  for	
  feedback	
  by	
  checking	
  their	
  model	
  at	
  each	
  step	
  before	
  running	
  the	
  model	
  (VanLehn	
  2011).	
  Figure	
  4	
  
shows	
  the	
  implementation	
  of	
  the	
  Tool	
  component.	
  Tool	
  component	
  consists	
  on	
  a	
  Canvas	
  in	
  which	
  a	
  Graph	
  is	
  
drawn.	
   A	
   Graph	
   is	
   composed	
   by	
   nodes	
   (Vertexes)	
   and	
   links	
   (Edges)	
   that	
   connect	
   the	
   nodes.	
   Each	
   Vertex	
  
maintains	
  a	
  register	
  of	
  all	
  vertexes	
  going	
  out	
  and	
  in.	
  Each	
  Edge	
  maintains	
  data	
  of	
  the	
  Vertex	
  in	
  which	
  it	
  starts	
  
and	
   ends.	
   Vertexes,	
   Edges,	
   and	
   Graph	
   can	
   be	
   selected	
   from	
   the	
   Canvas	
   and	
   be	
   manipulated	
   (drag	
   and	
   drop,	
  
deleted,	
  and	
  so	
  on).	
  	
  
	
  

From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  9	
  

Fig.	
  4.	
  UML	
  class	
  diagram	
  showing	
  the	
  Tool	
  component	
  encapsulated	
  in	
  the	
  model	
  as	
  Tool.	
  
	
  

4. EXPERIENCE	
  REPORT	
  AND	
  EVALUATION	
  
In	
   this	
   section	
   we	
   will	
   describe	
   our	
   experience	
   using	
   design	
   patterns	
   to	
   create	
   an	
   ITS	
   model	
   and	
   implementing	
  
it	
  to	
  create	
  the	
  AMT	
  software	
  project.	
  	
  
	
  
Stakeholders	
  mentioned	
  the	
  following	
  favorable	
  qualities:	
  	
  
	
  
• Incremental	
   development.	
   Building	
   the	
   AMT	
   project	
   in	
   an	
   incremental	
   way	
   over	
   the	
   course	
   of	
   three	
  
years	
  allowed	
  us	
  to	
  partition	
  the	
  implementation	
  of	
  new	
  functionality	
  into	
  discrete	
  tasks	
  that	
  were	
  worked	
  
on	
   independently	
   from	
   others,	
   therefore,	
   reducing	
   the	
   time	
   of	
   deployment	
   adding	
   programmers	
   in	
  
particular	
  moments	
  of	
  the	
  project.	
  Incremental	
  development	
  allows	
  us	
  to	
  provide	
  a	
  new	
  functionality	
  or	
  a	
  
new	
  version	
  of	
  a	
  current	
  functionality	
  in	
  a	
  window	
  time	
  of	
  two	
  or	
  three	
  weeks.	
  
• Shifting	
  development	
  team.	
  Programmers,	
  even	
  without	
  knowledge	
  of	
  patterns,	
  were	
  able	
  to	
  focus	
  their	
  
attention	
   on	
   the	
   requirements	
   assigned	
   to	
   them;	
   each	
   programmer	
   worked	
   on	
   completing	
   a	
   specific	
  
module	
  or	
  set	
  of	
  components	
  (defined	
  as	
  a	
  pattern	
  section)	
  and	
  relationships	
  between	
  components	
  were	
  
almost	
   entirely	
   defined	
   by	
   patterns.	
   We	
   used	
   Subversion	
   (Collins-­‐Sussman	
   2004)	
   to	
   maintain	
   a	
   common	
  
repository	
  of	
  the	
  project	
  as	
  a	
  tool	
  to	
  support	
  branching	
  and	
  merging	
  processes.	
  
• Communication.	
   Since	
   diverse	
   stakeholders	
   such	
   as	
   researchers	
   in	
   education	
   technology,	
   computer	
  
scientists,	
  developers,	
  and	
  instructional	
  designers	
  were	
  involved,	
  design	
  patterns	
  helped	
  us	
  agree	
  on	
  the	
  
structure	
   of	
   the	
   system	
   and	
   communicate	
   it	
   to	
   the	
   programmers	
   for	
   each	
   individual	
   component	
   in	
   the	
  
project.	
  The	
  use	
  of	
  pattern	
  names	
  such	
  as	
  FACTORY	
  and	
  STRATEGY	
  has	
  been	
  adopted	
  as	
  an	
  abstract	
  way	
  
to	
   refer	
   functionalities	
   between	
   stakeholders.	
   The	
   names	
   hide	
   complexity	
   from	
   non-­‐developers.	
   Non-­‐
developers	
  assume	
  an	
  easy	
  thing	
  must	
  be	
  done,	
  and	
  programmers	
  have	
  a	
  better	
  idea	
  about	
  the	
  boundaries	
  
of	
  changes,	
  bugs,	
  and	
  new	
  requirements.	
  
	
  
However,	
  some	
  disputes	
  emerged	
  with	
  stakeholders	
  regarding	
  the	
  following:	
  
	
  
• Size.	
   Stakeholders	
   point	
   to	
   the	
   increase	
   in	
   size	
   of	
   code	
   while	
   using	
   patterns	
   as	
   an	
   issue.	
   While	
   using	
  
patterns	
   generates	
   more	
   code	
   in	
   our	
   project,	
   this	
   is	
   not	
   only	
   due	
   to	
   patterns	
   (interfaces	
   and	
   abstract	
  
classes	
  declarations),	
  but	
  also	
  because	
  we	
  decided	
  to	
  maintain	
  the	
  cyclomatic	
  complexity	
  (McCabe	
  1976)	
  

From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  10	
  

•

for	
   every	
   method	
   under	
   10,	
   which	
   means	
   applying	
   a	
   “divide	
   and	
   conquer”	
   strategy	
   that	
   generates	
   more	
  
methods	
  in	
  the	
  system.	
  
Deployment	
   Time.	
   Since	
   in	
   each	
   iteration	
   our	
   first	
   step	
   focused	
   on	
   the	
   design	
   definition	
   (patterns),	
  
software	
  prototypes	
  delayed	
  its	
  appearance.	
  But	
  once	
  the	
  first	
  prototype	
  was	
  presented,	
  new	
  prototypes	
  
emerged	
  quicker	
  that	
  in	
  previous	
  projects.	
  	
  

	
  
Finally,	
  developers	
  mentioned	
  the	
  following	
  favorable	
  qualities:	
  
	
  
• Collaboration.	
  Sharing	
  constructions	
  between	
  developers	
  was	
  a	
  key	
  element	
  to	
  counterbalance	
  the	
  effect	
  
of	
  a	
  constant	
  shift	
  in	
  the	
  people	
  in	
  the	
  development	
  team.	
  	
  
• Productivity.	
  Several	
  prototype	
  versions	
  of	
  the	
  project	
  were	
  created	
  to	
  test	
  new	
  options	
  of	
  functionality	
  as	
  
well	
  as	
  new	
  pedagogical	
  approaches.	
  	
  
• Abstraction.	
   Providing	
   a	
   “controlled”	
   freedom	
   to	
   the	
   programmers	
   using	
   patterns	
   as	
   the	
   guidelines	
   of	
   a	
  
defined	
  design	
  was	
  highly	
  relevant	
  to	
  handle	
  changing	
  and	
  incremental	
  requirements.	
  
	
  
5. CONCLUSIONS	
  AND	
  ONGOING	
  WORK	
  
Many	
   authors	
   claim	
   that	
   their	
   ITS	
   follow	
   a	
   software	
   architecture	
   because	
   they	
   can	
   identify	
   components	
   and	
  
relationships	
  among	
  these	
  components	
  inside	
  their	
  systems.	
  However,	
  this	
  does	
  not	
  mean	
  that	
  standard	
  and	
  
good	
  practices,	
  such	
  as	
  design	
  patterns,	
  have	
  been	
  followed.	
  We	
  took	
  advantage	
  of	
  the	
  growing	
  experience	
  in	
  
the	
  field	
  of	
  software	
  design	
  patterns	
  to	
  both	
  design	
  and	
  implement	
  an	
  ITS	
  model	
  in	
  a	
  pattern-­‐based	
  approach.	
  
Applying	
   design	
   patterns	
   was	
   useful	
   to	
   create	
   a	
   high-­‐quality	
   software	
   solution	
   that	
   is	
   easy	
   to	
   maintain	
   and	
  
extend.	
   Designing	
   with	
   quality	
   attributes	
   as	
   drivers	
   has	
   resulted	
   in	
   a	
   design	
   that	
   has	
   proven	
   to	
   be	
   more	
  
reusable,	
   extensible,	
   and	
   adaptable.	
   Using	
   design	
   patterns	
   improved	
   our	
   communication,	
   collaboration,	
   and	
  
productivity.	
  Design	
  patterns	
  facilitate	
  the	
  knowledge	
  transfer	
  across	
  a	
  highly	
  shifting	
  development	
  team	
  and	
  
thus	
   the	
   development	
   of	
   the	
   system,	
   where	
   the	
   creation	
   of	
   new	
   versions	
   or	
   variants	
   of	
   the	
   software	
   was	
  
relatively	
   easy	
   in	
   terms	
   of	
   time	
   and	
   effort.	
   Adding	
   design	
   patterns	
   in	
   the	
   development	
   of	
   ITS	
   allowed	
   us	
   to	
  
create	
  a	
  common	
  vocabulary	
  among	
  stakeholders,	
  making	
  the	
  process	
  more	
  accurate	
  and	
  effective	
  design-­‐wise.	
  
We	
  applied	
  our	
  model	
  to	
  build	
  several	
  variants	
  of	
  the	
  AMT	
  system	
  in	
  three	
  years	
  of	
  work,	
  with	
  a	
  high	
  rate	
  of	
  
changes	
  in	
  requirements	
  for	
  the	
  product	
  and	
  a	
  high	
  shifting	
  development	
  team.	
  In	
  other	
  words,	
  we	
  have	
  been	
  
able	
  to	
  create	
  a	
  family	
  of	
  AMTs	
  around	
  the	
  same	
  design.	
  	
  
Future	
  research	
  will	
  focus	
  on	
  two	
  additions:	
  the	
  first	
  one	
  will	
  be	
  the	
  inclusion	
  of	
  a	
  module	
  for	
  companions	
  to	
  
provide	
   support	
   for	
   the	
   student	
   such	
   as	
   learning	
   companions,	
   affective	
   companions,	
   and	
   teachable	
   agents,	
   and	
  
the	
  second	
  one	
  will	
  be	
  the	
  inclusion	
  of	
  meta-­‐tutoring	
  components.	
  
	
  
REFERENCES	
  
AMT	
  -­‐	
  Affective	
  Meta	
  Tutor.	
  2012.	
  Arizona	
  State	
  University.	
  http://amt.asu.edu.	
  
	
  
Anderson,	
  J.	
  R.,	
  Corbett,	
  A.	
  T.,	
  Koedinger,	
  K.	
  R.,	
  and	
  Pelletier,	
  R.	
  1995.	
  Cognitive	
  Tutors:	
  Lessons	
  Learned.	
  Journal	
  of	
  the	
  Learning	
  Sciences,	
  
4(2),	
  167-­‐207.	
  
	
  
Baker,	
   R.	
   S.	
   J.	
   D.,	
   de	
   Carvalho,	
   A.,	
   Raspat,	
   J.,	
   Aleven,	
   V.,	
   Corbett,	
   A.	
   T.,	
   and	
   Koedinger,	
   K.	
   R.	
   2009.	
   	
   Educational	
   software	
   features	
   that	
  
encourages	
  and	
  discourage	
  “gaming	
  the	
  system”.	
  In	
  Proceedings	
  of	
  the	
  International	
  Conference	
  on	
  Artificial	
  Intelligence	
  in	
  Education.	
  IOS	
  
Press.	
  
	
  
Booch,	
  G.,	
  Maksimchuk,	
  R.,	
  Engle,	
  M.,	
  Young,	
  B.,	
  Conallen,	
  J.,	
  and	
  Houston,	
  K.	
  2007.	
  Object-­‐Oriented	
  Analysis	
  and	
  Design	
  with	
  Applications,	
  
Third	
  Edition.	
  Addison-­‐Wesley	
  Professional.	
  
	
  
Buschmann,	
   F.,	
   Meunier,	
   R.,	
   Rohnert,	
   H.,	
   Sommerlad,	
   P.,	
   and	
   Stal,	
   M.	
   1996.	
   A	
   system	
   of	
   patterns:	
   Pattern-­‐oriented	
   software	
   architecture.	
  
Wiley.	
  
	
  
Collins-­‐Sussman,	
  B.,	
  Fitzpatrick,	
  B.	
  W.,	
  and	
  Pilato,	
  C.	
  M.	
  2004.	
  Version	
  control	
  with	
  subversion.	
  O’Reilly	
  Media,	
  Inc.	
  	
  
	
  
Devedzic,	
  V.	
  and	
  Harrer,	
  A.	
  2005.	
  Software	
  Patterns	
  in	
  ITS	
  Architectures.	
  International	
  Journal	
  of	
  Artificial	
  Intelligence	
  in	
  Education,	
  15,	
  2	
  
(April	
  2005),	
  63-­‐94.	
  
	
  
From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  11	
  

Gamma,	
   E.,	
   Helm,	
   R.,	
   Johnson,	
   R.,	
   and	
   Vlissides,	
   J.	
   1995.	
   Design	
   Patterns:	
   Elements	
   of	
   Reusable	
   Object-­‐Oriented	
   Software.	
   Addison-­‐Wesley	
  
Longman	
  Publishing	
  Co.,	
  Inc.,	
  Boston,	
  MA,	
  USA.	
  
	
  
Gamma,	
   E.,	
   Helm,	
   R.,	
   Johnson,	
   R.,	
   and	
   Vlissides,	
   J.	
   2002.	
   Design	
   Patterns:	
   abstraction	
   and	
   reuse	
   of	
   object-­‐oriented	
   design.	
   In	
   Software	
  
pioneers.	
  Manfred	
  Broy	
  and	
  Ernst	
  Denert	
  (Eds.).	
  Springer-­‐Verlag	
  New	
  York,	
  Inc.,	
  New	
  York,	
  NY,	
  USA	
  701-­‐717.	
  
	
  
Graesser,	
  A.	
  C.,	
  Lu,	
  S.,	
  Jackson,	
  G.	
  T.,	
  Mitchell,	
  H.	
  H.,	
  Ventura,	
  and	
  M.,	
  Olney,	
  A.,	
  Louwerse,	
  M.M.	
  2004.	
  AutoTutor:	
  A	
  tutor	
  with	
  dialogue	
  in	
  
natural	
  language.	
  Behavioral	
  Research	
  Methods,	
  Instruments	
  and	
  Computers,	
  36,	
  180-­‐193.	
  
	
  
IEEE.	
  1999.	
  Standard	
  Glossary	
  of	
  Software	
  Engineering	
  Terminology.	
  610.12-­‐1990,	
  Vol.1.	
  IEEE	
  Press.	
  
	
  
Jacobson,	
  I.	
  1997.	
  Software	
  Reuse:	
  Architecture,	
  Process	
  and	
  Organization	
  for	
  Business	
  Success.	
  Addison-­‐Wesley	
  Professional.	
  	
  
	
  
Katz,	
  S.,	
  Lesgold,	
  A.,	
  Hughes,	
  E.,	
  Peters,	
  D.,	
  Eggan,	
  G.,	
  Gordin,	
  M.,	
   and	
  Greenberg.,	
  L.	
  1998.	
  Sherlock	
  2:	
  An	
  intelligent	
  tutoring	
  system	
  built	
  
upon	
   the	
   LRDC	
   Tutor	
   Framework.	
   In	
   C.	
   P.	
   Bloom	
   &	
   R.	
   B.	
   Loftin	
   (Eds.),	
   Facilitating	
   the	
   development	
   and	
   use	
   of	
   interactive	
   learning	
  
environments.	
  227-­‐	
  258.	
  
	
  
McCabe,	
  T.	
  1976.	
  A	
  complexity	
  measure.	
  IEEE	
  Trans.	
  Software	
  Engineering,	
  5,	
  45–50.	
  
	
  
Mitrovic,	
  A.	
  2003.	
  An	
  intelligent	
  SQL	
  tutor	
  on	
  the	
  web.	
  International	
  Journal	
  of	
  Artificial	
  Intelligence	
  in	
  Education,	
  13(2-­‐4),	
  197-­‐243.	
  
	
  
Nelson,	
   B.	
   C.	
   2007.	
   	
   Exploring	
   the	
   use	
   of	
   individualized,	
   reflective	
   guidance	
   in	
   an	
   educational	
   multi-­‐user	
   virtual	
   environment.	
   Journal	
   of	
  
Science	
  Education	
  and	
  Technology,	
  16(1),	
  83-­‐97.	
  
	
  
VanLehn,	
   K.	
   2006.	
   The	
   Behavior	
   of	
   Tutoring	
   Systems.	
  International	
   Journal	
   of	
   Artificial	
   Intelligence	
   in	
   Education.	
   Volume	
   16,	
  Issue	
   3,	
   Pages	
  
227-­‐265.	
  IOS	
  Press.	
  
	
  
VanLehn,	
   K.,	
   Lynch,	
   C.,	
   Schultz,	
   K.,	
   Shapiro,	
   J.	
   A.,	
   Shelby,	
   R.	
   H.,	
   Taylor,	
   L.,	
   	
   Treacy,	
   D.,	
   Weinstein,	
   A.,	
   and	
   Wintersgill,	
   M..	
   2005.	
   The	
   Andes	
  
physics	
  tutoring	
  system:	
  Lessons	
  learned.	
  International	
  Journal	
  of	
  Artificial	
  Intelligence	
  in	
  Education,	
  15(3),	
  147-­‐204.	
  	
  
	
  
VanLehn,	
  K.,	
  Burleson,	
  W.,	
  Chavez-­‐Echeagaray,	
   M.E.,	
   Christopherson,	
   R.,	
   Gonzalez-­‐Sanchez,	
  J.,	
  Hastings,	
  J.,	
  Hidalgo-­‐Pontet,	
  Y.,	
  and	
  Zhang,	
  L..	
  
2011.	
  The	
  Affective	
  Meta-­‐Tutoring	
  Project:	
  How	
  to	
  motivate	
  students	
  to	
  use	
  effective	
  meta-­‐cognitive	
  strategies.	
  T.	
  Hirashima	
  et	
  al.	
  (Eds.).	
  
In	
   Proceedings	
   of	
   the	
   19th	
   International	
   Conference	
   on	
   Computers	
   in	
   Education.	
   Chiang	
   Mai,	
   Thailand:	
   Asia-­‐Pacific	
   Society	
   for	
   Computers	
   in	
  
Education.	
  
	
  
Vygotsky,	
  L.	
  S.	
  1978.	
  Mind	
  in	
  Society:	
  The	
  Development	
  of	
  Higher	
  Psychological	
  Processes.	
  Cambridge,	
  MA:	
  Harvard	
  University	
  Press.	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
PLoP'11,	
  October	
  21-­‐23,	
  Portland,	
  Oregon,	
  USA.	
  Copyright	
  2011	
  is	
  held	
  by	
  the	
  author(s).	
  ACM	
  978-­‐1-­‐4503-­‐1283-­‐7	
  
From	
  Behavioral	
  Description	
  to	
  A	
  Pattern-­‐Based	
  Model	
  for	
  Intelligent	
  Tutoring	
  Systems:	
  Page	
  -­‐	
  12	
  

Spoken Versus Typed Human and Computer
Dialogue Tutoring
Diane J. Litman1 , Carolyn P. Rosé2 , Kate Forbes-Riley1 , Kurt VanLehn1 ,
Dumisizwe Bhembe1 , and Scott Silliman1
1

2

Learning Research and Development Center, University of Pittsburgh,
3939 O’Hara St., Pittsburgh, PA 15260
{litman,vanlehn}@cs.pitt.edu
Language Technologies Institute/Human-Computer Interaction Institute,
Carnegie Mellon University, Pittsburgh, PA 15260
{rosecp,forbesk,bhembe,scotts}@pitt.edu

Abstract. While human tutors typically interact with students using
spoken dialogue, most computer dialogue tutors are text-based. We have
conducted 2 experiments comparing typed and spoken tutoring dialogues,
one in a human-human scenario, and another in a human-computer scenario. In both experiments, we compared spoken versus typed tutoring
for learning gains and time on task, and also measured the correlations of
learning gains with dialogue features. Our main results are that changing
the modality from text to speech caused large diﬀerences in the learning
gains, time and superﬁcial dialogue characteristics of human tutoring,
but for computer tutoring it made less diﬀerence.

1

Introduction

It is widely believed that the best human tutors are more eﬀective than the best
computer tutors, in part because [1] found that human tutors could produce a
larger diﬀerence in the learning gains than current computer tutors (e.g., [2,3,4]).
A major diﬀerence between human and computer tutors is that human tutors use
face-to face spoken natural language dialogue, whereas computer tutors typically
use menu-based interactions or typed natural language dialogue. This raises the
question of whether making the interaction more natural, such as by changing
the modality of the tutoring to spoken natural language dialogue, would decrease
the advantage of human tutoring over computer tutoring.
Three main beneﬁts of spoken tutorial dialogue with respect to increasing
learning have been hypothesized. One is that spoken dialogue may elicit more
student engagement and knowledge construction. [5] found that students who
were prompted for self-explanations produced more when the self-explanations
were spoken rather than typed. Self-explanation is just one form of student
cognitive activity that is known to cause learning gains [6,7,8]. If it can be
increased by using speech, perhaps other beneﬁcial thinking can also be elicited
as well.
A second hypothesis is that speech allows tutors to infer a more accurate
student model, including long-term factors such as overall competence and motivation, and short-term factors such as whether the student really understood
J.C. Lester et al. (Eds.): ITS 2004, LNCS 3220, pp. 368–379, 2004.
c Springer-Verlag Berlin Heidelberg 2004


Spoken Versus Typed Human and Computer Dialogue Tutoring

369

the tutor’s utterance. Having a more accurate understanding of the students
should allow the tutor to adapt the instruction to the student so as to accelerate the student’s learning. In other work we have shown that the prosodic and
acoustic information of speech can improve the detection of speaker states such
as confusion [9], which may be useful for adapting tutoring to the student.
A third hypothesis is that learning will be enhanced in computational environments that prime a more social interpretation of the teaching situation, as
when an animated agent talks, and responds contingently (as in dialogue) to a
learner. While [10] found that the use of a dialogue agent improved learning,
there was no evidence that output media impacted learning. In [11], an interactive pedagogical agent using speech rather than text output improved student
learning, while the visual presence or absence of the agent did not impact performance.
It is thus important to test whether a move to spoken dialogues is likely to
cause higher learning gains, and if so, to understand why it accelerates learning.
It is particularly important given that natural language tutoring systems are
becoming more common. Although a few use spoken dialogues [12], most still use
typed dialogues (e.g. [13,14,15]), although as shown by our work it is technically
feasible to convert a tutor from typed dialogue tutor to spoken dialogue. While
the details of this conversion are not covered in this paper, it took about 9 personmonths of eﬀort. Thus, many developers may be wondering whether they should
aim for a spoken or a typed dialogue tutoring system.
It is also important to study the diﬀerence between spoken and typed dialogue in two contexts: human tutoring and computer tutoring. Given the current
limitations of both speech and natural language processing technologies, computer tutors are far less ﬂexible than human tutors, and also make more errors.
The use of human tutors provides a benchmark for estimating the performance
of an “ideal” computer system with respect to speech and natural language
processing performance. We thus conducted two experiments. Both used qualitative physics as the task domain, similar pretests and posttests, and similar
training sequences. However, one experiment used an experienced human tutor
who communicated with students either via speech or typing. The other used
the Why2-Atlas tutoring system [16] with either its original typed dialogue or a
new spoken dialogue user interface. The new system is called ITSPOKE [9].

2

The Common Aspects of the Experiments

In both experiments, the students learned how to solve qualitative physics problems, which are physics problems that can be answered without doing any mathematics. A typical problem is, “If a massive truck and a lightweight car have a
head-on collision, and both were going the same speed initially, which one suffers the greater impact force and the greater change in motion? Explain your
answer.” The answer to such a problem is a short essay.
The experimental procedure was as follows. Students who have not taken any
college physics were ﬁrst given a pretest measuring their knowledge of physics.

370

D.J. Litman et al.

Next, students read a short textbook-like pamphlet, which described the major
laws (eg., Newton’s ﬁrst law) and the major concepts. Students then worked
through a set of up to 10 training problems with the tutor. Finally, students
were given a posttest that was isomorphic to the pretest; both consisted of 40
multiple choice questions. The entire experiment took no more than 9 hours per
student, and was usually performed in 1-3 sessions. Subjects were University
students responding to ads, and were compensated with money or course credit.
The interface used for all experiments was basically the same. The student
ﬁrst typed an essay answering a qualitative physics problem. The tutor then
engaged the student in a natural language dialogue to provide feedback, correct
misconceptions, and to elicit more complete explanations. At key points in the
dialogue, the tutor asked the student to revise the essay. This cycle of instruction
and revision continued until the tutor was satisﬁed with the student’s essay, at
which point the tutor presented the ideal essay answer to the student.
For the studies described below, we compare characteristics of student dialogues with both typed and spoken computer tutors (Why2-Atlas and ITSPOKE,
respectively), as well as with a single human tutor performing the same task as
the computer tutor for each system. Why2-Atlas is a text-based intelligent tutoring dialogue system [16], developed in part to test whether deep approaches to
natural language processing (e.g., sentence-level syntactic and semantic analysis,
discourse and domain level processing, and ﬁnite-state dialogue management)
elicit more learning than shallower approaches. ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) [9] is a speech-enabled version of Why2-ATLAS.
Student speech is digitized from microphone input and sent to the Sphinx2 recognizer. The most probable “transcription” output by Sphinx2 is sent to the
Why2-Atlas natural language processing “back-end”. Finally, the text response
produced by Why2-Atlas is sent to the Cepstral text-to-speech system.

3
3.1

Human-Human Tutoring: Experiment 1
Experimental Procedure

Experiment 1 compared typed and spoken tutoring, using the same human tutor
in both conditions. In the typed condition, the interaction was in the form of a
typed dialogue between the student and tutor, where the human tutor performed
the same task that Why2-Atlas was designed to perform. A text-based chat web
interface was used, with student and tutor in separate rooms; students knew
that the tutor was human. In the spoken condition, the interaction was in the
form of a spoken dialogue, where the human tutor performed the same task
that ITSPOKE was designed to perform. (While the dialogue was changed to
speech, students still typed the essay.) The tutor and student spoke through
head-mounted microphones, allowing all speech to be digitally recorded to the
computer. The student and tutor were in the same room (due to constraints of
speech recording), but separated by a partition. The same web interface was used
as in the typed condition, except that no dialogue history was displayed (this
would have required manual transcription of utterances). In the typed condition

Spoken Versus Typed Human and Computer Dialogue Tutoring

371

Excerpt from 2 minutes into a typed dialogue:
Tutor: You have correctly said the balls are in free fall. What do you conclude from
this fact?
Student: There is no air resistance and the balls’ accelerations are constant in regards
to one another
Tutor: Right, so the conclusion is that they both have the same acceleration, the free
fall acceleration. Now you have to show that time taken by them to reach the
ground will be the same. How do you do that?
Student: F (net forces) / mass = f/m because F = m*a therefore ratio should equal
one another
Tutor: But that does not tell you anything about the time taken. What physical quantities will determine the time taken. Will the initial velocity of the balls matter?
Student: yes, assuming starting at rest? sorry, kind of at a loss at the moment
Excerpt from 8.3 minutes into a spoken dialogue:
Tutor: So the accel- the- both the balls will have same acceleration G is that right?
Student: Yeah.
Tutor: Ok. So now, can you uh argue about the time?
Student: Yeah. Uh the reason the two balls hit the ground at exactly the same instant
is because the net forces acting on it, which is acceleration due to gravity, are equal.
Tutor: Force is gravity and acceleration is the same, ok, and what else?
Student: Um -(sniﬀ)
Tutor: What other factors determine the time taken are the same?
Student: Uh, the height at which they’re dropped Tutor: mm-hm
Student: Uh, the the acceleration due to gravity. Uh Fig. 1. Excerpts from Human-Human Dialogues

strict turn-taking was enforced, while in the spoken condition interruptions and
overlapping speech were permitted. This was because we plan to add “bargein” to
ITSPOKE, which will enable students to interrupt ITSPOKE. Sample dialogue
excerpts from both conditions are displayed in Figure 1.
Pre and posttest items were scored as right or wrong, with no partial credit.
Students who were not able to complete all 10 problems due to lack of time took
the posttest after only working through a subset of the training problems.
Experiment 1 resulted in two human tutoring corpora. The typed dialogue
corpus consists of 171 physics problems with 20 students, while the spoken dialogue corpus consists of 128 physics problems with 14 students. In subsequent
analyses, a “dialogue” refers to the transcript of one student’s discussion of one
problem with the tutor.
3.2

Results

Table 1 presents the means and standard deviations for two types of analyses,
learning and training time, across conditions. The pretest scores were not reliably
diﬀerent across the two conditions, F(33) = 1.574, p = 0.219, MSe = 0.009. In

372

D.J. Litman et al.

Table 1. Learning and Time: Human Tutoring Spoken (14) and Typed (20) Conditions
Human Spoken Human Typed
Dependent Measure
Pretest Mean (standard deviation)
.42 (.10)
.46 (.09)
.72 (.11)
.67 (.13)
Posttest Mean (standard deviation)
Adjusted Posttest Mean (standard deviation)
.74 (.11)
.66 (.11)
166.58 (45.06) 430.05 (159.65)
Dialogue Time (standard deviation)

an ANOVA with condition by test phase factorial design, there was a robust
main eﬀect for test phase, F(67) = 90.589, p = 0.000, MSe = 0.012, indicating
that students in both conditions learned a signiﬁcant amount during tutoring.
However, the main eﬀect for condition was not reliable, F(33) = 1.823, p = 0.186,
MSe = 0.014, and there was no reliable interaction. In an ANCOVA, the adjusted
posttest scores show a strong trend of being reliably diﬀerent, F(1,33)=4.044,
p=0.053, MSe = 0.01173. Our results thus suggest that the human speech tutored
students learned more than the human text tutored students; the eﬀect size is
0.74. With respect to training time, students in the spoken condition completed
their dialogue tutoring in less than half the time than in the typed condition,
where dialogue time was measured as the sum over the training problems of the
number of minutes between the time that the student was shown the problem
text and the time that the student was shown the ideal essay. The extra time
needed for both the tutor and the student to type (rather than speak) each
dialogue turn in the typed condition was a major contributor to this diﬀerence.
An ANOVA shows that the diﬀerence in means across the two conditions was
reliably diﬀerent, with F(33) = 35.821, p = 0.00, MSe = 15958.787. For human
tutoring, our results thus support our hypothesis that spoken tutoring is indeed
more eﬀective than typed tutoring, for both learning and training time.
It is important to understand why the change in modality (and interruption
policy) increased learning. Table 2 presents the means for a variety of measures
characterizing diﬀerent aspects of dialogue, to determine which aspects diﬀer
across conditions, and to examine whether diﬀerent dialogue characteristics correlate with learning across conditions (although the utility of correlation analysis
might be limited by our small subject pool). For each dependent measure (explained below), the second through fourth columns present the means (across
students) for the spoken and typed conditions, along with the statistical signiﬁcance of their diﬀerences. The ﬁfth through eighth columns present a Pearson’s
correlation between each dialogue measure and raw posttest score. However, in
the spoken condition, the pre and posttest scores are highly correlated (R=.72,
p =.008); in the typed condition they are not (R=.29, p=.21). Because of the
spoken correlation, the last four columns show the correlation between posttest
and the dependent measure, after the correlation with pretest is regressed out.
The measures in Table 2 were motivated by previous work suggesting that
learning correlates with increased student language production. In pilot studies
of the typed corpus, average student turn length was found to correlate with
learning. We thus computed the average length of student turns in words (Ave.

Spoken Versus Typed Human and Computer Dialogue Tutoring

373

Table 2. Dialogue Aspects & Learning: Human Spoken (14) & Typed (20) Conditions
Dependent
Measure

Spoken Typed
mean mean

p

Tot. Stud. Words
2322.43 1569.30 .03
Tot. Stud. Turns
424.86 109.30 .00
Ave. Stud. Wds/Turn
5.21 14.45 .00
Slope: Stud. Wds/Trn
-.01
-.05 .04
Int: Stud. Wds/Trn
6.51 16.39 .00
Tot. Tut. Words
8648.29 3366.30 .00
Tot. Tut. Turns
393.21 122.90 .00
Ave. Tut. Wds/Turn
23.04 28.23 .01
S-T Tot. Wds Ratio
.27
.45 .00
S-T Wd/Trn Ratio
.25
.51 .00

Zero Order
Correlations
Spoken Typed
R p
R p
-.473 .09 .065 .78
-.340 .24 -.148 .53
-.167 .57 .491 .03
-.275 .34 -.375 .10
-.176 .55 .625 .00
-.482 .08 .027 .91
-.436 .12 -.171 .47
-.139 .64 .496 .03
.067 .82 .275 .24
.026 .93 .283 .23

Controlled for PreTest Correlations
Spoken Typed
R p
R
p
-.261 .39 .013 .96
-.016 .96 -.213 .38
-.209 .49 .515 .03
.379 .20 -.291 .23
-.441 .13 .593 .01
-.164 .59 -.034 .89
-.110 .72 -.239 .32
-.086 .78 .536 .02
-.202 .51 .268 .27
-.237 .44 .277 .25

Stud. Wds/Turn), as well as the total number of words and turns per student,
summed across all training dialogues (Tot. Stud. Words, Tot. Stud. Turns). We
also computed these ﬁgures for the tutor’s contributions (Ave. Tut. Wds/Turn,
Tot. Tut. Words, Tot. Tut. Turns). The slope and intercept measures will be
explained below. Similarly, the studies of [17] examined student language production relative to tutor language production, and found that the percentage of
words and utterances produced by the student positively correlated with learning. This led us to compute the number of students words divided by the number
of tutor words (S-T Tot. Wds Ratio), and a similar ratio of student words per
turn to tutor words per turn (S-T Wd/Trn Ratio).
Table 2 shows interesting diﬀerences between the spoken and typed corpora
of human-human dialogues. For every measure examined, the means across conditions are signiﬁcantly diﬀerent, verifying that the style of interactions is indeed
quite diﬀerent. In spoken tutoring, both student and tutor take more turns on
average than in typed tutoring, but these spoken turns are on average shorter.
Moreover, in spoken tutoring both student and tutor on average use more words
to communicate than in typed tutoring. However, in typed tutoring, the ratio of
student to tutor language production is higher than in speech.
The remaining columns attempt to uncover which aspects of tutorial dialogue
in each condition were responsible for its eﬀectiveness. Although the zero order
correlations are presented for completeness, our discussion will focus only on the
last four columns, which we feel present the more valid analysis.
In the typed condition, as in its earlier pilot study, there is a positive correlation between average length of student turns in words and learning (R=.515, p =
.03). We hypothesize that longer student answers to tutor questions reveal more
of a student’s reasoning, and that if the tutor is adapting his interaction to the
student’s revealed knowledge state, the eﬀectiveness of the tutor’s instruction
might increase as average student turn length increases. Note that there is no
correlation between total student words and learning; we hypothesize that how

374

D.J. Litman et al.

much a student explains (as estimated by turn length) is more important than
how many questions a student answers (as estimated by total word production).
There is also a positive correlation between average length of tutor turn and
learning (R=.536, p=.02). Perhaps more tutor words per turn means that the
tutor is explaining more or giving more useful feedback. A deeper coding of our
data would be needed to test all of these hypotheses. Finally, as in the typed
pilot study [18], student words per turn usually decreased gradually during the
sessions. In speech, turn length decreased from an average of 6.0 words/turn for
the ﬁrst problem to 4.5 words/turn by the last problem. In text, turn length decreased from an average of 14.6 words for the ﬁrst problem to 10.7 words by the
last problem. This led us to ﬁt regression lines to each subject and compare the
intercepts and slopes to learning. These measures indicate roughly how verbose
a student was initially and how quickly the student became taciturn. Table 2
indicates a reliable correlation between intercept and learning (R=.593; p=.01)
for the typed condition, suggesting that inherently verbose students (or at least
those who initially typed more) learned more in typed human dialogue tutoring.
Since there were no signiﬁcant correlations in the the spoken condition, we
have begun to examine other measures that might be more relevant in speech.
For example, the mean number of total syntactic questions per student is 35.29,
with a trend for a negative correlation with learning (R=-.500, p=.08). This
result suggests, that as with our text-based correlations, our current surface
level analyses will need to be enhanced with deeper codings before we can fully
interpret our results (e.g., by manually coding non-interrogative form questions,
and by distinguishing question types).

4
4.1

Human-Computer Tutoring: Experiment 2
Experimental Procedure

Experiment 2 compared typed and spoken tutoring using the Why2-Atlas and
ITSPOKE computer tutors, respectively. The experimental procedure was the
same as for Experiment 1, except that students worked through only 5 physics
problems, and the pretest was taken after the background reading (allowing us to
measure gains caused by the experimental manipulation, without confusing them
with gains caused by background reading). Strict turn-taking was now enforced
in both conditions as bargein had not yet been implemented in ITSPOKE.
While Why2-Atlas and ITSPOKE used the same web interface, during the
dialogue, Why2-Atlas students typed while ITSPOKE students spoke through
a head-mounted microphone. In addition, the Why2-Atlas dialogue history contained what the student actually typed, while the ITSPOKE history contained
the potentially noisy output of ITSPOKE’s speech recognizer. The speech recognizer’s hypothesis for each student utterance, and the tutor utterances, were
not displayed until after the student or ITSPOKE had ﬁnished speaking.
Figure 2 contains excerpts from both Why2-Atlas and ITSPOKE dialogues.
Note that for ITSPOKE, the output of the automatic speech recognizer (the

Spoken Versus Typed Human and Computer Dialogue Tutoring

375

Excerpt from 3.0 minutes into a typed dialogue:
Why2-Atlas: You have answered the problem correctly except that what you have in
your essay is not a suﬃcient explanation. Before the release, what is the relationship
between the man’s velocity and the keys’ velocity?
Student: They are the same
Why2-Atlas: Uh huh. If gravity is the only force acting on an object, will it be moving
or staying still?
Student: Moving
Why2-Atlas: Good. Not only are the person, keys, and elevator moving, they have
only gravitational forces acting on them. When an object is falling and has only
gravitational force on it, it is said to be in what?
Student: Free fall
Excerpt from 11.1 minutes into spoken dialogue:
ITSPOKE: If you push on the same box with three times the force, what happens
to the acceleration of the box?
Student: it increases (ASR: it increases)
ITSPOKE: Terriﬁc. OK, what is the box’s acceleration when you are pushing three
times relative to its acceleration when you were only pushing with one-third the
force?
Student: it decreases (ASR: it decreases since)
ITSPOKE: Could you please repeat that?
Student: it increases (ASR: it increases)
Fig. 2. Excerpts from Why2-Atlas and ITSPOKE Dialogues

ASR annotations) sometimes diﬀered from what the student actually said. Thus,
ITSPOKE dialogues contained rejection prompts (when ITSPOKE was not conﬁdent of what it thought the student said, it asked the student to repeat, as in the
third ITSPOKE turn). On average, ITSPOKE produced 1.4 rejection prompts
per dialogue. ITSPOKE also misrecognized utterances; when ITSPOKE heard
something diﬀerent than what the student said but was conﬁdent in its hypothesis, it proceeded as if it heard correctly. While the ITSPOKE word error rate
was 31.2%, semantic analysis based on speech recognition versus perfect transcription diﬀered only 7.6% of the time. Semantic accuracy is more relevant for
dialogue evaluation, as it does not penalize for unimportant word errors.
Experiment 2 resulted in two computer tutoring corpora. The typed Why2Atlas dialogue corpus consists of 115 problems (dialogues) with 23 students,
while the ITSPOKE spoken corpus consists of 100 problems (dialogues) with 20
students.
4.2

Results

Table 3 presents the means and standard deviations for the learning and training
time measures previously examined in Experiment 1. The pre-test scores were
not reliably diﬀerent across the two conditions, F(42) = 0.037, p= 0.848, MSe =
0.036. In an ANOVA with condition by test phase factorial design, there was a

376

D.J. Litman et al.

Table 3. Learning and Time: Computer Tutoring Spoken (20) and Typed (23)
Computer Spoken Computer Typed
Dependent Measure
Pretest Mean (standard deviation)
.48 (.17)
.49 (.20)
.69 (.18)
.70 (.16)
Posttest Mean (standard deviation)
Adjusted Posttest Mean (standard deviation)
.69 (.13)
.69 (.13)
97.85 (32.8)
68.93 (29.0)
Dialogue Time (standard deviation)

robust main eﬀect for test phase, F(85) = 29.57, p = 0.000, MSe = 0.032, indicating that students learned during their tutoring. The main eﬀect for condition was
not reliable, F(42)=0.029, p=0.866, MSe=0.029, and there was no reliable interaction. In an ANCOVA of the multiple-choice test data, the adjusted post-test
scores were not reliably diﬀerent, F(1,42)=0.004, p=0.950, MSe=0.01806. Thus,
the Why-Atlas tutored students did not learn reliably more than the ITSPOKE
tutored students. With respect to training time, students in the spoken condition
took more time to complete their dialogue tutoring than in the typed condition.
In the spoken condition, extra utterances were needed to recover from speech
recognition errors; also, listening to tutor prompts often took more time than
reading them, and students sometimes needed to both listen to, then read, the
prompts. An ANOVA shows that this diﬀerence was reliable, with F(42)=9.411,
p=0.004, MSe=950.792. In sum, while adding speech to Why2-Atlas did not
yield the hoped for improvements in learning, the degradation in tutor understanding due to speech recognition (and potentially in student understanding
due to text-to-speech) also did not decrease student learning. A separate analysis showed no correlation between word error or semantic degradation (discussed
in Section 4.1) with learning or training time.

Table 4. Dialogue & Learning: Computer Spoken (20) & Typed (23) Conditions
Dependent
Measure

Spoken Typed
mean mean

p

Tot. Student Words
296.85 238.17 .12
Tot. Student Turns
116.75 87.96 .02
Ave. Student Words/Turn
2.42
2.77 .29
Slope: Student Wds/Trn
-.02
.00 .02
Intercept: Stud. Wds/Trn
3.21
2.88 .40
Tot. Tutor Words
6314.90 4972.61 .03
148.20 110.22 .01
Tot. Tutor Turns
Ave. Tutor Words/Turn
42.11 44.33 .06
.05 .57
Stud-Tut Tot. Word Ratio
.05
.06
.06 .64
Stud-Tut Wds/Trn Ratio
3.29
1.98 .01
Tot. Subdial/KCD

Zero Order
Correlations
Spoken Typed
R p
R p
.043 .86 -.354 .10
-.093 .70 -.549 .01
.061 .80 .167 .45
-.179 .45 -.084 .70
.246 .30 .250 .25
-.100 .68 -.576 .00
-.061 .80 -.529 .01
-.261 .27 -.565 .01
.219 .35 .238 .27
.089 .71 .278 .20
-.304 .19 -.732 .00

Controlled for PreTest Correlations
Spoken Typed
R p
R
p
.394 .10 .050 .82
.210 .39 -.168 .46
.119 .63 .202 .37
-.287 .23 -.102 .65
.321 .18 .281 .21
.283 .24 -.159 .48
.252 .30 -.133 .56
.-062 .80 -.164 .47
.281 .25 .201 .37
.094 .70 .212 .35
-.018 .94 -.457 .03

Spoken Versus Typed Human and Computer Dialogue Tutoring

377

Table 4 presents the means for the measures used in Experiment 1 to characterize dialogue, as well as for a new “Tot. Subdialogues per KCD” measure
for our computer tutors. A Knowledge Construction Dialogue (KCD) is a line
of questioning targeting a speciﬁc concept (such as Newton’s Third Law). When
students answer questions incorrectly, the KCDs correct them through a “subdialogue”, which may involve more interactive questioning or simply a remedial
statement. Thus, subdialogues per KCD is the number of student responses
treated as wrong. We hypothesized that this measure would be higher in speech,
due the previously noted degradation in semantic accuracy.
Compared to Experiment 1, Table 4 shows that there are less diﬀerences between spoken and typed computer tutoring dialogues. The total words produced
by students, the average length of turns and initial verbosity, and the ratios
of student to tutor language production are no longer reliably diﬀerent across
conditions. As hypothesized, Tot. Subdialogues per KCD is reliably diﬀerent
(p=.01). Finally, the last four columns show a signiﬁcant negative correlation
between Tot. Subdialogues per KCD and posttest score (after regressing out
pretest) in the typed condition. There is also a trend for a positive correlation
with total student words in the spoken condition, consistent with previous results
on learning and increased student language production.

5

Discussion and Current Directions

The main results of our study are that changing the modality from text to
speech caused large diﬀerences in the learning gains, time and superﬁcial dialogue characteristics of human tutoring, but for computer tutoring it made
less diﬀerence. Experiment 1 on human tutoring suggests that spoken dialogue
(allowing interruptions) is more eﬀective than typed dialogue (prohibiting interruptions), with mean adjusted posttest score increasing and training time
decreasing. We also ﬁnd that typed and spoken dialogues are very diﬀerent for
the surface measures examined, and for the typed condition we see a beneﬁt for
longer turns (evidenced by correlations between learning and average and initial
student turn length and average tutor turn length). While we do not see these
results in speech, spoken utterances are typically shorter than written sentences
(and in our experiment, turn length was also impacted by interruption policy),
suggesting that other measures might be more relevant. However, we plan to investigate whether spoken phenomena such as disﬂuencies and grounding might
also explain the lack of correlation.
The results of Experiment 2 on computer tutoring are less conclusive. On
the negative side, we do not see any evidence that replacing typed dialogue in
Why2-Atlas with spoken dialogue in ITSPOKE improves student learning. However, on the positive side, we also do not see any evidence that the degradation
in understanding caused by speech recognition decreases learning. Furthermore,
compared to human tutoring, we see less diﬀerence between spoken and typed
computer dialogue interactions, at least for the dialogue aspects measured in our
experiments. One hypothesis is that simply adding a spoken “front-end”, with-

378

D.J. Litman et al.

out also modifying the tutorial dialogue system “back-end”, is not enough to
change how students interact with a computer tutor. Another hypothesis is that
the limitations of the particular natural language technologies used in Why2Atlas (or the expectations that the students had regarding such limitations)
are inhibiting the modality diﬀerences. Finally, if there were diﬀerences between
conditions, perhaps the shallow measures used in our experiments and/or our
small number of subjects prevented us from discovering them. In sum, while
the results of human tutoring suggest that spoken tutoring is a promising approach for enhancing learning, more exploration is required to determine how to
productively incorporate speech into computer tutoring systems.
By design, the modality change left the content of the computer dialogues
completely unchanged – the tutors said nearly the same words and asked nearly
the same questions, and the students gave their usual short responses. On the
other hand, the content of the human tutoring dialogues probably changed considerably when the modality changed. This suggests that modality change makes
a diﬀerence in learning only if it also facilitates content change. We will investigate this hypothesis in future work by coding for content and other deep features.
Finally, we had hypothesized that the spoken modality would encourage students to become more engaged and to self-construct more knowledge. Although a
deeper coding of the dialogues would be necessary to test this hypothesis, we can
get a preliminary sense of its veracity by examining the total number of words
uttered. Student verbosity (and perhaps engagement and self-construction) did
not increase signiﬁcantly in the spoken computer tutoring experiment. In the
human tutoring experiment, the number of student words did signiﬁcantly increase, which is consistent with the hypothesis and may explain why spoken
human tutoring was probably more eﬀective than typed human tutoring. However, the number of tutor words also signiﬁcantly increased, which suggests that
the human tutor may have “lectured” more in the spoken modality. Perhaps
these longer explanations contributed to the beneﬁts of speaking compared to
the text, but it is equally conceivable that they reduced the amount of engagement and knowledge construction, and thus limited the gains. This suggests that
although we considered how the modality might eﬀect the student, we neglected
to consider how it might eﬀect the tutor, and how that might impact the students’ learning. Clearly, these issues deserve more research. Our goal is to use
such investigations to guide the development of future versions of Why2-Atlas
and ITSPOKE, by modifying the dialogue behaviors in each system to best
enhance the possibilities for increasing learning.
Acknowledgments. This research is supported by ONR (N00014-00-1-0600,
N00014-04-1-0108).

References
1. Blom, B.S.: The 2 Sigma problem: The search for methods of group instruction as
aﬀective as one-to-one tutoring. Educational Researcher 13 (1984) 4–16

Spoken Versus Typed Human and Computer Dialogue Tutoring

379

2. Anderson, J.R., Corbett, A.T., Koedinger, K.R., Pelletier, R.: Cognitive tutors:
Lessons learned. The Journal of the Learning Sciences 4 (1995) 167–207
3. VanLehn, K., Lynch, C., Taylor, L., Weinstein, A., Shelby, R.H., Schulze, K.,
Treacy, D.J., Wintersgill, M.C.: Minimally invasive tutoring of complex physics
problem solving. In: Proc. Intelligent Tutoring Systems (ITS), 6th International
Conference. (2002) 367–376
4. Graesser, A.C., Wiemer-Hastings, K., Wiemer-Hastings, P., Kreuz, R.: Autotutor:
A simulation of a human tutor. Journal of Cognitive Systems Research 1 (1999)
5. Hausmann, R., Chi, M.: Can a computer interface support self-explaining? The
International Journal of Cognitive Technology 7 (2002)
6. Chi, M., Leeuw, N.D., Chiu, M., Lavancher, C.: Eliciting self-explanations improves
understanding. Cognitive Science 18 (1994) 439–477
7. Renkl, A.: Learning from worked-out examples: A study on individual diﬀerences.
Cognitive Science 21 (1997) 1–29
8. Chi, M.T.H., Siller, S.A., Jeong, H., Yamauchi, T., Hausmann, R.G.: Learning
from human tutoring. Cognitive Science (2001) 471–477
9. Litman, D.J., Forbes-Riley, K.: Predicting student emotions in computer-human
tutoring dialogues. In: Proc. Association Computational Linguistics (ACL). (2004)
10. Graesser, A.C., Moreno, K.N., Marineau, J.C., Adcock, A.B., Olney, A.M., Person,
N.K.: Autotutor improves deep learning of computer literacy: Is it the dialog or
the talking head? In: Proc. AI in Education. (2003)
11. Moreno, R., Mayer, R.E., Spires, H.A., Lester, J.C.: The case for social agency in
computer-based teaching: Do students learn more deeply when they interact with
animated pedagogical agents. Cognition and Instruction 19 (2001) 177–213
12. Schultz, K., Bratt, E.O., Clark, B., Peters, S., Pon-Barry, H., Treeratpituk, P.:
A scalable, reusable spoken conversational tutor: Scot. In: AIED Supplementary
Proceedings. (2003) 367–377
13. Michael, J., Rovick, A., Glass, M.S., Zhou, Y., Evens, M.: Learning from a computer tutor with natural language capabilities. Interactive Learning Environments
(2003) 233–262
14. Zinn, C., Moore, J.D., Core, M.G.: A 3-tier planning architecture for managing
tutorial dialogue. In: Proceedings Intelligent Tutoring Systems, Sixth International
Conference (ITS 2002), Biarritz, France (2002) 574–584
15. Aleven, V., Popescu, O., Koedinger, K.R.: Pilot-testing a tutorial dialoque system
that supports self-explanation. In: Proc. Intelligent Tutoring Systems (ITS): 6th
International Conference. (2002) 344–354
16. VanLehn, K., Jordan, P., Rosé, C., Bhembe, D., Böttner, M., Gaydos, A.,
Makatchev, M., Pappuswamy, U., Ringenberg, M., Roque, A., Siler, S., Srivastava, R., Wilson, R.: The architecture of Why2-Atlas: A coach for qualitative
physics essay writing. In: Proc. Intelligent Tutoring Systems (ITS), 6th International Conference. (2002)
17. Core, M.G., Moore, J.D., Zinn, C.: The role of initiative in tutorial dialogue.
In: Proc. 11th Conf. of European Chapter of the Association for Computational
Linguistics (EACL). (2003) 67–74
18. Rose, C.P., Bhembe, D., Siler, S., Srivastava, R., VanLehn, K.: The role of why
questions in eﬀective human tutoring. In: Proc. AI in Education. (2003)

Do Micro-Level Tutorial Decisions Matter:
Applying Reinforcement Learning to Induce
Pedagogical Tutorial Tactics
Min Chi1 , Kurt VanLehn2 , and Diane Litman3
1

2

Machine Learning Department, Carnegie Mellon University, PA, 15213 USA
minchi@cs.cmu.edu
School of Computing and Informatics, Arizona State University, AZ, 85287 USA
Kurt.Vanlehn@asu.edu
3
Department of Computer Science & Learning Research Development Center,
University of Pittsburgh, PA, 15260 USA
litman@cs.pitt.edu

Abstract. Pedagogical tutorial tactics are policies for a tutor to decide
the next action when there are multiple actions available. When the contents were controlled so as to be the same, little evidence has shown
that tutorial decisions would impact students’ learning. In this paper,
we applied Reinforcement Learning (RL) to induce two sets of tutorial
tactics from pre-existing human interaction data. The NormGain set was
derived with the goal of enhancing tutorial decisions that contribute to
learning while the InvNormGain set was derived with the goal of enhancing those decisions that contribute less or even nothing to learning. The
two sets were then compared with human students. Our results showed
that when the contents were controlled so as to be the same, diﬀerent
pedagogical tutorial tactics would make a diﬀerence in learning and more
speciﬁcally, the NormGain students outperformed their peers.
Keywords: Reinforcement Learning, Human Learning, Intelligent Tutoring Systems, Pedagogical Strategy.

1

Introduction

Human one-on-one tutoring is one of the most eﬀective educational interventions in that tutored students often perform signiﬁcantly better than students
in classroom settings[4]. One hypothesis as to the eﬀectiveness of human one-onone tutoring comes from the detailed management of “micro-steps” in tutorial
dialogue[12, 13]. A typical Intelligent Tutoring System (ITS) is step-based[20].
Once a student enters a step, then the tutor gives feedback and/or hints. For
example, in order to solve a physics problem, the student need to apply several
domain principles, some of which may need to be applied multiple times. Each
principle application can be seen as a step in the ITS. In a physics tutor, for
example, applying the deﬁnition of Kinetic Energy (KE = 12 mv 2 ) to solve for
the kinetic energy of a falling rock at T0 is a step. Human tutors, by contrast,
V. Aleven, J. Kay, and J. Mostow (Eds.): ITS 2010, Part I, LNCS 6094, pp. 224–234, 2010.
c Springer-Verlag Berlin Heidelberg 2010


Do Micro-Level Tutorial Decisions Matter

225

1. T: So let’s start with determining the value of KE0 .
2. T: Which principle will help you calculate the rock’s kinetic energy at
T0? Please provide the name of the principle, not an equation.
{ELICIT}
3. S: Deﬁnition of kinetic energy
4. T: Yes, I agree. Now I will write the equation for applying the deﬁnition
of kinetic energy to the rock at T0: KE0 = 1/2*m*v0ˆ2 {TELL}
Fig. 1. An example Dialog

often scaﬀold students via a series of micro-steps leading to the full step. In
the step mentioned above, for instance, a human tutor can take the following
micro-level steps: selecting the principle to apply; writing the corresponding
equation; solving the equation; and engaging in some qualitative discussion about
the principle.
Fig. 1 shows a sample dialog for two micro-steps. In Fig. 1, each numbered line
represents a dialog turn. The labels T and S designate tutor and student turns
respectively. In this example, the tutor and the student ﬁrst select a principle
(lines 2 & 3) and then write the corresponding equation (line 4). Some of the
tutor turns in Fig. 1 are labeled {ELICIT} or {TELL}. This label designates
a tutorial decision step wherein the tutor has to make a tutorial decision
deciding whether to elicit the requisite information with a question or to tell
the student the information. For example, in line 2, the tutor chooses to elicit
the answer from the student by asking the question, “Which principle will help
you calculate the rock’s kinetic energy at T0? Please provide the name of the
principle, not an equation.” If the tutor elected to tell the students, however,
then he or she would have stated, “To calculate the rock’s kinetic energy at T0,
let’s apply the deﬁnition of Kinetic Energy.” Both actions cover the same target
knowledge.
If the eﬀectiveness of human one-on-one tutoring lies in tutors’ ability to scaffold a series of micro-steps leading to a step entry, then we would expect human
tutors to be more eﬀective than step-based tutors as both require students to
enter the same major steps. In several tests of this hypothesis, neither human
tutors nor Natural Language (NL) tutoring systems designed to mimic human
tutors, outperformed step-based systems[10, 22]. All three types of tutors, however, were more eﬀective than no instruction (e.g., students reading material
and/or solving problems without feedback or hints). One possible conclusion
is that tutoring is eﬀective, but that the micro-steps of human tutors and NL
tutoring systems provide no additional value beyond conventional step-based
tutors[21].
On the other hand, such a conclusion would be premature. It could simply be
that neither human tutors nor their computer mimics are good at making microstep decisions. That is, the use of micro-steps is good, but human tutors (and
their mimics) lack the eﬀective pedagogical skills to select appropriately. Indeed,

226

M. Chi, K. VanLehn, and D. Litman

although it is commonly assumed that human expert tutors have eﬀective pedagogical skills, little evidence has been presented to date demonstrating that. In
order to execute pedagogical skills eﬀectively, it is assumed that tutors should
adapt their behaviors to students’ needs based upon students’ current knowledge
level, general aptitude, emotional state and other salient features. However, previous research has cast doubt on the assumption. Chi, Roy, and Hausman[8]
found that human tutors do not seem to maintain an accurate model of student’s knowledge level during the tutoring process. Similarly, Putnam[17] found
that experienced tutors did not attempt to form detailed models of the students’ knowledge before attempting remedial instruction. Rather, each teacher
appeared to move through a general curricular script irrespective of the student’s
state. For the purposes of this paper the term “pedagogical tutorial tactics” will
be used to refer to the policies for selecting the tutorial action at each micro-step
level when there are multiple actions available.
In this study, our primary research question is whether pedagogical tutorial
tactics would impact students’ learning. We focus on two types of tutorial decisions, Elicit vs.Tell (ET) and Justify vs. Skip-Justify (JS). When making ET
decisions the tutor decides whether to elicit the next step from the student or
to tell them the step directly. The JS decisions address points where the tutor may optionally ask students to justify a step they have taken or entry they
have made. Neither decision is well-understood. There are many theories, but
no widespread consensus on how or when an action should be taken[1, 7, 9, 14].
In order to investigate our research question, we applied a general data-driven
methodology, Reinforcement Learning (RL), to induce pedagogical tutorial tactics directly from pre-existing interactivity data. We used an NL Tutoring System
called Cordillera[23]. In order to avoid confounds due to imperfect NL understanding, we replaced the NL understanding module with a human wizard. During tutoring, the wizard’s sole task was to match students’ answers to one of the
available responses. The wizard made no tutorial decisions.
Previously we investigated whether the RL-induced pedagogical tutorial tactics would improve students’ learning[6]. This was done by ﬁrst collecting an Exploratory dataset in 2007. 64 college students, the Exploratory group, were trained
on a version of Cordillera, called random-Cordillera, where both ET and JS decisions were made randomly. From the Exploratory corpus, we applied RL to induce a set of policies, named DichGain policies. They were named after the fact
that when applying RL, we dichotomized the reward function so that there were
only two levels of reward. The induced DichGain policies were implemented back
to Cordillera and the new version of Cordillera was named DichGain-Cordillera.
Apart from following the policies (random vs. DichGain), the remaining components of Cordillera, including the GUI interface, the same training problems, and
the tutorial scripts, were left untouched. DichGain-Cordillera’s eﬀectiveness was
tested by training a new group of 37 college students in 2008. It was shown that
no signiﬁcant overall diﬀerence was found between the two groups on the pretest,
posttest, or the NLGs[6, 5]. There were at least two potential reasons for such
lack of diﬀerence. First, it might be caused by limitations in our RL approach;

Do Micro-Level Tutorial Decisions Matter

227

for example, in order to induce the DichGain policies, we deﬁned only 18 features
and used a greedy-like procedure to search for a small subset of it as the state
representation[6]. Second, rather than randomly assigning students into the two
groups, the Exploratory data was collected in 2007 while the DichGain data was
collected in 2008.
Therefore, in this study we included multiple training datasets, a larger feature set and more feature selection approaches in our RL approach and run a
full comparison by random assignment of students to two comparable groups.
More speciﬁcally, we induced two sets of tutorial tactics: the Normalized Gain
(NormGain) tactics were derived with the goal of making tutorial decisions that
contribute to students’ learning, while the Inverse Normalized Gain (InvNormGain) tactics were induced with the goal of making less beneﬁcial, or possibly
useless, decisions. The two sets were then compared by making all students
studying the same materials and training on the Cordillera that covered the
same subject matter and training problems, and used the same tutorial scripts
and user interface. If our application of RL to induce pedagogical tutorial tactics
is eﬀective, then we expect that the NormGain students will outperform their
InvNormGain peers. This would occur if the micro-level decisions on ET and JS
impact learning. In the following, we will brieﬂy describe how we applied RL to
induce the pedagogical tutorial tactics and then describe our study and ﬁnally
present our results.

2

Applying RL to Induce Pedagogical Tutorial Tactics

Previous research on using RL has typically used Markov Decision Processes
(MDPs)[18]. MDP is a formal state model, commonly used to model dialogue
data. Formally, an MDP is a 4-tuple (S, A, T, R), where: S = {S1 , · · · , Sn } is a
state space; A = {A1 , · · · , Am } is an action space represented by a set of action
variables; T : S × A × S → [0, 1] is a set of transition probabilities between
states describing the dynamics of the modeled system (e.g. P (Sj |Si , Ak ) is the
probability that the model will transition from state Si to state Sj by taking
action Ak ); R : S × A × S → R is a reward model that assigns reward values
to state transitions and models payoﬀs associated with the transitions. Finally,
π : S → A is a policy.
The central idea behind this approach is to transform the problem of inducing eﬀective pedagogical tactics into computing an optimal policy for choosing
actions in an MDP. Inducing pedagogical tutorial tactics can be easily represented using an MDP: the states are vector representations composed of relevant student-tutor interaction characteristics; A = {Elicit, T ell} for inducing
ET policies and {Justif y, Skip − Justif y} for inducing JS policies, and the
reward function is calculated from the system’s success measures and we used
learning gains. Given that a student’s learning gain will not be available until
the entire tutorial dialogue is completed, only terminal dialogue state has nonzero reward. Once the S, A, R has been deﬁned, the transition probabilities T
are estimated from the training corpus, which is the collection of dialogues, as:

228

M. Chi, K. VanLehn, and D. Litman

,m
T = {p(Sj |Si , Ak )}k=1,···
i,j=1,··· ,n . More speciﬁcally, p(Sj |Si , Ak ) is calculated by taking the number of times that the dialogue is in state Si , the tutor took action
Ak , and the dialogue was next in state Sj divided by the number of times the dialogue was in Si and the tutor took Ak . The reliability of these estimates clearly
depends upon the size and structure of the training data. Once a complete MDP
is constructed, a dynamic programming approach can be used to learn the optimal control policy π ∗ and here we used the toolkit developed by Tetreault and
Litman[19]. The rest of this section presents a few critical details of the process,
but many others must be omitted to save space.
In this study, the reward functions for inducing both the NormGain and the
InvNormGain sets were based on Normalized Learning Gain (NLG). This is
because NLG measures a student’s gain irrespective of his/her incoming com. Here posttest and pretest refer
petence and we have: N LG = posttest−pretest
1−pretest
to the students’ test scores before and after the training respectively; and 1 is
the maximum score. More speciﬁcally, the NormGain tutorial tactics induced by
using the student’s N LG × 100 as the ﬁnal reward while the InvNormGain ones
was induced by using the student’s (1 − N LG) × 100 as the ﬁnal reward. Apart
from the reward functions, they were induced using the same general procedure.

2.1

Knowledge Component (KC) Based Pedagogical Strategies

In the learning literature, it is commonly assumed that relevant knowledge in
domains such as math and science is structured as a set of independent but
co-occurring Knowledge Components (KCs) and that these KCs are learned independently. A KC is “a generalization of everyday terms like concept, principle,
fact, or skill, and cognitive science terms like schema, production rule, misconception, or facet”[23]. For the purpose of ITSs, these are the atomic units of
knowledge. It is assumed that a tutorial dialogue about one KC will have no
impact on the student’s understanding of any other KCs. This is an idealization,
but it has served ITS developers well for many decades, and is a fundamental assumption of many cognitive models[2, 16]. When dealing with a speciﬁc
KC, the expectation is that the tutor’s best policy for teaching that KC (e.g.,
to Elicit vs. to Tell) would be based upon the student’s mastery of the KC in
question, its intrinsic diﬃculty, and other relevant, but not necessarily known,
factors speciﬁc to that KC. In other words, an optimal policy for one KC might
not be optimal for another. Therefore, one assumption made in this paper is
that inducing pedagogical policies speciﬁc to each KC would be more eﬀective
than inducing an overall KC-general policy.
The domain selected for this project is a subset of the physics work-energy
domain, which is characterized by eight primary KCs. Given these independence
assumptions, the problem of inducing a policy for ET decisions and a policy
for JS decisions may be decomposed into 8 sub-problems of each type, one per
KC. More speciﬁcally, in order to learn a policy for each KC, we annotated our
tutoring dialogues and action decisions with the KCs covered by each action.
For each KC, the ﬁnal kappa was ≥ 0.77, fairly high given the complexity of the
task. A domain expert also mapped the pre- and post-test problems to relevant

Do Micro-Level Tutorial Decisions Matter

229

KCs. This resulted in a KC-speciﬁc NLG score for each student. KC 1 does not
arise in any JS decisions and thus only an ET policy was induced for it. For the
remaining seven KCs, a pair of policies, one ET policy and one JS policy, were
induced. So we induced 15 KC-based NormGain and 15 KC-based InvNormGain
policies. There were some decision steps that did not involve any of the eight
primary KCs. For them, two KC-general policies, an ET policy and a JS policy,
were induced. To sum, a total of 17 NormGain and 17 InvNormGain policies
were induced.
2.2

Inducing NormGain and InvNormGain Policies

In order to apply RL to induce pedagogical tutorial tactics, a training corpus is
needed. In this study, we have three training corpora available from the previous
study[6]: the Exploratory corpus collected in 2007, the DichGain corpus collected
in 2008, and a Combined training corpus. In order to examine a range of possible
tactics, we included 50 features based upon six categories of features considered
by previous research[15, 3, 11] to be relevant. We also used a diﬀerent method of
searching the power set of the 50 features and directly used the N LG × 100 for
inducing NormGain policies and(1−N LG)×100 for inducing InvNormGain ones
instead of dichotomizing the NLGs when inducing DichGain policies previously.
Fig. 2 shows an example of a learned NormGain policy on KC20 , “Deﬁnition
of Kinetic Enegy”, for ET decisions. The policy involves three features:
[StepDiﬃculty:] encodes a step’s diﬃculty level. Its value is estimated from
the Combined Corpus based on the percentage of correct answers given on the
step.
[TutorConceptsToWords:] which represents the ratio of the physics concepts
to words in the tutor’s dialogue. This feature also reﬂects how often the tutor
has mentioned physics concepts overall.
[TutorAverageWordsSession:] The average number of words in the tutor’s
turn in this session. This feature reﬂects how verbose the tutor is in the current
session.

[Feature:]
StepDiﬃculty:
[0, 0.38) → 0; [0.38, 1] → 1
TutorConceptsToWords:
[0, 0.074) → 0; [0.074, 1] → 1
TutorAverageWordsSession: [0, 22.58) → 0; [22.58, ∞) → 1
[Policy:]
Elicit: 0:0:0 0:0:1 1:0:1 1:1:0 1:1:1
Tell: 0:1:0
Else:0:1:1 1:0:0
Fig. 2. A NormGain Policy on KC20 For ET Decisions

230

M. Chi, K. VanLehn, and D. Litman

MDP generally requires discrete features and thus all the continuous features
need to be discretized. Fig. 2 listed how each of the three features was discretized.
For example, For StepDiﬃculty, if its value is above 0.38, it is 1 otherwise, it is 0.
There were 8 rules learned: in 5 situations the tutor should elicit, in one situation
it should tell; in the remaining 2 cases either will do. For example, when all three
features are zero (which means when the current step’s diﬃculty level is low, the
tutor ratio of physics concepts to words is low, and the tutor is not very wordy
in the current session), then the tutor should elicit as 0:0:0 is listed next to the
[elicit]. As you can see, three features already provide relatively complex tutorial
tactics and the induced policies were not like most of the tutorial tactics derived
from analyzing human tutorial dialogues.
The resulting NormGain and InvNormGain policies were then implemented
back into Cordillera yielding two new versions of the system, named NormGainCordillera and InvNormGain-Cordillera respectively. The induced tutorial
tactics were evaluated on real human subjects to see whether the NormGain
students would out-perform the InvNormGain peers.

3
3.1

Methods
Participants

Data was collected over a period of two months during the summer of 2009.
Participants were 64 college students who received payment for their participation. They were required to have a basic understanding of high-school algebra.
However, they could not have taken any college-level physics courses. Students
were randomly assigned to the two conditions. Each took from one to two weeks
to complete the study over multiple sessions. In total, 57 students completed the
study (29 in the NormGain group and 28 in the InvNormGain group).
3.2

Domain and Procedure

Our work used the Physics work-energy domain as covered in the ﬁrst-year college physics course. The eight primary KCs were: the weight law (KC1), deﬁnition of work (KC14), Deﬁnition of Kinetic Energy (KC20), Gravitational Potential Energy (KC21), Spring Potential Energy (KC22), Total Mechanical Energy
(KC24), Conservation of Total Mechanical Energy (KC27), and Change of Total
Mechanical Energy (KC28).
All participants experienced the identical procedure and materials. More specifically, participants all completed 1) a background survey; 2) read a textbook covering the target domain knowledge; 3) took a pretest; 4) solved the same seven
training problems in the same order on Cordillera; and 5) ﬁnally took a posttest.
The pretest and posttest were identical. Except following the policies (NormGain
vs. InvNormGain), the remaining components of Cordillera, including the GUI
interface, the same training problems, and the tutorial scripts, were identical for
all students.

Do Micro-Level Tutorial Decisions Matter

3.3

231

Grading

All tests were graded in a double-blind manner by a single experienced grader.
In a double-blind manner, neither the students nor the grader know who belongs
to which group. For all identiﬁed relevant KCs in a test question, a KC-based
score for each KC application was given. We evaluated the student’s competence
in the following sections based on the sum of these KC-based scores. This is
because the KC-based pre- and post-test scores were used to deﬁne the reward
functions when applying RL to induce policies. Later analysis showed that the
same ﬁndings stand for other scoring rubrics. The tests contained 33 test items
which covered 168 KC occurrences. For comparison purposes all test scores were
normalized to fall in the range of [0,1].

4

Results

Random assignment appears to have balanced the incoming student competence
across conditions. There were no statistically signiﬁcant diﬀerences between the
two conditions in the pre-test scores t (55) = 0.71, p = .484. Additionally, no
signiﬁcant diﬀerences were found between the two conditions on the mathSAT
scores and the total training time spent on Cordillera: t (39) = 0.536, p = 0.595
and t (55) = −.272, p = .787 respectively.
A one-way ANOVA was used to test for learning performance diﬀerences between the pre- and posttests. Both conditions made signiﬁcant gains from pretest to post-test: F (1, 56) = 31.34, p = .000 for the NormGain condition and
F (1, 54) = 6.62, p = .013 for the InvNormGain condition. Table 1 compares the
pre-test, post-test, adjusted-post-test, and NLG scores between the two conditions. In Table 1, the Adjusted Post-test scores were compared between the two
conditions by running an ANCOVA using the corresponding pre-test score as
the covariate. The second and third columns in Table 1 list the means and SDs
σ of the NormGain and InvNormGain groups’ corresponding scores. The fourth
column lists the corresponding statistical comparison and the ﬁfth column lists
the eﬀect size of the comparison and we used Cohen’s d. This is deﬁned as the
mean learning gain of the experimental group minus the mean learning gain of
the control group, divided by the groups’ pooled standard deviation. Table 1
shows that there was no signiﬁcant diﬀerence between the two groups on pretest scores. However, there were signiﬁcant diﬀerences between the two groups on
Table 1. NormGain vs. InvNormGain on Various Test Scores

NormGain InvNormGain Stat
cohen d
Pretest
0.42 (0.16) 0.39 (0.23) t (55) = 0.71, p = .484
0.15
Posttest
0.65 (0.15) 0.54 (0.20) t (55) = 2.32, p = 0.024
0.65 ∗ ∗
Adjusted Posttest 0.63 (.095) 0.55 (.095) F (1, 54) = 10.689, p = .002 0.86 ∗ ∗
NLG
0.41 (0.19) 0.25 (0.21) t (55) = 3.058, p = 0.003
0.81 ∗ ∗

232

M. Chi, K. VanLehn, and D. Litman

the post-test, adjusted-post-test, and NLG scores. Across all measurements, the
NormGain group performed signiﬁcantly better than the InvNormGain peers.
The eﬀect size, Cohen’s d, was large.
To summarize, our results showed that both groups had signiﬁcant learning
gains after training on Cordillera. More importantly, although no signiﬁcant
diﬀerence was found in time on task and in the pre-test scores, the NormGain
group out-performed the InvNormGain group on the post-test, adjusted posttest, and NLG scores regardless of the grading criteria. Therefore, the overall
results show that the micro-level pedagogical tutorial decisions made a signiﬁcant
diﬀerence in the students’ learning.
Later a post-hoc comparison was done across the NormGain, Exploratory
and DichGain groups because the NormGain policies were induced from the
Exploratory and DichGain corpora. Despite the lack of random assignments, no
signiﬁcant diﬀerence was found among the three groups in the pretest. However,
the NormGain group signiﬁcantly outperformed both groups in the posttest,
adjusted posttest scores, and NLGs[5]. Similarly, a post-hoc comparison was done
across the InvNormGain, Exploratory and DichGain groups but no diﬀerence was
found among the three groups on pretest, posttest, adjusted posttest scores or
NLGs. The lack of a signiﬁcant diﬀerence among the InvNormGain, DichGain,
and Exploratory groups seemingly contradicts the initial predictions since the
InvNormGain strategies were speciﬁcally induced to enhance those decisions
that contribute less or even none to the students’ learning. Therefore, a lower
performance on the students’ part there than in at least the DichGain group,
which sought to enhance the tutorial decisions that contribute to the students’
learning, was expected. One possible explanation for the lack of diﬀerence among
the three groups is that the tutorial tactics employed by the DichGain- and
Random-Cordillera systems were ineﬀective and thus presented a minimum bar.
By ’ineﬀective’ it does not mean that they prevented the students from learning
but rather that they were not able to make a positive impact on their learning
above and beyond the baseline provided by Cordillera itself. Here the basic
practices and problems, domain exposure, and interactivity of Cordillera set
a minimum bar of students’ learning that the tactics, however poor, cannot
prevent. This is only a post-hoc explanation not a tested hypothesis, however it
merits further investigation.

5

Conclusion

In this study, students were randomly assigned to balanced conditions and received identical training materials and procedures apart from the tutoring tactics
employed. After spending the same amount of time on training, the NormGain
group outperformed the InvNormGain group in terms of posttest scores, the adjusted post-test scores and the normalized learning gains. This results support
the hypothesis that micro-step interactive tutorial decisions such as the Elicit vs.
Tell and Justify vs. Skip-justify decisions do aﬀect students’ learning. Therefore,

Do Micro-Level Tutorial Decisions Matter

233

future work is needed to investigate the induced NormGain and InvNormGain
tutorial tactics and ﬁnd out what actually caused these learning diﬀerences.
Moreover, this study also suggests that RL is a feasible approach for inducing pedagogical policies by using a relatively small human interaction corpus.
However, it is not trivial. The DichGain tutorial tactics, for example, did not
seem to be more eﬀective than the random decisions in Random-Cordillera. As
future work, we would like to explore the use of richer POMDP models, and do
additional empirical evaluation of the RL approach.
Finally, this study suggests that the ﬁne-grained interaction (micro-steps) of
human tutoring are a potential source of pedagogical power, but human tutors
may not be particularly skilled at choosing the right micro-steps. Given how
much computation we had to perform in order to learn which micro-steps were
best, it is hardly surprising that human tutors have not (yet) acquired similar
skill. This raises an interesting question: Can a NL tutoring system that is extensively trained be signiﬁcantly more eﬀective than expert human tutors? This
would be an excellent question for future research.
Acknowledgments. NSF (#0325054) supported this work and NSF (#SBE0836012) supported its publication. We thank Learning Research Development
Center for providing all the facilities for this work. We also thank Collin Lynch
and the reviewers for helpful comments.

References
[1] Aleven, V., Ogan, A., et al.: Evaluating the eﬀectiveness of a tutorial dialogue
system for self-explanation. In: Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.)
ITS 2004. LNCS, vol. 3220, pp. 443–454. Springer, Heidelberg (2004)
[2] Anderson, J.R.: The architecture of cognition. Harvard University Press, Cambridge (1983)
[3] Beck, J., Woolf, B.P., et al.: Advisor: A machine learning architecture for intelligent tutor construction. In: AAAI, pp. 552–557. AAAI Press, Menlo Park (2000)
[4] Bloom, B.S.: The 2 sigma problem: The search for methods of group instruction
as eﬀective as one-to-one tutoring. Educational Researcher 13, 4–16 (1984)
[5] Chi, M.: Do Micro-Level Tutorial Decisions Matter: Applying Reinforcement
Learning To Induce Pedagogical Tutorial Tactics. Ph.D. thesis, School of Art
& Science University of Pittsburgh (December 2009)
[6] Chi, M., Jordan, P.W., et al.: To elicit or to tell: Does it matter? In: Dimitrova,
V., Mizoguchi, R., et al. (eds.) AIED, pp. 197–204. IOS Press, Amsterdam (2009)
[7] Chi, M.T.H., de Leeuw, N., et al.: Eliciting self-explanations improves understanding. Cognitive Science 18(3), 439–477 (1994)
[8] Chi, M.T.H., Siler, S., et al.: Can tutors monitor students’ understanding accurately? Cognition and Instruction 22(3), 363–387 (2004)
[9] Collins, A., Brown, J.S., et al.: Cognitive apprenticeship: Teaching the craft of
reading, writing and mathematics. In: Resnick, L.B. (ed.) Knowing, learning and
instruction: Essays in honor of Robert Glaser, ch. 14, pp. 453–494 (1989)
[10] Evens, M., Michael, J.: One-on-one Tutoring By Humans and Machines. Erlbaum,
Mahwah (2006)

234

M. Chi, K. VanLehn, and D. Litman

[11] Forbes-Riley, K., Litman, D.J., et al.: Comparing linguistic features for modeling learning in computer tutoring. In: AIED, vol. 158, pp. 270–277. IOS Press,
Amsterdam (2007)
[12] Graesser, A.C., Person, N., et al.: Collaborative dialog patterns in naturalistic
one-on-one tutoring. Applied Cognitive Psychology 9, 359–387 (1995)
[13] Graesser, A.C., VanLehn, K., et al.: Intelligent tutoring systems with conversational dialogue. AI Magazine 22(4), 39–52 (2001)
[14] Katz, S., O’Donnell, G., et al.: An approach to analyzing the role and structure of
reﬂective dialogue. International Journal of AI and Education 11, 320–343 (2000)
[15] Moore, J.D., Porayska-Pomsta, K., et al.: Generating tutorial feedback with aﬀect.
In: Barr, V., Markov, Z. (eds.) FLAIRS Conference. AAAI Press, Menlo Park
(2004)
[16] Newell, A. (ed.): Uniﬁed Theories of Cognition. Harvard University Press, Cambridge (1994) (reprint edition)
[17] Putnam, R.T.: Structuring and adjusting content for students: A study of live and
simulated tutoring of addition. American Educational Research Journal 24(1), 13–
48 (1987)
[18] Sutton, R.S., Barto, A.G.: Reinforcement Learning. MIT Press/Bradford Books
(1998)
[19] Tetreault, J.R., Litman, D.J.: A reinforcement learning approach to evaluating
state representations in spoken dialogue systems. Speech Communication 50(89), 683–696 (2008)
[20] VanLehn, K.: The behavior of tutoring systems. International Journal AI in Education 16(3), 227–265 (2006)
[21] VanLehn, K.: The interaction plateau: Answer-based tutoring < step-based tutoring = natural tutoring. In: Woolf, B.P., Aı̈meur, E., Nkambou, R., Lajoie, S.
(eds.) ITS 2008. LNCS, vol. 5091, p. 7. Springer, Heidelberg (2008)
[22] VanLehn, K., Graesser, A.C., et al.: When are tutorial dialogues more eﬀective
than reading? Cognitive Science 31(1), 3–62 (2007)
[23] VanLehn, K., Jordan, P., et al.: Developing pedagogically eﬀective tutorial dialogue tactics: Experiments and a testbed. In: Proc. of SLaTE Workshop on Speech
and Language Technology in Education ISCA Tutorial and Research Workshop
(2007)

User Modeling and User-Adapted Interaction 12: 371^417, 2002.
# 2002 Kluwer Academic Publishers. Printed in the Netherlands.

371

Using Bayesian Networks to Manage Uncertainty
in Student Modeling
CRISTINA CONATI1, ABIGAIL GERTNER2 and KURT VANLEHN3
1
Department of Computer Science, University of British Columbia, Vancouver,
BC, V6T1Z4, Canada. e-mail: conati@cs.ubc.ca
2
The MITRE Corporation, 202 Burlington Road, Bedford, MA, 01730, USA.
e-mail: gertner@mitre.org
3
Department of Computer Science and Learning and Research Development Center,
University of Pittsburgh, Pittsburgh, PA, 15260, USA. e-mail: vanlehn@cs.pitt.edu

(Received 12 November 2001; accepted in revised form 24 May 2002)
Abstract. When a tutoring system aims to provide students with interactive help, it needs to
know what knowledge the student has and what goals the student is currently trying to achieve.
That is, it must do both assessment and plan recognition. These modeling tasks involve a high
level of uncertainty when students are allowed to follow various lines of reasoning and are
not required to show all their reasoning explicitly. We use Bayesian networks as a comprehensive,
sound formalism to handle this uncertainty. Using Bayesian networks, we have devised the probabilistic student models for Andes, a tutoring system for Newtonian physics whose philosophy
is to maximize student initiative and freedom during the pedagogical interaction. Andes’
models provide long-term knowledge assessment, plan recognition, and prediction of students’
actions during problem solving, as well as assessment of students’ knowledge and understanding
as students read and explain worked out examples. In this paper, we describe the basic mechanisms that allow Andes’ student models to soundly perform assessment and plan recognition,
as well as the Bayesian network solutions to issues that arose in scaling up the model to a
full-scale, ¢eld evaluated application. We also summarize the results of several evaluations of
Andes which provide evidence on the accuracy of its student models.
Key words. student modelling, Intelligent Tutoring Systems, dynamic Bayesian networks

1.

Introduction

One of the key elements that distinguishes Intelligent Tutoring Systems (ITS) from
more traditional educational systems is their ability to interpret student actions to
maintain a model of student reasoning and learning – the student model (Shute and
Psotka, 1996). Like user models for non-ITS software, the student model allows
an ITS to adapt the interaction to its user’s speciﬁc needs. However, unlike nonITS software, the ultimate goal of an ITS’s interventions is to help its users learn
a target instructional domain. This adds a great deal of uncertainty to the user modeling problem, because it entails inferring from the student’s actions the student’s
degree of mastery of the target domain knowledge, a process known as assessment
or knowledge tracing (Anderson et al., 1995). The uncertainty regarding the student’s

372

CRISTINA CONATI ET AL.

domain knowledge affects the second kind of modeling common to many adaptive
systems that aim to provide interactive help: plan recognition, or understanding what
the user is trying to do. The uncertainty in assessment inﬂuences plan recognition
because the student’s knowledge of the underlying plans affects which goals are most
likely to be the focus of the student’s attention.
In this paper, we describe how we use Bayesian networks (Pearl, 1988) as a unifying
framework to manage the uncertainty in student modeling. The models that we present
have been implemented in Andes, an intelligent tutoring system designed to help students learn Newtonian mechanics at the university level. Beside providing tailored
support during problem solving (VanLehn, 1996; Gertner and VanLehn, 2000; Schulze
et al., 2000), Andes also helps students study examples e¡ectively, a novel task that
adds new sources of uncertainty to the student modeling problem (Conati and VanLehn, 2000; Conati et al., 1997).
Awell-known, fundamental problem of both plan recognition and assessment is the
assignment of credit problem. If there are multiple explanations for a student’s action,
which is the most likely? That is, how much credit should be given to each explanation?
This problem is exacerbated if a system does not require students to explicitly express
all the steps in their reasoning, so that the actions that the system sees can be the
result of a chain of inferences rather than of a single inference. Early solutions to this
problem were based on heuristics or on maximizing coverage of positive evidence
while minimizing coverage of negative evidence (Reggia and D’Autrechy, 1990; Polk
et al., 1995). Recognizing the limitations of these approaches, other tutoring systems
opted to reduce the number of cases where multiple explanations of student actions
could occur. This is done by constraining students to follow a prede¢ned line of reasoning and requiring them to make all of their reasoning explicit (Anderson et al.,
1995). However, this kind of limitation of student initiative and freedom is not an
option for Andes. Andes was designed in collaboration with physics professors at
the US Naval Academy, who felt strongly that all possible correct solutions should
be acceptable to Andes, and that their students would simply not use a system with
an overly constraining interface.
One of the main contributions of Andes’ student model is that it provides a comprehensive solution to the assignment of credit problem for both knowledge tracing
and plan recognition, without reducing student initiative. Andes’ solution scales
up an approach ¢rst introduced in (Conati and VanLehn, 1996b). Because this
approach uses the same Bayesian network for solving both problems, the solutions
work together. Plan recognition in£uences assessment while at the same time assessment in£uences plan recognition, and these mutual in£uences are mathematically
sound. This is crucial for accurate student modeling in Andes, because students often
do not have knowledge of all the available plans.Thus, the assessment of what a student
knows should in£uence the probability of what solution the student is following,
and vice versa. Although knowledge assessment and plan recognition are thus strongly
related, very little research in plan recognition has leveraged this connection (Jameson,
1996; Carberrry, 2001).

USING BAYESIAN NETWORKS

373

A second contribution of Andes’ student model is that it supports prediction of
student actions during problem solving, in addition to knowledge tracing and plan
recognition. Because in physics each solution can be implemented through di¡erent
action orderings, being able to predict what inferences and actions a student can perform in the context of a particular solution greatly improves the help that the system
can provide when the student does not know how to proceed. While several systems
have used probabilistic methods to perform one or two of knowledge tracing, plan
recognition and action prediction (Jameson, 1996), Andes student model has been
the ¢rst to support all these modeling tasks at once.
Finally, deploying a system that supports example studying in addition to problem
solving, and that covers a full-semester physics course, required ¢nding
solutions within the Bayesian network framework to several other student modeling
problems. In the rest of the paper, we will show how the Bayesian network approach
allowed us to devise fairly simple solutions to these problems. However, it should
be stressed that Bayesian networks are just a notation, albeit a very powerful one.
The real solutions to the student modeling problems we faced are certain cognitive
and pedagogical hypotheses, as we will discuss. They are represented in the Bayesian
network framework, but that only gives them precision and not validity. The merit
of these hypotheses can only be established through extensive empirical testing
and re¢nement. The evaluations that we describe at the end of the paper are a start
in this direction.
The paper is organized as follows. First, the user interfaces of Andes are presented,
followed by a brief description of the modeling problems that will be discussed. After
this introduction, the main part of the paper describes Andes’ Bayesian networks
and how they are used for tutoring. The ¢nal part discusses evaluations of Andes,
related research, some unsolved problems and proposed future work.
1.1. THE TASK DOMAIN AND THE STYLE OF TUTORING
The Andes system is intended to help students learn Newtonian mechanics as it is
taught in introductory physics courses at the university level. Although such courses
have several instructional objectives, a common one is that students learn how to
solve physics problems that involve algebraic analyses of physical situations. For
instance, a simple problem is
‘A car starts moving with constant acceleration a at time T0 and after 30 seconds has moved D meters and reached a velocity V1 of 30 Km/h. What is the
acceleration of the car?’
This problem requires knowing what ‘acceleration’ and ‘velocity’ are, and how they
are related. These are the key physics concepts involved. It also requires knowing
that the method to solve this problem involves the equations describing motion with
constant acceleration, which in turn requires knowing the distance traveled in a given
time interval as well as the initial and ﬁnal velocity in that interval. However, the
students must also know some mathematical principles – for instance how to operate

374

CRISTINA CONATI ET AL.

with vectors. Thus, the knowledge that students must master to solve such problems
is a mixture of physics principles, mathematical principles and methods for applying
them. In the knowledge base that represents Andes’ model of the task domain, condition-action rules are used to represent all of these different kinds of knowledge.
Thus, we use ‘rule’ to refer to any of the relevant pieces of knowledge, regardless
of whether the content is procedural or conceptual, physical or mathematical.
Andes is not designed to teach a physics course all by itself. Students must still
attend all class meetings and read their textbook. Andes’ role is to assist learners with
two homework activities: studying solutions of correctly solved problems (examples),
and solving problems. These two activities often consume the largest portion of a student’s study time, so if Andes can increase students’ learning during problem solving
and example studying, then it should have a major positive impact on their overall
learning in the course.
Because students have seen problems being solved in class, they are not completely
naive when they start working with Andes.Thus, its pedagogical policy is to let students
do as much of the work as possible on their own. Andes o¡ers help only when students
ask or when it sees a particularly compelling reason to o¡er unsolicited help.
1.2.

ANDES’ PROBLEM SOLVING INTERFACE

The Andes interface for problem solving consists of two main entry panes (Figure 1)
in which students can draw vector diagrams (upper left) and enter equations (lower
right), as well as the variable deﬁnitions pane, located above the equation pane, and
the hint window, below the diagram pane on the left.
Andes provides two kinds of pedagogical interventions during problem solving:
immediate feedback and help. Every entry the student makes in the problem solving
interface, both in the diagram pane and the equation pane, receives immediate feedback by changing the color of the entry: green for correct and red for incorrect.
The two main types of help that Andes provides are:
. Error help. Students can select an incorrect (red) entry and can click on a button
labeled ‘what’s wrong with that?’ In response, Andes generates hints suggesting
what to focus on in order to ¢x the error.
. Procedural help. At any time, the students can ask for a hint on what would be a
good next step to take.

These types of help are designed to help the student learn. This means that they often
do not directly answer a student’s help request. Instead, they give a hint sequence
designed to encourage the student to work out as much of the answer on her own
as she can. The ¢rst hint often mentions only a goal or feature involved of the correct
step. Subsequent hints become more speci¢c. The ¢nal hint in the sequence, called
the ‘bottom-out hint’ (Koedinger and Anderson, 1993), indicates exactly what step
the student should enter.

USING BAYESIAN NETWORKS

375

Figure 1. Andes problem solving interface.

1.3.

ANDES’ EXAMPLE STUDYING INTERFACE

In addition to the problem solving interface, Andes provides students with an interface to study examples under the supervision of the SE-Coach (Conati and VanLehn,
2000; Conati et al., 1997). The main pedagogical goal of the SE-Coach is to get students to self-explain examples, that is to clarify to themselves why and how each line
of an example solution is derived. The SE-Coach is speciﬁcally designed to counteract the tendency that many students have to not self-explain, a tendency that can hinder effective learning from examples (Chi, in press). Thus, it needs to monitor
whether students self-explain spontaneously and encourage self-explanation for
those students who do not.
Since natural language understanding technology is still not powerful enough to
reliably monitor self-explaining expressed verbally or as free text, the SE-Coach interface includes two alternative mechanisms: a masking interface to track the student’s
attention and a set of menu-based tools. The menu-based tools allow students to
express self-explanations explicitly, if they want to. The masking interface allows
the coach to measure how long the student focuses on a particular solution step. Such
latency data helps the coach determine probabilistically when the student constructed
a self-explanation mentally, without entering it using the menu-based tools.
Figure 2 shows one of the SE-Coach’s examples (on the left) as it is presented with
the masking interface (on the right).Viewing a solution step in the example requires

376

CRISTINA CONATI ET AL.

Figure 2. A physics example (on the left) as presented in the masking interface (on the right).

moving the mouse cursor over the box that covers it.When the student uncovers a step,
a ‘self-explain’ button appears next to it, as a reminder to self-explain. Clicking on
this button generates more speci¢c prompts that suggest two kinds of self-explanations
known to be highly e¡ective for learning (Renkl, 1997):
1. explaining an example solution step in terms of domain principles (step correctness);
2. explaining the role of a solution step in the underlying solution plan (step utility).
The SE-Coach’s menu-based tools are designed to help students generate these two
kinds of explanations, if they have trouble doing so by themselves.
To explain step correctness, the student can activate a Rule Browser (see Figure 3a)
containing the hierarchy of physics rules represented in the Andes’ knowledge base,
and select the rule that justi¢es the uncovered solution step. To explain more about
the actual content of a rule, the student can activate a Rule Template. The Template
contains a partial de¢nition of the rule that has blanks for the student to ¢ll in
(see Figure 3b). This de¢nition is in terms of preconditions and consequences of
the rule application, re£ecting the de¢nition of the rule in the Andes’ knowledge base.
The student can complete the blanks in the de¢nition through selection in associated
menus of possible ¢llers (see Figure 3b).
To explain step utility, the student activates a Plan Browser, that displays a hierarchical tree representing the goal structure of the solution plan for a particular example.
The student must try to select the plan goal that most closely motivates the uncovered
solution step.
The SE-Coach’s interventions during example studying include immediate
feedback and explicit support on the self-explanation process. Immediate feedback
is provided as a red or green marking on every student’s entry in the self-explanation
tools.

USING BAYESIAN NETWORKS

377

Figure 3. Rule browser and a rule template.

Explicit support for self-explanation is provided only when the student tries to leave
the example. The support is given if the SE-Coach believes that the student would
bene¢t from further explanation of some of the example’s lines, either because the
student does not have su⁄cient knowledge of the rules necessary to explain the lines
or because he has not spent su⁄cient time reasoning about them. In this case, the
SE-Coach generates a warning and highlights the appropriate masking interface boxes
by coloring them pink. It also labels each box to suggest whether the student should
explain step correctness or step utility or both, as an additional support for the student
to explain those steps. The student is free to ignore the SE-Coach’s suggestions
and leave the example with no further action.
1.4.

ANDES’ BASIC APPROACH TO STUDENT MODELING

As mentioned earlier, because Andes allows students to pursue different correct solutions during problem solving, one of the main issues that Andes’ student model
needs to address is the assignment of credit for both plan recognition and assessment. That is, if a student action belongs to different solutions, which solution
and corresponding student knowledge should be credited for it? Andes’ student
model handles this issue through a technique based on Bayesian networks. The technique was pioneered by the OLAE system for off-line assessment (Martin and VanLehn, 1995), and was extended to on-line assessment and plan recognition by the
POLA student modeler, but only in prototype form (Conati and VanLehn, 1996a;
Conati and VanLehn, 1996b).
In order to illustrate the basic technique, which is fundamental to the operation of
the Andes’ student model, consider a simple case where there are just two explanations
for a particular student action. Suppose each explanation involves just one rule,
and the preconditions of both rules are satis¢ed directly by the state immediately prior
to the student’s action. This would be represented with the Bayesian network shown

378

CRISTINA CONATI ET AL.

Figure 4. A simple example of a Bayesian network for assignment of credit.

in Figure 4. The two parent nodes represent the rules, and the third node represents the
student’s action. The two parent nodes represent binary variables: true means the student has mastered the rule, and false means the student has not. Each rule node
has a prior probability of mastery, which is the system’s best estimate of the student’s
competence prior to seeing this action. The action node is also binary, and its value
represents whether the action either has or has not occurred. The action node has
a conditional probability table that says, in essence, if either parent rule is mastered,
then the action will probably be executed; if both parent rules are non-mastered, then
the action will probably not be executed. When the action is observed to occur,
the action node is clamped to its ‘occurred’ value, and the network is updated. This
causes the two parent nodes to acquire a marginal posterior probability, which represents the system’s new best guess about the student’s competence. If one node has
a much higher prior probability than the other then, by the Bayesian update rule,
it will get most of the credit for explaining the student’s action in that its posterior
probability will increase more than the other node’s.
This method of assigning credit is mathematically sound. Of course, whether it
accurately predicts student competence depends on both the cognitive model qua network topology (e.g., are there really just two rules?) and on the prior and conditional
probabilities in the nodes.
A Bayesian network can also handle the dual problem, assignment of blame. For
instance, suppose that two rules must both be applied in order to produce a certain
action. If the student cannot do the action, then the student is probably unable to
use one or both rules. This would also be modeled by the simple network of Figure
4, except that the conditional probability table for the action node would be di¡erent:
if both parents are mastered, then the action will probably be executed; if either is
non-mastered, then the action will probably not be executed.1 If the student is unable
to perform the action, then the action node’s value is clamped to ‘non-occurred’
and the network is updated. This causes the posterior probabilities of mastery on both
rules to be less than their prior probabilities. Because the student must be ignorant
of at least one of the rules, the network has ‘blamed’ both of them for the non-occurrence of the action. If one rule has a much lower prior probability of mastery, then
1

This is a simplification of Andes’ actual network structure (described in Section 2.2.2), but it reflects the
same basic update mechanism.

USING BAYESIAN NETWORKS

379

it is more likely to be the missing rule, so it gets more of the ‘blame.’ This is a mathematically sound technique for solving the assignment of blame problem.
Although these illustrations have emphasized determining probabilities of mastery,
which is the assessment part of the student modeling task, the same basic approach
can be used for plan recognition. One represents possible student goals as parent nodes
of observable actions, and the network will assign posterior probabilities to them that
indicate the likelihood that those goals (intentions) underlie student actions.
Most importantly, such Bayesian networks cause assessment and plan recognition
to interact seamlessly. Goals from explanations that involve probably mastered rules
receive more credit for observed actions (and less blame for non-observed actions)
than goals from explanations that involve probably unmastered rules. The mathematical soundness and the elegance of this approach motivated our adoption of it for
Andes.
However, Andes is a real-world tutoring application, and thus we had to solve many
technical problems to adopt the described approach for its student model. One
was simply scaling the technology up. Many actions may be involved in solving a physics problem, and each action can require many rule applications to explain it. This
makes the Bayesian network representing the problem solution quite large. Moreover,
since the same rule can be used many times in solving a problem, each network
can be highly interconnected. The size and topology of the networks could make them
di⁄cult to update in real time.
A second scaling problem is simply managing all the networks representing
the many di¡erent exercises covered by Andes, and their relationships. Since the
Andes system has over a 100 physics problems in it, it is not feasible to create each
problem’s network by hand. Moreover, the networks are all related because they
all share the same rule nodes, yet it is clearly infeasible to use one monolithic network
that spans all actions in all physics problems. Moreover, if even a small change in
the knowledge representation is required, it can a¡ect the networks of many
problems.
In addition to these problems of scale, realistic student modeling requires interpreting much more than just problem solving actions made by students. In particular,
Andes’ Bayesian networks must deal with the following issues:
1. Context speci¢city: knowledge is sometimes acquired ¢rst in a more speci¢c
form then generalized, thus making near transfer (i.e., transfer to very similar
application contexts) easier to obtain than far transfer (i.e., transfer to dissimilar
application contexts) (Singley and Anderson, 1989). How can we track the generality of competence?
2. Guessing: some actions are easier to guess correctly than other actions. How
should assignment of credit re£ect this?
3. Mutually exclusive strategies: some problems have multiple, mutually exclusive
solution strategies. Should evidence that the student is following one strategy
be interpreted as evidence that they are not following the other strategy?

380

CRISTINA CONATI ET AL.

4. Old evidence: how should evidence from earlier problems a¡ect the interpretation
of evidence from the current problem? If students learn a rule, then old evidence
that they were ignorant of the rule should be ignored. Should forgetting also
be explicitly modeled?
5. Errors: should errors of omission (missing actions) be interpreted the same way as
errors of commission (incorrect actions)? When can we assume that a student
does not know how to do an action?
6. Hints: when the student has received hints before entering a correct action, how
much credit should the goals and rules that explain that action receive?
7. Reading latency: when students study examples, they pause longer as they read
some lines than others, and this may be evidence of self-explanation. How
can we properly interpret the latency of reading times?
8. Self-explaining ahead: some students prefer to self-explain solution steps before
reading them in the example, while others prefer to read steps then self-explain
them (Renkl, 1997). How should such preferences a¡ect the interpretation of
reading latencies?
9. Self-explanation menu selections: when students use menus to express self-explanations, they may make a few errors and get feedback from the SE-Coach on
each, then enter a complete self-explanation. How should a sequence of such
menu selections be interpreted for knowledge assessment?
Although Bayesian networks were adopted in order to handle the assignment of
credit and blame problems, they simpliﬁed handling the problems listed above as
well. By using Bayesian networks, we were able to express sensible policies for handling those issues. We do not know if the individual policies are correct – that would
require testing each one in isolation – but the policies did function well enough as a
group that students using Andes learned more than students using more conventional instruction, as we will illustrate in a later section.
The remaining sections ¢rst describe Andes’ Bayesian networks in general, then
how they are used for coaching problem solving and for coaching example studying.
These technical sections are followed by a summary of our evaluation results, which
have been published in more detail elsewhere. The ¢nal sections discuss what we have
learned, especially about the interpretation issues listed above, and how Andes’ solutions compare to others in the literature. Our overall claim is that Andes is an existence
proof that sensible policies for handling tricky interpretation issues can be easily
expressed in Bayesian networks and yet the networks can still be updated in real time,
even when brought to the scale required by an ITS that has been used daily by hundreds
of students for most of a semester.

2. The Networks of Andes
One of the scaling problems that Andes’ Bayesian approach to student modeling presented is how to efﬁciently build the networks to cover the many exercises that Andes

USING BAYESIAN NETWORKS

381

provides. Rather than create a Bayesian network by hand for each problem and
example, Andes’ networks are created by running a problem solver that for each problem and example generates a data structure called a solution graph, which is then
converted to a Bayesian network. Thus, Andes uses the knowledge-based model construction approach to building Bayesian networks that has been actively investigated
in recent years (Martin and VanLehn, 1993; Breese, Goldman and Wellman, 1994;
Huber, Durfee and Wellman, 1994; Mahoney and Laskey, 1998).
For problem solving, Andes’ solution graph represents all the correct solution paths
of a problem. For example studying, the solution graph represents the single solution
path that an example describes. In the next sections, we describe how the solution
graph is created and how it is converted into a Bayesian network.

2.1. THE SOLUTION GRAPH REPRESENTATION
The solution graph structure used by Andes’ student model is generated prior to run
time by a rule-based physics problem solver. The rules are being developed in collaboration with four professors from the U.S. Naval Academy (Schulze, et al., 1998).
Andes’ knowledge base is built in CLIPS and contains 540 rules that can solve about
120 mechanics problems. The problems cover the ﬁrst 11 weeks of physics teaching
at the Academy and include most topics in introductory mechanics.
Like other rule based ITSs (e.g., Anderson et al., 1995), Andes rules encode a solution approach that uses goals to focus the problem solving and qualitative reasoning
to prepare for the more algebraic, quantitative reasoning. Figure 5a and Figure 5b

Figure 5. Sample rules in the Andes’ knowledge base.

382

CRISTINA CONATI ET AL.

show two sample rules involving goals, while Figure 5c and Figure 5d show two sample
rules encoding qualitative physics principles.
When a new problem or example is created for Andes, the problem author must
encode the givens for the problem in the language of the Andes problem solver.
For instance, if the problem states that
A block (A) of mass 50 kg rests on top of a table. Another block (B) of mass
10 kg rests on top of block A. What is the normal force exerted by the table on
block A?
the givens to the problem solver would include the proposition:
(SCALAR (KIND MASS) (BODY BLOCK-A) (MAGNITUDE 50)
(UNITS KG))
meaning that there exists a scalar quantity of type mass which is a property of block
A and has a magnitude of 50 kg. The problem author also encodes the problem goal,
specifying what quantity (or quantities) the problem asks the student to ﬁnd:
(GOAL-PROBLEM (IS FIND-NORMAL-FORCE)
(APPLIED-TO BLOCK-A) (APPLIED-BY TABLE) (TIME 1 2))
Starting from a problem’s givens and goals, the problem solver iteratively applies
rules from its rule set, generating sub-goals and intermediate results until all the
sub-goals have been achieved. These goals and results are then written to a solution
graph, along with the connections between them (e.g., if results 1 and 2 were both
used by rule 3 to generate result 4, then 1 and 2, along with rule 3, are recorded
in the solution graph as being parents of 4).
For instance, consider the example problem statement given above and
shown in Figure 6A. The problem solver starts with the top-level goal of ¢nding
the value of the normal force Nat. From this, it forms the sub-goal of using Newton’s
second law to ¢nd this value. Next, it generates three sub-goals corresponding to
the three high-level steps speci¢ed in the procedure to apply Newton’s second law
(SFi ¼ m a):
1. choose a body/bodies to which to apply the law,
2. identify all the forces on the body,
3. write the component equations for SFi ¼ m a.
From here, it continues to apply rules until it generates a partially ordered network of
goals and sub-goals leading from the top-level goal to a set of equations that are suf¢cient to solve for the sought quantity. When the solution graph is generated for a
problem, it encodes all the alternative plans and solutions that are acceptable to solve
the problem, so that Andes can monitor and support the student in the development
of any of these solutions. When the solution graph is generated for an example, it
encodes only the solution that the example presents, because this is the solution that
the student needs to understand.

USING BAYESIAN NETWORKS

383

Figure 6. A physics problem and a segment of the corresponding solution graph. Some nodes are omitted
for readability. Node preﬁxes (R, RA, G, F) indicate the type of the node, which will be discussed in detail
in Section 2.2.2.

Figure 6B shows a (simpli¢ed) section of the solution graph for the problem in
Figure 6A, involving the application of Newton’s second law to ¢nd the value of the normal
force. Each node in the solution graph represents a particular type of information (fact,
goal, rule, rule application), as indicated by the node pre¢xes. These node types will
be discussed in detail in the next section. We will use the example in Figure 6 to show
how the solution graph is automatically converted into a Bayesian network, and describe
the di¡erent types of nodes in the network and the relationships between them.
2.2.

STRUCTURE OF ANDES’ BAYESIAN NETWORKS

Andes’ Bayesian networks encode two kinds of knowledge: (1) domain-general
knowledge, encompassing general concepts and procedures that deﬁne proﬁciency
in Newtonian physics, and (2) task-speci¢cknowledge, encompassing knowledge related to a student’s performance on a speciﬁc problem or example.
The part of the Bayesian network encoding domain-general knowledge is built
when the Andes knowledge base is de¢ned, and its structure is maintained across
problems and examples. At any time, the marginal probabilities in this network represent Andes’ assessment after the last performed exercise.

384

CRISTINA CONATI ET AL.

The part of the network encoding task-speci¢c knowledge is automatically generated
from the solution graph of each problem or example when the student begins to work
on it. When the student has ¢nished the current exercise, the task-speci¢c part of the
network is discarded, and the updated domain-general marginal probabilities are
‘rolled-up’ as priors for the next exercise. That is, Andes uses a dynamic belief network
(Dean and Kanazawa, 1989; Russell and Norvig, 1995), to solve a second scaling problem
of its student modeling approach: how to maintain a network of manageable size that
still accurately represents the complete history of a student’s interaction with the system.
The following sections describe the structure of the networks, how roll-up is done,
and how the conditional probabilities in the networks were determined.
2.2.1. The Domain-general Part of the Bayesian Student Model
The domain-general part of the network represents student long-term knowledge, via
two different types of nodes: Rule and Context-Rule. Every node has two values,
mastered or unmastered. For Rule nodes, mastery means that the student should
be able to apply that piece of knowledge correctly whenever it is required to solve
a problem, i.e., in all possible contexts. However, this simple mastered/unmastered
distinction is only a crude model of human learning. Even when learning just a single
fact, humans ﬁnd it easier to remember the fact if the context of recall matches the
context in which it was learned (Tulving and Thomson, 1973). When people learn a
whole skill, many studies (reviewed in (Singley and Anderson, 1989)) have found that
they can solve problems similar to the ones they were trained on (near transfer) long
before they can solve problems that are dissimilar (far transfer). In short, as a general
rule of thumb, knowledge is ﬁrst acquired in a speciﬁc form then gradually generalized, an issue we referred to as context speci¢city in Section 1.4.
Andes uses the distinction between Rule nodes and Context-Rule nodes in order to
model the context speci¢city of student knowledge. A Rule node represents a piece
of knowledge in its fully general form. A Context-Rule node represents mastery of
a rule in a speci¢c problem solving context.
In order to represent the relationship between the general and speci¢c versions of
a piece of knowledge, every Context-Rule node has one parent, the Rule node representing the corresponding general rule (see Figure 7). This structure is similar to the one
used by Hydrive (Mislevy and Gitomer, 1996), where the parent rules represented
general knowledge (e.g., hydraulics) and the child rules represented applications of
the knowledge in speci¢c, named contexts (e.g., the landing gear; the cockpit canopy).
The conditional probability table of a Context-Rule given its parent Rule is de¢ned
as follows:
. PðContext-Rulei ¼ MasteredjRule ¼ MasteredÞ is always equal to 1, because by de¢nition PðRule ¼ Mastered) means that the student can apply the rule in every context.
. PðContext-Rulei ¼ MasteredjRule ¼ Not masteredÞ represents the probability that
the student can apply the general rule in the corresponding context even if
she cannot apply it in all contexts.

USING BAYESIAN NETWORKS

385

Figure 7. Relation between rule and Context-Rule nodes.

This network structure represents a hypothesis about transfer. Rule nodes in the
domain-general part of the network are never directly observable, so sibling Context-Rules are always conditionally dependent. This means that user interface actions
that increase the probability of mastery of one Context-Rule will tend to increase
the probability of the parent Rule and thus increase the probability of mastery of sibling
Context-Rules. This is an empirically testable hypothesis about physics learning.
In particular, results on transfer could be used to adjust the conditional probabilities
in the Context-Rules, or even the topology of the network. Thus, the network provides
a particularly simple, precise representation of a‘mini-theory’of transfer, which invites
testing but does not replace it.
2.2.2. The Task-speci¢c Part of the Bayesian Student Model
The task-speciﬁc part of the Bayesian student model contains Context-Rule nodes
and four additional types of nodes: Fact, Goal, Rule-application and Strategy nodes.
All the nodes are read into the network from the solution graph for the current problem or example when the student opens it. The structure of the solution graph
already encodes the inferential structure of the problem solutions and therefore is
preserved isomorphically in the Bayesian network. For instance, a segment of the
Bayesian network created for the problem in Figure 6A corresponds exactly to the
solution graph segment in Figure 6B, where Context-Rule, Fact, Goal, Rule-Application and Strategy nodes are labeled respectively by the preﬁxes R-, F-, G-, RA
and S-.

2.2.2.1 Fact and Goal nodes. Fact and Goal nodes (collectively called Proposition
nodes) look the same from the point of view of the Bayesian network. They both represent information that is derived while solving a problem by applying rules from the
knowledge base. The di¡erence between Goal and Fact nodes is in their meaning
to Andes’ help system: the probabilities of Goal nodes will be used to construct tutorial interventions focused on the planning of the solution, while the probabilities
of Fact nodes will be used to provide more speci¢c interventions on the actual solution
steps.
Proposition nodes have a binary value indicating whether they are inferable. If a
node has the value T, it represents that the student either has already inferred that
fact/goal or can infer it given her current knowledge.

386

CRISTINA CONATI ET AL.

Figure 8. Probabilistic relations among nodes in the task-speciﬁc part of the Bayesian network.

Proposition nodes have as many parents as there are ways to derive them. Thus, if
there are two di¡erent rule applications that conclude the same fact, then the corresponding Fact node will have two parents. This is the basic structure that Andes networks use to handle the apportion of credit problem.
The conditional probabilities between Proposition nodes and their parents are
described by a Leaky-OR relationship (Henrion, 1989), as shown in the lower part
of Figure 8. In a Leaky-OR relationship, a node is true if at least one of its parents
is true, although there is a non-zero probability b of a ‘leak,’ i.e. that the node is true
even when all the parents are false.
The leak probability b is Andes’ solution to the guessing problem mentioned
in Section 1.4. It represents the likelihood that the student obtains the proposition
via guessing, asking a friend for help, or some other method that is not modeled
by the network. The leak probability can be adjusted to represent how easy it is
to correctly guess a fact. For instance, a fact that asserts that an object’s acceleration is constant is easier to guess than one that asserts the acceleration’s direction,
because acceleration is either constant or non-constant, whereas there are many more
possibilities for its direction. Thus, our Bayesian framework allows us to concisely
express a precise hypothesis about guessing, although empirical work is required
to test it.

2.2.2.2. Rule-application nodes. Rule-application nodes connect Context-Rule
nodes, Proposition nodes, and Strategy nodes (described later) to new derived
Proposition nodes. The parents of each Rule-application node include exactly one
Context-rule, some number of Proposition nodes corresponding to the Contextrule’s preconditions (see Figure 8) and, optionally, one Strategy node.
Rule-application nodes have a binary value indicating whether the student is capable of applying the rule or not. A Rule-application node’s value is T if the student
has applied or can apply the corresponding Context-Rule to the propositions representing its preconditions. The probabilistic relationship between a Rule-application

USING BAYESIAN NETWORKS

387

node and its parents is a Noisy-AND (Henrion, 1989). The Noisy-AND relation
models the following assumptions
. in order to apply a rule to derive an element in a problem or example solution, the
student needs to know the rule and all of the solution elements corresponding
to the rule preconditions.
. there is a non-zero probability a, the noise in the Noisy-AND, that the student will
not apply the rule even if the rule and all its preconditions are known.
The noise parameter a represents the fact that even if a Context Rule is mastered
and all its preconditions are known, a student may fail to apply it sometimes, a phenomenon referred to as a slip (Norman, 1981). Thus, a should be viewed as part
of the de¢nition of mastery, because it represents the expected frequency of slips. Once
again, the network allows us to precisely represent a testable hypothesis.

2.2.2.3. Strategy nodes. Because Andes allows students to follow di¡erent correct
solutions to a problem, it needs to deal with the issue we labeled as mutually exclusive
strategies in Section 1.4. Suppose we have the simple situation shown in Figure 9,
in which there are two rule applications, both achieving the same goal as part of
two di¡erent solutions. Suppose somewhere beneath the left rule application, an action
occurs. This raises the posterior probability of both the left rule application and
the goal. However, the credit £owing to the goal also raises the posterior probability
of the right rule application. If the two rule applications are actually mutually exclusive
means for achieving that goal, then raising the posterior probability of one should
lower the posterior probability of the other. In order to model this, the network topology was augmented with Strategy nodes, extending a solution ¢rst proposed in (Conati and VanLehn, 1996a).
A Strategy node is paired with a Goal node when there is more than one mutually
exclusive way to address the goal. The children of Strategy nodes are the Ruleapplication nodes that represent the implementation of the di¡erent strategies. In
Figure 6B, for instance, the Strategy node S-choose-body-strategy is paired with the
goal of choosing a body, and points to the application nodes representing the decisions
to choose block A and block B as separate bodies (node RA-choose-separate-bodies

Figure 9. A simple Bayesian network illustrating the lack of mutual exclusion.

388

CRISTINA CONATI ET AL.

in Figure 6B) and to choose as a body the compound of the two blocks (node RAchoose-compound-body in Figure 6B).
A Strategy node has two or more values, one for each of its children. The prior
probabilities of these values indicate the frequency with which students tend to choose
the corresponding strategies. A rule application node that is the child of a Strategy
node has a 1.0 probability of applying if the Strategy node has the value corresponding
to it, and a zero probability of applying otherwise. In short, the mutual exclusion
of strategy node values is used to enforce the mutual exclusion of rule applications.
2.2.3.

Calibration: Determining Prior Probabilities

Every node in a Bayesian network has a conditional probability table whose numbers must be determined before the network can be used. As the earlier discussion
indicated, many of these numbers calibrate hypotheses about the context speciﬁcity
of learning, guessing and so on, and thus the numbers ought to be determined via
experimentation designed to test these speciﬁc hypotheses. Because the same parameters appear many times in the network, there are many fewer parameters to estimate than there would be if the network were not constrained by these hypotheses
and every cell in every conditional probability table had to be determined separately.
Moreover, the parameters have meaning apart from their occurrence in the network.
For instance, a represents the frequency of slips for a mastered rule. When a parameter cannot be estimated from data, having a speciﬁc meaning makes it possible
to base estimates for it on results from the literature. Even subjective estimates are
probably improved when the parameters have well-deﬁned meanings.
Although the hypotheses cover most of the conditional probabilities in the network,
they do not cover the probabilities of the Rule nodes. Since a Rule node has no parents,
its conditional probability table represents the prior probability of the student’s mastery of the rule. Such priors are especially important, because, as illustrated earlier,
when two rules compete to explain evidence, the rule with the higher prior probability
receives more credit, all other things being equal. Similarly, rules with lower priors
get more blame for missing actions. Data from multiple-choice tests were used to
set the prior probabilities on 66 of the Rule nodes (VanLehn et al., 1998). The other
474 rules did not apply in the problems on the test, so we could not set their prior probabilities empirically. Their priors were set to 0.5.
Even though we had to use subjective judgments to set most of the parameters in the
network, this is still an advance over a purely heuristic approach since at least the
structure of the network is theoretically grounded and the calculations are sound.
We have conducted an evaluation of the sensitivity of the networks’assessment to their
parameters, and the results are summarized in a later section.
2.2.4.

Rollup: Summarizing Past Evidence

Although the interpretation of a student’s actions should be affected by all the
evidence seen so far from the student, it is not practical to use one huge Bayesian

USING BAYESIAN NETWORKS

389

network that spans all the problems that the student has worked on. As mentioned
earlier, we use Dynamic Bayesian Networks (Dean and Kanazawa, 1989; Russell and
Norvig, 1995) to reduce the size of the network that models each individual interaction with Andes. The roll-up mechanism in dynamic Bayesian networks allows one
to periodically summarize the constraints imposed by older data, then prune away
the network that interpreted that data. In particular, Andes keeps the structure of
the domain-general part of the network intact over all exercises, but it prunes away
the task-speciﬁc part of the network as soon as that task is exited by the student.
However, the posterior probability of each Rule node in the task-speciﬁc network
for the last solved exercise is ‘rolled-up’ in the domain-general network, to become
the prior probability of that Rule node in the task-speciﬁc network for the next problem or example that uses it.2
If one had a vast Bayesian network where actions in all problems were children of
the same rule nodes, then evidence from past problems would have the same weight
as evidence from recent ones. However, students may learn or forget rules. Thus,
recent evidence should count more than old evidence when determining which rules
are currently mastered. For instance, one could use an explicit ‘window,’ so that only
evidence that is recent enough to ¢t in the window are used to update the probabilities
of mastery. Corbett and Anderson (Corbett and Anderson, 1995) have developed
a simple model of learning that makes transition to rule mastery at time T a function
of both the evidence and the probability of mastery at time T1. Basically, given
the same evidence, the probability of a rule being mastered at time T is higher if
it was mastered at time T1 than if it was not mastered at time T1. This represents
hypotheses about the relative rates of learning and forgetting. We achieve the same
e¡ect in Andes by using the simple roll-up mechanism of changing posterior marginal
Rule probabilities to priors. This causes rules with high posterior probability of mastery at time T1 to attract slightly more credit for the evidence at time T, thus causing
them to have slightly higher probability of mastery at time T than they would have
it they had had low posterior probability of mastery at timeT1 and the same evidence
were present. Thus, the old evidence still has some in£uence, although the rise in
probability of mastery is determined mostly by the strength of the new evidence. Mislevy and Gitomer (Mislevy and Gitomer, 1996) propose essentially the same solution
to modeling learning, but appear not to have implemented it.

2
This simple roll-up mechanism loses any dependency among rules encoded in the task-specific part of
the network. For instance, if two rules both explain an action (as in Figure 4), they are conditionally dependent in the task-specific network. When the task-specific network is pruned away, the conditional dependency is lost. This loss of information problem can be addressed by determining the main conditional
dependencies among pairs of Rule nodes with a Chi-squared technique, then installing new nodes in the
domain-general part of the network, whose conditional probability tables encode the strengths of the dependencies (Martin and VanLehn, 1994). However, experimentation with OLAE suggested that in our task domain, the conditional dependencies are usually not very strong. Thus, Andes does not currently use this
more sophisticated roll-up technique.

390

3.

CRISTINA CONATI ET AL.

Probabilistic Student Modeling for Problem Solving

The preceding section discussed some basic aspects of Andes networks that are the
same for both problem-solving and example-studying. This section discusses how the
network is used during problem solving. It also describes a few structural details that
are unique to the problem-solving networks. The next section presents a similar discussion for the example-studying phase.
3.1.

EXAMPLE OF PROPAGATION OF EVIDENCE IN THE STUDENT MODEL FOR PROBLEM
SOLVING

In order to illustrate the operation of Andes’ student model during problem solving,
suppose that a student is trying to solve the problem in Figure 6A and that her
ﬁrst action is to select block A as the body. In response to this action, the fact node
F-A-is-body is clamped to T. Figure 10 shows the marginal probabilities in the
network before and after the action (probabilities on RA nodes have been omitted

Figure 10. The network before and after observing A-is-a-body. The ﬁrst number given for each node
shows the probability of that node before observing A-is-a-body. The second number shows how the probability changes after observing A-is-a-body.

USING BAYESIAN NETWORKS

391

for simplicity). Before the evidence is entered, these marginal probabilities reﬂect the
prior probabilities on the rule nodes (0.5 each), the high prior probability of the
given Goal node (G-ﬁnd-Nat), and the priors on the Strategy node, which indicates
that the two strategies are equally likely a priori.
After the evidence is entered, the marginal probabilities of the non-given Fact and
Goal nodes involved in deriving the observed action may seem rather low. However,
for each proposition, the network evaluates the probability that it was derived vs. guessed. The more rules involved in deriving a proposition, the lower the likelihood that
it was derived, since lack of mastery of any one of the rules would prevent its derivation.
The likelihood that a proposition is guessed is proportional to the parameter b in
its leaky-OR conditional probability table. In this example, all the rules have rather
low prior probabilities, so their products are even lower, thus giving more credit
to the possibility of guessing and keeping the posterior probabilities of the corresponding propositions low.
These remarks are just a crude summarization of what actually happens when the
network is updated. Nonetheless, they suggest how complicated it would be to do
the same interpretation without Bayesian computations.
In addition to changing the probability of the ancestor nodes of an observed action,
updating the network also changes the probabilities of non-ancestor nodes, thus predicting what inferences the student is likely able to make after that action. In particular,
the relationship enforced by the Strategy node S-choose-body-strategy on its children
will slightly diminish the probability of the Goal node G-de¢ne-compound-AB and
increase the probability of the Goal node G-de¢ne-bodies-A-B. This last change will
propagate downward to increase the probability of the Fact node F-B-is-body, while
the increased probability of the Fact node A-is-a-body will slightly increase the probability of the Goal node G-de¢ne-Forces-on-A, as shown in Figure 10. Using the sound
Bayesian propagation algorithm to predict the probability of student inferences is
one of the several advances that Andes’ student model brings to the basic Bayesian
approach adopted by its predecessors OLAE and POLA. It provides the problem
solving coach with a more principled, albeit still heuristic, mechanism to provide
its help, as we will illustrate in a later section.
3.2.
3.2.1.

INTERPRETATION PROBLEMS THATARE UNIQUE TO THE PROBLEM SOLVING COACH

Errors of Omission and Errors of Commission

Whenever a student enters a correct action, the corresponding Fact node is clamped
to T, indicating that the student is capable of doing that action. What should the
tutor do when the student’s action is incorrect (an error of commission) or missing
(an error of omission)? Should a Fact node be clamped to F?
Let us ¢rst consider the interpretation of errors of omission. If the tutor required
that a certain action be done, and the student did not do it, then the Fact node corresponding to that action should be clamped to F. However, due to Andes’ philosophy
of allowing students to perform steps in any order and even to skip steps, there

392

CRISTINA CONATI ET AL.

are only a few actions that Andes explicitly requires. In most cases, if the student has
not done an action, she may still be able to do it. Indeed, she may even have done
the action mentally or on a piece of scratch paper. Thus, Andes rarely clamps nodes
to false due to missing actions.
For errors of commission, where the student makes an incorrect action, there are
two cases. If the error implies that the student disbelieves a certain correct fact,
then the corresponding Fact node is clamped to F. For instance, if the correct fact
is that a vector is horizontal, and the student draws the vector pointing straight
up, then the student undoubtedly believes that the vector is not horizontal so Andes
clamps the horizontal Fact node to F. This ¢rst case, where the error implies disbelief
in a speci¢c correct fact, is relatively uncommon. The more common case is that
the student’s error is not inversely associated with a correct Fact. For instance,
if a student enters an incorrect equation, it is often not clear what correct equation
the student is trying to enter. A contradiction must be quite blatant (e.g., a vector
cannot point both horizontally and vertically at the same time) before we can
con¢dently infer that belief in one side of the contradiction implies disbelief in
the other side. Thus, only in a few cases will Andes clamp nodes false due to errors
of commission.
3.2.2.

Updating the Student Model After a Hint

An ITS must take into account the hints that it has given when interpreting student
actions and updating the student model. Typically, a student will ask for hints down
to a certain level of speciﬁcity before taking the action suggested by the hint. Thus,
the student model should interpret student actions taken in response to a hint differently depending on the hint’s speciﬁcity.
This problem has been solved di¡erently in di¡erent ITSs. For instance, the CMU
Cognitive Tutors (Anderson et al., 1995) do not count actions that are preceded by
hints for the purposes of knowledge tracing. The probabilities of mastery rise
only when the student enters an action without help from the tutor. Perhaps the most
elaborate and potentially accurate method of interpreting hints is used by the SMART
ITS (Shute, 1995), which uses a non-linear function derived from reports by experts
to boost the level of mastery by di¡erent amounts, depending on the speci¢city of
the hint and the level of mastery before the hint was given.
Our approach implements a simple theory of hints directly in the Bayesian network.
The theory is based on two assumptions: ¢rst, hints are worded so as to remind
the student of the requisite knowledge, rather than teach it. (Teaching missing pieces
of knowledge is handled by the Conceptual Help system (Albacete and VanLehn,
2000), which is not discussed here.) Thus, procedural hints do not directly cause students to master knowledge. Second, a strong hint increases the chance that the student
can guess the next action rather than derive it from knowledge. Thus, a hint can
cause an entry directly. In other words, hints a¡ect actions directly but not domain
knowledge.

USING BAYESIAN NETWORKS

393

This mini-theory of hints is encoded in the Bayesian network as follows.Whenever a
hint has been given for a Proposition node, a new node is added as its parent, representing the fact that a hint was given. The conditional probability table of the Proposition node is modi¢ed so that the node may be true if it was derived either
via the application of a known rule, or via guessing based on the hint. Moreover,
the higher the speci¢city level of the hint, the more likely it is that the target node
is true. In operation, this means that when the student makes the corresponding entry,
the hint node ‘explains away’ some of that evidence, so the probability of mastery
of the requisite knowledge is not raised as much as it would be if the student made
that entry without receiving a hint.
3.3.

USING THE NETWORK TO GENERATE HELP

Although the focus of this paper is on the construction of Andes’ student model, it is
worth a moment to explain how the student model is used to provide support during
problem solving. Because Andes’ policy is to help students solve problems their own
way, rather than in a predeﬁned optimal way, when a student asks for help Andes
needs to ﬁgure out what goal the student is probably trying to achieve (plan recognition) and which is the action the student is stuck on because of lack of the appropriate knowledge (action prediction).
Thus, after a help request, the procedure for selecting a hint topic (Gertner, Conati
and VanLehn,1998) starts from the most recentlyobserved node in the Bayesian network
and searches upward, looking for a goal that the student is most likely to be pursuing.
It then searches downward from that goal node, looking for a rule application with
a low probability of being performed. Such a rule application is probably where the
student is stuck for lack of knowledge, so it becomes the focus of the coach’s hint.
The search through the network is necessary because the semantics of aTvalue on a
Goal node do not indicate whether the goal has been achieved yet or whether the
student intends to achieve that goal next. It only indicates that the goal can be derived
by the student at this time. As a consequence, when Andes is searching for a goal
that the student is probably trying to achieve, it must explicitly check whether all
the actions beneath the goal have probably been done. If so, that goal has probably
been achieved already, and Andes should search for a di¡erent goal. This is an uncertain decision, as students are allowed to skip actions and do them mentally. Thus,
even if some of the actions beneath a goal have not actually been done, the goal might
still be considered achieved by the student. This uncertainty is currently handled
by heuristics in the search algorithm rather than by the Bayesian network itself.
The Bayesian network is also used to decide whether the student should be given
instruction on a rule. If Andes selects a certain rule application for a hint, and
the posterior probability of the corresponding Rule node is below a set threshold,
then it calls another coaching module, the Conceptual Helper, to give a mini-lesson
on that rule (Albacete and VanLehn, 2000). Otherwise, it just gives hints designed
to jog the students’ memory and refocus their attention.

394

CRISTINA CONATI ET AL.

The posterior probabilities on the rules could also be used to guide the selection of
exercises for the student. In particular, Andes could choose a problem that involves
only a few rules that the student has not yet mastered, on the theory that such problem
would be challenging but not too di⁄cult, and thus would maximize the change
of learning the unmastered rules. However, when Andes was evaluated at the US Naval
Academy, we decided to have Andes assign the same problems to all students so that
the evaluation would be more controlled.

4. Probabilistic Student Modeling for Example Studying
In order to help students study examples more effectively via self-explanation, the
Andes’ student model for example studying needs to assess how well a student is
self-explaining the current example and to identify those parts that may beneﬁt from
further self-explanation.
This modeling task involves challenging new student modeling issues. First, the
student model must be able to assess whether and how students self-explain even when
the students do not build their explanations with the SE-Coach interface. This must,
be done using only limited data on students’ attention and estimates of the students
domain knowledge. Second, because some students tend to ‘self-explain ahead’
parts of the examples that they have yet to uncover (Renkl, 1997), attention to a given
example line can indicate not only self-explanation of that line, but also of subsequent
lines. Third, because the SE-Coach menus provide strong sca¡olding to build selfexplanations, it is unclear how much credit a student’s physics knowledge should
get for a correct self-explanation that the student produced through these menus.These
are the issues that were labeled, respectively, reading latency, self-explaining ahead
and self-explanation menu selection in Section 1.4. In the following sections, we
describe how we extended Andes’ Bayesian networks for problem solving to handle
the additional interpretation challenges related to these issues.
4.1. THE BAYESIAN NETWORK FOR EXAMPLE STUDYING
The task-speciﬁc part of the Bayesian network for example studying is derived from
an example’s solution graph using the same mechanism that we described in Section
2.2. For the SE-Coach, this network represents a model of correct self-explanation
(SE model) for that example. In particular, because Rule-application nodes in the
Bayesian network encode how solution steps in the example derive from physics
and planning rules, they represent exactly the self-explanations for correctness and
utility that the SE-Coach targets, and that we introduced in Section 1.3:
. how a solution step can be explained in terms of domain rules (step correctness)
. what goal each solution step achieves in the example solution plan (step utility).
For instance, Figure 11b shows the segment of SE model for the part of the example
shown on the left of the ¢gure. The Rule-application node RA-body-by-force in

USING BAYESIAN NETWORKS

395

Figure 11. Segment of student model for the portion of example shown to the left. Part (b) of the ﬁgure
shows the model structure before any student’s action. The two numbers under each node indicate the
node probability before and after the student’s actions, represented by the nodes in part (a). The letters
labeling some of the links are used to refer to the links in the text.

Figure 11b encodes the explanation that Jake is chosen as the body because a physics
rule says that if we want to ¢nd a force on an object, that object should be selected
as the body to which to apply Newton’s 2nd law. The node RA-goal-choose-body
encodes the explanation that choosing Jake as the body ful¢lls the ¢rst subgoal of
applying Newton’s 2nd law, i.e., selecting a body to which to apply the law.
This interpretation of the task-speci¢c part of the Bayesian network as a model of
correct self-explanation for the current example requires a semantics that di¡ers from
the semantics in the networks for problem solving. The Boolean values of Proposition
nodes in the Bayesian networks for example studying represent the probability that
the student is aware of the corresponding facts or goals. The Boolean values of
Rule-application nodes represent the probability that the student has self-explained
the corresponding derivations. In particular, the probability
PðRA ¼ T j all parents ¼ TÞ ¼ 1  a

ð1Þ

(where a is the noise parameter in the Noisy-And relation for Rule-application
nodes, described in Section 2.2.2.2) is used in the student model to address the
issue of self-explaining ahead. This probability represents a student’s tendency to

396

CRISTINA CONATI ET AL.

self-explain an inference as soon as she has the knowledge to do so (Renkl, 1997).
Hence, a student who tends to self-explain ahead will be characterized by a low a
in the Noisy-AND relationship, while a student who tends to self-explain a line only
upon reading it in the example will be modeled by a high value of a. Although the
value of a generally depends on the student only,3 it can sometimes also depend
on the particular Context-rule in question. For instance, most students taking introductory physics understand whether they need to apply Newton’s 2nd law as soon as
they read a problem’s statement. Thus, the Context-rule R-try-Newton-2law in
Figure 11b is generally associated with a low a .
Prior probabilities of root Proposition nodes in the Bayesian network, representing
the example given, are set to 0 until the student starts reading the example text
(see for instance the ¢rst number under the node G-force-on-Jake in Figure 11b).
As a student performs reading and self-explanation (SE) actions in the SE-Coach
interface, the initial Bayesian network is dynamically updated with nodes representing
these actions. Figure 11a, for instance, shows the nodes that are added to the network
after a student
1. viewed the example line ‘Find the force exerted on Jake by the rope’ long enough for
reading it (node Read-L1);
2. viewed the line ‘To solve this problem, we choose Jake as the body’ somewhat longer
(node Read-L2).
3. self-explained ‘To solve this problem, we choose Jake as the body’ with the Plan
Browser (node pb-choose-body).
The second number under each node in Figure 11b shows the updated probability of
that node after the student’s actions in Figure 11a. In the next sections, we describe
how Andes student model uses the Bayesian network formalism to interpret reading
time and use of the self-explanation menus.
4.1.1.

Interpreting the Student’s Reading Time

As mentioned earlier, one of the main problems for the SE-Coach student model is
how to interpret student reading latency to assess self-explanation. The basic hypothesis we implemented in the student model is that the probability of self-explaining
a rule application is a function of both the time a student spends studying an example line derived from that rule application and the probability that the student knows
the rule and its preconditions. This section describes how that hypothesis is encoded
in the Bayesian network.
Read nodes (nodes with pre¢x Read in Figure 11a) are dynamically added to
the student model to represent viewing example lines in the SE-Coach masking interface. The values of Read nodes are LOW, OK or LONG. They re£ect the duration
of viewing time, and are assigned by comparing the total time a student spent viewing
3

This parameter could be set, for instance, by observing how a student studies a test example.

USING BAYESIAN NETWORKS

397

an item (TVT) with the minimum time necessary to read that item (MRT). We currently compute the MRT for each item by assuming 3.4 words per second, which
is the reading speed of an average-speed reader4 (Just and Carpenter, 1986). Depending upon the result of the comparison, the value of a Read node can be LOW
(TVT  MRT, time insu⁄cient for reading), OK (TVT  MRT, time su⁄cient
for reading only) or LONG (TVT 	 MRT, time su⁄cient for self-explanation).
Each Read node connects to the Proposition node re£ecting the semantic content of
the viewed item (e.g., link A and E in Figure 11). These links indicate that viewing
time in£uences the probability of being aware of the related content.When a Proposition node has input from both a Read node and a Rule-application node (e.g., Fjake-is-the-body in Figure 11b), then a student can acquire the corresponding proposition either by reading it in the example or by deriving it from rules and previous
propositions.
This relationship is represented in the conditional probability table for Proposition
nodes (see Table 1). If the parent Rule-application node is T (i.e., the student explained
the corresponding derivation), the proposition node is T (i.e., known by the student).
Otherwise, the numbers in the table indicate that the probability of knowing the Proposition node depends on reading time: it is small if reading time is LOW, high when
the time is OK, and even higher when the reading time is LONG.
Some Proposition nodes in the Bayesian network may not be connected to a Read
node, even after the student has viewed all the elements in the example solution. This
happens when the worked out solution omits some of the solution steps. In Figure
11, for instance, the nodes G-try-Newton-2law and G-goal-choose-body cannot have
any Read node pointing to them, because these goals are not explicitly mentioned
in the example solution. A student can become aware of unmentioned propositions
only by deriving them.
The fact that the student viewed an example item does not necessarily mean that the
student self-explained it. However, the longer the student viewed an example item,
the higher the probability that the student self-explained it, provided that the student
has su⁄cient knowledge to do so.5
This relationship is encoded in the student model by linking the Read node that
represents viewing an example line with the Rule-application nodes that represent
self-explanation for correctness (e.g., link C in Figure 11) and self-explanation for utility (e.g., link D in Figure 11) for that line. The conditional probability table for these
Rule-application nodes is modi¢ed to take reading time into account, as shown in
4
More accurate values for this parameter could be obtained by explicitly testing the reading speed of individual students before they start using the SE-Coach.
5
This relationship between probability of self-explanation and viewing time relies on the assumption
that viewing time reflects time spent thinking about a given example line. This assumption is of course not
always true, especially because we are currently measuring viewing time indirectly through the time a line
stays uncovered. The assumption would be more accurate if we had a way to track the student’s gaze to
measure the student’s reading patterns more accurately, or if the probability of self-explanation was also
conditioned on a node representing a student’s tendency to self-explain, determined a priori. We plan to
explore these extensions in future versions of the SE-Coach.

398

CRISTINA CONATI ET AL.
Table 1. Conditional probability table for a proposition node viewed by the student.
READ

LOW

OK

LONG

Rule application
T
F

1.0
<0.4

1.0
>0.9

1.0
	0.9

Table 2. Conditional probability table for a rule-application node after a student viewed the related
proposition nodes.
Knows
CONTEXT- RULE
T
otherwise

Knows

READ

All Preconditions

LOW

OK

LONG

T

1a
0

1a
0

max{ð1  aÞ; 0:9}
0

Table 2. No matter how long the student attends to the derivation, it cannot be selfexplained correctly if the student does not have the necessary knowledge of the corresponding Context-rule and its preconditions. Cases where such knowledge is missing
are grouped together in the row labeled ‘otherwise’ in Table 2. On the other hand,
if the student has the necessary knowledge, the probability that proper self-explanation
occurs increases with viewing time. If viewing time is LOW (i.e., insu⁄cient for reading) or OK (i.e., su⁄cient for reading only), self-explanation for this line could only
have occurred if a student reasoned forward from previous lines. The probability that
this happens is given by Equation (1), representing the student’s tendency to selfexplain ahead. Thus,
P(RA=T j Rule=T, All preconditions=T, Read 2 fLOW,OKg) ¼ 1  a
as shown in Table 2. If reading time is LONG, the probability that the student selfexplained becomes high.
4.1.2.

Modeling Student Use of the Self-explanation Menus

Another action interpretation problem that the SE-Coach student model must face
is how much credit should be given to student self-explanations built through the
SE-Coach menu-based tools, and how to count the feedback the SE-Coach provides
as students work through the menus. This section describes how this problem is
addressed in the Bayesian network.
Nodes representing the student’s self-explanation actions (SE nodes) are dynamically added to the Bayesian network. For instance, the node pb-choose-body in Figure
11a represents the action of using the Plan Browser on the line ‘To solve this problem
we choose Jake as the body.’As we described in Section 1.3, the SE-Coach’s templates
re£ect the content of the system’s physics rules. Hence, template ¢lling provides

USING BAYESIAN NETWORKS

399

evidence of the student’s understanding of the corresponding rule. This is encoded in
the Bayesian network by linking the SE node for a template ¢lling action with the
Context-rule node corresponding to that template’s content. Similarly, the selection
of goals in the Plan Browser provides evidence of how well a student understands
the mapping between solution steps and the underlying solution plan. This is encoded
in the Bayesian network by linking Context-rule nodes establishing goals in the
SE model with the SE nodes encoding goal selection in the Plan Browser (see link
B in Figure 11).
Even if a student has little knowledge of a rule, she may still be able to generate a
correct self-explanation involving that rule by using the SE-Coach tools, because
of the guidance that these tools provide. Moreover, the more guidance the tools
provide in the form of negative feedback on menu selections, the less evidence there
is that the student knows the relevant rule. To model this interpretation, we direct
the link from the Context-rule node to the SE node (see Figure 11a) and use the entry
P(SE ¼ TjContext-rule ¼ F) in the SE node conditional probability table to represent
the probability that a student can complete a correct SE action without knowing
the corresponding rule. This conditional probability is adjusted dynamically depending on the student’s use of the menus. In particular, the higher the number of wrong
attempts a student makes before generating a correct SE action, the higher we set
P(SE ¼ TjContext-rule ¼ F). This implements a sensible interpretation policy,
namely that such student’s behavior makes it more likely that the student achieved
the correct action through random selection in the SE-Coach tools, rather than
through reasoning.
4.1.3.

Using the Student Model to Support Self-explanation

As we described in Section 1.3, the SE-Coach delays its interventions until the student tries to leave an example. At that point, the marginal probabilities of Ruleapplication nodes in the student model represent the probability that the student
has self-explained the corresponding derivations. If a Rule-application node has a
marginal probability below a given threshold, the SE-Coach will suggest that the
student explain the associated example line. This section describes the process in
more detail.
When the student indicates the desire to leave the example, the SE-Coach performs
the following steps:
1. Check if the student model contains Proposition nodes that both correspond to
example lines and derive from a Rule-application node with probability below
the threshold for self-explanation.
2. For each of these Proposition nodes:
^ Add the corresponding example line to the list of lines that the student should
explain further (i.e., the list of lines that will have their cover boxes highlighted
in pink in the interface).

400

CRISTINA CONATI ET AL.

^ If the low probability of the parent Rule-application node is due to low probability of a Context-Rule node, add a prompt to self-explain the line using
the Rule Browser/Template (when the Context-Rule node represents a physics
rule) or a prompt to use the Plan Browser (when the Context-Rule node represents a planning rule).
^ If the low probability of the parent Rule-application node is due only to the low
value of a Read node, add a prompt suggesting to read the line more carefully.
As an illustration, consider Figure 11, which shows the probabilities in the student
model after a student performed the two reading actions and the Plan Browser action
whose nodes appear to the left of the dotted line. Suppose that the student tries to
leave the example now. The only Proposition node that derives from a Rule-application
node with low probability and that corresponds to an example line is F-Jake-is-the-body.
Therefore, the SE-Coach adds the corresponding example line ‘To solve this problem,
we chose Jake as the body’ to the list of lines to be further explained. Because the low
probability of RA-body-by-force is due to low probability of the Context-rule
R-body-by-force, the SE-Coach adds a prompt to use the Rule Browser/Template to
self-explain the line. If the Context-Rule node R-goal-choose body also had had low
probability, the SE-Coach would have added a prompt to self-explain this line using
the Plan Browser. On the other hand, if both rule nodes had had high probability
while the value of node Read-L2 had been low, the SE-Coach would have added only
a prompt to read this line more carefully.
If the SE-Coach did not use the Bayesian network, it would have to make many
more suggestions. It would probably suggest re-reading all lines whose latency was
normal or below normal. It would probably suggest using the self-explanation menus
for all lines that had not yet been explicitly self-explained.With the Bayesian network,
the suggestions of the coach are much more focused, and thus more likely to be useful
to the student, and thus more likely to be acted on by the student.

5.

Evaluations of Andes’ Student Models

In this section, we ﬁrst summarize the results of various evaluations that provide evidence of the effectiveness of Andes’ student model for problem solving. Next, we
describe an evaluation of the student model for example studying.
5.1.

EVALUATION OF THE STUDENT MODEL FOR PROBLEM SOLVING

Evaluation of a statistical approach to student modeling can use techniques from
both disciplines of machine learning and user modeling (Zukerman and Albrecht,
2001). From a machine learning perspective, we are interested in evaluating the accuracy of the model’s predictions on a known test set of data. User modeling evaluations focus on the accuracy of the model in predicting or inferring the state of real
users. There is little consensus on a methodology for performing such evaluations,

USING BAYESIAN NETWORKS

401

and very few are reported in the literature. We have performed a machine learning
style evaluation of Andes’ student model for problem solving (VanLehn and Niu,
2001) and, although we still don’t have a formal user-modeling-style evaluation,
we have empirical results showing that Andes enhances student learning, which provides indirect evidence of the model’s effectiveness.
In the next two sections we summarize the results of the machine learning style
evaluation and of the evaluations with real students.
5.1.1.

Machine Learning Style Evaluation

In the machine learning style evaluation of Andes (VanLehn and Niu, 2001), a set of
simulated students were created with predeﬁned knowledge proﬁles, and these students’ solutions to the Andes physics problems were simulated. The student model
was run to determine if it could accurately detect the knowledge proﬁle of the simulated students. Unlike some machine learning evaluations, where many researchers
use the same data to evaluate their software, Andes can only be compared to different versions of itself. Thus, this evaluation concentrated on varying numerical parameters (e.g., the prior probabilities) and structural features (e.g., whether students
are required to correct their errors).
The results show that Andes’ assessment, with its normal parameter settings and
structural features, correctly determined whether a rule was mastered or unmastered
about 65% of the time, aggregating over all synthetic students and all evaluation conditions. There were two major impediments to increased accuracy.
The ¢rst impediment was that not all the inferences that students can make are
visible on the user interface. For instance, when a student adopts a goal, there is
no way to tell Andes about it. If every goal or fact node in the Bayesian network
had a user interface action associated with it, then Andes’ accuracy would rise from
65% to 75%.
The second impediment is that Andes does not require students to make entries
even when its interface allows doing so. For instance, even if the student infers that
the velocity of a body is zero, the student doesn’t have to write down the equation
V ¼ 0. If Andes could assume that missing actions implied missing inferences, then
its accuracy would rise from 65% to 70%.
If both impediments were removed, then Andes’ accuracy would rise from 65% to
95%. In contrast, varying other parameters and features (e.g., prior probabilities, slip
and guess parameters) had relatively little in£uence on accuracy.
These results suggest that Andes is doing a good job of student modeling given the
amount of information that the user interface makes available about student reasoning. In particular, all the subjective probabilities that we had to include due to the
lack of empirical data are probably not hurting Andes’ accuracy much, as it proved
not to be sensitive to these numbers.
The only way to do better appears to be requiring students to display more of their
thinking. But this would not only slow them down and increase the time to learn

402

CRISTINA CONATI ET AL.

the user interface, it could also make Andes so tedious to use that students simply
would not accept it.
5.1.2.

Evaluation with Real Students

In the Fall of 1999 we performed a summative ﬁeld evaluation of Andes at the Naval
Academy (Schulze et al., 2000). Andes was used for four weeks by 173 students in
eight sections of the Naval Academy’s introductory physics course. At the end of this
time, they were given a midterm exam covering material that was taught by Andes
(and by the instructors during course lectures). The students’ performance on the
midterm was compared to a control group of 162 students whose sections did not
use Andes. The results of this comparison were encouraging. Students who used
Andes performed about a 1/3 of a letter grade better on average than students
who did not use Andes (T(334) ¼ 2.21, p ¼ 0.036, two-tailed).
Andes was also evaluated in the fall of 2000 (Shelby et al., 2001). This version of
Andes was essentially the same as the one evaluated the preceding year. However,
it covered more chapters of mechanics, had more problems per chapter, included some
more di⁄cult problems, and required the students to draw vectors explicitly (in the
previous version, students could de¢ne variables representing vector quantities without
actually drawing the vectors). The experimental design was the same, with 140 students
in the Andes condition and 135 students in the control condition. Students in the Andes
condition scored signi¢cantly higher on the post-test (T(274) ¼ 7.74, p < 0.00001,
two-tailed). The e¡ect size was 0.92 of a standard deviation, which at the US Naval
Academy corresponds to raising the students’ grade by about one letter (e.g., from
C to B). This e¡ect size compares well with other ITS, many of which also have an
e¡ect size of about 1.0 (Shute and Psotka, 1996).
The Andes design was based on the hypothesis that instruction could be improved
by simply coaching students as they did their homework and making no other changes
in the course. This hypothesis appears to have been borne out. Although the US Naval
Academy instructors collaborated with us in designing the system and were thus familiar with it when they began to use it in their classes, they used the same textbook,
the same labs and almost the same lectures as were used in the Control condition
and the rest of the course. In contrast, many other ITS that have also achieved a
1 standard deviation e¡ect size required instructors to use a new curriculum and sometimes even a new textbook (Koedinger et al., 1995; Corbett et al., 2000).
Although learning gains are the traditional measure of success for educational software, an arguably more important measure is voluntary acceptance by both
instructors and students, because if either refuse to use the software, then it doesn’t
matter how much the software improves learning. As just discussed, Andes seems
to work well even when instructors change very little in their courseOin particular,
they can continue to use their old textbook and lecture notes. This should increase
instructor acceptance. Student acceptance was tested in the fall 2000 US Naval Academy evaluation. In the physics courses at the US Naval Academy, doing the assigned
homework usually is not mandatory. Moreover, students in the Andes conditions were

USING BAYESIAN NETWORKS

403

encouraged to use Andes, but they could use paper and pencil if they wanted. Thus, the
number of problems done on Andes is a simple measure of student acceptance,
because if students found it too di⁄cult to use relative to its perceived bene¢ts, then
they would switch to pencil and paper or just not do their homework. From automated
analyses of student log ¢les, it was found that students solved 50 out of the 60 problems
assigned on Andes. This is encouraging, but unfortunately we lack standards to compare this against, as we have no similar measure for the Control condition or from
other educational software.
A ¢nal type of evaluation that may be done for a student model that does plan
recognition is to look at how accurate the system is at inferring the students plans.
Both positive learning gains and the students’ acceptance of Andes provide indirect
evidence that its student model for problem solving does quite a good job in predicting
the students’ intended goals, or at least that Andes’ interventions based on the model’s
predictions are not disruptive or annoying for the students. Gaining more direct evidence of the student model predictive accuracy is hard, because one must have a standard for judging what the students’ goals actually were. We attempted to overcome
this di⁄culty by using an approach that involves comparing the predictions produced
by a user model to subjective human judgments of the user’s goals. Using log ¢les
from the 1999 evaluation, we randomly selected episodes where the students asked
Andes for help. We generated a snapshot of the Andes screen just prior to its help
message.We then gave these screen snapshots to expert physics tutors, and asked them
to judge (a) the goal that the student was trying to accomplish and (b) what help they
would give to this student. Our plan was to evaluate Andes using only snapshots where
the experts agreed with each other. Unfortunately, the agreement among the experts
was so low that there were too few snapshots left for evaluating Andes. We intend
to repeat the study with a new version of Andes and a much larger sample of
snapshots. We will also give the judges a summary of the tutorial session up to the
point of the snapshot, since they said they sometimes needed that in order to make
an informed judgment.
5.2.

EVALUATION OF THE STUDENT MODEL FOR EXAMPLE STUDYING

The SE-Coach student model guides the SE-Coach to elicit students’ self-explanations on lines that are assessed to be problematic for them. Thus, an indirect way
to evaluate the effectiveness of this student model is to verify how the SE-Coach
interventions inﬂuence students’ learning. We obtained preliminary results on this
from a study that we conducted in our lab.
During the study, 29 subjects studied Newton’s 2nd Law examples with the complete SE-Coach (experimental condition), while 27 subjects studied the same examples
with the masking interface only and no coaching (control condition). All subjects took
a pretest and a posttest consisting of Newton’s 2nd Law problems. At the time of
the experiment, all subjects were taking introductory physics at one of three di¡erent
colleges and had started studying Newton’s laws in class. In this section, we focus

404

CRISTINA CONATI ET AL.

on results related to how the SE-Coach interventions, and thus the student model,
in£uenced the performance of students in the experimental group. More general
data on the positive results we obtained on the di¡erence between the performance
of the experimental and the control groups can be found in (Conati and VanLehn,
2000).
Constraints on the study duration prevented us from initializing the student model
with data on the students’ initial knowledge and studying style. Hence, we assigned
to most of Andes’ rules a prior of 0.5, and we assumed that students were very unlikely
to explain ahead (a ¼ 0.98) because other studies (Renkl, 1997) show that this is consistent with most students’ behavior.
We computed from log data how often students followed the SE-Coach’s adaptive
prompts to further self-explain. The results are summarized in Table 3.
For each type of prompt, the table reports: (i) the maximum number of prompts that
could appear in the interface for the three examples used in the study. These are
the prompts the SE-Coach would generate if there was no student model. (ii) The
average number of prompts adaptively generated by relying on the student model.
(iii) The average percentage of these prompts the students followed.We then computed
the correlation between the percentages of followed prompts and students’ post-test
scores, after regressing out pretest scores. All three measures signi¢cantly (or nearly
signi¢cantly) correlate with post-test performance (p ¼ 0.056, R2 ¼ 66% for Rule
Browser/Template prompts; p ¼ 0.024, R2 ¼ 64.5% for Plan Browser prompts;
p ¼ 0.016, R2 ¼ 63% for ‘Read more carefully’ prompts). These data are consistent with
the hypothesis that the adaptive prompts based on the student model e¡ectively elicited
self-explanations that improved students’ learning, although further data should be
gathered to control for other variables that might have caused the correlation, such
as general academic ability or conscientiousness.
The correlation exists despite the fact that students, on average, followed less
than half of the SE-Coach prompts (see Table 3). We conjecture two possible explanations for why students did not follow the prompts more extensively. The ¢rst is that
the low accuracy of the initial parameters in the student model caused the model
to underestimate the amount of spontaneous self-explanation that students generated.
Thus, students rightly ignored those prompts asking them for redundant selfexplanations.The second explanation is that those students who tended to overestimate
their understanding ignored the SE-Coach prompts, even when the prompts were

Table 3. Percentage of SE-Coach prompts that students followed

Prompt Type
Use RuleBrowser/Template
Use PlanBrowser
Read more carefully

Max.

Generated

Followed
(%)

43
34
43

22.6
22.4
7

38.6
42.0
34.0

USING BAYESIAN NETWORKS

405

justi¢ed. If this was the case, obligating the students to follow the SE-Coach suggestions would increase the e¡ectiveness of the system. Further evaluations of the student
model with more accurate initial parameters will clarify this issue.
We also examined the accuracy of the SE-Coach’s assessment. We found an
interaction between accuracy and when subjects had started studying Newton’s Laws
in their classes. In particular, we computed the correlation between posttest scores
and the number of rules that reached high probability in the student model. The correlation is very low (r ¼ 0.03) for the 17 subjects from classes that had started studying
the examples’ topic more than a week before the study (early-start subjects) and it
is higher (r ¼ 0.33) for the 12 subjects from classes that had started just a few days
before the study (late-start subjects). Since our data showed no signi¢cant di¡erences
in the two groups’ initial knowledge or in how they used the interface tools, we hypothesize that the di¡erence in the correlation exists because the SE-Coach examples
were more challenging for late-start subjects and therefore they put more e¡ort in
reasoning and learning from their SE actions. Hence, their long reading times and
use of the menus was really associated with self-explanation, whereas similar
behaviors from the early-start students was not.
If this interpretation is correct, then it suggests that the students’ learning
stage should be taken into account when assessing knowledge based on SE actions.
For instance, in the conditional probability table for SE nodes, the probability
P(SE ¼ T j Rule ¼ F) (e.g. the probability of generating a correct SE action without
knowing the corresponding rule), should be set to a higher value if a student has been
working on the example topic for some time (like our early-start subjects), so that
this student’s knowledge receives less credit for her self-explanations.

6. Related Work
The development of techniques for reasoning under uncertainty that have appeared
in the AI community since the mid eighties has facilitated a rapid growth of interest
in probabilistic approaches to user modeling. As Jameson (1996) outlines, these user
models tend to address three general purposes: (1) inferring the user’s knowledge or
general abilities (e.g., Mislevy, 1995; Petrushin, Sinitsa and Zherdienko, 1995; VanLehn and Martin, 1998), (2) recognizing the user’s plans or goals (e.g., Charniak and
Goldman, 1992; Huber et al., 1994; Conati and VanLehn, 1996b; Albrecht et al.,
1999), or (3) predicting the user’s inferences (e.g., Van Mulken, 1996; Conati and
Carenini, 2001) and future behavior (Horvitz and Barry, 1995; Horvitz et al.,
1998; Zukerman et al., 1999). Depending on the purpose of the system, one of these
functions may be treated as more important than the others. For instance, Charniak
and Goldman are primarily interested in recognizing the plans of characters in a
story, while Horvitz and Barry are interested mainly in how the user will respond
to information presented by their system. In a student model that supports provision
of help during a fairly unconstrained tutorial interaction all three functions are
important and interrelated. Thus, the Andes’ model combines all three of them into

406

CRISTINA CONATI ET AL.

a single Bayesian network representing both the students’ domain knowledge as well
as their past and future plans and goals.
Student modeling poses some unique challenges for probabilistic approaches
due to the fact that students’ knowledge changes over time as they learn. This makes
it even harder to perform plan recognition and behavior prediction because the student model cannot assume, as many plan recognition systems do (Carberrry,
2001), that the user always has perfect knowledge of the available plans throughout
the interaction. Both the models developed for the Carnegie Mellon Cognitive Tutors
(Corbett and Bhatnagar, 1997) and for the SMART system (Shute, 1995) are designed
to assess the student’s knowledge as it evolves during the interaction with the tutor,
and both of these systems have been favorably evaluated as accurately re£ecting students’ learning. While neither of these systems uses Bayesian networks, Reye (Reye,
1998) has shown that both models can be formulated as a dynamic Bayesian network
with the same basic behavior. However, none of these models make use of probabilistic
knowledge assessment to guide their inference of the student’s goals, as Andes’ student
model does. While Shute’s system does not need to infer students’ goals, the cognitive
tutors do. This is because, although they make the students explicitly enter all their
solution steps and thus reduce uncertainty about what the student is trying to do,
their solutions don’t have ¢xed step-ordering. Thus, like in Andes, when a student
has asked for help, there are several possible correct actions that could be done next.
The Cognitive Tutors resolve the ambiguity by using heuristics based on asking
the student, on the location of the student’s cursor (Anderson et al., 1995), or on
the student’s most recent action (Corbett et al., 2000).
As we discussed in Section 3.3, Andes relies on the student model probabilities
to predict which goal the student is probably focusing on. These predictions drive
a heuristic algorithm for selecting the content of the tutor’s hints. DT Tutor
(Murray and VanLehn, 2000) extends the Andes network by adding decision nodes
and utilities so that the choice of tutor action can be done completely probabilistically.
In particular, the tutor uses the extended network to predict the student’s reaction
to proposed tutor actions, evaluates the utility of the resulting student, tutorial
and dialogue states, then chooses the tutorial action that maximizes the expected
utility. However, this improved reasoning comes at the cost of greatly increased network size and therefore reduced speed for updating the network.
While Bayesian networks have been by far the most popular formalism in recent work
on numerical user models, other numerical methods have been used as well. In
particular, Jameson (Jameson, 1996) describes a number of systems that use DempsterShafer theory (Carberry, 1990; Bauer, 1995) and Fuzzy Logic (Katz et al., 1992).
A recent review article of the ¢eld (Zukerman and Albrecht, 2001) shows how
work on statistical techniques in user modeling has broadened in scope in the last
¢ve years.
An important and di⁄cult question with respect to probabilistic or numerical
approaches to student modeling is how they might be evaluated (see Section 5
for a discussion of evaluations of the Andes system). Student models that focus on

USING BAYESIAN NETWORKS

407

knowledge assessment may be evaluated by comparing their predictions of the student’s knowledge to actual student performance on post-tests. The SMART student
model (Shute, 1995), which uses a set of regression equations to update the estimate
of students mastery of each curriculum element, was evaluated and found both to
accurately predict students’ posttest performance and to contribute to impressive
learning gains of greater than 2.0 standard deviations when used to guide the tutoring
system’s behavior. Similarly, the knowledge tracing component of the ACT Programming Tutor was found to accurately predict student post-test performance (Corbett
et al., 2000), and to help students learn more and faster by using mastery learning
(Anderson et al., 1995). However, there are several problems with this approach.
The student model assesses student competence on many individual pieces of knowledge, but the post-test yields a single score. This can allow systematic inaccuracies
in the student model to remain undetected. To put it a little di¡erently, when the outcome of the evaluation is a single number (the correlation between predicted posttest score and actual post-test score), how can one interpret its value? As Corbett
et al. (2000) discovered, it takes substantially more work to discover why the correlations are low and make the appropriate changes to the student model to raise them.
Moreover, as with any assessment technique, there are the classic issues of reliability
and validity. For instance, a post-test can only sample the student’s knowledge, which
raises the issue of whether it is a ‘fair’ sample of the competence in the task domain.
VanLehn and Martin (1998) survey classic standards for evaluation of assessment systems, and indicate how OLAE (Andes’ predecessor) fares. Lastly, the main problem
with using post-tests for evaluation of knowledge assessment is that it confounds
the assessment technology with the cognitive task analysis. As Corbett et al.
(2000) show, their cognitive model sometimes used one rule when subjects actually
used two, for instance. Although this creates an inevitable inaccuracy in the assessment, it is a fault of the cognitive task analysis and not of the assessment technique
per se. For all these reasons, evaluations with synthetic students (like the Andes’
evaluation we discussed in Section 6.1) are a very promising alternative to provide
valuable insights on the details of a user-model performance.
User models that focus on inferring a user’s goals or intentions tend to be more
di⁄cult to evaluate, since they require judging a posteriori what goals the user had
at di¡erent times during the interaction. One approach that has been used so far
is that of (Calistri-Yeh, 1991), who compared the predictions produced by his system
to that of human judges. We used a similar technique to evaluate the prediction
capabilities of Andes’ student model for problem solving, as discussed in Section 5.1.2.

7.

Discussion and Future Work

We have presented a probabilistic approach to student modeling that relies on
Bayesian networks to deal with the high level of uncertainty inherent in providing
tailored support for homework-related activities in Andes, an intelligent tutoring
system for Newtonian physics. The homework-related activities that Andes supports

408

CRISTINA CONATI ET AL.

include learning from worked-out examples in addition to problem solving, because
example studying is one of the fundamental activities that complement classroom
instruction.
Much of the uncertainty that Andes’ student models face is due to Andes’ philosophy of giving students the initiative in the learning process. This means, for instance,
that students have the freedom to experiment with di¡erent solutions and learn from
their own mistakes, while Andes provides help and support when asked to but does
not constrain students to follow a prede¢ned solution path. Furthermore, Andes does
not require students to always express all their reasoning explicitly.
This focus on student initiative involves a higher level of uncertainty than more
constrained ITS do (e.g., Anderson et al., 1995) for two main reasons. First, allowing
students to follow multiple solutions forces Andes to confront the assignment of credit
problem, which arises every time there is more than one explanation for a user’s interface action. Second, not requiring students to make all their reasoning explicit
can drastically reduce the amount of information the student model has for generating
its predictions (the bandwidth problem discussed in (VanLehn, 1988)). For instance,
when discussing the VanLehn and Niu (2001) evaluation of the Andes’ student model,
we noted that student modeling accuracy would increase from 65% to 95% if Andes
required students to enter on its interface all facts or goals as they were inferred.
But Andes’example studying and problem solving components cannot feasibly require
students to explicate all their thinking, because this would so greatly add to the
students’ burden that few students would be willing to use such a tutor, especially
as they become more pro¢cient in the instructional domain. Hence, Andes’ student
models will often have little information on the student’s reasoning and thus
must guess.
In this paper, we illustrated how we rely on the probabilistic reasoning framework of Bayesian networks to enable Andes to make as much of an educated guess as
possible based on the available evidence. Given that one cannot have both freedom
and convenience for students while also having highly accurate student modeling,
it appears that our Bayesian approach has allowed Andes to made an appropriate compromise between the two, in that Andes is indeed an e¡ective tutoring system.
Both the Problem Solving Coach and the SE-Coach cause learning gains, and the
Problem Solving Coach appeared to be acceptable to students in a ¢eld testing situation.
The key to Andes’ success, as with all Bayesian network models, lies in accurately
representing the probabilistic dependencies in the task domain. Andes uses a knowledge-based model construction approach to generate the part of the network that
represents the probabilistic dependencies between domain knowledge and inferences
in problem solutions. The principles and parameters used in creating chunks of Andes’
networks are essentially little theories of how speci¢c events relate to each other
and to the student’s knowledge and goals. We list here a brief summary of how these
theories were encoded in the Bayesian network formalism, along with a few lessons
we learned along the way.

USING BAYESIAN NETWORKS

7.1.

409

A DEPLOYED, REALISTIC STUDENT MODEL BASED ON BAYESIAN NETWORKS

As we mentioned earlier, the main problem that Andes student model needs to
address is the assignment of credit problem. The Bayesian solution that it adopts
is built on the approach that OLAE pioneered for assignment of credit during off
line assessment and that POLA extended to on-line plan recognition. None of these
Andes’ predecessors went beyond the prototype stage. To scale up the basic
approach for use by a fully ﬂedged system deployed in the ﬁeld, we had to add
hypotheses about knowledge, learning and behavior that increased the realism of
the student model. In all cases, a sensible hypothesis was posed and implemented
in the Bayesian network, albeit without the beneﬁt of empirical calibration to determine the relevant parameters. In particular, we have presented solutions to the following issues:
1. Context speci¢city: knowledge is sometimes acquired ¢rst in a more speci¢c form
then generalized, thus making near transfer easier to obtain than far transfer
(Singley and Anderson, 1989). How can we track the generality of competence?
Andes distinguishes Context Rules from Rules (see Section 2.2.1).
2. Guessing: some actions make it easier to guess correctly than other actions. How
should assignment of credit re£ect this? The parameter b in the leaky-OR gate
(which de¢nes the conditional probability between an action and the di¡erent
ways to derive it) is higher when guessing is more likely to lead to success
(see Section 2.2.2.1).
3. Mutually exclusive strategies: some problems have multiple, mutually exclusive
solution strategies. Thus, evidence that the student is following one strategy
should be interpreted as evidence that they are not following the other strategy.
Andes student model uses Strategy nodes to enforce this behavior (see
Section 2.2.2.3).
4. Old evidence: how should evidence from earlier problems a¡ect the interpretation
of evidence from the current problem? Andes rolls up the task speci¢c network
of a completed problem by copying the marginal probabilities on Rule nodes
into prior probabilities (see Section 2.2.4).
5. Errors: errors of omission (missing actions) cause nodes to be clamped to False
only when the action is required, which happens rarely since there are only
few required actions in Andes. Errors of commission (incorrect actions) cause
nodes to be clamped to False only when the incorrect action directly and blatantly contradicts the node’s action (see Section 3.2.1).
6. Hints: when the student has received hints before entering a correct action, how
much credit should the goals and rules that explain that action receive? An additional node is added to those representing alternative derivations of the action,
so that the more speci¢c the hint, the less credit those derivations receive
(see Section 3.2.2).

410

CRISTINA CONATI ET AL.

7. Reading latency: when students study examples, they may pause longer as they
read some lines than others, and this may be evidence of self-explanation.
How can we properly interpret the latency of reading times? Andes uses Read
nodes whose values record the observed latency and combines them with knowledge assessment to judge whether a student is self-explaining (see Section 4.1.1).
8. Self-explaining ahead: some students prefer to self-explain solution steps before
reading them in the example, while others prefer to read steps then self-explain
them (Renkl, 1997). Andes uses the noise parameter a to model how these
two di¡erent studying behaviors a¡ect the interpretation of reading latencies
(see Section 4.1).
9. Self-explanation menu selections: when students use menus to express selfexplanations, they may make a few errors, get feedback from the SE-Coach
on each, and then enter a complete self-explanation. How should a sequence
of such menu selections be interpreted? Andes dynamically changes the conditional probability table of the SE nodes in order to re£ect the amount of feedback
the coach gives the student (see Section 4.1.2).
What we learned is that Bayesian networks, even when created by a rule-based
problem solver, are like any other knowledge representation formalism in that one
can fairly easily represent increasingly complicated and hopefully more realistic models. Like any other knowledge-based system, Andes should have its knowledge validated empirically, but this is unrealistic in most cases. We are fortunate to be able
to evaluate the system as a whole with real students, as described in Sections 5.1.2
and 5.2. The best we can do for testing individual hypotheses is to use synthetic students, as described in Section 5.1.1.
7.2. THE NEED FOR PLAN RECOGNITION IN UNCONSTRAINED LEARNING
ENVIRONMENTS

Although assessment is important in itself, because instructors often want to see just
which rules a student has mastered, plan recognition is only useful inside an ITS as a
decision aid. That is, it helps the ITS decide what tutorial action to make. In building
a comprehensive student modeling framework that could support both assessment
and plan recognition, we realized that they are sometime in conﬂict.
In Andes, plan recognition is necessary for the problem solving coach to select what
step to suggest when a student asks for help. Because Andes wants to help students solve
problems in their own way, rather than teach a problem solving plan of its own as other
tutoring systems do, it must determine whatgoal the student is probably trying to achieve,
and suggest the action the student cannot perform for lack of knowledge. This de¢nes
the kind of plan recognition that the problem solving coach needs.
As we discussed in Section 3.3, the semantics of the T values on proposition and
rule application nodes in the student model for problem solving don’t quite
match the plan recognition requirements, because they only tell what the student

USING BAYESIAN NETWORKS

411

can potentially derive, not what she has already derived or what she wants to derive next.
In short, the problem is that Andes’ nodes have the right semantics for assessment
but the wrong semantics for recognizing the student’s intentions. Thus, the problem
solving coach needs to resort to heuristic search to select a node for hinting, although
the search is guided by the student model’s probabilistic prediction of what inferences
the student can or cannot make.
One way to deal with this problem without resorting to heuristics as Andes does is to
augment the network so that it can represent both competence and intentions
(Murray and VanLehn, 2000). This requires Bayesian networks that have two nodes
for each goal. One node has the same assessment-based semantics as Andes’ goal
nodes, and the other node indicates whether the goal is probably one that the student
is currently intending to do. This allows the coaching system to perform hint selection
by relying solely on the network probabilities, but unfortunately generates much larger
student models that currently cannot provide the real-time responses that an
Andes-like interaction needs.
Andes only needs to recognize student intentions because it wants to help students do
what they intend to do. So one might wonder whether it is it really worthwhile to devise
intelligent assistants that support students in fairly unconstrained pedagogical activities,
considering the e¡ort and challenges involved in building the student models suitable
for these assistants. This question may be further justi¢ed by the fact that empirical
studies with the CMU cognitive tutors, (which support more constrained interactions
than Andes and therefore rely on less complex student models) have shown that these
tutors can greatly enhance students’ learning (Koedinger et al., 1995).
Although we agree that a more constrained interaction can be bene¢cial in speci¢c educational settings, for speci¢c types of learners and in particular phases of
the learning process, we also believe that supporting more unconstrained, studentled interactions is critical to building richer learning environments that can be
bene¢cial for di¡erent types of learners and at di¡erent learning stages (VanLehn
et al., 2000; Bunt et al., 2001). We argue that, ideally, the role of a comprehensive
model of the student’s knowledge and behavior in such environments should be to
allow an intelligent coach to dynamically switch from a directive tutoring style to
a more open one by taking into account both the student’s evolving understanding
and her preferred learning style (e.g. autonomous vs. passive). The goal would be
to achieve the best tradeo¡ between a pedagogical interaction that accommodates
the student’s preferences and one that is more suitable for her evolving level of
knowledge.
Our position is supported by empirical studies showing that open learning environments that rely solely on the student’s initiative and exploratory capabilities can
be highly bene¢cial for learners with the suitable level of knowledge and learning
skills, although they are not as e¡ective for learners who need more structure and
guidance in the learning experience (Shute and Glaser, 1990; Bunt et al., 2001).
Additional information on when and how more tutorial vs. more exploratory interactions can support better learning (and consequently when a richer student model

412

CRISTINA CONATI ET AL.

like Andes’ is necessary) could be gained by running user studies that compare the
version of Andes presented in this paper with a new, more restrictive version that
is currently under development for the U.S. Naval Academy.
7.3.

SCALING UP

In order to tutor students through most of a semester, Andes had to have Bayesian
networks for around a hundred physics problems. Every time the problem solvers’
knowledge base changed, we had to regenerate all those Bayesian networks. As we
discussed in Section 2.1, we were able to automate the network construction process
so that it could be done with little human intervention. In particular, we did not have
to hand-edit any conditional probability tables. Thus, the knowledge management
problem turned out to be no worse for Andes than for any other rule-based knowledge intensive application.
On a few physics problems, updating the Bayesian network using the exact algorithm sometimes took longer than 40 seconds on the 400 MHz Pentium computers
that were used in the fall 2000 evaluation. Thus, on some problems, we directed Andes
to use stochastic evaluation of the networks and to stop when 40 seconds were reached.
On other problems, the update was completely turned o¡. Hopefully, advances in
the Bayesian network update algorithms will allow us to remove these expediencies.
Because our networks are generated by a rule-based system, they may have regularities
that could be exploited by an improved update algorithm.
On the whole, the use of Bayesian networks for Andes’ student model seems to
have been a complete success. The design allowed elegant, precise representation
of sensible interpretation policies; it did not increase the knowledge management task,
and it did not slow the system down too much. Moreover, empirical evaluations
of the resulting coaches indicated that students learned more with them than with
conventional instruction.

Acknowledgments
This research was sponsored by ONR’s Cognitive Science Division under grant
N00014-96-1-0260, by ARPA’s Computer Aided Education and Training Initiative
under grant N660001-95-C-8367, and by AFOSR’s Artiﬁcial Intelligence Division
under grant F49620-96-1-0180.
The authors would like to thank Drs. Robert Shelby, Kay Shulze, Donald Treacy
and Mary Wintersgill of the U.S. Naval Academy for their contributions to
the development and evaluation of Andes.We also thank Anders Weinstein and Ellen
Dugan for helping implement the Andes interface and overall architecture, Marek
Druzdzel and Zendong Niu for their contributions to the design and implementation
of the student model Bayesian networks, Patricia Albacete for her input at
di¡erent stages of Andes’ design, and Charles Murray for his comments on the
manuscript.

USING BAYESIAN NETWORKS

413

References
Albacete, P. and VanLehn, K.: 2000, The conceptual helper: An intelligent tutoring system for
teaching fundamental physics concepts. Proceedings of Intelligent Tutoring Systems,
5th International Conference, ITS2000, Montreal, Canada, Lecture Notes in Computer
Science 1839, Springer, pp. 564^573.
Albrecht, D. W., Zukerman, I. and Nicholson, A. E.: 1999, Bayesian Models for Keyhole Plan
Recognition in an Adventure Game. User Modeling and User-Adapted Interaction,
8(1^2), 5^47.
Anderson, J. R., Corbett, A. T., Koedinger, K. R. and Pelletier, R.: 1995, Cognitive Tutors:
Lessons Learned. The Journal of the Learning Sciences, 4(2), 167^207.
Bauer, M.: 1995, A Dempster-Shafer approach to modeling agents preferences in plan recognition. User Modeling and User-Adapted Interaction, 5(3^4), 317^348.
Breese, J., Goldman, R. and Wellman, P.: 1994, Introduction to the special section on knowledge-based construction of probabilistic and decision models. IEEE Transactions on Systems, Man, and Cybernetics, 24, pp. 1577^1579.
Bunt, A., Conati, C., Hugget, M. and Muldner, K.: 2001, On Improving the E¡ectiveness of
Open Learning Environments through Tailored Support for Exploration. Proceedings
of AIED 2001, 10th World Conference of Arti¢cial Intelligence and Education, San Antonio, TX, U.S.A., pp. 365^376.
Calistri-Yeh, R. J.: 1991, An {A*} approach to robust plan recognition for intelligent interfaces. N. G. Bourbakis (ed.): Applications of Learning and Planning Methods. World
Scienti¢c Publishing Co., pp. 227^251.
Carberrry, S.: 2001, Techniques for Plan Recognition. User Modeling and User-Adapted
Interaction, 11, 31^48.
Carberry, S.: 1990, Incorporating default inferences into plan recognition. Proceedings of the
Eighth National Conference on Arti¢cial Intelligence, Menlo Park, CA, MIT Press,
pp. 471^478.
Charniak, E. and Goldman, R. P.: 1992, A Bayesian model of plan recognition. Arti¢cial
Intelligence, 64, 53^79.
Chi, M. T. H.: in press, Self-Explaining: The dual process of generating inferences and repairing mental models. Advances in Instructional Psychology.
Conati, C. and Carenini, G.: 2001, Generating Tailored Examples to Support Learning via
Self-explanation. Proceedings of IJCAI ‘01, 17th International Joint Conference on Arti¢cial Intelligence, Seattle, WA, U.S.A., pp. 1301^1306.
Conati, C. and VanLehn, K.: 1996a, POLA: A student modeling framework for probabilistic
on-line assessment of problem solving performance. Proceedings of UM-96, 5th International Conference on User Modeling, Kailua-Kona, Hawaii, U.S.A., User Modeling,
Inc., pp. 75^82.
Conati, C. and VanLehn, K.: 1996b, Probabilistic plan recognition for cognitive apprenticeship. Proceedings of the 18th Annual Meeting of the Cognitive Science Society, San Diego,
CA. U.S.A., Erlbaum, pp. 403^408.
Conati, C., Larkin, J., VanLehn, K.: 1997, A computer framework to support self-explanation. In duBouley B, Mizoguchi R. (eds) Arti¢cial Intelligence in Education: Proceedings
of the 8th World WNF, IOS Press, Ohmsha, pp. 279^286.
Conati, C. and VanLehn, K.: 2000, Toward Computer-based Support of Meta-cognitive
Skills: A Computational Framework to Coach Self-Explanation. International Journal
of Arti¢cial Intelligence in Education, 11(4), 389^415.
Corbett, A., McLaughlin, M. and Scarpinatto, K. C.: 2000, Modeling Student Knowledge:
Cognitive Tutors in High School and College. User Modeling and User-Adapted Interaction, 10(2^3), pp. 81^108.

414

CRISTINA CONATI ET AL.

Corbett, A. T. and Anderson, J. R.: 1995, Knowledge tracing: Modeling the acquisition of
procedural knowledge. User Modeling and User-Adapted Interaction, 4(4), 253^278.
Corbett, A. T. and Bhatnagar, A.: 1997, Student modeling in the ACT programming tutor:
Adjusting a procedural learning model with declarative knowledge. User Modeling: Proceedings of the Sixth International Conference, UM97, Chia Laguna, Italy, Springer,
Wien, New York, pp. 243^254.
Dean, T. and Kanazawa, K.: 1989, A Model for Reasoning about Persistence and Causation.
Computational Intelligence, 5(3), 142^150.
Gertner, A., Conati, C. and VanLehn, K.: 1998, Procedural help in Andes: Generating hints
using a Bayesian network student model. Proceedings of the 15th National Conference
on Arti¢cial Intelligence, Madison, Wisconsin, U.S.A., pp. 106^111.
Gertner, A. and VanLehn, K.: 2000, Andes: a coached problem solving environment for physics. Proceedings of Intelligent Tutoring Systems, 5th International Conference,
ITS2000, Montreal, Canada, Lecture Notes in Computer Science 1839, Springer, pp.
131^142.
Henrion, M.: 1989, Some practical issues in constructing belief networks. Proceedings of the
3rd Conference on Uncertainty in Arti¢cial Intelligence, Elsevier Science, pp. 161^173.
Horvitz, E. and Barry, M.: 1995, Display of Information for Time-Critical Decision Making.
Proceedings of the 11th Conference on Uncertainty in Arti¢cial Intelligence,
Montreal, Canada, Morgan Kaufmann: San Francisco, pp. 296^305.
Horvitz, E., Breese, J., Heckerman, D., Hovel, D. and Rommelse, R.: 1998, The Lumiere
Project: Bayesian User Modeling for Inferring the Goals and Needs of Software Users.
Proceedings of the 14th Conference on Uncertainty in Arti¢cial Intelligence, Madison,
WI, U.S.A., Morgan Kaufmann: San Francisco, pp. 256^265.
Huber, M. J., Durfee, E. H. and Wellman, M. P.: 1994, The automated mapping of plans for
plan recognition. Proceedings of the 10th Conference on Uncertainty in Arti¢cial Intelligence, Morgan Kaufmann, pp. 344^351.
Jameson, A.: 1996, Numerical uncertainty management in user and student modeling: An
overview of systems and issues. User Modeling and User-Adapted Interaction, 5(3^4),
193^251.
Just, M. and Carpenter, P.: 1986, The Psychology of Reading and Language Comprehension.
Boston.
Katz, S., Lesgold, A., Eggan, G. and Gordin, M.: 1992, Modelling the student in Sherlock II.
Journal of Arti¢cial Intelligence in Education, 3(4), 495^518.
Koedinger, K. and Anderson, J. R.: 1993, Reifying implicit planning in geometry: Guidelines for
model-based intelligent tutoring system design. S. P. Lajoie, and S. J. Derry (eds.).
Computers as cognitive tools. Hillsdale, NJ, Lawrence Erlbaum Associates, pp. 15^46.
Koedinger, K. R., Anderson, J. R., Hadley, W. H. and Mark, M. A.: 1995, Intelligent tutoring
goes to school in the big city. Proceedings of the 7th World Conference on Arti¢cial Intelligence and Education, Charlottesville, NC, AACE, pp. 421^428.
Mahoney, S. M. and Laskey, K. B.: 1998, Constructing situation-speci¢c Belief networks.
Proceedings of the 14th Conference on Uncertainty in Arti¢cial Intelligence, S. Francisco,
CA, U.S.A., Morgan Kaufmann, pp. 370^378.
Martin, J. and VanLehn, K.: 1993, OLAE: Progress toward a multi-activity, Bayesian
student modeller. Arti¢cial Intelligence in Education, 1993: Proceedings of AI-ED 93,
Charlottesville, VA, Association for the Advancement of Computing in Education,
pp. 410^417.
Martin, J. and VanLehn, K.: 1994, Discrete factor analysis: Learning hidden variables in
Bayesian networks, LRDC, University of Pittsburgh: Technical report.

USING BAYESIAN NETWORKS

415

Martin, J. and VanLehn, K.: 1995, Student assessment using Bayesian nets. International
Journal of Human-Computer Studies, 42, 575^591.
Mislevy, R.: 1995, Probability-based inference in cognitive diagnosis. P. Nichols, S. Chipman,
and R. Brennan (eds.). Cognitive Diagnostic Assessment. Hillsdale, NJ., Erlbaum, pp.
43^71.
Mislevy, R. J. and Gitomer, D. H.: 1996, The Role of Probability-Based Inference in an Intelligent Tutoring System. User Modeling and User-Adapted Interaction, 5(3^4), 253^282.
Murray, C. and VanLehn, K.: 2000, DT Tutor: A decision-theoretic dynamic approach for
optimal selection of tutorial actions. Proceedings of Intelligent Tutoring Systems, 5th
International Conference, ITS2000, Montreal, Canada, Lecture Notes in Computer
Science 1839, Springer, pp. 153^162.
Norman, D. A.: 1981, Categorization of action slips. Psychological Review, 88(1), 1^15.
Pearl, J.: 1988, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Mateo, CA, Morgan-Kaufmann.
Petrushin, V. A., Sinitsa, K. M. and Zherdienko, V.: 1995, Probabilistic approach to adaptive
student knowledge assessment: methodology and experiment. Arti¢cial Intelligence in
Education: Proceedings of AI-ED ’95, Washington, DC, U.S.A., pp. 51^58.
Polk, T. A., VanLehn, K. and Kalp, D.: 1995, ASPM2: Progress toward the analysis of symbolic parameter models. P. D. Nichols, S. F. Chipman, and R. L. Brennan (eds.). Cognitively Diagnostic Assessment. Mahwah, NH, Erlbaum, pp. 127^141.
Reggia, J. A. and D’Autrechy, C. L.: 1990, Parsimonious covering theory in cognitive
diagnosis and adaptive instruction. N. Frederiksen, R. Glaser, A. Lesgold and M. G.
Shafto (eds.). Diagnostic Monitoring of Skill and Knowledge Acquisition. Hillsdale,
NJ, Lawrence Erlbaum Associates, pp. 191^216.
Renkl, A.: 1997, Learning from worked-examples: A study on individual di¡erences. Cognitive Science, 21(1), 1^30.
Reye, J.: 1998, Two-phase updating of student models based on dynamic belief networks.
Proceedings of the 4th International Conference on Intelligent Tutoring Systems, ITS
‘98, San Antonio, TX, U.S.A., Lecture Notes in Computer Science 1452, Springer,
pp. 274^283.
Russell, S. and Norvig, P.: 1995, Arti¢cial Intelligence: A Modern Approach. Los Altos, CA,
Morgan-Kaufman.
Schulze, K. G., Correll, D., Shelby, R. N., Wintersgill, M. C. and Gertner, A.: 1998, A CLIPS
problem solver for Newtonian physics force problems. C. Giarratano, and G. Riley
(eds.). Expert Systems Principles and Programming. Boston, MA, PWS Publishing Company.
Schulze, K. G., Shelby, R. N., Treacy, D. J., Wintersgill, M. C., VanLehn, K. and Gertner, A.:
2000, Andes: An intelligent tutor for classical physics. The Journal of Electronic Publishing 6(1), The University of Michigan Press.
Shelby, R. N., Schulze, K. G., Treacy, D. J., Wintersgill, M. C. and VanLehn, K.: 2001, The
Andes Intelligent Tutor: an Evaluation. Proceedings of the Physics Education Research
Conference, Rochester, NY.
Shute, V. J.: 1995, SMART: Student Modeling Approach for Responsive Tutoring. User
Modeling and User-Adapted Interaction, 5(1), 1^44.
Shute, V. J. and Glaser, R.: 1990, A large-scale evaluation of an intelligent discovery world.
Interactive Learning Environments, 1, 51^76.
Shute, V. J. and Psotka, J.: 1996, Intelligent Tutoring Systems: Past, Present and Future. D.
Jonassen (ed.) Handbook of Research on Educational Communications and Technology.
Scholastic Publications.

416

CRISTINA CONATI ET AL.

Singley, M. K. and Anderson, J. R.: 1989, Transfer of Cognitive Skill. Cambridge, MA.,
Harvard University Press.
Tulving, E. and Thomson, D. M.: 1973, Encoding speci¢city and retrieval processes in episodic
memory. Psychological Review, 80, 352^373.
Van Mulken, S.: 1996, Reasoning about the user’s decoding of presentations in an intelligent
multimedia presentation system. Proceedings of UM ‘96, 5th International Conference
on User Modeling, Kailua Kona, HW, U.S.A., pp. 67^74.
VanLehn, K.: 1988, Student modeling. M. Polson and M. Richardson (eds.). Foundations of
Intelligent Tutoring Systems. Hillsdale, NJ, Lawrence Erlbaum Associates, pp. 55^78.
VanLehn, K.: 1996, Conceptual and meta learning during coached problem solving. Proceedings of Intelligent Tutoring Systems, 3rd International Conference, ITS ‘96, Montreal,
Canada, Lecture Notes in Computer Science 1086, Springer, pp. 29^47.
VanLehn, K., Freedman, R., Jordan, P., Murray, C., Osan, R., Ringenberg, M., Rose, C. P.,
Shultze, K., Shelby, R., Treacy, D., Weinstein, A. and Wintersgill, M.: 2000, Fading
and deepening: The next steps for Andes and other model-tracing tutors. Proceedings
of Intelligent Tutoring Systems, 5th International Conference, ITS2000, Montreal,
Canada, Lecture Notes in Computer Science 1839, Springer, pp. 474^483.
VanLehn, K. and Martin, J.: 1998, Evaluation of an assessment system based on Bayesian
student modeling. International Journal of Arti¢cial Intelligence in Education, 8(2),
179^221.
VanLehn, K. and Niu, Z. 2001, Bayesian student modeling, user interfaces and feedback: A
sensitivity analysis. International Journal of Arti¢cial Intelligence in Education, 12,
154^184.
VanLehn, K., Niu, Z., Siler, S. and Gertner, A. S.: 1998, Student modeling from conventional
test data: A Bayesian approach without priors. Proceedings of the 4th International Conference on Intelligent Tutoring Systems, ITS ’98, San Antonio, TX, U.S.A., Lecture Notes
in Computer Science 1452, Springer Verlag, pp. 434^443.
Zukerman, I. and Albrecht, D. V.: 2001, Predictive Statistical Models for User Modeling.
User-Modeling and User-Adapted Interaction, 11(1^2), 5^18.
Zukerman, I., Albrecht, D. and Nicholson, A.: 1999, Predicting Users’ Requests on the
WWW. Proceedings of UM ’99, the 7th International Conference on User Modeling, Ban¡,
Canada, Springer-Verlag, pp. 275^284.

USING BAYESIAN NETWORKS

417

Authors’ Vitae
Cristina Conati is an Assistant Professor of Computer Science at the University of
British Columbia. She received her ‘Laurea’ degree (M.Sc. equivalent) in Computer
Science at the University of Milan, and her M.Sc. and Ph.D. in Artiﬁcial Intelligence
at the University of Pittsburgh. Her research interests include user modeling, reasoning under uncertainty, adaptive interfaces and intelligent learning environments. Her
current research focuses on extending the range of user’s features that can be captured in a user model – from purely cognitive features (knowledge, goals, preferences), to meta-cognitive skills (such as learning capabilities), personality traits and
emotional reactions, in order to widen the spectrum of information that an interactive system can use to adapt its behavior to a user’s needs.
Abigail Gertner is a Lead Scientist at The MITRE Corporation, in the Information
Technology Center’s department of Information Management & Instructional Systems. She received her A.B. in Psychology from Harvard University, and her
M.S.E. and Ph.D. in Computer and Information Science from the University of
Pennsylvania. Her research interests are primarily in the areas of plan recognition,
user modeling, and cooperative response generation in decision support and intelligent tutoring systems. She has pursued these interests in the context of TraumAID, a
decision support system for emergency medicine, and Andes, an intelligent tutoring
system for university Physics. She is currently developing an ITS to train users of
complex software applications.
Kurt VanLehn is a Professor of Computer Science at Carnegie Mellas University a
Senior Scientist at the Learning Research and Development Center, and Director
of CIRCLE, an NSF Center. He received a BS from Stanford (Mathematics,
1974) and a PhD from MIT (Computer Science, 1983). Dr. VanLehn’s research
interests focus on applications of artiﬁcial intelligence to education. He is currently
engaged in 3 research projects. Circle is an NSF-supported research center that is
ﬁnding out why human tutoring is so effective and building computer tutors that will
equal or even exceed their effectiveness. Circle is a collaborative effort involving 7
professors from CMU and the University of Pittsburgh. The Andes project is developing an intelligent ‘homework helper’ that increases physics student learning without requiring changes in the way the course is taught. The Why2000 project is
developing a natural-language based tutoring system for helping students learn
how to explain ‘why’ physical systems work the way they do.

Reinforcement Learning-based Feature Selection For
Developing Pedagogically Effective Tutorial Dialogue
Tactics
Min Chi, Pamela Jordan, Kurt VanLehn, Moses Hall
{mic31, pjordan+, vanlehn+, mosesh}@pitt.edu
Learning Research Development Center & Intelligent System Program,
University of Pittsburgh
Abstract. Given the subtlety of tutorial tactics, identifying effective pedagogical
tactical rules from human tutoring dialogues and implementing them for dialogue
tutoring systems is not trivial. In this work, we used reinforcement learning (RL) to
automatically derive pedagogical tutoring dialog tactics. Past research has shown
that the choice of the features significantly affects the effectiveness of the learned
tactics. We defined a total of 18 features which we classified into four types. First,
we compared five feature selection methods and overall upper-bound method seems
to be most efficient. Then we compared the four types of features and found that
temporal situation and autonomy related features are significantly more relevant and
effective to tutorial decisions than either performance or situation related features.

1

Introduction

One challenging issue confronting dialogue tutoring systems is how to select the best tutoring
actions for any student state so as to maximize learning gains. We call this decision making
tutoring tactics since it generally governs brief episodes of tutoring dialogue, such as a single
step, and seems to be crucial for achieving further improvements in pedagogical effectiveness.
Bad tutorial decisions have the potential to bore or frustrate students to the point that they fail to
learn. Many existing dialogue systems use hand-crafted tactical rules derived from analyzing
human dialogues [6,13]. However, identifying and implementing these tactical rules is not trivial.
It has been shown that expert human tutors generally employ a range of tutoring actions and their
decisions depend in subtle ways on the student’s competence, student’s self-confidence and other
factors. For example, one important tutoring tactic is whether the tutor should tell the student the
answer directly for a step or whether he should elicit the step from the student with a prompt or a
series of questions. Collins et al. suggest that when a student is unfamiliar with the target
knowledge, the tutor should tell it directly; when a student becomes more familiar with the
knowledge, the tutor should elicit the answer via questions; and once a student has mastered the
knowledge, it doesn't matter whether it is told or elicited [4,5].
Most existing dialogue tutoring systems with hand-crafted rules react sensibly when students
exhibit less than optimal behaviors; however, their tutorial interactions are much stiffer than those
of human tutors and they have not yet achieved the same effectiveness that one-on-one, face-toface expert human tutors have [1]. In recent years, work in designing spoken dialogue systems
has proposed several data-driven methodologies; one of them is Reinforcement Learning (RL)
[3,12,13]. It has been shown that RL is effective at automatically learning the best action to take
at any state in a dialogue. And such success inspired us to use RL for dialogue tutoring systems.
Our work can be divided into three stages. In the first stage, we built an initial tutorial dialogue
system and collected an exploratory corpus by using the system to train 64 students. In this initial
system, decisions on certain types of tutoring actions were made randomly. In the second stage,
we used RL on the exploratory corpus to derive tutoring tactics for these actions and then
incorporated the learned tactics into the initial system. The modified system is identical to the
initial one except that the tutoring actions that were previously made randomly are now based on

the learned tutoring tactics. In the last stage, the modified system is being used to train a new
group of students so that we compare their learning performance to that of the students previously
trained on the initial system. We expect the modified system to be so well attuned to students that
it will be more effective than the initial system.
One tactical decision investigated in this study was whether the tutor should tell the next step to
the student or elicit it from the student. We defined 18 features to model the dialogue and
students’ state. They were selected based upon the four types of features that were shown in [7] to
be relevant for human tutors to making their tutorial decisions: autonomy, temporal situation,
situation, and performance respectively. Among 18 features, features 1-3 are autonomy related
based upon the amount of the work that the tutor has let students do; features 4-8 cover timerelated information such as time spent on the training so far; features 9-11 are situation related
including whether the student and tutor are now problem solving or in post discussion, the
difficulty level of the problem and so on; and finally, features 12-18 cover the performance
information such as the correctness of student’s previous answers and the ability of student.
In an RL model, the size of the state space increases exponentially as the number of involved
features increases. In order to learn effective tutoring tactics, we should have a corpus that covers
each of these states at least once, which means ≥ 218 in our case. However, it is almost impossible
to do so given the high cost of collecting educational data. On the other hand, the learned policy
may become too subtle to be necessary. Figure 1 shows an example of a learned policy involving
five features: [1, 2, 4, 15, 16], in which feature 2 is an autonomy feature defined as the percentage
of elicitation the students have received so far. Each of the five features was converted from a real
number to a binary value based upon their median scores, for example, f2='.4982' means if
feature 2 value is above .4982, it is 1 otherwise, it is 0. There were a total of 32 rules learned: in
10 situations the tutor should elicit, in 19 it should tell; in the remaining 3 cases either will do.
For example, when all of the features are 0 then the tutor should tell as 0:0:0:0:0 the first in the
list of tells. As you can see, five features provide a large space is already quite complex and is
already much more subtle than most of the tutorial tactics derived from analizing human turorial
dialogues [4,5].
KC22

'features'=[1, 2, 4, 15 ,16],
'cutoff'=[f1=’1.0000' f2='.4982' f4='56.0000' f15='.6154' f='.2683’ ],
'policy':
'elicit: [0:0:0:0:1, 0:0:0:1:0, 1:0:0:0:0, 1:0:0:0:1, 1:0:0:1:0, 1:1:0:1:1,
0:0:1:0:0, 0:0:1:0:1, 1:0:1:0:0, 1:0:1:0:1],
'tell: [0:0:0:0:0, 0:0:0:1:1, 0:1:0:0:0, 0:1:0:0:1, 0:1:0:1:0, 0:1:0:1:1, 1:1:0:0:1,
1:1:0:1:0, 0:0:1:1:0, 0:0:1:1:1, 0:1:1:0:0, 0:1:1:0:1, 0:1:1:1:0, 0:1:1:1:1,
1:0:1:1:1, 1:1:1:0:0, 1:1:1:0:1, 1:1:1:1:0, 1:1:1:1:1]
'else: [1:0:0:1:1, 1:1:0:0:0, 1:0:1:1:0]
Figure 1. A Learned Policy Based On Five Features On KC22

Based on the size of the exploratory corpus we collected in the first stage, we decided that no
more than six features should be used. Previous research involving RL and feature selection in
dialogue systems either focused on selecting features with certain characteristics [2,7,8] or
investigated a relatively small number of features [10,11]. Therefore, in this study, we proposed
four general RL-based feature selection methods.

2

Background

Past research on using RL to improve spoken dialogue systems has commonly used Markov
Decision Processes (MDP’s). An MDP [8,9] describes a stochastic control process and formally
corresponds to a 4-tuple (S,A,T,R), where S = {si}i=1..m, is a finite set of process states;
A={ak}k=1..m is a finite set of actions; T : S × A × S → [0, 1] is a set of transition probabilities
between states that describe the dynamics of the modeled system; for example: Pt(si|sj, ak) is the
probability that the model would transition from state sj to state si by taking action ak at time t;
and R : S × A × S → R denotes a reward model that assigns rewards to state transitions and
models payoffs associated with such transitions. The goal of using MDPs is to determine the best
policy π*: the set of actions the model should take at each state si to maximize its expected
cumulative utility (V-value), which can be calculated from the following recursive equation:

V ( si ) = ∑ Psπi s*j( si ) [ Rsπi s*j( si ) + γV( s j )]
sj

As long as a proper state space, action space and reward function are set up, an MDP allows one
to automatically derive and compute the best policy. For dialogue tutoring systems, deriving
effective tutorial tactics from tutoring dialogues can be naturally cast in the MDP formalism: for
the state space, each si can be viewed as a vector of features representing the tutoring dialogue
context, the student’s knowledge level and so on. Action space corresponds to tutoring actions,
e.g. elicit or tell; the reward function corresponds to students’ learning gains. For tutoring
dialogues, the reward is a delayed reward because for each state the reward is not known until
students have completed the tutoring and taken the post-test.
In this work, we used an MDP package that was designed for Tetreault & Litman’s studies since
it has proven to be both reliable and successful [10,11]. In previous studies, Tetreault & Litman
primarily investigated methods for evaluating whether certain features would improve policy
effectiveness. There are two main differences between our study and theirs: firstly, they used a
previously collected corpus that was not exploratory with respect to tutorial tactics to train an
MDP model in that the tutor often used only one type of action in many dialogue states, which
severely limited the types of questions that they could investigate[10,11]; while we use a more
exploratory corpus by training students on a dialogue tutoring system in which multiple actions
can often be taken and random tactical decisions were made. Thus it is better suited for creating
an MDP. Secondly, they did not need to address the problem of general feature selection methods
since they only used five features while we had to select up to six of 18.
To evaluate the learned policies, we use three criteria: Expected Cumulative Reward (ECR) and
lower-bound and higher-bound of the 95% confidence interval. ECR is the average reward one
would expect in the MDP and it is calculated by normalizing the V-value of each state by the
number of times it occurs as a start state in a dialogue and then summing over all states [11]. The
confidence interval represents how reliable the ECR of a learned policy is [10]. The wider it is,
the less reliable the policy’s ECR is.
For example, for a learned policy A derived from feature 2 alone on definition of spring potential
energy (KC22) is “if the percent of elicit so far is less than 49.82%, tutor should elicit otherwise
the tutor should tell.”; A has ECR = 3.02 (range [-100, 100]) with a 95% confidence interval= [2.71, 8.45], which means there is a 95% chance that the ECR of the learned policy is between a
lower-bound of -2.71 and an upper-bound of 8.45. In Figure 1, also on KC22 but involves five
features: 1, 2, 4, 15 and 16. It has ECR = 44.29 with a 95% confidence interval= [23.49, 50.51],
which is much more effective than policy A because even its lower-bound is much higher than
policy A’s upper-bound.

Sometimes we encounter situations in which the ECR for A is the same as for B; but the
confidence interval of A is much narrower than that of B. By only using the three criteria
described above, policy A and B cannot be compared. Therefore, we define the hedge of a
learned policy as:
Hedge =

ECR
({upper − bound } − {lower − bound })

By using the hedge, we can say that policy A is better than policy B since the hedge of A is
higher than the hedge of policy B.

3

Experiment

The domain chosen for this study is work and energy covered in college physics. The procedure
was as follows students: (1) read a short textbook, which describes the major principles and
concepts; (2) take a pre-test; (3) work through seven open-ended training problems with a
dialogue tutoring system (Cordillera [14]); and (4) take a post-test that is identical to the pre-test.
In the stage one, 64 college students who had not taken any college physics completed the
experiment, receiving payment for their participation. Students needed 8-15 hours to complete the
procedures and usually required 4-6 sessions of about 2 hours each. Thus, the collected corpus
comprises 64 human-computer tutoring dialogues and each dialogue is an extended interaction
with one student that covers seven different college-level physics problems. The number of stateaction pairs in each of the 64 collected tutorial dialogues varies from 700 to 900.
All training problems, and all pre- and post-tests problems were selected from 127 quantitative
and qualitative problems collected from various physics literature. In order to solve these 127
problems, 32 unique knowledge components (KCs) were identified as necessary. A KC is “a
generalization of everyday terms like concept, principle, fact, or skill, and cognitive science terms
like schema, production rule, misconception, or facet” [14]. For example, KC22 1 is both a
concept and a principle while KC23 2 is a fact. KC22 consists of both procedural knowledge and
declarative knowledge while KC23 is mainly declarative knowledge and thus learning these two
KCs clearly involves different cognitive skills. Therefore, for different KCs, we expect that
different features should be considered when making a tactical decision and different tactical
decisions should be derived.
In order to learn KC-specific tutorial tactics, students’ pre- and post-test scores were also
calculated per KC. It turned out our training, pre- and post-problems covered 27 of the 32
possible KCs. Three of the 27 KCs showed up in the tutoring dialogue but were not associated
with any action decision points; five KCs coincided only once or twice with decision points; and
one KC did not appear in the pre- and post-tests. Therefore, we were left with 18 KCs for which it
is possible for us to derive tutoring tactics. Comparisons of pre- and post-test scores indicated that
students did learn during their training with Cordillera: their post-test scores were significantly
higher than their pre-test scores: t(126)= 3.36, p= 0.001. Similar to Tetreault & Litman’s work,
we used students’ normalized learning gains (NLGs) for each KCi as the delayed reward function,
which is defined as:
NLG ( KCi ) =

1

Posttest ( KCi ) − Pr etest ( KCi )
.
1 − Pr etest ( KCi )

If an object and a spring are in a system, then their spring potential energy is 0.5* k*d^2, where k is the
spring constant and d is the displacement of the object relative to the equilibrium position of the spring.
2
The unit for energy is the Joule.

For each KCi, the 64 students were split into two groups by the median value of their NLG on it,
so the better-performing students’ dialogues were given a positive reward of +100, while the
other half were assigned a negative reward of -100. The rewards were assigned in the final
dialogue state.
Additionally, we annotated our tutoring dialogues and action decisions based on which KCs a
tutor action or tutor-student pair of turns covered; Table 1 shows an example. Here lines 1, 5, 6,
7, 9 and 11 are tutor turns, the action type and their target KCs are listed on the right. For
example, in the first utterance is an elicit action regarding KC22 while the fifth is a tell action
directly stating KC22. Lines 2 and 12 are student responses and they are both correct (as
indicated by the ‘+’); row 2 is about KC22 while row 12 is about KC27.
Table 1. Examples of Tutorial Dialogue with KC Labels

4

1. T: At T2 we also know the spring is uncompressed. So, what type of energy
can we easily find now?
2. S: spe
3. T: Yes, I agree.
4. T: At this point we know the values for KE2 and GPE2. It is time to compute
the value for SPE2.
5. T: It is important to remember that we cannot find the SPE of an OBJECT. We
need to find the spring potential energy of a pair: object-spring.
6. T: It is possible to compute the spring potential energy at T2 because we know
the spring is uncompressed. That is, d2=0 m.
7. T: Using this information we can infer that the spring potential energy of the
block-spring pair at T2 is SPE2=0 J.
8. T: Now we have all we need to…

elicit,
KC22
+KC22

9. T: TME of the system at T2 is the sum of the block`s KE and the block-Earth
pair`s GPE because the value for the spring potential energy is zero. In this
case TME2=11.88 J+1.62 J =13.5 J.
10. T: At this point we have found the value for TME2. We can now easily find the
system`s TME at all other time points since the system is isolated.
11. T: What principle gives us the system`s total mechanical energy at every time
point, knowing its value at T2 and also given that the system is isolated?
12. S: student conservation of energy
13. T: Yes.

tell, KC23
KC24

tell, KC22
tell
tell, KC22
KC23

elicit,
KC27
+ KC27

Methods

The four feature selection approaches we propose in this paper are fairly straightforward and
share the same procedure, which consists of the three phases described below.
Phase 1: For each of 18 features in the state space, use MDP to get a singlefeature-policy.
Phase 2: Sort all of the features based on the learned single-feature-policies from
high to low by one of the following measures:
– ECR
– Lower-bound
– Upper-bound
– Hedge
Phase 3: Start with the first feature in the sorted feature list, add one feature at a
time to the MDP and learn a new policy. Repeat this process repeat 5 times.

Based on the sorting criteria in phase 2, we named our four feature selection methods: ECR,
lower-bound, upper-bound, and hedge respectively. Since these values were calculated from the
single-feature-policies learned using MDP in phase 1, the four methods are RL-based. We expect
them to be more effective than a random selection. To test our expectations, we created 120
random policies by running the MDP package on randomly selected features (20 rounds for each
randomly selected feature set, where each set contained between one and six features). Therefore,
for each KC and each feature set size, we learned one tutoring tactic for each RL-based method
plus 20 for random feature selection. This gave us (1*4+20)*6 =144 policies.
For each KCi, we selected one policy that had the highest ECR, lower-bound and upper-bound
from the 144 learned policies and named it the “best policy”. To quantitatively evaluate how
much less effective a learned tutoring tactic is than the best policy, we defined a normalized ECR
(NECR) for a learned tutoring tactic as:
NECR(KC i , N, Method k ) =
Max _ ECR ( KC i ) =
Min _ ECR ( KC i j ) =

ECR(KC i , N, Method k ) - Min_ECR(KC i )
1
∑ Max_ECR(KC
C C
i ) - Min_ECR(KC i )
max

( ECR ( KC i , N , Method m )

min

( ECR ( KCi , N , Method m )

N ∈{1..6}, m∈{all methods}

N ∈{1..6}, m∈{ all methods }

Max_ECR(KCi) and Min_ECR(KCi) are the maximum and minimum ECR among all 144 of the
learned policies for KCi. C is a constant with C=1 for each of the four RL-based methods and
C=20 for random feature selection. The maximum NECR for a learned policy is 1 if the learned
policy is the best one and the minimum is 0 if the learned policy is the worst one.

5

Results

In order to evaluate the feature selection methods and how the feature effectiveness, we have two
main goals. First, we will compare the four RL-based feature selection methods against random
feature selection; second we will investigate which features seem to be more important for
deriving the best tutoring tactics for deciding to elicit/tell across all KCs.

5.1 Comparing four RL-based methods against random feature selection
Table 2. Comparing the Average NECR of Five Selection Methods for Increasing Feature Set Sizes
Number Features Involved
1
2
3
4
5
6

Upper-bound
0.345
0.355
0.416
0.550
0.682
0.673

ECR
0.372
0.370
0.447
0.515
0.579
0.614

Hedge
0.337
0.335
0.400
0.520
0.573
0.594

Lower-bound
0.355
0.314
0.352
0.419
0.435
0.485

Random
0.119
0.196
0.275
0.348
0.422
0.480

NECR is defined as how much a learned tutoring tactics is less effective than the best policy.
Table 2 shows the average NECR given the number of selected features and feature selection

method across all 18 KCs. As expected, random feature selection has the worst average NECR
regardless of the number of features involved. Overall, if the number of features ≤3, then the ECR
approach is the best feature selection method; but if the number of features is between 4 and 6,
then the upper-bound method is the best. As the number of features involved increases, the
effectiveness of the learned policies for elicit/tell tends to become better, except for upper-bound,
which has a better average NECR when there are five features than when there are six. Overall,
using the upper-bound method to select five features seems to be the most effective method
across all KCs on elicit/tell action decision.
On the other hand, across all 18 KCs the number of times that each method found the best policy
is: fourteen times for random, three for upper-bound, two for ECR, one for hedge and none for
lower-bound method. The total did not add up to 18 KCs because for KC1, three methods, ecr,
hedge, and upper-bound all found the same best policy. Therefore, although random feature
selection has the worse average NECR than other four RL-based methods, it is an effective way
to find best policies. However, note that the random feature selection method was repeated 120
times for each KC and so it has a total of 120*18=2160 chances to find the 14 best policies; while
the upper-bound method was applied only 6 times for each KC and thus has a total of 108
chances. Additionally, because our state space is still relatively small, we expect that the
performance of random selection would decrease significantly as the number of features in the
state space increases. However, for the four RL-based feature selection methods, increasing the
number of features would not decrease their effectiveness since they do not directly depend on
the number of features in the state space.
Moreover we compared the best policy learned for each KC by the upper-bound method and
those learned by random selection. Over all 18 KCs, the upper-bound policies with just 108
attempts were only 9.46% less effective than random feature selection policies with 2160
attempts. These results indicate that that in education, features that may result higher learning
gains should always be always be considered by the tutor when making decisions. This is likely
due to the fact that in the worst case a student will simply not learn rather than lose information
so the cost of incorporating superfluous feature is low..

5.2 Frequency of Features in Best Policies

Figure 2. The Frequency of Each Feature Shown In Best Policies.

Figure 2 shows the frequency with which each feature appears in the best policies. This
frequency differs significantly among the four feature types: F(3)= 7.47, p=0.003. There is no
significant difference between the three autonomy and five temporal situation related features.
When combined they are significantly more frequent than either the performance or situation
related features: t(12)= 2.74, p=0.18 and t(10)= 4.26, p=0.002 respectively. Consistent with
previous research based on analyzing human tutorial dialogue, autonomy related features seemed
to be more relevant in deriving effective tutorial tactics. Additionally, we found that temporal

situation related features were also relevant, even more so than the performance related ones
when deciding whether to elicit or tell. This was not indicated in previous literature on human
tutorial dialog analysis. One possible explanation for this was that in most of the prior literature,
temporal situation related factors were often not considered.

6

Conclusions and Future Work

In this paper, we described our work on applying RL methods to derive effective tutoring tactics
for elicit/tell. We showed that deriving effective tutoring tactics from tutoring dialogues can be
cast in the MDP formalism. Additionally, we proposed four RL-based domain-general feature
selection methods. We found the upper-bound method to be more effective than the others.
One of our goals for future work is to investigate how to still get reasonable policies without
annotating individual KCs in the dialogues. The annotation process is prohibitively timeconsuming and it is not unusual for domain experts to disagree [14]. Another of our goals for
future work is to determine how to learn one reasonable policy for all KCs without sacrificing too
much of the expected effectiveness. Another two important issues are how to avoid the expensive
initial data collection and how to combine the new data with the existing data so that we can learn
even more powerful policies.
Acknowledgments
We would like to thank the NLT group and Diane J. Litman for their comments. Support for this
research was provided by NSF grants #0325054.

References
[1] B. S. Bloom, “The 2 sigma problem: The search for methods of group instruction as effective as oneto-one tutoring.,” Educational Researcher, 1984. no. 13, p. 4–16,
[2] M. Frampton and O. Lemon. Reinforcement learning of dialogue strategies using the user's last
dialogue act. In IJCAI Wkshp. on K&R in Practical Dialogue Systems, 2005.
[3] J. Henderson, O. Lemon, and K. Georgila. Hybrid reinforcement/supervised learning for dialogue
policies from communicator data. In IJCAI Wkshp. on K&R in Practical Dialogue Systems, 2005.
[4] Collins, A., and Stevens, A Goals and Strategies of Inquiry Teachers: In R. Vlaser (Ed.) Advances in
Instructional Psychology, 1982. vol. 2 Hillsdale, NJ: Erlbaum Assoc. p. 65-119.
[5] Collins, A.. Design issues for learning environments. In S. Vosniadou, E. D. Corte, R. Glaser, & H.
Mandl (Eds.), International perspectives on the design of technology-supported learning environments,
1996. p. 347-361. Mahwah, NJ: Erlbaum,
[6] M. Evens and J. Michael, One-on-One Tutoring by Humans and Computers, Lawrence Erlbaum
Associates, Inc., 2006.
[7] Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian Varges, Claus Zinn: Generating Tutorial
Feedback with Affect. FLAIRS Conference, 2004.
[8] S. Singh, M. Kearns, D. Litman, and M. Walker. Reinforcement learning for spoken dialogue systems.
In Proc. NIPS ’99, 1999.
[9] R. Sutton and A. Barto. Reinforcement Learning. The MIT Press, 1998.
[10] J. Tetreault, D. Bohus, and D. Litman, “Estimating the reliability of mdp policies: a confidence interval
ap proach,” in NAACL, 2007.
[11] J. Tetreault and D. Litman. Comparing the utility of state features in spoken dialogue using
reinforcement learning. In NAACL, 2006.
[12] J. Williams, P. Poupart, and S. Young. Factored partially observable markov decision processes for
dialogue management. In IJCAI Wkshp. on K&R in Practical Dialogue Systems, 2005.
[13] M. Walker. An application of reinforcement learning to dialogue strategy selection in a spoken
dialogue system for email. JAIR, 12, 2000.
[14] VanLehn, K, Jordan, & Diane. Developing pedagogically effective tutorial dialogue tactics:
Experiments and a testbed. In proceedings of SLaTE Workshop, 2007.

A Comparison of Decision-Theoretic, Fixed-Policy
and Random Tutorial Action Selection
R. Charles Murray1 and Kurt VanLehn2
1

Carnegie Learning, Inc., Frick Building, 20th Floor,
437 Grant St., Pittsburgh, PA 15219
cmurray@carnegielearning.com
2
Computer Science Department & Learning Research and Development Center
University of Pittsburgh, Pittsburgh, PA 15260
vanlehn@cs.pitt.edu

Abstract. DT Tutor (DT), an ITS that uses decision theory to select tutorial actions, was compared with both a Fixed-Policy Tutor (FT) and a Random Tutor
(RT). The tutors were identical except for the method they used to select tutorial actions: FT employed a common fixed policy while RT selected randomly
from relevant actions. This was the first comparison of a decision-theoretic tutor with a non-trivial competitor (FT). In a two-phase study, first DT’s probabilities were learned from a training set of student interactions with RT. Then
a panel of judges rated the actions that RT took along with the actions that DT
and FT would have taken in identical situations. DT was rated higher than RT
and also higher than FT both overall and for all subsets of scenarios except help
requests, for which DT’s and FT’s ratings were equivalent.

1 Introduction
Intelligent tutoring systems (ITSs) that coach students as they attempt tasks often
emulate the turn taking observed in human tutorial dialog [1; 2]. The tutor’s main
task can be seen as deciding what action to take on its turn, or tutorial action selection. Selecting tutorial actions involves inherent difficulties.
A significant source of difficulty is that the tutor is uncertain about the student’s internal state because it is unobservable and changes over time (e.g., as the student
learns). Furthermore, the tutor is uncertain about the effects of the tutor’s actions on
the student. Many ITSs [see, e.g., 3] have modeled the tutor’s uncertainty in terms of
probability using Bayesian techniques [4] for sound yet relatively efficient inference.
Another difficulty is that just what constitutes effective tutorial action depends
upon the tutor’s objectives and priorities. The tutor’s objectives are likely to include
increasing the student’s knowledge, helping the student complete tasks, bolstering the
student’s affective state, and being a cooperative discourse partner. It may not be
possible to maximize attainment of all objectives at once, in which case the effectiveness of the tutorial action alternatives depends upon the tutor’s priorities. Tutors must
often strike a “delicate balance” among multiple competing objectives [5, p.280; 6; 7].
Decision theory extends probability theory by considering, in addition to uncertainty, the decision-maker’s objectives and priorities as a rational basis for making
decisions [8]. DT Tutor (DT) [9] uses decision-theory to decide the tutor’s actions.
M. Ikeda, K. Ashley, and T.-W. Chan (Eds.): ITS 2006, LNCS 4053, pp. 114 – 123, 2006.
© Springer-Verlag Berlin Heidelberg 2006

A Comparison of Decision-Theoretic, Fixed-Policy and Random Action Selection

115

For each alternative, DT computes (1) the probability of every possible outcome of
that tutorial action, (2) the utility of each possible outcome relative to the tutor’s objectives and priorities, and then (3) the alternative’s expected utility by weighting the
utility of each possible outcome by the probability that it will occur. DT then selects
the tutorial action with maximum expected utility. This approach unifies considerations regarding (1) the tutor’s uncertain beliefs about the changing tutorial state and
the tutor’s effects upon it, and (2) the tutor’s objectives and priorities.
One advantage of a decision-theoretic approach is the capability to balance multiple tutorial objectives in a principled way. DT leverages this capability by simultaneously considering the student’s knowledge, focus of attention, and affective state,
along with joint task progress and the student-tutor discourse state. Another advantage is that by looking ahead to anticipate student difficulties and the influence of the
tutor’s actions, DT can provide proactive help to attempt to prevent student difficulties in addition to the reactive help that most ITSs provide.
While many ITSs have used Bayesian networks for reasoning under uncertainty
[3], decision-theoretic approaches remain rare, and comparisons of decision-theoretic
approaches with non-trivial competitors are rarer still. CAPIT [10] and iTutor [11]
appear to be the only other decision-theoretic tutors that have been implemented and
evaluated. However, these tutors were compared only with no tutoring at all [10],
with consulting the teacher when required [11], and with randomized action selection
[10]. Our work does not directly assess effectiveness with students, but it does compare decision-theoretic tutoring against a higher standard: a Fixed-Policy Tutor (FT)
that selects tutorial actions by emulating the fixed policies employed by successful tutors such as Andes1 [12] and the Cognitive Tutors [13], which are theory-based [14],
widely-used and highly effective [15]. DT and FT were also compared with a Random Tutor (RT), which selects randomly from among relevant tutorial actions.

2 The Three Tutors: DT, FT and RT
DT, FT and RT shared the same user interface and help messages. All of the tutors
gave immediate flag feedback by highlighting correct responses in green and errors in
red. The only difference between the tutors was the method they used to select from
among the same tutorial action alternatives, which consisted of deciding whether to
provide a help message, and if so, which message to provide. The differences in their
performance were thus due solely to their action selection methods.
This study encompassed three different types of help situations. Most ITSs provide help (1) in response to help requests and (2) after some number of errors. Also
included were (3) step starts, which are opportunities for the tutor to provide proactive help at the start of a step before the student has reached an impasse or made an
error. Few ITSs provide help at step start, but human tutors sometimes do [e.g., 6;
16]. Help provided at step start or after an error is proactive since the student has not
asked for help and may not even want it. Responses to help requests are reactive.
The tutors could choose either to provide no help message (a null help message) or
to provide one of four different kinds of help messages: prompt, hint, teach, or do.
The prompt message pointed out pertinent information that was already available in

116

R.C. Murray and K. VanLehn

the interface without providing any new information. The hint message provided partial information about the step – not enough to teach the student how to do the step
but perhaps enough to either remind the student how to do the step or help the student
figure it out. The teach message provided all the information that the student needed
to understand the domain rule related to the step, including at least one example, and
thus to help the student complete the step correctly by learning the rule. The do message told the student exactly what to do for the current step (e.g., what text to enter)
without teaching anything about the related rule. These help messages are ranked in
order of increasing explicitness about what to do for the current step, from prompt
(the least explicit,), through hint, teach, and do (the most explicit).
RT randomly provided help relevant to the current step as follows: For proactive
help opportunities, RT decided randomly whether to provide proactive help. For reactive help opportunities, RT always provided help. When RT decided to provide
help, it decided in advance a random order for the four types of help messages and
then returned help types in that order, repeating the order cyclically if necessary.
FT, consistent with Andes1 [12] and the Cognitive Tutors [13], always provided
help after help requests, never provided help for step starts, and provided help after n
errors (two in this study). When FT provided help, it followed a strong successive
explicitness constraint: It always provided a help message that was minimally more
explicit than any help already provided. In other words, first it provided a prompt
message, then hint, then teach, before bottoming out with do. If the student continued
to request help after that, the do message was repeated.
DT’s help selection cannot be described in terms of a simple policy because it simultaneously considers multiple aspects of the tutorial state. However, one of these
aspects, the student-tutor discourse state (modeled so that DT can be a cooperative
discourse partner) did constrain DT’s help selections. The two discourse constraints
that DT followed were (1) to always provide help for help requests and (2) a weak
successive explicitness constraint: It never provided less explicit help than had already been provided (so, e.g., if the student requested more help after receiving a
teach message, the student wouldn’t be disappointed with a prompt message). Constraint (1) is the same as FT and RT. Constraint (2), weak successive explicitness, is
different than FT in that DT does not have to select the help message that is minimally
more explicit than any help already provided. This means, for instance, that DT can
provide a teach message as the first help provided, or progress directly from a prompt
message to a teach message. The other difference between DT and FT’s help selection is that DT always considers providing proactive help. It should be noted that DT
can easily be configured to emulate FT (and therefore Andes1 and the Cognitive Tutors, among others) by considering only the discourse state (giving it a utility of 1
while giving all other aspects of the tutorial state a utility of 0) and increasing DT’s
discourse constraints to (1) never provide help for step starts, (2) provide help after errors only after the nth error, and (3) follow a strong successive explicitness constraint.
Because DT and FT both followed a successive explicitness constraint, a subset of
the help opportunities were especially relevant for revealing differences between the
help selection strategies of DT and FT. These were first-message-opportunity scenarios (FMOs), in which the tutor has the opportunity to select the first help message to

A Comparison of Decision-Theoretic, Fixed-Policy and Random Action Selection

117

be displayed for the current step. For FMOs, which occurred in over half the 350
scenarios in the study described below, DT had free reign over which help message to
select (if any) while FT adhered to its fixed policy. First-message-opportunity scenarios were sometimes partitioned according to student performance on the pretest
problem that corresponded to the rule required to complete the problem step: pretestwrong and pretest-right. The idea behind this partitioning is that students who get a
pretest problem wrong are more likely than those who get it right to need help during
tutoring on steps that require knowledge of the rule tested by the pretest problem.
This is by no means a perfect test – e.g., the student might have merely slipped on the
pretest problem, or the student might have learned the rule since the pretest – but one
advantage is that it does not require subjective judgment by the experimenter. It must
be noted that DT was not given information about the pretest performance of students
in the test set. However, DT could glean information about the likelihood that a particular student in the test set knew a rule in two ways: (1) by the percentage of the
training set students who got the corresponding pretest problem correct (learned during phase I of the study as prior probabilities), and (2) by the student’s performance
during tutoring on steps related to the rule.

3 Study Design
A two-phase study design was employed. In the first phase, data collection and tuning, 60 students took a pretest, solved the same five multi-step calculus problems using RT, and then took a posttest. The students used RT so that we could collect data
about the effects of individual tutorial actions while statistically controlling for the effects of sequences of tutorial actions by randomizing over the sequences in which the
individual actions occurred. Students were allowed as much time as they needed to
complete the problems and most took about an hour. The student data was partitioned
into training and test sets of 30 students, which were matched according to pretest
scores. Logged student-tutor interactions from the training set, along with pre- and
posttest performance, were used to learn probabilities about student knowledge, student behavior, and the effects of tutorial actions. The data collection and tuning phase
is described in detail elsewhere [17]. This paper focuses on the assessment phase.
During the assessment phase, we replayed logged student-tutor interactions from
the test set while recording the responses that DT and FT would provide in the same
tutorial situations. When the actions selected by RT and DT differed, the action selected by RT was replayed in order to preserve the fidelity of the replay, and DT updated its model of the tutorial state to include the action actually provided by RT. A
similar process was undertaken to record the actions that FT would have taken for the
same situations. A panel of judges then rated the actions selected by RT, FT and DT
in a large sample of test set situations.
While this study design cannot provide conclusive information about the bottom
line – which tutor is most effective with students – it has other advantages. First, it
provided data for learning many of DT’s key probabilities. Second, it allowed us to
compare the action selections of different tutoring approaches in identical situations.
Third, it can provide information that is much more detailed than the bottom line

118

R.C. Murray and K. VanLehn

about what makes the tutors’ actions effective or not in particular situations [18],
information that can be used to improve not only DT but other tutors as well. Advantages two and three allowed us to decrease the grain size of the analysis from the student to the scenario – i.e., to help opportunities. With an estimated effect size for DT
over FT of 0.2 standard deviations, we needed about 320 samples from each of three
conditions (RT, FT and DT) using the conventional parameters of α = .05 and β = .20
[19], and 960 students was more than we could afford. Reducing the grain size to the
scenario allowed us to use many fewer students.

4 The Comparative Assessment
4.1 Subjects
Three paid judges were recruited from among graduate mathematics students who had
extensive experience tutoring calculus as well as other mathematics to college and
high school students. These judges were considered skilled, although not necessarily
expert, because of their extensive mathematical knowledge and tutoring experience.
4.2 Materials
For each scenario to be assessed, judges were given a detailed printed description that
included, among other things:
• A screen shot showing the student interface at the moment of the scenario.
• A description of the scenario, including whether the student had just requested help
or made an error, along with the correct entry for the current step.
• The student’s current number of correct entries, errors and help requests as a general indicator of the student’s problem-solving performance and help usage.
• The student’s performance on the pretest problem related to the current step.
• Relevant Action History: Previous student-tutor interactions on (1) any previous
steps that use the rule related to the current step, and (2) the current step.
Each scenario listed the five possible tutorial responses in random order (the help
messages corresponding to help types prompt, hint, teach and do, plus “no message”
for the null response) and the judges rated each on a scale of 1 (worst) to 5 (best).
The judges also chose their preferred response for each scenario.
Scenario Types and Stratified Sampling. For assessing the performance of the
different tutorial action selection methods, 350 scenarios were to be selected from the
test set. The intention was to select the scenarios randomly so as not to introduce any
bias. However, a completely random selection would have produced a highly skewed
sampling among help requests, errors and step starts. Of 5009 scenarios in the test
set, 57% were step starts, 35% were errors, and 8% were help requests. This was just
the opposite of what was desired for assessing help selection, for which help provided
for help requests is arguably most important, help provided for errors is probably next
most important, and it is debatable whether help should be provided for step starts at
all. With a completely random distribution, the judges’ ratings of the tutors would be

A Comparison of Decision-Theoretic, Fixed-Policy and Random Action Selection

119

dominated by their ratings for step start scenarios and only weakly influenced by their
ratings for help request scenarios. Therefore, a stratified sample was selected with the
sample for each stratum randomly selected from among all the scenarios in that
stratum: 175 help requests, 100 errors and 75 step starts.
One additional criteria was employed for selecting scenarios: Since RT selected
relevant actions randomly, it might for a specific step select, say, do help (the most
explicit), and then if the student was unsuccessful, select prompt (the least explicit
help). Such sequences of help messages violated even the weaker successive explicitness constraint. It was unclear just how DT or FT should respond following sequences of help messages that violated their own constraints. A related concern was
that it was unclear just how such seemingly odd (indeed, random) sequences of help
messages would affect the judges’ intuitions about what kind of help to provide next.
Therefore, scenarios whose Relevant Action History included sequences of tutorial
actions that violated the weaker successive explicitness constraint were excluded from
the sample.
4.3 Procedure
The judges rated all possible responses for 350 scenarios. Note that with this design it
is possible to use the same judges’ ratings to assess the tutorial action selections of
still more tutors, or updated versions of the same tutors, as long as they use the same
student interface and select from the same pool of help messages. The judges were
told that they were rating scenarios in order to provide information about what help
messages would be best to provide for various situations. They had no idea which tutor provided which responses or that their ratings would be used to compare tutors.
For comparing the tutors, we constructed composite judges’ ratings to better represent the population of skilled tutors. Our goal was to discount ratings that were outside the norm without excluding any ratings. To this end, we used the median rating
for each response. The median discounts the effect of the magnitude of outlying ratings while still taking their existence into account. With outlying ratings for individual responses thus discounted, composite ratings for sets of responses were computed
as the mean of the median ratings for each response.
Ratings for All Three Tutors by Scenario Type. Table 1 displays results of pairedsample t-tests comparing RT vs. DT and FT vs. RT for all scenarios and for each
scenario type, along with effect sizes and mean composite ratings. Effect sizes were
calculated as the difference in means divided by the standard deviation of the control
group: either RT or FT as applicable in their comparisons with DT.
As the table shows, the judges’ ratings for DT were higher than their ratings for RT
overall and for help requests, errors and first message opportunities, significant at
level p<.01 with effect sizes ranging from .33 to .49. Only for step start scenarios was
DT not rated significantly higher than RT after the Bonferroni correction for multiple
comparisons. However, the significance before the Bonferroni correction was p=.012
and the Bonferroni correction is known to be very conservative to protect against
Type I errors. The effect size for step starts was still a healthy .30.

120

R.C. Murray and K. VanLehn
Table 1. Tutor x Scenario Type, paired t-tests: RT vs. DT, FT vs. DT

Comparison
RT
Mean
RT vs. DT
All
2.94
Help Req
3.23
Errors
2.31
Step Starts
3.11
FMOs
2.99

DT
Mean
3.43
3.66
2.95
3.55
3.54

FT
Mean
3.08
3.59
2.10
3.19
3.12

DT
Mean
3.43
3.66
2.95
3.55
3.54

FT vs. DT
All
Help Req
Errors
Step Starts
FMOs
*

df

Bonferroni
Sig.*

Effect
Size

t

Sig.

349
174
99
74
187

5.746
3.937
3.324
2.572
5.057

<.001
<.001
.001
.012
<.001

<.01
<.01
.010
.120
<.01

.35
.33
.49
.30
.40

349
174
99
74
187

5.251
1.078
4.693
3.222
4.351

<.001
.282
<.001
.002
<.001

<.01
1.0
<.01
.020
<.01

.24
.06
.61
.22
.28

Significance with Bonferroni correction for 10 t-tests (Sig. x 10)

The judges’ ratings for DT were higher than their ratings for FT overall and for the
scenario types of errors, step starts and first message opportunities, all with significance p=.02 or less and with effect sizes ranging from .22 to .61. For help requests,
however, DT, with mean 3.66, and FT, with mean 3.59, were rated approximately
equivalently with a .06 effect size and a significance level (with Bonferroni correction) of approximately p=1.0.
DT vs. FT for Help Requests. Since DT’s and FT’s ratings were approximately the
same for help requests, one might expect that DT and FT selected mostly the same
tutorial responses in the same situations. However, their patterns of responses were
significantly different, with a Pearson’s chi-square test of association of χ2(3)=60.8,
p<.001. FT and DT also behaved significantly differently for FMO help requests, for
which FT always selected the prompt response according to its fixed-policy. DT’s
response selections varied: For the pretest-wrong scenarios, DT selected prompt only
34% of the time and teach 66% of the time, receiving a mean composite rating of 4.00
while FT received a mean composite rating of 3.55. This difference was significant,
t(28) = 2.218, p=.035. For pretest-right scenarios, DT selected prompt slightly more
often, 44% of the time, and received a mean composite rating of 3.80 while FT’s
responses (always prompt) received a higher mean composite rating, 4.02, although
this difference was not quite significant, t(44) = 1.634, p=.109. Apparently, the
judges generally preferred the teach response when the student was more likely to
need explicit help and the prompt response when the student was less likely to need
explicit help. DT adjusted its response selections according to the same preference
structure but did not adjust them enough when the student was less likely to need
explicit help.

A Comparison of Decision-Theoretic, Fixed-Policy and Random Action Selection

121

Table 2. FMO scenarios, paired t-tests: FT vs. DT
Comparison
FT
DT
df
t
Sig.
2.51
3.24
74 4.606 p<.001
Pretest wrong
3.53
3.73
112 1.776 p=.079
Pretest right
*
Significance with Bonferroni correction for 2 t-tests (Sig. x 2)

Bonferroni
Sig.*
p<.002
p=.158

Effect
Size
.55
.14

DT vs. FT for Errors. FT’s ratings for errors were significantly lower than DT’s
because it always selects a null response the first time the student makes an error. All
of the judges gave low ratings to null responses after errors. 68 out of the 100 error
scenarios involved the student’s first error, so FT received a low rating for most of the
error scenarios. For first errors, DT’s mean composite rating, 2.88, was significantly
higher than FT’s rating of 1.35, t(67)=8.516, p<.001, with a large effect size of 2.58.
On the 32 error scenarios that did not involve the student’s first error, FT, with a
mean composite rating of 3.69, was rated higher than DT, which had a mean of 3.09,
t(31) = 2.094, p=.044. This was in turn due to DT replying null on 13 of these 32
scenarios, for which it received a mean rating of only 1.23 compared to FT’s mean of
3.10. The bottom line is that our judges did not like null responses to errors.
DT vs. FT for Step Starts. As with errors, FT received lower ratings than DT for
step starts because of null responses. Per its fixed policy, FT always selected null
responses for step starts. DT did not reply null on 21 of the 75 step start scenarios,
and for these, DT’s mean composite rating, 3.67, was significantly higher than FT’s
mean composite rating of 2.38, t(20) = 3.959, p=.001, effect size .92. DT’s
significant advantage in ratings when it did not reply null led to a significant
advantage over FT in ratings for step scenarios overall, 3.55 versus 3.19, p=.020.
DT vs. FT for First-Message-Opportunity Scenarios (FMOs). FT always provided
either the null or the prompt response for FMO scenarios, while DT also included the
teach response and varied its responses according to the likelihood that the student
needed explicit help. These differences paid off as DT was rated significantly higher
than FT for FMOs, p<.01, effect size .28. A closer look shows that DT was nominally
rated more highly than FT both for pretest-wrong and for pretest-right scenarios, as
shown in Table 2. For pretest-wrong scenarios, DT’s mean composite rating is
significantly higher, p<.01, with effect size .55. For pretest-right scenarios, DT’s mean
composite rating is not significantly higher after the Bonferroni correction, p=.158.

5 Discussion
Fixed-policy tutors such as FT use a time-tested and proven, even theoretically-based
[13] policy for selecting the response type for tutorial actions. However, this policy
considers only (1) whether the student has just made a help request or the nth error,
and (2) the most recent response type for the current step. The result is response selections that are all the same regardless of other attributes of the tutorial situation.

122

R.C. Murray and K. VanLehn

FT emulates the policies of Andes1 and the Cognitive Tutors, which follow a
strong successive explicitness constraint and volunteer help only after n errors [13;
20]. This policy was based on psychological research showing that students remember material better when they generate it themselves. However, even the architects of
the Cognitive Tutors and the theory behind them admit that “these may not be the best
choices” since, for example, “[s]ome students stubbornly refuse to seek help even
when they need it” and “students are often annoyed with the vague initial messages
and decide there is no point in using the help facility at all” [13, p. 199]. Once students begin clicking past vague initial help messages, as many as 82-89% of students
click all the way through to bottom-out help [20].
DT, like human tutors [1; 5; 6], considers multiple tutorial state attributes to decide
when and how to provide help. These attributes include the student’s knowledge, affective state and focus of attention, along with task progress and the discourse state.
DT’s resulting sensitivity to the tutorial state was demonstrated, for instance, in its responses to first-message-opportunities, for which not only did DT’s responses vary
significantly for pretest-right versus pretest-wrong scenarios, but its ratings were
higher for both, significantly so for pretest-wrong scenarios. DT’s greater sensitivity
paid off in generally higher ratings from the judges.
A major reason why DT surpassed FT in the judges’ ratings was DT’s use of proactive help, which FT never provides for step start and first error scenarios. Proactive
help when a student would otherwise flounder can save time, prevent confusion, provide valuable information at a time when the student is prepared and motivated to
learn it, and avoid the negative affective consequences of frustration and failure.
DT’s consideration of multiple tutorial state attributes and the variability of its responses is more like human tutors, for whom the timing of feedback appears “to depend critically on the consequences of the particular error or impasse encountered” [5,
p.283]. When considering proactive help, human tutors “sometimes seek to forestall
errors, sometimes intervene as soon as errors occur; at other times they may allow errors to occur” [6, p.85]. Indeed, the very effectiveness of human tutorial help “may
arise because of the contingency of feedback style and content” [1, p. 346].
The bottom line in choosing a method for selecting tutorial actions is which technology delivers the desired capabilities for the least cost (in time and money). If the
desired behavior of the tutor is unambiguously defined and only simple capabilities
are required, fixed policy is best, no contest, because of its ease of implementation.
However, as more sensitivity is required, as the need for flexibility increases, or as the
desired behavior of the tutor becomes more ambiguous, decision-theoretic tutoring
becomes more attractive. Decision theory can enable a tutor to respond in a principled way to an unlimited variety of situations – even unanticipated situations – without having to come up with a fixed-policy for every combination of uncertain beliefs.

References
1. Merrill, D.C., B.J. Reiser, S.K. Merrill, and S. Landes (1995). Tutoring: Guided learning
by doing. Cognition and Instruction, 13(3): 315-372.
2. Graesser, A.C., N.K. Person, and J.P. Magliano (1995). Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive Psychology, 9: 495-522.
3. Jameson, A. (1996). Numerical uncertainty management in user and student modeling: An
overview of systems and issues. User Modeling and User-Adapted Interaction, 5(3-4):
193-251.

A Comparison of Decision-Theoretic, Fixed-Policy and Random Action Selection

123

4. Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. San Mateo, CA: Morgan-Kaufmann.
5. Merrill, D.C., B.J. Reiser, M. Ranney, and J.G. Trafton (1992). Effective tutoring techniques: A comparison of human tutors and intelligent tutoring systems. The Journal of the
Learning Sciences, 2(3): 277-306.
6. Lepper, M.R., M. Woolverton, D.L. Mumme, and J.-L. Gurtner (1993). Motivational techniques of expert human tutors: Lessons for the design of computer-based tutors, in Computers as Cognitive Tools, S.P. Lajoie and S.J. Derry, editors. Erlbaum. p. 75-105.
7. Reye, J. (1995). A goal-centred architecture for intelligent tutoring systems. In J. Greer
(eds.), 7th World Conference on Artificial Intelligence in Education, p. 307-314.
8. Russell, S. and P. Norvig (1995). Artificial Intelligence: A Modern Approach. Englewood
Cliffs, NJ: Prentice Hall.
9. Murray, R.C., K. VanLehn, and J. Mostow (2004). Looking ahead to select tutorial actions: A decision-theoretic approach. International Journal of Artificial Intelligence in
Education, 14(3-4): 235-278.
10. Mayo, M. and A. Mitrovic (2001). Optimising ITS behaviour with Bayesian networks and
decision theory. International Journal of Artificial Intelligence in Education, 12: 124-153.
11. Pek, P.-K. (2003). Decision-Theoretic Intelligent Tutoring System. PhD dissertation, National University of Singapore, Department of Industrial & Systems Engineering.
ftp://ftp.medcomp.comp.nus.edu.sg/pub/pohkl/pekpk-thesis-2003.pdf
12. Conati, C., A. Gertner, and K. VanLehn (2002). Using Bayesian networks to manage uncertainty in student modeling. User Modeling and User-Adapted Interaction, 12(4): 371-417.
13. Anderson, J.R., A.T. Corbett, K.R. Koedinger, and R. Pelletier (1995). Cognitive Tutors:
Lessons Learned. The Journal of the Learning Sciences, 4(2): 167-207.
14. Anderson, J.R. and C. Lebiere (1998). The atomic components of thought. NJ: Erlbaum.
15. Koedinger, K.R., J.R. Anderson, W.H. Hadley, and M.A. Mark (1997). Intelligent tutoring
goes to school in the big city. International Journal of Artificial Intelligence in Education,
8: 30-43.
16. Fox, B.A. (1993). The Human Tutorial Dialogue Project: Issues in the Design of Instructional Systems. Hillsdale, NJ: Lawrence Erlbaum Associates.
17. Murray, R.C. (2005). An evaluation of decision-theoretic tutorial action selection. PhD dissertation, University of Pittsburgh, Intelligent Systems Program. http://etd.library.pitt.edu/
ETD/available/etd-08182005-131235/
18. Mostow, J., C. Huang, and B. Tobin (2001). Pause the Video: Quick but quantitative expert evaluation of tutorial choices in a Reading Tutor that listens. In J.D. Moore, C.L. Redfield, and W.L. Johnson (eds.), 10th International Conference on Artificial Intelligence in
Education, p. 343-353.
19. Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences. Erlbaum.
20. Aleven, V. and K.R. Koedinger (2000). Limitations of student control: Do students know
when they need help? In G. Gauthier, C. Frasson, and K. VanLehn (eds.), Intelligent Tutoring Systems, 5th International Conference, ITS 2000, p. 292-303.

Intention-Based Scoring: An Approach to Measuring
Success at Solving the Composition Problem
H. Chad Lane

Kurt VanLehn

Institute for Creative Technologies
University of Southern California
13274 Fiji Way
Marina del Rey, CA 90292

Department of Computer Science
Learning Research and Development Center
University of Pittsburgh
Pittsburgh, PA 15260

lane@ict.usc.edu

vanlehn@cs.pitt.edu

ABSTRACT

1.

Traditional methods of evaluating student programs are not
always appropriate for assessment of different instructional
interventions. They tend to focus on the final product rather
than on the process that led to it. This paper presents
intention-based scoring (IBS), an approach to measuring
programming ability that requires inspection of intermediate programs produced over the course of an implementation
rather than just the one at the end. The intent is to assess a
student’s ability to produce algorithmically correct code on
the first attempt at achieving each program goal. In other
words, the goal is to answer question “How close was the
student to being initially correct?” rather than the the ability to ultimately produce a working program. To produce an
IBS, it is necessary to inspect a student’s online protocol,
which is defined as the collection of all programs submitted to a compiler. IBS involves a three-phase process of
(1) identification of the subset of all programs in a protocol
that represent the initial attempts at achieving programming goals, (2) analysis of the bugs in those programs, and
(3) rubric-based scoring of the resulting tagged programs.
We conclude with an example application of IBS in the evaluation of a tutoring system for beginning programmers and
also show how an IBS can be broken down by the underlying
bug categories to reveal more subtle differences.

Traditional methods of evaluating student programs tend
to involve scoring of the final program produced by a student
for a given project. Although such a score is certainly appropriate for classroom assessment, it reveals very little about
the process that went into creating the program. The final
program is also prone to influence from a variety of outside
sources, such as a tutor or helpful friends. For researchers
interested in isolating how different experimental manipulations affect programming skill in a finer-grained way, a
metric that targets students during the act of programming
would be more appropriate. In this paper, we propose such a
metric called intention-based scoring (IBS) and describe an
application of it in the evaluation of an intelligent tutoring
system for novice programmers.
Assessing process is a particularly challenging problem.
To do so for programming, one approach is to use a charette,
which requires that a student solve a programming problem
in a lab environment and under a time limit [1, 8]. Because
no assistance is available (it is typically given as a test),
there is no chance for outside influence. Secondly, since
most students are not able to complete the full task within
the time limit, the resulting score of the “final” version of
the program is a actually a measure of success of the student
at some point in the middle of their implementation.
Another approach is to collect a student’s online protocol,
which is defined as all files submitted to a compiler during
an implementation [12, 13]. This provides a chain of “snapshots” representing a path through the space of development
of a program. Lying between each pair of these intermediate programs are compile attempts, which can be explained
by a variety of underlying cognitive activities that programmers engage in during programming [2, 4]. In this paper,
our goal is not to provide a cognitively plausible account for
these activities, but rather to provide a method for scoring
such protocols. We seek to quantitatively answer the question “How close was the student to being initially correct?”

Categories and Subject Descriptors
K.3 [Computers & Education]: Computer and Information Science Education—Computer Science Education

General Terms
Measurement

Keywords
intention-based scoring, online protocols, novice programming, intelligent tutoring systems, structured programming

2.

INTRODUCTION

PROGRAMMING KNOWLEDGE

The knowledge that underlies programming is tacit: a
completed program is a poor representation of the knowledge and skills needed to produce it. Studies that focus
on the content and structure of this knowledge generally
identify structured “chunks” that achieve a variety of goals,
sometimes called schemata [9] or plans [10]. In terms of such
theories, two key problems have been suggested as a way of

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGCSE’05 February 23–27, 2005, St. Louis, Missouri, USA
Copyright 2005 ACM 1-58113-997-7/05/0002 ...$5.00.

373

understanding what programmers must do to produce a program [3]:
• Decomposition problem: identifying the goals and
corresponding plans needed to solve the problem.
• Composition problem: implementing and assembling these plans such that the problem is solved correctly.
Although both problems are known to be a challenge for
novices, the composition problem is particularly difficult because of subtle interactions and complications that can arise
when multiple plans are to be merged [12]. For example,
when two goals each imply the need for a loop, the programmer must determine if one loop in the proposed solution can
be used to satisfy both plans. Complications such as this are
common for novice programmers, and since inspecting only
the final version of a program will rarely reveal them, a more
targeted evaluation approach is necessary.

3.

Figure 1: First two stages of producing an IBS.

goal. Such issues arise frequently in the subjective tagging
of data, which is why it is recommended to tag some subset
of the data together under open discussion.

INTENTION-BASED SCORING

IBS derives elements from previous work on identifying
bugs in online protocols. Research from this stream has focused on remediation, such as that provided automatically
by PROUST [5], as well as on establishing cognitively plausible accounts for how novice bugs are produced [13]. In
this section, we present the three phases that are required
to produce an IBS.

3.1

3.2

Bug identification

A critical component of IBS involves the identification and
classification of bugs present in a student’s protocol. We follow the same two stage process described in [5, 13]. First,
the plans being implemented by the student must be identified, and next, compared to the known correct plans of an
implementation. Bugs then fall out as differences between
these two structures. For IBS, of course, only the plans
corresponding to the new goals being implemented at each
stage should be considered.
Although there are certainly many ways to characterize
the bugs (i.e., plan differences), we adopt here a simplification of the approach taken in [13]. Most generally, an
IBS scheme could be constructed from any similar bug classification strategy. Because our goal is not to provide an
account of cognitive plausibility, we limit ourselves to categories that relate to solving the composition problem. The
top-level categories of bugs in our coding scheme are:

Inspecting an online protocol

The first step in producing an IBS is to identify the subset of programs from an online protocol to analyze for bugs.
This subset of programs should be the student’s initial attempts at achieving each goal. A judge begins with the first
program submitted and works through the protocol chronologically, checking off goals along the way. This phase is
complete when attempts at all goals have been identified or
the protocol ends (leaving some goals un-attempted). The
process of protocol subset identification is depicted in the
top half of figure 1.
The identification process is not always straightforward,
however. The first program in a protocol is not always a
legitimate goal attempt, for example. Some novices prefer
to compile very simple programs to begin, including only
things like variable declarations or simple print statements.
In cases like this, we ignore such programs and continue
searching sequentially for the first substantive attempt at
achieving a goal. A related issue is the sometimes fuzzy
question of whether or not a program represents an attempt
at achieving a goal or not. It is not quite as simple as saying
“if any plan component is present, then count it as a goal
attempt.” For example, some students prefer to declare and
initialize all variables at once. This certainly does not imply
the student is attempting to implement all plans in which
these steps participate.
To handle problems like this, the criteria for selecting programs from a protocol must be agreed upon between multiple judges. In the example we present below, the consensus
with the variable declaration issue, for example, was to conclude that by itself, a declaration would not be counted as
an attempt at its plan. In other words, more plan components would need to be present than just a declaration or
initialization step to count as an outright attempt at that

• omission: A plan component is missing.
• malformation: A component is incorrectly implemented.
• arrangement error: A component was placed in the
wrong location.
In addition, when inspecting a program, it is also necessary
to identify those bugs that are a result of merging of plans
(e.g., the multiple loop issue mentioned above). We refer
to bugs that are not a result of confusion between multiple
plans as isolated. Of course, some bugs can fall under multiple categories. For example, a step can be malformed, out
of place, and be a result of confusion between two plans. Because this is a subjective tagging process, it is recommended
that multiple judges be used and agreement be checked.
An example of bug identification is shown in figure 2.
In this example, the student is attempting to implement a
counter plan (shown in the shaded box), but has made three
mistakes. First, the incorrect value is used for the initialization step (it should be 0). Second, the increment step is not

374

tempt at assembling an algorithm, in our view, is a more accurate representation of a student’s initial impression at how
to solve the composition problem. Also, we note that the
process is dubbed “intention-based” for two reasons: first,
programs are inspected by inferring what goals the student
is trying to achieve. Second, when a program statement is
not syntactically correct, it is necessary to infer what plan
component is being attempted. The line count + 1;, for example, is likely an attempt to increment a counter variable.
Thus, such a statement is considered equally as correct as a
syntactic one.
There are several problems with the IBS method of evaluating programs. First, it is extremely tedious. In no way
is IBS intended for regular classroom evaluations – it is only
reasonable for use in targeted evaluations that require a finegrained understanding of student success. Second, the creation of the rubric is subject to the bias of the researcher. In
other words, the weighting of the various plan components
may indirectly impact the outcome of the study. Finally, in
the form presented here, IBS is dependent on a plan-based
theory of programming knowledge. The difficulties with this
theory, then, are naturally inherited.

Figure 2: Identifying bugs by plan differences.

placed inside the loop body (an arrangement bug). Finally,
there is no print statement (a bug of omission). In this case,
the arrangement bug is also considered a plan-merging error
since the counter is being integrated into the looping code,
which was already in place in this program from a separate
plan.

3.3

Scoring

With a bug-tagged protocol, the final step in computing
an IBS is to apply a scoring rubric. Although simple bug
frequencies could be used, it is less fair since focal steps in
plans (i.e., those that are “more” central, such as the increment step of a counter plan) would count the same as
less critical components (such as an output statement). To
create a rubric, points need to be assigned to the various
plan components. Focal steps are weighted more heavily,
like updates and conditions, than are supporting steps, like
initializations and output statements. This allows us to discount slips (like forgetting to print a value) and highlight
errors in critical plan steps. Finally, points for each bug
identified during the analysis are taken away from an overall possible score, thereby producing a final intention-based
score. In sum, this score represents the accuracy of students’
first attempts at achieving programming goals. By looking
at points lost from each of the sub-categories (like merge
errors or omissions), one can get a better feel for the kinds
of errors novices produce.
As an example, for the counter plan in figure 2, one possible assignment could be 3 points for the initialization step,
5 for the increment step, and 2 for the print step. As mentioned, it is best to perform this stage with expert instructors who have experience creating rubrics. In the example,
the student might lose 1 point for the incorrect initial value,
3 for the improper location of the increment step, and 2
for forgetting the print step. These partial values need to
be agreed upon in the rubric. In sum, this student would
receive 4 out of 10 possible points for this attempt at implementing a counter plan.

3.4

4.

AN APPLICATION OF IBS

We now turn to an example application of IBS in the evaluation of ProPl (“pro-PELL”), a dialogue-based intelligent
tutoring system for beginning programmers [6, 7].

4.1

Experiment

ProPl is intended to help novices do pre-planning of their
programs. After the tutoring, students perform their usual,
independent implementation. Subjects in the study agreed
to allow collection of online protocols. In the sections below, we present the IBS results for the ProPl group when
compared to a baseline group of students whoe received no
tutoring whatsoever and a control group, who just read the
material in a similar environment. The hypothesis being
tested was that the program planning skills could be more
effectively taught if the interaction occurred as natural language dialogue as opposed to reading alone. In addition to
results for two tutored programs (called Hailstone and RockPaper-Scissors), results are also shown for a timed, post-test
charette which was not tutored (called Count/Hold). The
n’s were 9, 8, and 9 for the baseline, control, and ProPl
groups respectively.

4.2

Training and agreement

After solving the three problems in terms of goals and
plans, 15% of all programs were used to train together with
two expert judges. After this, another 20% were tagged independently, and then, to confirm agreement, a kappa statistic of .865 was computed on the tags.1 With consistency
of the bug identification procedure confirmed, the remaining
protocols were tagged independently.

Discussion

One difference of IBS with previous work using online protocols is that all attempts in a protocol are made available
for inspection. Most previous work considered only syntactically correct compile attempts. The reasoning behind
“opening” up the protocols in this way comes from the observation that for some students in our protocols, the algorithm intended by the first compile attempt was often different than that in the first syntactically correct attempt. This
means that students’ algorithms seemed to change, possibly
inadvertently, while fixing syntax errors. The very first at-

4.3

Composite intention-based scores

Intention-based scores are shown in table 1 for the three
programming projects involved in the study. Some significant and marginally significant differences exist between the
1

This measure is superior to percent agreement because it
factors out agreement by chance. Generally, a kappa value
above 0.80 is considered reliable.

375

problem
Hailstone
RPS
CH

baseline
69.3 (16.4)
67.7 (22.5)
n/a

ctrl
79.8 (15.4)
59.5 (18.7)
49.1 (26.3)

ProPl
86.1 (9.46)
77.5 (16.4)
64.1 (29.8)

Table 1: Composite IBSs, out of 100.

groups. We first consider how the baseline group compared
with each of the other groups. For Hailstone, the ProPl
students outscored baseline students to a statistically significant level (t(16) = 2.12, p = .0017) with a very large effect
size (es = 1.03).2 The control group also outperformed the
baseline group on Hailstone, but not significantly. On the
RPS problem, the baseline group outperformed the control
group, but the difference is not significant. ProPl students
did outperform the baseline students, but again, not to a
significant level.
We now turn our attention to the ProPl and control
groups. All students in these groups took the same pretest,
and so ANCOVAs were used for statistical tests in order to
factor out pretest performance. Although ProPl students
outperformed the control students on each project, the only
significant difference is on RPS. ProPl students were significantly better than control subjects (F (1, 15) = 7.88, p =
0.015, es = .96). On Count/Hold (the untutored posttest
charette), ProPl students outperformed those in the control group to a marginally significant level (F (1, 22) = 3.59,
p = .072, es = .57).

4.4

Figure 3: Points lost per plan-merging opportunity.
Standard error bars are shown.

Figure 4: Plan part omission points lost per plan
implementation attempt.

Decomposed intention-based scores

p = .008). On RPS, the ProPl group (M = .19, SD = .21)
outperformed both the baseline group (M = .85, SD = 1.0)
to a marginally significant level (t(17) = 1.98, p = .075, es =
.66) and the control group (M = .91, SD = 1.1), F (1, 15) =
3.71, p = .076, es = .65. Finally, on the Count/Hold project,
the ProPl group (M = .07, SD = .24) again surpassed the
control group (M = .61, SD = .74) but this time to a
highly significant level (F (1, 15) = 5.77, p = .026) and with
an extremely large effect size (es = 2.3).

The results shown in table 1 are composite scores; that
is, the bug categories are lumped together to produce the
overall score. To reveal how these points were distributed
across the various bug categories, we now proceed to break
down the points lost using two sub-categories of bugs.

4.4.1

Merging related errors

In this section, we present the number of points lost related to merging related problems per opportunity to make
an error. It would be misleading to use the raw points
missed. For example, a student who attempts two goals out
of a possible five would have far fewer opportunities to produce merging errors than someone who attempts to solve all
five. The resulting merge error score would be deceptively
low. Similar arguments can be made for plan component
omissions and isolated errors. We therefore normalize, and
use total number of goals attempted throughout the protocol
as a denominator. For merge errors, we use the total number
of attempted goals, minus one because at least two plans
are required for a merge error to be possible.
Figure 3 shows the points lost from merging related errors
over the three programming problems. For the Hailstone
problem, the control group (M = .38, SD = .65) produced
significantly fewer merging related errors than the baseline
group (M = 2.31, SD = 1.9), t(15) = 2.76, p = 0.015. The
ProPl group (M = .36, SD = .45) performed similarly
well when compared to the baseline group (t(16) = 3.02,
2

4.4.2

Component omission errors

Moving now to omission errors (figure 4), several differences were found to be significant. Interestingly, the baseline
group lost fewer points in Hailstone for missing plan parts
(M = .57, SD = .59) than the control group (M = 1.1,
SD = .53) to a marginally significant level (t(16) = −1.85,
p = .085, es = 1.0). This suggests the baseline group had
a greater opportunity for merging errors because they had
more plan components to deal with. When compared to the
ProPl group, the difference is not significant. This happened again on RPS with the baseline group (M = 1.22,
SD = .62) outperforming the control group (M = 1.95,
SD = .67), but to a significant level (t(16) = −2.20, p =
.046, es = 1.2). The control group, in general, seemed to
be more forgetful than the other two groups. The ProPl
group (M = .71, SD = .63) also was significantly better
than the baseline group on RPS (F (1, 15) = 15.6, p = .0017,
es = 1.9). A similar difference appeared on Count/Hold
with the ProPl group (M = .72, SD = .62) losing significantly fewer points than the control group (M = 1.84,
SD = 1.13), F (1, 15) = 9.22, p = .0065, es = .99.

Effect size was computed using Glass’ delta, that is,

Mexp −Mctrl
SDctrl

376

5.

DISCUSSION

statistical analyses, and the anonymous reviewers who provided insightful and useful feedback.

Several of the detected differences are suggestive of the impact ProPl had on students in the study. First, the higher
composite IBSs of ProPl students suggests that they benefited in general from the tutoring by having initial attempts
that were closer to correct than students in the other conditions. Looking at the decomposed scores, both groups receiving intervention did equally well over the baseline group
on the first program.
The longer term effect, however, seems to favor the ProPl
group. In the following two problems (and most importantly, the third, untutored program), ProPl students lost
significantly fewer points from errors related to the interactions between plans. That is, dialogue-based tutoring seems
to help novices establish a stronger understanding of the issues involved with achieving multiple goals in one program.
Given that this is known to be a major difficulty for novices
[12], this result bodes well for the efficacy of natural language tutoring to help novice programmers.
The plan part omission results are not as compelling given
the relationship between the baseline and control groups. It
is surprising that the baseline group produced more complete
plans than the control group – the control group received
some of this information beforehand. Given the very high
level of merging-related problems shown in figure 3, there
was a price for being more complete. On the positive side,
the ProPl group again demonstrates a trend in the correct
direction of getting better with respect to plan completeness.
Taking these differences to the most general conclusion, this
may suggest that they were able to adopt a more abstract
view of programming by thinking at the level of plans rather
than the line-by-line view normally adopted by novices.

6.

8.

SUMMARY AND FUTURE WORK

In this paper we presented intention-based scoring, an approach to assessing online protocols produced by novice programmers. This metric focuses on the process of programming by providing a score of a student’s ability to solve the
composition problem. An IBS is computed by inspecting
the first attempt at solving each programming goal over the
course of an entire implementation followed by the application of a traditional rubric to those programs. We then
demonstrated how to use IBS to evaluate a tutoring system
for novices and were able to break the scores down based on
the bug categories to evaluate the system in a finer-grained
way.
IBS is limited in its application, however. It is bound
to a plan-based theory of programming knowledge and is
tedious to compute. These problems suggest two lines of
future work. The first is to explore other methods of judging the quality of intermediate programs, perhaps in other
programming paradigms. The second, and perhaps more interesting, is to use the large amount of tagged data to build
automatic classifiers for bug identification.

7.

REFERENCES

[1] C. Daly and J. Waldron. Assessing the assessment of
programming ability. In Proceedings of the 35th
Technical Symposium on Computer Science Education
(SIGCSE), pages 210–213, Norfolk, VA, 2004. ACM
Press.
[2] W. D. Gray and J. R. Anderson. Change-episodes in
coding: When and how do programmers change their
code? In G. M. Olson, S. Sheppard, and E. Soloway,
editors, Empirical Studies of Programmers: Second
Workshop, pages 185–197. Ablex, Norwood, NJ, 1987.
[3] M. Guzdial, L. Hohmann, M. Konneman, C. Walton,
and E. Soloway. Supporting programming and
learning-to-program with an integrated cad and
scaffolding workbench. Interactive Learning
Environments, 6(1&2):143–179, 1998.
[4] M. C. Jadud. A first look at novice compilation
behavior using bluej. In 16th Annual Workshop of the
Psychology of Programming Interest Group (PPIG
2004), Institute of Technology, Carlow, Ireland, April
2004.
[5] W. L. Johnson. Understanding and debugging novice
programs. Artificial Intelligence, 42:51–97, 1990.
[6] H. C. Lane and K. VanLehn. A dialogue-based
tutoring system for beginning programming. In
Proceedings of the Seventeenth International Florida
Artificial Intelligence Research Society Conference
(FLAIRS), pages 449–454, Miami Beach, FL, 2004.
AAAI Press.
[7] H. C. Lane and K. VanLehn. Teaching program
planning skills to novices with natural language
tutoring. In S. Fitzgerald and M. Guzdial, editors,
Computer Science Education. Swets and Zeitlinger,
September 2005. Special issue on doctoral research in
CS Education.
[8] M. McCracken, V. Almstrum, D. Diaz, M. Guzdial,
D. Hagan, Y. B.-D. Kolikant, C. Laxer, L. Thomas,
I. Utting, and T. Wilusz. A multi-national,
multi-institutional study of assessment of
programming skills of first-year cs students. ACM
SIGCSE Bulliten, 33(4):125–140, 2001. Report by the
ITiCSE 2001 Working Group on Assessment of
Programming Skills of First-year CS.
[9] R. S. Rist. Program Structure and Design. Cognitive
Science, 19:507–562, 1995.
[10] E. Soloway and K. Ehrlich. Empirical studies of
programming knowledge. IEEE Transactions on
Software and Engineering, SE-10(5):595–609,
September 1984.
[11] E. Soloway and J. C. Spohrer, editors. Studying the
Novice Programmer. Ablex Corp., Norwood, New
Jersey, 1989.
[12] J. C. Spohrer and E. Soloway. Putting it all together
is hard for novice programmers. In Proceedings of the
IEEE International Conference on Systems, Man, and
Cybernetics, Tucson, Arizona, November 12-15 1985.
[13] J. C. Spohrer, E. Soloway, and E. Pope. A goal/plan
analysis of buggy pascal programs. In Soloway and
Spohrer [11], pages 355–399.

ACKNOWLEDGMENTS

This research was supported by NSF grant 9720359 to
CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the University of Pittsburgh and Carnegie-Mellon University. We also would like
to thank Mark Fenner for his help in tagging the protocols,
Bob Hausmann’s assistance with the experimental setup and

377

Developing an Intelligent Tutoring System Using
Natural Language for Knowledge Representation
Sung-Young Jung1 and Kurt VanLehn2
1

Intelligent Systems Program, University of Pittsburgh, Pennsylvania, USA
syjung5@asu.edu
2
School of Computing Informatics and Decision Systems Engineering,
Arizona State University, Arizona, USA
Kurt.Vanlehn@asu.edu

Abstract. Authoring the domain knowledge of an intelligent tutoring system
(ITS) is a well-known problem, and an often-mentioned approach is to use authors who are domain experts. Unfortunately, this approach requires that potential authors learn to write and debug knowledge written in a formal knowledge
representation language. If authors were able to use natural language to represent knowledge it would allow them to add and update knowledge far more
easily. In this paper, the design of such an authoring system, ‘Natural-K’ is
presented. Natural-K is an authoring system in which domain authors including
non-programmers are able to add problem statements and background knowledge such as commonsense, in natural language.
Keywords: Authoring System for ITS, Knowledge acquisition, Knowledge
representation using natural language, Natural language understanding.

1 Introduction
Intelligent tutoring systems (ITS) require authors to add a several types of knowledge
including problem statements, principles, commonsense knowledge, etc. A difficulty
in developing such systems is in how to acquire such knowledge from authors who
are usually not computer programmers. When authors have added knowledge, they
must be able to follow problem solving detect errors, explore fixes and finally updating the knowledge. But this has not been possible so far without help of a knowledge
engineer because knowledge has been represented in formal language.
For the ITS community, where tutoring systems often have a finite set of problems
and authors often merely want to add more problems, the ability to represent problem
statements in natural language is particularly appealing. ISSAC [1], MECHO [3], and
pSAT [4] receive a problem statement written in natural language text and then transform into formal language to solve the given problem. Although these projects
demonstrated that the problem statements read by students could also be read by the
tutor, this capability was limited because the background knowledge required
for translating the problem statements was encoded in formal language or computer
program code. Only computer programmers were able to add such knowledge.
V. Aleven, J. Kay, and J. Mostow (Eds.): ITS 2010, Part II, LNCS 6095, pp. 355–358, 2010.
© Springer-Verlag Berlin Heidelberg 2010

356

S.-Y. Jung and K. VanLehn

Several projects have explored the acquisition of inferential domain knowledge
from natural language. The basic idea was to transform natural language into formal
representations such as logic forms [7] or semantic networks [8], and the systems
perform inferences to accomplish a problem solving goal. Domain knowledge includes a great deal of commonsense knowledge. Several universal commonsense
knowledge bases are being developed (Opencyc [2]). However, it is unlikely that the
complete universal set of commonsense knowledge can be built soon.
The main idea presented in this paper is to have the system represent knowledge in
natural language. Thus, authors including non-programmers can add knowledge,
detect bugs, and fix them without help of a knowledge engineer.

2 A Problem Statement and Principles
The specific project is to replace the knowledge representation language of Pyrenees
with natural language, then use Natural-K to re-author the Pyrenees knowledge base.
Pyrenees is an intelligent tutoring system for equation-based problem solving [5] with
two main types of domain knowledge: principles and problem statements (Fig. 1).
The principle in the figure was modified using a natural language definition for each
variable (ex: “the direction of vector_ at time t_”).
(a) Problem skateboarder: “A skateboarder rolls at 1.3 m/s up an inclined plane angled
at 143 degrees. What is her vertical velocity?”

(b) Pa_equation (along(projection(offaxis, Vector, T), Axis)), V_x=V*Trig) :match (Axis, ‘xy_ component’, [xy_/XY]),
match(V_x, ‘xy_ component of vector_ at time t_’, [xy_/XY, vector_/Vector, t_/T]),
match(V, ‘the magnitude of vector_ at time t_’, [vector_/Vector, t_/T]),
match(V_dir, ‘the direction of vector_ at time t_’, [vector_/Vector, t_/T]).

Fig. 1. A problem example. (a) A problem statement. (b) A relevant principle.

Often an author wants to add new domain problems using existing principles.
Then, he needs to supply (1) a problem statement, and (2) commonsense knowledge
(or inference rules), always in natural language to the system.

3 Mapping to Standard Natural Language
The initial problem statement will be translated into precise natural language until the
resulting language exactly matches the variables in a principle, which are also written
in natural language as Fig. 1-(b); the principle then applies and produces an equation.
The precise natural language that appears in the principles is decided upon by the
knowledge engineer and is called standard natural language. Many different nonstandard phrases all get converted into the same standard sentence (for example, ‘A
skateboarder rolls at 1.3 m/s’ and ‘The speed of the skateboarder is 1.3 m/s’ into ‘The
magnitude of the velocity of a skateboarder is 1.3 m/s’).

Developing an ITS Using Natural Language for Knowledge Representation

357

Mapping non-standard natural language to standard natural language is done by inference rules. Rules are entered by authors as two pieces of natural language. The left
side of the rule corresponds to a condition, and the right side of the rule corresponds
to new knowledge produced.
‘A skateboarder rolls at 1.3 m/s up a plane’
implies ‘the magnitude of the velocity of the skateboarder is 1.3 m/s’.
The strings entered by authors are represented inside the authoring system as dependency graphs after parsing [7]. The parser does syntactic normalization for passive and active sentences. The translation process starts with a set of dependency
graphs that represent the problem statement. Inference rules run in a forward chaining
manner, augmenting the set with more dependency graphs until no new one can be
added. Then, the variables of the principles are matched against the set of dependency
graphs and produce equations.
As the amount of added rules increases, the system can automatically produce generalized rules. For instance, after entering the rule above and this new one:
‘A sled glides at 0.2 mph up a hill’
implies ‘The magnitude of the velocity of the sled is 0.2 mph’.
the authoring system would produce a generalized rule with semantically constrained
variables (ex: object_, moves_, etc). The generalization is driven by WordNet’s ontology [6]. If the system finds a hypernym (a superclass word), such as “object”
which is the least common ancestor of “skateboarder” and “sled”, then the system
produces a generalized rule with the semantically constrained variable, object_. In this
way, the system generalizes the existing rule so that it will subsume both the old rule
and the newly entered one.
‘An object_ moves_ at num_ unit_ up a hill_’
implies ‘the magnitude of the velocity of the object_ is num_ unit_’.

4 Discussion and Further Works
An important issue in using natural language is that there can be a large number of
different expressions that have the same meaning. This is called the paraphrasing
problem [9]. An important thing that should be noted is that it is not a serious issue in
an authoring system for ITS. Authoring tasks in ITS is task-specific. It is enough for
an author to add only the knowledge required to solve a given problem. Thus, the
required set of knowledge to recognize the paraphrases is fixed and determined by the
given problem statement. In other words, the author doesn’t have to make the system
recognize all paraphrases not seen in the given sentence. For this reason, the issue of
paraphrases is not a serious problem except the case of fully automatic text processing
without help of human authors.
So far, the authoring system succeeded in adding 15 physics problem statements
correctly; 98 inference rules were added for them. It showed that the average number
of rules per a problem was around seven (=98/15) which is small enough for an author
to add. The remaining work is to implement rule generalization, integrate with Pyrenees, and perform empirical evaluation of the system.

358

S.-Y. Jung and K. VanLehn

References
1. Novak Jr., G.S.: Representations of Knowledge in a Program for Solving Physics Problems.
In: IJCAI 1977, Cambridge, MA, pp. 286–291 (1977)
2. Opencyc.org, OpenCyc Tutorial, http://www.cyc.com/cyc/opencyc/overview
3. Bundy, A., et al.: Solving mechanics problems using meta-level inference. In: IJCAI 1979,
pp. 1017–1027 (1979)
4. Ritter, S., et al.: Authoring Content in the PAT Algebra Tutor. Journal of Interactive Media
in Education (9), 1–30 (1998)
5. VanLehn, K., et al.: Implicit versus explicit learning of strategies in a non-procedural cognitive skill. In: The Intern’l Conf’ on Intelligent Tutoring Systems, pp. 521–530 (2004)
6. Harabagiu, S.M., Miller, G.A., Moldovan, D.I.: WordNet 2 - A Morphologically and
Semantically Enhanced Resource. In: SIGLEX 1999 (1999)
7. Jurafsky, D., Martin, J.H.: Speech and Language Processing. Prentice-Hall, Englewood
Cliffs (2000)
8. Sowa, J.F.: Current Issues in Semantic Networks. In: Principles of Semantic Networks:
Explorations in the Representation of Knowledge. Morgan Kaufmann, San Francisco (1990)
9. Iftene, A.: Textual Entailment. PhD Thesis, Computer Science, University of Iasi, Iaşi, Romania (2009)

What’s in a Step? Toward General, Abstract
Representations of Tutoring System Log Data
Kurt VanLehn1, Kenneth R. Koedinger2, Alida Skogsholm2, Adaeze Nwaigwe2,
Robert G.M. Hausmann1, Anders Weinstein1, and Benjamin Billings2
1

LRDC, University of Pittsburgh, Pittsburgh, PA, USA
{vanlehn,andersw,bobhaus}@pitt.edu
2
HCII, Carnegie-Mellon University, Pittsburgh, PA, USA
{koedinger,alida,anwaigwe,bkb}@cmu.edu

Abstract. The Pittsburgh Science of Learning Center (PSLC) is developing a
data storage and analysis facility, called DataShop. It currently handles log data
from 6 full-year tutoring systems and dozens of smaller, experimental tutoring
systems. DataShop requires a representation of log data that supports a variety
of tutoring systems, atheoretical analyses and theoretical analyses. The theorybased analyses are strongly related to student modeling, so the lessons learned
in developing the DataShop’s representation may apply to student modeling
in general. This report discusses the representation originally used by the
DataShop, the problems encountered, and how the key concept of “step”
evolved to meet these challenges.
Keywords: Student modeling, educational data mining, tutoring systems.

1 The Pittsburgh Science of Learning Center DataShop
The PSLC DataShop (http://learnlab.web.cmu.edu/datashop/) provides the following
functions: (1) Data security with appropriate anonymity; (2) A standard, extensible
representation; (3) Easy export to standard tools, such as spreadsheets and statistical
packages; (4) Analytic tools specific to log data; and (5) Reification of the PSLC
theoretical framework. This last goal is explained below.
The DataShop grew out of Ritter and Koedinger’s [1] standard framework for
representing log data. Its analysis tools, which are described below, evolved from
Anderson and Koedinger’s early work on learning curves [2].
The DataShop is part of the PSLC LearnLab—an internationally shared facility
for doing in vivo experimentation (http://www.learnlab.org). Although the
DataShop is in daily operation supporting thousands of students, teachers and
researchers around the world, it is still developing in order to incorporate new kinds
of student-tutor interactivity. We report on the representational challenges that have
been faced.
C. Conati, K. McCoy, and G. Paliouras (Eds.): UM 2007, LNAI 4511, pp. 455–459, 2007.
© Springer-Verlag Berlin Heidelberg 2007

456

K. VanLehn et al.

2 Three Levels of Description
The log data are a chronological record of all the student’s interactions with a tutoring
system. These interactions are described at three levels: transactions, step histories
and knowledge component applications. Each level is described below.
The lowest level is the transaction [1], which is a communication between the
student and the system. For instance, the following is a sequence of transactions in an
algebra tutor:
1.
2.
3.
4.
5.

The tool displays “2x(3-4x)-13 =__x^2 + __x + __ = (__x + __)(__x + __)”.
The student puts the cursor in the first blank and enters “8”.
The tutor tells the student that the entry is incorrect.
The student asks for a hint.
The tutor tells the student “Check your signs.”
The student replaces the “8” with “-8”.
The tutor tells the student that the entry is correct.
The student puts the cursor in the next blank and enters “6”.
The tutor tells the student that the entry is correct.

The next level represents the log data as a sequence of episodes, called step-attempt
histories [3]. Each episode is terminated by a step, which is a user interface action that
is correct and advances the solution of the problem. The history of that step consists
of the student’s incorrect attempts at entering that step, help requests, hints, and any
other transactions that might aid the student to make the step. For instance, in the list
above, transactions 2 through 4 comprise the first step-attempt history; transaction 5 is
the second one. This level of description assumes that only some user interface
actions are steps, and that the correct/incorrect distinction makes sense for them.
Thus, this level of representation has some theoretical commitments, but fairly weak
ones.
The third level of description is based on the PSLC theoretical framework, which
assumes that domain knowledge can be usefully decomposed into knowledge
components [4]. This is intended to be a generic, neutral term that covers many kinds
of knowledge: procedural, conceptual, perceptual, etc. For example, in learning
Chinese as a second language, a single knowledge component (KC) might represent a
word’s phonological, orthographic, and semantic representations, as well as the
associations between them. In physics, Newton’s third law might be represented as a
single knowledge component. Most PSLC tutoring systems represent domain
knowledge as KCs, and they label every step with the KCs that must be applied to
generate that step. For instance, the step entered at line 7 above results from applying
two KCs: the Distributive Law and Simplification. Thus, at this level of description,
the log data are viewed as a sequence of knowledge component applications.

3 The DataShop’s Analytical Tools
We discuss only two tools, the Error Report and the Learning Curve generator, that
illustrate the need for the three levels of log data description.

What’s in a Step?

457

In its simplest usage, the Error Report is given a problem and prints a table
that lists each step in the problem along with a summary of the students’
step-attempt histories. For instance, the error report for filling in the first blank of
“2x(-4x+3)-13= __x^2+…”, might state that: 69% of the students entered the correct
response on the first attempt, 12% asked for a hint, 10% entered “8” and got the hint
“Check your signs,” and 9% entered “-4” and got the hint “Hmm; not what I got.
Please try again.” Such error reports are useful for determining which common errors
are not receiving pedagogically useful feedback. The Error Report uses only the step
level of description. KC applications play no role in its reports, so a tutoring system
that does not use KCs can still get Error Reports for its log data.

Fig. 1. Learning Curve for the KC Select-Given-Value-Reason

In our usage, a learning curve displays the students’ increasing mastery of a
knowledge component over time [2, 5]. As a simple illustration, suppose we want a
learning curve for a particular KC for a particular student. The tool first locates all the
step-attempt histories corresponding to applications of that KC. For each history, it
calculates the assistance score, which is simply the number of help requests plus the
number of errors in that episode. For instance, for the first step-attempt history
mentioned above (transactions 2 through 4), the assistance score is 2; for the second
step-attempt history, the assistance score is 0. Then the Learning Curve generator
plots a graph (see Fig. 1) where the points correspond to step-attempt histories, the yaxis is the assistance score, and the x-axis is ordinal and chronological (i.e., the Nth
KC application is at x = N on the graph.) Theory suggests that the learning curve
should start with large amounts of assistance on the first KC application, less on the
second, and so on. Often the learning curve for a single student is too noisy to see
such a pattern, so it is common to aggregate over all the students. In Fig. 1, for
instance, the point at x = 1 has a y-value that is the average over all students’
assistance scores for their first application of the KC.

4 Representational Lessons Learned
This section discusses representational lessons learned while trying to accommodate
an increasing set of tutoring systems. When log data from VLAB, a simulated
chemistry laboratory with a direct-manipulation interface (www.chemcollective.org),

458

K. VanLehn et al.

were added to the DataShop, we had to allow multiple transactions to be associated
with a single step. For instance, a single step “heat beaker A” should be associated
with the three transactions: (1) removing a Bunsen burner from storage, (2) placing it
under the beaker and (3) turning on the flame.
More recently, we added the inverse capability: a single student transaction may be
associated with multiple steps. When log data from Andes, a physics tutoring system
(www.andes.pitt.edu) were first added to the DataShop, each correct equation entered
by the student was treated as a step. However, this made the error reports nearly
useless because few students entered the same steps. For instance, one student might
enter “W_y = -W” as one step and “W = m*g” as another. A second student might
enter their algebraic combination, “W_y = -m*g”. Even if a problem needs only N
primitive equations to solve it, most subsets of the set of N equations correspond to a
possible compound equation. Thus, the error report for a problem with 10 primitive
equations may have as many as N^2 = 1028 steps. Moreover, each would probably
have just one or two step-attempt histories because only one or two students happened
to enter exactly that algebraic combination of primitive equations. Fortunately, Andes
decomposes a student equation into the primitive equations that comprise it. Each
such primitive equation became a step in the DataShop representation. Thus, if the
student entered “W_y = -m*g”, then this student action is associated with two steps,
“W_y = -W” and “W = m*g”. That is, a single student transaction may be associated
with multiple steps.
The third major issue involves partitioning the transactions into step-attempt
histories. We implied earlier that all the errors, help requests, and other non-step
transactions that occurred between two steps became the step-attempt history for the
second step. That is, the partitions were chronological. This does not make sense in
some cases. For instance, suppose the student makes the error mentioned earlier by
entering “8” in the first blank of “2x(-4x+3)-13 = __x^2 + __x + __”. The tutor gives
the hint “Check your signs,” but the student does not fix the error. Instead, the student
puts the cursor in the second blank and enters “6” which is correct. If we used only
the chronological scheme, the error and the hint would become part of the stepattempt history for “6.” This is wrong because the student actually didn’t have any
trouble entering the “6.” An Error Report that showed “-8” and “Check your signs”
associated with the “6” step would be very confusing. On the other hand, a partition
based on the location of the cursor at the time of the entry would assign the
appropriate step-attempt histories to the steps of this problem.
Chronology and location are just two cues that can be used for deciding how to
partition the log data into step-attempt history. The situation becomes more complex
when dealing with natural language tutoring systems. A single transaction, such as a
student saying “The block moves downward, speeding up,” might be analyzed as two
steps: “The block moves downward” and “The block speeds up”. We are currently
evaluating multiple heuristics by comparing their performance with human coders [6].
These explorations should be useful not only to the DataShop, but also to other
applications that do student modeling (e.g., [3]).

What’s in a Step?

459

5 Conclusions
The central concept in the DataShop log data representation has turned out to be the
step. It connects the transaction-level representation to the theoretically-derived
KC level. The step level also provides a way for tutoring systems that do not have
KC-level analyses to still get some use of the DataShop.
However, the concepts of “step” and “step-attempt history” have evolved in subtle
ways. Several years ago, “step” meant an actual student transaction that was a correct
part of the solution to the problem, and “step-attempt history” meant all the non-step
transactions that immediately preceded the step. Now there is no longer a one-to-one
relationship between steps and transactions, and the transactions that comprise a
step-attempt history need not immediately precede the step.
A step is now defined as the smallest possible correct entry that a student can
make. By “smallest”, we mean that the step cannot be re-expressed as two or more
steps.
Although the KC applications required to solve a problem are determined solely by
the problem and the KC-level analysis of the task domain, the steps required to solve
a problem are also a function of the user interface. For instance, in a natural language
interface, when the student enters “the baseball’s velocity is 10 m/s at 30º,” it
corresponds to two steps: “the baseball’s velocity is 10 m/s” and “the baseball’s
velocity is 30º” However, if the user interface were graphical instead, so that the
student specifies the baseball’s velocity by clicking and dragging out a vector, what
was once a compound of two steps now becomes one, because in the graphical user
interface, the vector drawing step cannot be decomposed.

References
1. Ritter, S., Koedinger, K.: Towards lightweight tutoring agents, in Artificial Intelligence in
Education. In: Greer, J. (ed.) Association for Advancement of Computers in Education, pp.
91–98. Charlottesville, NC (1995)
2. Anderson, J.R., et al.: Cognitive Tutors: Lessons Learned. The. Journal of the Learning
Sciences 4(2), 167–207 (1995)
3. VanLehn, K.: Intelligent tutoring systems for continuous, embedded assessment, in The
future of assessment: Shaping teaching and learning. Dwyer, C.A. (ed.) (In press) Erbaum:
Mahwah, NJ
4. VanLehn, K.: The behavior of tutoring systems. International Journal of Artificial
Intelligence and Education, vol.16, (2006)
5. Cen, H., Koedinger, K.R., Junker, B.: Learning Factors Analysis – A general method for
cognitive model evaluation and improvement, in Intelligent Tutoring Systems: In: 8th
International Conferenc. Ikeda, M., Ashley, K., Chan,T.-W. (eds.) pp. 164–175 Springer,
Berlin (2006)
6. Nwaigwe, A., et al.: Exploring alternative methods for error attribution in learning curve
analysis in intelligent tutoring systems. In: Proceedings of AI in Education, IOS Press,
Amsterdam (In Press) (2007)

A Hybrid Language Understanding Approach
for Robust Selection of Tutoring Goals
Carolyn P. Rosé, Dumisizwe Bhembe, Antonio Roque, Stephanie Siler,
Ramesh Srivastava, and Kurt VanLehn
LRDC, University of Pittsburgh
Pittsburgh, PA 15260 USA
rosecp@pitt.edu

Abstract. In this paper we explore the problem of selecting appropriate
Knowledge Construction Dialogues (KCDs) for the purpose of encouraging students to include important points in their qualitative physics
explanations that are missing. We describe a hybrid symbolic/statistical
approach developed in the context of the Why2 conceptual physics tutor (Vanlehn et al., 2002). Our preliminary results demonstrate that our
hybrid approach outperforms both the symbolic approach and the statistical approach by themselves.

1

Introduction

Recent studies of human tutoring suggest that a productive activity for teaching
is to have students explain physical systems qualitatively (Chi et al, 1981). The
goal of the Why2 project (Vanlehn et al., 2002) is to coach students as they
explain physics systems in natural language in response to short essay questions
such as, “Suppose you are running in a straight line at constant speed. You
throw a pumpkin straight up. Where will it land? Explain.” The Why2 system
has at its disposal a library of knowledge construction dialogues (KCDs), i.e.,
interactive directed lines of reasoning, each of which is designed either to elicit a
speciﬁc idea (i.e., an elicitation KCD) or to remediate a speciﬁc misconception
(i.e., a remediation KCD).
In order to do an eﬀective job at selecting appropriate KCDs in response to
student essays, the system must ﬁrst analyze the student’s essay to assess which
key points are missing from the student’s argument and which misconceptions
the student may have based on what the student said. Thus, in this paper we
explore a hybrid symbolic/statistical approach for robustly diagnosing student
essay answers to qualitative physics questions in the context of the Why2 conceptual physics tutor. In order to build an eﬀective system, it is important both
to give students the KCDs that they do need, and to avoid giving them extraneous KCDs that they do not need. Our preliminary results reported in this paper
demonstrate that in both respects our hybrid approach outperforms both the
symbolic approach and the statistical approach by themselves.
S.A. Cerri, G. Gouardères, and F. Paraguaçu (Eds.): ITS 2002, LNCS 2363, pp. 552–561, 2002.
c Springer-Verlag Berlin Heidelberg 2002


A Hybrid Language Understanding Approach

2

553

The Role of KCDs in Why2

When students interact with Why2, they are ﬁrst presented with an essay question. After reading the essay question, the student types in an initial essay.
The system then analyzes the student’s essay in order to detect the presence
of misconceptions and required concepts, which are determined when each of
Why2’s problems are designed. The system then uses KCDs both for coaching
students to insert missing required concepts (elicitation KCDs) and to remove
the expression of misconceptions (remediation KCDs).
1-Tutor Let’s say a car is moving along a straight road with a constant velocity, V c.
Let’s say the car moves along the x-axis. An airplane ascends on take-oﬀ with a
velocity V a, at an angle, theta, to the x-axis. Say the car and airplane have the
same (constant) velocity in the x-direction, or V cx = V ax .
For any given time interval, what is the relationship between the displacement of the car and the displacement of the airplane along the x-axis?
2-Student not sure
3-Tutor The displacement of a body in a given direction and over a given time interval
is equal to what quantity multiplied by that time interval?
4-Student speed
5-Tutor The displacement of a body over a given time interval is equal to its average
velocity divided by that time interval.
If something moves at a constant velocity of magnitude V c, what is the
magnitude of its average velocity across any time interval?
6-Student V c
7-Tutor Super. Thus, since the car and airplane move at the same constant velocity
in the x-direction, what is the relationship between their average velocities in the
x-direction over any interval of time?
8-Student same
9-Tutor Correct. So if the average velocity in the x-direction is the same for the
car and airplane for any given time interval, what is the relationship between the
displacements of the car and airplane in the x-direction?
10-Student same
11-Tutor Okay. In general, if two bodies have the same constant velocity along a
given direction, for any time point, their displacements along that direction will
be the same.
Fig. 1. This ﬁgure contains an example remediation KCD for teaching the connection
between velocity and displacement.

The ultimate goal of Why2 is to coach students through the process of constructing explanations that are complete and do not contain any misconceptions.
Knowledge construction dialogues (KCDs) are the main instructional tools used
by Why2 for coaching students. KCDs are interactive directed lines of reason-

554

C.P. Rosé et al.

ing that each teach a speciﬁc concept. Through these KCDs, students are lead
step by step by a sequence of questions designed to guide them into discovering conceptual knowledge as independently as possible. They have a very simple underlying dialogue management mechanism, speciﬁcally a ﬁnite state push
down automaton. And they make use of very simple shallow semantic parsing
grammars to analyze student input, classifying it into one of a small number of
pre-deﬁned answer classes. KCDs have already been demonstrated to be eﬀective
for increasing students’ conceptual knowledge of physics (Rosé et al., 2001).
To illustrate the revision cycle that Why2 engages students in, take as an
example the case where the student has the misconception that two objects with
the same velocity and initial position may have diﬀerent displacements from their
points of origin. This can be remediated by engaging them in a dialogue about
the relationship between velocity and displacement, as in Figure 1. Typical of
KCDs, this KCD begins by introducing a scenario to make the underlying physics
concept easy for students to relate to. The tutor’s turn ends with a question.
In Turn 2, the student indicates that he does not know the answer. Turns 3-10
form a subdialogue meant to respond to the lack of knowledge communicated by
the student’s Turn 2. In general, remediation subdialogues are provided to students each time the student fails to give a correct answer to a tutor’s question.
Thus, with main lines of reasoning having embedded remediation subdialogues,
which in turn may have other embedded remediation subdialogues, KCDs follow a hierarchical structure. This hierarchical structure allows KCDs to tailor
their instruction to the speciﬁc needs of the student. After the student completes
the remediation KCD by demonstrating a correct understanding of the underlying physics principle, the student is then asked to correct the essay where the
misconception was expressed.
1-Tutor If two objects have the same horizontal velocity, and if they start out in the
same position, then what will be the relationship between the horizontal displacement of the two objects from the point of release?
2-Student same
3-Tutor Good. So if the horizontal velocity of the pumpkin is the same as the horizontal velocity of the runner, then what is the relationship between their respective
displacements from the point of release?
4-Student they will equal each other
Fig. 2. This Figure contains an example Elicitation KCD about the connection between
velocity and displacement.

If the system determines that a required concept is absent from a student’s
essay, the system engages the student in an elicitation KCD in order to encourage the student to articulate that concept. An example elicitation KCD is found
in Figure 2. In this case the system is attempting to encourage the student to
include a statement about the displacements of two objects being equal because

A Hybrid Language Understanding Approach
Recall:
Precision:
IncorrectPoints
IncorrectlyIdentiﬁed
False alarm rate:
EssayQuality:

NumberCorrectlyIdentiﬁed / CorrectPoints
NumberCorrectlyIdentiﬁed / NumberIdentiﬁed
TotalNumberPoints - CorrectPoints
NumberIdentiﬁed - NumberCorrectlyIdentiﬁed
IncorrectlyIdentiﬁed / IncorrectPoints
CorrectPoints / TotalNumberPoints

NumberCorrectlyIdentiﬁed:
NumberIdentiﬁed:

Recall * CorrectPoints
NumberCorrectlyIdentiﬁed / Precision

TotalKcdsGiven:
KcdsNeeded:

TotalNumberPoints - NumberIdentiﬁed
TotalNumberPoints - CorrectPoints

CorrectButNotIdentiﬁed:
KcdsCorrectlyGiven:

CorrectPoints - NumberCorrectlyIdentiﬁed
TotalKcdsGiven - CorrectButNotIdentiﬁed

KCD recall:
KCD precision:
KCDsNotNeeded
KCDsIncorrectlyGiven
KCD false alarm rate:

KcdsCorrectlyGiven / KcdsNeeded
KcdsCorrectlyGiven / TotalKcdsGiven
TotalNumberPoints - KcdsNeeded
TotalKcdsGiven - KcdsCorrectlyGiven
KCDsIncorrectlyGiven / KCDsNotNeeded

555

Fig. 3. This Figure summarizes our model for predicting KCD precision, recall, and
false alarm rate from analysis precision, recall, and false alarm rate.

their respective velocities are equal. Elicitation KCDs are typically shorter than
remediation KCDs. The idea behind them is that the student may already know
the idea that they are meant to elicit and just has neglected to mention it in
the essay. So they are short and ask questions meant to prompt the student to
articulate the desired concept. If the student does not in fact know the desired
concept, then the student will not be able to answer the questions correctly.
In this way elicitation KCDs can be used as a tool for identifying student misconceptions and missing knowledge. In the case of discovering such a lack, the
system will engage the student in a remediation KCD to remediate the student’s
incorrect answer. Once the student has demonstrated the ability to articulate
the desired concept, the elicitation KCD is complete, and the system asks the
student to insert that required point in the essay.
Essay Quality
0.10-0.30
0.40-0.70
0.80-1.00

KCD Precision KCD Recall
0.98
0.98
0.86
0.86
0.23
0.23

Fig. 4. This Table illustrates how KCD precision and recall vary with essay quality,
keeping 0.90 analysis precision and 0.90 analysis recall.

556

C.P. Rosé et al.
Essay Quality
0.10-0.30
0.40-0.70
0.80-1.00

KCD Precision KCD Recall
0.94
0.97
0.72
0.86
0.13
0.22

Fig. 5. KCD precision and recall with 0.88 analysis Precision, 0.75 analysis Recall,
and .08 analysis False Alarm Rate. Note that this is the result we get with our best
combined approach to essay analysis described below in the Results section.

3

Selecting Appropriate KCDs

In order to build an eﬀective system, it is important both to give students the
KCDs that they do need, and to avoid giving them extraneous KCDs that they
do not need. Neglecting to give a student a KCD that is needed means losing an
opportunity to teach that student something that student needs to know. Giving
a KCD that a student does not need means wasting a student’s time, possibly
distracting that student from what that student really needs to learn, and likely
annoying or even confusing that student. Thus, we would like to build a system
with a high KCD recall and low KCD false alarm rate, where we deﬁne KCD
recall as the percentage of KCDs that a student needs that the system gives.
And KCD false alarm rate as the percentage of KCDs that the student does not
need that the system gives.
Nevertheless, analyzing student essays is a computational linguistics problem,
and performance on this task is most naturally measured in terms of analysis
precision, recall, and false alarm rate over a corpus of student essays. Analysis
precision is the percentage of required points and misconceptions identiﬁed in
the student essays that were actually present in those essays. Note that this
is undeﬁned in the case that no required points are identiﬁed. Related to this
notion is analysis false alarm rate, which is the percentage of required points
not present in the essay that were incorrectly identiﬁed by the system. Analysis
recall is the percentage of misconceptions and required points present in student
essays that were actually identiﬁed by the system. Note that this is undeﬁned
whenever there are no required points present in a student essay. Naturally, a
system that is good at accurately identifying required points and misconceptions
in student essays will also be good at selecting appropriate KCDs to engage
students in. However, the relationship between analysis precision, recall, and
false alarm rate and KCD precision, recall, and false alarm rate varies widely
depending upon the quality of student essays. Thus, in order to make valid
predictions about student experience with the system based on experiments over
corpora of previously collected student essays, we built a mathematical model
to compute KCD precision, recall, and false alarm rate from analysis precision,
recall, and false alarm rate as it varies with diﬀerent essay qualities. The model
is summarized in Figure 3. From this model it is possible to predict how well

A Hybrid Language Understanding Approach

557

we need to do at analyzing student essays in order to do a good job at selecting
appropriate KCDs. It also makes it possible to make informed decisions about
which out of a set of alternative language understanding approaches is most
suitable based on their relative levels of analysis precision, recall, and false alarm
rate.
We deﬁne Recall for analysis as the number of required points that Why2
correctly identiﬁes as present in a student essay (NumberCorrectlyIdentiﬁed)
divided by the total number of required points actually present in the essay
(CorrectPoints). Precision is NumberCorrectlyIdentiﬁed divided by the total
number of required points that Why2 identiﬁed, correctly or incorrectly (NumberIdentiﬁed). The number of points not correctly encoded in an essay (IncorrectPoints) is computed by subtracting CorrectPoints from TotalNumberPoints.
To compute the false alarm rate, then, simply substract NumberCorrectlyIdentiﬁed from NumberIdentiﬁed and divide the resulting number by IncorrectPoints.
EssayQuality is CorrectPoints divided by the total number of required points
(TotalNumberPoints).
In order to project KCD precision, recall, and false alarm rate for diﬀerent
essay qualities, we need to transform these equations in order to compute values
for NumberCorrectlyIdentiﬁed and NumberIdentiﬁed as they vary with essay
quality. Thus, from the Recall equation we derive the equation that NumberCorrectlyIdentiﬁed is Recall multiplied by CorrectPoints. And from the Precision equation we derive the equation that NumberIdentiﬁed equals NumberCorrectlyIdentiﬁed divided by Precision. Why2 gives an elicitation KCD for every
required point not identiﬁed in the student essay. Thus, the total number of elicitation KCDs given correctly or incorrectly (TotalKcdsGiven) is TotalNumberPoints minus NumberIdentiﬁed. However, the number of KCDs that the student
actually needs (KcdsNeeded) is TotalNumberPoints minus CorrectPoints. In order to determine how many KCDs were correctly given (KcdsCorrectlyGiven),
we ﬁrst need to know how many required points the student included in the essay
that were not identiﬁed by Why2 (CorrectButNotIdentiﬁed). If we know CorrectPoints and NumberCorrectlyIdentiﬁed, we can get CorrectButNotIdentiﬁed
by subtracting NumberCorrectlyIdentiﬁed from CorrectPoints. Then, KcdsCorrectlyGiven will be TotalKcdsGiven - CorrectButNotIdentiﬁed, since a KCD will
be incorrectly given if the student expressed the corresponding point but Why2
missed it. Now we have enough information to compute KCD precision and recall. KCD recall is the percentage of KCDs that were given that the student
needed, thus, KcdsCorrectlyGiven divided KcdsNeeded. And KCD precision is
the percentage of KCDs given that were actually needed, thus, KcdsCorrectlyGiven divided by TotalKcdsGiven. To compute KCD false alarm rate, you must
ﬁrst determine the number of KCDs not needed (KCDsNotNeeded). You can
compute this by subtracting KcdsNeeded from TotalNumberPoints. You also
need to know how many KCDs were incorrectly given (KCDsIncorrectlyGiven).
You can compute this by subtracting KcdsCorrectlyGiven from TotalKcdsGiven.
Note that this is equivalent to CorrectButNotIdentiﬁed. Thus, KCD false alarm
rate is KCDsIncorrectlyGiven divided by KCDsNotNeeded. Note that the pro-

558

C.P. Rosé et al.

jection of analysis precision and recall onto KCD precision and recall works out
most accurately if we treat the undeﬁned cases for analysis precision and recall
discussed above as 1.0.
From this mathematical model we determined that as essay quality increases,
it becomes much more diﬃcult to do a good job at selecting appropriate KCDs
for students. In fact, selecting appropriate KCDs for students with essay qualities
of 0.80 or higher may well be completely out of our reach. In particular, even if
analysis precision and recall are at 0.90, KCD precision, recall, and false alarm
rate become unsatisfyingly low once essay quality is 0.70 or higher. See Figure 4.
Thus, helping excellent students improve their ability to construct high quality
conceptual physics explanations may require an entirely diﬀerent approach. From
this model we have also determined, not too surprisingly, that performance can
remain reasonable even if analysis recall is low. A low analysis precision means
students will not get KCDs that are needed. On the other hand, a low recall
means that students will get KCDs that they do not need. In Figure 5 we see
that if analysis recall is low but precision remains near the 0.90 level, KCD recall
remains high. Although this phenomenon seems counter-intuitive at ﬁrst glance,
it makes sense when one considers that if precision remains the same but recall
is decreased, then the total number of points identiﬁed will be smaller, thus the
total number of KCDs given will be higher. When essay quality is low and many
KCDs are needed, the likelihood is that increasing the number of KCDs given
will increase the number of KCDs correctly given. Nevertheless, KCD precision
seriously suﬀers for higher quality essays. By the time essay quality is at 0.40,
a quarter of the KCDs given will be inappropriate, and over half of the KCDs
given for essays of quality 0.70 or more will be inappropriate.

4

Combining Deep and Shallow Approaches to Language
Understanding

Many successful tutoring systems that accept natural language input employ
shallow approaches to language understanding. For example, CIRCSIM-tutor
(Glass, 1999) and Andes-Atlas (Rosé et al, 2001) parse student answers using shallow semantic grammars to identify key concepts embedded therein. The
Auto-Tutor (Wiemer-Hastings et al, 1998) system uses Latent Semantic Analysis (LSA) to process lengthy student answers. “Bag of Words” approaches such
as LSA (Landauer et al., 1998) HAL (Burgess et al., 1998), and Rainbow (McCallum, 1996), have enjoyed a great deal of success in a wide range of applications.
Recently a number of dialogue based tutoring systems have begun to employ
more linguistically sophisticated techniques for analyzing student language input, namely the Geometry tutor (Aleven et al., 2001), BEETLE (Core et al.,
2001), and Why2 (Vanlehn et al., 2002). Each approach has its own unique
strengths and weaknesses. “Bag of Words” approaches require relatively little
development time, are totally impervious to ungrammatical input, and tend to
perform well because much can be inferred about student knowledge just from
the words they use. On the other hand, symbolic, knowledge based approaches

A Hybrid Language Understanding Approach

559

require a great deal of development time and tend to be more brittle than superﬁcial “Bag of Words” types of approaches, although robustness techniques
can increase their level of imperviousness (Rosé 2000). To their credit, linguistic
knowledge based approaches are more precise and capture nuances that “Bag
of Words” approaches miss. For example, they capture key aspects of meaning
that are communicated structurally through scope and subordination and do
not ignore common, but nevertheless crucial, function words such as ’not’.
Recent work suggests that symbolic and “Bag of Words” approaches can
be productively combined. For example, syntactic information can be used to
modify the LSA space of a verb in order to make LSA sensitive to diﬀerent word
senses (Kintsch, 2002). Along similar lines, syntactic information can be used, as
in Structured Latent Semantic Analysis (SLSA), to improve the results obtained
by LSA over single sentences (Wiemer-Hastings and Zipitria, 2001).
A detailed description of our approach to language understanding is beyond
the scope of this paper, but can be found in (Vanlehn et al., 2002). In brief, we use
the CARMEL core understanding component (Rosé, 2000) for symbolic sentence
level language understanding. It takes natural language as input and produces a
set of ﬁrst order logical forms to pass on to the discourse language understanding
(DLU) module (Jordan et al., 2002). We use Rainbow (McCallum, 1996), a naive
Bayes classiﬁer, for an alternative “Bag of Words” sentence level language understanding approach. It assigns sentences to classes that are associated with sets of
logical forms in the same representation language as CARMEL produces. Thus,
output from either source is appropriate input for the DLU module. However,
the classiﬁcation approach has the drawback that it embodies the underlying
simplifying assumption that students always express required points in a single
sentence, which is not always the case. After sentence level processing, the DLU
module combines the sentence level information by making abductive inferences
about how the pieces of information ﬁt together using Tacitus-Lite+ (Jordan et
al., 2002). The resulting proof trees are then used as the basis for determining
which required points are missing from student essays, when optional points are
not mentioned or inferable from what is mentioned, and which misconceptions
may be present. For our combined approach, we use a decision tree trained with
the ID3 decision tree learning algorithm (Mitchel, 1997) to combine Rainbow’s
prediction with syntactic information in order to formulate a hypothesis about
the classiﬁcation of each sentence. we extract syntactic features for each sentence from the representation constructed by the parser. These features encode
functional relationships between syntactic heads (e.g., (subj-throw man)), tense
information (e.g., (tense-throw past)), and information about passivization and
negation (e.g., (negation-throw +) or (passive-throw -)). We also extract word
features that indicate the presence or absence of a root form of a word from the
sentence. ID3 uses these features to construct a decision tree for identifying the
correct classiﬁcation of novel sentences.

560

5

C.P. Rosé et al.

Results

We conducted a series of experiments to evaluate our statistical, symbolic, and
combined approach. We used as our test set a corpus of 33 essays collected during web-based tutoring sessions that were not used as development data. The
web based tutoring sessions during which we collected this corpus involved university students and a human tutor where students were answering the question
“Suppose you are running in a straight line at constant speed. You throw a
pumpkin straight up. Where will it land? Explain.” We divided these essays into
a total of 130 sentence segments. For 77% of the data, three diﬀerence coders
hand-classiﬁed each segment as having one or none of the 6 points required to
solve the essay problem. We computed a pairwise Kappa coeﬃcient to measure
the agreement between coders, which was always greater than .75. We then selected one coder to complete the coding of the remainder of the data. We used
that coder’s data as a gold standard to use for measuring the performance of
our alternative approaches. We computed average per essay performance over
25 trials of randomly selecting essays covering 10% of the corpus, training the
decision tree using ID3 on the rest, and then testing the selected essays.
Since Why2’s domain speciﬁc knowledge sources are early in their development, we expected the symbolic only approach to perform poorly, and it did.
It got an analysis precision of 17%, recall of 19%, and false alarm rate of 33%.
Averaged over the essays in our test set, this translates in to a KCD precision of
64%, recall of 78%, and false alarm rate of 81%. The statistical only approach
performed better overall with an anlysis precision of 75%, recall of 73%, and
false alarm rate of 15%. This translates into an average KCD precision of 88%,
recall of 90%, and false alarm rate of 27%. The combined approach performed
best of all with an analysis precision of 88%, recall of 75%, and false alarm rate
of 8%. Notice that the combined approach performs as well as or better than
both the statistical and the symbolic approach on analysis precision, recall, and
false alarm rate as well as KCD selection precision, recall, and false alarm rate.
The most striking aspects of the results are that it achieves a 95% KCD recall,
a full 5% increase over the statistical approach, which cutting the statistical approach’s analysis false alarm rate in half. The results for this combined approach
are displayed in Figure 5.

6

Conclusions and Current Directions

In this paper we have discussed the problem of selecting appropriate Knowledge Construction Dialogues (KCDs) for the purpose of encouraging students to
include important points in their qualitative physics explanations that are missing. We have presented a model for projecting analysis precision, recall, and false
alarm rate into KCD selection precision, recall, and false alarm rate. We used
this model to inform the design of a heuristic for combining predictions from
a symbolic and a statistical approach to essay analysis. We have demonstrated
that our combined approach outperforms both the symbolic and the statistical

A Hybrid Language Understanding Approach

561

approach alone in terms of both KCD selection precision, recall, and false alarm
rate.
Acknowledgments. The authors would like to thank the rest of the Natural
Language Tutoring group for their collaboration.
This research is supported by the Oﬃce of Naval Research, Cognitive
and Neural Sciences Division MURI Grant N00014-00-1-0600 and NSF Grant
9720359 to CIRCLE, a center for research on intelligent tutoring.

References
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

V. Aleven, O. Popescu, and K. Koedinger. 2001. Pedagogical content knowledge in
a tutorial dialogue system to support self-explanation. In Papers of the AIED-2001
Workshop on Tutorial Dialogue Systems.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations in context space: Words,
sentences, discourse. Discourse Processes, 25(2):211–257.
M. Chi, N. de Leeuw, M. Chiu, and C. LaVancher. 1981. Eliciting self-exsplanations
improves understanding. Cognitive Science, 18(3).
M. G. Core, J. D. Moore, and C. Zinn. 2001. Initiative management for tutorial
dialogue. In Proceedings of the NAACL Workshop Adaption in Dialogue Systems.
M. S. Glass. 1999. Broadening Input Understanding in an Intelligent Tutoring
System. Ph.D. thesis, Illinois Institute of Technology.
Pamela W. Jordan, Maxim Makatchev, Michael Ringenberg, and Kurt VanLehn.
2002. Engineering the Tacitus-lite weighted abductive inference engine for use in
the Why-Atlas qualitative physics tutoring system. submitted.
W. Kintsch. 2002. Predication. to appear in the Cognitive Science Journal.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Introduction to latent semantic
analysis. To Appear in Discourse Processes.
Andrew Kachites McCallum. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classiﬁcation and clustering. http://www.cs.cmu.edu/ mccallum/bow.
Mitchel, T. 1997. Machine Learning. McGraw Hill.
C. P. Rosé, P. Jordan, M. Ringenberg, S. Siler, K. VanLehn, and A. Weinstein.
2001. Interactive conceptual tutoring in atlas-andes. In Proceedings of Artiﬁcial
Intelligence in Education.
C. P. Rosé. 2000. A framework for robust sentence level interpretation. In Proceedings of the First Meeting of the North American Chapter of the Association
for Computational Lingusitics.
K. VanLehn, P. Jordan, C. P. Rosé, and The Natural Language Tutoring Group.
2002. The architecture of why2-atlas: a coach for qualitative physics essay writing.
In Proceedings of the Intelligent Tutoring Conference.
P. Wiemer-Hastings and I. Zipitria. 2001. Rules for syntax, vectors for semantics.
In Proceedings of the Twenty-third Annual Conference of the Cognitive Science
Society.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tutoring Research Group.
1998. The foundations and architecture of autotutor. In B. Goettl, H. Halﬀ,
C. Redﬁeld, and V. Shute, editors, Intelligent Tutoring Systems: 4th International
Conference (ITS ’98), pages 334–343. Springer Verlag.

User Model User-Adap Inter (2011) 21:99–135
DOI 10.1007/s11257-010-9086-0
ORIGINAL PAPER

An analysis of students’ gaming behaviors
in an intelligent tutoring system: predictors
and impacts
Kasia Muldner · Winslow Burleson ·
Brett Van de Sande · Kurt VanLehn

Received: 30 April 2010 / Accepted in revised form: 17 November 2010 /
Published online: 4 January 2011
© Springer Science+Business Media B.V. 2010

Abstract Students who exploit properties of an instructional system to make
progress while avoiding learning are said to be “gaming” the system. In order to investigate what causes gaming and how it impacts students, we analyzed log data from
two Intelligent Tutoring Systems (ITS). The primary analyses focused on six college
physics classes using the Andes ITS for homework and test preparation, starting with
the research question: What is a better predictor of gaming, problem or student? To
address this question, we developed a computational gaming detector for automatically labeling the Andes data, and applied several data mining techniques, including
machine learning of Bayesian network parameters. Contrary to some prior findings,
the analyses indicated that student was a better predictor of gaming than problem.
This result was surprising, so we tested and confirmed it with log data from a second
ITS (the Algebra Cognitive Tutor) and population (high school students). Given that
student was more predictive of gaming than problem, subsequent analyses focused
on how students gamed and in turn benefited (or not) from instructional features of
the environment, as well as how gaming in general influenced problem solving and
learning outcomes.

K. Muldner (B)
Department of Psychology, Arizona State University, Tempe, AZ, USA
e-mail: katarzyna.muldner@asu.edu
W. Burleson · B. Van de Sande · K. VanLehn
School of Computing, Informatics and Decision Systems Engineering, Arizona State University,
Tempe, AZ, USA
e-mail: winslow.burleson@asu.edu
B. Van de Sande
e-mail: bvds@asu.edu
K. VanLehn
e-mail: kurt.vanlehn@asu.edu

123

100

K. Muldner et al.

Keywords Educational data mining · Gaming · Utility of hints ·
Bayesian network parameter learning

1 Introduction
Students have long found ways to avoid reasoning about instructional materials, e.g.,
by copying from examples to generate problem solutions (VanLehn 1998), by paraphrasing text instead of self-explaining it more deeply (Chi et al. 1989), or by passively
listening to tutors’ didactic explanations without providing substantial contributions,
even though active participation is needed for effective learning (Chi et al. 2001). A
name given to shallow reasoning in the context of an Intelligent Tutoring System (ITS)
is gaming: “attempting to succeed by exploiting properties of the system rather than by
learning the material” (Baker et al. 2009). Not surprisingly, gaming is associated with
reduced learning (Baker et al. 2004b), and so there have been efforts to detect gaming
(Baker et al. 2006b, 2008a), understand gaming (Baker et al. 2008b; Rodrigo et al.
2008; Baker et al. 2009) and reduce gaming (Murray and VanLehn 2005; Walonoski
and Heffernan 2006b). While progress has been made on each front, challenges still
remain.
A key challenge pertains to understanding the causes of gaming. Several projects
have tackled this challenge by looking for statistical associations with gaming (Arroyo
and Woolf 2005; Baker 2007; Rodrigo et al. 2008; Baker et al. 2009). Early work
focused on student features and how they correlate with gaming, such as students’
goal orientation (Baker et al. 2005), attitudes (Baker et al. 2008b), and affect (Rodrigo
et al. 2008). Of the studies that reported statistical relationships between student features and gaming, the maximum variance accounted for by student features was about
9% (Arroyo and Woolf 2005). In contrast, Baker et al. (2009) found that 56% of the variance in gaming was associated with lesson features, such as confusing hints and poor
interface design. Because the lesson features explained more of the gaming variance
than prior work involving student features, the argument was made that instructional
defects are a key predictor of gaming. These findings are logical: if the student is at
an impasse that appears to be caused by poor system design rather than by a lack of
knowledge, then the student will game in order to work around the impasse. On the
other hand, non-ITS research has shown that even when students use exactly the same
instructional materials, they vary dramatically in how they choose to process them
(e.g., Chi et al. 1989; Renkl 1997; VanLehn 1998). These non-ITS studies suggest that
some kind of student features (e.g., knowledge, motivation) might be more important
than instructional features in determining gaming.
To investigate the predictors of gaming further, we conducted an in-depth analysis
of log data corresponding to several years worth of students interacting with Andes,
an ITS for introductory physics (VanLehn et al. 2005). To identify gaming episodes
in this data, we applied a computational gaming detector that we calibrated with a
hand-analysis of the data. This detector provided the input to a series of data mining
analyses that we conducted to better understand gaming. The initial research question
was: What is a better predictor of gaming, problem or student?

123

An analysis of students’ gaming behaviors

101

Contrary to some prior work (e.g., Baker et al. 2009), we found that gaming is best
predicted by student, and that by accounting for both student and problem features
our approach explains 61% of the variance in the Andes data set. Four separate analyses of the Andes data support this conclusion. To extend and validate these findings,
we applied our approach to a different ITS (the Algebra Cognitive Tutor, Carnegie
Learning Inc. 2010) and population (high school students); we found that student was
again a better predictor than problem for the majority of the analyses.
Having established that frequency of gaming depends more on who rather than what
is being gamed, we next sought to characterize where students were gaming, and how
gaming behaviors and subsequent learning and problem-solving outcomes depended
on students who gamed frequently (high gamers) versus infrequently (low gamers).
While we found that low gamers did indeed game differently than high gamers, in
general a frequently gamed feature was high-level hints. Thus, to see if poor hint
usability was driving gaming, the subsequent analysis focused on how hints scaffolded problem solving and learning. The findings show that when students actually try
to use high-level hints, they eventually obtain the correct solution, although this may
require several attempts, particularly for high gamers. However, neither high-level nor
bottom-out hints elicited as much learning as we anticipated. As follow-up analysis
demonstrated, this result was likely mediated by gaming. In particular, high gamers
did not appear to benefit from high-level hints and in general learned less from hints
than low gamers. Altogether, these analyses highlight the need for further research to
obtain a better understanding of the utility of various kinds of help in ITSs and how
to best promote learning for different types of students from it.
One of the techniques we relied on corresponded to Dynamic Bayesian Network
(DBN) parameter learning. Bayesian networks, of which DBNs are a specific class, are
established in the user modeling community as a tool for representing and reasoning
about user states of interest (e.g., Mayo and Mitrovic 2001; Conati et al. 2002; Reye
2004); Bayesian networks have also been used in a data mining context outside of the
educational community (e.g., for understanding tuberculosis epidemiology (Getoor
et al. 2004)). However, within educational data mining (EDM), the application of
Bayesian networks is more recent (Beck et al. 2008), although it shows high promise:
DBN parameter mining obtained comparable results to other, arguably more established, techniques (Zhang et al. 2008).
To summarize, this paper includes three key analyses: (1) whether student or problem better predicts gaming, (2) how and where students are gaming and (3) the utility
of ITS help features, including the impact of gaming and various kinds of help on
problem solving and learning. Each analysis is motivated by its predecessor. In particular, because we found that student was such a strong predictor of gaming (analysis 1),
we wanted to better understand how and what students were gaming, and whether certain groups of students (e.g., low gamers) were gaming differently than other groups
(analysis 2). Since we found that students in general and high gamers in particular had
a very high rate of help abuse, this led us to explore how ITS help scaffolded students,
and how gaming influenced this process (analysis 3).
Our work brings the following contributions: (1) Through a variety of EDM techniques, we show that student is a better predictor of gaming than problem; (2) We
identify individual differences in terms of how students game and in turn benefit (or

123

102

K. Muldner et al.

not) from instructional features of the environment; (3) We extend the work in (Beck
et al. 2008) on Bayesian network parameter learning, both through its application to
a novel data set and by subsequent analysis exploring the impact of gaming and various types of help on problem solving and learning for low versus high gamers. We
presented a subset of this work in (Muldner et al. 2010), namely the gaming detector
and its application to the Andes data for exploring problem versus student predictors
of gaming, as well as a basic analysis on the short-term impact of ITS help. Here,
we extend this work by applying our approach to a second data set, namely from the
Cognitive Tutor, as well as presenting substantial new results from Bayesian parameter
learning not in (Muldner et al. 2010).
We begin with a survey of related work (Sect. 2). We then present the gaming detector (Sect. 3) and the log data analysis to investigate predictors of gaming (Sect. 4);
we validate this analysis by applying it to data from a different tutor and population
(Sect. 5). Next, we present students’ gaming profiles (Sect. 6). Since we found that
high-level hints were one of the most abused features, we performed an in-depth analysis of how students used hints (Sect. 7). We then present findings on the impact of
gaming on hint utility, problem solving and learning (Sect. 8). We conclude with a
discussion of the findings and future work.

2 Related work
Students who frequently game tend to score lower on a post-test than students who
never game (Baker et al. 2004b; Walonoski and Heffernan 2006a); likewise, a specific
form of gaming, namely help abuse corresponding to skipping hints, is negatively
correlated with learning (Aleven et al. 2006). Research also shows that some students
are less likely to be hurt by gaming, such as ones with high initial domain knowledge
(Baker et al. 2004a)—this makes sense in that if a concept is already known, gaming
is not likely to erode that knowledge.
Given that some work suggests gaming hurts learning, the next question is: How
wide-spread is this behavior? One study found that only about 10% of students ever
gamed (Baker et al. 2004b), while in another study, all students gamed, albeit only some
were frequent gamers (i.e., 15% were high gamers and another 25% were above-average gamers) (Walonoski and Heffernan 2006a). This discrepancy in gaming frequency
may be due to the type of data collection method employed in each study. Baker et al.
(2004b) used human observers to identify and record gaming episodes in real-time as
students interacted with an ITS; it is possible that these observers missed some episodes. In contrast, Walonoski and Heffernan (2006a) employed a post-hoc rule-based
detection algorithm to identify gaming frequencies (more on gaming detection below).
For the remainder of this section, we first survey work on detecting and understanding gaming, using EDM and other techniques, respectively. To discern between EDM
versus other work, we rely on the following definition: “educational data mining . . .
is defined as the area of scientific inquiry centered around the development of methods
for making discoveries within the unique kinds of data that come from educational
settings, and using those methods to better understand students and the settings which

123

An analysis of students’ gaming behaviors

103

they learn in” (Baker 2010). We then provide an overview of interventions designed
to discourage gaming, and wrap up with EDM research on ITS help functionality.
2.1 EDM work on detecting and understanding the causes of gaming
Some of the EDM gaming-related work has focused on building machine detectors
for gaming identification (Baker et al. 2004a, 2006b; Walonoski and Heffernan 2006a;
Baker et al. 2008a). For instance, Baker et al. (2004a) used a density estimator with a
set of Latent Response Models to predict how often a student would game the system.
The gold standard to evaluate this detector was classroom observation of students’
gaming behaviors. In this study, only some students’ learning was hurt by gaming,
and the detector accurately identified these students, but it was not accurate in recognizing the “gamed-not hurt” students (i.e., ones whose learning was unaffected by
gaming). Another detector relied on various algorithms in WEKA, a machine learning
package, to identify gaming (Walonoski and Heffernan 2006a)—while this detector
correctly identified lack of gaming (98%), it’s ability to recognize when gaming was
occurring was quite low (19%).
There is also EDM work on understanding why gaming occurs. Some researchers
propose that gaming is due to features of the instructional materials, including poor
ITS design (Baker et al. 2009). This conjecture is based on analysis that involved (1)
identifying salient lesson features, such as the total number of skills in a lesson and/or
clarity of lesson interface icons; (2) clustering the features using Principal Component
Analysis, and (3) checking for significant correlations between the clusters and a students’ frequency of gaming. The full model explained 56% of the data variance, which
was much higher than prior attempts, suggesting that lesson features are a strong predictor of gaming; likewise, Baker (2007) reported that lesson features were a stronger
predictor of gaming than student features.
As far as student characteristics that drive gaming are concerned, Baker et al.
(2004a) use the Latent Response Model described above to show that students’ domain
knowledge influences gaming frequency. In that model, relevant features such as number of errors and/or prior knowledge were included only if they reduced prediction
error. Thus, the features remaining provided insight into factors influencing gaming.
One of the remaining features encapsulated that if a student knew a skill then s/he had
a low probability of gaming. Furthermore, the model predicted that students who were
most hurt by gaming gamed on steps they did not know—this lead to the conjecture
that students game on difficult steps (i.e., an unknown step is a difficult step). Other
work has explored the relationship between student affect and gaming, and found that
boredom is the most frequent emotion to precede gaming (Rodrigo et al. 2008).
2.2 Other work on detecting and understanding the causes of gaming
Some work does not rely on EDM for gaming detection, using instead other methods
for labeling the data as gamed or not, but does use EDM for subsequent analysis of the
labeled data (Baker et al. 2009; Cohen and Beal 2009)—we describe a representative
sample of such work here.

123

104

K. Muldner et al.

One gaming detection approach entails human observation (Baker et al. 2004b;
Walonoski and Heffernan 2006a). For instance, in the seminal work on gaming, Baker
et al. (2004b) used several human observers in the classroom, as students interacted
with an ITS. The observers identified gaming by visual inspection. This approach
is challenging for several reasons. First, to avoid making students uncomfortable,
observers only watched students out of the corner of their eye, and so may have missed
nuances in a fast-paced classroom environment. Second, it’s challenging for a human
observer to be consistent in terms of ensuring all students are observed equally, and
so the results may be biased towards certain students. Given these challenges related
to human observation, there has subsequently been work on post hoc gaming identification. One method entails hand labeling of log data by a human coder (Baker
et al. 2009). This approach affords the coder time to consider all student actions and
attend to context-specific details that may be hard for a machine algorithm to recognize. However, hand-coding of data is very labor intensive and human coders may
be inconsistent, particularly given the copious amounts of data that typically need to
be coded. Other approaches have focused on machine-based gaming detection. For
instance, the Help Tutor (Roll et al. 2006) relied on if-then rules that used latency
between actions to recognize when students were gaming hints by skipping them. In
contrast to the EDM approaches described above, these rules were designed by hand,
using an expert-centric approach of relying on existing psychology work to guide the
rule design. Likewise, latency between successive attempts and/or hint requests was
used to identify gaming in the Wayang Outpost tutor (Cohen and Beal 2009).
In addition to detection, there is also work outside of the EDM community on
understanding how student characteristics impact gaming. For instance, surveys were
administered to students to study how attitudes, e.g., towards computers, influence
gaming (Baker et al. 2008b). Survey methods were also used to understand how goal
orientation influences gaming frequency (Baker et al. 2005), and in particular whether
students with performance goals game more than those without. Contrary to expectations, no link was found between gaming and performance goal orientation; in fact,
students with this orientation solved problems more slowly than other students.

2.3 Interventions for discouraging gaming
Various interventions have been developed to discourage gaming and so foster learning—the design of these interventions is often informed by EDM findings, and so for
the sake of completeness, here we review a sample of this work. One strategy involved
the introduction of a mandatory delay before a student could ask for a hint, to discourage students from rushing through hints (Aleven 2001; Murray and VanLehn 2005).
Murray and VanLehn (2005) found that this design significantly reduced help requests,
but overall did not impact learning (although students with low initial expertise did
marginally learn more when given the intervention). Interestingly, the introduction of
a mandatory delay resulted in a new form of gaming, where students would repeatedly
cancel a step until they received proactive help from the tutor.
Other strategies for discouraging gaming include the incorporation of (1) supplementary exercises and (2) an animated agent that shows disapproval upon detection of

123

An analysis of students’ gaming behaviors

105

inappropriate student behaviors (Baker et al. 2006a). These interventions had a marginal impact on reducing overall gaming, as compared to a control condition. However,
while fewer students gamed, those students who did game did not game less. Furthermore, the interventions did not affect overall student learning. Yet another approach
to discourage gaming relies on visualizations of various student behaviors (Walonoski
and Heffernan 2006b; Arroyo et al. 2007). Walonoski and Heffernan (2006b) found
that a visualization of student behaviors did significantly reduce gaming in an ITS;
impact on learning was not assessed. This was done by Arroyo et al. (2007), who found
that visualizations of student progress fostered learning, while reducing some types of
gaming. There is also work on reducing help abuse, a form of gaming, through written
messages delivered upon its detection (Roll et al. 2006). This intervention significantly
reduced help abuse, but did not impact learning. In summary, while progress has been
made in discouraging gaming, more work is needed to understand how do so for all
students and how to improve learning outcomes in the process.

2.4 EDM work on understanding the utility of its help-related functionality
As we mentioned above, our analysis of gaming prompted us to investigate the utility
of tutor help. Therefore, here we review a representative sample of EDM work related
to understanding and/or modeling ITS help functionality.
Typically, skipping through high-level hints to reach the bottom-out hint that essentially provides the solution step is labeled as gaming. In contrast, Shih et al. (2008)
conjecture that bottom-out hints could be considered analogous to worked-out examples, and so viewing only the bottom-out hint might not be harmful to learning. To
test this conjecture, a model was used that took into account the time a student thinks
about the bottom-out hint. The model embedded the assumption that the time before
an answer is produced corresponds to a student thinking about applying the hint, while
the time after an answer is produced corresponds to a student reflecting about the hint.
Analysis of log file data shows that bottom-out hint usage is indeed correlated with
learning, as are the model time-related parameters. While this study suggests there
is utility to bottom-out hints, it does not tell us whether skipping high-level hints is
harmful, or what added value (if any) there is to those kinds of hints.
In general, high-level hints are intended to elicit information from students, while
bottom-out hints tell information. Human tutors use a variety of elicit versus tell strategies during one-on-one tutoring sessions; these strategies are much more flexible than
the ITS standard of including several high-level hints always followed by a bottom-out
hint. Some researchers have investigated whether Reinforcement Learning (RL) could
be applied to data from a tutoring corpus to learn elicit versus tell rules that would
enable an ITS to more flexibly provide help (Chi et al. 2010a,b). While an earlier
study did not show the RL approach to be significantly better than a random strategy, a subsequent model did generate better learning gains over a random approach.
In this latter model, a larger set of features was used than in the original model,
which included student, domain and system features. Since these features were not
always ones predicted by current theories of learning, this work highlights how EDM

123

106

K. Muldner et al.

can extend existing theories by providing fine-grained insights about the pedagogical
process.
Yet another example of EDM work investigating instructional interventions related
to ITS help provision used learning decomposition (Feng et al. 2009). Three pedagogical approaches, embedded in an ITS, were compared, including: (1) scaffolding
questions that appeared automatically when students generated an incorrect response
and that students had to answer correctly before proceeding; (2) scaffolding questions
that were instead available on demand, and that students were not required to answer;
(3) delayed feedback that was provided in the form of worked-out examples after
all problem solving was done. While the results are only marginally reliable, they
suggest that providing delayed feedback might be better than providing scaffolding
questions that students must answer. There was no difference, however, between scaffolding questions and delayed feedback when students were not forced to answer the
scaffolding questions.
The work described above relied on pre and post test data to help train EDM models. Such data, however is not always available, and so alternatively, students’ actions
during their interaction with an ITS may be used to infer variables of interest. For
instance, Beck et al. (Beck et al. 2008) relied on the BNT-SM toolkit (Chang et al.
2006) to learn the parameters of a Bayesian network designed to assess the impact of
tutor help on students’ problem solving and learning. The resulting parameters suggest that help is slightly more “helpful” in terms of fostering learning than no help
(we provide more details on this work in Sect. 7.3). There is also work on biasing
the parameter learning process through Dirichlet wavelets, initialized to represent the
domain’s parameters (Rai et al. 2009)—to date, however, this method has not been
shown to result in better model prediction. Alternatively to using Dirichlet priors,
others propose using a k-means clustering approach to find parameters for clusters of
skills rather than individual skills, thereby reducing the space of parameters that need
to be learned (Ritter et al. 2008).
3 The primary data and gaming detector
We now describe the primary data source and gaming detector.
3.1 The primary data
Our primary data, obtained from the Pittsburgh Learning Center DataShop, corresponds to logs of students using the Andes ITS (VanLehn et al. 2005) for assigned
class homework and test preparation. This data was collected from a total of six different college physics classes over the span of about three years.
Andes tutors Newtonian physics by providing corrective feedback and hints on the
target domain. Students solve problems in the Andes interface by drawing diagrams
via the provided tools and typing equations—we refer to such interface actions as
entries. Andes does not constrain students’ solution generation, in that students are
free to produce entries in any order they wish and/or to skip entries. Andes provides
immediate feedback for correctness on students’ entries, by coloring the entry red

123

An analysis of students’ gaming behaviors

107

Table 1 Tutor-Student turn pairs
(a) Student: hint request

(b) Student: entry

Fast

Fast

Slow

Slow

(i) Tutor: bottom-out hint

(S)Ski p hint

–

(C)Copy hint

–

(ii) Tutor: high-level hint

(S)Ski p hint

–

–

–

(iii) Tutor: incorrect (red)

–

–

(G)Guess (r ed only)

(iv) Tutor: correct (green)

(P)N o planning

–

–

–

–
–

gamed cells italici zed

(incorrect) or green (correct). As students solve problems, they can ask Andes for
hints. The Andes hint sequence starts out with general information (e.g., “Why don’t
you continue with the solution by working on setting the pressure at a point open to
the atmosphere”) and ends with a bottom-out hint that indicates the step to enter (e.g.,
“Write the equation Pa = Pr0”). To discourage students from always clicking through
to the bottom-out hint, Andes assigns a score to each problem, which is decremented
slightly every time a bottom-out hint is requested. Full details on the system may be
found in (VanLehn et al. 2005).
3.2 The gaming detector
We now describe the gaming detector (Muldner et al. 2010). After irrelevant actions
are removed from the log data, a log consists of a time-stamped sequence of tutor-student turn pairs (e.g., tutor indicates an entry is incorrect, student responds by asking
for a hint). To address our research questions, we needed to know which of these turn
pairs corresponded to gaming. Given that the data comprised about 900,000 pairs,
manual analysis was not feasible. Therefore, we first hand-analyzed a fragment of the
log data to identify patterns indicative of gamed turn pairs (described below). Next,
we encoded these patterns into a computational rule-based gaming detector that could
automatically label the data. We then applied the detector to the data, hand-checking
its output on a new data fragment, and revising as necessary.
For purposes of the analyses reported here, we classified the tutor turns as follows:
(i) coloring an entry red (incorrect), (ii) coloring an entry green (correct), (iii) providing a bottom-out hint, or (iv) providing a high-level hint (we did not further subdivide
the high-level hints since the number and characteristics of such hints varied considerably). We classified a student’s turn as either (a) asking for a hint or (b) generating
an entry. Thus, there are 4 × 2 = 8 types of turn pairs (see Table 1).
Each turn pair has a time duration associated with it, which is how long the student
paused between seeing the tutor’s turn and starting to take action. We assume that turn
pairs with long durations are not gaming. Of the eight possible turn pairs with short
durations (see Table 1), we consider the following five to be gaming:
(1–2) Skipping a hint: the tutor presents a hint and the student skips the hint by quickly
asking for another hint (see ‘S  cells in Table 1). This suggests the student is
trying to reach the bottom-out hint without even reading the preceding hints.

123

108

K. Muldner et al.

(3) Copying a hint: the tutor presents a bottom-out hint and the student quickly
generates a solution entry, suggesting a shallow copy of the hint instead of
learning of the underlying domain principle1 (see ‘C’ cell, Table 1).
(4) Guessing: the tutor signals an incorrect entry, and the student quickly generates
another incorrect entry, suggesting s/he is guessing instead of reasoning about
why the entry is incorrect (see ‘G’ cell in Table 1). This is the only student entry
for which we take into account correctness, as not doing so might incorrectly
classify fixing slips as gaming (i.e., the other half of the fast cell next to the ‘G’
cell, Table 1).
(5) Lack of planning: the tutor signals a correct entry and the student quickly asks
for a hint, suggesting reliance on hints for planning the solution (see ‘P’ cell,
Table 1).
Other gaming detectors also consider hint abuse (i.e., skipping, lack of planning) and
guessing as gaming (e.g., Walonoski and Heffernan 2006a; Cohen and Beal 2009). We
also labeled copying hints as a gaming behavior because we wanted to capture instances
when students were not reasoning about the bottom-out hint content (although as
Sect. 6 describes, the copy category was gamed very infrequently). Note that copying
a hint does not take into account the possibility that a student may copy the hint and
then reason about it. This was explored in (Shih et al. 2008), by analyzing time spent
after a hint was copied. However, a time lag after a hint could also correspond to a
student reasoning about the next solution entry. How students reason after copying a
bottom-out hint could be verified by obtaining verbal protocols of students interacting
with an ITS. Since at this point we do not have such data, for the time being we only
consider time before an entry is generated, as we felt this was more likely to correspond
to reasoning about the entry.
Setting the time thresholds. Gaming detection relies on having reasonable time
thresholds, one for each of the five gamed turn pairs. To set the thresholds, we obtained
a value for each turn pair through a visual inspection of (1) the log file data and (2)
the time distribution for each turn-pair under consideration (e.g., see Fig. 1). We were
conservative when setting the thresholds. For instance, we set the skipping hint threshold T < 3 s 2 . The Andes log files only afford precision rounded to a full second, so
skipping a hint means a student spent zero, one or two seconds on the hint before
asking for another hint. While this range likely does not afford sufficient time to read
all of a hint, the threshold captures instances when students are skipping most of the
hint.
Unless otherwise stated, the results reported throughout this paper are based on
applying the above-described gaming detector to the full Andes data set, corresponding to a set of 318 unique problems and 286 students.

1 A high-level hint followed by a fast entry is not gaming since you can’t copy high-level hints.
2 The other thresholds were as follows: copying <4 s; guessing related to equations that were typed <4 s;

guessing related to free-body diagrams that were drawn with interface tools <6 s. lack of planning<4 s.

123

An analysis of students’ gaming behaviors

109

Fig. 1 Viewing time distribution for turn pairs corresponding to cells (i–ii)-(a) in Table 1

4 What is a better predictor of gaming: student or problem?
A primary research question we wanted to explore is whether student or problem
features better predict gaming. To do so, we first obtained measures on gaming, as
follows:
PerGamingsp
 p=N
p=1 perGaming p /N
s=M
s=1

perGaming p /M

percentage of gaming by a student s on a problem p

(1)

average gaming by a student s across all N
problems p solved by that student
(2)
average gaming on a problem p across all M students s (3)

We use problem as the unit of analysis (see Eq. 1; Eq. 2 and 3 rely on it). Some research
has used lesson as the primary unit of analysis (Baker et al. 2009). In fact, the ideal
unit would correspond to tutor-student turn pairs, as these are where a student makes
a game versus no-game decision. However, we need a unit of analysis that can be
compared across students, so that we can determine whether all students tend to game
at “the same” place. It would be difficult to determine if turn-pairs from one student are
“the same” as turn-pairs from another student. The smallest unit of analysis that allows
simple equivalence across students is the problem. Thus, we chose problem as the unit
of analysis instead of lesson (too large) or tutor-student turn pairs (not equatable; too
small). In these calculations, we use percentage of gaming (see Eq. 1) instead of raw
values to avoid biasing the analysis towards, for instance, short problems.
We did not consider a series of rapid hint requests within a given problem as a single gamed action, for several reasons. First, lumping together a series of hint requests
abstracts information on how many hints and which hints are skipped, and so fails to
provide information on students’ hint usage. Second, it does not make sense to lump
certain gamed actions if other actions are not lumped (e.g., a series of non-gamed
correct entries, or a series of slow hint requests), since this could skew the results.
An alternative approach includes using a step as the base measure within a problem
instead of individual actions, where a step includes all the actions corresponding to a

123

110

K. Muldner et al.

single entry (which includes asking for hints about that entry, incorrect attempts, and
the final correct entry). In this scheme, a step would be labeled as gamed or not, and
the overall percentage of gamed steps within a problem would be used in the various
models. This approach suffers from its own drawbacks—a prominent one being that
it is not clear when a step should be labeled as gamed (e.g., when a student games
a single action while generating a step? Multiple actions—if so how many?). Given
these considerations, for the analysis reported here we label each action as gamed
or not, and then obtain the overall percentage of gamed actions for a given problem,
although we may revisit the step analysis in the future.
To investigate predictors of gaming we conducted four different analyses, enabling
us to triangulate results across them. The first three analyses were presented in (Muldner et al. 2010), extended here with analysis on mined Bayesian parameters in Sect. 4.4.
4.1 Linear regression analysis
One way of investigating predictors of gaming is through a linear regression analysis.
In the linear regression, we used Per Gamingsp (see Eq. 1 above) as the dependent
variable, and two independent variables: (1) student, the average gaming by a student
s across all N problems p solved by that student (see Eq. 2 above), and (2) problem,
the average gaming on a problem p across all M students s who solved that problem (see Eq. 3 above). The model we obtained is significant (F=16915, p < 0.001),
and accounts for 60.8% of the variance (R2 = .608). In this model, both student
and problem yield a significant correlation with the dependent variable, but student is
more strongly associated with gaming (student: standardized coefficient = .658, t =
152.7, p < 0.001; problem: standardized coefficient = .325, t = 74.23, p < 0.001).
Thus, the overall regression equation is as follows:
perGameds,p = const + .658students + .325problemp
If we enter the independent variables separately to analyze the variance explained
by each, the student variable accounts for 49.6% of the variance (standardized
coefficient = .713), while the problem variable accounts for 18.6% of the variance
(standardized coefficient = .434).
To explore the impact of a given data set (i.e., class or course section), we re-ran the
regression analysis with a third independent variable, namely data set id. This variable
explained only an additional 1% of the variance, showing that data set had at best a
weak effect on gaming, and so we did not consider it in subsequent analysis.
4.2 Self-correlation analysis
Another way to verify whether students are more consistently gaming across problems
or if instead problems are more consistent across students is to randomly sub-divide
students (or problems) into buckets and then check for correlation between the buckets.
To this end, to check how consistent students were in terms of gaming across problems
(student self-correlation), we created two buckets by (1) randomly splitting problems

123

An analysis of students’ gaming behaviors

111

Fig. 2 Scatter plot for student (le f t) and problem (right) self correlations from the Andes data (percentage
of gaming shown on X and Y axes)

solved by a given student across the two buckets and (2) storing in each bucket the
average gaming across that bucket’s problem subset, obtained using Eq. 2 above but
applied to the bucket subset. A correlation analysis yielded a high degree of association between the two buckets (r = .963, p < 0.001). That is, if a student tended to
game problems in the A bucket, then that student also tended to game problems in
bucket B.
We used an analogous technique to check how consistent problems were across students (problem self-correlation). Specifically, we created two buckets by (1) randomly
splitting students who solved a given problem across the two buckets and (2) storing
in each bucket the average gaming across all students for that problem, obtained using
Eq. 3 above. We also found a high degree of association between the two buckets
(r = .89, p < 0.001). That is, if a problem was often gamed by students in bucket A,
then it was also often gamed by students in bucket B (Fig. 2 shows the scatter plots
for the two analyses). However, the correlation coefficient for the latter analysis was
lower than for the former, suggesting that students are more consistent than problems.
Specifically, if a student is a high gamer on half of the problems, then the student is also
likely to be a high gamer on the other half. In contrast, if a problem is a high-gaming
problem for half the students, then it is less likely to be a high-gaming problem for
the other half. We verified the difference between the two correlation coefficients was
reliable using Z scores (Lowry 2010); this was indeed the case (Z = 6.9, p < 0.05).

4.3 Gaming frequency distributions
Yet another way to investigate predictors of gaming is to examine histograms of gaming
frequency. That is, we can look at how many students are high frequency gamers versus
middle versus low frequency gamers. If differences among students are completely
unimportant, and all students tend to solve roughly the same set of problems, then
gaming frequency should be normally distributed (i.e., most of the gaming frequencies should cluster around the average). In fact, the student distribution is significantly
different from the normal (Shapiro-Wilks test of normality W = .89, p = 0.02), and

123

112

K. Muldner et al.

Fig. 3 Andes data on student (top) and problem (bottom) gaming distributions. Each bucket contains
students (or problems) with a 3% gaming range (e.g., bucket 6 has 3% <gaming <6%)

appears bimodal (see Fig. 3, top). As the graph shows, there is one group of students
who frequently game, and another group who seldom game. This again suggests that
differences between students play an important role in gaming frequency.
Likewise, if the characteristics of problems are completely unimportant, then a
histogram of the number of problems (y-axis) gamed at a certain range of frequencies (x-axis) should be normally distributed (Fig. 3, bottom). This is the case: the
Shapiro-Wilks test of normality showed that the problem distribution is not significantly different from normal (W = .92, p > 0.05). Thus, it appears once again
that characteristics of students are more important than characteristics of problems in
predicting gaming.

4.4 Parameter learning for predicting gaming
We also rely on Bayesian network parameter learning to investigate how student versus problem predicts gaming, using the Bayesian network shown in Fig. 4 (top). In
this network, all nodes are observable and binary, and have the following semantics:

123

An analysis of students’ gaming behaviors

113

Fig. 4 Predictive Bayesian network (top) and corresponding parameters obtained from the Andes log data
via the Netica counting algorithm (bottom). The parameters for P(gamed = false |. . .), not shown here, are
simply 1− P (gamed = true |. . .)

• ‘studentClass’: whether a student is a low or high gamer
• ‘problemClass’: whether a problem is a low or high gamed problem
• ‘gamed’: true if a tutor-student turn pair was gamed and false otherwise (see Table 1
for a listing of the turn pairs)
Thus, the network models how student and problem predict gaming. To obtain data for
setting the values of the student and problem class nodes, we classified: (i) each student
as a low or high gamer, based on a median split of average gaming obtained using
Eq. 2 presented above and (ii) each problem as a low or high gamed problem based on
a median split of average gaming obtained using Eq. 3. The data for setting the value
of the ‘gamed’ node was provided by the gaming detector described in Sect. 3.2.
Since all nodes in the network are observable, we used Netica (Netica Reference
Manual 2010), a Bayesian network toolkit, and specifically its counting algorithm to
learn the network parameters from the Andes data. The counting algorithm uses a traditional one-pass method to determine the probabilities, which essentially amounts to
counting the number of times a node takes on a certain value given each configuration
of the parents (for details, see Russell and Norvig 2009). The resulting parameters
are shown in Fig. 4 (bottom) and support the above findings that student is a stronger
predictor of gaming than problem. In particular, the probability of gaming is higher
when the ‘studentClass’ node is high and the ‘problemClass’ node is low, as compared
to when the ‘problemClass’ node is high and the ‘studentClass’ node is low. Another
way of looking at this is to consider the fact that when studentClass is low, indicating
a student is not a gamer, then the probability of gaming is below 20%, regardless of
whether a low or high gaming problem is solved by that student. In contrast, when
studentClass is high, indicating that a student is a gamer, the probability of gaming
is much higher, both when a low and a high gamed problem is solved. Conversely,
when problem class is low, then the probability of gaming more depends on the type
of student solving that problem (alternating between about 10% if the student is a low
gamer and 37% if the student is a high gamer); likewise, when the problemClass is
high. All together, this suggests that while problem does have an impact, it is less than
the type of student solving that problem.

123

114

K. Muldner et al.

Fig. 5 Naïve Bayes classifier (top) and corresponding parameters obtained from the Andes log data via the
Netica counting algorithm (bottom). The parameters for P([problem,student]Class = low |. . .), not shown
here, are simply 1− P ([problem, student]Class = high |. . .)

An alternative network structure to the one in Fig. 4 is a naïve Bayes classifier, such
as the one shown in Fig. 5. The disadvantage of this network, however, is that by definition its structure assumes that student class and problem class are independent given
evidence of gaming, which is not necessarily the case (in fact, the predictive network
in Fig. 4 suggests that the two variables are related). For the sake of completeness,
we used Netica to learn the parameters for this network. As Fig. 5 (bottom) shows,
the results confirm the above analysis, in that the probability of gaming is higher if
‘studentClass’ is high than when ‘problemClass’ is high (see, Fig. 5, bottom; in fact,
if we set the value of ‘studentClass’ = high and ‘problemClass’ = low, the probability of gaming is 0.48, higher than the p = 0.28 obtained when we set the value of
‘studentClass’ = low and ‘problemClass’ = high).
5 How do Gaming predictors in other tutors compare to those in Andes?
Past research has shown that both student and instructional aspects influence gaming, but to date there does not exist agreement as to which is the stronger predictor.
Work suggesting that instructional features drive gaming (Baker et al. 2009) relied on
data from the Algebra Cognitive Tutor (Koedinger et al. 1995; Carnegie Learning Inc.
2010). To see whether the particular data we used influenced the finding that student
was the strongest predictor of gaming, we applied the gaming detector and analyses
to the Cognitive Tutor data used in (Baker et al. 2009).
Before presenting the results, we provide a brief overview of the Cognitive Tutor.
This tutor provides problems for students to solve, which they do by typing and/or
drawing objects in the interface, much like in Andes. Also as in Andes, the Cognitive
Tutor allows students to ask for hints, which start out general and end in a bottom-out
hint that informs students of the step needed to generate the solution. Given this, the
general gaming detection framework presented in Sect. 3.2 is also appropriate for
gaming detection in the Cognitive Tutor data. Prior to applying it to the new data
set we refined the gaming thresholds to make them appropriate in the context of the
Cognitive tutor, using the method described in Sect. 3.2 to set the thresholds.

123

An analysis of students’ gaming behaviors

115

Once the gaming detector labeled the Cognitive Tutor data, we obtained information for PerGaming, problem and student using Eqs. 1, 2, 3 (i.e., as for the Andes
data set, see Sect. 4), and analyzed the relationship between these variables using the
methods presented in Sect. 4. The results, presented below, are based on data from 53
individual students 775 unique problems.
Linear Regression Analysis. A linear regression with Per Gamingsp (Eq. 1, Sect. 4)
as the dependent variable, and as the independent variables, student (Eq. 2, Sect. 4)
and problem (Eq. 3, Sect. 4) produced a significant model (F = 4588, p < 0.001)
that accounted for 42% of the variance (R 2 = .420). In this model, both student and
problem are correlated with the dependent variable, although student yields a higher
correlation than problem (student: standardized coefficient = .52, t = 76, p < 0.001;
problem: standardized coefficient = .35, t = 51, p < 0.001).
If we enter the independent variables separately to analyze the variance explained
by each, the student variable accounts for 30% of the variance, while the problem
variable accounts for 15% of the variance.
Self-correlation Analysis. We used the method in Sect. 4.2 to analyze whether students were consistently gaming on problems, i.e., by randomly assigning problems
a given student solved to two buckets and obtaining an overall problem average. We
found a high degree of association (r. = .973, p < 0.001). In contrast, when we
analyzed that problems were consistently gamed on by randomly assigning students
to two buckets and obtaining an overall average, the analysis yielded a lower degree
of association (r = .55, p < 0.001); the difference between the two correlation
coefficients is reliable (Z = 10.5, p < 0.05). Thus, if a student is a high gamer on
half of the problems, then the student is also likely to be a high gamer on the other
half. In contrast, if a problem is a high-gaming problem for half the students, then it
is less likely to be a high-gaming problem for the other half. This analysis confirms
the Andes findings that students are more consistent and thus better predictors than
problems when it comes to gaming.
Gaming frequency distributions. As we proposed in Sect. 4.3, distributions of gaming frequency across students (or problems) should be normally distributed if differences between students (or problems) do not impact gaming. The histograms for the
student and problem gaming distributions from the Cognitive Tutor data are shown in
Fig. 6.3 Based on a visual inspection, the student gaming distribution is not as cleanly
separated into low and high gamers as the Andes data. This distribution is not normally
distributed according to the Shapiro-Wilks test of normality (W = .64, p < 0.05).
However, if we remove the outlier (see bucket 38, Fig. 6, top), then the Shapiro-Wilks
test reports a normal distribution (W = .912, p > 0.05).
The problem gaming distribution appears normally distributed in the middle of the
graph, but not on the extreme ends. Specifically, there is a large number of problems
in the Cognitive Tutor that receive very little gaming (bucket 1, Fig. 6, bottom), and
there is a long tail in the upper end of the distribution, suggesting that a few Cognitive
3 The distributions are divided into smaller buckets in Fig. 6 than in Fig. 2, because the Cognitive Tutor
data had an overall lower gaming frequency, and so bigger buckets would have collapsed the data into
very few buckets. The previously reported Andes results hold if we subdivide the Andes buckets into the
Cognitive Tutor-sized buckets.

123

116

K. Muldner et al.

Fig. 6 Cognitive Tutor data on student (top) and problem (bottom) gaming distributions. Each bucket
contains students (or problems) with a 1% gaming range (e.g., bucket 1 has 0% <gaming <1%)

Fig. 7 Parameters for the Bayesian network shown in Fig. 4 obtained with the Netica counting algorithm
for the Cognitive Tutor data. The parameters for P(gamed = false | . . .), not shown here, are simply 1− P
(gamed = true | . . .)

Tutor problems are highly gamed. These two factors likely contribute to the result
that this distribution is not normally distributed according to the Shapiro-Wilks test
(W = 0.71, p < 0.05), which is not consistent with our hypothesis.
Parameter learning for predicting gaming. We used the Cognitive Tutor data to
learn the parameters for the Bayesian network shown in Fig. 4—the results are shown
in Fig. 7 (see Sect. 4.4 for a complete description of this technique). As was the case
with the Andes log data, in the Cognitive Tutor the probability of gaming is higher

123

An analysis of students’ gaming behaviors

117

Table 2 Gaming opportunities for each Tutor–student turn pair

(a) Student: Hint Request
fast

slow

(1) Tutor: B-O Hint

S: 0.02 (.3)

(2) Tutor: H-L Hint

S:18.4 (58.6)

5.8 (18.5)

3.0 (12.4 )

2.5 (10.3 )

P: 5.3 (14.5)

3.6 (10.0)

(3) Tutor: Incorrect
(4) Tutor: Correct

.2 (2.1)

(b) Student: Entry
fast

slow

C: 1.8 (23.6)
.7 (2.3)
G: 5.4 (22.1)
(RED)

5.7 (73.9)
6.4 (20.6)

4.8 (20)
(GREEN)

14.1 (38.9)

8.7 (35.8)
13.3 (36.6)

Shown in each cell: (1) the mean % of a student responses overall (i.e., the 17 numbers in the table sum to
100%), (2) in parentheses: mean % of a student response for that row’s tutor action
Fast: student action <gaming threshold; slow: student action >gaming threshold; B-O: Bottom-out, H-L:
High-level

when student is true/problem is false than vice versa, again suggesting that student is
a stronger predictor of gaming than problem.
In summary, the majority of the analyses with the Cognitive Tutor data confirms the
Andes-related findings in Sect. 4 that student is a stronger predictor of gaming than
problem. The ‘gaming distribution’ analysis was the only one that did not support this
conjecture. This latter analysis suggested that compared to the Andes problems, some
problems in the Cognitive Tutor appeared to be less consistently gamed; likewise that
differences between students were not as pronounced as in the Andes data set.
6 Gaming profiles: how much and where are student gaming?
The analyses above established that frequency of gaming depends more on who rather
than what is being gamed, but do not provide any insight into how gaming is occurring, nor its consequences. Thus, for the remainder of the paper, we will focus on
obtaining details on students’ gaming and related behaviors and how these impact
outcomes of interest, including problem solving and learning. For these analyses, we
will use the Andes data (while it would be interesting to compare whether differences
exist between the Andes and Cognitive Tutors, we leave doing so for future work).
We begin with a descriptive analysis of the Andes data, which will set the stage for
guiding subsequent analysis.
On average, 22.5% of tutor-student turn pairs were gamed. We first analyzed where
the gaming was occurring—the results are shown in Table 2, which extends Table 1
with numeric data (as in Table 1, the italicized cells indicate gamed turn pairs). For
this analysis, because we were interested in general in how students gamed, we collapsed across problems (i.e., obtained for each type of turn pair the total number of
corresponding actions across all problems for a given student); Table 2 reports the
average percentage for each type of turn pair.
As shown in Table 2, students most frequently took advantage of the opportunity
to game when the tutor presented a high-level hint: on average, 18.4% of all student actions corresponded to gaming on these hints; when given such a hint, students
gamed on it 58.6% of the time. In contrast, bottom-out hint turn-pairs were rarely

123

118

K. Muldner et al.

Table 3 Gaming opportunities for each tutor–student turn pair for low and high gamers

(a) Student: Hint Request

(1) Tutor: B-O Hint

(2) Tutor: H-L Hint

(3) Tutor: Incorrect

(4) Tutor: Correct

(b) Student: Entry

fast

slow

fast

slow
3.2

LG

S: 0.02

0.07

C: 0.34

HG

S: .03

.25

C: 3.3

8.3

LG

S: 7.1

6.4

0.53

7.3

HG

S: 29.7

5.1

LG

3.2

3.2

G: 5.5 (RED)

5.3 (GREEN)

HG

2.8

1.8

G: 5.2 (RED)

4.2 (GREEN)

LG

P: 1.5

3.4

20.3

21.1

HG

P: 9.1

3.9

8.1

5.5

0.92

5.7
11.4
6.0

Shown in each cell is the mean % of a student response given a tutor action over all 17 possible combinations
for the low gamers (LG upper half cell) and high gamers (H G lower half cell)
Fast: student action <gaming threshold; slow: student action >gaming threshold; B-O: Bottom-out, H-L:
High-level

gamed. After skipping of high-level hints, guessing and lack of planning (see ‘G’ and
‘P’ cells, Table 2) were the next most frequently gamed turn-pairs (5.4% and 5.2%,
respectively). Gaming by copying bottom-out hints was rare (see ‘C’ cells, Table 2).
In order to compare the gaming patterns of students who frequently gamed with
those who infrequently gamed, we divided students into low gamers and high gamers
based on a median split. The gaming profiles for each group are shown in Table 3. As
the table highlights, high-level hints are a highly-gamed feature by both low and high
gamers. Furthermore, low gamers are more likely to game by guessing than high gamers; this is the only gaming turn pair that low gamers abuse more than high-gamers.
Another way to investigate differences between low and high gamers is to subdivide
the data slightly differently: instead of considering at all the possible tutor-student
turn pairs, as we do in Tables 2 and 3, we can instead analyze how much students
game on a certain turn-pair in proportion only to the total five gaming opportunities
(e.g., skipping high-level hints/total gamed events). The results of doing so are shown
graphically in Fig. 8. On average, low gamers were significantly more likely than high
gamers to game by guessing (46% vs. 13.2%; F(1, 283) = 126, p < .01). On the
other hand, in contrast to low gamers, high gamers had a significantly higher proportion of skipped high-level hints (61.6% vs. 43.4%; F(1, 283) = 64, p < .01), lack of
planning (18.5% vs. 8.9%; F(1, 283) = 159, p < .01) and bottom-out hint copying
(6.5% vs. 1.7%; F(1, 283) = 215, p < .01).

7 Exploring the utility of hints for supporting problem solving and learning
Over all students’ gaming opportunities, as well as proportion of gaming for high
gamers, high-level hints elicited the most gaming. Some investigators propose that
gaming is in part due to the fact that reading hints does not influence solution entry
success, i.e., that hints are not helpful (Baker et al. 2009). We wanted to see if this was

123

An analysis of students’ gaming behaviors

119

Fig. 8 Proportion of gamed-turn pairs for low and high gamers

the case for the Andes data—if we found evidence that hints were not helpful, then this
would provide insight into why the high-level hints were gamed as much as they were.
7.1 Hint viewing
The most basic analysis we started with was to calculate the time students spent on
hints. To do so, we obtained the latency between the provision of a hint and the next
student action. On average, students spent 9.2 s vs. 5.7 s. on bottom-out versus highlevel hints. High gamers spent significantly less time on hints than low gamers, both
on bottom-out hints (7.5 s vs. 10.9 s; F(1, 277) = 71, p < .01) and high-level hints
(3.2 s vs. 8.1 s; F(1, 286) = 246, p < .01). Thus, both low and high gamers devoted
more time to bottom-out than high-level hints. High gamers’ average viewing time
for high-level hints was quite low, suggesting that in contrast to low gamers, these
students did not pay much attention to high-level hints.
7.2 Hint utility: basic analyses
In this section, we report Andes hints’ impact on short-term performance, i.e., does
the hint scaffold the student to generate the entry after seeing a hint (Muldner et al.
2010). In the subsequent section, we describe hints’ impact on long-term outcomes,
namely learning.
If for a moment we don’t consider entry correctness, high gamers tried to generate
an entry only about 16% of the time after receiving a high-level hint, asking for another
hint the other 84% of the time. Low gamers, on the other hand, responded to a highlevel hint with an entry about 36% of the time. This is in contrast to bottom-out hints,
when both low and high gamers responded to the hint with an entry about 97% of the
time. The latter is probably due to the fact that asking for a hint after the bottom-out
hint merely repeats the hint, so students only do so by accident.
For each student, we obtained the percentage of time s/he was successful at (eventually) generating the correct entry after receiving each type of hint (bottom out, high
level). Note that (1) students may require several attempts in order to generate a correct
entry and (2) if hint B is requested after hint A but prior to generating a correct entry,
then hint A is not counted as “successful” for helping the student. We acknowledge
that in some situations, hint information might be additive, i.e., seeing hint A might

123

120

K. Muldner et al.

add information to hint B. If this is the case, then high-level hints are at a disadvantage
in this analysis.
If students did generate a correct entry (or entries) after seeing a bottom-out hint,
on average, they were successful in 90% of instances (i.e., the entry was correct, albeit
possibly after several tries, as we show below). There was little difference between
low and high gamers for this analysis (89% vs. 92%, respectively, NS difference).
After high-level hints, students (eventually) generated a correct entry 73% of the time
(again, this may have required several tires, as we analyze below). There was also little
difference between low and high gamers (72% vs. 73%, respectively, NS difference).
This suggests that high-level hints scaffolded students to generate the solution entry
in about three out of four instances, albeit doing so may have taken a number of tries,
as we now show.
After bottom-out hints, students required 1.1 attempts on average (1.23 for low
gamers versus 1.19 for high gamers, NS), and took 29 sec. to answer correctly (34 s for
low gamers versus 23 s for high gamers, F(284, 1) = 4, p = .052). This makes sense,
assuming that most students copied entries from the bottom out hints, but the entries
were complex enough that it took about a half-minute to do the copying, and occasionally generated typos and other minor errors. After high-level hints, on average students
required 1.83 attempts to generate the correct entry; here low gamers needed significantly fewer attempts than high gamers (1.66 vs. 2.01, F(1, 284) = 17, p < 0.001),
suggesting that perhaps the low gamers were more diligent about applying high-level
hints. This conjecture is supported by the fact that low-gamers spent significantly
longer than high-gamers to generate a correct entry after seeing a high-level hint (37 s
vs. 28 s; F(1, 286) = 9, p < 0.01).

7.3 Hint utility: machine learning analyses
Above, we analyzed the short-term impact of hints with a relatively simple technique,
i.e., by counting the number of times a student was able to (eventually) generate a
correct response after seeing a hint. A more complex alternative is to rely on Bayesian
network parameter learning to explore the impact of help on both short-term performance and long-term learning. This was originally implemented in (Beck et al. 2008),
where it was applied to data from the Reading Tutor, an ITS designed to help children
learn to read (Beck et al. 2004). Here, we apply this method to the Andes data. In
this section, we explore the impact of hints in general for all students, while in the
subsequent section we tease apart hints’ influence on low versus high gamers.
For parameter learning, we relied on the BNT-SM Bayesian network parameter
learning toolkit (Chang et al. 2006), which was also used in (Beck et al. 2008) and
is based on the established MatLab Bayesian toolkit BNT (Murphy 2004). Learning
in BNT-SM is accomplished using Expectation Maximization (Dempster et al. 1977),
since as we shall see below, some of the network variables are latent, i.e., not observable. The toolkit is especially useful for parameter learning of dynamic Bayesian
networks, DBNs (Dean and Kanazawa 1989). DBNs model the evolution of states of
interest over time, by including so called time slices to represent particular points in
time (e.g., a basic dynamic Bayesian network for modeling the evolution of knowledge

123

An analysis of students’ gaming behaviors

121

Fig. 9 Basic knowledge-tracking DBN model (top) & corresponding parameters mined from Andes data
(bottom)

is shown in Fig. 9, top). BNT-SM implements parameter tying for dynamic Bayesian
networks (Murphy 2002), meaning that expected statistics are pooled for all nodes that
share the same parameters (e.g., ‘knowledge’ nodes in Fig. 9). Consequently, when
machine learning is applied, the same parameter for a given node class is obtained,
which is the desirable outcome (although not all Bayesian toolkits yet implement
parameter tying, e.g., Netica does not).
7.3.1 The basic knowledge-tracing model
We begin by presenting a simple knowledge-tracing model, one that does not take
into account the impact of hints at all, shown in Fig. 9 (referred to as the Basic model
below). This model is advocated as one appropriate for inferring student learning from
problem-solving actions (Corbett and Anderson 1995; Reye 2004); Beck et al. (2008)
use it as the baseline model to compare against a “Help” model, something we also
do here. The Basic model, a dynamic Bayesian network, encodes two variables: (1)
‘knowledge’, i.e., the probability that a student knows a particular domain principle,
and (2) ‘solution entry’, i.e., the probability that a student will generate a correct
problem-solving entry as a result of applying the corresponding domain principle. In
this model, all nodes have binary values, namely correct/incorrect for solution entries
and mastered/unmastered for knowledge. Note that ‘knowledge’ is not observable, but
‘solution entry’ is observable (i.e., its value can be set given evidence corresponding
to a student’s correct or incorrect solution entry).
The Basic model captures four key variables relevant to student modeling of
problem solving and learning in instructional contexts (Conati et al. 2002; Reye 2004;
Beck et al. 2008):
– guess: the probability the student will generate a correct entry (captured by the
‘solution entry’ node in the network in Fig. 9) by guessing when the corresponding knowledge is in the unmastered state;

123

122

K. Muldner et al.

– slip: the probability the student will not generate a correct entry when the corresponding knowledge is in the mastered state;
– learn: the probability the student will learn the domain principle, which was unmastered in the past and which is needed to generate the corresponding entry;
– forget: the probability the student will forget a domain principle that was mastered
in the past, needed to generate the corresponding entry.
Figure 9, bottom, operationalizes these four variables in terms of Bayesian network
parameters (probabilities shown are learned from the Andes data, as we describe
below). BNT-SM requires that initial conditional parameter table (CPT) values are
provided: to do so, we rely on the ones specified in (Beck et al. 2008), taking into
account parameters advocated for Andes-specific Bayesian networks (Conati et al.
2002). These, however, are only starting values, as they are subsequently refined by
the learning algorithm.
When we used BNT-SM to learn the four parameters (guess, slip, learn, forget) for
the Andes data in the Basic model shown in Fig. 9, top, we obtained the values shown
in Fig. 9, bottom. Note that BNT-SM learns parameters for individual skills—in (Beck
et al. 2008), a single value was reported, suggesting that an average over the whole
set of parameters learned was obtained; that is what we report here. The parameters
learned from the data for the Basic model suggest that in Andes, there is a moderate
probability of guessing and learning (interestingly, the probability of learning is quite
a bit higher than that reported in the Reading Tutor network, i.e., Andes learn = 0.25
versus Reading Tutor learn = .08).

7.3.2 The help model
The network in Fig. 9 does not represent the influence of hints on problem solving
or learning. Thus, as the next step we used BNT-SM to learn parameters for a DBN
that does account for hints, referred to as the Help model below. The Help model was
proposed in (Beck et al. 2008), and is shown in Fig. 10, top. This model extends the
one in Fig. 9 with an additional observable node, ‘hint(s)’. The Help model in (Beck
et al. 2008) used a binary ‘hint(s)’ node, with values true/false to indicate the presence/absence of tutor help, respectively. While we are also interested in investigating
the impact of various kinds of hints, something not captured with a binary-valued
node, we begin with this binary ‘hint(s)’ node network.
The ‘hint(s)’ node has the following semantics: If a student asked for one or more
hints related to (described below) a given entry, then hint(s)=true, and hint(s)=false
otherwise. We say a hint could be related to an entry because when Andes displays a
hint, it is shown and remains visible in a side panel, so a student could, for instance,
ask for a hint, generate an incorrect entry, look back at the hint, and generate a correct
entry. Thus, once a student asks for a hint, the value of ‘hint(s)’ node will be true until
that entry concludes (i.e., student generates a correct entry for the current solution step
or gives up and moves on). The ‘hint(s)’ node includes links to both the ‘knowledge’
and ‘solution entry’ nodes in the DBN, which model several hint-related influences,
shown in Fig. 10, bottom; key ones include:

123

An analysis of students’ gaming behaviors

123

Fig. 10 Help DBN model (top) and corresponding parameters mined from Andes data (bottom)

– scaffolding, the short-term influence of a hint(s): the probability of a correct entry
given that a hint(s) was requested and the relevant knowledge was unmastered;
– learnWithHint(s): learning of a domain principle via a requested hint(s);
– learnNoHint(s): learning of a domain principle without a hint request.
Beck et al. (Beck et al. 2008) used BNT-SM to learn the parameters for the Help network using data from the Reading Tutor, and reported that tutor help was useful both
in terms of scaffolding problem solving and learning—albeit its influence on learning
was very minor: learning with a hint obtained a learnHint parameter of 0.088, while
the learnNoHint parameter was only very slightly lower at 0.083.
To explore the impact of hints in Andes, we used BNT-SM to learn the Help model
parameters with the Andes log data; the results are shown in Fig. 10, bottom. We
found, as did (Beck et al. 2008), that when students did not posses the necessary
domain knowledge, a hint(s) increased the probability of a correct entry, i.e., scaffolded problem solving, as compared to episodes where students did not ask for a
hint(s) (and thus probably guessed, see scaffold versus guess, p = .38 vs. p = .31,
Fig. 10, bottom), confirming the analysis in Sect. 7.2. In contrast to the results in (Beck
et al. 2008), we found that overall, students learned slightly worse when they received
hint(s), as compared to whey they did not receive hints (i.e., p = .21 vs. p = 23,
learnWithHint versus learnNoHint, Fig. 10, bottom). We will return to why this may
have been the case shortly.
To explore the utility of different types of hints we refined the domain of the ‘hint(s)’
node to include three values: high-level (HL), bottom-out (BO), none. To set the value
of this node, we followed the approach above by looking for hints related to an entry,

123

124

K. Muldner et al.

Fig. 11 Parameters for the Help DBN model shown in Fig. 10 but revised with a three-valued hint node:
high-level (HL, high-level hint obtained), bottom-out (BO, bottom-out hint obtained), none (step generated
without requesting a hint)

but set the value of ‘hint(s)’ to: (1) bottom-out if a student requested a bottom-out hint
related to the current entry, (2) high-level if a student requested a high-level hint, and
no bottom-out hint(s), related to the current entry, and (3) none otherwise. When we
applied BNT-SM to this revised network, we obtained the parameters shown in Fig. 11.
The results highlight that bottom-out hints provide more short-term scaffolding for
entry generation than high-level hints, something we also found with the basic analysis
in Sect. 7.2. In fact, the guess parameter is slightly higher than the scaffold-HL parameter (see Fig. 11, p = .25 vs. p = .21), suggesting that high-level hints are not very helpful for immediate correct entry generation. This doesn’t contradict the finding obtained
via the basic analysis in Sect. 7.2, because in the latter, we analyzed whether a student would eventually generate a correct entry after seeing a high-level hint (possibly
requiring several attempts), while here, the network assesses hints’ immediate impact.
As was the case for the scaffold parameter, the learn parameter is also higher for
bottom-out hint(s), as compared to high-level hint(s). However, when the network
includes a three-valued ‘hint(s)’ node, learning without help obtains the lowest score,
i.e., learning with hint(s) is better than without (see Fig. 11, bottom). Thus, one possibility for why the original Help network with a two-valued ‘hint(s)’ node did not
suggest hints fostered learning is that it did not consider the impact of different types
of hints. There are, however, other possibilities that we explore in the next section.
8 Impact of gaming on hint utility and learning
The initial analysis on the impact of help did not show that hint(s) fostered learning.
While the subsequent analysis that teased apart the impact of high-level and bottom-out
hints did provide indications of learning with both types of hints, learning was not as
high as we hoped, particularly for high-level hints. One possibility for this finding is
related to gaming. Past research suggests that gaming in general can be harmful to
learning (Baker et al. 2004b; Walonoski and Heffernan 2006a), as well as help abuse
in particular (Aleven et al. 2006). Furthermore, help abuse has been associated with
poor learning gains in cognitive science research. For instance, VanLehn (1998) found

123

An analysis of students’ gaming behaviors

125

that when students copy from examples instead of trying to generate the solution on
their own, a form of help abuse, their learning is diminished. Altogether these findings
suggest that at least some of the Andes students may be learning less precisely because
they obtain hints without trying to generate the solution on their own and/or trying to
learn from the hints provided.
A way to test this hypothesis is to re-run the parameter learning with the original
Help network, but with a refined ‘hint(s)’ node that includes information on gaming, as follows: ‘hint(s)’=gamed, ‘hint(s)’=not-gamed and ‘hint(s)’=none. That is, the
‘hint(s)’=true category would be replaced by ‘hint(s)’=gamed if one or more hints
related to an entry were gamed (e.g., skipped, see “fast” in Table 2; latency below
threshold), and ‘hint(s)’=not-gamed if all hints were not gamed (“slow” in Table 2;
latency above threshold). Unfortunately, when we applied this method, there were
very few values for the ‘hint(s)’=not-gamed parameter: when students asked for a hint
(or series of hints), they tended to game on at least one of these. Note that the data in
Table 2 shows that overall, 18.1% of all student hint requests were slow (not gamed)
after receiving a high-level hint—this does not contradict the previous statement on
lack of data, because in Table 2, only single tutor-student pairs were considered (e.g.,
tutor provides a hint, student asks for another hint). In contrast, for the Help model,
we consider all the hints related to an entry, and also consider a hint as requested if it is
related to a entry (but not necessarily requested directly prior to generating the entry,
since as we mention above, hints remain visible once requested). While students don’t
always game on a single turn pair related to hints, if they ask for hints then they tend
to game on at least one of these. An alternative is to consider only the last hint request
prior to a solution entry attempt; unfortunately, this also results in sparse data, since
as shown in Table 2, students rarely copy in a shallow manner from hints (and this is
the only type of gaming possible from the hint-provided/solution-entry turn pair).
An alternative is to investigate the utility of hints for two groups of students: the
low and the high gamers. As the analysis in Sect. 7.2 suggests, low gamers are more
diligent than high gamers in terms of using hints, both in terms of their willingness
to use high-level hints and time spent on all types of hints. To see if the BNT-SM
parameter learning also showed differences between low and high gamers and hint
impact, we re-ran the learning algorithms separately for the low and the high gamers,
for each of the Help networks (original 2-valued ‘hint(s)’ node and subsequent 3valued ‘hint(s)’ node that differentiated between the two types of hints). The results,
shown in Figs. 12 and 13, suggest that low gamers benefit from hints in terms of
learning, while high-gamers’ learning is hurt by at least some of the hints (for the
sake of brevity, the forget parameter is not shown in the figures). Specifically, for the
low gamers, the mined learnWithHint(s) parameter is higher than learnNoHint(s), i.e.,
p = .26 vs. p = .22 (see Fig. 12, bolded items, top). The opposite is true for the high
gamers (learnWithHint(s) = .17 versus learnNoHint(s) = .19, see Fig. 12, bolded items,
bottom). If the impact the two types of hints (bottom out, high-level) is considered,
then the learn parameter for both types of hints is higher for low gamers than high
gamers (see Fig. 13, bolded items). In fact, according to this network’s parameters,
low gamers’ learning is not affected by high-level hints.
The above analysis suggests that gaming can be harmful to learning, since it shows
that high gaming students who abused hints more also learned less from them. It

123

126

K. Muldner et al.

Fig. 12 Parameters for the DBN Help network shown in Fig. 10 with a 2-valued ‘hint(s)’ node for low and
high gamers

Fig. 13 Parameters for the DBN Help network shown in Fig. 10 revised with a 3-valued ‘hint(s)’ node for
low and high gamers

does not, however, directly tell us about the impact of gaming on learning. To explore
this, we applied BNT-SM parameter learning to the network shown in Fig. 14. This
network is designed to capture the impact of gaming on both problem solving and
learning, by including a ‘gamed’ node that represents gaming behaviors. In this network, all nodes have binary values, and all nodes except knowledge are observable.
The ‘gamed’ node is set to true if the gaming detector indicates that gaming occurred

123

An analysis of students’ gaming behaviors

127

Fig. 14 Gaming DBN model (top) and corresponding parameters mined from the Andes data (bottom)

for the corresponding solution entry, either (1) because students abused hints prior to
generating the entry (here, only hints between two successive entries are considered,
since gamed hints corresponding to previous entries are accounted for by the corresponding ‘gamed’ node in a previous DBN time slice), or (2) because they generated
the entry by guessing or with a lack of planning. Note that in contrast to the earlier
attempt to learn parameters for gamed help requests that we did not have sufficient
data to do, here there is sufficient data, because in the former, we only considered
gamed/ungamed help requests, while in the current Game model, all types of actions
are included (solution entries, help requests, copying etc).
As the parameters obtained from BNT-SM show (Fig. 14, bottom), the probability of learning is indeed higher in the absence of gaming (the mined learnNoGaming
parameter is higher than learnWithGaming, p = .33 vs. p = .19, see Fig. 14, bottom).
This network also suggests that the probability of making a slip is higher when gaming
occurs than without gaming (the slipWithGaming parameter is higher than slipNoGaming, p = .26 vs. p = .15, see Fig. 14, bottom). That is, when students has gamed, for
instance by speeding through the hints, their next entry is more likely to be incorrect
than if they had processed the hints slowly.

9 Discussion and future work
A prerequisite for reducing gaming in ITSs is understanding when and why it occurs.
Past research has shown that both student and instructional aspects influence gaming,

123

128

K. Muldner et al.

but to date there does not exist agreement as to which is the stronger predictor, making
it difficult to understand where to focus research efforts. Some researchers argue that
it is the latter, i.e., instructional aspects, that drive gaming (Baker et al. 2009). This
argument stems from data mining related to instructional differences between ITS
lessons that resulted in a model explaining 56% of the gaming variance (we refer to
the model as the Baker model below). Since 56% was considerably higher than other
models at that time considering student characteristics, the conclusion was made that
instructional features are the better gaming predictors.
To analyze predictors of gaming, we took a slightly different approach than the one
in (Baker et al. 2009). Rather than extracting fine-grained features, such as number of
hints in a lesson, we obtained an overall gaming statistic for a given student or problem. While this means that we did not have fine-grained information on exactly which
aspects of student or problem are driving gaming, our approach does very cleanly
separate problem versus student (more on this below), as well as is lightweight to
implement. One of the EDM techniques we used to analyze gaming predictors corresponded to a linear regression model that took into account the average percentage
of gaming by a student over all the problems s/he solved, and the average percentage
of gaming on a problem over all the students. We initially applied this model to data
from the Andes tutor, and found that it explained 61% of the variance, and that in this
model, student was a stronger predictor of gaming than problem. We re-analyzed the
data using a range of techniques: each analysis confirmed that student was a stronger
predictor of gaming.
Given the above-stated prior findings on gaming predictors, to shed more light on
the issue, we applied the gaming detector to the same data set as that used in (Baker
et al. 2009). The full model explained 43% of the variance, somewhat less than the
56% explained by the Baker model. In this model, student was again a better predictor
than problem, confirmed by all but one analyses. One could argue that in the Baker
model only instructional features (i.e., lesson) are considered, and so for a fair comparison, we should use a subset of the model, i.e., one that includes only problem (and
excludes student). However, a closer analysis of the Cognitive Tutor features used
in the Baker model, which are characterized as “lesson features”, suggests that these
features may sometimes blur the instructional/student boundary. For instance, the category ‘Difficulty, Complexity of Material, and Time Consumingness’ includes aspects
such as ‘student already knows the skill’ and ‘average probability the student will
learn the skill at each application’. The category of ‘Help Features’ includes ‘average
amount that reading hints improves future performance’—this is in fact one of the
features included in the final full model. These features, however, are not only lesson
specific, since they clearly take into account student characteristics. For instance, the
amount that reading hints improves performance should be influenced by a students’
willingness and/or ability to learn from the hint. Thus, it seems that even in the Baker
model, some aspects of student were included in the gaming predictor analysis.
As far as the cause of the difference between our findings and ones in (Baker et al.
2009), there are a number of possibilities. First and foremost, the argument in (Baker
et al. 2009) that instructional features better predict gaming was based on the fact that
the corresponding model predicted more of the variance in the data than prior work
with models incorporating only student features. However, as we suggest above, even

123

An analysis of students’ gaming behaviors

129

the Baker model may have relied on some student features. This makes it more difficult to compare the output of the two models, since ours is the only one that explicitly
divides student and problem features. Second, it is possible that the labeling of the data
influenced the findings. While we relied on a computational gaming detector, in (Baker
et al. 2009) a human observer coded the data, and only coded a representative sample.
There are trade offs with either approach: while a computational detector might miss
some nuances that a human could pick up, the former is also more consistent than a
human who is tasked with the rather arduous job of labeling thousands of episodes.
In general, our model does quite well in explaining the variance in the Andes data,
surpassing existing approaches. The amount of variance explained by the model does
decrease somewhat when applied to the Cognitive Tutor data. There are a number of
possible reasons for this. First, the Andes ITS might have less instructional variability than the Cognitive Tutor, as suggested by the ‘gaming distribution analysis’, and
the gaming detector may need to take into account more fine-grained features than
problem to capture that variability. Second, we found that in the Andes model student
was a very strong predictor of gaming, more so than in the Cognitive Tutor model;
this helped to increase the Andes model’s ability to explain the gaming variance. The
two sets of data came from very different contexts and populations: the Andes data
corresponds to college students working at home while the Cognitive Tutor data came
from high school students working in classrooms. When we recently did a preliminary
analysis on a set of high school honors students using Andes mostly in their classroom, we found that their gaming levels were much lower (in the 5% range) than
those of the college students reported here; thus it is possible that gaming behaviors
differ between these two populations and contexts, and in particular are more predictable in older populations/non-classroom settings where students are not observed by
researchers/teachers, something that warrants further analysis and validation.
In addition to exploring predictors of gaming, we also analyzed differences between
students in terms of gaming behaviors. We found that when we looked at gaming opportunities over the tutor-student turn pairs, students most often seized the opportunity to
game after the tutor presented them with a high-level hint. However, when we analyzed
the proportion of each type of gaming over the total gaming events for each class of
gamers, in contrast to high-gamers (who primarily skipped hints), the low gamers had
a slightly higher incidence of guessing on entries. One possible explanation for this
difference, supported by literature on individual differences in help seeking behaviors
(Nelson-Le Gall 1985), is that the low gamers preferred to obtain the solution on their
own, without the tutor’s help. Another possibility relates to Andes’ scoring system.
Recall that students were penalized for asking for a bottom out hint but were not
penalized for guessing, and so perhaps the low gamers were simply more concerned
about their Andes score than high gamers. The analysis also showed, however, that
low gamers spent more time with hints and took longer to generate a solution entry
after seeing a hint. Since no points were awarded for taking time, this suggests that
obtaining a higher score was not the only incentive for the high-gamers, indicating that
perhaps these students were motivated and/or diligent in the problem-solving process.
Prior research suggests that poor hint usability is associated with gaming (Baker
et al. 2009). To see if this was the case, we investigated how students used hints in the
Andes data; doing so was particularly pertinent since certain hints were a highly-gamed

123

130

K. Muldner et al.

feature of the Andes tutor. The basic analysis consisted of obtaining statistics on students’ hint usage and utility. This analysis showed that when Andes presented a highlevel hint, high gamers were quite unlikely to even try generating a corresponding
solution entry, as compared to low gamers. If students did try to generate a solution
entry, both low and high gamers were moderately successful when given high-level
hints. This conclusion was based on determining if students could (eventually) generate a correct entry after being presented with a hint, prior to seeing another hint. An
alternative possibility, however, is that students gave up using the high-level hint and
generated the solution entry on their own.
While the basic analysis provided some indication for the utility of hints for scaffolding short term performance, it did not provide information on whether hints fostered
long-term shifts, i.e., learning. To investigate this aspect, we applied the data mining
techniques advocated in (Beck et al. 2008), by learning Bayesian network parameters
related to the utility of hints.
Bayesian networks have been long been used in the user modeling community for
representing and inferring various user states of interest, such as knowledge (Mayo
and Mitrovic 2001; Conati et al. 2002), affect (Conati and Maclaren 2009) and metacognitive tendencies (Bunt et al. 2004; Muldner and Conati 2005). Another application
of Bayesian networks, however, pertains to data mining. Parameter mining has been
used in the data mining community outside of educational applications, for instance
to analyze tuberculosis epidemiology (Getoor et al. 2004) and/or understand gene
variation (Rodin et al. 2005). As discussed in (Heckerman 1997), there are a number
of advantages of the graphical model afforded by a Bayesian network for traditional
data mining, including: (1) the ability to gracefully handle missing data points, (2)
during exploratory analysis, support for insight into a particular domain, including
causal relationships in that domain, (3) affordance for the integration of prior knowledge about a domain into the learning process and (4) Bayesian statistical methods
afford a principled approach to avoid over fitting the data. Although EDM has some
unique needs as compared to traditional mining (Baker 2010), these advantages clearly
also apply to educational domains. Bayesian networks offer other advantages as well.
First, they can be easily extended to provide support for decision tasks, enabling
them to be used for risk analysis. While traditionally this approach has not been used
in educational applications, one can imagine doing so could be highly informative;
for instance, “risk” could correspond to analyzing when a student looses motivation
and therefore disengages from the learning task. Second, Bayesian networks afford
a clear semantic interpretation of the model parameters. This is especially beneficial
for EDM applications since it means that potentially novel relationships encoded in
the network are directly visible. Thus, for data mining tasks in general, and EDM
tasks in particular, Bayesian networks are well suited in helping to identify and/or
model relationships among a large variety of variables. One disadvantage of Bayesian
learning is the computational expense, although this is becoming less of an issue with
the increasingly-available processor resources.
Our results using Bayesian parameter learning confirmed the basic analysis that
hints scaffolded short-term problem solving, both in the original DBN Help model
that only considered whether a hint was requested or not, and in a refined version that
took into account the type of hint (high-level versus bottom-out). However, the former

123

An analysis of students’ gaming behaviors

131

analysis also showed that students learned slightly better in the absence of hint(s)
as compared to when given hint(s), while in prior work students did very slightly
better when given help (Beck et al. 2008). When we dug deeper into this issue, we
found that learning from hints was influenced by several factors. First, the type of
hint had an impact on learning: the network parameters indicated that students learned
the most from bottom-out hints, as opposed to high-level hints. Second, gaming also
had an impact. In particular, low gamers appeared to benefit from both high-level and
bottom-out hints, while the high gamers only (slightly) learned from bottom-out hints.
This discussion highlights how data mining can guide subsequent research by pointing to the need to more closely investigate (1) the utility of high-level hints and (2) how
student characteristics interact to influence learning and problem-solving outcomes
with various kinds of hints. One way this could be achieved is through talk-aloud
protocol, where students are asked to verbally express their thoughts as they interact
with an ITS. While this technique is established as a means of obtaining rich data on
users’ cognitive processes (e.g., Chi et al. 2008), surprisingly it is not often applied
in the context of ITS evaluation (although exceptions exist, e.g., D’Mello et al. 2006;
Muldner and Conati 2010). We should point out that there already is substantial work
on the utility of help, both in the cognitive science and the ITS communities (e.g., Phye
and Sanders 1994; Chi et al. 2008; Hays et al. 2009; Razzaq and Heffernan 2009).
However, there is a lack of agreement across studies as to which type of feedback is
best, as is pointed out by various meta-analyses (e.g., Shute 2008). Some researchers
suggest that bottom-out hints can be useful for learning (Shih et al. 2008), but do
not compare the utility of various types of hints. Work directly comparing various
levels of feedback found that specific feedback, i.e., providing the answer, is better
than general high-level advice (Phye and Sanders 1994). In contrast, however, there
is also evidence that high-level scaffolding, which is at least somewhat comparable
to general hints, fosters learning better than simply providing didactic explanations
(e.g., the answer) (Chi et al. 2008; Chi 2010). Furthermore, student characteristics,
such as learning orientation, may interact with the utility of various types of help.
For instance, Davis et al. (Davis et al. 2005) found that students with low and high
learning orientation perform better with more specific help, as compared to general
help. We also found that low and high gamers, who could arguably be labeled as high
and low achievers, respectively, had better problem-solving performance and slightly
better learning with bottom-out, specific hints, but that only low gamers benefited in
terms of learning from high-level hints.
In addition to using it for analyzing hint impact, we also relied on Bayesian network parameter learning to analyze impact of gaming on various outcomes. Our results
confirm other findings that gaming in general can be harmful to learning (Baker et al.
2004b), but to the best of our knowledge, ours is the first work to show this in the
absence of pre/post test data. The analysis also highlights another reason why gaming
is not an effective strategy: in the Gaming model, the probability of making a slip
when gaming is present is higher than in the absence of gaming. One way to apply this
particular result is through the design of tutorial interventions. For instance, students
are willing to shift to strategies that foster learning if they understand that it is more
cost effective to do so, for instance in terms of time (Chi and VanLehn 2007b,a). Thus,
perhaps we can convince students, for instance via visualizations extending existing

123

132

K. Muldner et al.

ones (Walonoski and Heffernan 2006b), that gaming is not effective strategy because
it results in more errors (slips).
As the next steps, we plan to follow up on some of the avenues highlighted by
the data mining analyses, including evaluating further the usage of high-level hints
through verbal protocols. We are also in the process of designing tailored interventions to reduce gaming in Andes. As we described in Sect. 2, interventions for reducing
gaming have been successful in reducing gaming overall, but only in the less-frequent
gamers, and often without influencing learning outcomes. Since findings suggest that
student features are a key predictor of gaming, we believe that to be successful on all
these fronts related to discouraging gaming, the system needs to tailor its interventions according to a user model that takes into account user traits relevant to gaming.
To this end, we are planning on extending the gaming model we proposed here to
include user traits of interest, e.g., gaming tendency, and using it in Andes to inform
the interventions to discourage gaming.
Acknowledgements The authors thank the anonymous reviewers for their helpful suggestions. This research was funded the National Science Foundation, including the following grants: (1) IIS/HCC Affective
Learning Companions: Modeling and supporting emotion during learning(#0705883); (2) Deeper Modeling
via Affective Meta-tutoring(DRL-0910221) and (3) Pittsburgh Science of Learning Center(SBE-0836012).

References
Aleven, V.: Helping students to become better help seekers: towards supporting metacognition in a cognitive
tutor. In: Paper Presented at German-USA Early Career Research Exchange Program: Research on
Learning Technologies and Technology-Supported Education (2001)
Aleven, V., McLaren, B., Roll, I., Koedinger, K.: Toward meta-cognitive tutoring: a model of help seeking
with a cognitive tutor. Int. J. Artif. Intell. Educ. 16, 101–128 (2006)
Arroyo I., Woolf B.: Inferring learning and attitudes from a Bayesian network of log file data. In: Proceedings
of the 12th International Conference on Artificial Intelligence in Education (AIED’05), Amsterdam,
Netherlands, pp. 33–40 (2005)
Arroyo, I., Ferguson, K., Johns, J., Dragon, T., Meheranian, H., Fisher, D., Barto, A., Mahadevan, S., Woolf,
B.: Repairing disengagement with non-invasive interventions. In: Proceedings of the 13th International
Conference on Artificial Intelligence in Education (AIED’07), Los Angeles, United States, pp. 195–
202 (2007)
Baker, R.: Is gaming the system state-or-trait? educational data mining through the multi-contextual application of a validated behavioral model. In: Proceedings of the Workshop on Data Mining for User
Modeling, Corfu, Greece, pp. 76–80 (2007)
Baker, R., Corbett, A., Koedinger, K.: Detecting student misuse of intelligent tutoring systems. In: Proceedings of the 7th International Conference on Intelligent Tutoring Systems (ITS’04), Maceio, Brazil, pp.
531–540 (2004a)
Baker, R., Corbett, A., Koedinger, K., Wagner, A.: Off-task behavior in the cognitive tutor classroom: when
students “game the system”. In: Proceedings of the ACM CHI 2004: Computer-Human Interaction
(CHI’04), Vienna, Austria, pp. 383–390 (2004b)
Baker, R.S., Roll, I., Corbett, A., Koedinger, K.: Do performance goals lead students to game the system. In: Proceedings of the 12th International Conference on Artificial Intelligence and Education
(AIED2005), Amsterdam, Netherlands, pp. 57–64 (2005)
Baker, R., Corbett, A., Koedinger, K., Evenson, E., Roll, I., Wagner, A., Naim, M., Raspat, J., Baker, D.,
Beck, J.: Adapting to when students game an intelligent tutoring system. In: Proceedings of the 8th
International Conference on Intelligent Tutoring Systems (ITS’06), Jhongli, Taiwan, pp. 392–401
(2006a)
Baker, R.S.J.d., Corbett, A.T., Koedinger, K., Roll, I.: Generalizing detection of gaming the system across
a tutoring curriculum. In: Proceedings of the 11th International Conference on Intelligent Tutoring
Systems (ITS’06), Jhongli, Taiwan, pp. 402–411 (2006b)

123

An analysis of students’ gaming behaviors

133

Baker, R., Corbett, A., Roll, I., Koedinger, K.: Developing a generalizable detector of when students game
the system. User Model. User-Adap. Inter. 18(3), 287–314 (2008a)
Baker, R., Walonoski, J., Heffernan, N., Roll, I., Corbett, A., Koedinger, K.: Why students engage in “gaming the system”. Behavior in interactive learning environments. J. Interact. Learn. Res. 19(2), 185–
224 (2008b)
Baker, R., Corbett, A., Koedinger, K.: Educational software features that encourage and discourage “gaming
the system”. In: Proceedings of the 14th international conference on artificial intelligence in education
(AIED’09), Brighton, UK, pp. 475–482 (2009)
Baker, R.: Data mining for education. In: McGaw, B., Peterson, P., Baker, E. (eds.) International Encyclopedia of Education, vol. 7, 3rd edn, pp. 112–118. Elsevier Oxford, UK (2010)
Beck, J.E., Jia, P., Mostow, J.: Using automated questions to assess reading comprehension, vocabulary,
and effects of tutorial interventions. Technol. Instr. Cogn. Learn. (TICL) 2, 97–134 (2004)
Beck, J., Chang, K., Mostow, J., Corbett, A.: Does help? Introducing the Bayesian evaluation and assessment methodology. In: Proceedings of the 9th International Conference on Intelligent Tutoring Systems
(ITS’08), Montreal, Canada, pp. 383–394 (2008)
Bunt, A., Conati, C., Muldner, K.: Scaffolding self-explanation to improve learning in exploratory learning
environments. In: Proceedings of the 7th International Conference on Intelligent Tutoring Systems
(ITS’04), Maceio, Brazil, pp. 656–667 (2004)
Carnegie Learning Inc.: Retrieved April 1, 2010, from http://www.carnegielearning.com/ (2010)
Chang, K., Beck, J., Mostow, J., Corbett, A.: A Bayes net toolkit for student modeling in intelligent tutoring systems. In: Proceedings of the 11th International Conference on Intelligent Tutoring Systems
(ITS’06), Jhongli, Taiwan, pp. 104–113 (2006)
Chi, M.T.H.: How adaptive is an expert human tutor? In: Proceedings of the 10th International Conference
on Intelligent Tutoring Systems (ITS’10), Pittsburgh, United States, pp. 401–113 (2010)
Chi, M., VanLehn, K.: The impact of explicit strategy instruction on problem-solving behaviors across
intelligent tutoring systems. In: Proceedings of the 29th annual conference of the cognitive science
society, Nashville, Tennessee, pp. 167–172 (2007a)
Chi, M., VanLehn, K.: Accelerated future learning via explicit instruction of a problem solving strategy.
In: Proceedings of the 13th international conference on artificial intelligence in education (AIED’07),
Los Angeles, United States, pp. 409–416 (2007b)
Chi, M.T.H., Bassok, M., Lewis, M., Reimann, P., Glaser, R.: Self-explanations: how students study and
use examples in learning to solve problems. Cogn. Sci. 15, 145–182 (1989)
Chi, M.T.H., Siler, S.A., Jeong, H., Yamauchi, T., Hausmann, R.G.: Learning from human tutoring. Cogn.
Sci. 25, 471–533 (2001)
Chi, M.T.H., Roy, M., Hausmann, R.: Observing tutoring collaboratively: insights about tutoring effectiveness from vicarious learning. Cogn. Sci. 32(2), 301–341 (2008)
Chi, M., VanLehn, K., Litman, D.: Do micro-level tutorial decisions matter: applying reinforcement learning to induce pedagogical tutorial tactics. In: Proceedings of the 10th International Conference on
Intelligent Tutoring Systems (ITS’10), Pittsburgh, United States, pp. 184–193 (2010a)
Chi, M., VanLehn, K., Litman, D., Jordan, P.: Inducing effective pedagogical strategies using learning context features. In: Proceedings of the 18th International Conference on User Modeling, Adaptation and
Personalization (UMAP’10), Kona, Hawaii, pp. 147–158 (2010b)
Chi, M., VanLehn, K., Litman, D., Jordan P.: Empirically evaluating the Application of reinforcement learning to the induction of effective and adaptive pedagogical strategies. User model User-Adap. Inter.
(to appear). Special issue on educational data mining for personalized educational systems
Cohen, P., Beal, C.: Temporal data mining for educational applications. Int. J. Softw. Inform. 3(1),
31–46 (2009)
Conati, C., Gertner, A., VanLehn, K.: Using Bayesian networks to manage uncertainty in student modeling. User Model. User-Adap. Inter. 12(4), 371–417 (2002)
Conati, C., Maclaren, H.: Empirically building and evaluating a probabilistic model of user affect. User
Model. User-Adap. Inter. 19(3), 267–303 (2009)
Corbett, A.T., Anderson, J.R.: Knowledge tracing: modeling the acquisition of procedural knowledge. User
Model. User-Adap. Inter. 4(4), 253–278 (1995)
D’Mello, S., Craig, S., Sullins, J., Graesser, A.: Predicting affective states expressed through an emote-aloud
procedure from autotutor’s mixed-initiative dialogue. Int. J. Artif. Intell. Educ. 16(1), 3–28 (2006)
Davis, W., Carson, C., Ammeter, A., Treadway, D.: The interactive effects of goal orientation and feedback
specificity on task performance. Human Perform. 18, 409–426 (2005)

123

134

K. Muldner et al.

Dean, T., Kanazawa, K.: A model for reasoning about persistence and causation. Comput. Intell. 5(3), 142–
150 (1989)
Dempster, L., Laird, N., Rubin, D.: Maximum likelihood from incomplete data via the EM algorithm. J. R.
Stat. Soc. 39(1), 1–38 (1977)
Feng, M., Beck, J., Heffernan, N.T.: Using learning decomposition and bootstrapping with randomization
to compare the impact of different educational interventions on learning. In: Proceedings of the 2nd
International Conference on Educational Data Mining (EDM’09), Cordoba, Spain, pp. 51–60 (2009)
Getoor, L., Rhee, J., Koller, D., Small, P.: Understanding tuberculosis epidemiology using probabilistic
relational models. J. Artif. Intell. Med. 30, 233–256 (2004)
Hays, M., Lane, C., Auerbach, D., Core, M., Gomboc, D., Rosenberg, M.: Feedback specificity and the
learning of intercultural communication skills. In: Proceedings of the 14th International Conference
on Artificial Intelligence in Education (AIED’09), Brighton, UK, pp. 391–398 (2009)
Heckerman, D.: Bayesian networks for data mining. Data Min. Knowl. Discov. 1, 79–119 (1997)
Koedinger, K.R., Anderson, Hadley, W.H., Mark, M.A.: Intelligent tutoring goes to school in the big city. In:
Proceedings of the 3rd International Conference on Artificial Intelligence and Education (AIED’95),
Washington, DC, United States, pp. 421–428 (1995)
Mayo, M., Mitrovic, A.: Optimizing ITS behaviour with Bayesian networks and decision theory. Int. J.
Artif. Intell. Educ. 12, 124–153 (2001)
Muldner, K., Conati, C.: Using similarity to infer meta-cognitive behaviors during analogical problem solving. In: Proceedings of the 10th International Conference on User Modeling (UM’05), Edinborough,
Scottland, pp. 134–143 (2005)
Muldner, K., Burleson, W., de Sande, B., VanLehn, K.: An analysis of gaming behaviors in an intelligent
tutoring system. In: Proceedings of the 10th International Conference on Intelligent Tutoring Systems
(ITS’10), Pittsburgh, United States, pp. 195–205 (2010)
Muldner, K., Conati, C.: Scaffolding meta-cognitive skills for effective analogical problem solving via
tailored example selection. Int. J. Artif. Intell. Educ. 20(2), 99–136 (2010)
Murphy, K.: Dynamic Bayesian Networks: Representation, Inference and Learning. Ph.D. Thesis, UC,
Berkeley (2002)
Murphy, K.: Bayes Net Toolbox for Matlab. http://bnt.sourceforge.net. Accessed 2010 April 1 (2004)
Murray, R.C., VanLehn, K.: Effects of dissuading unnecessary help requests while providing proactive
help. In: Proceedings of the 12th International Conference on Artificial Intelligence in Education
(AIED’05), Amsterdam, Netherlands, pp. 887–889 (2005)
Nelson-Le Gall, S.: Help-seeking behavior in learning review of research in education. Rev. Res.
Educ. 12, 55–90 (1985)
Netica Reference Manual. Retrieved March 1, 2010, from www.norsys.com/netica-j/docs/NeticaJ_Man.
pdf (2010)
Phye, G.D., Sanders, C.: Advice and feedback: elements of practice for problem solving. Contemp. Educ.
Psychol. 19(3), 286–301 (1994)
Rai, D., Gong, Y., Beck, J.: Using Dirichlet priors to improve model parameter plausibility. In: Proceedings
of the international conference on educational data mining (EDM’09), pp. 141–150 (2009)
Razzaq, L., Heffernan, N.: To tutor or not to tutor: that is the question. In: Proceedings of th 2nd International
Conference on Artificial Intelligence in Education (AIED’09), Cordoba, Spain, pp. 457–464 (2009)
Renkl, A.: Learning from worked-examples: a study on individual differences. Cogn. Sci. 21(1), 1–30 (1997)
Reye, J.: Student modeling based on belief networks. Int. J. Artif. Intell. Educ. 14, 1–33 (2004)
Ritter, S., Harris, T., Nixon, T., Dickison, D., Murray, R.C., Towle, B.: Reducing the knowledge tracing
space. In: Proceedings of the 1st International Conference On Educational Data Mining (EDM’08),
Montreal, Canada, pp. 151–160 (2008)
Rodin, A., Mosley, T.H., Clark, A.G., Sing, C.F., Boerwinkle, E.: Mining genetic epidemiology data
with Bayesian networks application to APOE gene variation and plasma lipid levels. J. Comput.
Biol. 12(1), 1–11 (2005)
Rodrigo, M., Baker, R., d’Mello, S., Gonzalez, M.C.T., Lagud, M., Lim, S., Macapanpan, A., Pascua, S.,
Santillano, J., Sugay, J., Tep, S., Viehland, N.: Comparing learners affect while using an intelligent
tutoring systems and a simulation problem solving game. In: Proceedings of the 9th International
Conference on Intelligent Tutoring Systems (ITS’08), Montreal, Canada, pp. 40–49 (2008)
Roll, I., Aleven, V., McLaren, B., Ryu, E., Baker, R.S.J.d., Koedinger, K.: The help tutor: does metacognitive feedback improve students’ help-seeking actions, skills and learning? In: Proceedings

123

An analysis of students’ gaming behaviors

135

of the Eight International Conference on Intelligent Tutoring Systems (ITS’06), Jhongli, Taiwan,
pp. 360–369 (2006)
Russell, S., Norvig, P.: Artificial Intelligence: A Modern Approach. Los Altos, CA, Morgan-Kaufman
(2009)
Shih, B., Koedinger, K., Scheines, R.: A response time model for bottom-out hints as worked examples.
In: Proceedings of the 1st International Conference on Educational Data Mining (EDM’08), Montreal
Canada, pp. 117–126 (2008)
Shute, V.: Focus on formative feedback. Rev. Educ. Res. 78(1), 153–189 (2008)
VanLehn, K.: Analogy events: how examples are used during problem solving. Cogn. Sci. 22(3), 347–
388 (1998)
VanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A.,
WintersgillM.TheAndesphysics tutoring, system: Lessons learned. Int. J. Artif. Intell. Educ. 15(3), 1–
47 (2005)
VassarStats: Website for Statistical Computation. Retrieved April 1, 2010, from http://faculty.vassar.edu/
lowry/VassarStats.html
Walonoski, J.A., Heffernan, N.T.: Detection and analysis of off-task gaming behavior in intelligent tutoring
systems. In: Proceedings of the 8th International Conference on Intelligent Tutoring Systems (ITS’06),
Jhongli, Taiwan, pp. 382–391 (2006a)
Walonoski, J.A., Heffernan, N.T.: Prevention of off-task gaming behavior in intelligent tutoring systems.
In: Proceedings of the 8th International Conference on Intelligent Tutoring Systems (ITS’06), Jhongli,
Taiwan, pp. 722–724 (2006b)
Zhang, X., Mostow, J., Beck, J.: A case study empirical comparison of three methods to evaluate tutorial behaviors. In: Proceedings of the 9th International Conference on Intelligent Tutoring Systems
(ITS’08), Montral, Canada, pp. 122–131 (2008)

Author Biographies
Kasia Muldner is currently a Post-Doctoral Scholar in the department of Psychology at Arizona State University (ASU); prior to this appointment, she worked as a Post-Doctoral Scholar in the School of Computing,
Informatics, and Decision Systems Engineering, also at ASU. She received her Ph.D. from the University
of British Columbia, Canada, from the Computer Science Department at the Laboratory for Computational
Intelligence. Her research interests span Artificial Intelligence, Cognitive Science and Human Computer
Interaction, with a particular focus on pedagogical applications, including the design and evaluation of
intelligent tutoring systems, educational data mining, and investigating ways to foster constructive learning, for instance during analogical problem solving and learning from observing.
Winslow Burleson is an Assistant Professor of Human-Computer Interaction in the School of Computing,
Informatics, and Decision Systems Engineering at Arizona State University, and directs the Motivational
Environments research group (http://hci.asu.edu). Author of over 60 scientific publications, including “best
paper” at AIED 2009, and 10 patents, he received a Ph.D. in Affective Computing from the MIT Media
Lab, a BA from Rice, and MSE from Stanford. He worked with the Life Long Kindergarten group and
Entrepreneurial Management Unit at Harvard Business School, IBM Research, NASA-SETI Institute, the
Space Telescope Science Institute and UNICEF.
Brett Van de Sande is an Assistant Research Professional in the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University, working in the area of intelligent tutoring systems. He completed his Ph.D. at Ohio State University in theoretical physics, and has since taught physics
and conducted research in physics education, including the development of adaptive physics tutors.
Kurt VanLehn is a Professor of Computer Science at Arizona State University. He completed his Ph.D.
at MIT, did post-doctoral research at Xerox PARC, joined the faculty of Carnegie-Mellon University in
1985, moved to the University of Pittsburgh in 1990 and joined ASU in 2008. His work involves applications of artificial intelligence to education, including intelligent tutoring systems, cognitive modeling and
educational data mining.

123

A Supervised Clustering Method for Text Classification
Umarani Pappuswamy, Dumisizwe Bhembe,
Pamela W. Jordan, and Kurt VanLehn
Learning Research and Development Center,
3939 0’Hara Street, University of Pittsburgh,
Pittsburgh, PA 15260, USA
umarani@pitt.edu

Abstract. This paper describes a supervised three-tier clustering method for
classifying students’ essays of qualitative physics in the Why2-Atlas tutoring
system. Our main purpose of categorizing text in our tutoring system is to map
the students’ essay statements into principles and misconceptions of physics. A
simple `bag-of-words’ representation using a naïve-bayes algorithm to categorize text was unsatisfactory for our purposes of analyses as it exhibited many
misclassifications because of the relatedness of the concepts themselves and its
inability to handle misconceptions. Hence, we investigate the performance of
the k-nearest neighborhood algorithm coupled with clusters of physics concepts
on classifying students’ essays. We use a three-tier tagging schemata (cluster,
sub-cluster and class) for each document and found that this kind of supervised
hierarchical clustering leads to a better understanding of the student’s essay.

1

Introduction

Text Categorization (or Classification)1 can be seen either as an Information Retrieval
task or a Machine Learning task of automatically assigning one or more well-defined
categories or classes to a set of documents. Starting with the work of Maron [1] in the
early 60s, Text Classification (TC) has found a significant place in a variety of applications including: automatic indexing, document filtering, word sense disambiguation, and
information extraction. Our main focus is on the machine learning aspect of TC with the
goal to devise a learning algorithm capable of generating a classifier which can categorize text documents into a number of predefined concepts. This issue has been considered in several learning approaches both with a supervised learning scheme [2, 3] and
with an unsupervised and semi-supervised learning scheme [4, 5, 6].
In its simplest form, the text classification problem can be formulated as follows: We
are given a set of documents D = {d1, d2, d3 … dn} to be classified and C = {c1, c2, c3,
…cn} a predefined set of classes and the values {0, 1} interpreted as a decision to file a
document dj under ci where 0 means that dj is not relevant to the class defined and 1
means that dj is relevant to the class defined. The main objective here is to devise a
1

We prefer the term `Text Classification’ to `Text Categorization’ and hence use the same in
the rest of our paper.

A. Gelbukh (Ed.): CICLing 2005, LNCS 3406, pp. 704 – 714, 2005.
© Springer-Verlag Berlin Heidelberg 2005

A Supervised Clustering Method for Text Classification

705

learning algorithm that will be able to accurately classify unseen documents from D
(given the training set with the desired annotations in the case of supervised learning).
In our paper, we describe a three-tier clustering method for classifying students’
essay strings in the Why2-Atlas system. The students’ essays are the answers to
qualitative questions of physics. The task of the classifier is to map these essay
strings into the corresponding principles and misconceptions of physics. A simple
`Bag-Of-Words (BOW)’ approach using a naïve-bayes algorithm to categorize text
was unsatisfactory for our purposes of analyses as it exhibited many misclassifications because of the relatedness of the concepts themselves and its inability to handle misconceptions. Hence, we investigate the performance of k-nearest neighborhood algorithm coupled with pre-defined clusters of physics concepts on classifying
students’ essays. Though there have been many studies on word clustering for language modeling and word co-occurrence [7], very little work has been done on
word/concept clustering for document classification.
We present the results of an empirical study conducted on a corpus of students’
essay strings. The approach uses a three-tier tagging schemata (cluster, sub-cluster
and class) for each document. Let C and SC refer to the Cluster and Sub-cluster
respectively, and `Class (Cl)’ refers to the actual principle or misconception being
identified. Thus, C in the original definition now takes the form: C = {(C1, SC1, Cl1),
(Cn, SCn, Cln)}. This kind of supervised clustering approach helps us to reduce the
dimensionality of the texts and thereby leads to a better understanding of the student’s essay.
The next section, namely Section 2 describes text classification in the Why2Atlas tutoring system; Section 3 describes the three-tier clustering method and its
experimental design, Section 4 presents the results and discussion of our experiment
and Section 5 provides conclusions and directions for future work.

2

Text Classification in the Why2-Atlas System

The Why2-Atlas system presents students with qualitative physics problems and encourages them to write their answers along with detailed explanations to support their
answers [8]. As shown in Fig. 1, the student explanation from our corpus of humanhuman computer-mediated tutoring sessions, illustrates the type of explanation the
system strives to elicit from students. It is a form of self-explanation so it has the
potential to lead students to construct knowledge [9], and to expose deep misconceptions [10].
Question: Suppose you are in a free-falling elevator and you hold your keys
motionless right in front of your face and then let go. What will
happen to them Explain.
Explanation (Essay): Free-fall means without gravity. The keys should stay
right in front of your face since no force is acting on the keys to move
them.
Fig. 1. An actual problem statement and student explanation

706

U. Pappuswamy et al.

In the above example, there is a clear statement of misconception `Freefall means
without gravity’. Unless we evaluate the answers that students type in, we would not
be able to help them reconstruct their knowledge. There are a variety of ways in
which a student essay can be evaluated or graded. Autotutor [11] uses Latent Semantic Analysis (LSA) to analyze student essays. AutoTutor comprehends the student
input by segmenting the contributions into speech acts and matching the student’s
speech acts to the tutor’s expectations. If the expectations are covered in the student’s
essay, the essay is considered to be `good’.
In Why2-Atlas system, we use a similar method. We first look for the correctness
of the answer and then use a list of Principles (P) and Misconceptions (M) and look
for the presence of a P or M in the student essay. We have an `ideal answer’ for each
problem statement which is clearly marked for the necessary principles to be mentioned in the respective essay. If the essay contains all of the Ps stated in the ideal
answer, then it is considered to be a reasonably good essay and we allow the student
to move on to the next problem. Thus, it is important to classify the students’ essay
strings into Ps and Ms in order to subject it to further processing.
Several other attempts have been made in the project to analyze students’ essays in
the past using TC methods. Rose et al.’s experiments [12] used `keypoints (correct
answer aspects)’ and `nothing’ (in case of absence of a correct answer aspect) to classify essay strings; the precision and recall measures for the pumpkin problem2 was
81% and 73% respectively. The limitation of this approach was the inability to generalize the training across problems and to identify misconceptions (if any) expressed
by the student in his/her essay. There was an attempt to extend to more problems later
in the project by identifying only `Principles’ for each problem. The classifier’s performance is measured in terms of accuracy and standard error. Accuracy is the percentage of correctly labeled documents in the test set. Standard error of the prediction
is computed over the accuracy. The results are shown in Table 1. As the number of
classes increased, the accuracy declined. Hand-checking of the tags assigned to these
examples revealed many misclassifications. It was clear that the complexity of the
problem lies in the nature of the data itself.
Table 1. Performance of NB classifier on subsets
3

Subset
Pumpkin
Packet
Keys
Sun
Truck

No. of classes
17
14
20
8
8

No. of examples
465
355
529
216
273

Accuracy
50.87
55.49
48.46
60.60
65.22

Std Error
1.38
1.99
1.62
1.42
0.93

Furthermore, as this approach did not include training examples for misconceptions, the classifier grouped all such instances as `nothing’ (false negatives) or put
them under different `wrong’ classes (false positives) neither of which was desirable
by us. Since these problems share principles and misconceptions between them, yet
2
3

Pumpkin was one of the 10 problems given to the students in the tutoring session.
Subset includes data for the specific problems (pumpkin, keys, etc).

A Supervised Clustering Method for Text Classification

707

another approach was made to combine the examples from the subsets (in Table 1)
into one. We included training examples for misconceptions as well. We tested this
new dataset using the same NB algorithm and the results of this experiment are shown
in Table 2:
Table 2. Performance of NB classifier on global data
Set

No. of
classes

No. of
examples

Accuracy

Std.
Error

38

586

56.83

0.45

4

Global (all
problems)

Due to the similarity of the words present in the list of principles and misconceptions, there were still many misclassifications. To get a better understanding of the
nature of the problem, we tested 15 documents that belong to one concept. We expected the classifier to tag all the documents for only one class `prin-only-gravityimplies-freefall' (The description of this principle is: “When gravity is the only force
acting on an object, it is in freefall”). The classifier’s predictions5 reveal the following:
0 tagged for the expected principle `prin-only-gravity-implies-freefall' (Class1)
12 tagged for `prin-drop-obj-only-grav’ (Class2)
1 tagged for `prin-release-freefall’ (Class3)
4 tagged for `prin-freefall-same-accel’ (Class4)
1 tagged for `nothing’ (Class5)
Based on the training data, the classifier thus, encountered different but related
principles for the above set of data. This led us to examine the probability distribution
of the words used by each of these classes. Table 3 shows the probability distribution
of the top 10 words.
It can be observed that the significant words `gravity, free and fall' are found in all
the classes (2–4) and hence the problem of ambiguity arose. However, it should be
noted that the tags obtained above are related to each other. One can say that they are
partially correct and are related to the top principle in question. For instance, `prindrop-obj-only-grav’ is a subset of `prin-only-gravity-implies-freefall’. So, based on
the combined probability of the key words that are common for both these principles,
the classifier learned `prin-drop-obj-only-grav’ as in "The only force acting on the
man and the keys is the force of gravity". Later on, we tested a few more sentences
chosen randomly that contained words like `freefall’ and `gravity’. Hand-checking of
the predictions revealed that a sentence like `Freefall means without gravity’ (a misconception) was classified as a principle. This is not surprising because `without' was
4
5

This included data from all the ten problems.
The mismatch in the number of tags (18) and the number of sentences (15) is due to some
segmentation problems. Some of the documents were broken into more than one due to the
presence of a `period’. The principles corresponding to Classes 2, 3, and 4 are related to the
concept of `freefall’ but do not correspond to the exact description of the concept in Class1.

708

U. Pappuswamy et al.
Table 3. Info-gain of the top 10 words using NB
Class2
words

Class3

probability

words

probability

Class4
words

probability

force
gravity
keys

0.056206
0.046838
0.039813

force
free
fall

0.021341
0.018293
0.018293

keys
freefall
elevator

0.027356
0.024316
0.024316

acting
elevator

0.035129
0.023419

gravity
acting

0.015244
0.015244

free
person

0.018237
0.015198

fall

0.018735

keys

0.015244

fall

0.015198

free
rate

0.014052
0.011710

freefall
acceleration

0.012195
0.009146

release
accelerating

0.012158
0.006079

accelerating
front

0.009368
0.009368

elevator
problem

0.009146
0.009146

sentence
previous

0.006079
0.006079

listed as a stop word (whose `info-gain’ is lower than the rest of the words) in our
experiment. So we decided `not to’ use a stop-list in our future experiments. But, still
this would not solve the problem completely because the naïve bayes algorithm ignores the relationships of significant words that do not co-occur in the document.
Hence, we investigated the performance of various other classifiers on this issue and
decided to use k-nearest neighborhood algorithm along with the new clustering technique6.

3

Experimental Design

In this section, we describe our new experiment, the datasets used in the experiment
and the coding scheme at length.
3.1

Dataset

All of the datasets used in this work are extracted from the WHY-Essay7 corpus that
contains 1954 sentences (of essays). A list of Principles and Misconceptions that
corresponds to physics concepts of interest in the Why2-Atlas project is used as the
set of classes to be assigned to these essay strings. There are 50 principles and 53
misconceptions in total.
The training and test data are representative samples of responses to physics problems drawn from the same corpus. We created tag-sets for both principles and misconceptions (a total of 103) and used these to tag the data. We carried out many trials

6

7

For reasons of space, the statistical results of the various other classifiers used for this purpose are not shown here.
The Why-essay corpus consists of students’ essay statements mostly from Spring and Fall
2002 experiments of human-human tutoring sessions.

A Supervised Clustering Method for Text Classification

709

of classification and the performance on `old data' was used to do data-cleaning and to
revise the relations between classes that we want to identify/predict. Due to scarcity
of quality-data of essays containing misconceptions, we had to write some studentlike statements in order to expand the corpus of training data for misconceptions. This
required human expertise and a good understanding of the subject matter.
3.2.

Creation of Clusters

The Principles and Misconceptions used for tagging the essay segments have similar
topics (e.g. gravity-freefall and gravitational force, second law etc) and therefore
share common words. The classification task is typically hard because of lack of
unique terms and thus increases the feature dimensionality of these documents. Thus,
it is highly desirable to reduce this space to improve the classification accuracy. The
standard approach used for this kind of task is to extract a `feature subset’ of single
words through some kind of scoring measures (for example, using `Info-gain’). The
basic idea here is to assign a score to each feature (assigned to each word that occurred in the document), sort these scores, and select a pre-defined number of the best
features to form the solution feature subset (as in Latent Semantic Indexing approaches). In contrast to this standard approach, we use a method to reduce the feature
dimensionality by grouping “similar” words belonging to specific concepts into a
smaller number of `word-clusters’ and viewing these `clusters’ as features. Thus, we
reduce the number of features from `hundreds’ to `tens’.
Though there have been many studies (for example, [13] ) that use word-clusters to
improve the accuracy of unsupervised document classification, there are very few
studies that have used this kind of indirect `supervised’ clustering techniques for text
classification. Baker and McCallum [14] showed that word-clustering reduced the
feature dimensionality with a small change in classification performance. Slonim and
Tishby [4] use an information-bottleneck method to find word-clusters that preserve
the information about the document categories and use these clusters as features for
classification. They claim that their method showed 18% improvement over the performance of using words directly (given a small training set). Our work is unique in
that it uses a three-tier word-clustering method to label each student essay statement.
We endorse the same claims as the other two works, that word-clustering even when
done on classes instead of directly on the data improves the classification performance significantly.
3.2.1 The Three-Tier Clustering Method
Determining the `similarity’ of words in these physics documents is a difficult task.
Given the list of the principles and misconceptions used for tagging the students’
essay strings, we examined the semantics of the descriptions of each principle and
misconception and extracted those words (word clusters) that seemed to best describe
a particular concept and put them together. Fig. 2 illustrates this idea.
Thus, we have a three-tier tagging schemata that we built by hand in a bottom-up
fashion:
cluster, sub-cluster and class

710

U. Pappuswamy et al.

The upper levels (cluster and sub-cluster) describe the topic of discussion and the
lower level describes the specific principle or misconception. The + sign in each node
means the presence of that particular `word(s)’ in a concept description. For example,

Fig. 2. Chart showing the features related to the cluster `Gravity-Freefall’

from the trees in Fig 2, we can see that +freefall and +only force of gravity describe
Principle `P6’ while +freefall and +0gravity describe a Misconception `M53’. Thus,
words in the lower level that are shared across concepts migrate into an upper tier.
The top-most level was created using the concepts described at the middle level. We
created ten such clusters based on the prominent keywords for the training data (see
Table 4 for specifics8).
This information was used to extend the original corpus annotations9 so that the
training data took the form (mapping each D to C):
C = {(clustername, subclustername, class)
as exemplified below:
Freefall acceleration is the same for all objects and the keys the person and the elevator are all accelerating downwards with the same acceleration. {gravity-freefall,
freefall-prin, prin-freefall-same-accel}.
If the two forces were equal they would cancel each other out because they are in
opposite directions. {3rdLaw, act-react, misc-act-react-cancel}
In addition, there was also a `nothing’ class. The student statements that were neither a `P’ nor a `M’ are in this class.
8

Absence of sub-clusters in some groups means that there was no ambiguity between the principles and misconceptions in that cluster. The numbers found under the third column indicate
the number of principles and misconceptions that fall under the respective sub-cluster.
9
Annotations were for the principles, misconceptions and `nothing’.

A Supervised Clustering Method for Text Classification

711

Table 4. The three-tier clusters of principles and misconceptions
Classes
Cluster

Subcluster

Gravityfreefall
Gravitational-force
Secondlaw

Freefall
Release

P
3
2

M
7
0

3

11

Netforce
Force
One-object
2obj
Act-react

3
2
1
4
0

1
8
0
1
5

Kinematics
and vectors

Force

2

1

Zero- netforce

4

0

One-objectsecond-thirdlaw

Lightobj

0

4

heavyobj

0

2

Thirdlaw

Two-objectsmotion
Accelerationvelocitydisplacement
Weight-mass
General

3.3

-

objhit

0

1

Samevel
cons.vel- over-t
jointmotion

7
1
3

2
0
0

-

4

1

-

0
5

4
5

Document Modeling

Our main interest is to prove that `BOW approach with clusters’ outperforms `BOW
approach without clusters’ on students’ essay strings. Additionally, we are concerned
with how this comparison is affected by the size and the nature of the training set. In
this section, we discuss the various stages of our `document modeling’.
Document Indexing
Texts cannot be directly interpreted by a classifier or by a classifier-building algorithm. Therefore, it is necessary to have an indexing procedure that maps a text (dj)
into a compact representation of its content. This should be uniformly applied to training, validation, and test documents. We used the bag-of-words representation to index our documents with binary weights (1 denoting presence and 0 absence of the
term in the document). A document for us is a whole proposition and not a general
topic (commonly used in most BOW approaches to classify web pages).

712

U. Pappuswamy et al.

The `k- Nearest Neighborhood (kNN) Classifier
We used a simple k-Nearest Neighborhood (kNN) algorithm10, which is an instance
based learning approach for the classification task. Fix and Hodges defined a metric
to measure “closeness” between any two points and formulated a kNN rule: `Given
new point x and a training set of classified points, compute the kNN to x in the training data. Classify x as Class y if more k-nearest neighbors are in class y than any
other class’ [15]. In the context of TC, the same rule is applied where documents are
represented as `points’ (vectors with term weights). We used the Euclidean distance
formula to compute the measure of “nearness’.
Procedure
kNN was used for the three-tier clustering model that included the following stages:
1. Modeling the dataset (X) at the cluster level,
2. Dividing the dataset (X) into sub-datasets (Y) for sub-clusters, and bifurcating
them into two (one for principles and another for misconceptions)
3. Modeling the sub-datasets (Y) at the subcluster level
4. Dividing the dataset (X) into sub-datasets (Z) for the third level (classes),
5. Modeling the subdatasets (Z) at the class level.
The classification outputs at each level were the cluster, subcluster and class tags
respectively. At runtime, the output of a level is used to select a model in the next
level.
Cross-Validation
We used the 2/3 and 1/3 split of training and test data for this experiment. We set the
value of `k’ to 1 in kNN and evaluated kNN on the test set. A stratified ten-fold crossvalidation was repeated over 10 trials to get a reliable error estimate.

4

Results and Discussions

The metrics used to measure the performance of the learner are: accuracy, standard
error, and precision and recall. In order to define precision and recall, we need to
define `true positives, false positives, and false negatives. In our context, if a document D is related to C, it will be considered to be a `True Positive (TP)’, with a value
of `1’. If a document D is not related to C, it will have a value of `0’ and can either be
marked as `nothing’ which constitutes the `False Negatives (FN)’ for us or it can be
misclassified (as some other C) which means that it is a `False Positive (FP)’. For
example, if a student string `Freefall means without gravity, is correctly classified as
misconception statement (M53), it is a TP. On the other hand, if it is categorized as
`nothing’ then it is a `FN’ and if it is misclassified as anything else then it is `FP’.
Precision and Recall can thus be defined as:
10

We used the kNN algorithm from the RAINBOW software devised by McCallum (1996).
McCallum, Andrew Kachites. "Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering", www.cs.cmu.edu/~mccallum/bow.

A Supervised Clustering Method for Text Classification

713

Precision = TP/ (TP + FP),
Recall
= TP/ (TP + FN).
Using this formula, we computed the recall and precision of the bow-approach
with three-tier clusters (see Table 5):
11

Table 5. Precision and Recall results for the three-tier model
Model
Cluster(one level)
Three-tier clustering
Subcluster(two levels)
Classes (three levels)
Without clustering (using NB)

Precision (%)
80.88
74.25
62.58
68.59

Recall (%)
92.13
88.75
90.75
83.41

The accuracy and standard error of prediction at each level of clustering are shown
in Table 6 below along with the statistics of the bow-only approach using naïve bayes
classifier without clustering:
Table 6. Accuracy and Standard Error of the three-tier model
Model
Cluster (one level)
Three-tier clustering Subcluster(two levels)
Classes(three levels)
Without clustering (using NB)

Accuracy
78.01
74.50
64.16
50.99

Std. Error
0.016
0.020
0.185
0.019

The above results show that the three-tier clustering indeed helped to improve the
performance of the classification. Ambiguity (or noise) among classes was significantly reduced as the documents were forced to traverse the whole path (cluster →
subclusters → classes). Our model significantly outperformed the bow-only approach
using the naïve bayes classifier (27.02%, 23.51% and 13.17% of improvement in the
classification accuracy for the levels 1, 2 and 3 respectively).

5

Conclusions and Future Directions

This paper discussed a three-tier clustering approach of classifying data pertaining to
students’ essay statements of qualitative physics problems in a tutoring system. We
claim that `supervised three-tier clustering’ outperforms the non-clustering models
related to this domain. We conjecture that expansion of the training corpus for more
examples for misconceptions will further improve the clustering results and thereby
aid us in effective evaluation of the students’ essays.

11

The measures at each level use the output of the previous level regardless of correctness.

714

U. Pappuswamy et al.

Acknowledgements. This work was funded by NSF grant 9720359 and ONR grant
N00014-00-1-0600.

References
1. Maron, M. Automatic indexing: an experimental inquiry. Journal of the Association for
Computing Machinery (1961) Vol. 8(3): 404–417.
2. Duda, R., and Hart, P. Pattern Classification and Scene Analysis. John Wiley & Sons.
(1973) 95-99.
3. Sebastiani, Fabrizio. Machine learning in automated text categorization. ACM Computing
Surveys, (2002) Vol.34(1):1–47.
4. Slonim, N. and Tishby, N. The power of word clusters for text classification. In Proceedings of ECIR-01, 23rd European Colloquium on Information Retrieval Research, Darmstadt, Germany, (2001).
5. Slonim. N. N. Friedman, and N. Tishby. Unsupervised document classification using sequential information maximization. Proceedings of SIGIR'02, 25th ACM intermational
Conference on Research and Development of Information Retireval, Tampere, Finland,
ACM Press, New York (2002).
6. El-Yaniv, R and Oren Souroujon. Iterative Double Clustering for Unsupervised and Semisupervised Learning. European Conference on Machine Learning (ECML) (2001) 121-132.
7. Periera, F, N. Tishby, and L. Lee. Distributional clustering of English words. In 31st Annual Meeting of the ACL, (1993) 183-190.
8. VanLehn, Kurt, Pamela Jordan, Carolyn Rose´, Dumisizwe Bhembe, Michael Bottner,
Andy Gaydos, Maxim Makatchev, Umarani Pappuswamy, Michael Ringenberg, Antonio
Roque, Stephanie Siler, and Ramesh Srivastava. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proceedings of Intelligent Tutoring Systems Conference, volume 2363 of LNCS, Springer. (2002) 158–167.
9. Chi, Michelene, Nicholas de Leeuw, Mei-Hung Chiu, and Christian LaVancher. Eliciting
self explanations improves understanding. Cognitive Science, (1994) Vol. 18:439–477.
10. Slotta, James, Michelene T.H. Chi, and Elana Joram. Assessing students’ misclassifications of physics concepts: An ontological basis for conceptual change. Cognition and Instruction, (1995) Vol. 13(3):373–400.
11. Graesser, A. C.; Wiemer-Hastings, P.; Wiemer-Hastings, K.; Harter, D.; Person, N.; and the
Tutoring Research Group. Using Latent Semantic Analysis to Evaluate the Contributions of
Students in AUTOTUTOR. Interactive Learning Environments (2000) Vol. 8:129–148.
12. Rosé C,P, A. Roque, D. Bhembe, K. VanLehn. A Hybrid Text Classification Approach for
Analysis of Student Essays, Proceedings of the Human Language Technology conference/
North American chapter of the Association for Computational Linguistics annual meeting.
Workshop on Educational Applications of Natural Language Processing. (2003).
13. Hotho, Andreas, Steffen Staab and Gerd Stumme. Text Clustering Based on Background
Knowledge. Institute of Applied Informatics and Formal Description Methods AIFB,
Technical Report No. 425. (2003).
14. Baker, L.D. and A. K. McCallum. Distributional Clustering of Words for Text Classification. In ACM SIGIR 98. (1998).
15. Fix, E. and J.L. Hodges. Discriminatory Analysis -- Nonparametric Discrimination: Consistency Properties, Project 21--49--004, Report No. 4, USAF School of Aviation Medicine, Randolf Field, TX (1951) 261--279.

Combining Competing Language Understanding
Approaches in an Intelligent Tutoring System
Pamela W. Jordan, Maxim Makatchev, and Kurt VanLehn
Learning Research and Development Center, Intelligent Systems Program and
Computer Science Department, University of Pittsburgh, Pittsburgh PA 15260
{pjordan,maxim,vanlehn}@pitt.edu

Abstract. When implementing a tutoring system that attempts a deep
understanding of students’ natural language explanations, there are three
basic approaches to choose between; symbolic, in which sentence strings
are parsed using a lexicon and grammar; statistical, in which a corpus
is used to train a text classiﬁer; and hybrid, in which rich, symbolically
produced features supplement statistical training. Because each type of
approach requires diﬀerent amounts of domain knowledge preparation
and provides diﬀerent quality output for the same input, we describe
a method for heuristically combining multiple natural language understanding approaches in an attempt to use each to its best advantage.
We explore two basic models for combining approaches in the context
of a tutoring system; one where heuristics select the ﬁrst satisﬁcing representation and another in which heuristics select the highest ranked
representation.

1

Introduction

Implementing an intelligent tutoring system that attempts a deep understanding of a student’s natural language (NL) explanation is a challenging and time
consuming undertaking even when making use of existing NL processing tools
and techniques [1,2,3]. A motivation for attempting a deep understanding of an
explanation is so that a tutoring system can reason about the domain knowledge expressed in the student’s explanation in order to diagnose errors that are
only implicitly expressed [4] and to provide substantive feedback that encourages
further self-explanation [5]. To accomplish these tutoring system tasks, the NL
technology must be able to map typical student language to an appropriate domain level representation language. While some NL mapping approaches require
relatively little domain knowledge preparation there is currently still a trade-oﬀ
with the quality of the representation produced especially as the complexity of
the representation language increases.
Although most NL mapping approaches have been rigorously evaluated, the
results may not scale-up or generalize to the tutoring system domain. First it
may not be practical to carefully prepare large amounts of domain knowledge in
the same manner as may have been done for the evaluation of an NL approach.
This is especially a problem for tutoring systems since they need to cover a large
J.C. Lester et al. (Eds.): ITS 2004, LNCS 3220, pp. 346–357, 2004.
c Springer-Verlag Berlin Heidelberg 2004


Combining Competing Language Understanding Approaches

347

amount of domain knowledge to have an impact on student learning. Second,
acceptable performance results may vary across applications if the requirements
for representation ﬁdelity vary. For example, a document retrieval application
may not require a deep understanding of every sentence in the document to be
successful whereas providing tutorial feedback to students on the content of what
they write may. Finally, while one approach may be more promising than another
for providing a better quality representation, the time required to prepare the
domain knowledge to achieve the desired ﬁdelity is not yet reliably predictable.
For these reasons, it may be advisable to include multiple approaches and to
re-examine how the approaches are integrated within the tutoring system as the
domain coverage expands and improves over time.
Our goal in this paper is to examine ways in which multiple language mapping
approaches can be integrated within one tutoring system so that each approach
is used to its best advantage relative to a particular time-slice in the life-cycle of
the knowledge development for the tutoring system. At a given time-slice, one
approach may be functioning better than another but we must anticipate that
the performances may change when there is a signiﬁcant change in the domain
knowledge provided. Our approach for integrating multiple mapping approaches,
each with separate evolving knowledge sources, is to set up a competition between them and allow a deliberative process to decide for every student sentence
processed which representation is the best one to use. This approach is similar
to what is done in multi-agent architectures [6]. We will experimentally explore
a variety of ways of competitively combining three types of NL understanding
approaches in the context of the Why2-Atlas tutoring system; 1) symbolic, in
which sentence strings are parsed using an NL lexicon and grammar 2) statistical, in which a corpus is used to train a text classiﬁer and 3) hybrid, in which
rich symbolic features are used to supplement the training of a text classiﬁer.
First we will describe the Why2-Atlas tutoring domain and representation
language to give an impression of the diﬃculty of the NL mapping task. Next
we will characterize the expected performance diﬀerences of the individual approaches. Next we will describe how we measure performance and discuss how to
go about selecting the best conﬁguration for a particular knowledge development
time-slice. Next we will describe two types of competition models and their selection heuristics where the heuristics evaluate representations relative to typical
(but generally stated) representation failings we anticipate and have observed for
each approach. Finally, we will examine the performance diﬀerences for various
ways of combining the NL understanding approaches and compare them to two
baselines; the current best single approach and tutoring on all possible topics.

2

Overview of the Why2-Atlas Domain and
Representation Language

The Why2-Atlas system covers 5 qualitative physics problems on introductory
mechanics. For each problem the student is expected to type an answer and
explanation which the system analyzes in order to identify appropriate elicita-

348

P.W. Jordan, M. Makatchev, and K. VanLehn

Table 1. Slots for one body vector quantities with examples of slot ﬁller constants.
Description

Slot sorts (examples of
slot ﬁller constants)
quantity
Quantity1b (velocity, acceleration)
identiﬁer
Id (ID100)
body (or two bodies in case of force) Body (pumpkin, man)
axial component or not
Comp (horizontal, vertical)
qualitative derivative of the magnitude D-mag (constant, increase, decrease)
quantitative derivative of the magnitude D-mag-num (none)
zero or non-zero magnitude
Mag-zero (zero, nonzero)
quantitative magnitude
Mag-num (none)
sign for axial component
Dir (pos,neg)
qualitative derivative of the direction
D-dir (constant, nonconstant)
beginning of time interval
Time (problem speciﬁc)
end of time interval
Time (problem speciﬁc)

tion, clariﬁcation and remediation tutoring goals. The details of the Why2-Atlas
system are described in [1] and only the mapping of an isolated NL sentence to
the Why2-Atlas representation language will be addressed in this paper. In this
section we give an overview of the rich domain representation language that the
system uses to support diagnosis and feedback.
The Why2-Atlas ontology is strongly inﬂuenced by previous qualitative
physics reasoning work, in particular [7], but makes appropriate simpliﬁcations
given the subset of physics the system is addressing. The Why2-Atlas ontology
comprises bodies, states, physical quantities, times and relations. The ontology
and representation language are described in detail in [4].
For the sake of simplicity, most bodies in the Why2-Atlas ontology have the
semantics of point-masses. Body constants are problem speciﬁc. For example the
body constants for one problem covered by Why2-Atlas are pumpkin and man.
Individual bodies can be in states such as freefall. Being in a particular
state implies respective restrictions on the forces applied on the body. There is
also the special state of contact between two bodies where attached bodies
can exert mutual forces and the positions of the two bodies are equal, detached
bodies do not exert mutual forces, and moving-contact bodies can exert mutual
forces but there is no conclusion on their relative positions. The latter type of
contact is introduced to account for point-mass bodies that are capable of pushing/pulling each other for certain time intervals (a non-impact type of contact),
for example the man pushing a pumpkin up.
Physical quantities are represented as one or two body vectors. The one body
vector quantities are position, displacement, velocity, acceleration, and
total-force and the only two body one in the Why2-Atlas ontology is force.
The single body scalar quantities are duration, mass, and distance.
Every physical quantity has slots and respective restrictions on the sort of a
slot ﬁller as shown in Table 1, where examples of slot ﬁller constants of the proper
sorts are shown in parentheses. Note that the sorts Id, D-mag, and D-mag-num

Combining Competing Language Understanding Approaches

349

do not have speciﬁc constants. These slots are used only for cross-referencing
between diﬀerent propositions.
Time instants are basic primitives in the Why2-Atlas ontology and a time
interval is a pair (ti , tj ) of instants. This deﬁnition of time intervals is suﬃcient
for implementing the semantics of open time intervals in the context of the
mechanics domain.
Some of the multi-place relations in our domain are before, rel-position
and compare. The relation before relates time instants in the obvious way.
The relation rel-position provides the means to represent the relative position of two bodies with respect to each other, independently of the choice of
a coordinate system—a common way to informally compare positions in NL.
The relation compare is used to represent the ratio and diﬀerence of two quantities’ magnitudes or for quantities that change over time, magnitudes of the
derivatives.
The domain propositions are represented using order-sorted ﬁrst-order logic
(FOL) (see for example [8]). For example, “force of gravity acting on the pumpkin
is constant and nonzero” has the following representation in which the generated
identiﬁer constants f1 and ph1 appear as arguments in the due-to relation
predicate (sort information is omitted):
(force f1 ?body1 pumpkin ?comp constant ?d-mag-num nonzero ?mag-num ?dir
?d-dir ?t1 ?t2)
(due-to d1 f1 ph1)
(phenomenon ph1 gravity)

There is no explicit negation so a negative student statement such as “there
is no force” is represented as the force being zero. The version of the system
currently under development is extending the knowledge representation to cover
disjunctions, conditional statements and other types of negations.

3

Overview of the Language Understanding Approaches

In general, symbolic approaches are expected to yield good coverage and accuracy if suﬃcient knowledge of the domain can be captured and eﬃciently
utilized. Whereas statistical and hybrid approaches are much easier to develop
for a domain than symbolic ones and can provide just as good of coverage, those
that use little more than a text corpus are expected to provide less accurate
representations of what the student meant than pure symbolic approaches (once
the knowledge engineering problem is adequately addressed).
Although there are many tools available for each type of approach, we developed Why2-Atlas domain knowledge sources for the symbolic approach Carmel
[9], the statistical approach Rainbow [10] and the hybrid symbolic and statistical approach Rappel [11]. The knowledge development for each approach is
still ongoing and at diﬀerent levels of completeness, yet the system has been
successfully used by students in two tutoring studies. Below we describe each of
the approaches, as well as the tools we use, in more detail. We use the theoretical

350

P.W. Jordan, M. Makatchev, and K. VanLehn

strengths and weaknesses of each general type of approach as the basis for our
hand-coded selection heuristics.
3.1

Symbolic Approach

The traditional approach for mapping NL to a knowledge representation language is symbolic; sentence strings are parsed using an NL lexicon and grammar.
There are many practical and robust sentence-level syntactic parsers available
for which wide coverage NL lexicons and grammars exist [12,13,9], but syntactic
analysis can only canonicalize relative to syntactic aspects of lexical semantics
[14]. For example, the similarity of “I baked a cake for her” and “I baked her
a cake” is found but their similarity to “I made her a cake” is not.1 The latter
sort of canonicalization is typically provided by semantic analysis. But there is
no general solution at this level because semantic analysis falls into the realm
of cognition and mental representations [15] and must be engineered relative to
the domain of interest.
Carmel provides combined syntactic and semantic analysis using the
LCFlex robust syntactic parser, a broad coverage grammar, and semantic constructor functions that are speciﬁc to the domain to be covered [9]. Given a
speciﬁcation of the desired representation language, it then maps the resulting
analysis to the domain representation language. Until recently, semantic constructor functions had to be completely hand-generated for every lexical entry.
Although tools to facilitate and expedite this level of knowledge representation
are currently being developed [16,17], it is still a signiﬁcant knowledge engineering eﬀort.
Because the necessary lexical-level knowledge engineering is diﬃcult and time
consuming and it is unclear how to predict when such a task will be suﬃciently
completed, there may be unexpected gaps in the semantic knowledge. Also robust
parsing techniques can produce partial analyses and typically have a limited
ability to self-evaluate the quality of the representation into which it maps a
student sentence. So the ability to produce partial analyses in conjunction with
gaps in the knowledge sources suggest that symbolic approaches will tend to
undergenerate representations for sentences that weren’t anticipated during the
creation of their knowledge sources.
3.2

Statistical Approach

More recent approaches for processing NL are statistical; a corpus is used to train
a wide variety of approaches for analyzing language. Statistical approaches are
popular because there is relatively little eﬀort involved to get such an approach
working, if a representative corpus already exists. The most useful of these approaches for intelligent tutoring systems has been text classiﬁcation in which a
subtext is tagged as being a member of a particular class of interest and uses just
1

The need to distinguish the semantic diﬀerences between “bake” and “made” depends on the application for which the representation will be used.

Combining Competing Language Understanding Approaches

351

the words in the class tagged corpus for training a classiﬁer. This particular style
of classiﬁcation is called a bag of words approach because the meaning that the
organization of a sentence imparts is not considered. The classes themselves are
generally expressed as text as well and are at the level of an exemplar of a text
that is a member of the class. With this approach, the text can be mapped to
its representation by looking up a hand-generated propositional representation
for the exemplar text of the class identiﬁed at run-time.
Rainbow is one such bag of words text classiﬁer; in particular it is a Naive
Bayes text classiﬁer. The classes of interest must ﬁrst be decided and then a
training corpus developed where subtexts are annotated with the class to which
it belongs. For the Why2-Atlas training, each sentence was annotated with one
class. During training Rainbow computes an estimate of the probability of a
word in a particular class relative to the class labellings for the Why2-Atlas
training sentences. Then when a new sentence is to be analyzed at run-time,
Rainbow calculates the posterior probabilities of each class relative to the words
in the sentence and selects the class with the highest probability [10].
Like most statistical approaches, the quality of Rainbow’s analysis depends
on the quality of its training data. Although good annotator agreement is possible for the classes of interest for the Why2-Atlas domain [18], we found the
resulting training set for a class sometimes includes sentences that depend on a
particular context for the full meaning of that class to be licensed. In practice
the necessary context may not be present for the new sentence that is to be
analyzed. This suggests that the statistical approach will tend to overgenerate
representations. It is also possible for a student to express more than one key
part of an explanation in a single sentence so that multiple class assignments
would be more appropriate. This suggests that the statistical approach will also
sometimes undergenerate since only the best classiﬁcation is used. However, we
expect the need for multiple class assignments to happen infrequently since the
Why2-Atlas system includes a sentence segmenter that attempts to break up
complex sentences before sentence understanding is attempted by any of the
approaches.
3.3

Hybrid Approach

Finally, there are hybrids of symbolic and statistical approaches. For example,
syntactic features can be used to supplement the training of a text classiﬁer. Although the syntactic features often are obtained via statistical parsing methods,
they are sometimes obtained via symbolic methods instead since the resulting
feature set is richer [18]. With text classiﬁcation, the classes are still generally
deﬁned via an exemplar of the class so the desired propositional representation
must still be obtained via a look-up according to the class identiﬁed at run-time.
Rappel is a hybrid approach that uses symbolically-derived syntactic dependency features (obtained via Minipar [13,19]) to train for classes that are
deﬁned at the representation language level [11] instead of at an informal text
level. There is a separate classiﬁer for each type of proposition in the knowledge representation language. Each classiﬁer indicates whether a proposition of

352

P.W. Jordan, M. Makatchev, and K. VanLehn

the type it recognizes is present and if so, which class it is. The class indicates
which slots are ﬁlled with which slot constants. There is then a one-to-one correspondence between a class and a proposition in the representation language.
To arrive at the representation for a single sentence, Rappel applies all of the
trained classiﬁers and then combines their results during a post-processing stage.
For Why2-Atlas we trained separate classiﬁers for every physics quantity,
relation and state for a total of 27 diﬀerent classiﬁers. For example, there is a
separate classiﬁer for velocity and another for acceleration. Bodies are also
handled by separate classiﬁers; one for one body propositions and another for two
body propositions. The basic approach for the body classiﬁers is similar to that
used in statistical approaches to reference resolution (e.g. [20,21]). The number
of classes within each classiﬁer depend on the number of slot constant ﬁller combinations possible. For example, the class v h encodes the proposition (velocity
id1 horizontal ?body ?var1 . . . ?varn ) and the class v hip encodes the proposition
(velocity id2 horizontal ?body increase ?mag-zero ?mag-num pos ?t1 ?t2) where
v represents the predicate velocity, h represents the slot constant horizontal,
i represents the slot constant increase and p represents the constant pos.
Having a large number of classiﬁers and classes requires a larger, more comprehensive set of training data than is needed for a typical text classiﬁcation
approach. And just as with the preparation of the training data for the statistical approach, the annotator may still be inﬂuenced by the context of a sentence.
However, we expect the impact of contextual dependencies to be less severe
since the representation-deﬁned classes are more formal and ﬁner-grained than
text-deﬁned classes. For example, annotators may still resolve intersentential
anaphora and ellipsis but the content related inferences needed to select a class
are much ﬁner-grained and therefore a closer ﬁt to the actual meaning of the
sentence.
Although we have classiﬁers and classes deﬁned that cover the entire Why2Atlas representation language, we have not yet provided training for the full
representation language. Given the strong dependence of this approach on the
completeness of the training data, we expect this approach to sometimes undergenerate just as an incomplete symbolic approach would and sometimes to
overgenerate because of overgeneralizations during learning, just as with any
statistical approach.

4

Computing and Comparing Performances

To measure the overall performance of the Why2-Atlas system when using different understanding approach conﬁgurations, we use a test suite of 35 held-out
multi-sentence student essays (235 sentences total) that are annotated for the
elicitation and remediation topics that are to be discussed with the student. Elicitation topics are tagged when prescribed, critical physics principles are missing
from the student’s explanation and remediation topics are tagged when the essay implicitly or explicitly exhibits any of a small number of misconceptions or
errors that are typical of beginning students. From a language analysis perspec-

Combining Competing Language Understanding Approaches

353

tive, the representation of the essay must be accurate enough to detect when
physics principles are both properly and improperly expressed in the essay.
For the entire test suite we compute the number of true positives (TP), false
positives (FP), true negatives (TN) and false negatives (FN) for the elicitation
topics selected by the system relative to the elicitation topics annotated for the
test suite essays. From this we compute recall = TP/(TP+FN), precision =
TP/(TP+FP), and false alarm rate = FP/(FP+TN).
As a baseline measure, we compute the recall, precision and false alarm rate
that results if all possible elicitations for a physics problem are selected. For
our 35 essay test suite the recall is 1, precision is .61 and false alarm rate is 1.
Although NL evaluations compute an F-measure (the harmonic average of recall
and precision) in order to arrive at one number for comparing approaches, it
does not allow errors to be considered as fully as with other analysis methods
such as receiver operating characteristics (ROC) areas [22] and d [23]. These
measures are similar in that they combine the recall and the false alarm rates
into one number but allow for error skewing [22]. Rather than undertaking a full
comparison of the various NL understanding approach conﬁgurations for this
paper, we will instead look for those combinations that result in a high recall
and a low false alarm rate. Error skewing depends on what costs we need to
attribute to false negatives and false positives. Both potentially have negative
impacts on student learning in that the former leaves out important information
that should have been brought to the student’s attention and the latter can
confuse the student or cause lack of conﬁdence in the system.

5

The Selection Heuristics

Although an NL understanding approach is not strictly an agent in the sense
of [24] (e.g. it doesn’t reason about goals or other agents) it can be treated architecturally as a service agent in the sense of [25] as has been done in many
dialogue systems (e.g. [26,3]). Generally the service agents supply slightly diﬀerent information or are relevant in slightly diﬀerent contexts so that the evaluator
or coordinator decides which single service agent will be assigned a particular
task. For example, [26] describes a system architecture that includes competing discourse strategy service agents and an evaluator that rates the competing
strategies and selects the highest rated strategy agent to perform the communication task.
However, in the case of competing NL understanding approaches, an evaluator would need to predict which approach will provide the highest quality
analysis of a sentence that needs to be processed in order to decide which one
should be assigned the task. Because such a prediction would probably require
at least a partial analysis of the sentence, we take the approach of assigning the
task to all of the available language understanding approaches and then assessing the quality of the results relative to the expected typical accuracy faults of
each approach.

354

P.W. Jordan, M. Makatchev, and K. VanLehn

The ﬁrst competition model tries each approach in a preferred sequential ordering, stopping when a representation is acceptable according to a general ﬁltering heuristic and otherwise continuing. The ﬁltering heuristic estimates which
representations are over or undergenerated and excludes those representations
so that it appears that no representation was found for the sentence. A representation for a sentence is undergenerated if any of the word stems in a sentence are
constants in the representation language and none of those are in the representation generated or if the representation produced is too sparse. For Why2-Atlas,
it is too sparse if 50% of the propositions in the representation for a sentence
have slots with less than two constants ﬁlling them. Most propositions in the
representation language contain six slots which can be ﬁlled with constants.
Propositions that are deﬁned to have two or fewer slots that can be ﬁlled with
constants are excluded from this assessment (e.g. the relations before and relposition are excluded). Representations are overgenerated if the sentences are
shorter than 4 words since in general the physics principles to be recognized
cannot be expressed in fewer words.
For the sequential model, we use a preference ordering of symbolic, statistical
and hybrid in these experiments because of the way in which Why2-Atlas was
originally designed and our expectations for which approach should produce the
highest quality result at this point in the development of the knowledge sources.
We also created some partial sequential models as well to look at whether the
more expensive understanding approaches add anything signiﬁcant at this point
in their development.
The other competition model requests an analysis from all of the understanding approaches and then uses the ﬁltering heuristic along with a ranking
heuristic (as described below) to select the best analysis. If all of the analyses
for either competition model fail to meet the selection heuristics then the sentence is regarded as uninterpretable. The run-time diﬀerence between the two
competition models are nearly equivalent if each understanding approach in the
second model is run in parallel using a distributed multi-agent architecture such
as OAA [25].
The ranking heuristic again focuses on the weaknesses of all the approaches.
It computes a score for each representation by ﬁrst ﬁnding the number of words
in the intersection of the constants in the representation and the word stems
in the sentence (justif ied), the number of word stems in the sentence that are
constants in the representation language that do not appear in the representation
(undergenerated) and the number of constants in the representation that are
not word stems in the sentence (overgenerated). It then selects the one with
the highest score, where the score is; justif ied − 2 ∗ undergenerated − .5 ∗
overgenerated. The weightings reﬂect both the importance and approximate
nature of the terms.
The main diﬀerence between the two models is that the ranking approach
will choose the better representation (as estimated by the heuristics) as opposed
to one that merely suﬃces.

Combining Competing Language Understanding Approaches

6

355

Results of the Combined Competing Approaches

The top part of Table 2 compares the baseline of tutoring all possible topics and
the individual performances of the three understanding approaches when each is
used in isolation from the others. We see that only the statistical approach lowers
the false alarm rate but does so by sacriﬁcing recall. The rest are not signiﬁcantly
diﬀerent from tutoring all topics. However, the results of the statistical approach
are clearly not good enough.
Table 2. Performance of language understanding approaches for actions taken in the
Why2-Atlas system
approach
baseline1 (tutor all topics)
symbolic
statistical (baseline2)
hybrid
all (satisﬁcing)
hybrid + statistical (satisﬁcing)
symbolic + statistical (satisﬁcing)
all (highest ranked)

recall
1.0
1.0
.60
.94
.67
.70
.69
.73

precision
.61
.61
.93
.59
.80
.78
.80
.76

false alarm rate
1.0
1.0
.07
1.0
.26
.31
.26
.36

The bottom part of Table 2, shows the results of combining the NL approaches. The satisﬁcing model that includes all three NL mapping approaches
performs better than the individual models in that it modestly improves recall
but at the sacriﬁce of a higher false alarm rate. The satisﬁcing model checks
each representation in order 1) symbolic 2) statistical 3) hybrid, and stops with
the ﬁrst representation that is acceptable according to the ﬁltering heuristic. We
also see that both of the satisﬁcing models that include just two understanding
approaches perform better than the model in which all approaches are combined; with the symbolic + statistical model being the best since it increases
recall without further increasing the false alarm rate. Finally, we see that the
model, which selects the best representation from all three approaches, provides
the most balanced results of the combined or individual approaches. It provides
the largest increase in recall and the false alarm rate is still modest compared
to the baseline of tutoring all possible topics. To make a ﬁnal selection of which
combined approach one should use, there needs to be an estimate of which errors
will have a larger negative impact on student learning. But clearly, selecting a
combined approach will be better than selecting a single NL mapping approach.

7

Discussion and Future Work

Although none of the NL mapping approaches adequately represent the physics
content covered by the Why2-Atlas system at this point in their knowledge de-

356

P.W. Jordan, M. Makatchev, and K. VanLehn

velopment, they can be combined advantageously by estimating representations
that are over or undergenerated.
We are considering two future improvements. One is to automatically learn
ranking and ﬁltering heuristics using features that represent diﬀerences between
annotated representations and the representations produced by the understanding approaches. The heuristics can then be tuned to the types of representations that the approaches are producing at a particular time-slice in the domain
knowledge development. The second future improvement is to add reference resolution to the heuristics in order to canonicalize words and phrases to their body
constants in the representation language. Although we could try canonicalizing
other lexical items to their representation language constants, this might not be
as fruitful. While a physics expert could use push and pull and know that this
implies that forces are involved, this is not a safe assumption for introductory
physics students.
Acknowledgments. This research was supported by ONR Grant No. N0001400-1-0600 and by NSF Grant No. 9720359.

References
1. VanLehn, K., Jordan, P., Rosé, C., Bhembe, D., Böttner, M., Gaydos, A.,
Makatchev, M., Pappuswamy, U., Ringenberg, M., Roque, A., Siler, S., Srivastava, R.: The architecture of Why2-Atlas: A coach for qualitative physics essay
writing. In: Proceedings of Intelligent Tutoring Systems Conference. Volume 2363
of LNCS., Springer (2002) 158–167
2. Aleven, V., Popescu, O., Koedinger, K.: Pilot-testing a tutorial dialogue system
that supports self-explanation. In: Proceedings of Intelligent Tutoring Systems
Conference. Volume 2363 of LNCS., Springer (2002) 344
3. Zinn, C., Moore, J.D., Core, M.G.: A 3-tier planning architecture for managing
tutorial dialogue. In: Proceedings of Intelligent Tutoring Systems Conference (ITS
2002). (2002) 574–584
4. Makatchev, M., Jordan, P., VanLehn, K.: Abductive theorem proving for analyzing
student explanations and guiding feedback in intelligent tutoring systems. Journal
of Automated Reasoning: Special Issue on Automated Reasoning and Theorem
Proving in Education (2004) to appear.
5. Aleven, V., Popescu, O., Koedinger, K.R.: A tutorial dialogue system with
knowledge-based understanding and classiﬁcation of student explanations. In:
Working Notes of 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems. (2001)
6. Sandholm, T.W.: Distributed rational decision making. In Weiss, G., ed.: Multiagent Systems: A Modern Approach to Distributed Artiﬁcial Intelligence. The MIT
Press, Cambridge, MA, USA (1999) 201–258
7. Ploetzner, R., VanLehn, K.: The acquisition of qualitative physics knowledge during textbook-based physics training. Cognition and Instruction 15 (1997) 169–205
8. Walther, C.: A many-sorted calculus based on resolution and paramodulation.
Morgan Kaufmann, Los Altos, California (1987)

Combining Competing Language Understanding Approaches

357

9. Rosé, C.P.: A framework for robust semantic interpretation. In: Proceedings of the
First Meeting of the North American Chapter of the Association for Computational
Linguistics. (2000) 311–318
10. McCallum, A., Nigam, K.: A comparison of event models for naive bayes text
classiﬁcation. In: Proceeding of AAAI/ICML-98 Workshop on Learning for Text
Categorization, AAAI Press (1998)
11. Jordan, P.W.: A machine learning approach for mapping natural language to a
domain representation language. in preparation (2004)
12. Abney, S.: Partial parsing via ﬁnite-state cascades. Journal of Natural Language
Engineering 2 (1996) 337–344
13. Lin, D.: Dependency-based evaluation of MINIPAR. In: Workshop on the Evaluation of Parsing Systems, Granada, Spain (1998)
14. Levin, B., Pinker, S., eds.: Lexical and Conceptual Semantics. Blackwell Publishers,
Oxford (1992)
15. Jackendoﬀ, R.: Semantics and Cognition. Current Studies in Linguistics Series.
The MIT Press (1983)
16. Rosé, C., Gaydos, A., Hall, B., Roque, A., VanLehn, K.: Overcoming the knowledge
engineering bottleneck for understanding student language input. In: Proceedings
of of AI in Education 2003 Conference. (2003)
17. Dzikovska, M., Swift, M., Allen, J.: Customizing meaning: building domain-speciﬁc
semantic representations from a generic lexicon. In Bunt, H., Muskens, R., eds.:
Computing Meaning. Volume 3. Academic Publishers (2004)
18. Rosé, C., Roque, A., Bhembe, D., VanLehn, K.: A hybrid text classiﬁcation approach for analysis of student essays. In: Proceedings of HLT/NAACL 03 Workshop
on Building Educational Applications Using Natural Language Processing. (2003)
19. Lin, D., Pantel, P.: Discovery of inference rules for question answering. Journal of
Natural Language Engineering Fall-Winter (2001)
20. Strube, M., Rapp, S., Müller, C.: The inﬂuence of minimum edit distance on
reference resolution. In: Proceedings of Empirical Methods in Natural Language
Processing Conference. (2002)
21. Ng, V., Cardie, C.: Improving machine learning approaches to coreference resolution. In: Proceedings of Association for Computational Linguistics 2002. (2002)
22. Flach, P.: The geometry of ROC space: Understanding machine learning metrics
through ROC isometrics. In: Proceedings of 20th International Conference on
Machine Learning. (2003)
23. MacMillan, N., Creelman, C.: Detection Theory: A User’s Guide. Cambridge
University Press, Cambridge, UK (1991)
24. Franklin, S., Graesser, A.: Is it an agent, or just a program?: A taxonomy for
autonomous agents. In: Proceedings of the Third International Workshop on Agent
Theories, Architectures, and Languages, Springer-Verlag (1996)
25. Cheyer, A., Martin, D.: The open agent architecture. Journal of Autonomous
Agents and Multi-Agent Systems 4 (2001) 143–148
26. Jokinen, K., Kerminen, A., Kaipainen, M., Jauhiainen, T., Wilcock, G., Turunen,
M., Hakulinen, J., Kuusisto, J., Lagus, K.: Adaptive dialogue systems - interaction
with interact. In: Proceedings of the 3rd SIGdial Workshop on Discourse and
Dialogue. (2002)

A Hybrid Approach to Content Analysis for Automatic Essay Grading

Carolyn P. Rosé, Antonio Roque, Dumisizwe Bhembe, and Kurt VanLehn
LRDC, University of Pittsburgh, 3939 O’hara St., Pittsburgh, PA 15260
rosecp@pitt.edu

Abstract
We present CarmelTC, a novel hybrid text classification approach for automatic essay grading. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two “bag
of words” approaches, namely LSA and a Naive
Bayes, as well as a purely symbolic approach.

1

Introduction

In this paper we describe CarmelTC , a novel automatic
essay grading approach using a hybrid text classification technique for analyzing essay answers to qualitative
physics questions inside the Why2 tutorial dialogue system (VanLehn et al., 2002). In contrast to many previous approaches to automated essay grading (Burstein et
al., 1998; Foltz et al., 1998; Larkey, 1998), our goal is
not to assign a letter grade to student essays. Instead, our
purpose is to tally which set of “correct answer aspects”
are present in student essays. Previously, tutorial dialogue systems such as A UTO -T UTOR (Wiemer-Hastings
et al., 1998) and Research Methods Tutor (Malatesta et al.,
2002) have used LSA (Landauer et al., 1998) to perform
the same type of content analysis for student essays that
we do in Why2. While Bag of Words approaches such as
LSA have performed successfully on the content analysis task in domains such as Computer Literacy (WiemerHastings et al., 1998), they have been demonstrated to
perform poorly in causal domains such as research methods (Malatesta et al., 2002) because they base their predictions only on the words included in a text and not on
the functional relationships between them. Thus, we propose CarmelTC as an alternative. CarmelTC is a rule
learning text classification approach that bases its predictions both on features extracted from CARMEL’s deep

syntactic functional analyses of texts (Rosé, 2000) and a
“bag of words” classification of that text obtained from
Rainbow Naive Bayes (McCallum and Nigam, 1998).
We evaluate CarmelTC in the physics domain, which is
a highly causal domain like research methods. In our
evaluation we demonstrate that CarmelTC outperforms
both Latent Semantic Analysis (LSA) (Landauer et al.,
1998) and Rainbow Naive Bayes (McCallum and Nigam,
1998), as well as a purely symbolic approach similar to
(Furnkranz et al., 1998). Thus, our evaluation demonstrates the advantage of combining predictions from symbolic and “bag of words” approaches for content analysis
aspects of automatic essay grading.

2

Student Essay Analysis

We cast the Student Essay Analysis problem as a text classification problem where we classify each sentence in the
student’s essay as an expression one of a set of “correct
answer aspects”, or “nothing” in the case where no “correct answer aspect” was expressed. Essays are first segmented into individual sentence units. Next, each segment is classified as corresponding to one of the set of key
points or “nothing” if it does not include any key point.
We then take an inventory of the classifications other than
“nothing” that were assigned to at least one segment. We
performed our evaluation over essays collected from students interacting with our tutoring system in response to
the question “Suppose you are running in a straight line at
constant speed. You throw a pumpkin straight up. Where
will it land? Explain.”, which we refer to as the Pumpkin
Problem. Thus, there are a total of six alternative classifications for each segment:
Class 1 After the release the only force acting on the
pumpkin is the downward force of gravity.



This research was supported by the ONR, Cognitive Science Division under grant number N00014-0-1-0600 and by
NSF grant number 9720359 to CIRCLE.

Class 2 The pumpkin continues to have a constant horizontal velocity after it is released.

Class 3 The horizontal velocity of the pumpkin continues to be equal to the horizontal velocity of the man.
Class 4 The pumpkin and runner cover the same distance
over the same time.
Class 5 The pumpkin will land on the runner.
Class 6 Sentence does not adequately express any of the
above specified key points.
Often what distinguishes sentences from one class and
another is subtle. For example, “The pumpkin’s horizontal velocity, which is equal to that of the man when he released it, will remain constant.” belongs to Class 2. However, it could easily be mistaken for Class 3 based on the
set of words included, although it does not express that
idea since it does not address the relationship between the
pumpkin’s and man’s velocity after the release. Similarly,
“So long as no other horizontal force acts upon the pumpkin while it is in the air, this velocity will stay the same.”,
belongs to Class 2 although looks similar on the surface to
either Class 1 or 3. Nevertheless, it does not express the
required propositional content for either of those classes.
The most frequent problem is that sentences that express
most but not all of the content associated with a required
point should be classified as “nothing” although they have
a lot of words in common with sentences from the class
that they are most similar to. Similarly, sentences like “It
will land on the ground where the runner threw it up.”
contain all of the words required to correctly express the
idea corresponding to Class 5, although it does not express that idea, and in fact expresses a wrong idea. These
very subtle distinctions pose problems for “bag of words”
approaches since they base their decisions only on which
words are present regardless of their order or the functional relationships between them.
The hybrid CarmelTC approach induces decision trees
using features from a deep syntactic functional analysis
of an input text as well as a prediction from the Rainbow
Naive Bayes text classifier (McCallum and Nigam, 1998).
Additionally, it uses features that indicate the presence or
absence of words found in the training examples. From
these features CarmelTC builds a vector representation
for each sentence. It then uses the ID3 decision tree learning algorithm (Quinlin, 1993) to induce rules for identifying sentence classes based on these feature vectors.
From CARMEL’s deep syntactic analysis of a sentence, we extract individual features that encode functional relationships between syntactic heads (e.g., (subjthrow man)), tense information (e.g., (tense-throw past)),
and information about passivization and negation (e.g.,
(negation-throw +) or (passive-throw -)). Syntactic feature structures produced by the grammar factor out those
aspects of syntax that modify the surface realization of

a sentence but do not change its deep functional analysis, including syntactic transformations such as passivization and extraction. These deep functional relationships
give CarmelTC the information lacking on Bag of Words
approaches that is needed for effective content analysis
in highly causal domains, such as research methods or
physics.

3

Evaluation

We conducted an evaluation to compare the effectiveness of CarmelTC at analyzing student essays in comparison to LSA, Rainbow, and a purely symbolic approach
similar to (Furnkranz et al., 1998), which we refer to
here as CarmelTCsymb. CarmelTCsymb is identical to
CarmelTC except that it does not include in its feature set
the prediction from Rainbow. We conducted our evaluation over a corpus of 126 previously unseen student essays
in response to the Pumpkin Problem described above,
with a total of 500 text segments, and just under 6000
words altogether. Each text segment was hand tagged
by at least two coders, and conflicts were resolved at a
consensus meeting. Pairwise Kappas between our three
coders computed over initial codings of our data was always above .75.
The LSA space used for this evaluation was trained
over three first year physics text books. The Rainbow
models used to generate the Rainbow predictions that are
part of the feature set provided to CarmelTC were trained
over a development corpus of 248 hand tagged example
sentences extracted from a corpus of human-human tutoring dialogues, just like those included in the 126 essays mentioned above. However, when we evaluated the
performance of Rainbow for comparison with CarmelTC,
LSA, and the symbolic approach, we ran a 50 fold cross
validation evaluation using the complete set of examples
in both sets (i.e., the 248 sentences used to train the Rainbow models used to by CarmelTC as well as the 126 essays) so that Rainbow would have access to the exact
same training data as CarmelTC, to make it a fair comparison between alternative machine learning approaches.
On each iteration, we randomly selected a subset of essays
such that the number of text segments included in the test
set were greater than 10 but less than 15 and then training Rainbow using the remaining text segments. Thus,
CarmelTC uses the same set of training data, but unlike
the other approaches, it uses its training data in two separate parts, namely one to train the Rainbow models it uses
to produce the Rainbow prediction that is part of the vector representation it builds for each text and one to train
the decision trees. This is because for CarmelTC, the data
for training Rainbow must be separate from that used to
train the decision trees so the decision trees are trained
from a realistic distribution of assigned Rainbow classes
based on its performance on unseen data rather than on

Figure 1: This Table compares the performance of the 3 alternative approaches
Approach
LSA
Rainbow
CarmelTCsymb
CarmelTC

Precision
93%
81%
88%
90%

Recall
54%
73%
72%
80%

Rainbow’s training data. Thus, for CarmelTC, we also
performed a 50 fold cross validation, but this time only
over the set of 126 example essays not used to train the
Rainbow models used by CarmelTC.
Note that LSA works by using its trained LSA space
to construct a vector representation for any text based on
the set of words included therein. It can thus be used
for text classification by comparing the vector obtained
for a set of exemplar texts for each class with that obtained from the text to be classified. We tested LSA using
as exemplars the same set of examples used as Rainbow
training data, but it always performed better when using a
small set of hand picked exemplars. Thus, we present results here using only those hand picked exemplars. For
every approach except LSA, we first segmented the essays at sentence boundaries and classified each sentence
separately. However, for LSA, rather than classify each
segment separately, we compared the LSA vector for the
entire essay to the exemplars for each class (other than
“nothing”), since LSA’s performance is better with longer
texts. We verified that LSA also performed better specifically on our task under these circumstances. Thus, we
compared each essay to each exemplar, and we counted
LSA as identifying the corresponding “correct answer aspect” if the cosine value obtained by comparing the two
vectors was above a threshold. We used a threshold value
of .53, which we determined experimentally to achieve
the optimal f-score result, using a beta value of 1 in order
to treat precision and recall as equally important.
Figure 1 demonstrates that CarmelTC out performs the
other approaches, achieving the highest f-score, which
combines the precision and recall scores into a single
measure. Thus, it performs better at this task than two
commonly used purely “bag of words” approaches as well
as to an otherwise equivalent purely symbolic approach.

References
J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,
L. Braden-Harder, and M. D. Harris. 1998. Automated
scoring using a hybrid feature identification technique.
In Proceedings of COLING-ACL’98, pages 206–210.
P. W. Foltz, W. Kintsch, and T. Landauer. 1998. The
measurement of textual coherence with latent semantic

False Alarm Rate
3%
9%
7%
8%

F-Score
.70
.77
.79
.85

analysis. Discourse Processes, 25(2-3):285–307.
J. Furnkranz, T. Mitchell Mitchell, and E. Riloff. 1998.
A case study in using linguistic phrases for text categorization on the www. In Proceedings from the
AAAI/ICML Workshop on Learning for Text Categorization.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25(2-3):259–284.
L. Larkey. 1998. Automatic essay grading using text categorization techniques. In Proceedings of SIGIR.
K. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the short answer question with research
methods tutor. In Proceedings of the Intelligent Tutoring Systems Conference.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Classification.
J. R. Quinlin. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers: San Mateo, CA.
C. P. Rosé. 2000. A framework for robust semantic interpretation. In Proceedings of the First Meeting of the
North American Chapter of the Association for Computational Linguistics, pages 311–318.
K. VanLehn, P. Jordan, C. P. Rosé, and The Natural Language Tutoring Group. 2002. The architecture of
why2-atlas: a coach for qualitative physics essay writing. Proceedings of the Intelligent Tutoring Systems
Conference.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tutoring Res earch Group. 1998. The foundations and
architecture of autotutor. In B. Goettl, H. Halff, C. Redfield, and V. Shute, editors, Intelligent Tutoring Systems: 4th International Conference (ITS ’98 ), pages
334–343. Springer Verlag.

Scaﬀolding Problem Solving with Annotated,
Worked-Out Examples to Promote
Deep Learning
Michael A. Ringenberg and Kurt VanLehn
University of Pittsburgh, Learning Research and Development Center
3939 O’Hara St., Pittsburgh PA 15260, USA
412-624-3353
mringenb@pitt.edu, vanlehn@cs.pitt.edu
http://www.pitt.edu/~mringenb/

Abstract. This study compares the relative utility of an intelligent tutoring system that uses procedure-based hints to a version that uses
worked-out examples for learning college level physics. In order to test
which strategy produced better gains in competence, two versions of Andes were used: one oﬀered participants graded hints and the other oﬀered
annotated, worked-out examples in response to their help requests. We
found that providing examples was at least as eﬀective as the hint sequences and was more eﬃcient in terms of the number of problems it
took to obtain the same level of mastery.

1

Introduction

At the heart of most educational research is the search for ways to improve the
instruction of novices. One strategy that has been found to be very eﬀective is
one-on-one human tutoring [1]. The economics of providing one-on-one tutoring
has prompted the investigation of other techniques to boost learning. Another
technique is to use intelligent tutoring systems to supplement classroom instruction and to substitute for individualized instruction. Another technique is to use
embedded examples in instructional material [2], [3], [4], [5], [6]. As both paths
have met with some success, it is worth comparing them and exploring ways to
combine them.
Our study was done with modiﬁcation of Andes, an intelligent tutoring system
that aids the instruction of college-level introductory physics [7]. The main function of Andes is to present students with problems and to let the students solve
them with the option of receiving adaptive scaﬀolding from the system. The two
types of adaptive scaﬀolding in Andes are ﬂag feedback and hints. Flag feedback
marks the student’s input as either correct or incorrect. When the student asks
for help, Andes presents the student with a hint. The hint either points out what
is wrong with the input or suggests a step to do next. The hint is based on the
anticipated next step in solving the problem. It is designed to help the student
identify and apply the missing relevant basic principles and deﬁnitions. In this
M. Ikeda, K. Ashley, and T.-W. Chan (Eds.): ITS 2006, LNCS 4053, pp. 625–634, 2006.
c Springer-Verlag Berlin Heidelberg 2006


626

M.A. Ringenberg and K. VanLehn

way, Andes tries to link the current problem-solving step with facts the student
has already been taught.
Each hint is staged in a graded fashion known as a hint sequence. The student is typically presented ﬁrst with a vague suggestion to prompt self explanation of the next step or to identify and correct the current error. The student can then ask for the next level in the hint sequence if the student judges
that the previous hint was insuﬃcient. The hints become more concrete as
the sequence is followed. The last level in a hint sequence typically supplies
the entire anticipated next correct problem-solving step. This is referred to
as the bottom-out hint. This graded structure of hints has been used in several intelligent tutoring systems (For more information on Andes, please see
http://www.andes.pitt.edu/). Students can and do resort to “help abuse”
when this form of adaptive scaﬀolding is oﬀered [8]. Students can click through
the hints rapidly in order to get to the bottom-out hint and will ignore the rest
of the hint sequence. This strategy is a problem because it is associated with
shallow learning [8].
Our basic hypothesis is that novice students will learn more eﬀectively if
we replace Andes’ hint sequences with worked-out near-transfer examples. A
worked-out example is a solved problem with all of the anticipated problemsolving steps explicitly stated. A near-transfer example has a deep structure
similar to that of the current problem and uses the same basic principles. Several
lines of evidence suggest that worked-out examples will be more eﬀective for
novices than hint sequences.
First, based on an observation from previous Andes studies, some students will
ﬁnd the solution to one problem through help abuse and then refer back to that
solved problem when faced with a similar problem. In essence, they are using
the ﬁrst problem to create a worked-out example. This observation is consistent
with studies showing that novices prefer to learn from examples as opposed to
procedural instructions [9].
Second, we suspect that the hints provided by Andes can provide good targeted help to students who are already familiar with the subject material and
have an adequate understanding of the underlying principles. However, for
novices, the ﬁrst hints in the graded hint sequence probably make little sense.
Novices are not suﬃciently familiar with the subject material for the hints to
activate the reasoning needed to ﬁnish the anticipated next step, nor are they
familiar enough with the problem solving structure to understand why the hint
is relevant.
Third, worked-out examples have been shown to be eﬀective instruction in
some cases. In one study, worked-out examples were more eﬀective than presenting procedural rules [3]. However, examples are more eﬀective when they
alternate with problem solving, presumably because studying large blocks of examples becomes boring [10]. By using a single example in place of a hint sequence
for each problem, we can avoid the boredom of large example blocks.
On the other hand, worked-out examples are not always eﬀective. Their usefulness requires that students self-explain the solution steps listed in the example.

Scaﬀolding Problem Solving with Annotated, Worked-Out Examples

627

A self-explanation for an example is a meaningful and correct explanation of
a step in the student’s own words [11]. Unfortunately, students do not tend to
produce self-explanations spontaneously and many students produce ineﬀective
self-explanations. Useful self-explanations can be categorized as either derivations or procedural explanations [12]. Derivations answer the question “Where
did this step come from?” and procedural explanations answer the question
“Why was this step done?”
When students do not engage in self-explanation, they do not tend to develop
a deep understanding of the material. Novices tend to match surface features of a
problem, like diagrams and problem statement wording, with those in a workedout example. In contrast, experts use the principles and deep structure as criteria
for matching a worked-out example to a problem [13]. The deep structure refers
to a general plan or sequence of principle applications that can be followed in order to solve the problem. By providing worked-out examples with well-structured
explicit steps to the solution and annotations of the relevant principles for each
step, we are presenting students with examples of good self-explanations. This
is expected to promote identiﬁcation of the underlying problem structure and
facilitate recognition of similar problem structure in diﬀerent problems. Providing an annotated, worked-out example during problem solving enables a direct
comparison and should encourage the student to focus on the common deep
structure between the problem and the example. This will lead the students
who are provided with these examples to perform better on tasks that test the
deep structural understanding of the problems than those who are not provided
with them.

2

Methodology

This was a hybrid study in that it was both naturalistic and experimental.
The experiment was conducted during a second semester, college level physics
course. As part of the graded homework for this course, students solved problems
with Andes. Students who volunteered to participate in the experiment used a
modiﬁed version of Andes to do this homework. The post-test for this study was
administered either three or four days before the in-class exam, depending on
students’ regular lab sessions. The time frame of homework completion was at
the students’ discretion, and ranged from a few weeks before the relevant inclass exam to many weeks after. This unanticipated confound was resolved by
the creation of a new category “No-Training” for participants who had not done
any of their homework before this study’s post-test.
The study had two experimental conditions: Examples and Hints. In the Examples condition, participants were presented with annotated, worked-out examples in response to any help request while using Andes. Each problem was
mapped to a single example, but several problems were mapped to the same
example if they shared the same deep structure. In the Hints condition, participants were given Andes’ normal graded, step-dependent hints in response to
help requests. The dependent variable for this experiment was performance on a

628

M.A. Ringenberg and K. VanLehn

problem matching task. Participants were asked to choose which of two problem
statements would be solved most similarly to the given problem statement. This
task is meant to evaluate deep learning by measuring participants’ recognition
of deep structure similarities.
The study participants were recruited from students already participating in
the physics section of Pittsburgh Science of Learning Center LearnLab (http://
www.learnlab.org/). The physics section was run as part of the General Physics
I/II classes in the United States Naval Academy in Annapolis, Maryland. A total
of forty-six volunteers were recruited from two sections of this course taught
by the same professor. Participants were instructed to download the version
of Andes which was modiﬁed for this study to use for the assigned homework
problems on the topic of “Inductors.” Because use of Andes was demonstrated
in class and required for homework throughout the course, students in these
sections were expected to be familiar with it. No restrictions were placed on
use of the study-assigned Andes program, textbooks, professors, peers, or any
other supplementary material. Due dates for Andes homework in the course
were not rigidly enforced. Only the unmodiﬁed Andes version of the homework
on Inductors was made available to the participants for in the Hints condition.
Those in the Examples condition were assigned the same homework problems
but instead were given access only to a modiﬁed version of Andes with the graded
hints replaced with a worked-out example problem.
The worked-out examples were designed to be near-transfer problems where
numeric values and some other surface features were changed. The solution to
a homework problem requires solving for a variable in an equation while the
worked-out example shows steps to solving a diﬀerent variable in the same
equation. For example, the equation for Ohm’s law is V = IR (voltage = current*resistance). If one homework problem gives values for V and R and asks
the student to calculate I, and another gives values for V and I and asks for R,
then the one worked-out example used for both of these questions would show
steps for calculating V from given values for I and R. This relationship means
that only ﬁve worked-out examples were needed for the ten homework problems.
The problem solving steps in the examples were written and annotated with the
principle used in each step, or with a list of the equations that were algebraically
combined for a given step. The example was designed to show completed problem
solving steps and solutions identical to those used in unmodiﬁed Andes problems. The principles in the annotations were linked to the appropriate Andes
subject matter help pages so that the same body of problem-solving information
was available to all participants.
The post-test was administered during the last lab session of the class prior
to the in-class examination on this material. The test format was adapted from
the similarity judgment task described by Dufresne, et. al [14]. It consisted of
twenty multiple choice questions in random order, with randomly ordered answer
choices, presented one at a time with thirty minutes given to complete the test.
Each question contained three unsolved problems: a model problem and two
comparison problems. Each of these problems consisted of a few sentences and

Scaﬀolding Problem Solving with Annotated, Worked-Out Examples

629

a diagram. There were four possible types of relationship between the model
problem and the two comparison problems:
I.
II.
III.
IV.

Same surface features with diﬀerent deep structure
Same surface features with the same deep structure
Diﬀerent surface features with diﬀerent deep structure
Diﬀerent surface features with the same deep structure

Only one of the comparison problems in each question had the same deep
structure as the model problem (Type II and IV). The homework covered ﬁve
diﬀerent deep structure concepts. In the post-test, four questions were related to
each deep structure concept, each with a diﬀerent combination of surface feature
relatedness. The theoretical strengths of this method of measuring competence
include emphasis on deep structure and de-emphasis of algebraic skills [14]. The
participants were given the following written instructions:
“In the following evaluation, you will be presented with a series of problem statements. You do not have to solve the problems! Your task will
be to read the ﬁrst problem statement and then decided which of the
following two problems would be solved most similarly to the ﬁrst one.”
In contrast to the format used by Dufresne, et. al [14] The model problems
in this study were repeated as infrequently as possible (given the small number of variables in each equation). The present study also drew all correctly
matched deep structure problems in the post-test from the assigned homework
and worked-out examples.
The participants were assigned to the two experimental groups in a pairwise
random fashion based on their cumulative Grade Point Average (GPA). This
single criterion was used to balance the two groups in terms of previous performance without regard for other variables such as class section, gender, academic
major, or age. Ten speciﬁc homework problems from the fourteen Inductance
problems available in Andes were assigned to the class.

3

Results

Of the twenty-three participants assigned to each condition, only nine from the
Examples condition and twelve from the Hints condition had asked for help from
Andes to solve at least one of the homework problems before the study’s posttest. Twenty other participants had not started working on the assignment and
two of these did not complete the post-test either. Five participants had solved
at least one homework problem in Andes without using the any of the available
help. Only those participants who completed the post-test were included in the
ﬁnal analysis. Of those who did any of the homework, only those who asked
for help at least once were exposed to the manipulated variable, and so the ﬁve
participants who did not ask for help were excluded.
There were no signiﬁcant diﬀerences in performance on circuit questions
among the three conditions on the in-class examination administered before

630

M.A. Ringenberg and K. VanLehn
Variable Averages
No-Training
Hints
Examples
In-Class Circuit Exam
187 ± 21
178 ± 34
201 ± 38
Total Training Time (s)
7942 ± 3681 4189 ± 2407
Time per Problem (s)
672 ± 238
508 ± 162
# of Problems Solved
11 ± 1.53
8.1 ± 2.8
Weighted Post-Test
3.56 ± 0.49
4.30 ± 0.37 4.88 ± 0.56
Problem Eﬃciency
0.413 ± 0.076 0.711 ± 0.215

p
0.5684
0.0735
0.2540
0.0427
0.0010
0.0034

Fig. 1. Results are reported as mean±95% conﬁdence limits

this study. This suggests that even though the participants who did not do
homework self-selected the No-Training condition, all three conditions ended
up with equivalently competent students. (see Fig. 1: In-Class Circuit Exam;
F(2,36) = 0.57, p = 0.5684). There was a notable but not statistically signiﬁcant
diﬀerence in the total time participants chose to spend solving homework problems between the Examples and Hints groups, with the Hints group spending
more time on problems (see Fig. 1: Total Training Time; t17.6 corrected = 1.90,
p = 0.0735). In contrast, the average time spent per problem (see Fig 1: Time
per Problem; t19 = 1.18, p = 0.2540) was more consistent between the two
groups. There was a signiﬁcant diﬀerence in the average number of problems attempted between the two groups, with the Hints groups working on more problems than the Examples group (see Fig. 1: # of Problems Solved; t19 = 2.17,
p = 0.0427).
By construction, the post-test questions varied considerably in their diﬃculty;
for instance, it should be easier to identify similar deep structure when the surface features are also similar, and harder to identify them when the surface
features are diﬀerent. To more accurately measure competence, a weighted score
was used. The post-test questions were weighted according to their diﬃculty as
determined by the performance of the No-Training participants on each of the
, where #correct
questions. The weight given to each question was 1 − #correct
18
was the number of participants from the No-Training condition who answered
the given question correctly The calculated weights on the problems were in
agreement with a priori expected performance diﬀerences on the diﬀerent problem types.
When an ANOVA model was ﬁt using the weighted post-test scores (see Fig. 1:
Weighted Post-Test), a statistically signiﬁcant diﬀerence among the three groups
was detected (F(2,36) = 8.49, p = 0.0010). With the Tukey-Kramer adjustment
for multiple comparisons, it was found that the participants in the Examples
condition did signiﬁcantly better on the post-test than those in the No-Training
condition (t = 3.98, p = 0.0009). The Hints condition also did better than the
No-Training condition (t = 2.45, p = 0.0496). However, it was not possible to
distinguish a diﬀerence between the Hints condition and the Examples condition
based solely on the weighted post-test score (t = 1.61, p = 0.2525). Other
dependent variables, such as GPA and in-class examination scores, were not
found to be signiﬁcant factors in any ANCOVA models.

Scaﬀolding Problem Solving with Annotated, Worked-Out Examples

631

Mean Problem Efficiency by
Condition

Weighted Post-Test Score by Condition
6

1.0
0.9

5

Training Efficiency

Weighted Score

0.8
4
3
2

0.7
0.6
0.5
0.4
0.3
0.2

1

0.1
0

0.0
No Training

Hints

Examples

Fig. 2. Results are reported as means with
error bars showing the ±95% conﬁdence
limits. Either form of training is better
than none, but the diﬀerence in weighted
post-test scores of the Hints and Examples
conditions are not statistically signiﬁcant.

Hints

Examples

Fig. 3. Problem eﬃciency is deﬁned as
by condition where training eﬃciency
is the weighted post-test score divided
by the number of problems solved. Results are reported as means with error bars showing the ±95% conﬁdence
limits. Training problems with examples were more eﬃcient at raising posttest scores than training problems with
hints.

Problem eﬃciency was also calculated, that is, the increase in weighted posttest score per training problem done. The Examples and Hints conditions had
weighted post-test scores that were not signiﬁcantly diﬀerent from each other but
the participants in the Examples condition chose to do fewer training problems.
The problem eﬃciency for the Examples condition was signiﬁcantly higher than
for the Hints condition (see Fig. 1:Problem Eﬃciency and 3; t = 3.34 p = 0.0034).
An ANCOVA model was ﬁt using the weighted post-test scores with the number of problems attempted as the covariate and the Hints or Examples condition
as the categorical variable (see Fig. 4. It was determined that the interaction
eﬀect between the condition and the number of problems was not signiﬁcant
(p = 0.8290). When the number of training problems was controlled by estimating the mean weighted post-test score at the overall mean number of training
problems (μ∗ = 9.76), the diﬀerence in scores for the two training conditions condition was signiﬁcant (μExamples = 4.88 ± 0.46; μHints = 4.14 ± 0.50; t = 2.30,
p = 0.0338). This was consistent with the Problem Eﬃciency results and demonstrates that given the same number of training problems, participants in the
Examples condition performed better on the post-test than participants in the
Hints condition.

632

M.A. Ringenberg and K. VanLehn

Fig. 4. Weighted post-test score versus number of problems solved with a ﬁtted regression lines for the Hints and Examples conditions

4

Discussion

The results of this study demonstrate the value of working on Andes training problems to improve competence, whether with worked-out examples or graded hints.
Although students in the No-Training condition were self-selected, they showed no
signiﬁcant diﬀerence in competence with basic circuit concepts prior to the study
(as measured by scores on an in-class exam). One important diﬀerence between
the two training conditions was the time on task, measured by the amount of time
spent on the training homework. Participants in the Example condition chose to
solve fewer training problems on average than the participants in the Hints condition. This was not due to the participants in the examples condition taking longer
to solve problems, as the average time to solve each problem was not signiﬁcantly
diﬀerent, but due to participants in the Hints condition choosing to spend more
time working on more problems. Though participants in the Examples condition
solved fewer problems on average than those in the Hints condition, they did at
least as well on the post-test. This evidence supports the hypothesis that workedout examples are a more eﬃcient form of problem-solving help than graded hints.
Due to the small number of participants involved in this study, aptitude treatment
interactions could not be examined. A larger study might reveal an expertise reversal eﬀect, where worked-out examples are more eﬀective than graded hints for
novices and less eﬀective than graded hints for experts [15].

Scaﬀolding Problem Solving with Annotated, Worked-Out Examples

633

While previous studies have shown that providing worked-out examples can
lead to shallow learning [13], this study indicates that worked-out examples may
in fact be more eﬃcient at promoting deep learning than graded hints. This has
implications for tutoring system design in that examples may be a valuable addition to intelligent tutoring systems. Moreover, adding worked-out examples to
an intelligent tutoring system should be fairly easy. The examples used in this
study were easily added to Andes, mostly because there was no “intelligence”
that needed to be designed to implement this strategy. One possible disadvantage of graded hint sequences is that they may be too rigid to accommodate
the individual thought processes of diﬀerent students. If the intelligent tutoring
system that provides graded hints is good at assessing the participant’s thought
process, then the hints it can provide are likely to be eﬀective. If the system can’t
identify and provide feedback relevant to a student’s thought process, the hints
will probably seem close to meaningless. If this happens too often, the student
may decide that the hints are useless.
Worked-out examples can be a way of making sure that the system can provide a shared context for the student. They may be of particular value when the
system has not collected enough data to evaluate the student eﬀectively or if
communication seems to have failed. One way to integrate worked-out examples
into a graded hint system is to replace the bottom-out hint with the relevant
worked-out example. This change may be particularly useful in addressing the
help abuse [8]. It would be informative to see whether this strategy would reduce help abuse or provide incentive for the participants to click through a hint
sequence rapidly just to see the worked-out example at the end. Worked-out
examples could be integrated into more complicated intelligent tutoring systems
that can assess the utility of diﬀerent actions and provide these examples when
appropriate [16]. Increasing the diversity of possible useful actions in such a
system could only improve its performance.

References
1. Bloom, B.S.: The 2 sigma problem: The search for methods of group instruction
as eﬀective as one-to-one tutoring. Educational Researcher 13 (1984) 4–16
2. Sweller, J., Cooper, G.A.: The use of worked examples as a substitute for problem
solving in learning algebra. Cognition and Instruction 2 (1985) 59–89
3. Cooper, G., Sweller, J.: Eﬀects of schema acquisition and rule automation on
mathematical problem-solving transfer. Journal of Educational Psychology 79
(1987) 347–362
4. Brown, D.E.: Using examples and analogies to remediate misconceptions in physics:
Factors inﬂuencing conceptual change. Journal of Research in Science Teaching 29
(1992) 17–34
5. Catrambone, R.: Aiding subgoal learning - eﬀects on transfer. Journal of Educational Psychology 87 (1995) 5–17
6. Koehler, M.J.: Designing case-based hypermedia for developing understanding children’s mathematical reasoning. Cognition and Instruction 20 (2002)
151–195

634

M.A. Ringenberg and K. VanLehn

7. VanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., Shelby, R., Taylor, L., Treacy,
D., Weinstein, A., Wintersgill, M.: Andes physics tutoring system: Five years of
evaluations. In Looi, G.M.C.K., ed.: Proceedings of the 12th International Conference on Artiﬁcial Intelligence in Education, Amsterdam, IOS Press (2005)
8. Aleven, V., Koedinger, K.R.: Investigations into help seeking and learning with a
cognitive tutor. Papers of the AIED-2001 Workshop on Help Provision and Help
Seeking in Interactive Learning Environments (2001) 47–58
9. LeFevre, J.A., Dixon, P.: Do written instructions need examples? Cognition and
Instruction 3 (1986) 1–30
10. Trafton, J.G., Reiser, B.J.: The contribution of studying examples and solving
problems to skill acquisition. In: Proceedings of the 15th Annual Conference of
the Cognitive Science Society, Hillsdale: Lawrence Erlbaum Associates, Inc. (1993)
1017–1022
11. Chi, M.T.H., Bassok, M., Lewis, M.W., Reimann, P., Glaser, R.: Self-explanations:
How students study and use examples in learning to solve problems. Cognitive
Science 13 (1989) 145–182
12. Chi, M.T.H., VanLehn, K.A.: The content of physics self-explanations. Journal of
the Learning Sciences 1 (1991) 69–105
13. VanLehn, K., Johns, R.M.: Better learners use analogical problem solving sparingly. In Utgoﬀ, P.E., ed.: Machine Learning: Proceedings of the Tenth Annual
Conference, San Mateo, CA, Morgan Kaufmann Publishers (1993) 338–345
14. Dufresne, R.J., Gerace, W.J., Hardiman, P.T., Mestre, J.P.: Constraining novices
to perform expertlike problem analyses: Eﬀects on schema acquisition. Journal of
the Learning Sciences 2 (1992) 307–331
15. Kalyuga, S., Ayres, P., Chandler, P., Sweller, J.: The expertise reversal eﬀect.
Educational Psychologist 38 (2003) 23–31
16. Murray, R.C., VanLehn, K.: Dt tutor: A decision-theoretic, dynamic approach
for optimal selection of tutorial actions. In Gauthier, Frasson, VanLehn, eds.:
Intelligent Tutoring Systems: 5th International Conference. Volume 1839 of Lecture
Notes in Computer Science., Montreal, Canada, Berlin: Springer (2000) 153–162

A

Appendix

– For examples of the training problems; the annotated, worked-out examples;
and the post-test problems, visit http://www.pitt.edu/~mringenb/AWOE/.
– For more information about Andes, visit http://www.andes.pitt.edu/.
– For more information about LearnLab, visit http://www.learnlab.org/.

The Advantages of Explicitly Representing Problem
Spaces
Kurt VanLehn
Computer Science Department, University of Pittsburgh, USA
http://www.pitt.edu/~vanlehn/

Newell and Simon (1972) coined the term "problem space" for a virtual
structure: all possible lines of reasoning that can be employed by an agent to
solve a problem. For certain toy problems (e.g., Tower of Hanoi), the problem
space can be represented explicitly as labeled, directed graph. For non-toy
problems, cognitive scientists have sometimes employed the concept of a
problem space to analyze tasks, but seem to feel that explicit representation of
the whole problem space for a problem is probably not worthwhile, and perhaps
not even feasible. I will present techniques developed over a decade of research
that make explicit representation of large problem spaces feasible. I will
demonstrate how explicit representations of problem spaces have been used in
systems that do non-trivial user modeling, task analysis, intelligent tutoring, and
natural language dialogues.

P. Brusilovsky et al. (Eds.): UM 2003, LNAI 2702, p. 3, 2003.
© Springer-Verlag Berlin Heidelberg 2003

Bi-directional Search for Bugs: A Tool for Accelerating
Knowledge Acquisition for Equation-Based Tutoring
Systems
Sung-Young Jung and Kurt VanLehn
Intelligent System Program
University of Pittsburgh
{chopin,vanlehn}@cs.pitt.edu

Abstract. Authoring the knowledge base for an intelligent tutoring system
(ITS) is difficult and time consuming. In many ITS, the knowledge base is used
for solving problems, so authoring it is an instance of the notoriously difficult
knowledge acquisition problem of expert systems. General tools for knowledge
acquisition have shown only limited success, which suggests developing tools
that apply only to specific kinds of knowledge bases. Pyrenees is an ITS whose
knowledge base is composed mostly of conditioned equations. We have developed several tools for authoring Pyrenees knowledge bases. This paper focuses
on a novel and particularly powerful tool that uses bidirectional search to locate
bugs in the knowledge base. In several evaluations, human authoring was significantly faster when the tool was available than when it was unavailable
Keywords: Bi-directional search, authoring tools, automatic error detection on
knowledge.

1 Introduction
ITS can be classified depending on how their inner loops represent domain knowledge.
One classification, which is the focus of this paper, is tutors whose domain knowledge
solves the same problems that the students does and thus “model” the desired ways to
solve them. These ITS are sometimes called model-tracing tutors, although that term is
often taken to denote the particular technology used by CMU tutors [8] and Carnegie
Learning (http://www.carnegielearning.com/).
In this paper, we describe the authoring tools used with Pyrenees [10] [11]. Pyrenees
uses a large set of knowledge components, called principles, to solve a problem. However, what makes Pyrenees unusual is that most of its principles are conditioned equations. That is, such a principle asserts that under certain conditions, a certain equation
is true. This allows a sophisticated error detection method to be used to located buggy
principles.
In Pyrenees, Each principle is represented by a condition and an equation, where the
condition indicates when the equation holds (Fig. 1). A set of problems, like the principles, are expressed in terms of the ontology. Fig. 2 illustrates a problem named “isobaric
expansion of water” which would be stated in English as “A reservoir contains a liquid,
B. Woolf et al. (Eds.): ITS 2008, LNCS 5091, pp. 758–762, 2008.
© Springer-Verlag Berlin Heidelberg 2008

Bi-directional Search for Bugs

759

called water, of mass 0.001 kg and heat capacity 4,196 J/(kg*C). The pressure is held
constant at 200,000 Pa. As the water is heated, it increases its volume by 0.000001 m^3
and its temperature by 31 C. What is the work done during this time?”
Pa_true(isobaric_expansion(Gas, T) ):gas(Gas),
time_interval(T),
constant(var(at(pressure(Gas),
T)).

Pa_equation(isobaric_expansion(Gas,
T),
W=P*Vdiff):W=var(at(work_done_by(Gas), T)),
P=var(at(pressure(Gas), T)),
Vdiff=var(at(diff(volume(Gas), T))).

Fig. 1. Predicates defining a condition (pa_true) and equation (pa_equation)
p_definition(isobaric_expansion_of_water,
[substance(liquid),
reservoir(liquid, _, _),
known(var(mass(liquid)), dnum(0.001, kg)),
%heat capacity
known(var(heat_capacity(liquid)), dnum(4196, 'J/(kg*C)')),
known(var(pressure(liquid)), dnum(200000, 'Pa')),
known(var(diff(volume(liquid))), dnum(0.00000001, 'm^3')),
known(var(diff(temperature(liquid))), dnum(31, 'C')),
sought(var(work(liquid)), dnum(0.002, 'J'))
]).

%answer; 0.002

Fig. 2. An example of domain problem

Pyrenees solves problems via a version of backward chaining called the Target
Variable Strategy [10] [11]. The basic idea is simple: Given a sought quantity, generate an equation that applies to this problem and contains the sought quantity. Include
it in the set of equations that comprise the solution. If the equation has any quantities
in it that are neither known nor previously sought, then treat them as sought and recur.
When the Target Variable Strategy stops, it has generated a set of equations that are
guaranteed to be solvable.

2 Error Detection Using Bidirectional Search
Whenever we add a new problem, we call the problem solver to see if it can be solved.
The most frequent sign of a bug is that a problem cannot be solved. This occurs when
some principle that should have applied did not get applied. One heuristic to try to find
the spot where the principle should have applied is to focus on the dead ends. A dead end
is a sought quantity that cannot be calculated from the known quantities. It is likely, but
not certain, that a principle should be applied to the dead end, but it failed to apply because the principle was buggy. Although one could examine the dead ends of the tree by
hand, there can be hundreds of them.
Bugs that prevent a principle from applying can appear in 3 locations. The bug could
be in (1) the principle’s condition or (2) in the problem statement. Pyrenees also has a
few knowledge components that are now conditioned equations, but instead draw inferences that bridge between a principles condition and the problem statements. (3) Bugs in

760

S.-Y. Jung and K. VanLehn

these rules can also prevent a principle from applying. Fig. 3 illustrates 3 different kinds
of errors. These errors are common, hard to notice by inspection and will block a principle from applying.
The bi-directional tool starts by creating all possible forward chaining trees. That is, it
generates all applicable equations, then use them repeatedly to generate values for all
possible quantities. None of the dead end quantities will be among these quantities with
known values, because otherwise they would not be dead ends.
pa_true(isobaric_expanson(Gas, T) ):gas(Gas),
time_interval(T),
constant(var(at(pressure(Gas)), T)).
constant(Quantity):known(Quantity, _),
\+( Quantity=at(_, T)
; time_interval(T) ).

% Typo. Should be “expansion”

% Wrong parenthesis. Should be …Gas), T)))

% OR (;) should be AND (,)

Fig. 3. Examples of bugs

However, a dead end may be “one equation away” from the known quantities.
Thus, given a dead end quantity, we search for a principle such that one (or more) of
its variable specifications unifies with a dead end quantity (W in Fig. 4) and all of the
remaining variable specifications unify with known quantities (P and Vdiff in Fig. 4).
If we find such a principle, then we know that if it had applied, the dead end would
not exist. Thus, a bug must be preventing the principle’s condition from unifying with
the problems’ description.

known
known
known

Var P
Principle
sought

Var W

isobaric_
expansion

Solution tree

Var Vdiff

known

Forward chaining trees

Fig. 4. Bi-directional search identifies a principle whose condition isn’t satisfied

3 Evaluation and Conclusions
In order to evaluate the effects of using the bi-directional tool, two kinds of experiments were performed. The task domain was thermodynamics, and it included 14
problems and 15 conditioned-equation principles. The lead author of this paper was
the debugger.

Bi-directional Search for Bugs

700

Average time spent per a test

900
800
700

With the tool
Without the tool

600
500

600

400

500

300

400

120

Average time spent per a test
Error detection succeeded
Error detection failed
without the tool

80

200

300

0

100
0

With the tool

(a)

Without the tool

60

40

100

200

Thermodynamics + sound and wave Domain

100

m inutes

800

tim e s p e n t (s e c o n d s )

tim e spen t (seconds)

900

761

20

Error detection Error detection without the tool
succeeded
failed

(b)

0
with the tool

without the tool

(c)

Fig. 5. The result of experiments

Fig. 5-(a) and (b) shows the results for error detection tests on automatically generated errors. The average time spent per a test with the tool was shorter than without
the tool (281 vs. 634 seconds, p = 0.00008). Fig. 5-(b) shows debugging time using
the tool either the cases when error detection succeeded or failed. The average time
spent when error detection succeeded was shorter than when failed (151 vs. 474 seconds). When error detection failed, the time spent didn’t show significant difference
from the time without using the tool (p = 0.35).
For the authoring test, the number of domain problems was 13 for each domain (total 26). Thermodynamics and sound/wave domains were implemented in each of two
passes. Fig. 5-(c) shows the result of knowledge adding tests using the alternation
scheme. Authoring the tool was significantly faster than without using the tool (58.38
minutes vs. 90.36 minutes, p =0.017).
Although Pyrenees already had state-of-the-art tools for authoring, including a
visualization tool, ontology-based checking of principle semantics and regression
testing, we found that a new tool, based on bi-directional search, significantly accelerated authoring. When a problem cannot be solved, the tool searches through the dead
ends in the tree generated by the normal, backward chaining problem solver used by
Pyrenees. If it can find a dead end that is “one principle away” from a known quantities developed by forward chaining, then it is likely that this principle should have
applied but did not because its condition failed to match the problem’s description.
This localizes the bug, which makes it much easier to spot. Evaluations indicated that
the tool saved time, and the difference was statistically reliable.

References
1. Murray, T.: Authoring Intelligent Tutoring Systems: An Analysis of the State of the Art.
International journal of Artificial Intelligence in Education 10, 98–129 (1999)
2. Gil, Y., Melz, E.: Explicit Representations of Problem-Solving Strategies to Support
Knowledge Acquisition, ISI Technical Report ISI/RR-96-436 (1996)
3. Yost, et al.: Acquiring Knowledge in Soar. IEEE EXPERT (1993)
4. Noy, N.F., Grosso, W.E., Musen, M.A.: Knowledge-Acquisition Interfaces for Domain
Experts: An Empirical Evaluation of Protege-2000. In: Twelfth International Conference
on Software Engineering and Knowledge Engineering (SEKE 2000), Chicago, IL (2000)
5. Tallis, M., Kim, J., Gil, Y.: User studies of knowledge acquisition tools: methodology and
lessons learned. J. Expt. Theor. Artif. Intell. 13, 359–378 (2001)

762

S.-Y. Jung and K. VanLehn

6. Gil, Y., Kim, J.: Interactive Knowledge Acquisition Tools: A Tutoring Perspective. In:
Proceedings of the 24th Annual Meeting of the Cognitive Science Society (CogSci), Fairfax, VA, August 8-10 (2002)
7. Turner, T.E., Koedinger, K., et al.: The Assistment Builder: An Analysis of ITS Content
Creation Lifecycle. In: The 19th International FLAIRS Conference, May 11-13 (2006)
8. Anderson, J., Skwarecki, E.: The Automated Tutoring of Introductory Computer Programming. Communications of the ACM 29(9), 842–849 (1986)
9. Aleven, V., McLaren, B., Sewall, J., Koedinger, K.R.: The Cognitive Tutor Authoring
Tools (CTAT): Preliminary evaluation of efficiency gains. In: Ikeda, M., Ashley, K., Chan,
T.-W. (eds.) ITS 2006. LNCS, vol. 4053, pp. 61–70. Springer, Heidelberg (2006)
10. Chi, M., VanLehn, K.: Accelerated Future Learning via Explicit Instruction of a Problem
Solving Strategy. In: Koedinger, K.R., Luckin, R., Greer, J. (eds.) Artificial Intelligence in
Education, pp. 409–416. IOS Press, Amsterdam (2007)
11. Koedinger, K.R., Aleven, V., Heffernan, N.T., McLaren, B., Hockenberry, M.: Opening
the door to non-programmers: Authoring intelligent tutor behavior by demonstration. In:
Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.) ITS 2004. LNCS, vol. 3220, pp. 162–174.
Springer, Heidelberg (2004)
12. VanLehn, K., Bhembe, D., Chi, M., Lynch, C., Schulze, K., Shelby, R., et al.: Implicit vs.
explicit learning of strategies in a non-procedural skill. In: Lester, J.C., Vicari, R.M., Paraguaca, F. (eds.) Intelligent Tutoring Systems: 7th International Conference, pp. 521–530.
Springer, Berlin (2004)

Inducing Eﬀective Pedagogical Strategies Using
Learning Context Features
Min Chi1 , Kurt VanLehn2 , Diane Litman3 , and Pamela Jordan4
1
2

Machine Learning Department, Carnegie Mellon University, PA, 15213 USA
minchi@cs.cmu.edu
School of Computing and Informatics, Arizona State University, AZ, 85287 USA
Kurt.Vanlehn@asu.edu
3
Department of Computer Science, University of Pittsburgh, PA, 15260 USA
litman@cs.pitt.edu
4
Department of Biomedical Informatics, University of Pittsburgh,
Pittsburgh, PA 15260
pjordan@pitt.edu

Abstract. Eﬀective pedagogical strategies are important for e-learning
environments. While it is assumed that an eﬀective learning environment should craft and adapt its actions to the user’s needs, it is often
not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning
(RL) to induce pedagogical strategies directly from pre-existing human
user interaction corpora. 50 features were explored to model the learning
context. Of these features, domain-oriented and system performance features were the most inﬂuential while user performance and background
features were rarely selected. The induced pedagogical strategies were
then evaluated on real users and results were compared with pre-existing
human user interaction corpora. Overall, our results show that RL is a
feasible approach to induce eﬀective, adaptive pedagogical strategies by
using a relatively small training corpus. Moreover, we believe that our
approach can be used to develop other adaptive and personalized learning environments.

1

Introduction

Natural Language (NL) Tutoring Systems are a form of Intelligent Tutoring Systems (ITSs) that use natural dialogue for instructional purposes such as helping
students to learn a subject by engaging in a natural language conversation.
Why2-Atlas and Why2-AutoTutor [1], for example, are NL tutoring systems
that teach students conceptual physics. One central component of NL Tutoring
Systems is the dialogue manager, which uses dialogue strategies to decide what
action to take at each point during the tutorial dialogue. For tutoring systems,
dialogue strategies are also referred to as pedagogical strategies.
It is commonly believed that an eﬀective tutoring system would craft and
adapt its actions to the students’ needs based upon their current knowledge
P. De Bra, A. Kobsa, and D. Chin (Eds.): UMAP 2010, LNCS 6075, pp. 147–158, 2010.
c Springer-Verlag Berlin Heidelberg 2010


148

M. Chi et al.

level, general aptitude, and other salient features [2]. However, most pedagogical strategies for ITSs are encoded as hand-coded rules that seek to implement
cognitive and/or pedagogical theories. Typically, the theories are considerably
more general than the speciﬁc decisions that designers must make, which makes
it diﬃcult to tell if a speciﬁc pedagogical strategy is consistent with the theory.
Moreover, it is often not easy to empirically evaluate these decisions because the
overall eﬀectiveness of the system depends on many factors, such as the usability of the system, how easily the dialogues are understood, and so on. Ideally,
several versions of a system are created, each employing a diﬀerent pedagogical
strategy. Data is then collected with human subjects interacting with these different versions of the system and the results are compared. Due to the high cost
of experiments, only a handful of strategies are typically explored. Yet, many
such other reasonable ones are still possible.
In recent years, work on the design of NL non-tutoring Dialogue Systems
has involved an increasing number of data-driven methodologies. Among these,
Reinforcement Learning (RL) has been widely applied [3]. RL is a machine learning method that centers on the maximization of expected rewards. It has many
features well-suited to the problem of designing the dialogue manager such as
unobservable states, delayed rewards, and so on. Its primary advantage is its
ability to compute an optimal policy within a much larger search space, using
a relatively small training corpus. In this work, rather than implementing pedagogical strategies drawn from human experts or theories, we applied RL to derive
pedagogical strategies using pre-existing interactivity data.
While most previous work on using RL to train non-tutoring dialogue systems
has been successful [3], whether it can be used to improve the eﬀectiveness of NL
tutoring systems is still an open question. One major source of uncertainty comes
from the fact that the rewards used in RL are much more delayed in NL tutoring
systems than those in non-tutoring dialogue systems. Much of this work in NL
non-tutoring Dialogue Systems is focused on systems that obtain information
or search databases such as querying bus schedules [4]. For example, in nontutoring Systems like the train scheduler, the interaction time is often less than 20
minutes, and the number of interactions within user-dialogue systems is generally
less than 20 turns [3]. In the training corpora reported here, the time is roughly
4-9 hours and the number of interactions is about 280 turns. More immediate
rewards are more eﬀective than more delayed rewards for RL induction. This is
because the issue of assigning credit for a decision, attributing responsibility to
the relevant decision is substantially easier in the former case. The more we delay
rewards, the more diﬃcult it becomes to identify the decision(s) responsible for
our success or failure. Additionally, to train an RL model, a large amount of
data is generally needed. In this work, we use human data only instead of data
from simulators as in applying RL in non-tutoring dialogue systems. This is
because the cause of human learning is still an open question and thus it would
be diﬃcult to accurately simulate students’ responses to the tutor and simulate
how students would learn. Given the high cost of collecting human data, we were
more likely to encounter the issue of data sparsity.

Inducing Eﬀective Pedagogical Strategies Using Learning Context Features

149

For RL, as with all machine learning tasks, success is dependent upon an
eﬀective state representation or state model. An eﬀective state representation
should be an accurate and compact model of the learning context. Compared
with non-tutoring Dialogue Systems, where success is primarily a function of
communication eﬃciency, communication eﬃciency is only one of the factors determining whether a student learns well from an NL tutoring system. Moreover,
the other factors are not well understood, so to be conservative, states need to
contain features for anything that is likely to aﬀect learning. Hence, state models
for RL applications to tutoring systems tend to be much larger than state models
for non-tutoring applications. Unfortunately, as states increase in size and complexity, we risk making the learning problem intractable or the decision space
too large to sample eﬀectively. In order to obtain an eﬀective state model that
both minimizes state size while retaining suﬃcient relevant information about
the learning context, we began with a large set of features to which we applied a
series of feature-selection methods in order to reduce them to a tractable subset.
Before describing our approach in detail, we will brieﬂy describe the two types
of tutorial decisions covered by the induced pedagogical policies.

2

Two Types of Tutorial Decisions

Among the many tutorial decisions that must be made, we focus on two types of
decisions, Elicit/Tell (ET) and Justify/Skip-Justify (JS). The ET decision asks
“should the tutor elicit the next problem-solving step from the student, or should
he or she tell the student the next step directly?”. For example, when the next
step is to select a principle to apply and the target principle is the “deﬁnition of
Kinetic Energy’, the tutor can choose to elicit this from the student by asking
the question, “Which principle will help you calculate the rock’s kinetic energy
at T0?” By contrast, the tutor can elect to tell the student the step by stating,
“To calculate the rock’s kinetic energy at T0, let’s apply the deﬁnition of Kinetic
Energy.” The JS decision asks “should the tutor include a justify for a step just
taken or or not”. For example, after deciding to use the “deﬁnition of Kinetic
Energy”, the tutor can choose to ask the student why the principle is applicable
or to skip to ask. There is no widespread consensus on how or when any of these
actions should be taken [5–8]. This is why our research objective is to derive
policies for them from empirical data.

3

Applying RL to Induce Pedagogical Strategies

Previous research on using RL to improve non-tutoring dialogue systems (e.g.
[9]) has typically used Markov Decision Processes (MDPs) [10] to model dialogue
data. The central idea behind this approach is to transform the problem of
inducing eﬀective pedagogical strategies into computing an optimal policy for
an agent that is choosing actions in an MDP. An MDP formally corresponds
to a 4-tuple (S, A, T, R), in which: S = {S1 , · · · , Sn } is a state space; A =
{A1 , · · · , Am } is an action space represented by a set of action variables; T :

150

M. Chi et al.

S × A × S → [0, 1] is a set of transition probabilities P (Sj |Si , Ak ), which is the
probability that the model would transition from state Si to state Sj after the
agent takes action Ak ; R : S × A × S → R assigns rewards to state transitions.
Finally, π : S → A is deﬁned as a policy, which determines which action the
agent should take in each state in order to maximize the expected reward.
The set of possible actions, A, is small and well-deﬁned. In our application,
we have A = {Elicit, T ell} for inducing pedagogical strategies on ET decisions
and A = {Justif y, Skip − Justif y} for inducing those on JS decisions. The set
of possible states, S, however is not well-deﬁned in advance and can potentially
be astronomically large if we include everything that could possibly inﬂuence
the eﬀectiveness of a tutorial action. In this study, we assumed that S is the
Cartesian product of a set of state features F = {F1 , · · · , Fp } and our challenge
now becomes ﬁnding a set of features F to model the state or learning context
compactly and yet eﬀectively. Features must be operational, in that there is
some way to determine their value prior to just before each tutor action in the
dialogue. For instance, one operational feature would be a count of the number
of words uttered by the student since the last tutor turn.
Each student-system interaction dialogue d can be viewed as a trajectory in
the chosen state space determined by the system actions and student responses:
A1 ,R1

A2 ,R2

An ,Rn

S1 −−−−→ S2 −−−−→ · · · Sn −−−−→
Ai ,Ri

Here Si −−−−→ Si+1 indicated that at the ith turn in the tutorial dialogue
d, the system was in state Si , executed action Ai , received reward Ri , and
then transferred into state Si+1 . Because our primary interest is to improve
students’ learning, we used Normalized Learning Gain (NLG) as the reward
because it measures students’ gain irrespective of their incoming competence.
. Here posttest and pretest reThe NLG is deﬁned as: N LG = posttest−pretest
1−pretest
fer to the students’ test scores before and after the training respectively; and 1
is the maximum score. Given that a student’s NLG will not be available until
the entire tutorial dialogue is completed, only terminal dialogue states have nonzero rewards. Thus for a tutorial dialogue d, R1 · · · , Rn−1 are all equal to 0 and
only the ﬁnal reward equal to the student’s N LG × 100, which is in the range
of (-∞, 100].
Once the MDP structure {S, A, R} has been deﬁned, the transition probabilities T are estimated from the training corpus, which is the collection of dialogues,
,m
as: T = {p(Sj |Si , Ak )}k=1,···
i,j=1,··· ,n . More speciﬁcally, p(Sj |Si , Ak ) is calculated by
taking the number of times that the dialogue is in state Si , the tutor took action Ak , and the dialogue was next in state Sj divided by the number of times
the dialogue was in Si and the tutor took Ak . The reliability of these estimates
clearly depends upon the size and structure of the training data. Once a complete MDP is constructed, a dynamic programming approach can be used to
learn the optimal control policy π ∗ and here we used the toolkit developed by
Tetreault and Litman [11]. The rest of this section presents a few critical details
of the process, but many others must be omitted to save space.

Inducing Eﬀective Pedagogical Strategies Using Learning Context Features

3.1

151

Knowledge Component (KC) Based Pedagogical Strategies

In the learning literature, it is commonly assumed that relevant knowledge in
domains such as math and science is structured as a set of independent but
co-occurring Knowledge Components (KCs) and that KC’s are learned independently. A KC is “a generalization of everyday terms like concept, principle, fact,
or skill, and cognitive science terms like schema, production rule, misconception,
or facet” [12]. For the purposes of ITSs, these are the atomic units of knowledge.
The domain selected for this project is a subset of the physics work-energy
domain, which is characterized by eight primary KCs. For instance, one KC is
the deﬁnition of kinetic energy (KE = 12 ∗ m ∗ v 2 ) and another is the deﬁnition
of gravitational potential energy (GP E = m ∗ g ∗ h). It is assumed that a tutorial
dialogue about one KC (e.g., kinetic energy) will have no impact on the student’s
understanding of any other KC (e.g, of potential energy). This is an idealization,
but it has served ITS developers well for many decades, and is a fundamental
assumption of many cognitive models [13, 14].
When dealing with a speciﬁc KC, the expectation is that the tutor’s best
policy for teaching that KC (e.g., when to Elicit vs, when to Tell) would be based
upon the student’s mastery of the KC in question, its intrinsic diﬃculty, and
other relevant, but not necessarily known, factors speciﬁc to that KC. In other
words, an optimal policy for one KC might not be optimal for another. Therefore,
one assumption made in this paper is that inducing pedagogical policies speciﬁc
to each KC would be more eﬀective than inducing an overall KC-general policy.
In order to learn a policy for each KC, we annotated our tutoring dialogues
and action decisions with the KCs covered by each action. For each KC, the
ﬁnal kappa was ≥ 0.77, which is fairly high given the complexity of the task.
Additionally, a domain expert also mapped the pre-/post test problems to the
sets of relevant KCs. This resulted in a KC-speciﬁc NLG score for each student.
Thus, for the decision of when to Elicit vs. Tell about the deﬁnition of kinetic
energy KC20 , we consider all and only the dialogue about that KC and consider
only the learning gains on that KC.
Given these independence assumptions, the overall problem of inducing a
policy for ET decisions and a policy for JS decisions is decomposed into 8 subproblems of each kind, one per KC. Among the eight KCs, KC 1 does not arise in
any JS decisions and thus only an ET policy was induced for it. For each of the
remaining seven KCs, a pairs of policies, one ET policy and one JS policy, were
induced. So we induced 15 KC-based NormGain policies. During the tutoring
process, there were some decision steps that did not involve any of the eight
primary KCs. For them, two KC-general policies, an ET policy and a JS policy,
were induced. To sum, a total of 17 NormGain policies were induced in this
study.
3.2

Training Corpora

In order to apply RL to induce pedagogical strategies and evaluate the induced
strategies, we used Cordillera. Cordillera is a NL tutoring system that teaches

152

M. Chi et al.

introductory physics[12]. To reduce confounds due to imperfect NL understanding, the NL understanding module was replaced with human wizards whose only
task is to match students’ answers to the closest response from a list of potential
responses and they cannot make the tutorial decisions. As the ﬁrst step, we developed an initial version of Cordillera, called random-Cordillera on which both
ET and JS decisions on it were made randomly. 64 college students were then
trained on random-Cordillera in 2007 and the collected training data is called
the Exploratory corpus.
From the Exploratory corpus, we tried our ﬁrst round of policy induction. It
is done by ﬁrst deﬁning 17 state features and then used some sort of greedy-like
procedure to search for a small subset of it as the state representation. For the
reward functions, we had dichotomized the NLGs scores so that there were only
two levels of reward and thus the derived policies were named DichGain policies.
We next tested our hypothesis that these RL-induced policies would improve the
eﬀectiveness of a tutoring system. The version of Cordillera that implemented
the DichGain policies was named DichGain-Cordillera. Except following the policies (random vs. DichGain), the remaining components of Cordillera, including
the GUI interface, the same training problems, and the tutorial scripts, were
left untouched. DichGain-Cordillera’s eﬀectiveness was tested by training a new
group of 37 college students in 2008. Results showed that although the DichGain
policies generated signiﬁcantly diﬀerent patterns of tutorial decisions than the
random policy, no signiﬁcant diﬀerence was found between the two groups on
the pretest, posttest, or the NLGs.
3.3

Inducing NormGain Strategies

Although the previous experiment seemingly failed to conﬁrm our hypothesis,
it did generate more training data. We now have three training corpora: the
Exploratory corpus in 2007, the DichGain corpus in 2008, and a combined training corpus dataset consisting of the 101 dialogues from both the Exploratory
and the DichGain corpora. This time we started with a larger set of possible
state features. We included 50 features based upon six categories of features
considered by previous research [15–17] to be relevant. They include not only
student’s performance and background related features such as student’s overall performance but also domain-oriented and system behavior related features.
Moreover, we explored more domain -general methods of searching the power
set of the 50 features and instead of dichotomizing learning gains as rewards, we
used the N LG × 100 directly. Based on the reward function, the induced policies
are named normalized Gain (NormGain) policies in the following.
Figure 1 shows an example of a learned NormGain policy on KC20 , “Deﬁnition
of Kinetic Enegy”, for JS decisions. The policy involves ﬁve features. They are:
TimeInSession: The total time spent in the current session. This feature reﬂects a student’s fatigue level.
nKCs: The number of times the present KC has occurred in the current dialogue. This feature reﬂects the students’ familiarity with the current KC.

Inducing Eﬀective Pedagogical Strategies Using Learning Context Features

153

pctElicit: The percentage of ET decisions turned out to be elicit during the
dialogue. This feature reﬂects how active a student is overall.
stuAverageWords: The average number of words per student turn. This reﬂects the student’s level of activity and verbosity.
stuAverageConceptSession: The ratio of the number of the student’s turns
which involves at least one physics concept to all the student turns in this
session. This feature reﬂects how often the student’s answers involved at
least one physics concepts since the start of the training.

[Feature:]
TimeInSession:
[0, 3040.80) → 0; [3040.8, ∞] → 1
nKCs:
[0, 66) → 0;
[66, ∞] → 1
pctElicit:
[0, 0.49) → 0;
[0.49, 1) → 1
stuAverageWords:
[0, 4.18) → 0;
[4.18, ∞] → 1
stuAverageConceptSession: [0, 0.29) → 0;
[0.29, 1] → 1
[Policy:]
Justify:
0:0:0:0:0 0:0:1:1:0 0:1:0:0:1 0:0:1:0:0 0:1:0:1:1 0:1:1:0:0 0:1:1:0:1 0:1:1:1:0
0:1:1:1:1 1:0:0:0:0 1:0:0:1:0 1:0:1:0:0 1:0:1:0:1 1:0:1:1:0 1:0:1:1:1 1:1:0:0:1
1:1:1:0:0 1:1:1:0:1 1:1:1:1:0 1:1:1:1:1
Skip-Justify:
0:0:0:0:1 0:0:0:1:0 0:0:0:1:1 0:0:1:0:1 0:0:1:1:1 0:1:0:0:0 0:1:0:1:0 1:0:0:0:1
1:0:0:1:1 1:1:0:0:0 1:1:0:1:0 1:1:0:1:1
Fig. 1. An NormGain Policy on KC20 For JS Decisions

MDP generally requires discrete features and thus all the continous features
need to be discretized. Figure refFig.ExampleNormGainPolicy describes how
each of the ﬁve features was discretized. For example, for TimeInSession, if its
value is above 3040.80 sec (50.68 min), it is 1 otherwise, it is 0. There were a
total of 32 rules learned: in 20 situations the tutor should execute the justiﬁcation
step, in the other 12 situations the tutor should skip. For example, 0:0:0:0:0 is
listed as the ﬁrst situation under the [Justify], it means that when the student
has spend less than 50.68 min in this session, the occurrence of KC20 in the
student’s dialogue history is less than 66, the student has got less than 49%
of elicit in the past, the average number of words in student’s entries is less
than 4.18 words, and the percentage of times times that the student mention a
physics concept in his/her turn is less than 29%, then the tutor should execute
the justiﬁcation. As you can see, the RL induced policies are very subtle and
adaptive to the learning context and they are not like most of the tutorial tactics
derived from analyzing human tutorial dialogues.
The resulting 17 NormGain policies were implemented back into Cordillera
yielding a new version of the system, named NormGain-Cordillera. In order to
test our hypothesis that RL can be used to improve tutoring systems, we tested
the eﬀectiveness of NormGain-Cordillera on a new group of students as described

154

M. Chi et al.

in the next section. The section is written as if one large experiment was done
with 3 conditions, when in fact the 3 groups of students were run sequentially,
as described above.

4

Methods

The purpose of this experiment is to compare the learning gains of students using
random-Cordillera, DichGain-Cordillera and NormGain-Cordillera respectively.
All participants were required to have basic knowledge of high-school algebra,
no experience with college-level physics, and were paid for their time. Each
participant took between six and fourteen hours (3-7 sessions) to ﬁnish the study
in a period of two to three weeks. Each session typically lasted about two hours.
The domain selected here is Physics work-energy domain as covered in a ﬁrstyear college physics course. The eight primary KCs were: the weight law (KC1),
deﬁnition of work (KC14), Deﬁnition of Kinetic Energy (KC20), Gravitational
Potential Energy (KC21), Spring Potential Energy (KC22), Total Mechanical
Energy (KC24), Conservation of Total Mechanical Energy (KC27), and Change
of Total Mechanical Energy (KC28).
All three groups experienced the identical procedure and materials. More
speciﬁcally, participants all completed a background survey; read a textbook
covering the target domain knowledge; took a pretest; solved the same seven
training problems in the same order on Cordillera; and ﬁnally took a posttest.
The pretest and posttest were identical.
Only three salient diﬀerences existed across the three groups:
1. The Exploratory group with a population of 64 was recruited in 2007; the
DichGain group with a population of 37 was recruited in 2008; and the
NormGain group with a population of 29 was recruited in 2009.
2. Random-Cordillera made random decisions and the DichGain-Cordillera and
NormGain-Cordillera followed the induced DichGain and NormGain policies
respectively.
3. A group of six human wizards were used by the Exploratory and DichGain
groups; but only one of six wizards were involved in the NormGain group.
4.1

Grading

All tests were graded by a single experienced grader who did not know which
student belonged to which group. For all identiﬁed relevant KCs in a test question, a KC-based score for each KC application was given. We assigned an overall
competence to a student by the sum of these KC-based scores and normalizing
to a [0,1] interval. We also tried other methods of computing an overall score,
and this did not aﬀect the pattern of results discussed below.

5

Results

The primary goal reported below is twofold: ﬁrst, to test whether our improved
RL methodology and software produced more eﬀective pedagogical strategies

Inducing Eﬀective Pedagogical Strategies Using Learning Context Features

155

than either random policies or the policies used by the DichGain group; and
second, to determine the features selected in the state models in the NormGain
policies.
5.1

Learning Results

A one-way ANOVA showed that there were no signiﬁcant diﬀerences among the
three groups on overall training time: F (2, 122) = 1.831, p = .17. After solving
seven training problems on Cordillera, all three groups scored signiﬁcantly higher
in the posttest than pretest: F (1, 126) = 10.40, p = 0.002 for the Exploratory
group, F (1, 72) = 7.20, p = 0.009 for the DichGain group, and F (1, 56) = 32.62,
p = 0.000 for the NormGain group respectively. The results suggested that the
basic practices and problems, domain exposure, and interactivity of Cordillera
might cause students to learn even from tutors with non-optimal pedagogical
skills.
A one-way ANOVA was used for comparing the learning performance diﬀerences among the three groups. While no signiﬁcant pre-test score diﬀerences were
found: F (2, 127) = 0.53, p = 0.59, there were signiﬁcant diﬀerences among the
three groups on both post-test scores and NLG scores: F (2, 127) = 5.16, p = .007
and F (2, 127) = 7.57, p = 0.001 respectively. Figure 2 compares the three groups
on the pre-test, post-test, and NLG scores. Moreover, a t-test comparison showed
that the NormGain group out-performed the DichGain on both post-test scores
and NLG scores: t (64) = 3.28, p = .002, d 1 = 0.82 and t (64) = 3.68, p =
0.000, d = 0.95 respectively. Similar results were found between the NormGain
and Exploratory groups: t (91) = 2.76, p = .007, d = 0.63 on post-test, and
t (91) = 3.61, p = 0.000, d = 0.84 on NLG scores respectively.
To summarize, the comparison among the three groups shows that the NormGain group signiﬁcantly outperformed both the Exploratory and DichGain
groups. These results were consistent both for the post-test scores and the NLGs
and the eﬀect sizes were large by Cohen’s d criteria.
5.2

Feature Choices in INDUCED POLICIES

Only 30 out of 50 deﬁned features occurred among the 17 NormGain policies.
Among them, the most frequent feature appeared seven times. Four features
appeared in more than three induced policies and they are:
StepDiﬃculty (7 Occurrences): Which encodes a step’s diﬃculty level and
its value is roughly estimated from the Combined Corpus based on the percentage of answers that were correct on the step.
ConceptToWordRatio (5 Occurences): Which represents the ratio of the
physics concepts to words in the tutor’s dialogue.
1

Cohen’s d, which is deﬁned as the mean learning gain of the experimental group
minus the mean learning gain of the control group, divided by the groups’ pooled
standard deviation.

156

M. Chi et al.

Fig. 2. Compare Three Groups Learning Performance under Overall Grading

NumberTellsSinceElicit (5 Occurences): Which represents the number of
tells the student has received since the last elicit.
TimeBetweenDecisions (4 Occurences): Which represents the time since
the last tutorial decision was made on the current KC.
While StepDiﬃculty can be seen as domain-oriented feature, the remaining three
features are all the system-behavior related features. The high occurrence of
StepDiﬃculty in the NormGain policies is not very surprising because it has
been widely believed that diﬃculty level is an important factor for the system to
behave adaptively and eﬀectively. The frequent involvement of System-behavior
related features in the induced policy maybe because these features might reﬂect
student’s general aptitude, the activeness of their knowledge on a speciﬁc KC,
and so on. For example, NumberTellsSinceElicit reﬂects how interactive a student has been recently and TimeBetweenDecisions reﬂect how active a student’s
knowledge on the current KC is. When TimeBetweenDecisions is high, it means
that the tutor has not mentioned the KC recently so the student’s knowledge on
the current KC may be still or forgotten.
Much to our surprise, the features related to the students’ overall or recent performance (e.g., error rate) and background (e.g., MSAT, VSAT, gender, pretest
score) appeared the least or none in the NormGain policies. Although space
does not permit a detailed discussion of the prevalence of features, it appears to
be a mixture of easily anticipated dependencies (e.g., step diﬃculty) and a few
surprises (why doesn’t error rate matter?).

6

Conclusions

We presented a general data-driven method that can be used to improve NL
tutoring system over time. We built and improved a large NL tutoring system

Inducing Eﬀective Pedagogical Strategies Using Learning Context Features

157

using our methodology, and showed that RL is able to eﬀectively search a very
large continous space of dialogue policies (After discretized, the space is ≥ 250 in
size) using a relatively small amount of training dialogue data (64 subjects in Exploratory group and 37 in the DichGain group). A post-hoc comparison showed
that our learned policy outperformed both sets of training policies in terms of
learning performance. This success supports the hypothesis that RL-induced
rules are eﬀective and that the approach taken in this project was a feasible
one. However, inducing eﬀective tutorial tactics was not trivial. The DichGain
tutorial tactics did not seem to be more eﬀective than the random decisions in
Random-Cordillera. A number of factors were changed in deriving NormGain
policies from the process of inducing DichGain policies. These included the feature choices, the choice of training corpora, feature selection methods, and so
on. So it is still not clear which factor or factors caused a change in eﬀectiveness.
Although the discussion of induced features has been cursory, it nonetheless
appears that the learning context features that make the most diﬀerence for
determining when to Tell vs. Elicit and when to Justify vs. Skip-Justify are not
always the ones that one would ﬁrst think of given current theories of learning
and tutoring. For instance, it is widely believed that eﬀective tutors adapt their
behavior to the individual student knowledge level. However, such feature did not
appear in the NormGain policies. Indeed, individualized tutoring is considered
a Grand Challenge by the National Academy of Engineering. However, such
features appeared to play little role in the eﬀective tutorial policies induced from
our data. Overall, our results suggested that when building an accurate learning
context model, adding domain-oriented and the system behavior related features
would be beneﬁcial.
Acknowledgments. NSF (#0325054) supported this work and NSF (#SBE0836012) supported its publication. We thank Collin Lynch and the reviewers
for helpful comments.

References
1. VanLehn, K., Jordan, P.W., Rosé, C.P., Bhembe, D., et al.: The architecture
of why2-atlas: A coach for qualitative physics essay writing. In: Cerri, S.A.,
Gouardéres, G., Paraguaçu, F. (eds.) ITS 2002. LNCS, vol. 2363, pp. 158–167.
Springer, Heidelberg (2002)
2. Chi, M.T.H., Siler, S.A., Jeong, H., Yamauchi, T., Hausmann, R.G.: Learning from
human tutoring. Cognitive Science 25, 471–533 (2001)
3. Singh, S.P., Litman, D.J., Kearns, M.J., Walker, M.A.: Optimizing dialogue management with reinforcement learning: Experiments with the njfun system. J. Artif.
Intell. Res. (JAIR) 16, 105–133 (2002)
4. Raux, A., Langner, B., Bohus, D., Black, A.W., Eskenazi, M.: Let’s go public!
taking a spoken dialog system to the real world. In: Proceedings of Interspeech,
Eurospeech (2005)
5. Collins, A., Brown, J.S., Newman, S.E.: Cognitive apprenticeship: Teaching the
craft of reading, writing and mathematics. In: Resnick, L.B. (ed.) Knowing, learning and instruction: Essays in honor of Robert Glaser, pp. 453–494. Lawrence
Erlbaum Associates, Hillsdale (1989)

158

M. Chi et al.

6. Chi, M.T.H., de Leeuw, N., Chiu, M.H., LaVancher, C.: Eliciting self-explanations
improves understanding. Cognitive Science 18(3), 439–477 (1994)
7. Conati, C., VanLehn, K.: Toward computer-based support of meta-cognitive skills:
a computational framework to coach self-explanation. International Journal of Artiﬁcial Intelligence in Education 11, 398–415 (2000)
8. Katz, S., O’Donnell, G., Kay, H.: An approach to analyzing the role and structure of
reﬂective dialogue. International Journal of Artiﬁcial Intelligence and Education 11,
320–343 (2000)
9. Singh, S.P., Kearns, M.J., Litman, D.J., Walker, M.A.: Reinforcement learning for
spoken dialogue systems. In: Solla, S.A., Leen, T.K., Müller, K.R. (eds.) NIPS,
pp. 956–962. The MIT Press, Cambridge (1999)
10. Sutton, R.S., Barto, A.G.: Reinforcement Learning. MIT Press Bradford Books,
Cambridge (1998)
11. Tetreault, J.R., Litman, D.J.: A reinforcement learning approach to evaluating
state representations in spoken dialogue systems. Speech Communication 50(8-9),
683–696 (2008)
12. VanLehn, K., Jordan, P.W., Litman, D.: Developing pedagogically eﬀective tutorial
dialogue tactics: Experiments and a testbed. In: Proceedings of SLaTE Workshop
on Speech and Language Technology in Education ISCA Tutorial and Research
Workshop (2007)
13. Anderson, J.R.: The architecture of cognition. Harvard University Press,
Cambridge (1983)
14. Newell, A. (ed.): Uniﬁed Theories of Cognition. Harvard University Press,
Cambridge (1994); Reprint edition
15. Moore, J.D., Porayska-Pomsta, K., Varges, S., Zinn, C.: Generating tutorial feedback with aﬀect. In: Barr, V., Markov, Z. (eds.) FLAIRS Conference. AAAI Press,
Menlo Park (2004)
16. Beck, J., Woolf, B.P., Beal, C.R.: Advisor: A machine learning architecture for
intelligent tutor construction. In: AAAI/IAAI, pp. 552–557. AAAI Press / The
MIT Press (2000)
17. Forbes-Riley, K., Litman, D.J., Purandare, A., Rotaru, M., Tetreault, J.R.:
Comparing linguistic features for modeling learning in computer tutoring. In:
Luckin, R., Koedinger, K.R., Greer, J.E. (eds.) AIED. Frontiers in Artiﬁcial Intelligence and Applications, vol. 158, pp. 270–277. IOS Press, Amsterdam (2007)

Using HCI Task Modeling Techniques to Measure How
Deeply Students Model

Sylvie Girard, Lishan Zhang, Yoalli Hidalgo-Pontet, Kurt VanLehn,
Winslow Burleson, Maria Elena Chavez-Echeagary, Javier Gonzalez-Sanchez
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe,
AZ, 85281, U.S.A.

{sylvie.girard, lzhang90, yhidalgo, kurt.vanlehn, winslow.burleson, helenchavez, javiergs}@asu.edu

Abstract: User modeling in AIED has been extended in the past decades to
include affective and motivational aspects of learner’s interaction in intelligent
tutoring systems. An issue in such systems is researchers’ ability to understand
and detect students’ cognitive and meta-cognitive processes while they learn. In
order to study those factors, various detectors have been created that classify
episodes in log data as gaming, high/low effort on task, robust learning, etc.
When simulating students’ learning processes in an ITS, a question remains as
to how to create those detectors, and how reliable their simulation of the user’s
learning processes can be. In this article, we present our method for creating a
detector of shallow modeling practices within a meta-tutor instructional system.
The detector was defined using HCI (human-computer interaction) task modeling as well as a coding scheme defined by human coders from past users’
screen recordings of software use. The detector produced classifications of student behavior that were highly similar to classifications produced by human
coders with a kappa of .925.
Keywords: intelligent tutoring system, shallow learning, robust learning, human-computer interaction, task modeling

1

Introduction

Advances in student modeling in the past two decades enabled the detection of
various cognitive [3, 4, 8, 11, 13, 16, 17], meta-cognitive [1,6], and affective [2, 9]
processes during learning based on classification of episodes in log data. Steps have
been taken toward detecting when learning occurs [4] and to predict how much of the
acquired knowledge students can apply to other situations [5, 6]. However, an obstacle in such research is how to gain an understanding of the user’s cognitive or metacognitive processes while learning. While some of the indicators used in the literature

are common to any intelligent tutoring system, others are closely linked to the activities and pedagogical goals of a specific application. The adaptation of such indicators
to the design of a new system often necessitates a detailed analysis of the new domain
and how the tutoring system guides learners to acquire its skills and knowledge. In
particular, an issue within this process is the ability to reach common ground between
learner scientists that perform an analysis of learners (meta-)cognitive actions at a
high level - via video or log analysis of student’s past actions for example – and the
definition of the indicators by software engineers, related to how the system was implemented, that can be used to simulate such processes in agreement with the constraints and functionalities of software. We view the specificity of detectors as unavoidable, so the best solution is to develop good methods for analyzing the new tutoring system and designing the detectors. This short article describes our method
and its application to out project, AMT. In the AMT project, a choice was made to use
HCI (human computer interaction) task modeling - a method for formally representing human activity, and by extension, the behavior of an interactive system -, as well
as video coding schemes from human coders, to develop the detectors. The detectors
aim to evaluate student’s use of shallow and deep modeling practices with and without being guided by a meta-tutor, on the domain of dynamic systems modeling.
In Section 2, the AMT learning environment, for which the detectors were created,
is introduced. In a third section, the task model of the user’s activity in AMT is described. Next, the process of defining a coding scheme for the detector with human
coders is presented, followed by the definition of the different classifications that
define the value, the implementation and empirical evaluation of the detector. The
final section summarizes the uses of task modeling within this work, and how it could
be applied in future to other applications.

2

AMT software: a meta-tutor to teach deep modeling of
dynamic systems.

AMT software teaches students how to create and test a model of a dynamic system. In our modeling language, a model is a directed graph with one type of link, as
illustrated in Figure 1. Each node represents both a variable and the computation that
determines the variable’s value. There are three types of nodes.
• A fixed value node represents a constant value that is directly specified in the problem. A fixed value node has a diamond shape and never contains incoming links.
• An accumulator node accumulates the values of its inputs. That is, its current
value is the sum of its previous value plus or minus its inputs. An accumulator
node has a rectangular shape and always has at least one incoming link.
• A function node’s value is an algebraic function of its inputs. A function node has
a circular shape and at least one incoming link.

The students’ learning objective is to draw a model representing a situation that is
described in the form of a relatively short text. In the example of Figure 1, the description of the problem was “ Rust destroys steel and can spread quickly. Suppose
you take a large sheet of steel, such as one that might be used as the roof of the boxcar on a train, and you put it outside in the weather. Suppose it starts with a spot of
rust that is 10 square inches in area. However, each week the rust spot gets bigger, as
it grows by 30%. Therefore at the end of the first week, the rust spot is 13 square
inches in area.” and the objective of the problem was to “Graph the size of the rust
spot over 10 weeks.”

Fig. 1. The left image is the example of model, with gray callouts added to explain the
contents of nodes. The right image is the example of a node editor.

The student constructs the model node by node, by filling in all information within
each node in the form of four interactive tabs (description, plan, inputs, and calculations). During construction, students can use the Check button to evaluate the correctness of the current tab, or the Solve it for me button to ask the system to fill out the tab
automatically.
The instruction is divided into three phases: (1) an introduction phase where students learn basic concepts of dynamic system model construction and how to use the
interface; (2) a training phase where students are guided by a tutor and a meta-tutor to
create several models; and (3) a transfer phase where all scaffolding is removed from
soft-ware and students are free to model as they wish. The tutor gives feedback and
corrections on domain mistakes.
The meta-tutor requires students to follow a goal-reduction problem solving strategy, the Target Node Strategy [18]. The basic idea is to focus on one node at a time
(the target node) and completely define it before working on any other node. This
process decomposes the whole problem of modeling a system into a series of atomic
modeling problems, one per node. Like Pyrenees [2], it teaches students that if they
just master this one difficult but small skill, then the rest of the problem solving will
be straight-forward. In addition, the meta-tutor complains if students appear to be
guessing too much or giving up too early, just as the Help Tutor did [3].
While students learn, their motivation, attention to details, and modeling depth can
fluctuate. To assess students, the project needed detectors that detect shallow and
deep modeling practices both with and without the meta-tutor. The measure should be
usable in the transfer phase of the experiment as a dependent variable, because deep

modeling is the skill/knowledge that AMT teaches. The depth measure should also
apply to student’s behavior during the training phase so that we can check whether the
instructional manipulations done during that phase have their intended effects (i.e.,
the measure serves as a manipulation check). The detector should further operate in
real time (i.e., it doesn’t require to know future actions or states in order to interpret
the current action) so that it can be eventually be used by the system itself to condition its behavior.

3

Task Modeling: analysis of user’s actions on software

A task model is a formal representation of the user’s activity. It is represented by a
hierarchical task tree to express all sub-activity that enables the user to perform the
planned activity. The tasks need to be achieved in a specific order, defined in the task
tree by the ordering operators. In AMT, every modeling activity follows the same
procedure involving the same help features, task flow, and meta-tutor interventions.
With a single task model of a prototypical modeling task, it is therefore possible to
account for all of the user’s activity in software. Due to the complexity of the final
model, only one sub-activity will be described in this paper, illustrated in Figure 2.
Only part of the model is deployed in the figure, and some subtasks will not be detailed here. In this part of the model the sub-activity the learner wishes to perform is
to create a new node for the dynamic system s/he is currently modeling. We will first
describe the task tree, and then insert the iterations and conditions that enable a formal
verification of the flow of the task within the task model.
Figure 2: Sub-task “Creating a Node” in the AMT activity task model using K-MADe

Short description of the sub-task to model:
In order for a node to be created, the description tab of the node editor needs to be
completed by selecting a node description, which corresponds to a valid quantity in
the system to model. Each node is unique and cannot be created more than once. The
user can engage in the task only if at least one node still needs to be created for the
model to be complete.
Task tree and order of the tasks:
At the top level of the task tree “Creating a node”, the learner can either attempt to
create the node (task 1) or give up on the creation (task 2). The second task is represented in software by the user closing the node editor window, and can be done at any
time during the task. The task “Creating a node” is over when a good description has
been found and validated. The system can then try to initialize the selection and create
the node.
In the first level of the task “Attempting”, the learner first needs to select a node
description (task 1.1), i.e.: what quantity the node will represent. S/he is then allowed
to finish the creation of the node by validating the selection (task 1.2).
In order to select a node description, the user first needs to choose a node description (task 1.1.1) among the set of node descriptions offered by the system. This process involves the user choosing mentally one description (task 1.1.1.1), exploring the
help features offered by software (task 1.1.1.2) and exploring the set of node descriptions displayed (task 1.1.1.3). S/he can then select the node (task 1.1.2). This subtask
is not described in Figure 1 for a lack of space.
In order to validate the selection, the learner can choose to go back to the description of the problem to verify the correctness of his solution according to the problem
to be simulated (task 1.2.1), and then has to validate the selection (task 1.2.1.2). When
the user checks the validity of the selection, it can either be performed by checking
the solution against the set of nodes still remaining to be modeled (task 1.2.1.2.1) or
asking software to produce the solution (task 1.2.1.2.2). The user is allowed to ask for
the solution only when a description has been checked at least once.
Now that the different actions of the learner are defined, the iterations and conditions will help represent the flow of the activity on the subtask “Selecting a node description” (task 1.1).
Iterative and Optional tasks
• Task 1.1 is iterative: it is possible to make several selections before trying
to finish the description by validating.
• Task 1.1.1.2 is optional: The learner is not forced to explore the help features to choose a description, this is merely a choice on the learner’s part.
• The main task, “creating a node”, is iterative until the node is created or
the activity is abandoned. The later is represented in the task model by an
interruptible task: the learner can stop his/her creation of node activity any
time by choosing to close the node editor window.
Conditions on tasks:
• Main task 1 has a pre-condition attached to it: the software only allows the
user to engage in a creation of a new node if there is at least one node re-

lated to the modeling of the dynamic system that still remains to be created.
A first task model was created to represent learner’s activity on software without
the presence of the meta-tutor. This corresponds to the first version of software, which
was evaluated against the interface including the meta-tutor in [18]. This second software interface includes a text-based agent that intervenes as the students engage in
modeling to help them achieve deeper modeling behaviors, by applying constraints to
the user’s actions and giving meta-cognitive feedback. The meta-tutor was therefore
added to the task model under the type “system” and the model was completed to
include the constraints and interventions of the meta-tutor.
The final task model produced represented all possible actions of the learner on
software in order to model a dynamic system. Next, a study of these actions, which
led to the definition of the depth detectors, is detailed.

4

Detecting when students are modeling using shallow practices

The task model developed with K-MADe was used to define the episode structure.
The first step in creating a coding scheme is to define a unit of measurement for the
user’s modeling actions. The task model clearly highlighted the different subactivities the learner could engage in, referred to as goals. All goals are interruptible
tasks in favor to accessing the help features1 or abandoning the completion of the
current goal for a new one. After a brainstorming session where researchers studied
how students’ actions fell in line with those goals, the following unit of depth, called
“segment”, was defined. This established the unit of coding to be used in the next
phase.
Screen videos representing the learners’ use of the AMT software with and without
the meta-tutor were recorded during an experimental study described in [6]. These
videos were studied to determine how much shallow vs. deep modeling occurred and
the contexts, which tended to produce each type. A coding system was then created
for video recordings of the learners’ behavior. Three iterations of design for this coding scheme were performed, ending with a coding scheme that reached a multi-rater
pairwise kappa of .902. The final coding scheme mapped learners’ behavior to six
classifications, which were implemented as the following depth detectors[AIED short
paper]
• GOOD_METHOD: The students followed a deep method in their modeling. They used the help tools appropriately, including the one for planning
each part of the model.
• VERIFY_INFO: Before checking their step for correctness, students
looked back at the problem description, the information provided by the instruction slides, or the meta-tutor agent.
1

It is to be noted that two help systems are available to users: (1) referring back to the instructions always available for viewing, and (2) looking at the problem situation where all details
of the dynamic system to model are described.

• SINGLE_ANSWER: The student’s initial response for this step was correct, and the student did not change it.
• SEVERAL_ANSWERS: The student made more than one attempt at
completing the step. This includes guessing and gaming the system:
o The user guessed the answer, either by clicking on the correct answer by mistake or luck, or by entering a loop of click and guessing to find
the answer.
o The user “games the system” by using the immediate feedback
given to guess the answer: series of checks on wrong answers that help deduce the right answer.
• UNDO_GOOD_WORK: This action suggests a modeling misconception
on the students’ part. One example is when students try to run the model
when not all of the nodes are fully defined.
• GIVEUP: The student gave up on finding how to do a step and clicked on
the “give up” button.
Another detector was defined as a linear function of the six episode detectors. It
was intended to measure the overall depth of the students’ modeling, therefore providing an outcome measure in the transfer phase in future experimental studies. It considered two measures (GOOD_ANSWER, VERIFY_INFO) to indicate deep modeling, one measure (SINGLE_ANSWER) to be neutral, and three measures
(SEVERAL_ANSWERS, UNDO_GOOD_WORK, and GIVE_UP) to indicate shallow modeling.
Once the coding scheme reached a sufficient level of agreement between coders,
the task model was used to adapt the coding to students’ actions on the software. The
episodes that were coded for depth by human analysts in the sample video were analyzed by creating scenarios from the task model within K-MADe. The validation of
six detectors’ implementation involved three human coders, who watched a sample of
50 episodes, paying attention to the depth of modeling exhibited by the student’s actions, and chose the classification that best represented the depth of the learner modeling at the time of the detected value. A multi-rater and pairwise kappa was then performed, reaching a level of inter-reliance of .925.

5

The different uses of the Task Model

The task modeling language K-MAD and its task model creation and simulation
environment, K-MADe [7] were chosen for the following reasons: the environment
enables the creation and replay of scenarios of student’s actions, a set of functionalities not described here enable a formal verification of the model. Additionally the
associated simulation environment ProtoTask [14] allows non-specialists in task modeling to visualize the flow of the task model, via scenarios in a clear and simple manner.
The use of K-MAD helped in the creation of the detectors and are a first step in offering an alternative technique to simulated learners, by tackling the following problems:

•

Breaching the gap between learner scientists’ understanding of how the
learning process works and programmers’ definition of the application
flow, functionalities, and indicators.
• Enabling a formal validation of software flow, understandable by all.
• Using simulated learners scenarios to define the detectors.
A researcher in educational technology - expert in teaching modeling and part of the
AMT project - and an HCI practitioner, realized the task model. The former was an
expert on how AMT software was designed in terms of pedagogical content and task
flow. His expertise focused in particular on the actions the students were allowed/incited/forbidden to do within software at each moment of the modeling task.
The HCI practitioner was not familiar with intelligent tutoring systems or meta-tutors.
She was involved in the creation of the task model in a consulting capacity, in regards
to her expertise in task modeling of interactive systems.
The task model could be defined at the level of the user’s planning of actions and
system flow, with iterations and conditions alone. However, the objects in K-MADe
enable us to represent the constraints of the learner’s actions concretely and to apply a
formal verification of task flow. It was therefore possible to represent the set of descriptions as either valid or invalid, to detect when a node has been checked and the
result of that check, and to add constraints on the checking procedure such as to avoid
node duplication. This enabled a formal verification of software flow prior to validate
its fidelity to learner scientists’ ideas about possible actions on software and the underlying processes involved.
Once the model was constructed, the use of ProtoTask to visualize software flow
and follow learners’ possible sets of actions allowed by software enabled the ability to
simulate learners by creating scenarios of use that could be played and replayed at
will, focusing on the cognitive and meta-cognitive levels of learner’s experience on
software. In the process of creating our detectors, a video analysis of learner’s past
actions was performed. The model could be used to check the possible actions of
users with what the designer of the system wanted to offer as functionalities and software flow. During this analysis, the task model could be used once again to define
scenarios that simulated learner’s pertinent behaviors using ProtoTask. Once those
scenarios were formed, the task analyst came back to the original K-MAD modeling
language and studied the similarities and contrasts between scenarios to define the
rules that govern the detection of shallow and deep modeling practices within AMT.
Once the task model identified points of detection of such practices, it became easy
for programmers to go back to software and implement the rules.

6

Conclusion and Future Work

In this paper, a method to create a detector of deep modeling within a meta-tutor
using HCI task modeling and video coding schemes was described. The main outcome of this process was the creation of detectors inferring the depth of students’
modeling practices while they learn on a meta-tutoring system, reaching a multi-rater
and pairwise kappa score of .925. We believe the use of the task model to define shal-

low and deep modeling practices by helping to create the detectors to be of value for
any simulated learning environments, in particular for indicators that a common to all
learning tasks present in a tutoring system.
In interdisciplinary teams, the design of indicators can lead to communication issues due to misunderstandings and a lack of common ground between analysis made
at a high level of learners’ cognitive and meta-cognitive processes, and the representation of those behaviors within software. In particular, video-coding processes can
become costly when the coders’ understanding of the details of how the system works
differs from how the system actually works. Our experience using K-MADe and ProtoTask highlighted an ease in this project in gaining a better view of the tutoring system and the detection of deep modeling within the interface. In particular, the use of
ProtoTask by the non-specialists in task modeling helped clarify issues of task flow
and the definition of the set of user’s actions at each moment of interaction.
A limitation of the method is the applicability to different types of tutoring systems. In AMT, a single task model was able to represent the entirety of a users’ learning activity. In tutoring systems that teach a set of skills through different pedagogical
approaches for diverse types of learning tasks, the creation of such task models might
prove more costly and may not be completely adapted to the creation of detectors that
need to be adapted to each task specifically.

Acknowledgements
This material is based upon work supported by the National Science Foundation
under Grant No. 0910221. We would like to thank Sybille Caffiau for consulting in
the project and sharing her expertise in task modeling of interactive systems.

References
1. Aleven, V., McLaren, B.M., Roll, I., Koedinger, K.R.(2006): Toward meta-cognitive tutoring: A model of help seeking with a Cognitive Tutor. International Journal of Artificial Intelligence and Education 16, 101–128
2. Arroyo, I., and Woolf, B.P., 2005. Inferring learning and attitudes from a Bayesian Network of log file data. In Proceedings of the 2005 conference on Artificial Intelligence in
Education: Supporting Learning through Intelligent and Socially Informed Technology,
Chee-Kit Looi, Gord McCalla, Bert Bredeweg, and Joost Breuker (Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 33-40.
3. Baker, R. S. J. d., Corbett, A. T., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., …
Beck, J. E. (2006). Adapting to when students game an intelligent tutoring system, Proceedings of the 8th international conference on Intelligent Tutoring Systems, Jhongli, Taiwan Berlin, Heidelberg.
4. Baker, R.S.J.d., Goldstein, A.B., Heffernan, N.T.: Detecting the Moment of Learning. In:
Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010. LNCS, vol. 6094, pp. 25–34. Springer,
Heidelberg (2010)
5. Baker, R. S. J. D., Gowda, S. M., & Corbett, A. T. (2011). Towards predicting future
transfer of learning, Proceedings of the 15th international conference on Artificial intelli-

6.

7.

8.

9.

10.

11.

12.

13.
14.

15.

16.

17.

18.

gence in education. Proceedings from AIED’11, Auckland, New Zealand Berlin, Heidelberg.
Baker, R. S. J. D., Gowda, S. M., Corbett, A. T., & Ocumpaugh, J. (2012). Towards automatically detecting whether student learning is shallow., Proceedings of the 11th international conference on Intelligent Tutoring Systems, Chania, Crete, Greece Berlin, Heidelberg.
Caffiau, S., Scapin, D., Girard, P., Baron, M., & Jambon, F. (2010). Increasing the expressive power of task analysis: Systematic comparison and empirical assessment of toolsupported
task
models.
Interacting
with
Computers,
22(6),
569–593.
doi:10.1016/j.intcom.2010.06.003
Corbett, A.T., MacLaren, B., Kauffman, L., Wagner, A., Jones, E.A.: Cognitive Tutor for
Genetics Problem Solving: Learning Gains and Student Modeling. Journal of Educational
Computing Research 42(2), 219–239 (2010)
D’Mello, S. K., Lehman, B., & Person, N. (2010). Monitoring affect states during effortful
problem solving activities. International Journal of Artificial Intelligence in Education,
20(4), 361–389., doi:10.3233/JAI-2010-012
Girard, S., Chavez-Echeagary, H., Gonzalez-Sanchez, J., Hildalgo-Pontet, Y., Zhang, L.,
Burleson, W., and VanLehn, K., (2013), Defining the behavior of an affective learning
companion in the affective meta-tutor project, in K. Yacef et al. (Eds.): Proceedings of the
16th international conference on Artificial Intelligence in EDucation (AIED’13), LNAI
7926, pp. 21--30. Springer-Verlag, Berlin, Heidelberg.
Gowda, S.M., Pardos, Z.A., and Baker, R. S. J. D. 2012. Content learning analysis using
the moment-by-moment learning detector. In Proceedings of the 11th international conference on Intelligent Tutoring Systems (ITS'12), Stefano A. Cerri, William J. Clancey, Giorgos Papadourakis, and Kitty Panourgia (Eds.). Springer-Verlag, Berlin, Heidelberg, 434443. DOI=10.1007/978-3-642-30950-2_56
Koedinger, K.R., Corbett, A.T., Perfetti, C. (2010): The Knowledge-Learning-Instruction
(KLI) Framework: Toward Bridging the Science-Practice Chasm to Enhance Robust Student Learning. Carnegie Mellon University Technical Report, June, 2010
Martin, J., VanLehn, K.: Student assessment using Bayesian nets. International Journal of
Human-Computer Studies 42, 575–591 (1995)
Lachaume, T., Girard, P., Guittet, L., & Fousse, A. (2012). ProtoTask, new task model
simulator. In M. Winckler, P. Forbrig, & R. Bernhaupt (Eds.), Human-Centered Software
Engineering (Vol. 7623, pp. 323– 330). Berlin, Heidelberg: Springer Berlin Heidelberg.
doi:10.1007/978-3-642-34347-6
Muldner, K., Burleson, W., Van, D. S., Brett, & Vanlehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, April 2011, 21(1-2), 99–135m
doi:10.1007/s11257-010-9086-0
Shih, B., Koedinger, K.R., Scheines, R.: A response time model for bottom-out hints as
worked examples. In: Proceedings of the 1st International Conference on Educational Data
Mining, pp. 117–126 (2008)
Walonoski, J. A., & Heffernan, N. T. (2006). Prevention of off-task gaming behavior in intelligent tutoring systems Proceedings of the 8th international conference on Intelligent
Tutoring Systems. Jhongli, Taiwan Berlin, Heidelberg.
Zhang, L., Burleson, W., Chavez-Echeagary, H., Girard, S., Gonzalez-Sanchez, J., Hildalgo-Pontet, Y., and VanLehn, K., (2013), Evaluation of a meta-tutor for constructing
models of dynamic systems, in K. Yacef et al. (Eds.): AIED’13, LNAI 7926, pp. 666--669.
Springer-Verlag, Berlin, Heidelberg.

The Interaction Plateau: Answer-Based Tutoring < StepBased Tutoring = Natural Tutoring
Kurt VanLehn
LRDC, University of Pittsburgh, Pittsburgh, PA 15260 USA
VanLehn@cs.pitt.edu

Face-to-face tutoring by an expert human tutor is widely thought to be more effective
than intelligent tutoring systems (ITS), which are in turn thought to be more effective
than computer-aided instruction (CAI), computer-based training (CBT), etc. The latter
tutoring systems have students work out complex solutions on paper, then enter their
answer into the tutor, which gives them feedback and hints on their answer. Thus,
CAI, CBT, etc. are answer-based tutoring systems. This is a low level of interactivity,
in that the student may make many inferences between the time they start the problem
and when they first get feedback on their thinking. With a typical ITS, such as the
Andes physics tutoring system, students enter every step of a complex solution into
the tutor, which gives them feedback and hints, either immediately or when they have
finished entering all the steps. These systems are step-based tutoring systems, because
the feedback and hints are directed at steps rather than the final answer. They are
moderately interactive, because students make a moderate number of inferences per
step. When interacting face-to-face with a human tutor, students often talk aloud as
they reason, and thus allow the tutor to hear and intervene at almost every inference
made by the student. Thus, human tutoring is highly interactive. Natural language
tutoring systems, such as Why2-Atlas and Cordillera, are engineered to act like human tutors, so they too are highly interactive. If we use “natural tutoring” to cover
both human tutoring and natural language tutoring, then the three types of tutoring can
be ordered:
answer-based tutoring < step-based tutoring < natural tutoring
This certainly holds for their degree of interactivity, as just argued. This is also
thought to be the ordering for their learning effectiveness. Moreover, it is sometimes
thought that higher interactivity affords or perhaps even causes higher learning gains.
This talk will debunk that myth. In particular, experiments with human and computer tutors usually find that learning gains are ordered this way:
answer-based tutoring < step-based tutoring =natural tutoring
Increasing interactivity beyond the step level appears to neither afford nor cause
higher learning gains.

B. Woolf et al. (Eds.): ITS 2008, LNCS 5091, p. 7, 2008.
© Springer-Verlag Berlin Heidelberg 2008

Bridging Principles and Examples through Analogy and Explanation
Timothy J. Nokes, Kurt VanLehn, Learning Research and Development Center
University of Pittsburgh, Pittsburgh, PA 15260
Email: nokes@pitt.edu, vanlehn@cs.pitt.edu
Abstract: Previous research in cognitive science has shown that analogical comparison and
self-explanation are two powerful learning activities that can improve conceptual learning in
laboratory settings. The current work examines whether these results generalize to students
learning physics in a classroom setting. Students were randomly assigned to one of three
worked example learning conditions (reading, self-explanation, or analogical comparison) and
then took a test assessing conceptual understanding and problem solving transfer. Students in
the self-explanation and analogy conditions showed improved conceptual understanding
compared to students in the more traditional worked example condition.

Introduction
What learning activities lead to a deep understanding of new concepts and support problem solving
transfer? One approach to addressing this question is to examine what knowledge comprises ‘expert
understanding’ and then design learning environments to help novices construct that knowledge. Research on
expertise suggests that a key aspect of expert knowledge is understanding how the domain principles are
instantiated in the problem features (Chi, Feltovich, & Glaser, 1981). The purpose of the current work was to
design learning activities based on cognitive science principles to help students acquire this knowledge and
improve their conceptual understanding.
Two learning paths that have been hypothesized to facilitate deep learning are self-explanation and
analogical comparison. Self-explanation has been shown to facilitate both procedural and conceptual learning
and transfer of that knowledge to new contexts (Chi, 2000). Of particular interest to the current work are some
promising results from the Chi, Bassok, Lewis, Reimann, & Glaser (1989) laboratory study showing that good
learners were more likely than poor learners to generate inferences relating worked examples to the principles
and concepts of the problem. This result suggests that prompting students to self-explain the relations between
principles and worked examples will further facilitate learning and conceptual understanding. Prior laboratory
work has also shown that analogical comparison can facilitate schema abstraction and transfer (e.g., Gentner,
Lowenstein, & Thompson, 2003; see Gentner, Holyoak, & Kokinov, 2001 for a general overview). However,
this work has not examined how learning from problem comparison impacts understanding of how abstract
principles relate to the problem features. The current work examines how self-explanation and analogical
comparison may help bridge students’ learning of the relations between principles and examples.

Experiment
The purpose of this experiment was to test whether self-explanation and analogical comparison of
worked examples facilitates conceptual learning and problem solving transfer for students learning rotational
kinematics in a classroom environment.

Methods
Seventy-eight students from the United States Naval Academy (USNA) participated as a part of their
normal course work. The students participated from 4 sections of the introductory physics course taught at the
USNA. The number of students in each section ranged between 18-26 students and the experiment took place
during their normally scheduled lab time.
A between-subjects design was used with students randomly assigned to one of three learning
conditions: reading (n = 26), self-explanation (n = 26), and analogy (n = 26). The lab session was divided into a
learning and test phase.
Learning Phase. The learning materials were presented in paper booklets. All participants received a
principle booklet that gave an introduction to the principles and concepts of rotational motion (e.g., angular
displacement, angular velocity, angular acceleration, etc.). The principles each had written descriptions, graphic
illustrations, and formulae. Students were given 9 minutes to read through the introductory booklet and had
access to it through the entire learning phase. Participants were then randomly assigned to one of the three
learning conditions (reading, self-explanation, and analogical comparison). Each condition received a learning
booklet.
The booklet for the reading condition consisted of six worked-out examples (word problems with the
step-by-step solution) that included detailed explanations and principle justifications for each solution step. Part

of the solution to the worked example was left blank for the participants to fill in. The problems were presented
sequentially and the participants’ task was to read aloud the problem filling in the blanks as they went. They
were then given the solutions to the fill-in-the-blanks and repeated this procedure for the second worked
example. Next, they solved two isomorphic practice problems (one more problem than the other two learning
conditions to control for time on task). This procedure was then repeated for the remaining two sets of worked
examples. The learning booklet for the explanation condition consisted of the same six worked examples.
However, the participants in this condition were not given the explanations right away but were first instructed
to try and generate the explanation and principle justification for each step. After generating their explanations
they read through the ‘expert’ explanations (the same ones given to the reading group). After generating and
reading through the explanations for the two worked examples they solved one isomorphic practice problem.
They then repeated this procedure for the remaining two sets of worked examples. The booklet for the analogy
condition used the same worked examples and explanations as the reading booklet without the fill-in-the-blanks.
After reading the worked examples they were asked to compare and contrast the two examples writing out the
similarities and differences between them. These compare and contrast questions were designed to focus the
learner on various aspects of the underlying concepts. They then solved one practice problem and repeated this
procedure for the remaining two sets of problems. The total training time was 55 minutes.
Test Phase. After the learning phase all participants were given a test to assess their understanding of
the concepts. The test booklets consisted of three sections including a multiple-choice test (13 questions) and
two problem solving tasks. The multiple choice test assessed qualitative reasoning and conceptual
understanding. The problem solving tasks assessed application of the concepts in new contexts. The first
problem was to apply the concepts in a slightly different way than they had practiced during the learning phase
(a different set of steps) and in a new context (cover story). The second problem was similar in structure to one
of the worked examples however it included some new irrelevant information (extraneous values) and a new
context (cover story).

Results
Test performance was highly variable within each condition. To best assess the effects of instruction on
test performance we examined the upper half of the learning performers within each condition (median spilt of
the practice problem scores). Test performance for the high learners in each condition is shown in Table 1.
Table 1: Mean Test Performance for the High Learners for Each Condition.
Training
Reading (n = 13)
Explanation (n = 13)
Analogy (n = 13)

Multiple Choice
Conceptual
.51
.59 (d = .45)
.59 (d = .48)

Test
Problem Solving 1
New Context / Diff. Steps
.77 (d = .70)
.76 (d = .70)
.55

Problem Solving 2
New Context / Extra Info
.51
.71 (d = .69)
.75 (d = .86)

Discussion
The results are consistent with the laboratory predictions and show that self-explanation and analogical
comparison can support conceptual learning in a classroom setting. Both the self-explanation and analogy
groups showed conceptual learning gains beyond the traditional worked example group on the multiple-choice
test (Cohen’s d effect size scores of .45 and .48 respectively). The results for the problem solving tests were
more mixed. The reading and explanation groups showed an advantage over the analogy group on problem 1
whereas the explanation and analogy group performed better on problem 2. The reading group was expected to
perform closer to the other groups on problem solving because they had more practice (solving an additional
practice problem during learning). Current work examines the solutions in more detail to differentiate
conceptual from procedural errors.

References
Chi, M. T. H. (2000). Self-explaining expository texts: The dual processes of generating inferences and
repairing mental models. In R. Glaser (Ed.) Advances in instructional psychology. Mahwah, NJ:
Erlbaum.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Self-explanations: How students
study and use examples in learning to solve problems. Cognitive Science, 13, 145-182.
Chi, M. T. H., Feltovich, P. J., & Glaser, R. (1981). Categorization and representation of physics problems by
experts and novices. Cognitive Science, 5, 121-152.
Gentner, D., Holyoak, K. J., & Kokinov, B. N., (Eds.), (2001). The analogical mind. Cambridge, MA: MIT
Press.

Gentner, D., Loewenstein, J., & Thompson, L. (2003). Learning and transfer: A general role for analogical
encoding. Journal of Educational Psychology, 95, 393-408.

Acknowledgments
This work was supported by Grant SBE0354420 from the Pittsburgh Science of Learning Center
(www.learnlab.org). I thank Max Lichtenstein, Brett Van De Sande, and Bob Shelby for help in designing and
preparing the physics materials as well as Dan Belenky, Ted McClanahan, and Mary Wintersgill for help
collecting the data. I also thank Robert G. M. Hausmann and Anders Weinstein for discussions.

Coached Program Planning:
Dialogue-Based Support for Novice Program Design
H. Chad Lane and Kurt VanLehn
Department of Computer Science
Learning Research and Development Center
University of Pittsburgh
{hcl,vanlehn}@cs.pitt.edu

Abstract

1

Coached program planning is a dialogue-based style of tutoring aimed at helping novices during the early stages of
program writing. The intent is to help novices understand
and solve problems in their own words through the construction of natural-language style pseudocode as the first step in
solving a programming problem. We have designed an environment supporting coached program planning and have
used it in a human-to-human, computer-mediated evaluation of 16 novice programmers enrolled in a pre-CS1 programming course at the University of Pittsburgh. The results show that students who underwent coached program
planning, compared to those who did not, were more prolific
with comments in their programs, committed fewer structural mistakes, and exhibited less erratic programming behavior during their implementation. The dialogues collected
from this experiment followed a clear 4-step pattern. Starting with this observation, we are developing a dialogue-based
intelligent tutoring system called the Pseudocode Tutor to
support coached program planning.

Nearly every aspect of writing a program can be a struggle for a novice, and the situation is worsened by attempts
to deal with multiple impasses at once [4]. Additionally,
novices spend little time planning their code before keying
it in [10] and have a tendency to believe a program that
produces correct answers is all that matters [7]. Of course,
novices rarely produce perfect programs in their early attempts, especially if no planning was done. Soloway and
Spohrer have found that many of the bugs they encounter reflect misconceptions about the underlying algorithm rather
than the language being used [14].

Introduction

In an effort to make programming less difficult for beginners, a number of researchers have advocated the design of
programming languages that better match natural ways of
thinking and communicating [9, 2]. The idea is to “close
the gap” between a problem statement and a programmed
solution by limiting the cognitive hurdles imposed by traditional programming languages, thereby making it easier
for novices to express their solutions in the desired language. A slightly different approach, but also based on finding common ground in language with novices, was adopted
in BRIDGE [1]. This system helped students to construct
a solution description in natural language, followed by successive rewrites in more precise forms, ultimately resulting
in a working program. The idea of having students explain
and elaborate ideas in their own words is a known effective
method for improving learning [3]. It is unclear if these benefits were reaped in BRIDGE because the natural language
solutions were created via menu selections.

Categories & Subject Descriptors
K.3 Computers & Education: Computer and Information
Science Education - Computer Science Education
General Terms
Algorithms, Design, Human Factors
Keywords

In this paper, we introduce coached program planning
(CPP), a dialogue-based style of tutoring aimed at helping novices during the earliest stages of programming. We
show an environment supporting CPP and report the results of a human-to-human evaluation revealing that students who used it exhibit more desirable behaviors during
programming than students who did not. We conclude with
a discussion of the results, and a look forward to the ultimate goal of this research: creation of an intelligent tutoring
system for CPP.

novice programming, structured programming, intelligent
tutoring systems, coached program planning, dialogue systems
Permission to make digital or hand copies of all or part of
this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit
or commercial advantage and that copies bear this notice
and the full citation on the first page. To copy otherwise,
or republish, to post on servers or to redistribute to lists,
require prior specific permission and/or a fee.
SIGCSE ’03, February 19-23, 2003, Reno, Nevada, USA.
Copyright 2003 ACM 1-58113-648-X/03/0002...$5.00

2

Coached Program Planning

In a CPP tutoring session, the student and tutor collaborate
to build a natural-language-style pseudocode solution to a
problem. Ideally, the student has already read the problem
statement, but has not yet attempted an implementation.
Dialogue is the vehicle for this collaboration, and the result-

148

ing artifact (i.e., the pseudocode) can then be used by the
student as a blueprint during the non-tutored implementation phase (using an editor, compiler, etc.).
2.1

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Dialogue Structure

A CPP dialogue consists of the tutor repeatedly asking the
student to (1) identify a programming goal, (2) describe
a technique for attaining this goal, (3) suggest pseudocode
steps that attain the goal, and finally (4) place the steps appropriately within the pseudocode. This continues until all
programming goals have been satisfied, and the pseudocode
is complete. This 4-step pattern governed the dialogues in
our corpus (described later). At the tutor’s discretion, the
student’s own words are used as much as possible in the
text of the pseudocode steps. It is likely that many of the
answers the four questions above will be flawed (partially
correct, imprecise, or simply wrong). The collaborative nature of dialogue is ideal for refining these kinds of answers
into better ones.

Figure 1: pseudocode from the Hailstone problem

In addition, it is common for the tutor to enter sub-dialogues
involving the layout and organization of the pseudocode.
These arise from either algorithmic errors by the student
(e.g., improperly sequencing steps) or structural issues (e.g.,
improper indentation).
2.2

0 T what do we need to work on next?
1 S Finding the how many numbers are in the
sequence.
2 T are you sure? [pause]
3 T does our program generate an entire sequence?
4 S We need to see if the new number is now
odd or positive.
5 T right... and after we do that, are we done?
6 S Not unless we reach the ground state.
7 T how can that be determined in the program?
8 S by a loop
9 T right!

Pedagogy

Pseudocode is a well established approach to teaching novice
program design. At one extreme, Shackelford promotes the
exclusive use of (formal) pseudocode [13], while others have
argued for a gradual introduction along with a particular
programming language [8]. Our approach is closer to the
latter, but with informal pseudocode in an effort to better
match the language of the student. The aim of CPP is to
instill a mindset in novices of thinking about a problem first
in their own words, rather than implementation and programming language specific issues.

Figure 2: determining the need for a loop

Because the tutor acts as a filter for the student’s ideas, the
resulting pseudocode will be correct. Thus, heading into implementation, students are armed with a better understanding of the problem and a better idea of the overall algorithm.
The individual pseudocode steps are implicitly approved and
often edited by the tutor before making their way to the solution. Thus, the style of the pseudocode is up to the tutor.
The basic requirement is that the student should require
minimal (if any) training to generate the text for the steps.
In summary, each step is suggested by the student, created
by the tutor, and finally placed by the student. CPP is a
preventative approach in that program designs with serious
mistakes and omissions should be avoided entirely because
of the tutor’s presence. Of course, mistakes can still be made
during implementation, but they should rarely be algorithm
related (assuming the student understands the pseudocode).
Making serious mistakes and learning from them is essential to maturing as a programmer, however, so we do not
advocate using CPP for all assignments in an introductory
programming course.1
2.3

problem in question – the “Hailstone” series – requires the
student to produce a sequence of integers according to simple rules. The student is asked to have the program report
the number of items in the resulting sequence, as well as
the largest value encountered. Figure 1 shows a student’s
solution to the Hailstone problem. This pseudocode is in its
final form, the same as what the student physically takes
away from the tutoring session.
There are several subtleties resulting from the openendedness of the pseudocode that should be noted. For
example, the input in line 3 refers to the user input accomplished in line 1. Similarly, the variable holding the changing
Hailstone value is referred to in several different ways: number generated (line 4), the number (5), it (6), the num (8),
and current value (10). Also, the steps involving calculations
(6 and 8) only implicitly suggest an update. In other words,
assignment of a new value is not represented by the step itself. While these rarely pose a problem for a human tutor,
they represent significant challenges for an ITS attempting
to do natural language understanding.

Examples

In the dialogue shown in figure 2, the student seems to lack
a heuristic of the form if a sequence of values needs to be
generated, use a loop, or at least needs help in applying it to
this problem. It is interesting to note that the student suggested the loop body (4) and the condition on the loop (6)

In this section, we present a pseudocode sample along with
two dialogues (all taken from actual tutoring sessions). The
1

get a positive number from the user
initialize count to one
initialize largest to the input
while number generated is not 4 do
if the number is odd then
multiply it by 3 and add one
else
divide the num by 2
increase count by 1
if current value is larger than largest then
store the new largest value
end while
print count
print largest

Perhaps just the first half or two-thirds.

149

0 T What condition will we need on that loop?
1 S while hailnumber is even do
2 T think about that. you want to repeat the
even/odd check over and over.
3 T in the example you did, when did you stop?
4 S when it went to the same numbers
5 T yep. that was called the “ground state”.
6 T so we’ll loop while what is true?
7 S hailnumber is not equal to 4
8 T good.
Figure 3: finding the condition on a loop
before mentioning a looping construct at all. This observation is in harmony with previous findings that novices prefer
a depth-first search approach to programming [11] and not
the breadth-first (“top-down”) approach typically taught in
introductory programming courses. This student is given
encouragement and positive feedback for relevant and important suggestions, but is gently brought to what expert
programmers do first: identify higher level goals. Dialogue
seems to be an effective medium to achieve this pedagogical goal. It should also be noted that this dialogue does
not specifically follow the 4-step pattern: the tutor seems to
switch from looking for a programming goal to looking for a
technique.

Figure 4: CPP Environment

gorithmic skills, express programming concepts in their own
words, and to build pseudocode. Thus, we predicted that
students undergoing CPP would (1) show less erratic behavior in dealing with algorithmic difficulties during implementation, (2) be more prolific with their program comments,
and (3) make fewer indentation mistakes in their source
code.

The second dialogue, shown in Figure 3 (and from a different
subject), shows the tutor helping the student to determine
the termination condition on the loop. Two tutoring strategies are employed here. First, the tutor refers to an example
in line 3. Second, in line 6, the tutor poses a completion
question. In our experiments, it was common to see the tutor ask the student specify a loop condition only (answered
in line 7), and subsequently create the step augmenting with
while and do.
3

4.1

Subjects were volunteers from Introduction to Computer
Programming, a pre-CS1 service course at the University of
Pittsburgh and were paid $7/hour for their participation. 16
students volunteered and 2 were removed for not completing
the course. None of the subjects had programming experience beyond a few weeks of BASIC or spreadsheet skills.

An Environment for CPP

We have built an interface supporting CPP for the purpose
of evaluation and to act as the front end of an intelligent tutoring system. The interface consists of three windows (figure 4). The mini-browser (upper left) displays HTML pages
and remains available throughout the tutoring session. The
dialogue window (lower left) allows communication between
a human tutor and the student. Finally, the pseudocode window (right half) contains draggable tiles that contain text,
each of which represents a step in the solution.

4.2

Procedure

Four projects from the course were used in this study, starting roughly one month into the course. The first assignment
required the students to convert a four-digit number into
words. The second assignment was the Hailstone series problem, the third was to play games of “rock, paper, scissors,”
and the last required loading and processing of arrays.

A tutoring session begins with the student reading the problem statement in the browser, followed by collaborative pseudocode construction. When steps are agreed on, the tutor
creates tiles and the student drags them into the rest of the
pseudocode. Upon completion, the tile outlines are removed
leaving the pseudocode in a more traditional form for the
student to print out and use during implementation.
4

Subjects

Students were assigned to one of two conditions according
to their preferences: the experimental condition with subjects using the CPP environment over a network with a human tutor, and a control condition allowing students to go
about writing their programs as they normally would (without CPP). The first project acted as our pre-test: all students did the assignment on their own with no interaction.
For the experimental condition, subjects engaged in CPP to
prepare for the middle two projects. No CPP interaction
was provided in the final assignment, thus permitting it to
play the role of post-test.

Experiment

In the fall of 2001 and spring of 2002, we conducted a
computer-mediated, human-to-human study to assess both
the usability of the environment and the effect of CPP on
novices. CPP is intended to help novices develop their al-

Data collection consisted of recording compiler and editor
activities during implementation of all subjects. Addition-

150

ally, all files submitted to the compiler were saved (similar
to [14]). This was done with the students’ knowledge and
consent.

averaged 2.71 (sd = 2.63). The difference is marginally statistically significant (F (1, 11) = 3.53, p = 0.0872) and moderately large (effect size = 0.65).

To determine if CPP subjects displayed less erratic behavior
during implementation, we borrow the idea of floundering
from the fields of user interfaces and intelligent tutoring systems. For computer programming, we define floundering as
repeated attempts to repair a bug leaving the program no
closer to being correct after each attempt. We applied two
measures to gauge floundering. The first was simply to count
the number of successful compile attempts (i.e., no syntax
errors) in the hope that students who floundered more spent
more time compiling and running their programs. The second was to look at the content of the differences between
the syntactically correct versions of their programs. We categorized the changes between each pair of files as involving
an algorithmic bug-fix or something else.2 We then counted
the number floundering episodes involving algorithmic bugs.
An episode is defined as one more recompiles intended to
fix the same error. Intuitively, the number of floundering
episodes found in an implementation represents the number
of algorithmic impasses the student struggles to repair.

Because the identification of floundering is somewhat subjective, the implementation logs were coded independently
by two experienced programming instructors. Using a specialized coding tool that displayed subsequent versions of
the programs, each compile attempt was classified as an algorithmic flounder or not. After identifying the beginning
and ends of all floundering episodes, the intercoder reliability was computed with a +/- 1 cushion on the boundaries
for episodes of 2 in length or longer. In this study, four randomly chosen implementations were used for training and
the rest to measure agreement. The result was a kappa value
of 0.872.3
Commenting In their final post-test programs, CPP subjects had a mean of 35.0 (sd = 23.1) comment lines. The
mean for control subjects was 12.3 (sd = 9.76), leaving a
difference of nearly 23 more lines of comment lines on average for CPP subjects over control subjects. This difference
is statistically significant (F (1, 11) = 5.97, p = 0.0325) and
large (effect size = 2.33).

To see if students who used CPP were more comfortable
using natural language in their programs, we simply counted
the number of comment lines in the final version of the posttest program. Single line comments counted as 1, as did each
line within multi-line comments. Administrative comments
(name, date, class, etc.) were not counted.

Indentation Because indentation had been covered in
class and gradually learned throughout the semester, there
was a ceiling effect when analyzing the indentation of the
post-test program. Thus, we report results for the second
project understanding that the CPP students had already
created a pseudocode solution prior to their real solution.

Lastly, to determine if CPP subjects gained a better understanding of structural concepts, we examined indentation in
all files submitted to the compiler, including those with syntax errors. Each improperly indented line was counted upon
its first appearance. Our rules for proper indentation were
liberal (one space was considered enough), but consistent
with those presented in popular introductory programming
textbooks.
4.3

During the implementation of the second project, CPP subjects produced an average of 2.43 (sd = 2.88) improperly
indented lines of code per implementation. The mean for
the control subjects was 12.7 (sd = 10.8). This means that
CPP subjects maintained the structure of their code by incorrectly indenting roughly 10 less lines than the control subjects. This difference is statistically significant (F (1, 11) =
5.61, p = 0.0373) and large (effect size = 0.95).

Results

5

Because of a possible self-selection bias (subjects were assigned to one of the two conditions according to their stated
preferences), the first project included in the study, also the
first non-trivial assignment given in the class, was used as a
pre-test. The scores on these projects revealed no significant
correlation between condition and pre-test (t(12) = −0.269,
p = 0.79), allowing us to conclude that both conditions consisted of subjects of equivalent competence.

Discussion

Overall, we feel these results justify a dialogue-based approach to novice program design. If difficulty at programming is a reason for novices to drop out of introductory
courses, it might be that extra support early in the programming process, such as that provided by CPP, might avert
many of the frustrations they encounter. It also seems that
students receiving CPP adopt more desirable behaviors during implementation. By helping students build pseudocode
first, they are receiving a certified correct outline of a program that they are responsible for creating. They still face
the unavoidable struggles of learning a specific programming
language, but can do so without as much confusion regarding
the particular algorithms they are trying to implement.

Floundering The first test for floundering was to simply count all successful compile attempts. CPP subjects
compiled an average of 25.0 (sd = 32.9) times during their
post-test project implementation, and control subjects compiled 49.4 (sd = 41.38) times. The difference was not statistically significant. However, by throwing out a statistical
outlier in the CPP condition who was 2.2 standard deviations from the mean, the difference was marginally significant (F (1, 10) = 4.08, p = 0.071).
The second test was to count episodes of algorithmic floundering. In the post-test assignment, CPP subjects had a
mean of 1.00 (sd = 2.24) episodes and the control subjects

Regarding comments, we feel that these results support our
argument that coached program planning encourages students to think about solutions in their own words, and to
feel more comfortable inserting comments into their code.
The mere existence of comments is only a suggestion, however. An analysis of the content of these comments may be
necessary to better test our hypothesis.

2
These include things like tweaking i/o behavior, adding
comments, superficial rearrangement of program code, and
adding large code segments.

3
Kappa is a popular measure for intercoder reliability
because it factors out agreement by chance. Generally, a
kappa value above 0.80 is considered reliable.

151

As stated above, CPP students saw the proper indentation
in the pseudocode in the second project. The results support
the claim that CPP students produced fewer incorrectly indented lines in their actual code. Since we could not use the
final project for this test, the result is not as strong regarding the longer term effect of CPP. For indentation, this is
perhaps not a serious criticism.

7

This research was supported by NSF grant number 9720359
to CIRCLE, the Center for Interdisciplinary Research on
Constructive Learning Environments at the University of
Pittsburgh and Carnegie-Mellon University. We would also
like to thank Mark Fenner for his help with the coding work,
and Bob Hausmann for assistance in the data analysis.

The results involving floundering were not as conclusive as
we had hoped, but still suggestive that students who are
trained to think about pseudocode and their algorithm in
a larger sense will spend less time floundering during implementation. There are several possible reasons for the
inconclusive gains with the floundering measure. Students
were only trained on two assignments – at about an hour
per tutoring session, this amounted to roughly two hours of
training per student in the experimental group. Thus, perhaps more programs with CPP would have helped. It is also
possible that individual differences and tendencies were too
great suggesting that more subjects might be necessary for
this particular measure. This was certainly a problem given
the cost of using a human tutor, but will not be when we
repeat the evaluation with an ITS.
6

Acknowledgements

References
[1] Bonar, J. G., and Cunningham, R. Bridge: Tutoring
the Programming Process. In Intelligent Tutoring Systems: Lessons Learned, J. Psotka, L. D. Massey, and
S. A. Mutter, Eds. Lawrence Erlbaum Associates, Hillsdale, New Jersey, 1988, pp. 409–434.
[2] Bruckman, A., and Edwards, E. Should We Leverage
Natural-Language Knowledge? An Analysis of User Errors in a Natural-Language-Style Programming Language. In Proceedings of the Conference on Human
Factors in Computing Systems (Pittsburgh, PA, 1999),
pp. 207–214.
[3] Chi, M., Bassock, M., Lewis, M., Reimann, P., and
Glaser, R. Eliciting Self-explanations Improves Understanding. Cognitive Science 18 (1994), 439–477.

Future Work

We are building a dialogue-based intelligent tutoring system
called the Pseudocode Tutor to perform CPP. Currently, the
system rigidly follows the 4-step pattern found in the dialogues and uses keywords to understand student input. Advanced understanding and tutoring strategies are being developed, but not part of the tutor as of this writing. Our
goal is to have the ITS behave as closely as possible to the
human tutor in this study. At the very least, we hope to
simulate many of the effective tutoring strategies present
in our corpus (e.g., referring to an example, posing completion and Socratic-style questions, simulating execution of the
pseudocode, etc.). As noted earlier, free-form pseudocode
presents difficult natural language understanding challenges
that still lack general solutions. To skirt this problem somewhat, the Pseudocode Tutor is being built to follow the standard presented by Robertson [12] which requires explicit
references to variable names, limited use of reserved words,
and some restrictions on the format of steps. The overall
feel is nonetheless intuitive and natural.

[4] DuBoulay, B. Some Difficulties of Learning to Program.
Journal of Educational Computing Research 2, 1 (1986),
57–73.
[5] Felleisen, M., Findler, R. B., Flatt, M., and Krishnamurthi, S. How to Design Programs. MIT Press, 2001.
[6] Flatt, M. Personal Communication, March 2002.
SIGCSE02 Dr. Scheme Workshop.
[7] Joni, S.-N., and Soloway, E. But My Program Runs!
Discourse Rules for Novice Programmers. Journal of
Educational Computing Research 2, 1 (1986), 95–125.
[8] Lee, P., and Phillips, C. Programming Versus Design:
Teaching First Year Students. SIGCSE Bulliten 30, 3
(1998), 289.
[9] Pane, J. F., Ratanamahatana, C. A., and Myers,
B. Studying the language and structure in nonprogrammers’ solutions to programming problems. International Journal of Human-Computer Studies 54
(2001), 237–264.

Regarding the analysis of the data, we are also interested in
analyzing various aspects of the student projects, including
the quality of the designs, quality of the identifiers, content
and quality of the comments, the timing of when comments
are added, and overall attitude regarding their work. We
believe that CPP should have an impact on these aspects of
novice programming in positive ways.

[10] Pintrich, P. R., Berger, C. F., and Stemmer, P. M.
Students’ Programming Behavior in a Pascal Course.
Journal of Research in Science Teaching 24, 5 (1987),
451–466.

While we have focused on structured programming exclusively for now, we believe the idea of using natural language
to prepare a student to write real code would also be effective in other paradigms. For example, in their book How to
Design Programs, Felleisen et. al. present design recipes to
help students write functions in Scheme [5]. Each phase in
these recipes has the student draw on intuition and use natural language to guide code writing. When students ask for
help, most tutorial interaction involves asking general questions about the student’s status within the design recipe
[6]. The role of natural language in novice programming is
clearly an important avenue that deserves continued attention.

[11] Rist, R. S. Schema Creation in Programming. Cognitive
Science 13 (1989), 389–414.
[12] Robertson, L. A. Simple Program Design.
Technology – Thompson Learning, 2000.

Course-

[13] Shackelford, R. Introduction to Computing and Algorithms. Addison-Wesley, 1998.
[14] Spohrer, J. C., and Soloway, E. Putting It All Together
is Hard For Novice Programmers. In Proceedings of the
IEEE International Conference on Systems, Man, and
Cybernetics (Tucson, Arizona, November 12-15 1985).

152

User Model User-Adap Inter (2011) 21:137–180
DOI 10.1007/s11257-010-9093-1
ORIGINAL PAPER

Empirically evaluating the application of reinforcement
learning to the induction of effective and adaptive
pedagogical strategies
Min Chi · Kurt VanLehn · Diane Litman ·
Pamela Jordan

Received: 21 April 2010 / Accepted in revised form: 1 December 2010 /
Published online: 5 January 2011
© Springer Science+Business Media B.V. 2011

Abstract For many forms of e-learning environments, the system’s behavior can be
viewed as a sequential decision process wherein, at each discrete step, the system is
responsible for selecting the next action to take. Pedagogical strategies are policies to
decide the next system action when there are multiple ones available. In this project we
present a Reinforcement Learning (RL) approach for inducing effective pedagogical
strategies and empirical evaluations of the induced strategies. This paper addresses
the technical challenges in applying RL to Cordillera, a Natural Language Tutoring
System teaching students introductory college physics. The algorithm chosen for this
project is a model-based RL approach, Policy Iteration, and the training corpus for the
RL approach is an exploratory corpus, which was collected by letting the system make
random decisions when interacting with real students. Overall, our results show that

M. Chi (B)
Machine Learning Department, Carnegie Mellon University, 5000 Forbes Ave.,
Pittsburgh, PA 15213, USA
e-mail: minchi@cs.cmu.edu
K. VanLehn
School of Computing, Informatics and Decision Science Engineering, Arizona State University,
Tempe, AZ, USA
e-mail: kurt.vanlehn@asu.edu
D. Litman
Department of Computer Science and Intelligent Systems Program, University of Pittsburgh,
5105 Sennott Square, 210 South Bouquet Street, Pittsburgh, PA, USA
e-mail: litman@cs.pitt.edu
D. Litman · P. Jordan
Learning Research and Development Center, University of Pittsburgh, Pittsburgh, PA 15260, USA
P. Jordan
e-mail: pjordan@pitt.edu

123

138

M. Chi et al.

by using a rather small training corpus, the RL-induced strategies indeed measurably
improved the effectiveness of Cordillera in that the RL-induced policies improved
students’ learning gains significantly.
Keywords Reinforcement learning · Pedagogical strategy · Machine learning ·
Human learning

1 Introduction
For many forms of e-learning environments, the system’s behaviors can be viewed as a
sequential decision process wherein, at each discrete step, the system is responsible for
selecting the next action to take. Pedagogical strategies are defined as policies to decide
the next system action when there are multiple ones available. Each of these system
decisions affects the user’s successive actions and performance. Its impact on student
learning cannot often be observed immediately and the effectiveness of one decision
also depends on the effectiveness of subsequent decisions. Ideally, an effective learning environment should craft and adapt its decisions to users’ needs (Anderson et al.
1995; Phobun and Vicheanpanya 2010). However, there is no existing well-established
theory on how to make these system decisions effectively.
In this paper, we focus on one form of highly interactive e-learning environment,
Intelligent Tutoring systems (ITSs). Most existing ITSs either employ fixed pedagogical policies providing little adaptability or employ hand-coded pedagogical rules
that seek to implement existing cognitive or instructional theories (Anderson et al.
1995; Koedinger et al. 1997; VanLehn 2006). These theories may or may not have
been well-evaluated. For example, in both the CTAT (Anderson et al. 1995; Koedinger
et al. 1997) and Andes systems (VanLehn et al. 2005), help is provided upon request
because it is assumed that students know when they need help and will only process
help when they desire it. Research on gaming the system, however, has raised some
doubts about this. It showed that students sometimes exploit these mechanisms for
shallow gains thus voiding the help value (Baker et al. 2004a,b).
Previous researchers have largely treated the specification of pedagogical policies
as a design problem: several versions of a system are created, the only difference
among them being the pedagogical policies employed (Corbett and Anderson 2001;
McKendree 1990; Ringenberg and VanLehn 2006). Data is then collected from human
subjects interacting with each version of the system, and the students’ performance is
then statistically compared. Due to cost limitations, typically, only a handful of alternative policies are explored. Yet, many such other reasonable ones are still possible.
In recent years, work on the design of ITSs and Natural Language (NL) dialogue systems has involved an increasing number of data-driven methodologies. Among these,
Reinforcement Learning (RL) has been widely applied (Beck et al. 2000; Iglesias
et al. 2003, 2009a,b; Martin and Arroyo 2004; Singh et al. 2002; Tetreault et al. 2007;
Williams et al. 2005; Walker 2000). In the project reported here, we applied a form of
RL, Policy Iteration, to improve the effectiveness of an ITS by inducing pedagogical
policies directly from an exploratory corpus. The exploratory corpus was collected by
letting the ITS make random decisions when interacting with real students.

123

Applying reinforcement learning to pedagogical strategies

139

Compared with previous studies on applying RL to induce pedagogical policies on
ITSs, this project makes at least three major contributions. First, we show that using
a relatively small exploratory training corpus is a feasible approach to induce effective pedagogical policies. Second, we empirically show that the RL induced policies
indeed improved students’ learning gains significantly. Third, while much of the previous research on applying RL to ITSs and non-tutoring NL dialogue systems used
pre-defined state representation, our approach in this project is to begin with a large
set of features to which a series of feature-selection methods were applied to reduce
them to a tractable subset. We will shed some light on the relative effectiveness of
different feature-selection methods and which features among the ones defined were
most involved in the final induced policies.
Overall, our results provide empirical evidence that, when properly applied, RL
can be applied to reach rather complicated and important instructional goals such as
learning and quantitatively and substantially improve the performance of a tutoring
system. In this paper, we will describe our detailed methodology for using RL to optimize the pedagogical policies based on limited interactions with human students and
then present empirical results from validating the induced policies on real students.
In the following, Sect. 2 gives a brief overview of prior related research on applying RL to either ITS or NL dialogue systems. Section 3 explains how RL can be used
to optimize pedagogical policies with a set of given features, Sect. 4 explains our
procedure on how a series of feature-selection methods were applied to a large set of
features in this project in order to induce the “best” policy. Section 5 describes our general approach and Sect. 6 describes the general methods including an introduction of
the Cordillera system and procedures, while Sect. 7 describes how Cordillera optimizes
its decisions from experimentally obtained dialogue data. Section 8 reports empirical
results evaluating the performance of Cordillera’s induced pedagogical strategies and
demonstrates that the effectiveness of Cordillera was improved.

2 Background
Generally speaking, a RL model consists of three elements: 1. S = {S1 , . . . , Sn } is
a state space; 2. A = {A1 , . . . , Am } is an action space represented by a set of action
variables; 3. R = r (si , s j , ak ) denotes a reward model that assigns rewards to state
transitions. The goal of RL is to find an optimal policy π ∗ that maps each state to the
proper action that would generate the maximum rewards.
The empirical procedure of applying RL to either ITS or non-tutoring NL dialogue
systems can be divided into two phases: a training phase and a test phase. The training phase mainly involves defining an appropriate state representation S, a reasonable
action space A, and an appropriate reward function R, collecting a training corpus ,
and applying some RL algorithms to induce policies. In order to apply RL to induce
an effective policy, it is important for the system to explore the relevant S of possible
decision action sequences during the training phase. A common problem in RL is
finding a balance between exploration (attempting to discover more about the world)
and exploitation (using what we already know about the world to get the best results
we can). On one hand, if the system does not explore enough, the RL approach might

123

140

M. Chi et al.

not find an effective policy at all. On the other hand, if the system explores too much,
it cannot stick to a path; in fact, it is not really learning as it cannot exploit its knowledge, and so acts as though it knows nothing. It is often unclear how much exploration
should be done in order to induce an effective policy. The test phase generally involves
some sorts of evaluations to find out whether the RL-induced policies indeed fulfill
their promises.
There are two major categories of RL, model-free algorithms and model-based algorithms (Kaelbling et al. 1996). Model-free algorithms learn a value function or policy
directly from the experience while interacting with the agent; at the end of learning,
the agent knows how to act, but does not explicitly know anything about the environment. Whereas model-based algorithms first construct a model of the state transition
and outcome structure of the environment, and then evaluate actions by searching
this model. In other words, Model-based methods do explicitly learn state transitions.
Generally speaking, model-free methods are appropriate for domains where collecting
data is not a big issue; their corresponding algorithms include Monte Carlo methods
and temporal difference methods. Model-based methods, on the other hand, are appropriate for domains where collecting data is expensive; their corresponding algorithms
include dynamic programming such as Policy Iteration and Value Iteration.
Previously both model-free and model-based RL algorithms were applied to
improve conventional ITSs. Next, we will briefly overview related previous research.

2.1 Previous research on applying RL to ITSs
Beck et al. (2000) investigated applying RL to induce pedagogical policies that would
minimize the time students take to complete each problem on AnimalWatch, an ITS
that teaches arithmetic to grade school students. In the training phase of their study,
they used simulated students. Given that the cost of collecting data with simulated
students is relatively low, a model-free RL method, Temporal Difference learning,
was applied. In the test phase, the induced policies were added to AnimalWatch and
the new system was empirically compared with the original version of AnimalWatch.
Results showed that the policy group spent significantly less time per problem than
their no-policy peers. Note that their pedagogical goal was to reduce the amount of
time per problem, however faster learning does not always result in better learning
performance. Nonetheless, their results showed that RL can be successfully applied
to induce pedagogical policies for ITSs.
Iglesias et al. (2009a,b, 2003), on the other hand, focused on applying RL to improve
the effectiveness of an Intelligent Educational System that teaches students DataBase
Design. They applied another model-free RL algorithm, Q-learning. The goal for the
induced policy was to provide students with direct navigation support through the system’s content with the expectation that this would help students learn more efficiently.
In the training phase, similar to Beck et al.’s approach, Iglesias and her colleagues
used simulated students. In the test phase, the induced policy was also empirically
evaluated on real students. Results showed that while the policy led to more effective
system usage behaviors from students, the policy students did not out-perform the
no-policy peers in terms of learning outcomes.

123

Applying reinforcement learning to pedagogical strategies

141

Martin and Arroyo (2004) applied a model-based RL method, Policy Iteration, to
induce pedagogical policies that would increase the efficiency of hint sequencing on
the Wayang Outpost web-based ITS. During the training phase, the authors used a
student model to generate the training data for inducing the policies. Here the student
model was similar to the simulated students used in Beck et al and Iglesias et al. In
the test phase, the induced policies were tested on the student model again and results
showed that the induced policies increased its predicted learning. However, since the
policies were not tested with human students, their effectiveness with real students is
still an open question.
Tetreault et al. (2007) and Tetreault and Litman (2006b) used an Intelligent Spoken Tutoring System, ITSPOKE, which teaches students college physics (Litman and
Silliman 2004). In their work, they used a previously collected corpus of physics tutorial dialogues as a training corpus and investigated applying Policy Iteration to induce
pedagogical policies from it. The focus of their work was introducing a novel method
for evaluating state representations and thus the learning gains of human students
using the induced policy system were not measured and compared to the prior system.
Additionally, note that because the training corpus used in this work was not collected
with the goal of exploring the full range of tutorial decisions, the tutor often executed
only one type of action in many dialogue states.
Although there have been other studies on application of RL to ITSs, they mostly
involved inducing domain models rather than pedagogical policies. For example,
Barnes and Stamper (2008) and Stamper et al. (2007) have applied RL to construct
problem solutions from existing students’ solutions for an ITS called Proofs Tutorial,
which teaches college-level discrete mathematics. They used a form of the modelbased RL method Value Iteration and the resulting problem solutions were then used
to generate hints for new students. They found that the extracted solutions and the
proposed hint-generating functions were able to provide hints over 80% of the time
for the new students.
In short, both model-free and model-based RL algorithms have been applied to
induce pedagogical policies in ITSs but only model-free RL induced policies were
empirically evaluated on real students (Beck et al. 2000; Iglesias et al. 2003, 2009a,b).
As far as we know, none of the previous research has empirically shown that an
RL induced policy significantly improved students’ learning performance. Given that
improving learning is a primary goal for any ITS, a better RL approach together with
empirical evaluations is needed and is attempted in this project.
Note that previous research on applying RL to induce pedagogical policies either
used pre-existing data that was collected for other purposes (Tetreault et al. 2007;
Tetreault and Litman 2006b) or simulated students or models (Beck et al. 2000; Iglesias
et al. 2003, 2009a,b; Ai and Litman 2007; Martin and Arroyo 2004) during the training
phase. We argue that neither approach is optimal.
Using existing datasets is complicated by the fact that pre-existing systems often
explore a small part of the state-action space and thus may yield biased or limited
information. When using simulated students or models, however, the effectiveness
of the induced policies would largely depend on the accuracy of the simulated students. Building accurate simulated students is not easy and it is especially challenging
because human learning is a rather complex, poorly understood process. For instance,

123

142

M. Chi et al.

one possible explanation for the difference between the policy and no-policy groups
that was found in Beck et al. (2000) but not in Iglesias et al’s work (2003, 2009a,
2009b) may be that while it is possible to accurately model time on task in Beck
et al. (2000), modeling how students would respond to the system and simulating how
students would learn is much more challenging in Iglesias et al. (2003, 2009a,b).
On the other hand, previous research on applying RL to improve dialogue systems
employed an alternative approach that involved collecting an exploratory corpus from
human subjects in the training phase. Next, we will briefly describe how previous
research in dialogue systems tackled the issue.
2.2 Collecting an exploratory corpus
Dialogue Systems are a field of Computer Science that focuses on the construction
of computer systems that interact with human users via natural-language dialogues.
Much of the work in this area is focused on systems that obtain information or search
databases such as querying bus schedules (Raux et al. 2005) and booking airline tickets
(Rudnicky et al. 1999). In recent years, RL has been widely applied to the design of
dialogue systems (Williams et al. 2005; Walker 2000; Singh et al. 2002).
Singh et al. (2002) explored avoiding biased training data by collecting an exploratory corpus. In their study, they used a dialogue system called NJFun, a real-time
spoken dialogue system that provides users with information about things to do in
New Jersey. In order to let the system explore the relevant space of possible decision
sequences, they collected an exploratory corpus from a system that makes random
system decisions as it spoke with human users, thus ensuring that the transitions are
adequately explored (Singh et al. 2002). Then a model-based RL method, Value Iteration (Singh et al. 1999), was applied to induce an effective dialogue policy from the
collected exploratory training corpus. In the test phase, they empirically evaluated
the induced policy on real users and showed that the dialogue policies significantly
improve the likelihood of task completion on NJFun. Similar approaches have been
shown to be successful in other studies (Williams et al. 2005; Walker 2000).
Therefore, in this project we use neither pre-existing system-user interaction data
nor simulated students and instead followed the approach of Singh et al. (2002). More
specifically, in the training phase, we collected an exploratory corpus by training
human students on an ITS that makes random decisions and then applied RL to induce
pedagogical policies from the corpus; and in the test phase, we re-implemented the
ITS using the learned pedagogical policies and measured the learning gains of the
induced policies with a new group of students. The ITS involved in this project is
called Cordillera, a NL tutoring system which teaches students introductory physics
(VanLehn et al. 2007b).
Although NL tutoring systems can be seen as a type of dialogue system, it is an open
question whether using an exploratory corpus as training data would induce policies
that improve learning gains as opposed to task completion or user satisfaction.
One major source of uncertainty comes from the fact that the rewards used in RL
are much more delayed in NL tutoring systems than in non-tutoring dialogue systems. The most preferable rewards for NL tutoring systems are student learning gains
and the rewards for non-tutoring dialogue systems are often user satisfaction or task

123

Applying reinforcement learning to pedagogical strategies

143

completion. Even though the rewards in both types of systems will not be available
until the entire system-user interaction is over, NL tutoring dialogues are longer and
more complex than the database-access dialogues described above.
In dialogue systems like a train scheduler or NJFun, the interaction time is often
less than 20 min and the number of user-system interactions is generally less than 20
turns (Singh et al. 1999, 2002). In NL tutoring systems, on the other hand, the preparatory training materials and tests typically exceed these timeframes significantly. In
this project, it took students roughly 4–9 h to complete the training on Cordillera and
the number of Cordillera-student interactions was more than 280.
More immediate rewards are more effective than more delayed rewards for
RL-based policy induction. This is because the issue of assigning credit for a decision
or attributing responsibility to the relevant decision is substantially easier with more
immediate rewards. The more we delay success measures from a series of sequential
decisions, the more difficult it becomes to identify which of the decision(s) in the
sequence are responsible for our final success or failure.
Another major source of uncertainty for using an exploratory corpus to induce
effective pedagogical policies comes from the fact that the amount of training data is
severely limited by the need for student-ITS interactions. Compared with non-tutoring
dialogue systems, collecting data on NL tutoring systems is even more expensive. In
this study, we only included 64 students’ interaction logs in the exploratory corpus
which is quite small compared with 311 dialogues used in NJFun (Singh et al. 2002)
whose domain task is much simpler than ours.
To summarize, given the much delayed reward functions and the high cost of collecting data, it is still an open question whether applying Policy Iteration to induce
pedagogical policies directly from a relatively small exploratory training corpus would
indeed be effective in this project. Moreover, while previous research on applying RL
to ITSs and dialogue systems gave us some guidance on the type of RL algorithms
we should apply and how to collect the training corpus, we still face one primary
challenge: how to define the state representation.
2.3 Prior work on state representation
For RL, as with all machine learning tasks, success depends upon an effective state
representation S. Ideally S should include all of the relevant dialogue history necessary
to determine which action should be taken next. One obvious but impractical choice
is to use a complete record of the dialogue to the present point; however, in practice
we need to compress the dialogue history to make the space tractable. In other words,
an effective representation S should be an accurate and compact model of the learning
context. The challenge thus lies in identifying useful state features.
Early work on applying RL on non-tutoring dialogue systems focused largely on
relatively simple task domains and used expert defined slot-based state representations.
In applying RL to improve NJFun, for example, seven features were included in their
representation S which includes information such as the confidence of ASR (automatic
speech recognition), whether the system has greeted the user, which information is
being worked on (time, activity, location), and how many times a given piece of information has been asked for (Singh et al. 1999). Additionally, some previous studies

123

144

M. Chi et al.

focused on the type of features that should be included in the state representation. For
instance, Frampton and Lemon (2005, 2006) showed that incrementally adding highlevel contextual information (such as the user’s last dialogue act and the last system
move) into a state representation was beneficial.
As RL and MDP are applied to more complex domains, S may increase rapidly in
size and complexity. For example, when a student is trained on an ITS, there are many
factors that might determine whether the student learns well from the ITS. Compared
with non-tutoring NL dialogue systems, where success is primarily a function of communication efficiency, communication efficiency is only one of the factors determining
whether a student learns well from an ITS. Moreover, many other factors are not well
understood, so to be conservative, states need to contain features for anything that is
likely to affect learning.
To make the RL problem tractable, much prior work on applying RL to ITSs also
used pre-defined state representation. More specifically, the representation S often
consists of features that were suggested by the learning literature. In the work of
Iglesias et al. (2003, 2009a,b), the state representation involves the student’s knowledge level on various knowledge items. By doing so, they assumed that the system
should adapt its behavior based on student knowledge levels. However, much other
useful information about the learning environment, such as the difficulty of the problem, student motivation and affect, was largely ignored. By contrast, Tetreault et al.
included a broader range of features in their state representation: Certainty, Correctness, Percent Correct, Concept Repetition, and Frustration (Tetreault et al. 2007;
Tetreault and Litman 2006b). Note that some of their features were manually annotated. Beck et al. (2000) included 15 state features from four main categories that are
suggested by the learning literature. The four main categories included the student’s
level of prior proficiency, level of cognitive development, and background, and the
difficulty of the current problem (Beck et al. 2000).
While existing learning literatures and theories give helpful guidance on state representation S, we argue that such guidance is often considerably more general than
the specific state features chosen. The following, for example, is a list of six features
describing a student’s knowledge level from different perspectives.
1. [Percentage Correct:] Defined as the number of the correct student entries divided
by the total number of the student entries.
2. [Number of Correct:] Defined as the number of the correct student entries.
3. [Percent Correct in This Session:] Defined as the number of the correct student
entries in this session divided by the total number of student entries in this session.
4. [Number of Correct in This Session:] Defined as the absolute number of the correct
student entries in this session.
5. [Number of Incorrect:] Defined as the number of the incorrect student entries.
6. [Number of Incorrect in This Session:] Defined as the number of the incorrect
student entries in this session.
When making specific decisions about including a feature on student knowledge
level in S, for example, it is often not clear which one of these six features should be
included. Therefore a more general state representation approach is needed. To this
end this project began with a large set of features to which a series of feature-selection

123

Applying reinforcement learning to pedagogical strategies

145

methods were applied to reduce them to a tractable subset. While much previous
research on the use of RL to improve ITSs and dialogue systems has focused on
developing the best policy for a set of given features (Beck et al. 2000; Iglesias et al.
2009a,b, 2003; Walker 2000; Henderson et al. 2005), only a little work has been done
on feature selection.
Paek and Chickering’s work, for example, showed how a state-space can be reduced
by only selecting features that are parents of the local immediate reward in that the
former performs just as well as a more complicated model with other non-parent variables (Paek and Chickering 2005). Rieser and Lemon (2006) used logistic regression
to select the best state features for a multi-modal dialogue system and showed marked
improvement over the baseline and some supervised learning methods. Most recently,
Tetreault and Litman (2008) tackled the feature selection issue by exploring three evaluation metrics for assessing the utility of adding a particular state feature to a model
of user state. The feature selection procedure employed in this project is motivated
by their work (Tetreault and Litman 2008). However, our approach is fundamentally
different from Tetreault et al.’s work because they explored three evaluation metrics
and used a relatively simple feature selection procedure. Here we explore a series of
different feature selection procedures, but use only one evaluation metric, the Expected
Cumulative Reward (ECR).
To summarize, while previous work on applying RL to induce pedagogical policies
in ITSs indicated some potential benefits from RL-induced policies, little empirical
evidence has shown that an RL induced policy indeed improved students’ learning
significantly. In this project, to tackle this problem directly, our approach was: (1) to
involve human subjects in both training and test phases; (2) to begin with a large set
of features to which a series of feature-selection methods were applied to reduce them
to a tractable subset.
Previously, we reported our first round of RL results in Chi et al. (2009). Overall,
our results showed that even though the RL-induced policies generated significantly
different patterns of tutorial decisions than the random policy, no significant difference
was found between the two groups on overall learning performance (Chi et al. 2009).
Only on one specific skill and one particular post-test measurement did the induced
policy students score higher than the random policy students (Chi et al. 2009).
There are many possible explanations for the lack of improvement in our first round
of RL policy induction. We argue it was because our RL approach was limited. More
specifically, a greedy-like feature selection approach selected four features out of a
total of 18 feature choices. In Chi et al. (2008), we explored four RL-based feature
selection methods on the 18 features. We found that simply improving the feature
selection methods resulted in policies with much higher ECR than those employed
in Chi et al. (2009). The higher the ECR value of a policy, the better the policy is
supposed to perform. However, the effectiveness of induced policies from Chi et al.
(2008) were not tested with human students.
We then executed a second round of policy induction. As reported earlier (Chi et al.
2010), the empirical results showed that the second round induced policies indeed
caused students to learn more and deeper. To investigate why this welcome benefit occurred, for this paper we analyzed computer logs to characterize the induced

123

146

M. Chi et al.

policies. Given the page limitations in the previous publications, a complete and
detailed description of our RL methodology has not been reported previously.
In this paper, we will present our RL methodology, empirical results from validating the induced policies, log analysis on the characteristics of the induced policies,
findings on which features among the ones defined were most involved in the final
induced policies, and the relative effectiveness of different feature-selection methods.
More specifically, we will mainly focus on the second round of RL policy induction
because it turned out to be more successful.
3 Applying RL to induce policies with a set of given state features
Previous research on using RL to improve dialogue systems (e.g. Levin and Pieraccini
1997; Singh et al. 1999) has typically used MDPs (Markov decision process) (Sutton
and Barto 1998) to model dialogue data. An MDP describes a stochastic control process
and formally corresponds to a 4-tuple (S, A, T, R), in which:
S = {S1 , . . . , Sn } is a state space.
A = {A1 , . . . , Am } is an action space represented by a set of action variables;
T : S× A×S → [0, 1] is a set of transition probabilities between states that describe
the dynamics of the modeled system; for example: P(S j |Si , Ak ) is the probability
that the model would transition from state Si to state S j by taking action Ak .
R : S × A × S → R denotes a reward model that assigns rewards to state transitions
and models payoffs associated with such transitions.
Additionally, π : S → A is defined as a policy.
From the RL and MDP perspective, pedagogical strategies are simply a set of policies which are mappings from the set of states in the state space S to a set of actions in
A. Note that in order for RL to be feasible, the number of states and actions should not
be too large. On the other hand, when an MDP is constructed for tutorial dialogues,
it can have millions of distinct dialogue states and tutor utterances. Here both states
and actions in an MDP are abstract. For instance, a state in the MDP might be a set of
features that represent thousands of real dialogue states and the action “Elicit” in the
MDP might denote any information-seeking questions asked by the tutor. Thus, “state”
and “action” have different meanings in an MDP versus in a tutorial dialogue. In order
to induce a policy from the MDP perspective, there must be deterministic functions for
mapping from dialogue states to MDP states and from a dialogue action to an MDP
action. For instance, one type of tutorial decision we investigated is elicit/tell (ET)
decisions; a policy would determine, based on the features present in a particular dialogue state, whether the tutor should elicit the next step from students, or tell students
the next step directly.
In this project, we applied Policy Iteration (Sutton and Barto 1998), which is a
model-based RL approach. One major difference between the model-based and modelfree RL methods is that the former need to learn transition probabilities T explicitly.
Generally T is estimated from a training corpus .
As each student solves a series of training problems on an ITS, a system-student tutorial dialogue is generated. For each tutorial dialogue, a scalar performance
measure called reward, R, is calculated. For example, a common choice for R in ITSs is

123

Applying reinforcement learning to pedagogical strategies

147

student learning gains. In this project, the reward function R is based on a Normalized
Learning Gain (NLG). This is because NLG measures a student’s gain irrespective of
pr etest
. Here posttest
his/her incoming competence and we have: N LG = posttest−
1− pr etest
and pr etest refer to the students’ test scores before and after the training respectively;
and 1 is the maximum score.
After training a group of students on the ITS, we get a training corpus , which is a
collection of system-student tutorial dialogues. Following Singh et al. (1999), we can
view each system-student interaction log di as a trajectory in the chosen state space
determined by the system actions and student responses:
sd1i
j

j
i

j
i

ad ,rd

j+1

Here sdi −−−→ sdi

ad1 ,rd1
i

i

−−−→

sd2i

ad2 ,rd2
i

i

nd

−−−→

nd

i
i
n d adi ,rdi
· · · sdi i −−−−−→

indicates that at the jth turn in the tutorial dialogue di , the
j

j

j

system is in MDP state sdi , executes MDP action adi , receives MDP reward rdi , and
j+1

then transfers into MDP state sdi . The number of turns in di is n di . In this project,
only terminal dialogue states have non-zero rewards because a student’s learning gain
will not be available until the entire tutorial dialogue is completed.
Dialogue sequences obtained from the training corpus  then can be used to empirically estimate the transition probabilities T as: T = { p(S j |Si , Ak )}i,k=1,...,m
j=1,...,n . More
specifically, p(S j |Si , Ak ) is calculated by taking the number of times that the dialogue
is in MDP state Si , the tutor took action Ak , and the dialogue was next in state S j
divided by the number of times the dialogue was in Si and the tutor took Ak . The
reliability of these estimates depends upon the size and structure of the training data.
Once an MDP model has been completed, calculation of an optimal policy is
straightforward. This project employed an RL toolkit developed by Tetreault and
Litman (2008).
3.1 Tetreault and Litman’s RL toolkit
Tetreault and Litman’s toolkit (Tetreault and Litman 2006a,b, 2008) uses a dynamic
programming algorithm for Policy Iteration (Sutton and Barto 1998). The code was
originally built on the MDP toolkit written in Matlab (Chadés et al. 2005). The purpose of this algorithm is to handle the problem of reward propagation. As noted above,
rewards, in this case learning gains, are not assigned until the end of the tutoring process, long after most actions have occurred. The dynamic programming algorithm
propagates the rewards back to the internal states weighting the V-value of each state,
s, via the following recursive equation:
R(s, a) +
V (s) = max

a



P(s  |s, a)γ V (s  )

(1)

s

Here P(s  |s, a) is the estimated transition model T, R(s, a) is the estimated reward
model, and 0 ≤ γ ≤ 1 is a discount factor. If γ is less than 1, then it will discount

123

148

M. Chi et al.

rewards obtained later. For all the studies reported here, a discount factor of 0.9 was
used, which is common in other RL models (Tetreault and Litman 2008).
The V-values, as defined by Eq. 1, can be estimated to within a desired threshold
using policy iteration (Sutton and Barto 1998). Here an estimated V-value and a best
possible action to take for each state are recorded. These are then iteratively updated
based on the values of its neighboring states. This iteration stops when each update
yields a difference below some threshold . Once the policy iteration process is complete, the optimal dialogue policy π ∗ is obtained by selecting the action that produces
the highest expected reward (or V-value) for each state.
Besides inducing an optimal policy, Tetreault and Litman’s toolkit also calculate the
ECR and a 95% confidence interval for the ECR (hereafter, 95% CI) for the optimal
policy (Tetreault and Litman 2008). The ECR of a policy is derived from a side calculation in the policy iteration algorithm: the V-values of each state, the expected reward
of starting from that state and finishing at one of the final states. More specifically, the
ECR of a policy π can be calculated as follows:
EC Rπ =

n

i=1

Ni
× V (si )
N1 + · · · + Nn

(2)

where s1 , . . . , sn is the set of all starting states and v(si ) is the V-values for state si ; Ni
is the number of times that si appears as a start state in the model and it is normalized
Ni
. In other words, the ECR of a policy π is calculated by summing
by dividing N1 +···+N
n
over all the initial start states in the model space and weighting them by the frequency
with which each state appears as a start state. The higher the ECR value of a policy,
the better the policy is supposed to perform.
Tetreault and Litman pointed out one limitation of using the ECR as an evaluation
metric for a policy: it assumes that there is sufficient collected data to derive a reliable
policy (Tetreault and Litman 2008). However, in practice researchers frequently have
to deal with issues of data sparsity. They proposed a novel approach of taking into
account the reliability of the transition probability T and constructing a confidence
interval for the ECR for the learned policy.
As described earlier, the transition probabilities T were derived from the training
corpus. Note that these transition probabilities T are simply estimates which are more
or less accurate, depending on how much data is available. As an illustration, Tetreault
and Litman used the following example (Tetreault and Litman 2008): in an MDP
model, we have S = {S1 , S2 , S3 }, A = {A1 , A2 }. From a training corpus , there
were ten cases that an action A1 was taken from state S1 . Out of these, three times
the system transitioned back to state S1 , two times it transitioned to state S2 , and five
times to state S3 . Thus we have
3
= 0.3
10
2
P(S2 |S1 , A1 ) =
= 0.2
10
5
P(S3 |S1 , A1 ) =
= 0.5
10
P(S1 |S1 , A1 ) =

123

(3)
(4)
(5)

Applying reinforcement learning to pedagogical strategies

149

From the same corpus, there were 1,000 times that action A2 was taken from state
S2 . In 300 of those cases it transitioned to state S1 ; in 200 cases to state S2 ; and the
remaining 500 times to state S3 . Thus,
300
= 0.3
1000
200
P(S2 |S2 , A2 ) =
= 0.2
1000
500
= 0.5
P(S3 |S2 , A2 ) =
1000
P(S1 |S2 , A2 ) =

(6)
(7)
(8)

While both sets of transition parameters have the same value, the second set is
more reliable. In order to take reliability into account, Tetreault and Litman proposed
a CI estimate based upon the available data (Tetreault and Litman 2008). It is done by
taking a transition matrix T for a slice and sampling from each row using a Dirichlet
distribution for q times (q = 1,000 in this project). As a result, it generates a large
number of new transition metrics T1 , T2 , . . . , Tq that are all very similar to T . They
then run an MDP on all q transition matrices to get a range of ECR’s. Both the ECR
and the 95% CI are used for feature selection as described in Sect. 4.
To summarize, for each given < S, A, R > and selected training data , an optimal
pedagogical policy π can be induced by applying Tetreault, & Litman’s toolkit. In this
case, our RL approach is quite straightforward (see Algorithm 1).
Algorithm 1 Apply RL To Induce Policies with < S, A, R > and Given 
Represent the training corpus  with < S, A, R >.
Estimate the transition probabilities T.
Compute the optimal dialogue policy, π , based on < S, A, R, T >.

3.2 A simple example of induced policies
Figure 1 shows an example of an RL-induced policy. The policy is for Elicit/Tell decisions so we have A = {Elicit, T ell} (shown in the second line in Fig. 1) and we used
the exploratory corpus as the training corpus  E x plorator y . The reward function R is
students’ N LG × 100.
The first line in Fig. 1 shows that the example policy involves only one state feature: [StepSimplicit y P S], which is estimated from the training corpus based on the
Fig. 1 Using StepSimplicityPS
to induce a single feature policy
on ET decisions

123

150

M. Chi et al.

percentage of correct answers when the tutor has done an Elicit (i.e., asked a question)
in a specific dialogue state.1 Thus the higher the value of StepSimplicityPS, the easier the dialogue state. By only including one feature in the state, we assume that the
decision to elicit or to tell should depend only on the simplicity level of the dialogue
“state”.
The RL toolkit developed by Tetreault and Litman requires all state features in the
model to be discrete variables. [StepSimplicit y P S], however, is numeric and must
be discretized before a suitable MDP can be constructed. In this project, the discretization procedure used two clustering procedures: the TwoStep procedure bounded the
number of clusters in SPSS and the K-means procedure used K-means clustering to
locate the optimal cluster centers. But other discretization procedures such as simple
median split can also be applied.
After the discretization procedure, a continuous numeric variable [StepSimpli
cit y P S] is binned into two binary values: 0 and 1. We have: “[StepSimplicit y P S] :
[0, 0.38) → 0; [0.38, 1] → 1”, which means if StepSimplicityPS value is below 0.38,
it is 0 (hard) otherwise, it is 1 (easy).
The number of MDP states in S determines the number of induced pedagogical rules
in π . Because we only have one binary feature in the state representation, two pedagogical rules, rule 1 and rule 2, are induced (shown under the [ policy] in Fig. 1). Rule 1 says
that when StepSimplicityPS is low (the content of this dialogue state is hard), the tutor
should elicit; while rule 2 says when StepSimplicityPS is high, either elicit or tell will
do. Figure 1 also shows that the example policy has: EC R = 8.09 (range (−∞, 100])
with a 95% CI [4.37, 12.07], which means there is a 95% chance that the ECR of the
learned policy is between a lower-bound of 4.37 and an upper-bound of 12.07.
So far, this section began with a description of how the problem of inducing pedagogical policies fits into the general RL and MDP framework. Then it described the
induction toolkit employed and the assessment metrics used. In short, the promise of
the MDP and RL approach is that once we build an estimated MDP that models the
user population and learning context accurately, the induced policy should maximize
the reward obtained from future students. With this approach, the problem of inducing
effective pedagogical policies is thus reduced to computing the optimal policy for
choosing actions in an MDP—that is, the tutoring system should take actions so as to
maximize expected reward.
In this section, we described the abstract methodology by which pedagogical policies are induced when < S, A, R > is defined and T is estimated from a given training
corpus . While this approach is theoretically appealing, the cost of obtaining human
tutorial dialogues makes it crucial to limit the size of the MDP state space in order to
minimize data sparsity problems, while retaining enough information in the states to
represent accurately the human population and learning context. As mentioned above,
our approach in this project is to begin with a large set of features to which a series
of feature-selection methods were applied to reduce them to a tractable subset. More
specifically, we applied 12 feature selection methods. In the next section, we will
1 Cordillera’s dialogue manager is a finite state network, so dialogue “state” here refers to a state in that
network. Many students will traverse the same network state, and some of those will leave the state via the
tutor asking a question. This percentage refers to those students’ answers.

123

Applying reinforcement learning to pedagogical strategies

151

describe our feature selection procedures in details. In Sect. 7, we will describe the
instantiation of this methodology in Cordillera.
4 Applying 12 feature selection methods for inducing pedagogical policies
To differentiate from the specific state representation S used in the RL and MDP formalism in Sect. 2, we use  to represent a large set of potential state features. In other
words, for any induced policy in this project, its corresponding state representation
S is a subset of . In the following, we will describe for a defined < A, R > and
a given system-student interactivity training corpus  how to select a subset S from
a large feature choice set  that will generate the best policy.
In order to do feature selection, first we need to decide the maximum number of
features, namely m̂, to be included in the final state representation. In order to determine m̂, it is necessary to consider the amount of available data and computational
power. m̂ should be small so that we have enough training data to cover each state,
yet be large enough to include enough features to represent states without losing the
information necessary to make good system decisions. In this project, for example,
based on the minimum data available from the training corpora, we capped the number
of features in each policy at six (m̂ = 6), which means that there can be as many as
26 = 64 rules in the learned policy.
Before getting into the details of our 12 feature selection methods, we first define an
initially empty set  for accumulating the induced policies for a defined < , A, R >
and a given training corpus . The final best policy π ∗ will be selected from  by
ECR. This is because ECR has been widely used as the criteria for evaluating induced
policies, for example in the field of applying RL to induce policies from simulated
corpora (Janarthanam and Lemon 2008; Williams and Young 2007a,b).
Almost every feature selection approach described below involves inducing singlefeature policies first. That is, for each state feature choice in , the RL toolkit was
used to induce a single-feature policy such as the one used earlier as an illustration
(see Fig. 1). Although generating single-feature policies does not involve any feature
selection procedure, it is counted as a feature selection method labeled as “single” for
convenient reasons.
In the following, the focus is on using feature selection methods to induce policies
with at least two features, referred to as multi-feature policies, and here we explored
eleven feature-selection methods. Four were based upon the RL toolkit and were previously described in Chi et al. (2008); one was based on Principal Component Analysis
(PCA); four were combinations of PCA and RL-based; and the final pair were based
upon stochastic selection. Next, we will describe each approach in detail.
4.1 Four RL-based feature selection methods
Detailed descriptions of the four RL-based approaches can be found in Chi et al. (2008).
Here we will give a brief summary. As described above, Tetreault and Litman’s toolkit
calculates an optimal policy together with the policy’s ECR and 95% CI (Tetreault
and Litman 2008). Lower-Bounds and Upper-Bounds were used to refer to the 95%

123

152

M. Chi et al.

confidence bounds calculated for the ECR. For example, the example policy in Fig. 1
has EC R = 8.09 with a 95% CI = [4.37, 12.07].
To this point ECR has always been used as the criteria for selecting the best policies.
However, a policy’s Lower-Bound or Upper-Bound can also be used as the criteria.
More specifically, the former evaluates the performance of policies in the worst case,
while the latter describes how well the policy can perform. Additionally, we defined
EC R
a new criterion named Hedge as: H edge = U pper Bound−Lower
Bound . Any of these
criteria, ECR, Lower-Bound, Upper-Bound, or Hedge can be used to evaluate policies. Thus they are used as four different criteria for our RL-based feature selections.
These feature-selection methods are fairly straightforward and use the same general
procedure, described in Algorithm 2.
Algorithm 2 Four RL-based Feature Selection Procedure
for Ranking Metric in [ECR, Lower-Bound, Upper-Bound, Hedge] do
Rank the features in  in descending order based upon the Ranking Metric of their corresponding
single-feature-policies.
for i = 2 to m̂ do
S = the top i features from the ranked 
Induce a pedagogical policy π with < S, A, R > and 
add π to 
end for
end for

Based upon the ranking metrics used, the four RL-based feature-selection methods
are named as ECR, Lower-Bound, Upper-Bound, and Hedge respectively. If we set
m̂ = 6, then each of the four methods would add m̂ −1 = 5 multi-feature policies to .
4.2 PCA-based feature selection method
Sometimes, the state features in  were highly correlated which reduced their expressiveness when used together. For example, three state features defined in this project
are: the number of correct student responses namely “nCorrectKCPM”, the number
of incorrect student responses namely “nIncorrectKCPM”, and the percentage of correct student responses namely “pctCorrectKCPM”. The third variable can be easily
calculated from the first two in that we have:
pctCorr ect K C P M =

nCorr ect K C P M
nCorr ect K C P M + n I ncorr ect K C P M

Therefore, it was necessary to apply an analysis procedure to avoid redundant
features. Here we used Principal Component Analysis (PCA) (Jolliffee 2002). More
specifically, all state feature choices in  were first normalized; then PCA was applied
to the normalized features to generate principal components and their corresponding
eigenvalues. These eigenvalues were arranged in descending order, and all components whose eigenvalues were less than 1 were removed. For each eigenvalue, the
feature that was maximally correlated with the corresponding principal component
was identified.

123

Applying reinforcement learning to pedagogical strategies

153

The resulting features were a subset of  and thus we designated them the PCAfeature subset, labeled as  PC A .  PC A is an ordered list arranged by the eigenvalues
of its corresponding principal components. Once  PC A was identified, the PCA-only
feature selection procedure was straightforward. It began with the first feature in  PC A
and added one feature at a time to induce a new policy. This process was repeated until
the number of features in the state representation S reaches m̂. Thus, for m̂ = 6, the
PCA feature selection method would add (m̂ − 1 = 5) multi-feature policies to .
4.3 Four PCA and RL-based feature selection methods
Next four new feature selection approaches are created by simply combining PCAonly feature selection with the four RL-based feature selection methods. In these
approaches, PCA is used to winnow  into  PC A and then the four RL-based methods,
ECR, Upper_Bound, Lower_Bound, and Hedge are applied on  PC A only. The four
feature selection methods were thus named PCA–ECR, PCA–LowerBound, PCA–
UpperBound, and PCA–Hedge respectively. Similar to previous approaches, if we set
m̂ = 6, then each of four PCA and RL-based combined feature selection methods
adds m̂ − 1 = 5 multi-feature policies to .
4.4 Random feature selection methods
So far nine relatively straightforward feature selection methods have been introduced.
In order to evaluate their relative effectiveness, two random feature selection methods
were employed. The expectation was that the nine methods would be at least more
effective than random selection.
The two random feature selection methods were named Random and PCA–Random
respectively. This is because features were randomly selected from  and  PC A
respectively. Moreover, for a given m̂, both random and PCA–Random selections ran
two rounds, generating m̂ − 1 policies in each round. In other words, 2 × (m̂ − 1)
multi-feature policies were generated for either random method. If we set m̂ = 6, then
the two random selection methods would add in a total of 20 (4×(m̂ −1)) multi-feature
policies to .
4.5 Summary on the general feature selection procedure
To summarize, in this project we first defined a large set of features that represent
relevant information about the learning environment and then applied 12 (including
single) feature selection approaches to compress the learning environment to a small
but carefully selected feature space. Our procedure can be summarized as:
1. Define < , A, R > and choose .
2. Decide m̂ to be included in the final policy.
3. Set  = ∅
(a) Inducing all single-feature policies and add them to .

123

154

M. Chi et al.

Fig. 2 An example of selected
“best” policy on ET decisions

(b) Apply eleven feature selection methods to induce a total of 13 × (m̂ − 1)
multi-feature policies2 and add them to .
4. Select the best policy from  by ECR.
4.6 An example of the best policy
In this project we defined 50 features in  (details in Sect. 7). After running the 12
feature selection methods on , a resulting “best” policy is shown in Fig. 2. The feature
selection method involved in inducing this policy is one of the four RL-based methods: ECR feature selection. Note that here we used the same < A, R > and the same
training corpus  E x plorator y as those used when inducing the single-feature policy in
Fig. 1. Thus, we have A = {Elicit, T ell} (shown in the second line in Fig. 2), the
reward function is students’ N LG × 100, and the training corpus is  E x plorator y .
Compared with the single-feature policy in Fig. 1 which has only StepSimplicityPS in the state representation, the policy in Fig. 2 has three state features. Their
corresponding descriptions and discretization information are presented below:
[StepSimplicityPS [0, 0.38) → 0; [0.38, 1] → 1]: encodes a step’s simplicity level.
Its value is estimated from the training corpus based on the percentage of correct
answers given on the dialogue state. The discretization procedure binarized the feature into two values: 0 and 1. If less than 38% of the answers given were correct, then
this is considered to be ’hard’ content and we set StepSimplicityPS = 0; Otherwise,
StepSimplicityPS = 1.
[TuConceptsToWordsPS [0, 0.074) → 0; [0.074, 1] → 1]: represents the ratio of
the physics concepts to words in the tutor’s utterances so far. The higher this value,
the greater the percentage of physics content included in tutor turns. Dialogue states
with less than 7.4% on this measure have TuConceptsToWordsPS=0 and 1 otherwise.
2 Either the random or PCA–Random selection would result in 2 × (m̂ − 1) multi-feature policies each and

each of the rest nine feature selection would result in (m̂ − 1) policies.

123

Applying reinforcement learning to pedagogical strategies

155

[TuAvgWordsSesPS [0, 22.58) → 0; [22.58, ∞) → 1]: encodes the average number of words in tutor turns in this session. This feature reflects how verbose the tutor
is in the current session. The discretization procedure set the threshold at 22.58, so
dialogue states when the tutor had above 22.58 words per tutor turn in the current
session were represented with 1 for TuAvgWordsSesPS and 0 otherwise.
Since each of the three features was discretized into two values, a three-feature state
representation would result in a state space of 23 = 8. Thus, eight pedagogical rules
are learned. Figure 2 shows that in five situations the tutor should elicit (rules 1–5),
in one situation it should tell (rule 6); in the remaining two cases either will do (rules
7–8).
For example, let’s explain rule 6 since it is the only situation in which the tutor
should tell. In rule 6, the state is [0 : 1 : 0], which represents the values of the
three corresponding features: StepSimplicityPS, TuConceptsToWordsPS and TuAvgWordsSesPS respectively. Rule 6 suggests that when the next dialogue content step
is hard (as StepSimplicityPS is 0), the ratio of physics concepts to words in the tutor’s
entries is high (as TuConceptsToWordsPS is 1), and the tutor is not very wordy in the
current session (as TuAvgWordsSesPS is 0), then the tutor should tell. As you can
see, a three-feature policy is already quite subtle and adaptive to the learning context.
Moreover, it is not like most of the tutorial policies derived from analyzing human
tutorial dialogues.
Figure 2 also shows that the induced three-feature policy has: EC R = 14.25
with a 95% CI [10.04, 18.12]. It shows that adding TuConceptsToWordsPS and
TuAvgWordsSesPS to the single state representation of StepSimplicityPS is effective.
Compared with the single feature policy StepSimplicityPS in Fig. 1, the three-feature
policy in Fig. 2 not only has higher ECR (14.25. vs. 8.09), but also has a higher lowerbound (10.04 vs. 4.37) and upper-bound (18.12 vs. 12.07). Thus, in theory it should
be more effective than the single-feature policy.
5 General approach
In the learning literature, it is commonly assumed that the relevant knowledge in
domains such as math and science is structured as a set of independent but co-occurring
Knowledge Components (KCs) and that KC’s are learned independently. A KC is “a
generalization of everyday terms like concept, principle, fact, or skill, and cognitive
science terms like schema, production rule, misconception, or facet” (VanLehn et al.
2007b). For the purposes of ITSs, these are the atomic units of knowledge. It is assumed
that a tutorial dialogue about one KC (e.g., kinetic energy) will have no impact on the
student’s understanding of any other KC (e.g, of gravity). This is an idealization, but
it has served ITS developers well for many decades, and is a fundamental assumption
of many cognitive models (Anderson 1983; Newell 1994).
When dealing with a specific KC, the expectation is that the tutor’s best policy
for teaching that KC (e.g., when to Elicit vs. when to Tell) would be based upon the
student’s mastery of the KC in question, its intrinsic difficulty, and other relevant, but
not necessarily known factors specific to that KC. In other words, an optimal policy
for one KC might not be optimal for another. Therefore, one assumption made in

123

156

M. Chi et al.

this project is that inducing pedagogical policies specific to each KC would be more
effective than inducing an overall KC-general policy.
The domain chosen for this project is the Physics work-energy domain, a common
component of introductory college physics courses. Two domain experts, who are also
knowledge representation experts (not the authors), identified 32 KCs in the domain.
They had experience identifying KCs for a series of previous studies involving college
physics. Note that a complicated domain like physics can often be broken into many
KCs. Here the 32 identified KCs are believed to cover the most important knowledge
in the domain. In order to induce effective pedagogical policies, we investigated KC
specific pedagogical policies in our RL applications.
This was a three-year project that can be divided into three stages, one stage per
year since Fall 2007. In each stage, a group of students was trained on Cordillera. All
three groups followed the same procedure: completing a background survey, reading
a textbook, taking a pre-test, training on Cordillera, and finally, taking a post-test. All
three groups used the training problems and instructional materials but on different
versions of Cordillera. The versions differed only in terms of the pedagogical policies
employed for interactive tutorial decisions.
In Stage 1, Cordillera made interactive decisions randomly and we collected an
exploratory corpus that examined the consequences of each tutorial decision with real
students. The student group is thus referred to as the Exploratory Group. In order to
differentiate this version of Cordillera from the ones used in subsequent studies, this
version is referred to as Random–Cordillera.
In Stage 2, we tried our first round of policy induction on all 32 KCs. Then these
RL-induced policies were empirically evaluated. More specifically, Tetreault and
Litman’s toolkit was applied to the Exploratory corpus to induce a set of pedagogical
policies. Because we dichotomized the students’ NLGs into +100 and −100 as reward
functions, the induced policies were referred to as Dichotic Gain (DichGain) policies
(Chi 2009; Chi et al. 2009). The induced DichGain policies were added to Cordillera
and this version of Cordillera was named DichGain–Cordillera. Except for following
different policies (Random vs. DichGain), the remaining components of Cordillera,
including the GUI interface, the training problems, and the tutorial scripts, were left
untouched. DichGain–Cordillera’s effectiveness was tested by training a new group
of 37 college students in 2008. Results showed that although the DichGain policies
generated significantly different patterns of tutorial decisions than the random policy,
no significant overall difference was found between the two groups on the pretest,
posttest, or the NLGs (Chi et al. 2009).
There are many possible explanations for the lack of difference in learning outcomes between the DichGain and Exploratory groups. We argue that the exploration
of our RL approach in Stage 2 was limited. As described above, applying RL to induce
effective tutorial policies may not be a simple task for which we can plug a training
corpus into a toolkit. Rather it depends on many factors, such as state feature choices,
feature selection methods, and the definition of reward functions. In Stage 2, only 18
features were included in our state feature choices and no more than four appeared in
the final induced tutorial policies. Thus the defined 18 features may be insufficient to
adequately represent the state. Moreover our greedy-like feature selection process may
also have limited our success. More details on the procedure of inducing DichGain

123

Applying reinforcement learning to pedagogical strategies

157

policies and an example of DichGain policies can be found in Chi et al. (2009). Therefore Stage 3 was designed to address these limitations in hopes of producing more
effective pedagogical policies.
In Stage 3, the approach to RL-related issues was greatly modified. Instead of focusing on all 32 KCs, we only focused on the eight primary KCs. We directly used students’
Normalized Learning Gain (NLG) × 100 as the reward function instead of dichotomizing the NLG. The induced set of tutorial policies is thus named Normalized Gain
(NormGain) tutorial policies and the version of Cordillera was named NormGain–
Cordillera. We again ran a new group of students, named the NormGain group, using
the same educational materials as used by the previous two groups. Next, we will
describe the general methods involved in this project.
6 Method
In this section, we will first describe the NL tutoring system involved in this project,
Cordillera. Then we will describe the two types of tutorial decisions, elicit/tell and
justify/skip-justify, that the RL-induced pedagogical policies make. After that, we will
describe the domain chosen for this project and specifically focus on the eight major
KCs on which KC-specific NormGain policies were induced in Stage 3. Then we will
describe the experimental procedure which is identical for all three studies in all three
stages. Finally, we will describe our grading criteria for this project.
6.1 Cordillera
Cordillera is an NL Tutoring System that teaches students introductory college physics (VanLehn et al. 2007b) and was developed using the TuTalk NL tutorial dialogue
toolkit (Jordan et al. 2007, 2006). TuTalk is an authoring tool which enables domain
experts to construct natural language tutoring systems without programming. Instead,
the domain experts focus on defining the tutoring content by writing tutorial scripts,
which are then used for automating interactions. In other words, the script authors
determine the flow of the dialogue and the content of each tutor turn.
The student interface is used by students to read the tutor’s tutorial instructions and
to answer questions by means of natural language entries. Figure 3 shows a screen
shot of the student interface. The Message Window, located in the bottom-left corner
is where the dialogue interaction takes place. The remaining panes are the Dialogue
History Pane (upper-left), Problem Statement pane (upper-right), and Variable Pane
(lower-right).
To reduce potential confounds due to imperfect NL understanding, the NL
understanding module in Cordillera was replaced with a human interpreter called
the language understanding wizard (Bernsen and Dybkjaer 1997). The only task performed by the human wizards is to match students’ answers to the closest response
from a list of potential responses and they cannot make any tutorial decisions. In
this format, Cordillera works as a communications framework that connects a student
interface to a wizard interface.
Across three stages of this project, three different versions of Cordillera were constructed, each of which differed only in terms of the pedagogical policies employed.

123

158

M. Chi et al.

Fig. 3 Cordillera student interface

The remaining components of the system, including the GUI interfaces and domain
experts’ tutorial scripts, were identical for all participants. In Cordillera the pedagogical policies are used to make two types of tutorial decisions. Next, we will briefly
describe them in detail.
6.2 Two types of tutorial decisions
Two types of tutorial decisions: Elicit/Tell (ET) and Justify/Skip-justify (JS) were the
focus of our attention. For both ET and JS decisions, there is no widespread consensus
on how or when either of these actions should be taken. This is why our research
objective is applying RL to learn policies on them.
6.2.1 Elicit/tell
During the course of one-on-one tutoring, the tutor often faces a simple decision, to
elicit the next step from a student, or to tell a student the next step directly. We refer
to such tutorial decisions as elicit/tell (ET) decisions. While a lecture can be viewed
as a monologue consisting of an unbroken series of tells, human one-on-one tutoring
is characterized by a mixture of elicits and tells. Some existing theories of learning
suggest that when making tutorial decisions, a tutor should adapt its actions to the
students’ needs based upon their current knowledge level, affective state, and other
salient features (Collins et al. 1989; D’Mello et al. 2008; D’Mello and Graesser 2010;
Koedinger and Aleven 2007; Pain and Porayska-Pomsta 2006; Porayska-Pomsta et al.

123

Applying reinforcement learning to pedagogical strategies

159

(a)

(b)

Fig. 4 Elicit versus Tell

2008; Vygotsky 1978). Typically, these theories are considerably more general than
the specific interaction decisions that system designers must make. This makes it difficult to instantiate these theories as specific pedagogical policies in ITSs. Therefore
when facing a decision whether to elicit or to tell a new step, most existing tutoring
systems always decide to elicit (Anderson et al. 1995; Graesser et al. 2001; Koedinger
et al. 1997; Litman and Silliman 2004; VanLehn et al. 2005, 2007a). For example,
existing NL tutoring systems often mimic a pervasive five-step dialogue pattern found
in human tutoring that starts with the tutor posing a question or problem (Graesser
et al. 1995; VanLehn et al. 2007a).
Figure 4 presents a pair of sample dialogues comparing elicit and tell versions of a
single tutorial dialogue extracted from the log files collected during this project. Both
dialogues begin and end with the same tutor turns (lines 1 and 6 in (a) and 1 and 4
in (b)). However, in dialogue (a) the tutor chooses to elicit twice (lines 2–3 and 4–5
respectively) while in dialogue (b) the tutor decides to tell twice (lines 2 and 3). Note
that the two dialogues cover the same domain content.
6.2.2 Justify/skip-justify
The second tutorial decision was whether to execute a justification step, also referred
to as self-explanation in much of the learning literature. During the tutoring process,
human tutors sometimes ask students to justify a step they have taken or an entry they
have made. Their apparent goal appears to be to help students understand domain
knowledge in a deeper way. The open question is whether or not the tutor should
conduct an elaborate discussion of a problem solving step given that this discussion is

123

160

M. Chi et al.

(a)

(b)

Fig. 5 Justify versus Skip-justify

not necessary for the solution. We refer to such tutorial decisions as justify/skip-justify
(JS) decisions.
Much previous research including Aleven et al. (2004), Chi et al. (1994) and Conati
and VanLehn (2000) found that asking students to justify their solution steps improves
student learning. However, eliciting such a discussion may not always be desirable
if, for example, the student is well aware of the rationale. If so, typing in a justification can be slow, frustrating, and distracting. Indeed, in domains like second language
acquisition, Wylie et al. found that tutors asking students to justify did not lead to
better learning outcomes but did significantly increase student training time when
compared to a control group that was not asked to enter justifications (Wylie et al.
2009). Additionally, Katz et al. (2000) found that in some cases it may be better to
delay the justifications until the problem has been solved, especially if the justification
is abstract, plan-based, or lengthy.
Figure 5 presents a pair of sample dialogues comparing justify and skip-justify
versions of a single tutorial dialogue extracted from project log files. In part (a), a justification is employed to guide the student (lines 3–4); while in part (b), the justification
is skipped.
6.3 The domain and eight primary knowledge components
The domain chosen for this project, Physics work-energy problem solving, is a common component of introductory college physics courses. As mentioned before, two
domain experts identified 32 KCs for this domain. However, in a domain like physics, solving a problem requires producing an argument, proof or derivation consisting
of one or more inference steps; each step is the result of applying a domain principle, operator or rule. Thus the major domain principles are more challenging and

123

Applying reinforcement learning to pedagogical strategies

161

Table 1 Major principles of work and energy
KC

Principle description

Expressions

K C1

Weight law (w)

W = mg

K C14

Definition of work (W)

W = Fdcos(α)

K C20

Definition of kinetic energy (KE)

K E = 21 mv 2

K C21

Gravitational potential energy (GPE)

G P E = mgh

K C22

Spring potential energy (SPE)

K C24

Total mechanical energy (TME)

S P E = 21 kd 2
T ME = K E + GPE + SPE

K C27

Conservation of total mechanical
energy (CTME)
Change of total mechanical
energy for non-isolated systems
(TMENC)

K C28

T M E1 = T M E2
N etW = T M E 2 − T M E 1

important than other KCs since the student’s overall learning performance depends
more on learning domain principles. Therefore, when inducing NormGain policies,
the decision was made to focus only on the eight primary KCs corresponding to the
eight major domain principles.
The eight major principles in the domain are shown in Table 1. In Table 1, the first
column lists its corresponding KC number. The second column describes the name
of the principle. The last column is the formula or mathematical expression of the
principle.
6.4 Procedure
All participants in this project experienced the same five standard phases: (1) background survey, (2) pre-training, (3) pre-test, (4) training, and (5) post-test. Unless
specified explicitly in the following, the procedure, reading contents, training materials, GUI, and test items were identical across all groups and in each phase there were
no time limits.
The background survey asked students for demographic information such as gender, age, SAT scores, high school GPA, experience with algebra, calculus, physics,
and other information.
Following the background survey, students read the physics textbook during the
pre-training and took the pre-test. The physics textbook was only available during
phase 2, pre-training.
In phase 4, students were first trained to solve a demonstration problem, which
did not include physics content, on Cordillera. The sole purpose of this step was to
familiarize them with the GUI interface. They then solved the same seven training
problems in the same order on corresponding versions of Cordillera.
Finally, students took the post-test. The pre- and post-tests were identical in this
project. Both contained a total of 33 problems selected from the Physics literature by
two domain experts (not the authors). The 33 problems covered 168 KC applications.
The tests were given online and consisted of both multiple-choice and open-ended
questions. Open-ended questions required the students to derive an answer by writing

123

162

M. Chi et al.

or solving one or more equations. Once an answer was submitted, students automatically proceeded to the next question without receiving any feedback on the correctness
of a response. Students were not allowed to return to prior questions.
As mentioned above, the reward function for the RL algorithm is the NLG. Therefore we used identical pre- and post-tests to avoid the need to factor out test differences.
Students were not informed that the tests would be identical at any point; they received
no feedback on their test answers or test scores; and the minimum time between the
pre- and post-test was 1 week.
Overall, only three salient differences exist between the three groups:
1. The Exploratory group with a population of 64 was recruited in 2007; the DichGain
group with a population of 37 was recruited in 2008; and the NormGain group
with a population of 29 was recruited in 2009.
2. Random–Cordillera made random decisions and the DichGain–Cordillera and
NormGain–Cordillera followed the induced DichGain and NormGain policies
respectively.
3. A group of six human wizards were used by the Exploratory and DichGain groups;
but only one of the six wizards was available for the NormGain group.
6.5 Grading
All tests were graded in a double-blind manner by a single experienced grader by mixing all students’ test answers together. In a double-blind manner, the grader does not
know the group to which each answer belongs nor the test, pre-test or post-test to which
each answer belongs. For all identified relevant KCs in a test question, a KC-specific
score for each KC application was given. We evaluated the student’s competence in
the following sections based on the sum of these KC-specific scores, named cumulative KC-specific scores. This is because the KC-specific pre- and post-test scores
were used to define the reward functions when applying RL to induce KC-specific
policies. Later analysis showed that the same findings held for other scoring rubrics.
For comparison purposes all test scores were normalized to fall in the range of [0,1].
7 Inducing NormGain pedagogical policies
When inducing NormGain policies we mainly focused on the eight primary KCs, one
for each major domain principle. Thus, the overall problem of inducing a policy for
ET decisions and a policy for JS decisions is decomposed into 8 sub-problems of each
kind, one per KC. Among the eight KCs, K C 1 does not arise in any JS decisions and
thus only an ET policy was induced for it. For each of the remaining seven KCs, a
pairs of policies, one ET policy and one JS policy, were induced. So we induced 15
KC-specific NormGain policies. During the tutoring process, there were some decision steps that did not involve any of the eight primary KCs. For them, two KC-general
policies, an ET policy and a JS policy, were induced. To sum, a total of 17 NormGain
policies were induced in Stage 3.
Both inducing KC-general and KC-specific policies shared the same common procedure. First, we need to define < , A, R >, choose a training corpus , and

123

Applying reinforcement learning to pedagogical strategies

163

determine the maximum number of features, m̂ included in the induced policy. Once
these factors are decided, the general procedure for the 12 feature selection methods
described in Sect. 4 are followed. In this project, we used the same state feature choices
in , , and m̂ for inducing each of the 17 NormGain policies.
7.1 State representation set 
As described above, an effective state representation should minimize state size while
retaining sufficient relevant information about the learning context. In this project, 
consists of only features that could be computed automatically or evaluated objectively, such as gender. Hand-annotated dialogue features were omitted as the tutor
would require the features to be available in real time when the induced policies are
employed. Next, we will describe the 50 feature choices in our .
The 50 feature choices were defined based upon six categories of features considered by previous research (Beck et al. 2000; Forbes-Riley et al. 2007; Moore et al.
2004) to be relevant for making tutorial decisions.
Autonomy (A) Features relate to the amount of work performed by the student in
the dialogue. We defined five numeric features, which end with an ‘A’ in their names.
For example, [tellsSinceElicitA] is one of the five features in this category. It refers to
the number of tells the student has received since the last elicit prompt, irrespective
of the KC involved. For example, tellsSinceElicitA = 2 means that two tell decisions
have been made since the last elicit decision. This feature reflects how active a student
may be currently, that is, how much work the student has performed recently.
Background (BG) features describe general background information about the
student. The five Background Features include gender, age, Math SAT, Verbal SAT, and
pre-test scores. None of these features change during training on Cordillera. All five
background features end with “BG”. One important note was that for DichGain group,
the following features, gender, age, Math SAT, and Verbal SAT, were not available
because of an administrative error.
Problem Solving Contextual (PS) features encode information about the current
problem-solving context. All fifteen problem solving-related features end with ‘PS.’
For example; one feature defined in this category is StepSimplicityPS. It is used when
inducing the single-feature policy in Fig. 1 and the three-feature policy in Fig. 2).
StepSimplicityPS is always in the range of [0, 1]. “StepSimplicityPS” = 1 means it is
a easy step, whereas if it is close to 0, it means it is a difficult question.
Performance (PM) features describe information about student performance during the training. All 12 performance-related features end with “PM.” For example,
pctCorrectKCPM refers to the percentage of the student’s correct entries on a KC. It
is calculated by assessing all of the correct cases on the present KC in the student’s
entries divided by the total number of cases the present KC appeared in the student’s
entries. This feature reflects the student’s overall competence on the current KC.
Student Dialogue (SD) features characterize students language. All SD related features end with “SD”. They are simple linguistic features that are computed from the student’s dialogue turns. For example, one feature in this category is stuAverageWordsSD,
which refers to the average number of words per student turn. This feature reflects
how verbose the student was overall.

123

164

M. Chi et al.

Temporal Situation (T) features encode time-related information about the problemsolving process. All three temporal situation features are numeric and end with a ‘T’
in their names. For example, one feature in this category is durationKCBetweenDecisionT. It refers to the time since the last tutorial decision was made on the current
KC. This feature reflects how active a student’s knowledge of the current KC is. If
it is high, it means that the tutor has not mentioned the KC recently so the student’s
knowledge about the current KC may be less activated.
7.2 Three training corpora
The choice of training corpus  is a complex one. In Stage 3, we have three training corpora available: the Exploratory corpus ex plorator y consisted of 64 complete
tutorial dialogues; the DichGain corpus  DichGain contained 37; and the combined
corpus combined comprised a total of 101 dialogues. ex plorator y was collected for
RL and designed to explore the feature space evenly and without bias.  DichGain , by
contrast, is similar to many other pre-existing corpora by following a set of specific
pedagogical strategies. Inducing a successful policy from  DichGain would show the
potential for applying RL to induce effective tutorial policies from most pre-existing
data. combined , in theory, offers the benefits of both as well as an increased dataset.
Across three corpora, the total number of ET decisions in a tutorial dialogue ranged
from 250 to 332 and we have: M = 273.89, S D = 12.46 in ex plorator y and M =
270.54, S D = 10.00 in  DichGain ; the number of JS tutorial decisions ranged from 52
to 71, we have M = 56.61, S D = 3.43 in ex plorator y and M = 58.43, S D = 2.82
in  DichGain ; the total number of the tutorial decisions regardless of decision types
for each system-student interaction dialogue ranged from 288 to 372,3 we have M =
305.48, S D = 14.01 in ex plorator y and M = 307.57, S D = 12.45 in  DichGain .
On a KC by KC basis, however, the average number of tutorial decisions varies
significantly across KCs: from as few as four on K C1 to more than 80 on K C20 . The
average number of tutorial decisions on elicit/tell (ET) and justify/skip-justify (JS)
also varies across the eight primary KCs. There are only 4 ET decisions on K C1 and
more than 70 ET decisions on K C20 . Similarly, there are only two JS decisions for
K C14 on average and more than 16 for K C21 . Overall, the ET tutorial decisions were
much more frequent than the JS ones.
In this project, when inducing NormGain policies, rather than selecting one corpus
a priori, all three were used. More specifically, a set of tutorial policies were derived
from each training corpus separately and then the best policy from all sets were selected
by ECR.
7.3 Maximum number of features m̂
As mentioned above, in order to determine m̂, it is necessary to consider the amount
of available data and available computational power. In the worst case scenario,
3 overall decisions < ET decisions + JS decisions because on certain tutorial decision steps, the tutor

makes both types of decisions: JS first and then ET. When we calculated the overall decisions, such a step
was counted as one decision step.

123

Applying reinforcement learning to pedagogical strategies

165

there were only two JS tutorial decision steps in the DichGain training corpus for
K C14 . Therefore, based on the minimum data available from the three training corpora, we capped the number of features in each policy at six, m̂ = 6, which means
that there are at least 26 = 64 states in the learned policy. Alternatively, we could
have used a flexible number for different KCs. However, it is not the case that
learned tutorial policies with six features were most effective and instead the final
17 induced NormGain policies primarily have 3–5 features in their state representation. Only one of 17 final policies has six features. As shown earlier, the threefeature policy in Fig. 2 is already quite subtle so it appears that six is a reasonable
number.
Once < , A, R >, , and m̂ are determined we just need to follow the 12 feature
selection procedures described in Sect. 4 to induce the 17 NormGain policies. Inducing KC-specific policies can be seen as a special case of inducing KC-general policies.
Therefore, we will start with our policy induction procedure on two KC-general policies first.
7.4 Inducing two KC-general policies
When inducing KC-general pedagogical policies on ET or JS decisions, we have
A = {Elicit, T ell} or A = {J usti f y, Ski p-NLGJ usti f y} respectively. The reward
function R is the same for inducing either action A, which is calculated based upon
students’ NLGs in that we have: R = N LG ×100. Moreover, the KC-specific features
defined in  become KC-general features by taking into account all of the previous
instances regardless of KC. For example, nCorr ect K C P M becomes the number of
correct responses on all the KCs instead of on a specific KC.
Once < , A, R >, , and m̂ are all defined, we can apply our 12 feature selection methods to get the best policy for corresponding action decision A. The overall
procedure is described in Algorithm 3.
Algorithm 3 Induce Two KC-General Pedagogical Policy on ET and JS
for Action in [E T, J S] do
if Action = E T then
A = {Elicit, T ell}
else
if Action = J S then
A = {J usti f y, Ski p − J usti f y}
end if
end if
A = ∅
for  in [ E x plorator y ,  DichGain , Combined ] do
Calculate reward function R = N LG × 100
 A, = Apply 12 feature selection on < , A, R >, , and m̂ (see Sect. 3)
 A =  A ∪  A,
end for
∗ from  by ECR
Select π A
A
∗
RETURN π A
end for

123

166

M. Chi et al.

As described in Sect. 4, the general feature selection procedure would result in:
one single-feature-policy for each feature choice in  and 13 × (m̂ − 1) multi-feature policies for a given . In this project, we defined 50 features in  and m̂ = 6.
For ET decisions, a total of 50 + 13 ∗ (6 − 1) = 115 KC-general ET policies were
induced from each training corpus. Taken together, all three corpora resulted in a total
of 115 × 3 = 445 KC-general ET policies. The final best KC-general ET policy was
selected from the pool by ECR. For the purposes of this project, the highest ECR
irrespective of the confidence bounds or hedging was selected. The same procedure
was also followed for inducing the KC-general JS NormGain policy.
Inducing 15 KC-specific NormGain policies followed the same general procedure
as inducing the two KC-general NormGain policies except that < , A, R > are
KC-specific. Next, we will briefly describe the procedure.
7.5 Inducing 15 KC-specific pedagogical policies
In order to learn KC-specific policies, KC-specific < , A, R > are needed. For
example, to induce policies on the ET decisions involving K C20 , we consider the ET
decisions involving K C20 only, the state representation for K C20 only, and use the
learning gains on K C20 only. Therefore, we annotated our tutoring dialogues with the
KCs covered by the content and action decisions with the KCs covered by each action.
A group of five individuals (including the first author) annotated each of the tutoring
dialogues and action decisions with the relevant KCs. The KCs were drawn from the
set of 32 identified KCs. For each of the seven training problems, there were at least
two annotators. For each of 32 identified KCs, the final kappa was ≥0.77 which is
fairly high given the complexity of the task. After each utterance in the system-student
interaction dialogue was annotated with the corresponding KCs, a KC-specific  can
be defined and a KC-specific A can be identified.
Additionally, a domain expert (not the authors) also mapped the pre-/post test problems to the sets of relevant KCs. So we can calculate students’ KC-specific reward
functions R defined as: N LG K Ci × 100.
Once KC-specific <  K C , A K C , R K C > are defined, we applied a similar procedure as for inducing KC-general NormGain in Algorithm 3.
To summarize, to induce each of 17 NormGain policies, three training corpora, a
space of 50 features, and 12 feature selection methods were explored and a total of
115 × 3 = 445 potential policies were generated. Each final NormGain policy was
selected from its corresponding pool by ECR. For example, the three-feature example
policy shown in Fig. 2 is one of the final 17 NormGain policies. It is a KC-specific ET
policy on K C20 that is induced from the Exploratory Corpus ex plorator y .
The resulting 17 NormGain policies were added to Cordillera yielding a new version of the system, named NormGain–Cordillera. In order to execute these tutorial
policies, the dialogue manager needed to keep a record of the student’s current states
on each KC. Moreover, it also retained a KC-specific record for the tutorial decision
steps. So when a tutorial decision step occurred, the dialogue manager first looked up
the KC(s) involved in that step and then looked up the corresponding policies. When
a tutorial decision did not involve any specific KCs, the dialogue manager followed
the KC-general tutorial policies. Next, the induced tutorial policies were evaluated on

123

Applying reinforcement learning to pedagogical strategies

167

real human subjects to see whether the students whose interactions were guided by the
NormGain tutorial policies would out-perform those guided by random or DichGain
ones.
8 Experimentally evaluating the NormGain policies
The goal of the evaluation reported below is twofold: first, to test whether our improved
RL methodology and software produced more effective pedagogical strategies than
either random policies or the DichGain policies; and second, to determine the characteristics of the NormGain policies.
8.1 Overall learning results
A one-way ANOVA showed that there were no significant differences among the three
groups on overall training time: F(2, 122) = 1.831, p = 0.17. More specifically,
the average total training time (in minutes) across the seven training problems, was
M = 278.73 min, S D = 67.38 for Exploratory group, M = 294.33 min, S D = 87.51
for DichGain group, and M = 259.99 min, S D = 59.22 for NormGain group.
After solving seven training problems on Cordillera, all three groups scored significantly higher in the posttest than pretest: F(1, 126) = 10.40, p = 0.002 for
the Exploratory group, F(1, 72) = 7.20, p = 0.009 for the DichGain group, and
F(1, 56) = 32.62, p = 0.000 for the NormGain group respectively. The results suggested that the basic practices and problems, domain exposure, and interactivity of
Cordillera might help students learn even from tutors with non-optimal pedagogical
strategies.
A one-way ANOVA was used for comparing the learning performance differences
among the three groups. While no significant pre-test score differences were found:
F(2, 127) = 0.53, p = 0.59, there were significant differences among the three
groups on both post-test scores and NLG scores: F(2, 127) = 5.16, p = 0.007 and
F(2, 127) = 7.57, p = 0.001 respectively. Figure 6 compares the three groups on
the pre-test, post-test, and NLG scores. Moreover, a t-test comparison showed that
the NormGain group out-performed the DichGain group on both post-test scores and
NLG scores: t (64) = 3.28, p = 0.002, d 4 = 0.82 and t (64) = 3.68, p = 0.000, d =
0.95 respectively. Similar results were found between the NormGain and Exploratory
groups: t (91) = 2.76, p = 0.007, d = 0.63 on post-test, and t (91) = 3.61, p =
0.000, d = 0.84 on NLG scores respectively.
To summarize, the comparison among the three groups shows that the NormGain
group significantly outperformed both the Exploratory and DichGain groups. These
results were consistent both for the post-test scores and the NLGs and the effect sizes
were large by Cohen’s d criteria.

4 Cohen’s d is defined as the mean learning gain of the experimental group minus the mean learning gain

of the control group, divided by the groups’ pooled standard deviation.

123

168

M. Chi et al.
Maximum Score is 1

DichGain

0.53

0.60

0.50

0.42

0.50

NormGain

0.65

0.70

0.38

Exploratory

0.42

0.41

0.40
0.22

0.30

0.22

0.20
0.10
0.00
0.10

Pretest

Posttest

NLG

Fig. 6 Compare three groups learning performance under overall grading

8.2 Log analysis
Having compared the individual groups’ learning performance, this subsection will
compare the log file variations across the three groups. In order to quantify the amount
of different decisions, we define an Interactivity ratio (I-ratio) and a Justification ratio
(J-ratio).
I-ratio is defined as the number of elicit decisions a student received divided by
the total number of ET decisions received in the entire tutorial dialogue. The higher
this value, the more interactive the tutorial dialogue. Similarly, J-ratio is defined as the
number of times the tutor executes a justification step divided by the total number of
JS decisions the tutor made in a tutorial dialogue. The higher this value, the deeper and
more elaborate the dialogue may be. Both values range from 0 (no elicits or justifies)
to 1 (all elicitation or justification).
8.2.1 I-ratio
Table 2 summarizes t-test comparisons on the I-ratio among the three tutorial corpora.
In Table 2, the first two columns list the two groups in comparison and their corresponding mean and SD scores. The last column lists the statistical results of the
t-test comparisons. From Table 2, the I-ratios for the three student groups were: 0.76
(NormGain), 0.44 (DichGain), and 0.50 (Exploratory) respectively and the differences
among them were significant: we have N or mGain > E x plorator y > DichGain.
Therefore, the NormGain policies seemingly resulted in more interactive tutorial dialogue than either the random or the DichGain policies.
8.2.2 Justify ratio
Similarly, Table 3 summarizes t-test comparisons on J-ratio among the three tutorial
corpora. In Table 3, the first two columns list the two groups in comparison and their
corresponding mean and SD scores. The last column lists the statistical results of the

123

Applying reinforcement learning to pedagogical strategies

169

Table 2 Pairwise comparison among three groups on I-ratio
Group 1

Group 2

Group 1 vs. Group 2

NormGain

0.76 (0.07)

Exploratory

0.50 (0.03)

t (91) = 24.72, p = 0.000

NormGain

0.76 (0.07)

DichGain

0.44 (0.04)

t (64) = 22.08, p = 0.000

Exploratory

0.50 (0.03)

DichGain

0.44 (0.04)

t (99) = 7.967, p = 0.000

Table 3 Pairwise comparison among three groups on J-ratio
Group 1

Group 2

Group 1 vs. Group 2

NormGain

0.82 (0.07)

Exploratory

0.53 (0.06)

t (91) = 18.95, p = 0.000

NormGain

0.82 (0.07)

DichGain

0.43 (0.07)

t (64) = 22.85, p = 0.000

Exploratory

0.53 (0.06)

DichGain

0.43 (0.07)

t (99) = 7.894, p = 0.000

t-test comparisons. Table 3, shows that the mean of J-ratios for the three student groups
were: 0.82 (NormGain), 0.43 (DichGain), and 0.53 (Exploratory) and the pair wise
t-test comparisons show that on J-ratio, we have: N or mGain > E x plorator y >
DichGain.
To summarize, the NormGain policies produced substantially more elicit and more
justifications than the random or DichGain ones. In the next section the discussion
will focus on some general characteristics of the induced tutorial policies. The 17
induced NormGain tutorial policies will be described by identifying the training corpus that each final tutorial tactic was derived from, which feature categories were most
frequently involved in the final tutorial policies, and which feature selection method
discovered the most final tutorial policies.

8.3 The impact of induction decisions on the 17 NormGain policies
The purpose of this section is to determine how the RL-related induction decisions
described in the previous sections impacted the induced tutorial policies. For example, one decision was made to use all three training corpora; did the final induced
NormGain policies come from one corpus or from all three corpora? Moreover, which
features appeared in the final induced NormGain policies? Which feature selection
method(s) seemed to be more effective? This section begins with a discussion of the
training corpus involved in the final 17 NormGain tutorial policies.
8.3.1 Source training corpus, 
Table 4 shows the source training corpus used to induce each of the 17 NormGain
tutorial policies. As described above, among the 17 NormGain policies, two were
KC-general policies, an ET policy and a JS policy. All of the remaining 15 NormGain
policies are KC-specific policies; eight were on ET decisions, one for each of eight

123

170
Table 4 The source training
corpus of the induced 17
NormGain tutorial policies

M. Chi et al.
ET

JS

1

K C1

 DichGain

−

2

K C14

Combine

 E x plorator y

3

K C20

 E x plorator y

 E x plorator y

4

K C21

 E x plorator y

 E x plorator y

5

K C22

 E x plorator y

 E x plorator y

6

K C24

 DichGain

 E x plorator y

7

K C27

 E x plorator y

 E x plorator y

8

K C28

 DichGain

 DichGain

9

K C-general

 E x plorator y

 DichGain

major domain principles, and seven were on JS decisions, one for each major domain
principle except for K C 1 . K C 1 does not arise in any JS decisions.
In Table 4, the first column lists the row number. The second column lists the KC
on which the induced NormGain policy is specific. The third and fourth columns show
the source training corpus used in deriving NormGain tutorial policies on ET and JS
for corresponding KCs respectively. For example, the cell “ DichGain ” in the first row
shows that the final NormGain ET policy on K C1 is induced by using  DichGain .
While the last cell in the first row is empty because K C 1 does not arise in any JS
decisions and thus no corresponding policy was induced.
Table 4 shows that all three Corpora were involved in generating the final tutorial
policies. However, the majority of the NormGain tutorial policies were induced by
using  E x plorator y , 11 (5 ET and 6 JS) out of 17;  DichGain was used to generate five
(3 ET and 2 JS); and Combine only generated one NormGain policy. It is not very
intuitive to determine why most of the NormGain policies were from  E x plorator y .
Future work is needed to explore the characteristics of a training corpus and how to
choose a training corpus. However, this result reinforces the importance of collecting
an Exploratory Corpus when applying RL to induce effective pedagogical policies.
8.3.2 Feature selection
To induce each of the 17 NormGain policies, we applied 12 feature selection methods
on 50 feature choices. It would be interesting to see which feature selection method(s)
found the most final NormGain tutorial policies. Table 5 lists all the feature selection
methods that were followed to get the final 17 NormGain tutorial policies. “single”
means it is a single feature policy.
In Table 5, the first column lists the row number. For example, the ninth row shows
that the final KC-general NormGain policy on ET decisions is induced through applying “Lower-Bound” feature selection while the KC-general NormGain policy on JS
decisions is induced through applying “ECR” feature selection.
From Table 5, it can be concluded that the five feature selection approaches: PCAonly, PCA–ECR, PCA–UpperBound, PCA–LowerBound, and random did not elicit
any of the final tutorial policies. All other six approaches resulted in at least one.

123

Applying reinforcement learning to pedagogical strategies
Table 5 Applying 11 feature
selection methods to induce 34
tutorial policies

171
ET

JS
−

1

K C1

Single

2

K C14

Single

Single

3

K C20

ECR

PCA–Hedge

4

K C21

Upper_Bound

PCA–Hedge

5

K C22

Hedge

Upper_Bound

6

K C24

ECR

Upper_Bound

7

K C27

PCA–Random

ECR

8

K C28

Upper_Bound

Upper_Bound

9

K C-general

Lower_Bound

ECR

Among them, the two RL-based feature selection methods appeared to be most effective. The Upper_Bound method found five NormGain tutorial policies and the ECR
based method discovered four NormGain tutorial policies.
Previously, we explored four RL-based feature selection methods on 18 features in
Chi et al. (2008). That work also showed that the Upper-Bound selection seemed to
be the best among the four. When inducing NormGain policies, we explored the four
RL-based feature selection methods together with eight other methods on 50 features.
Our results here seems to be consistent with our previous results in Chi et al. (2008)
in that the Upper_Bound method seems to be an effective feature selection method in
applying RL to induce pedagogical policies.
One possible explanation may be due to our pedagogical goal. In this project, we
were mainly interested in improving student learning. Generally speaking, learning
occurs with low frequency but also with low risk. In other words, while learning is
difficult to achieve, the risk of not learning is also low (no learning gains). However
when learning does occur, the reward is always positive. Therefore, it is reasonable for
Upper_Bound to be a more effective feature selection method since it can take risks
toward selecting features that can cause learning to occur. In short, this result indicates
that in education, features that may result in higher learning gains should always be
considered by the tutor when making decisions. This is likely due to the fact that in
the worst case a student will simply not learn rather than lose information so the cost
of considering superfluous features is low.
Overall, the results show the relative effectiveness of our 12 feature selection
methods in that they at least beat the random feature selection. However, our feature selection may still need to be improved because one of the final induced policies
is from the PCA–Random feature selection method—the ET NormGain policy on
K C27 .
8.3.3 Feature choices
Table 6 summarizes features appearing in each of 17 NormGain policies. The first
column lists the row number. The second and third columns show the corresponding
KC and action decisions. The fourth column lists the number of features involved in

123

172

M. Chi et al.

Table 6 Features involved in 17 NormGain tutorial policies
KC

Action

No. of features

Features

1

K C1

ET

1

v24PS

2

K C14

ET

1

v5T

3

K C14

JS

1

v12PS

4

K C20

ET

3

v18PS v23PS v26PS

5

K C20

JS

5

v9T v15PS v28A v56SD v62SD

6

K C21

JS

3

v13PS v18PS v20PS

7

K C21

ET

3

v15PS v18PS v30A

8

K C22

JS

5

v23PS v24PS v27A v46PM v47PM

9

K C22

ET

2

v23PS v27A

10

K C24

JS

6

v14PS v18PS v49BG v59SD v60SD v62SD

11

K C24

ET

4

v10T v14PS v18PS v55SD

12

K C27

ET

4

v5T v17PS v23PS v35PM

13

K C27

JS

4

v9T v25PS v27A v56SD

14

K C28

JS

3

v15PS v24PS v54SD

15

K C28

ET

5

v5T v16PS v18PS v29A v41PM

16

K C-general

ET

4

v5T v18PS v27A v46PM

17

K C-general

JS

5

v16PS v17PS v23PS v25PS v27A

the corresponding NormGain policies and the last column lists all the feature names
in the corresponding policy’s state representation.
For illustration purpose, we used simplified variable names here that mainly indicate
the category of the features. Recall that the six categories are: Autonomy (A) features,
background (BG) related feature, Performance (PM) related features, Problem Solving
(PS) Contextual features, SD features, and Temporal Situation (T) features.
For example, row 10 is about the JS NormGain policy on K C24 . Row 10 shows
that there are six features, “v14PS v18PS v49BG v59SD v60SD v62SD”, in the state
representation for this policy. Among the six features, “v14PS” and “v18PS” are
Problem Solving Contextual features since their name ends with “PS”; “v49BG” is a
background feature; and the remaining three features are all SD features.
From Table 6, we can see that certain state features were involved in multiple
NormGain policies. For example, “v18PS” represents the state feature StepSimplicityPS. Table 6 shows that StepSimplicityPS appears in 7 NormGain policies as “v18PS”
is listed in row 4, 6, 7, 10, 11, 15, and 16. On the other hand, not every feature choice
in  occurred in the final 17 induced NormGain policies. For example, “v44PM” represents the feature pctCorrectKCPM (percentage of correct student entries involving
the current KC), which was not involved in any NormGain Policy. In fact, only 30 out
of 50 features occur in at least one induced NormGain policy.
By summing column 4 in Table 6, we see that the total number of feature occurrences across 17 tutorial policies was 59. For illustration reasons, Table 7 lists the
number of features defined in each of the six categories and the feature occurrences in

123

Applying reinforcement learning to pedagogical strategies
Table 7 Occurrence of six
category features in the final
NormGain tutorial policies

173
Defined
features

Feature
occurrences

1

Autonomy (A)

5

8

2

Background (BG)

5

1

3

Performance (PM)

12

5

4

Problem Solving Contextual (PS)

15

30

5

Student Dialogue (SD)

10

8

6

Temporal Situation (T)

7

Total

3

7

50

59

the final 17 NormGain policies. For example, the third and fourth columns in row 4 in
Table 7 show that there are fifteen PS features defined and they account for thirty out
of 59 feature occurrences in the final 17 tutorial policies. In other words, more than
half of all feature occurrences in the 17 NormGain policies were from PS.
Table 7 shows that both the number of features defined for each category and the
feature occurrences in the six categories is not even. However, we argue that the uneven
distribution of the feature occurrences among the six categories was not caused by the
uneven number of features defined in each category.
Across the 50 features, the most frequent feature appears seven times. Four features
appear in more than three induced policies and they are:
1. StepSimplicityPS (7 Occurrences): labeled as “v18PS” in Table 6. It is a Problem
Solving Contextual feature which encodes a step’s simplicity level and its value is
roughly estimated from the Combined Corpus based on the percentage of answers
that were correct for a step.
2. TuConceptsToWordsPS (5 Occurrences): labeled as “v23PS” in Table 6. It is a
Problem Solving Contextual feature which represents the ratio of physics concepts to words in the tutor’s dialogue.
3. tellsSinceElicitA (5 Occurrences): labeled as “v27A” in Table 6. It is an Autonomy
feature which represents the number of tells the student has received since the last
elicit.
4. durationKCBetweenDecisionT (4 Occurrences): labeled as “v5T” in Table 6. It
is a Temporal Situation feature which represents the time since the last tutorial
decision was made on the current KC.
While StepSimplicityPS can be seen as a domain-oriented feature, the remaining
three features are primarily system-behavior related features. The high occurrence
of StepSimplicityPS in the NormGain policies is not very surprising because it is
widely believed that difficulty level is an important factor in a system behaving adaptively and effectively. The frequent involvement of system-behavior related features
in the induced policy may be because these features could reflect a student’s general
aptitude and the degree to which their knowledge on a specific KC is activated. For
example, tellsSinceElicitA reflects how interactive a student has been recently and durationKCBetweenDecisionT reflects how active a student’s knowledge on the current
KC is. When durationKCBetweenDecisionT is high, it means that the tutor has not

123

174

M. Chi et al.

mentioned the KC recently so the student’s knowledge on the current KC may be less
activated.
Much to our surprise, the features related to the students’ overall or recent performance and background (e.g., MSAT, VSAT, gender, pretest score) appeared the least
in the NormGain policies. Row 4 in Table 7 shows that the 12 PM feature choices
account for only five occurrences of this category in the final 17 NormGain policies.
They seem to be less involved even than the Temporal Situation (T) related features.
With only three features in Temporal Situation category, they account for seven feature
occurrences.
Among the 12 PM features, nIncorrectKCPM (the number of incorrect responses
in the student’s dialogue so far) is the most frequently occurring feature in that it
appeared in two final NormGain tutorial policies. A feature such as pctCorrectKCPM
(percentage of correct student’s entries involving the current KC) did not appear in
any of the final tutorial policies. Additionally, only one out of five background features
occurred in one final tutorial tactic: ageBG** (the age of the student). The remaining four background features, such as MSAT, VSAT, gender, pretest score, were not
involved in the final 17 NormGain policies.
To summarize, Problem Solving Contextual Features occurred most frequently,
thirty times, in the final 17 induced tutorial policies. The features related to the students’ overall or recent performance and background appeared the least in the NormGain policies. Although space does not permit a detailed discussion of the prevalence
of features, it appears to be a mixture of easily anticipated dependencies (e.g., step
simplicity) and a few surprises (e.g., students’ overall and immediate performances
directly reflect their level of knowledge. So why don’t these factors matter?).

9 Discussion
To summarize, we described a practical methodology for using RL to improve the
effectiveness of an ITS over time. In a nutshell, our RL methodology is to:
1. Choose an appropriate reward measure for the instructional goals, an appropriate
list of features for the state representations, and identify a set of reasonable system
decisions for which a policy is needed.
2. Build an initial training system that collects an exploratory dataset (one that tries
many times from each state each of the actions between which the policy will
decide). Despite being exploratory, this system should still provide the desired
basic functionality.
3. Apply feature selection methods when necessary, based on the size of the exploratory corpus, to select a subset of features that capture the most effective factors in
the learning environment. Then we use the exploratory corpus to build an empirical
MDP model for the subset of state features. The transitions of this MDP model the
user population’s reactions and rewards for the various system action sequences.
4. Compute the optimal dialogue policy according to this learned MDP.
5. Add the learned policy to the system and evaluate the policy on a new group of
users.

123

Applying reinforcement learning to pedagogical strategies

175

In this project, we investigated pedagogical skills at a micro-step level. i.e. pedagogical
tutorial tactics. These tactics do not govern the domain solution path selected for presentation or the problems presented. They only govern low-level tutorial interactions,
e.g. whether the student is told what principle to apply or whether the system elicits it
with a prompt, and whether a student, once he/she has made a step, is asked to justify
his/her answer. If fine-grained pedagogical skills of this type turn out to be effective, then more complex or content-oriented tactics, such as problem or sub-problem
selection may be similarly effective.
Compared with previous studies on applying RL to induce pedagogical policies
for ITSs, this project makes at least three major contributions. First, we bypassed the
need for building simulated students by collecting an exploratory corpus; moreover,
we showed that a relatively small exploratory corpus is enough for inducing effective
policies. Second, we empirically showed that the RL induced policies indeed help
students learn better. Third, while much of the previous research on applying RL to
ITSs used pre-defined state representations, we defined a large set of features  to
which several feature-selection methods were applied to reduce them to a tractable
subset.
Moreover, we believe that this project also contributes to applying RL to induce
dialogue policies for dialogue systems. We showed that using a relatively small exploratory corpus, model-based RL methods such as Policy Iteration can still induce effective policies even when the reward function is much delayed and the task domain is
rather complex. We argue that this is done by a rather comprehensive definition of the
state feature choices and exploration of various feature selection methods.
More specifically, we showed that RL is able to effectively search a very large
continuous space of dialogue policies (after being discretized, the space is ≥250 in
size) using a relatively small amount of training dialogue data (64 subjects in the
Exploratory group and 37 in the DichGain group). A post-hoc comparison showed
that our learned policy outperformed both sets of training policies in terms of learning performance. Our results demonstrate that the application of RL allows one to
optimize pedagogical strategies by searching through a much larger search space than
can be explored with more traditional methods. This success supports the hypothesis
that RL-induced rules are effective and that the approach taken in this project was a
feasible one.
However, inducing effective tutorial policies was not trivial. The DichGain tutorial
policies did not seem to be more effective than the random decisions in Random–
Cordillera. A number of factors were changed in deriving NormGain policies compared to inducing DichGain policies. These changes included the feature choices, the
choice of training corpora, feature selection methods and the definition of reward functions. So it is still not clear which factor or factors caused the change in effectiveness.
We also note that our learned polices made dialogue decisions based on Problem
Solving Contextual features in conjunction with other features such as Autonomy features, and also made very subtle decisions compared with existing learning theories. As
such, our RL-induced policies are not the standard pedagogical strategies investigated
in the learning literature. For instance, it is widely believed that effective tutors adapt
their behavior to the individual student knowledge level and incoming competence
(Collins and Stevens 1982; Koedinger and Aleven 2007; Vygotsky 1978). Indeed,

123

176

M. Chi et al.

individualized tutoring is considered a Grand Challenge by the National Academy
of Engineering. However, such features appeared to play little role in the effective
tutorial policies induced from our data. Rather it appears that the Problem Solving
Contextual features are most involved in the final induced NormGain tutorial policies.
Overall it appears that the learning context features that make the most difference for
determining when to tell vs. elicit and when to Justify vs. Skip-Justify are not always
the ones that first come to mind given current theories of learning and tutoring. Overall, our results suggest that when building an accurate learning context model, adding
domain-oriented and system behavior related features would be beneficial.
Finally, our approach has begun to address some of the challenges of the prevailing
theory and application of RL (e.g., balancing the competing concerns of random exploration with user experience in the training corpus; keeping the state space as small as
possible in order to make learning data-efficient while retaining enough information for
decision-making; and providing a general methodology for reducing the state space to
a manageable size). Our RL approach showed that most of the NormGain tutorial policies were derived from the Exploratory Corpus and among the 12 feature selection
methods tried in this project, the two RL-based feature selection methods, UpperBound and ECR, were most effective. However, in order to investigate why these are
the case, we need more exploration. Additionally, one of the issues we would like to
investigate is how different choices of training corpora or feature selection methods
are correlated with learning gains. Also note that we keep the training material and
experimental procedure identical for all three studies across all three stages, therefore
it is not clear whether the induced policies would still be valid if some aspect of the
training material or experimental procedure changes.
In this project, the induced NormGain policies is at best an approximation, we may
be introducing the problem of hidden state or partial observability into the problem of
choosing optimal tutorial actions in each state. For situations with hidden state a richer
Partially observable Markov decision process (POMDP) model is often more appropriate (Hauskrecht 1997). In future work, we wish to explore POMDP as POMDPs
allow for realistic modeling of the student’s knowledge levels, the student’s intentions,
and other hidden state components by incorporating them into the state space.
Acknowledgments NSF (#0325054) supported this work. We also thank the Learning Research and
Development Center at the University of Pittsburgh for providing all the facilities used in this work.

References
Ai, H., Litman, D.J.: Knowledge consistent user simulations for dialog systems. In: Proceedings of Interspeech-2007, pp. 2697–2700, Antwerp, Belgium, 2007
Aleven, V., Ogan, A., Popescu, O., Torrey, C., Koedinger, K.R.: Evaluating the effectiveness of a tutorial
dialogue system for self-explanation. In: Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.) Intelligent
Tutoring Systems, 7th International Conference, ITS 2004, vol. 3220 of Lecture Notes in Computer
Science, pp. 443–454, Maceiò, Alagoas, Brazil, 30 August–3 September. Springer, Berlin (2004)
Anderson, J.R.: The Architecture of Cognition. Harvard University Press, Cambridge (1983)
Anderson, J.R., Corbett, A.T., Koedinger, K.R., Pelletier, R.: Cognitive tutors: lessons learned. J. Learn.
Sci. 4(2), 167–207 (1995)
Baker, R.S., Corbett, A.T., Koedinger, K.R.: Detecting student misuse of intelligent tutoring systems.
In: Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.) Intelligent Tutoring Systems, 7th International

123

Applying reinforcement learning to pedagogical strategies

177

Conference, ITS 2004, vol. 3220 of Lecture Notes in Computer Science, pp. 531–540, Maceiò, Alagoas, Brazil, 30 August–3 September. Springer, Berlin (2004a)
Baker, R.S., Corbett, A.T., Koedinger, K.R., Wagner, A.Z.: Off-task behavior in the cognitive tutor classroom: when students “game the system”. In: Dykstra-Erickson, E., Tscheligi, M. (eds.) CHI, pp. 383–
390. ACM, New York (2004b)
Barnes, T., Stamper, J.C.: Toward automatic hint generation for logic proof tutoring using historical student
data. In: Woolf, B.P., Aïmeur, E., Nkambou, R., Lajoie, S.P. (eds.) Intelligent Tutoring Systems, vol.
5091 of Lecture Notes in Computer Science, pp. 373–382. Springer, Berlin (2008)
Beck, J., Woolf, B.P., Beal, C.R.: Advisor: a machine learning architecture for intelligent tutor construction.
In: AAAI/IAAI, pp. 552–557. AAAI Press/The MIT Press, Menlo Park/Cambridge (2000)
Bernsen, N.O., Dybkjaer, L.: Designing Interactive Speech Systems: From First Ideas to User Testing. Springer-Verlag New York Inc, Secaucus (1997)
Chadés, I., Cros, M.-J., Garcia, F., Sabbadin, R.: Markov decision process (MDP) toolbox v2.0 for MATLAB
(2005). http://www.inra.fr/internet/Departements/MIA/T/MDPtoolbox
Chi, M.: Do Micro-level tutorial decisions matter: applying reinforcement learning to induce pedagogical
tutorial tactics. PhD thesis, Intelligent Systems Program, University of Pittsburgh, December (2009)
Chi, M.T.H., de Leeuw, N., Chiu, M.-H., LaVancher, C.: Eliciting self-explanations improves understanding. Cogn. Sci. 18(3), 439–477 (1994)
Chi, M. Jordan, P.W., VanLehn, K., Hall, M.: Reinforcement learning-based feature selection for developing
pedagogically effective tutorial dialogue tactics. In: de Baker, R.S.J., Barnes, T., Beck, J.E. (eds.) The
1st International Conference on Educational Data Mining (EDM), pp. 258–265. Montreal, Québec,
Canada (2008). www.educationaldatamining.org
Chi, M., Jordan, P.W., VanLehn, K., Litman, D.J.: To elicit or to tell: does it matter?. In: Dimitrova, V.,
Mizoguchi, R., du Boulay, B., Graesser, A.C. (eds.) AIED, pp. 197–204. IOS Press, Amsterdam (2009)
Chi, M., VanLehn, K., Litman, D.J., Jordan, P.W.: Inducing effective pedagogical strategies using learning
context features. In: De Bra, P., Kobsa, A., Chin, D.N. (eds.) UMAP, vol. 6075 of Lecture Notes in
Computer Science, pp. 147–158. Springer, Berlin (2010)
Collins, A., Stevens, A.: Goals and strategies for inquiry teachers. Adv. Instr. Psychol. 2, 65–119 (1982)
Collins, A., Brown, J.S., Newman, S.E.: Cognitive apprenticeship: teaching the craft of reading, writing
and mathematics. In: Resnick, L.B. (ed.) Knowing, learning and instruction: essays in honor of Robert
Glaser, chap. 14, pp. 453–494. Lawrence Erlbaum Associates, Hillsdale (1989)
Conati, C., VanLehn, K.: Toward computer-based support of meta-cognitive skills: a computational framework to coach self-explanation. Int. J. Artif. Intell. Educ. 11, 398–415 (2000)
Corbett, A.T., Anderson, J.R.: Locus of feedback control in computer-based tutoring: impact on learning
rate, achievement and attitudes. In: CHI, pp. 245–252, Seattle, Washington, USA, 2001
D’Mello, S.K., Graesser, A.C.: Multimodal semi-automated affect detection from conversational cues, gross
body language, and facial features. User Model. User-Adapt. Interact. 20(2), 147–187 (2010)
D’Mello, S.K., Craig, S.D., Witherspoon, A.M., McDaniel, B., Graesser, A.C.: Automatic detection of
learner’s affect from conversational cues. User Model. User-Adapt. Interact. 18(1–2), 45–80 (2008)
Forbes-Riley, K., Litman, D.J., Purandare, A., Rotaru, M., Tetreault, J.R.: Comparing linguistic features for
modeling learning in computer tutoring. In: Luckin, R., Koedinger, K.R., Greer, J.E. (eds.): Artificial
Intelligence in Education, Building Technology Rich Learning Contexts that Work, Proceedings of
the 13th International Conference on Artificial Intelligence in Education, AIED 2007, vol. 158 of
Frontiers in Artificial Intelligence and Applications, pp. 270–277, Los Angeles, California, USA, July
9–13. IOS Press, Amsterdam (2007)
Frampton, M., Lemon, O.: Reinforcement learning of dialogue strategies using the user’s last dialogue act.
In: Proceedings of the IJCAI Workshop on K&R in Practical Dialogue Systems, pp. 62–67 (2005)
Frampton, M., Lemon, O.: Learning more effective dialogue strategies using limited dialogue move features. In: Calzolari, N., Cardie, C., Isabelle, P. (eds.) ACL 2006, 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, Sydney, Australia, pp. 185–192, 17–21 July 2006. The Association
for Computational Linguistics, Uppsala (2006)
Graesser, A.C., Person, N.K., Magliano, J.P.: Collaborative dialog patterns in naturalistic one-on-one tutoring. Appl. Cogn. Psychol. 9(6), 422–495 (1995)
Graesser, A.C., VanLehn, K., Rosé, C.P., Jordan, P.W., Harter, D.: Intelligent tutoring systems with
conversational dialogue.. AI Magazine, 22(4), 39–52 (2001)

123

178

M. Chi et al.

Hauskrecht, M.: Planning and control in stochastic domains with imperfect information. PhD thesis, MIT
(1997) (Available as Technical Report: MIT-LCS-TR-738, 1997)
Henderson, J., Lemon, O., Georgila, K.: Hybrid reinforcement/supervised learning for dialogue policies
from communicator data. In: IJCAI Workshop on K&R in Practical Dialogue Systems, pp. 68–75,
2005
Iglesias, A., Martínez, P., Fernández, F.: An experience applying reinforcement learning in a web-based
adaptive and intelligent educational system. Infor. Educ. 2(2), 223–240 (2003)
Iglesias, A., Martínez, P., Aler, R., Fernández, F.: Learning teaching strategies in an adaptive and intelligent
educational system through reinforcement learning. Appl. Intell. 31, 89–106 (2009a). doi:10.1007/
s10489-008-0115-1
Iglesias, A., Martínez, P., Aler, R., Fernández, F.: Reinforcement learning of pedagogical policies in adaptive and intelligent educational systems. Knowledge-Based Syst. 22(4), 266–270 (2009b) (Artificial
Intelligence (AI) in Blended Learning)
Janarthanam, S., Lemon, O.: User simulations for online adaptation and knowledge-alignment in troubleshooting dialogue systems. In: Proceedings of LonDial the 12th SEMdial Workshop on the Semantics
and Pragmatics of Dialogues, pp. 51–58, Stockholm, 2008
Jolliffee, I.T.: Principal Component Analysis, Springer Series in Statistics, 2nd edn. Springer,
New York (2002)
Jordan, P.W., Ringenberg, M.A., Hall, B.: Rapidly developing dialogue systems that support learning studies. In: ITS06 Workshop on Teaching with Robots, Agents and NLP, pp. 29–36 (2006). http://facweb.
cs.depaul.edu/elulis/ITS2006RobotsAgentsWorkshop.html
Jordan, P.W., Hall, B., Ringenberg, M., Cue, Y., Rosé, C.: Tools for authoring a dialogue agent that participates in learning studies. In: Luckin, R., Koedinger, K.R., Greer, J.E. (eds.) Artificial Intelligence in
Education, Building Technology Rich Learning Contexts that Work, Proceedings of the 13th International Conference on Artificial Intelligence in Education, AIED 2007, vol. 158 of Frontiers in Artificial
Intelligence and Applications, pp. 43–50, Los Angeles, CA, USA, July 9–13. IOS Press, Amsterdam
(2007)
Kaelbling, L.P., Littman, M.L., Moore, A.W.: Reinforcement learning: a survey. J. Artif. Intell. Res. 4, 237–
285 (1996)
Katz, S., O’Donnell, G., Kay, H.: An approach to analyzing the role and structure of reflective dialogue. Int.
J. Artif. Intell. Educ. 11(3), 320–343 (2000)
Koedinger, K.R., Aleven, V.: Exploring the assistance dilemma in experiments with cognitive tutors. Educ.
Psychol. Rev 19(3), 239–264 (2007)
Koedinger, K.R., Anderson, J.R., Hadley, W.H., Mark, M.A.: Intelligent tutoring goes to school in the big
city. Int. J. Artif. Intell. Educ. 8(1), 30–43 (1997)
Levin, E., Pieraccini, R.: A stochastic model of computer–human interaction for learning dialogue strategies.
In: EUROSPEECH 97, pp. 1883–1886, 1997
Litman, D.J., Silliman, S.: Itspoke: an intelligent tutoring spoken dialogue system. In: Demonstration
Papers at HLT-NAACL 2004, pp. 5–8. Association for Computational Linguistics, Morristown, NJ,
USA (2004)
Martin, K.N., Arroyo, I.: Agentx: using reinforcement learning to improve the effectiveness of intelligent
tutoring systems. In: Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.) Intelligent Tutoring Systems, 7th
International Conference, ITS 2004, vol. 3220 of Lecture Notes in Computer Science, pp. 564–572,
Maceiò, Alagoas, Brazil, 30 August–3 September. Springer, Berlin (2004)
McKendree, J.: Effective feedback content for tutoring complex skills. Human–Computer Interaction, 5(4), 381–413 (1990)
Moore, J.D., Porayska-Pomsta, K., Varges, S., Zinn, C.: Generating tutorial feedback with affect. In: Barr,
V., Markov, Z. (eds.) FLAIRS Conference, pp. 923–928. Menlo Park, (2004)
Newell, A.: Unified Theories of Cognition, Reprint edition. Harvard University Press, Cambridge (1994)
Paek, T., Chickering, D.: The Markov assumption in spoken dialogue management. In: 6th SIGDial Workshop on Discourse and Dialogue, pp. 35–44, 2005
Pain, H., Porayska-Pomsta, K.: Affect in one-to-one tutoring. In: Ikeda, M., Ashley, K.D., Chan, T.-W.
(eds.): Intelligent Tutoring Systems, 8th International Conference, ITS 2006, p. 817, Jhongli, Taiwan,
26–30 June 2006, Proceedings, vol. 4053 of Lecture Notes in Computer Science. Springer, Berlin
(2006)
Phobun, P., Vicheanpanya, J.: Adaptive intelligent tutoring systems for e-learning systems. Procedia Soc.
Behav. Sci. 2(2), 4064–4069 (2010)

123

Applying reinforcement learning to pedagogical strategies

179

Porayska-Pomsta, K., Mavrikis, M., Pain, H.: Diagnosing and acting on student affect: the tutor’s perspective. User Model. User-Adapt. Interact. 18(1–2), 125–173 (2008)
Raux, A., Langner, B., Bohus, D., Black, A.W., Eskenazi, M.: Let’s go public! Taking a spoken dialog
system to the real world. In: Proceedings of Interspeech (Eurospeech), pp. 885–888, Lisbon Portugal,
2005
Rieser, V., Lemon, O.: Using machine learning to explore human multimodal clarification strategies. In:
Calzolari, N., Cardie, C., Isabelle, P. (eds.) ACL 2006, 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of
the Conference, Sydney, Australia, pp. 659–666, 17–21 July 2006. The Association for Computational
Linguistics, Uppsala (2006)
Ringenberg, M.A., VanLehn, K.: Scaffolding problem solving with annotated, worked-out examples to
promote deep learning. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.): Intelligent Tutoring Systems,
8th International Conference, ITS 2006, pp. 625–636, Jhongli, Taiwan, 26–30 June 2006, Proceedings,
vol. 4053 of Lecture Notes in Computer Science. Springer, Berlin (2006)
Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., Shern, R., Lenzo, K., Xu, W., Oh, A.: Creating
natural dialogs in the Carnegie Mellon communicator system. In: Proceedings of Eurospeech, vol. 4,
pp. 1531–1534, 1999
Singh S.P., Kearns, M.J., Litman, D.J., Walker, M.A.: Reinforcement learning for spoken dialogue systems.
In: Solla S.A., Leen, T.K., Müller, K.-R. (eds.) NIPS, pp. 956–962. The MIT Press, Cambridge (1999)
Singh, S.P., Litman, D.J., Kearns, M.J., Walker, M.A., Marilyn, A.: Optimizing dialogue management with
reinforcement learning: Experiments with the NJfun system. J. Aritif. Intell. Res. (JAIR) 16, 105–
133 (2002)
Stamper, J.C., Barnes, T., Croy, M.J.: Extracting student models for intelligent tutoring systems. In: AAAI,
pp. 1900–1901, Vancouver, British Columbia, Canada, July 22–26. AAAI Press, Stanford, CA (2007)
Sutton, R.S., Barto, A.G.: Reinforcement Learning. MIT Press Bradford Books, Cambridge (1998)
Tetreault, J.R., Litman, D.J.: Comparing the utility of state features in spoken dialogue using reinforcement learning. In: Moore, R.C., Bilmes, J.A., Chu-Carroll, J., Sanderson, M. (eds.) Proceedings of
the Human Language Technology Conference of the NAACL, Main Conference, pp. 272–279. The
Association for Computational Linguistics, New York (2006a)
Tetreault, J.R., Litman, D.J.: Using reinforcement learning to build a better model of dialogue state. In: Proceedings 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL), pp. 289–296, Trento, Italy, 2006b
Tetreault, J.R., Bohus, D., Litman, D.J.: Estimating the reliability of MDP policies: a confidence interval
approach. In: Sidner, C.L., Schultz, T., Stone, M., Zhai, C. (eds.) HLT-NAACL, pp. 276–283. The
Association for Computational Linguistics, Boston (2007)
Tetreault, J.R., Litman, D.J.: A reinforcement learning approach to evaluating state representations in spoken
dialogue systems. Speech Commun. 50(8–9), 683–696 (2008)
VanLehn, K.: The behavior of tutoring systems. Int. J. Artif. Intell. Educ. 16(3), 227–265 (2006)
VanLehn, K., Graesser, A.C., Jackson, G.T., Jordan, P.W., Olney, A., Rosé, C.P.: When are tutorial dialogues
more effective than reading?. Cogn. Sci. 31(1), 3–62 (2007a)
VanLehn, K., Jordan, P., Litman, D.: Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed. In: Proceedings of SLaTE Workshop on Speech and Language Technology in
Education ISCA Tutorial and Research Workshop, pp. 17–20, 2007b
VanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A.,
Wintersgill, M.: The andes physics tutoring system: lessons learned. Int. J. Artif. Intell.
Educ. 15(3), 147–204 (2005)
Vygotsky, L.S.: Interaction between learning and development. In: Mind and Society, pp. 79–91. Harvard
University Press, Cambridge MA (1978)
Walker, M.A.: An application of reinforcement learning to dialogue strategy selection in a spoken dialogue
system for email. J. Artif. Intell. Res. 12, 387–416 (2000)
Williams, J.D., Poupart, P., Young, S.J.: Factored partially observable Markov decision processes for dialogue management. In: 4th Workshop on Knowledge and Reasoning in Practical Dialog Systems,
International Joint Conference on Artificial Intelligence (IJCAI), pp. 76–82, Edinburgh, 2005
Williams, J.D., Young, S.: Partially observable Markov decision processes for spoken dialog systems. Comput. Speech Lang. 21(2), 231–422 (2007a)
Williams, J.D., Young, S.: Scaling POMDPs for spoken dialog management. IEEE Trans. Audio Speech
Lang. Process. 15(7), 2116–2129 (2007b)

123

180

M. Chi et al.

Wylie, R., Koedinger, K., Mitamura, T.: Is self-explanation always better? the effects of adding self-explanation prompts to an english grammar tutor. In: Proceedings of the 31st Annual Conference of the
Cognitive Science Society, COGSCI 2009, pp. 1300–1305, Amsterdam, The Netherlands, 2009

Author Biographies
Min Chi is a Post-Doctoral Fellow in the Machine Learning Department and Pittsburgh Science of Learning Center at Carnegie Mellon University. Dr. Chi received her B.A. degree in Information Science and
Technology from Xi’an Jiaotong University and in 2009 received her Ph.D. degree in Intelligent Systems
from the University of Pittsburgh. Her research focuses on the applications of machine learning techniques
to interactive learning environments. In particular, she has focused on applying machine learning techniques
to induce pedagogical strategies that can automatically adapt effectively to differences in students’s performances, the subject matter, and instructional goals. She is also interested in comparing machine-learned
pedagogical strategies to some of the existing learning theories.
Kurt VanLehn is a Professor of Computer Science at Arizona State University. He completed his Ph.D.
at MIT, did post-doctoral research at Xerox PARC, joined the faculty of Carnegie-Mellon University in
1985, moved to the University of Pittsburgh in 1990 and joined ASU in 2008. His work involves applications of artificial intelligence to education, including ITSs, cognitive modeling and educational data mining.
Diane Litman is presently Professor of Computer Science, Senior Scientist with the Learning Research
and Development Center, and faculty in Intelligent Systems, all at the University of Pittsburgh. Previously
she was a member of the Artificial Intelligence Principles Research Department, AT&T Labs—Research
(formerly Bell Laboratories), and an Assistant Professor of Computer Science at Columbia University. Dr.
Litman received her B.A. degree in Mathematics and Computer Science from the College of William and
Mary, and her M.S. and Ph.D. degrees in Computer Science from the University of Rochester. Dr. Litman’s
current research focuses on enhancing the effectiveness of ITSs through spoken language processing, affective computing, and machine learning.
Pamela Jordan is a Research Associate at the University of Pittsburgh, working in the area of natural language tutorial dialogue. She received her B.S. in Computer Science from the University of Virginia, M.S.
in Computer Science from George Mason University, M.S. in Computational Linguistics from Carnegie
Mellon University and Ph.D. in Intelligent Systems from University of Pittsburgh. She completed her Ph.D.
under the supervision of Richmond Thomason, in the area of natural language discourse and dialogue.
The joint research described in this volume reflects her interest in building tutorial dialogue systems and
understanding how educational dialogues impact human learning.

123

Computers & Education 75 (2014) 196–217

Contents lists available at ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Evaluation of a meta-tutor for constructing models of
dynamic systems
Lishan Zhang*, Kurt VanLehn, Sylvie Girard, Winslow Burleson,
Maria Elena Chavez-Echeagaray, Javier Gonzalez-Sanchez, Yoalli Hidalgo-Pontet
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe, AZ 85281, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 5 September 2013
Received in revised form
17 February 2014
Accepted 25 February 2014
Available online 13 March 2014

Modelling is an important skill to acquire, but it is not an easy one for students to learn. Existing
instructional technology has had limited success in teaching modelling. We have applied a recently
developed technology, meta-tutoring, to address the important problem of teaching model construction.
More speciﬁcally, we have developed and evaluated a system that has two parts, a tutor and a meta-tutor.
The tutor is a simple step-based tutoring system that can give correct/incorrect feedback on student’s
steps and can demonstrate steps for students when asked. Because deep modelling requires difﬁcult
analyses of the quantitative relationships in a given system, we expected, and found, that students
tended to avoid deep modelling by abusing the tutor’s help. In order to increase the frequency of deep
modelling, we added a meta-tutor that coached students to follow a learning strategy that decomposed
the overall modelling problem into a series of “atomic” modelling problems. We conducted three experiments to test the effectiveness of the meta-tutor. The results indicate that students who studied with
meta-tutor did indeed engage in more deep modelling practices. However, when the meta-tutor and
tutor were turned off, students tended to revert to shallow modelling. Thus, the next stage of the
research is to add an affective agent that will try to persuade students to persist in using the taught
strategies even when the meta-tutoring and tutoring have ceased.
Ó 2014 Elsevier Ltd. All rights reserved.

Keywords:
Meta-tutor
Gaming the system
Intelligent tutoring systems
Modelling
Learning strategies

1. Introduction
This paper reports progress on two research problems: (1) teaching students how to construct mathematical models of dynamic systems,
and (2) teaching students to use effective learning strategies. Both research problems have long histories, which are covered in the next few
sections.
1.1. A brief history of educational uses of system dynamics modelling
There are two distinct reasons why students should learn model construction. First, modelling is an important cognitive skill in itself. The
Common Core State Standards for Mathematics (CCSSO, 2011) considers modelling to be one of 7 essential mathematical practices that
should be taught at all grade levels. The Next Gen standards for science instruction (National, 2012) also have 7 strands that are threaded
throughout the standards, and modelling is one of them.
Second, modelling is widely believed to be an important method for learning domain knowledge. For instance, modelling has been
claimed to help in achieving a deep understanding of scientiﬁc systems, economic systems and other systems (Chin et al., 2010; Metcalf,
Krajcik, & Soloway, 2000; Stratford, 1997), removing misconceptions and making of conceptual changes (Booth Sweeney & Sterman,
2000; Bredeweg & Forbus, 2003; Hestenes, 2007; Lee, Jonassen, & Teo, 2011; Mandinach & Cline, 1994b; Wilensky, 2003; Wilensky &
* Corresponding author.
E-mail addresses: lishan.zhang@asu.edu (L. Zhang), kurt.vanlehn@asu.edu (K. VanLehn), sylvie.girard@asu.edu (S. Girard), winslow.burleson@asu.edu (W. Burleson),
mchaveze@asu.edu (M.E. Chavez-Echeagaray), javiergs@asu.edu (J. Gonzalez-Sanchez), yhidalgo@asu.edu (Y. Hidalgo-Pontet).
http://dx.doi.org/10.1016/j.compedu.2014.02.015
0360-1315/Ó 2014 Elsevier Ltd. All rights reserved.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

197

Reisman, 2006), understanding the epistemology of models in science (Treagust, Chittleborough, & Mamiala, 2002), and developing intuitions, predilections and skills at understanding complex phenomena in general (Hogan & Thomas, 2001; Mandinach & Cline, 1994a;
Schecker, 1993; Steed, 1992).
In short, modelling is both an important cognitive skill and a potentially powerful means of learning many topics. That is, it is both an end
goal and a means to other end goals.
The modelling activity addressed here is traditionally called system dynamics modelling (see Collins and Ferguson (1993) for a particularly comprehensive taxonomy of modelling). The model is comprised of real-valued variables that are constrained by temporal differential
equations. The variables denote quantities in the system whose values change over time. The job of the model is to predict those changing
values as accurately as parsimony allows.
There is a long history of using system dynamics model construction as in instructional activity. According to the oral history of the
System Dynamics Society (http://www.systemdynamics.org/oral-history/), system dynamics began to be used for university instruction
around 1957 with Jay Forrester’s formulation of system dynamics for teaching management. When a graphical language, Stella, became
available (Richmond, 1985), instructional usage dramatically increased and extended to high school. Many early Stella projects trained
teachers in modelling and let them invent activities (Mandinach & Cline, 1994a; Zaraza & Fisher, 1997).
After years of experience by hundreds of teachers, observers began to report that getting students to actually construct models took so
much class time that most teachers used Stella only for model exploration activities, wherein students were given a model and were asked
to observe how graphs of the variables values changed as the students manipulated parameters of the model (Alessi, 2000; Doerr, 1996;
Mandinach & Cline, 1994b; Stratford, 1997).
Laboratory studies conﬁrmed the observers’ reports about both the length of time required for model construction and the importance of
model construction. For example, Hashem and Mioduser (2011) found that students who constructed NetLogo models learned more about
emergence, self-organization and other complex system concepts than students who explored NetLogo models that were given to them. The
comparison took place during two 90-min lessons on complex systems, which were ﬂanked by a pre-test and a post-test. However, prior to
the pre-test, it took only 2 h to train the model exploration group whereas it took 48 h to train the model construction group. In a review of
the modelling literature, VanLehn (2013) found that the only experiments that produced reliable positive results for model construction also
devoted at least 5 h to training the students before the main lessons.
This history motivates the speciﬁc research problem addressed here: How can we speed up students’ acquisition of skill in constructing
system dynamics models?
Many methods for accelerating the acquisition of skill in model construction have been implemented, but only a few have been
compared to baseline versions of the model construction activity in order to test their effectiveness (see VanLehn, 2013, for a review). Of
those that have been evaluated, one form of scaffolding has shown considerable promise: The use of feedback and hints on student’s
steps in constructing the model. A whole model is usually composed of many parts (e.g., nodes, links, equations, labels, icons, numbers,
etc.) which the student enters one at a time. Entering such a part is called a “step”. Systems that give feedback and hints on steps are
called step-based tutoring systems (VanLehn, 2006). Step-based tutoring systems have been used for a wide variety of tasks besides
model construction, and appear to be almost as effective as human tutors (VanLehn et al., 2011). Thus, this project decided early on to
build a step-based tutoring system for system dynamics modelling in the hope that it would accelerate students’ acquisition of
modelling skill.
1.2. Learning strategies research
A learning strategy is a process, procedure or method that meets two criteria (Donker, de Boer, Kostons, Dignath van Ewijk, & van der
Werf, 2014): (1) Students can use the strategy when studying, but it is not required by the material that they are studying. (2) Using the
learning strategy is believed to affect the student’s learning. A good learning strategy is thought to improve students’ learning, while a poor
learning strategy is thought to harm the students’ learning. When used without modiﬁcation, “learning strategy” generally means a good
learning strategy. Some examples are:
 When memorizing facts, a good learning strategy is to construct a mental image and associate each fact with a part of the image.
 When studying an example, a good learning strategy (called self-explanation) is to explain each step in the example to yourself, asking
“Why is this true? Why did the author include this step?”
 When reading a text, a good learning strategy is to reﬂect afterwards on what you have learned.
 When reading a text, a poor learning strategy is to ignore words or passages that you don’t understand.
Learning strategies have been studied for decades, and comprehensive meta-analytic reviews exist (Donker et al., 2014; Hattie, Biggs, &
Purdie, 1996). Some of the main ﬁndings are:
A. Students often exhibit poor learning strategies.
B. Good learning strategies can be taught, often with little difﬁculty.
C. When students use the taught learning strategies, their domain learning often increases compared to students who are not taught to use
the learning strategies.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones.
These ﬁndings mean that research on learning strategies is intimately linked to advances in instruction. Whenever a new instructional
method or subject matter is developed, there are likely to be poor learning strategies that are speciﬁc to it (ﬁnding A) as well as speciﬁc
good learning strategies that are likely to be effective (ﬁnding E) and easily taught (B). The key question is whether they are effective for

198

L. Zhang et al. / Computers & Education 75 (2014) 196–217

domain learning (C) and students can be persuaded to continue using them after being taught (D). We have developed a step-based
tutoring system for teaching students how to construct models, so it is likely that for this new form of instruction, students tend to
exhibit poor learning strategies, but that good strategies can be easily taught; the main questions are whether the good strategies really
are effective at increasing domain learning and whether students can be taught to persist in using them. These are the main research
questions addressed here.
The next introductory section focuses on reviewing projects that are similar to the one described here. However, their relationship to
learning strategies can be a bit hard to see, so a few prefatory remarks may be helpful.
Because this paper is concerned with teaching students how to construct models, and constructing a model is a kind of problem solving,
it is worth clarifying how learning strategies differ from problem solving strategies. When students are taught an effective strategy for
solving problems, the strategy qualiﬁes as either a learning strategy, a problem solving strategy or both depending on the instructional
objectives.
 If the objective is for students to solve problems well, then a strategy for solving problems counts as domain knowledge and is called a
problem solving strategy. In the jargon of students, it’s on the test.
 On the other hand, when problem solving is done only to give student practice in applying certain concepts and principles and the
problem solving itself is not an instructional objective, then the strategy counts as a learning strategy because it is optional and yet it
probably impacts the students’ learning of the concepts and principles. In the jargon of students, it is not on the test.
 As pointed out earlier, the process of constructing models is both an instructional objective (especially in math classes) and a means for
acquiring domain knowledge (especially in science classes). Thus, an effective strategy for constructing models counts as both a problem
solving strategy in some cases (it’s on the test) and a learning strategy in other cases (it’s not on the test).
For example, suppose the instructional objective is to learn which botanical features go with which plants, and the instruction involves
arranging cards labelled with plants and features. If this matching activity does not appear on the exams, then teaching students a
methodical method for arranging the cards counts as a learning strategy even though it is also a problem solving strategy. From the students’
point of view, what matters is whether the strategy is “on the test.” That is, if a problem solving strategy is an instructional objective and is
part of a test or other assessment, then students have a different attitude towards it than a strategy that is merely helpful for learning the
material that will be on the test. As we discuss various methods for teaching model construction, it is important to note whether students
believe a taught strategy is required or merely helpful.
Interactive instructional systems, such as the tutoring system described herein, have their own unique learning strategies. Here are some
examples:
1. Teachers often complain that their students would rather click than think. Actuating buttons, menus, etc. without thinking or even
reading the rest of the screen is a poor learning strategy.
2. When a tutoring system gives immediate feedback on the correctness of a student’s entry, then students often guess instead of think.
That is, they rapidly revise and resubmit an incorrect entry until the tutoring system says that it is correct.
3. If the tutoring system gives a sequence of hints that get gradually mores speciﬁc until they tell the student exactly what to do,
then students abuse the hint sequences by clicking rapidly on the Hint button until they get the ﬁnal hint that tells them what
to do.
4. A good learning strategy for tutoring systems is to ask for a hint only when you need one (Aleven, McLaren, Roll, & Koedinger, 2004).
5. After asking for hints from a tutoring system and ﬁnally making a correct entry, a good learning strategy is to reﬂect on why it is correct
(self-explanation) and whether the hint makes sense (Shih, Koedinger, & Scheines, 2008).
6. Examples 2, 3 and 4 are help-seeking strategies (Aleven, Stahl, Schworm, Fischer, & Wallace, 2003). Examples 2 and 3 are often referred
to as “gaming the system” (Baker, Corbett, Koedinger, & Wagner, 2004).
Feedback and hints are the signature methods of tutoring, but several projects have used feedback and hints for two distinct purposes, so
let us distinguish them as follows:
 Let domain knowledge refer to what students are supposed to learn. It is typically measured with a post-test and sometimes a pre-test.
 When the hints address the domain and the feedback indicates whether the domain knowledge has been correctly applied, the system
is said to be tutoring and the module responsible for it is called the tutor.
 A learning strategy is an optional method for using the system (i.e., it is not domain knowledge) that is believed to increase student’s
learning of domain knowledge.
 When the feedback and hints refer only to the learning strategy and whether it is being applied correctly, the system is said to be metatutoring and the module responsible for it is called the meta-tutor.
In this paper, meta-tutoring will only means a method for teaching students a learning strategy. However, in the tutoring literature,
“meta-tutoring” is used more broadly to mean using feedback and hints to teach anything other than domain knowledge. For instance,
meta-tutoring is sometimes used to increase motivation or change beliefs about self-efﬁcacy. Du Boulay, Avramides, Luckin, MartinezMiron, and Rebolledo-Mendez (2010) propose a framework that includes many types of meta-tutoring.
Now we can turn to reviewing prior work on using learning strategies to help students learn how to construct models.
1.3. Prior work on learning strategies for model construction
Betty’s Brain (Leelawong & Biswas, 2008) was a step-based tutoring system for constructing models that also taught a learning strategy. It
could give feedback and hints on the student’s model (tutoring) or it could give feedback and hints on the way that the student was using the

L. Zhang et al. / Computers & Education 75 (2014) 196–217

199

system to create the model (meta-tutoring).1 For instance, sometimes it would not permit the student to evaluate the model with an
instructor-provide test suite (called a “quiz”) until the student had ﬁrst examined speciﬁc predictions of their model (e.g., if air temperature
goes down, what does body temperature do?). The system included some multimedia resources on the task domain, so another part of the
learning strategy was encouraging students to read them. As these examples indicate, the learning strategy was speciﬁc to the particular
instructional features provided by system. This is consistent with the ﬁndings in the learning strategies literature, which suggest that such
speciﬁcity provides better results than general learning strategies.
Three evaluations of the effectiveness of Betty’s meta-tutor were conducted (Biswas, Leelawong, Schwartz, & Vye, 2005, study 2;
Leelawong & Biswas, 2008; Tan, Biswas, & Schwartz, 2006). Their methods will be described fully here, as the experiments to be reported
later used similar methods. The Betty’s Brain experiments all had two phases, called the training phase and the transfer phase here. During
the training phase, ﬁfth-grade students worked with Betty for approximately seven 45-min sessions on constructing a model of a river
system. One group of students used Betty’s Brain with the meta-tutor turned on, and another group used the same system with the metatutor turned off. Two months later the transfer phase occurred, where all the students used Betty’s Brain with the meta-tutor turned off to
create models for the nitrogen cycle. This transfer phase was used to assess their modelling skill.
The results were roughly the same in all three studies. Using the general results on learning strategy mentioned above as a framework,
the results from the Betty’s Brain studies were:
A. Students often exhibit poor learning strategies. True of the control conditions in all three studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of all three studies, meta-tutored students’
behaviour was consistent with the learning strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, using conventional science tests of ecology concepts, there were few signiﬁcant differences between
the meta-tutored students and the students who used Betty’s Brain without meta-tutoring.2 Using four measures of model quality,
meta-tutored students’ models were better than control students’ models on only one measure.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In short, although the meta-tutor in Betty’s Brain was successful at teaching the learning strategy and persuading students to continue
using it, the learning strategy had little impact on domain learning. More recent work has focused on ﬁnding out what behaviours
distinguish good learners from poor learners (Segedy, Kinnebrew, & Biswas, 2012a, 2012b).
The Help Tutor (Aleven et al., 2004; Roll, Aleven, McLaren, & Koedinger, 2007a, 2007b; Roll, Aleven, McLaren, & Koedinger,
2011; Roll et al., 2006) was a meta-tutor that augmented an existing step-based tutoring system, the Cognitive Geometry Tutor
(www.carnegielearning.com). Although the tutoring system did not teach students to construct models, it is included in this review of
past work because it is an often-cited representative of using meta-tutoring to improve students’ learning from step-based tutoring.
The Geometry Cognitive Tutor allowed students to ask for a hint. The ﬁrst hint was rather general, but if the student kept asking for hints,
then the last hint told the student exactly what to do. This was called the “bottom-out” hint. Students sometimes clicked rapidly on the Help
button so that they could get to the bottom-out hint. As mentioned earlier, this abuse of the help system is a kind of “gaming the system”
(Baker, Corbett, Koedinger, et al., 2004). In order to reduce the frequency of gaming the system, the Help Tutor gave students feedback and
hints on their use of the help system.
Two studies evaluated the Help Tutor. As in the Betty’s Brain studies, the Help Tutor studies had both a training phase where the metatutor was used by half the students and a transfer phase where none of the students used the meta-tutor. Using the general ﬁndings
mentioned above as a framework, the results were:
A. Students often exhibit poor learning strategies. True of the control conditions in all both studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of both studies, meta-tutored students’ behaviour
was consistent with the taught strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, students taught the learning strategy did not differ from the control group in their acquisition of
geometry knowledge.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off, albeit with less frequency.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
Betty’s Brain and the Help Tutor provided most of their instruction via feedback and hints. A somewhat more didactic approach is to
provide forms and phases that constrain students’ modelling behaviour. For instance, the ﬁnal version of Model-It (Metcalf et al., 2000) had

1
Those familiar with Betty’s Brain might be surprised to see it described as a tutoring system because the students see the system as two agents: Betty (a teachable agent)
and Mr. Davis (a mentor). Students were told that the model they were constructing comprised the knowledge of Betty, so editing the model comprised “teaching Betty.”
When they ask Betty take a quiz, Mr. Davis gives the student feedback on the correctness of the model’s predictions which would sometimes include unsolicited hints about
ﬁsh, algae, carbon dioxide and other domain entities. Mr. Davis personiﬁes the system’s tutoring. On the other hand, meta-tutoring was done by both agents. For instance,
Betty would sometimes refuse to take a quiz, and Mr. Davis would discourage students from using trial-and-error methods.
2
In a fourth study, the meta-tutored students produced slightly better river system models than the control students (Tan, Wagster, Wu, & Biswas, 2007; Wagster, Tan,
Biswas, & Schwartz, 2007; Wagster, Tan, Wu, Biswas, & Schwartz, 2007). However, results on the other measures were not reported.

200

L. Zhang et al. / Computers & Education 75 (2014) 196–217

4 phases, which were selected by clicking on one of four buttons labelled Plan, Build, Test and Evaluate. The Build mode was the actual model
editor. The other phases presented forms to be ﬁlled in by the student. Fig. 1 shows the form for the Test phase. Students were given feedback
and hints, which they could suppress if they desired, when they attempted to bypass a suggested activity. The Carnegie Learning’s Algebra
Cognitive Tutor (www.carnegielearning.com) and the Word Problem Solving Tutor (Wheeler & Regian, 1999) also scaffolded problem
solving strategies via lightweight constraints, implemented with phases and forms. None of these meta-tutors were evaluated separately
from the rest of the system.
On the other hand, Mulder, Lazonder, de Jong, Anjewierden, and Bollen (2011) did assess the effects of a phase-based learning strategy.
The experiments used Co-Lab, a system that taught system dynamics modelling. Co-Lab was not a step-based tutoring. Instead, students
received feedback only on the accuracy of the model’s predictions. The learning strategy was to encourage students to do their model
construction in three stages. In Stage 1, they deﬁned variables and drew undirected links between variables that directly affected each other
somehow. In Stage 2, they added qualitative labels to the links indicating whether an increase in one variable caused an increase or decrease
in the other variable. In Stage 3, they added equations to the model that further speciﬁed the relationships between variables. Three versions
of the learning strategy were compared to a control version of Co-Lab that lacked phases. The three versions differed in how strictly they
enforced the learning strategy. The restricted strategy required students to achieve a certain level of success in one stage before moving to
the next. The semi-restricted version of the strategy allowed students to move from one stage to the next at will, but they were not allowed
to move backwards. The unrestricted version allowed students to move at will between stages. Compared to the control version, all three
versions of the learning strategy increased the number of correct model elements generated by students as they used the system. The three
versions were not signiﬁcantly different in productivity from each other, but there was a trend for the semi-restricted version to be better
than the other two. However, this experiment included only a training phase and not a transfer phase, and it did not assess students’ domain
knowledge with pre- and post-testing. Thus, its consistency with the 5 general ﬁndings mentioned above (A through F) cannot be determined. Nonetheless, this work is interesting because the learning strategy was taught without using a meta-tutor, and the system was not a
step-based tutoring system.
The meta-tutoring of Betty’s Brain and the Help Tutor placed only weak constraints on students’ behaviour. The phases and forms of CoLab, Model-It, the Cognitive Tutors and the Algebra Word Problem tutor placed somewhat stronger constraints on students’ behaviour. At
the far end of this progression is procedural scaffolding, which places very strong constraints on student behaviour. The basic idea of procedural scaffolding is to require students to temporarily follow a speciﬁc procedure for constructing a model. Although the procedure is not
required by the task and there are many other ways to successfully construct models, the procedure is used as a temporary scaffolding to
guide students who might otherwise be quite lost.

Fig. 1. Scaffolding for the Test mode of Model-It (Metcalf, 1999).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

201

Although procedural scaffolding was ﬁrst used by (Marshall, Barthuli, Brewer, & Rose, 1989) to scaffold arithmetic story problem solving,
its beneﬁts were not evaluated. Procedural scaffolding was ﬁrst evaluated with Pyrenees (Chi & VanLehn, 2010; Vanlehn & Chi, 2012), which
required students to construct models using a version of goal reduction, which is a well-known general purpose reasoning strategy used in
artiﬁcial intelligence applications (Russell & Norvig, 2009).
Pyrenees’ procedural scaffolding was evaluated in a two-phase experiment. In the training phase, students learned to construct model of
probabilistic systems. In the transfer phase, student learned to construct models of mechanical energy systems. Using the framework
mentioned earlier, the ﬁndings were:
A. Students often exhibit poor learning strategies. True of the control conditions in both phases.
B. Good learning strategies can be taught, often with little difﬁculty. This could not be determined, because Pyrenees required students in the
experimental condition to follow the learning strategy during the training phase.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. In both the training phase and the transfer phase, the experimental group acquired more domain knowledge than
control group. The effect sizes were large (d z 1.0).
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. The experimental group tended to use the learning strategy during the
transfer phase on difﬁcult problems, but not on simple problems. However, even on simple problems, they did not use poor learning
strategies.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In summary, Betty’s Brain and the Help Tutor used the standard methods of hints and feedback and their meta-tutoring succeeded in
improving students’ behaviour, but there were only weak improvements at best in their learning of the domain. Co-Lab’s learning strategy
increased performance, but the effect on learning was not measured. Pyrenees used procedural scaffolding, and its meta-tutoring caused
large improvements in both behaviour and domain learning.
1.4. Our research questions
Our research questions are the same 4 questions that our predecessors have focused one:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
Although these 4 questions about learning strategies are the central focus of the research, we are also interested in seeing if the time
required to achieve adequate competence in model construction can be reduced from 5 or more hours to 2 for fewer hours.
Because Pyrenees was arguably more successful than the other methods, we chose to use procedural scaffolding as the basic method of
teaching learning strategies. However, there is a major difference between our research problem and the one addressed by Pyrenees.
Pyrenees assumed that students had already been taught all the relevant domain principles. Indeed, the students repeatedly selected
principles from a menu. In contrast, when students construct system dynamics models, they are seldom taught domain principles in
advance. They are instead asked to infer domain relationships from multimedia resources, common sense and/or experimentation. They do
this while constructing the model. This allows modelling to be used as part of enquiry-based instruction where students construct the
domain knowledge themselves. Having to infer domain relationships while learning to model may be one of the reasons why it takes
students so long to master system dynamics model construction. Nonetheless, inferring quantities and their relationships is exactly the skill
we want students to learn.
This suggests that if we require students to follow a procedure, as Pyrenees did, then they will learn that the key to modelling is a
single non-mechanical step: ﬁguring out the mathematical relationship among a set of directly related quantities. Once they have
mastered that step, we expect that they will be able to construct models well, even if they don’t follow the procedure. Thus, our research
question is whether applying the Pyrenees approach of heavy-handed procedural scaffolding to the cognitive skill of model construction
will cause deeper, more effective modelling skills to develop during the training phase. If the tutor does work in the training phase, our
next question is whether the beneﬁts will persist in the transfer phase. To answer these two questions, we developed an instructional
system, called AMT, and conducted experiments to evaluate it. The following sections described the system, how it was implemented, and
the evaluation at last.
The name “AMT” is an acronym for the overall project, which is called the Affective Meta-Tutoring project. The ﬁrst objective of the
project is to develop and test a meta-tutor that improves student’s learning of modelling by using procedural scaffolding as well as the
traditional feedback and hints. The results of that phase of the project are reported here. The second phase of the project will be to add an
affective learning companion to the AMT system; hence the term “affective” in the project name. The second phase will be described further
in the discussion section.
2. The AMT system’s design and behaviour
This section has three parts. The ﬁrst describes the student’s task, and in particular, the graphical language in which they write models.
The second section describes the tutoring system. The third section describes the meta-tutor and the strategy that it teaches students to use.

202

L. Zhang et al. / Computers & Education 75 (2014) 196–217

2.1. Constructing models in the AMT modelling language
In order to decrease the difﬁculty of learning how to construct system dynamics models, most prior work has used graphical modelling
languages where a model consists of several types of nodes and links. These graphical languages are easier to learn than text-based languages (Löhner, Van Joolingen, & Savelsbergh, 2003). The traditional “stock and ﬂow” language has two types of links, and one of them (the
ﬂow links) acts somewhat like nodes. In pilot studies, our high school students found this confusing, so we removed the confusing type of
link.
In our modelling language, a model is a directed graph with one type of link. Each node represents both a variable and the computation
that determines the variable’s value. The inputs of that computation, which are themselves variables, are indicated by incoming links. In
Fig. 2 for example, the computation for “births” requires the values of “growth rate” and “population.” The value of a variable is a real
number that can change over time, where time is represented discretely.
There are three types of nodes:
 A ﬁxed value node represents a constant value that is directly speciﬁed in the problem. A ﬁxed value node has a diamond shape. It never
has incoming links.
 An accumulator node accumulates the values of its inputs. That is, its current value is the sum of its previous value plus its inputs. An
accumulator node has a rectangular shape and always has at least one incoming link.
 A function node’s value is an algebraic function of its inputs. A function node has a circular shape and at least one incoming link.
The students’ task is to draw a model that represents a situation that is completely described by a relatively short text. For instance, Fig. 2
is a correct model for the following problem:
Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.
Such a text contains all the information that students need. However, it sometimes contains extra quantities (e.g., the rust spot at the end
of the ﬁrst week) that are not needed in the model. Students are asked to draw only the necessary nodes, so drawing a node for the rust spot
at the end of the ﬁrst week is an indication of shallow modelling.
2.2. The tutoring system
Many tutoring systems have a sequence of feedback and hints (VanLehn, 2006). When the student makes an entry, the tutoring system
ﬁrst just tells the student whether the entry is correct or incorrect. If the student asks for help again, the system gives a general hint.
Subsequent requests for help on the same entry generate increasingly speciﬁc hints. Eventually, the ﬁnal hint (called the “bottom-out hint”)
tells the student what the correct entry is. There is some evidence that the mid-level hints are not pedagogically useful (Muldner, Burleson,
van de Sande, & VanLehn, 2011; Timms, 2007), so our tutoring system does not use them. Instead, it generates just the ﬁrst and last members
of the typical hint sequence. More speciﬁcally, it has a Check and a Give-up button. Clicking on the Check button causes the tutoring system
to give minimal feedback: It colours entries red if they are incorrect and green if they are correct. Clicking on the Give-up button causes the
tutoring system to ﬁll in entries correctly, that is, to give a bottom-out hint. This section describes the AMT model editor, pointing out where
the Check and Give-up buttons appear.
The system presents itself to the student as a large tabbed window (Fig. 3). The Model tab (shown in the ﬁgure) is for drawing models.
The Situation tab shows a textual representation of the problem, and is illustrated by a static picture. The Instructions tab contains a slide
deck that teaches students the basics of the modelling language, the user interface, the modelling process and the learning strategy. Students can access the Introduction and Situation tab at any moment during the construction of the model (cf. Section 4.4).
The Model tab has three buttons: Create Node, Run Model and Done. The Done button is disabled (grey) until the student has completed a
problem successfully (i.e. created an accurate model of the system), at which time clicking on the Done button advances the system to the
next problem.

Fig. 2. A model. The grey bubbles have been added to this ﬁgure in order to show how the value of each variable is calculated.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

203

Fig. 3. The tutoring system’s screen.

The Create Node button creates a new node symbol in the model area and opens the Node Editor on it. The Node Editor (Fig. 4) is divided
into ﬁve tabs: Description, Plan, Inputs, Calculations and Graph. Students can edit all tabs with the exception of the Graph tab, where a graph
showing the evolution of the node’s value over time is generated by the tutoring system once the model is run. To create a node, the student
ﬁlls out the four tabs in order, left to right. Students can also change the information within a node by double clicking on the node shape
within the model area, which opens the Node Editor, and then selecting the tab they wish to modify.
The Description tab was engineered to facilitate grounding, where “grounding” is the process of negotiating mutual understanding of the
meaning of a term between two participants in a conversation (Clark & Brennan, 1991). In system dynamics model editors that are not used
for tutoring, such as Stella, Vensim and Powersim, users merely type in the name of the node that they want. This allows them to use names
such as “x” that do not match anything in the problem. While this is unproblematic for such editors, it makes it difﬁcult or impossible for a

Fig. 4. The node editor, showing the Description tab.

204

L. Zhang et al. / Computers & Education 75 (2014) 196–217

tutoring system to determine what quantity the student’s node denotes, and hence the tutoring system cannot determine whether the node
is deﬁned correctly. Urging the students to choose adequately precise names is only partially successful. In one study, only 78% of the node
names could be identiﬁed by the tutoring system (Bravo, van Joolingen, & de Jong, 2009). This is a case of bad grounding: the tutoring system
did not understand the meaning of 22% of the students’ terms.
On the other hand, some tutoring systems for system dynamics modelling, including the ﬁrst version of this system, provide students
with nodes that already have names but are otherwise undeﬁned (VanLehn et al., 2011). Unfortunately, students often do not pay enough
attention to the names, which can be rather similar. Students sometimes create models that are correct except that the names of two nodes,
such as “rust growth factor” and “new rust area per week”, have been switched. This is also a case of bad grounding: the student did not
understand the meaning of the system’s terms.
The Description tab of AMT, illustrated in Fig. 4, is intended to prevent bad grounding. First, the student walks through the tree in the
Description tab located in the large box at the top of the window. Clicking on a leaf in the displayed tree selects both a description and a
name for the node. Each problem has a different tree, and the tree’s contents are engineered to make the student select among subtly
different descriptions. After selecting a leaf, the student clicks on the Check button. If the selected description denotes a quantity that the
system understands, and that quantity does not already have a node deﬁned for it, then clicking on the Check button turns some boxes
green, as shown in Fig. 4. Otherwise, the boxes turn red. Students may also click on the Give-up button that ﬁlls out the tab correctly but
colours the boxes yellow. When the student exits the node editor, the node’s name, which is shown beneath the node, is highlighted by the
colour of the Description tab’s boxes. Thus, a student who had given up on the Description tab would forever see yellow highlighting on the
node’s name. This feature is intended to discourage giving up.
When the Description tab is completed correctly and its boxes are either green or yellow, then students can go on to the Plan tab. The Plan
tab lets students choose among 7 different plans (Fig. 5). The Instruction tab describes the meaning of these plans and gives an example of
each plan. After a selection is made, students may click on the Check button and see their selection coloured green (correct) or red
(incorrect). Clicking on the Give-up button causes the correct plan to be selected and coloured yellow. Unlike the other tabs, ﬁlling out the
Plan tab correctly is optional. Students can go to the Inputs tab regardless of how their plan selection is coloured, and they can skip the Plan
tab entirely if they want.
The Inputs tab (Fig. 6) allows students to indicate whether the node’s value is a ﬁxed, given constant or if it is computed from other nodes’
values. In the latter case, they click on “Inputs:” and choose some of the existing nodes as inputs, which causes links are drawn between
those nodes and the current node. When students see that the input they want is not in the list because they have not yet created a node for
it, they can click on the convenient “Create a new node” button, deﬁne the desired node using a pop-up version of the Description tab, then

Fig. 5. The plan tab.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

205

Fig. 6. The Inputs tab.

return to this Input tab by closing the pop-up. As always, the Check and Give-up buttons colour the student’s choices in either red (incorrect),
green (correct) or yellow (gave up). While red choices can be revised later on, green or yellow choices cannot be changed.
When the Inputs tab is ﬁlled out correctly, students can go to the Calculation tab (Fig. 7). If they had selected “Fixed Value” on the Inputs
tab, then “has a ﬁxed value” is checked here on the Calculation tab, and so all they need to do here is enter the number that is the value of the
Fixed Value node. On the other hand, if they had selected “Inputs:” on the Inputs tab, then they must click either the “accumulates the values
of its inputs” button or the “is a function of its inputs” button. This selection determines which form appears in the lower part of the tab.
Fig. 7 shows the tab to ﬁll for the Accumulator node type on the left, and the Function node type on the right. For both the Function and
Accumulator nodes, the inputs appear initially in the “Available inputs:” box, and then move rightward into the calculation box as they are
clicked. In this fashion, students enter a calculation.
As a model is being constructed, the state of the node’s deﬁnition is indicated by little circular indicators (“i” for input, “c” for calculation
and “g” for graph) and the node’s outline (see Fig. 8). A dotted border indicates that the type of the node hasn’t been deﬁned yet. A blue
border means that the node’s deﬁnition is complete.
When all the nodes have blue borders, the student can click on the Run Model button. If there are syntactic errors in the model that
prevent running it, a pop-up window describes them. Otherwise, Run Model colours the “g” indicators on every node either green or red,
depending on whether the node’s graph is correct or not. Opening the Graph tab of a node displays both the user’s model’s graph and the
expected graph (see Fig. 9). Students can use the difference in the graphs as a clue to where the bug in the model could be found.
A model is considered completely correct when all the graphs match, in which case all the “g” indicators are green. At this point, students
can click on the Done button and go to the next problem.
This is a step-based tutoring system (VanLehn, 2006) because it can give feedback on every step taken by the student. It only gives such
feedback when the student clicks on the Check button. The feedback is minimal: just correct vs. incorrect. Absent are the usual sequences of
hints that many step-based tutoring systems have. However, the “Give-up” button implements the bottom-out hint in that it gives away
exactly what step the student should do at this point.
The AMT system has a mode, called test mode, which is used to assess students’ skill at modelling. Although students can still debug their
model by running it and seeing which graphs are correct, they cannot use the Check and Give-up buttons on any of the tabs that they ﬁll out,
with one exception. The Check button is always enabled on the Description tab. This is because the system and the student must agree on the
meaning of nodes. If the system doesn’t know which quantity is denoted by the student’s node, then it can’t know which graph is correct and
can’t colour the “g” indicators. In test mode, the Check button is enabled only on the Description tab, whereas in training mode, it is enabled
everywhere. In test mode, the Give-up button is never enabled.

206

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Fig. 7. The Calculations tab.

2.3. The meta-tutor
So far, only the tutoring system has been described. It is essentially just a model editor with Check and Give-up buttons. This section
described the meta-tutor, which is the remaining module of the AMT system. This section begins by describing and motivating the learning
strategy that the meta-tutor teaches, then describes how it teaches that strategy.
Like Pyrenees (Chi & VanLehn, 2010), the meta-tutor teaches students a simple, goal reduction procedure for constructing models called
the Target Node Strategy. The basic idea is to focus on one node at a time (the target node) and complete all the tabs for this node before
working on other nodes. As students complete the target node, they may create new nodes as a side effect, but the new nodes will only be
named and not fully deﬁned. These nodes are displayed with dotted borders. Thus, when students have ﬁnished the target node, they can
pick any node that has a dotted border as the next target node, and begin working on deﬁning it. When there are no more nodes with dotted
borders, the model is complete.
There are sound reasons to think that the Target Node Strategy might help students learn more effectively, but it will take several
paragraphs to describe them. These paragraphs also illustrate the distinction between deep and shallow modelling practices.
The key steps in the Target Node Strategy are ﬁlling out the Inputs tab and the Calculation tab. These steps correspond to the moment
where students must analyze the given system information and determine the quantitative relationships between the target node’s
quantity and other quantities in the problem. That is, given a particular target quantity, students must ﬁnd a set of quantities such that the

Fig. 8. An incomplete model.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

207

Fig. 9. The Graphs tab.

target quantity’s value can be computed as a simple function of their values. The whole problem of constructing a model of a system can be
decomposed into making a series of decisions like this one. One might call these decisions “atomic modelling problems”, as they cannot be
easily decomposed further.
For example, consider the system described earlier:
“Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.”
Suppose students are following the Target Node Strategy and have decided that the ﬁrst target node is the size of the rust spot. They
create a node and select “rust spot area” as its description. Now they face an atomic modelling problem. How to determine the value of rust
spot area? What we want students to think is something like, “I know the rust spot area is changing, and the value increases every week”.
This reasoning sufﬁces for ﬁlling out the Plan tab correctly. Next the students need to ﬁll out the Input tab, so they should think, “The rust
spot area is increased by the rust produced during the week”. There are no other nodes at the moment, so the student must click on “Create a
new node.” The student browses the available descriptions, and chooses the one that comes closest to the student’s idea of “rust produced
during the week.” Clicking on this description deﬁnes the node new rust area per week. The student can then select this node as an input for
the node rust spot area. Next comes the Calculation tab, which requires for the student to turn his/her qualitative understanding of the
relationship into a formal, mathematical one: the student should decide that the next value of rust spot area is its current value plus new rust
per week. This illustrates how the user interface decomposes the key reasoning into three major steps, corresponding to the Plan, Input and
Calculation tabs. These correspond roughly to the phases of Co-Lab (Mulder et al., 2011) and Model-It (Metcalf et al., 2000). This paragraph
also illustrates how the key reasoning should be done: by thinking hard about the system, its quantities and their interrelationships, and
then abstracting the mathematical relationships from them. This is the “deep” way to solve an atomic modelling problem.
However, there are shallow ways to solve the atomic modelling problem as well. When students need to create nodes in order to ﬁll out
the Inputs tab, there are only a ﬁnite number of choices: the leaves in the tree of descriptions on the Description tab. Thus, they can work
through all combinations until they have found the appropriate inputs. When the Check button is available and the problem is simple, this
can be done rapidly. For instance, the rust spot area problem’s Description tab has 7 possible descriptions, of which only 4 are legal
quantities in the problem (1 is an extra node; 3 are necessary for the model). It will not take the student long to create all the legal nodes, and

208

L. Zhang et al. / Computers & Education 75 (2014) 196–217

then test each one to see if it turns the Inputs tab green. Such extensive guessing is one “shallow” method for solving atomic modelling
problems. The Give-up button is another. Other methods involve looking for keywords (e.g., “initially”) or patterns.
Many of our students’ shallow modelling methods involve abusing the Check button or the Run Model button, so their behaviour is a
form of gaming the system (Baker et al., 2006; Baker, Corbett, & Koedinger, 2004), which is what the Help Tutor (Roll et al., 2011), Scooter the
Tutor (Baker et al., 2006), and other tutoring systems have tried to address. However, studies of modelling that did not use tutoring systems
have also noted this propensity of students to do shallow modelling (Alessi, 2000; Booth Sweeney & Sterman, 2000; Doerr, 1996; Mandinach
& Cline, 1994b; Nathan, 1998; Zaraza & Fisher, 1999). Thus, we prefer to refer to the phenomenon as “shallow modelling” rather than
“gaming the system.”
Teaching the Target Node Strategy does not require that student do deep modelling, so one might wonder why we think it could help
students model better. However, it does decompose the whole problem of modelling a system into a series of atomic modelling problems,
and even decomposes an atomic modelling problem into three major steps. Like Pyrenees, it teaches students that if they just master this
one difﬁcult but small skill (deep modelling applied to atomic modelling problems), then the rest of the problem solving is purely
mechanical.
Having described and motivated the Target Node Strategy, it is ﬁnally time to describe how the meta-tutor teaches it. First, the Introduction slides describe the strategy brieﬂy. Even students who use AMT with the meta-tutor turned off see this presentation of the strategy.
Also when the meta-tutor is turned off, students can edit any node at any time. Moreover, they can ﬁll out some of a node’s tab, then quit
editing the node and come back to it later. This freedom is removed when the meta-tutor is turned on. Students are required to correctly ﬁll
out each tab before moving on to the next, and they must ﬁll out all tabs before closing the node editor. When they are on the Description tab
and choosing which quantity to create a node for, if they choose one that would not be selected by Target Node Strategy and yet it will
eventually be include in the model, then the selection turns blue and a pop-up window says, “That quantity is in the correct model, but it is
too early to deﬁne it now.” This message is typical of others that pop up when students stray from the Target Node Strategy. In short, the
main job of the meta-tutor is simply to keep students on the one of the paths that are consistent with the Target Node Strategy.
In addition to requiring students to follow the Target Node Strategy, the meta-tutor enacts a bit more scaffolding. For completeness, this
section describes the remaining forms of help.
When the meta-tutor is turned on, it complains if students appear to be guessing too much or giving up too early, just as the Help Tutor
did. When a student clicks on the Check button on the same tab twice within 3 s and gets red both times, the meta-tutor complains that the
student is guessing. If the student clicks on the Give-up button without ﬁlling out anything and checking it, the meta-tutor complains that
the student is giving up too early.
Some of the training problems let students practice debugging an incorrect model. They present a model that will run but generates
incorrect graphs. This kind of situation occurs frequently when the system is in test model (the tutor and meta-tutor are turned off). When
the model has been run, students see that some nodes have red “g” indicators and some have green ones. They face a decision of where to
start looking for an error in the model. The Introduction slides teach all students two heuristics:
 When possible, start by examining a node that has a red “g” indicator and no incoming links from nodes with red “g” indicators. Such a
node is guaranteed to have an error inside it.
 Avoid editing nodes that have green “g” indicators, because if the graph is correct, the node’s deﬁnition is probably correct.
When the meta-tutor is on and students are working on a debugging problem, it constrains them to obey these two heuristics.
In summary, the meta-tutor actually uses three types of scaffolding: (1) it requires students to follow the Target Variable Strategy; (2) it
complains when they abuse the Check or Give-up buttons; and (3) it teaches some heuristics for locating errors in buggy models. Although
(1) is procedural scaffolding, for convenience, we refer to this collection as “the meta-strategy.”

3. System architecture and implementation
The AMT system can be divided into two parts: the tutor and the meta-tutor. The tutor’s main functionalities included drawing a model,
checking correctness and running the model. It is an example-tracing tutor (VanLehn, 2006) in that an author must provide a correct model
with each problem; the students’ work is checked against that model. On the other hand, the meta-tutor is driven by algorithms (e.g., the
Target Node Strategy) that work for any problem. The student’s actions are check against the actions selected by the meta-strategy.
Although the meta-tutor and the tutor belong to the same Java project, they don’t share objects in the memory. This made it possible to
develop the tutor and meta-tutor programme independently. It also facilitates data analysis as will be explained later. More details about the
tutor’s architecture and implementation are presented in (Gonzalez-Sanchez, Chavez-Echeagaray, VanLehn, & Burleson, 2011). This section
describes the meta-tutor only.
Between the tutor and meta-tutor, there are three kinds of communication, each with its own channel as shown in Fig. 10:
(1) Every action made by student is sent to the meta-tutor via the activity channel.
(2) Before executing certain user actions, such as closing a node, the tutor sends a message to the meta-tutor via the block channel and
meta-tutor responds to the tutor, indicating whether the action should be blocked or not.
(3) Whenever the meta-tutor detects that the student needs an unsolicited hint, it sends a command to the tutor via the message channel to
tell it to initialize the tutorial dialogue. The tutor sends back the student’s response. Based on the response, the meta-tutor tells the tutor
either to display another dialogue box or to close the dialogue.
The meta-tutor is a production system, implemented in Java using Drools Expert (http://www.jboss.org/drools/drools-expert.html). The
production system implements two major functionalities: requiring the student to follow the Target Node Strategy and discouraging
gaming. The following sections describe the implementations of each function.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Tutor

Activity
channel

Student’s actions
Query at student’s actions

Message Block
channel channel

Response[block or not]

Activity
channel

209

Metatutor

Block Message
channel channel

Tutorial dialog
Student’s answer to dialog
Fig. 10. How the tutor and meta-tutor communicate with each other.

3.1. Constraining students to follow the Target Node Strategy
One main function of the meta-tutor is to constrain the student to follow the Target Node Strategy. In order to do so, it implements the
algorithm shown in Fig. 11. Aside from a little bookkeeping, the procedure’s actions are either “Prompt and constrain the student to do
<step>,” or “Constrain the student to do <step>”.
Prompting means that as soon as the problem state allows a step to be done, the meta-tutor gives the student an unsolicited hint about
the step. Here are examples of production rules that implement prompting:
 IF the activity channel reports that the student’s action is “closed a node”
AND there are at least two nodes that can become the next target node
THEN
send a command via the message channel to show a dialogue box that lists the nodes and ask which one the student wants to choose
as the target node.
 IF the context is that the node editor is open and the current tab is the plan tab,
AND the activity channel reports that the student’s action is “clicked on the Check”
AND the correct plan of this node is “a ﬁxed, given number”
AND the student has selected this plan
THEN
send a command via the message channel to show a message that says that the next step is to ﬁll in the Inputs tab by clicking on
“Value is ﬁxed, so no inputs.”
Constraining the student to do a step means that if the problem state is such that the student should do a certain action, and the student
tries to do another action instead, then the meta-tutor blocks the student’s action from occurring and pops up a message indicating the right
action to do. Some examples of production rules that implement constraining are:
 IF the context is that the student is in the model tab and the node editor is not open
AND the student’s action is clicking on a node in order to open it in the node editor
AND the node that student is trying to open is not the target node
THEN
send via the block channel the message “Please focus on your target node, <target node name>” and block opening the clicked-on
node.
 IF the context is that the node editor is open
AND the student’s action is closing the node
AND the node’s calculation tab is not ﬁnished yet
THEN
Send via the block channel the message “Please ﬁnish the target node, <target node name>, before leaving the editor” and block
closing of the node editor.
In order for the meta-tutor to track the student’s progress and respond appropriately, its rules need to have up-to-date information about
the state of the problem. Using the activity channel, the tutoring system sends all student actions to the meta-tutor, which converts some of
them (e.g., not node movements) into elements in the production system’s working memory. The working memory is also used to store the
identity of the target node, the contents of the sought set and other bookkeeping information that is kept updated via production rules.
3.2. Discouraging gaming
As mentioned earlier, the Give-up and Check buttons can be easily misused by students to construct models without thinking deeply or
learning, e.g., “gaming the system” (Baker et al., 2006; Baker, Corbett, Koedinger, et al., 2004). Although constraining students to follow the
Target Node Strategy may discourage gaming, the meta-tutor also looks for patterns of student actions that indicate gaming is occurring.
When it sees such a pattern, it pops up a warning message but does not block the student’s actions.
Detecting and responding to gaming of the Check button is implemented by the ﬁnite state machine shown in Fig. 12. The students are
always in one of 5 states. When they start working on a problem, they start in state S1 (the node editor is not open). Clicking on a node opens
the node editor and moves to state S2 (no check yet). Edits to the contents of the open tab can occur in any of the states except S1, and are not
shown in Fig. 12, as they merely add a loop from a state back to itself. The arc label “Wrong check” means that the student clicked on the
Check button and got red, indicating the tab was not ﬁlled out correctly. The arc label “Got the answer right” means the student either

210

L. Zhang et al. / Computers & Education 75 (2014) 196–217

1. Initialize the target node to the top level goal (i.e., the quantity the problem wants graphed).
Initialize the sought set to null.
2. Constrain the student to create a node and fill out its Description tab to correspond to the top level
goal quantity.
3. Constrain the student to fill out the Plan tab correctly.
4. Prompt and constrain the student to fill out the Input tab correctly. Add any newly created nodes
to the sought set.
5. Prompt and constrain the student to fill out the Calculation tab correctly.
6. Constrain the student to close the node editor.
7. If there are no nodes in the sought set, then prompt and constrain student to run the model.
If there is just one node in the sought set, then set it as the target node and tell the student.
If there are two or more nodes in the sought set, ask the student which one should be the target
node and set it to be the target node.
8. Remove the target node from the sought set.
9. Prompt and constrain the student to create a node and fill out its Description tab to correspond to
the target node.
10. Go to step 3 above.
Fig. 11. The Target Node Strategy implemented by the meta-tutor.

clicked on the Check button and got green, or clicked on the Give-up button. Whenever the student enters state S4 (gaming detected), the
meta-tutor sends a randomly chosen message such as, “Guessing so quickly wastes the opportunity to learn.”
In short, because the main job of the meta-tutor was to teach students to stay on a certain path deﬁned by the meta-strategy by blocking
off-path actions, and the meta-strategy was simple and easily deﬁned, the only signiﬁcant technical challenge was integrating the metatutor with the tutor. The tutor was implemented with a standard model-view-controller architecture, and the meta-tutor essentially was
given a chance to intervene after the user’s actions had actuated the control and before the model was updated.
At this point, the design and implementation of the AMT system have been deﬁned, so it is time to consider whether it works. That is,
does meta-tutoring improve students’ learning compared to tutoring without meta-tutoring?
4. Evaluating the meta-tutor
So far, we have conducted 5 studies of the AMT system. In the ﬁrst two studies, which were conducted in the summer of 2010, students
used the tutoring system without the meta-tutor. This led to signiﬁcant changes in the design of the tutoring system (VanLehn et al., 2011),
the materials and the experimental procedure (VanLehn et al., 2011). The two studies also produced data revealing students’ deep and
shallow modelling practices, and thus allowed us to design the meta-tutor. This article presents the results of the next three studies. There
were slight changes to the tutor and the experimental procedure in between the studies 3 and 4, which were conducted in summer 2011.
Analysis of the data from these two studies over the next year led to signiﬁcant changes to the AMT system, such as adding the Plan tab,
which led to study 5, which was conducted in summer 2012.
All the three studies had two phases, training and transfer, as did most of the studies reviewed earlier. During the training phase, a metatutor taught students both to use a good learning strategy (adapted from Pyrenee’s) and to avoid using poor learning strategies. During the
transfer phase, the meta-tutor and the tutor were turned off, thus allowing us to measure both domain learning and spontaneous use of the
learning strategy.
4.1. Research hypotheses
Recall that our framework, adopted from the literature on learning strategies, poses four research questions:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
Close the node
/leave the tab

Close the node
/leave the tab

S1

Wrong check
& interval > 3s
Wrong check

Open a node
Get to the input tab

S2

S3

Got the answer Got the answer right
right

Interval
>3s

Wrong check
& interval < 3 sec

Close the node
/leave the tab

S5

Got the answer right

Close the node
/leave the tab

S4
Wrong check
& interval < 3s

S1: no opened node editor
S2: no check
S3: one wrong check
S4: gaming
S5: got right

Fig. 12. Finite state machine for gaming the Check button.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

211

D. After instruction in the learning strategy ceases, do students revert to poor learning strategies?
Our observations during studies 1 and 2 answered question A by demonstrating that our students often use poor learning strategies
when they are not taught good ones. Moreover, because the meta-tutor requires students to follow the Target Node Strategy during the
training phase, question B cannot be addressed by our studies. This leaves us to focus on C and D.
Addressing question C requires that we clearly deﬁne the expected domain learning. As mentioned earlier, two entirely different
instructional objectives are associated with model construction: Using model construction to learn domain concepts and principles (usually
in a science classes) and learning how to construct models (usually in math classes). In these studies, we focus only on the second objective,
so “domain learning” in question C means learning how to do deep model construction. Moreover, question C can be asked of both the
training phase, where the learning strategy is taught, and the transfer phase, when the meta-tutor and tutor are turned off. Thus, the speciﬁc
hypotheses we tested in studies 3, 4 and 5 are:
1. In the transfer phase, do meta-tutored students display deep modelling more frequently than students who were not meta-tutored?
2. In the training phase, do meta-tutored students display deep modelling more frequently than students who do not receive metatutoring?
3. In the transfer phase, do meta-tutored students follow the Target Node Strategy more frequently than students who were not metatutored?
Although we predict positive answers for all three questions, there is a caveat concerning the third hypothesis. The instruction used in
studies 3, 4 and 5 did not provide much meta-cognitive and motivational encouragement to use the learning strategy, because we are
developing that part of the instruction for use in later studies. Thus, we have low expectations for hypothesis 3.
4.2. Method
Students were randomly assigned to one of two conditions: with and without meta-tutoring. The difference between the conditions
occurred only during a training phase where students learned how to solve model construction problems. The meta-tutor group solved
problems with the meta-tutor turned on, while the control group solved the same problems with the meta-tutor turned off. During the
training phase, all students could use the Check and Give-up buttons at any time.
In order to assess how much students learned, a transfer phase followed the training phase. During the transfer phase, all students solved
model construction problems with almost no help. That is, the meta-tutor and the Give-up button were turned off, and the Check button was
turned off everywhere except on the Description tab where it remained enabled in order to facilitate grounding, as mentioned earlier.
Because system dynamics is rarely taught in high school, the procedure did not include a pre-test in modelling dynamic systems.
In order to provide a motivation similar to that which occurs in school, we told students that prizes would be awarded to students who
solved the most problems during the transfer phase. In particular, we repeatedly told them to use the Check and Give-up buttons judiciously
during the training, trying always to learn as much as possible, so that they could rapidly solve problems during the transfer phase when the
Check and Give-up buttons would not be available.
4.3. Participants
There were 34 student participants in the ﬁrst experiment, 44 students participated in the second experiment, and 34 students in the
third experiment. No person participated in more than one experiment. All were high school students who were participating in summer
camps at our university.
4.4. Procedure
The three experiments followed the same procedure. Each lasted two and a half hours and had two main phases, training and transfer.
Sensors recorded students’ physiological states throughout both the training and transfer phases. The sensors were: wireless skin
conductance bracelets, facial expression cameras, posture-sensing chairs, and pressure sensitive mice. Periodically, a window would pop up
and ask students to report their affective state. These data are being used to develop algorithms for affect detection, and will not be discussed further here. Also in the ﬁrst two experiments, but not the third, students were asked to speak their thoughts aloud into a headset
microphone, and their verbal protocols were recorded by screen-capture software.
The summer camp students were available for only a ﬁxed period of time and all needed to be kept occupied productively during that
period. Thus, each phase of the experiment lasted a ﬁxed period of time, and students solved as many problems as they could during that time.
We obtained parental consent for minors prior to the experiment. During the experiment, student gave informed consent, ﬁlled out a
background questionnaire, donned sensors and started the training phase. The training phase lasted a total of 75 min. It consisted of ﬁrst
studying a sequence of PowerPoint slides that introduced them to the user interface, model construction and the Target Node Strategy, and
then solving a sequence of model construction problems. The introduction slides remained available while students were solving problems.
During pilot testing, we noticed that some students spent a long time studying the introduction then rarely referred back to it, whereas
other studied the introduction brieﬂy and referred back to it frequently as they solved problems. Thus, we let students decide how to allocate
their 75 min between the studying introduction and solving the training problems. The training phase was followed by a 15-min break
where students went to another room and had a snack. After the break, all the students began the transfer phase, where both conditions
were identical. The transfer phase lasted for 30 min, and a debrieﬁng of the students followed.
Students in both conditions solved the same problems in the same order. Except for a few debugging problems during the training phase,
where student were asked to correct a given faulty model, both training and transfer problems required students to construct a correct

212

L. Zhang et al. / Computers & Education 75 (2014) 196–217

model, which is deﬁned as a model whose graphs match graphs of correct values. When the model was correct, students were allowed to go
on to the next problem. This procedure was followed for both the training and transfer phases.
The only procedural difference between the training and transfer phases was the amount of scaffolding available. During the training
phase, the Check and Give-up buttons were enabled, and the meta-tutor was active for students in the meta-tutor condition. During the
transfer phase, the Check button was enabled only on the Description tab because it was necessary for grounding, as discussed earlier. None
of the other scaffolding was available during the transfer phase.
4.5. Measures
To test the hypotheses and answer the research questions, 7 measures were deﬁned by extracting information about student’s interaction with software from students’ log ﬁles. The measures are described in this section.
Hypothesis 1 is that the meta-tutored students will use deep modelling more frequently than the control students during the transfer
phase. Deep modelling is not easy to measure directly, so we used three indirect indicators:
 The number of the “Run model” button presses for completing one problem in the transfer phase indicates how much help the student needs
from the system. Deep modellers should be able to complete problems using the Run model button only a couple of times per model.
 Another measure was the number of extra nodes created during the transfer phase, where extra nodes are deﬁned as the nodes that can
be legally created for the problem but are not required for solving the problem. Deep modellers should realize that these quantities are
irrelevant and therefore avoid modelling them.
 The number of problems completed during the 30 min transfer period was considered to be an indicator of deep modelling with the
assumption that it would be faster for students to solve atomic modelling problems deeply than shallowly.
Hypothesis 2 is that meta-tutored students should use deep modelling more frequently than the control group students during the
training phase. The three dependent measures used to evaluate this hypothesis are described next.
 Help button usage: Ideally, a deep modeller would ﬁll out a tab, click on the Check button and the tab’s ﬁelds would turn green (correct).
A student who is trying to do deep modelling but hasn’t mastered the skill might have to click the Check button twice, thinking hard
before each attempt, in order to get them right. On the other hand, shallow modellers might click on the Check button repeatedly as they
guess or use the Give-up button. To measure usage of the help buttons, the following measure was calculated:

Help usage ¼ nwc þ 3ngu



nrn

where nwc is the number of Check button presses that yielded red, ngu is the number of Give-up buttons the student clicked, and nrn is the
number of nodes required by the problem. The “3” here is a weight to make ngu be roughly comparable to nwc. The denominator represents
the number of nodes required for a correct, minimal solution rather than the actual number of nodes the student completed. Should the
prediction for Hypothesis 2 hold true, meta-tutored students’ usage of the help button should be lower than the control students’ one.
 Correct on ﬁrst Check: This measure for Hypothesis 2 is a traditional one in tutoring system research. It represents how frequently
students succeed on their ﬁrst attempt at ﬁlling out a tab. That is, what proportion of the tabs turned green when students ﬁrst clicked
on the Check button? Deep modellers should get almost everything right the ﬁrst time, whereas a student who is confused or guessing
might rarely get elements right on the ﬁrst try. Unfortunately, this measure could only be applied to experiment 5, as insufﬁcient log
data was kept during the 2011 experiments.
 Training efﬁciency: This measure is based on the (now dubious) assumption that deep modelling is faster than shallow modelling. Speed
is often measured by counting the number of problems solved during a ﬁxed training period. However, our problems’ solutions varied in
their complexity. Some had many nodes and some had few nodes. Moreover, students could always use the Give-up button, which does
part of the problem solving for them. There was one Give-up button per tab, and completing a node requires ﬁlling out three tabs, so
clicking on three Give-up buttons per node would solve the problem. Thus, a better measure of the amount of problem solving
accomplished than “problems completed” is the number of tabs the student completed without using the Give-up button, which is
given by:

Training efficiency ¼ 3ncn  ngu
where ncn is the number of nodes that the student completed correctly (so 3ncn is the number of tabs), and ngu is the number of Give-up
buttons the student clicked. The prediction for Hypothesis 2 is that the training efﬁciency of meta-tutored students should be higher than
the training efﬁciency of control students.
Hypothesis 3 is that the experimental group, which was required to follow the Target Node Strategy during training, would continue to
use it during the transfer phase. To evaluate this hypothesis, we calculated the proportion of student steps consistent with the target node
strategy. The algorithm of Fig. 12 describes how it is calculated.
5. Results
Experiments 1 and 2 found, as expected, that students often exhibit shallow learning strategies (VanLehn et al., 2011a). They also led to
many revisions in the software and the experimental procedure (VanLehn et al., 2011b). However, both experiments had only one condition,
and it did not include the meta-tutor. This section reports on comparisons of the system with the meta-tutor turned on and turned off.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

213

5.1. Experiment 3 results
Experiment 3 was conducted in June 2011. Of the 34 participants, some students’ data needed to be omitted. Probably because there were
too many introduction slides (101 in total), 11 out of 34 students were still in the introduction phase at the break, and thus had no time in the
training phase where the manipulation occurred. These 11 students were omitted from the analyses. Two students, one in each condition,
performed much better than others. Both students’ training measures and test measures were greater than three standard deviations from
the means. These two students were also excluded from the analyses. The ﬁnal number of students left for each group was 11 (control) and
12 (meta-tutor). All the analyses below were computed based on these 23 students.
5.1.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: Of the 23 students, there were only 9 Meta-tutored students and 4 control students that ﬁnished the ﬁrst
problem in the transfer phase. Among the 13 students, meta-tutored students used the Run Model button 4.88 times per problem, while
students in control group used it 6.67 times. Due to the limited number of subjects, it is not surprising that the difference is not signiﬁcant
(p ¼ 0.72, d ¼ 0.12). This T-test, and all other tests reported here, are two-tailed.
Extra nodes: The ﬁrst problem of the transfer phase allowed 2 extra nodes, thus allowing us to measure the number of extra nodes created
by students who completed that problem. Meta-tutor students deﬁned 0.44 (SD ¼ 0.8) extra nodes vs. 0.75 (SD ¼ 0.96) extra nodes for the
control students. The difference was not reliable (p ¼ 0.61, d ¼ 0.58) although it was in the expected direction.
Number of problems completed: The average number of problems completed during the transfer phase was 0.82 (SD ¼ 0.60) for the metatutor students vs. 0.35 (SD ¼ 0.52) for the control students. The difference was marginally signiﬁcant (p ¼ 0.057) even though the effect size
was large (d ¼ 0.88) and in the expected direction. These ﬁgures show that on average, students completed less than one problem. More
speciﬁcally, 9 meta-tutored and 4 control students ﬁnished one or more problems, which was a marginally reliable difference (c2 ¼ 3.486,
p ¼ 0.06).
In short, trends in the data support Hypothesis 1 but the differences were not reliable, probably due to the small sample size.
5.1.2. Hypothesis 2 (training phase deep modelling)
Help button usage: This measure is a weighted sum of help button uses divided by the total number of nodes completed. On this measure,
the meta-tutor students averaged 4.78 (SD ¼ 2.25) vs. 7.03 (SD ¼ 3.44) for the control students. The difference was marginally signiﬁcant
according with a large effect size (p ¼ 0.08, d ¼ 0.82).
Training efﬁciency: The average training efﬁciency measure the amount of correct work done by the student during the ﬁxed-length
training period. For the meta-tutor group, training efﬁciency was 13.00 (SD ¼ 6.94) vs. 11.45 (SD ¼ 6.77) for the control group. The difference was not signiﬁcant (p ¼ 0.30; d ¼ 0.23).
Thus, although there was a trend in the data supporting Hypothesis 2, the reliability was poor, perhaps due to the small sample size.
5.1.3. Hypothesis 3 (meta-strategy usage)
Hypothesis 3 was that meta-tutored students would voluntary the Target Node Strategy during the transfer phase more frequently than
the student who was not meta-tutored. During the transfer phase, 0.39 (SD ¼ 0.35) of the meta-tutor students’ steps matched the Target
Node Strategy’s steps, vs. 0.34 (SD ¼ 0.30) for the control students. This difference was quite small (p ¼ 0.70, d ¼ 0.16), suggesting that
hypothesis 3 may be false for this study.
5.1.4. Summary of results and next step forward
Although there were some trends in the expected directions, the students in both conditions performed quite poorly. This was probably
due to the large number of slides in the introduction phase as well as the difﬁculty of the tasks themselves.
In order to increase the number of training and transfer problems solved by students, thus allowing us to differentiate their performance
statistically, we reduced the number of slides from 101 to 64, and simpliﬁed some of the tasks. As the second experiment conﬁrmed, these
changes led to an improvement on the performance students in both phases.

5.2. Experiment 4 results
Experiment 4 took place in July 2011. Data from all 44 participants (22 in each condition) were used in the analyses here.
5.2.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: To complete the ﬁrst problem in the transfer phase, meta-tutored students used the Run model button 3.05
times on average. In comparison, students in the control group used the Run model button 5.13 times. However, the difference was not
signiﬁcant (p ¼ 0.31, d ¼ 0.32). Because the standard deviation was high (SD ¼ 6.77) due to extreme values, we divided all the students into
two types with the threshold being 2, which is the median. The students who used the run model button once or twice were considered
deep modellers. The rest of the students were considered shallow modellers. Chi-square test is then used to compare the number of deep
modellers in meta-tutored group to the number in control group. There was a trend in the expected direction, but the signiﬁcance was
marginal (c2 ¼ 3.36, p ¼ 0.067).
Extra nodes: Because most of the students ﬁnished at least two tasks in the transfer phase (only 3 students in the control group did not)
and the second task allowed up to two extra nodes, we used the second task to count extra nodes. As predicted by Hypothesis 1, metatutored students produced fewer extra nodes (0.27, SD ¼ 0.70) than control students (0.95, SD ¼ 1.03). The difference was signiﬁcant
with a large effect size (p ¼ 0.02, d ¼ 0.80).
Problems completed: The meta-tutor students solved 3.27 (SD ¼ 1.03) transfer problems vs. 3.23 (SD ¼ 1.57) for the control students. The
difference was small and not signiﬁcant (p ¼ 0.65, d ¼ 0.04).

214

L. Zhang et al. / Computers & Education 75 (2014) 196–217

5.2.2. Hypothesis 2 (training phase deep modelling)
Help button usage: Consistent with Hypothesis 2, meta-tutor students’ help button usage averaged 2.35 (SD ¼ 1.75) vs. 3.55 (SD ¼ 1.85)
for the control students. The difference was signiﬁcant (p ¼ 0.04, d ¼ 0.68).
Training efﬁciency: Contrary to Hypothesis 2, the control students had higher training efﬁciency 72.77 (SD ¼ 32.12) than the meta-tutor
students, 54.36 (20.17), and the difference was marginally signiﬁcant (p ¼ 0.05, d ¼ 0.70).
These results suggest that meta-tutor students did increase deep modelling in the training phase than the control students, but they also
moved slower than control students.
5.2.3. Hypothesis 3 (use of Target Node Strategy)
Missing data precluded testing this hypothesis in experiment 4.
5.2.4. Summary of results, revisions and the next iteration
Both hypothesis 1 (transfer) and hypothesis 2 (training) were supported, albeit by one measure each. We were not satisﬁed with the pace
of the students in the training phase, especially the meta-tutored students. Watching the screen-capture video suggested that meta-tutored
students spent a lot of time in reading and answering the tutorial pop-ups. Students also became “stuck” (Burleson & Picard, 2007) when
ﬁlling out the current Inputs tab and needing to create new nodes. In order to do so, students had to close the node editor and click on the
Create Node button on the canvas in the Model tab. However, many students could not realize that they had to close the node editor. They
complained that no button in the Inputs tab could help them get out of the stuck state. Thus, we spent several months reﬁning both the
tutoring system and the meta-tutor. We installed the Plan tab in order to reduce the time spent reading the meta-tutor’s pop-ups. We also
improved the interface, adding a “create a new node,” button on the Input tab with the goal of reducing students’ state of stuck in this stage.
The transfer phase proved to be extremely challenging for the students. This was evident in the relatively small number of problems
solved as well as direct observation of the students. The null result on our productivity measure, number of problems solved in the transfer
phase, might be due to students in both conditions becoming discouraged and ceasing to try hard. Thus, we modiﬁed the transfer phase so
that besides colouring the “g” indicator, the system coloured the “i” indicator and “c” indicator as well to show the correctness of the
corresponding tabs. This was intended to make it easier to locate errors while leaving unchanged the logic required for ﬁxing the errors, and
thus allowing students to make faster progress through the test problems while still allowing us to assess their skill.
5.3. Experiment 5 results
Experiment 5 was conducted in June 2012. Of the 34 participants, data from 33 were used in the analyses below (16 in the control group
and 17 in meta-tutor group). One student was excluded due to his extraordinary performance. He ﬁnished all 7 problems in the transfer
phase well before the end of the transfer phase, while the second fastest person only completed 4 problems.
5.3.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: On average, students in the control group used the Run model button 7.76 times, while meta-tutored students
used the Run model button 7.82 times. So they almost had the same performance (p ¼ 0.98, d ¼ 0.0093).
Extra nodes: Most of the students ﬁnished at least two tasks in the transfer phase (one in each group didn’t), so we again used the second
task to measure extra nodes. As predicted by Hypothesis 1, the meta-tutor students produced fewer extra nodes (0.88, SD ¼ 0.96) than the
control students (1.13, SD ¼ 0.99). However, the difference was not reliable (p ¼ 0.47, d ¼ 0.26).
Problems solved: Contrary to Hypothesis 1, the meta-tutor students solved 2.18 (SD ¼ 0.53) transfer problems vs. 2.56 (SD ¼ 0.78) for the
control students. Students in the control group outperformed meta-tutored student with marginal signiﬁcance (p ¼ 0.094, d ¼ 0.57).
Once again, the meta-tutor students tended to work more slowly than the control students. This time, there was only a trend to show that
they might be doing more deep modelling than the control students.
5.3.2. Hypothesis 2 (training phase deep modelling)
Help button usage: As expected, meta-tutored students’ help button usage averaged 3.92 (SD ¼ 2.19) vs. 6.13 (SD ¼ 2.73) for the control
students. The difference was signiﬁcant with a large effect size (p ¼ 0.016, d ¼ 0.89).
Correct on ﬁrst attempt: To provide an additional test of Hypothesis 2, we calculated the percentage of time that the ﬁrst Check on a tab
was correct. Meta-tutored students achieved a higher percentage (0.77, SD ¼ 0.068) than control students (0.68, SD ¼ 0.11), and the difference was signiﬁcant with a large effect size (p ¼ 0.015, d ¼ 0.98).
Training efﬁciency: Meta-tutor students scored 73.18 (SD ¼ 27.53), a little bit higher in training efﬁciency than control students, 68.88
(SD ¼ 17.16), but the difference was not reliable (p ¼ 0.59, d ¼ 0.19).
So students in both groups kept the same pace in the training session this time, and there was strong evidence that the meta-tutor
students were engaged in deeper modelling than the control students.
5.3.3. Hypothesis 3 (use of Target Node Strategy)
Contrary to hypothesis 3, the meta-tutored students had nearly the same level of Target Node Strategy usage (0.66, SD ¼ 0.23) as control
students (0.70, SD ¼ 0.19), and the difference is not reliable (p ¼ 0.59, d ¼ 0.19).
6. Discussion
Our results are summarized in Table 1. This section discusses our interpretation of them.
As mentioned in the introduction, whenever a new kind of instruction is developed, there are four classic questions to ask about learning
strategies that are speciﬁc to it:

L. Zhang et al. / Computers & Education 75 (2014) 196–217

215

Table 1
Summary of the results.
Measure (predicted dir.)

Experiment 3 (N ¼ 23)

Experiment 4 (N ¼ 44)

Experiment 5 (N ¼ 33)

Transfer phase (Hypothesis 1)
Run model button usage (E < C)
Extra nodes (E < C)
Probs completed (E > C)

Not available
E < C (p ¼ 0.61, d ¼ 0.58)
E > C (p ¼ 0.06, d ¼ 0.88)

E < C (p ¼ 0.31, d ¼ 0.32)
E < C (p [ 0.02, d [ 0.80)
E z C (p ¼ 0.65, d ¼ 0.04)

E z C (p ¼ 0.98, d ¼ 0.0093)
E < C (p ¼ 0.47, d ¼ 0.26)
E < C (p ¼ 0.09, d ¼ 0.57)

Training phase (Hypothesis 2)
Help button usage (E < C)
Correct on 1st Chk (E > C)
Efﬁciency (E > C)

E < C (p ¼ 0.08, d ¼ 0.82)
Missing data
E > C (p ¼ 0.30, d ¼ 0.23)

E < C (p [ 0.04, d [ 0.68)
Missing data
E < C (p ¼ 0.05, d ¼ 0.70)

E < C (p [ 0.02, d [ 0.89)
E > C (p [ 0.015, d [ 0.98)
E > C (p ¼ 0.59, d ¼ 0.19)

Missing data

E z C (p ¼ 0.59, d ¼ 0.19).

Transfer phase use of Target Node Strategy (Hypothesis 3)
Usage (E ¼ C)
E z C (p ¼ 0.70, d ¼ 0.16)

E stands for the meta-tutor group, and C stands for the control group. Reliable results are bold.

A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
After we developed a step-based tutoring system for model construction, experiments 1 and 2 answered question A by ﬁnding that
student did indeed exhibit poor learning strategies when using it. This led us to develop a meta-tutor that taught students a meta-strategy
that we hoped would increase their acquisition of skill in model construction.
Answering question B requires that even during the training phase, students have the freedom to choose between following or not
following the learning strategy. This freedom allows experimenters to measure the growth in compliance during the training phase. Of the
four systems reviewed earlier (Betty’s Brain, the Help Tutor, Co-Lab and Pyrenees; see Table 2), Betty’s Brain and the Help Tutor provide such
freedom, and thus were able to show that while students were being meta-tutored, their behaviours were more often consistent with the
learning strategy than the behaviours of students who were not being meta-tutored. This suggests that their meta-tutoring was effective at
getting students to use the taught learning strategy. AMT, Co-Lab and Pyrenees all required students to follow the taught learning strategy
during the training phase, so they could not answer question B.
Given a learning strategy that designers think is good, question C asks whether it really does increase domain learning. Of the four
systems reviewed earlier, only Pyrenees’ learning strategy increased domain learning. Thus, we adapted its learning strategy for use in AMT.
Because the goal of our system is teach students to construct models properly, most of the measures addressed the frequency of deep
modelling. During the training phase, on the measures “help button usage” and “correct on ﬁrst check” meta-tutored students scored
reliably higher than students who were not meta-tutored, except on experiment 3, which appears to have been underpowered. Moreover,
the effect sizes were large (d ¼ 0.89; d ¼ 0.98) or moderately larger (d ¼ 0.68). Thus, the AMT results for domain learning in the training
phase were nearly as good as those from Pyrenees, and substantially better than the other three meta-tutoring systems.
Unfortunately, neither the Target Node Strategy nor the domain learning advantages transferred. During the transfer phase, on the
measures “Run model button usage”, “extra nodes” and “Target Node Strategy usage”, the meta-tutored students were no better than the
students who had not been meta-tutored earlier, with one exception. On the measure “extra nodes” in experiment 4, meta-tutored students
outscored the control students. It is always difﬁcult to get learning to transfer from a supported context to an unsupported one, so this lack of
transfer should perhaps not be surprising. However, it does appear to be a bit weaker than the transfer obtained by Pyrenees, Betty’s Brain
and the Help Tutor.
In both the training and transfer phases, we attempted to measure deep modelling using efﬁciency: the amount of modelling done in a
ﬁxed period of time. This measurement made the tacit assumption that deep modelling is faster than shallow modelling. That is, thinking
hard to solve an atomic modelling problem should take less time than guessing, overusing the Give-up button, overusing the Run button,
scanning the problem statement for keywords and other shallow model construction tactics. However, the efﬁciency measures all produced
null results. If anything, there was trend for non-meta-tutored students to get more done than the meta-tutored students. This is consistent
with our informal analyses of the log data. It appears that guessing was actually quite a bit faster than thinking, especially in the training
phase when the Check button was enabled. Even when students could only use the Run Model button for feedback in the transfer phase,
guessing was fast because the models were so small for the ﬁrst few problems. After that, guessing became must less efﬁcient, but few
students got that far.
As the experience with Betty’s Brain and the Help Tutor shows, not every supposedly good learning strategy actually turns out to increase
domain learning. In the case of AMT, we have found a learning strategy that does increase domain learning, and thus deserves to be taught.
On the other hand, our method of teaching the learning strategy appears not to have had enough meta-cognitive or motivational impact
on students, because their gains while being meta-tutor did not persist when the meta-tutoring was turned off.
Thus, the next stage of the AMT project is to augment the instruction with an affective learning companion. Its job will be to persuade
students of the beneﬁts of using the Target Node Strategy and of not abusing the Check and Give-up buttons. The agent cannot use the
argument that the learning strategy and deep modelling will speed up the students’ work, because we have found that shallow modelling
strategies are actually faster at least on these simple problems. Thus, we plan on having the agent use Dweck’s well-known argument
(Dweck & Leggett, 1988) that the “mind is a muscle; the harder you exercise it, the stronger it becomes.” Our summer school students
presumably want stronger minds, so if they believe the agent, they should more often use both the learning strategy and deep modelling. In
order to make the agent easier to believe, we plan to use attribution shifting, empathy, rapport-building chit-chat, and other non-cognitive

216

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Table 2
Comparison of four meta-tutors of model construction.
Meta-tutor

AMT
Pyrenees
Help Tutor
Betty’s Brain
Co-Lab

Training phase

Transfer phase

Knowledge/skill

Meta-strategy

Knowledge/skill

Meta-strategy

þ
þþ
NS
NS
NS

Required
Required
þ
þ
Required

þ
þþ
NS
þ

NS
NS
þ
þ

NS ¼ non-signiﬁcant difference, two-tailed. þ ¼ Signiﬁcant but weak. þþ ¼ Signiﬁcant and strong. Required ¼ meta-tutor required student to follow the learning strategy.

techniques. In order to optimize the timing and selection of these non-cognitive interventions, we plan to monitor the students’ affective
state using physiological sensors.
Lastly, by experiment 5, students seem to be acquiring decent amounts of competence in system dynamics modelling in only 1 h and
15 min total. This is a considerable reduction from the 5 or more hours required of Model-It students and others. Although we have no way
to actually compare our results to the early ones, because the systems, students, domains and almost everything else were quite different,
we nonetheless are quite encouraged. The combination of tutoring and meta-tutoring may be the key to getting model construction out into
the classrooms at last.
Acknowledgements
This material is based upon work supported by the National Science Foundation, United States (_100000001) under Grant No. 0910221.
References
Alessi, S. M.. (December 2000). The application of system dynamics modeling in elementary and secondary school curricula. In Paper presented at the RIBIE 2000 – The ﬁfth
Iberoamerican conference on informatics in education. Viña del Mar, Chile.
Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking (applying cognitive modeling to help-seeking skills). In J. C. Lester, R. M. Vicari, &
F. Paraguacu (Eds.), Intelligent tutoring systems: Seventh international conference: ITS 2005 (pp. 227–239). Berlin: Springer.
Aleven, V., Stahl, E., Schworm, S., Fischer, F., & Wallace, R. M. (2003). Help seeking and help design in interactive learning environments. Review of Educational Research, 73(2),
277–320.
Baker, R. S. J. d., Corbett, A., & Koedinger, K. R. (2004). Detecting student misuse of intelligent tutoring systems. In Proceedings of the 7th international conference on intelligent
tutoring systems (pp. 531–540).
Baker, R. S., Corbett, A., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., et al. (2006). Adapting to when students game an intelligent tutoring system. In Intelligent tutoring
systems (pp. 392–401). Berlin: Springer.
Baker, R. S. J. d., Corbett, A., Koedinger, K. R., & Wagner, A. Z. (2004). Off-task behavior in the cognitive tutor classroom: when students “game the system”. In E. DykstraErickson, & M. Tscheligi (Eds.), Proceedings of the SIGCHI conference on human factors in computing systems (pp. 383–390). New York, NY: ACM.
Biswas, G., Leelawong, K., Schwartz, D. L., & Vye, N. J. (2005). Learning by teaching: a new agent paradigm for educational software. Applied Artiﬁcial Intelligence, 19, 363–392.
Booth Sweeney, L., & Sterman, J. D. (2000). Bathtub dynamics: initial results of a systems thinking inventory. System Dynamics Review, 16(4), 249–286.
Bravo, C., van Joolingen, W. R., & de Jong, T. (2009). Using Co-Lab to build system dynamics models: students’ actions and on-line tutorial advice. Computer and Education, 53,
243–251.
Bredeweg, B., & Forbus, K. D. (2003). Qualitative modeling in education. AI Magazine, 24(4), 35–46.
Burleson, W., & Picard, R. W. (2007). Affective learning companions. Educational Technology, Special Issue on Pedagogical Agents, Saddle Brook, N.J., 47(1), 28–32.
CCSSO. (2011). The Common Core State Standards for mathematics. Downloaded from www.corestandards.org. on 31.10.11.
Chi, M., & VanLehn, K. (2010). Meta-cognitive strategy instruction in intelligent tutoring systems: how, when and why. Journal of Educational Technology and Society, 13(1), 25–
39.
Chin, D., Dohmen, I. I. M., Cheng, B. H., Oppezzo, M., Chase, C. C., & Schwartz, D. L. (2010). Preparing students for future learning with teachable agents. Educational Technology
Research and Development, 58, 649–669.
Clark, H. H., & Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine, & S. D. Teasley (Eds.), Perspectives on socially shared cognition (pp. 127–149).
Washington, DC: American Psychological Association.
Collins, A., & Ferguson, W. (1993). Epistemic forms and epistemic games: structures and strategies to guide inquiry. Educational Psychologist, 28(1), 25–42.
Doerr, H. M. (1996). Stella ten-years later: a review of the literature. International Journal of Computers for Mathematical Learning, 1, 201–224.
Donker, A. S., de Boer, H., Kostons, D., Dignath van Ewijk, C. C., & van der Werf, M. P. C. (2014). Effectiveness of learning strategy instruction on academic performance: a metaanalysis. Educational Research Review, 11, 1–26.
Du Boulay, B., Avramides, K., Luckin, R., Martínez-Mirón, E., & Rebolledo-Méndez, G. (2010). Towards systems that care: a conceptual framework based on motivation,
metacognition and affect. International Journal of Artiﬁcial Intelligence in Education, 20(3), 197–229.
Dweck, C. S., & Leggett, E. L. (1988). A social-cognitive approach to motivation and personality. Psychological Review, 95(2), 256–273.
Gonzalez-Sanchez, J., Chavez-Echeagaray, M.-E., VanLehn, K., & Burleson, W. (2011). From behavioral description to a pattern-based model for intelligent tutoring systems. In
Paper presented at the Proceedings of the 18th international conference on pattern languages of programs (PLoP). Portland, OR.
Hashem, K., & Mioduser, D. (2011). The contribution of learning by modeling (LbM) to students’ understanding of complexity concepts. International Journal of e-Education, eBusiness, e-Management and e-Learning, 1(2), 151–157.
Hattie, J., Biggs, J., & Purdie, N. (1996). Effects of learning skills interventions on student learning: a meta-analysis of ﬁndings. Review of Educational Research, 66, 99–136.
Hestenes, D. (2007). Modeling theory for math and science education. In Paper presented at the ICTMA-13: The international community of teachers of mathematical modelling
and applications. Indiana, IL.
Hogan, K., & Thomas, D. (2001). Cognitive comparisons of students’ systems modeling in ecology. Journal of Science Education and Technology, 10(4), 319–345.
Lee, C. B., Jonassen, D., & Teo, T. (2011). The role of model building in problem solving and conceptual change. Interactive Learning Environments, 19(3), 247–265.
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: the Betty’s brain system. International Journal of Artiﬁcial Intelligence and Education, 18(3), 181–208.
Löhner, S., Van Joolingen, W. R., & Savelsbergh, E. R. (2003). The effect of external representation on constructing computer models of complex phenomena. Instructional
Science, 31, 395–418.
Mandinach, E. B., & Cline, H. F. (1994a). Classroom dynamics: Implementing a technology-based learning environment. Mahwah, NJ: Erlbaum.
Mandinach, E. B., & Cline, H. F. (1994b). Modeling and simulation in the secondary school curriculum: the impact on teachers. Interactive Learning Environments, 4(3), 271–289.
Marshall, S. P., Barthuli, K. E., Brewer, M. A., & Rose, F. E. (1989). Story problem solver: A schema-based system of instruction. San Diego, CA: Center for Research in Mathematics
and Science Education, San Diego State University.
Metcalf, S. J. (1999). The design of guided learner-adaptable scaffolding in interactive learning environment (Doctoral Dissertation). University of Michigan.
Metcalf, S. J., Krajcik, J., & Soloway, E. (2000). Model-It: a design retrospective. In M. J. Jacobson, & R. B. Kozma (Eds.), Innovations in science and mathematics education:
Advanced designs for technologies of learning (pp. 77–115).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

217

Mulder, Y. G., Lazonder, A. W., de Jong, T., Anjewierden, A., & Bollen, L. (2011). Validating and optimizing the effects of model progression in simulation-based inquiry learning.
Journal of Science Education and Technology, 21, 722–729.
Muldner, K., Burleson, W., van de Sande, B., & VanLehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, 21(1–2), 99–135.
Nathan, M. J. (1998). Knowledge and situational feedback in a learning environment for algebra story problem solving. Interactive Learning Environments, 5, 135–159.
National Research Council. (2012). A framework for K–12 science education: Practices, crosscutting concepts, and core ideas. Washington, DC: National Academies Press.
Richmond, B. M. (1985). STELLA: software for bringing system dynamics modeling to the other 98%. In Paper presented at the Proceedings of the 1985 international conference of
the System Dynamics Society: 1985 International system dynamics conference.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007a). Can help seeking be tutored? Searching for the secret sauce of metacognitive tutoring. In Proceedings of the international conference on artiﬁcial intelligence in education (pp. 203–210). Amsterdam: IOS Press.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007b). Designing for metacognition – applying cognitive tutor principles to the tutoring of help seeking. Metacognition and
Learning, 2(2).
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2011). Improving students’ help-seeking skills using metacognitive feedback in an intelligent tutoring system. Learning and
Instruction, 267–280.
Roll, I., Aleven, V., McLaren, B., Ryu, E., Baker, R. S. J. d., & Koedinger, K. R. (2006). The Help Tutor: does metacognitive feedback improve student’s help-seeking actions, skills
and learning. In M. Ikeda, K. Ashley, & T.-W. Chan (Eds.), Intelligent tutoring systems: 8th International conference, its 2006 (pp. 360–369). Berlin: Springer.
Russell, S., & Norvig, P. (2009). Artiﬁcial intelligence: A modern approach (2nd ed.). Upper Saddle River, NJ: Prentice Hall.
Schecker, H. (1993). Learning physics by making models. Physics Education, 28, 102–106.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012a). Relating student performance to action outcomes and context in a choice-rich learning environment. In S. A. Cerri,
W. J. Clancey, & G. Papadourakis (Eds.), Intelligent tutoring systems: 11th International conference its 2012 (pp. 505–510). Berlin: Springer-Verlag.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012b). Supporting student learning using conversational agents in a teachable agent environment. In Paper presented at the
Proceedings of the 10th international conference of the learning sciences. Sydney, Australia.
Shih, B., Koedinger, K. R., & Scheines, R. (2008). A response time model for bottom-out hints as worked examples. In C. Romero, S. Ventura, M. Pechenizkiy, & R. S. J. d Baker
(Eds.), Handbook of educational data mining (pp. 201–211). Boca Raton, FL: Taylor & Francis.
Steed, M. (1992). Stella, a simulation construction kit: cognitive process and educational implications. Journal of Computers in Mathematics and Science Teaching, 11, 39–52.
Stratford, S. J. (1997). A review of computer-based model research in precollege science classroom. Journal of Computers in Mathematics and Science Teaching, 16(1), 3–23.
Tan, J., Biswas, G., & Schwartz, D. L. (2006). Feedback for metacognitive support in learning by teaching environments. In Proceedings of the twenty-eighth annual meeting of the
Cognitive Science Society. Mahwah, NJ: Erlbaum.
Tan, J., Wagster, J., Wu, Y., & Biswas, G. (2007). Effect of metacognitive support on student behaviors in learning by teaching environments. In R. Luckin, K. R. Koedinger, &
J. Greer (Eds.), Proceedings of the 13th international conference on artiﬁcial intelligence in education (pp. 650–652). Amsterdam: IOS Press.
Timms, M. J. (2007). Using item response theory (IRT) to select hints in an ITS. In R. Luckin, K. R. Koedinger, & J. Greer (Eds.), Artiﬁcial intelligence in education (pp. 213–221).
Amsterdam: IOS Press.
Treagust, D. F., Chittleborough, G., & Mamiala, T. (2002). Students’ understanding of the role of scientiﬁc models in learning science. International Journal of Science Education,
24(4), 357–368.
VanLehn, K. (2006). The behavior of tutoring systems. International Journal of Artiﬁcial Intelligence and Education, 16, 227–265.
VanLehn, K. (2013). Model construction as a learning activity: a design space and review. Interactive Learning Environments, 21(4), 371–413.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011a). The level up procedure: how to measure learning
gains without pre- and post-testing. In T. Hirashima (Ed.), Proceedings of the 19th international conference on computers in education (pp. 96–100). Chiang-Mai, Thailand:
Asia-Paciﬁc Society for Computers in Education.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011b). The affective meta-tutoring project: how to motivate
students to use effective meta-cognitive strategies. In Paper presented at the 19th International conference on computers in education. Chiang Mai, Thailand.
Vanlehn, K., & Chi, M. (2012). Adaptive expertise as acceleration of future learning: a case study. In P. J. Durlach, & A. Lesgold (Eds.), Adaptive technologies for training and
education. Cambridge: Cambridge University Press.
Wagster, J., Tan, J., Biswas, G., & Schwartz, D. L. (2007). How metacognitive feedback affects behavior in learning and transfer. In Paper presented at the 13th International
conference on artiﬁcial intelligence in education: Workshop on metacognition and self-regulated learning in ITSs. Marina del Rey, CA.
Wagster, J., Tan, J., Wu, Y., Biswas, G., & Schwartz, D. L. (2007). Do learning by teaching environments with metacognitive support help students develop better learning
behaviors?. In Proceedings of the twenty-sixth annual meeting of the Cognitive Science Society. Mahwah, NJ: Erlbaum.
Wheeler, J. L., & Regian, J. W. (1999). The use of a cognitive tutoring system in the improvement of the abstract reasoning component of word problem solving. Computers in
Human Behavior, 15, 243–254.
Wilensky, U. (2003). Statistical mechanics for secondary school: the GasLab multi-agent modeling toolkit. International Journal of Computers for Mathematical Learning, 8(1),
1–41.
Wilensky, U., & Reisman, K. (2006). Thinking like a wolf, a sheep, or a ﬁreﬂy: learning biology through constructing and testing computational theories – an embodied
modeling approach. Cognition and Instruction, 24(2), 171–209.
Zaraza, R., & Fisher, D. (1997). Introducing system dynamics into the traditional secondary curriculum: the CC-STADUS project’s search for leverage points. In Paper presented
at the 15th International system dynamics conference. Istanbul, Turkey.
Zaraza, R., & Fisher, D. (1999). Training system modelers: the NSF CC-STADUS and CC-SUSTAIN projects. In W. Feurzeig, & N. Roberts (Eds.), Modeling and simulation in science and
mathematics education (Vol. 1); (pp. 38–69). New York, NY: Springer.

The Conceptual Helper: An Intelligent Tutoring System
for Teaching Fundamental Physics Concepts
Patricia L. Albacete and Kurt VanLehn
1

1

2

Intelligent Systems Program, University of Pittsburgh, Albacete@isp.pitt.edu
2
LRDC, University of Pittsburgh, VanLehn@cs.pitt.edu
Abstract. This paper describes an intelligent tutoring system designed to help
students solve physics problems of a qualitative nature. The tutor uses a unique
cognitive based approach to teaching physics, which presents innovations in
three areas. 1) The teaching strategy, which focuses on teaching links among
the concepts of the domain that are essential for conceptual understanding yet
are seldom learned by the students. 2) The manner in which the knowledge is
taught, which is based on a combination of effective human tutoring techniques,
successful pedagogical methods, and less cognitively demanding approaches.
3) The way in which misconceptions are handled. The tutor was implemented
using the model-tracing paradigm and uses probabilistic assessment to guide the
remediation. Some preliminary results of the evaluation of the system are also
presented.

1 Introduction
Several studies conducted during the past fifteen years revealed that students in
traditional elementary mechanics classes can master problem solving of a quantitative
nature, but perform poorly in solving qualitative problems [Halloun & Hestenes,
1985; Hake, 1998]. (An example of a quantitative problem is “A 2kg create slides
down a frictionless inclined plane. Determine the acceleration of the crate given that
the angle of the plane is 30 degrees.” An example of a qualitative problem can be
seen in Figure 1.) Moreover, students’ naïve conceptions of physics remain intact
after finishing their classes, having not been modified or replaced by the newly
acquired scientific knowledge.
A few approaches have been proposed to improve this situation, though none has
met with great success [Hake, 1998]. Considering that elementary mechanics is a
required course for almost all science majors, the above results make it clear that there
is a need to improve the instruction of the subject. Toward this end, we developed an
intelligent tutoring system, called the Conceptual Helper, which presents a novel
cognitive-based approach to teaching conceptual physics. The Conceptual Helper
coaches students through homework problems in the area of Newtonian mechanics
that deals with linear motion and projectile motion in kinematics and dynamics.
The Conceptual Helper is part of a larger enterprise called Andes [VanLehn, 1996;
Gertner et al., 1998]. Andes is basically an immediate feedback model-tracing tutor
designed to coach first year physics students through problem solving. It has three
help systems. A Procedural Helper that provides procedural help during quantitative
problem solving. A Conceptual Helper [Albacete, 1999] that teaches conceptual
knowledge of the subject matter to students and tries to get them to abandon
G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, pp. 564-573, 2000.
 Springer-Verlag Berlin Heidelberg 2000

The Conceptual Helper: An ITS for Teaching Fundamental Physics Concepts

565

misconceptions (this system is described herein). And a Self-Explanation coach that
guides students through example studying. At present, each help system works in
isolation; however in the future the Conceptual Helper and the Procedural Helper will
work cooperatively in an attempt to integrate conceptual and quantitative problem
solving.
Two steel balls, one of which weights twice as much as the other, roll off of a horizontal
table with the same speeds. In this situation:
a) both balls impact the floor at approximately the same horizontal distance from the base
of the table.
b) the heavier ball impacts the floor closer to the base of the table than does the lighter.
c) the lighter ball impacts the floor closer to the base of the table than does the heavier.
Fig. 1. Example of a qualitative problem.

2 The Conceptual Helper from a Technical Point of View
The Conceptual Helper follows the model-tracing paradigm. In its simplest form a
model-tracing tutor contains a cognitive model that is capable of correctly solving any
problem assigned to the student. Then the technique basically consists of matching
every problem-solving action taken by the student with the steps of the expert’s
solution model of the problem being solved. This matching is used as the basis for
providing ongoing feedback to students while they progress through a problem. In
the Conceptual Helper when the students make a correct action, the input is turned
green to emulate the typical confirmation given by human tutors. On the other hand,
when the action is incorrect the input is turned red and some specific feedback is
provided.
One tool that the tutor uses to decide what knowledge to teach the student when he
makes a mistake is the student model. The student model is represented by a Bayesian
network. Each node in the network represents a piece of conceptual knowledge that
the student is expected to learn or a misconception that the tutor can help remedy. For
example, a node in the network is “if the velocity of an object is constant, then its
acceleration is zero.” Each node in the network has a number attached to it which
indicates the probability that the student has mastered it. In the case of
misconceptions the probabilities represent the likelihood that the student holds such a
misconception. As the student solves a problem, the probabilities are updated
according to the actions taken by the student and the feedback provided by the tutor.
The Conceptual Helper makes use of the student model in deciding which pieces of
knowledge it should try to teach to the students and when it should do so. When the
student makes a mistake while solving a problem, and the probability of his knowing
the corresponding piece of correct knowledge is higher than 0.8, then the tutor will
not try to teach it. It will just turn the entry red and then store a short explanation as
to what was incorrect. This explanation could be accessed by the student by
selecting, “what is wrong with that?” from a help menu. However, if the probability
of the student’s knowing the corresponding piece of knowledge is lower than 0.8, then
the Tutor will try to convey the corresponding knowledge to the student. In the case
when the mistake is most likely attributed to a known misconception, the helper just
tutors the student on it. The logic behind this decision is that misconceptions are

566

Patricia L. Albacete and Kurt VanLehn

rarely encountered more than once; hence regardless of the probability of the student
harboring such a misconception, it is safer to clarify it.

3 The Teaching Strategy Followed by the Conceptual Helper
The Conceptual Helper has two main goals: to teach qualitative physics, and to try to
get the students to abandon common misconceptions.
To accomplish its first goal, the Conceptual Helper follows a novel teaching
strategy that concentrates on teaching students the links that connect the domain’s
concepts of interest rather than the concepts in themselves. This strategy is based on
the cognitive science theory which describes the knowledge base of experts as well
structured and highly connected (e.g., Chi & Koeske 1983). Several studies (e.g. Van
Heuvelen, 1991) suggest that the knowledge of students when they begin an
introductory physics course typically consists of a small number of unstructured,
disconnected facts and concepts, and they leave the courses with more facts and
concepts but their knowledge is equally disconnected and unstructured. Hence the
teaching strategy tries to build the students’ knowledge bases akin to that of experts.
The links between the concepts of any domain can be of various different types.
The word “links” has been traditionally used in Semantic Networks to describe twoplace predicates such as “is-a” or “part-of”. However, we use the word “links” to
describe rich qualitative rules that integrate pieces of knowledge. The kinds of links
that the Conceptual Helper focuses on are those which can be inferred from the
principles or from the definitions of the concepts of the domain. For example, one of
the target links is “the direction of the net force applied to an object is the same as the
direction of the object’s acceleration.” This connection between the concept of
acceleration and the concept of net force can be inferred from Newton’s second law.
Likewise, the link “if the acceleration of an object is zero, then the object’s velocity is
constant” can be inferred from the definition of the concept of acceleration. These
types of links are not evident to the students, in the sense that, even if students can
repeat without hesitation the definition of acceleration and Newton’s second law, by
and large, they are generally not able to assert the links between concepts that follow
from those definitions [Reif, 1995]. However, these types of links are essential for
reasoning qualitatively about the motion of objects and for solving the qualitative
problems. Additionally, the tutor helps students understand some concepts in
themselves, such as the concepts of normal force and friction force.
The second goal of the Conceptual Helper is to help students replace their
misconceptions with scientifically correct knowledge. The word “misconception” is
taken to mean the knowledge that the students bring to the class, having acquired it
through interaction with the world’s physical phenomena, but that does not agree with
scientific knowledge. For example, in Figure 1 the problem is designed to uncover
the misconception that weight influences the horizontal motion of an object. The
Conceptual Helper handles misconceptions by presenting students with the basic line
of reasoning underlying the correct interpretation of the phenomena that are the base
of the misconception. This is as opposed to using discovery environments or
computer-simulated experiments, which are the two common ways in which teachers
have tried to correct misconceptions. We believe that it is not setting up the
(simulated) equipment, making the runs, recording the data, and inducing a pattern
that convinces a student of a certain piece of knowledge, but rather the line of

The Conceptual Helper: An ITS for Teaching Fundamental Physics Concepts

567

argument itself. Knowing the correct line of reasoning enables the student to selfexplain the phenomenon. An example can be seen in Figure 6.

4 The Libraries of Lessons and Dialogues to Convey Knowledge to
the Student
According to the teaching strategy that the Conceptual Helper follows, its main goal
is to make sure the student both learn the links that connect the concepts of interest of
the domain and abandon common misconceptions. To accomplish this task, it uses
two different kinds of interactions with the student, namely dialogues and minilessons. Both of these were intended to emulate human tutors as closely as possible,
to incorporate some pedagogical techniques that have proven to be effective and to
present the knowledge in a way that is less cognitive demanding. To clarify their use
the examples given will refer to the problem shown in Figure 2.
Mini-lessons and dialogues were automatically generated from templates where
objects, motion directions and graphics were instantiated as needed.
A coin is tossed upward. Considering that there is no effect of air resistance, draw a
motion diagram1 for the coin from the time it is released until it reaches its apex.
Fig. 2. Example of a qualitative problem requiring an explicit solution.

4.1 The Dialogues
When the student makes a mistake while solving a problem, which is judged as not
coming from applying a misconception (see definition of misconception in section 3),
the tutor will try to correct this mistake by helping the student build the link between a
known concept and the concept corresponding to the correct action. One of the ways
in which the tutor does this is by engaging the student in a short dialogue, emulating
the behavior of human tutors when they use hints and leading questions [Fox, 1993].
The dialogue consists of two statements. Each statement is incomplete, but has a
menu from which the student can select a completion. The first statement in the
dialogue is aimed at eliciting from the student the value of the antecedent of the target
link. Conversely, the second statement is aimed at getting the student to provide the
consequent of the link. An example of a dialogue can be seen in Figure 3. In the
example, the link of interest is: “if (in a linear motion) the velocity of an object is
decreasing then the object’s velocity and its acceleration have opposite directions.”
This is how the dialogues are used. Suppose that a student is solving the problem
shown in Figure 2 and that he draws the velocities correctly as pointing up and
decreasing, but then he draws the acceleration with an upward direction. Then the
tutor turns the input red and verifies that it does not correspond to a misconception. If
it does not, the Conceptual Helper finds in the solution graph of the current problem
the rule that the student should apply to correct its error and which is on the most

1

A motion diagram consists of describing the position, velocity and acceleration of the system
of interest at regular time intervals. The description is achieved through drawing the
corresponding vectors.

568

Patricia L. Albacete and Kurt VanLehn

likely solution path2 that the student is following. If the probability of the student
knowing the piece of knowledge is lower than 0.8, as revealed by the student model,
the tutor presents to the student the first statement of the dialogue.
The velocity of the coin is: menu choice:

increasing
decreasing
constant
I don’t know

Therefore, the coin’s acceleration and its velocity have:

opposite directions
equal directions
none of the above

Fig. 3. Example of a dialogue used by the Conceptual Helper.

If the student completes the first statement incorrectly the Tutor will try to teach an
alternative rule, if such a rule exists. To this end it will present a dialogue
corresponding to this second rule. On the other hand, if the student completes the first
statement correctly, the second statement comes up. If the student gives the correct
completion, he is informed of it and nothing else happens. On the other hand, if the
student answers it incorrectly the Conceptual Helper evaluates what further
intervention it will pursue as it is explained in the next section.
4.2 The Mini-Lessons
The main way in which the tutor explains knowledge to the student is through the use
of short lessons, which are called mini-lessons. There are four kinds of mini-lessons.
Mini-lessons that explain a particular link or a concept. Suppose that a student
solving the problem of Figure 2 draws the velocity correctly at two time points but
then he draws the acceleration incorrectly pointing upward. Moreover, suppose the
Conceptual Helper has already tried to help the student by hinting with the dialogue
shown in Figure 3 but that the student has completed the second statement incorrectly.
At this point, the tutor will try a more directive intervention, like a human tutor would
do [McArthur et al., 1990] by presenting the student with an explanation of the
knowledge of interest through the mini-lesson shown in Figure 4.
Most mini-lessons consist of a short piece of text and a graphic or animation,
which illustrates what the text describes. The textual part of the mini-lesson begins
with a general definition of the concept or principle that constitutes the theoretical
basis for the existence of the link of interest. For example, in Figure 4, the definition
of acceleration is given as the theoretical basis for explaining the relationship between
acceleration and velocity. When appropriate, this abstract definition is followed by an
anthropomorphic interpretation. In this example, for instance, the definition of
acceleration is brought to life by making the acceleration an agent in changing the
velocity. Next, a general definition of the link of interest is presented as well as its
application to the particulars of the problem. In the example, this comprises the
2

The system has a program capable of estimating the most likely solution path that the student
is following.

The Conceptual Helper: An ITS for Teaching Fundamental Physics Concepts

569

second paragraph. Additionally, when there is an animation or graphic, a brief
explanation is included to highlight the knowledge of interest. Furthermore, the text
has some words or phrases, such as the question at the bottom of Figure 4, that are
hyperlinks. By clicking on them, the student can find information or pursue a further
dialogue with the tutor regarding the underlined topic.
The graphics and animations of the mini-lessons were designed with two main
ideas in mind: a) people tend to reason with objects belonging to the material
ontology [Chi, 1992], and b) people tend to provide anthropomorphic explanations of
how the physical world works [diSessa, 1993; Rochelle, 1992]. Additionally, a
microscopic view of matter was used when appropriate [Murray et al., 1990].
Acceleration is a vector defined as the rate of change in velocity with time. You can
think of the acceleration vector as what changes the velocity vector. Acceleration can
change the velocity's magnitude, its direction, or both.
In this case, the magnitude of the velocity of the coin, i.e., its speed, is decreasing.
The acceleration is making it shorter. For that to happen in a linear motion, the
velocity vector and the acceleration, have to have opposite directions.
In the animation below, you can see the acceleration vector, with an imaginary
arm, making the velocity vector shorter. Notice that the velocity and the acceleration
have opposite directions.

Why is the speed of the coin decreasing?
Fig. 4. Example of a mini-lesson explaining one relationship between the concepts of velocity
and acceleration for the problem shown in Fig. 2.

To implement the first of these ideas, it was decided that the tutor would use
vectors as much as possible when presenting graphical explanations of the abstract
concepts whose connecting links it is trying to teach. Vectors are the correct
scientific representation of the concepts and their representation as arrows is concrete,
amenable to direct manipulation and has all of the characteristics of material objects.
Hence this may facilitate the understanding of the knowledge.
In addition, since people spontaneously use anthropomorphism to explain how
objects behave we believe that incorporating this technique into the explanations that
the Conceptual Helper gives would make learning the target knowledge less cognitive
demanding. Furthermore it may facilitate the production of explanations that the

570

Patricia L. Albacete and Kurt VanLehn

students generate to themselves, which has been argued [Chi, 1996] to be an effective
means for learning.
Reminder lessons. The reminder lessons are aimed at refreshing the student’s
memory of a piece of knowledge. They consist of a textual explanation of the target
knowledge which is presented at a more general level. Also, there is no detailed
tailoring to the particulars of the problem as there is with explanatory mini-lessons.
An example of this kind of lesson can be found in Figure 5 (this is the reminder minilesson corresponding to that presented in Figure 4).
Acceleration is a vector defined as the rate of change in velocity with time. You can
think of the acceleration vector as what changes the velocity vector. Acceleration can
change the velocity's magnitude, its direction, or both.
In this case, the magnitude of the velocity is decreasing. For that to happen in a linear
motion, the velocity vector and the acceleration, have to have opposite directions.
Fig. 5. Example of a reminder mini-lesson.

Reminder lessons are used by the tutor when it has already presented the
corresponding detailed mini-lesson, but the student has made a mistake which
involves applying the same piece of knowledge. The belief is that, if the student has
already received a detailed explanation of the knowledge of interest while solving a
problem, but he fails to use it properly in a further step, then a reminder of the
knowledge should suffice to get the student to correct his mistakes. This models
human tutors as they progressively fade away the support they give to students
[Collins et al., 1989].
Mini-lessons that summarize knowledge. When students finish solving a problem,
they click on a done button. When this happens, the Conceptual Helper presents to
the student a mini-lesson that summarizes the most important pieces of knowledge
that were used to solve the current problem. The summary mini-lessons were
designed to include only a few main ideas based on studies which suggest that very
detailed explanations after completion of problem solving can be confusing for the
students [Katz & Lesgold, 1994].
There are three main reasons why the tutor uses summary mini-lessons. The first
is that, because the student in not engaged in problem solving anymore, he may be
more receptive to thinking about specific pieces of conceptual knowledge than while
trying to finish the problem [Katz et al., 1996]. The second is that the student could
have solved parts of the problem by just guessing. Hence going through the pieces of
knowledge that he should know might help rectify the guessing. The third reason for
using summary mini-lessons is that, if the student made several mistakes while
solving the problem, he may not recall all the corrections made by the tutor and which
were really relevant and worth remembering (students may make mistakes that are
related to the use of the interface and not to their knowledge of physics). The
summary of knowledge presented in the mini-lesson may help in this respect.

The Conceptual Helper: An ITS for Teaching Fundamental Physics Concepts

571

Mini-lessons that address misconceptions. Misconceptions are addressed both
during regular problem solving and through multiple choice questions. The
explanations presented in this kind of mini-lesson are aimed at replacing the student’s
misconception. They follow the philosophy presented in the teaching strategy
(section 3). They consist of a line of reasoning that is based on the scientific
knowledge that the student should follow to self-explain the phenomenon of interest.
An example of this kind of lesson can be seen in Figure 6. It is the mini-lesson a
student would receive if, for example, he clicked on answer b) of the problem
presented in Figure 1.
In the case of multiple-choice questions, the tutor will present a mini-lesson
regardless of whether the student’s answer choice is correct or not. The only
difference is in the introductory sentence which states the correctness of the answer.
Your answer is incorrect. Here is why.
We will begin by analyzing the vertical motion of both balls. The only force acting on
each ball, on the vertical direction, is its weight. If you apply Newton's second law F=ma,
in the vertical direction, you get w1=m1g for the first ball and w2=m2g for the second ball.
Therefore, the acceleration of both balls, in the vertical direction, is g, even if the weight
of one ball is twice the weight of the other ball.
You may recall that acceleration is what changes velocity. In this case, the
acceleration will make the vertical velocity of the balls increase. And since both balls
have the same acceleration, their velocities will vary at the same rate. This means that at
any instant, on their trip down, they will have the same vertical velocity. Hence, both
balls will cover the distance in the same amount of time.
Now, let’s analyze the horizontal motion of the balls. The only force acting on the
balls is their weight, which is straight down. Hence, if we apply Newton’s second law in
the horizontal direction for each ball we find that the acceleration is zero because the total
force in that direction is zero. Additionally, the problem states that both balls have the
same horizontal velocity. And since the acceleration is zero, the velocity of both balls is
constant. Additionally, we know from the analysis of the vertical motion of the balls that
it takes both balls the same amount of time to get to the ground. Hence if both balls fly
with the same horizontal velocity for the same amount of time they will travel the same
horizontal distance. In other words, both balls will hit the ground at approximately the
same horizontal distance from the base of the table.
Fig. 6. Mini-lesson addressing the misconception “Influence of weight on horizontal motion”

5 Preliminary Analysis of the Evaluation of the System
An evaluation of the system was conducted to test its effectiveness. To this end 42
students taking Introductory Mechanics classes were recruited and randomly divided
into a Control group and an Experimental group. Both groups took a paper-andpencil pre-test that consisted of 29 qualitative problems, 15 of which belonged to the
Force Concept Inventory test3. Then they solved some problems with the Andes
system receiving appropriate feedback according to the group they belonged to. The
students in the Control Group had their input turned green or red depending on the
3

The Force Concept Inventory test has become the standard across the US to measure
conceptual understanding of elementary mechanics [Hake, 1998].

572

Patricia L. Albacete and Kurt VanLehn

correctness of the entry. Then, in the case of an incorrect action, the students could
ask for help making a choice from a help menu. The kind of help they received
consisted of simple hints such as “the direction of the vector is incorrect” or just
telling them the answer. On the other hand the students in the experimental group
received the green/red feedback depending on whether their action was correct but
when the input was incorrect the Conceptual Helper intervened as explained above.
After the students finished solving the problems with the system they took a post-test
which was the same as the pre-test with the exception of a few changes in the cover
stories of some problems.
A preliminary analysis of the data found that the mean gain score (the subject’s
post-test score minus his or her pre-test score) of the control group was 4.12 with a
standard deviation of 5.33, while the mean of the experimental group was 7.47 with a
standard deviation of 5.03 -a statistically significant difference (t(40)=2.094,
p=0.043). Before this calculation was made, a comparison of the pre-test scores was
performed and no statistically significant difference was found between the two
groups (t(40)=0.965, p=0.34). The statistically significant difference found between
the means of the gain scores suggests that the intervention of the Conceptual Helper
had a positive impact on the students’ understanding of the concepts as well as on
their ability to abandon common misconceptions.
Additionally the effect size was calculated. Effect size is a standard way to
compare the results of one pedagogical experiment to another. One way to calculate
effect size, used in Bloom (1984) and many other studies, is to subtract the mean of
the gain scores of the control group from the mean of the gain scores of the
experimental group, and divide by the standard deviation of the gain scores of the
control condition. That calculation yields (7.47-4.12/5.33 = 0.63). The effect size of
0.63 was comparable with peer and cross-age remedial tutoring (effect size of 0.4
according to Cohen et al., 1982). Some better results have been obtained with
interventions that lasted a whole semester or academic year. For example, Bloom
(1984) found an effect size of 2.0 for adult tutoring in replacement of classroom
instruction and Anderson et al. (1995) reported an effect size of 1.0 for their tutoring
systems. However, the results reported here were achieved with only two hours of
instruction.

6 Conclusions
An intelligent coach was described which presents a novel cognitive-based approach
to teaching conceptual physics. Moreover, the manner in which the desired
knowledge is presented embeds many successful human tutoring techniques, such as
providing hints and supporting post-problem reflection, as well as effective
pedagogical techniques, like the use of a microscopic view of matter, and techniques
that seem to be less cognitively demanding such as the use of anthropomorphism.
A preliminary analysis of the evaluation of the system is encouraging since it
seems to reveal that the proposed methodology can be effective in accomplishing the
task it was designed to perform.

The Conceptual Helper: An ITS for Teaching Fundamental Physics Concepts

573

References
Albacete, P.L. (1999). An Intelligent Tutoring System for teaching fundamental physics
concepts. Unpublished doctoral dissertation. Intelligent Systems Program, University of
Pittsburgh. Pittsburgh, Pennsylvania.
Anderson, J.R., Corbett, A.T., Koedinger, K.R., & Pelletier, R. (1995). Cognitive tutors:
Lessons learned. The Journal of the Learning sciences, 4(2) 167-207.
Bloom, B.S. (1984). The 2 sigma problem: The search for methods of group instruction as
effective as one-to-one tutoring. Educational Researcher, 13, 4-16.
Chi, M.T.H. (1992). Conceptual change within and across ontological categories. In Gier, R.
(Ed.) Cognitive models of science: Minnesota studies in the philosophy of science.
University of Minnesota Press, Minneapolis, MN.
Chi, M.T.H. (1996). Constructing Self-Explanations and Scaffolded Explanations in Tutoring.
Applied Cognitive Psychology, Vol. 10, S33-S49.
Chi, M.T.H. & Koeske, R.D. (1983). Network Representation of a Child’s Dinosaur
Knowledge. Developmental Psychology 19(1), 29-39.
Cohen, P.A., Kulik, J.A., & Kulik, C.C. (1982). Educational outcomes of tutoring: A metaanalysis of findings. American Educational Research Journal, 19, 237-248.
Collins, A., Brown, J.S., & Newman, S.E. (1989). Cognitive apprenticeship: Teaching the
crafts of reading, writing, and mathematics. In L. B. Resnick (Ed.), Knowing, learning, and
instruction: Essays in honor of Robert Glaser (pp. 453-494). Hillsdale, NJ: Lawrence
Erlbaum Associates, Inc.
diSessa, A.A. (1993). Toward an Epistemology of Physics. Cognition and Instruction, 10(2&3).
Fox, B.A. (1993). The Human Tutorial Dialogue Project: Issues in the Design of Instructional
Systems. Lawrence Eribaum Associates, Hillsdale, NJ.
Gertner, A.S., Conati, C., & VanLehn K. (1998). Procedural help in Andes: Generating hints
using a Bayesian network student model. Proceedings of the 15th National Conference on
Artificial Intelligence. Madison, Wisconsin.
Hake, R.R. (1998). Interactive-engagement versus traditional methods: A six-thousand-student
survey of mechanics test data for introductory physics courses. American Journal of Physics,
66(64).
Halloun, I.A., & Hestenes, D. (1985). The initial knowledge state of college physics students.
American journal of Physics 53 (11) 1043-1055.
Katz, S., Lesgold, A., Eggan, G., Greenberg, L. (1996). Towards the Design of a More
Effective Advisors for Learning by Doing Systems. Proceedings of the Third International
Conference on Intelligent Tutoring Systems, ITS'96. Montreal, Canada. Springer-Verlag.
McArthur, D., Stasz, C., & Zmuidzinas, M. (1990). Tutoring techniques in algebra. Cognition
and Instruction, 7(3) 197-244.
Murray, T., Schultz, K., Brown, D., & Clement, J. (1990). An Analogy-Based Computer Tutor
for Remediating Physics Misconception. Interactive Learning Environments 1(2), 79-101.
Reif, F. (1995). Understanding and Teaching Important Scientific Thought Processes.
American Journal of Physics, January 1995.
Roschelle, J. (1992). Learning by Collaborating: Convergent Conceptual Change. The Journal
of the Learning Sciences, 2(3), 235-276.
Van Heuvelen, A. (1991). Learning to think like a physicist: A review of research-based
instructional strategies. American Journal of Physics, 59(10), 891-896.
VanLehn, K. (1996). Conceptual and Meta learning during Coached Problem Solving. In
Proceedings of the Third International conference on Intelligent Tutoring Systems.
Montreal, Canada. Springer-Verlag.

Acknowledgements
This research was supported by a grant from the Cognitive Science Division of ONR,
N00014-96-1-0260.

Machine Learning 2: 39-74, 1987
© 1987 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands

A Version Space Approach to Learning
Context-free Grammars
KURT VANLEHN
(VANLEHN@A.PSY.CMU.EDU)
WILLIAM BALL
(BALL@A.PSY.CMU.EDU)
Department of Psychology, Carnegie-Mellon University, Pittsburgh, PA 15218 U.S.A.
(Received: June 17, 1986)
(Revised: January 21, 1987)
Keywords: Induction, grammatical inference, version space, context-free grammars,
learning from examples.
Abstract. In principle, the version space approach can be applied to any induction
problem. However, in some cases the representation language for generalizations is so
powerful that (1) some of the update functions for the version space are not effectively
computable, and (2) the version space contains infinitely many generalizations. The
class of context-free grammars is a simple representation that exhibits these problems.
This paper presents an algorithm that solves both problems for this domain. Given a
sequence of strings, the algorithm incrementally constructs a data structure that has
nearly all the beneficial properties of a version space. The algorithm is fast enough to
solve small induction problems completely, and it serves as a framework for biases that
permit the solution of larger problems heuristically. The same basic approach may be
applied to representations that include context-free grammars as special cases, such as
And-Or graphs, production systems, and Horn clauses.

1. Introduction
The problem addressed here arose in the course of studying how people
learn arithmetic procedures from examples (VanLehn, 1983a; VanLehn,
1983b). Our data allowed us to infer approximations of the procedures
the subjects had learned and the examples they received during training.
Thus, the inputs and outputs to the learning process were known, and the
problem was to describe the learning process in detail. However, because
the subjects' learning occurred intermittently over several years, we were
not immediately interested in developing a detailed cognitive simulation of
their learning processes. Even if such a simulation could be constructed,
it might be so complicated that it would not shed much light on the basic
principles of learning in this task domain. Therefore, our initial objective
was to find principles that could act as a partial specification of the learning process. The principles we sought took the form of a representation

40

K. VANLEHN AND W. BALL

language for procedures, together with inductive biases that would postdiet the procedures learned by our subjects. More precisely, our problem
was:
• Given:
o a training sequence, consisting of examples of a procedure being
executed, and
o a set of observed procedures, represented in some informal language (i.e., English),
• Find:
o a representation language for procedures, and
o a set of inductive biases, expressed as predicates on expressions in
the representation language,
such that the set of all procedures that are consistent with the examples
and preferred by the biases
o includes the observed procedures, and
o excludes implausible procedures (e.g., ones that never halt).
This method for studying the structure of mental representations and processes has much to recommend it (VanLehn, Brown & Greeno, 1984; Fodor,
1975), but here we wish to discuss only the technical issues involved in implementing it. The central technical problem is calculating the sets mentioned above. The calculation must be done repeatedly, once for each
combination of representation language, biases and training sequence. Although the calculations could be done by hand, it is probably easier to
program a computer to perform them. Rather than build one program
that could handle all combinations, or one program for each combination,
we chose a hybrid approach.
The approach was to build a different program for each representation
language. The programs are induction programs, in that they take a sequence of training examples and calculate expressions in the representation
language that are generalizations of those examples. The inducers are unbiased, in that they produce all expressions in the language consistent with
their inputs. An unbiased inducer provides a framework on which we can
install explicit biases in an attempt to fit its output to the data. The advantage of this approach is that tuning an unbiased inducer is much easier
than building a different biased inducer for each set of biases. The main
technical problem of implementing this approach is devising an unbiased
inducer for each of the hypothesized representation languages.
It is very important to understand that these inducers are merely tools
for generating certain sets that we are interested in studying. They are not
meant to be models of human learning processes.

A VERSION SPACE FOR GRAMMARS

41

This approach works fine for some representation languages, but not
for others. Some procedure representation languages (e.g., those used by
Anderson, 1983, and VanLehn, 1983c) are based on recursive goal hierarchies that are isomorphic to context-free grammars.1 For several reasons, it is impossible to construct an inducer that produces the set of all
context-free grammars consistent with a given training set. First, such
a set would be infinite. Second, the standard technique for representing such a set, Mitchell's (1982) version space technique, seems inapplicable because the crucial 'more-specific-than' relationship is undecidable for
context-free grammars.2 The proofs for these points will be presented later.
Although we could have abandoned exploration of procedure representation languages with recursive goal hierarchies, we chose instead to attack
the subproblem of finding a suitable induction algorithm for context-free
grammars.
The impossibility of an unbiased inducer means that a biased one must
be employed as the framework on which hypothesized biases are installed
for testing their fit to the data. Because we will not be able to test the
fit with the built-in bias removed, the built-in bias must be extremely
plausible a priori. Moreover, there must be an algorithm for calculating
the set of grammars consistent with it, and that set must be finite.
We found such a bias and called it reducedness. A grammar is reduced if
removing any of its rules makes it inconsistent with the training examples.
Later, we will argue for the plausibility of reducedness and, more importantly, we will prove that there are only finitely many reduced grammars
consistent with any given training sequence. This proof is one of the main
results presented in this paper.
The proof contains an enumerative algorithm for generating the set of
reduced grammars consistent with a training sequence, but the algorithm
is far too slow to be used. In order to experiment with biases, we needed an
algorithm that could take a training sequence of perhaps a dozen examples,
and produce a set of reduced grammars in a day or less time.
The obvious candidate for a faster algorithm is Mitchell's (1982) version space strategy. Applying the strategy seems to involve conquering
the undecidability of the 'more-specific-than' relationship for grammars.
However, we discovered that it was possible to substitute a decidable relationship for 'more-specific-than' and thereby achieve an algorithm that
had almost all the beneficial properties of the version space technique. In
particular, it calculates a finite, partially ordered set of grammars that can
be represented compactly by the maximal and minimal grammars in the
1
A context-free grammar is a set of rewrite rules, similar to a simple production
system. The next section gives precise definitions of the relevant terms from formal
language theory.
2
The version space technique is explained in the next section.

42

K. VANLEHN AND W. BALL

order. Unfortunately, the set is not exactly the set of reduced grammars,
but it does properly contain the set of reduced grammars. We call it the
derivational version space.
The derivational version space satisfies our original criterion: it is a set of
consistent grammars which is arguably a superset of the set of grammars
qua procedures that people learn. Moreover, the algorithm for calculating it is fast enough that small training sequences can be processed in a
few hours, and the structure of the algorithm provides several places for
installing interesting biases. The derivational version space is the second
result to be presented in the paper.
The main interest for machine learning researchers lies in the generality
of the techniques we used. The reducedness bias can be applied directly
to many representation languages. For instance, an expression in disjunctive normal form (i.e., a disjunction of conjunctions) is reduced if deleting
any of its disjuncts makes the expression inconsistent with the training
examples. The finiteness result for reduced grammars suggests that sets of
reduced expressions in other representations are also finite and effectively
computable.3 Moreover, the technique of substituting an easily computed
relation for the 'more-specific-than' relation suggests that such sets of reduced expressions can be efficiently computed using the derivational version
space strategy.
Indeed, the fact that substituting another relation for 'more-specificthan' leads to a useful extension of the version space strategy suggests
looking for other relationships that provide the benefits of version spaces
without the costs. This idea is independent of the idea of reducedness.
Both ideas may be useful outside the context of grammar induction.
There are four main sections to the paper. The first introduces the
relevant terminology on grammars, grammar induction and version spaces.
The second discusses reducedness and the finiteness of the set of grammars
consistent with a set of examples. The third discusses the derivational
version space, while the fourth presents the induction algorithm for this
structure and demonstrates the results of incorporating certain biases. The
concluding section speculates on the larger significance of this work.

3It might be argued that although the bias can be applied to other representation
languages, one might not want to. However, reducedness is already obeyed by all constructive induction programs that we are familiar with, including the systems of Quinlan (1986), Michalski (1983), and Vere (1975). (Reducedness is not usually obeyed by
enumeration-based induction algorithms, such as those found in the literature on language identification in the limit (Osherson, Stob & Weinstein, 1985).) Apparently, the
designers of constructive inducers believe that it is natural for an induced generalization to include only parts (e.g., rules, disjuncts) that have some support in the data.
Reducedness is a precise statement of this belief.

A VERSION SPACE FOR GRAMMARS

43

2. Terminology
2.1 Introduction to grammars and grammar induction
A grammar is a finite set of rewrite rules. A rule is written a — B, where
a and B are strings of symbols. Grammars are used to generate strings by
repeatedly rewriting an initial string into longer and longer strings. In this
article, the initial string is always 'S'. For instance, the following grammar
S-b
S- aS
generates the string 'aab' via two applications of the second rule and one
application of the first rule:
S — aS — aaS — aab
Such a sequence of rule applications is called a derivation. There are two
kinds of symbols in grammars. Terminals are symbols that may appear in
the final string of a derivation, whereas nonterminals are not allowed to
appear in final strings. In the above grammar, a and b are terminals, and
S is a nonterminal.
The grammar induction problem is to infer a grammar that will generate
a given set of strings.4 The set of strings given to the learner is called the
presentation. It always contains strings that the induced grammar should
generate (called positive strings) and it may or may not contain strings
that the induced grammar should not generate (called negative strings).
For instance, given the presentation
- a, + ab, + aab, - ba

the grammar given earlier (call it Grammar 1) could be induced because
it generates the two positive strings, 'ab' and 'aab,' and it cannot generate
the two negative strings, 'a' and 'ba'. A grammar is said to be consistent
with a presentation if it generates all the positive strings and none of the
negative strings.5
There are usually many grammars consistent with a given presentation.
For instance, here are two more grammars consistent with the presentation
mentioned above:
4

Grammar induction is studied in at least three fields - philosophy, linguistics, and
artificial intelligence. For reviews from the viewpoints of each, see, respectively, Osherson, Stob and Weinstein (1985), Pinker (1979), and Langley and Carbonell (1986). In
addition, Cohen and Feigenbaum (1983) give an excellent overview.
5
Some authors use 'deductively adequate' (Horning, 1969) or 'consistent and complete' (Michalski, 1983) for the same concept. We use the term 'consistent' in order to
bring the terminology of this paper into line with the terminology of Mitchell's (1982)
work on version spaces.

K. VANLEHN AND W. BALL

44

Grammar 2
S- A
A - b
A — aA

Grammar 3
S - Sb
Sb - ab
S — aa

Grammar 2 is equivalent to grammar 1 in that it generates exactly the
same set of strings: {b, ab, aab, aaab, aaaab, ... }. The set of all strings
generated by a grammar is called the language generated by the grammar.
The language in this case is an infinite set of strings. However, languages
can be finite. The language generated by Grammar 3 is the finite set {ab,
aa, aab}.
Grammar induction is considerably simpler if restrictions are placed on
the class of grammars to be induced. Classes of grammars are often defined
by specifying a format for the grammars that are members of the class. For
instance, grammars 1 and 2 obey the format restriction that the rules have
exactly one nonterminal as the left side. Grammars having this format are
called context-free grammars. Grammar 3 is not a context-free grammar.

2.2 Version spaces
One of the most widely studied forms of machine learning is learning
from examples, or induction, as it is more concisely called. The following
is a standard way to define an induction problem:6
• Given:
o A representation language for generalizations;
o A predicate of two arguments, a generalization and an instance,
that is true if the generalization matches the instance;
o A set of 'positive' instances (which should be matched by the
induced generalizations) and a set of 'negative' instances (which
should not be matched);
o A set of biases that indicate a preference order for generalizations;
• Find: One or more generalizations that are
o consistent with the instances; and
o preferred by the biases;
where 'consistent' means that the generalization matches all the positive
instances and none of the negative instances.
This formulation is deliberately vague in order to encompass many specific induction problems. For instance, the instances may be ordered. There
may be no negative instances. There may be no biases, or biases that rank
6 Throughout, we follow Mitchell's (1982) choice of terminology, with two exceptions.
First, we use Generalizes(x,y) instead of more-specific-than(y,x). Second, if x Generalizes
y, then we visualize x as above y; Mitchell (1982) would say that x is below y.

A VERSION SPACE FOR GRAMMARS

45

generalizations on a numerical scale, or biases that partially order the set
of generalizations. Much work in machine learning is encompassed by this
definition.
Mitchell defines a version space to be the set of all generalizations consistent with a given set of instances. This is just a set, with no other
structure and no associated algorithm. However, Mitchell also defines the
version space strategy to be a particular induction technique, based on a
compact way of representing the version space. Although popular usage
of the term 'version space' has drifted, this paper will stick to the original
definitions.
The central idea of the version space strategy is that the space of generalizations defined by the representation language can be partially ordered
by generality. One can define the relation Generalizes(x,y) in terms of the
matching predicate:
Definition 1 Generalizes(x,y) is true if and only if the set of instances matched
by x is a superset of the set of instances matched by y.
Note that the Generalizes relationship is defined in terms of the denotations of expressions in the representation language, and not the expressions
themselves. This will become important later, when it is shown that the
Generalizes relation is undecidable for context-free grammars.
It is simple to show that the Generalizes relation partially orders the
space of generalizations. Thus, no matter what the specific induction problem may be, one can always imagine its answer as lying somewhere in a
vast tangled hierarchy which rises from very specific generalizations that
cover only a few instances, all the way up to generalizations that cover
many instances.
Given a presentation, the version space for that presentation will also be
partially ordered by the Generalizes relation. Given some mild restrictions
(e.g., that there are no infinite ascending or descending chains in the partial
order), the version space has a subset of maximal elements and a subset of
minimal elements. The maximal set is called G, because it contains the set
of maximally general generalizations. The minimal set is called S, because
it contains the maximally specific generalizations. The pair [S,G] can be
used to represent the version space. Mitchell proved that
Given a presentation, x is contained in the version space for that presentation if and only if there is some g in G such that g Generalizes x
and there is some s in S such that x Generalizes s.
Three algorithms are usually discussed in connection with the [S,G] representation of version spaces:

K. VANLEHN AND W. BALL

46

• Updated(i [S.G]) - [ S ' . G ' l
The Update function takes the current version space boundaries and
an instance that is marked as either positive or negative. It returns
boundaries for the new version space. If the instance makes the version
space empty (i.e., there is no generalization that is consistent with the
presentation, as when the same instance occurs both positively and
negatively), then some marker, such as Lisp's NIL, is returned. The
Update algorithm is the induction algorithm for version space boundaries. Its implementation depends on the representation language.
• DoneP ([S. G]) - true or false
Unlike many induction algorithms, it is possible to tell when further
instances will have no effect because the version space has stabilized.
DoneP is implemented by a test for set equality, S = G.
• Classifyd, [S.G]) - +, -, or ?
Classify an instance that is not marked positively or negatively, using
the version space boundaries. It returns '+' if the instance would be
matched by all the generalizations in the version space. It returns ' —'
if it would be matched by no generalizations. It returns '?' otherwise.
Classify is useful for experimental design. If instances are marked
by some expensive-to-use teacher (e.g., a proton collider), then only
instances that receive '?' from Classify are worth submitting to the
teacher.
Applying the version space strategy to a representation language means
that one must devise only an appropriate Update function, because the
Classify and DoneP functions come for free with the strategy. This is
sometimes cited as the chief advantage of the version space approach. In
our work on skill acquisition, we make only minor use of them. Our main
reason for preferring the version space strategy over other induction strategies is that it computes exactly the set we need, the version space, and
represents it compactly.

3. Reduced version spaces
The first problem encountered in applying the version space strategy
to grammar induction is that the version space will be always be infinite.
This does not necessarily imply that the version space boundaries will be
infinite; a finite S and G can represent an infinite version space. However,
for grammars, the boundaries also turn out to be infinite. To begin, let us
consider a well-known theorem about grammar induction, which is:
Theorem 1 For any class of grammars that includes grammars for all the finite
languages, there are infinitely many grammars in the class that are consistent with
any given finite presentation.

A VERSION SPACE FOR GRAMMARS

47

That is, the version space is infinite for any finite presentation. This theorem has a significance outside the context of version space technology. For
instance, it has been used to justify nativist approaches to language acquisition (Pinker, 1979). This section is written to address both the concerns
of version space technology and the larger significance of this theorem.

3.1 Normal version spaces are infinite
Three ways to prove the theorem will be presented. Simply amending the
statement of the theorem to prevent the use of each of the proof techniques
yields a new theorem, which is one of the results of this article.
All three proofs employ mathematical induction. The initial step in all
the proofs is the same. Because the class of grammars contains grammars
for all finite languages, and the positive strings of the presentation constitute a finite language, we can always construct at least one grammar that is
consistent with the presentation. This grammar initializes the inductions.
The inductive steps for each of the three proofs are, respectively:
1. Let a be any string not in the presentation. Add the rule S — a to
the grammar. The new grammar generates exactly the old grammar's
language plus a as well. Since the old language was consistent with
the presentation, and a does not appear in the presentation, the new
grammar is also consistent with the presentation. Because there are
infinitely many strings a that are not in the presentation, infinitely
many different grammars can be constructed this way. One might
object that the rule S — a may be in the grammar already. However,
because a grammar has finitely many rules, there can be only finitely
many such a, and these can be safely excluded when the a required
by the proof is selected.
2. Let A be a nonterminal in the grammar, and let B be a nonterminal
not in the grammar. Add the rule A — B to the grammar. For
some or all of the rules that have A as the left side, add a copy of
the rule to the grammar with B substituted for A. These additions
create new grammars that generate exactly the same strings as the
original grammar. Because the original grammar is consistent with the
presentation, so are the new grammars. This process can be repeated
indefinitely, generating an infinite number of grammars consistent with
the presentation.
3. Form a new grammar by substituting new nonterminals for every nonterminal in the old grammar (except S). Create a union grammar
whose rules are the union of the old grammar's rules and the new
grammar's rules. The union grammar generates exactly the same language as the original grammar, so it is consistent with the presentation.

48

K. VANLEHN AND W. BALL

The union process can be repeated indefinitely, yielding an infinite set
of grammars consistent with the presentation.
It is hard to imagine why a machine or human would seriously entertain
the grammars constructed above. The grammars of the last two proofs are
particularly worthless as hypotheses, because they are notational variants
of the original grammar. In a moment, we will add restrictions to the class
of grammars that will bar such irrational grammars.
We have mentioned that an infinite version space can, in principle, be
represented by finite boundaries. Unfortunately, this does not work for
grammars. The second two proofs above will generate infinitely many
grammars that generate exactly the same language as the initial grammar.
If the initial grammar is from S, then S can be made infinite; similarly, G
can be made infinite. The G set can also be made infinite by the first proof
above. These comments prove the following theorem:
Theorem 2 If the representation language for generalizations specifies a class
of grammars that includes grammars for all finite languages, then for any finite
presentation, the version space boundaries, S and G, are each infinite.

3.2 Reducedness makes the version space finite
One way to make the version space finite is to place restrictions on the
grammars to be included in it. As some of these restrictions are most
easily stated as restrictions on the form of grammar rules, we will limit our
attention to context-free grammars, although the same general idea works
for some higher order grammars as well (as shown in Appendix 1). The
first restriction blocks the grammars produced by the second proof:
Definition 2 A context-free grammar is simple if (1) No rule has an empty right
side,7 (2) if a rule has just one symbol on its right side, then the symbol is a
terminal, and (3) every nonterminal appears in a derivation of some string.

The class of simple grammars can generate all the context-free languages.
Hopcroft and Ullman (1979) prove this (theorem 4.4) by showing how to
turn an arbitrary context-free grammar into a simple context-free grammar.
For our purposes, the elimination of rules of the form A — B, where both
A and B are nonterminals, blocks the second proof.
Proofs 1 and 3 can be blocked by requiring that all the rules in an induced
grammar be necessary for the derivation of some positive string in the given
presentation. To put this formally:
7This reduces the expressive power of the class somewhat, because a grammar without
such epsilon rules, as they are commonly called, cannot generate the empty string.

A VERSION SPACE FOR GRAMMARS

49

Definition 3 Given a presentation P, a grammar is reduced if it is consistent with
P and if there is no proper subset of its rules that is consistent with P.
Removing rules from a grammar will only decrease the size of the language
generated, not increase it. So removing rules from a grammar will not
make it generate a negative string that it did not generate before. However,
deleting rules may prevent the grammar from generating a positive string,
thus making it inconsistent with the presentation. If any deletion of rules
causes inconsistency, the grammar is reduced.
In proof 1, adding the rules S — a creates a new grammar that is
reducible. Similarly, the union grammar formed by proof 3 is reducible.
This leads to the theorem:
Theorem 3 Given a finite presentation, there are finitely many reduced simple
context-free grammars consistent with that presentation.
The proof of this theorem is presented in Appendix 1. We call the version
space of reduced, simple grammars a reduced version space for grammars.
Any finite partially ordered set has a finite subset of minimal elements
and a finite subset of maximal elements. Define the reduced G and S as
the maximal and minimal sets, respectively, of the reduced version space
under the partial order established by the Generalizes relation. It follows
immediately that:
Theorem 4 Given a finite presentation, the reduced G and S sets are each finite.

3.3 The behavior of reduced version spaces
This subsection describes some of the ways in which a reduced version
space differs from a normal version space.
Normally, a version space can only shrink as instances are presented. As
each instance is presented, generalizations are eliminated from the version
space. With a reduced version space, negative instances cause shrinking,
but positive instances usually expand the reduced version space. To see
why, suppose that at least one of the grammars in the current version space
cannot generate the given positive string. There are usually several ways
to augment the grammar in order to generate the string. For instance, one
could add the rule S — a, where a is the string. Or one could add the rules
S — AB and A — r, where a = rB. Each way of augmenting the current
grammar in order to generate the new string contributes one grammar to
the new version space. So positive strings cause the reduced version space
to expand.
Because presenting a positive string can cause the reduced version space
to expand, the equality of S and G no longer implies that induction is

50

K. VANLEHN AND W. BALL

done. That is, the standard implementation of DoneP does not work. We
conjecture that Gold's (1967) theorems would allow one to show that there
is no way to tell when induction of a reduced version space is finished.
The S set for the reduced version space turns out to be rather boring.
It contains only grammars that generate the positive strings in the presentation. We call such grammars trivially specific because they do nothing
more than record the positive presentation. The version space Update algorithm described below does not bother to maintain the S set, although
it could. Instead, it maintains P+, the set of positive strings seen so far.
In order to illustrate the efficiency gained by this substitution, consider
the Classify function, whose normal definition is: where i is an instance
to be classified, if all s in S match i, then return '+'; else if no g in G
matches i, then return '—'; else return '?'. With P+, the first clause of the
definition becomes: if i is in P+, then return '+'. Because S contains only
the trivially specific grammars, these two tests are equivalent. Clearly, it
is more efficient to use P+ instead of S. Similar efficiencies are gained in
the implementation of the Update algorithm. Nowlan (1987) presents an
alternative solution to this problem with some interesting properties.

3.4 Why choose reducedness for an inductive bias?
The basic idea of reducedness applies to other representation languages.
For instance, suppose the representation is a first order logic whose expressions are in disjunctive normal form (i.e., a generalization is one large
disjunction, with conjunctions inside it). The rules in a grammar are like
disjuncts in a disjunction. Therefore, a disjunctive normal form expression
is reduced if removing a disjunct makes it inconsistent with the presentation. We conjecture that the reduced version space for disjunctive normal
forms will turn out to be finite. There may be a general theorem about reducedness and finiteness that would apply, at the knowledge level perhaps
(Newell, 1982; Dietterich, 1986), to many representation languages.
As mentioned earlier, it seems that reducedness is a 'common sense'
restriction to place on induction. All heuristic concept induction programs
with which we are familiar (e.g., Michalski, 1983; Vere, 1975; Quinlan,
1986) consider only reduced concepts. Reducedness seems to be such a
rational restriction that machine learning researchers adopt it implicitly.
There are other ways to restrict grammars so that there are only finitely
many grammars consistent with a finite presentation. For instance, there
are only finitely many simple, trivially specific grammars consistent with
a finite presentation. However, the restriction to reduced, simple grammars seems just strong enough to block the procedures that produce an
infinitude of grammars without being so strong that interesting grammars
are blocked as well. This makes it an ideal restriction to place on version

A VERSION SPACE FOR GRAMMARS

51

spaces for grammars. The chief advantage of version spaces is that they
contain all the generalizations consistent with the presentation. In order to
retain the basic spirit of version spaces while making their algorithms effective, one should add the weakest restrictions possible. For grammars, the
conjunction of reducedness with simplicity seems to be such a restriction.

4. Applying the version space strategy to reduced version
spaces
The proof of theorem 3 puts bounds on the size of reduced grammars
and their rules. In principle, the reduced version space could be generated
by enumerating all grammars within these bounds. However, such an algorithm would be too slow to be useful. This section discusses a technique
that yields a much faster induction algorithm.
4.1 The undecidability of the Generalizes relationship
The version space strategy is the obvious choice for calculating a reduced
version space, but it cannot, we believe, be applied directly. The problem
is that the version space strategy is based on the Generalizes relationship,
which is defined by a superset relationship between the denotations of two
generalizations. If the generalizations are grammars, then the denotations
are exactly the languages generated by the grammars. Implementing Generalizes(x,y) is equivalent to testing whether the language generated by x
includes the language generated by y. This test is undecidable for contextfree grammars or grammars of higher orders (Hopcroft & Ullman, 1979,
theorem 8.12). This means that there is no algorithm for implementing
Generalizes(x,y) over the context-free grammars.
This result does not prove that the version space strategy is inapplicable,
because only the Update algorithm is required in order to construct a
version space, and there is no proof (yet) that a computable Generalizes
is necessary for a computable Update. On the other hand, we have never
seen a version space Update algorithm that did not call Generalizes as a
subroutine, and we have no idea how to build a Generalizes-free Update
algorithm for grammars. So the undecidability of the Generalizes predicate
is a practical impediment, at the very least.
The Generalizes predicate may be decidable if its arguments are restricted to be reduced grammars for the same presentation. If so, then
it may be possible to use Generalize in an Update algorithm that only
works for the reduced version space, and not the normal version space. We
did not explore this approach. Instead, we sought a way to apply the spirit
of the version space strategy while avoiding the troublesome Generalizes
predicate entirely.

K. VANLEHN AND W. BALL

52

The 'trick' to the version space strategy is using the boundaries of a
partial order to represent a very large, partially ordered set. In principle,
this trick can be based on any partial order, and not necessarily on the
partial order established by Generalizes. This idea led us to seek a partial
order that was 'like' Generalizes and yet was computable. Moreover, the
partial order had to be such that there was an Update algorithm for the
sets of maximal and minimal elements in the order.
It was not difficult to find a computable partial order on grammars, but
we never found an Update algorithm that could maintain sets that were
the boundaries of exactly the reduced version space. Instead, we discovered
one for a superset of the reduced version space. In particular, we found:
• A set, called the derivational version space, that is a superset of the
reduced version space and a subset of the version space.
• A computable predicate, called FastCovers, that is a partial order over
grammars in the derivational version space.
• An Update algorithm for the maximal and minimal elements in FastCovers of the derivational version space.
The remainder of this section presents the derivational version space and
the FastCovers relation. The next section discusses the Update algorithm.

4.2 The derivational version space
In order to define the derivational version space, it will be helpful to
define some ancillary terms first. A derivation tree is a way to indicate the
derivation of a string by a grammar. (These are sometimes called parse
trees.) The derivation tree's leaves are the terminals in the string. The
non-leaf nodes of the tree are labelled by nonterminals. The root node is
always labelled by the root nonterminal, S. An algorithm can 'read off'
the rules used by examining mother-daughter subtrees. If the label of the
mother is A and the labels of the daughters are B, C and D, then the rule
A — B C D has been applied. This reading off process can be used to
convert derivation trees into a grammar.
For simple grammars, derivation trees are constrained to have certain
possible shapes. Simple grammars have no rules of the form A — B, where
both A and B are nonterminals. Therefore, if a node in the derivation tree
has a single daughter, that daughter must be a terminal, because a rule
may have a singleton right side only if that side consists of a terminal.
Let us call trees with this shape simple trees. The definition of simple
grammars makes it impossible for a simple tree to have long, unbranching
chains. Consequently, there are only finitely many unlabelled simple trees
for any given string.

A VERSION SPACE FOR GRAMMARS

53

Figure J. The simple tree product for the presentation 'b', 'ab'.
If a string has more than one element, then there is more than one
unlabelled simple tree. Given a finite sequence of strings, one can calculate
all possible sequences of unlabelled simple trees by taking the Cartesian
product over the sets of unlabelled simple trees for each string. Let us
call this set of simple tree sequences the simple tree product of the strings.
Because there are only finitely many unlabelled simple trees for each string,
the simple tree product will be finite. The definition of the derivational
version space can now be stated:
Definition 4 Given a set of positive strings, the derivational version space is the
set of grammars corresponding to all possible labellings of each tree sequence in
the simple tree product for those strings. Given a set of positive and negative
strings, the derivational version space is the derivational version space for the
positive strings minus those grammars that generate any of the negative strings.

An example may clarify this definition. Suppose the positive strings are
'b' and 'ab.' The construction of the derivational version space begins by
considering the simple tree product for the strings. There is one unlabelled
tree for 'b.' There are four unlabelled trees for 'ab.' So there are four tree
sequences in the Cartesian product of the trees for 'a' and the trees for
'ab.' These four tree sequences constitute the simple tree product, which
is shown in Figure 1. For each of the four tree sequences, the construction
process partitions the nodes in the trees and assigns labels. Figure 2 illustrates how the fourth unlabelled tree sequence is treated. At the top of
the figure, the unlabelled tree sequence is shown with its nodes numbered.
Trees 1 through 5 show all possible partitions of the four nodes and the
labellings of the trees that result. Because the root nodes of the trees must

54

K. VANLEHN AND W. BALL

always receive the same node label, S, they are given the same number,
which forces them to be in the same partition element, and hence receive
the same labelling. Each of the resulting labelled tree sequences is converted to a grammar. These grammars are shown in the third column of
the figure. The derivational version space is the union of these grammars,
which derive from the fourth tree sequence, with the grammars from the
other tree sequences.
The motivation for the derivational version space is the following: If a
grammar is going to parse all the positive strings, then there must be a
sequence of simple derivation trees, one for each string. Such a sequence
must be some possible labelling of some possible sequence of unlabelled
simple trees. The derivational version space is constructed from all such
sequences, i.e., from the simple tree product. Consequently, it must somehow represent all possible grammars, except those grammars which have
rules that were not used during the parsing of those strings. Those grammars are, by definition, the reducible grammars. So the derivational version
space contains all the reduced grammars. These observations lead to the
following theorem, which is proved in Appendix 2:
Theorem 5 The derivational version space for a given presentation contains the
reduced version space for that presentation.

Usually, the reduced version space is a proper subset of the derivational
version space. That is, the derivational version space often contains reducible grammars. In the illustration discussed earlier, where the positive
strings (P+) were 'b' and 'ab,' no reducible grammars would be generated.
However, if P+ was {'b,' 'ab,' 'ab'} or if P+ was {'b,' 'ab,' 'abb'}, then
many reducible grammars would be generated. In general, if a subset of
P+ is sufficient to produce grammars that will generate all of it, then the
derivational version space will contain reducible grammars.
The following theorem shows that the 'version space' component of the
name 'derivational version space' is warranted:
Theorem 6 The derivational version space for a given presentation is contained
in the version space for that presentation.

The proof follows from the observation that the grammars in the derivational version space were constructed so that each positive string has a
derivation, and grammars that generate negative strings are filtered out.
Consequently, the grammars are consistent with the presentation. Lastly,
we note that:
Theorem 7 The derivational version space for a finite presentation is finite.

The proof follows from the earlier observation that the simple tree product
is finite. Because each tree sequence in the product has only finitely many

A VERSION SPACE FOR GRAMMARS

Figure 2. Partitions, labelled trees, and grammars of tree sequence 4.

55

K. VANLEHN AND W. BALL

56

nodes and there are only finitely many ways to partition a finite set into
equivalence classes, there are only finitely many ways to label each of the
finitely many simple tree sequences. Hence, the derivational version space
for P+ is finite. The derivational version space for the whole presentation
is a subset of the one for P+, so it too is finite.
The derivational version space is a finite set that contains all the reduced
simple grammars, and moreover, all its members are consistent with the
presentation. This set suffices for the purposes we outlined in the introduction. It contains all the 'plausible' grammars and it is finite. We show next
that there is a partial order for the set that allows a boundary updating
algorithm to exist.

4.3 The FastCovers predicate
The definition of the partial order is simplified if grammars in the derivational version space are represented in a special way, as a triple. The first
element of the triple is a sequence of unlabelled simple derivation trees,
with the nodes numbered as in Figure 1. The second element of the triple
is a partition of the trees' nodes. The third element is the grammar's rules.
For instance, grammar 4.4 of Figure 2 is represented by the following triple:
Tree sequence:
(1 b), (1 (2 a)(3 b))
Partition:
{1}, (2 3}
Rules:

S - b
S- AA
A-a
A-b

The triple representation allows the FastCovers relation to be defined as
follows:
Definition 5 Given two grammars, X and Y, in triple form, grammar X FastCovers grammar Y if (1) both grammars are labellings of the same tree sequence
(i.e., the first elements of their triples are the same), and (2) the partition (i.e.,
second element of the triple) of Y is a refinement8 of the partition of X.

For instance, a grammar whose partition is ({1},{2},{3}) is FastCovered
by the grammar above. In contrast, a grammar whose partition is ({1
2},{3}) is not FastCovered by the above grammar, nor does it FastCover
that grammar.
8 A partition PY is a refinement of another partition PX if and only if every partition
element of PY is a subset of some partition element of PX.

A VERSION SPACE FOR GRAMMARS

57

FastCovers is named after Covers, a partial order used in early work on
grammar induction (Reynolds, 1968; Horning, 1969; Pao, 1969). Although
we will not pause to define Covers, it can be shown that FastCovers (x,y)
implies Covers(x,y), but Covers(x,y) does not imply FastCovers(x,y). FastCovers is used instead of Covers for ordering the derivational version space
because it is faster to compute than Covers and it makes the Update algorithm simpler.
It is simple to show that the FastCovers relationship is transitive and
reflexive, because these hold for the refinement relationship. Moreover,
because every grammar in the derivational version space has a triple form,
FastCovers applies to every pair of grammars in a derivational version
space. Thus, FastCovers partially orders the derivational version space.
A second property of FastCovers is needed to show that the Update
algorithm is correct:
Theorem 8 For any two grammars, X and Y, in triple form, FastCovers(X,Y)
implies Generalizes(X,Y).

The proof follows from observing that the refinement relationship between
the nonterminals (= partition elements) of X and the nonterminals of Y
establishes a mapping that takes Y's nonterminals onto X's nonterminals.
Every derivation in grammar Y can be turned into a derivation in grammar
X by mapping Y's nonterminals onto X's nonterminals. Thus, every string
that has a derivation in Y must have a derivation in X as well. So the
language generated by Y is a subset of the language generated by X, i.e.,
Generalizes(X,Y).
Given a derivational version space, there is always a finite set of maximal
elements in FastCovers and a finite set of minimal elements. The finiteness
of the boundaries follows from the finiteness of the space itself. We will
call the maximal and minimal sets the derivational G and S, respectively.
From the preceding theorem, it follows immediately that:
Theorem 9 The derivational G (S) includes the subset of the derivational version
space that is maximal (minimal) with respect to the Generalizes relationship.

Given a derivational [S,G], the FastCovers relationship can be used to determine whether a given grammar is contained in the derivational version
space represented by the pair.
Theorem 10 Given a grammar x in triple form and a derivational [G,S], x is in
the derivational version space represented by [G,S] if and only if there is some g
in G such that g FastCovers x, and some s in S such that x FastCovers s.

The proof of the theorem is given in Appendix 3.

58

K. VANLEHN AND W. BALL

5. An Update algorithm for the derivational version space
The preceding section discussed the definition of the structure that we
wish to generate. This section presents the algorithm that generates the
structure, then reports the results of several experiments with it. It begins
by presenting an informal account of what happens as positive and negative
strings are presented.

5.1 The 'sliced bread' metaphor for derivational version spaces
The derivational version space under FastCovers is a set of partition
lattices,9 one lattice for each tree sequence in the simple tree product. One
can visualize the space as a loaf of sliced bread, one slice for each tree
sequence. All the FastCovers relationships run inside slices; none cross
from one slice to another. Each slice is a partition lattice. It has one
maximal partition on top and one minimal partition on the bottom. The
top partition has just one element, and the element has all the nodes in the
tree sequence for that slice. The top partition for tree sequence of Figure 2
is ({1, 2, 3}). The bottom partition in each lattice has a singleton partition
element per node in the tree sequence. The bottom partition for the tree
sequence of Figure 2 is ({1},{2},{3}). All the slices/lattices have unique
top and bottom partitions.
If there are no negative instances in the presentation, then G consists of
the top partition in each lattice. As negative instances are presented, the
maximal set for each lattice may descend. Thus, the G set expands and
the derivation version space shrinks as negative strings are presented. The
S set always consists of the bottom partition in each lattice. Presentation
of negative instances does not effect the S set.
When a new positive instance is presented, the derivational version space
grows horizontally, so to speak (i.e., the loaf gets more slices, and the slices
get larger). If the newly added positive string has more than one member,
there will be more than one unlabelled simple derivation tree for it. Hence,
the simple tree product will increase in size and the set of partition lattices
will increase as well (i.e., the loaf gets more slices). Moreover, each of the
new tree sequences is longer than the corresponding old one, because some
unlabelled derivation tree for the new string has been added to it. The new,
longer tree sequence will have more nodes (again, assuming that the string
has more than one member). With more nodes available for partitioning,
the partition lattices will expand. Thus, the loaf's slices get larger. In
9A lattice is a partial order with the additional property that every pair of nodes in
the lattice has a singleton maximal set and a singleton minimal set. A partition lattice
consists of the set of all partitions of some finite set of objects, ordered by the refinement
relationship.

A VERSION SPACE FOR GRAMMARS

59

short, presenting a positive string increases the number of partition lattices
and the sizes of the partition lattices.
Presenting a new positive string affects the derivational S and G sets
in the following ways. The increase in the number of partitions implies
that the derivational S grows because its members are always the bottom
partitions of the partition lattices. The effect on the derivational G is
more subtle. If there are no negative instances, then G grows because
its members are the top elements of the partition lattices. If there are
negative instances, then G may grow as positive instances are presented,
but we have no proof that it must grow. Although the number of maximal
sets grows, the size of the sets may decrease, leaving the overall G set the
same size, or perhaps even decreasing it.
5.2 The Update algorithm
As mentioned earlier, our algorithm does not bother to maintain the S
set, although it could easily do so. Instead, it maintains P+, the set of
positive strings seen so far. This makes the algorithm more efficient.
The Update algorithm is incremental. It takes an instance and the current [P+, G] pair and returns a revision of the pair that is consistent with
the given instance. If there is no such revision, then the algorithm returns
some error value, such as NIL. The following describes the top level of the
algorithm:
1. If the string is positive and a member of P+, then do nothing and
return the current version space. If the string is not a member of P+,
then add it to P+ and call Update-G+.
2. If the string is negative and a member of P+, then return NIL. If the
string is not a member of P+, then call Update-G—.
The subroutine Update-G— is simpler than Update-G+, so it will be described first.
The task of Update-G— is to modify G so that none of the grammars
will parse the negative string. The easiest way to do this is with a queue,
which is initialized to contain all the grammars in G. The basic cycle is to
pick a grammar off the queue, and see if it parses the negative string. If
it does not, then it can be placed in New-G, the revised version of G. If it
does parse the string, then the algorithm refines the node partition once,
in all possible ways. That is, it takes a partition such as ({1 2 3},{4 5}),
and breaks one of the partition elements in two. In this case, there are four
possible one-step refinements:

60

K. VANLEHN AND W. BALL

1. {1}, {2 3}, (4 5}

2. {1 2}, {3}, {4 5}
3. {1 3}, {2}, {4 5}
4. {1 2 3}, {4}, {5}

Each of these corresponds to a new grammar. These grammars have the
property that they are FastCovered by the original grammar, and there is
no grammar that FastCovers them and not the original grammar. That
is, they are just below the original grammar in the partial order of FastCovers. This process is called splitting in the grammar induction literature
(Horning, 1969).10
All the grammars produced by splitting are placed on the queue. Eventually, the new grammars will be taken off the queue, as described above,
and tested to see if they parse the negative string. Those that fail to parse
the negative string are placed in New-G. Such grammars are maximal in
the FastCovers order in that there is no grammar above them that fails
to parse the negative string. The basic cycle of testing grammars, splitting them, and queuing the resulting new grammars continues until the
queue is exhausted. At this point, New-G contains the maximal set of the
grammars that fail to parse the negative string.
There is one technical detail to mention. It is possible for the same
grammar to be generated via several paths. Before queuing a new grammar,
the queue and New-G are checked to make sure the new grammar is not
FastCovered11 by some existing grammar.
New-G should contain only grammars that are (1) simple, (2) consistent
with the positive presentation, (3) consistent with the negative presentation, and (4) maximal in the FastCovers partial order. The following
comments prove that the Update-G— algorithm satisfies those criteria.
1. The grammars are simple, because the unlabelled derivation trees from
which they are constructed are simple.
2. The grammars are consistent with the positive presentation, because
they are a labelling of a set of derivation trees for those strings. Therefore, they are guaranteed to parse those strings.
10Normally, splitting takes one nonterminal (i.e., a partition element), and divides all
its occurrences in two. Some of the occurrences are replaced by a new nonterminal.
Thus, the old nonterminal is 'split' in two. The triple representation presented earlier
allows a very simple implementation of splitting, but it applies only to grammars that
can be represented as triples, and it generates only a subset of the grammars that normal
splitting would produce.
11 In order to make the algorithm function correctly, FastCovers must be used for
this and not Covers. Filtering grammars that are covered by grammars from distinct
unlabelled tree sequences will prune search paths that may lead to valid New-G members.
This is the main reason for working with FastCovers rather than Covers.

A VERSION SPACE FOR GRAMMARS

61

3. The grammars are consistent with the the negative string just received,
because the test puts the grammars in New-G only if they fail to parse
that string. The grammars are consistent with the negative strings
received prior to this one, because the grammars from the old G were
consistent, and splitting moves down the FastCovers order, so splitting
reduces the language generated by a grammar and never expands it.
4. The grammars are maximal in the FastCovers order because splitting
moves down the order one step at a time, and the movement is stopped
as soon as the grammar becomes consistent with the presentation.12
This completes the discussion of Update-G—. We now turn to Update-G+,
the function that revises the G set when some of the grammars in it do not
parse a newly received positive string.
The easiest way to explain Update-G+ is to first describe an algorithm
that is not incremental: it takes the whole presentation at once and builds
the appropriate G set. The non-incremental algorithm proceeds in the
following steps:
1. Form the simple tree product by taking the Cartesian product of the
unlabelled simple derivation trees for each positive string.
2. For each such tree sequence in the simple tree product, form a triple
to represent the grammar that has only one nonterminal, S. The partitions for these grammars all have just one partition element, and the
element contains all the nodes in the derivation tree sequence. These
grammars are the maximal grammars in the FastCovers partial order.
They would be the G set if the presentation had only the positive
strings.
3. For each string in the set of negative strings, apply Update-G—.
This algorithm is not incremental, since it must process all the positive
strings before it processes any of the negative strings. An incremental
Update algorithm must be able to handle strings in the order they are
received. The incremental algorithm should take a G set whose grammars
may have already been split somewhat by Update-G—, and modify it to
accommodate a new positive string.
In the non-incremental algorithm, the effect of adding a new string is
to increase the length of the sequences of unlabelled derivation trees, and
hence to increase the number of nodes in the partitions. In the incremental
algorithm, this must be done in all possible ways, so the resulting Update
algorithm is:
12A complete proof would require using the fact that the derivational version space is
a set of lattices, and lattices are particularly well-connected.

62

K. VANLEHN AND W. BALL

1. Given a positive string, form the set of all unlabelled simple derivation
trees for that string.
2. For each grammar in the old G and for each tree for the new positive
string,
(a) append the tree onto the end of the tree sequence of the grammar's triple, and
(b) allocate the new tree's nodes to the partition elements in all possible ways. Thus, if there are N partition elements in the partition,
then there are N choices for where to put the first tree node, N
choices for where to put the second tree node, etc. If the tree has
M nodes, then NM new partitions will be generated. Each one
becomes a grammar that is a candidate for New-G.
3. Place all the candidate grammars generated in the preceding step on
the queue for the Update-G— algorithm. However, instead of testing
that a grammar is consistent with just one negative string, as the
Update-G— algorithm does, test that the grammar is consistent with
all the negative strings in the presentation that have been received so
far.
The first two steps generalize the old grammars by adding rules to them.
The new grammars might be too general, in that they may parse some of
the negative strings given earlier in the presentation. Hence, the last step
must check all the negative strings. This requires saving all the negative
strings as they are presented. Thus, the version space needs to be a triple:
[P+, P-, G].
This means that one of the usual benefits of the version space technique
is lost. Usually, version space induction allows the learner to forget about
an instance after having processed it. This algorithm requires the learner
to remember the instances in the presentation. However, it is still an
incremental algorithm. After each string is presented, an up-to-date G
set is produced. Moreover, it is produced with less processing and memory
than would be required to generate that same G set completely from scratch
using the entire presentation. In short, the algorithm is an incremental
version space update with respect to computation, but not with respect to
instance memory.
5.3 Illustrations of the algorithm's operation
In order to illustrate the operation of the algorithm, this subsection
presents a simple example. The next subsection will continue this example
by showing how the algorithm performs when it is modified to incorporate
certain biases.

A VERSION SPACE FOR GRAMMARS

63

Table 1. Learning a command language.
Instances
+
—
+
+
+

Size of G set

CPU seconds

4

0.03
0.25
0.52
11.80
0.32
526.00
20300.00

delete all-of-them
all-of-them delete
delete delete
delete it
it it
print it
print all-of-them

5
5
25
25
197
2580

The illustration is based on learning a command language for a file system. The algorithm receives strings of command words, marked positive
or negative, and from these data it must infer a grammar for the command
language. Suppose the first string is positive: 'delete all-of-them.' There
are four possible unlabelled simple trees for this string, and they lead directly to four grammars for the G set. These grammars are listed below in
their triple representation.
1. (1 delete all-of-them)
{1}
(S — delete all-of-them)
2. (1 delete (2 all-of-them))
{12}
(S - delete S) (S - all-of-them)
3. (1 (2 delete)(3 all-of-them))
{12}
(S - S all-of-them) (S - delete)
4. (1 (2 delete)(3 all-of-them))
{123}
(S -+ S S)(S - delete) (S - all-of-them)
The first three grammars generate the finite language consisting only of
the single string 'delete all-of-them.' The fourth grammar generates all
possible strings over the two word vocabulary of 'delete' and 'all-of-them.'
Suppose the next string is a negative string, 'all-of-them delete.' This string
cannot be parsed by grammars 1, 2 or 3, so they remain unchanged in the
G set. The fourth grammar is overly general, so it is split. There are only
three legal partitions. Two of them survive, becoming grammars 5 and 6
shown below. The other partition, {1}{2 3}, yields a grammar that parses
the negative string, so it is split further, into {1}{2}{3}. This partition is
FastCovered by the two survivors, so it is abandoned. The survivors are:

K. VANLEHN AND W. BALL

64

5. (1 (2 delete) (3 all-of-them))
(1 2}{3}
(S - S A)(S - delete) (A - all-of-them)
6. (1 (2 delete)(3 all-of-them))
{1 3}{2}
(S - A S)(A - delete) (S - all-of-them)
Suppose the next string is 'delete delete,' a negative instance. None of
the grammars in G parse this string, so the G set remains unchanged. This
illustrates that the algorithm has made an inductive leap while processing
the preceding strings. This string is new, but there is no change in the
version space.
Suppose the next string is positive, 'delete it.' There are four possible unlabelled simple derivation trees for this string. Each is paired with each of
the five grammars in the current G, yielding 20 combinations. The resulting 20 grammars are queued for testing against P—. Some splitting occurs
during the processing of the queue. When the queue is finally exhausted,
New-G has 25 grammars.
Table 1 summarizes the results so far and shows what happens as more
instances are presented. As a rough indication of the practicality of the
algorithm, the table shows the number of CPU seconds used in processing
each instance by a Xerox 1109 running Interlisp. The combinational explosion inherent in the Update-G+ algorithm is quite evident. However, the
algorithm is fast enough to construct small version spaces.
Table 2. A bias for minimum number of nonterminals.
Instances
+
—
—
+
+
+

delete all-of-them
all-of-them delete
delete delete
delete it
it it
print it
print all-of-them

Size of G set

CPU seconds

4
3
3
7
7
17
55

0.03
0.28
0.05
1.66
0.09
5.87
19.40

5.4 Biasing the Update algorithm
Better performance can be obtained by using the Update algorithm as
a framework upon which biases can be mounted. There are several places
in the algorithm where biases can be installed. One place is in the queuebased loop of Update-G—. Currently, new grammar triples are placed on
the queue only if they are not FastCovered by existing grammar triples.

A VERSION SPACE FOR GRAMMARS

65

Table 3. The effects of limiting the splitting ply.
Instances
+
+
+
+

delete all-of-them
all-of-them delete
delete delete
delete it
it it
print it
print all-of-them

one ply

two ply

G

Secs.

G

Secs.

4
5
5
25
25
188
2406

0.02
0.39
0.09
6.99
0.31
75.20
1110.00

4
5
5
25
25
197
2580

0.01
0.54
0.08
13.50
0.32
204.00
3460.00

This filter can be made stronger. For instance, suppose we queue only
grammars that have a minimal number of nonterminals, that is, grammar
triples with partitions of minimal cardinality. Table 2 shows the results of
running the previous illustration with this bias installed.
The bias reduces the G set from 2580 grammars to 55 grammars. All of
these grammars happen to use a single nonterminal, e.g.,
S - S all-of-them
S-S it
S - delete
S — print
Processing time is drastically reduced since many grammar triples - those
with partitions having cardinality larger than that of some existing consistent grammar triple are not even generated.
Another filter that can be placed on the Update-G- loop is one which
limits how deeply into the partition lattice the search may delve. We
implemented a filter which allows the user to set a 'ply.' If a grammar triple
with partition of cardinality m needs to be split, the search will proceed
only to partitions of cardinality m+n, where n is the ply set by the user.
Table 3 indicates that this bias approximates the results of the unbiased
algorithm more closely than does minimizing the number of nonterminals.
Note especially that for a ply of two, all the grammars of the unbiased
algorithm were produced at a fraction of the processing time.
The desirability of these biases will depend on the task domain. The
point is only that the algorithm provides a convenient framework for implementing such biases.
Another place to install biases is in the subroutine of Update-G+ that
generates unlabelled derivation trees. This placement allows the biases
to control the form of the grammars. For example, if the tree generator
produces only binary trees, then the induced grammars are in Chomsky
normal form. If the tree generator is constrained to produce only right

K. VANLEHN AND W. BALL

66

branching trees, then only regular grammars are considered. In the latter
case, there is only one right branching simple tree for each string. Consequently, there is only one unlabelled tree sequence for any given presentation. Under these circumstances, FastCovers is equivalent to Covers, and
our algorithm becomes similar to Pao's (1969) algorithm for learning finite
state machines. The main difference is that Pao's algorithm employs an
explicit representation for the whole version space, whereas our algorithm
uses the more compact [P+, P—, G] representation. Table 4 shows the
results of our algorithm on the test case discussed above when the bias for
regular grammars is introduced. Tables 5 and 6 show the results for a more
challenging case, inferring the syntax of Unix file names.
The point of this section is that the Update algorithm is good for more
than just exploring induction problems. It can be used as a framework for
the development of practical, task-specific learning machines.
Table 4. Inducing regular grammars for file system commands.
Instances
+
—
+
+
+

delete all-of-them
all-of-them delete
delete delete
delete it
it it
print it
print all-of-them

Size of G set

CPU seconds

1
1
1
1
1
1
1

0.07
0.02
0.01
0.07
0.02
0.11
0.12

6. Conclusions
The introduction motivated the results presented here from a narrow
perspective, viz. their utility in our research on cognitive skill acquisition.
However, they may have wider application. This section relates the results
to more typical applications of grammar induction, which, for purposes of
discussion, can be divided into three classes:
• Grammar induction is used as a formal account of natural language acquisition (Osherson, Stob & Weinstein, 1985; Berwick, 1985; Langley
& Carbonell, 1986; Pinker 1979). Learning the syntax of a language
is regarded by some as an important component of learning a language, and grammar induction is one way to formalize the syntactic
component of the overall language-learning task.

A VERSION SPACE FOR GRAMMARS

67

• Grammars are sometimes used in software engineering; e.g., for command languages or pattern recognition templates (Gonzalez & Thomason, 1978). Some applications require that the grammars change over
time or in response to different external situations. For instance, a
command language could be tailored for an individual user or a pattern recognizer might need to learn some new patterns. Grammar
induction may be useful in such applications (Fu & Booth, 1975; Biermann & Feldman, 1972).
• Knowledge bases in AI programs often have recursive hierarchical
structures, such as calling structures for plans or event schemata for
stories. The hierarchical component of such knowledge is similar to a
grammar. Grammar induction can be used to acquire the hierarchical
structure, although it must be used in combination with other induction engines that acquire the other structures. For instance, the Sierra
learner (VanLehn, 1987) represents knowledge as hierarchical production rules. It uses a grammar induction algorithm to learn the basic
skeleton of its rules, a BACON-like function inducer (Langley, 1979)
to learn details of the rules' actions, and a version space algorithm
(Mitchell, 1982) to learn the exact content of the rules' conditions.
In all these applications, the problem is to find an algorithm that will infer
an 'appropriate' grammar whenever it receives a 'typical' training sequence.
The definition of 'appropriate grammar' and 'typical training sequence'
depends, of course, on the task domain. However, it is usually the case that
only one grammar is desired for any given training. If so, then the Update
algorithm for derivational version spaces is not immediately applicable,
because it produces a set of grammars. In fact, the set tends to grow
larger as the training sequence grows longer. This is why we do not claim
that the algorithm models human learning, even though it was developed as
part of a study of human learning. The algorithm produces a set, whereas
a person probably learns only one (or a few) grammars qua procedures.
Similarly, the algorithm is not a plausible model of how children learn
natural language, even in the liberal sense of 'plausible' employed by studies
of language identification in the limit.13

13It might seem that after a large number of examples had been received, all the
grammars in the derivational version space will generate exactly the same language, and
that language is the target language. This would imply that our algorithm identifies
languages in the limit, even though it produces a set of grammars instead of a single
grammar. Kevin Kelly and Clark Glymour (personal communication) have proved this
conjecture false. Kelly (personal communication) has shown that any identifiable class of
context-free grammars is identifiable by a machine whose conjectures are always reduced,
simple grammars that are consistent with the data. Kelly and Glymour conclude, along
with us, that our results have little bearing on the language identification literature.

K. VANLEHN AND W. BALL

68

Table 5. Learning regular grammars for Unix file names.
Instances
+ foo . bar
+ foo
+ bar
+ / gram / foo
- foo / / foo
- / / foo
+ / usr / vsg / bar
- / / bar
- / / / bar
— / usr / / bar
— / / gram / bar
— / / vsg / bar
— vsg / / usr / bar
— / usr / / gram / bar
— / usr / / foo
— / usr / / gram / foo
- / usr / / vsg / ba

Size of G set

CPU seconds

1
1
1

0.01
0.01
0.01
0.04
1.03
0.82
88.30
20.90
9.49
19.50
9.76
6.20
10.10
17.20
10.40
7.55
8.72

1
5
2
43
32
25
16
32
14
22
15
10
5
2

Of course, not all applications of grammar induction desire an algorithm
to produce just one grammar. An application might have an inducer to
produce a set of grammars and leave some other process to choose among
them. For instance, when tailoring a command language, one might have
the user choose from a set of grammars generated by the grammar inducer.
If grammar induction is viewed as search, then producing just one grammar is a form of depth-first or greatest-commitment search. The version
space strategy, as pointed out by Mitchell (1982), is a form of breadth-first
or least commitment search. This gives it the capability of producing informative intermediate results. Halfway through the presentation of the
training sequence, one can determine unequivocally which generalizations
have been rejected and which generalizations are still viable. If this capability, or any other feature of the least commitment approach, is important,
then the algorithm presented here should be considered.
Even if the application does not use grammars as the representation
language or even as a component of the representation, the techniques presented here may be useful. The definition of reducedness extends readily to
many representation languages. For instance, a Prolog program is reduced
if deleting any of its Horn clauses makes the program inconsistent with
the training examples. Similarly, the technique for building a derivational
version space can be extended to representations other than grammars. It
remains to be seen whether there is any utility in these extensions, but
they are at least possible.

A VERSION SPACE FOR GRAMMARS

69

Table 6. The contents of the final G set of Table 5.
Grammar 1

Grammar 2

S- // A

S- / A
A — gram S

A — gram S
A
A
S
S
S
S

— usr S
— vsg S
- / foo
- / bar
- . bar
- foo

S-

bar

S - foo S

A — usr S
A — vsg S
S - / foo
S - / bar
A — . bar
S - foo
S - bar
S - foo A

However, the applications that seem most likely to benefit from the
derivational version space approach are those that are most similar to our
application, in that their research problem is to understand the influence of
representation languages and biases on learning. This amounts to studying
the properties of induction problems, rather than studying the properties
of induction algorithms. In such research, it is often a useful exercise to
study the set of generalizations consistent with a given training sequence
and see how that set changes as the biases and representation language
are manipulated. Such sets are exactly what the derivational version space
strategy calculates.

Acknowledgments
This research was supported by the Personnel and Training Research
Programs, Psychological Sciences Division, Office of Naval research, under
Contract N00014-86-K-0349. We would like thank Peter Gordon, Brian
MacWhinney, Tom Mitchell, and Peter Reimann for their critiques. Special
thanks go to Steve Nowlan and Kevin Kelly for debugging our claims, and
to Pat Langley and Tom Dietterich for debugging our exposition.

References
Anderson, J. R. (1983). The architecture of cognition. Cambridge, MA:
Harvard University Press.
Berwick, R. (1985). The acquisition of syntactic knowledge. Cambridge,
MA: MIT Press.
Biermann, A. W., & Feldman, J. A. (1972). A survey of results in grammatical inference. In S. Watanabe (Ed.), Frontiers of pattern recognition.
New York: Academic Press.

70

K. VANLEHN AND W. BALL

Cohen, P. R., & Feigenbaum, E. A. (1983). The handbook of artificial
intelligence. Los Altos, CA: Morgan Kaufmann.
Dietterich, T. G. (1986). Learning at the knowledge level. Machine Learning, 1, 287-316.
Fodor, J. A. (1975). The language of thought. New York: Crowell.
Fu, K., & Booth, T. (1975). Grammatical inference: Introduction and
survey. IEEE Transactions on Systems, Man, and Cybernetics, 5, 95111.
Gold, E. M. (1967). Language identification in the limit. Information and
Control, 10, 447-474.
Gonzalez, R. C., & Thomason, M. G. (1978). Syntactic pattern recognition.
Reading, MA: Addison-Wesley.
Hopcroft, J. E., & Ullman, J. D. (1969). Formal languages and their relation to automata. Reading, MA: Addison-Wesley.
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction to automata theory,
languages, and computation. Reading, MA: Addison-Wesley.
Horning, J. J. (1969). A study of grammatical inference (Technical Report
CS-139). Stanford, CA: Stanford University, Department of Computer
Science.
Langley, P. (1979). Rediscovering physics with BACON.3. In Proceedings
of the Sixth International Joint Conference on Artificial Intelligence
(pp. 505-507). Tokyo, Japan: Morgan Kaufmann.
Langley, P., & Carbonell, J. G. (1986). Language acquisition and machine
learning. In B. MacWhinney (Ed.), Mechanisms of language acquisition. Hillsdale, NJ: Lawrence Erlbaum.
Michalski, R. S. (1983). A theory and methodology of inductive inference.
In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), Machine
learning: An artificial intelligence approach. Los Altos, CA: Morgan
Kaufmann.
Mitchell, T. M. (1982). Generalization as search. Artificial Intelligence,
18, 203-226.
Newell, A. (1982). The knowledge level. Artificial Intelligence, 18,87-127.
Nowlan, S. (1987). Parse completion: A technique for inducing contextfree grammars (Technical Report AIP-1). Pittsburgh, PA: CarnegieMellon University, Department of Psychology.
Osherson, D., Stob, M., & Weinstein, S. (1985). Systems that learn. Cambridge, MA: Bradford Books/MIT Press.
Pao, T. W. (1969). A solution of the syntactical induction-inference problem for a non-trivial subset of context-free languages (Interim Report

A VERSION SPACE FOR GRAMMARS

71

69-19). Philadelphia, PA: University of Pennsylvania, Moore School
of Electrical Engineering.
Pinker, S. (1979). Formal models of language learning. Cognition, 7, 217283.
Quinlan, J. R. (1986). The effect of noise on concept learning. In R. S.
Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), Machine learning:
An artificial intelligence approach (Vol. 2). Los Altos, CA: Morgan
Kaufmann.
Reynolds, J. C. (March 1968). Grammatical covering (Technical Memorandum 96). Argonne National Laboratory.
VanLehn, K. (1983a). Human skill acquisition: Theory, model and psychological validation. In Proceedings of the National Conference on Artificial Intelligence (pp. 420-423). Washington, D.C.: Morgan Kaufmann.
VanLehn, K. (1983b). Felicity conditions for human skill acquisition: Validating an Al-based theory (Technical Report CIS-21). Palo Alto, CA:
Xerox Palo Alto Research Center.
VanLehn, K. (1983c). The representation of procedures in repair theory.
In H. P. Ginsberg (Ed.), The development of mathematical thinking.
Hillsdale, NJ: Lawrence Erlbaum.
VanLehn, K. (1987). Learning one subprocedure per lesson. Artificial
Intelligence, 31, 1-40.
VanLehn, K., Brown, J. S., & Greeno, J. G. (1984). Competitive argumentation in computation theories of cognition. In W. Kintsch, J. Miller,
& P. Poison (Eds.), Methods and tactics in cognitive science. Hillsdale,
NJ: Lawrence Erlbaum.
Vere, S. (1975). Induction of concepts in the predicate calculus. In Proceedings of the Fourth International Joint Conference on Artificial Intelligence (pp. 281-287). Tbilisi, USSR: Morgan Kaufmann.

Appendix 1. Proof of Theorem 3
The following proof shows that there are finitely many reduced contextsensitive grammars for any given finite presentation. Context-sensitive
grammars are used in the theorem not only because they give the theorem
broader coverage, but because they make the proof simpler. The proof
is a counting argument, and context-sensitive grammars are defined by
counting the relative sizes of the left and right sides of their rules.
Definition 6 A grammar is a context-sensitive grammar if for all rules a — B,
we have |a| < |B|, where \x\ means 'the length of string x'.14
14Another

definition of context-sensitive grammars requires that the rules have the

72

K. VANLEHN AND W. BALL

Definition 7 A context-sensitive grammar is simple if (1) for all rules a — 0,
\a\ = |B| implies that B has more terminals than a, and (2) every nonterminal
occurs in some derivation of some string.
Lemma 1 The longest derivation of a string s using a simple context-sensitive
grammar is 2 s\ — 1.

Proof: Consider an arbitrary step in the derivation, a — B. If |a| —
|B|, then B must contain at least one more terminal than a, because the
grammar is a simple one. Consequently, there can be at most |s| such steps
in the derivation, because there are s\ terminals in the string. For all the
other steps in the derivation, B must be at least one longer than a. There
can be at most \s\ — 1 such steps in the derivation, because the string is
only \s\ long. So the longest possible derivation using a simple grammar is
2|s| — 1, where \s\ steps have \a\ = \B\ and \s\ — 1 steps have |a| < |B|.
Theorem 3 There are finitely many simple reduced context-sensitive grammars
for any given finite presentation.

Proof: There are finitely many positive strings in the presentation. By
the lemma just proved, the longest derivation of each string a is 2\a\ — 1,
Therefore, the largest number of rule firings in deriving the positive strings
is less than 2T, where T is the total of the lengths of positive strings:

where P+ designates the set of positive strings in the presentation. If a
grammar has more than 2T rules, then there must be rules that were not
used in any string's longest derivation. Such rules can be eliminated from
the grammar without affecting the existence of the longest derivations. So
such a grammar is reducible. Thus, the largest possible reduced grammar
will have less than 2T rules.
This result alone is not enough to show that there are finitely many
reduced grammars, because the rules could in principle be arbitrarily long
or contain arbitrarily many distinct symbols. However, if a rule is used in
a derivation of some string in P+, its right side must be shorter than L,
where L is the length of the longest string in P+. The rule's left side must
also be shorter than L. If we could show that the grammar cannot have
arbitrarily many symbols, then we would be done. Because the terminals
in the rules must appear in the strings and there are finitely many strings,
there are finitely many possible terminals in the rules. In fact, the largest
form aAB — crB, where A is a nonterminal and 7 is nonempty. The definition given
above is from Hopcroft and Ullman (1969), who comment that the two definitions are
equivalent.

A VERSION SPACE FOR GRAMMARS

73

number of terminals is T. Because each nonterminal must participate in
some string's derivation (by simplicity), each nonterminal must appear in
some rule's left side. There are less than 2T rules, each with at most L
symbols on the left side, so there are less than 2LT possible nonterminals.
Thus, the number of simple reduced grammars is finite because: (1) the
number of rules is less than 2T, (2) the length of the left and right sides is
at most L, and (3) there are less than 2LT+T symbols used in the rules.
This completes the proof of theorem 3.

Appendix 2. Proof of Theorem 5
Theorem 5 states that the derivational version space contains the reduced
version space. The critical part of the proof deals with the positive strings,
P+, because both version spaces specifically exclude grammars that generate negative strings. Hence, we prove the following theorem, from which
Theorem 5 follows immediately.
Algorithm A Given a set of strings P+, produce the grammars corresponding
to all possible labellings of all possible sequences of all possible simple unlabelled
derivation trees for each string.
Theorem 11 Algorithm A produces a set of grammars that contains all the reduced, simple grammars for P+.

To prove the theorem, we need to show that every reduced grammar is generated by the algorithm. Suppose that R is a reduced grammar. Because
R generates every string in P+, it generates at least one derivation tree
for every string in P+. First we will consider the case where R generates
exactly one derivation tree for each string, then consider the case where R
generates more than one derivation tree for some (or all) of the strings.
Since R generates exactly one derivation tree for each string, we merely
need to show that that sequence of derivation trees is among the set of
labelled parse tree sequences generated by the algorithm. Because R is
simple, its parse trees must conform to the structural constraints that
were imposed in generating the set of unlabelled derivation trees. In other
words, if a node has just one daughter, then the daughter is a leaf. Moreover, the algorithm generates all such unlabelled derivation trees, so R's
derivation trees must be among those generated by the algorithm. Thus,
R's derivation trees are some labelling of one of the unlabelled derivation
tree sequences. However, the algorithm generates all possible labellings
of these. So R's parse trees must be among the set of labelled derivation
tree sequences generated by the algorithm. The only way for R to have
nonterminals other than those induced by labelling the unlabelled derivation trees would be for R to have rules that are not used in generating its

74

K. VANLEHN AND W. BALL

derivation trees for P+. However, R is reduced, so this cannot be the case.
Thus, R must be one of the grammars induced by the algorithm.
Next we consider the case where R generates more than one derivation
tree for at least one string in P+. For each string p in P+, let Treesp be the
set of derivation trees for p generated by R. As shown above, the algorithm
generates at least one of the derivation trees in each Treep. From that
derivation tree sequence, it generates rules for a grammar. The generated
grammar will not be the same as R if some of the other derivation trees in
Treesp use rules that are not in this derivation tree sequence. However, if
this were the case, then those rules could be deleted from R, and yet all
the strings could still be parsed. Thus, R would be reducible, contrary to
hypothesis. So R must be generated by the algorithm. This completes the
proof of the theorem.

Appendix 3. Proof of Theorem 10
The following theorem states that the derivational version space is properly represented by its boundaries:
Theorem 10 Given a grammar x in triple form and a derivational [G,S], x is in
the derivational version space represented by [G,S] if and only if there is some g
in G such that g FastCovers x, and some s in S such that x FastCovers s.

The 'if half of the 'if and only if follows immediately from the definition
of G and S; if x is in the space, then there must be maximal and minimal
elements above and below it. To show the 'only if half, suppose that x is
not in the space, and yet there is a g that FastCovers it and an s that it
FastCovers. A contradiction will be derived by showing that x should be in
the derivational version space. First, we show that x is consistent with the
presentation. Because x FastCovers s, the language generated by x includes
the language generated by s. Because s's language includes the positive
strings of the presentation, so does x's language. Thus, x is consistent with
the positive strings of the presentation. Because g FastCovers x, and g's
language excludes all the negative strings, x's language must also exclude all
the negative strings. So x is consistent with the negative strings. Therefore,
x is consistent with the whole presentation. The remaining requirement for
membership in the derivational version space is that x be a labelling of some
tree sequence from the simple tree product of the presentation. Clearly, x
is a labelling of the tree sequence which is the first element of its triple.
Because x FastCovers s, it must have the same tree sequence as x, so its
tree sequence is a member of the simple tree product. It follows that x
should be in the derivational version space. This contradiction completes
the proof of the theorem.

Instructional Factors Analysis: A Cognitive Model For Multiple
Instructional Interventions
Min Chi, Stanford University, CA USA
Kenneth Koedinger, Carnegie Mellon University, PA USA
Geoff Gordon, Carnegie Mellon University, PA USA
Pamela Jordan, University of Pittsburgh, PA USA
Kurt VanLehn, Arizona State University, AZ USA

In this paper, we proposed a new cognitive modeling approach: Instructional Factors Analysis Model (IFM). It belongs to a
class of Knowledge-Component-based cognitive models. More specifically, IFM is targeted for modeling student’s performance
when multiple types of instructional interventions are involved and some of them may not generate a direct observation of
students’ performance. We compared IFM to two other pre-existing cognitive models: Additive Factor Models (AFMs) and
Performance Factor Models (PFMs). The three methods differ mainly on how a student’s previous experience on a Knowledge
Component is counted into multiple categories. Among the three models, instructional interventions without immediate direct
observations can be easily incorporate into the AFM and IFM models. Therefore, they are further compared on two important
tasks—unseen student prediction and unseen step prediction—and to determine whether the extra flexibility afforded by additional parameters leads to better models, or just to over fitting. Our results suggested that, for datasets involving multiple types
of learning interventions, dividing student learning opportunities into multiple categories is beneficial in that IFM out-performed
both AFM and PFM models on various tasks. However, the relative performance of the IFM models depends on the specific
prediction task; so, experimenters facing a novel task should engage in some measure of model selection.

1.

INTRODUCTION

For many existing Intelligent Tutoring Systems (ITSs), the system-student interactions can be viewed as a
sequence of steps [VanLehn 2006]. Most ITSs are student-driven. That is, at each time point the system elicits
the next step from students, sometimes with a prompt, but often without any prompting (e.g., in a free form
equation entry window where each equation is a step). When a student enters an attempt on a step, the ITS
records whether it is a success or failure without the tutor’s assistance and may give feedbacks and/or hints
based on the entry. Students’ first attempt records on each step are then collected for student modeling.
Often times in ITSs, completion of a single step requires students to apply multiple Knowledge Components.
A Knowledge Component (KC) is: “a generalization of everyday terms like concept, principle, fact, or skill,
and cognitive science terms like schema, production rule, misconception, or facet” [VanLehn et al. 2007].
They are the atomic units of knowledge. Generally speaking, students’ modeling on conjunctive-KC steps
are more difficult than that on steps that require a single KC.
The three most common student modeling methods are: Knowledge Tracing (KT) [Corbett and Anderson 1995], Additive Factor Models (AFM) [Cen et al. 2006; 2008], and Performance Factor Models
(PFM) [Pavlik et al. 2009]. When performing student modeling we seek to construct a cognitive model
based upon these observed behaviors and to apply the model to make predictions. Generally speaking, we
are interested in three types of predictions: type 1 is about how unseen students will perform on the observed
steps same as those in the observed dataset; type 2 is about how the same students seen in the observed data
will perform on unseen steps; and type 3 is about how unseen students will perform on unseen steps, that
is, both. For the present purposes we classifie students or steps that appear in the observed training data

as seen and those that appear only in the unobserved test data as unseen. In this paper we will examine
prediction types 1 and 2 and leave type 3 for future work.
Previously KT and PFM have been directly compared both on datasets involved single-KC steps [Pavlik
et al. 2009] and those involved conjunctive-KC steps[Gong et al. 2010]. Results have shown that PFM is as
good or better than KT for prediction tasks under Bayesian Information Criteria (BIC) [Schwarz 1978] in
[Pavlik et al. 2009] or using Mean Squared Error (MSE) as criteria in [Gong et al. 2010]. For both BIC and
MSE, the lower the value, the better.
While PFM and KT have been compared on datasets involved conjunctive-KC step, prior applications of
AFM and PFM have mainly been with single-KC steps and indicated no clear winner. More specifically, while
AFM is marginally superior to PFM in that the former has lower BIC and cross-validation Mean Absolute
Deviance (MAD) scores in [Pavlik et al. 2009], PFM performed better than AFM under MAD scores in
[Pavlik et al. 2011]. For MAD, same as MSE, the lower the value, the better. On the other hand, previous
research have shown that AFM can, at least in some cases, do a fine job in modeling conjunctive KCs [Cen
et al. 2008]. Therefore, in this paper we will compare AFM and PFM directly on a dataset involving many
conjunctive-KC steps.
Moreover, most prior research on cognitive modelings was conducted on datasets collected from classical
student-driven ITSs. Some ITSs, however, are not always student-driven in that they may involve other
instructional interventions that do not generate direct observations on student’s performance. The dataset
used in this paper, for example, was collected from a tutor that, at each step chose to elicit the next step
information from students or to tell them the next step. In our view these tell steps should also be counted
as a type of Learning Opportunity (LO) as they do provide some guidance to students. Yet on the other
hand, these steps do not allow us to directly observe students’ performance. KT model is designed mainly for
student-driven ITSs in that its parameters are directly learned from the sequences of student’s performance
(right or wrong) on each step. When there are multiple instructional interventions and some of them do
not generate direct observations, it is not very clear how to incorporate these interventions directly into
conventional KT models. Therefore, in this paper we are mainly interested in comparing AFM and PFM.
Our dataset was collected from an ITS that can either elicit the next step from the student or tell them
directly. Incorporating tell steps into AFM model is relatively easy in that tells can be directly added to
total LO counts. The PFM, however, uses student’s prior performance counts, the success or failure, in the
equation. Since tells do not generate any observed performance, it is hard to include them in the PFM.
Therefore, we elected to add a new feature to represent instructional interventions such as tells. As shown
later, the new model can be easily modified for modeling datasets with multiple instructional interventions
and thus it is named as Instructional Factors Analysis Model (IFM).
To summarize, in this paper we will compare three models, AFM, PFM and IFM, on a dataset involving
many conjunctive-KC steps and multiple instructional interventions. Previous research has typically focused
on how well the models fit the observed data. In the following, we also investigated how well they perform at
making the predictions of unseen students’ performance on seen steps (type 1) and seen students’ performance
on unseen steps (type 2). Before describing our general methods in details we will first describe the three
models.
2.

THREE MODELS: AFM, PFM, AND IFM

All three models, AFM, PFM, and IFM, use a Q-matrix to represent the relationship between individual
steps and KCs. Q-matrices are typically encoded as a binary 2-dimensional matrix with rows representing
KCs and columns representing Steps. If a given cell Qkj = 1, then step j is an application of KC k. Previous
researchers have focused on the task of generating or tuning Q-matrices based upon a dataset [Barnes 2005;
Tatsuoka 1983]. For the present work we employed a static Q-matrix for all our experiments. Equations 1,

2, and 3 present the core of each model. Below the equations are the detailed descriptions of each term used
in the three equations.
The central idea of AFM was originally proposed by [Draney et al. 1995] and introduced into ITS field by
[Cen et al. 2006; 2008]. Equation 1 shows that AFM defines the log-odds of a student i completing a step j
correctly to be a linear function of several covariates. Here pij is a student i’s probability of completing a step
j correctly, Nik is the prior LO counts. AFM models contain three types of parameters: student parameters
θi , KC (or skill) parameters βk , and learning rates γk . While AFM is sensitive to the frequency of prior
practice, it assumes that all students accumulate knowledge in the same manner and ignores the correctness
of their individual responses.
PFM, by contrast, was proposed by [Pavlik et al. 2009] by taking the correctness of individual responses
into account. It can be seen as a combination of learning decomposition [Beck and Mostow 2008] and AFM.
Equation 2 expresses a student i’s log-odds of completing a step j correctly based upon performance features
such as Sik (the number of times student i has previously practiced successfully relevant KC k) and Fik (the
number of times student i has previously practiced unsuccessfully relevant KC k). PFM may also include
student parameters such as θi and skill parameters, such as βk . Additionally, PFM employs parameters to
represent the benefit of students’ prior successful applications of the skill µk and the benefit of prior previous
failures ρk .
While PFM was originally proposed without a θi , it is possible to include or exclude these student parameters from either PFM or AFM. In prior work, Corbett et al. noted that models which tracked learning
variability on a per-subject basis, such as with θ outperform models that do not [Corbett and Anderson
1995]. Pavlik [Pavlik et al. 2009] further noted that the full AFM model seemed to outperform PFM without θ which in turn outperformed AFM without θ. Pavlik et al. also hypothesized that PFM with θ would
outperform the other models and they investigated it in their recent work. In this study, our analysis showed
that prediction is better with student parameters, especially for AFM models, thus we include θi in our
versions of both AFM and PFM.
From PFM, IFM can be seen as adding a new feature to represent the tells together with the success
or failure counts, shown in Equation 3. Equation 3 expresses a student i’s log-odds of completing a step
j correctly based upon performance features including Sik , Fik , Tik (the number of times student i has
previously got told on relevant KC k). IFM also includes student parameters θi , skill parameters βk , µk , ρk ,
and the benefit of prior previous tells νk .
AFM:
PFM:
IFM:

X
X
pij
= θi +
βk Qkj +
Qkj (γk Nik )
1 − pij
k
k
X
X
pij
ln
= θi +
βk Qkj +
Qkj (µk Sik + ρk Fik )
1 − pij
k
k
X
X
pij
= θi +
βk Qkj +
Qkj (µk Sik + ρk Fik + νk Tik )
ln
1 − pij
ln

k

k

Where:
i. represents a student i.
j. represents a step j.
k. represents a skill or KC k.
pij . is the probability that student i would be correct on step j.
θi . is the coefficient for proficiency of student i.
βj . is coefficient for difficulty of the skill or KC k.

(1)
(2)
(3)

Qkj . is the Q-matrix cell for step j using skill k.
γk . is the coefficient for the learning rate of skill k (AFM only);
Nik . is the number of practice opportunities student i has had on the skill k (AFM only);
µk . is the coefficient for the benefit of previous successes on skill k (PFM & IFM);
Sik . is the number of prior successes student i has had on the skill k (PFM & IFM);
ρk . is the coefficient for the benefit of previous failures on skill k (PFM & IFM);
Fik . is the number of prior failures student i has had on the skill k (PFM & IFM);
νk . the coefficient for the benefit of previous tells on skill k (IFM only);
Tik . the number of prior Tells student i has had on the skill k (IFM only);
3.

TRAINING DATASET AND EIGHT LEARNING OPPORTUNITY MODES

The original dataset was collected by training 64 students on a natural-language physics tutoring system
named Cordillera [VanLehn et al. 2007; Jordan et al. 2007] over a period of four months in 2007. The physics
domain contains eight primary KCs including the weight law (KC1 ), Definition of Kinetic Energy (KC20 ),
Gravitational Potential Energy (KC21 ), and so on. All participants began with a standard pretest followed
by training 7 physics problems on Cordillera and then a post-test. The pre- and post-tests are identical in
that they both have the same 33 test items. The tests were given online and consisted of both multiple-choice
and open-ended questions. Open-ended questions required the students to derive an answer by applying one
or multiple KCs.
In this study, our training dataset comprises 19301 data points resulted from 64 students solving 7 training
problems on Cordillera. Each student completed around 300 training problem steps. Note that the training
dataset does not include the pre- or posttest. In other words, a data point in our training dataset is either
the first attempt by a student on an elicit step or a system tell during his/her training on Cordillera only.
There are two types of steps in Cordillera. The primary steps are necessary problem-solving and conceptual
discussion steps. The justification steps, on the other hand, are optional steps that occur when students are
asked to justify the primary step they have just completed. The primary steps are designed to move the
solution process forward while the justification steps are designed to help the students engage with the
domain knowledge in a deeper way. When collecting our dataset the Cordillera system decided whether to
elicit or tell each step randomly. Thus, we have two types of LOs: elicit and tell for the primary steps; and
self-explain or explain for the justifications.
Figure 1 shows a pair of sample dialogues taken from the cordillera system for the same series of primary
steps with the same domain content. In dialogue (1.a) the system elects to elicit the students’ answer (steps
2- 3), while in dialogue (1.b) the system chooses to tell the student the answer (steps 2). Similarly in Figure 2
we present a similar comparison between a pair of self-explain and explain dialogues. As before both dialogues
cover the same domain content. In dialogue (2.a) the system asks the student to self-explain their answer
to the question in qualitative terms (steps 3-4). In dialogue (2.b) they are provided with a short qualitative
explanation (step 3).
For the primary steps, the average number of decisions ranges from 2.5 for KC1 to 31 for KC20 for elicit
steps and ranges from 4 for KC1 to 47 for KC20 for the tell steps. Compared with primary steps, justification
steps are significantly less frequent in that the average number ranges from 0 for KC1 to 3.5 for KC21 for
self-explain steps and ranges from 0 for KC1 to 2.5 for KC20 for explain steps.
In most of the prior research that we have surveyed the authors mainly used datasets involving quantitative
problem-solving steps similar to the primary steps in Cordillera. So when counting LOs in this study, we
first need to decide whether or not to include justification steps. Additionally, most previous research did not
include students’ performance on the pretest when counting LOs. While this makes some sense as students

(1.a) Elicit Version

(1)
(2)
(3)
(4)

T: So let’s start with determining the value of v1.
T: Which principle will help you calculate the rock’s instantaneous magnitude of velocity at T1? {ELICIT}
S: definition of kinetic energy
T: Let me just write the equation for you: KE1 = 0.5*m*v1ˆ2.
(1.b) Tell Version

(1) T: So let’s start with determining the value of v1.
(2) T: To calculate the rockś instantaneous magnitude of velocity at T1, we will apply the definition of kinetic
energy. {TELL}
(3) T: Let me just write the equation for you: KE1 = 0.5*m*v1ˆ2.
Fig. 1. Learning Opportunities on Problem-Solving Steps: Elicit vs. Tell

(2.a) Self-Explain
(1)
(2)
(3)
(4)
(5)

T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic energy at T1? {ELICIT}
S:nope.
T:Excellent! Please explain why. {Self-Explain}
S:Only the magnitude of the velocity and not the direction of it is part of the definition of kinetic energy.
T:Excellent! Now that we know v1, · · ·

(2.b) Explain
(1) T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic energy at T1? {ELICIT}
(2) S:nope.
(3) T:Excellent! This is because the kinetic energy only depends on mass and the magnitude of velocity, not the direction
of velocity.{Explain}
(4) T:Now that we know v1, · · ·

Fig. 2. SelfExplain vs. Explain

receive no feedback indicating their successes or failures during the test, it is still the case that they do
practice their skills. Secondly we need to decide whether or not to include student’s pretest performance in
the LO counts.
In order to explore how different choices of LOs would impact different cognitive models, we defined four
ways to count the LOs. In the primary mode we count only the primary steps within the ITS. In pretestprimary we count the primary mode steps plus the pretest (each test item is treated as one step for training).
Primary-Justify mode counts the primary and justification steps within the ITS alone. And finally the overall
mode counts all steps in both the pretest and ITS training.
Note that using different modes of LOs neither changes the size of the training dataset which is generated
along students’ logs when training on Cordillera nor changes the number of parameters to be fit. Using
pretest in the LO count means that various LOs do not start with 0 for the pretest-primary and overall

modes but are based on the frequency of KC appearances (and, in the case of PFM, the accuracy) in the
pretest. For example, if a KC20 is tested 20 times in the pretest and a student was correct 5 times and
wrong 15 times, then the student’s LOs on KC20 for pretest-primary and overall mode would start with
LO = 20, Success = 5, F ail = 15, T ell = 0. For Primary and Primary-Justify modes, all LOs start with 0.
Coupled with this variation we can also count LOs additively or logarithmically. Using logarithmic count
is inspired by the power law relationship between measures of performance (reaction time or error rate) and
the amount of practice [Newell and Rosenbloom 1981]. But others [Heathcote et al. 2000] have argued that
the relationship is an exponential, which corresponds to additive counting. To summarize, we have {Primary,
Pretest-Primary, Primary-Justify, Overall} × {count, ln(count)}, a total of eight LO modes.
4.

RESULTS

Two measures of quality, the Bayesian Information Criteria (BIC) and the cross-validation Root Mean
Squared Error (RMSE), are used to evaluate how well various instantiated models perform. For both BIC
and cross-validation RMSE, the lower the value, the better. BIC [Schwarz 1978] is a criterion for model
selection among a class of parametric models with different numbers of parameters. In prior research on the
evaluation and comparison of different cognitive models [Cen et al. 2006; Pavlik et al. 2009; Gong et al. 2010]
the authors used BIC as a measure of success. In machine learning, however, it is conventional to use the
cross-validation RMSE, which is a more interpretable metric and, we believe, a more robust measure. For
the purposes of this paper, we will report both BIC and RMSE.
4.1 AFM, PFM, vs. IFM.
First, we will investigate whether considering Tell and Explains into the LOs is beneficial. In traditional
cognitive modeling the focus is solely on steps where the student’s performance is observed. In the context
of Cordillera that means counting only the elicit and self-explain steps as both require students to apply
their knowledge without support and their performance can be directly evaluated. For AFM models, we thus
compared the AFM algorithms shown in equation 1 by either including Tells and Explains into Nik or by
excluding them out of Nik . The two resulted models are referred as AFM-Tell and AFM+Tell respectively.
Therefore, in this section we compared four models: AFM-Tell, AFM+Tell, PFM and IFM across eight LO
modes.
For each of the four models, its corresponding count LOs on corresponding {Primary, Pretest-Primary,
Primary-Justify, Overall} modes are defined in Table I. For example, the IFM has three LO counts: prior
success Sik , prior failures Fik , and prior tells Tik . Under the Primary-Justify mode (shown in the left bottom
of the table), Sik = Success in (Elicit + Self-Explain) on the KC k, Fik = prior failure in (Elicit + SelfExplain) on the KC k, and Tik = prior tells and explains on the KC k. Once the count mode is defined, the
corresponding Ln(Count) mode is simply taking each count logarithmically. For example, under {PrimaryJustify, Ln(Count)} mode, we have Sik = ln[Success in (Elicit + Self-Explain) on KC k], Fik = ln[prior
failure in (Elicit + Self-Explain) on KC k], and Tik = ln[prior tells and explains on the KC k].
For each model on each mode, we carried out a 10-fold cross-validation. Such procedure resulted in 8
(modes) × 4 (models) = 32 BIC values and CV RMSE values. Table II shows the comparisons among the
four models when using {Primary-Justify, Count} and {Primary-Justify, Ln(Count)} LO modes respectively.
It shows that across both modes, the IFM is more accurate (both lower BIC and RMSE) than the PFM;
similarly, the latter is more accurate than AFM+Tell and AFM-Tell. However, it is harder to compare
AFM-Tell and AFM+Tell. For example, on {Primary-Justify, Count} mode, although AFM-Tell has lower
BIC than AFM+Tell 9037 vs. 9058, the latter has lower RMSE than the former: 4.456E-01 vs. 4.459E-01.
So on both {Primary-Justify, Count} and {Primary-Justify, Ln(Count)} modes, we have IFM > PFM >
AFM+Tell, AFM-Tell. Such pattern is consistence across all eight modes.

Table I. {Primary, Pretest-Primary, Primary-Justify, Overall} Learning Opportunity Modes
AFM-Tell
AFM+Tell
PFM
IFM

AFM-Tell
AFM+Tell
PFM
IFM

Primary

Pretest-Primary

Nik
Nik
Sik
Fik
Sik
Fik
Tik

Elicit
Elicit+Tell
Success(Elicit)
Failure(Elicit)
Success(Elicit)
Failure(Elicit)
Tell

Pretest+Elicit
Pretest+Elicit+Tell
Success in (Pretest + Elicit)
Failure in (Pretest + Elicit)
Success in (Pretest + Elicit)
Failure in (Pretest + Elicit)
Tell

Primary-Justify

Overall

Nik
Nik
Sik
Fik
Sik
Fik
Tik

Elicit + SelfExplain
Elicit+Tell + SelfExplain +Explain
Success in (Elicit + Self-Explain)
Failure in (Elicit + Self-Explain)
Success in (Elicit + Self-Explain)
Failure in (Elicit + Self-Explain)
Tell+ Explain

Pretest+ Elicit+SelfExplain
Pretest+ Elicit+Tell + SelfExplain+Explain
Success in (Pretest+ Elicit + Self-Explain)
Failure in (Pretest+ Elicit + Self-Explain)
Success in (Pretest+ Elicit + Self-Explain)
Failure in (Pretest+ Elicit + Self-Explain)
Tell+ Explain

Table II. Compare AFM-Tell, AFM+Tell, PFM and IFM on
{Primary-Justify, Count} and {Primary-Justify, Ln(Count)} mode
Model

{Primary-Justify, Count}
BIC
10-fold RMSE

{Primary-Justify, Ln(Count)}
BIC
10-fold RMSE

AFM-Tell
AFM+Tell
PFM
IFM

9037
9117
8474
8347

9037
9058
8461
8321

4.460E-01
4.470E-01
4.235E-01
4.217E-01

4.459E-01
4.456E-01
4.236E-01
4.211E-01

In order to compare the performance among four models, Wilcoxon Signed Ranks Tests were conducted
on resulted BICs and RMSEs. Results showed that IFM significantly outperformed the PFMs across eight
modes: Z = −2.52, p = 0.012 for both BIC and cross-validation RMSE. Similarly, it was shown that
across all eight modes IFM beat corresponding AFM-Tell across eight modes significantly on both BIC and
RMSE: Z = −2.52, p = 0.012. Similar results were found between IFM and AFM+Tell in that the former
out-performed the latter across eight modes significantly on both BIC and RMSE: Z = −2.52, p = 0.012.
Comparisons between PFM and AFM-Tell and AFM+Tell showed that PFM beats corresponding AFMTell across eight modes significantly on both BIC and RMSE: Z = −2.52, p = 0.012; and PFM also beat
AFM+Tell significantly on both BIC and RMSE: Z = −2.52, p = 0.012. Finally, comparisons between AFMTell and AFM+Tell showed that adding Tells and Explains into LOs did not statistically significantly improve
the BIC and RMSE of the corresponding AFM model: Z = −0.28, p = 0.78 for BIC and Z = −1.35, p = 0.18
for RMSE respectively. Therefore, our overall results suggested: IFM > PFM > AFM-Tell, AFM+Tell.
Next, we investigated which way of counting LOs is better, using logarithmic or additive tabulation?
Wilcoxon Signed Ranks Tests were conducted on comparing the BIC and RMSE of the performances when
using Count versus using Ln(Count) on the same model and mode. Results showed using Ln(Count) performed significantly better than using Count: Z = −2.27, p = 0.008 for BIC and Z = −2.33, p = 0.02
for RMSE respectively. This analysis is interesting in relation to a long-standing debate about whether the
learning curve is exponential (like additive tabulation) or a power law (logarithmic tabulation) [Heathcote
et al. 2000]. Our results appear to favor the power law.
Next, we investigated the impact of four LO modes. The BICs and RMSEs were compared among the
{Primary, Pretest-Primary, Primary-Justify, Overall} modes regardless of Count and Ln(Count). A pairwise
comparisons on Wilcoxon Signed Ranks Tests showed that the {Primary-Justify} modes generated signifi-

cantly better models than using {Primary} modes Z = −2.1, p = 0.036; the {Primary} modes generated
better models than using {Pretest-Primary} and {Overall} Z = −2.27, p = 0.018 and Z = −2.521, p = 0.012
respectively. While no significant difference was found between {Pretest-Primary} and {Overall} modes. Similar results was found on RMSE. Therefore, it suggested that adding justification steps into LOs is beneficial
in that Primary-Justify mode beats Primary; however, adding pretest into the LOs did not produce better
models and it may even have resulted worse models: the benefit of adding justification steps into LOs was
seemingly washed out by including pretest in the LOs in that {Overall} modes generate worse models than
{Primary-Justify} and {Primary}.
To summarize, for modeling the training data, applying IFM model and using {Primary-Justify, Ln(Count)}
as LOs generated the best fitting model. Additionally, comparisons among the IFM, PFM, AFM-Tell,and
AFM+Tell showed that IFM > PFM > AFM-Tell, AFM+Tell. In this paper, our goal is to compare cognitive
models on datasets involving multiple types of instructional interventions. As shown above, for AFM the tell
steps can be directly added into existing opportunity count Nik ; For the PFM model, however, there is no
direct way how tells should be incorporated. Therefore, in the following we will mainly compare IFM and
AFM+Tell. For the convenient reasons, we will refer to AFM+Tell as AFM.
4.2 IFM vs. AFM for Unseen Student Prediction (Type 1)
Next we compared the AFM and IFM models on the task of unseen student prediction. In order to predict
unseen student’s performance, Student ID was treated as a random factor in both AFM and IFM models.
Here we conducted Leave-one-student-out cross-validation. In other words, 64 students resulted in a 64-fold
cross validation. Thus, we have 8 (modes) × 2 (AFM vs.IFM) BIC values and Cross-Validation RMSE values.
Table III shows the correpsonding BIC and RMSE values of AFM and IFM models using {Primary-Justify,
Ln(Count)} mode. Table III shows that IFM generates better prediction models (both lower BIC and RMSE)
than AFM and the difference is large. Such pattern is consistence across all eight modes.
Table III. AFM vs. IFM On Unseen Students
with Random Effect Student Parameters
Model

BIC

64-fold Cross-Validation RMSE

AFM
IFM

8724
7952

4.6144E-01
4.1661E-01

To compare IFM and AFM across eight modes, Wilcoxon Signed Ranks Tests were conducted on both
BICs and cross-validation RMSEs. Consistent with the patterns shown in Table III, results showed that
IFM is significant better than AFM across eight modes: Z = −2.52, p = 0.012 for both BIC and crossvalidation RMSE. To summarize, IFM with random student parameter is a better model for predicting
unseens students’ performances on seen steps than AFM model with random student parameter. The best
performance was generated IFM model using {Primary-Justify, Ln(Count)} as LOs.
4.3 AFM vs. IFM for Unseen Step prediction (Type 2).
Finally we compared AFM and IFM models on the task of unseen step prediction. Here we used training
dataset and tested each models’ prediction using students’ post-test performance. For each model on each
mode, we carried out a 10-fold cross-validation. Such procedure again resulted in 8 × 2 BIC values and CV
RMSE values.
Table IV shows the results on comparisons for the AFM and IFM models on both {Primary-Justify,
Ln(Count)} and {Overall, Ln(Count)} modes. Across the eight LO modes, the performance of AFM reaches
its best when using {Primary-Justify, Ln(Count)} mode and IFM reaches its best when using {Overall,
Ln(Count)} mode. Table III shows that when using {Primary-Justify, Ln(Count)} mode, the AFM is even

more accurate (both lower BIC and RMSE) than the corresponding IFM model; while when using {Overall,
Ln(Count)} LO mode, the IFM is more accurate (both lower BIC and RMSE) than the corresponding AFM.
Moreover, the best IFM model, using {Overall, Ln(Count)} LO mode, is still better than the best AFM
which using {Primary-Justify, Ln(Count)} LO mode. Thus, cross 8 modes on both AFM and IFM, the best
prediction model is still generated by IFM but using {Overall, Ln(Count)} LO mode.
Table IV. AFM vs. IFM On Predicting Post-test
Performance by {Primary-Justify, Ln(Count)} and {Overall,
Ln(Count)} modes
Mode

Model

BIC

10-fold RMSE

{Primary-Justify, Ln(Count)}

AFM
IFM
AFM
IFM

2414
2428
2443
2252

4.6632E-01
4.6791
4.7027E-01
4.4529E-01

{Overall, Ln(Count)}

In order to compare AFM and IFM across eight modes, Wilcoxon Signed Ranks Tests were again conducted
on resulted 8 × 2 BIC and RMSE results. Result showed that IFM is marginally significant better than AFM
across eight modes: Z = −1.68, p = 0.093 for BIC and Z = −1.82, p = 0.069 for 10-fold CV RMSE
respectively. Previously, the best model for fitting the training dataset and type 1 predictions are generated
by IFM using {Primary-Justify, Ln(Count)} LOs; on the task of predicting students’ posttest performance
(type 2), however, the best model is still IFM but using {Overall, Ln(Count)} LO counts. To summarize, the
best performance of IFM is better than the best AFM and across the eight LO modes and IFM is marginally
better than AFM model on type 2 prediction.
5.

CONCLUSION

In this paper we investigated student modeling on a dataset involving multiple instructional interventions. We
proposed a cognitive model named IFM. We compared IFM with AFM and PFM on the training dataset.
We determined that including non-standard LOs such as tells and explains as a separated parameter is
effective in that the IFM models’ out-performance PFM, AFM-Tell, and AFM+Tell across all modes; but
for AFM modes, simply adding tells into AFM LO counts did not seemingly significantly improved the AFM
model’s performance. This is probably because AFM gives a same learning rate for different instructional
interventions. For example, under the {Primary, Count} mode, the Nik in AFM+Tell model is Elicit + T ell.
On one KC, KC20 , the AFM had: the learning rate γk = 0.011462. By contrast, the corresponding IFM
has three parameters: µk for benefit of previous successes on skill k; ρk is the coefficient for the benefit of
previous failures, and νk the coefficient for the benefit of previous tells on skill k. For the same KC, the
IFM resulted µk = 0.083397; ρk = −0.213746, νk = 0.031982. The values of the three parameters are quite
different from each other, which suggested the the benefit of tells is in the middle of the benefit of success
and failure. Such patterns on learned parameters between AFM and IFM showed throughout our analysis.
It suggested that rather than using one learning rate parameters for different instructional interventions, it
is better to break them into categories and learn seperated parameters.
In order to fully exploring the effectiveness of three models, we further compared them on two prediction
tasks – unseen student prediction (type 1) and unseen step prediction (type 2). Our results indicate that the
IFM model is significantly better than the AFM model on predicting unseen student’s performance on seen
steps (type 1) and marginal significant better on predicting seen students’ performance on posttest (type 2).
Additionally, we examined the impact of including pretest performance in the LOs as well as qualitative
justification steps in the LOs. We found that the Primary-Justify mode seems to be most effective. Generally
speaking, models trained with logarithmic tabulation outperformed those trained with additive tabulation

probably because the number of prior LOs counts in this study can be ralatively large. For example, the
average number of primary steps (including both elicits and tells) in the training data varies from 6 for KC1
to 83 for KC20 .
Even though IFM model performed the best on modeling the training data on both type 1 and type 2
predictions, its performance is heavily dependent upon the specific prediction task being performed and the
way in which the specific LOs were counted. For modeling the training data and type 1 prediction, it is the
best to using (Primary-Justify,Ln(Count)) mode; but for type 2 predictions, it was best to include the pretest
data as well and thus using(Overall,Ln(Count)) mode for LO counts. Thus we conclude that, for datasets
involving multiple learning interventions, IFM is a more robust choice for student and cognitive modeling.
However the performance of IFM is heavily dependent upon the specific prediction task being performed and
the way in which the specific LOs were counted. Experimenters facing a novel task should engage in some
measure of parameter-fitting to determine the best fit.
ACKNOWLEDGMENTS

NSF (#SBE-0836012) and NSF (#0325054) supported this work.
REFERENCES
Barnes, T. 2005. The q-matrix method: Mining student response data for knowledge.
Beck, J. E. and Mostow, J. 2008. How who should practice: Using learning decomposition to evaluate the efficacy of different
types of practice for different types of students. See Woolf et al. [2008], 353–362.
Cen, H., Koedinger, K. R., and Junker, B. 2006. Learning factors analysis - a general method for cognitive model evaluation
and improvement. In Intelligent Tutoring Systems, M. Ikeda, K. D. Ashley, and T.-W. Chan, Eds. Springer, 164–175.
Cen, H., Koedinger, K. R., and Junker, B. 2008. Comparing two irt models for conjunctive skills. See Woolf et al. [2008],
796–798.
Corbett, A. T. and Anderson, J. R. 1995. Knowledge tracing: Modelling the acquisition of procedural knowledge. User
Model. User-Adapt. Interact. 4, 4, 253–278.
Draney, K., Pirolli, P., and Wilson, M. 1995. A Measurement Model for a Complex Cognitive Skill. Erlbaum, Hillsdale,
NJ.
Gong, Y., Beck, J., and Heffernan, N. 2010. Comparing knowledge tracing and performance factor analysis by using multiple
model fitting procedures. In Intelligent Tutoring Systems, V. Aleven, J. Kay, and J. Mostow, Eds. Lecture Notes in Computer
Science Series, vol. 6094. Springer Berlin / Heidelberg, 35–44. 10.1007/978-3-642-13388-6 8.
Heathcote, A., Brown, S., and D.J.K., M. 2000. The power law repealed: The case for an exponential law of practice.
Psychonomic Bulletin and Review 7, 2, 185207.
Jordan, P. W., Hall, B., Ringenberg, M., Cue, Y., and Rosé, C. 2007. Tools for authoring a dialogue agent that participates
in learning studies. In AIED, R. Luckin, K. R. Koedinger, and J. E. Greer, Eds. Frontiers in Artificial Intelligence and
Applications Series, vol. 158. IOS Press, Los Angeles, California, USA, 43–50.
Newell, A. and Rosenbloom, P. 1981. Mechanisms of Skill Acquisition and the Law of Practice. Erlbaum Hillsdale NJ.
Pavlik, P. I., Cen, H., and Koedinger, K. R. 2009. Performance factors analysis –a new alternative to knowledge tracing. In
Proceeding of the 2009 conference on Artificial Intelligence in Education. IOS Press, 531–538.
Pavlik, P. I., Yudelson, M., and Koedinger, K. 2011. Using contextual factors analysis to explain transfer of least common
multiple skills.
Schwarz, G. E. 1978. Estimating the dimension of a model. Annals of Statistics. 6, 2, 461464.
Tatsuoka, K. 1983. Rule space: An approach for dealing with misconceptions based on item response theory. Journal of
Educational Measurement. 20, 4, 345–354.
VanLehn, K. 2006. The behavior of tutoring systems. International Journal Artificial Intelligence in Education 16, 3, 227–265.
VanLehn, K., Jordan, P., and Litman, D. 2007. Developing pedagogically effective tutorial dialogue tactics: Experiments
and a testbed. In Proceedings of SLaTE Workshop on Speech and Language Technology in Education ISCA Tutorial and
Research Workshop. 17–20.
Woolf, B. P., Aı̈meur, E., Nkambou, R., and Lajoie, S. P., Eds. 2008. Intelligent Tutoring Systems, 9th International
Conference, ITS 2008, Montreal, Canada, June 23-27, 2008, Proceedings. Lecture Notes in Computer Science Series, vol.
5091. Springer.

Defining the Behavior of an Affective Learning
Companion in the Affective Meta-tutor Project
Sylvie Girard, Maria Elena Chavez-Echeagaray, Javier Gonzalez-Sanchez,
Yoalli Hidalgo-Pontet, Lishan Zhang, Winslow Burleson, and Kurt VanLehn
Arizona State University, Computing, Informatics, and Decision Systems Engineering,
Tempe, AZ, 85281, U.S.A.
{sylvie.girard,helenchavez,javiergs,lzhang90,yhidalgo,
winslow.burleson,kurt.vanlehn}@asu.edu

Abstract. Research in affective computing and educational technology has
shown the potential of affective interventions to increase student’s self-concept
and motivation while learning. Our project aims to investigate whether the use
of affective interventions in a meta-cognitive tutor can help students achieve
deeper modeling of dynamic systems by being persistent in their use of metacognitive strategies during and after tutoring. This article is an experience
report on how we designed and implemented the affective intervention. (The
meta-tutor is described in a separate paper.) We briefly describe the theories of
affect underlying the design and how the agent’s affective behavior is defined
and implemented. Finally, the evaluation of a detector-driven categorization
of student behavior, that guides the agent’s affective interventions, against a
categorization performed by human coders, is presented.
Keywords: affective computing, affective learning companion, intelligent tutoring
system, robust learning, meta-cognition.

1

Introduction

Research in AIED has taken interest in the potential of using interventions of affective
nature in intelligent tutoring systems to improve learning [2, 19, 23] and motivation
[8, 13, 20] and to reduce undesirable behaviors such as gaming [3-5] and undesirable
affective states such as disengagement [17]. The interventions have been designed to
either respond to student’ specific behavior [14, 19], or to elicit a certain emotional
state in the student [9], often by providing cognitive support and scaffolds within the
learning environment.
The hypothesis of our project [24] is that affective interventions in a metacognitive tutor can help students achieve robust learning by being persistent in their
use of meta-cognitive strategies during and after tutoring. In order to test this
hypothesis, an affective intervention was designed, using an affective learning companion to convey the affective message. This article describes the design of the affective intervention. In the first section, a three-dimensional design space of affective
interventions is outlined, along with our choice along each dimension. The second
K. Yacef et al. (Eds.): AIED 2013, LNAI 7926, pp. 21–30, 2013.
© Springer-Verlag Berlin Heidelberg 2013

22

S. Girard et al.

section describes the implementation of the design using categorization of student
behavior based on log data detectors. The last section describes an empirical evaluation of the classification accuracy.

2

Design of the Affective Intervention

2.1

Definition of the Affective Intervention

Over the past decade, numerous affective interventions have been designed and evaluated with respect to alternate techniques in the field of educational technology. In
order to define a design space of the affective intervention for the AMT project, a
review of current research was performed. The design space has three dimensions:
mechanism for delivery of the affective intervention, timing of the intervention, and
type of message delivered during the intervention. We briefly describe each dimension, then indicate where along it our design falls.
Mechanism: How Is the Intervention Message Conveyed?
There are various ways to intervene affectively in tutoring systems, ranging from the
presentation of an affective message via a user-interface component [2, 19], to the use
of bio-feedback and affect-sensitive tutors that respond to the user’s emotional state
[9]. Some results [2,8,12,23] have shown the potential of using pedagogical agents, or
Affective Learning Companion (ALC), to portray the affective message. These interventions involve design decisions concerning the different components of a pedagogical agent that can impact learning, such as the presence of facial expressions or
deictic gestures [2,14], vocal intonation [6], gender [2,8,16], or ethnicity and student’s
cultural background [12,19].
In this phase of our project affective messages in the form of pop-up text messages
are provided by a pedagogical agent, represented by an image with neutral facial expression. The agent is a humanoid comic-like gendered character, representing a
student of a similar age to our target population (16-21 yrs olds). This decision took
into account the results from [12] for the agent’s image type, and [2, 23] where pairing students’ gender to the agent’s gender was found beneficial for user’s self-concept
and learning.
Timing: When Is the Affective Intervention Happening in the Learning Process?
The affective intervention can happen before any tutoring takes place, between learning
tasks during the tutoring, and at different moments while a learner is performing a
specific task or learning a specific set of skills. In order to describe when the affective
intervention occurs, we first must describe the instruction.
The AMT software teaches students how to create and test a model of a dynamic
system. The instruction is divided into three phases: (1) an introduction phase where
students learn basic concepts of dynamic system model construction and how to
use the interface; (2) a training phase where students are guided by a tutor and a
meta-tutor to create several models; and (3) a transfer phase where all scaffolding is

Defining the Behavior of an Affective Learning Companion

23

removed from software and students are free to model as they wish. The tutor gives
feedback and corrections on domain mistakes. The meta-tutor requires students to
follow a goal-reduction problem solving strategy, using the Target Node Strategy
[24], which decomposes the overall modeling problem into a series of “atomic” modeling problems whose small scope encourages students to engage in deep modeling
rather than shallow guess-based modeling strategies. Using various measures of
deep and shallow learning [5], an experiment demonstrated that requiring students to
follow this strategy during training did indeed increase the frequency of deep modeling compared to students who were not required to follow the strategy. However, the
effect was not strong, and the amount of deep modeling could certainly be improved.
The goal of the ALC is to encourage students to do even more deep modeling.
The pedagogical agent conveying the affective message in AMT intervenes at three
different moments of software interaction:
•

•

•

At the beginning and the end of the introduction: These interventions aim to
introduce the agent and its role in the instruction, as well as building rapport
between the student and the ALC which has been shown in [7] to help keep
students motivated and on task.
Between each modeling task in the training phase: The main purpose of these
interventions is to invite the student to reflect on his/her actions and decisions during the task, as well as maintain the interest of the student. As performing a given task can require from 3 to 15 minutes, the ALC intervenes
after each task rather than intervening after a pre-defined number of tasks as
in [1,2,23].
At the end of the training phase: This intervention tries to convince the student to persevere in the use of the deep modeling strategy during the forthcoming transfer phase.

Type: What Type of Message Is Given/Transmitted During the Intervention?
Finally, the third dimension of the intervention represents its affective or motivational
content: what does the ALC say and what emotional tone does it use when saying it?
Our design is based on the following policies:
•

Baylor and Kim [6] showed that a combination of cognitive and affective
interventions (the “Mentor”) led to better student self-regulation and selfefficacy than the presence of either type of intervention alone. Our meta-tutor
and tutor already provide cognitive information without affect (like the “Expert”
of [6]). To avoid boring redundancy, the ALC presents as little cognitive and
meta-cognitive content as possible (just enough to maintain context) while
presenting motivational messages (described below) in a friendly, encouraging
manner.

The content of the intervention has been designed to help low-achievers and shallow
learners get back on track and avoid gaming [3-5, 9, 19], while not interrupting highachievers who might not benefit from an affective intervention [2, 19, 23]. It involves
the following theories:

24

S. Girard et al.

•

•
•

•

Dweck’s “the mind is a muscle” theory [10]: the more you exercise your
mind, the more competent you become. Before the introduction phase, all
students read a text introducing this theory. The between-task interventions
reinforce the message by mentioning passages of the reading and referring to
how different activities help to improve the brain’s function.
Attribution theory [21]: failures should be attributed to the difficulty of the
task or lack of preparation, whereas success should be attributed to the student’s effort.
Theory of reflection [15]: Students have been found to be more receptive after completing a problem rather than during problem solving [15]. Every
time a task is finished the ALC invites students to reflect on what they have
experienced. It encourages them to replicate the action if it was positive or
to change the action if it was negative.
Use of a meta-cognitive representation of student’s modeling depth [1, 22]:
Alongside the ALC is a bar showing the depth of the student’s modeling
while working on the current task. That is, it shows the proportion of student
actions that were classified as deep, based on the detectors described in [11].
ALC messages often refer to the modeling depth bar in combination with the
other theories listed above.

The following section illustrates how we defined the ALC behavior by using learners’
prior interactions with the system.

3

Implementing the ALC’s Behavior

While students learn, their motivation and attention to detail can fluctuate. In the context of a problem solving activity requiring modeling skills, the depth of the modeling
techniques used by students can also vary. The ALC should adapt to these fluctuations, presenting different affective messages depending on the student’s recent behavior. Simply mapping the student’s behavior onto competence would not suffice, so
we defined several behavioral classifications such as “engaged,” “gaming” and “lack
of planning.” We then defined log data detectors relevant to each behavioral classification. We also paired affective messages with each behavioral classification. In
the first subsection, the detectors that measure the user’s behavior are described. The
second sub-section then describes the behavioral classification, how they were created
and how they are mapped to the detectors’ output.
3.1

How to Detect Shallow Modeling Practices?

The detectors process a stream of user interface activity (log data) and output behavioral measures. The detectors require no human intervention and run in real time,
because they will eventually be used to regulate the system’s responses to the student.
Our detectors extend the gaming detectors of [4] by including measures relevant to
depth of modeling and other constructs.

Defining the Behavior of an Affective Learning Companion

25

Nine detectors were defined. The first six detectors were based on classifying and
counting segments in the log, where a segment corresponds roughly to a correct step
in the construction or debugging of a model. Each segment holds the value of the
detector that best represents the situation, for example a student showing both a single_answer and good_method behavior would be defined as following a
good_method behavior for this segment. The output per task for each detector is a
proportion: the number of segments meeting its criteria divided by the total number of
segments in the log for the task. Based on an extensive video analysis of student’s
past actions and HCI task modeling techniques [11], six segmental detectors were
defined:
•
•
•
•
•
•

GOOD_METHOD: The students followed a deep method in their modeling.
They used the help tools1 provided appropriately including the one for planning each part of the model.
VERIFY_INFO: Before checking their step for correctness, students looked
back at the problem description, the information provided by the instruction
slides, or the meta-tutor agent.
SINGLE_ANSWER: The student’s initial response for this step was correct, and the student did not change it.
SEVERAL_ANSWERS: The student made more than one attempt at completing the step. This includes guessing and gaming the system.
UNDO_GOOD_WORK: This action suggests a modeling misconception on
the students’ part. One example is when students try to run the model when
not all of the nodes are fully defined.
GIVEUP: The student gave up on finding the answer and clicked on the
“give up” button.

A limitation of the above detectors is the inability to distinguish between a student
trying hard to complete a step but making a lot of errors versus a student gaming or
guessing a lot. This led to the development of two additional detectors based on earlier work in detecting robust learning and gaming [5, 9, 18, 23]: (1) the time spent on
task and (2) the number of times the learner misused the “run model” button. While
the former is self-explanatory and commonly used in ITSs, the latter is specific to the
AMT software. As students construct a system dynamics model, they can reach a
point where all elements are sufficiently defined to “run the model” (the model is
correct in terms of syntax) and therefore test whether its semantics corresponds to the
system they were asked to model. Students clicking on this button before the model’s
syntax is correct, or clicking repetitively on the model without making changes once
it is correct in syntax but not in semantics, is considered shallow behavior that shows
a lack of planning, a lack of understanding of the task to perform, or a tendency to
guess/game the answer rather than think it through.

1

Two help systems are available to users: (1) referring back to the instructions always available for
viewing, and (2) looking at the problem situation where all details of the dynamic system to
model are described.

26

S. Girard et al.

The ninth and last detector is a function of the six segmental detectors. It is intended to measure the overall depth of the students’ modeling. Although it is used as
an outcome measure in the transfer phase, it helps drive the ALC during the training
phase. It is based on considering two measures (GOOD_ANSWER, VERIFY_INFO)
to indicate deep modeling, one measure (SINGLE_ANSWER) to be neutral, and three
measures (SEVERAL_ANSWERS, UNDO_GOOD_WORK, and GIVE_UP) to indicate shallow modeling.
In order to facilitate writing rules that defined the students’ behavioral category
(e.g., engaged, gaming, etc.) in terms of the detector outputs, we triaged the output of
each detector so it reports its output as either low, medium and high. The rules are
mostly driven by the values: low and high. To implement the triage, we collected
logs from 23 students. For each of the nine detectors, we determine the 33rd and 66th
percentile points and used them as thresholds. Thus, for each detector, roughly a
third of the 23 students were reported as low, as medium and as high. Because the
tasks vary in complexity, different thresholds were calculated for each task.
3.2

From Shallow Learning Detection to the ALC Intervention

A series of 6 types of ALC behavioral categories were defined using video analysis of
past user’s actions on software. Human coders reviewed screen-capture videos and
verbal protocols of a pool of 20 students using the meta-cognitive tutor. Following
their recommendations and a review of messages transmitted in affective interventions in the literature, the following set of ALC categories was defined:
• Good Modeling: The students think about their steps, do not hesitate to go
back to the introduction or the situation to look for answers, use the plan feature judiciously in their creation of nodes, and have a minimum of guessing and wrong actions
performed on task.
• Engaged: The students respond by thinking about the problem rather than
guessing, refer back to the instructions or problem situation when they find themselves stuck rather than trying all possible answers. The students take a medium to a
high amount of time to complete the task, favoring reflection to quick decisions.
• Lack of Planning: The students answer quickly, relying heavily on the feedback given in the interface to get the next steps. While the students sometimes refer to
instructions and the situation, they only use the features when they are stuck, not
when planning the modeling activity.
• Help Avoidance: The students attempt a lot of answers without referring back
to the instructions or the problem situation. They rarely make use of the information
filled in the plan tab and try to skip the meta-tutor instructions. Instead of using help
when they are confused, they spend a lot of time trying to get the interface green or
give up rather than thinking about the problem.
• Gaming: The students try multiple possible answers within the interface
without pausing long enough to think about the problem. They may give up when this
random guessing doesn't work. They rarely refer to the instructions or the problem
situation and pay little attention to the plan tab or the meta-tutor instructions.
• Shallow Modeling (default, not recognized as the above mentioned categories): The students tend to try several answers on the interface rather than pausing
and thinking about the problem. They sometimes refer back to the instructions and
problem situation, but not frequently.

Defining the Behavior of an Affective Learning Companion

27

Table 1. Examples of ALC intervention between-task
Behavior
Good Modeling
Engaged

Lack of Planning

Help Avoidance

Gaming

Shallow Modeling
(default)

Example
You’re a Green Master! What was your secret? I know… you make your reading
count and thus your brain is getting rewired.
Even though it might take a little bit longer, it is worth it to explore the available
resources. You are giving your brain a great workout. Look at that green bar! Keep
up the good work!
Going fast is good, but it doesn't always help you reach your potential… Why don't
you stop and think about what you want to model when you are confused. To
make more of the bar green, try re-reading the problem description and noting what
it asks you to do.
It might be worth rereading the problem description and paying more attention to
the suggestions presented by the pop-up messages. Our brain needs to engage the
material deeply so it can create good connections. That’s how we can get more of
the bar green!
Hmmm! It seems that you need to put quality time into your tasks. Maybe "trial
and error" is not always the best strategy. Although you might move quickly
through the problem, your brain doesn’t get a workout, and it shows in the length
of the green bar.
You are getting there! Look at that bar! But remember that to strengthen your
brain you have to engage the problem and all its details.

Once these six behaviors were defined, human coders applied them to a sample of
100 tasks and students. The outputs of the detectors on the sample were obtained,
and rules were defined to map their values to the behavioral categories.
Using the theories of affect defined in section 2, ALC messages were created for
each behavior in order to provide affective support to the learner. A stereotypical
message was first created, as illustrated in table 1, for each behavior. The research
group then created many synonymous versions of each message, so that the ALC
would not repeat itself and thus reduce the student’s perception of the ALC as an
artificial agent. A separate message was produced for the first and last task performed by the user in the training phase, in order to introduce and wrap-up the ALC
interventions.

4

Evaluation of the Behavior’s Accuracy

Before working with students, we first tested the detectors and behavioral categorizer
via test cases. We wrote scenarios of software use that typified each of the six
behavioral categories. A member of the research group enacted each scenario, and
we confirmed that the detector outputs fell in the anticipated range (low, typical or
high) and that the rules assigned the anticipated behavioral classification.
The second part of the validation of ALC behaviors involved pilot subjects and
human coders. Seven college students used the AMT system with the ALC turned on.
They were asked to speak aloud as they worked. Their voice and screen were recorded as videos. A sample video was made from screen recordings. It included 15
tasks. Three human coders watched each task, paying attention to the depth of modeling shown by the student’s actions. Independently of what the software chose, they

28

S. Girard et al.

chose the ALC intervention that they felt best matched the student’s modeling practices. A multi-rater and pairwise kappa was then performed, and showed a sufficient
level of inter-reliance with a level of .896.

5

Conclusion and Future Work

This article described the development of an affective intervention based on an affective learning companion (ALC) that works with a meta-tutor and a tutor. It described
the theories of affect underlying the interventions, and how we defined and implemented the ALC’s behavior. The ALC’s messages were based on deciding which of
six behavioral categories best represented the student’s work on the most recently
completed task. This categorization was driven by log data. When compared to
human coders working with screen captures and verbal reports of students, the detector-driven categorizations agreed with the human coding with a kappa of .896.
The next step in the research is to measure the benefits of this version of the ALC
in a two-condition experiment. One group of students will use the system with the
ALC turned on during the training phase, and the other will used it without the ALC
turned on. We hypothesize that this will cause measurable differences in the depth of
students’ modeling during the transfer phase.
The forthcoming evaluation will also have students wear physiological sensors
while they work so that we can collect calibration data that will be used to supplement
the detectors’ assessment of the students’ affective state. This extra information will
be used to help define affective interventions not only between tasks but also while
the learner performs on task.
Acknowledgements. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References
1. Arroyo, I., Ferguson, K., Johns, J., Dragon, T., Meheranian, H., Fisher, D., et al.: Repairing disengagement with non-invasive interventions. Frontiers in Artificial Intelligence and
Applications, vol. 158, p. 195 (2007)
2. Arroyo, I., Woolf, B.P., Cooper, D.G., Burleson, W., Muldner, K.: The Impact of Animated Pedagogical Agents on Girls’ and Boys’ Emotions, Attitudes, Behaviors and Learning. In: Proceedings of the 2011 IEEE 11th International Conference on Advanced Learning Technologies. Proceedings from ICALT 2011, Washington, DC, USA (2011)
3. Baker, R.S.J.d., et al.: Adapting to when students game an intelligent tutoring system. In:
Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS, vol. 4053, pp. 392–401.
Springer, Heidelberg (2006)
4. Baker, R.S.J.d., Gowda, S.M., Corbett, A.T.: Towards predicting future transfer of learning. In: Biswas, G., Bull, S., Kay, J., Mitrovic, A. (eds.) AIED 2011. LNCS, vol. 6738, pp.
23–30. Springer, Heidelberg (2011)

Defining the Behavior of an Affective Learning Companion

29

5. Baker, R.S.J.d., Gowda, S.M., Corbett, A.T., Ocumpaugh, J.: Towards automatically detecting whether student learning is shallow. In: Cerri, S.A., Clancey, W.J., Papadourakis,
G., Panourgia, K. (eds.) ITS 2012. LNCS, vol. 7315, pp. 444–453. Springer, Heidelberg
(2012)
6. Baylor, A.L., Kim, Y.: Simulating instructional roles through pedagogical agents. International Journal of Artificial Intelligence in Education 15(2), 95–115 (2005)
7. Bickmore, T.W., Picard, R.W.: Establishing and maintaining long-term human-computer
relationships. ACM Transactions on Computer-Human Interaction (TOCHI) 12(2), 293–
327 (2005)
8. Burleson, W., Picard, R.W.: Gender-Specific Approaches to Developing Emotionally Intelligent Learning Companions. IEEE Intelligent Systems 22(4), 62–69 (2007),
doi:10.1109/MIS.2007.69
9. D’Mello, S.K., Lehman, B., Person, N.: Monitoring affect states during effortful problem
solving activities. International Journal of Artificial Intelligence in Education 20(4), 361–
389 (2010), doi:10.3233/JAI-2010-012
10. Dweck, C.: Self-Theories: Their role in motivation, personality and development. Psychology Press, Philadelphia (2000)
11. Girard, S., Zhang, L., Hidalgo-Pontet, Y., VanLehn, K., Burleson, W., Chavez-Echeagary,
M.E., Gonzalez-Sanchez, J.: Using HCI task modeling techniques to measure how deeply
students model. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P. (eds.) AIED 2013.
LNCS (LNAI), vol. 7926, pp. 766–769. Springer, Heidelberg (2013)
12. Gulz, A.: Benefits of Virtual Characters in Computer Based Learning Environments:
Claims and Evidences. International Journal of Artificial Intelligence in Education 14(3),
313–334 (2004)
13. Gulz, A., Haake, M., Silvervarg, A.: Extending a teachable agent with a social conversation module – effects on student experiences and learning. In: Biswas, G., Bull, S., Kay, J.,
Mitrovic, A. (eds.) AIED 2011. LNCS, vol. 6738, pp. 106–114. Springer, Heidelberg
(2011)
14. Hayashi, Y.: On pedagogical effects of learner-support agents in collaborative interaction.
In: Cerri, S.A., Clancey, W.J., Papadourakis, G., Panourgia, K. (eds.) ITS 2012. LNCS,
vol. 7315, pp. 22–32. Springer, Heidelberg (2012)
15. Katz, S., Connelly, J., Wilson, C.: Out of the lab and into the classroom: An evaluation of
reflective dialogue in Andes. In: Proceeding of the 2007 Conference on Artificial Intelligence in Education: Building Technology Rich Learning Contexts That Work, pp. 425–
432 (2007)
16. Kim, Y., Baylor, A., Shen, E.: Pedagogical agents as learning companions: the impact of
agent emotion and gender. Journal of Computer Assisted Learning 23(3), 220–234 (2007)
17. Lehman, B., D’Mello, S., Graesser, A.: Interventions to regulate confusion during Learning. In: Cerri, S.A., Clancey, W.J., Papadourakis, G., Panourgia, K. (eds.) ITS 2012.
LNCS, vol. 7315, pp. 576–578. Springer, Heidelberg (2012)
18. Muldner, K., Burleson, W., Van de Sande, B., VanLehn, K.: An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User Modeling and
User-Adapted Interaction 21(1-2), 99m–135m (2011), doi:10.1007/s11257-010-9086-0
19. Rodrigo, M.M.T., Baker, R.S.J.d., Agapito, J., Nabo, J., Repalam, M.C., Reyes, S.S., San
Pedro, M.O.C.Z.: The Effects of an Interactive Software Agent on Student Affective Dynamics while Using an Intelligent Tutoring System. IEEE Transactions on Affective Computing 3, 224–236 (2012), doi:http://doi.ieeecomputersociety.org/10.
1109/T-AFFC.2011.41

30

S. Girard et al.

20. Wang, N., Johnson, W.L., Mayer, R.E., Rizzo, P., Shaw, E., Collins, H.: The politeness effect: Pedagogical agents and learning outcomes. International Journal of Human-Computer
Studies 66(2), 98–112 (2008), doi:10.1016/j.ijhcs.2007.09.003
21. Weiner, B.: An attributional theory of achievement motivation and emotion. Psychological
Review 92(4), 548 (1985)
22. Walonoski, J.A., Heffernan, N.T.: Prevention of off-task gaming behavior in intelligent tutoring systems. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 722–724. Springer, Heidelberg (2006)
23. Woolf, B.P., Arroyo, I., Muldner, K., Burleson, W., Cooper, D.G., Dolan, R., Christopherson, R.M.: The Effect of Motivational Learning Companions on Low Achieving Students
and Students with Disabilities. In: Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010, Part I.
LNCS, vol. 6094, pp. 327–337. Springer, Heidelberg (2010)
24. Zhang, L., Burleson, W., Chavez-Echeagaray, M.E., Girard, S., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., VanLehn, K.: Evaluation of a meta-tutor for constructing models of dynamic systems. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P. (eds.) AIED 2013.
LNCS (LNAI), vol. 7926, pp. 666–669. Springer, Heidelberg (2013)

Modeling Students’ Reasoning About
Qualitative Physics:
Heuristics for Abductive Proof Search
Maxim Makatchev, Pamela W. Jordan, and Kurt VanLehn
Learning Research and Development Center, University of Pittsburgh
{maxim,pjordan,vanlehn}@pitt.edu

Abstract. We describe a theorem prover that is used in the Why2Atlas tutoring system for the purposes of evaluating the correctness of
a student’s essay and for guiding feedback to the student. The weighted
abduction framework of the prover is augmented with various heuristics
to assist in searching for a proof that maximizes measures of utility and
plausibility. We focus on two new heuristics we added to the theorem
prover: (a) a speciﬁcity-based cost for assuming an atom, and (b) a rule
choice preference that is based on the similarity between the graph of
cross-references between the propositions in a candidate rule and the
graph of cross-references between the set of goals. The two heuristics are
relevant to any abduction framework and knowledge representation that
allow for a metric of speciﬁcity for a proposition and cross-referencing of
propositions via shared variables.

1
1.1

Introduction
Why2-Atlas Overview

The Why2-Atlas tutoring system is designed to encourage students to write their
answers to qualitative physics problems along with detailed explanations to support their arguments [1]. For the purpose of eliciting more complete explanations
the system attempts to provide students with substantive feedback that demonstrates understanding of a student’s essay. A sample problem and a student’s
explanation for it is shown in Figure 1.
The sentence level understanding module in Why2-Atlas parses a student’s
essay into a ﬁrst-order predicate representation [2]. The discourse-level understanding module then resolves temporal and nominal anaphora within the representation [3] and uses a theorem prover that attempts to generate a proof,
treating propositions in the resolved representation as a set of goals, and the
problem statement as a set of given facts. An informal example proof for a fragment of the essay in Figure 1 is shown in Figure 2. The proof is interpreted as
a model of the reasoning the student used to arrive at the arguments in the essay, and provides a diagnosis when the arguments are faulty in a fashion similar
to [4,5]. For example, the proof in Figure 2 indicates that the student may have
J.C. Lester et al. (Eds.): ITS 2004, LNCS 3220, pp. 699–709, 2004.
c Springer-Verlag Berlin Heidelberg 2004


700

M. Makatchev, P.W. Jordan, and K. VanLehn

Question: Suppose a man is in a free-falling elevator and is holding his keys motionless
right in front of his face. He then lets go. What will be the position of the keys relative
to the man’s face as time passes? Explain.
Explanation: The keys are aﬀected by gravity which pulls them to the elevator
ﬂoor, because the keys then have a combined velocity of the freefall and the eﬀect
of gravity. If the elevator has enough speed the keys along with my head would be
pressed against the ceiling of the elevator, because the acceleration of the elevator car
along with me and the keys would overwhelm the gravitational pull.
Fig. 1. The statement of the problem and an example explanation.
Step # Proposition
1
before the release, the keys have been in contact with
the man, and the man has been in contact with the elevator
2
at the moment of release, velocity of the keys is equal
to velocity of the elevator
3
after the release, nothing is touching the keys
4
after the release, the keys are in freefall
5
6
7

after the release, the keys’ acceleration is not equal
to the elevator’s acceleration
after the release, the keys’ velocity is not equal
to the elevator’s velocity
the keys touch the ceiling of the elevator

Justiﬁcation
given
bodies in contact over a time
interval have same velocities
given
if there is no any contact then
the body is in freefall
*elevator is not in freefall
if initial velocity is the same
and accelerations are diﬀerent
the ﬁnal velocities are diﬀerent
if the keys’ velocity is smaller
than the elevator’s velocity,
the keys touch the ceiling

Fig. 2. An informal proof of the excerpt “The keys would be pressed against the ceiling
of the elevator” (From the essay in Figure 1). The buggy assumption is preceded by
an asterisk.

wrongly assumed that the elevator is not in freefall. A highly plausible wrong
assumption in the student’s reasoning triggers an appropriate tutoring action [6].
The theorem prover, called Tacitus-lite+, is a derivative of SRI’s Tacituslite that, among other extensions, incorporates sorts (sorts will be described in
Section 2.3) [7, p. 102]. We further adapted Tacitus-lite+ to our application by
(a) adding meta-level consistency checking, (b) enforcing a sound order-sorted
inference procedure, and (c) expanding the proof search heuristics. In the rest of
the paper we will refer to the prover as Tacitus-lite when talking about features
present in the original SRI release, and as Tacitus-lite+ when talking about more
recent extensions.
The goal of the proof search heuristics is to maximize (a) the measure of
plausibility of the proof as a model of a student’s reasoning and (b) the measure
of utility of the proof for generating tutoring feedback. The measure of plausibility can be evaluated with respect to the misconceptions that were identiﬁed as
present in the essay by the prover and by a human expert. A more precise plausibility measure may take into account plausibility of the proof as a whole. The
measure of utility for the tutoring task can be interpreted in terms of relevance

Modeling Students’ Reasoning About Qualitative Physics

701

of the tutoring actions (triggered by the proof) to the student’s essay, whether
the proof was plausible or not.
A previous version of Tacitus-lite+ was evaluated as part of the Why2-Atlas
evaluation studies, as well as on its own. The stand-alone evaluation uses manually constructed propositional representation of essays, to measure the performance of the theorem prover (in terms of the recognition of misconceptions in
the essay) on ‘gold’ input [8]. The results of the latter evaluation were encouraging enough for us to continue development of the theorem proving approach
for essay analysis.
1.2

Related Work

In our earlier paper [9] we argued that statistical text classiﬁcation approaches
that treat text as an unordered bag of words (e.g. [10,11]) do not provide a
suﬃciently deep understanding of the logical structure of the student’s essay
that is essential for our application. Structured models of conceptual knowledge,
including those based on semantic networks and expert systems, are described
in [12]. Another structured model, Bayesian belief networks, is a popular tool for
learning and representing student models [13,14]. By appropriately choosing the
costs of propositions in rules, weighted abductive proofs can be interpreted as
Bayesian belief networks [15,4]. In general, the costs of propositions in abductive
theorem proving do not have to adhere to probabilistic semantics, providing
greater ﬂexibility while also eliminating the need to create a proper probability
space. On the other hand, the task of choosing a suitable cost semantics in
weighted abduction remains a diﬃcult problem and it is out of scope of this
paper.
Theorem provers have been used in tutoring systems for various purposes, e.g.
for building the solution space of a problem [16] and for question answering [17],
to mention a few. Student modeling from the point of view of formal methods
is reviewed in [18]. An interactive construction of a learner model that uses a
theorem proving component is described in [19].
In this paper we focus on the recent additions to the set of proof search
heuristics for Tacitus-lite+: a speciﬁcity-sensitive assumption cost and a rule
choice preference that is based on the similarity between the graph of crossreferences between the propositions in a candidate rule and the graph of crossreferences between the set of goals. The paper is organized as follows: Section 2
introduces knowledge representation aspects of the prover; Section 3 deﬁnes the
order-sorted abductive inference framework and describes the new proof search
heuristics; ﬁnally, a summary is given in Section 4.

2

Knowledge Representation for Qualitative Mechanics

In addition to the domain knowledge that is normally represented in qualitative
physics frameworks (e. g. [20]), a natural language tutoring application requires

702

M. Makatchev, P.W. Jordan, and K. VanLehn

a representation of possibly erroneous student beliefs that captures the diﬀerences between beliefs expressed formally and informally, as allowed by natural
language. The process of building a formal representation of the problem can be
described in terms of envisionment and idealization.
2.1

Envisionment and Idealization

The internal (mental) representation of the problem plays a key role in problem solving among both novices and experts [21,22]. The notion of an internal
representation, described in [22] as “objects, operators, and constraints, as well
as initial and ﬁnal states,” overlaps with the notion of envisionment [23], i.e.
the sequence of events implied by the problem statement. While envisionment
can be expressed as a sequence of events in common-sense terms, a further step
towards representing the envisionment in formal physics terms (bodies, forces,
motion) is referred to as idealization [8].
For example, consider the problem in Figure 1. A possible envisionment is:
(1) the man is holding the keys (elevator is falling); (2) the man releases the keys;
(3) the keys move up with respect to the elevator and hit the elevator ceiling.
The idealization would be:
Bodies: Keys, Man, Elevator, Earth.
Forces: Gravity, Man holding keys
Motion: Keys’ downward velocity is smaller than the downward velocity of
the elevator.
Because envisionment and idealization are important stages for constructing
an internal representation, they fall under the scope of Why2-Atlas’ tutoring.
However, reasoning about the multitude of possible envisionments would require
adding an extensive amount of common-sense knowledge to the system. To bypass this diﬃculty, we consider problems that would typically have few likely
envisionments. Fortunately (for the knowledge engineers), there is a class of interesting qualitative physics problems that falls into this category. We therefore
developed a knowledge representation that is capable of representing common
correct and erroneous propositions at both the levels of envisionment and idealization.
2.2

Qualitative Mechanics Ontology

The ontology is designed to take advantage of the additional capability provided
by an order-sorted language (described in Section 2.3). Namely, constants and
variables, corresponding to physical quantities (e. g. force, velocity), physical
bodies (man, earth) and agents (air) are associated with a sort symbol. The
domains of the predicate symbols are restricted to certain sorts (so that each
argument position has a corresponding sort symbol). These associations and
constraints constitute an order-sorted signature [24].
The ontology consists of the following main concept classes: bodies, physical
quantities, states, time, relations, as well as their respective slot-ﬁller concepts.
For details of the ontology we refer the reader to [8].

Modeling Students’ Reasoning About Qualitative Physics

703

((acceleration a1 keys vertical ?d-mag
?d-mag-num nonzero ?mag-num neg ?dir-num ?d-dir ?t1 ?t2)
(Quantity1b Id Regular-body Axial D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))
((due-to d1 a1 ph1) (Due-to Id Id Id))
((phenomenon ph1 gravity) (Phenomenon Id Field-interaction))
Fig. 3. Representation for “The keys have a downward acceleration due to gravity.”
The atoms are paired with their sorted signatures.

2.3

Order-Sorted First-Order Predicate Language

We adopted ﬁrst-order predicate logic with sorts [24] as the representation language. Essentially, it is a ﬁrst-order predicate language that is augmented with
an order-sorted signature for its terms and predicate argument places. For the
sake of computational eﬃciency and since function-free clauses are the natural
output of the sentence-level understanding module (see Section 1), we do not
implement functions, instead we use cross-referencing between atoms by means
of shared variables. There is a single predicate symbol Mi for each i-place relation. For this reason predicate symbols are omitted in the actual representation.
Each atom is indexed with a unique identiﬁer, a constant of sort Id. The identiﬁers, as well as variable names, can be used for cross-referencing between atoms.
For example, the proposition “The keys have a downward acceleration due to
gravity” is represented as shown in Figure 3, where a1, d1, and ph1 are atom
identiﬁers. For this example we assume (a) a ﬁxed coordinate system, with a
vertical axis pointing up (thus Dir value is neg); (b) that the existence of an
acceleration is equivalent to existence of a nonzero acceleration (thus Mag-zero
value is nonzero).
2.4

Rules

As we mentioned in Section 2.1, it is important to have rules about both envisionment and idealization when modeling students’ reasoning. The idealization
of the canonical envisionment is represented as a set of givens for the theorem
prover, namely rules of the form → a. A student’s reasoning may contain false
facts, including an erroneous idealization and envisionment, and erroneous inferences. The former are represented via buggy givens and the latter are represented
via buggy rules. Buggy rules normally have their respective correct counterparts
in the rule base. Certain integrity constraints apply when a student model is
generated, based on the assumption that the student is unlikely to use correct
and buggy versions of a rule (or given) within the same argument.
An example of a correct rule, stating that “if the velocity of a body is zero
over a time interval then its initial position is equal to its ﬁnal position”, is
shown in Figure 4. Note that the rules are extended Horn clauses, namely the
head of the rule is an atom or a conjunction of multiple atoms.

704

M. Makatchev, P.W. Jordan, and K. VanLehn

((velocity v1 ?body ?comp ?d-mag
?d-mag-num zero ?mag-num ?dir ?dir-num ?d-dir ?t1 ?t2)
(Quantity1b Id Body Comp D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))
→

((position p1 ?body ?comp ?d-mag1
?d-mag-num1 ?mag-zero1 ?mag-num1 ?dir1 ?dir-num1 ?d-dir1 ?t1 ?t1)
(Quantity1b Id Body Comp D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))
((position p2 ?body ?comp ?d-mag1
?d-mag-num1 ?mag-zero1 ?mag-num1 ?dir1 ?dir-num1 ?d-dir1 ?t2 ?t2)
(Quantity1b Id Body Comp D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))
Fig. 4. Representation for the rule “If the velocity of a body is zero over a time interval
then its initial position is equal to its ﬁnal position.”

3
3.1

Abductive Reasoning
Order-Sorted Abductive Logic Programming

Similar to [25] we deﬁne the abductive logic programming framework as a triple
T, A, I, where T is the set of givens and rules, A is the set of abducible atoms
(potential hypotheses) and I is a set of integrity constraints. Then an abductive
explanation of a given set of sentences G (observations) consists of (a) subset
∆ of abducibles A such that T ∪ ∆  G and T ∪ ∆ satisﬁes I together with
(b) the corresponding proof of G. Since an abductive explanation is generally
not unique, various criteria can be considered for choosing the most suitable
explanation (see Section 3.2).
An order-sorted abductive logic programming framework T  , A , I   is an abductive logic programming framework with all atoms augmented with the sorts
of their argument terms (so that they are sorted atoms) [8]. Assume the following notation: a sorted atom is of the form p(x1 , . . . , xn ) : (τ1 , . . . , τn ), where the
term xi is of the sort τi . Then, in terms of unsorted predicate logic, formula
∃x p(x) : (τ ) can be written as ∃x p(x) ∧ τ (x). For our domain we restrict the
sort hierarchy to a tree structure that is naturally imposed by set semantics and
that has the property ∃x τi (x) ∧ τj (x) → (τi  τj ) ∨ (τj  τi ), where τi  τj is
equivalent to ∀x τi (x) → τj (x).
Tacitus-lite+ does backward chaining using the order-sorted version of modus
ponens:
q(x , z  ) : (τ5 , τ6 )
p(x, y) : (τ1 , τ2 ) ← q(x, z) : (τ3 , τ4 )
τ5  τ 3 , τ 6  τ 4
p(x , y  ) : (min(τ5 , τ1 ), τ2 )

(1)

Modeling Students’ Reasoning About Qualitative Physics

3.2

705

Proof Search Heuristics

In building a model of the student’s reasoning, our goal is to simultaneously
increase a function of measures of utility and plausibility. The utility measure
is an estimate of the utility of the choice of a particular proof for the tutoring
application given a plausibility distribution on a set of alternative proofs. The
plausibility measure indicates which explanation is the most likely.
For example, even if a proof does not exactly coincide with the reasoning the
student used to arrive at a particular conclusion that she stated in her essay, the
proof may be of a high utility value, provided it correctly indicates the presence of
certain misconceptions in the student’s reasoning. However, generally plausible
explanations have a high utility value and we deploy a number of heuristics to
increase the plausibility of the proof.
Weighted abduction. One of the characteristic properties of abduction is that
atoms can be assumed as hypotheses, without proof. Normally it is required
that the set of assumptions is minimal, in the sense that no proper subset of it
is suﬃcient to explain the observation (or, in other words, to prove the goals).
While this preference allows us to compare two explanations when one is a subset
of another, weighted abduction provides a method to grade explanations so we
can compare two arbitrary explanations.
Tacitus-lite extends the weighted abductive inference algorithm described in
[26] for the case where rules are expressed as Horn clauses to the case where
rules are expressed as extended Horn clauses, namely the head of a rule is an
atom or a conjunction of atoms. Each conjunct pi from the body of the rule has
a weight wi associated with it:
wm
1
pw
1 ∧ · · · ∧ pm → r 1 ∧ · · · ∧ r n

The weight is used to calculate the cost of abducing pi , instead of proving
it, via the formula cost(pi ) = cost(g) · wi , where g is the goal atom that has
been proved via the rule at a preceding step (by unifying, say, with atom rj ).
The costs of the observations are supplied with the observations as input to the
prover.
Given a subgoal or observation atom to be proven, Tacitus-lite takes one of
three actions; (a) assumes the atom at the cost associated with it; (b) uniﬁes it
with an atom that is either a fact or has already been proven or is another goal
(in the latter case the cost of the resultant atom is counted once in the total cost
of the proof, as the minimum of the two costs); (c) attempts to prove it with a
rule. Tacitus-lite calls the action (b) factoring.
To account for the fact that in the order-sorted abductive framework a rule
can generate new goals of various speciﬁcity (depending on the goals that were
uniﬁed with the head of the rule), we adjust the weight of the assumed atom
according to the sorts of its terms: a more general statement is less costly to
assume, but a more speciﬁc statement is more costly. For example, the rule

706

M. Makatchev, P.W. Jordan, and K. VanLehn

from Figure 4 can be applied to prove the goal “(Axial, or total) position of
?body3 has magnitude ?mag-num3”:

((position p3 ?body3 ?comp3 ?d-mag3
?d-mag-num3 ?mag-zero3 ?mag-num3 ?dir3 ?dir-num3 ?d-dir3 ?t3 ?t3)
(Quantity1b Id Body Axial D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time)),

which generates the subgoal “(Axial or total) velocity of ?body3 is zero”:

((velocity v2 ?body3 ?comp3 ?d-mag2
?d-mag-num2 zero ?mag-num2 ?dir2 ?dir-num2 ?d-dir2 ?t3 ?t4)
(Quantity1b Id Body Axial D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))

The same rule can be applied to prove the more speciﬁc goal “Horizontal
position of ?body3 has magnitude ?mag-num3”:

((position p4 ?body3 horizontal ?d-mag3
?d-mag-num3 ?mag-zero3 ?mag-num3 ?dir3 ?dir-num3 ?d-dir3 ?t3 ?t3)
(Quantity1b Id Body Axial D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time)),

and will generate the more speciﬁc subgoal “Horizontal velocity of ?body3 is
zero”:

((velocity v3 ?body3 horizontal ?d-mag2
?d-mag-num2 zero ?mag-num2 ?dir2 ?dir-num2 ?d-dir2 ?t3 ?t4)
(Quantity1b Id Body Axial D-mag
D-mag-num Mag-zero Mag-num Dir Dir-num D-dir Time Time))

Since the variables are assumed to be existentially quantiﬁed, in accordance
with the sort semantics (see Section 3.1), the latter, more speciﬁc subgoal implies
the former subgoal. Also, according to the ordered version of modus ponens (1),
more rules can be used to prove the more general atom, increasing the chances
for the atom to be proven, rather than assumed. These considerations suggest
that it should be less costly to assume more general atoms than more speciﬁc
atoms. The cost adjustment for the assumptions is implemented by computing
a metric of speciﬁcity for the sorted signature of each assumed atom.
Rule choice heuristics. Although the rules in Tacitus-lite are applied to prove
individual goal atoms, a meaningful proposition usually consists of a few atoms
cross-referenced via shared variables (see Section 2.3). When a rule is used to
prove a particular goal atom, (a) a uniﬁer is applied to the atoms in the head
and the body of the rule; (b) atoms from the head of the rule are added to the
list of proven atoms; and (c) atoms from the body of the rule are added to the
list of goals. Consequently, suppose there exists a uniﬁer θ that uniﬁes both (a)
a goal atom g1 with an atom r1 from the head of the rule R : p1 ∧ p2 → r1 ∧ r2
so that g1 can be proved with R via modus ponens, and (b) a goal atom g2

Modeling Students’ Reasoning About Qualitative Physics

707

with an atom r2 from the head of the rule R so that g2 can be proved via R.
Then, proving goal g1 via R (and applying θ to g1 and r1 ) adds the atom r2 θ
to the list of provens thus allowing for its potential factoring with goal g2 . In
eﬀect, a single application of a rule in which its head atoms match multiple goal
atoms can result in proving multiple goal atoms via a number of subsequent
factoring steps. This property of the prover is consistent (a) with backchaining
using modus ponens (1), and (b) with the intuitive notion of cognitive economy,
namely that the shortest (by the total number of rule applications) proofs are
usually considered good by domain experts.
Moreover, if an atom p1 θ in the body of R can be uniﬁed with a goal g3 θ,
then the application of rule R will probably not result in an increase of the total
cost of the goals due to the new goal p1 θ, since it is possible to factor it with
g3 θ and set the cost of the resultant atom as the minimum of the costs of p1 θ
and g3 θ. In other words, applying a rule where multiple atoms in its head and
body match multiple goal atoms is likely to result in a faster reduction of the
goal list, and therefore a shorter ﬁnal proof.
The new version of Tacitus-lite+ extends the previous rule choice heuristics
described in [9] with rule choice based on the best match between the set of
atoms in a candidate rule and the set of goal atoms. To account for the structure
of cross-references between the atoms, a labeled graph is constructed oﬄine for
every rule, so that the atoms are vertices labeled with respective sorted signatures
and the cross-references are edges labeled with pairs of respective argument
positions. Similarly a labeled graph is built on-the-ﬂy for the current set of goal
atoms. The rule choice procedure involves comparison of the goal graph and
graphs of candidate rules so that the rule that maximizes the graph matching
metric is preferred.
The match metric between two labeled graphs is based on the size of the
largest common subgraph (LCSG). We have implemented the decision-tree-based
LCSG algorithm proposed in [27]. The advantage of this algorithm is that the
time complexity of its online stage is independent of the size of the rule graph:
if n is the number of vertices in the goal graph, then the time complexity of the
LCSG is O(2n n3 ).
Since the graph matching includes independent subroutines for matching vertices (atoms with sorted signatures) and matching edges (cross-referenced atom
arguments), the precision of both match subroutines can be varied to balance the
trade-oﬀ between search precision and eﬃciency of the overall matching procedure. Currently we are evaluating the performance of the theorem prover under
various settings.

4

Conclusion

We described an application of theorem proving for analyzing student’s essays
in the context of an interactive tutoring system. While formal methods have
been applied to student modeling, there are a number of challenges to overcome:
representing varying levels of formality in student language, the limited scope of

708

M. Makatchev, P.W. Jordan, and K. VanLehn

the rule base, and limited resources for generating explanations and consistency
checking. In our earlier paper [9] we argued that a weighted abduction theorem
proving framework augmented with appropriate proof search heuristics provides
a necessary deep-level understanding of a student’s reasoning. In this paper we
describe the recent additions to our proof search heuristics that have the goal of
improving the plausibility of the proofs as models of students’ reasoning as well
as the computational eﬃciency of the proof search.
Acknowledgments. This work was funded by NSF grant 9720359 and ONR
grant N00014-00-1-0600. We thank the entire Natural Language Tutoring group,
in particular Michael Ringenberg and Roy Wilson for their work on Tacitus-lite+,
and Uma Pappuswamy, Michael Böttner, and Brian ‘Moses’ Hall for their work
on knowledge representation and rules.

References
1. VanLehn, K., Jordan, P., Rosé, C., Bhembe, D., Böttner, M., Gaydos, A.,
Makatchev, M., Pappuswamy, U., Ringenberg, M., Roque, A., Siler, S., Srivastava, R.: The architecture of Why2-Atlas: A coach for qualitative physics essay
writing. In: Proceedings of Intelligent Tutoring Systems Conference. Volume 2363
of LNCS., Springer (2002) 158–167
2. Rosé, C., Roque, A., Bhembe, D., VanLehn, K.: An eﬃcient incremental architecture for robust interpretation. In: Proceedings of Human Language Technology
Conference, San Diego, CA. (2002)
3. Jordan, P., VanLehn, K.: Discourse processing for explanatory essays in tutorial applications. In: Proceedings of the 3rd SIGdial Workshop on Discourse and
Dialogue. (2002)
4. Poole, D.: Probabilistic Horn abduction and Bayesian networks. Artiﬁcial Intelligence 64 (1993) 81–129
5. Young, R.M., O’Shea, T.: Errors in children’s subtraction. Cognitive Science 5
(1981) 153–177
6. Jordan, P., Makatchev, M., Pappuswamy, U.: Extended explanations as student
models for guiding tutorial dialogue. In: Proceedings of AAAI Spring Symposium
on Natural Language Generation in Spoken and Written Dialogue. (2003) 65–70
7. Hobbs, J., Stickel, M., Martin, P., Edwards, D.: Interpretation as abduction. In:
Proc. 26th Annual Meeting of the ACL, Association of Computational Linguistics.
(1988) 95–103
8. Makatchev, M., Jordan, P.W., VanLehn, K.: Abductive theorem proving for analyzing student explanations to guide feedback in intelligent tutoring systems. To
appear in Journal of Automated Reasoning, Special issue on Automated Reasoning
and Theorem Proving in Education (2004)
9. Jordan, P., Makatchev, M., VanLehn, K.: Abductive theorem proving for analyzing
student explanations. In: Proceedings of International Conference on Artiﬁcial
Intelligence in Education, Sydney, Australia, IOS Press (2003) 73–80
10. Landauer, T.K., Foltz, P.W., Laham, D.: An introduction to latent semantic analysis. Discourse Processes 25 (1998) 259–284

Modeling Students’ Reasoning About Qualitative Physics

709

11. McCallum, A., Nigam, K.: A comparison of event models for naive bayes text
classiﬁcation. In: Proceeding of AAAI/ICML-98 Workshop on Learning for Text
Categorization, AAAI Press (1998)
12. Jonassen, D.: Using cognitive tools to represent problems. Journal of Research on
Technology in Education 35 (2003) 362–381
13. Conati, C., Gertner, A., VanLehn, K.: Using bayesian networks to manage uncertainty in student modeling. Journal of User Modeling and User-Adapted Interaction 12 (2002) 371–417
14. Zapata-Rivera, J.D., Greer, J.: Student model accuracy using inspectable bayesian
student models. In: International Conference of Artiﬁcial Intelligence in Education,
Sydney, Australia (2003) 65–72
15. Charniak, E., Shimony, S.E.: Probabilistic semantics for cost based abduction. In:
Proceedings of AAAI-90. (1990) 106–111
16. Matsuda, N., VanLehn, K.: GRAMY: A geometry theorem prover capable of
construction. Journal of Automated Reasoning 32 (2004) 3–33
17. Murray, W.R., Pease, A., Sams, M.: Applying formal methods and representations
in a natural language tutor to teach tactical reasoning. In: Proceedings of International Conference on Artiﬁcial Intelligence in Education, Sydney, Australia, IOS
Press (2003) 349–356
18. Self, J.: Formal approaches to student modelling. In McCalla, G.I., Greer, J.,
eds.: Student Modelling: the key to individualized knowledge-based instruction.
Springer, Berlin (1994) 295–352
19. Dimitrova, V.: STyLE-OLM: Interactive open learner modelling. Artiﬁcial Intelligence in Education 13 (2003) 35–78
20. Forbus, K., Carney, K., Harris, R., Sherin, B.: A qualitative modeling environment
for middle-school students: A progress report. In: QR-01. (2001)
21. Ploetzner, R., Fehse, E., Kneser, C., Spada, H.: Learning to relate qualitative and
quantitative problem representations in a model-based setting for collaborative
problem solving. The Journal of the Learning Sciences 8 (1999) 177–214
22. Reimann, P., Chi, M.T.H.: Expertise in complex problem solving. In Gilhooly,
K.J., ed.: Human and machine problem solving. Plenum Press, New York (1989)
161–192
23. de Kleer, J.: Multiple representations of knowledge in a mechanics problem-solver.
In Weld, D.S., de Kleer, J., eds.: Readings in Qualitative Reasoning about Physical
Systems. Morgan Kaufmann, San Mateo, California (1990) 40–45
24. Walther, C.: A many-sorted calculus based on resolution and paramodulation.
Morgan Kaufmann, Los Altos, California (1987)
25. Kakas, A., Kowalski, R.A., Toni, F.: The role of abduction in logic programming.
In Gabbay, D.M., Hogger, C.J., Robinson, J.A., eds.: Handbook of logic in Artiﬁcial
Intelligence and Logic Programming. Volume 5. Oxford University Press (1998)
235–324
26. Stickel, M.: A Prolog-like inference system for computing minimum-cost abductive explanations in natural-language interpretation. Technical Report 451, SRI
International, 333 Ravenswood Ave., Menlo Park, California (1988)
27. Shearer, K., Bunke, H., Venkatesh, S.: Video indexing and similarity retrieval by
largest common subgraph detection using decision trees. Pattern Recognition 34
(2001) 1075–1091

An Analysis of Gaming Behaviors in an Intelligent
Tutoring System
Kasia Muldner, Winslow Burleson, Brett Van de Sande, and Kurt VanLehn
Arizona State University
{katarzyna.muldner,winslow.burleson,bvds,kurt.vanlehn}@asu.edu

Abstract. We present results from an analysis of students’ shallow behaviors,
i.e., gaming, during their interaction with an Intelligent Tutoring System (ITS).
The analysis is based on six college classes using the Andes ITS for homework
and test preparation. Our findings show that student features are a better predictor of gaming than problem features, and that individual differences between
students impact where and how students game.

1 Introduction
Students have long found ways to avoid reasoning about instructional materials, e.g.,
by copying from examples to generate problem solutions [1] or by avoiding effective
study strategies such as self explaining domain principles [2]. In human tutoring
contexts, students often passively listen to tutors’ didactic explanations, without providing substantial follow ups even though more active participation is needed for
effective learning [3]. These behaviors also occur in students’ interactions with intelligent tutoring systems (ITSs). A name given to shallow reasoning in the context of an
ITS is gaming, “attempting to succeed in a learning environment by exploiting properties of the system rather than by learning the material” [4]. Not surprisingly, gaming can be detrimental to learning [5], and so there have been efforts in detecting [6,
7], understanding [4, 8, 9] and preventing [10, 11] gaming.
We add to this research by presenting an in-depth analysis of log data corresponding to several years worth of students interacting with Andes, a tutoring system for
Newtonian physics [12]. To identify gaming episodes in this data, we applied a computational gaming detector that we calibrated with a hand-analysis of the data. Contrary to some prior findings (e.g., [4]), we found that gaming is best predicted by
student features, rather than instructional aspects. This lead us to perform a descriptive analysis of students’ gaming behaviors that focused in part on understanding
which tutor actions lead students to game. While we found individual differences
between low and high gamers, high-level hints were one of the most gamed features.
However, in contrast to other work [4], our analysis suggests that poor hint usability
may not be the culprit, and so that other factors such as student motivation (or lack of)
are at play.
We begin with a survey of related work, and then present our gaming detector, the
log data analysis and results, and finally a discussion of our findings and future work.
V. Aleven, J. Kay, and J. Mostow (Eds.): ITS 2010, Part I, LNCS 6094, pp. 184–193, 2010.
© Springer-Verlag Berlin Heidelberg 2010

An Analysis of Gaming Behaviors in an Intelligent Tutoring System

185

2 Related Work
Several approaches for detecting gaming have been used. Some research has relied on
human observers for real-time gaming identification [5]. This approach is challenging
as observers may miss nuances in a fast-paced classroom environment and so others
have turned to post hoc hand labeling of log data [4]. The latter approach affords the
human coder time to consider all student actions but is costly, since copious amounts
of data must be hand labeled. A potential issue with using human coders is that they
may be inconsistent in identifying gaming episodes, something that machine algorithms for gaming identification address [6, 7, 13, 14].
A key challenge is understanding what causes gaming. Some researchers propose
that gaming is due to features of the instructional materials, including (poor) ITS
design. For instance, Baker et al. [4] found that ITS features, such as unhelpful hints
and non-intuitive toolbar icons, explained more of the variance in the data than prior
approaches using student features; a similar result was obtained in [15]. Other work
focuses on identifying student characteristics that drive gaming. For instance, since
students game on steps that they do not know [13], it has been proposed that item
difficulty leads to gaming. Another student characteristic influencing gaming is affect,
with boredom being the most frequent emotion to precede gaming [8]. There has also
been research on how performance goal orientation impacts gaming [16]; this work
failed to find the anticipated link between gaming and performance goals.
As far as gaming prevention is concerned, a number of strategies have been developed, including the use of animated agents that show disapproval when gaming
occurs [17], software design via a mandatory delay before a student can ask for a hint
[10, 18], and/or by letting students choose the hint level [19].

3 The Data and Gaming Detector
The Data. Our data, obtained from the Pittsburgh Learning Center DataShop, corresponds to logs of students using the Andes ITS [12] for assigned class homework and
test preparation (from six different physics classes over the span of about three years).
Andes tutors Newtonian physics and is described in detail elsewhere (e.g., [12]); here
we provide a very brief overview. Students solve problems in the Andes interface by
drawing diagrams and typing equations. Such a user interface action will be called an
entry. Andes provides immediate feedback for correctness on students’ entries, by
coloring the entry red (incorrect) or green (correct). As students solve problems, they
can ask Andes for a hint; the Andes hint sequence starts out general and ends with a
bottom-out hint that indicates precisely the step to enter (e.g., “Why don't you continue with the solution by working on setting the pressure at a point open to the atmosphere “ … “Write the equation Pa = Pr0”). To discourage students from always
going to the bottom-out hint, Andes assigns a score to each problem, which is decremented slightly every time a bottom-out hint is requested.
The Gaming Detector. After irrelevant actions are removed from the log data, a log
consists of a time-stamped sequence of tutor-student turn pairs (e.g., tutor indicates an
entry is incorrect, student responds by asking for a hint). To address our research
questions, we needed to know which of these turn pairs corresponded to gaming.

186

K. Muldner et al.
Table 1. Tutor-Student turn pairs (gamed cells shaded)

(1) Tutor: bottom-out hint
(2) Tutor: High-level hint
(3) Tutor: Incorrect (Red)
(4) Tutor: Correct
(Green)

(a) Student: hint request
fast
slow
Skip hint (S)
Skip hint (S)
No planning (P)

-

(b) Student: Entry
fast
slow
Copy hint (C)
Guess (G)
-

-

Given that our data comprised over 900,000 pairs, manual analysis was not feasible.
Thus, we first hand-analyzed a fragment of the log data to identify rules to detect
gamed turn pairs, which we then encoded into a computational gaming detector that
could automatically label the data. We then applied the detector, hand-checking its
output on a new data fragment, revising as necessary.
For purposes of this analysis, we considered the following tutor turns: (1) coloring
an entry red (incorrect), (2) coloring an entry green (correct), (3) giving a bottom-out
hint, or (4) giving a high-level hint (we did not further subdivide the high-level hints
since the number and characteristics of such hints varied considerably). We classified
a student’s turn as either (a) asking for a hint or (b) generating an entry. Thus, there
are 4*2=8 types of turn pairs (see Table 1). Each turn pair has a time duration associated with it, which is how long the student paused between seeing the tutor’s turn and
starting to take action. We assume that turn pairs with long durations are not gaming.
Of the eight possible turn pairs with short durations (see Table 1), we consider the
following five to be gaming: (1-2) Skipping a hint: the tutor presents a hint and the
student skips the hint by quickly asking for another hint (see ‘S’ cells in Table 1); (3)
Copying a hint: the tutor presents a bottom-out hint and the student quickly generates
a solution entry, suggesting a shallow copy of the hint instead of learning of the underlying domain principle1 (see ‘C’ cell, Table 1); (4) Guessing: after the tutor signals
an incorrect entry, the student quickly generates another incorrect2 entry, suggesting
s/he is guessing instead of reasoning about why the entry is incorrect (see ‘G’ cell in
Table 1); (5) Lack of planning: after the tutor signals a correct entry, the student
quickly asks for a hint, suggesting reliance on hints for planning the solution (see ‘P’
cell, Table 1). Note that item 1, skipping hints, does not take into account the possibility that a student may copy the hint and then reason about it. This was explored in
[20], by analyzing time after a hint was copied. Time alone, however, does not necessarily indicate that the student is reasoning about the hint, since they may be, for instance, thinking about the next step. Thus, for the time being, we decided to only
consider time before an entry is generated, as we felt this was more likely to correspond to reasoning about the entry.
Accurate gaming detection relies on having reasonable time thresholds, one for
each of the five gamed turn pairs. To set the threshold, we obtained a value for each
1
2

A high-level hint followed by a fast entry is not gaming since you can’t copy high-level hints.
This is the only student entry where we take into account correctness of the student entry, as
not doing so might incorrectly classify fixing slips as gaming.

An Analysis of Gaming Behaviors in an Intelligent Tutoring System

187

pair based on our review of the log file data. As a final check, we obtained a frequency distribution graph for each of the gamed pairs. The graph allowed us to ensure
that the threshold we chose was not unrealistic (e.g., so high that all students would be
considered gaming). Note that we were conservative when setting our thresholds: for
instance, we set the skipping hint threshold T = < 3sec. While this threshold may not
afford enough time to read all of a hint, it captures instances when students are
skipping most of the hint.

4 Results
Our analysis is based on applying the above-described gaming detector to data from a
set of 318 unique problems and 286 students. We now describe our results.
4.1 What Is a Better Predictor of Gaming: Student or Problem?
As we mentioned above, a central question pertains to what causes gaming, and in
particular, whether student or problem features better predict gaming. To address this
question, we obtained the following measures:
PerGamings p

percentage of gaming by a student s on a problem p

(1)

p= N

i.e., average gaming by a student s across all N
problems p solved by that student

(2)

i.e., average gaming on a problem p across all M
students s

(3)

∑ PerGaming s p / N

p= 0

s= M

∑ PerGaming s p / M

s= 0

We used problem as the unit of analysis (see equation 1; equations 2 and 3 rely on it).
Some research has used lesson as the primary unit of analysis [4]. In fact, the ideal
unit would correspond to tutor-student turn pairs, as these are when student makes a
game vs. no-game decision. However, we need a unit of analysis that can be compared across students, so that we can determine whether all students tend to game at
“the same” place. It would be difficult to determine if turn-pair from one student are
“the same” as a turn-pair from another student. The smallest unit of analysis that allows simple equivalence across students is the problem. Thus, we chose problems as
the unit of analysis instead of lesson (too large) or tutor-student turn pairs (not
equatable; too small). We used percentage of gaming (see equation 1) instead of raw
values to avoid biasing the analysis towards, for instance, short problems.
To investigate predictors of gaming we conducted a linear regression analysis, with
PerGamingsp (equation 1 above) as the dependent variable, and two independent variables: (1) student, the average gaming by a student s across all N problems p solved
by that student (equation 2 above), and (2) problem, the average gaming on a problem
p across all M students s who solved that problem (equation 3 above). The model is
significant (F=16915, p < 0.001), and accounts for 60.7% of the variance (R2 = .607).
In this model, both student and problem yield a significant correlation with the

188

K. Muldner et al.

dependent variable (student: standardized coefficient=.658, t=152.7, p<0.001; problem: standardized coefficient=.325, t=74.23, p<0.001). If we enter the independent
variables separately to analyze the variance explained by each, (1) the student variable
accounts for 49.6% of the variance, while (2) the problem variable accounts for
18.6% of the variance.
To identify the impact of a particular data set (i.e., class/semester), we re-ran the
regression analysis with a third independent variable, namely data set id. This variable explained only an additional 1% of the variance, showing that data set had at best
a weak effect on gaming, and so we did not consider it in subsequent analysis.
Another way to verify whether students are more consistently gaming across problems or if instead problems are more consistent across students is to randomly subdivide students (or problems) into buckets and then check for correlation between the
buckets. To this end, we created two buckets by randomly assigning students to a
given bucket. For each bucket, we obtained the average percentage of gaming for
each problem in that bucket (equation 3 above), and then performed correlation analysis between buckets A and B. We found a high degree of association between the two
data sets (R2=.89 p < 0.001). That is, if a problem was often gamed by students in
bucket A, then it was also often gamed by students in bucket B. We used an analogous technique to verify that problems were consistently gamed on between students
(i.e., obtained two bucket by randomly assigning problems to a given bucket, and
applied equation 2 above to obtain the average percentage of gaming by a student);
the analysis yielded a high degree of association (R2=.963, p<0.001). That is, if a
student tended to game the problems in the A bucket, then that student also tended to
game problems in the B bucket. Jointly, these analyses show that students are more
consistent than problems: if a student is a high gamer on one half of the problem, then
the student is also a high gamer on the other half; in contrast, if a problem is a highgaming problem for half the students, then it is less likely to be a high-gaming problem for the other half. Thus, these analyses support the above regression results.
Yet another way to test our hypotheses is to examine histograms of gaming frequency. That is, we can look at how many students are high frequency gamers vs.
middle vs. low frequency gamers. If individual differences among students are completely unimportant, and all students tend to solve roughly the same set of problems,
then gaming frequency should be normally distributed. In fact, the distribution is
significantly different from the normal (Shapiro-Wilks test of normality W=.8,
p<0.01), and appears bimodal (see Figure 1, left). There seems to be one group of
students who are frequent gamers, and another group who seldom game. This again
suggests that individual differences play an important role in gaming frequency.
On the other hand, if the characteristics of problems are completely unimportant,
then a histogram of the number of problems (y-axis) gamed at a certain range of frequencies (x-axis) should be normally distributed (Figure 1, right). This is in fact the
case: the Shapiro-Wilks test of normality showed that the problem distribution is not
significantly different from normal (W=.9, p>0.05). Thus, it appears once again that
characteristics of students are more important than characteristics of problems in
determining the frequency of gaming.

An Analysis of Gaming Behaviors in an Intelligent Tutoring System

189

Fig. 1. Student (left) and problem (right) gaming distributions. Each bucket contains students
(or problems) with a gaming range (e.g., bucket 10 has 5% < gaming < 10%).

4.2 Gaming Profiles: How Much and Where Are Student Gaming?
This section presents a descriptive analysis of the data, starting with how much students are gaming overall. On average, 22.5% of the tutor-student turn pairs were
gamed. While this is higher than reported in [5], in that study students were in the
presence of observers. This may have provided social deterrents for gaming, while in
our study students were using Andes in private. We then analyzed where the gaming
was occurring; Table 2 shows the results (the shaded cells indicate gamed turn pairs).
In general, students most frequently took advantage of the opportunity to game when
the tutor presented a high-level hint: on average, 18.4% of all actions corresponded to
gaming on these hints; when given such a hint, students gamed 58.5% of the time.
In order to compare the gaming patterns of students who frequently gamed with
those who infrequently gamed, we divided students into low gamers and high gamers
based on a median split. On average, low gamers were significantly more likely than
high gamers to game by guessing (46% vs. 13.2%; F(1,283)=126, p<.01). On the
other hand, in contrast to low gamers, high gamers had a significantly higher
proportion of skipped high-level hints (61.6% vs. 43.4%; F(1, 283)=64, p<.01), lack
of planning (18.5% vs. 8.9%; F(1,283)=159, p<.01) and bottom-out hint copying
(6.5% vs. 1.7% ; F(1,283)=215, p<.01).
4.3 Are Hints the Culprit or Is It the Students?
Over all students’ gaming opportunities (see Table 2), as well as proportion of gaming
for high gamers, high-level hints elicited the most gaming. Thus, we wanted to explore how students used hints and if hints were helpful during problem solving.
Hint Viewing. The most basic analysis is to calculate time students spent on hints. To
do so, we obtained the latency between the provision of a hint and the next student
action. On average, students spent 9.2 sec. vs. 5.7 sec. on bottom-out vs. high-level
hints. High gamers spent significantly less time on hints than low gamers, both on
bottom-out hints (7.5sec vs. 10.9sec.; F(1, 277)=71, p<.01) and high-level hints
(3.2sec vs. 8.1sec; F(1,286)=246, p<.01). Since the bottom-out viewing is well above
the gaming threshold, on average, neither low or high gamers skipped bottom-out
hints. In contrast, high gamers average viewing time for high-level hints is just above
the gaming threshold of 3 seconds, showing that in contrast to low gamers, these
students did not pay much attention to high-level hints.

190

K. Muldner et al.

Table 2. Gaming opportunities for each Tutor–Student turn pair. Shown in each cell: (1) the
mean % of a student response given a tutor action over all 16 possible combinations, (2)
(mean % of a student response for that row’s tutor action).

(1) Tutor: B-O Hint
(2) Tutor: H-L Hint
(3) Tutor: Incorrect
(4) Tutor: Correct

(a) Student: Hint Request
fast
slow
0.02 (.3)%
.2 (2)%
18.4 (58.5)% 5.8 (18.3)%
2.5 (10.1)%
3 (12.4) %
5.2 (15.6) % 3.7 (10.2)%

(b) Student: Entry
fast
slow
1.8 (23.6)%
5.7 (74)%
.7 (2.3)%
6.4 (20.6)%
5.4 (21.9) %
13.6 (55.5)%
14.1 (38.2)%
13.3 (36)%

Legend: fast: student action < gaming threshold; slow: student action >gaming threshold; BO: Bottom-out, H-L: High-level.

Are Hints Helpful? Prior work suggests that a factor related to gaming is reading
hints does not influence solution entry success [4]. Sophisticated techniques exist for
analyzing the utility of help by looking at its impact on future student performance
[21]. It is not clear, however, how these methods account for gaming, which can make
it difficult to interpret results (e.g., if a student skips a hint repeatedly, is the hint not
helpful or is the student unmotivated to use it?). Thus, for the time being, we analyze
hint impact on short-term performance, i.e., can the student generate an entry after
seeing a hint. Specifically, for each student, we obtained the percentage of time s/he
was successful at generating a correct entry after receiving each type of hint (bottom
out, high level). Note that (1) students may require several attempts to generate a
correct entry and (2) if hint B is requested after hint A but prior to generating a correct
entry, then hint A is not counted as “successful” for helping the student.
If for a moment we don’t consider entry correctness, high gamers tried to generate
an entry only 18% of the time after receiving a high-level hint, immediately asking for
another hint the other 82% of the time. Low gamers, on the other hand, responded to
a high-level hint with an entry 43% of the time. This is in contrast to bottom-out hints,
when both low and high gamers responded to the hint with an entry about 97% of the
time. When students did generate an entry after seeing a bottom-out hint, on average,
they were successful in 90% of instances (i.e., obtained a correct entry). There was
little difference between low and high gamers for this analysis (89% vs. 92%, respectively, NS difference). After high-level hints, students generated a correct entry 73%
of the time. Again, there was little difference between low and high gamers (72% vs.
73%, respectively, NS difference). This suggests that high-level hints helped students
generate the solution in about three out of four instances.
Now let’s look at time and number of attempts needed to produce a correct entry.
After bottom-out hints, on average students required 1.1 attempts (1.23 for low gamers vs. 1.19 for high gamers, NS), and took 29 sec. to do so (34sec. for low gamers vs.
23sec. for high gamers, F(284,1)=4, p = .052). After high-level hints, on average
students required 1.83 attempts; here low gamers needed significantly fewer attempts
than high gamers (1.66 vs. 2.01, F(1,284)= 17, p<0.001), suggesting that perhaps the
low gamers were more diligent about applying high-level hints. This conjecture is
supported by the fact that low-gamers spent significantly longer than high-gamers to
generate a correct entry after seeing a high-level hint (37 sec vs. 28 sec; F(1,286)=9,
p<0.01).

An Analysis of Gaming Behaviors in an Intelligent Tutoring System

191

5 Discussion and Future Work
A prerequisite for the design of effective interventions to discourage gaming is understanding its causes. Past research has shown that both student and instructional aspects influence gaming, but to date there does not exist agreement as to which is the
stronger predictor. Baker et al. [4] argue that it is the latter, i.e., instructional aspects,
that drive gaming. In contrast, our findings suggest that student features, namely the
average percentage of gaming by a student over all the problems s/he solved, was a
stronger predictor of gaming. There are a number of possibilities as to the cause of the
difference between our findings and those in [4]. First, the Andes system might have
less instructional variability than the one in [4]. Second, [4] used lesson as the grainsize, while we used problem, a smaller grain size. We did not use lesson since as
already described in Section 2 we felt such a large grain size may obscure the results.
Third, we used data from college students working at home while data in [4] came
from high school students working in classrooms. When we recently did a preliminary
analysis on a set of high school honours students using Andes mostly in their classroom, we found that their gaming levels were lower than those of college students;
thus it is possible that gaming behaviors differ between these two populations and
contexts, something that warrants further analysis and validation. A fourth possibility
pertains to the way the analysis was done. Baker et al. [4] considered lesson features
(e.g., does a lesson have many problems that use the same number for different quantities), and determined how much gaming variance was associated with each feature.
Similarly, Baker et al. [15] determined the variance explained by features of students.
Our analysis did not use problem features or student features, but rather individual
problems and individual students. Our logic was that if there was something “wrong”
with a problem, then almost all students should game on that problem; similarly, if
there was something “wrong” with a student, then that student should game on almost
all problems. In general, this discrepancy in findings in terms of whether problem or
student features best predict gaming highlights the need for more work and validation
of the factors influencing gaming.
In addition to exploring predictors of gaming, we also analyzed the impact of individual differences on how students were gaming. We found that when we looked at
gaming opportunities over the tutor-student turn pairs, students tended to seize the
opportunity to game after the tutor presented them with a high level hint. However,
when we analyzed the proportion of each type of gaming over the total gaming events
for each class of gamers, in contrast to high-gamers (who primarily skipped hints), the
low gamers had a higher incidence of guessing on entries. One possible explanation
for this difference, supported by literature on individual differences in help seeking
behaviors [22], is that the low gamers preferred to obtain the solution on their own,
without the tutor’s help. Another possibility relates to Andes’ scoring system. Recall
that students were penalized for asking for a bottom out hint but were not penalized
for guessing, and so perhaps the low gamers were simply more concerned about their
Andes score than high gamers. Our analysis also showed, however, that low gamers
spent more time with hints and took longer to generate a solution entry after seeing a
hint. Since no points were awarded for taking time, this suggests that obtaining a
higher score was not the only incentive for the high-gamers, indicating that perhaps
these students were motivated and/or diligent in the problem-solving process. Jointly,

192

K. Muldner et al.

these findings point to the need to tailor gaming interventions to student characteristics in ITS design.
Prior research suggests that poor hint usability leads to gaming [4]. To see if this
was the case in our data, we analyzed how students used hints. We found that when
the tutor presented a high-level hint, high gamers were quite unlikely to even try generating a corresponding solution entry, as compared to low gamers. If students did try
to generate a solution entry, both low and high gamers were moderately successful
when given a high-level hints. This provides some indication for the utility of these
hints, suggesting that their abuse may be driven by other factors. It is possible, however, that students didn’t bother to use the high-level hint at all, and were successful
because they generated the solution on their own. To have a better understanding of
hint utility, one could obtain students’ base-rate performance. However, Andes makes
hints available on demand, and students sometimes abuse these. This makes it less
clear how to determine this base performance, and is something we leave for future
work. We also plan to analyze deeper the student (and problem) features that predict
gaming frequency, as well as analyze how gaming influences learning outcomes –
although preliminary steps have been taken (e.g., [5]), more work is needed.
Acknowledgements. The authors thank the anonymous reviewers for their helpful
suggestions. This research was funded the National Science Foundation, including the
following grants: (1) IIS/HCC Affective Learning Companions: Modeling and supporting emotion during learning (#0705883); (2) Deeper Modeling via Affective
Meta-tutoring (DRL-0910221) and (3) Pittsburgh Science of Learning Center (SBE0836012).

References
1. VanLehn, K.: Analogy Events: How Examples Are Used During Problem Solving. Cognitive Science 22(3), 347–388 (1998)
2. Renkl, A.: Learning from Worked-Examples: A Study on Individual Differences. Cognitive Science 21(1), 1–30 (1997)
3. Chi, M., Siler, S.A., Jeong, H., Yamauchi, T., Hausmann, R.G.: Learning from Human Tutoring. Cognitive Science 25, 471–533 (2001)
4. Baker, d.C., Raspat, J., Aleven, V., Corbett, A.T., Koedinger, K.R.: Educational Software
Features That Encourage and Discourage “Gaming the System”. In: 14th International
Conference on Artificial Intelligence in Education, pp. 475–482 (2009)
5. Baker, R.S., Corbett, A., Koedinger, K., Wagner, A.Z.: Off-Task Behavior in the Cognitive Tutor Classroom: When Students “Game the System”. In: ACM CHI 2004: ComputerHuman Interaction, pp. 383–390 (2004)
6. Baker, R., Corbett, A., Roll, I., Koedinger, K.: Developing a Generalizable Detector of
When Students Game the System. User Modeling and User-Adapted Interaction 18(3),
287–314 (2008)
7. Baker, R., Corbett, A., Koedinger, K., Roll, I.: Generalizing Detection of Gaming the System across a Tutoring Curriculum. In: 11’th International Conference on Intelligent Tutoring Systems, pp. 402–411 (2006)

An Analysis of Gaming Behaviors in an Intelligent Tutoring System

193

8. Rodrigo, M., Baker, R., d’Mello, S., Gonzalez, M., Lagud, M., Lim, S., Macapanpan, A.,
Pascua, S., Santillano, J., Sugay, J., Tep, S., Viehland, N.: Comparing Learners’ Affect
While Using an Intelligent Tutoring Systems and a Simulation Problem Solving Game. In:
Woolf, B.P., Aïmeur, E., Nkambou, R., Lajoie, S. (eds.) ITS 2008. LNCS, vol. 5091, pp.
40–49. Springer, Heidelberg (2008)
9. Baker, R., Walonoski, J., Heffernan, N., Roll, I., Corbett, A., Koedinger, K.: Why Students
Engage in “Gaming the System”. Journal of Interactive Learning Research 19(2), 185–224
(2008)
10. Murray, R.C., VanLehn, K.: Effects of Dissuading Unnecessary Help Requests While Providing Proactive Help. In: Artificial Intelligence in Education, pp. 887–889 (2005)
11. Walonoski, J., Heffernan, N.: Prevention of Off-Task Gaming Behavior in Intelligent Tutoring Systems. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 722–724. Springer, Heidelberg (2006)
12. VanLehn, K., Lynch, C., Schulze, K., Shapiro, J., Shelby, R., Taylor, L., Treacy, D.,
Weinstein, A., Wintersgill, M.: The Andes Physics Tutoring System: Lessons Learned. International Journal of Artificial Intelligence and Education 15(3), 1–47 (2005)
13. Baker, R., Corbett, A., Koedinger, K.: Detecting Student Misuse of Intelligent Tutoring
Systems. In: Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.) ITS 2004. LNCS, vol. 3220,
pp. 531–540. Springer, Heidelberg (2004)
14. Walonoski, J., Heffernan, N.: Detection and Analysis of Off-Task Gaming Behavior in Intelligent Tutoring Systems. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006.
LNCS, vol. 4053, pp. 382–391. Springer, Heidelberg (2006)
15. Baker, R.: Is Gaming the System State-or-Trait? Educational Data Mining through the
Multi-Contextual Application of a Validated Behavioral Model. In: Workshop on Data
Mining for User Modeling, pp. 76–80 (2007)
16. Baker, R.S., Roll, I., Corbett, A., Koedinger, K.: Do Performance Goals Lead Students to
Game the System. In: International Conference on Artificial Intelligence and Education
(AIED 2005), pp. 57–64 (2005)
17. Baker, R., Corbett, A., Koedinger, K., Evenson, E., Roll, I., Wagner, A., Naim, M., Raspat,
J., Baker, D., Beck, J.: Adapting to When Students Game an Intelligent Tutoring System.
In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS, vol. 4053, pp. 392–401.
Springer, Heidelberg (2006)
18. Aleven, V.: Helping Students to Become Better Help Seekers: Towards Supporting Metacognition in a Cognitive Tutor. Paper Presented at German-USA Early Career Research
Exchange Program: Research on Learning Technologies and Technology-Supported Education (2001)
19. Harris, A., Bonnett, V., Luckin, R., Yuill, N., Avramides, K.: Scaffolding Effective HelpSeeking Behaviour in Mastery and Performance Oriented Learners. In: Artificial Intelligence in Education, pp. 425–432 (2009)
20. Shih, B., Koedinger, K., Scheines, R.: A Response Time Model for Bottom-out Hints as
Worked Examples. In: International Conference on Educational Data Mining, pp. 117–126
(2008)
21. Beck, J., Chang, K., Mostow, J., Corbett, A.: Does Help Help? Introducing the Bayesian
Evaluation and Assessment Methodology. In: Woolf, B.P., Aïmeur, E., Nkambou, R., Lajoie, S. (eds.) ITS 2008. LNCS, vol. 5091, pp. 383–394. Springer, Heidelberg (2008)
22. Gall, S.N.-L.: Help-Seeking Behavior in Learning Review of Research in Education 12,
55–90 (1985)

Machine Learning, 16, 11-36 (1994)
Q 1994 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.

Acquisition of Children's Addition Strategies:
A Model of Impasse-Free, Knowledge-Level
Learning
RANDOLPH M. JONES

rjones@eecs.umich.edu

Artificial Intelligence Laboratory, University of Michigan, 1101 Beal Avenue, Ann Arbor, M148109-2110
KURT VANLEHN

vanlehn@cs.pitt.edu

Learning Research and Development Center, and Department of Computer Science, University of Pittsburgh,
Pittsburgh, PA 15260
Editor: Michael Pazzani
Abstract. When children learn to add, they count on their fingers, beginning with the simple SuM strategy and
gradually developing the more sophisticated and efficient MIN strategy. The shift from SUM to MIN provides an
ideal domain for the study of naturally occurring discovery processes in cognitive skill acquisition. The SUM-toMIN transition poses a number of challenges for machine-learning systems that would model the phenomenon.
First, in addition to the SUM and MIN strategies, Siegler and Jenkins (1989) found that children exhibit two
transitional strategies, but not a strategy proposed by an earlier model. Second, they found that children do not
invent the MtN strategy in response to impasses, or gaps in their knowledge. Rather, MIN develops spontaneously
and gradually replaces earlier strategies. Third, intricate structural differences between the SUM and MIN strategies
make it difficult, if not impossible, for standard, symbol-level machine-learning algorithms to model the transition.
We present a computer model, called GIPS, that meets these challenges. GIPS combines a relatively simple
algorithm for problem solving with a probabilistic learning algorithm that performs symbol-level and knowledgelevel learning, both in the presence and absence of impasses. In addition, GIPS makes psychologically plausible
demands on local processing and memory. Most importantly, the system successfully models the shift from SUM
to MIN, as well as the two transitional strategies found by Siegler and Jenkins.
Keywords:
strategies

1.

cognitive simulation, impasse-free learning, probabilistic learning, induction, problem-solving

Introduction

This research focuses on modeling naturally occurring discovery processes in cognitive
skill acquisition. In particular, it provides an explanation of the well-known SUM-to-MIN
transition that children exhibit when they are learning to add (Ashcraft, 1982, 1987; Groen
& Parkman, 1972; Groen & Resnick, 1977; Kaye, Post, Hall, & Dineen, 1986; Siegler &
Jenkins, 1989; Svenson, 1975). On the surface, this transition appears to be a case of symbollevel or speed-up learning (Dietterich, 1986). The SUM and MIN strategies are both correct
and complete addition algorithms, but the MIN strategy is much faster. However, closer
inspection reveals that the transition involves changes to the structure of the solution, which
cannot be explained by conventional symbol-level learning methods. In addition, children
appear to invent the MIN strategy spontaneously, rather than in response to any failures or
impasses in problem solving. Thus, a successful model of the SUM-to-MIN transition must
make dramatic changes in the strategies, and it must be able to do so without the benefit of
impasses to drive learning.

12

R.M. JONES AND K. VANLEHN

Earlier work attests to the complexity of modeling this structurally intricate strategy shift.
Neches (1987) was able to model the transition using a machine learner based on compiler
optimization techniques, but the model required implausibly large amounts of extremely
detailed information about both ongoing processes and related past experiences. Moreover,
it predicted that subjects would briefly display certain strategies on their way to the MIN
strategy, but these strategies were not observed in subsequent empirical work (Siegler &
Jenkins, 1989). Other intermediate strategies were observed instead.
The research problem is to find a learning method (or methods) that can make the SUMto-MIN transition, use only plausible amounts of computation and memory, and explain the
observed intermediate strategies. In this paper, we concentrate on explaining the results of a
longitudinal study carried out by Siegler and Jenkins (1989). They found that children invent
the MIN strategy and two intermediate strategies independently, without any instruction on
the new strategies. More importantly, Siegler and Jenkins discovered that the invention of
the MIN strategy does not appear to be driven by failures or impasses in solving problems.
Finally, we argue that learning the MIN strategy requires a form of knowledge-level learning
(Dietterich, 1986) that introduces new, more efficient behavior, rather than simply tuning
or composing old knowledge.
We describe a computational model, called GIPS (for General Inductive Problem Solver),
that invents the MIN strategy as well as the correct transitional strategies. In addition, GIPS
smoothly integrates a general problem-solving architecture with a simple, independently
motivated learning algorithm. The learning algorithm applies a probabilistic concept learner
to all of GIPS' major decision points, allowing it to combine impasse-driven, impasse-free,
symbol-level, and knowledge-level learning in a single, uniform framework. Remarkably,
the relatively simple problem-solving and learning algorithms interact so as to explain
intricate strategy shifts that previous systems could not account for.
In the following section, we describe the SUM-to-MIN transition, and explain its complexities in detail. Next, we present the GIPS system, its representation of the addition
domain, and its account of the SUM-to-MIN shift. The last section discusses GIPs' account
and compares it to those offered by other models.
2.

The SuM-to-MIN transition

When young children first learn to add two small numbers, they use the so-called SUM
strategy. They create sets of objects to represent each addend, then count the objects in the
union of the two sets. For example, suppose a child is asked, "What is 2 plus 3?" In order
to solve this problem, the child says, "1, 2"' while raising two fingers on the left hand; then
"1, 2, 3," while raising three fingers on the right hand; then "1, 2, 3, 4, 5," while counting all
the raised fingers. This is called the SUM strategy because its execution time is proportional
to the sum of the two addends. Older children use a more efficient strategy, called the MIN
strategy. In following this strategy, the child first announces the value of the larger addend,
then counts onward from it. For instance, in order to solve 2 ÷ 3, the child would say,
"3," then say, "4, 5," while raising two fingers on one hand. The execution time for the
MIN strategy is proportional to the minimum of the two addends. Algorithms for the two
strategies appear in Table 1.

13

CHILDREN'S ADDITION STRATEGIES
Table 1. A comparison of the SUM and MIN strategies.
Initial

MIN s t r a t e g y

SUM s t r a t e g y

i. A s s i g n f i r s t a d d e n d
to l e f t hand;
2. A s s i g n s e c o n d a d d e n d
to r i g h t hand;
3. L e t C o u n t e r b e 0;
4. L o o p
5.
R a i s e f i n g e r o n l e f t hand;
6.
L e t C o u n t e r b e C o u n t e r + i;
7. U n t i l C o u n t e r =
left hand addend;
8 • L e t C o u n t e r b e 0;
9 • Loop
R a i s e f i n g e r o n r i g h t hand;
i0 •
ii •
L e t C o u n t e r b e C o u n t e r + i;
12 • U n t i l C o u n t e r =
right hand addend;
13 • L e t C o u n t e r b e 0;
14 • L o o p
M a r k r a i s e d finger;
15 •
L e t C o u n t e r b e C o u n t e r + i;
16 •
17 • U n t i l n u m b e r of m a r k e d f i n g e r s
n u m b e r of r a i s e d f i n g e r s ;
18 • L e t A n s w e r b e C o u n t e r ;

Assign larger addend
to l e f t h a n d ;
Assign smaller addend
to r i g h t hand;

Let

Counter

be

left

hand

addend;

Loop

Raise f i n g e r o n r i g h t h a n d ;
L e t C o u n t e r b e C o u n t e r + i;
U n t i l n u m b e r of r a i s e d f i n g e r s
right hand addend;

=

=
Let Answer

be Counter;

Although the SUM strategy is taught in school, the MIN strategy appears to be invented
by the children themselves. The best evidence for this comes from a longitudinal study by
Siegler and Jenkins (1989)• They interviewed eight children weekly for 11 weeks, each time
asking them to solve about 15 orally presented addition problems• After each problem, they
asked the children how they got their answers. They also told each child whether the answer
was correct, and gave the child a gold star if it was. Finally, they analyzed videotapes of the
session and classified the child's behavior on each problem according to the strategy that the
child used. As far as Siegler and Jenkins could determine, the only instruction that the subjects received during this period was their school's normal instruction on the SUM strategy.
Nonetheless, seven of the eight children eventually began to use the MIN strategy. Moreover, the children appear to have discovered this strategy during the video-taped sessions.
The tapes make it clear that they received no help from the experimenter, so the MIN strategy
appears to have been invented by the subjects themselves. In addition, Siegler and Jenkins
found two transitional counting strategies that the subjects used while proceeding from SUM
tO MIN. These are the SHORTCUT SUM strategy, in which a subject raises and counts fingers from one to the final sum across both hands, and the FIRST strategy, which is similar to
MIN, except that the order for adding two addends is not determined by their relative sizes.

2.1. Impasse-free learning during strategy invention
A central issue for computational learning systems is deciding when to learn. A popular
method is to learn when an impasse occurs, suggesting a hole in the system's knowledge
base. The exact definition of "impasse" depends on the problem-solving architecture,
but roughly speaking, an impasse occurs for a problem solver when it comes across a

14

R.M. JONES AND K. VANLEHN

goal that cannot be achieved by any operator that is believed to be relevant to the task at
hand. The essential idea of impasse-driven learning is to resolve the impasse somehow,
then store the resulting experience in such a way that future impasses will be avoided or
at least handled more efficiently. Many systems use impasse-driven learning, including
LPARSIFAL (Berwick, 1985), OCCAM (Pazzani, Dyer & Flowers, 1986), SWALE (Schank,
1986), SOAR (Newell, 1990), SIERRA (VanLehn, 1990), and CASCADE (VanLehn & Jones,
1993; VanLehn, Jones & Chi, 1992). SOAR is perhaps the best-known impasse-driven
learning system, but its definition of impasse is a bit idiosyncratic. It uses impasse-driven
learning for all changes to memory. Because people automatically store a dense record
of their on-going experiences (Tulving's episodic memory), a proper SOAR model must
have impasses very frequently, perhaps several per second. Unlike SOAR, other models
record their personal experiences with mechanisms that are separate from their impassedriven learning mechanism. For them, an impasse corresponds to the subjective experience
of getting stuck and knowing that you are stuck. In one detailed psychological study
(VanLehn, 1991), this occurred about once every half hour. In this paper, we use "impasse"
only for these higher level impasses.
Because of the importance of impasse-driven learning in current models of intelligence,
Siegler and Jenkins looked specifically for signs of impasses in their study. In particular,
they designed some of the problems to cause impasses by making one of the addends very
large (e.g., 23 + 1). They found that "The specific problems on which the children first
used the MIN strategy were 2 + 5, 4 + 1, 3 + 1, 1 + 24, 5 + 2, and 4 + 3. These problems
did not deviate from the characteristics of the overall set in any notable way" (p. 67). In
fact, some of the children had earlier successfully solved exactly the same problem that
they were working on when they discovered the MIN strategy. Although the large-addend
problems did cause subjects who had already invented the MIN strategy to start using it more
frequently, the problems did not cause those who had not invented the strategy to do so.
In addition, Siegler and Jenkins sought signs of impasses by examining solution times and
errors in the vicinity of the discovery events. Solution times were longer than normal for
the problems where the discovery occurred (a median of 17.8 seconds vs. overall median
of 9.8 seconds) and for the problems immediately preceding the discovery trial (median 18
seconds). This might suggest some kind of impasse. However, the specific problems being
worked on at those points were not particularly difficult. On the discovery trial, 71% of the
problems involved addends that were both 5 or less and thus could each be stored on a single
hand. This rate is almost identical to the rate of 72% for the set as a whole. Moreover, 88%
of the problems encountered in the session prior to the discovery did not include a large
addend. Using error rates as a measure of difficulty yielded a similar finding. Siegler and
Jenkins report,
Prior to discovering the min strategy, children had answered correctly 12 of the 16
problems that they had encountered within the session. This level of accuracy, 75%,
was not substantially worse than the 85% correct answers that children generated
across the entire practice set. Further, three of the four errors were generated by a
single child; the other four children collectively made only one error on the 12 trials
they encountered in the same session but before their discoveries. This, together
with the fact that two other children used the min strategy for the first time on the

CHILDREN'S ADDITION STRATEGIES

15

first trial of a session, indicated that incorrect answers are not necessary' to motivate
discovery of a new strategy. However, the long solution times just prior to the
discoveries do suggest a heightening of cognitive activity even witheut incorrect
answers to motivate it. (p. 69)
The absence of impasses near the critical learning events presents a challenge for current
theories of learning. However, Siegler and Jenkins suggest a reconciliation between their
findings and the impasse-driven learning theories:
Two types of strategy changes can be distinguished: changes in which the main
difference between the strategies is in the answers themselves, and changes in
which the main differences are not in the answers that are generated but rather
in the efficiency with which answers are generated and/or the aesthetic appeal of
the procedures. The first type of strategy change may occur primarily as a result
of encountering impasses, but the second may typically occur for other reasons.
(p. 104)

2.2. Symbol-level vs. knowledge-level learning
Dieterrich (1986) defines symbol-level learning as learning that improves the performance
of a system, but does not increase the deductive closure of the system's knowledge. In
contrast, learning at the knowledge level involves actually changing the system's knowledge
base or domain theory, thus changing what the system can possibly deduce (given enough
time). Most current problem-solving systems learn at the symbol level, achieving better
performance by improving their search through a problem space. In general, this type of
learning has taken one of two forms: search tuning and macro-operator formation. Search
tuning involves methods for decreasing the average branching factor of the search via
search-control rules (Minton, 1988), selection conditions on operators (Anderson, 1983;
Mitchell, Utgoff, & Banerji, 1983), numerical strengths on operators (Langley, 1985), or
similar methods. In contrast, macro-operators decrease the average depth of the space
by composing the conditions and actions of operator sequences into individual operators
(Anderson, 1983; Iba, 1989; Korf, 1985). Both of these forms of learning can greatly
improve the quality of a system's search for a solution to a problem, and sometimes they
can also improve the quality of the solution itself.
A close examination of the transition from SUM to MIN indicates that a model based
strictly on symbol-level learning can explain some shifts, but has difficulties explaining
others. Let us consider in turn the strategy differences that appear in Table 1. In lines
1 and 2, subjects learn to assign addends to their hands based on the addends' relative
sizes. Provided this type of feature is included in its representation language, a symbollevel learner can easily discover the feature's relevance, based on failures in generating
correct answers.
The procedure for representing an addend on the left hand (lines 4-7) in the SUM strategy
is replaced by a single line in the MIN strategy, which simply asserts the output of the
procedure. This shift could possibly be modeled with macro-operators, except that they
would also force the appropriate number of fingers to be raised on the left hand. However,

16

R.M. JONES AND K. VANLEHN

children can generate the correct counter value without raising fingers. This indicates that
they can determine which output of the procedure is relevant to the task at hand. Macrooperators could not model this, because incrementing the counter is always paired with
raising a finger in the SUM strategy. As we shall see later, our model predicts that children
can identify the relevant output of the procedure by learning new preconditions on the
operator that terminates the procedure.
The next difference between the two strategies appears in the procedure for representing
the right-hand addend (lines 9-12). The only difference between the two procedures is the
termination criterion for the loop. This is a somewhat simpler transition than the previous
one, but it still causes problems for a symbol-level learner. The children appear to learn that
recognizing the number of raised fingers on a hand is a better stopping criterion than using
the value of the counter. This discovery occurs even though both criteria generate correct
answers. However, recognizing the number of raised fingers serendipitously leads to a more
efficient solution, because the counter no longer has to be zeroed in order to represent each
addend (lines 8 and 9). This in turn makes the final loop (lines 14-17) unnecessary in the
MIN strategy, because the correct answer is already available.
Once again, it is difficult to see how a symbol-level learner could account for this representation shift. Our model determines that the number of fingers raised on a hand is highly
correlated with the value of the addend being represented. It eventually replaces the counter
value as the loop termination criterion because it allows the loop to terminate faster. This
transition requires the system to change its preconditions for the operator that terminates the
loop. Thus it involves knowledge-level adjustment of the domain representation and cannot
be explained simply in terms of knowledge tuning or the formation of macro-operators.
Our analysis of the differences between the SUM and MIN strategies, together with Siegler
and Jenkins' findings, provide some strict criteria that a model of the SUM-to-MIN transition
should meet. First, the model should proceed from usage of the SUM strategy to the MIN
strategy without any outside instruction (other than feedback on the correctness of the
answers). It should invent the same transitional strategies that Siegler and Jenkins found in
their subjects. It also must account for the ability to invent new strategies even when there
are no impasses to drive learning. Finally, the model must incorporate a mechanism for
knowledge-level learning, so that it can adapt its representation of the task domain. GIPS,
the model we describe in the next section, meets these criteria.
3.

The General Inductive Problem Solver

GIPS is a problem solver that uses flexible means-ends analysis as its performance mechanism (Jones, 1993; Langley & Allen, 1991). Its learning mechanism is based on Schlimmer's (1987; Schlimmer & Granger, 1986a, 1986b) STAGGER system, which uses a probabilistic induction technique to learn concept descriptions from examples. GIPS uses its
induction algorithm to learn search-control knowledge for its operators, assigning credit and
blame in a manner similar to SAGE (Langley, 1985) and LEX (Mitchell, Utgoff, & Banerji,
1983). However, GIPS also uses probabilistic induction to learn new preconditions on its
operators, thus modifying the descriptions of the operators themselves. Inductive modification of preconditions (as opposed to inductive modification of search-control knowledge)

CHILDREN'S ADDITION STRATEGIES

17

Table2. A GIPSoperatorto incrementthe value of a counter.
COUNT(?Hand, ?Initvalue, ?Finalvalue)
Preconditions:
Hand(?Hand)
Just-raised(?Hand)
Counter-value(?Initvalue)
Add conditions:
Counter-value(?Finalvalue)
Delete conditions:
Counter-value(?Initvalue)
Just-raised(?Hand)
Constraints:
?Finalvalue is ?Initvalue + 1

appears to be a new machine-learning technique. Although it could be risky, in that it seems
capable of destroying the correctness of the operator set, we show that when properly controlled, it can produce correctness-preserving speed increases that standard techniques have
not been able to produce. From a cognitive-modeling perspective, both learning about
search control and learning new operator representations play crucial roles in the SUM-toMIN transition.

3.1.

Representation of the addition domain

In this section, we describe GIPS' representation of the task domain. GIPS describes the
world as a set of relations between objects. In the domain of addition, these objects and
relations include the numbers that are part of the problem, the state of the problem solver's
"hands" while it is adding, and the value of a counter that the problem solver keeps "in its
head." In addition, GIPS represents possible actions in the domain with operators that are
similar in representation to those used by STRIPS (Fikes & Nilsson, 1971). Each includes
a set of preconditions, add conditions, delete conditions, and possibly a set of constraints
on the variable bindings.
As an example, consider the operator in Table 2, which increments the value of the counter.
This operator has three variable parameters, ?Hand, ? I n i t v a l u e , and ? F i n a l v a l u e (throughout this paper, an atom beginning with "?" represents a variable). The
preconditions for the operator check the current value of the counter and make sure that
the system has just raised a finger that needs to be counted. The constraint generates a
final value for the counter by incrementing the initial value. GIPS' constraint mechanism
allows constraints to propagate forwards or backwards, so this constraint can also compute
the necessary initial value if it is given the final value as a goal. Finally, When the operator
executes, it will delete the initial value of the counter and record the final value. In addition,
it will delete the "just-raised" condition so that the finger will not be counted twice.
GIPS represents the addition domain with the 16 operators presented in Table 3. There
are two particular operators, which we refer to as the ADDEND-REPRESENTED operators,
that are involved in most of the strategy shifts. For future reference, the series of precon-

18

R.M. JONES AND K. VANLEHN

Table 3. Operators for the addition domain.

SELECT-HAND: Select an addend to be counted on each hand. The left hand is
always counted first.
COUNT-OUT-LEFTHAND: Represent or count the left-hand addend.
COUNT-OUT-KIGHTHAND: Represent or count the right-hand addend.
START-COUNT: Keep track of the counter value while raising fingers.
START-RAISE: Begin raising fingers in order to represent an addend.
RAISE-FINGER: Raise a finger.
COUNT: Count the last raised finger by incrementing the counter value.
LEFT-ADDEND-REPRESENTED: Stop counting and raising fingers on the
left hand.
RIGHT-ADDEND-KEPRESENTED: Stop counting and raising fingers on the
right hand.
CLOBBER-COUNTER: Set the counter value to zero.
COUNT-UP-BOTH-ADDENDS: Make sum both addends have been counted together.
START-MARK-COUNT: Keep a running count while marking raised fingers.
MARK-FINGER,: Mark a finger that has already been raised.
~V[ARK-COUNT: Count the last marked finger by incrementing the counter value.
END-MARK-COUNT: Stop marking fingers on a hand.
DETERMINE-ANSWER: Announce the answer.

Table 4. A series of preconditions for LEFT-ADDEND-REPR/~SENTED.

SoMstrategy(~:

Raising(LeftHand)
Assigned(LeftHand,?Value)
Counter-value(?Value)
SHOR~ StrMs~at~y(c):

Raising(LeftHand)
Assigned(LeftHand,?Value)
Raised-fingers(LeftHand,?Value)

SL~s~ategy ~):

Raising(LeftHand)
Assigned(LeftHand,?Value)
Counter-value(?Value)
Raised-fingers(LeftHand,?Value)
MiNsU~y(d):

Raising(LeftHand)
Assigned(LeftHand,?Value)

ditions that the LEFT-ADDEND-REPRESENTED operator acquires in going from S U M to
MIN appears in Table 4. For our study, we initialized GIPS' search-control and precondition knowledge for the 16 operators such that the system generates the SUM strategy on
addition problems. We will discuss this initialization in more detail after presenting Gins'
performance algorithm and low-level knowledge representation.

3.2. Performance algorithm
As mentioned above, GIPS' problem-solving algorithm (see Table 5) is a form of flexible
means-ends analysis, borrowed from the EUREKA system (Jones, 1993). As with standard
means-ends analysis, the algorithm is based on trying to achieve a state change. The
desired change is represented by a T R A N S F O R M , which is simply a pair consisting of
the current state and some goals (an example appears in Table 6). In order to achieve

CHILDREN'S ADDITION STRATEGIES

19

Table 5. GrPS' algorithmfor solving problems.

T R A N S F O R M ( C u r S t a t e , G o a l s ) : Returns N e w S t a t e
If C u r S t a t e satisfies Goals
T h e n Return N e w S t a t e as C u r S t a t e
Else Let OpSet be the ordered set of s e l e c t e d
operator instantiations;
Let FailedOps be Nil;
Loop for O p e r a t o r in OpSet
Let T e m p S t a t e be A P P L Y ( C u r S t a t e , O p e r a t o r ) ;
If T e m p S t a t e is "Failed State"
T h e n p u s h O p e r a t o r onto FailedOps and c o n t i n u e loop;
Let T e m p S t a t e be T R A N S F O R M ( T e m p S t a t e , G o a l s ) ;
If T e m p S t a t e is "Failed State"
T h e n p u s h O p e r a t o r onto FailedOps and c o n t i n u e loop;
Store (CurState,Goals) as a p o s i t i v e example for the
s e l e c t i o n concept of Operator;
Store (CurState,Goals) as a n e g a t i v e example for the
s e l e c t i o n concept of each operator in FailedOps;
R e t u r n N e w S t a t e as T e m p s t a t e
End loop;
Return N e w S t a t e as "Failed State";
A P P L Y ( C u r S t a t e , O p ) : Returns N e w S t a t e
Let P be PRECONDITIONS(Op);
If C u r S t a t e satisfies the e x e c u t i o n concept of Op
T h e n If the user says Op is executable
T h e n Store C u r S t a t e as a p o s i t i v e example for the
e x e c u t i o n concept of Op;
Return N e w S t a t e as EXECUTE(CurState,Op)
Else Store C u r S t a t e as a n e g a t i v e example for the
e x e c u t i o n concept of Op;
Let T e m p S t a t e be T R A N S F O R M ( C u r S t a t e , P ) ;
If T e m p S t a t e is "Failed State"
Then Return N e w S t a t e as "Failed State"
Else Return N e w S t a t e as A P P L Y ( T e m p S t a t e , O p ) ;

this transformation, GIPS selects an operator and attempts to apply it. If the operator's
preconditions are met, GIPS executes it and the current state changes. If some of the
preconditions are not met, a new TRANSFORM is created with the preconditions as the
new goals. When this TRANSFORM is achieved, GIPS returns to the old TRANSFORM
and attempts again to apply the operator. So far, this is simply a description of standard
means-ends analysis.
The difference between standard and flexible means-ends analysis occurs in the selection
of an operator to apply. Standard means-ends analysis requires that the actions of any
selected operator directly address the goals of the TRANSFORM. In flexible means-ends
analysis, operator selection is determined by a selection algorithm that can use any criteria
to choose an operator. In order for the selection algorithm to be useful, it is usually under
the direct control of the system's learning mechanism. In GIPS, operator selection is determined by selection concepts. Each operator is associated with an explicit concept that

20

R.M. JONES AND K. VANLEHN

Table 6. An example of a Tt~ANSFORM for the addition domain.

Current State:
On-Paper(First,Two)
On-Paper(Second,Three)
Assigned(LeftHand,Three)
Counter-Value(Zero)
Raised-Fingers(LeftHand, Zero)

Goals:
Raising(LeftHand)
Assigned(LeftHand,?Value)
Counter-Value(?Value)

indicates when it should be selected. If the concept depends mostly on the current state of the
TRANSFORM, then the operator will act like a forward-chaining inference rule and execute
whenever the state is appropriate, regardless of the current goals. If the concept depends
mostly on the goals of the TRANSFORM, then it will act like a backward-chaining inference rule. Typically, forward and backward operators intermingle during problem solving,
yielding a psychologically plausible blend of goal-directed and opportunistic behavior.
In (lips, each operator has a selection concept. The representation of a selection concept
is similar to the representation of a TRANSFORM, consisting of a set of literals (predicates
that may or may not be negated) describing the current state and goals. In addition, however,
each literal in a selection concept has two numerical values associated with it: sufficiency
and necessity. In order to evaluate the selection value of an operator, QIps matches the
literals against the current TRANSFORM. It determines the subset of literals that match
(M) and fail to match (F), then calculates
Selection Value = Odds(C) I I
LEM

SL I I NL,
LEF

where Odds(C) is the prior odds that the concept's operator is worth selecting, SL is the
sufficiency of the literal, L, with respect to the concept, and NL is the necessity of L
with respect to the concept. A sufficiency score that is much greater than 1 indicates that
a literal is very sufficient for the selection concept. That is, if SL is a high value, then
the selection value will be high if the literal, L, appears in the current TRANSFORM. In
contrast, a literal is very necessary if the necessity value is much less than 1. In other words,
if NL is low, it means that the selection value will likely be low unless L appears in the
c u r r e n t TRANSFORM.

The above formula is used by STACGER, Schlimmer's (1987) concept formation system,
to estimate the odds that a given object is an instance of a particular concept. However,
a major difference between STAGGER and GIPS is that STAGGER worked exclusively
with propositional knowledge representations. In contrast, the literals in GIPS' concepts
are general predicates that can also contain variables. This means that the relations in a
given TRANSFORM will generally only partially match the relations in a concept, and the
TRANSFORM m a y in fact match the concept in more than one way. In these cases, G I P S
finds a number of partial matches and calculates a selection value for each one. Each of these
matches in turn represents a different instantiation of the operator attached to the selection
concept. Thus, the selection procedure typically returns a number of different instantiations
of a number of different operators. When all the operator instantiations have been found

21

CHILDREN'S ADDITION STRATEGIES

Table 7. The initial selection and execution concepts for LEFT-ADDEND-REPRESENTED.
Sele~Mn
CURR~TSTA~
Raising(LeftHand)
Assigned(LeftHand,?Value4)
Counter-Value(?Value4)
Raised(LeftHand)
Counted(LeftHand)

S
1.03
1.03
1.03
0.01
0.71

N
0.98
0.98
0.98
1.01
1.01

S

N

GOALS
Counter-Value(?Value4)
Raised(LeftHand)
Counted(LeftHand)

S

N

1.00 1.00
I0.00 0.91
i0.00 0.91

Execution

CURR~TSTA~

Assigned(LeftHand,?Value4)
Raising(LeftHand)
Counter-Value(?Value4)

2.71 0.05
2.71 0.05
2.71 0.05

and their selection values have been calculated, GIPS throws out all the instantiations with
a selection value less than 1. The remaining instantiations are ordered according to their
selection values.
GIPS differs from standard means-ends systems in one more important way. In standard
problem-solving systems, each operator has a set of preconditions, which are used in two
ways. First, they determine when the operator can execute. Second, they dictate which
subgoals should be set up via means-ends analysis when an operator is not yet executable.
GIPS uses the preconditions to set up subgoals, but it does not use them to determine the
executability of the operators. Rather, each operator has an associated execution concept
that dictates when the system will try to execute it. GIPS' execution concepts are similar in
form to selection concepts, except they contain literals describing the current state but not
the current goals.
As mentioned previously, GIPS' initial selection and execution concepts were set up to
generate the SUM strategy for addition. The literals of each operator's selection concept
were set to the preconditions and the goals that the operator could satisfy. The necessity and
sufficiency of these literals were set so that they would be retrieved in either a backwardchaining or forward-chaining fashion, depending on the role of the operator in the domain.
For example, pure backward-chaining operators had each of their goal literals set with high
sufficiency. Forward-chaining operators had each of their current state literals set with
high necessity. Finally, each operator had an initial set of preconditions, and the execution
concept for each operator was initialized to the literals occurring in the preconditions, each
with high necessity.
As an example, the initial preconditions for LEFT-ADDEND-REPRESENTED appear in
Table 4(a), with its initial selection and execution concepts in Table 7. An examination of
Table 7 shows that LEFT-ADDEND-REPRESENTED is likely to be selected when the current
TRANSFORM'S goals include R a i s e d ( L e f t H a n d )
or C o u n t e d ( L e f t H a n d )
(high S value), unless these literals also appear in the TRANSFORM'S current state (low S
value). The operatoris setto execute only when allthreeof As s i g n e d (Le f tHand,
?Value4), R a i s i n g (LeftHand), and C o u n t e r - V a l u e
(?Value4) are
matched by literals in the current TRANSFORM's state description (medium S value and
low N value).

22

R.M. JONES AND K. VANLEHN

From our description of GIPS' general problem-solving algorithm, it is clear that there are
exactly two types of choice that the system has to make while solving problems: the choice
of which operator to apply next, and the choice of whether to execute an operator or subgoal
on its preconditions. It is appealing to apply a uniform learning and decision mechanism
in a system's performance algorithm, so GIPS uses its probabilistic concept-matching and
learning mechanism for both of these decision points. Other problem solvers include
additional types of decisions. For example, PRODIGY (Minton, 1988) makes an explicit
decision for which subgoal to work on next, whereas that decision is implicit in (liPS'
operator-selection decision. Our experiences indicate that a STACGER-like algorithm can
be used at any type of decision point, providing the same kinds of learning benefits to each.
Thus, if we decided to have GIPS make an explicit choice about which subgoal to work on
next, we would also use the concept-matching algorithm for that, enabling the system to
learn and improve its behavior for choosing subgoals as well. In the following section, we
discuss how execution and selection concepts change with experience. More importantly,
we explain how changes in the execution concepts directly lead to representation changes
in the operator preconditions.

3.3. Learning in GIPS
GIPS adjusts its selection concepts on the basis of its successes and failures while solving
problems. When a TRANSFORM is finally solved, GIFS adjusts the sufficiency and necessity
values of the successful operator so that the operator will be rated even higher the next time
a similar TRANSFORM occurs. For each operator that initiated a failure path (i.e., it took the
first step offa TRANSFORM's solution path), GIFS adjusts the values in its selection concept
so that it will receive a lower value next time. Note that GIPS considers every TRANSFORM
to be a "problem," so it can learn about any particular TRANSFORM even if it doesn't lie
on the solution path to some global problem. In order to do this kind of learning, GIPS
must store the current solution path and every operator that led off it. However, as soon as
each individual TRANSFORM in a problem is finished, and the updating is completed, that
portion of the solution path is discarded.
This method of assignment for credit and blame is similar to the method used by other
problem-solving systems that include concept-learning mechanisms (Langley, 1985;
Mitchell, Utgoff, & Banerji, 1983). These systems (and GIPS) can easily assign credit
and blame, because they backtrack until they find a solution to the current problem. Then,
each decision that leads off the final solution path is classified as a bad decision (a negative example), and each decision that lies on the final solution path is classified as a good
decision (a positive example).
However, GIPS differs from these previous systems in the details of its concept-learning
algorithm. GIFS computes the sufficiency and necessity scores for literals in a concept (SL
and NL) with the following equations:

SL = P(L matches I I I C C)
P(L matches I ] I ¢~ C ) '

23

CHILDREN'S A D D I T I O N STRATEGIES

NL -- P(L does not match I I I E C)
P(L does not match I I I ¢ C ) '
where I E C means that TRANSFORM instance, I, is a positive example of the concept,
C, and "L matches I " means that literal, L, of the concept is matched by some literal
in I. Thus, sufficiency and necessity for each literal are determined by four conditional
probabilities.
(liPS learns by updating its estimates of these conditional probabilities. For each literal
in a selection concept, GIPS records four values: t, the total number of examples (positive and negative) that the system has stored into this selection concept; p, the number
of those that were positive examples; l, the total number of times this literal has been
matched by any (positive or negative) example; and c, the number of times the literal has
been matched in a positive example. In precise form, the conditional probabilities are
estimated by
c

P(L matches I I I E C) -- - ,
P

1-c
P(L matches I I I ¢ C) - t - p'
P(L does not match I

I I C C) - p - c,

P(L does not match I I I ¢ C)

P
-

t+c-p-1
t-p

As indicated in the algorithm in Table 5, GIPS learns by storing an instance (the literals
describing the state and goals of the current TRANSFORM) as a positive or negative example
of an operator's selection concept (depending on whether the operator led to a solution
or a failed search path). Every time the system stores a TRANSFORM as a positive or
negative example, it matches the literals in the TRANSFORM to the literals in the selection
concept. If there are any literals in the new instance that do not already appear in the
selection concept, they are added into the selection concept's representation. Finally, GIPS
increments the appropriate counts for each literal: always incrementing t, incrementing p
if the instance is a positive example, incrementing l if the literal is matched by a literal
in the instance, and incrementing c if the instance is a positive example and the literal
is matched. For the interested reader, Schlimmer (1987; Schlimmer & Granger, 1986a,
1986b) provides excellent, detailed descriptions of the learning algorithm and its behavior
in classification tasks.
We have so far described how GIPS updates its selection concepts. These concepts
determine when operators are selected to achieve a TRANSFORM, SOthey represent searchcontrol knowledge. As we have mentioned, the system also must adapt its execution concepts. The conditional probabilities are updated identically to selection concepts. However,
the assignment of credit and blame is a bit different.
Assignment of credit and blame for execution concepts can be computed in a manner
similar to credit and blame for selection concepts. When GIPS thinks that a particular

24

R.M. JONES AND K. VANLEHN

operator should execute, it blindly carries on and eventually generates an answer. However,
it is possible that the answer will be wrong, indicating that the operator should not have
executed (i.e., GIPS' execution concept for that operator is wrong). If GIPS is allowed to
backtrack until it eventually generates the correct answer, it can precisely determine which
situations should be stored as negative and positive instances of the execution concept, just
as with selection concepts. We discovered that in some instances, when GIPS unfortunately
generated multiple "bad" execution concepts, backtracking could take quite a while before
the system would generate a correct answer and do the appropriate learning. We finessed
this problem by giving the system immediate feedback on whether it would generate a
correct answer when it attempted to execute operators.
Unfortunately, this does not tell the whole story on credit and blame assignment. In
Siegler and Jenkins' study, they awarded each subject a gold star after the subject gave a
correct answer, but they did not force the subjects to keep working on the problems until
they could give a correct answer, as we do with GIPS. For a strict model of this experiment,
we would give GIPS feedback after it generates a complete solution, and not force the
system to backtrack. However, if GIPS is not allowed to backtrack, it must incorporate an
incremental credit-assignment algorithm, such as the bucket-brigade algorithm (Holland,
Holyoak, Nisbett, & Thagard, 1986). In our study, we were more concerned with the
order of acquired strategies than the speed of acquisition, so we did not implement such
an algorithm in the current version of GIPS. We are convinced that a more realistic creditassignment algorithm would slow down learning, but would not disturb the order of strategy
acquisition. However, future research with GIPS should certainly address this issue.
The final aspect of learning in Gins involves changing the preconditions on operators.
When GIPS successfully predicts that an operator should execute, but the probabilistic
execution concept does not agree with the current preconditions of the operator, the system
changes the preconditions appropriately. Operator preconditions in Gins contain only the
literals from the execution concept that GIPS has decided are very necessary. This symbolic
representation is used to post subgoals when the system wants to apply an operator that it
believes cannot yet be executed. Recall that a TRANSFORM includes literals representing
the current goals, and these are matched against selection concepts for the operators. Thus,
changing operator preconditions can lead directly to subsequent changes in the selection of
operators while solving problems.
Logically, GIPS should include in its preconditions for an operator exactly the literals that
are highly necessary (i.e., have very low values for NL). In problem-solving terms, all the
literals in the preconditions of an operator should be true (or matched) for the operator to be
executable. Thus, it should add literals from the execution concept that have low NL values
to the preconditions, and it should drop literals that do not have low NL values from the
preconditions. However, in order to speed up GIPS' learning, we have adopted a heuristic
approach for each of these cases.
First, consider the criterion for adding a new literal to the preconditions of an operator.
Again, GIPS should ideally consider this action for any literal with a low value for NL. An
examination of the equation for NL shows that it decreases as P(L does not match I I [
C) increases. Learning about necessity poses some difficulties, because GIPS can increase
its estimate of P(L does not match I I I ~ C) only when it predicts that the operator

CHILDREN'S ADDITION STRATEGIES

25

associated with C should execute, but the user tells the system that it made an incorrect
prediction (an error of commission). However, GIPS is generally conservative in attempting
to execute operators, so this type of event is relatively rare. Thus, GIPS takes a long time
to learn that any new literal is necessary for execution. To overcome this difficulty, we
allow GIPS to use a different criterion for adding preconditions. Rather than looking for
literals that are very necessary, it looks for literals that are somewhat sufficient (i.e., have
relatively high values for S¢). Mathematically speaking, sufficiency is not a valid predictor
of new preconditions, but it does have some heuristic value, because literals that are very
necessary are also always somewhat sufficient (if not very sufficient). This heuristic can
encourage GIPS to make errors of commission, and thus learn whether the new literal really
is necessary to the execution concept.
Now let us consider the case of dropping a literal from the preconditions of an operator
when its value for NL becomes too big. Again looking at the equation for NL, we see that
NL increases as P(L does not match I I I E C) increases. This corresponds to the case
where GleS correctly predicts that the operator associated with C should execute, but L
does not appear. Intuitively, this means that L is not necessary for the operator to execute,
so we have some evidence that it should be removed from the preconditions. As far as the
system is concerned, it is learning that there is no reason to subgoal on a literal if it is not
actually a necessary precondition for the operator.
A "proper" implementation of the STAGGER algorithm for this case would increment the
estimate for P(L does not match I I I E C) slightly every time the operator successfully
executes "early?' Thus, it would slowly gather evidence that L is not necessary, and it
would eventually delete L from the preconditions of the operator. The algorithm requires
substantial evidence before it will do this, because it must learn that L really is unnecessary and that the system has not simply encountered a noisy instance. In order to speed
up GIPS' learning of preconditions, we assume that the feedback on operator execution
from the user is always correct (i.e., not noisy). This allows us to bias GIPS to increase
P(L does not match I I [ E C) by a large value for this type of instance rather than just by
a small increment. Thus, GIPS can drop a literal, L, from its preconditions for an operator
thefirst time it successfully executes that operator without the presence of L, rather than
waiting for a large number of confirming experiences.

4.

Strategy acquisition in the addition domain

This section presents GIPS' behavior through a series of different strategies for adding
numbers. These strategy shifts arise from the learning algorithm incorporated into the
system, and they correspond well with the shifts observed by Siegler and Jenkins. Siegler
and Jenkins classified their subjects' behavior into eight strategies, of which four were based
on counting (the others involved various kinds of recognition and guessing, which GIPS
does not model). In this section, we describe each of the four counting strategies in the
order in which they generally appear. However, it is important to note that children always
intermingle their strategies, sometimes even on a trial-by-trial basis. We will discuss the
issue of strategy variability in the following section.

26

R.M. JONES AND K. VANLEHN

4.1. The SVM strategy

GIPS' initial strategy for addition is the SUM strategy. To better follow GIPS' behavior,
we have provided a trace of the sum strategy for the problem 2 + 3 in Table 8. In this
trace, we have omitted calls to the T r a n s f o r m function, only listing when operators
are selected to apply and when they actually execute. The first thing the system does
is assign an addend to each hand. For example, when adding 2 and 3, the system may
assign the number 2 to the left hand and the number 3 to the right hand. However, in this
strategy the order of the addends does not make a difference, so it could just as easily have
switched them.
Next, the system begins its procedure of raising and counting a set of fingers on each
hand. To accomplish this task, the ADDEND-REPRESENTED operators use a counter to
determine when an addend is finished being represented on each hand (see Table 4(a)). For
example, the preconditions of LEFT-ADDEND-REPRESENTED demand that the system be
raising fingers on the left hand, and that the value of the counter be equal to the value of
the left-hand addend. These preconditions are set up as subgoals, causing the selection
of the START-RAISE and START-COUNT operators, which initialize the forward-chaining
procedure of raising and counting fingers one at a time. These operators execute alternately
until LEFT-ADDEND-REPRESENTED can execute, when the correct number of fingers have
been counted on the left hand.
After the left hand has been counted, the CLOBBER-COUNTER operator immediately
executes. This operator executes when all the fingers of a hand have been raised along with
a running count. Its effects are to zero the value of the counter to prepare it for the next
hand, and to mark the current hand as uncounted, because the counter's value has been
changed. This entire procedure then repeats with the right hand.
After both hands have been counted, DETERMINE-ANSWER checks whether it can execute. It can only execute if both hands are marked as counted, but CLOBBER-COUNTER
has caused this to be false. Therefore, the system again attempts to count up fingers on each
hand, this time marking fingers that are already raised. For this procedure, no CLOBBERCOUNTER is necessary, because the number of raised fingers (rather than the value of the
counter) is used to terminate the count for each hand. Finally, after each hand has been
counted for the second time, GIPS announces the answer.
As the system repeatedly solves addition problems, it continuously updates the execution concepts for the two ADDEND-REPRESENTED operators. After a while, these two
concepts encode several regularities that are always true when these operators execute.
For example, there are always two addends in the problem description, and the number
of "marked" fingers is always zero. Most importantly, however, the concepts encode the
number of raised fingers as always equal to the counter value (which in turn is equal to
the goal value for counting an addend). Literals representing this fact eventually get added
into the preconditions for the ADDEND-REPRESENTED operators (see Table 4(b)). This
action alone does not change the system's outward behavior, but it proves important for
later strategies.

27

CHILDREN'S ADDITION STRATEGIES

Table 8. GIPS' initial SUM strategy for addition.

Apply SELECT-HAND
(LeftHand, Two, RightHand, Three)
Execute SELECT-HAND
(LeftHand, Two, RightHand, Three)
Apply DETERMINE-ANSWER(?Answer)
Apply COUNT-OUT-LEFTHAND(?Value70)
Apply LEFT-ADDEND-REPRESENTED
(?Value70)
Apply START-RAISE(LeftHarld)
Execute START-RAISE(LeftHand)
Apply START-COUNT(LeftHand)
Execute START-COUNT(LeftHand)
Apply RAISE-FINGER(LeftHand, l)
Execute RAISE-FINGER(LeftHand, l)
Apply COUNT (l)
Execute COUNT (1)
Apply RAISE-FINGER(LeftHand, 2)
Execute RAISE-FINGER(LeftHand, 2)
Apply COUNT (2)
Execute COUNT (2)
Execute LEFT-ADDEND-REPRESENTED(2)
Execute COUNT-OUT-LEFTHAND(2)
Apply CLOBBER-COUNTER
Execute CLOBBER-COUNTER
Apply COUNT-OUT-RIGHTHAND(?Valuel20)
Apply RIGHT-ADDEND-REPRESENTED
(?Value120)
Apply START-RAISE(RightHand)
Execute START-PAISE(RightHand)
Apply START-COUNT(RightHand)
Execute START-COUNT(RightHand)
Apply RAISE-FINGER(RightHand, 1)
Execute RAISE-FINGER(RightHand, 1)
Apply COUNT (1)
Execute COUNT (1)
Apply RAISE-FINGER(RightHand, 2)
Execute RAISE-FINGER(RightHand, 2)
Apply COUNT (2)
Execute COUNT (2)
Apply RAISE-FINGER(RightHand, 3)
Execute RAISE-FINGER(RightHand, 3)
Apply COUNT (3)
Execute COUNT (3)
Execute RIGHT-ADDEND-REPRESENTED(3)
Execute COUNT-OUT-RIGHTHAND(3)

Apply COUNT-UP-BOTH-ADDENDS(?Answer)
Apply CLOBBER-COUNTER
Execute CLOBBER-COUNTER
Apply COUNT-OUT-LEFTHAND(?ValueI58)
Apply END-MARK-COUNT
(LeftHand, ?Value158)
Apply START-MARK-COUNT(LeftHand)
Execute START-MARK-COUNT(LeftHand)
Apply MARK-FINGER(LeftHand, 1)
Execute MARK-FINGER(LeftHand, 1)
Apply MARK-COUNT(1)
Execute MARK-COUNT(1)
Apply MARK-FINGER(LeftHand, 2)
Execute MARK-FINGER(LeftHand, 2)
Apply MARK-COUNT(2)
Execute MARK-COUNT(2)
Execute END-MARK-COUNT(LeftHand, 2)
Execute COUNT-OUT-LEFTHAND(2)
Apply COUNT-OUT-RIGHTHAND(?Value208)
Apply END-MARK-COUNT
(RightHand, ?Value208)
Apply START-MARK-COUNT(RightHand)
Execute START-MARK-COUNT(RightHand)
Apply MARK-FINGER(RightHand, 1)
Execute MARK-FINGER(RightHand, 1)
Apply MARK-COUNT(3)
Execute MARK-COUNT(3)
Apply MARK-FINGER(RightHand, 2)
Execute MARK-FINGER(RightHand, 2)
Apply MARK-COUNT(4)
Execute MARK-COUNT(4)
Apply MARK-FINGER(RightHand, 3)
Execute MARK-FINGER(RightHand, 3)
Apply MARK-COUNT(5)
Execute MARK-COUNT(5)
Execute END-MAP,K-COUNT(RightHand, 5)
Execute COUNT-OUT-RIGHTHAND
(RightHand, 5)
Execute COUNT-UP-BOTH-ADDENDS(5)
Execute DETERMINE-ANSWER(5)

28

R.M. JONES AND K. VANLEHN

Table 9. GIPS' SHOI%TCUT SUM strategy.
A p p l y SELECt-HAND
(LeftHand, Two, RightHand, Three)
E x e c u t e SELECT-RAND
(LeftHand, Two, RightHand, Three)
A p p l y DEa~hM~NE-ANSWER (?Answer)
A p p l y COUNT OUT-LEFmAND (?Value988 )
A p p l y LEFT-ADDEND-REPRESENTED
(?Value 988 )
A p p l y START-RA*SE (LeftHand)
E x e c u t e START-RArsE (LeftHand)
A p p l y RmSE-~GER (LeftHand, i)
E x e c u t e RASE-F~OER (LeftHand, i)
A p p l y COUNT (i)
E x e c u t e COUNT (i)
A p p l y RAISE-FINGER (LeftHand, 2)
E x e c u t e RAISE-~NOER (LeftHand, 2)
A p p l y COUNT (2 )
E x e c u t e COUNT (2)
E x e c u t e LF2r-ADDEND-REP~SmCmD (2)
E x e c u t e COUNT-OUT-LEFII{AND (2)
A p p l y COUNT-OUT-RIOHTHAND (?Value1022 )

4.2.

PdGHT-ADDEND-Rm,~ENTED
( ?Value1022 )
A p p l y START-RAISE (RightHand)
E x e c u t e START-~SE (RightHand)
A p p l y RASE-~GER (RightHand, i)
E x e c u t e RAtSDnNGER (RightHand, i)
A p p l y COUNT (3 )
E x e c u t e COUNT (3)
A p p l y RAtSE-~NGER (RightHand, 2)
E x e c u t e RASm~NOER (RightHand, 2)
A p p l y COUNT (4)
E x e c u t e COUNT (4)
A p p l y RmSE-~NGEN (RightHand, 3 )
E x e c u t e RAISmF~GEN (RightHand, 3)
A p p l y COUNT (5)
E x e c u t e COUNT (5)
E x e c u t e RIGHT-ADDEND-REPRBSENTED (3 )
E x e c u t e COUNT-OUT-RIGHYHAND (3)
A p p l y COUNT-UP-BOTH-ADDENDS (?Answer)
E x e c u t e COUNT-UP-BOTH-ADDENDS (5 )
E x e c u t e DmmRM~E-ANSWER (5 )

Apply

The SHoRrcvr SvM strategy

After the new preconditions have been added and a number of addition problems have been
solved, the new literals in GIPS' execution concepts for LEFT-ADDEND-REPRESENTED
and RIGHT-ADDEND-REPRESENTEDbecome so strong that GIPS decides that the operators should execute when the number of fingers raised on a hand is equal to the goal value
even though the system has not yet incremented its count for the last finger. It turns out
that the system can successfully solve the addition problem even if it executes this operator
prematurely, so it deletes the condition that the current counter value must be equal to the
goal value in the preconditions of the ADDEND-REPRESENTED operators (see Table 4(c)).
This change has a direct effect on GIPS' behavior (see Table 9). When attempting to
apply LEFT-ADDEND-REPRESENTED,the value of the counter no longer appears in the
preconditions, so it is not posted as a subgoal. This means that the START-COUNT operator
is no longer selected. Thus, a running count is still kept while raising fingers, but the
counter is not marked for use as the termination criterion. This means that CLOBBEKCOUNTER will not execute, and that leads to two changes in strategy. First, the counter is
not reset to zero after counting the left hand, and counting continues from the left hand's
final value. Second, the hands are not marked as uncounted, so there is no need to count
up the raised fingers again after the two hands have initially been counted. This behavior
corresponds to the SHORTCUT SUM strategy, which was invented by all eight of Siegler
and Jenkins' subjects.
This representation assumes that the student can determine without counting when the
number of raised fingers on either hand is equal to the goal value. For example, in adding
2 + 3, just after saying, "five," and raising a third finger on the right hand, the subject
must see that the number of fingers on the right hand is equal to 3, the number of fingers

CHILDREN'S ADDITION STRATEGIES

29

that they intended to raise. Before they began their study, Siegler and Jenkins tested their
subjects' ability to recognize small numbers of objects without counting, and all the subjects
could perform the skill adequately. Thus, we represent it in GIPS as a primitive skill, or a
simple literal.

4.3. The SHORTCUTMIN strategy
The next shift leads to an intermediate strategy between SHORTCUT SUM and MIN, which
we call SHORTCUT MIN. Although Siegler and Jenkins do not classify SHORTCUT MIN
as a distinct strategy from SHORTCUT SUM, they do note (p. 119) that some of their subjects
begin to switch addends during SHORTCUT SUM So that they start counting with the larger
addend on the left hand, rather than just picking whichever addend appears first in the
problem. GIPS also accounts for this behavior.
An important feature of the SHORTCUT SUM strategy is that the problem solver's counter
value is not equal to the number of fingers being raised on the right hand (i.e., the second
hand). We hypothesize that this causes interference and subsequent failure. Such interference would not occur with the left hand, because the number of raised fingers in the
SHORTCUT SUM strategy is always equal to the value of the counter for that hand. Unfortunately, interference is a phenomenon that GIPS does not yet model, so we were forced
to simulate its effects. We assumed that interference between the value of the counter and
the number of fingers raised on the right hand would cause a child to become confused
and fail to solve the current problem. This behavior is confirmed by Siegler and Jenkins:
"when children used the shortcut-sum approach, they were considerably more accurate on
problems where the first addend was larger than on ones where the second addend was"
(p. 71).
We simulated this process by causing GIPS to fail sometimes during the SHORTCUT
SUM strategy when it decided to count the larger addend on its right hand. These failures
caused the system to update its selection concept for the SELECT-HAND operator. Thus,
GIPS learned to prefer assigning the larger of the two addends to its left hand (information
on the relative sizes of the addends was explicitly included in the state representation). Note
that the learning algorithm does not require a model of interference to make this strategy
shift. It simply records the fact that failure is somewhat correlated with assigning a larger
addend to the right hand.

4.4. The MIN strategy
The final strategy shift occurs in a similar manner to the shift from SUM to SHORTCUT
SUM. At this point, GIPS has attempted to execute the ADDEND-REPRESENTED operators
at various times and has been given feedback each time as to whether it would be able to
solve the current problem if it executed the operator at that time. Thus, it is slowly learning
a "good" concept for when the ADDEND-REPRESENTED operators are executable. One of
the things that proves to be true every time these operators execute is that the goal value for
counting out a hand is equal to the addend assigned to that hand.

30

R.M. JONES

Table 10.

AND

K. VANLEHN

GIPS' MIN strategy.

Apply SELECT-HAND
(LeftHand, Three,
RightHand, Two)
E x e c u t e SELECT-HAND

(Lef tHand, Three,
RightHand, Two)
Apply DETERMINE-ANSWER (?Answer)
Apply COUNT-OUT-LEFTHAND
(?Value1498 )
Apply LEFT-ADDEND-REPRESENTED
(?Value 1498 )
Apply START-RAISE
(LeftHand)
Execute START-RAISE
(LeftHand)
Execute LEFT-ADDEND-REPRESENTED (3 )
E x e c u t e COUNT-OUT-LEFTHAND (3)
Apply COUNT-OUT-RIGHTHAND
(?Value 1516 )
A p p l y RIGHT-ADDEND-REPRESENTED

(?Valuel 516 )

Apply START-RAISE (RightHand)
Execute START-RAISE (RightHand)
Apply RAISE-FINGER
(RightHand, i)
E x e c u t e RAISE-FINGER

(RightHand,
Apply COUNT (4)
Execute COUNT (4)
Apply RAISE-FINGER
(RightHand,

i)

2)

E x e c u t e RAISE-FINGER

(RightHand,
Apply COUNT (5)
Execute COUNT (5)
Execute
Execute

2)

RIGHT-ADDEND-REPRESENTED
COUNT-OUT-RIGHTHAND
(3)

(3)

Apply COUNT-UP-BOTH-ADDENDS
(?Answer)
Execute COUNT-UP-BOTH-ADDENDS
(5)
Execute DETERMINE-ANSWER (5)

Eventually, the system attempts to execute the LEFT-ADDEND-REPRESENTED operator
without having raised any fingers at all (see Table 10). When it succeeds in doing this, it
deletes the precondition that the number of fingers raised on the hand be equal to the goal
value (see Table 4(d)). The system has learned that it can simply start counting from the
goal value for the left hand rather than starting from zero. Note that this behavior could not
be generated directly from the initial SUM strategy, because it requires the counter to be
used for counting the total sum, so it cannot aid in representing the right-hand addend. As
with LEFT-ADDEND-REPRESENTED, GIPS also attempts to execute the RIGHT-ADDENDREPRESENTED operator early, but this leads to failure. Thus, the system begins to exhibit
the MIN strategy, in which the largest number (the left-hand number) is simply announced
and used to continue counting the smaller number as in the SHORTCUT MIN strategy.

4.5.

The FIRst strategy

The only other counting strategy identified by Siegler and Jenkins is the FIRST strategy. It
was used on only six trials, all by the same subject. FIRST is similar to the MIN strategy,
except that it does not assign the larger addend to the left hand. Rather, it starts with
whichever addend is presented first, and continues counting with the second. In GIPS, this
strategy follows from the SHORTCUT SUM strategy when the system does not encounter
interference, and thus does not learn about ordering the addends. While using the FIRST

CHILDREN'S ADDITION STRATEGIES

31

strategy, the system can still eventually generate the MIN strategy through the same type
of failure-driven learning that leads from SHORTCUT SUM to SHORTCUT MIN.

5. Summary and analysis
Both the SUM strategy and the MIN strategy have three main subgoals: to represent the
first addend, to represent the second addend, and to count the union of the representations.
The SUM-to-MIN transition involves three independent modifications to the' SUM strategy:
1. The process of representing the addends is run in parallel with counting up the union.
In the SUM strategy, representing the two addends must be completed before counting
up the union begins.
2. The order of addends is made conditional on their sizes so that the larger addend is
represented by the easier process. That is, any interference that occurs with the second
addend is more likely to occur if the addend is large. Clearly, this strategy change
must take place after the first one, because there is no interference when representing
an addend and counting the union take place separately.
3. The subgoal of representing one addend changes from explicitly constructing a set of
objects to simply saying the addend.
The GIPS account for each of these transitions is as follows. The first transition is caused
by a combination of correlational learning of preconditions and search-control learning.
Initially, GIPS represents an addend on a hand by raising fingers and counting until the
counter's value is equal to the addend value. In two steps, the system learns that recognizing
the number of fingers raised on a hand is a better stopping criterion than the value of the
counter. When the value of the counter disappears as a subgoal of representing an addend,
it is still free to be used to count the union of objects.
Because the counter is no longer required for representing addends, the question arises
of why the system should continue to count as it raises fingers on each hand. This is an
example of impasse-free search-control learning. The COUNT operator is responsible for
incrementing the oral counter. Initially, it is selected only when the subgoal of counting up
an addend is present. Eventually, correlated relations that are present in the current state
(e.g., that a finger has just been raised) come to dominate the selection concept, and the operator becomes a forward-chaining operator. Basically, people have developed the habit of
counting whenever they raise a finger even if that count doesn't serve any direct purpose. The
combination of this habit with learning of new preconditions causes GIPS serendipitously
to achieve the goal of counting the union as it deliberately represents the second addend.
Although GIPS can learn this habit, we gave COUNT a forward-chaining selection concept
in this experiment because there is no direct evidence that the subjects did not learn this
behavior even before they learned the SUM strategy (e.g., when they learned to count on
their hands). However, GIPS predicts that subjects that do not have this habit could generate
a fifth strategy, which we call the LAYOUT SUM strategy. In this strategy (which GTPS
successfully generates), we would expect to see subjects silently represent an addend on

32

R.M. JONES AND K. VANLEHN

each hand and then count up the union of fingers aloud. Siegler and Jenkins did not explicitly
report observing this strategy, but it is possible that they included this behavior as a form
of the SUM strategy.
The second transition from SUM to MIN is caused by failure-driven, search-control learning. Given two apparently equivalent methods, persistent errors in one of them causes the
other one eventually to dominate. Because the errors are correlated with the relative sizes
of the addends, GIPS learns that this is the relevant attribute upon which to base its decision
of which hands to represent addends on.
The third transition once again involves impasse-free correlational learning at the knowledge level. GIPS keeps track of which literals in the situation are correlated with the final
achievement of the goal of representing the first addend. Eventually, it considers these correlated literals to be just as essential as the originally specified preconditions. It eventually
discovers that the originally specified preconditions can be ignored as long as the correlated
literals are achieved. This summary makes clear that impasse-free, correlational learning
of operator preconditions is crucial to the GIPS account for the first and third transitions.
Ordinary symbol-level learning can handle the second.

6.

Discussion

The GIPS analysis helps clarify several important, general issues about strategy change.
Siegler and Jenkins observe that, "Not one child adopted a strategy that could be classified
as indicating a lack of understanding of the goals of addition" (p. 107). In this respect, the
subjects are similar to those of Gelman and Gallistel (1978), who found that very young
children would invent correct strategies for counting a set of objects even when unusual
constraints were placed on them to thwart their normal strategy. Gelman and Gallistel
explain this remarkable competence by hypothesizing that children possess innate (or at
least predetermined) principles of numerousity. Although linguists had earlier proposed the
existence of innate constraints on language development, Gelman and Gallistel provided
the first empirical evidence of innate constraints on non-linguistic development. This set
off a heated debate in the developmental community. Siegler and Jenkins (p. 115) suggest
that such constraints may exist on the development of addition strategies, but they do not
give a specific list.
The initial knowledge given to GIPS does not involve any explicit principles of addition
or counting. It is merely a set of operators and selection preferences that happen to generate
the correct answers. It is able to avoid developing bad strategies because of the feedback
it receives while solving problems. GIPS occasionally attempts to execute an operator in
situations that would produce a wrong answer. If it were not told that the execution was
wrong, it would develop wrong strategies. Thus, GIPS suggests one possible account for
learning without the hypothesized innate constraints.
However, as we have discussed, the feedback we provided to GIPS did not correspond
exactly to the type of feedback that children usually receive. In the Siegler and Jenkins
study, students were told after each trial whether they got the problem right, and then
presented with a new problem. GIPS assumes that it can precisely assign blame to bad
operators by receiving early feedback from the user or by searching until a correct solution

CHILDREN'S A D D I T I O N STRATEGIES

33

can be found. A more proper model of Siegler and Jenkins' feedback would require the
incorporation of an incremental credit-assignment algorithm. On the other hand, if GIPS
w e r e supplied with knowledge about constraints on numbers and arithmetic, it would be
able to assign credit and blame as it solved problems (Ohlsson & Rees, 1991), rather than
after receiving its "gold star" for each problem. Such constraints would generate behavior
similar to GIPS' current implementation, providing the same type of immediate feedback
that the user provided when GIPS attempted to execute operators inappropriately.
A common misconception about discovery is that the newly discovered strategy, concept,
or idea instantly and totally supplants its predecessor. In all protocol-based studies of
discovery (e.g., Kuhn, Amsel, & O'Laughlin, 1988; Siegler & Jenkins, 1989; VanLehn,
1991), the transition between the old strategy and the new one is gradual. For instance,
Siegler and Jenkins (p. 73) report, "In the first five sessions after children discovered the
min strategy, they used the strategy on only 12% of the trials in which they used any of
the three counting strategies." We have not tried to model the gradual transition to the
use of the MIN strategy with GIPS because doing it right would require implementing
several memory-based strategies. However, it is clear that the probabilistic nature of GIPS'
selection and execution concepts would tend to predict a gradual transition.
Starting in the eighth week of the study, Siegler and Jenkins began including "impasse
problems," such as 2 + 23. They had hoped that these would encourage discovery of the
MIN strategy, but they did not, for only one child first used the MIN strategy on an impasse
problem. However, children who had already discovered the MIN strategy began to use it
much more frequently on the impasse problems and even on the non-impasse problems that
followed the eighth week. Glps would tend to do the same thing if it were given impasse
problems. The larger addend would invite errors during the SHORTCUT,SUM and FIRST
strategies, which would lower the values of their selection concepts. The inclusion of
impasse problems would not affect the error rate of the MIN strategy, so it would gradually
become the preferred strategy for all counting trials.
Siegler and Jenkins noticed that some children seemed consciously aware that they had
invented a new strategy in that they could explain it on the first trial where they used it, and
some even recognized that it was a "smart answer," in the words of one child. Other children
denied using the MIN strategy even when the videotape showed that they had used it. For
instance, one child said, "I never counted... I knew i t . . . I just blobbed around." Siegler
and Jenkins divided children into those who seemed conscious of the strategy and those
who did not, and measured the frequency of their subsequent usage of the MIN strategy.
The high awareness group used the MIN strategy on about 60% of the trials where they
used any counting strategy. The low awareness group used the MIN strategy on less than
10% of the trials. This suggests that being aware of a newly discovered strategy facilitates
subsequent usage of it.
This finding cannot be modeled by GIPS because Gins has no way to distinguish a
strategy that can be explained from one that is inaccessible to consciousness. However, the
finding could probably be modeled by combining Gins with a system that uses an analytical
learning algorithm. The basic idea is simple. GtPS would discover a new strategy just as
it does now, and a trace of the strategy's actions would remain in memory. This trace
would be used as an example that is explained by the analytical system. (Siegler and

34

R.M. JONES AND K. VANLEHN

Jenkins asked subjects after each problem to explain how they got their answer--a sort of
mandatory reflection.) If enough of the trace can be recalled for the explanation to succeed,
it annotates the steps in the trace and perhaps the operators whose executions produced the
steps. These elaborations make it easier to retrieve the modified operators from memory,
and they may help in assigning credit and blame, thus speeding the adjustment of the
preconditions, selection, and execution concepts. These influences increase the usage of
the new strategy on subsequent problems.
To conclude our discussion, we address a competing model of the SUM-to-MIN transition. Neches' (1987) HPM is designed to effect strategy changes whenever it detects an
opportunity for improving the efficiency of a procedure. To achieve this, HPM stores a
complete trace of its processing, and constantly monitors this memory with heuristics, such
as "If a sub-procedure produces an output, but no other sub-procedure receives that result by
the time the overall procedure finishes, then modify the overall procedure to eliminate the
superfluous computation" Neches demonstrated that this heuristic and two others sufficed
for changing the SUM strategy into the MIN strategy.
HPM had to produce two transitional strategies before it could get to MIN. Siegler and
Jenkins sought evidence for these transitional strategies in their data. One of the strategies
(the FrosT strategy) occurred six times, all in the protocol of one subject. Moreover, all of
these instances occurred after the subject invented the M~N strategy, whereas HPM must
invent it before it can get to MIN. The second transitional strategy predicted by Neches did
not appear at all. These unfulfilled predictions cast doubt on the HPM model.
Another problem with HPM's account is that it requires the storage of an entire search
tree over several problem-solving attempts. In contrast, GIPS only stores the trace of
the current solution attempt and discards it after learning. As mentioned previously, it
may be useful to explore the use of a different credit-assignment method in GIPS, such
as the bucket-brigade algorithm (Holland et al., 1986). Such an algorithm could allow
the system to avoid storing any of the solution path. As Neches noted, the HPM model
"assumes the relative accessibility of extremely detailed information about both ongoing
process and related past experiences, How can this be reconciled with known limitations on the ability to report this information?" (p. 213). Although HPM is computationally sufficient to produce the SUM-to-MIN transition, it makes dubious empirical and
mnemonic assumptions.
To summarize, GIPS achieves its main research objective, providing a computational
account of the several strategy shifts observed during the SUM-to-MIN transition. It
uses plausible local processes, rather than the global optimization processes of Neches'
HPM. In addition, Gins uses modest amounts of storage, in contrast to HPM, which stores
complete solution traces for indefinite periods. Most importantly, Grps produces all and
only the transitional strategies observed in the Siegler and Jenkins study. It predicts an
additional possible strategy, but does not require it to occur before the invention of the
MIN strategy.
The GIPS analysis solves a number of puzzles raised by Siegler and Jenkins' study. These
include the source of the various strategies that appear in the SUM-to-MIN transition and
their order of appearance, as well as the ability to make significant strategy shifts without
impasse-driven learning. GIPS also suggests a role for innate knowledge of the principles of

CHILDREN'S ADDITION STRATEGIES

35

addition in the ability to avoid inventing bad strategies, although this depends on the specific
type of feedback given to the system. Thus, GIPS provides a plausible, computationally
sufficient account of the discovery of the MIN strategy. However, Siegter and Jenkins
produced a second set of findings on the gradual increase in usage of the newly discovered
strategy. We have not yet tried to model these findings, but GIPS seems to provide an
appropriate framework for doing so.
Finally, the SUM-to-MIN transition does not appear to be explainable by conventional,
symbol-level learning mechanisms. Rather, some of the important shifts require changes to
the representation of the domain. GIPS models these changes by altering preconditions on
some of its operators. Adjusting operator preconditions is somewhat dangerous, because it
can allow the system to corrupt a previously correct domain theory, but GIPS demonstrates
that such a mechanism can generate useful behavior shifts when controlled by feedback on
its decisions.

Acknowledgments
This research benefited from discussions with Jeff Schlimmer and Bob Siegler. Comments
from Clare Bates Congdon and the reviewers improved the technical quality of the paper.
The research was supported in part by contract N00014-88-K-0080 from the Office of
Naval Research, Cognitive Sciences Division, and a postdoctoral training grant from the
Department of Health and Human Services.

References
Anderson, J.R. (1983). The architecture of cognition. Cambridge,MA: Harvard UniversityPress.
Ashcraft, M.H. (1982). The developmentof mental arithmetic: A chronometricapproach.Developmental Review,
2, 213-236.
Ashcraft, M.H. (1987). Children'sknowledgeof simple arithmetic: A developmentalmodel and simulation. In C.J.
Brainerd, R. Kail, and J. Bisanz(Eds.), Formal methods in developmentalpsychology. New York:Springer-Verlag.
Berwick, R. (1985). The acquisition of syntactic knowledge. Cambridge, MA: MIT Press.
Dietterich, T.G. (1986). Learning at the knowledgelevel. Machine Learning, 1,287-316.
Fikes, R.E., and Nilsson, N.J. (1971). STRIPS:A new approachto the application of theorem provingto problem
solving. Artificial Intelligence, 2, 189-208.
Gelman, R., and Gallistel, C.R. (1978). The chiM's understanding of number. Cambridge,MA: HarvardUniversity
Press.
Groen, G.J., and Parkman, J.M. (1972). A chronometric analysis of simple addition. Psychological Review, 79,
329-343.
Groen, G.J., and Resnick, L.B. (1977). Can preschoolchildren invent addition algorithms?Journal of Educational
Psychology, 69, 645-652.
Holland, J.H., Holyoak,K.J., Nisbett, R.E., and Thagard, P.R. (1986). Induction: Processes of inference, learning,
and discovery. Cambridge, MA: MIT Press.
lba, G.A. (1989). A heuristic approachto the discoveryof macro-operators.Machine Learning; 3, 285-317.
Jones, R.M. (1993). Problem solving via analogical retrieval and analogical search control. Int A. L. Meyrowitz
and S. Chipman (Eds.), Foundations of knowledge acquisition: Machine learning. Boston: Kluwer Academic.

36

R.M. JONES AND K. VANLEHN

Kaye, D.B., Post, T.A., Hall, V.C., and Dineen, J.T. (1986). The emergence of information retrieval strategies in
numerical cognition: A development study. Cognition and Instruction, 3, 137-166.
Korf, R.E. (1985). Macro-operators: A weak method for learning. Artificial Intelligence, 26, 35-77.
Kuhn, D., Amsel, E., and O'Laughlin, M. (1988). The development of scientific thinking skills. New York: Academic
Press.
Langley, P. (1985). Learning to search: From weak methods to domain-specific heuristics. Cognitive Science, 9,
217-260.
Langley, P. and Allen, J.A. (1991). The acquisition of human planning expertise. In L.A. Bimbaum and G.C.
Collins (Eds.), Machine Learning: Proceedings of the Eighth International Workshop (pp. 80-84). Los Altos, CA:
Morgan Kaufmann.
Minton, S. (1988). Learning efJective search control knowledge: An explanation-based approach. Boston: Kluwer
Academic.
Mitchell, T.M., Utgoff, P,E., and Banerji, R. (1983). Learning by experimentation: Acquiring and refining problemsolving heuristics. In R.S. Michalski, J.G. Carbonell, and T. M. Mitchell (Eds.), Machine learning: An artificial
intelligence approach. Los Altos, CA: Morgan Kaufmann.
Neches, R. (1987). Learning through incremental refinement of procedures. In D. Klahr, P. Langley, and R. Neches
(Eds.), Production system models of learning and development. Cambridge, MA: MIT Press,
Newell, A. (1990). Unified theories of cognition: The William James lectures. Cambridge, MA: Harvard University
Press.
Ohlsson, S. and Rees, E. (1991). The function of conceptual understanding in learning of arithmetic procedures.
Cognition and Instruction, 8, 103-179.
Pazzani, M., Dyer, M., and Flowers, M. (1986). The role of prior causal theories in generalization. Proceedings
of the Fifth National Conference on Artificial Intelligence (pp. 545-550). Los Altos, CA: Morgan Kaufmann.
Schank, R. (1986). Explanation patterns: Understanding mechanically and creatively. Hillsdale, N J: Lawrence
Erlbaum,
Schlimmer, J.C. (1987). Incremental adjustment of representations for learning. Proceedings of the Fourth International Workshop on Machine Learning (pp. 79-90), Los Altos, CA: Morgan Kaufmann.
Schlimmer, J.C. and Granger, R.H., Jr. (1986a), Beyond incremental processing: Tracking concept drift. Proceedings of the Fifth National Coni[erence on Artificial Intelligence (pp. 502-507). Los Altos, CA: Morgan Kaufmann.
Schlimmer, J.C. and Granger, R.H., Jr. (1986b). Incremental learning from noisy data. Machine Learning, 1,
317-354.
Siegler, R.S. and Jenkins, E. (1989). How children discover new strategies. Hillsdale, NJ: Lawrence Erlbanm.
Svenson, O. (1975). Analysis of time required by children for simple additions. Acta Psychologica, 39, 289-302.
VanLehn, K. (1990). Mind bugs: The origins of procedural misconceptions. Cambridge, MA: MIT Press.
VanLehn, K. (1991). Rule acquisition events in the discovery of problem solving strategies. Cognitive Science,
15, 1~,7.
VanLehn, K., and Jones, R.M. (1993). Integration of explanation-based learning of correctness and analogical
search control. In S. Minton and P. Langley (Eds.), Planning, scheduling and learning. Los Altos, CA: Morgan
Kaufmann.
VanLehn, K., Jones, R.M., and Chi, M.T.H. (1992). A model of the self-explanation effect. Journal of the Learning
Sciences, 2(1), 1-60.
Received April 2, 1992
Final Manuscript January 18, 1994

Evaluation of a Meta-tutor for Constructing Models
of Dynamic Systems
Lishan Zhang, Winslow Burleson, Maria Elena Chavez-Echeagaray, Sylvie Girard,
Javier Gonzalez-Sanchez, Yoalli Hidalgo-Pontet, and Kurt VanLehn
Arizona State University, Computing, Informatics, and Decision Systems Engineering,
Tempe, AZ, 85281, U.S.A.
{lishan.zhang,winslow.burleson,mchaveze,sylvie.girard,
javiergs,yhidalgo,kurt.vanlehn}@asu.edu

Abstract. While modeling dynamic systems in an efficient manner is an important skill to acquire for a scientist, it is a difficult skill to acquire. A simple stepbased tutoring system, called AMT, was designed to help students learn how to
construct models of dynamic systems using deep modeling practices. In order to
increase the frequency of deep modeling and reduce the amount of guessing/gaming, a meta-tutor coaching students to follow a deep modeling strategy
was added to the original modeling tool. This paper presents the results of two
experiments investigating the effectiveness of the meta-tutor when compared to
the original software. The results indicate that students who studied with the
meta-tutor did indeed engage more in deep modeling practices.
Keywords: meta-tutor, intelligent tutoring systems, empirical evaluation.

1

Introduction

Modeling is both an important cognitive skill [1] and a potentially powerful means of
learning many topics [5]. The Affective Meta-Tutoring (AMT) system teaches students how to construct system dynamics models. Such models are widely used in
professions, often taught in universities and sometimes taught in high schools.
1.1

The Modeling Language

In our modeling language, a model is a directed graph with one type of link. Each
node represents both a variable and the computation that determines the variable’s
value. There are three types of nodes.
• A fixed value node represents a constant value that is directly specified in the problem. A fixed value node has a diamond shape and never contains incoming links.
• An accumulator node accumulates the values of its inputs. That is, its current value is the sum of its previous value plus or minus its inputs. An accumulator node
has a rectangular shape and always has at least one incoming link.

K. Yacef et al. (Eds.): AIED 2013, LNAI 7926, pp. 666–669, 2013.
© Springer-Verlag Berlin Heidelberg 2013

Evaluation of a Meta-tutor for Constructing Models of Dynamic Systems

667

• A function node’s value is an algebraic function of its inputs. A function node has
a circular shape and at least one incoming link.
The students’ task is to draw a model that represents a situation that is described in
the form of a relatively short text. During construction, students can use the Check
button to evaluate the correctness of the current tab or the Give up button to ask the
system to fill out the tab automatically.
1.2

The Target Node Strategy

The meta-tutor teaches students a goal reduction procedure for constructing models.
It is called the Target Node Strategy. The basic idea is to focus on one node at a time
(the target node) and completely define it before working on any other node. This
process decomposes the whole problem of modeling a system into a series of atomic
modeling problems, one per node. Like Pyrenees [2], it teaches students that if they
just master this one difficult but small skill, then the rest of the problem solving
is straight forward. In addition, the meta-tutor complains if students appear to be
guessing too much or giving up too early, just as the Help Tutor did [3].

Fig. 1. The left image is the example of model, with gray callouts added to explain the contents
of nodes. The right image is the example of a node editor.

2

Evaluation

2.1

Experiment Design

The experiment was designed as a between-subject single treatment experiment with a
control condition, where the meta-tutor was off, and an experiment condition, where
the meta-tutor was on. The difference between the conditions occurred only during a
training phase where students learned how to solve model construction problems. In
order to assess how much students learned, a transfer phase followed the training

668

L. Zhang et al.

phase. During the transfer phase, all students solved model construction problems
with almost no help: the meta-tutor, the Check button and the Give-up button were all
turned off, except in the Description tab where the Check button remained enabled to
facilitate grounding. Because system dynamics is rarely taught in high school, no pretest was included in the procedure. We conducted two experiments with 44 students
participating in the first experiment and 34 students in the second experiment.
2.2

Hypotheses and Measures

Hypothesis 1 is that the meta-tutored students will use deep modeling more frequently than the control students during the transfer phase. We used the three measures
below to assess it.
• The number of the Run Model button presses per problem.
• The number of extra nodes created, where extra nodes are defined as the nodes that
can be legally created for the problem but are not required for solving the problem.
• The number of problems completed during the 30 minute transfer period.
Hypothesis 2 is that meta-tutored students will use deep modeling more frequently
than the control group students during the training phase. The three dependent measures used to evaluate this hypothesis are described below:
•

•
•

Help button usage: was calculated as
3
/ , where
is the
number of Check button presses that yielded red,
is the number of Giveis the number of nodes required by the problem.
up button presses, and
The percentage of times the first Check was correct.
where
is the number
Training efficiency: was calculated as 3
of nodes the student completed correctly ( 3
is the number of tabs), and
is the number of Give-up buttons presses.

Hypothesis 3 is that the experimental group students, who were required to follow the
Target Node Strategy during training, would seldom use it during the transfer phase.
To evaluate this hypothesis, we calculated the proportion of student steps consistent
with the target node strategy.
2.3

Results

Table 1 summarizes the results of experiment 1 and experiment 2.

3

Conclusion and Future Work

Although we achieved some success in encouraging students to engage in deep modeling, there is much room for improvement. If the meta-tutor had been a complete
success at teaching deep modeling, we would expect to see students supported by the
meta-tutor working faster than the control students. The stage is now set for the last
phase of our project, where we add an affective agent to the system [4], in order to
encourage engagement and deep modeling.

Evaluation of a Meta-tutor for Constructing Models of Dynamic Systems

669

Table 1. Results of Experiment 1 and 2: E stands for the meta-tutor group, and C stands for
the control group. Reliable results are bold.

Measure (predicted dir.)

Experiment 1 (N=44)

Experiment 2 (N=33)

Transfer phase (Hypothesis 1)
Run model button usage (E<C)

E<C (p=0.31, d=0.32)

E≈C (p=0.98, d=-0.0093)

Extra nodes (E<C)
Probs completed (E>C)

E<C (p=0.02, d=0.80)
E≈C (p=0.65, d=0.04)

E<C (p=0.47, d=0.26)
E<C (p=0.09, d=−0.57)

Training phase (Hypothesis 2)
E<C (p=0.04, d=0.68)
E<C (p=0.02, d=0.89)
Missing data
E>C (p=0.015, d=0.98)
E>C (p=0.59, d=0.19)
E<C (p=0.05, d=0.70)
Transfer phase use of Target Node Strategy (Hypothesis 3)
Usage (E=C)
Missing data
E≈C (p=0.59, d=−0.19).
Help button usage (E<C)
Correct on 1st Check (E>C)
Efficiency (E>C)

Acknowledgements. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References:
1. CCSSO: The Common Core State Standards for Mathematics, Downloaded from
http://www.corestandards.org (October 31, 2011)
2. Chi, M., Van Lehn, K.: Meta-cognitive strategy instruction in intelligent tutoring systems:
How, when and why. Journal of Educational Technology and Society 13(1), 25–39 (2010)
3. Roll, I., Aleven, V., McLaren, B.M., Ryu, E., Baker, R.S.J.d., Koedinger, K.R.: The Help
Tutor: Does Metacognitive Feedback Improve Students’ Help-Seeking Actions, Skills and
Learning? In: Ikeda, M., Ashley, K., Chan, T.-W. (eds.) ITS 2006. LNCS, vol. 4053, pp.
360–369. Springer, Heidelberg (2006)
4. Girard, S., Chavez-Echeagaray, M.E., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., Zhang,
L., Burleson, W., VanLehn, K.: Defining the behavior of an affective learning companion
in the Affective Meta-Tutor project. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P.
(eds.) AIED 2013. LNCS (LNAI), vol. 7926, pp. 21–30. Springer, Heidelberg (2013)
5. Treagust, D.F., Chittleborough, G., Mamiala, T.: Students’ understanding of the role of
scientific models in learning science. International Journal of Science Education 24(4),
357–368 (2002)

The Architecture of Why2-Atlas: A Coach for
Qualitative Physics Essay Writing
Kurt VanLehn, Pamela W. Jordan, Carolyn P. Rosé, Dumisizwe Bhembe,
Michael Böttner, Andy Gaydos, Maxim Makatchev, Umarani Pappuswamy,
Michael Ringenberg, Antonio Roque, Stephanie Siler, and Ramesh Srivastava
LRDC, University of Pittsburgh, Pittsburgh, PA 15260
vanlehn@cs.pitt.edu

Abstract. The Why2-Atlas system teaches qualitative physics by having
students write paragraph-long explanations of simple mechanical phenomena.
The tutor uses deep syntactic analysis and abductive theorem proving to convert
the student’s essay to a proof. The proof formalizes not only what was said, but
the likely beliefs behind what was said. This allows the tutor to uncover
misconceptions as well as to detect missing correct parts of the explanation. If
the tutor finds such a flaw in the essay, it conducts a dialogue intended to
remedy the missing or misconceived beliefs, then asks the student to correct the
essay. It often takes several iterations of essay correction and dialogue to get
the student to produce an acceptable explanation. Pilot subjects have been run,
and an evaluation is in progress. After explaining the research questions that the
system addresses, the bulk of the paper describes the system’s architecture and
operation.

1 Objectives
The Why2 project has three objectives. The first is to build and evaluate qualitative
physics tutors where all student communication is via natural language text. In
particular, we will compare their performance to text-based expository instruction and
human tutoring. Text-based natural language (NL) technology has improved
significantly but is still far from perfect. Are the inevitable disfluencies so confusing
that they significantly retard learning, or are they no worse than the disfluencies of
human-to-human text-based tutoring?
The second objective is to compare several different NL processing techniques. In
particular, we are collaborating with the AutoTutor Research Group [1, 2] who are
building a version of Why2 using latent semantic analysis, a statistical technique. Our
version of Why2 is called Why2-Atlas; their version is called Why2-AutoTutor.
Why2-Atlas is based on deep syntactic analysis and compositional semantics. It is
more difficult to build but may yield better performance.
The third objective is to develop authoring tools that facilitate development of NLbased tutoring systems. Whereas a typical tutoring system’s size is roughly
determined by the amount of material it covers, an NL-based tutor’s size is also a
function of the diversity of the words and linguistic structures that students use.
S.A. Cerri, G. Gouardères, and F. Paraguaçu (Eds.): ITS 2002, LNCS 2363, pp. 158–167, 2002.
© Springer-Verlag Berlin Heidelberg 2002

The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing

159

Since this is a large effort, we chose a worthy pedagogical goal: qualitative physics
understanding. It is well known that college physics students are often unable to
construct acceptable qualitative explanations for simple physics questions, such as
“what happens when two balls of different masses fall from the same height?”
Sometimes they know what happens (the balls hit the ground at the same time) but
they cannot explain why it occurs. Decades of experimental instruction have produced
only limited progress toward universal qualitative understanding [3]. Even students
with top grades in their classes get low scores on standardized measures of qualitative
understanding, such as the Force Concepts Inventory [4].
The Why2-Atlas user interface is remarkably simple. There are three windows: the
problem window, the essay window and the chat window. The problem window
merely states a question, such as “Suppose you are running in a straight line at
constant speed. You throw a pumpkin straight up. Where will it land? Explain.”
Students type their explanation into the essay window, then click on a submit button.
The student and the tutor take turns typing in the chat window. For instance, one
student’s explanation was, “It would seem as though the ball would fall back into the
man’s hands at the end of T, but I do not think that this will happen…after the ball is
thrown there is a lack of a force which results in the decrease of horizontal velocity.”
When this is entered in the essay window, Why2-Atlas analyzes it, which involves
determining what “the ball” and “the man” refer to, among other things. The tutor
determines that the student has a common misconception, which is that a moving
object always slows down unless there is a force to propel it along. Switching to the
chat window, the tutor engages the student in a dialogue, such as the one shown in
Table 1. At its end, the tutor suggests that the student change the essay. If all goes
well, the student will remove the misconception from the explanation, in which case
the tutor can address any flaws that remain. If the student does not do so, then the
tutor will either try a different dialogue.
Why2-Atlas is composed of several modules: A sentence-level understander
(SLU), a discourse-level understander (DLU), a tutorial strategist and a dialogue
engine. These four modules are controlled by a discourse manager. As the student
enters an essay, the SLU parses it and produces a set of propositions expressed in a
version of first-order logic. The discourse-level understander (DLU) assimilates them
into a explanation. If a proposition is new, then the DLU augments the explanation
with inferences that explain (prove abductively) the proposition. Next, the tutorial
strategist analyzes the augmented explanation’s completeness and correctness. If the
explanation used rules that represent misconceptions (e.g., “If there is no force to
propel an object along, it slows down.”), then the tutorial strategist creates tutorial
goals to remedy those misconceptions. If the explanation does not explicitly mention
one of the points that the project physicists have indicated must be present in any
acceptable essay, then the strategist creates a tutorial goal of eliciting that mandatory
point. Next, the strategist uses heuristics to select the highest priority tutorial goal and
attempts to achieve it by calling the dialogue engine to conduct a knowledge
construction dialogue (KCD). If the student fails to remove the flaw after one or more
KCDs, the system tells the student how to do so.
The remaining sections describe each of the modules in more detail and the
relationship of this work to earlier work.

160

K. VanLehn et al.
Table 1. A Knowledge Construction Dialogue (KCD)

 Tutor: OK, let’s consider the relationship between force and acceleration in
a different context. A hockey puck is resting on the rough ice surface of a
skating rink (a hockey team has just finished practicing on it). You push
with a force, F, during the time from t1 to t2. Assume that the only force
acting horizontally on the puck is friction due to the ice surface (there is no
air resistance). What will happen to the velocity of the puck after your
push?
 Student: it will get slower
 Tutor: Good. A zamboni smoothes the ice so that the friction caused by the
roughness of the ice surface is reduced. You push the same hockey puck
with the same force as before (and for the same amount of time). How does
the change in velocity of the hockey puck after this push compare to its
change in velocity following the initial push?
 Student: slows more slowly
 Tutor: Correct. Now assume there exists a machine that can make the ice
surface perfectly smooth, so that the friction caused by the ice surface is
zero. Again assume the force of air resistance is zero. If you were to push a
puck on this surface, what would happen to the velocity of the puck
following the push?
 Student: nothing
 Tutor: Right. In general, according to Newton’s first law, if there are no
forces acting on a body, what happens to the body’s velocity?
 Student: not a darn thing
 Tutor: Okay. In general, these and similar observations are summarized in
Newton’s first law, which states that if there are no forces acting on a body,
its velocity will remain constant (or its acceleration will be zero)

2 The Sentence-Level Understander
The sentence-level understander (SLU) converts each sentence in the student’s essay
into a set of propositions. The propositions are expressed in a version of first-order
logic. For instance, consider the following sentence: “Should the arrow have been
drawn to point down?” Roughly speaking, this should be converted to:
$e³events, $v³vectors, $s draw(e, s, v)&tense(e, past)&mood(e, interrog)&direction(v, down)

This is only an approximation of the real output. It illustrates the challenge of
converting words into the appropriate domain-specific predicates. Notice how
“arrow” has been converted to “vector.” Another challenge is to interlink the
predicate arguments correctly: Notice how the variable that denotes “the arrow”
appears in both “draw” and “direction.” Other aspects of the meaning of the utterance
are not yet processed. For instance, “the arrow” probably refers to some specific
vector mentioned earlier in the dialog, but the SLU makes no attempt to resolve that
reference.

The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing

161

The SLU is composed of a lexical preprocessor, a parser, a repair module (which
together comprise a package called CARMEL) and a statistical analyzer [5, 6]. As
the student types in a sentence, the text is sent to the lexical preprocessing module,
which looks up the words in a lexicon, doing spelling correction as necessary. It will
also strip off prefixes and suffixes in order to uncover the root forms of the words.
The resulting graph of root word forms is sent to the next module, the parser.
LCFlex [5, 7], is a flexible left-corner parser that does a deep syntactic analysis of
each sentence. For instance, for the illustration sentence, the parser determines that
“the vector” is both the deep object of “draw” and the deep subject of “point.” As it
builds a syntactic analysis, the parser builds the logical form as well. This allows it to
check that the predicates are assigned arguments of the right type.
LCFlex copes with ungrammatical input by skipping words, inserting missing
categories and relaxing grammatical constraints as necessary in order to parse the
sentence. For instance, “Should the arrow has been drawn point down” would parse.
When the parser produces too many analyses of the sentence, it uses statistical
information on the frequency of word roots and grammatical analyses to determine
the most probable parse.
If the parser cannot produce any complete analysis of the sentence, then its
fragmentary analysis is passed to the repair module. The fragments can be viewed as
domain-specific predicates that are looking for argument fillers, and domain-specific
typed variables that are looking for arguments to fill. A genetic search is used to build
a complete analysis from these fragments.
If the repair module fails, then the symbolic approach is abandoned, and the SLU’s
fourth module takes over. It uses text classification techniques. Given a statistical
language model that indicates how strongly each word is associated with domain
classifications (e.g., misconceptions), it computes the most likely classification of
each sentence using either a naïve-Bayesian approach [8] or LSA.
Besides this statistical language model, the SLU requires three other large
knowledge sources: a meaning-representation language definition, a grammar and a
lexicon. The meaning-representation language definition specifies the domain
predicates and types that can occur in the logical form output by the SLU. The
definition of a predicate specifies type restrictions on its arguments. For instance, the
definition of the “move” predicate specifies that its “agent” argument be an instance
of the “concrete entity” type.
The grammar is a unification-augmented context-free grammar based on both
Functional Grammar [9] and Lexical Functional Grammar [10]. Its main job is to
assign deep syntactic roles to phrases in the sentence. For instance, in both the active
sentence “I drew the vector” and the passive sentence “The vector was drawn,” the
noun phrase “the vector” is assigned to the same deep syntactic role (argument) of the
verb “draw.”
The lexicon defines word roots. For each one, it specifies their syntactic
categorization, the possible semantic predicates or types, and the mapping from
syntactic roles to semantic roles. For words like verbs that can have arguments, the
lexical entry also specifies information about the syntactic and semantic argument
restrictions. Words with multiple senses have multiple lexical entries. For instance,
“draw” has 7 entries. The lexicon was built by adding semantic information and
idioms to COMLEX [11].

162

K. VanLehn et al.

Parts of the SLU (e.g., LCFlex) have been evaluated separately [5, 7], and some
are being used by other projects [12, 13].

3 Discourse-Level Understanding
The discourse-level understander (DLU) receives logical forms and outputs a proof.
Topologically, the proof is a forest of interwoven trees. The leaves are facts given in
the problem statement or assumptions made while the proof is being constructed. The
roots (conclusions) are student propositions. Other student propositions may occur in
the middle of the forest. As an illustration, consider the following question and
student essay:
Q: Suppose you are in a free-falling elevator and you hold your keys
motionless in front of your face and then let go. What will happen to
them? Explain.
A:The keys will fall parellel to the persons face because of the constant
acceleration caused by gravity but later the keys may go over your head
because the mass of the keys are less.
Although this essay has a misspelling, missing punctuation and grammatical errors,
the SLU parses it and sends four propositions to the DLU. The first proposition
corresponds to the student statement that the keys will fall parallel to the person’s
face. This conclusion is correct and becomes a root in the proof. The second
proposition, that gravitational acceleration is constant, corresponds to an interior node
in this proof. The third proposition, that the keys go over the person’s head, is based
on a common misconception, that heavier objects fall faster. It becomes a root in the
proof. The last proposition, that the mass of the keys is less, corresponds to a node in
the interior of the proof of the third proposition.
Proofs are constructed by Tacitus-lite+ [14], which is an extension of Tacitus [15].
Its knowledge base is a set of Horn clauses that represent both correct physics beliefs,
such as “If a vector is vertical, its horizontal component is zero,” and incorrect beliefs,
such as “more massive objects have larger vertical accelerations.” The goals in the
body (the antecedents) of a rule each have a cost. Tacitus-lite+ can choose to assume
the goal rather than prove it, in which case its cost is added to the overall cost of the
proof. Tacitus-lite+ prefers proofs with the lowest costs. This mechanism, which is a
form of abduction, is necessary to allow Tacitus-lite+ to “prove” false student
statements.
When a goal that has unbound variables is proved, the variables often become
bound to constants, terms or other variables. When Tacitus-lite+ assumes a goal, it
can also bind unbound variables. To reduce the combinatorics when this occurs, all
arguments in predicates and terms have types associated with them. The types
constrain what existing constants, terms and variables may be bound to the unbound
arguments of the goal that is being assumed. If a variable is used in several goals, it
accumulates constraints from all of them.
Because variables accumulate constraints, the process of resolving referring
expressions is interwoven with the construction of the proof. For instance, the student
statement, “the mass of the keys is less,” becomes a proposition similar to

The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing

163

less(mass(keys1), X) where keys1 is a constant denoting the keys, mass(keys1) is a
compound term denoting their mass, and X is a variable. That is, the student did not
say what the mass was less than, so the DLU must resolve the implicit reference. Via
accumulation of constraints and some discourse heuristics based on centering [16],
the DLU ends up binding X to mass(person1). Sometimes objects must be created in
order to refer to them. For instance, the temporal reference implicit in the student’s
statement, “later the keys may go over your head,” requires dividing the fall of the
keys into two time intervals, and stating that the keys are parallel to the person’s face
during the first interval but above the person’s head during the second interval.
Creating time intervals in order to refer to them is just one of many complexities
that must be handled when converting language to proofs. We have only tried to solve
those that are necessary for our purposes. For instance, the modal “may” is ignored in
“later the keys may go over your head.”

4 The Tutorial Strategist
Once a proof has been constructed, the tutorial strategist analyzes it to find flaws.
Each flaw is associated with patterns that match occurrences of the flaw in the proof.
In the proof mentioned earlier, there are several flaws. The main one is that the
misconception “heavier objects fall faster” has been used. In addition, several points
that physicists consider mandatory for a sufficiently complete explanation have been
implied but not stated. For instance, one is “the keys’ acceleration and the person’s
acceleration are the same.” Each detected flaw is queued as a tutorial goal to remedy
the flaw.
Once tutorial goals have been queued, the strategist picks the highest priority goal
from the queue. The priorities are: fix misconceptions before anything else, then fix
self-contradictions, errors and incorrect assumptions, and lastly elicit missing
mandatory points. Since most essays are missing several mandatory points, the choice
of which one to tutor first is determined by a hand-authored list for each problem.
This insures that the points will be elicited in a natural order.

5 The Dialogue Engine
The tutorial goals for remedying a misconception are associated with a specific
remediation KCD, and those for eliciting a mandatory point are associated with a
specific elicitation KCD. If one of these goals is chosen, then the dialogue manager,
APE, is called with the name of the KCD.
KCDs are managed by finite state networks whose nodes are questions to the
students. The links exiting a node correspond to expected responses to the question.
The questions are written to invite short responses from the students so that simple
techniques can be used to match the expected responses to the actual ones. For
instance, Table 1 shows one KCD.
In order to determine which expected response is the best match to the student’s
answer, APE calls LCFlex with a simple semantic grammar. Each question has its

164

K. VanLehn et al.

own semantic grammar, although the grammar may share rules with other questions’
grammars. The root categories in the grammar correspond to the expected responses.
For instance, if the expected responses are “down” and “up,” then the semantic
grammar would have two rules such as “Ques32_down_resp => down_cat” and
“Ques32_up_resp => up_cat” where down_cat and up_cat are categories used in
several semantic grammars and are reduced by rules such as “down_cat => ‘down’,”
“down_cat => ‘downwards’,” “down_cat => ‘towards earth’,” etc. Because LCFlex
can skip words, it can find certain key words or phrases in the student’s response even
if they are surrounded by extra words, as in “Is it downwards?”
When the student has finished the KCD, the discourse manager inserts a segue,
such as “Given what you’ve learned, please change your essay.” The student should
edit the essay, and resubmit it. When students fail to correct the essay, the same flaw
will be detected and will probably be selected again for fixing. If the tutorial goal is
important enough that multiple KCDs exist for remedying it, then a second KCD will
be tried. If the discourse manager runs out of KCDs for fixing a flaw, then it gently
suggests an explicit change to the essay. For example, if one of the required points is
missing, the tutor says, “It could be that you have what I’m looking for in mind, but
I’m just not able to understand what you’re saying. Let me show you what I’d say
was the point that should be covered in your essay: ‘After their release, the only force
acting on the keys is the downward force of earth’s gravity.’”

6 Authoring Tools
The major knowledge sources in Why2-Atlas are the Tacitus-lite+ rules, the KCDs,
the semantic grammars used in the KCDs, the syntactic grammar and the lexicon. We
are working on tools for authoring all these knowledge sources. However, the only
tools that are currently in routine use are the KCD editor and DAIENU, the semantic
grammar authoring system.
The KCD editor is a graphically oriented editor designed for non-programmers [17,
18]. Typically the author starts by creating a main line of reasoning consisting of
alternating questions and correct responses. Next the author enters expected incorrect
answers for each question. The author then defines subsidiary KCDs, some of which
may also be remediation KCDs, for the incorrect answers. It is typical to call the same
KCDs from many different locations. As a result, KCDs can become so complicated
that it would be difficult to navigate their links without the aid of the KCD editor.
The second tool, DAIENU, is used to create the semantic grammars required by
the KCDs [19]. There are several hundred KCDs in the current version of Why2Atlas, and each has several questions, so several hundred semantic grammars are
needed. DAIENU begins by collecting all the expected student responses in the
KCDs. Each is a string of expected student words, entered by the author, and a
category standing for that particular expected response. The job of DAIENU is to
create intermediate categories, similar to the down_cat and up_cat mentioned earlier,
that will be used in the multiple semantic grammars. DAIENU suggests clusters of
strings, based on shared words, to the author. The author can also add or subtract
strings from the initial set. Together they come up with a set of strings that occurred

The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing

165

in the KCD responses and have similar meaning, according to the author. DAIENU
then finds commonalities between the sets such that it can construct a compact set of
general grammar rules that will correctly classify the members of each set. When it
has produced a candidate set of grammars, it generates novel strings labeled by the
grammars, and presents them to the author. The author corrects their classifications,
and the whole grammar induction process repeats.

7 Related Work
Development of Why2-Atlas is possible only because we borrowed many ideas from
earlier efforts. This section highlights some of the similarities between Why2-Atlas
and other NL-based tutoring systems.
Perhaps the first fully operational NL-based tutor was CIRCSIM-tutor [20]. It used
information-extraction techniques and short-answer questions in order to conduct a
dialogue about the student’s qualitative analysis of a cardiophysiological feedback
system. We also use short answer questions and word-based analysis of student
responses in order to conduct a robust, fluent dialogue. In fact, the two tutors use
nearly the same dialogue engine [21]. However, the CIRCSIM-tutor students express
their cardiophysiological analysis by putting +/-/0 marks in a table, whereas our
students enter an NL essay. Most of the current complexity in Why2-Atlas is devoted
to analysis of this essay.
Another early NL-based tutor is AutoTutor [2]. It opens the dialogue by simply
asking the student a question, such as “What happens when you boot up a computer?”
It has a list of mandatory points that it wants the student to explicitly articulate, and
will prompt the student more and more pointedly until the student says (types) them
clearly. It also has a set of misconceptions that it anticipates the student will enter, and
has remedial messages to deal with them. We use the same basic idea for analyzing
the student essays: a set of mandatory points and a set of misconceptions. The main
difference is that AutoTutor uses a statistical technique (LSA) to determine if a point
has been mentioned, whereas Why2-Atlas uses symbolic analyses.
Whereas CIRCSIM-tutor and AutoTutor use shallow, word-based analyses of
student text, Aleven, Popescu and Koedinger [12] are building a tutor that uses deep
analyses of student explanations. Their tutor asks students simple geometry questions,
such as whether two angles are the same. Students must provide both an answer and a
NL explanation, such as “The sum of complementary angles is 180 degrees.”
Students typically do not provide such precise, complete explanations on their first
attempt, so the tutor must prompt them more and more pointedly to add precision to
their justification. Shallow, word-based analyses of these explanations would clearly
not suffice, so the tutor uses the LCFlex parser [5, 7], a deep syntactic grammar, the
Loom symbolic classifier, and a geometry ontology to analyze the students’
explanations. For the same reasons, we also use the LCFlex parser and a deep
syntactic grammar. However, our students’ explanations often contain several clauses,
compared to the single clause or phrase that typifies student justifications in the
Geometry tutor. Consequently, we use an abductive theorem prover and a physics
axiom set instead of a classifier and an ontology.

166

K. VanLehn et al.

BEETLE [13] and PACO [22] are NL-based tutors for procedural tasks. The
students need to do steps in a hierarchical, partial order which they may or may not
know. A major challenge for these systems is allowing students to do steps when
they can and offering hints when they can’t. We have avoided (at least in this first
version of the system) the problems of conducting a mixed initiative dialogue by
giving the student all the initiative during the essay-entering phase, and having the
tutor take the lead otherwise.

8 Conclusions
Our mission is to explore the cost-benefit space of NL-based tutoring. As an initial
data point, we have constructed a baseline system and are evaluating it this spring.
The evaluation compares Why2-Atlas to 3 other methods for teaching the same
qualitative physics knowledge using the same 10 essay questions: (1) human tutors,
(2) Why2-AutoTutor, a statistical NLP system described earlier, and (3) expository
physics texts. Although Why2-Atlas is proving to be fast and robust enough, its
knowledge sources are not nearly as well developed as we would like, so it often
misunderstands the students. However, we anticipated this, and engineered its
responses to yield reasonable dialogue despite its misunderstandings. We hypothesize
that this baseline system should do about as well as Why2-AutoTutor. We expect both
tutoring systems to be better than the expository texts, but not as good as the human
tutors. Regardless of how this first evaluation turns out, we will use its log file data to
improve our knowledge sources, then run the evaluation again. This will yield a
second data point on the tradeoff between NLP sophistication and learning gains.
Simultaneously, we are building tools that should lower the development effort for
NL-based tutoring. Although the technology for NL-based tutoring is clearly still in
its early stages, we hope to understand its benefits, its costs, and how to change both.
Acknowledgements. This research was supported by MURI grant N00014-00-1-0600
from ONR Cognitive Science, and by NSF grant 9720359. We’re delighted to
acknowledge the help of Art Graesser and his Tutoring Research Group, our partners
in the Why2 project.

References
1.
2.
3.
4.

Graesser, A.C., et al., Using latent semantic analysis to evaluate the contributions of
students in AutoTutor. Interactive Learning Environments, 2000.
Person, N.K., et al., Simulating human tutor dialog moves in AutoTutor. International
Journal of Artificial Intelligence in Education, in press.
Hake, R.R., Interactive-engagement vs. traditional methods: A six-thousand student survey
of mechanics test data for introductory physics students. American Journal of Physics,
1998. 66(4): p. 64-74.
Hestenes, D., M. Wells, and G. Swackhamer, Force concept inventory. The Physics
Teacher, 1992. 30: p. 141-158.

The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.

22.

167

Rose, C.P. A framework for robust semantic interpretation. In The First Meeting of the
North American Chapter of the Association for Computational Linguistics. 2000.
Rose, C.P. A syntactic framework for semantic interpretation. In The ESSLLI Workshop on
Linguistic Theory and Grammar Implementation. 2000.
Rose, C.P. and A. Lavie, Balancing robustness and efficiency in unification-augmented
context-free parsers for large practical applications, in Robustness in Language and Speech
Technology, J.C. Junqua and G.V. Noord, Editors. 2001, Kluwer Academic press.
McCallum, A.K., Bow: A toolkit for statistical language modeling, text retrieval,
classification and clustering. 1996, CMU: Pittsburgh, PA.
Halliday, M.A.K., An Introduction to Functional Grammar. 1985: Adward Arnold: A
division of Hodder and Stoughton.
Bresnan, J., The Mental Representation of Grammatical Relations. 1982, Cambridge, MA:
MIT Press.
Grishman, R., C. Mcleod, and A. Meyers. COMLEX syntax: Building a computational
lexicon. In Proceedings of the 15th International Conference on Computational
Linguistics (COLING-94). 1994.
Aleven, V., O. Popescu, and K.R. Koedinger. A tutorial dialogue system with knowledgebased understanding and classification of student explanations. In Second IJCAI Workshop
on Knowledge and Reasoning in Practical Dialogue Systems. 2001. Seattle, WA.
Core, M.G., J.D. Moore, and C. Zinn. Supporting constructive learning with a feedback
planner. In AAAI Fall Symposium on Building Dialogue Systems for Tutorial Applications.
2000. Cape Cod, MA.
Jordan, P.W., et al. Engineering the Tacitus-lite weighted abduction inference engine for
use in the Why-Atlas qualitative physics tutoring system. submitted.
Hobbs, J., et al., Interpretation as abduction. Artificial Intelligence, 1993. 63(1-2): p. 69142.
Strube, M. Never look back: An alternative to centering. In Proceedings of the 17th
International Conference on Computational Linguistics and the 36th Annual Meeting of
the ACL. 1998.
Jordan, P.W., C.P. Rose, and K. VanLehn, Tools for authoring tutorial dialogue knowledge, in Artificial Intelligence in Education: AI-Ed in the Wired and Wireless future, J.D.
Moore, C. Redfield, and W.L. Johnson, Editors. 2001, IOS: Washington, DC. p. 222-233.
Graesser, A.C., et al., Intelligent tutoring systems with conversational dialogue. AI
Magazine, 2001. 22(4).
Rose, C.P. Facilitating the rapid development of language understanding interfaces for
tutoring systems. In AAAI Fall Symposium on Building Tutorial Dialogue Systems. 2000.
Cape Cod, MA.
Evens, M.W., et al. Circsim-Tutor: An intelligent tutoring system using natural language
dialogue. In Twelfth Midwest AI and Cognitive Science Conference. 2001. Oxford, OH.
Freedman, R., et al., ITS Tools for natural language dialogue: A domain-independent
parser and planner, in Intelligent Tutoring Systems: 5th International Conference, ITS
2000, G. Gauthier, C. Frasson, and K. VanLehn, Editors. 2000, Springer: Berlin. p. 433442.
Rickel, J., et al., Building a bridge between intelligent tutoring and collaborative dialogue
systems, in Artificial Intelligence in Education: AI-Ed in the Wired and Wireless Future,
J.D. Moore, C. Redfield, and W.L. Johnson, Editors. 2001, IOS: Washington, DC. p. 592693.

Andes: A Coached Problem Solving
Environment for Physics ?
Abigail S. Gertner1 and Kurt VanLehn2
1

2

The MITRE Corporation, Bedford, MA 01730
LRDC, University of Pittsburgh, Pittsburgh, PA 15260

Abstract. Andes is an Intelligent Tutoring System for introductory college physics. The fundamental principles underlying the design of Andes
are: (1) encourage the student to construct new knowledge by providing
hints that require them to derive most of the solution on their own, (2)
facilitate transfer from the system by making the interface as much like a
piece of paper as possible, (3) give immediate feedback after each action
to maximize the opportunities for learning and minimize the amount of
time spent going down wrong paths, and (4) give the student flexibility
in the order in which actions are performed, and allow them to skip steps
when appropriate. This paper gives an overview of Andes, focusing on
the overall architecture and the student’s experience using the system.

1

Introduction

This paper is an overview of problem solving in Andes – an Intelligent Tutoring
System for introductory college physics. Andes interacts with students using
coached problem solving [12], a method of teaching cognitive skills in which the
tutor and the student collaborate to solve problems. In coached problem solving,
the initiative in the student-tutor interaction changes according to the progress
being made. As long as the student proceeds along a correct solution, the tutor
merely indicates agreement with each step. When the student gets stuck or makes
an error, the tutor helps the student overcome the impasse by providing hints
that lead the student back to the correct solution path.
The fundamental principles underlying the design of Andes are: (1) encourage
the student to construct new knowledge by providing hints that require them
to derive most of the solution on their own, (2) facilitate transfer from the
system by making the interface as much like a piece of paper as possible, (3) give
immediate feedback after each action to maximize the opportunities for learning
and minimize the amount of time spent going down wrong paths, and (4) give the
student flexibility in the order in which actions are performed, and allow them to
?

This research was supported by ARPA’s Computer Aided Education and Training
Initiative under grant N660001-95-C-8367, by ONR’s Cognitive Science Division under grant N00014-96-1-0260, and by AFOSR’s Artificial Intelligence Division under
grant F49620-96-1-0180. The authors would like to thank the members of the Andes
group.

G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, pp. 133–142, 2000.
c Springer-Verlag Berlin Heidelberg 2000


134

Abigail S. Gertner and Kurt VanLehn

Authoring Environment
Graphical author
interface

Student Environment
Workbench

Problem
Presentation

Assessor (BN)
Physics
Rules

Problem
Definition

Physics
Problem
Solver

Action
Interpreter

Solution
Graph

Help System
Procedural
help

Student Model

tutoring strategy
Conceptual
help

Example
study help

Fig. 1. The Andes System Architecture

skip steps when appropriate. This paper gives an overview of the system that we
designed following these principles, focusing on the overall architecture and the
student’s experience using Andes. Several of the modules that Andes comprises
are described in other papers and so we will not discuss them in detail here.
In particular, we will not talk at all about the Self-Explanation coach or the
Conceptual help system, which are described in [4, 5] and [1] respectively.
The following section provides a brief summary of the Andes architecture and
implementation. Section 3 gives an example of the typical student interaction
with Andes while solving a problem. In Section 4, we describe the underlying
system for providing feedback and help. Finally, in Section 5 we present some of
the work we have done to evaluate Andes with students.

2

Andes System Overview

The Andes project began in September 1995, and is a collaboration between
the University of Pittsburgh and the US Naval Academy. Andes is implemented
in Allegro Common Lisp and Microsoft Visual C++ and runs on Pentium PCs
under Windows 95.
Andes has a modular architecture, as shown in Figure 1. The left side of
Figure 1 shows the authoring environment for creating new problems. Prior to
run time, a problem author creates both the graphical description of the problem,
and the corresponding coded problem definition. Andes’ problem solver uses this
definition to automatically generate a model of the problem solution space called
the solution graph.
The right side of the figure shows the run-time student environment. The
Workbench is the graphical interface with which the student studies examples

Andes: A Coached Problem Solving Environment for Physics

135

and solves physics problems. The Workbench communicates with the Action
Interpreter, which looks up the student’s entries in the solution graph and provides immediate feedback as to whether the entries are correct or incorrect. More
detailed feedback is provided by Andes’ Help System [12]. Both the Action Interpreter and the Help System refer to the student model to make decisions about
what kind of feedback and help to give the student. The central component of
the student model is a Bayesian network that is constructed and updated by the
Assessor, and provides probabilistic estimates of the student’s mental state [3].
The student model also contains information about what problems the student
has worked on, what interface features they have used, and what help they have
received from the system in the past.

3

Solving Problems with Andes: An Example

One of the principles underlying the design of Andes was that the interface
ought to be as much like a piece of paper as possible, so as to facilitate fading
of tutorial support as students become more familiar with the physics domain,
and the eventual transfer from solving problems with Andes to solving problems
on paper. We attempted to keep the number of structured entry fields to a
minimum, since every piece of structure in the interface might serve as scaffolding
to the student, on which they could become dependent. As a result, the interface
initially appears quite simple, consisting of two main entry panes (Figure 2), in
which students can draw diagrams (upper left) and enter equations (lower right),
as well as the variable definitions pane, located above the equation pane, and
the hint window, below the diagram pane on the left.
3.1

Drawing Diagrams

When a problem is first opened, the diagram pane contains a statement of the
problem and a (read-only) picture of the problem situation. The lower part of the
pane is initially blank. This area is provided for the student to perform a qualitative analysis of the problem before they begin working out the quantitative
solution (in fact, some problems only ask for the qualitative analysis and do not
require the student to write any equations). This type of qualitative reasoning
is an important part of physics problem solving. It is used by expert physicists
both in talking about simple physics problems [2] and when discussing real-world
research results [10]. In addition, requiring qualitative reasoning in solving problems has been found to uncover students’ misconceptions better than allowing
students to use algebraic reasoning alone [11, 8].
Students use the drawing tools to the left of the diagram pane to enter
elements of a free body diagram such as force vectors. The student can also draw
motion vectors such as velocity and acceleration. To draw a vector, the student
clicks and drags the mouse in the diagram pane, rotating the vector until it
points in the desired direction and then releasing the mouse. Other items that
can be included in a diagram include coordinate axis systems, angles between

136

Abigail S. Gertner and Kurt VanLehn

Fig. 2. The Andes problem solving interface

vectors and axes, and the radius of a circular path. All of these are drawn with
the mouse using the tools on the left of the screen.
After the student has drawn an item in the diagram window using the mouse,
a dialog box appears in which the student must enter information defining the
object they have just drawn. This departure from the “piece of paper” principle
is necessary so that Andes can give appropriate feedback based on not just what
the student drew (eg. a vector pointing straight up) but also what they meant
by that entry (eg. a normal force exerted by the driveway on the car).
3.2

Variable Definitions

Another significant way that the Andes interface differs from a piece of paper
is in the definition of variables and how they are used in equations. When a
student is solving a physics problem on paper, she can start out right away by
writing down an equation, such as F = m ∗ a without explicitly stating what F,
m, and a refer to. Andes, on the other hand, requires all quantities to be defined
explicitly before they may be entered in equations. This enforces a systematic
approach to problem solving which can greatly reduce the number of careless
mistakes students make.
Variables may either be defined by assigning a label to an item in the diagram,
or by using a special purpose variable definition menu. Certain quantities (such
as scalar quantities like speed) cannot be drawn in a diagram, so the variable
menu must be used to define variables for these quantities. Using the variable
menu involves choosing the type of quantity to be defined and then filling in a
dialog box similar to the ones for diagram entries.

Andes: A Coached Problem Solving Environment for Physics

137

Whether a variable was defined as a diagram entry or using the variable
menu, after it has been defined it will appear in the variable definitions pane.
This pane lists all defined variables with their definitions and, for vectors, the
names of their components.
3.3

Entering Equations

Equations are entered in the text fields in the lower right pane of the Workbench,
a single equation per line. There is no structured equation editor in Andes.
Students use a conventional syntax for entering equations (operators *, /, +,
and -; ˆ for exponentiation; for subscripts). Students can enter anything they
want into the equation field and Andes will attempt to parse it and determine
whether it corresponds to a correct equation [6]. There may be more than one
possible parse for an equation, in which case Andes will look up whether any of
the parses represents a correct equation.
Variables in equations must first be defined so that they are listed in the
variable pane. If Andes finds tokens in the equation that can only be interpreted
as undefined variables, it displays an error message listing those tokens. If the
equation cannot be parsed for some other reason, it displays an error message
informing the student that it could not interpret the entry. Otherwise, Andes
gives simple correct/incorrect feedback on the equation entry.

4

Feedback and Help

As illustrated in Figure 1, the Action Interpreter is the central module of Andes.
It is responsible for getting student input from the Workbench, recording the
input in its databases, and returning feedback to the Workbench which includes
information about whether the student’s action was correct or incorrect, as well
as hints and help messages to be displayed. The following sections describe how
the Action interpreter and related modules do their jobs.
4.1

The Student Model

Modeling a student in an ITS involves a great deal of inherent uncertainty regarding not only the student’s beliefs and goals, but also the level of knowledge
that she has about the domain. There is additional uncertainty in Andes since
the student is not constrained to perform actions in a particular order. This
means that even if Andes has not observed the student performing a certain action, it cannot assume that the student doesn’t know how to perform that action
because the student may intend to perform it in the future. To address these multiple sources of uncertainty, Andes’ student model combines information about
the current state of the problem solving process with long-term assessment of the
student’s knowledge of physics in a probabilistic representation using Bayesian
networks [3].

138

Abigail S. Gertner and Kurt VanLehn

Each physics problem is represented by a separate Bayesian network. As a
student moves from one problem to the next, Andes’ assessment of her general physics knowledge is updated and used to initialize the model for the next
problem. Thus the modeling of problem-specific knowledge about each problem
is related through the domain-general assessment of physics knowledge that the
problems have in common.
The Bayesian network for each problem is constructed from a data structure called a solution graph which represents alternative solution paths for each
problem, including some that involve “buggy” rules and thus represent typical
incorrect student solutions. Andes’ Bayesian networks may have anywhere from
around 100 to over 200 nodes, depending on the complexity of the problem. Using a network that represents the entire solution space for the problem means
that the student model always has estimated probability values, even for steps
that haven’t been explicitly observed. This supports the Andes design principle
of allowing the student maximum flexibility in performing solution steps in any
order and skipping steps.
Bayesian networks provide us with a principled framework for combining
all of the different sources of uncertainty about the student’s problem-solving
processes. For example, if a given fact may be derived in two different ways,
and a student is observed to know the fact, the Bayesian network provides a
straightforward way of sharing the credit for that knowledge between the two
alternative derivations. Furthermore, if one of the derivations is known to be less
likely than the other (e.g., because it depends on a rule that the student probably
does not know), then the credit will be apportioned accordingly – giving more
weight to the derivation that the student is likely to know.
The Action Interpreter interacts with the Bayesian network in two ways.
First, when Andes observes the student performing an action that corresponds
to a node in the Bayesian network, the value of that node is clamped to True,
and the network is updated to reflect the new evidence. Second, when Andes
needs to respond to the student in a way that depends on some aspect of her
current knowledge or mental state, the Action Interpreter queries the Bayesian
network to find out the current probabilities associated with the relevant nodes.
This can be used for resolving ambiguities in determining the student’s current
goal, as well as for determining the appropriate level of help to give for part of
the problem.
4.2

Immediate Feedback

When a student makes an entry in Andes, the workbench always responds with
immediate flag feedback by changing the color of the entry – green for correct and
red for incorrect. This feedback is generated using information from the solution
graph. Here, we will describe how the feedback is generated for diagram entries
and variable definitions. Feedback for equation entries is more complicated, and
is described in some detail in [6].
Each student entry may reflect several different pieces of information, as
represented by the multiple fields of the dialog boxes for defining diagram entries.

Andes: A Coached Problem Solving Environment for Physics

139

For example, the definition of a force vector includes fields for the type of force,
the object it is acting on, the agent of the force, and its direction. When the
student enters a definition, the Action interpreter looks at the information in the
solution graph to see if any quantity exists there with exactly the same features
as the student’s entry. If it finds such an entry, the dialog box disappears and
the entry turns green on the screen. If no matching quantity is found, the Action
Interpreter has to determine what part of the entry is incorrect. To do this, it
attempts to determine what the student was most likely to have been trying to
enter, and then finds the features that differ between that intended entry and
the actual entry.
Finding the most likely intended entry is done using a combination of matching features and probabilities. The solution graph quantity with the most features
in common with the student’s entry, and the highest probability in the Bayesian
network, is selected as the intended entry. This entry is then compared to the
student’s entry to finds those features that differ between the two. Andes then
turns red only the fields in the dialog box corresponding to the mismatched features. This gives the student specific feedback on what part of their entry was
in error.
4.3

What’s Wrong with That?

As noted earlier, one of the design principles for Andes was to encourage constructive, as opposed to passive, learning. Therefore, Andes gives away as little
information as possible unless the student asks for it. In the case of errors, Andes
always starts by giving flag feedback. This feedback is accompanied by a hint
or error message only in the case of simple syntactic errors. For all other errors,
the student is expected to attempt to fix the problem on her own if she can.
If the student is not able to fix an erroneous entry on her own, she can select
the entry and ask “what’s wrong with that?” using a menu, and Andes will
respond with a short hint intended to point the student toward the feature of
her entry that was incorrect. For example, if the entry was an acceleration vector
labeled ‘a’ whose direction was incorrect, the hint might say “think about the
direction of ‘a’.”
When a non-specific hint is given, there will also be a link labelled “Explain
Further” in the hint window, which the student can click on to get more information. Clicking repeatedly on “Explain Further” will eventually result in a hint
that explicitly tells the student how to fix their entry. For example, the bottomout hint for the direction of the acceleration vector might be “The direction of
‘a’ is horizontal and to the left.”
4.4

Procedural Help

Procedural help is the part of Andes’ help system that is responsible for generating a hint when the student gets stuck and asks for help [7]. Hints are generated
based on the state of the student model at the time the student asks for help. To
produce a hint, Andes first selects a node in the solution graph that will be the

140

Abigail S. Gertner and Kurt VanLehn

topic of the hint. The topic node should represent a proposition that is relevant
to what the student has been doing recently and that the student is likely to
want to address next. The hint topic node is selected by first identifying a goal
node in the solution graph that is likely to be the reason for the student’s most
recent action, and then following a path from that goal to find a node representing an action that the student has not yet done and, according to the Assessor,
probably does not know about (this procedure is described in more detail in [7]).
Once a hint topic node has been selected, the hint template belonging to the
proposition the node represents is instantiated with the contextually relevant
information from the problem to generate an English string to be displayed to
the student. As in the case of the “what’s wrong with that” hints, the initial
hint given for a particular topic node will be quite general, to encourage students
to recover from the impasse on their own. Students can then click on a link to
get successively more specific hints until they are able to continue solving the
problem.

5

Evaluations of Andes

Andes’ development has been carried out in conjunction with an ongoing series
of formative evaluations, which were used to assess the usability of the interface,
suggest new features, and evaluate the effectiveness and clarity of hints and help
messages. Prototype versions of the system have been used by students at the US
Naval Academy every semester since the Fall of 1996. Additionally, several more
in-depth user studies were carried out at the University of Pittsburgh during
that time.
At the Naval Academy, students taking the introductory physics course were
asked to use Andes to do some of their homework. To do this, each student
had to download the Andes installation file from the local network, install it
on their personal computer in their dorm room, and start using on their own
after just a short demonstration. Right away, this required us to implement an
easily used installation program, to make sure that the Andes interface was
simple enough that students could get up and running without much help, and
to include extensive on-line help files that they could access while they were
using the program.
We used several methods for getting information from students during these
formative evaluations. First, they were encouraged to enter comments in a special
dialog box as they worked with Andes. We found that students did not enter
comments very often, but when they did we were able to bring them to the
attention of the instructors who would quickly address the student’s question
or concern. Second, students filled out a questionnaire about their experience
using the program. These questionnaires turned out to be not very informative
because students did not give much detailed information about what they would
like to see changed in the system. Third, they communicated frequently with
their instructors about problems they were having using the system. This was
extremely important as it allowed the instructors to develop a very complete

Andes: A Coached Problem Solving Environment for Physics

141

overall view of the problems people were having and what we could do to fix
them. Finally, all student activity within Andes is recorded in log files which the
students were prompted to upload each time they exited the program. These
log files, including students’ comments as well as a complete record of every
action performed on the interface, provided a wealth of information to guide us
in improving the system. Log files were also used by the instructors to get a
better sense of what a student had been doing when they came in for extra help.
At the University of Pittsburgh our approach in evaluating the system was
to record a small number of sessions with students using Andes, and to have
the students “think out loud” as they worked with the program. We could then
identify significant events in the session records where students failed to learn
something due to a flaw in Andes’ help, and rank them according to frequency
and importance. We used these events as targets for improving the system and
tested our changes by performing the same actions on the new version of Andes
and seeing if the help it gave had improved. In this way, we were able to fix many
of the major problems students had both with the interface design and the help
system.
In the Fall of 1999 we performed a summative field evaluation of Andes at the
Naval Academy. Andes was used for four weeks by 173 students in eight sections
of the Naval Academy’s introductory physics course. At the end of this time, they
were given a midterm exam covering material that was taught by Andes (and
by instructors during course lectures and sections). The students’ performance
on the midterm was compared to a control group of 162 students whose sections
did not use Andes. The results of this comparison were encouraging. Students
who used Andes performed 2.9 percent (1/3 of a letter grade) better on average
than students who did not use Andes (p ≤ 0.033). This compares favorably with
other successfully evaluated tutoring systems. For instance, the PUMP Algebra
Tutor had an effect size1 of 0.3σ on standardized math tests [9], whereas Andes
effect size was 0.2σ on the normal exam used by the whole course. Although
the PUMP Algebra Tutor also had an effect size of approximately 1.0σ on tests
designed for its content, we did not use such tests in our evaluation of Andes.
Perhaps more interestingly, when the Andes results are broken down by the
students’ major, we see that the effect of Andes for humanities majors was the
largest (7.3 percent), with the effect for science majors next largest (3.9 percent).
In the case of the engineering students (who were also taking a course on statics
concurrently) there was actually a small (1.3 percent) negative effect for Andes.
Thus, Andes appears to be most effective with students who are most likely to
need help learning physics.

6

Conclusions

In this paper we have provided an overview of the Andes system, showing how
we were guided by the four design principles listed in the Introduction. We have
1

Effect size is calculated as the difference between the score of the experimental group
and the score of the control group, divided by the standard deviation of the control.

142

Abigail S. Gertner and Kurt VanLehn

learned a great deal in the process of developing and deploying Andes, both
about designing an ITS to teach complex problem-solving skills and about the
integration of such a system into an existing instructional environment. Future
work on this project should continue to yield many more insights (see [13] in
this volume).

References
[1] P. L. Albacete and K. A. VanLehn. The conceptual helper: An intelligent tutoring
system for teaching fundamental physics concepts. In Proceedings of the Fifth
International Conference on Intelligent Tutoring Systems, 2000.
[2] M. Chi, P. Feltovich, and R. Glaser. Categorization and representation of physics
problems by experts and novices. Cognitive Science, 5:121–152, 1981.
[3] C. Conati, A. S. Gertner, K. VanLehn, and M. J. Druzdzel. On-line student
modeling for coached problem solving using Bayesian networks. In Proceedings
of UM-97, Sixth International Conference on User Modeling, pages 231–242, Sardinia, Italy, June 1997. Springer.
[4] C. Conati and K. VanLehn. Teaching meta-cognitive skills: implementation and
evaluation of a tutoring system to guide self-explanation while learning from examples. In In ¿ Proceedings of AIED 99, 9th World Conference of Artificial
Intelligence and Education, Le Man, France, 1999.
[5] C. Conati and K. A. VanLehn. Further results from the evaluation of an intelligent
computer tutor to coach self-explanation. In Proceedings of the Fifth International
Conference on Intelligent Tutoring Systems, 2000.
[6] A. S. Gertner. Providing feedback to equation entries in an intelligent tutoring
system for physics. In Proceedings of the 4th International Conference on Intelligent Tutoring Systems, San Antonio, August 1998.
[7] A. S. Gertner, C. Conati, and K. VanLehn. Procedural help in Andes: Generating
hints using a Bayesian network student model. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence, Madison, WI, 1998.
[8] J. I. Heller and F. Reif. Prescribing effective human problem-solving processes:
Problem descriptions in physics. Cognition and Instruction, 1(2):177–216, 1984.
[9] K. R. Koedinger, J. R. Anderson, W. H. Hadley, and M. A. Mark. Intelligent
tutoring goes to school in the big city. In J. Greer, editor, Proceedings of the
7th World Conference on Artificial Intelligence and Education, pages 421–428,
Charlottesville, NC, 1995.
[10] A. Van Heuvelen. Learning to think like a physicist: A review of research-based
instructional strategies. American Journal of Physics, 59(10):891–897, 1991.
[11] A. Van Heuvelen. Overview, case study physics. American Journal of Physics,
59(10):898–907, 1991.
[12] K. VanLehn. Conceptual and meta learning during coached problem solving.
In C. Frasson, G. Gauthier, and A. Lesgold, editors, Proceedings of the Third
International Conference on Intelligent Tutoring Systems ITS ’96, pages 29–47.
Springer, 1996.
[13] K. VanLehn, R. Freedman, P. Jordan, R. C. Murray, R. Osan, M. Ringenberg,
C. Rosé, K. Schulze, R. Shelby, D. Treacy, A. Weinstein, and M. Wintersgill.
Fading and deepening: The next steps for andes and other model-tracing tutors. In
Proceedings of the Fifth International Conference on Intelligent Tutoring Systems,
2000.

660

Poster Paper

A Reification of a Strategy for Geometry
Theorem Proving
Noboru Matsuda1 and Kurt VanLehn2,?
1

2

Intelligent Systems Program, University of Pittsburgh,
Learning Research and Development Center, University of Pittsburgh
3939 O’Hara Street, Pittsburgh PA 15260

This study addresses a novel technique to build a graphical user interface
(GUI) for an intelligent tutoring system (ITS) to help students to learn geometry theorem proving with construction – one of the most challenging and creative
parts of elementary geometry. Students’ task is not only to prove theorems, but
also to construct missing points and/or segments to complete a proof (called
auxiliary lines). The problem space of theorem proving with construction is
generally huge, thus understanding a search strategy is a key issue for students
to succeed in this domain. Two major challenges in building a GUI for an intelligent learning environment are (a) to build a theorem prover that is capable of
construction, and (b) to establish a cognitive model of understanding a complex
problem-solving strategy.
So far, we have built a geometry theorem prover, GRAMY, which can automatically construct auxiliary lines when needed to complete a proof. GRAMY
utilizes a simple single heuristic for construction, which says that “apply a known
axiom or theorem backwards by overlapping the related configuration with the
diagram given in the problem while allowing the match to omit segment(s) in the
configuration.” The auxiliary lines are those which match the omitted segments.
Surprisingly, this simple heuristic works very well. This suggests that it might
be possible to teach students how to construct auxiliary lines.
In order to develop an ITS based on GRAMY, we need a GUI to display
the reasoning in a graphical, manipulable form — to “reify” (make real) the
reasoning. Some common techniques for reification have flaws, so suggest a new
one. Our basic idea is to reify the search process rather than the structure of
the ultimate solution. The resulting GUI shows an intermediate state of proof.
It consists of the diagram of a theorem to prove, and applicable axioms (or
theorems) in that state. Using tools that look like those of a web browser, the
student can select an applicable axiom to proceed a state, or go back and forth
between states.
We show how this reification technique can be applied to teach students three
basic search strategies; forward chaining, backward chaining, and backing up at
dead-ends. We discuss why this reification model should be better than the ones
that reifies the entire solution space as trees.
?

Email: mazda@isp.pitt.edu and vanlehn@cs.pitt.edu
This research was supported by NSF grant number 9720359 to CIRCLE:
Center for Interdisciplinary Research on Constructive Learning Environments.
http://www.pitt.edu/˜circle

G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, p. 660, 2000.
c Springer-Verlag Berlin Heidelberg 2000


Fading and Deepening: The Next Steps for Andes
and Other Model-Tracing Tutors
1

1

1

1

1

Kurt VanLehn , Reva Freedman , Pamela Jordan , Charles Murray , Remus Osan ,
1
1
3
2
2
Michael Ringenberg , Carolyn Rosé , Kay Schulze , Robert Shelby , Donald Treacy ,
1
2
Anders Weinstein , and Mary Wintersgill

Abstract. Model tracing tutors have been quite successful in teaching cognitive
skills; however, they still are not as competent as expert human tutors. We
propose two ways to improve model tracing tutors and in particular the Andes
physics tutor. First, tutors should fade their scaffolding. Although most model
tracing tutors have scaffolding that needs to be gradually removed (faded),
Andes’ scaffolding is already “faded,” and that causes student modeling
difficulties that adversely impact its tutoring. A proposed solution to this
problem is presented. Second, tutors should integrate the knowledge they
currently teach with other important knowledge in the task domain in order to
promote deeper learning. Several types of deep learning are discussed, and it is
argued that natural language processing is necessary for encouraging such
learning. A new project, Atlas, is developing natural language based
enhancements to model tracing tutors that are intended to encourage deeper
learning.
1

1

University of Pittsburgh, Learning Research and Development Center, Pittsburgh,
PA, 15260 (vanlehn@cs.pitt.edu)
2
United States Naval Academy, Department of Physics
3
United States Naval Academy, Computer Science Department

Model Tracing Tutors

This paper considers how to improve an already successful type of intelligent tutoring
system, the model-tracing tutor (MTT). Although Anderson, Boyle and Reiser (1985)
coined the term, MTT refers to a relatively broad class of intelligent tutoring systems,
namely those that contain a model of the cognition that one would like students to
engage in and a means of encouraging students to reason in that fashion. An MTT
usually has three modules: an expert model, a graphical user interface and a
pedagogical module. After describing these modules below, we describe in the
following sections the two main problems that MTT face and our proposed solutions
to them. We conclude with a short comparison of the proposed MTT and human
tutors.
An MTT contains an expert model which models how the designers would like
students to reason. It has problem solving strategies that are coherent, precise,
complete, and often quite simple. Sometimes the strategies are designed to enhance
G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, pp. 474-483, 2000.
 Springer-Verlag Berlin Heidelberg 2000

Fading and Deepening: The Next Steps for Andes and other Model-Tracing Tutors

475

learning rather then accurately replicate expert strategies. They may require doing
steps that students and even some instructors do not usually do, such as solving
algebra word problems by writing arithmetic equations before writing algebraic ones
(Aleven, Koedinger, Sinclair, & Snyder, 1998).
Most MTT have a high bandwidth graphical user interface (GUI). A GUI has
high bandwidth if it has students display most of their reasoning, typically by
requiring them to enter more information than they would if they were working as
normal on a sheet of paper (VanLehn, 1988). For instance, students might have to
define variables, as in the Andes physics tutor (Gertner & VanLehn, 2000), or provide
coherent labels for columns in tables and axes in graphs (Aleven, Koedinger, &
Cross, 1999; Aleven et al., 1998). Some GUI even have students maintain a goal tree
(e.g., Koedinger & Anderson, 1993; Reiser, Beekelaar, Tyle, & Merrill, 1991;
Singley, 1990). Many MTT designers claim that explicating such information
increases learning, and there is some evidence for that claim (e.g., Merrill & Reiser,
1994; Singley, 1990).
An MTT also contains a pedagogical module that can provide immediate feedback
and hints. Feedback is given whenever the student’s action does not match the expert
model’s action. Help is usually provided via a hint sequence. A hint sequence starts
with a general hint, then allows the student to try again. If the student’s new action is
also incorrect or the student ask for more help, then the tutor gives the next hint in the
sequence, which provides more information about what the target action should be.
The hints become more specific until the student enters the target action correctly. If
the tutor runs out of hints, it either tells the student exactly what to enter or does the
action for the student. Clearly, the pedagogical module is based on the hypothesis
that immediate feedback and hint sequences facilitate learning. There have been
several studies of this hypothesis (e.g., Anderson, Corbett, Koedinger, & Pelletier,
1995; Mark & Greer, 1995).

2

Fading

Although MTTs have an enviable track record (e.g., Anderson et al., 1995;
Koedinger, Anderson, Hadley, & Mark, 1995; McKendree, Radlinski, & Atwood,
1992; Reiser, Copen, Ranney, Hamid, & Kimberg, in press; Shelby et al., in prep.),
they are sometimes criticized as being too rigid. Below, we review such claims then
discuss “fading,” which is an obvious solution to the rigidity problems. However, the
bulk of this section concerns some non-obvious problems that fading causes, how
they emerged in the Andes tutoring system, and what we propose to do about them.
MTTs are sometimes criticized for allowing only one strategy for problem solving
when many are possible. For instance, Anderson and Corbett’s Lisp tutor has been
criticized for forcing students to enter code top-down (e.g., Reiser, Kimberg, Lovett,
& Ranney, 1992). The critics argue that when the tutor keeps students on the solution
path of a single strategy, it prevents the students from inventing their own strategies,
testing them and refining them.
A second complaint is that the tutor often forces students to enter information that
they often try to hold in their working memory. This could prevent students from
learning how to manage their use of memory. For instance, when solving complex

476

Kurt VanLehn et al.

algebra equations, some students try to write down fewer intermediate steps then
expert mathematicians (Lewis, 1981). Such students need to learn to write more
down and trust their memory less, but the rigid scaffolding of MTT GUIs thwarts
such learning because it controls how much they must write down.
A third criticism is that the tutor provides too much scaffolding of error handling.
Because the tutor detects errors for the student and hints at how to correct them, when
students of MTTs are tested with the tutor absent, they are often worse at detecting
and correcting their own errors than students who covered the same material without
the tutor (Reiser et al., in press).
As Collins, Brown and Newman (1989), McArthur (1990) and others have noted,
expert human tutors often start with large amounts of scaffolding, then fade it as
students exhibit more competence. Similarly, MTTs should fade the procedural
restrictions on problem solving, thus allowing students to solve problems any way
they want, to experiment with strategies and to manage their use of memory. MTTs
should also fade their support for error handling, thus allowing student to learn how to
detect and correct errors by themselves.
2.1

Andes is Already Partially Faded

Although most MTTs should fade their scaffolding, our MTT, Andes (Gertner &
VanLehn, 2000; VanLehn, 1996) has the opposite problem. It has turned out to have
too little scaffolding. Although Andes flags incorrect entries by turning them red, it
allows students to enter steps in any order and to omit almost any step. This gives
Andes’ students the freedom to discover strategies and manage their use of memory.
On the other hand, the lack of procedural restrictions has caused 2 pedagogical
problems.
First, some students failed to develop effective problem solving strategies, and
Andes’ hints about what step to do next only frustrated and confused them. The log
files indicate that many Andes students do not follow a single strategy. They often
mix steps from multiple strategies, probably without knowing that they are doing so.
When a student asks for advice on what to do next, Andes uses probabilistic reasoning
to guess the step the student might be trying to do next and construct a hint sequence
leading up to it (Gertner, Conati, & VanLehn, 1998; Gertner, 1998). The students
sometimes can’t figure out why that step should be next. Indeed, there usually is no
good explanation because their strategy wasn’t coherent. Some students felt quite
frustrated by the apparent randomness of Andes’ advice, not realizing that it was
caused by their own random behavior.
Second, when students received hints on errors, they often could not fix the errors
themselves. Instead, they proceeded all the way to the last hint in the sequence,
which would tell them exactly what to enter. Andes hint sequences are no different in
design from those used successfully by other MTT, which suggests that it is the
context of the hints which is causing them to fail. For instance, a common error was
to omit a negative sign that was introduced by projecting a vector. Andes’ first hint
on a sign error is to “check your signs.” If the student has drawn the appropriate
vector and is working on projecting it (e.g., entering an equation such as
VX=−V*cosθ), then this hint would probably work fine. However, the students who
make this error have often skipped both drawing the vector and writing the projection

Fading and Deepening: The Next Steps for Andes and other Model-Tracing Tutors

477

equation down. When they receive the “check signs” hint, they can’t figure out where
the negative sign should have come from because they did the calculations in their
head. Many hints fail not because they are bad hints, but because Andes let student
do so many critical steps in their head that the students cannot reconstruct their
reasoning in order to find the error.
From these observations during formative evaluations, it has become clear that
although Andes’ faded scaffolding allows students to invent their own strategies,
repair their own errors, and manage their use of memory, some students seem to need
more scaffolding. They should be constrained to use a single strategy and to not skip
key steps. That is, some students need the rigid, procedural scaffolding that most
MTT have but Andes lacks.
2.2

Micro-adaptive Fading

A straightforward approach to fading is to equip the MTT with different levels of
scaffolding. Novice students start with a high level of scaffolding. As their
competence increases, the tutor reduces (fades) the level of scaffolding. This is a
macro-adaptive approach to fading (cf. Shute, 1993). The tutor changes its level of
scaffolding in between problems, but during a problem’s solution, the level remains
the same. Inspired by our analysis of human tutors, we are revising Andes to use a
micro-adaptive approach to fading. The scaffolding only occurs at the moment the
student seems to need it.
The key idea is that although Andes will be able to recognize a wide variety of
strategies, skipped steps, etc., it will only give advice and help on a single strategy.
The strategy to (a) select a quantity whose value is sought, (b) decide which major
physics principle is appropriate for finding it, (c) execute the procedure for that
principle, (d) figure out which quantities still need to be found, then repeat the cycle
from step (a). This strategy is well known in physics, and the procedures often appear
in textbooks. Some tutoring systems (e.g., Reif & Scott, 1999) constrain students to
follow this strategy. The physicists on the Andes project have designed the specific
strategy to be taught by determining which physics principles are the major ones and
designing procedures for them. For instance, Newton’s second law (F=m*a) is a
major principle, but the weight law (W=m*g) is not. Instead, it is included in the
Newton’s law procedure.
The physicists also determined which steps in the
procedures must be entered by student on the Andes GUI, and which can be done
mentally if the student wishes.
The new Andes will not constrain students to follow the strategy. Just as before,
students can enter steps in any order, skipping as many as they like. As long as they
enter a correct step, it will turn green. Moreover, if they can fix their incorrect (red)
steps without help, then they will see no signs of the target strategy. However, if they
ask for help, then Andes will try to get them to follow the target strategy.
More specifically, if they are stuck and ask for a hint on what step to do next,
Andes will start by asking them, “What quantities are you seeking?” and offering a
menu. Incorrect menu selections evoke feedback and a hint sequence. When the
student has selected a quantity that is actually sought in this problem, then Andes
asks, “What principle should be used to find it?” and offers a menu. Again students
get feedback and hints until they have chosen a principle appropriate for the problem
and the sought quantity. Andes then matches the principle’s procedure to the

478

Kurt VanLehn et al.

student’s entries and determines the first step that the student has not yet done. It
gives hints on that step. When the student finally does the target step, this help
episode is over, and the student resumes solving the problem. If the student
immediately asks for another hint on what to do next, then Andes may skip some of
the dialog and just give hints on the next step in the principle’s procedure. In other
words, the procedural scaffolding is there if the student wants it, but they have to ask
for it.
When the student asks “what’s wrong?” with an incorrect step, then the new Andes
will not always given them help on that step. If the student has skipped some critical
steps, then Andes will identify which one should be done first and hint it by saying,
“Your errors are probably caused by skipping <step>. If you do it, you’ll probably
figure out what your errors are. If not, ask me for help again.” If the student does ask
for help again, they will get it in a context which allows the hints to succeed. For
instance, they won’t actually get hints on fixing a sign error until they have first
drawn the relevant vector.
This approach to fading is micro-adaptive in that scaffolding is done only on the
part of the strategy whose absence is causing errors or confusion. As students become
more competent, they ask for less help and thus receive less scaffolding. Thus, Andes
fades the scaffolding without even maintaining a student model.
Andes’ micro-adaptive fading should cause more transfer than macro-adaptive
fading. If students are forced to follow a specific procedure, then they will only learn
how to do the procedure. They won’t learn about the errors and confusions that
happen if they don’t follow it. With Andes’ micro-adaptive fading, students learn the
value of the procedure the hard way, by making errors and getting lost.
This
increases the chance that they will use the target strategy when working on their own,
thus increasing transfer.

3

Deep Learning

MTTs have sometimes been criticized for failing to encourage deep learning. The
following are probably only a few of the criticisms that fall under this category, but
they are certainly enough to set a challenging research agenda:
1. If students don’t reflect on the tutor’s hints, but merely keep guessing until they
find an action that gets positive feedback, they can learn to do the right thing for
the wrong reasons, and the tutor will never know such shallow learning occurred
(Aleven et al., 1999; Aleven et al., 1998).
2. Since the tutor does not ask students to explain their actions, students may not
learn the domain’s language. Our verbal protocols are replete with domain
language misuse. Educators have become increasingly concerned that students
learn to “talk science,” as that appears to be part of a deep understanding of the
science, as well as facilitating scientific writing, working collaboratively in
groups, and beginning to participate in the culture of science.
3. The high bandwidth GUI, which asks students to display many of the details of
their reasoning, doesn’t promote stepping back to see the “basic approach” one
has used to solve a problem. Even students who have gotten high grades in a
physics course can seldom describe their basic approaches, nor tell when two
problems have similar basic approaches (Chi, Feltovich, & Glaser, 1981).

Fading and Deepening: The Next Steps for Andes and other Model-Tracing Tutors

479

4.

Students of quantitative skills, such as algebra or physics problem solving, are
usually not encouraged to see their work from a qualitative, semantic perspective,
so they fail to induce versions of the skills that can be used to solve qualitative
problems and to check quantitative ones for reasonableness. Even physics
students with high grades often score poorly on tests of qualitative physics (e.g.,
Hestenes, Wells, & Swackhamer, 1992).
Many of these objections can be made to just about any form of instruction. Even
expert tutors and teachers have difficulty getting student to learn deeply. Therefore,
these criticisms of MTT should only encourage us to improve them, not reject them.
There are two common themes in the list above. First, all four involve integrating
problem-solving knowledge with other knowledge, namely: (1) principles or
rationales, (2) domain language, (3) abstract, basic approaches and (4) qualitative
rules of inference. Second, the kinds of instructional activities that are currently used
to tap these other kinds of knowledge make critical use of natural language. Although
one can invent graphical or formal notations to teach these kinds of knowledge on a
computer, they might be more confusing to the students and instructors than the
knowledge that they are trying to convey. Moreover, students and instructors are
likely to resist learning a new formalism, even a graphical one, if they will only use
temporarily.
3.1

Atlas: A Natural-Language Enhancement for Model-Tracing Tutors

We believe that if MTTs are to become more effective at encouraging deep learning,
they must use natural language. Therefore, we have begun building Atlas, a module
that can be added to Andes or other MTT in order to conduct natural language dialogs
that will promote deep learning. Atlas uses natural-language generation technology
originally developed for CIRCSIM tutor (Freedman & Evens, 1996), the LC-FLEX
parser (Rose & Lavie, in press), and the COCONUT model of collaborative dialog
(DiEugenio, Jordan, Thomason, & Moore, in press). See (Freedman, Rose,
Ringenberg, & VanLehn, 2000) for a description of the system architecture.
Initially, Atlas will support only a simple form of interaction. Most of the time, the
students interact with Andes just as they ordinarily would. However, if Atlas notices
an opportunity to promote deep learning, it takes control of the interaction and begins
a natural language dialog. Although Atlas can ask students to make Andes actions as
part of the dialog (e.g., it might have the student draw a single vector), most of the
dialog is conducted in a scrolling text window. When Atlas decides the dialog is
complete, it signs off and lets the student return to solving the problem with Andes.
The dialogs are called knowledge construction dialogs, because they are designed
to encourage students to infer or construct the target knowledge. Like a Socratic tutor
(Collins & Stevens, 1982), Atlas tries to avoid telling the student what the student
needs to know, but it may do so as a last resort.
3.2

Knowledge Construction Dialogs to Teach Principles

So far, Atlas conducts just one kind of knowledge construction dialog. The dialogs
are designed to teach a domain principle. They occur when the student has made an

480

Kurt VanLehn et al.

error or gotten stuck, and has asked Andes for help. Atlas takes over when Andes
would have given its final hint. Instead of telling the student the target principle, it
conducts a dialog designed to teach the principle in a Socratic fashion.
The specific dialogs we are implementing were observed in transcripts of human
tutors (VanLehn, Siler, Murray, & Baggett, 1998; VanLehn, Siler, Murray,
Yamauchi, & Baggett, in press). For instance, consider the target principle “When an
object is moving in a straight line and slowing down, its acceleration is opposite its
velocity.” VanLehn et al. (in press) observed 15 knowledge construction dialogs for
this rule. However, they can be reduced to three basic types. One derives the target
principle from the definition of acceleration, another uses analogy, and a third shows
that the students’ belief (that acceleration is in the same direction as velocity) leads to
contradictory and absurd conclusions.
Knowledge construction dialogs are often nested. For example, suppose the tutor
starts by asking the student for the definition of acceleration. Most students will say,
“velocity divided by time,” which is almost right, so the tutor corrects it in a subtle
way (Graesser, Person, & Magliano, 1995) by splicing in the missing information
“Yes, it’s the change in velocity divided by time.” However, if the student’s response
indicates greater confusion than that (e.g., “It’s the derivative of time.”), then the tutor
may drop into a knowledge construction dialog on the definition of acceleration.
Empirically, human tutors seldom nest knowledge construction dialogs more than two
deep. If the student seems hopelessly confused, then the tutor may abandon the top
level knowledge construction dialog and start a different one by saying, e.g., “Well,
forget about the definition of acceleration. Let’s try an analogy. Suppose....”
Implementing even one knowledge construction dialog strategy is a major
endeavor. Not only must the dialog strategy itself by developed, but types of student
responses must be anticipated, each with an appropriate tutorial response. Moreover,
the knowledge mentioned in the dialogs must be represented in such a way that the
natural language processing modules can both recognize it in the students’
contributions to the dialog and render it as fluent, easily understood text for the tutor’s
contributions. Our progress has been slow thus far, but should pick up as we develop
more and more knowledge construction dialog strategies, because we expect them to
share many parts.
3.3

Other Knowledge Construction Dialogs

We are in the process of collecting examples of other types of knowledge construction
dialogs
from
a
public
archive
of
tutorial
dialogs
(see
http://www.pitt.edu/~circle/Archive.htm) and designing Atlas dialog strategies to
encourage deep learning of several different kinds. The following list indicates our
plans and is numbered to correspond to the list of deep learning types mentioned
earlier:
1. Avoiding superficial learning. Critics say that MTT students often learn how to
do the right thing for the wrong reasons. That is, they induce conditions for their
operators that have roughly the same extension as the correct conditions. To
detect such shallow learning, Andes-Atlas should periodically ask students to
describe and justify their actions. For instance, if the student enters F–
W−m*a=0, Atlas-Andes should ask, “What are you doing here?” Hopefully, the

Fading and Deepening: The Next Steps for Andes and other Model-Tracing Tutors

2.

3.

4.

481

student will answer, “I’m applying Newton’s law along the vertical axis.”
However, if they say, “I’m solving F−W=m*a,” then the tutor should probe
deeper to see if there is any knowledge behind the algebra.
Using the domain language. Critics say that MTTs should teach students how to
use the language of the domain. For example, if the students, when asked the
question above, fail to give a recognizable answer, they should be coached on
how to use physics language more accurately. For instance, the tutor might say,
“I didn’t understand your explanation. Could you say something like, ‘I applied
<a principle> to <objects> because I wanted <goal>.’? For example, you might
say, ‘I applied Newton’s Second Law to the car because I wanted to find its
acceleration.’”
Inducing and using abstract plans. Critics say that MTTs should encourage
students to see the basic approach behind their problem solving and abstract plans
from the details. For instance, after students have finished the classic Atwood’s
machine problem (two blocks hung from either end of a string that is draped over
a massless, frictionless pulley), Atlas-Andes could ask them: “What was your
basic approach to this problem?” They will hopefully say something like, “I
applied Newton’s law twice, once for each block.”
Connecting qualitative and quantitative reasoning. MTTs should teach students
how to reason qualitatively and how to connect that qualitative reasoning with
their quantitative reasoning. For example, the tutor could interject qualitative
questions into the student’s work, such as “If the acceleration and the tension
force are both upward, then increasing the tension should increase the
acceleration, right? Is your equation consistent with that fact?” After the
problem is solved, the tutor can ask the student to indicate what will happen
under other conditions specified qualitatively. For instance, a common question
that textbooks ask after students have solved the Atwood’s pulley system is,
“What would you expect to happen if the two blocks had the same mass? What
do your equations say? What would happen if the left block’s mass were zero?
What do your equations say?”

Andes-Atlas is intended to close the gap between human tutors and MTTs. It
provides enrichments to the usual MTT dialog in the form of knowledge construction
dialogs. Although it might be possible to provide these enrichments with a GUI
solution, the nature of the enrichments makes that unlikely. They clearly call for a
language-based solution and that is what Andes-Atlas will provide.

Acknowledgements
This research was supported by Grant N00014-96-1-0260 from the Cognitive
Sciences Division of the Office of Naval Research and by Grant 9720359 from the
LIS program of the National Science Foundation. We gratefully acknowledge the
contributions of the “Andes Alumni,” Abigail Gertner, Cristina Conati, Patricia
Albacete, Stephanie Siler, Ellen Dugan, Zhendong Niu and David Correll.

482

Kurt VanLehn et al.

References
1.
2.

3.
4.
5.
6.

7.
8.
9.
10.
11.
12.

13.
14.
15.
16.
17.

18.

Aleven, V., Koedinger, K. R., & Cross, K. (1999). Tutoring answer-explanation fosters
learning with understanding, Artificial Intelligence in Education (pp. 199-206).
Amsterdam: IOS Press.
Aleven, V., Koedinger, K. R., Sinclair, H. C., & Snyder, J. (1998). Combating shallow
learning in a tutor for geometry problem solving. In B. P. Goettle, H. M. Halff, C. L.
Redfield, & V. J. Shute (Eds.), Intelligent Tutoring Systems, Proceedings of the Fourth
International Conference (pp. 364-373). Berlin: Spring-Verlag.
Anderson, J. R., Boyle, C. F., & Reiser, B. J. (1985). Intelligent tutoring systems. Science,
228, 456-462.
Anderson, J. R., Corbett, A. T., Koedinger, K. R., & Pelletier, R. (1995). Cognitive Tutors:
Lessons Learned. The Journal of the Learning Sciences, 4(2), 167-207.
Chi, M. T. H., Feltovich, P., & Glaser, R. (1981). Categorization and representation of
physics problems by experts and novices. Cognitive Science, 5, 121-152.
Collins, A., Brown, J. S., & Newman, S. E. (1989). Cognitive apprenticeship: Teaching
the craft of reading, writing and mathematics. In L. B. Resnick (Ed.), Knowing, learning
and instruction: Essays in honor of Robert Glaser (pp. 543-494). Hillsdale, NJ: Lawrence
Erlbaum Associates.
Collins, A., & Stevens, A. (1982). Goals and methods for inquiry teachers. In R. Glaser
(Ed.), Advances in Instructional Psychology, Vol. 2 . Hillsdale, NJ: Lawrence Erlbaum
Associates.
DiEugenio, B., Jordan, P. W., Thomason, R. H., & Moore, J. D. (in press). The agreement
process: An empirical investigation of human-human computer-mediated dialogues.
International Journal of Human-Computer Studies.
Freedman, R., & Evens, M. W. (1996). Generating and revising hierarchical multi-turn
text plans in an ITS. In C. Frasson, G. Gauthier, & A. Lesgold (Eds.), Intelligent Tutoring
Systems: Proceedings of the 1996 Conference (pp. 632-640). Berlin: Springer.
Freedman, R., Rose, C. P., Ringenberg, M. A., & VanLehn, K. (2000). ITS Tools for
natural language dialogue: A domain-independent parser and planner. In C. Frasson (Ed.),
Proceedings of ITS 2000. . Berlin: Springer-Verlag.
Gertner, A., Conati, C., & VanLehn, K. (1998). Procedural help in Andes: Generating
hints using a Bayesian network student model., Proceedings of the 15th national
Conference on Artificial Intelligence .
Gertner, A. S. (1998). Providing feedback to equation entries in an intelligent tutoring
system for Physics. In B. P. Goettl, H. M. Halff, C. L. Redfield, & V. J. Shute (Eds.),
Intelligent Tutoring Systems: 4th International Conference (pp. 254-263). New York:
Springer.
Gertner, A. S., & VanLehn, K. (2000). Andes: A coached problem solving environment
for physics. In C. Frasson (Ed.), Proceedings of ITS 2000 . New York: Springer.
Graesser, A. C., Person, N., & Magliano, J. (1995). Collaborative dialog patterns in
naturalistic one-on-one tutoring. Applied Cognitive Psychology, 9, 359-387.
Hestenes, D., Wells, M., & Swackhamer, G. (1992). Force concept inventory. The Physics
Teacher, 30, 141-158.
Koedinger, K., & Anderson, J. R. (1993). Reifying implicit planning in geometry:
Guidelines for model-based intelligent tutoring system design. In S. P. L. a. S. J. Derry
(Ed.), Computers as cognitive tools . Hillsdale, NJ: Lawrence Erlbaum Associates.
Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1995). Intelligent
tutoring goes to school in the big city. In J. Greer (Ed.), Proceedings of the 7th World
Conference on Artificial Intelligence and Education (pp. 421-428). Charlottesville, NC:
AACE.
Lewis, C. (1981). Skill in algebra. In J. R. Anderson (Ed.), Cognitive skills and their
acquisition (pp. 85-110). Hillsdale, NJ: Lawrence Erlbaum Associates.

Fading and Deepening: The Next Steps for Andes and other Model-Tracing Tutors

483

19. Mark, M. A., & Greer, J. E. (1995). The VCR tutor: Effective instruction for device
operation. The Journal of the Learning Sciences, 4(2), 209-246.
20. McArthur, D., Stasz, C., & Zmuidzinas, M. (1990). Tutoring techniques in algebra.
Cognition and Instruction, 7(3), 197-244.
21. McKendree, J., Radlinski, B., & Atwood, M. E. (1992). The Grace Tutor: A qualified
success. In C. Frasson, G. Gautheir, & G. I. McCalla (Eds.), Intelligent Tutoring Systems:
Second International Conference (pp. 677-684). Berlin: Springer-Verlag.
22. Merrill, D. C., & Reiser, B. J. (1994). Scaffolding effective problem solving strategies in
interactive learning
environments. In A. R. a. K. Eiselt (Ed.), Proceedings of the
Sixteenth Annual Conference of the Cognitive Science Society (pp. 629-634). Hillsdale,
NJ: Lawrence Erlbaum Associates.
23. Reif, F., & Scott, L. A. (1999). Teaching scientific thinking skills: Students and computers
coaching each other. American Journal of Physics, 67(9), 819-831.
24. Reiser, B. J., Beekelaar, R., Tyle, A., & Merrill, D. C. (1991). Gil: Scaffolding learning to
program with reasoning-congruent
representations. In L. Birnbaum (Ed.), The
International Conference of the Learning Sciences: Proceedings of the 1991 Conference
(pp. 382-388). Charlottesville, NC: Association for the Advancement of Computers in
Education.
25. Reiser, B. J., Copen, W. A., Ranney, M., Hamid, A., & Kimberg, D. Y. (in press).
Cognitive and motivational consequences of tutoring and discovery learning. Cognition
and Instruction.
26. Reiser, B. J., Kimberg, D. Y., Lovett, M. C., & Ranney, M. (1992). Knowledge
representation and explanation in GIL, an intelligent tutor for programming. In J. H. a. C.
Larkin, R.W. (Ed.), Computer Assisted Instruction and Intelligent Tutoring Systems:
Shared Goals and Complementary Approaches (pp. 111-150). Hillsdale, NJ: Lawrence
Erlbaum Associates.
27. Rose, C. P., & Lavie, A. (in press). Balancing robustness and efficiency in unificationaugmented context-free parsers for large practical applications. In J. C. Junqua & G. V.
Noord (Eds.), Robustness in Language and Speech Technology : Kluwer Academic press.
28. Shelby, R. N., Schulze, K. G., Treacy, D. J., Wintersgill, M. C., Gertner, A. G., &
Vanlehn, K. (in prep.). The Andes Intelligent Tutor: an Evaluation .
29. Shute, V. J. (1993). A macroadaptive approach to tutoring. Journal of Artificial
Intelligence in Education, 4(1), 61-93.
30. Singley, M. K. (1990). The reification of goal structures in a calculus tutor: Effects on
problem solving performance. Interactive Learning Environments, 1, 102-123.
31. VanLehn, K. (1988). Student modeling. In M. Polson & J. Richardson (Eds.), Foundations
of Intelligent Tutoring Systems (pp. 55-78). Hillsdale, NJ: Lawrence Erlbaum Associates.
32. VanLehn, K. (1996). Conceptual and meta learning during coached problem solving. In
C. Frasson, G. Gauthier, & A. Lesgold (Eds.), ITS96: Proceeding of the Third
International conference on Intelligent Tutoring Systems. . New York: Springer-Verlag.
33. VanLehn, K., Siler, S., Murray, C., & Baggett, W. B. (1998). What makes a tutorial event
effective? In M. A. Gernsbacher & S. J. Derry (Eds.), Proceedings of the Twentieth
Annual Conference of the Cognitive Science Society (pp. 1084-1089). Hillsdale, NJ:
Erlbaum.
34. VanLehn, K., Siler, S., Murray, C., Yamauchi, T., & Baggett, W. B. (in press). Human
tutoring: Why do only some events cause learning? Cognition and Instruction.

ITS Tools for Natural Language Dialogue:
A Domain-Independent Parser and Planner
Reva Freedman, Carolyn Penstein Rosé, Michael A. Ringenberg, and
Kurt VanLehn?
Learning Research and Development Center
University of Pittsburgh
Pittsburgh, PA 15260
{freedrk, rosecp, mringenb, vanlehn}@pitt.edu
http://www.pitt.edu/˜circle

Abstract. The goal of the Atlas project is to increase the opportunities
for students to construct their own knowledge by conversing (in typed
form) with a natural language-based ITS. In this paper we describe two
components of Atlas—APE, the integrated planning and execution system at the heart of Atlas, and CARMEL, the natural language understanding component. These components have been designed as domainindependent rule-based software, with the goal of making them both
extensible and reusable. We illustrate the use of CARMEL and APE
by describing Atlas-Andes, a prototype ITS built with Atlas using the
Andes physics tutor as the host.

1

Motivation

The goal of the Atlas project is to enable the involvement of students in a more
active style of learning by engaging them in a typed dialogue with an ITS. This
dialogue can include both natural language and GUI actions. In this paper we
motivate the use of dialogue in intelligent tutoring. We also describe resources
developed on the Atlas project that are available for use on tutoring projects
interested in including dialogue capabilities in their applications. The two key
domain-independent components described here are APE, the Atlas Planning
Engine, and CARMEL, the natural language understanding component. APE is
a “just-in-time” planner specialized for easy construction and rapid generation
of hierarchically organized dialogues. CARMEL is a general purpose engine for
language understanding composed of robust and efficient algorithms for parsing,
semantic interpretation, and repair. We explain how we used these components
to build a prototype for a new tutor, Atlas-Andes, that adds a dialogue capability
to the existing Andes physics tutor.
?

This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the
University of Pittsburgh and Carnegie-Mellon University.

G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, pp. 433–442, 2000.
c Springer-Verlag Berlin Heidelberg 2000


434

Reva Freedman et al.

Collaborative dialogue between student and tutor is a well-documented prominent component of effective human tutoring [1,2,3]. A recent corpus study of reflective follow-up dialogues [4] demonstrates the potential for natural language
dialogue to enhance the ability of tutoring systems to effectively diagnose student misconceptions. Furthermore, recent research on student self-explanations
supports the view that when students express their thinking in words it enhances their learning [5,6]. Students learn more effectively when they are forced
to construct knowledge for themselves.
Without natural language dialogue, the best remediation tools available to
tutoring systems are hint sequences. Hints are a unidirectional form of natural
language: the student can’t take the initiative or ask a question. In addition, there
is no way for the system to lead the student through a multi-step directed line of
reasoning or to ask the student a question, except via a list of pre-coded answers.
As a result, there is no way to use some of the effective rhetorical methods used
by skilled human tutors, such as analogy and reductio ad absurdum. Thus, the
use of natural language dialogue allows us to extend the tutor’s repertoire to
include the types of remediation subdialogues seen in corpus studies.

An elevator slows to a stop from an
initial downward velocity of 10 ms in
2 sec. A passenger in the elevator is
holding a 3 kg package by a string.
What is the tension in the string?

y
x
a
v

Fig. 1. Sample problem from the Andes physics tutor

2

Natural Language Dialogue in Atlas-Andes

Atlas-Andes is a dialogue-extended version of the Andes [7] tutoring system using
the capabilities offered by the Atlas tool set. Andes is an ITS whose domain is
first-year college physics. It tracks the student as the latter attempts to solve
a problem. If the student gets stuck or deviates too far from a correct solution
path, it provides hints and other assistance. The left-hand side of Fig. 1 shows
a typical Andes problem with its accompanying drawing. On the right is seen
the student’s partially completed free-body diagram, or sketch of the vectors
involved.
In this example, the elevator is decelerating, so the acceleration vector should
face the opposite direction from the velocity vector.1 Andes responds to this
frequent student error by turning the incorrect item red. If requested, it will
1

If the acceleration vector went in the same direction as the velocity vector, the
elevator would speed up and smash into the ground.

ITS Tools for Natural Language Dialogue

435

provide the first hint of a sequence, in this case “Think about the direction of
the acceleration vector.” Follow-up hints include further information about the
direction of acceleration and, eventually, the answer. The prototype Atlas-Andes
system replaces these hints by a choice of generated subdialogues, such as the
one shown in Fig. 2.
S:
T:
S:
T:
S:
T:
S:
T:
S:

<draws acceleration vector in same direction as velocity>
What is the definition of acceleration?
I don’t have any clue.
OK, let’s try this. If a car was driving along east, which way would
you have to push on it to make it stop?
West.
Exactly, the opposite direction. What does that tell you about the
direction of acceleration?
It goes west too.
Right. The net force goes the opposite direction, and so does the
acceleration. Try to draw the acceleration vector again now.
<draws acceleration vector correctly>
Fig. 2. Example of generated dialogue

This example shows some prominent features of both APE and CARMEL.
From the planning point of view, after the student’s initial response, AtlasAndes was able to drop an unpromising line of attack and try a different one.
Later, it was able to give specific responses to statements by the student. From
the language understanding point of view, we can see that Andes-Atlas can
effectively interpret student responses, even idioms like “I don’t have a clue.”

3

Implementation of a Prototype Tutor

In this section we describe how a pre-existing tutoring system can be extended to
offer dialogue capabilities using Atlas. Figure 3 illustrates the architecture of the
resulting extended system. While we focus our discussion on the prototype AtlasAndes system, the same approach could be used to add dialogue capabilities to
a wide range of tutoring systems. In Fig. 3 we see that students interact with the
dialogue-extended system through the User Interface Manager, which interprets
mouse clicks and key presses. GUI actions are then channeled through the GUI
Interpreter which interprets them and stores a representation of the interpreted
input for the Tutorial Planner (APE), described in Section 4. Natural language
input is channeled through the Input Understander (CARMEL), which interprets
the student’s input. Just as the GUI Interpreter does, it stores a representation
of the interpreted natural language input for the Tutorial Planner. The Tutorial
Planner then uses the input representation, as well as data from the host system
(in this case Andes) and other sources, to formulate a response which it sends
back to the student via the User Interface Manager.

436

Reva Freedman et al.

User Interface
Manager

Grammar
GUI Interpreter
(Andes)

Language
Understanding
(CARMEL)

Lexicon
Semantic Rules

Tutorial Agenda
Host Tutor
(Andes)

Tutorial Planner
(APE)

Tutorial History
Plan Library

Tutor's Response
Fig. 3. Architecture of Atlas-Andes

Two domain-specific knowledge sources are required to apply the Atlas tools
(APE and CARMEL) to a new domain, namely a plan library to guide the
Tutorial Planner and semantic mapping rules to guide the Input Understander.
A corpus of transcribed, spoken human-human dialogues using two experienced
tutors and 20 students attempting to solve physics problems informed the development of the prototype Atlas-Andes system. The prototype system contains 21
semantic mapping rules and a plan library of approximately 100 plan operators,
including roughly equal numbers of operators for dialogue creation, responding
to specific student misconceptions, and handling domain-independent dialogue
issues. In addition to API and GUI handling, the latter category includes general tutoring policies such as whether the student should be allowed to take
the initiative and return to the GUI without finishing a subdialogue in process.
With this knowledge the system can generate a large number of variations of
the dialogue in Fig. 2 as well as selected examples of other ways of teaching
about the direction of acceleration, such as the mini-reductio in Fig. 4. Thus
the resulting system has the ability to tailor its approach to a wide variety of
student inputs. Operators are selected based on historical information gathered
by the tutor (discourse/interaction history), information about the current situation (the tutor’s current goal and the student’s latest response), and domain

ITS Tools for Natural Language Dialogue

437

knowledge. As an example of the latter, if a student draws an acceleration vector
which is incorrect but not opposite to the velocity vector, a different response
will be generated.
In the remainder of the paper, we will discuss the APE tutorial planner and
the CARMEL input understander in greater depth.

4

APE: The Atlas Tutorial Planner

Planning is required in dialogue-based ITSs in order to ensure that a coherent
conversation ensues as the tutor’s pedagogical goals are accomplished. If the
system just responds to student actions, the resulting conversation will not necessarily be coherent, and the tutor has no way to ensure that its own teaching
goals are met. Although Wenger [8] wrote in 1987 that using a global planner to
control an ITS would be too inefficient, developments in reactive planning have
made this goal a realistic possibility.
One cannot plan a conversation in advance unless the student’s responses are
classified into a small number of categories, and even then it would be wasteful.
Furthermore, depending on the quality of the student’s answers, one might need
to change the plan during the conversation. For these reasons we work with
partial plans that are expanded and refined only as needed. This style of planning
is often called reactive planning [9,10].
For adding dialogue to ITSs, we have developed a reactive planner called
APE (Atlas Planning Engine) that is specialized for dialogue. In a previous
study [11], we showed how modeling human-human tutorial dialogues according to the hierarchical structure of task-oriented dialogues [12] can make them
tractable for plan-based generation. In the tutoring dialogues we have studied,
a main building block of the discourse hierarchy, corresponding to the transaction level in Conversation Analysis [13], matches the tutoring episode defined
by VanLehn [14]. A tutoring episode consists of the turns necessary to help the
student accomplish one correct problem-solving step, e. g. to make one correct
entry on a graphical interface. Our planner makes it convenient to satisfy local
goals without disturbing the basic hierarchical structure.
Figure 4 shows a sample plan operator from Atlas-Andes. For legibility, we
have shown the key elements in English instead of in Lisp.
To initiate a planning session, the user invokes the planner with an initial
goal. The system searches its operator library to find all operators whose goal
field matches the next goal on the agenda and whose filter conditions and preconditions are satisfied. Goals are represented by first-order logic without quantifiers
and are matched using full unification. Since APE is intended especially for the
generation of hierarchically organized task-oriented discourse, operators have
multi-step recipes. When a match is found, the matching goal is removed from
the agenda and replaced by the steps in the recipe. This operation is repeated
until a primitive (non-decomposable) step is reached. If the primitive step corresponds to a question, the tutor asks the question and ends its turn. If the

438

Reva Freedman et al.

(def-operator handle-same-direction
:goal (...)
:filter (...)
:precond (...)
; We have asked a question about acceleration
; ... and the student has given an answer
; ... from which we can deduce that he/she thinks acceleration and
velocity go in the same direction
; and we have not given the explanation below yet
:recipe (...)
; Tell the student: "But if the acceleration went the same direction
as the velocity, then the elevator would be speeding up."
; Mark that we are giving this explanation
; Tell the student that the tutor is requesting another answer
;
("Try again.")
; Edit the agenda so that tutor is ready to receive another answer
:hiercx ())
Fig. 4. Sample plan operator

primitive step corresponds to a statement, the tutor utters the statement but
continues to plan, allowing the generation of multi-sentence turns.
To tailor the tutor’s responses to the student as much as possible, one needs
the ability to change plans during a conversation. This ability is provided in
APE through the use of three types of recipe steps that can update the agenda.
APE can skip the remainder of a strategy if circumstances have changed; it
can replace a strategy with another strategy that has the same goal; and it can
replace a sequence of goals at the top of the agenda. The last type is especially
useful for responding to a student utterance without disturbing the global plan.
In this way our approach differs from that of Vassileva [15]. Her work, based on
and-or graphs, uses a separate set of rules for reacting to unexpected events.
A second way to tailor the tutor’s response to the student is to take context
into account before choosing a response. APE provides this ability in two ways.
The “hierarchical context” or hiercx slot of an operator, shown in the last line
of Fig. 4, provides a way for the planner to be aware of the goal hierarchy in
which a decomposition is proposed. Additionally, operators can update and test
predicates in a dynamic knowledge base.
APE communicates with the host system via an API. It obtains information
from the world—the GUI interface, the natural language understanding component (CARMEL), and the host tutoring system—through preconditions on its
plan operators. It returns information and action requests through recipe steps
that update its knowledge base and execute external actions. Further details
about the APE planner can be found in [16], and a deeper treatment of the role
of reactive planning in dialogue generation can be found in [17].
Many previous dialogue-based ITSs have been implemented with finite-state
machines, either simple or augmented. In the most common finite-state model,

ITS Tools for Natural Language Dialogue

439

each time the human user issues an utterance, the processor reduces it to one of
a small number of categories. These categories represent the possible transitions
between states. There are several problems with this approach. First, it limits
the richness of the student’s input that can be appreciated. With APE, on the
other hand, the author can write arbitrarily complex predicates, evaluable at run
time, to define a class of input. Second, one can only take history and context
into account by expanding the number of states, putting an arbitrary restriction
on the amount of context or depth of conversational nesting that can be considered. Third, the finite-state approach misses the significant generalization that
tutorial dialogues are hierarchical: larger units contain repeated instances of the
same smaller units in different sequences and instantiated with different values.
Finally, the finite-state machine approach does not allow the author to drop
one line of attack and replace it by another without hard-coding every possible
transition, thus limiting the tutor’s ability to tailor its responses.
The prototype Atlas-Andes system described above shows that APE permits
one not only to build more sophisticated ITSs but to build them faster. Since
the domain-specific tutorial strategies are built from a small vocabulary of lowerlevel operators, there is a considerable economy of scale when expanding such
a prototype to a full-scale tutoring system. Additionally, many of the operators
that express general tutoring policies and conversational strategies are domainindependent and do not need to be repeated when expanding domain coverage.

5

CARMEL: The Atlas Input Understander

The task of the Atlas input understander is to extract relevant information from
student explanations and other natural language input to pass back to the planner. This information can take the form of single atomic values or collections of
flat propositional clauses, depending upon what the planner requires in specific
contexts. In either case, CARMEL, the Core component for Assessing the Meaning of Explanatory Language, is used to parse the student input onto a feature
structure representation that contains both syntactic and semantic information.
Domain specific pattern matchers called semantic mapping rules are then used
to match against particular patterns of features in order to identify and extract
the needed information.
The overarching goal behind the design of the Atlas input understander is to
facilitate the rapid development of robust natural language understanding interfaces for multiple domains. While interest in language understanding interfaces
for tutoring systems has grown in recent years, progress towards making such interfaces commonplace has been greatly hindered by the tremendous time, effort,
and expertise that is normally required for such an endeavor. Our long term goal
is to build a tool set to semi-automate the process by applying machine learning
techniques that require system developers only to annotate corpora with information pertinent to tutoring, thus insulating them from the underlying linguistic
aspects of the development. At the heart of our design is the CARMEL core language understanding component, which is available for use on other tutoring

440

Reva Freedman et al.

projects.2 Its underlying robust understanding technology [18,19,20] has already
proven successful in the context of a large scale multi-lingual speech-to-speech
translation system [21,22].
CARMEL provides a broad foundation for language understanding. It is composed of a broad coverage English syntactic parsing grammar and lexicon; robust
and efficient algorithms for parsing, semantic interpretation, and repair; and a
formalism for entering idiomatic and domain-specific semantic knowledge. Current dialogue-based tutoring systems, such as Circsim-Tutor [23] and AutoTutor
[24], rely on shallow processing strategies to handle student input. This technology has so far proven effective for efficiently processing short student answers
and for evaluating content based on inclusion of relevant vocabulary. In contrast,
the goal of CARMEL is to support a deeper level of analysis in order to identify arbitrarily complex relationships between concepts within longer student
answers.
Our approach is to achieve the most complete deep analysis possible within
practical limits by relaxing constraints only as needed. CARMEL first attempts
to construct analyses that satisfy both syntactic and semantic well-formedness
conditions. A spelling corrector [25] is integrated with the lexical look-up mechanism in order to robustly recognize the student’s intended input in the face of
typos and spelling errors. The robust parser [19] has the ability to efficiently
relax syntactic constraints as needed and as allowed by parameterized flexibility
settings. For sentences remaining beyond the coverage of its syntactic knowledge,
a repair stage [18], relying solely on semantic constraints compiled from a meaning representation specification, is used to assemble the pieces of a fragmentary
parse. Thus, robustness techniques are applied at each stage in processing student input in order to address the wide variety of phenomena that make language
understanding challenging.
In a recent evaluation of CARMEL’s syntactic coverage, we measured the
parser’s ability to robustly analyze student input by testing it on a subset of
our corpus of tutoring dialogues that had not been used for development of the
prototype. The test corpus contained 50 student sentences and 50 multi-sentence
student turns randomly extracted from the full corpus. The utterances ranged
in length from 1 to 20 words, with an average length of 8 words per utterance.
The parser was able to construct analyses covering 87% of the corpus when a
high flexibility setting was used, taking on average .1 seconds per sentence.
When the parser is unable to construct an analysis of a sentence that deviates
too far from the grammar’s coverage, a fragmentary analysis is passed on to the
repair module that quickly assembles the fragments [18]. Our approach to repair
is unique in that no hand-coded repair rules are required as in other approaches
to recovery from parser failure [26,27]. A recent evaluation demonstrates that
CARMEL’s repair stage can increase the number of acceptable interpretations
produced by between 3% (when using a high flexibility setting) and 9% (when
a restricted flexibility setting is used), taking on average only .3 seconds per
sentence.
2

Interested parties should contact Carolyn Rosé at rosecp@pitt.edu.

ITS Tools for Natural Language Dialogue

6

441

Conclusions

One goal of the Atlas project is to develop reusable software for implementing
natural-language based ITSs. In this paper we described CARMEL and APE,
the parser and planner, respectively, for Atlas. We illustrated this work with
an example from Atlas-Andes, a prototype physics tutor built using the Atlas
framework. We showed how using these components could enable not only better
tutoring but reduced authoring time as well.
Acknowledgments
We are grateful to Abigail Gertner for her generous assistance with the Andes
system. Mohammed Elmi and Michael Glass of Illinois Institute of Technology
provided the spelling correction code. Pamela Jordan provided constructive commentary on the manuscript.

References
1. Merrill, D.C., Reiser, B.J., Landes, S.: Human tutoring: Pedagogical strategies and
learning outcomes (1992) Paper presented at the annual meeting of the American
Educational Research Association.
2. Fox, B.A.: The Human Tutorial Dialogue Project: Issues in the design of instructional systems. Hillsdale, NJ: Erlbaum (1993)
3. Graesser, A.C., Person, N.K., Magliano, J.P.: Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive Psychology 9 (1995) 495–522
4. Rosé, C.P.: The role of natural language interaction in electronics troubleshooting.
In: Proceedings of the Energy Week Conference and Exhibition, Houston (1997)
5. Chi, M.T.H., Bassok, M., Lewis, M.W., Reimann, P., Glaser, R.: Self-explanations:
How students study and use examples in learning to solve problems. Cognitive
Science 13 (1989) 145–182
6. Chi, M.T.H., de Leeuw, N., Chiu, M.H., LaVancher, C.: Eliciting self-explanations
improves understanding. Cognitive Science 18 (1994) 439–477
7. Gertner, A., VanLehn, K.: Andes: A coached problem solving environment for
physics. In: Proceedings of the Fifth International Conference on Intelligent Tutoring Systems (ITS ’00), Montreal (2000)
8. Wenger, E.: Artificial Intelligence and Tutoring Systems: Computational and Cognitive Approaches to the Communication of Knowledge. San Mateo, CA: Morgan
Kaufmann (1987)
9. Georgeff, M.P., Ingrand, F.F.: Decision-making in an embedded reasoning system. In: Proceedings of the Eleventh International Joint Conference on Artificial
Intelligence (IJCAI ’89), Detroit (1989) 972–978
10. Wilkins, D., Myers, K., Lowrance, J., Wesley, L.: Planning and reacting in uncertain and dynamic environments. Journal of Experimental and Theoretical Artificial
Intelligence 7 (1995) 121–152
11. Kim, J., Freedman, R., Evens, M.: Responding to unexpected student utterances
in Circsim-Tutor v. 3: Analysis of transcripts. In: Proceedings of the Eleventh
Florida Artificial Intelligence Research Symposium (FLAIRS ’98), Sanibel Island,
Menlo Park: AAAI Press (1998) 153–157

442

Reva Freedman et al.

12. Grosz, B.J., Sidner, C.L.: Attention, intentions, and the structure of discourse.
Computational Linguistics 12 (1986) 175–204
13. Sinclair, J.M., Coulthard, R.M.: Towards an Analysis of Discourse: The English
Used by Teachers and Pupils. London: Oxford University Press (1975)
14. VanLehn, K., Siler, S., Murray, C., Baggett, W.: What makes a tutorial event
effective? In: Proceedings of the Twenty-first Annual Conference of the Cognitive
Science Society, Madison (1998) 1084–1089
15. Vassileva, J.: Reactive instructional planning to support interacting teaching
strategies. In: Proceedings of the Seventh World Conference on AI and Education (AI–ED ’95), Washington, D. C., Charlottesville, VA: AACE (1995)
16. Freedman, R.: Using a reactive planner as the basis for a dialogue agent. In:
Proceedings of the Thirteenth Florida Artificial Intelligence Research Symposium
(FLAIRS ’00), Orlando (2000)
17. Freedman, R.: Plan-based dialogue management in a physics tutor. In: Proceedings
of the Sixth Applied Natural Language Processing Conference (ANLP ’00), Seattle
(2000)
18. Rosé, C.P.: A framework for robust semantic interpretation. In: Proceedings of
the First Annual Conference of the North American Chapter of the Association
for Computational Linguistics (NAACL ’00), Seattle (2000)
19. Rosé, C.P., Lavie, A.: Balancing robustness and efficiency in unification augmented
context-free parsers for large practical applications. In Junqua, J.C., Noord, G.V.,
eds.: Robustness in Language and Speech Technologies. Dordrecht: Kluwer (1999)
20. Rosé, C.P.: Robust Interactive Dialogue Interpretation. PhD thesis, School of
Computer Science, Carnegie Mellon University (1997)
21. Woszcyna, M., Coccaro, N., Eisele, A., Lavie, A., McNair, A., Polzin, T., Rogina, I.,
Rosé, C.P., Sloboda, T., Tomita, M., Tsutsumi, J., Waibel, N., Waibel, A., Ward,
W.: Recent advances in JANUS: a speech translation system. In: Proceedings of
the ARPA Human Languages Technology Workshop, Princeton, NJ (1993)
22. Suhm, B., Levin, L., Coccaro, N., Carbonell, J., Horiguchi, K., Isotani, R., Lavie,
A., Mayfield, L., Rosé, C.P., Dykema, C.V.E., Waibel, A.: Speech-language integration in a multi-lingual speech translation system. In: Proceedings of the
AAAI Workshop on Integration of Natural Language and Speech Processing, Seattle (1994)
23. Glass, M.S.: Broadening Input Understanding in an Intelligent Tutoring System.
PhD thesis, Illinois Institute of Technology (1999)
24. Wiemer-Hastings, P., Graesser, A., Harter, D., the Tutoring Research Group: The
foundations and architecture of AutoTutor. In Goettl, B., Halff, H., Redfield,
C., Shute, V., eds.: Intelligent Tutoring Systems: 4th International Conference
(ITS ’98). Berlin: Springer (1998) 334–343
25. Elmi, M., Evens, M.: Spelling correction using context. In: Proceedings of the 17th
COLING/36th ACL (COLING-ACL ’98), Montreal (1998)
26. Danieli, M., Gerbino, E.: Metrics for evaluating dialogue strategies in a spoken
language system. In: Working Notes of the AAAI Spring Symposium on Empirical
Methods in Discourse Interpretation and Generation, Stanford (1995)
27. Kasper, W., Kiefer, B., Krieger, H., Rupp, C., Worm, K.: Charting the depths
of robust speech parsing. In: Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL ’99), College Park (1999)

“Yes!”: Using Tutor and Sensor Data to Predict Moments
of Delight during Instructional Activities
Kasia Muldner, Winslow Burleson, and Kurt VanLehn
Arizona State University
{Katarzyna.Muldner,Winslow.Burleson,Kurt.VanLehn}@asu.edu

Abstract. A long standing challenge for intelligent tutoring system (ITS)
designers and educators alike is how to encourage students to take pleasure and
interest in learning activities. In this paper, we present findings from a user
study involving students interacting with an ITS, focusing on when students
express excitement, what we dub “yes!” moments. These findings include an
empirically-based user model that relies on both interaction and physiological
sensor features to predict “yes!” events; here we describe this model, its
validation, and initial indicators of its importance for understanding and
fostering student interest.
Keywords: interest, motivation, empirically-based model, sensing devices.

1 Introduction
In some cultures, the classic “yes!” gesture is to clench the fist of one’s dominant arm,
jerk the arm downward and exclaim “yes!” - everyone understands this as an
expression of triumphal victory. When we noticed this behavior among students using
our physics tutoring system, we began to wonder about it. For instance, what causes a
“yes!” during tutoring? Is the “yes!” behavior a desirable outcome in itself or is it also
associated with other desirable outcomes?
Because we are interested in building affective learning companions, we are also
interested in how a companion could use students’ “yes!” behavior for its own ends,
such as increased bonding with the student. This requires, however, that the
companion can detect “yes!” behaviors in real time. This paper reports our progress
on addressing these issues and questions, including:
1. Is the “yes!” behavior a desirable outcome for a tutoring system or associated
with one? We argue from the literature that it is both.
2. What causes “yes!” events and how can we increase their frequency? We
compare “yes!” episodes with ones where a “yes!” could have occurred but did
not. This descriptive analysis sets the stage for future work on what could cause
an increase in “yes!” events.
3. How can “yes!” events by used by tutors, learning companions or other agents?
We present a review of the literature that suggests some possibilities.
P. De Bra, A. Kobsa, and D. Chin (Eds.): UMAP 2010, LNCS 6075, pp. 159–170, 2010.
© Springer-Verlag Berlin Heidelberg 2010

160

K. Muldner, W. Burleson, and K. VanLehn

4. Can a “yes!” event be detected more accurately than a baseline approach? We
developed a regression model based on sensor and tutor log data analysis that
has high accuracy.
The rest of this introduction contains literature reviews that address points 1 and 3,
and a review of related work on affect detection (point 4).
1.1 The Likely Role of “yes!” in Learning and Interest
As we describe in Sect. 3, we view “yes!” as a class of brief expressions of (possibly
highly exuberant) positive affect. Positive affect has been linked to increased personal
interest [1, 2], which is in turn associated with a facilitative effect on cognitive
functioning [3], and improved performance on creative problem solving and other
tasks [4], persevering in the face of failure, investing time when it is needed and
engaging in mindful and creative processing (for a review see [5]). Although there is
work in the psychology community on how interest develops and is maintained (e.g.,
[6, 7]), to date there does not yet exist sufficient work on these topics to understand
the role of positive affect in general and of “yes!” events in particular, so calls for
additional research are common (e.g., [8]).
We should point out, however, that while positive affect could itself be considered
a desirable property during tutoring, it has not always shown strong correlations with
learning [9]. For instance, doing unchallenging problems may make students happy
but may not cause learning. However, the “yes!” expression of positive affect may
well be correlated with learning, because as we show later, “yes!” occurs only after
the student has been challenged, and challenge fosters learning [10].
1.2 How Can “yes!” Events Be Used during Tutoring and Learning?
In general, about 50% of human tutor interventions relate to student affect [11],
highlighting the importance of addressing affect in pedagogical interactions. As far as
addressing “yes!” events, work on the impact of tutorial feedback provides some
direction regarding how “yes!” detection can be valuable to a tutoring system for
generating subsequent responses. For instance, praise needs to be delivered at the
right moment, e.g., be perceived as representative of effort and sincere, to be effective
[12], and so a “yes!” event may be exactly the right time for an agent to give praise.
If “yes!” events do predict increased learning, interest and motivation, then they
can be used as proximal rewards for reinforcement learning of agent policies. For
instance, Min Chi et al. [13] found that a tutorial agent’s policies could be learned
given a distal reward, namely, a students’ learning gains at the end of six hours of
tutoring. It seems likely that even better policies could be learned if the rewards
occurred more frequently. That is, if a “yes!” event occurs, then perhaps the most
recent dialogue moves by the agent could be credited and reinforced.
1.3 Related Work on Detecting Brief Affective States
Affect recognition has been steadily gaining prominence in the user modeling
community, motivated by the key role of affect in various interactions. Like us,

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

161

some researchers have proposed models for identifying a single emotion. For
instance, Kappor et al. [14] rely on a sensor framework , incorporating a mouse,
posture chair, video camera and skin conductance bracelet, to recognize
frustration. McQuiggan and Lester [15] describe a data-driven architecture called
CARE for learning models of empathy from human social interactions. In contrast
to Kappor’s and our work, CARE only uses situational data as predictors for
empathy assessment. Others have focused on identifying a set of emotions. Cooper
et al.’s [16] four linear regression models each predict an emotion (frustration,
interest, excitement, confusion). Like our work, these models are built from a
combination of tutor and sensor log data, although only we explore the utility of
eye tracking and student reasoning data. D’Mello et al. [17] use dialog and posture
features to identify four affective states (boredom, flow, confusion, and
frustration). In Conati’s model, [18] a set of six emotions are assessed (joy/regret,
admiration/reproach, pride/shame) from tutor log data, subsequently refined to
include one sensor modality, namely an EEG [19].
While there is some work on modeling users with eye tracker information, most of
it has focused on how attention shifts predict focus (e.g., [20]), or how pupillary
response predicts cognitive load [21]. This latter work is inspired by findings in
psychology showing that pupillary response is increased by cognitive load [22];
likewise, affect also increases pupillary size [23]. However, results from experiments
less tightly controlled than traditional psychology ones have been mixed, with many
failing to find the anticipated link between pupillary response and state of interest
(e.g., [24]). In the past we investigated how only pupillary response distinguishes
different types of affect [25], and did not propose a model based on our results. In
contrast, here we present a model that relies on a broad range of features across both
interaction and sensor data to predict “yes!” moments. In doing so, we provide insight
into the utility of pupillary information for predicting “yes!” events.
In short, although others have investigated predicting positive affective states,
including joy [26], engagement [17] and excitement [16], our work distinguishes
itself in several ways. First, we identify a novel set of features unique to “yes!”,
including time on task, degree of reasoning and pupillary response. A more
important difference relates to our methodology. A fundamental challenge in
inferring affect from data is finding the appropriate gold standard against which to
compare a model’s predictions. A common approach is to elicit affect information by
explicitly querying users [16, 26]. This approach has the potential to be disruptive,
thus resulting in inaccurate affect labels; it can also miss salient moments of interest
(i.e., when affect is actually occurring). Another common approach relies on using
human coders to identify affect in users [17], a technique that also suffers from
limitations since human coder performance can be variable [17]. In contrast, we rely
on talk-aloud for obtaining affective labels. Doing so has the potential to avoid the
above pitfalls, because it is a form of naturally occurring data that has been shown to
not interfere with the task at hand [27]. Talk-aloud is also used in [28], although
there, only conversational cues are considered as affect predictors, while we use an
array of tutor and sensor features.

162

K. Muldner, W. Burleson, and K. VanLehn

2 Obtaining Data on “yes!” Moments
We obtained data on “yes!” moments from a previous user study we conducted
[25], which involved students interacting with an intelligent tutoring system (ITS)
for introductory Newtonian physics. This ITS, referred to as the Example Analogy
(EA)-Coach [29], provides support to students during problem solving in the
presence of worked-out examples. To solve problems with the EA-Coach, students
use the problem window (Fig. 1, left) to draw free body diagrams and type
equations; students are free to enter steps in any order and/or skip steps. For each
solution entry, the EA-Coach responds with immediate feedback for correctness,
realized by coloring entries red or green, indicating correct vs. incorrect entries.
Instead of providing hints, for instance on instructional material, the EA-Coach
makes examples available to students (accessed with the “GetExample” button);
these are displayed in the example window (Fig. 1, right). The system relies on a
decision-theoretic approach to tailor the choice of example to a student’s needs by
considering problem/example similarity, a student’s knowledge and reasoning
capabilities (see [29] for details).
The study involved 15 participants, all Arizona State University students, who
either were taking or had taken an introductory-level physics course. Each
participant solved two physics problems with the EA-Coach of the type shown in
Fig. 1; each problem solution involved about 15 steps (for further study details, see
[25]). We used a variety of data collection techniques. First, the EA-Coach logged
all interface actions. Second, we used talk aloud protocol [27]: we asked students
to verbalize their thoughts; all sessions were taped and subsequently transcribed.
Third, a sensor logger captured students’ physiological responses from four
sensing devices (see Fig. 2): (1) a posture chair pad measured position shifts (the
pad included three pressure points on the chair seat and three on the back); (2) a
skin-conductance (SC) bracelet captured skin conductance; (3) a pressure mouse
measured the pressure exerted on the mouse (via six “pressure points”); (4) an eye
tracker captured pupillary responses (the tracker was an integrated model that
appeared as a regular computer screen).

Fig. 1. EA-Coach problem and example windows

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

163

Fig. 2. Affective sensors (from left to right): posture chair, skin conductance (SC) bracelet,
pressure mouse, Tobii eye tracker

3 Data Pre-processing
As our “gold standard” for “yes!” moments during the study, we relied on the verbal
protocol data. Since a “yes moment” corresponds to excitement and/or positive affect,
the transcripts were coded by the first author to identify such instances. As a starting
point, we used data from an earlier affect coding [25], reanalyzing the codes to
identify “yes!”. We identified 68 “yes!” moments; all but one were directly associated
with subjects generating a correct solution step and were expressed directly after
doing so (recall that the EA-Coach provided immediate feedback for correctness so
students were aware of the status of their entries). The one “yes!” that was not
associated with a solution step occurred when a participant was reading the second
problem statement after having already successfully solved the first problem.
While some of the “yes!” events were expressed in a very effusive manner (“yes!
I’m smart!”, “oh yay!”), others were more subdued (“I got it right and that makes me
feel good”). In general, we found that when participants expressed a “yes!”, it varied
in terms of tone, expression, etc. Because we found it very difficult to disambiguate
between the various forms of positive affect related to a “yes!”, we decided to keep all
instances in the analysis without trying to further distinguish between them.
We also had data on how subjects were reasoning during the study, obtained from
an earlier coding [25] that included information on various types of reasoning, e.g.,
whether students were self-explaining by deriving physics principles, drawing
comparisons between the problem/example, and/or expressing some form of cognitive
processing (for examples, see [25]). For the purposes of this study, we collapsed the
various types of reasoning into a single “reasoning” code, because as a starting point
we were interested in how reasoning was related to “yes!” events.
Data Features. To analyze what events predict “yes!” moments, we identified a set of
features we believed could be relevant. Note that the list presented here is not meant
to be exhaustive, but rather to provide a starting point for understanding predictors of
excitement/positive affect in instructional situations. First, we identified interaction
data features we obtained from the EA-Coach logger corresponding to events in the
tutor’s interface, as follows:
- Time: The amount of time taken to generate a correct solution step (as described in
Sect. 4, we focus on correct solution entries);
- NumAttempts: The number of attempts required to generate a correct solution step;

164

K. Muldner, W. Burleson, and K. VanLehn

- NumReasoning: The number of “reasoning” utterances a student expressed in the
process of generating a solution step;
- Type of step: The type of solution step (e.g., a force, an axis, an equation).
Second, we identified sensor features that we obtained from the sensor logger:
- Pupillary response: The mean change in pupil dilation around a point of interest
(described in Sect. 4). For instance, if the point of interest is when a student
generates a solution step, then mean change = (mean pupil size over time span T
directly following the step) - (mean pupil size over time span T directly preceding
the step). We set the threshold T=2 seconds, since this comparable to that used in
other related work involving analysis of pupillary response (e.g., [30]).
- Skin Conductance (SC) response: The mean change in SC response around a point
of interest (calculated as for pupillary response). We set the threshold T=2 seconds,
based on the timeframe containing a SC response [14].
- Mouse response: The mean change in mouse pressure before and after an event of
interest, using the method in [16] (where the mean pressure was obtained by
summing over the pressure points, dividing by a constant and finding the mean).
We set the threshold T=10 seconds, because this sensor does not measure
instantaneous responses (like SC and pupillary response) but rather longer scale
transitions in behavior.
- Chair: The number of “sitForward” events, when a student leaned forward prior to
generating a solution step, calculated by obtaining the pressure on the seat back via
the formula in [16]. Here, we used a threshold T=10 seconds, as for the mouse.

4 Results
In order to understand predictors of “yes!” in instructional activities, we compared
“yes!” moments to other instances when students obtained a correct solution step but
did not generate a “yes!”. Since the “yes!” moments directly followed the generation
of a correct solution step, we felt this would be the most appropriate comparison; this
gave us 67 “yes!” instances1 and 218 other events. As a final pre-processing step, for
each logged correct step we extracted the above-described features, merging across
the different log files (transcript, EA-Coach, sensor) to produce a single file.
Our hypotheses were that students would only express a “yes!” if they invested
some effort into generating the solution step, and that there would be physiological
manifestations of “yes!” that differed from other correct entries. To analyze whether
these hypotheses were correct we carried out several types of analysis.
4.1 The Unique Nature of “yes!”
As a starting point, we wanted to determine if “yes!” moments differed from other
correct entries (referred to as other below) in terms of the features listed above. Thus,
we compared data on these two types of entries for our set of features through
1

There was one exception where a student expressed “yes!” when reading an example; given
our scheme, we did not consider this one data point in our analysis.

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

165

univariate ANOVA. As far as the interaction features are concerned, we found that
students took significantly longer to generate a correct solution step corresponding to
a “yes!” than other correct entries (on average, 206 sec. vs. 54 sec.; F(1,297) = 77.27;
p < 0.001). Students also generated significantly more attempts for “yes!” entries, as
compared to other correct entries (on average, 5.1 vs. 1.7; F(1,297) = 40.47, p <
0.001), and expressed significantly more reasoning episodes for “yes!” (on average,
1.34 vs. 0.58 F(1,283) = 11.614, p = 0.001). Our data was too sparse to analyze
whether type of step had an effect.
As far as the sensor features are concerned, students had a significantly larger
pupillary response for a correct solution step associated with a “yes!”, as compared to
other correct entries (on average, .043mm vs. -.037mm; F(1,271)=8.422, p=0.004).
Skin conductance response had a marginal effect on “yes!” as compared to other
entries (.000388µS vs. -.0000422µS, F(1,291)=3.257, p=0.07), suggesting a higher
level of arousal for “yes!”. Likewise, students had significantly fewer sitForward
events before a “yes!”, as compared to other entries (6.4 vs. 10.8; F(1,296)=4.63,
p=0.032). One possibility for why this was the case is that students were more
focused for “yes” entries and so were fidgeting less. We did not find “yes!” to have a
significant effect on mouse response.
4.2 An Empirically-Based Model for Predicting “yes!”
The above analysis showed that “yes!” moments are uniquely distinguishable. To
develop a user model, however, we need to understand how the various features
predict “yes!” events. Thus, we conducted regression analysis. Because we have a
nominal dependent variable (“yes!” vs. other), we used a logistic regression. A key
consideration behind our choice of modeling technique was our data set size: while
acceptable for modeling with logistic regression, where the rule of thumb is at least 20
data points per independent variable, it was not large enough for some other machine
learning techniques, e.g., support vector machine. Of the applicable techniques,
regression was chosen based on prior research showing its suitability for classifying
affect ([16, 31]); [31] found that regression yielded the highest affect classification
accuracy over other machine learning methods.
We begin by presenting the baseline model, one that always predicts the most
likely event (here, lack of a “yes!”). Given the base rates of the two decision options
and no other information, the best strategy is to predict that each step is not a “yes!”.
This model achieves 76% accuracy (# of correctly classified events / total # of
events), but obviously completely misses our goal of predicting when “yes!” occurs
(i.e., never predicts “yes!”, and so has a true positive value of 0%, see Table 1, top).
Using the Step method, we then added our features to the logistic regression
model2. The resulting model containing time, numReasoning, pupil response, SC
response and chair was significantly better from the baseline model (p<0.001, see
Table 1, top). Below, we will analyze the contribution of some of our features to the
model’s accuracy, but first we examine the full model accuracy.
2

Because increasing the number of predictors decreases experimental power, we omitted
numTries from this analysis, as it was redundant due to its high correlation with time; we also
omitted mouse response since it did not significantly distinguish “yes!” from other entries.

166

K. Muldner, W. Burleson, and K. VanLehn

Table 1. Logistic regression “yes!” models (TP=Sensitivity, TN=Specificity; Acc= TP + TN / N)
Overall Logistic Regression Equation
-2.206
-2.206+time*.008+ numReasoning*.309 +
pupilResponse*1.68 +SC*126.4+chair*-.019
Time*+numReasoning* -2.437 + time*.01 + numReasoning*.345
Time*+pupilResponse* -2.157+time*.009+pupilResponse*2.082
Time*+SC
-2.308 + time*0.01+ SC*128.97
Time*+Chair
-2.084 +time*0.01 + chair*-.017
Baseline model
Full model **

TP
0
60.3

TN
100
87.2

Acc.
76
81.4

55.2
54.2
57.6
56.7

89.2
85.0
89.9
88.3

81.6
78.4
82.6
81.2

** Significantly better than baseline model, p<0.05
* Each feature significantly improves model fit over previous model (i.e., model 1=baseline,
model 2=time, model 3= time+2nd feature), p<0.05

The output of a logistic regression equation is a probability that a given event
belongs to a particular class. In order to use the model for prediction, it is therefore
necessary to have a decision rule: if the probability of an event is greater or equal to
some threshold then we will predict that event will take place (and not take place
otherwise). To choose the optimal threshold, we built a Receiver Operating
Characteristic (ROC) curve (Fig. 3). The ROC curve is a standard technique used in
machine learning to evaluate the extent to which a classifier can successfully
distinguish between data points (episodes correctly classified as positive, or true
positives) and noise (episodes incorrectly classified as positive, or false positives),
given a choice of different thresholds. Figure 3 shows the ROC curve we obtained for
our “yes!” models, where each point on the curve represents a model with a different
threshold value. As is standard practice, we chose as our final threshold the point on
the curve that corresponds to a reasonable tradeoff between too many false positives
vs. too few true positives (P=0.26, labeled by a cross on the curve in Fig. 3).
When reporting classifier accuracy, it is standard to provide sensitivity (true
positives) and specificity (true negatives), since these are more informative then
overall accuracy (true positives + true negatives / total number of instances). Our
classifier is significantly better than the baseline model (p < 0.05) and obtained a
sensitivity of 60.3%, a specificity of 87.2% (and overall accuracy of 81.4% - see
Table 1, top). Thus, this classifier correctly identifies 60% of “yes” moments, without
incorrectly classifying other entries as “yes!” for 87% of the time.
Model Validation. To validate the above model, we conducted a leave-one-out cross
validation. Specifically, we trained the classifier using N-1 data points and tested on
the remaining data point, repeating this process N times (where N is equal to the
number of samples, 269 full samples, i.e., without any missing data points that were
the result of, for instance, the eye tracker failing to find a valid pupil reading). The
validation showed that our model accuracy does not degrade substantially (i.e.,
sensitivity=55.2%, specificity = 87.1%, accuracy = 79.5%).
Parsimonious Models. We wanted to explore what kind of model fit we could obtain
with a subset of our features, which helps to make an informed decision as to which
sensors to use if not all are available. Thus, we ran a series of regressions using time
as the tutor variable (as this variable was highly significant in our regression model)

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

167

Fig. 3. ROC curve for various decision rule thresholds

and one of the other features. As Table 1 illustrates, we obtained reasonable results in
terms of sensitivity and specificity with these reduced models, although only
reasoning and pupil response resulted in significantly better models over a model that
only included time (SC response and chair both improved the model fit, but this did
not reach significance, i.e., p=.151 for the SC response and p=.153 for the chair).

5 Discussion and Future Work
In this paper, we reported on our analysis of moments of excitement and positive
affect during instructional activities, which we refer to as “yes!” events. We found
that “yes!” always followed a correct solution step, but conversely, a correct step was
not always followed by a “yes!”. In particular, students were significantly more likely
to express a “yes!” after investing more time, generating more attempts, and
expressing more reasoning episodes, as compared to correct entries for which
corresponding enthusiasm was not expressed. Note that in addition to verbal
expression of “yes!”, another indication of arousal related to these events was
provided by the pupil dilation and skin conductance data. These findings imply that
students experience excitement and/or positive affect in tutoring situations when they
have invested effort into the process and that effort pays off (i.e., correct solution is
obtained). It is possible, however, that students express “yes!” not because they
invested thoughtful, deliberate processing but because they guessed and/or arrived at
the solution by luck. Our analysis does provide some indication that this is not the
case, as students engaged in significantly more reasoning (captured by the
“reasoning” code that included self-explanation, a form of deep processing) prior to
“yes!”. This does not guarantee every student behavior related a “yes!” is an instances
of “deep” reasoning – in the future, we plan to delve deeper into this issue of mindful
processing and “yes!”.
To the best our knowledge, ours is the first work to propose a model for affect
recognition incorporating pupillary response data. Although in contrast to the other
low-cost sensors we used, eye tracking technology is more expensive, it is becoming
more and more accessible, and so investigating its utility for user modeling is
important. In a prior study [25], we also found a significant difference in pupil size
between affective responses, but there are four key differences between that study and
the present. First, in [25] we analyzed how pupillary response differs between positive
and negative affect, without developing a model based on this data. Second, here we

168

K. Muldner, W. Burleson, and K. VanLehn

focus on “yes!” while in [25], we focused on differences between four affective states.
Third, in [25] we normalized the pupil data using Z scores – while this approach is
sometimes used (e.g., [19]) and increases experimental power, it requires subtracting
the overall signal mean from each data point. Since this mean can only be obtained
after a user finishes interacting with a system, the findings are difficult to apply for
real-time user models. In contrast, here we use the raw signal values, making our
findings more applicable to real-time modeling. Fourth, our feature set includes an
array of sensors and tutor features, while in [25], we analyzed only pupillary data.
Overall, the tutor and sensor features resulted in a model that predicted “yes!” with
60% sensitivity and 87% specificity, a significant improvement over the baseline
model. We also analyzed how using subsets of features impacts model fit: although
the model incorporating the full set of features allowed the best trade-off between
sensitivity and specificity, using a subset of features also resulted in models with
reasonable fit. For instance, a model that includes only information on time and
reasoning performs quite well – this may be useful if a system already has the tools to
capture reasoning style (e.g., as in [32]) but sensors are not available. As far as the
sensor features are concerned, when we explored parsimonious models, each sensor
improved model fit over the time-only model. However, this improvement was only
reliable, as reported by the p value, for the pupillary response feature. Compared to
the pupil-based model, the models incorporating the other sensors resulted in higher
specificity and/or specificity. These results, however, have to be interpreted with
caution, since they approached but did not reach significance. This may be due to our
modest sample size, and so more data is needed to confirm these sensors’ utility.
While there is room for improvement, our model is a first step in providing
information on “yes!” moments, which in turn can be used for tailoring pedagogical
scaffolding to foster interest. For this purpose, it is key that the classifier not
misclassify too many other entries as “yes!” (i.e., has high specificity), while still
identifying some “yes!” moments, as is the case for our classifier. Given our limited
sample size and particular instructional context, however, more work is needed to
validate and generalize our findings.
Returning to our original four questions, we summarize the progress made so far
and directions for future work.
1. Are “yes!” events desirable outcomes or associated with desirable outcomes?
We argue that it is both. We now know that “yes!” occurs after students appear to
have overcome a challenge related to generating a solution step, as indicated by
time spent and number of tries produced. Since challenge fosters interest, this
suggests that “yes!” events may be suitable as a predictor of increased learning,
interest and motivation, something we plan to explore in future studies.
2. What causes “yes!” events and how can we increase their frequency? We now
know that “yes!” events occur after a challenge is overcome with an example as
the only the aid from the tutor. This is consistent with Lepper’s advice of keeping
the student optimally challenged [10].
3. How can “yes!” events be useful to tutors, learning companions and other
agents? We offer some suggestions based on theory, but this remains to be
empirically explored.
4. Can “yes!” events be detected more accurately than a baseline approach? Yes!

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

169

Acknowledgements. The authors thank the anonymous reviewers for their helpful
suggestions and David Cooper, who contributed the sensor logging software. This
research was funded the National Science Foundation, including the following grants:
(1) IIS/HCC Affective Learning Companions: Modeling and supporting emotion
during learning (#0705883); (2) Deeper Modeling via Affective Meta-tutoring (DRL0910221) and (3) Pittsburgh Science of Learning Center (SBE-0836012).

References
1. Reeve, J.: The Interest-Enjoyment Distinction in Interest Motivation. Motivation and
Emotion 13, 83–103 (1989)
2. Isen, A., Reeve, J.: The Influence of Positive Affect on Intrinsic and Extrinsic Motivation:
Facilitating Enjoyment of Play, Responsible Work Behavior, and Self-Control. Motivation
and Emotion 29(4), 297–325 (2005)
3. Hidi, S.: Interest and its Contribution as a Mental Resource for Learning. Rev. of Ed.
Research 60(4), 549–571 (1990)
4. Isen, A., Daubman, K., Nowicki, G.: Positive Affect Facilitates Creative Problem Solving.
J. of Personality and Social Psychology 52, 1122–1131 (1987)
5. Lepper, M.: Motivational Considerations in the Study of Instruction. Cognition and
Instruction 5(4), 289–309 (1988)
6. Hidi, S., Renninger, A.: The Four-Phase Model of Interest Development. Educational
Psychologist 41(2), 559–575 (2006)
7. Deci, E., Koestner, R., Ryan, R.: Extrinsic Rewards and Intrinsic Motivation in Education:
Reconsidered Again. Rev. of Ed. Research 71, 1–27 (2001)
8. Baker, R., D’Mello, S., Rodrigo, M., Graesser, A.: Better to Be Frustrated Than Bored:
The Incidence, Persistence, and Impact of Learners’ Cognitive-Affective States During
Interactions with Three Different Computer-Based Learning Environments. Int. J. of
Human-Computer Studies (in press)
9. Boyer, K., Phillips, R., Wallis, M., Vouk, M., Lester, J.: Balancing Cognitive and
Motivational Scaffolding in Tutorial Dialogue. In: Woolf, B.P., Aïmeur, E., Nkambou, R.,
Lajoie, S. (eds.) ITS 2008. LNCS, vol. 5091, pp. 239–249. Springer, Heidelberg (2008)
10. Lepper, M., Malone, T.: Intrinsic Motivation and Instructional Effectiveness in ComputerBased Education. In: Snow, R., Farr, M. (eds.) Aptitude, Learning and Instruction, vol. 3,
pp. 255–296. Erlbaum, Hillsdale (1987)
11. Lepper, M., Woolverton, M., Mumme, D., Gurtner, J.: Motivational Techniques of Expert
Human Tutors: Lessons for the Design of Computer-Based Tutors. In: Lajoie, S., Derry, S.
(eds.) Computers as Cognitive Tools, pp. 75–105. Lawrence Erlbaum Associates,
Hillisdale (1993)
12. Henderlong, J., Lepper, M.: The Effects of Praise on Children’s Intrinsic Motivation: A
Synthesis and Review. Psychological Bulletin 128(5), 774–795 (2002)
13. Chi, M., VanLehn, K., Litman, D., Jordan, P.: Inducing Effective Pedagogical Strategies
Using Learning Context Features. In: Proc. of the 18th Int. Conference on User Modeling,
Adaptation and Personalization (in press)
14. Kapoor, A., Burleson, W., Picard, R.: Automatic Prediction of Frustration. Int. J. of
Human-Computer Studies 65(8), 724–736 (2007)
15. McQuiggan, S., Lester, J.: Diagnosing Self-Efficacy in Intelligent Tutoring Systems: An
Empirical Study. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 565–574. Springer, Heidelberg (2006)

170

K. Muldner, W. Burleson, and K. VanLehn

16. Cooper, D., Arroyo, I., Woolf, B., Muldner, K., Burleson, W., Christopherson, R.: Sensors
Model Student Self Concept in the Classroom. In: Houben, G.-J., McCalla, G., Pianesi, F.,
Zancanaro, M. (eds.) UMAP 2009. LNCS, vol. 5535, pp. 30–41. Springer, Heidelberg (2009)
17. D’Mello, S., Graesser, A.: Mind and Body: Dialogue and Posture for Affect Detection in
Learning Environments. In: Luckin, R., Koedinger, K., Greer, J. (eds.) Proc. of the 13th
Int. Conference on Artificial Intelligence in Education, pp. 161–168. IOS Press,
Amsterdam (2007)
18. Conati, C., Zhou, X.: Modeling Students’ Emotions from Cognitive Appraisal in
Educational Games. In: Cerri, S.A., Gouardéres, G., Paraguaçu, F. (eds.) ITS 2002. LNCS,
vol. 2363, pp. 944–954. Springer, Heidelberg (2002)
19. Conati, C., Maclaren, H.: Modeling User Affect from Causes and Effects. In: Houben, G.-J.,
McCalla, G., Pianesi, F., Zancanaro, M. (eds.) UMAP 2009. LNCS, vol. 5535, pp. 4–15.
Springer, Heidelberg (2009)
20. Conati, C., Merten, C.: Eye-Tracking for User Modeling in Exploratory Learning
Environments: An Empirical Evaluation. Know. Based Systems 20(6), 557–574 (2007)
21. Iqbal, S., Zheng, X., Bailey, B.: Task-Evoked Pupillary Response to Mental Workload in
Human-Computer Interaction. In: Dykstra, E., Tscheligi, M. (eds.) Proc. of the ACM
Conference on Human Factors in Computing Systems, pp. 1477–1480. ACM, NY (2004)
22. Marshall, S.: Identifying Cognitive State from Eye Metrics. Aviation, Space and
Environmental Medicine 78, 165–175 (2007)
23. Vo, M., Jacobs, A., Kuchinke, L., Hofmann, M., Conrad, M., Schacht, A., Hutzler, F.: The
Coupling of Emotion and Cognition in the Eye: Introducing the Pupil Old/New Effect.
Psychophysiology 45(1), 130–140 (2008)
24. Schultheis, H., Jameson, A.: Assessing Cognitive Load in Adaptive Hypermedia Systems:
Physiological and Behavioral Methods. In: De Bra, P.M.E., Nejdl, W. (eds.) AH 2004.
LNCS, vol. 3137, pp. 225–234. Springer, Heidelberg (2004)
25. Muldner, K., Christopherson, R., Atkinson, R., Burleson, W.: Investigating the Utility of
Eye-Tracking Information on Affect and Reasoning for User Modeling. In: Houben, G.-J.,
McCalla, G., Pianesi, F., Zancanaro, M. (eds.) UMAP 2009. LNCS, vol. 5535, pp. 138–149.
Springer, Heidelberg (2009)
26. Conati, C., Maclaren, H.: Empirically Building and Evaluating a Probabilistic Model of
User Affect. User Modeling and User-Adapted Interaction (in press)
27. Ericsson, K., Simon, H.: Verbal Reports as Data. Psych. Rev. 87(3), 215–250 (1980)
28. D’Mello, S., Craig, S., Sullins, J., Graesser, A.: Predicting Affective States Expressed
through an Emote-Aloud Procedure from Autotutor’s Mixed-Initiative Dialogue. Int. J. of
Artificial Intelligence in Education 16(1), 3–28 (2006)
29. Muldner, K., Conati, C.: Evaluating a Decision-Theoretic Approach to Tailored Example
Selection. In: Veloso, M. (ed.) Proc. of 20th Int. Joint Conference on Artificial
Intelligence, pp. 483–489. AAAI Press, Menlo Park (2007)
30. Van Gerven, P., Paas, F., Van Merrienboer, J., Schmidt, H.: Memory Load and the
Cognitive Pupillary Response in Aging. Psychophysiology 41(2), 167–174 (2001)
31. D’Mello, S., Picard, R., Graesser, A.: Towards an Affect Sensitive Auto Tutor. IEEE
Intelligent Systems 22(4), 53–61 (2007)
32. Conati, C., VanLehn, K.: Toward Computer-Based Support of Meta-Cognitive Skills: A
Computational Framework to Coach Self-Explanation. Int. J. of Artificial Intelligence in
Education 11, 389–415 (2000)

Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
Philadelphia, July 2002, pp. 74-83. Association for Computational Linguistics.

Discourse Processing for Explanatory Essays in Tutorial Applications
Pamela W. Jordan and Kurt VanLehn
Learning Research and Development Center
University of Pittsburgh
Pittsburgh PA 15260
[pjordan,vanlehn]@pitt.edu

Abstract
The Why-Atlas tutoring system presents
students with qualitative physics questions
and encourages them to explain their answers via natural language. Although
there are inexpensive techniques for analyzing explanations, we claim that better
understanding is necessary for use within
tutoring systems. In this paper we describe how Why-Atlas creates and utilizes
a proof-based representation of student essays. We describe how it creates the proof
given the output of sentence-level understanding, how it uses the proofs to give
students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional benefits
from a proof-based approach for tutoring
applications.

(Aleven and Koedinger, 2000; Chi et al., 2001). To
give substantive feedback the system has to be able
to understand student explanations to some degree.
The Why-Atlas system presents students with
qualitative physics problems and encourage them to
write their answers along with detailed explanations
for their answers. While physics misconceptions
have proven to be particularly resistant to repair,
practice with qualitative physics questions helps in
overcoming some of these misconceptions (Hake,
1998).
The student explanation shown in (1), which
is from our corpus of human-human computermediated tutoring sessions, illustrates how challenging these explanations are for a system to understand. The problems we have examined require a
short essay with an average of 6.9 sentences to fully
explain to the satisfaction of experienced physics instructors.
(1) Question: Suppose you are running in a straight line
at constant speed. You throw a pumpkin straight up.
Where will it land? Explain.

1 Introduction
Whereas most explanations are produced and
adapted to benefit or inform a hearer, a selfexplanation is produced for the benefit of the
speaker. If there is a hearer he often already knows
all about the topic as in a tutoring context. Selfexplanation is a cognitively valuable pedagogical activity because it leads students to construct knowledge (Chi et al., 1994), and it can expose deep misconceptions (Slotta et al., 1995). But it is difficult to encourage self-explanation without giving the
student substantive feedback on what they generate

Explanation: Once the pumpkin leaves my hand,
the horizontal force that I am exerting on it no longer
exists, only a vertical force (caused by my throwing
it). As it reaches it’s maximum height, gravity (exerted
vertically downward) will cause the pumpkin to fall.
Since no horizontal force acted on the pumpkin from
the time it left my hand, it will fall at the same place
where it left my hands.

Statistical text classification approaches, such as
latent semantic analysis (Landauer et al., 1998),
have shown promise for classifying a student explanation into medium-grained good and bad categories
(Graesser et al., 2000). For instance, a medium-

grained category that should match (1) is the oftenobserved impetus misconception:
If there is no force on a moving object, it
slows down.
Such medium-grained categories typically have
multiple propositions and contain multiple content
words. While successful with medium-grained
classes, statistical approaches are not yet able to distinguish subtle but important differences between
good and bad explanations. Statistical classification
is insensitive to negations1 , anaphoric references2 ,
and argument ordering variations3 and its inferencing is weak4 . To capture these subtle differences
and to allow us to respond more directly to what the
student actually said5 , we need the precision possible so far only with symbolic approaches. So WhyAtlas parses each sentence into a propositional representation.
The PACT Geometry Tutor is an operational prototype that does a finer-grained symbolic classification (Aleven et al., 2001). PACT also parses a
student explanation into a propositional representation but then uses LOOM to classify these into finegrained categories that typically express one proposition. This approach looks promising (Aleven et
al., 2001), but the system’s goal is to elicit a justification for a step in a geometry proof and generally
these can be expressed with a single sentence that
succinctly translates into a small number of propositions. It isn’t clear that this approach will work well
for the longer, more complex explanations that the
Why-Atlas system elicits.
Instead of classifying propositions, the WhyAtlas system constructs abductive proofs of them.
1

A good explanation followed by “But I don’t think that will
happen.” would be classified as good.
2
In (1) above, it would tend to misclassify the last clause as
the correct answer “the pumpkin will land in my hands” because
it does not understand the temporal anaphora.
3
The difference between x accelerates faster than y and y
accelerates faster than x would not be detected.
4
In (1), the student has the extreme belief that the pumpkin
has no horizontal velocity. This would probably not be recognized as a case of “slowing down” by statistical classification.
5
When a true statement lacks precision, the tutor should acknowledge the correct statement and elicit more precision rather
than continuing as if it were wrong. For example, if a student
makes a correct statement about the velocity of an object but did
not report it in terms of the horizontal and vertical components
of the velocity, the tutor should ask which was intended.

A proof-based approach gives more insight into
the line of reasoning the student may be following across multiple sentences because proofs of the
propositions share subproofs. Indeed, one proposition’s entire proof may be a subproof of the next
proposition. Moreover, subtle misconceptions such
as impetus are revealed when they must be used to
prove a proposition.
Abductive inference has a long history in plan
recognition, text understanding and discourse processing (Appelt and Pollack, 1992; Charniak, 1986;
Hobbs et al., 1993; McRoy and Hirst, 1995; Lascarides and Asher, 1991; Rayner and Alshawi,
1992). We are using an extended version of SRI’s
Tacitus-lite weighted abductive inference engine
(Hobbs et al., 1993) as our main tool for building
abductive proofs. We had to extend it in order to use
it for domain as well as language reasoning. As advised in (Appelt and Pollack, 1992), abductive inference requires some application specific engineering
to become a practical technique.
In this paper we describe how the system creates
and utilizes a proof-based representation of student
essays. We describe how it creates the proof given
the output of sentence-level understanding, how it
uses the proofs to give students feedback, some preliminary run-time measures, and the work we are
currently doing to derive additional benefits from a
proof-based approach for tutoring applications.
First we give an overview of the Why-Atlas tutoring system architecture. Next we give some background on weighted abduction and Tacitus-lite+ and
describe how it builds an abductive proof. Next we
describe how the system uses the proofs to give students feedback on their essays. Finally, we discuss
efficiency issues and our future evaluation plans.

2 Overview of the Why-Atlas Tutoring
System
The architecture for the Why-Atlas qualitative
physics tutoring system is shown in Figure 1. The
user interface for the system is a screen area in which
the physics question is displayed along with an essay
entry window and a dialogue window. As the student enters an answer and explanation for a qualitative physics question the sentence-level understanding module builds sets of propositions and passes

Interface
student
strings

tutor
strings

Sentence−level
Understanding

Sentence−level
Realization

(Carmel/Rainbow)

(RealPro)

propositions or
response classes
History

directive goal
and propositions

tutor string

KCD
goal or
response
class

tutor string

Discourse Manager
propositions

proofs

Discourse−level
Language and
Domain axioms

proofs

Understanding
search
queue
and
axioms

Dialogue
Engine
(APE)

ordered search
queue

Inference Engine
(Tacitus−lite+)

ordered
goals
Tutorial
Strategist

Figure 1: Why-Atlas Tutoring System Architecture
them, via the discourse manager, to the discourselevel understanding module. Each set of propositions represents one interpretation of a sentence. The
user interface and the sentence-level understanding
components are described in detail in (Rosé, 2000;
Freedman et al., 2000).
The discourse-level understanding module uses
language and domain reasoning axioms and the
Tacitus-lite+ abductive inference engine to create a
set of proofs that offer an explanation for the student’s essay and give some insight into what the
student may believe about physics and how to apply that knowledge. The discourse-level understanding module updates the propositions and the
search queue for proofs in the history with the results
from Tacitus-lite+. This part of the history supports
anaphora resolution and processing of revisions a
student may make to his essay. The discourse manager module selects and sends the best proofs to the
tutorial strategist.
The tutorial strategist identifies relevant communicative goals. Currently there are four categories of
communicative goals. Two of these, disambiguating
terminology and clarifying the essay, are addressed

via directives to modify the essay. The other two,
remediating misconceptions and eliciting more complete explanations, are addressed via dialogue. Misconceptions are detected when the proof includes
an axiom that is incorrect or inapplicable. Incompleteness is detected under two conditions. First,
there may be multiple proofs that are equally good.
This condition indicates that the student did not say
enough in his explanation for the system to decide
which proof best represents what the student’s reasoning may be. Each possible line of reasoning
could point to different underlying problems with
the student’s physics knowledge. The second condition occurs when the student fails to explicitly
state a mandatory point, which is a proposition that
domain instructors require of any acceptably complete essay. Once the tutorial strategist has identified communicative goals it prioritizes them according to curriculum constraints and sends them to the
discourse manager, which selects the highest priority goal after taking dialogue coherency into account
and sends the goal to either the dialogue engine or
the sentence-level realization module.
The dialogue engine initiates and carries out a dia-

logue plan that will either help the student recognize
and repair a misconception or elicit a more complete explanation from the student. The main mechanism for addressing these goals are what we call a
knowledge construction dialogue (KCD) specification. A KCD specification is a hand-authored pushdown network. Nodes in the network are either the
system’s questions to students or pushes and pops
to other networks. The links exiting a node correspond to anticipated responses to the question. Each
question is a canned string, ready for presentation
to a student. The last state of the network is saved
in the history and the sentence-level understanding
module accesses this in order to get information for
analysis of student responses. The sentence-level
understanding module uses a classification approach
for dialogue responses from the student since currently the dialogue plans are limited to ones that expect short, direct responses. During a dialogue, response class information is delivered directly to the
dialogue engine via the discourse manager. The dialogue engine is described further in (Rosé et al.,
2001).
The other communicative goals, disambiguating
terminology and clarifying the essay, are addressed
by the discourse manager as directives for the student to modify the essay. It passes propositions and a
goal to the sentence-level realization module which
uses templates to build the deep syntactic structures
required by the RealPro realizer (Lavoie and Rambow, 1997) for generating a string that communicates the goal.
When the discourse manager is ready to end its
turn in the dialogue, it passes the accumulated natural language strings to the user interface. This output may also include transitions between the goals
selected for the turn.
While a dialogue is in progress, the discourselevel understanding and tutorial strategist modules
are bypassed until the essay is revised. Once the student revises his essay, it is reanalyzed and the cycle repeats until no additional communicative goals
arise from the system’s analysis of the essay.
Although the overall architecture of the system is
a pipeline, there is feedback to earlier modules via
the history. Only the discourse-level understanding and discourse manager modules are internally
pipelines, the rest are rule-based.

3 Background on Weighted Abduction and
Tacitus-lite+
Abduction is a process of reasoning from an observation to possible explanations for that observation.
In the case of the Why-Atlas system the observations
are what the student said and the possible explanations for why the student said this are the physics
qualitative axioms (both good and bad) and orderings of those axioms that support what the student
said. To arrive at the explanation, some assumptions have to be made along the way since all the
inferences that underly an explanation will not be
expressed.
Weighted abduction is one of several possible formalisms for realizing abductive reasoning. With
weighted abduction there is a cost associated with
making an assumption during the inference process.
Following the weighted abductive inference algorithm described in (Stickel, 1988), Tacitus-lite is a
collection of axioms where each axiom is expressed
as a Horn clause. Further, each conjunct p i has a
weight wi associated with it, as in (2). The weight is
used to calculate the cost of assuming p i instead of
proving it where cost(pi ) = cost(r) ∗ wi .
(2) p1 w1 ∧ · · · ∧ pn wn ⇒ r
Given a goal or observation to be proven, Tacituslite takes one of four actions; 1) assumes the observation at the cost associated with it 2) unifies with a
fact for zero cost 3) unifies with a literal that has already been assumed or proven at no additional cost
4) attempts to prove it with an axiom.
All possible proofs could be generated. However, Tacitus-lite allows the applications builder to
set depth bounds on the number of axioms applied
in proving an observation and on the global number of proofs generated during search. Tacitus-lite
maintains a queue of proofs where the initial proof
reflects assuming all the observations and each of
the four above actions adds a new proof to the queue.
The proof generation can be stopped at any point and
the proofs with the lowest cost can be selected as the
most plausible proofs for the observations.
Tacitus-lite uses a best-first search guided by
heuristics that select which proof to expand, which
observation or goal in that proof to act upon, which
action to apply and which axiom to use when that is

the selected action. Most of the heuristics in WhyAtlas are specific to the domain and application.
SRI’s release of Tacitus-lite was subsequently extended by the first author of this paper for the research project described in (Thomason et al., 1996).
It was named Tacitus-lite+ at that time. Two main
extensions from that work that we are making use
of are: 1) proofs falling below a user defined cost
threshold halt the search 2) a simple variable typing
system reduces the number of axioms written and
the size of the search space (Hobbs et al., 1988, pg
102).
Unlike the earlier applications of Tacitus-lite+,
Why-Atlas uses it for both shallow qualitative
physics reasoning and discourse-level language reasoning. To support qualitative physics reasoning
we’ve made a number of general inference engine
extensions, such as improved consistency checking,
detecting and avoiding reasoning loops and allowing
the axiom author to express both good and bad axioms in the same axiom set. These recent extensions
are described further in (Jordan et al., 2002).

4 Building an Abductive Proof
The discourse-level understanding module uses language axioms and the Tacitus-lite+ abductive inference engine to resolve pronominal and temporal
anaphora and make other discourse-level language
related inferences. It transforms the sentence-level
propositions into more complete propositions given
the context of the problem the student is solving
(represented as facts) and the context of the preceding sentences of the essay.
From these discourse-level propositions, proofs
are built and analyzed to determine appropriate communicative actions. To build these proofs, the
discourse-level understanding module uses domain
axioms, the above resulting propositions and again
the Tacitus-lite+ abductive inference engine.
We’ve separated the discourse-level language axioms from the domain axioms both for efficiency
and modularity because there is generally only a
small amount of interaction between the language
and domain axioms. Separating them reduces the
search space. In cases where interaction within a
single axiom is necessary, we’ve place these axioms
in the set of language axioms. The system currently

has 90 language axioms and 95 domain axioms. The
domain axioms fully cover 5 problems as well as
parts of many other problems.
We will describe in more detail each of these
stages of building the proof in the sections that follow.
4.1

Applying Discourse-level Language Axioms
to Sentence-level Propositions

The discourse-level language axioms are currently
addressing the local resolution of pronominal and
temporal anaphora, flattening out embedded relationships and canonicalizing some lexical choices
that can only be resolved given the context of the
problem. We are still developing and testing axioms that will better address pronominal and temporal anaphora inter-sententially and axioms that will
generate additional propositions for quantifiers and
plurals.
Pronominal Anaphora. It is generally easy to resolve pronominal anaphora in the context of a qualitative physics problem because there are a small
number of candidates to consider. For example, in
the case of the pumpkin problem in (1), there are
only four physics bodies that are likely to be discussed in a student essay; the pumpkin, the runner,
the earth and air.
The system is able to resolve simple intrasentential pronominal references using language axioms. The objects described in a single sentence
are the candidate set and argument restrictions rule
out many of these candidates. But to resolve intersentential anaphora, as in (3), the system currently
relies on the domain axioms. The domain axioms
will bind the body variables to their most likely
referents during unification with facts, and previously assumed and proven propositions similarly to
(Hobbs et al., 1988).
(3) The man is exerting a force on it.
But in the case of anaphoric references to physical
quantities such as velocity, acceleration and force,
as in (4), we need to extend the language axioms to
handle these cases because it involves too much unconstrained search for the domain axioms to resolve
these. This is because the physical quantities are the

predicates that most strongly influence the domain
reasoning.
(4) The velocity is constant before the pumpkin is
thrown. But after the release, it will decrease
because there is no force.
To extend the language axioms to address intersentential anaphora we need to implement and test
a recency ordering of the physics bodies and quantities that have already been discussed in the essay.
But we expect this to be simple to do since the essays
generally only involve one discourse segment.
Temporal Anaphora. As with pronominal
anaphora, temporal anaphora is usually clear because the student often explicitly indicates when
an event or state occurs relative to another event or
state as with the first sentence of the explanation
presented in (1). In these cases, the domain-level
reasoning will be able to unify the anchor event or
state with an already known event or state in the
proof it is constructing.
When there is no temporal anchor the domainlevel search is too under-constrained so the language
axioms resolve the temporal orderings. In some
cases world knowledge is used to infer the temporal
relationships as in (5). Here we know that to catch
an object it must have been thrown or dropped beforehand and so the event in (5a) must occur after
the event in (5b).
(5) a. The man catches the pumpkin.
b. This is because they had the same velocity
when he threw it.
Otherwise, the language axioms use information
about tense and aspect and default orderings relative to these to guide inferences about temporal relationships ((Kamp, 1993; Dowty, 1986; Partee, 1984;
Webber, 1988) inter alia).
Embedded Relationships. In the physics essays
we are addressing, there is a tendency to express
multiple relations within a single sentence as in (6).
Here the “equal” and “opposite” relations are embedded in a temporal “when” relation. In this case
the sentence-level understanding module is not in
the best position to indicate the specific constraints

that each of these relations imposes so this is handled by discourse-level understanding. It would
also impose a greater burden on the domain-level
proof building if these relationships were not resolved beforehand. For example, in the case of the
last clause in (6) there is an elliptical reference that
could cause the domain-level a great deal of unconstrained search.
(6) When the magnitude of the pumpkin’s velocity equals the man’s, the pumpkin’s velocity
is in the opposite direction.
Canonicalizing Lexical Usage. One simple case
in which the language axioms canonicalize lexical
items has to do with direction. For example, saying
“move up the inclined plane” should be interpreted
as a positive direction for the horizontal component
even though the phrase contains “up”. The axioms
are able to canonicalize references such as up, down,
left, right, north, south into a positive or negative direction relative to an axis in a coordinate system that
may be tilted slightly to align with planes. This is an
example of the kinds of axioms in which language
and domain knowledge are interacting within a single axiom.
Quantifiers and Plurals In our target essays,
there is frequent usage of quantifiers and plurals
with respect to physics bodies and frequent use of
quantifiers with respect to parameters of physical
quantities (e.g. “at all times” “all the magnitudes
of the velocities”).
We have recently completed our specification for
a sentence-level representation of quantifiers and
plurals. From this representation the language axioms will generate an appropriate number of new
propositions to use in the proof building stage, given
the context of the problem and the expression recognized from sentence-level processing.
Although we have not yet implemented and tested
this set of language axioms, we have successfully
hand-encoded sentences such as (7) into both their
sentence-level and discourse-level representations
and have used the latter successfully in the final
proof building process. For example, for (7), the
system creates two equivalent propositions about acceleration, each referring to different balls. In addition, both of these propositions are related to two

Student said:

velocity of the pumpkin is decreasing

man applies a force of 0 to the pumpkin

horizontal component of velocity of pumpkin is decreasing

horizontal component of the total force on pumpkin is 0

have impetus bug
(assume)

horizontal component of force of air on pumpkin is 0

horizontal component of force of man on pumpkin is 0

(given)

(assume)

Figure 2: Example of Simplified Abductive Proof for “The pumpkin moves slower because the man is not
exerting a force on it.”
additional propositions about the force of gravity applying to the same ball as in its related acceleration
proposition.
(7) The acceleration of both balls is increasing
due to the force of earth’s gravity.
4.2

Applying Domain-level Axioms to Build an
Explanatory Proof

The propositions produced by applying the language
axioms are the goals that are to be proven using
domain-level axioms. Figure 2 is an example of a
simplified abductive proof for sentence (8).
(8) The pumpkin moves slower because the man
is not exerting a force on it.
Each level of downward arrows from the gloss of
a proposition in Figure 2 represents a domain axiom that can be used to prove that proposition. One
way to prove that the velocity of the pumpkin is decreasing is to prove that just the horizontal component of the velocity vector is the one that is decreasing since the context of the question (see (1)) makes
this a likely interpretation. Alternatively, the system could request that the student be more precise
by asking which components of the velocity vector
are decreasing.
In the case of trying to prove that the horizontal component is decreasing, Tacitus-lite+ is applying a bad physics axiom that is one manifestation of
the impetus misconception; the student thinks that a
force is necessary to maintain a constant velocity. In
this case it assumes the student has this misconception but alternatively the system could try to gather
more evidence that this is true by asking the student
diagnostic questions.

Next Tacitus-lite+ proves that the total force on
the pumpkin is zero by proving that the possible addend forces are zero. In the context of this problem,
it is a given that air resistance is negligible and so it
unifies with a fact for zero cost. Next it assumes that
the student believes the man is applying a horizontal
force of 0 to the pumpkin.
Finally, it still needs to prove another proposition
that was explicitly asserted by the student; that the
force of the man on the pumpkin is 0. As with the
velocity, it will try to prove this by proving that the
horizontal component of that force is zero. Since it
has already assumed that this is true, the abductive
proof is finished and ready to be further analyzed
by the tutorial strategist module to give additional
feedback to the student.
4.3

Incrementally Processing an Essay

We have also extended Tacitus-lite+ to run incrementally so that it can start processing before the
student completes his essay. In this way it can take
advantage of the processing lull as the student composes his essay. In simulations of various typing
speeds, (Rosé et al., 2002) estimated that there is a
60 second processing lull during the completion of a
sentence after subtracting out a 5 second average incremental parsing cost. During this lull it can build
proofs using the previous sentences in the essay.
To run Tacitus-lite+ incrementally, we added a
function that takes as input a proof queue and the
new goals that are to be proven and returns a new
proof queue. The discourse-level understanding
module builds the input proof queue by finding the
proofs in the most recent queue with which the new
goals are consistent and adding the new goals to a
copy of each of those proofs. We then modified

Tacitus-lite+ to take an arbitrary proof queue as input.
The discourse-level understanding module stores
and selects proof queues, which are returned by
Tacitus-lite+ after it attempts to prove a sentence.
Suppose for example that each sentential input is
treated as a separate input to Tacitus-lite+ and that
sentence Sk has already been processed and yielded
proof queue Qk . As the next sentence Sk+1 arrives,
a copy of Qk is updated with proofs that include
Sk+1 as new information to be proven. But if S k+1
conflicts with every proof in the copy of Q k , then an
earlier proof queue is tried. Similarly, if a student
modifies a previously processed sentence, the original sentence is regarded as having been deleted. The
inference process backs up to the point just before
the deleted sentence was processed and reprocesses
the substituted sentence and all that follows it. This
mechanism for backing-up allows the inference process to be incremental.
At the end of composing an essay, the student will
in the best case have to wait the length of time that
it takes to finish parsing the last sentence of the essay plus the length of time that it takes to extend the
proof by one sentence. In the worst case, which is
when he modifies the first sentence or inserts a new
first sentence, he will have to wait the same amount
of time as he would for non-incremental discourselevel understanding.

5 Deriving Feedback for Students From
Plausible Proofs
To identify communicative goals the tutorial strategist next analyzes the best proofs. Currently it examines just one of the best proofs by applying a set of
test patterns to parts of the proof. It can test for combinations of patterns for givens (mainly to get bindings for variables in a pattern), for assumed propositions, for propositions asserted in the student’s essay,
and for inferred propositions. In addition it can also
test for missing patterns in the proof and for particular domain axioms to have been used. Each goal that
the system is capable of addressing is linked to sets
of patterns that are expected to be indicative of it.
In the case of the proof for (8), the tutorial strategist
identifies a dialogue goal that addresses the impetus misconception as being relevant since an impetus

axiom is part of the proof.
In addition to engaging students in a dialogue, the
system can also give direct, constructive feedback on
the essays they are composing. When there are multiple interpretations, it is better to ask the student to
make certain things in the essay clearer. The tutorial
strategist includes test patterns that target important
details that students often leave out. For example,
suppose the student says that the velocity is increasing but this is only true for the vertical component
of the velocity vector. It may then be important to
clarify which component of the velocity the student
has in mind since thinking that the horizontal component is increasing indicates a misconception.
It is also possible that two propositions in an essay
will be contradictory. In this case the system points
out that there is a conflict, describes the conflict and
directs the student to repair it.
We expect to extend the tutorial strategist module
so that if there are multiple best proofs, it will ask
the student questions that will help it disambiguate
which proof is most representative of the student’s
intended meaning for the essay.

6 Preliminary Results and Future Plans
Although we’ve found that incremental understanding is successful at taking advantage of the processing lull during which the student composes his
essay, we still need to fine-tune it so as to minimize both the need to back-up and how much underconstrained searching it does (i.e. the more Tacituslite+ has of the student’s explanation the more constrained the search is). Currently, Tacitus-lite+ runs
after every new sentence that is recognized by the
sentence-level understanding module. During each
of these runs Tacitus-lite+ continues until one of its
run-time thresholds is exceeded.
We plan to also experiment with other ways of
bounding the run-time for Tacitus-lite+ during incremental processing. For example, we might impose a
specific time-limit that is based on the expected 60
second processing lull while the student composes
his next sentence.
In initial timing tests, using a set of 5 correct essays that involved no backing up, the average incremental processing time per sentence when we set the
search bound to 50 proofs and the assumption cost

threshold to .056 , is 21.22 seconds. The worst case
time for extending a proof by one sentence was 98
seconds and the best was 1 second. So in the best
case, which is when no previous sentences have been
modified, the student will wait on average 21.22 seconds after he completes the last sentence in his essay
for a response from Why-Atlas.
In human-human computer-mediated tutoring, we
found that in the worst case the student waits 2 minutes for a reply from the tutor after completing the
essay. The wait time in the case of the human tutor is a combination of the time it takes to read and
analyze the student’s response and then compose a
reply.7 Although the timings are inconclusive and
not directly comparable, it gives us an order of magnitude for tolerable wait times.
We will complete a 5 week formative evaluation
of the Why-Atlas system in which we will compare
the learning gains of 24 students to other sets of
students in three other conditions; 1) a text control
2) human tutoring 3) another tutoring system that
uses statistical classification only. During these trials, we will log decisions and processing times for
each module of the system. From these detailed logs
we will be able to better evaluate the speed and correctness of each system module.

Acknowledgments
This research was supported by MURI grant
N00014-00-1-0600 from ONR Cognitive Science
and by NSF grant 9720359. We thank the entire
NLT team for their many contributions in creating
and building the Why-Atlas system. In particular
we thank Michael Ringenberg, Maxim Makatchev,
Uma Pappswamy and Michael Böttner for their
work with Tacitus-lite+ and the domain axioms and
Roy Wilson for his work with the sentence-level realization module.
6

An assumption cost of 1 means everything is assumed and
a cost of 0 means that nothing was assumed.
7
In these timing studies, we also did not allow the tutor to
see the student input until the student had finished composing
it. This was because our previous experiences with computermediated human tutoring have shown that some human tutors
have a propensity for referring to something the student had
started to write and then deleted. Our goal was to try to collect
interactions that would be closer to those we expected with an
intelligent tutoring system and was not primarily for comparing
efficiency of a computer tutor to a human one.

References
Vincent Aleven and Kenneth R. Koedinger. 2000. The
need for tutorial dialog to support self-explanation. In
Building Dialogue System for Tutorial Applications,
Papers of the 2000 AAAI Fall Symposium.
Vincent Aleven, Octav Popescu, and Kenneth R.
Koedinger. 2001. A tutorial dialogue system with
knowledge-based understanding and classification of
student explanations. In Working Notes of 2nd IJCAI
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems.
Douglas Appelt and Martha Pollack. 1992. Weighted abduction for plan ascription. User Modeling and UserAdapted Interaction, 2(1 – 2):1 – 25.
Eugene Charniak. 1986. A neat theory of marker passing. In Proceedings of the 5th National Conference on
Artificial Intelligence (AAAI’86), pages 584 – 588.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting selfexplanations improves understanding. Cognitive Science, 18:439–477.
Michelene T. H. Chi, Stephanie A. Siler, Heisawn Jeong,
Takashi Yamauchi, and Robert G. Hausmann. 2001.
Learning from human tutoring. Cognitive Science,
25(4):471–533.
David Dowty. 1986. The effects of aspectual class on
the temporal structure of discourse: Semantics or pragmatics? Linguistics and Philosophy, 9(1).
Reva Freedman, Carolyn Rosé, Michael Ringenberg, and
Kurt VanLehn. 2000. ITS tools for natural language
dialogue: A domain-independent parser and planner.
In Proceedings of the Intelligent Tutoring Systems
Conference.
Arthur C. Graesser, Peter Wiemer-Hastings, Katja
Wiemer-Hastings, Derek Harter, Natalie Person, and
the TRG. 2000. Using latent semantic analysis to
evaluate the contributions of students in autotutor. Interactive Learning Environments, 8:129–148.
Richard R. Hake. 1998. Interactive-engagement versus
traditional methods: A six-thousand student survey of
mechanics test data for introductory physics students.
American Journal of Physics, 66(4):64–74.
Jerry Hobbs, Mark Stickel, Paul Martin, and Douglas Edwards. 1988. Interpretation as abduction. In Proc.
26th Annual Meeting of the ACL, Association of Computational Linguistics, pages 95–103.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1–2):69–142.

Pamela W. Jordan, Maxim Makatchev, Michael Ringenberg, and Kurt VanLehn. 2002. Engineering the
Tacitus-lite weighted abductive inference engine for
use in the Why-Atlas qualitative physics tutoring system. Manuscript, University of Pittsburgh.

Mark Stickel. 1988. A prolog-like inference system
for computing minimum-cost abductive explanations
in natural-language interpretation. Technical Report
451, SRI International, 333 Ravenswood Ave., Menlo
Park, California.

Hans Kamp. 1993. From Discourse to Logic; Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation
Theory. Kluwer Academic Publishers, Dordrecht Holland.

Richmond H. Thomason, Jerry Hobbs, and Johanna D.
Moore. 1996. Communicative goals. In K. Jokinen,
M. Maybury, M. Zock, and I. Zukerman, editors, Proceedings of the ECAI 96 Workshop Gaps and Bridges:
New Directions in Planning and Natural Language
Generation.

Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25:259–284.
Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In 29th Annual
Meeting of the Association for Computational Linguistics, pages 55 – 62.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Proceedings of the Fifth Conference on Applied Natural
Language Processing Chapter of the Association for
Computational Linguistics, pages 265–268, Washington, D.C.
Susan McRoy and Graeme Hirst. 1995. The repair
of speech act misunderstandings by abductive inference. Computational Linguistics, 21(4):435–478, December.
Barbara Partee. 1984. Nominal and temporal anaphora.
Linguistics and Philosophy, 7:243 – 286.
Manny Rayner and Hiyan Alshawi. 1992. Deriving
database queries from logical forms by abductive definition expansion. In Proceedings of the Third Conference of Applied Natural Language Processing, pages
1 – 8, Trento, Italy.
Carolyn Rosé, Pamela Jordan, Michael Ringenberg,
Stephanie Siler, Kurt VanLehn, and Anders Weinstein.
2001. Interactive conceptual tutoring in atlas-andes.
In Proceedings of AI in Education 2001 Conference.
Carolyn P. Rosé, Antonio Roque, Dumisizwe Bhembe,
and Kurt VanLehn. 2002. An efficient incremental architecture for robust interpretation. In Proceedings of
Human Language Technology Conference, San Diego,
CA.
Carolyn P. Rosé. 2000. A framework for robust semantic interpretation. In Proceedings of the First Meeting
of the North American Chapter of the Association for
Computational Linguistics.
James D. Slotta, Michelene T.H. Chi, and Elana Joram. 1995. Assessing students’ misclassifications of
physics concepts: An ontological basis for conceptual
change. Cognition and Instruction, 13(3):373–400.

Bonnie Webber. 1988. Tense as discourse anaphor.
Computational Linguistics, 14(2):61 – 71.

Implicit Versus Explicit Learning of Strategies in a
Non-procedural Cognitive Skill
Kurt VanLehn1, Dumiszewe Bhembe1, Min Chi1, Collin Lynch1,
Kay Schulze2, Robert Shelby3, Linwood Taylor1, Don Treacy3, Anders Weinstein1,
and Mary Wintersgill3
1

Learning Research & Development Center, University of Pittsburgh, Pittsburgh, PA, USA
{VanLehn, Bhembe, mic31, collinl,lht3, andersw}@pitt.edu
2 Computer Science Dept., US Naval Academy, Annapolis, MD, USA
schulze@artic.nadn.navy.mil
3 Physics Department, US Naval Academy, Annapolis, MD, USA
{treacy, mwinter}@artic.nadn.navy.mil

Abstract. University physics is typical of many cognitive skills in that there is
no standard procedure for solving problems, and yet a few students still master
the skill. This suggests that their learning of problem solving strategies is implicit, and that an effective tutoring system need not teach problem solving
strategies as explicitly as model-tracing tutors do. In order to compare implicit
vs. explicit learning of problem solving strategies, we developed two physics
tutoring systems, Andes and Pyrenees. Pyrenees is a model-tracing tutor that
teaches a problem solving strategy explicitly, whereas Andes uses a novel
pedagogy, developed over many years of use in the field, that provides virtually
no explicit strategic instruction. Preliminary results from an experiment comparing the two systems are reported.

1 The Research Problem
This paper compares methods for tutoring non-procedural cognitive skills. A cognitive skill is a task domain where solving a problem requires taking many actions, but
the challenge is not in the physical demands of the actions, which are quite simple
ones such as drawing or typing, but in deciding which actions to take. If the skill is
such that at any given moment, the set of acceptable actions is fairly small, then it is
called a procedural cognitive skill. Otherwise, let us call it a non-procedural cognitive skill. For instance, programming a VCR is a procedural cognitive skill, whereas
developing a Java program is a non-procedural skill because the acceptable actions at
most points include editing code, executing it, turning tracing on and off, reading the
manual, inventing some test cases and so forth. Roughly speaking, the sequence of
actions matters for procedural skills, but for non-procedural skills, only the final state
matters. However, skills exists at all points along the continuum between procedural
and non-procedure. Moreover, even in highly non-procedural skills, some sequences

J.C. Lester et al. (Eds.): ITS 2004, LNCS 3220, pp. 521–530, 2004.
© Springer-Verlag Berlin Heidelberg 2004

522

K. VanLehn et al.

of actions may be unacceptable, such as compiling an error-free Java program twice
in a row without changing the code or the compiler settings.
Tutoring systems for procedural cognitive skills can be quite simple. At every
point in time, because there are only a few actions that students should take, the tutor
can give positive feedback when the student’s action matches an acceptable one, and
negative feedback otherwise. When the student gets stuck, the tutor can pick an acceptable next action and hint it. Of course, in order to give feedback and hints, the
tutor must be able to calculate at any point the set of acceptable next actions. This
calculation is often called “the ideal student model,” the “expert model.” Such tutors
are often called model tracing tutors.
It is much harder to build a tutoring system for non-procedural cognitive skills.
Several techniques have been explored. The next few paragraphs review three of
them.
One approach to tutoring a non-procedural skill is to teach a specific problemsolving procedure, method or strategy. The strategy may be well-known but not ordinarily taught, or the strategy may be one that has been invented for this purpose. For
instance, the CMU Lisp tutor (Corbett & Bhatnagar, 1997) teaches a specific strategy
for programming Lisp functions that consists of first inferring an algorithm from
examples, then translating this algorithm into Lisp code working top-down and leftto-right. The basic idea of this approach is to convert a non-procedural cognitive skill
into a procedural one. This allows one to use a model tracing tutor. Several model
tracing tutors have been developed for non-procedural cognitive skills (e.g., Reiser,
Kimberg, Lovett, & Ranney, 1992; Scheines & Sieg, 1994).
A second approach is to simply ignore the students’ actions and look only at the
product of those actions. Such tutoring systems act like a grader in a course, who can
only examine the work submitted by a student, and has no access to the actions taken
while creating it. Such tutors are usually driven by a knowledge base of conditionadvice pairs. If the condition is true of the product, then the advice is relevant. Recent examples include tutors that critique a database query (Mitrovic & Ohlsson,
1999) or a qualitative physics essay (Graesser, VanLehn, Rose, Jordan, & Harter,
2001). Let us call this approach product critiquing.
Instead of critiquing the product, a tutoring system can critique the process even if
it doesn’t understand the process completely. Like product critiquing tutors, such a
tutor has a knowledge base of condition-advice pairs. However, the conditions are
applied as the student solves the problem. In particular, after each student action, the
conditions are matched against the student’s action and the state that preceded it. For
instance, in the first tutoring system to use this technique (Burton & Brown, 1982),
students played a board game. If they made a move that was significantly worse than
the best available move, the tutor would consider giving some advice about the best
available more. Let us call this approach process critiquing.
The distinctions between a process critiquing tutor and a model tracing tutor are
both technical and pedagogical. The technical distinction is that a model tracing tutor
has rules that recognize correct actions, whereas the process critiquing tutor has rules
that recognize incorrect actions. Depending on the task domain, it may be much easier to author one kind of rule than the other. The pedagogical distinction is that model

Implicit Versus Explicit Learning of Strategies in a Non-procedural Cognitive Skill

523

tracing tutors are often used when learning the problem solving strategy is an instructional objective. The strategy is usually discussed explicitly by the tutor in its hints,
and presented explicitly in the texts that accompany the tutor. In contrast, the process
critiquing tutors rarely teach an explicit problem solving strategy.
All three techniques have advantages and disadvantages. Different ones are appropriate for different cognitive skills. The question posed by this paper is which one is
best for a specific task domain, physics problem solving. Although the argument
concerns physics, elements of it may perhaps be applied to other task domains as
well.

2 Physics Problem Solving
Physics problem solving involves building a logical derivation of an answer from
given information. Table 1 uses a two-column proof format to illustrate a derivation.
Each row consists of a proposition, which is often an equation, and its justification. A
justification refers to a domain principle, such as Newton’s second law, and to the
propositions that match the principle’s premises. The tradition in physics is to display
only the major propositions in a derivation. The minor propositions, which are often
simple equations such as a_x=a, are not displayed explicitly but instead are incorpoTable 1. A derivation of a physics problem

Problem: A motorboat cruises slowly to the mouth of the harbor, covering 230 m
in 460 s at a constant velocity. It then speeds up to a cruising speed of 5 m/s in
60 seconds. What is its average acceleration while speeding up? Assume it always travels in a straight line. Let time 1 be when the motorboat starts the 230 m
journey; let time 2 be when it starts speeding up; and let time 3 be when it
reaches cruising speed.
Proposition

Justification

1

d12_x = 230 m

2
3
4
5

t12 = 460 s
t23 = 60 s
v3_x = 5 m/s
v12_x = d12_x / t12

6
7

v12_x = 0.50 m/s
a23_x =
(v3_x-v2_x)/t23

8
9
10

v2_x = v12_x
a23_x = a23
a23 = 0.075

Given (where d12 is the displacement of the motorboat
from time 1 to 2, the x-axis is horizontal, and d12_x is the
x-component of d12)
Given (where t12 is the duration from time 1 to 2)
Given (where t23 is the duration from time 2 to 3)
Given (where v3 is the motorboat’s velocity at time 3)
Definition of average velocity applied over the time interval from time 1 to time 2 (where v12 is the average velocity of the motorboat during time 1 to 2)
Algebraically solve 1, 2, 5 for v12_x
Definition of average acceleration applied over the time
interval from time 2 to 3 (where a23 is the average acceleration of the motorboat during time 2 to 3, and v2 is the
velocity of the motorboat at time 2)
Constant velocity
Projection
Algebraically solve 3, 6, 7, 8, 9 for a23

524

K. VanLehn et al.

rated algebraically into the main propositions. The justifications are almost never
displayed by students or instructors, although textbook examples often mention a few
major justifications. Such proof-like derivations are the solution structures of many
other non-procedural skills, including geometry theorem proving, logical theorem
proving, algebraic or calculus equation solving, etc.
Although AI has developed many well-defined procedures for deductive problem
solving, such as forward chaining and backwards chaining, they are not explicitly
taught in physics. Explicit strategy teaching is also absent in many other nonprocedure cognitive skills.
Although no physics problem solving procedures are taught, some students do
manage to become competent problem solvers. Although it could be that only the
most gifted students can learn physics problem solving strategies implicitly, two facts
suggest otherwise. First, for simpler skills than physics, many experiments have demonstrated that people can learn implicitly, and that explicit instruction sometimes has
no benefit (e.g., Berry & Broadbent, 1984). Second, the Cascade model of cognitive
skill acquisition, which features implicit learning of strategy, is both computationally
sufficient to learn physics and an accurate predictor of student protocol data
(VanLehn & Jones, 1993; VanLehn, Jones, & Chi, 1992).
If students really are learning how to select principles from their experience, as this
prior work suggests, perhaps a tutoring system should merely expedite such experiential learning rather than replace it with explicit teaching/learning. One way to do
that, which is suggested by stimulus sampling and other theories of memory, is to
ensure that when students attempt to retrieve an experience that could be useful in the
present situation, they draw from a pool of successful problem solving experiences.
This in turn suggests that the tutoring system should just keep students on successful
solution paths. It should prevent floundering, generation of useless steps, traveling
down dead end paths, errors and other unproductive experiences. This pedagogy has
been implemented by Andes, a physics tutoring system (VanLehn et al., 2002). The
pedagogy was refined over many years of evaluation at the United States Naval
Academy. The next section describes Andes’ pedagogical method.

3

The Andes Method for Teaching a Non-procedural Skill

Andes does not teach a problem solving strategy, but it does attempt to fill students’
episodic memory with appropriate experiences. In particular, whenever the student
makes an entry on the user interface, Andes colors it red if it is incorrect and green if
it is correct. Students almost always correct the red entries immediately, asking Andes
for help if necessary. Thus, their memories should contain either episodes of green,
correct steps or well-marked episodes of red errors and remediation.
The most recent version of Andes does present a small amount of strategy instruction in one special context, namely, when students get stuck and ask for help on what
to do next. This kind of help is called “next-step help” in order to differentiate it from
asking what is wrong with a red entry. Andes’ next-step help suggests applying a
major principle whose equation contains a quantity that the problem is seeking. Even

Implicit Versus Explicit Learning of Strategies in a Non-procedural Cognitive Skill

525

if there are other major principles in the problem’s solution, it prefers one that is
contains a sought quantity. For instance, suppose a student were solving the problem
shown in Table 1, had entered the givens and asked for next-step help. Andes would
elicit a23 as the sought quantity and the definition of average velocity (shown on line
7 of Table 1) as the major principle.
Andes’ approach to tutoring non-procedural skills is different from product critiquing, process critiquing and model tracing. Andes gives feedback during the problem solving process, so it is not product critiquing. Like a model-tracing tutor, it uses
rules to represent correct actions, but like a process-critiquing tutor, it does not explicitly teach a problem solving strategy. Thus, is pedagogically similar to a processcritiquing system and technically similar to a model-tracing system.
Andes is a highly effective tutoring system. In a series of real-world (not laboratory) evaluations conducted at the US Naval Academy, effect sizes ranged from 0.44
to 0.92 standard deviations (VanLehn et al., 2002).
However, there is still room for improvement, particularly in getting students to
follow more sensible problem solving strategies. Log files suggest that students
sometimes get so lost that they ask for Andes’ help on almost every action, which
suggests that they have no “weak method” or other general problem solving strategy
to fall back upon when their implicit memories fail to show them a way to solve a
problem. Students often produce actions that are not needed for solving the problem,
and they produce actions in an order that conforms to no recognizable strategy. The
resulting disorganized and cluttered derivation makes it difficult to appreciate the
basic physics underlying the problem’s solution.
We tried augmenting Andes’ next-step help system to explicitly teach a problem
solving strategy (VanLehn et al., 2002). This led to such long, complex interactions
that students generally refused to ask for help even when they clearly needed it. The
students and instructors both felt that this approach was a failure.
It seems clear in retrospect that a general problem solving strategy is just too complex and too abstract to teach in the context of giving students hints. It needs to be
taught explicitly. That is, it should be presented in the accompanying texts, and students should be stepped carefully through it for several problems until they have
mastered the procedural aspects of the strategy. In other words, students may learn
even better than Andes if taught in a model-tracing manner.

4

An Experiment: Andes Versus Pyrenees

This section describes an experiment comparing two tutoring systems, a model tracing tutor (Pyrenees) with a tutor that encourages implicit learning of strategies (Andes). Pyrenees teaches a form of backward chaining called the Target Variable Strategy. It is taught to the students briefly using the instructions shown in the appendix.
Although Pyrenees uses the same physics principles and the same physics problems
as Andes, its user interface differs because it explicitly teaches the Target Variable
Strategy.

526

K. VanLehn et al.

4.1 User Interfaces
Both Andes and Pyrenees have the same 5 windows, which display:
• The physics problem to be solved
• The variables defined by the student
• Vectors and axes
• The equations entered by the student
• A dialogue between the student and the tutor
In both systems, equations and variable names are entered via typing, and all other
entries are made via menu selections. Andes uses a conventional menu system (pull
down menus, pop-up menus and dialogue boxes), whereas Pyrenees uses teletypestyle menus.
For both tutors, every variable defined by the student is represented by a line in the
Variables window. The line displays the variable’s name and definition. However, in
Pyrenees, the window also displays the variable’s state, which is one of these:
• Sought: If a value for the variable is currently being sought, then the line
displays, e.g., “mb = SOUGHT: the mass of the boy.”
• Known: If a value has been given or calculated for a variable, then the line
displays the value, e.g., “mb = 5 kg: the mass of the boy.”
• Other: If a variable is neither Sought nor Known, then the line displays
only the variables name and definition, e.g., “mb: the mass of the boy.”
The Target Variable Strategy’s second phase, labeled “applying principles” in the
Appendix, is a form of backwards chaining where Sought variables serve as goals.
The student starts this phase with some variables Known and some Sought. The student selects a Sought variable, executes the Apply Principle command, and eventually
changes the status of the variable from Sought to Other. However, if the equation
produced by applying the principle has variables in it that are not yet Known, then the
student marks them Sought. This is equivalent to subgoaling in backwards chaining.
The Variables window thus acts like a bookkeeping device for the backwards chaining strategy; it keeps the current goals visible.
As an illustration, suppose a student is solving the problem of Table 1 and has entered the givens already. The student selects a23 as the sought variable, and it is
marked Sought in the Variable window. The student executes the Apply Principle
command, selects “Projection” and produces the equation shown on line 9 of Table 1,
a23_x=a23. This equation has an unknown variable in it, a23_x, so it is marked
Sought in the Variable window. The Sought mark is removed from a23. Now the
cycle repeats. The student executes the Apply Principle command, selects “definition
of average acceleration,” produces the equation shown on line 7 of Table 1, removes
the Sought mark from a23_x, and adds a Sought mark to v2_x. This cycle repeats
until no variables are marked Sought. The resulting system of equations can now be
solved algebraically, because it is guaranteed to contain all and only the equations
required for solving the problem.
In Andes, students can type any equation they wish into the Equation window, and
only the equation is displayed in the window. In Pyrenees, equations are entered only
by applying principles in order to determine the value of a Sought variable, so its

Implicit Versus Explicit Learning of Strategies in a Non-procedural Cognitive Skill

527

equation window displays the equation plus the Sought variable and the principle
application, e.g., “In order to find W, we apply the weight law to the boy: W = mb*g.”
Some steps, such as defining variables for the quantities given in the problem
statement, are repeated so often that students master them early and find them tedious
thereafter. Both Andes and Pyrenees relieve students of some of these tedious steps.
In Andes, this is done by predefining certain variables in problems that appear late in
the sequence of problems. In Pyrenees, steps in applying the Target Variable Strategy, shown indented in the Appendix, can be done by either the student or the tutor.
When students have demonstrated mastery of a particular step by doing it correctly
the last 4 out of 5 times, then Pyrenees will take over executing that step for the student. Once it has taken over a step, Pyrenees will do it 80% of the time; the student
must still do the step 20% of the time. Thus, student’s skills are kept fresh. If they
make a mistake when it is their turn, then Pyrenees will stop doing the step for them
until they have re-demonstrated their competence.
4.2 Experimental Subjects, Materials, and Procedures
The experiment used a two-condition, repeated measures design with 20 students per
condition. Students were required to have competence in high-school trigonometry
and algebra, but to have taken no college physics course. They completed a pre-test, a
multi-session training, and a post-test.
The training had two phases. In phase 1, students learned how to use the tutoring
system. In the case of Pyrenees, this included learning the target variable strategy.
During Phase 1, students studied a short textbook, studied two worked example
problems, and solved 3 non-physics algebra word problems. In phase 2, students
learned the major principles of translational kinematics, namely, the definition of
average velocity v=d/t, the definition average acceleration a=(vf-vi)/t, the constantacceleration equation v=(vi+vf)/2 and freefall acceleration equation, a=g. They
studied a short textbook, studied a worked example problem, solved 7 training problems on their tutoring system and took the post-test.
4.3 Results
The post-test consisted of 4 problems similar to the training problems. Students were
not told how their test problems would be scored. They were free to show as much
work as they wished. Thus, we created two scoring rubrics for the tests. The “Answer
rubric” counted only the answers, and the “Show work” rubric counted only the derivations leading up to the answers but not including the answers themselves. The
Show-work rubric gave more credit for writing major principles’ equations than minor ones. It also gave more credit for defining vector variables than scalar variables.
Table 2 presents the results. Scores are reported as percentages. A one-way
ANOVA showed that the pre-test means were not significantly different. When students post-tests were scored with the Answer rubric, their scores were not significantly different according to both an one-way Anova (F(29)=.888, p=.354) and an

528

K. VanLehn et al.

Ancova with the pre-test as the covariate (F(28)=2.548, p=.122). However, when the
post-test were scored with the Show-work rubric, the Pyrenees students scored reliably higher than the Andes students according to both an Anova (F(29)=6.076,
p=.020) and an Ancova with the pre-test as the covariate (F(28)=5.527, p=.026).

5 Discussion
Pyrenees requires students to focus on applying individual principles, whereas Andes
requires only that students write equations. Moreover, Andes allows students to combine several principle applications algebraically into one equation. Thus, the Andes
students may have become used to deriving answers while showing less work. This
would explain why they had lower Show-work scores.
However, having learned an explicit problem solving strategy did not seem to help
Pyrenees students derive correct answers. This may be due to a floor effect—three of
the four test problems were too difficult for most students regardless of which training they received. Also, during the test, students had to do their own algebraic manipulations, while during training, the tutors handled all the algebraic manipulations
for them so that they could concentrate on learning physics.
This was the first laboratory evaluation of Andes and of Pyrenees, so we learned a
great deal about how to improve such evaluations. In the next experiment in this series, we plan to pace the instruction more slowly and to give students more examples.
We need to devise a testing method that doesn’t require students to do their own algebra. Most importantly, we need a way to measure floundering, which we expect Pyrenees will reduce, and across-chapter transfer, which we expect Pyrenees will increase.
Although this experimental results should be viewed with caution due to the many
improvements that could be made to the evaluation methods, the results are consistent
with our hypothesis that Andes students learn problem solving strategies implicitly,
which limits their generality and power relative to an explicitly taught strategy.
When Pyrenees taught a problem solving strategy explicitly, its students employed a
qualitatively better strategy on post-tests, but this did not suffice to raise their Answer
score relative to the Andes students.

Table 2. Means and Standard Errors

N
Pretest
Posttest Show-work
Posttest Answer
Adjusted Show-work
Adjusted Answer

Andes
17
39.9 (4.99)
30.4 (2.71)
35.3 (6.45)
30.8 (2.54)
36.8 (4.94)

Pyrenees
14
44.4 (5.62)
40.1 (2.84)
26.8 (6.13)
39.7 (2.80)
25.0 (5.44)

p (2-tailed)
.555
.020
.354
.026
.122

Implicit Versus Explicit Learning of Strategies in a Non-procedural Cognitive Skill

529

Acknowledgements. This research was supported by the Cognitive Science Program
of the Office of Naval Research under grant N00014-03-1-0017 to the University of
Pittsburgh and grant N0001404AF00002 to the United States Naval Academy.

References
1.

Berry, E. C., & Broadbent, D. E. (1984). On the relationship between task performance
and associated verbalizable knowledge. The Quarterly Journal of Experimental Psychology, 36A, 209-231.
2. Burton, R. R., & Brown, J. S. (1982). An investigation of computer coaching for informal
learning activities. In D. Sleeman & J. S. Brown (Eds.), Intelligent Tutoring Systems. New
York: Academic Press.
3. Corbett, A. T., & Bhatnagar, A. (1997). Student modeling in the ACT programming tutor:
Adjusting a procedural learning model with declarative knowledge, Proceedings of the
Sixth International Conference on User Modeling.
4. Graesser, A. C., VanLehn, K., Rose, C. P., Jordan, P. W., & Harter, D. (2001). Intelligent
tutoring systems with conversational dialogue. AI Magazine, 22(4), 39-51.
5. Lesgold, A., Lajoie, S., Bunzo, M., & Eggan, G. (1992). Sherlock: A coached practice
environment for an electronics troubleshooting job. In J. H. a. C. Larkin, R.W. (Ed.),
Computer Assisted Instruction and Intelligent Tutoring Systems: Shared Goals and Complementary Approaches (pp. 201-238). Hillsdale, NJ: Lawrence Erlbaum Associates.
6. Mitrovic, A., & Ohlsson, S. (1999). Evaluation of a constraint-based tutor for a database
language. International Journal of Artificial Intelligence and Education, 10, 238-256.
7. Reiser, B. J., Kimberg, D. Y., Lovett, M. C., & Ranney, M. (1992). Knowledge representation and explanation in GIL, an intelligent tutor for programming. In J. H. Larkin & R.
W. Chabay (Eds.), Computer Assisted Instruction and Intelligent Tutoring Systems:
Shared Goals and Complementary Approaches (pp. 111-150). Hillsdale, NJ: Lawrence
Erlbaum Associates.
8. Scheines, R., & Sieg, W. (1994). Computer environments for proof construction. Interactive Learning Environments, 4(2), 159-169.
9. VanLehn, K., & Jones, R. M. (1993). Learning by explaining examples to oneself: A
computational model. In S. Chipman & A. Meyrowitz (Eds.), Cognitive Models of Complex Learning (pp. 25-82). Boston, MA: Kluwer Academic Publishers.
10. VanLehn, K., Jones, R. M., & Chi, M. T. H. (1992). A model of the self-explanation
effect. The Journal of the Learning Sciences, 2(1), 1-59.
11. VanLehn, K., Lynch, C., Taylor, L., Weinstein, A., Shelby, R., Schulze, k., Treacy, D., &
Wintersgill, M. (2002). Minimally invasive tutoring of complex physics problem solving.
In S. A. Cerri & G. Gouarderes & F. Paraguacu (Eds.), Intelligent Tutoring Systems 1001:
Proceedings of the 6th International Conference (pp. 158-167). Berlin: Springer-Verlag.

530

K. VanLehn et al.

Appendix: The Target Variable Strategy
The Target Variable Strategy is has three main phases, each of which consists of
several repeated steps. The strategy is:
1
Translating the problem statement. For each quantity mentioned in the problem
statement, you should:
1.1
define a variable for the quantity; and
1.2
give the variable a value if the problem statement specifies one, or mark the
variable as "Sought" if the problem statement asks for its value to be determined. The tutoring system displays a list of variables that indicates which are
Sought and which have values.
2
Applying principles. As long as there is at least one variable marked Sought in
the list of variables, you should:
2.1
choose one of the Sought variables (this is called the "target" variable);
2.2
select a principle application such that when the equation for that principle is
written, the equation will contain the target variable;
2.3
define variables for all the undefined quantities in the equation;
2.4
write the equation, replacing its generic variables with variables you have
defined
2.5
(optional) rewrite the equation by replacing its variables with algebraic expressions and simplifying
2.6
remove the Sought mark from the target variable; and
2.7
mark the other variables in the equation Sought unless those variables are
already known or were marked Sought earlier.
3
Solving equations. As long as there are equations that have not yet been solved,
you should:
3.1
pick the most recently written equation that has not yet been solved;
3.2
recall the target variable for that equation;
3.3
replace all other variables in the equation by their values; and
3.4
algebraically manipulate the equation into the form V=E where V is the target
variable and E is an expression that does not contain the target variable (usually E is just a number).
On simple problems, the Target Variable Strategy may feel like a simple mechanical procedure, but on complex problems, choosing a principle to apply (step 2.2)
requires planning ahead. Depending on which principle is selected, the derivation of
a solution can be short, long or impossible. Making an appropriate choice requires
planning ahead, but that is a skill that can only be mastered by solving a variety of
problems. In order to learn more quickly, students should occasionally make inappropriate choices, because this lets them practice detecting when an inappropriate
choice has been made, going back to find the unlucky principle selection (use the
Backspace key to undo recent entries), and selecting a different principle instead.

Proceedings of the Twenty-Ninth International
Florida Artificial Intelligence Research Society Conference

Designing a Personal Assistant for Life-Long Learning (PAL3)
William Swartout,1* Benjamin D. Nye,1 Arno Hartholt1, Adam Reilly,1 Arthur C. Graesser,2
Kurt VanLehn,3 Jon Wetzel,3 Matt Liewer,1 Fabrizio Morbini,1
Brent Morgan,2 Lijia Wang,2 Grace Benn,1 & Milton Rosenberg 1
1

University of Southern California, Institute for Creative Technologies; 2University of Memphis; 3Arizona State University
*
swartout@ict.usc.edu

more advanced level (C School). In the Navy, there can be
a long delay (6 weeks to 6 months) between these schools.
During this delay, sailors are assigned to other tasks, not
relevant to their training, and significant knowledge decay
occurs. Accordingly, our goals for this implementation of
PAL3 were: 1) to prevent skill decay, 2) to practice and
build knowledge and skills, 3) to track skills persistently,
and 4) to monitor, engage and motivate the student.
Intelligent Tutoring Systems (ITS) represent one possible approach to addressing these goals. But ITSs are expensive to develop and often narrow in coverage. Furthermore, a broad range of non-ITS resources already exist that
could help with skill retention, such as Wikipedia entries
and online instructional videos. If we could find a way to
make use of these resources in an integrated fashion with
ITSs where they exist, we could significantly lower costs
and broaden coverage. To achieve this, we designed PAL3
not as an ITS, but instead as an intelligent learning guide
that understands students’ current skills, where they need
to go, and can recommend learning resources to get them
there. The major elements of the PAL3 system are: a persistent learning record, the Pal mentoring agent, a library
of learning resources, an algorithm for recommending resources, and mechanisms to promote engagement.
This paper explains the design principles, choices, user
implications, and process to achieve these goals. To note,
while a number of AI innovations drive this system (e.g., a
goal-seeking dialog manager, a data-driven user model,
two types of ITS), the implementation of these elements is
not the focus. Instead, we report how this AI was adapted
to learner needs, based on multiple rounds of feedback.

Abstract
Learners’ skills decay during gaps in instruction, since they
lack the structure and motivation to continue studying. To meet
this challenge, the PAL3 system was designed to accompany a
learner throughout their career and mentor them to build and
maintain skills through: 1) the use of an embodied pedagogical
agent (Pal), 2) a persistent learning record that drives a student
model which estimates forgetting, 3) an adaptive recommendation engine linking to both intelligent tutors and traditional
learning resources, and 4) game-like mechanisms to promote
engagement (e.g., leaderboards, effort-based point rewards,
unlocking customizations). The design process for PAL3 is
discussed, from the perspective of insights and revisions based
on a series of formative feedback and evaluation sessions.

Introduction1
Educational transitions are often difficult due to forgetting,
combined with changing expectations and roles. For example, during the summer for K-12 schools, knowledge is
often forgotten, leaving students less prepared when they
enter the next grade. For professionals, similar issues can
arise as they progress through training or as they move
from one assignment to another. Human mentors can help
with these transitions, but they are often in short supply.
The Personal Assistant for Life-Long Learning (PAL3)
project was designed to provide computer support for such
transitions. The long-term goal is to create an agent that
can accompany a learner throughout their career. This system will need to know the learner’s background (e.g., what
they studied and how they performed), where the learner is
headed, what is needed for success at the next level, and
what resources to recommend based on their progress.
Since the long-term goals for PAL3 system are quite
broad, our first prototype focuses on a more restricted
problem: supporting US Navy sailors as they move from
one level of electronics technician training (A School) to a

Prior Work
The design of this system draws from games (both educational and traditional), ITS and adaptive learning management systems (ALMS), animated pedagogical agents, and
cognitive modeling. Overall, evidence that gamification

1

Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

491

certain design decisions were made based on knowledge
about the sailor demographics (e.g., wide range of ages,
but mostly 18-25 and fairly competitive). Formative feedback was primarily collected from military volunteers: two
formative evaluations with A-school sailors (presented
here), a pre-pilot with two Army cadets, and a quality assurance tester with prior Navy experience.
The Home Screen for PAL3 is shown in Figure 1, with
an angered Pal in the center, three topics recommended on

improves the efficiency of learning is mixed (Clark et al.
2015): games typically produce higher time on task, but
sometimes no increase in efficiency or even no overall
improvement compared to a traditional system. However,
since PAL3 targets self-directed out-of-classroom learning
(i.e., competes for free time), increasing time spent on
learning is central and game features should help. Four
mechanisms were identified as most promising to increase
engagement:
A. Flow: Promoting interaction and flow by presenting a
steady stream of short, varied learning activities.
B. Gamified Learner Models: Presenting progress and
loss-of-progress (forgetting) with open learner models.
C. Social Motivation: Encouraging social use and competition, such as through leaderboards.
D. Accumulated Rewards: Progress-based system expansion (e.g., unlocking content and customizations).
These design principles are found in most highlysuccessful games, ranging from 3D games like World of
Warcraft (A. quests, B. gold/experience points, C. reputation and clan battles, D. unlocked items and quests) to casual games like Candy Crush (A. levels, B. points, C. sharing lives, D. new game modes). Of these principles, the
most debated for inclusion was competition, due to research showing potential negative effects based on individual traits and/or gender differences. This issue was decided based on the expected users, as noted later.
To implement these mechanisms, our research draws
from established methods for learner modeling
(Brusilovsky and Millán 2007), where assessment activities are linked to knowledge components and the resulting
mastery-model estimates can rank future activities. Mastery models can also drive open learner models, which can
be used to increase engagement, select topics more effectively, and promote metacognition (Bull and Kay 2010).
Since PAL3 sequences qualitatively different tasks, it
was necessary to consider the complexity and interactivity
of activities (e.g., Chi’s Active-Constructive-Interactive;
Chi 2009). As such, the task interactivity and initiative is
also considered (e.g., ranging from passive, to systeminitiated constructive, to student-initiated interactive). Given the goal for PAL3 to be a “life-long” learning system,
the mastery model also needed to address forgetting. While
there has been research on forgetting for scheduling practice (Jastrzembski et al. 2009), less research has looked at
real-time task selection (Pavlik Jr et al. 2007) and no systems (to our knowledge) have included forgetting into open
learner models, as done in PAL3.

Figure 1: PAL3 Home Screen with Pal Showing Emotion

the right, and four mechanisms to support learning and
engagement on the left: the User Roadmap (open learner
model), User Profile (accumulated rewards for effort),
Leaderboards (social competition), and Resource Submission (social collaboration). PAL3 has four key design areas
that will be discussed: the Pal animated pedagogical agent
that acts as a mentor and guide, the library of learning resources, the persistent learner record that drives learner
models and recommender models, and engagement mechanisms. These components work together. When the student logs in, PAL3 loads the student’s learning record and
selects topics based on student mastery. The Pal character
describes the topics to the student and suggests a topic.
After a topic is chosen, PAL3 recommends resources (Figure 2), though the learner can manually find any resource
in a topic. In some cases (e.g., after reviewing a video), Pal
suggests an ITS to measure and solidify that knowledge.
After completing a resource, the score is shown and Pal
comments on the student’s performance (Fig. 4).
Pal as a Mentor. PAL3 is embodied by a virtual character
called Pal. This character is the student’s guide, designed
to engage, and to direct them to appropriate resources for
learning. Preliminary designs called for Pal to be a virtual
human Navy instructor. However, any particular Navy
instructor would have a rank, and that would usually be
above (or below) the rank of the student, raising questions
about how the student would perceive Pal: As a superior
officer who must be obeyed? Or, as a subordinate who
could be ignored? In either case, these questions would
complicate Pal’s real role as mentor. Using a civilian was

PAL3 Design
While PAL3 is general enough to cover a wide range of
content and users (at least Grades 6-12 and adult learners),

492

saliency and/or rarity of an achievement). Pal’s behaviors
include lines to greet and motivate the user, introductions
that explain certain panels (e.g., the leaderboard), suggestions about what to do next (e.g., recommending an assessment after watching a video or suggesting a resource if
the learner hesitates) and lines for color commentary after
completing a resource or an achievement. The end result is
that Pal is perceived primarily as a supporter (“He’s your
buddy,” according to one student), but is also encouraging
the learner to use the system productively and persistently.
Learning Resources: Leveraging ITS & Reusing Content
Navy sailors receive a broad range of technical training in
A School, of which a subset of critical topics were chosen
to add to the PAL3 database (Basic RLC Circuits, Diodes,
Rectifiers, Voltage Regulators, and Transistors). Four
types of resources were used: URL’s to existing web resources, URL’s to a custom Wiki, AutoTutor dialogs
(Graesser et al. 2014), and Dragoon model-building exercises(VanLehn et al. in press). Each resource was tagged
with metadata for the associated knowledge components
(KC’s), which was used by the student model to recommend the resource. Interactive resources (e.g., the ITSs)
report back scores associated with KC’s.
Web-Based URL’s. A goal for PAL3 was to blend custom ITS content with existing web-based resources (e.g.,
links to online tutorials and how-to videos) to decrease cost
and increase coverage over a custom ITS-only approach. In
addition to existing resources, for learners who need a
quick review, we created custom Wiki resources that give a
brief summary of a device, circuit, or system.
AutoTutor. AutoTutor resources (Figure 3) were used for
two activities: short review questions and longer deepreasoning tutoring questions that help students understand
causal and qualitative relationships in circuits. AutoTutor
is an ITS that simulates the dialogue moves of human tutors as well as ideal pedagogical strategies. AutoTutor poses questions to a student and allows them to answer in natural language. AutoTutor excels at deeper reasoning, as
opposed to shallower facts and procedures (Graesser, Li et
al. 2014). For each question, experts provide exemplars of
good and bad answers that reflect misconceptions. Using

also considered (i.e., no rank), but civilian instructors are
not common in the Navy. Given that young adults are increasingly familiar with non-human, droid-like characters,
our design moved toward a robot-like character (see Figure
2). Not only does this open up options for the personality
and tone of the character, it also matches the current state
of technology (e.g. tinny Text-To-Speech).
The Pal character uses Virtual Human technology
(Swartout et al. 2006), including a procedural animation
system and advanced dialogue manager. Based on user
feedback, the role for the Pal character crystalized as being
a supporter and motivator for the student, while other characters and systems within PAL3 teach, critique, and assess
the learner. Pal’s personality is designed to provide a level
of entertainment and engagement to keep the student using
PAL3 longer and to keep the student returning.
To bring home the role of being the student’s peer, initial personality designs aimed to create an edgy, joking,
and even teasing character (e.g., see AutoTutor’s “Rude
Tutor”; Graesser 2011). While learners were surprised and
delighted at first, the frequency and severity of Pal’s teasing, together with the sometimes opaque scoring of the ITS
systems, led learners to perceive the character’s feedback
as snarky rather than good-humored. As a result, Pal now
offers more encouraging feedback when the student has
lower mastery and when system evaluations are less certain, while reserving teasing for lapses in an otherwise
good performance, thinking that learners will be more open
to a funny remark when they are mostly doing well.
To drive the character behavior we use the FLoReS dialogue manager (DM), which takes a forward-looking, reward-seeking approach to select among dialogue alternatives (Morbini et al. 2012). FLoRes is coupled with a natural language generation module that uses templates to pro-

Figure 2: Pal on the Resource Screen

vide a wide variety of verbal behaviors. A partially persistent information state tracks each user and remembers important parameters, like the history of visited panels. When
multiple behaviors are available, the DM selects one based
on prioritized goals that consider multiple factors (e.g., the
Figure 3: AutoTutor

493

tive to students and instructors, which require very different granularity. Second, multiple qualitatively different
activities exist in PAL3 and some lack any assessments
(e.g., videos). Third, the recommender should space out
repetition of activities, even if they target needed skills.
Finally, since forgetting is an important factor, the model
must simultaneously estimate both the learner’s current
mastery and an estimate of their likely long-term mastery
(i.e., after forgetting). These requirements led to three interacting models: a Knowledge Component Mastery model, a Topic Mastery Model, and a Resource Recommender.
Due to space limits it is impossible to show the full algorithms, but the principles behind each will be discussed.
The KC Mastery model is updated based on the raw
scores for knowledge components. The KCs for electronics
were determined based on the element (a device, circuit, or
system) combined with the aspect being studied (structure,
behavior, function, parameters, physics). These could have
subtypes, such as “Diode-Behavior-ReverseBias.” The KC
Mastery model updates the mastery for each KC and also
addresses forgetting. The estimate of each KC is an exponential moving average of the current observed score and
the prior mastery level modified by forgetting. Forgetting
is modeled using a variant of Averell & Healthcote’s
(2011) exponential decay model with a non-zero asymptote. Our model attempts to estimate both the asymptote
and the current mastery simultaneously, where each observation is weighted based on the expected amount of forgetting (i.e., three high scores each a month apart raise the
asymptote greatly, but three high scores a minute apart will
raise current mastery but do little to change the asymptote).
Forgetting is applied every time a new score is added or
when calculating mastery after the learner has not practiced
a KC for at least one day. This allows Pal to warn the
learner that their skills are decaying after longer absences.
The Topic Mastery model is designed to aggregate the
fine-grained KC Mastery elements into broader topics.
Each topic contains a set of resources and a set of KC’s
whose values are averaged to determine the mastery of the
topic (e.g., “PN Junction Diodes”). This makes it easy to
quickly revise the topics, without changing the underlying
KC mastery. The Topic Mastery model also has topic prerequisites, which are used so that certain topics are not
recommended until others reach a high mastery. Topic
Mastery is shown in the Leaderboard and User Roadmap.
The Resource Recommender calculates ranking scores
for each resource, with the top three shown as recommended resources. Three factors are modeled: KC Mastery,
Novelty, and Exploration Level. The main input is the KC
Mastery, which recommends resources based on their
alignment to student KC deficiencies for a topic. Novelty
exists so that the recommender will prefer less-viewed resources and decays exponentially based on the number of
exposures (N) to a resource (1-erN, where r is a static decay

natural language processing techniques, including latent
semantic analysis and pattern matching, AutoTutor compares students’ answers with the good and bad exemplars
and provides feedback. AutoTutor detects partial answers
and asks for elaboration. Using AutoTutor is substantially
better than reading texts on the content for the same
amount of time (Graesser, Li et al. 2014). In PAL3, the
scores learners receive from AutoTutor are recorded in the
learning record.
Dragoon. The Dragoon ITS for systems dynamics modeling helps student understand the behavior of a circuit in
terms of propagating voltages, currents and signals
throughout the circuit. Dragoon activities are based on a
directed-graph notation for systems: if node Voltage1 is
defined as Current2*Resistance3, then the Current2 and
Resistance3 nodes will have arrows to Voltage1. University students often solve problems like “How would the voltage across the load change if the load resistance decreased
slightly?” by analyzing a model of the circuit as a system
of equations. While the Navy A School course limits equation-solving, it still requires these core intuitions.
To meet this need, the Dragoon system provides several
types of exercises: incremental, waveform identification,
and model construction. Each task shows both a schematic
and an associated model graph, and gives step-by-step
feedback. When students hover over a node, the corresponding part of the schematic is highlighted. For an incremental model, a pre-made model has one of the nodes is
labelled as an increment or a decrement. The student’s task
is to label all the other nodes with the symbols for increment, decrement, no-change or can’t-be-determined. In the
waveform activity a student labels nodes with waveforms
(e.g., sine waves, truncated sine waves, flat lines) selected
from a menu. The model construction activity is the most
complex: students start with a schematic and a partially
completed node-link model, and attempt to complete the
model. When done, they can use sliders to vary circuit parameters and see in real time how graphs for the values
change. Studies with high school science students indicate
that constructing such Dragoon models is a more effective
than baseline instructional methods (VanLehn, Chung et al.
in press). As with AutoTutor, all Dragoon activities report
KC scores to PAL3.
Life-Long Learning: Student Models & Recommender
As learners complete (or abort) tasks, these activities submit scores to a persistent and (in principle) life-long learning record that uses the xAPI standard. Each score is associated with both the task and the knowledge component
(KC) involved. This produces an event stream of records,
which are processed to build a persistent mastery model.
Our student model needed to balance a number of competing concerns. First, the model must support both adaptive
recommendations and open-learner models that are intui-

494

priate for all groups, they were identified as a likely motivating factor in early discussions with instructors and
learners. Resource submission is a second social factor,
where learners recommend a resource for a topic, which is
then logged in the database for review by instructors.

rate). Otherwise, if a student is struggling on a resource,
they might see it repeatedly (i.e., wheel-spinning). The
Exploration Level considers the average studentinteractivity and complexity for resources the student has
completed in a topic. For learners with low mastery of a
topic, less-interactive resources are suggested to establish a
base (e.g., text, videos, AutoTutor reviews). As mastery
increases, this factor favors increasingly complex resources
that require greater contributions from the learner (e.g.,
deep AutoTutor dialogs, Dragoon Model Construction).
Creating Engagement
Open Learner Models. One mechanism to increase engagement was the use of gamified open learner models that
display both student mastery on topics (Mastery Points)
and student effort (Experience Points/XP). Mastery ranges
from 0 to 100, with high mastery (e.g., 85+) indicating the
student has mastered a topic. Mastery is gained by successfully completing interactive resources. Resource scores
impact multiple topics, if they monitor the same KC’s.
Since Mastery is an estimate of student knowledge, it can
be adjusted down and decays without practice.
On the Resource Screen for a topic (where resources are
recommended and selected), the associated Mastery score
is displayed prominently at the top, next to a graph of Mastery changes over time (see Fig. 2). This allows the student
to see their current score and recent trend at a glance. After
completing the resource, the student is presented with a
Score Screen (Fig. 4), containing 1) their score for this
Resource session, visualized as 1 to 4 badges, 2) the
change in Mastery for the associated Topic as a result of
the resource score, and 3) the XP gained. In addition, Pal
provides feedback and commentary on the student’s performance and overall context (e.g., if they moved up on the
leaderboard). Binning percentile (0-100) scores into badges
improved users’ talk-aloud understanding of the Score
Screen, since percentile scores confused learners with unnecessary precision. At any time, learners can open the
User Roadmap to view their Mastery score for all topics as
a bar graph. A user-selected time horizon provides immediate visual feedback about changes in mastery, through
upward or downward arrows for each topic.
Social Engagement: Leaderboards. A leaderboard is provided where learners can see where they rank based on
their Mastery score. The Leaderboard can be filtered by
Topic and by Class. To avoid shaming, the Leaderboard
only shows the top tier of students and the rank of the current student. While leaderboards are not necessarily appro-

Figure 4: Scoring Screen

User Profile/Customization (Effort Builds “Character”).
The User Profile shows the user’s level and XP total. Experience points are gained through completing PAL3 resources and earning achievements, regardless of performance on those resources. This rewards overall effort. Total XP determines the learner’s level (e.g., a progress bar of
XP toward the next level). Pal Customizations are unlocked for the player when they reach a new level. Players
can customize Pal’s paint job, speech lights, and face display. All customizations are viewable through the User
Profile menu, to incentivize learners to gain enough levels
to unlock their desired customization. Learners can also
earn achievements in PAL3 for completing a specific missions. These missions range from very easy (e.g., Click on
Pal on the home screen) to difficult (e.g., Earn the top position on a leaderboard.) Players can view all the achievements that can be earned, to encourage effort and exploration of the system. In combination, experience points,
achievements, and customizations are designed to increase
persistence by offering long term goals and rewards.

Results of Usability Pilot Testing
Two rounds of usability testing were conducted, with 9 and
17 sailors respectively. All sailors were studying Navy
electronics, though there was significant variation in training level, ranging from 2 weeks to over 12 weeks. Between
the two tests, improvements were made to the overall stability and Pal’s natural language policies, and tutorials and
usability upgrades were created for resources (particularly
Dragoon). Also, the delay before experimenters intervened

Table 1:PAL3 Usability and User Impressions (N=26, Both Rounds)

Clear/Easy

5.0 (0.5)

PAL3
Agent
5.3 (0.7)

Good Idea

5.4 (0.7)

5.3 (1.0)

Overall

Resource
Panel
5.2 (0.8)
5.3 (0.8)

3.8 (0.9)

User
Roadmap
5.5 (0.7)

Leaderboard
5.2 (0.8)

User
Profile
5.5 (0.7)

5.0 (0.8)

N/A

4.9 (1.2)

5.4 (0.7)

AutoTutor

Dragoon

4.7 (1.0)
5.0 (1.0)

495

like many ITSs that are used in structured settings, PAL3 is
intended to be used informally during the students’ free
time. This use-case means that PAL3 needs to be motivating at multiple levels: surface interactions (e.g., Pal), learning interactions (e.g., activities that promote flow), and
connecting to learner goals (e.g., feeling that they are gaining and retaining useful skills). Future work will study the
efficacy of PAL3 for supporting learning among sailors
between A School and C School.

to help learners was increased from 5-10 seconds of confusion in the first study, to 30-60 seconds in the second. Total time interacting with the system was 45-90 minutes per
participant. Surveys were variants of the Unified Technology Acceptance Model (Venkatesh et al. 2003), applied to
PAL3 as a whole (e.g., “I think PAL3 will help me learn
more quickly”) and to components of the system (e.g.,
“Using Dragoon models is a good idea.”). Table 1 shows
the average of the two main usability items (“Interacting
with <x> was clear and easy to understand.”, “I found <x>
easy to use.”) and the general impression item (“Using <x>
is a good idea.”) for all major components and the system
overall (standard deviation in parentheses). The User
Roadmap is largely non-interactive, so it only asked one of
the two usability items (“clear and easy”).
Usability results were uniformly high: on a 6-point Likert scale for usability and intention to use, the average
score was a 5.0 (Agree) with a standard deviation of only
0.4. Despite differences between the usability testing conditions, survey results were very similar across both
rounds. Even taking a very generous threshold of p<0.2 for
t-test comparisons between the two rounds, only 8 items
out of 51 were significantly different. Round 2 subjects
thought that the system would not increase their productivity as much (“Using PAL3 will increase my productivity”) but thought it was better to use (“Using PAL3 is a
good idea.”). They also found both AutoTutor and Dragoon resources easier to use (e.g., “Interacting with Dragoon was clear and easy to understand.”), though creating
models with Dragoon was still rated as the hardest activity.
As such, adding tutorials for resources appeared to cause
the strongest positive effects for usability but did not
change the rank-order for usability ratings of the components. Overall intent to use the system was lower for the
second round, reduced from an average of Daily intent
closer to 2-3 Times per Week but still well within our
goals for frequency of use. In talk-alouds, participants also
reported strong engagement and self-directed use PAL3.

Acknowledgements
PAL3 was supported by the Office of Naval Research
through ARL W911NF-04-D-0005. However, the contents
of this paper are the responsibility of the authors alone.

References
Averell, L. and A. Heathcote (2011). "The form of the forgetting
curve and the fate of memories." Journal of Mathematical
Psychology 55(1): 25-35.
Brusilovsky, P. and E. Millán (2007). User models for adaptive
hypermedia and adaptive educational systems. The adaptive web,
Springer-Verlag.
Bull, S. and J. Kay (2010). Open learner models. Advances in
intelligent tutoring systems, Springer: 301-322.
Chi, M. T. (2009). "Activeconstructiveinteractive: A conceptual
framework for differentiating learning activities." Topics in
Cognitive Science 1(1): 73-105.
Clark, D. B., E. E. Tanner-Smith and S. S. Killingsworth (2015).
"Digital Games, Design, and Learning A Systematic Review and
Meta-Analysis."
Review
of
educational
research:
0034654315582065.
Graesser, A. C. (2011). "Learning, thinking, and emoting with
discourse technologies." American Psychologist 66(8): 746.
Graesser, A. C., H. Li and C. Forsyth (2014). "Learning by
communicating in natural language with conversational agents."
Current Directions in Psychological Science 23(5): 374-380.
Jastrzembski, T. S., K. A. Gluck and S. Rodgers (2009). The
Predictive Performance Optimizer: An Adaptive Analysis
Cognitive Tool for Performance Prediction. Proceedings of the
Human Factors and Ergonomics Society Annual Meeting, SAGE
Publications.
Morbini, F., D. DeVault, K. Sagae, A. Nazarian and D. Traum
(2012). FLoReS: A Forward looking, reward seeking, dialogue
manager. International Workshop on Spoken Dialogue Systems.
Pavlik Jr, P. I., N. Presson, G. Dozzi, S.-m. Wu, B. MacWhinney
and K. Koedinger (2007). The FaCT (Fact and Concept Training)
System: A new tool linking cognitive science with educators.
Proceedings of the Twenty-Ninth Annual Conference of the
Cognitive Science Society.
Swartout, W., J. Gratch, R. Hill, E. Hovy, S. Marsella, J. Rickel
and D. Traum (2006). "Toward Virtual Humans." AI Magazine
27(1).
VanLehn, K., G. Chung, S. Grover, A. Madni and J. Wetzel (in
press). "Learning about dynamic systems and domain principles."
International Journal of Artificial Intelligence in Education.
Venkatesh, V., M. G. Morris, G. B. Davis and F. D. Davis (2003).
"User acceptance of information technology: Toward a unified
view." MIS quarterly: 425-478.

Conclusions and Future Directions
Compared to other approaches to providing intelligent
support for learning, we believe PAL3 is novel in several
regards. First, the focus in PAL3 is not on tutoring per se,
but instead on guiding learners toward a broad array of
resources that can help them learn. This approach allows
for a smooth integration of existing and novel resources.
Second, few learning systems use a persistent, long-term
learning record as PAL3 does. Third, PAL3 directly models and addresses forgetting, which has received comparatively little attention by ITSs. Fourth, PAL3 uses an engaging, embodied character to motivate students to continue
using the system. This engagement is necessary since un-

496

In Proceedings of the Sixth International Conference on Cognitive Modeling, 166-171. Mahwah, NJ:
Lawrence Earlbaum.

Abductive Proofs as Models of
Students’ Reasoning about Qualitative Physics
Maxim Makatchev (maxim@pitt.edu), Pamela W. Jordan (pjordan@pitt.edu),
Umarani Pappuswamy (umarani@pitt.edu) and Kurt VanLehn (vanlehn@pitt.edu)
Learning Research and Development Center
University of Pittsburgh
3939 O’Hara Street, Pittsburgh, PA 15260 USA
Question: Suppose a man is in a free-falling elevator and
is holding his keys motionless right in front of his face. He
then lets go. What will be the position of the keys relative
to the man’s face as time passes? Explain.

Abstract
In this paper we describe a part of the Why2-Atlas tutoring system that models students’ reasoning in the
domain of qualitative physics. The main goals of the
model are (1) to evaluate correctness of the student’s
essay, and, in case the essay contains errors, (2) to direct remedial tutoring actions according to plausible errors in the student’s reasoning. To meet these goals, a
backchaining theorem prover generates a set of assumptions and a chain of reasoning (a proof) that plausibly
led the student to write the observed essay. A proof
can include correct as well as buggy reasoning steps
and assumptions. After a proof is generated, it is analyzed for correctness and the analysis is used to generate
appropriate feedback to the student. We describe the
weighted abductive theorem proving framework, outline
previous and upcoming evaluations and discuss possible
future directions.

Explanation: The keys are affected by gravity which
pulls them to the elevator floor, because the keys then have
a combined velocity of the freefall and the effect of gravity.
If the elevator has enough speed the keys along with my
head would be pressed against the ceiling of the elevator,
because the acceleration of the elevator car along with me
and the keys would overwhelm the gravitational pull.

Figure 1: The statement of the problem and an example
explanation.

Introduction
The Why2-Atlas tutoring system is designed to encourage students to write their answers to qualitative physics
problems as essays that include explanations of their
arguments (VanLehn, Jordan, Rosé, Bhembe, Böttner,
Gaydos, Makatchev, Pappuswamy, Ringenberg, Roque,
Siler, & Srivastava, 2002). If the essay is incomplete or
incorrect, the system generates elicitation or remediation feedback, respectively. For the purpose of evaluating
completeness and correctness of the essay, a deep understanding of its contents is necessary. In the domain of
qualitative physics, a deep understanding of the essay involves reasoning about logical relationships between the
statements in the essay. The theorem proving approach
that we use in Why2-Atlas provides the means to recover a structure of logical dependencies that connects
the propositions representing (a) the essay text, (b) the
problem statement, and (c) plausible student reasoning
steps not explicitly stated in the essay.
Previously, formal methods for analyzing natural language text have encountered a number of challenges,
such as the difficulty of obtaining propositional representations for sentences and the need for large amounts of
commonsense knowledge in order to interpret the many
concepts expressed. Consider, for example the qualitative physics problem presented in Figure 1 along with
an actual student explanation. To address all the errors
in the essay, the propositional representation of the essay must account for such commonsense knowledge as
“An elevator has a ceiling” and “A human has a head.”

166

In addition, even if the system is able to represent the
respective statements, it also needs to be able to reason about correctness of logical relations between these
statements (a statement b can be false in the context of
the problem, while the statement a → b can be true).
Such reasoning would also be desirable for the purpose
of providing a more substantive feedback to the student,
e. g. excessive argumentation. An informal example of
a possible chain of reasoning that student used to arrive
at the statement “The keys would be pressed against the
ceiling of the elevator” is shown in Figure 2.
An abductive theorem proving approach allows one
to cope with propositions that cannot be proven due to
lack of applicable rules by assuming such propositions
are true without a proof (the operation is also referred to
in the literature as abducing) when such assumptions allow a proof to be completed. The fewer the assumptions
made, the better the proof. Weighted abduction takes
this approach farther and assigns a cost to the set of assumptions depending on their individual weights and the
chain of reasoning that led to the generation of these assumptions. A proof can include correct as well as buggy
reasoning steps and assumptions.
In this paper we describe a part of the Why2-Atlas tutoring system that models students’ reasoning of qualitative physics. The theorem prover, called Tacitus-lite+, is
a derivative of SRI’s Tacitus-lite (Hobbs, Stickel, Martin,
& Edwards, 1988, p. 102) that, among other extensions,
incorporates sorts. First we provide an overview of the
knowledge representation. Next we describe the abduc-

Step #
1
2
3
4
5

Proposition
before the release, the keys have been in contact with the man, and
the man has been in contact with the elevator
at the moment of release, velocity of the keys is equal to velocity of
the elevator
after the release, nothing is touching the keys
after the release, the keys are in freefall

6

after the release, the keys’ acceleration is not equal to the elevator’s
acceleration
after the release, the keys’ velocity is not equal to the elevator’s velocity

7

the keys touch the ceiling of the elevator

Justification
given
bodies in contact over a time interval
have same velocities
given
if there is no any contact then the body
is in freefall
*elevator is not in freefall
if initial velocity is the same and accelerations are different the final velocities
are different
if the keys’ velocity is smaller than the
elevator’s velocity, the keys touch the
ceiling

Figure 2: An informal proof of the excerpt “The keys would be pressed against the ceiling of the elevator” (From
the essay in Figure 1). The buggy assumption is preceded by an asterisk.
tive theorem proving framework and the heuristics we
developed that aim at maximization of the plausibility
of the proof as a model of the student’s reasoning and
the utility of the proof for the tutoring system. The
measure of plausibility is evaluated with respect to (a)
the misconceptions that were identified as present in the
essay by the prover and by a human expert, and (b) the
proof as a whole. The utility for the tutoring task can be
interpreted in terms of relevance of the tutoring actions
(triggered by the proof) to the student’s essay, whether
the proof was plausible or not. We also discuss the assumptions of cognitive economy and concept-level consistency that we make about the student in relation to the
plausibility of the model. Next we summarize previous
evaluations of the Why2-Atlas system (VanLehn et al.,
2002) and of an early version of the abductive reasoning
engine (Jordan, Makatchev, & VanLehn, 2003). Finally
we conclude with a section on our future work.

Knowledge Representation for Students’
Reasoning about Qualitative Physics
Envisionment and idealization
Generating an internal (mental) representation plays a
key role for both novice and expert problem solving
(Ploetzner, Fehse, Kneser, & Spada, 1999; Reimann &
Chi, 1989). (Reimann & Chi, 1989) describes the internal representation in terms of “objects, operators, and
constraints, as well as initial and final states.” This notion of internal representation overlaps with envisionment, which is defined in qualitative physics problem
solving (de Kleer, 1990) as a sequence of events described
in the problem or implied by the description. A further
step, translating the envisionment into the domain terminology (bodies, forces, motion properties) is referred
to as idealization in (Makatchev, Jordan, & VanLehn,
2004a).
For the problem in Figure 1, for example, a possible
envisionment is: (1) the man is holding the keys (elevator
is falling); (2) the man releases the keys; (3) the keys
move up with respect to the man and hit the ceiling of
the elevator. The idealization would be:

167

Bodies: Keys, Man, Elevator, Earth.
Forces: Gravity, Man holding keys
Motion: Keys’ downward velocity is smaller than the
downward velocity of the man and the elevator.
Many misconceptions that students have are rooted
in the envisionment and idealization (Ploetzner et al.,
1999). To make the task of representing possible correct and erroneous envisionments feasible we restrict ourselves to problems with few plausible envisionments. The
rules of mechanics, which rely mostly on the formal domain terminology, are augmented by rules for reasoning
about most common envisionments, which use a looser
language. Further we briefly describe these representations.

Qualitative mechanics ontology
The ontology for the subset of qualitative mechanics
that the system addresses consists of bodies (e. g., keys,
man), agents (air), phenomena (e. g., gravity, friction),
and conventional physical quantities (e. g., force, velocity, position). To adequately represent justifications,
we also have representations for physics laws (Newton’s
First Law) and basic algebraic expressions (F = ma).
While internally the reasoning is done within a coordinate system that is fixed for each problem (for example,
horizontal axis x directed to the right and vertical axis y
directed up), a student’s reasoning can be independent
of coordinate system choice, operating instead in relative terms (up, down, in front of). The representation
and corresponding translation rules are described in the
following section.
Logical constants and variables, corresponding to bodies, agents, and quantities are associated with a sort symbol. Sorts are partially ordered by a natural subset order. Domains of the predicate symbols are restricted to
certain sorts (so that each argument position has a corresponding sort symbol). These associations and constraints constitute an order-sorted signature (Walther,
1987).

Description
quantity
identifier
body (or two bodies in case of force)
axial component or not
qualitative derivative of the magnitude
quantitative derivative of the magnitude
zero or non-zero magnitude
quantitative magnitude
sign for axial component
quantitative direction
qualitative derivative of the direction
beginning of time interval
end of time interval

Sort
Quantity1b
Id
Body
Comp
D-mag
D-mag-num
Mag-zero
Mag-num
Dir
Dir-num
D-dir
Time
Time

Table 1: Slots of a vector quantity of sort Quantity1b.
Time is represented using time instants as basic primitives. Time intervals are denoted as a pair (ti , tj ) of
instants. This and the order relation before on time
points enables us to reason about a reasonably rich subset of the mechanics domain.
Argument slots and an order-sorted signature for a
predicate representing a vector quantity that involves a
single body (for example velocity, total-force) are
shown in Table 1.
A number of relation predicates are used to specify
various algebraic and logical relations between physical
quantities (see Table 2).
Two bodies can also be related via a state of
contact with possible fillers detached, attached, and
moving-contact (for the case of relative motion between
bodies in contact).

Rules
The rules cover correct reasoning at the formal domain
level of an idealized problem (“zero acceleration implies
constant velocity”), buggy reasoning at this same level
(“zero force implies decreasing velocity”), and some common relevant aspects of the idealization and envisionment stages (“if axis y is directed upward and velocity
is vertical and positive then velocity is upward”).
The rules are represented as extended Horn clauses,
namely the head of the rule is an atom or a conjunction of multiple atoms. Further details of the knowledge
representation are covered in (Makatchev et al., 2004a).

Weighted Abductive Theorem Proving
Order-sorted abductive logic programming
framework
Similar to (Kakas, Kowalski, & Toni, 1998) we define
the abductive logic programming framework as a triple
hT, A, Ii, where T is the set of givens and rules, A is
the set of abducible atoms (potential hypotheses) and
I is a set of integrity constraints. Then an abductive
explanation of a given set of sentences G (goals) is (a)
a subset ∆ of abducibles A such that T ∪ ∆ ` G and
T ∪ ∆ satisfies I, and (b) the corresponding proof of G.
The set ∆ is assumptions that explain the goals G. Since
an abductive explanation is generally not unique, various

168

criteria can be considered for choosing the most suitable
explanation (see Section “Proof search heuristics”).
An order-sorted abductive logic programming framework hT 0 , A0 , I 0 i is an abductive logic programming
framework with all atoms augmented with the sorts of
their argument terms (so that they are sorted atoms)
(Makatchev et al., 2004a). Assume the following notation: a sorted atom is of the form p(x1 , . . . , xn ) :
(τ1 , . . . , τn ), where the term xi is of the sort τi . Then, in
terms of unsorted predicate logic, formula ∃x p(x) : (τ )
can be written as ∃x p(x) ∧ τ (x). For our domain we restrict the sort hierarchy to a tree structure that is naturally imposed by set semantics and that has the property
∃x τi (x) ∧ τj (x) → (τi 4 τj ) ∨ (τj 4 τi ) where τi 4 τj is
equivalent to ∀x τi (x) → τj (x).
Tacitus-lite+ uses backward chaining with the ordersorted version of modus ponens:
q(x0 , z 0 ) : (τ5 , τ6 )
p(x, y) : (τ1 , τ2 ) ← q(x, z) : (τ3 , τ4 )
τ5 4 τ 3 , τ 6 4 τ 4
p(x0 , y 0 ) : (min(τ5 , τ1 ), τ2 )

Proof search heuristics
The aim of the proof search heuristics is to quickly find
a proof that optimizes a measure of utility of the proof
for tutoring applications and a measure of plausibility of
the proof as a model of a student’s reasoning. A highly
plausible proof has a high value for its utility measure
since it potentially allows the tutoring system to generate
feedback that is more relevant to the student’s actual
mental state. However a less plausible proof would have
the same utility measure if it results in the same tutoring
action as a more plausible proof. In fact, we would prefer
a less plausible proof over the more plausible proof, their
utility measures being same, if the former takes less time
to compute.
The plausibility measure is based on two cognitive assumptions. The first assumption, cognitive economy, can
be interpreted in the context of the abductive proofs
as a preference for a simpler proof structure (for example a smaller proof) and a smaller cost for the propositions that have to be assumed. The second assumption,
concept-level consistency, is based on the fact that even
young children are unlikely to make mistakes in tasks involving taxonomic categories (Chi & Ceci, 1987). Thus
we assume that, while proofs can have errors, errors in
categorical and taxonomic reasoning are less plausible.
For example, the consistency constraints that we enforce
for proofs prevent propositions such as “velocity of the
keys is increasing” and “velocity of the keys is constant”
from appearing within the same proof.
A proof is considered sufficiently cheap if the total cost
of its assumed atoms is below a certain threshold. The
cost is computed for each proposition of the proof via the
following procedure. First, costs are uniformly assigned
to the goal atoms (observations), namely the propositional representation of the student’s essay. Conjunct
atoms pi in the body of a rule have pre-assigned weights

Relation
non-equal
before
rel-position
compare
compare-dir
dependency

1st and 2nd arguments
any terms
Time
Body
Mag-num or D-mag-num of any scalar or vector quantity
Dir-num of any vector quantity
any terms

3rd argument
Rel-location
Ratio
Rel-dir
Rel-type

4th argument

Difference
time interval

Table 2: Relations.

Evaluation

wi (Stickel, 1988):
1
pw
1

∧ ··· ∧

m
pw
m

→ r1 ∧ · · · ∧ r n .

If this rule is used to prove a goal g by unifying it
with atom rj , then the cost of assuming pi , 1 ≤ i ≤ m,
is computed according to the following cost propagation
formula: cost(pi ) = cost(g) · wi . The cost of the proof is
the total cost of all assumed atoms.
A weighted abductive proof for the student’s statement “The keys would be pressed against the ceiling of
the elevator” is shown in Figure 3. Total cost of the proof
is 0.15, the cost of its only assumption. Incidentally, the
proof indicates a possible presence of the wrong assumption “The elevator is not in freefall,” which is made by
the student likely due to a wrong interpretation of a
problem given.
Since the cost of a proposition is a penalty for assuming it without a proof, it can also be interpreted as a
degree of disbelief in the proposition. This interpretation suggests that more general existentially quantified
propositions should be cheaper to assume than more specific propositions. The mechanism for such cost adjustment is implemented in the most recent version of the
theorem prover.
Various rule choice heuristics have the aim of finding
a sufficiently cheap proof of a small size. Generally, if
atoms in the head of the rule are unifiable with a subset of goals then application of such a rule will result in
the subset of atoms being removed from the goal list.
If a rule has atoms in its body that are unifiable with
the goals, then the new subgoals will be factored with
the unifiable goals, namely only the most specific of the
unifiable atoms will be left on the goal list. These nuances imply that proving via rules that have heads and
bodies that are unifiable with larger subsets of goals lead
to a faster reduction of the goal list and consequently a
smaller resultant proof.
In addition, a set of atoms can be cross-referenced
via shared variables. The cross-reference graph encodes
a large amount of semantics for the proposition corresponding to the respective set of atoms. One of the rule
choice heuristics currently being evaluated in the theorem prover is based on the similarity between the graph
of cross-references between the propositions in a candidate rule and the graph of cross-references between the
set of goals. The metric for the match between two labeled graphs is computed as the size of the largest common subgraph using the decision-tree-based algorithm
proposed in (Shearer, Bunke, & Venkatesh, 2001). For
further details on the proof search heuristics we refer the
reader to (Makatchev, Jordan, & VanLehn, 2004b).

169

Although students in a baseline evaluation of the Why2Atlas system showed significant learning gains (VanLehn
et al., 2002), the sentence-level representations of the
students’ essays produced by the system, that are the
input to Tacitus-lite+, were too sparse for any misconceptions to be correctly identified. To evaluate Tacituslite+ we developed a test suite of 45 student generated
essays in which we manually corrected and completed the
input generated by the system for input to Tacitus-lite+
and annotated the misconceptions expressed in each essay that Tacitus-lite+ should identify. The student essays were randomly selected from those collected during
the baseline evaluation and from subsequent experiments
with students and human tutors. In the 45 essays of the
test suite, three essays have two misconceptions each,
eight essays have one misconception each, and the rest
of the essays don’t have any misconceptions from the
list of 54 misconceptions that could arise for the training problems according to our physics experts.
There are two types of evaluations of interest to us
for the abductive theorem prover: (1) the accuracy of
the misconceptions revealed by the proofs and (2) the
accuracy of the proofs as models of the students. We
summarize here the results of both for an earlier version
of Tacitus-lite+, as described in (Jordan et al., 2003),
and plan to repeat both in the near future for the newer
version described in this paper.
To assess the accuracy of the misconceptions identified
by the theorem prover, we compare the misconceptions
revealed by the proofs of each essay to those annotated
for each test suite essay. We accumulated the number
of true positives TP, false positives FP, true negatives
TN, and false negatives FN for each essay; and from this
computed recall TP/(TP+FN), precision TP/(TP+FP),
and positive false alarm rate FP/(FP+TN). In addition,
we calculated these measures for the theorem prover’s
results at various proof cost thresholds to see how the
performance changes as we move closer toward building
a complete proof. The results are shown in Figure 4.
The recall increases from 0 at a proof cost of 1 (where
everything is assumed without proof) to 62% at a proof
cost threshold of 0.2. As the recall increases, the precision degrades but then levels off. These results mean
that the earlier theorem prover can help to reveal up to
62% of the misconceptions that a human would recognize, but at the cost of identifying some misconceptions
that are not justified by the essays. We consider recall to
be the more important measure for misconceptions since
it is important to find and address the misconceptions

Student said:

keys and ceiling are in contact (1)

7

Bodies in contact have same positions
final pos(keys) = final pos(ceiling) (1)
If diff. initial pos. and some diff. velocities then same final pos.
initial pos(keys) != initial pos(ceiling) (0.5)

vel(keys) after release != vel(ceiling) (0.5)

(given)

Ceiling and elevator move together
vel(keys) after release != vel(elevator) (0.5)

6

vf=vi+a*t
initial vel(keys) = initial vel(elevator) (0.25)

2

acc(keys) after release != acc(elevator) (0.25)

Freefall acceleration is the same for all bodies

If fixed contact then same velocity
initially, man, keys,
elevator are in contact (0.25)
(given)

5

1

elevator is not in freefall (0.15)

keys are in freefall (0.1) 4

(wrong assumption)

If no contact then freefall
after release, keys are not in contact with anything (0.1)

3

(given)

Figure 3: A weighted abductive proof of the proposition representing the excerpt “The keys would be pressed against
the ceiling of the elevator.” Rule names are in italics; arrows are in the direction of abductive inference; costs of the
propositions are in parenthesis; the references to the steps in Figure 2 are in bold. Total cost of the proof is 0.15.

Figure 4: Recall, precision and false alarm measures as
proof cost threshold decreases.

that are expected to be obvious to a human tutor. The
positive false alarm is quite low and although our goal is
to reduce this value as close to 0 as possible, we consider
a high recall to be a higher priority as we expect that
it is more important not to miss misconceptions. On
the other hand, some possible drawbacks of not also trying to lower the positive false alarms are inadvertently
strengthening the reasoning that leads to a misconcep-

170

tion and a loss of student motivation and cooperation if
the student perceives the system is too frequently giving
inappropriate feedback.
While these results are encouraging, we expect that
the recent improvements we’ve made to Tacitus-lite+,
along with additional testing and fine-tuning of rules,
will further improve the results. In addition, an evaluation of misconceptions revealed is only a coarse measure
of the quality of the proofs generated. To determine
the fitness of the theorem prover’s modeling to support
assessments of completeness, we must also consider the
accuracy of the proof structure generated. Assessing the
accuracy of the proof structure is more difficult because
the proofs must be hand verified. It is difficult to create a reliable gold standard against which to evaluate
the accuracy of proofs for essays and the reasons for any
inaccuracy. This is because, in general, language in context gives rise to many inferences (Austin, 1962; Searle,
1975). For this assessment we judged whether the lowest
cost proofs generated for 15 of the test suite essays was a
plausibly good, satisfactory or bad model of the student
essay. As shown in Table 3, as the proof cost threshold
decreased and consequently the number of assumptions
made fell, the number of good proofs increased and the
number of bad ones fell to 0.

Conclusions and Future work
In this paper we described an approach to modeling
of a student’s reasoning about qualitative physics problems by treating the student’s essay as an observation,
the problem statement as a set of given facts, and us-

Threshold
good
satisfactory
bad

0.8
7
4
4

0.6
7
4
4

0.4
10
4
1

0.2
11
4
0

& de Kleer, J. (Eds.), Readings in Qualitative Reasoning about Physical Systems, pp. 40–45. Morgan
Kaufmann, San Mateo, California.
Hobbs, J., Stickel, M., Martin, P., & Edwards, D. (1988).
Interpretation as abduction. In Proc. 26th Annual
Meeting of the ACL, Association of Computational
Linguistics, pp. 95–103.
Jordan, P., Makatchev, M., & VanLehn, K. (2003). Abductive theorem proving for analyzing student explanations. In Proceedings of International Conference on Artificial Intelligence in Education, pp.
73–80, Sydney, Australia. IOS Press.
Kakas, A., Kowalski, R. A., & Toni, F. (1998). The
role of abduction in logic programming. In Gabbay, D. M., Hogger, C. J., & Robinson, J. A.
(Eds.), Handbook of logic in Artificial Intelligence
and Logic Programming, Vol. 5, pp. 235–324. Oxford University Press.
Makatchev, M., Jordan, P. W., & VanLehn, K. (2004a).
Abductive theorem proving for analyzing student
explanations to guide feedback in intelligent tutoring systems. To appear in Journal of Automated
Reasoning, Special issue on Automated Reasoning
and Theorem Proving in Education.
Makatchev, M., Jordan, P. W., & VanLehn, K. (2004b).
Modeling students’ reasoning about qualitative
physics: Heuristics for abductive proof search. In
Proceedings of Intelligent Tutoring Systems Conference, LNCS. Springer. To appear.
Ploetzner, R., Fehse, E., Kneser, C., & Spada, H. (1999).
Learning to relate qualitative and quantitative
problem representations in a model-based setting
for collaborative problem solving. The Journal of
the Learning Sciences, 8, 177–214.
Reimann, P., & Chi, M. T. H. (1989). Expertise in complex problem solving. In Gilhooly, K. J. (Ed.), Human and machine problem solving, pp. 161–192.
Plenum Press, New York.
Searle, J. R. (1975). Indirect Speech Acts. In Cole, P., &
Morgan, J. (Eds.), Syntax and Semantics 3. Speech
Acts. Academic Press. Reprinted in Pragmatics.
A Reader, Steven Davis editor, Oxford University
Press, 1991.
Shearer, K., Bunke, H., & Venkatesh, S. (2001). Video indexing and similarity retrieval by largest common
subgraph detection using decision trees. Pattern
Recognition, 34 (5), 1075–1091.
VanLehn, K., Jordan, P., Rosé, C., Bhembe, D., Böttner,
M., Gaydos, A., Makatchev, M., Pappuswamy, U.,
Ringenberg, M., Roque, A., Siler, S., & Srivastava, R. (2002). The architecture of Why2-Atlas:
A coach for qualitative physics essay writing. In
Proceedings of Intelligent Tutoring Systems Conference, Vol. 2363 of LNCS, pp. 158–167. Springer.
Walther, C. (1987). A many-sorted calculus based on resolution and paramodulation. Morgan Kaufmann,
Los Altos, California.

Table 3: Evaluation of plausibility of proofs generated
for different proof cost thresholds.
ing an abductive proof of this observation as a plausible approximation of the student’s reasoning. Abductive proofs provide an intuitively natural representation
for logical relations between the arguments of the essay. The problem of insufficient coverage of the domain and common-sense knowledge—one of the difficulties that formal methods face when applied to natural
language text analysis—is alleviated by allowing proofs
to include assumptions, namely propositions that cannot be proven. Weighted abduction provides a facility
to rate such proofs by assigning costs to their respective
sets of assumptions. The weighted abductive theorem
prover has been implemented and evaluated with respect
to plausibility of proofs as models of students’ reasoning.
There are a number of challenges still to address. One
is to handle various degrees of formalism in the input
representations of the student’s language. For example,
if a student says “throw,” the current representation input to Tacitus-lite+ is “apply an upward vertical force.”
But the student’s actual lexical choices need additional
reasoning relative to the model of the student in order to
determine whether the correct formal representation is
plausible for the student. Otherwise, the student is credited with understanding more about physics than may be
plausible. So the goal is to take over more of the natural
language semantic interpretation process within Tacituslite+. The favorable evaluation results we have obtained
so far make it more promising that such a move will be
successful. Other challenges include increasing the coverage of the rule-base, further improving the efficiency
of the theorem prover, and further improved consistency
checking.

Acknowledgments
This work was funded by NSF grant 9720359 and ONR
grant N00014-00-1-0600. We thank the entire Natural
Language Tutoring group, in particular Michael Ringenberg and Roy Wilson for their work on Tacitus-lite+,
and Brian ‘Moses’ Hall and Michael Böttner for their
work on knowledge representation and rules.

References
Austin, J. L. (1962). How to Do Things With Words.
Oxford University Press, Oxford.
Chi, M. T. H., & Ceci, S. J. (1987). Content knowledge:
Its role, representation and restructuring in memory development. Advances in Child Development
and Behavior, 20, 91–142.
de Kleer, J. (1990). Multiple representations of knowledge in a mechanics problem-solver. In Weld, D. S.,

171

Modeling Hinting Strategies for Geometry
Theorem Proving
Noboru Matsuda1 and Kurt VanLehn
Intelligent Systems Program
University of Pittsburgh
mazda@pitt.edu, vanlehn@cs.pitt.edu
Abstract. This study characterizes hinting strategies used by a human tutor to
help students learn geometry theorem proving. Current tutoring systems for
theorem proving provide hints that encourage (or force) the student to follow a
fixed forward and/or backward chaining strategy. In order to find out if human
tutors observed a similar constraint, a study was conducted with students proving geometry theorems individually with a human tutor. When working successfully (without hints), students did not consistently follow the forward and/or
backward chaining strategy. Moreover, the human tutor hinted steps that were
seldom ones that would be picked by such tutoring systems. Lastly, we discovered a simple categorization of hints that covered 97% of the hints given by the
human tutor.

1 Introduction
As a first step in designing an improved intelligent tutoring system for geometry theorem proving, we sought to characterize the hints given by a human tutor to students
trying to prove geometry theorems. Little is known about the mechanism of effective
hinting strategy [1, 2], but current tutoring systems have relatively simple, inflexible
hinting policies. Some tutoring systems demand that the students follow a prescribed
problem solving strategy, such as forward or backward chaining [3], so their hints are
always aimed at the next step taken by the prescribed strategy. Other tutoring systems
accept any correct inference even if it is not on an ideal solution path [4], but when a
student reaches an impasse, the tutor provides a hint on the next step that is a strict
backward or forward inference no matter what assertions the student has made so far.
Not only are the steps targeted by hints often quite inflexibly chosen, the hints themselves are usually a simple human-authored sequence that proceeds from general hints
to specific hints, and usually culminates in a “bottom out” hint that describes exactly
what the student should enter. We hypothesize that human tutors have less rigid hinting policies, and this might cause increased learning. This paper tests the first conjecture by characterizing the hinting strategy of a single human tutor.

1

This research was supported by NSF Grant 9720359.

P. Brusilovsky et al. (Eds.): UM 2003, LNAI 2702, pp. 373–377, 2003.
© Springer-Verlag Berlin Heidelberg 2003

374

N. Matsuda and K. VanLehn

2 The Study
Nine students were randomly selected from a Japanese middle school. Three geometry
proof-problems were used. Two problems were construction problems, which require
students to draw additional lines by compasses and straightedges to complete a proof.
Each student solved problems individually while thinking out aloud. The tutor was
asked to provide hints only when the students could not otherwise proceed. The sessions were videotaped and transcribed. The students’ utterances were segmented so
that a single segment corresponds to a proof step or a response to the tutor’s assistance. The tutor’s utterances were segmented so that a single segment corresponds to a
hint. The following sections present an analysis of these protocol data.

3 Students’ Problem Solving Strategies
In order to determine whether students followed the forward and/or backward chaining
strategies prescribed by tutoring system for theorem proving, we located individual
students’ utterances in a proof tree and observed a pattern of progress in their proof.
As an example, Fig. 1 shows a chronological progress of a student’s reasoning. The
goal to be proven is shown at the top of the tree, with the givens at the bottom. A
branching link shows a conjunctive justification. Nodes with a rectangle show the
propositions that this student asserted. The numbers on their shoulder show the order
of assertion. Since the proposition Bx//AP is a premise for both ∠BxM=∠APM and
∠PAM=∠MBx, the first assertion is located on two places.
As shown in the figure, this student built up a proof neither in a strict forward
chaining nor in a strict backward chaining manner. Rather she seems to assert facts
(i.e., propositions) that were eventually recognized. This opportunistic ordering is not
peculiar to this particular student. All students participating in our study showed the
same behavior.

4 Topics of Hint Events
We observed 31 hint events, each consisting of a sequence of hints on the same topic.
They were categorized into 4 types of hint events; (a) 10 hint events for a next step,
(b) 14 hint events for a justification of proposition that the student had just mentioned,
(c) 3 hint events for a geometry construction, (d) 1 hint event to get started on a proof,
and (e) 3 hint events that do not fall under any of these types. Because tutoring systems often follow rigid policies when selecting the target step for a next-step hint, we
analyzed the 10 next-step hint events in more detail.
If we define a step to be applying a postulate to some premises and producing a
conclusion, then the human tutor always provided a next-step hint on a single step (as
opposed to discussing a generic strategy and no steps). Steps can be categorized by
which elements (premises, conclusions) have been mentioned already by the student

Modeling Hinting Strategies for Geometry Theorem Proving

375

or tutor. In particular, let us use the first two letters of the classification to show
whether the conclusion is asserted (C1) or not asserted (C0), and the second two letters
for whether all the premises are asserted (Pa), none are asserted (P0), only some of the
premises are asserted (Ps), or all but one premises are asserted (P1). Table 1 shows
the results of applying this classification to the protocol data. It indicates the number
of times a step was chosen as target (first row) and the number of steps available at the
time a next-step help event began (second row).
µAPM = µBQM
µBQM = µBxM

µBxM = µAPM

Isosceles Tri.

Bx // AP

1

2
BQ = Bx

5
AP = BQ

Bx = AP

DPAM  DBxM

µPAM=µMBx

3

µAMP=µBMx

AM=MB

4

1
Bx // AP

M midpoint AB

Vertical Angle

Fig. 1. A typical progress of student’s input over a proof tree

The human tutor always chose either C0Pa or C0P1 as a target of a next-step hint
event. Several existing tutoring systems, such as GPT [3], ANGLE [4], and CPT [5],
choose target steps that would be picked by forward or backward chaining, which
means either C0Pa or C1P0. Clearly, the human tutor’s target steps seldom agreed
with those that would be chosen by these tutoring systems.
Table 1. Frequency of motivation of hinting in the ‘next step’ hint events
6WDWHRIDVVHUWLRQVIRUDVWHS

&KRLFH

2FFXUHQFH

&3D

&3

&3D

&3V

&3

&3

























376

N. Matsuda and K. VanLehn

5 A Classification of Hints
So far, we have discussed only hint events and their targets, but not the hints that comprise hint events. In order to understand the structure of human tutoring better, this
section categorizes the hints from the hint-events for justifications, next-steps, and the
first step of the proof. There were 90 hints observed in these 25 hint events.
The individual hints were organized into a Cartesian product with respect to the focus and format of the hint. There are four categories regarding the focus of hint: (1) a
hint on a whole application of a postulate (e.g., “Remember if two sides of triangle are
equal, then the base angles are also equal”), (2) a hint on a premise of a postulate
application (e.g., “If you want to prove these two angles are equal, what should be true
among these two segments?”), (3) a hint on a conclusion of a postulate application
(e.g., “What can you conclude about the base angles in a triangle with two equal
sides?”), and (4) a hint on a proposition apparently involved in a postulate application
but not mentioning it explicitly (e.g., “Can you say anything about these two segments?”).
We observed five different forms of hint; (1) a direct exhibition, (2) a question
asking a whole postulate/proposition, (3) a question asking about a relationship in the
proposition, (4) a question asking about the elements involved in a proposition, and (5)
mentioning or pointing to a related configuration in the problem figure.
As an illustration of this Cartesian product categorization, Table 2 shows all possible hints for a proof step that invokes the theorem of isosceles triangle (i.e., if two
sides of a triangle are equal, then the base angles are also equal).
We could classify 87 hints (out of 90; 97%) with the coding schema shown in
Table 2. The parenthesized numbers in Table 2 shows the number of hints in each
category.
Table 2. The type of hints for a next-step hint
4XHVWLRQ

Form

([KLELW

3RLQWLQJ

Focus

:KROHSURSRVLWLRQ

5HODWLRQ

(0)

(0)

(3)

(OHPHQW

(0)

(16)

&DQ\RXVD\DQ\WKLQJ
:KROH

,I$%

DSSOLFDWLRQ

$&WKHQ

$%&

$&%

:KDWFDQ\RXGR

DERXWVHJPHQWV$%

QRZ"

DQG$&DQGDQJOHV



/RRNDWWKLVWULDQJOH

$%&DQG$&%"

(2)
3UHPLVHRI
DSSOLFDWLRQ

,WLVVXIILFLHQWWRVKRZ
$%

$&WRFRQFOXGH
$%&

$&%

(4)
:KDWVKRXOG\RX
WRFRQFOXGH$%&

&RQFOXVLRQ
RI
DSSOLFDWLRQ

*LYHQWKDW$%

$&

$&%1RZ

ZKDWVKRXOGEHWUXH

(6)

FRQFOXGHZKHQ$%

HTXDO

DQG$&DUHHTXDO"

(0)
:KDWLVNQRZQ"

PXVWEHHTXDOWR
FRQFOXGH
$%&

(0)


$&%"

(5)
:HNQRZ$%

$%&DQG$&%DUH

$%DQG$&DUHHTXDO

$%&

DPRQJ$%DQG$&"

:KDWFDQ\RX

(21)
3URSRVLWLRQ



$&%"

(1)

(2)

(0)

<RXZDQWWRFRQFOXGH :KLFKWZRVHJPHQWV

SURYHZKHQ\RXZDQW

(0)

$&6R :HNQRZ$%

(0)

$&6R

ZKDWFDQZHFRQFOXGH ZKLFKWZRDQJOHVFDQ
ZLWK$%&DQG

\RXFRQFOXGHWREH

$&%"

HTXDO"

(24)

(0)

FDQ\RXVD\DQ\WKLQJ

:KLFKVHJPHQWLV

DERXW$%DQG$&"

HTXDOWR$%"



(3)
/RRNDW$%DQG$&

Modeling Hinting Strategies for Geometry Theorem Proving

377

6 Conclusion
The analysis of protocol data gathered from students in middle school has shown several aspects of hinting in a learning context where the tutor acts as a helper for students to overcome an impasse.
We found that students tend to make opportunistic assertions that follow neither a
strict forward nor backward chaining order.
Accepting their reasoning style might be beneficial for students, but it requires that
the tutoring system be more complex so that it can provide an appropriate hint depending on the students’ reasoning. We discovered that human tutors prefer to hint
steps where one or more premises have been mentioned, although not necessarily
recently, and the conclusion has not been mentioned. It is not clear yet how the tutor
decides which step to pick when there are several that meet this criterion. The human
tutor’s policy for choosing target steps does not correspond to the policies of existing
tutoring systems for theorem proving, but it might be easy to modify such systems to
follow the human tutor’s policy.
Moreover, 97% of the hints observed in our study fell into a simple Cartesian product categorization. This categorization appears amenable to incorporation in the hint
generation module of a tutoring system.

References
1. Hume, G., J. Michael, A. Rovick, and M. Evens, Hinting as a tactic in one-on-one tutoring.
Journal of the Learning Sciences, 1996. 5(1): p. 23–47.
2. DiPaolo, R.E., A.C. Graesser, D.J. Hacker, and H.A. White, Hints in Human and Computer
Tutoring, in The impact of media on technology of instruction, M. Rabinowitz, Editor.
2002, Erlbaum: Mahwah, NJ.
3. Anderson, J.R., C.F. Boyle, and G. Yost, The geometry tutor. Proceedings of the International Joint Conference on Artificial Intelligence, 1985: p. 1–7.
4. Koedinger, K.R. and J.R. Anderson, Reifying implicit planning in geometry: Guidelines for
model-based intelligent tutoring system design, in Computers as cognitive tools, S.P. Lajoie
and S.J. Derry, Editors. 1993, Lawrence Erlbaum Associates: Hillsdale, NJ. p. 15–45.
5. Scheines, R. and W. Sieg, Computer Environments for Proof Construction. Interactive
Learning Environments, 1994. 4(2): p. 159–169.

DT Tutor: A Decision-Theoretic, Dynamic Approach
for Optimal Selection of Tutorial Actions
R. Charles Murray∗ and Kurt VanLehn
Intelligent Systems Program & Learning Research and Development Center
University of Pittsburgh, Pittsburgh, PA 15260
{rmurray,vanlehn}@pitt.edu
Abstract. DT Tutor uses a decision-theoretic approach to select tutorial actions
for coached problem solving that are optimal given the tutor’s beliefs and objectives. It employs a model of learning to predict the possible outcomes of each
action, weighs the utility of each outcome by the tutor’s belief that it will occur,
and selects the action with highest expected utility. For each tutor and student
action, an updated student model is added to a dynamic decision network to reflect the changing student state. The tutor considers multiple objectives, including the student’s problem-related knowledge, focus of attention, independence, and morale, as well as action relevance and dialog coherence. Evaluation
in a calculus domain shows that DT Tutor can select rational and interesting
tutorial actions for real-world-sized problems in satisfactory response time. The
tutor does not yet have a suitable user interface, so it has not been evaluated
with human students.

1

Introduction

Tutoring systems that coach students as they solve problems often emulate the turn
taking observed in human tutorial dialog [7, 15]. Student turns usually consist of entering a solution step or asking for help. The tutor’s main task can be seen as simply
deciding what action to take on its turn. Tutorial actions include a variety of action
types, including positive and negative feedback, hinting, and teaching. The tutor must
also decide the action topic, such as a specific problem step or related concept. DT
Tutor’s task is to select the optimal type and topic for each tutorial action.
How to select optimal tutorial actions for coached problem solving has been an
open question. A significant source of difficulty is that much of the useful information
about the student is not directly observable. This information concerns both the student’s cognitive and emotional state. Compounding the difficulty, the student’s state
changes over the course of a tutoring session.
Another complication is that just what constitutes optimal tutoring depends upon
the tutorial objectives. A tutor’s objectives normally include various student-centered
goals and may also include dialog objectives and action type preferences. Furthermore, the tutor may have to balance multiple competing objectives.
DT Tutor uses decision-theoretic methods to select tutorial actions. The remainder
of the Introduction describes the basis of our approach, DT Tutor’s general architecture, and prior work. Subsequent sections describe DT Tutor in more detail, a preliminary evaluation, future work and conclusions.
∗

Research supported by ONR’s Cognitive Science Division, grant number N0014-98-1-046h7.

G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, pp. 153-162, 2000.
 Springer-Verlag Berlin HeidTelberg 2000

G. Gauthier, C. Frasson, K. VanLehn (Eds.): IST 2000, LNCS 1839, pp. 153-162, 2000
 Springer-Verlag Berlin Heidelberg 2000

154
1.1

R. Charles Murray and Kurt VanLehn

Belief and Decision Networks

Probability has long been the standard for modeling uncertainty in diverse scientific
fields. Recent work with belief network (equivalently, Bayesian network) algorithms
has made modeling complex domains using probabilistic representations more feasible. Unfortunately, belief network inference is still NP-hard in the worst case. However, many stochastic sampling algorithms have an anytime property that allows an
approximate result to be obtained at any point in the computation [4].
DT Tutor represents the tutor’s beliefs about the student’s problem-related knowledge using a belief network obtained directly from a problem solution graph, a hierarchical dependency network representing solutions to a problem [2, 11]. Nodes in the
graph represent (1) problem steps, and (2) domain rules licensing each step. Problem
steps include the givens and every goal and fact along any path towards the solution.
We currently model incorrect steps as errors. Arcs represent dependence between
nodes. For instance, knowledge of a step depends on knowledge of both its antecedent
steps and the rule required to derive it. In belief network form, the nodes represent the
tutor’s beliefs about problem-related elements of the student’s cognitive state and arcs
represent conditional dependence between the elements.
Nodes within a belief network represent variables whose values are fixed. However, a student’s mental state and the problem solution state change over the course of
a tutoring session. To represent variables that change over time, it is more accurate to
use a separate node for each time. Dynamic belief networks (DBNs) do just that. For
each time in which the values of variables may change, a new slice is created. Each
slice consists of a set of nodes representing values at a specific point in time. Rather
than fixed time intervals, slices can be chosen so that each corresponds to the student
model after a student or tutor action. Nodes may be connected to nodes within the
same or earlier slices to represent the fact that a variable's value may depend on concurrent values of other variables (synchronic influences) and on earlier values of the
same and other variables (diachronic influences). The evolution of a DBN can be represented while keeping in memory at most two slices at a time [10].
Decision theory extends probability theory to provide a normative theory of how a
rational decision-maker should behave [13]. Utilities are used to express preferences
among possible future states of the world. To decide among alternative actions, the
expected utility of each alternative is calculated by taking the sum of the utilities of all
possible future states of the world that follow from that alternative, weighted by the
probabilities of those states occurring. Decision theory holds that a rational agent
should choose the alternative with the maximum expected utility. A belief network,
which consists entirely of chance nodes, can be extended into a decision network
(equivalently, an influence diagram) by adding decision and utility nodes along with
appropriate arcs [13]. For tutoring systems, decision nodes could represent tutorial
action alternatives, chance nodes could represent possible outcomes of the actions,
and utility nodes could represent the tutor’s preferences among the possible outcomes.
A dynamic decision network (DDN) is like a DBN except that it has decision and
utility nodes in addition to chance nodes. DDNs model decisions for situations in
which decisions, variables or preferences can change over time. Just as for DBNs,
simple algorithms exist to represent the evolution of a DDN while keeping in memory
at most two slices at a time [10].

DT Tutor

1.2

155

General Architecture

Our basic approach is to use a DDN to implement most of the intelligent, non-userinterface part of DT Tutor. The DDN is formed from dynamically created decision
networks. These networks are called tutor action cycle networks (TACNs) because
they each represent a single cycle of tutorial action, where a cycle consists of
• deciding a tutorial action and carrying it out,
• observing the next student action, and
• updating the student model based on these two actions.
TACNs consist of three slices, as illustrated in Figure 1. A TACN is used both for
deciding the tutor’s action and for updating the student model. When deciding the tutor’s action, slice 0 represents the tutor’s beliefs about the student’s current state.

5NKEG 

5 VWFG PV
/ Q FGNA 

5NKEG 

5NKEG 

6W VQ T
#E VKQPA 

5 VWFG PV
/ Q FGNA 

5 VWFG PV
#E VKQPA 

5 VWFG PV
/ Q FGNA 

7VKNKV[A 

Fig. 1. Tutor action cycle network, high-level overview

Slice 1 represents the tutor’s possible actions and the influence of those actions on the
tutor’s beliefs about the student. Slice 2 represents the student’s possible actions and
the influence of those actions on the tutor’s beliefs about the student. In other words,
slice 0 represents the current student state and the other slices represent predictions
about the student’s state after the tutor’s action and after the next student action. Slice
2 also includes the utility model since most of the outcomes in which the tutor might
be interested concern the final effects of the tutor’s current action.
The DDN update algorithm calculates the action with maximum expected utility.
The tutor executes that action and waits for the student to respond. When the tutor has
observed the student’s action, the student model update phase begins.
The tutor clamps the student’s action and updates the network. At this point, the
posterior probabilities in Student Model21 represent the tutor’s current beliefs about
the student. It is now time for another tutorial action selection, so another TACN is
created. Posterior probabilities from Student Model2 of the old TACN are copied as
prior probabilities to Student Model0 of the new TACN. The old TACN is discarded.
The tutor is now ready to begin the next phase, deciding what action to take next.
With this architecture, the tutor not only reacts to past student actions, but also anticipates future student actions and their ramifications. Thus, for instance, it can act to
prevent errors and impasses before they occur, just as human tutors often do.

1

For sub-network and node names, a numeric subscript refers to the slice number. A subscript
of n refers to any appropriate slice.

156
1.3

R. Charles Murray and Kurt VanLehn

Prior Work

Although probabilistic reasoning is become increasingly common in tutoring systems
and AI in general, we believe this is the first application of a DDN to tutoring. Probabilistic reasoning is often used in student and user modeling. In particular, Bayesian
networks are used in the student models of Andes [2], HYDRIVE [16] and other systems [12]. However, even with a probabilistic student model, other systems select tutorial actions using heuristics instead of decision-theoretic methods. Reye [19] has
suggested the use of a decision-theoretic architecture for tutoring systems and the use
of dynamic belief networks to model the student’s knowledge [20, 21].

2

Detailed Solution

This section describes TACNs in more detail, along with their implementation to form
DT Tutor’s action selection engine.
2.1

Major TACN Components and Their Interrelationships

Figure 2 provides a closer look at a TACN. The student model includes components
to represent the student’s knowledge state (sub-network Knowledge Networkn), the
student’s problem completion status and focus of attention (sub-network Relevance
Networkn), and the student’s emotional state (nodes Moralen and Independencen). Tutor
and student actions are represented by two nodes each: one for the action topic (Tutor/Student Topicn) and another for the action type (Tutor/Student Typen). Tutorial action relevance and coherence are represented by the Relevance1 and Coherence1 nodes
respectively. The utility model (Utility2) represents the tutor’s preferences.
In Figure 2, the Knowledge and Relevance Networks are shown as large rounded
rectangles. Each arc into or out of these sub-networks actually represents multiple
arcs to and from various sub-network nodes. For instance, there is a diachronic arc
from each Knowledge Network0 node to the corresponding Knowledge Network1 node.
Student Knowledge Network. For each problem, a Knowledge Network is created
from the problem solution graph to represent the tutor’s beliefs about the student’s
problem-related knowledge.
Within the Knowledge Network, each node has possible values known and unknown. Rule nodes represent the tutor’s belief about the student’s knowledge of the
corresponding rule. Problem step nodes represent the tutor’s belief about the student’s
potential to know the corresponding step given the student’s current rule knowledge.
At the beginning of a problem, the nodes representing the givens and the problem
goal are clamped to known, since these are given in the problem statement. The student’s potential knowledge of the remaining problem steps depends upon the student’s knowledge of the rules required to complete each problem step in turn.
Influences on Knowledge Network nodes include (1) synchronic influences to
model the interdependence of the student’s problem-related knowledge, and (2) diachronic influences between corresponding nodes in different slices to model the stability of the student’s knowledge over time. The tutor can also influence Knowledge
Network1 nodes directly, with an influence depending on the tutor action type. For in-

DT Tutor

5NKEG


5VWFGPV
/QFGN

5NKEG 

5VWFGPV
/QFGN

5NKEG 

157

5VWFGPV
/QFGN

+PFGRGPFGPEG
A

+PFGRGPFGPEG
A

+PFGRGPFGPEG
A

/QTCNGA

/QTCNGA

/QTCNGA

-PQYNGFIG
0GVYQTMA

-PQYNGFIG
0GVYQTMA

-PQYNGFIG
0GVYQTMA

4GNGXCPEG
0GVYQTMA

4GNGXCPEG
0GVYQTMA

4GNGXCPEG
0GVYQTMA

7VKNKV[
A

4GNGXCPEGA

%QJGTGPEGA
5VWFGPV
6QRKEA

5VWFGPV
6[RGA

5VWFGPV
6QRKEA
6WVQT
6QRKEA

5VWFGPV
6[RGA

6WVQT
6[RGA

Fig. 2. Tutor action cycle network in more detail

stance, if the tutor teaches a domain rule, there is normally a greater probability that
the rule will become known than if the tutor hints about it.
Student Relevance Network. Like the Knowledge Network, the Relevance Network
is a belief network created from the problem solution graph. It represents the tutor’s
beliefs about the student’s problem solving progress and problem-related focus of
attention. The representation of the student’s focus of attention is used to select
relevant tutorial actions and to predict the topic of the student’s next action.
Rule nodes have possible values relevant and irrelevant, where relevant means that
the rule is part of the student’s focus of attention. If the tutor addresses a rule directly,
the rule becomes relevant with a probability dependent on the tutor action type. For
instance, teaching a rule is more likely than hinting to make a rule relevant.
Problem step nodes have possible values complete, not ready, ready, and relevant.
Completed step nodes have the value complete. If a step node is not complete and if it
has any synchronic step node parents that are not complete, its value is not ready,
meaning that it is not ready to be completed until its parent steps have been completed
(but this does not preclude the student from completing it anyway – e.g., by guessing).
Otherwise, a step node has some distribution over the values ready and relevant. Such
nodes represent the frontier of the student’s work within the problem space, possible
next steps and thus potentially the focus of the student’s attention. The value ready
means that a step is ready to be completed since all of its parent steps have been completed. The value relevant means that not only is the step ready to be completed, but it
is also part of the student’s focus of attention. For instance, a step is likely to be rele-

158

R. Charles Murray and Kurt VanLehn

vant if the tutor addresses it (e.g., with a hint) or if the student makes an error on it.
Steps that are relevant at some point in time become a little less relevant with each
passing time slice. This is to model relevance aging: steps that were relevant slowly
become less relevant as the student moves on to other topics.
When there are multiple steps that could be relevant by virtue of being the next uncompleted step along a solution path, DT Tutor assumes a depth-first bias to decide
how likely the various steps are to be part of the student’s focus of attention: When
applying a rule produces multiple ready steps, students usually prefer to pick one and
complete work on it before starting work on another. Such a bias corresponds to a
depth-first traversal of the problem solution graph and is consistent with both activation-based theories of human working memory [1] and observations of human problem solvers [18]. However, a depth-first bias is not absolute. At any given step, there
is some probability that a student will not continue depth-first.
To model depth-first bias, when a step first becomes ready or relevant because the
last of its outstanding parent steps has become complete, that step becomes relevant
with high probability. This is because with a depth-first bias, having just completed
the step’s parent, the student is likely to continue working with the step itself. Relevance aging helps to model another aspect of depth-first bias: preferring to continue
with more recently raised steps. When the student completes or abandons a portion of
the solution path, steps that were recently highly relevant but that are still not complete have had less relevance aging than steps that were highly relevant in the more
distant past, so the more recently raised steps remain more relevant.
Student Emotional State. Human tutors consider the student’s emotional or
motivational state in deciding how to respond [5, 14]. Concern for student morale is
likely to be one reason why tutors tend to give negative feedback subtly, to play up
student successes and downplay student failures, etc., while maximizing the student’s
feeling of independence is likely to be one reason why tutors tend not to intervene
unless the student needs help, to minimize the significance of the tutor’s help, etc.
[15]. Such behaviors cannot be explained in terms of concern for the student’s
knowledge and the problem solving state alone.
The student’s emotional state is modeled with the Moralen and Independencen
nodes. Each of these nodes has possible values level 0 through level 4, with higher
levels representing greater morale or independence. Both tutor and student actions influence these nodes with an influence dependent on the action type. In addition, diachronic influences model the stability of the student’s emotional state over time.
Tutor Action Nodes. The Tutor Type1 alternatives include fairly fine distinctions to
model some of the subtlety that human tutors exhibit when working with students.
These alternatives include prompt, hint, teach, positive feedback, negative feedback,
do (do a step for the student), and null (no tutor action).
Tutor Topic1 can be any problem-related topic, so there is an alternative for each
rule or step node in the problem solution graph. The value null is also supported to
model (1) a tutor action with a type but no topic (e.g., general positive feedback), and
(2) no tutor action at all, in which case Tutor Type1 is null as well.
Student Action Nodes. First, the values of the student action nodes in slice 0 are
simply the values of the student action nodes in slice 2 of the previous TACN, except
for the very first TACN, in which they both have the value null.

DT Tutor

159

Student Topicn can be any step in the problem solution graph. It can also be null to
model either no student action at all (in which case Student Typen is null as well) or a
student action with a null topic (e.g., a general impasse or an error which the tutor
cannot interpret as an attempt at a particular step). Student Topic2 is influenced by the
relevance of the steps that are most likely to be the topic of the student’s next action –
i.e., by Relevance Network1 step nodes that are ready or relevant.
Student Typen has possible values correct, error, impasse, and null. Impasse means
that the student does not know what action to take – for instance, when the student
asks for help. Null is used to model no student action. Student Type2 is influenced both
by the student action topic and by the student’s knowledge of that topic – i.e., by Student Topic2 and by the Knowledge Network1 step nodes.
Utility Model. Node Utility2 is actually a structured utility model consisting of several
nodes to represent tutor preferences regarding the following outcomes:
1.
2.
3.
4.
5.
6.
7.

Student rule knowledge in slice 2 (rule nodes in Knowledge Network2)
Student problem step progress in slice 2 (step nodes in Relevance Network2)
Student independence in slice 2 (Independence2)
Student morale in slice 2 (Morale2)
Tutor action type in slice 1 (Tutor Type1)
Tutor action relevance in slice 1 (Relevance1)
Tutor action coherence in slice 1 (Coherence1)

2.2

Implementation

DT Tutor was implemented using software developed at the Decision Systems Laboratory, University of Pittsburgh: GeNIe, a development environment for graphical
models, and SMILE-, a platform independent library of C++ classes for reasoning
with graphical probabilistic models. From the problem solution graph structure, DT
Tutor creates a TACN with default values for node outcomes, prior probabilities, conditional probabilities, and utilities. An optional file can be loaded to specify any prior
probability or utility values that differ from the default values. After creating the initial TACN, DT Tutor recommends tutorial actions, accepts inputs representing actual
tutor and student actions, updates the network, and adds new TACNs to the DDN as
appropriate. We have not yet developed a suitable graphical interface for students, so
a simple text interface was created for evaluation.
While DT Tutor will work with most any problem solution graph, for the initial
implementation we selected the domain of calculus related rates problems for two reasons. First, the number of steps per problem is non-trivial without being too large, so
results obtained should be generalizable to other real world domains. Second, Singley
[23] developed an interface for this domain with the purpose of reifying goal structures. We assume an extension to Singley’s interface that makes all problem solving
actions observable. This makes it easier to determine the student’s current location in
the problem solution space and thus to model the student’s current focus of attention
and predict the student’s next action.

160

3

R. Charles Murray and Kurt VanLehn

Evaluation

The goals for evaluation were to evaluate whether DT Tutor’s approach can be used
to select actions within reasonable space and time that are not only optimal but that
also correspond to some of the more interesting behaviors of human tutors.
3.1

Evaluate Tractability

One of the major challenges facing Bayesian models for real world domains is tractability in terms of both space and time. A number of measures were taken to reduce
space requirements [see 17] which were then considered tractable since the tutor was
able to successfully perform the tests described below.
It is important to provide real-time responses in order to keep the student engaged.
With an early version of the Andes physics tutor [2], students tolerated response times
of up to 40 seconds. However, considering both (1) the variety of domains for which
ITSs might be constructed, and (2) ever-improving computer hardware and algorithms
for evaluating probabilistic models, no exact response time requirement can be determined. Rather, it is more important to begin to evaluate how such systems will scale.
Test results are shown in Table 1. Both of the approximate algorithms using 1,000
samples returned responses for both problems well within the tolerated limit, as did
the exact algorithm for the smaller of the two problems. Response times for the approximate algorithms grew linearly in the number of samples and in the number of
nodes. The approximate algorithms using 10,000 samples and the exact algorithm on
the larger problem did not return responses quickly enough, and in any case, faster response times are desirable. Fortunately, a number of speedups are feasible, as discussed in [17]. In addition, the anytime property of the approximate algorithms could
be used to continually improve results until a response is required. For many applications, including this one, it is sufficient to correctly rank the optimal decision alternative. When only the rank of the optimal decision alternative was considered, the approximate algorithms using 1,000 samples were correct on every trial.
Table 1. Action selection response times
Response time mean (range)
a
b
Problem B
Problem A
108 (107-109)
11 (11-12)

Algorithm
Exact: Clustering [9]
Approximate:
Likelihood Sampling [22]
1,000 samples
10,000 samples
Heuristic Importance [22]
1,000 samples
10,000 samples

12 (12-13)
106 (104-110)

8 (7-8)
64 (62-66)

12 (12-13)
104 (101-106)

8 (7-8)
64 (60-66)

Note. Response time is the number of seconds required to determine the optimal tutorial action.
Mean and range are over 10 trials. All tests were performed on a 200MHz Pentium MMX PC
with 64MB of RAM. The algorithms were tested with Cooper’s [3] algorithm for decision network inference using belief network algorithms.
a

10-step problem, 185-node TACN.

b

5-step problem, 123-node TACN.

DT Tutor

3.2

161

Evaluate Tutorial Action Selections

DT Tutor’s decision-theoretic representation guarantees that its decisions will be optimal given the beliefs and objectives it embodies. Therefore, besides a sanity check
of the implementation, the purpose of this part of the evaluation was to find out
whether DT Tutor’s approach and choices about which outcomes and objectives to
model were sufficient to endow the tutor with some of the more interesting capabilities of human tutors. While detailed results would be too lengthy to report here [see
17], testing showed that DT Tutor is indeed capable of selecting rational tutorial actions that correspond in interesting ways to the behavior of human tutors. Notable behaviors included the following:
• DT Tutor did not provide help when it believed the student did not need it. Human
tutors foster their students’ independence by letting them work autonomously [14].
• When the student was likely to need help, DT Tutor often intervened before the
student could experience failure. Human tutors often provide help proactively
rather than waiting for a student error or impasse [14, 15].
• As the student moved around the problem space, DT Tutor adapted to support the
student’s current line of reasoning, assuming a depth-first topic bias.
• All other things being equal, DT Tutor preferred to address rules rather than problem-specific steps. Effective human tutoring is correlated with teaching generalizations that go beyond the immediate problem-solving context [24].
• DT Tutor considered the effects of its actions on the student’s emotional state as
well as the student’s knowledge state. Human tutors consider both as well [14].
• DT Tutor prioritized its actions based on current beliefs and objectives. Likewise,
human tutors prioritize their actions based on the student’s needs and tend not to
waste time addressing topics that the student does not need to know [15].

4

Future Work and Conclusions

We still need to develop a graphical interface or embed DT Tutor’s action selection
engine within an existing ITS and evaluate it with human students. Efficiently obtaining more accurate probability and utility values would be beneficial as well. However, an encouraging result from prior research is that Bayesian systems are often surprisingly insensitive to imprecision in specification of numerical probabilities [8] and
may be accurate enough to infer the correct decision even if some of their assumptions are violated [6], so that precise numbers may not always be necessary.
This research has shown that a decision-theoretic approach can indeed be used to
select tutorial actions that are optimal, given the tutor’s beliefs and objectives, for
real-world-sized problems in satisfactory response time. The DDN representation
handles uncertainty about the student in a theoretically rigorous manner, balances
tradeoffs among multiple objectives, automatically adapts to changes in beliefs or
objectives, and increases the accuracy of the information upon which the tutor’s decisions are based. By modeling not only the student’s problem-related knowledge but
also the student’s focus of attention and emotional state, DT Tutor can select actions
that correspond to some of the more interesting behaviors of human tutors.

162

R. Charles Murray and Kurt VanLehn

References
1. Anderson, J. R. (1993). Rules of the Mind. Lawrence Erlbaum Associates.
2. Conati, C., Gertner, A., VanLehn, K., & Druzdzel, M. (1997). On-line student modeling for
coached problem solving using Bayesian networks. 6th International Conference on User
Modeling, pp. 231-242.
3. Cooper, G. F. (1988). A method for using belief networks as influence diagrams. Workshop
on Uncertainty in Artificial Intelligence, pp. 55-63.
4. Cousins, S. B., Chen, W., & Frisse, M. E. (1993). A tutorial introduction to stochastic
simulation algorithms for belief networks. AI in Medicine 5, pp. 315-340.
5. del Soldato, T., & du Boulay, B. (1995). Implementation of motivational tactics in tutoring
systems. Journal of Artificial Intelligence in Education 6(4), pp. 337-378.
6. Domingos, P., & Pazzani, M. (1997). On the optimality of the simple Bayesian classifier
under zero-one loss. Machine Learning 29, pp. 103-130.
7. Graesser, A. C., Person, N. K., & Magliano, J. P. (1995). Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive Psychology 9, pp. 495-522.
8. Henrion, M., Pradhan, M., Del Favero, B., Huang, K., Provan, G., & O'Rorke, P. (1996).
Why is diagnosis in belief networks insensitive to imprecision in probabilities? Twelfth Annual Conference on Uncertainty in Artificial Intelligence.
9. Huang, C., & Darwiche, A. (1996). Inference in belief networks: A procedural guide. International Journal of Approximate Reasoning 15, pp. 225-263.
10. Huang, T., Koller, D., Malik, J., Ogasawara, G., Rao, B., Russell, S., & Weber, J. (1994).
Automated symbolic traffic scene analysis using belief networks. Twelfth National Conference on Artificial Intelligence, pp. 966-972.
11. Huber, M. J., Durfee, E. H., & Wellman, M. P. (1994). The automated mapping of plans for
plan recognition. Tenth Conference on Uncertainty in Artificial Intelligence, pp. 344-350.
12. Jameson, A. (1996). Numerical uncertainty management in user and student modeling: An
overview of systems and issues. User Modeling and User-Adapted Interaction 5(3-4), pp.
103-251.
13. Keeney, R., & Raiffa, H. (1976). Decisions with Multiple Objectives. Wiley.
14. Lepper, M. R., Woolverton, M., Mumme, D. L., & Gurtner, J.-L. (1993). Motivational
techniques of expert human tutors: Lessons for the design of computer-based tutors. Computers as Cognitive Tools (pp. 75-105). Lawrence Erlbaum Associates.
15. Merrill, D. C., Reiser, B. J., Merrill, S. K., & Landes, S. (1995). Tutoring: Guided learning
by doing. Cognition and Instruction 13(3), pp. 315-372.
16. Mislevy, R. J., & Gitomer, D. H. (1996). The role of probability-based inference in an intelligent tutoring system. User Modeling and User-Adapted Interaction 5(3-4).
17. Murray, R. C. (1999). A dynamic, decision-theoretic model of tutorial action selection. Unpublished MS Thesis, University of Pittsburgh. http://www.isp.pitt.edu/~chas/
18. Newell, A., & Simon, H. A. (1972). Human Problem Solving. Prentice-Hall, Inc.
19. Reye, J. (1995). A goal-centred architecture for intelligent tutoring systems. World Conference on Artificial Intelligence in Education, pp. 307-314.
20. Reye, J. (1996). A belief net backbone for student modeling. Intelligent Tutoring Systems,
Third International Conference, pp. 596-604.
21. Reye, J. (1998). Two-phase updating of student models based on dynamic belief networks.
Fourth International Conference on Intelligent Tutoring Systems, pp. 274-283.
22. Shachter, R. D., & Peot, M. A. (1990). Simulation approaches to general probabilistic inference on belief networks. Uncertainty in Artificial Intelligence, pp. 221-231.
23. Singley, M. K. (1990). The reification of goal structures in a calculus tutor: Effects on
problem solving performance. Interactive Learning Environments 1, pp. 102-123.
24. VanLehn, K., Siler, S., Murray, C., Yamauchi, T., & Baggett, W. B. (in press). Human tutoring: Why do only some events cause learning? Cognition and Instruction.

Further Results from the Evaluation of an Intelligent
Computer Tutor to Coach Self-Explanation
Cristina Conati1 and Kurt VanLehn2
1

Department of Computer Science, University of British Columbia
2366 Main Mall, Vancouver, B.C. Canada V6T 1Z4
email: conati@cs.ubc.ca
2
Department of Computer Science, University of Pittsburgh,
Pittsburgh, PA, 15260, U.S.A
vanlehn@cs.pitt.edu

Abstract
We present further results on the educational effectiveness of an intelligent
computer tutor that helps students learn effectively from examples by coaching
self-explanation – the process of explaining to oneself an example worked-out
solution. An earlier analysis of the results from a formative evaluation of the
system provided suggestive evidence that it could improve students’ learning.
In this paper, we present additional results derived from a more comprehensive
analysis of the experimental data. They provide a stronger indication of the
system’s effectiveness and suggest general guidelines for effective support of
self-explanation during example studying.

1

Introduction

The research presented in this paper represents a step toward exploring innovative
ways in which computers can enhance education and learning. While most intelligent
tutoring systems support students during problem solving and teach domain specific
skills, we have devised a computational framework that supports learning from
examples and that coaches the general learning skill known as self-explanation generating explanations and justifications to oneself to clarify an example solution.
Several studies show that self-explanation greatly improves learning from examples
(for overviews of these studies see [4] and [10]) and that coaching self-explanation
can extend these benefits ([3], [4]). Our framework, known as the SE-Coach, aims to
provide the individualized monitoring and guidance to self-explanation that has been
proven so beneficial when administered by human tutors. It has been implemented
and tested within Andes [11], a tutoring system that helps students learn Newtonian
physics through both example studying and problem solving.
Other tutoring systems rely on examples as instructional means, but they use them
to support students as they solve problems, not as a specific learning phase prior to
and complementary to problem solving. These systems present students with relevant
examples as they are solving problems and help students understand the connection
between the example and the problems [12], [7], [1]. However, none of these systems
G. Gauthier, C. Frasson, K. VanLehn (Eds.): ITS 2000, LNCS 1839, pp. 304-313, 2000.
 Springer-Verlag Berlin Heidelberg 2000

Evaluation of an Intelligent Computer Tutor to Coach Self-Explanation

305

monitor how students study and understand the presented examples. Moreover, the
systems themselves, rather than the students, generate explanations to help the
students understand the examples. The Geometry Tutor [2] explicitly encourages
students to explain the solution steps they have used to build geometry proofs, in
terms of geometry axioms. However, the explanations are generated during problem
solving and consist simply of selecting an item from a list of geometry axioms. The
student does not have to explain the content of the axiom. Furthermore, the tutor
makes the student explain each solution step, instead of trying to assess if some
explanations may be more beneficial for the student than others.
Unlike the systems above, the SE-Coach includes an interface designed to
encourage spontaneous, constructive self-explanation of examples. It also includes a
help module that explicitly elicits further self-explanation tailored to a student’s
needs, as assessed by the SE-Coach probabilistic student model, when the interface
scaffolding is not sufficient to overcome the natural reticence to self-explain that
many students show [4], [10].
Self-explanation is a learning process whose underlying mechanisms are still
unclear and under investigation. Since the SE-Coach is built on existing hypotheses
about the features that make self-explanation effective for learning, an accurate
evaluation of its effectiveness may allow us to shed light on the validity of the
hypotheses and possibly suggest new ones. In [6], we presented initial results of a
formal evaluation that we performed to test the usability and effectiveness of the
system. These results indicated that the SE-Coach’s interface is easy to use and
generally effective in stimulating self-explanation. They also provided initial support
on the SE-Coach’s educational effectiveness. In this paper, we present a more detailed
analysis of the experimental data that reveals a significant interaction between
experimental condition and the learning stage in which students used the system, and
provides insights on how the SE-Coach can more effectively bring students to
constructively learn from examples.

2 Overview of the System
The SE-Coach’s interface includes three different levels of scaffolding for selfexplanation, to accommodate the varied propensity to self-explain that different
students have, so as to provide each student with the minimum intervention sufficient
to trigger constructive self-explanation.
The first level of scaffolding is given by a masking interface that presents different
parts of the example covered by grey boxes (see Figure 1). In order to read the text or
graphics hidden under a box, the student must move the mouse pointer over it. The
fact that not all the example parts are visible at once helps students focus attention and
reflect on individual example parts, and allows the SE-Coach to track student’s
attention [6]. The second level of scaffolding is provided by explicit prompts to selfexplain. These prompts go from a generic reminder to self-explain, that appears when
a student uncovers an example part, to more specific prompts for self-explanations
that have been shown to correlate with learning in the self-explanation studies: (a)
justify solution steps in terms of domain principles; (b) relate solution steps to goals
in the underlying solution plan.

306

Cristina Conati and Kurt VanLehn

Figure 1: A physics example (left), as it is presented in the masking interface (right)

The third level of scaffolding consists of menu-based tools designed to provide
constructive but controllable ways to generate the above self-explanations, to help
those students that would be unable to properly self-explain if left to their own
devices [10]. If a student selects the prompt to self-explain in terms of domain
principles (”This is true because...”), a Rule Browser is displayed in the right half of
the window (see Figure 2a), while if the student selects the prompt to self-explain in
terms of the solution plan (”The purpose of this step is...”), a Plan Browser is
activated instead.

If we want to find
Then we can
choose that
object as the
body

(a)

(b)

Figure 2: (a) Selections in the Rule Browser and (b) Template filling

The rule browser contains a hierarchy of physics rules, reflecting the content of the
SE-Coach’s knowledge base. The student can browse the rule hierarchy to find a rule
that justifies the currently uncovered part. The SE-Coach will use a green check or a
red cross to provide feedback on the correctness of the student’s selection (see Figure
2a). To explain more about the actual content of a rule, the student can click on the
”Template” button in the rule browser. A dialog box comes up (see Figure 2b) with a
partial definition of the rule that the student can complete by selecting appropriate
fillers from available pull down menus. The SE-Coach gives immediate feedback on
the student’s selections.

Evaluation of an Intelligent Computer Tutor to Coach Self-Explanation

307

The plan browser is similar to the rule browser, but it displays a hierarchical tree
representing the solution plan for a particular example instead of the SE-Coach’s
physics rules. The student explains the role of the uncovered part by selecting in the
plan hierarchy the step that most closely motivates the fact.
The SE-Coach includes a probabilistic student model based on a Bayesian network.
The Bayesian network comprises a model of correct self-explanation for the current
example, probabilities estimating the student’s physics knowledge and nodes
representing the student’s reading and self-explanation actions. At any time during the
interaction, probabilities in the Bayesian network assess how well the student
understands the example solution and how the student’s knowledge changes as a
result of the interaction with the system [5]. Using this assessment, the SE-Coach
prompts the student to generate further self-explanation to fix gaps in the student’s
example understanding.
Initially, self-explanation is voluntary. However, if a student tries to close the
example when the student model indicates that there are still some lines left to selfexplain, then the SE-Coach generates a warning and colors pink the corresponding
masking interface boxes. It also provides more directive advice as of what interface
tool should be used to better self-explain each line. The SE-Coach’s tutorial
interventions represent a fourth, stronger level of scaffolding for self-explanation,
directed to help those students that do not self-explain because they tend to
overestimate their understanding [4].

3

Empirical Evaluation of the SE-Coach

To test the system’s effectiveness for learning, we performed a formal study with 56
college students. The SE-Coach does not provide any introductory physics
instruction, because it is meant to complement regular classroom activities. Therefore,
an evaluation of the SE-Coach requires subjects who have the right level of domain
knowledge for using the system. Students generally benefit more from examples
when they are studying a new topic, whereas as the students’ knowledge improves,
problem solving becomes more effective for learning [8]. Hence, to evaluate the SECoach adequately, subjects need to have enough knowledge to understand the topic of
the examples, but not so much knowledge to find the examples not worthy of
attention. The ideal evaluation setting for the SE-Coach would be in the context of
an introductory physics course, where it is possible to control when students are ready
to study examples on a new topic. Unfortunately, we could not coordinate the SECoach’s evaluation with a specific physics course. Instead, we conducted the study in
our laboratory, with students who were taking introductory physics classes at four
different colleges: the University of Pittsburgh (20 students), Carnegie Mellon
University (14 students), Community College of Allegheny County (5 students) and
U.S. Naval Academy (17 students). The best we could do to get subjects at
comparable learning stages was to run the subjects after their first class on Newton’s
Second Law and before they took a class test on the topic.
The one-session study comprised: 1) solving four pre-test problems on Newton’s
Second Law; 2) studying examples on Newton’s Second Law with the system; 3)
solving post-test problems equivalent but not identical to the pre-test ones; The study
had two conditions. In the experimental (SE) condition, 29 students studied examples

308

Cristina Conati and Kurt VanLehn

with the complete SE-Coach. In the control condition, 27 students studied examples
with the masking interface and Plan Browser only1. They had no access to the Rule
Browser and Templates, nor feedback or coaching.
3.1. Effectiveness of the SE-Coach
As we reported in [6], the analysis of the log data file from the study shows that the
SE-Coach’s interface is easy to use and is quite successful at stimulating selfexplanation. The gains scores between post-test and pretest were higher for the SE
condition, although the difference between gain scores of the two conditions was not
statistically significant. Since then, we have sought to better understand the reason
behind the above outcome by restricting the analysis to the subgroups of subjects
coming from different colleges. We found that the SE condition of CMU (Carnegie
Mellon) and CCAC (Community College of Allegheny County) students performed
better than the control condition (see Figure 3). The performance difference, as
measured by an Analysis of Covariance with post-test as dependent variable, pre-test
as covariate and condition as main effect, was statistically significant for CMU
students (p < 0.04) and nearly significant (p = 0.0576) for CCAC students. In contrast,
in the Pitt (Univ. of Pittsburgh) and USNA (U.S. Naval Academy) subgroups,
students in the control condition performed slightly better than students in the SE
condition (see Figure 3), although the difference was not statistically significant
10
9
8
7
6
5
4
3
2
1
0

30
25
20
15
10
control

5

SE

0
CCAC

CMU

Pitt

USNA

CMU

Pitt

USNA

CACC

Figure 3: Gains scores for the four
subgroups

Figure 4: pretest scores for the four
subgroups

The commonality of behavior between CMU and CACC students is quite
surprising, because CMU and CCAC are supposed to be, respectively, the best and
the worst among the four colleges in the study. This ranking is confirmed by the
pretest scores shown in Figure 4. The difference in pretest performance between
CMU and CCAC is the only one that approaches significance (p = 0.0561), among the
pretest performances of the four groups.
To understand what may have caused this different learning behavior, we collapsed
and analyzed the data in two subgroups with the same learning outcome, CMU1

We let the control students access the Plan Browser because introductory physics courses usually do not
address solution planning, therefore control students would have had too much of a disadvantage if they
had not been able to see what a solution plan is through the Plan Browser.

Evaluation of an Intelligent Computer Tutor to Coach Self-Explanation

309

CCAC and Pitt-USNA. Within the CMU-CCAC group, students in the SE condition
performed significantly better than students in the control condition, after covarying
out the pretest (p = 0.021). Pitt-USNA students in the control condition performed
slightly better than those in the SE condition, but the difference is not statistically
significant (p > 0.2).
3.2.

Possible Differences in the Student Populations

One possible explanation for the above results could be a difference in physics and
background knowledge between the two subgroups of CMU-CCAC and Pitt-USNA
students. However, an ANCOVA with post-test as dependent variable and subgroup
and condition as main effects, shows that there is still a significant interaction (p <
0.01) of subgroup with condition after covarying out pretest only and both pretest and
SAT scores. Although 10 subjects are excluded from the latter ANCOVA (we did not
have these subjects’ SAT scores), these data still provide a strong indication that
physics and background knowledge do not explain the different performance of the
two subgroups.
A second explanation for the different learning behavior of the CMU-CCAC and
Pitt-USNA subgroups could be that subjects in the two subgroups used the system
differently. The one thing that CMU and CCAC have in common, and that
distinguishes them from Pitt and USNA students, is that they start the semester more
than a week later. Therefore, although all the subjects participated to the experiment
after they had their lectures on Newton’s laws and before they took a class test on the
topic, Pitt and USNA subjects were ahead in the course schedule and had likely spent
more time on Newton’s laws than CMU and CCAC subjects when they participated to
the study. Our data show that this did not significantly influence the pretest
performance of the two subgroups.
However, it may have caused the students
8
in the two subgroups to have a different
6
attitude toward the example study task we
cmu-ccac
made them perform.
4
pitt-usna
If we analyse the learning patterns of the
2
two subgroups within each condition, we
find that in the SE condition, CMU-CCAC
0
students learned more than Pitt-USNA
control
SE
students (see figure 5), although the
Figure 5: gains scores of the two subgroup
difference is not statistically significant (p
in each condition
> 0.1). In the Control condition, PittUSNA students learned significantly more than CMU-CCAC students (p < 0.03).
These outcomes could be due to two reasons:
− In the SE condition, Pitt-USNA students did not use the SE-Coach as extensively
and effectively as the CMU-CCAC students did.
− In the Control condition, Pitt-USNA students self-explained spontaneously more
that the CMU-CCAC students did.
We will now verify these two hypotheses by comparing tyhe log data of the two
subgroups within the SE and the control condition.

310

Cristina Conati and Kurt VanLehn

Log data analysis of the two subgroups within the SE condition
To test whether CMU-CCAC students used the SE-Coach better than the Pitt-USNA
students in the SE condition, we compared time on task, statistics describing how
subjects used the interface self-explanation tools (Rule Browser, Plan Browser and
Templates) and how they reacted to the SE-Coach’s advice to further self-explain.
Rule Browser
Initiated
Correct
Attempts before correct
Max # attempts
Attempts before abandon

CMU-CCAC (12)
63.6%
88%
1.1
7.8
4.3

Pitt-USNA (17)
61.4%
86%
1.3
10.2
3.7

p
0.8
0.6
0.35
0.45
0.7

Template
Initiated
Correct
Attempts before correct
Max # attempts
Attempts before abandon

CMU-CCAC (12)
57.6%
97.2%
0.47
2.2
3

Pitt-USNA (17)
53.8%
96.8%
0.51
2.7
0.15

p
0.7
0.8
0.8
0.3
0.011

Plan Browser
Initiated
Correct
Attempts before correct
Max # attempts
Attempts before abandon

CMU-CCAC (12)
36.2%
92%
1
3.9
1.4

Pitt-USNA (17)
45%
81%
1
3.8
1.1

p
0.55
0.15
0.9
0.96
0.77

Table 1: Statistics on interface tools usage for CMU-CCAC and Pitt-USNA students

Rule prompts followed
Plan prompts followed
Read prompts followed

CMU-CCAC (12)
41%
50%
31%

Pitt-USNA (17)
37%
36%
35%

p
0.8
0.36
0.88

Table 2: SE-Coach prompts statistics for CMU-CCAC and Pitt-USNA students

For each interface tool, we computed the following data summaries (see Table 1):
Initiated: percentage of the explanations that students initiated out of all the
explanations that could be generated with that tool for the available examples.
Correct: percentage of the initiated explanations that were generated correctly.
Attempts before correct: average number of attempts the students made before
achieving a correct self-explanation. An attempt is the submission of an incorrect selfexplanation. Max # attempts: average maximum number of attempts needed to
achieve a correct self-explanation. Attempts before abandon: average number of
attempts before abandoning a self-explanation. We also computed how many of the
different prompts generated during the SE-Coach tutorial interventions (prompts to
self-explain using the Rule Browser, the Plan Browser or by reading more carefully)
the students actually followed (see Table 2). There is no statistically significant
difference in the average time on task for the two subgroups (p > 0.1). The only

Evaluation of an Intelligent Computer Tutor to Coach Self-Explanation

311

significant difference in the way CMU-CCAC and Pitt-USNA students used the
system in the SE condition is that CMU-CCAC students performed a significantly
higher number of attempts before giving up on a Template explanation (see Table 1,
Template data). This suggests that the CMU-CCAC students had a higher level of
motivation to learn from the SE-Coach self-explanation tools, consistently with the
fact that students in the CMU-CCAC group had started studying Newton’s Laws later
than Pitt-USNA students and thus they were likely more willing to put substantial
effort in learning from examples on the topic.
The CMU-CCAC students’ higher level of motivation can explain why they learned
more from the SE-Coach than the Pitt-USNA students did, although in general they
did not use the system more easily and extensively (as Table 1 and Table 2 show).
Selecting items in the browsers and filling templates does not necessarily trigger
constructive learning if students do not reflect on what they are doing. Indeed, if
students are not motivated to put substantial effort in studying examples, the actions
of browsing and Template filling may act as distracters from learning. Students may
concentrate their attention on selecting items to get positive feedback on their
interface actions, but not actually reflect on the physics behind the actions and behind
the worked out solution. Thus, we argue that CMU-CCAC students in the SE
condition learned more from the same self-explanation actions than Pitt-USNA
students because, being more motivated, they reasoned more constructively on their
self-explanation actions and on the physics underlying them.
This argument is supported by the correlation between post-test scores and the
number of rules that reached high probability in the student model. The correlation is
very low (r < 0.1) for Pitt-USNA students and it is higher (r = 0.33) for the CMUCCAC students. Since the probabilities in the student model are driven upward by
correct self-explanations conducted on the SE-Coach’s interface, the high correlation
of the CMU-CCAC group suggests that their self-explanations drove their
understanding upward just as they drove the model’s probabilities upward, whereas
the low correlation of the Pitt-USNA group suggests that their learning was
independent of their use of the SE-Coach’s self-explanation tools.
Log data analysis of the two subgroups within the control condition
The hypothesis that the learning of Pitt-USNA students in the control condition is due
to spontaneous self-explanation is not easy to verify, because in this condition
students could not express their self-explanation through the SE-Coach. The only log
data file that could indirectly indicate self-explanation in the control condition are: (1)
average number of multiple accesses to example lines; (2) standard deviation of the
above measure; (3) average time spent on each example line; (4) standard deviation of
the above; (5) time on task; (6) number of accesses to the Plan Browser; (7) number
of selections in the Plan Browser.
We ran a regression analysis of post-test on the above variables for the Pitt-USNA
control group and we found a marginally significant correlation of post-test scores
with average and standard deviation of line accesses (p = 0.083 and p = 0.057
respectively). We found no significant correlations in the same regression analysis
for the CMU-CCAC control group. These results support the hypothesis that PittUSNA control students were selectively reviewing example lines because they were
self-explaining specific example parts, while the CMU-CCAC control students’

312

Cristina Conati and Kurt VanLehn

reviewing actions were not accompanied by effective self-explanation. The
hypothesis that Pitt-USNA students self-explained more in the control condition is
consistent with the fact that Pitt-USNA students had started studying Newton’s Laws
earlier and had probably gained more knowledge on the topic. This knowledge was
not strong enough to make Pitt-USNA students perform better in problem solving
tasks (their pretest performance was comparable to the CMU-CCAC students’ one).
However, it was sufficient to enable Pitt-USNA control subjects to generate effective
self-explanations under the minimal scaffolding provided by the masking interface.
We argue that it is indeed the minimality of the scaffolding that allowed Pitt-CMU
control students to bring to bear their knowledge at best. Because of their more
advanced learning stage, spontaneous self-explanation triggered by the masking
interface likely came quite effortlessly to Pitt-USNA control students and therefore
was not suffocated by the lower level of motivation that prevented Pitt-USNA
students in the SE condition to learn effectively from the SE-Coach self-explanation
tools.

4

Conclusions and Future Work

In this paper, we discussed the results of a formal study to evaluate an intelligent
computer tutor that coaches the meta-cognitive skill known as self-explanation –
generating explanations to oneself to clarify an example worked out solution. The
tutor provides different levels of tailored scaffolding for self-explanation, to provide
each student with the minimum intervention sufficient to trigger self-explanation
while maintaining the spontaneous, constructive nature of this learning strategy.
Formal studies are fundamental to assess why and how a computer tutor does or
does not support learning. Understanding how students use and learn from the SECoach is especially important, because the SE-Coach focuses on a learning process
that no other tutoring system has tackled so far and whose underlying mechanisms are
still unclear and under investigation. In particular, different studies have shown that
both simple prompting [4] and more elaborate scaffolding [3] enhance selfexplanation and learning, but no study has yet addressed the explicit comparison of
these different kinds of intervention. The study that we performed provides initial
insights on this issue. In this paper, we have presented data analysis results indicating
that the stage of learning in which the students use the system influences how much
they benefit from versions of the system that provide different amounts of scaffolding
for self-explanation. The data suggest the following conclusions on the SE-Coach
effectiveness and, in general, on the effectiveness of support for self-explanation
during example studying.
• Rich scaffolding for self-explanation, like the one provided by the complete SECoach in the experimental condition, can improve students’ performance at an early
learning stage. At this stage, students are still unfamiliar with the subject matter.
Hence, they benefit more from structured help in using domain knowledge to
generate effective self-explanations and are more motivated to put substantial effort
in exploiting this help.
• As students become more proficient in the subject matter, even minimal prompting,
like the one provided by the masking interface in the control condition, can help
improve their self-explanations. At this stage, more elaborate scaffolding can

Evaluation of an Intelligent Computer Tutor to Coach Self-Explanation

313

actually be less effective, if it requires students to put too much effort in studying
examples, because they may lack the motivation to do so.
Of course, more data is necessary to confirm these conclusions. We plan to gather
the data by running a study in the context of classroom instruction, where it is easier
to control at what stage of learning the students use the system. If the study confirms
the results presented in this paper, it may be beneficial to add to the SE-Coach the
capability to automatically tailor the available levels of scaffolding depending upon
the student’s familiarity with the examples topic.

5 References
1.

Aleven, V., & Ashley, K. D. (1997). Teaching case-based argumentation through a model
and examples: Empirical evaluation of an intelligent learning environment. In Proc. of
AIED ‘97, 8th World Conference of Artificial Intelligence and Education, Kobe, Japan.
2. Aleven, V., Koedinger, K. R., & Cross, K. (1999). Tutoring answer-explanation fosters
learning with understanding. In Proc. of AIED ‘99, 9th World Conference of Artificial
Intelligence and Education, Le Mans, France.
3. Bielaczyc, K., Pirolli, P., & Brown, A. L. (1995). Training in self-explanation and selfregulation strategies: Investigating the effects of knowledge acquisition activities on
problem-solving. Cognition and Instruction, 13(2), 221-252.
4. Chi, M. T. H. (in press). Self-Explaining: The dual process of generating inferences and
repairing mental models. Advances in Instructional Psychology.
5. Conati, C. (1999). An intelligent computer tutor to guide self-explanation while learning
from examples. Unpublished Ph.D. thesis, University of Pittsburgh, Pittsburgh.
6. Conati, C., & VanLehn, K. (1999). Teaching meta-cognitive skills: implementation and
evaluation of a tutoring system to guide self-explanation while learning from examples. In
Proc. of AIED'99, 9th World Conference of Artificial Intelligence and Education, Le
Mans, France.
7. Gott, S. P., Lesgold, A., & Kane, R. S. (1996). Tutoring for transfer of technical
competence. In B. G. Wilson (Ed.), Constructivist Learning Environments (pp. 33-48).
Englewood Cliffs, NJ: Educational Technology Publications.
8. Nguyen-Xuan, A., Bastide, A., & Nicaud, J.-F. (1999). Learning to solve polynomial
factorization problems: by solving problems and by studying examples of problem
solving, with an intelligent learning environment. In Proc. of AIED '99, 9th World
Conference of Artificial Intelligence and Education, Le Mans, France.
9. Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo, CA: Morgan-Kaufmann.
10. Renkl, A. (1997). Learning from worked-examples: A study on individual differences.
Cognitive Science, 21(1), 1-30.
11. VanLehn, K. (1996). Conceptual and meta learning during coached problem solving. In
C. Frasson, G. Gauthier, & A. Lesgold (Eds.), ITS96: Proc. of the 3rd Int. Conference on
Intelligent Tutoring Systems, Montreal, Canada. New York: Springer-Verlag.
12. Weber, G., & Specht, M. (1997). User modelling and adaptive navigation support in
WWW-based tutoring systems. In Proc. of User Modeling '97.

Eliminating the Gap between the High and Low Students
through Meta-cognitive Strategy Instruction
Min Chi and Kurt VanLehn
Learning Research and Development Center & Intelligent System Program
University of Pittsburgh, PA, 15260
{mic31,vanlehn+}@pitt.edu

Abstract. One important goal of Intelligent Tutoring Systems (ITSs) is to bring
students up to the same level of mastery. We showed that an ITS teaching a
domain-independent problem-solving strategy indeed closed the gap between
High and Low learners, not only in the domain where it was taught (probability)
but also in a second domain where it was not taught (physics). The strategy includes two main components: one is solving problems via Backward-Chaining
(BC) from goals to givens, named the BC-strategy, and the other is drawing
students’ attention on the characteristics of each individual domain principle,
named the principle-emphasis skill. Evidence suggests that the Low learners
transferred the principle-emphasis skill to physics while the High learners
seemingly already had such skill and thus mainly transferred the other skill, the
BC-strategy. Surprisingly, the former learned just as effectively as the latter in
physics. We concluded that the effective element of the taught strategy seemed
not to be the BC-Strategy, but the principle-emphasis skill.
Keywords: Intelligent Tutoring Systems, meta-cognitive skills, domainindependent Problem-Solving Strategies.

1 Introduction
Bloom [2] argued that human tutors not only raised the mean of scores, but also decrease the standard deviation of scores. That is, students generally start with a wide
distribution in test scores; but as they are tutored, the distribution becomes narrower—the students on the low end of the distribution begin to catch up with those on
the high end. Another way to measure the same phenomenon is to split students into
High and Low groups based on their incoming competence. One then measures the
learning gains of both groups. According to Bloom, a good tutor should exhibit an
aptitude-treatment interaction: both groups should learn, and yet the learning gains of
the Low students should be so much greater than the High ones’ that their performance in the post-test ties with the High ones. That is, one benefit of tutoring is to narrow or even eliminate the gap between High and Low.
Previously, we found that Pyrenees [11], an ITS that explicitly taught a problemsolving strategy, was more effective than Andes [12], an ITS that did not explicitly
teach any strategy not only in the domain where it was used, but in a second domain
where it was not used [3]. The strategy seemed to have lived up to our expectations
B. Woolf et al. (Eds.): ITS 2008, LNCS 5091, pp. 603–613, 2008.
© Springer-Verlag Berlin Heidelberg 2008

604

M. Chi and K. VanLehn

and transferred from one domain to another. In this paper, we investigated whether
explicit strategy instruction exhibited an aptitude-treatment interaction, that is,
whether it narrows or even eliminates the gap between High and Low; moreover,
whether both High and Low indeed transferred the strategy to the second domain.

2 Background
A task domain is deductive if solving a problem requires producing an argument,
proof or derivation consisting of one or more inference steps, and each step is the
result of applying a domain principle, operator or rule. Deductive domains are common parts of math and science courses. Two common problem-solving strategies in
deductive domains are forward chaining (FC) and backward chaining (BC) [7]. In FC,
reasoning proceeds from givens toward goals; while in BC, it works backward from
goals to givens. FC and BC have been widely used in computer science; however,
they are rarely observed in a pure form in natural human problem solving. Early studies suggested that novices used BC and experts used FC [5], but later studies showed
that both used fairly similar mixtures [6]. It appears that most human solvers use a
mixture of strategies, heuristics, and analogies with past solutions as well as other
general knowledge. Although human solvers don’t seem to use FC and BC in their
pure form, the strategies' success in guiding computer problem solvers suggests that
teaching human solvers to use FC or BC might improve their problem-solving performance. Several ITS-based studies were conducted to test this hypothesis.
Trafton and Reiser [10] tested the benefits of explicit strategy instruction on an ITS
called Graphical Instruction in Lisp. Three forms of instruction were compared: FConly, BC-only or freely. After 13 training problems were completed in less than one
hour, all three groups achieved the same learning gains. Scheines and Sieg [8] gave
students over 100 training problems in sentential logic and they found students who
were taught and required to use FC or BC learned just as effective as those who were
not taught any strategy. VanLehn et al. [10] compared two ITSs that teach introductory college physics. One system explicitly taught students a version of BC; while the
other did not teach or require students to follow any explicit strategy. Although some
outcome measures differed between groups, overall performance on the post-test was
quite poor, suggesting a floor effect.
In summary, most previous studies were conducted in a single domain and contrasted students who were taught a strategy and those who were not. In this paper, we
investigated the impact of explicit strategy instruction on eliminating the gap between
High and Low across two unrelated domains and two different ITSs. The problemsolving strategy chosen is the Target Variable Strategy (TVS) [11], a domainindependent BC strategy, and the two selected domains were probability and physics.
Probability covered 10 major principles in Axiom of Probability and Conditional
Probability; and physics covered 10 principles in Work and Energy. During probability instruction, the Experimental students were trained on an ITS, Pyrenees, that
explicitly taught the TVS; while the Control students were trained on another ITS,
Andes, without explicit strategy instruction. During subsequent physics instruction,
both groups were trained on the same ITS, which did not teach any strategy. On both
probability and physics post-tests, we expect:

Eliminating the Gap between the High and Low Students

605

High-Experimental = Low-Experimental = High -Control > Low-Control.
That is, for both task domains, the Low students should catch up with the High
students, but only if they were taught the TVS.

3 Methods
3.1 Participants
Participants were 44 college students who received payment for their participation.
They were required to have a basic understanding of high-school algebra, but not to
have taken college-level statistics or physics courses. Students were randomly assigned to the two conditions. Two students were eliminated: one for a perfect score on
the probability pre-test and one for deliberately wasting time.
3.2 Three ITSs
The three ITSs involved in this study were Pyrenees, Andes-probability, and Andesphysics respectively. The first two taught probability while the third taught physics.
Apart from domain knowledge, Andes-probability and Andes-physics were the same
and we use ‘Andes’ to refer to both. Pyrenees required students to follow the TVS
while Andes did not require students to follow any explicit problem-solving strategy.
Next, we will compare Pyrenees and Andes from the perspectives of both the user
interface and students’ behaviors.
User Interfaces Perspectives: Both Pyrenees and Andes provide a multi-paned
screen that consists of a problem-statement window, a variable window for listing
defined variables, an equation window, and a dialog window. The tutor-student interactions are quite different for each system.
Pyrenees is a restrictive tutor-initiative ITS. It guides students in applying the TVS by
prompting them to take each step as dictated by the strategy. For example, when the TVS
determines that it is time to define a variable, Pyrenees will pop up a tool for that purpose. Thus the tutor-student interactions in Pyrenees take the form of a turn-taking dialogue, where the tutor’s turns end with a prompt or question to which the student must
reply and all interactions only takes place in the dialogue window. Andes, on the other
hand, is a nonrestrictive mixed-initiative ITS. Students use GUI tools to construct and
manipulate a solution, so the interaction is event-driven. Students may edit or interact
with any of the four windows: by drawing vectors in vector window, writing or editing
equations in the equation window, and so on. Once an entry or edit has been made successfully, Andes provides no further prompt to make the next step.
Interactive Behaviors Perspectives: Both Andes and Pyrenees provide immediate
feedback. However, their standard of correctness differs. Andes considers an entry
correct if it is true, regardless of whether it is useful for solving the problem; on Pyrenees, however, an entry is considered correct if it is true and strategically acceptable
to the TVS. Moreover, students can enter an equation that is the algebraic combination of several principle applications on Andes but not on Pyrenees because the TVS
requires students to apply one principle at a time.

606

M. Chi and K. VanLehn

Both systems provide hints when students asked. When an entry is incorrect, students can either fix it independently, or ask for what’s-wrong help. When they do not
know what to do next, they can ask for next-step help. Both next-step help and what’swrong help are provided via a sequence of hints that gradually increase in specificity.
The last hint in the sequence, called the bottom-out hint, tells the student exactly what
to do. Pyrenees and Andes give the same what’s-wrong help for any given entry, but
their next-step help differs. Because Pyrenees requires students to follow the TVS, it
knows what step they should be doing next so it gives specific hints. In Andes, however, students can always enter any correct step, so Andes does not attempt to determine their problem-solving plans. Instead, it asks students what principle they are
working on. If students indicate a principle that is part of a solution to the problem,
Andes hints an uncompleted step from the principle application. If no acceptable
principle is chosen, Andes picks an unapplied principle from the solution that they are
most likely to be working on.
3.3 Procedure
The study had 4 main parts: background survey, probability instruction, Andes Interface training, and physics instruction (shown in the left column of Table 1). All materials were online. The background survey asked for High school GPA, SAT scores,
experience with algebra and other information.
Table 1. Experiment Procedure
Part
Survey
Probability Instruction

Andes Interface
Training

Experimental
Control
Background survey
Pre-training
Pre-test
Training on Pyrenees
Training on AndesProbability
Post-test
Solve a probability problem on Andes-Probability
Pre-training

Physics Instruction

Pre-test
Training on Andes-Physics
Post-test

The probability and physics instruction each consisted of four phases: 1) Pretraining, 2) Pre-test,3) Training on the ITS, and 4) Post-test. During pre-training,
students studied domain principles. For each principle, they read a text description,
reviewed some worked examples, and solved some single-principle and multipleprinciple problems. After solving a problem, their answer was marked correct or
incorrect, and the expert’s solution was also displayed. The students then took the
pretests. All students took the same pre- and post-tests. All test problems were

Eliminating the Gap between the High and Low Students

607

open-ended and required students to derive answers by writing and solving one or
more equations. In phase 3, students in both conditions solved the same problems in
the same order, albeit on different ITSs. Each of the domain principles was applied at
least twice in both trainings. The Experimental group learned probability in Pyrenees
and physics in Andes-physics while the Control group learned both domains in Andes. Students could access the domain textbook at any time during the training. Finally, students took the post-tests. On each post-test, 5 problems were isomorphic to a
training problem in phase 3. There were also 5 novel, non-isomorphic multipleprinciple problems on the probability post-test and 8 on the physics post-test.
Only the Experimental students took the third part, Andes Interface Training. Its
purpose was to familiarize them with the Andes GUI without introducing any new
domain knowledge. The problem used was one of the twelve probability training
problems that they had previously solved on Pyrenees. Pilot studies showed that one
problem was sufficient for most students to become familiar with Andes GUI.
To summarize, the procedural differences between the two conditions were: 1) during the probability training, the Experimental condition trained on Pyrenees while the
Control condition trained on Andes-probability; 2) the Experimental students learned
how to use Andes’ GUI before physics instruction.
3.4 Grading Criteria
We used two scoring rubrics: binary and partial credit. Under binary, a solution is
worth 1 point if it was completely correct or 0 if not. Under partial credit, each problem score is a proportion of correct principle applications evident in the solution. If
they correctly apply 4 of 5 possible principles they would get a score of 0.8. Solutions
were scored by a single grader blind to condition.

4 Results
In order to measure aptitude-treatment interaction, we needed to define High and Low
groups based on some measure of incoming competence. We chose to use MSAT
scores because probability and physics are both math-like domains. Our split point
was 640, which divide into: High (n = 20) and Low (n = 22). Except for the MSAT
scores and High school GPA, no significant difference was found between High and
Low on other background information such as age, gender, VSAT scores and so on.
As expected, the High group out-performed the low group during the probability pretraining and the probability pre-test under the binary scoring rubric: t(40)= 3.15, p=
0.003, d= 0.96, t(40)= 2.15, p= 0.038, d= 0.66, and t(40)= 2.27, p <0.03, d=0.70 on
single-principle, multiple-principle problems during probability pre-training and
overall in probability pre-test respectively . The same pattern was found under partial
rubric in the probability pretest. Thus, the MSAT score successfully predicted the
incoming competence of the students, which justifies using it to define our High vs.
Low split.
Incoming competence combined with conditions partitioned the students into four
groups: High-Experimental (n = 10), Low-Experimental (n = 10), High-Control (n =
10), and Low-Control (n = 12). Fortunately, random assignment balanced the

608

M. Chi and K. VanLehn

Experimental vs. Control conditions for ability, and this balance persisted even with
the groups were subdivided into High and Low via MSAT score. On every measure of
incoming competence, no significant difference was found between the Experimental
and Control groups, the Low-Experimental and Low-Control ones, or the HighExperimental and High-Control ones. These measures were: the background survey,
the probability pre-test; probability pre-training scores, the time spent reading the
probability textbook, and the time spent solving the pre-training problems. Averaged
over all students, the total times for each training phase were: 2.4 hrs and 2.7 hrs for
probability pre-training and training; 1.5 hrs and 3.0 hrs for physics pre-training and
training respectively. No significant differences were found among the four groups on
any of these times.
4.1 Test Scores
Figure 1 shows that the test score results are consistent with our hypothesis: after
trained on Pyrenees, the Low-Experimental students scored significantly higher than
their Low-Control peers on all three assessments: probability post-test, physics pretest and physics post-tests: t(20) = 4.43, p < 0.0005, d = 1.90; t(20) = 3.23, p < 0.005,
d = 1.34; and t(20) = 4.15, p < 0.0005, d = 1.84 respectively. More importantly, the
Low-Experimental students even seemed to catch up with the High ones: no significantly difference was found among the High Experimental, Low-Experimental, and
High-Control on all three assessments even though the two Experimental groups
seemed to out-perform the High-Control in Figure 1.
90.00

Low
Control
70.00

Low
Experiment
High
Control

50.00

High
Experiment

30.00

Probability Probability
Pre-test
Post-test

Physics
Pre-test

Physics
Post-test

Fig. 1. Compare four groups on four tests (maximum score = 1)

Thus, explicit strategy instruction in probability caused the Low-Experimental
group to learn more effectively than the Low-Control group during probability training, physics training and even physics pre-training. They seemed to have caught up to
the High ones while the Low-Control ones did not. Moreover, while the HighExperimental group didn’t benefit much from the TVS, they were not harmed either.
Dynamic Assessments
While test results are the most common assessment of learning performance, one can
also compare students’ behaviors as they learn. Such comparisons are called dynamic

Eliminating the Gap between the High and Low Students

609

assessments [2]. In so doing, we can identify students who are effective learners even
though their test scores may be equal to or even lower than others. Here we investigated students’ interactive behaviors on Andes during physics training, as all students
received the identical procedure during that period.
Frequency of help requests: Andes-Physics logs every user’s interface action performed, including help requests, tool usage, and equation entries. We first tried to
characterize the overall difference in students’ solutions via the amount of help they
requested. On each of 8 physics training problems, the Low-Experimental students
made significantly fewer next-steps help requests than the Low-Control ones. No
significant difference was found among the Low-Experimental, the HighExperimental and High-Control groups. This suggests that the Low-Experimental
may have transferred the TVS. However, there are other possible explanations, so we
conducted several other analyses.
Triage of Logs: Solution logs were grouped into 3 categories: smooth, help-abuse,
and rocky:
Smooth solutions included no help requests, except on problems that required more
than eight principle applications. There students were permitted up to two what’swrong help requests.
Help-abuse solutions are produced when every entry was derived from one or more
next-step helps.
Otherwise, the solution was categorized as Rocky because students appeared capable of solving part of the problem on their own, but needed help on the rest.
Figure 2 shows there was a significant difference among four groups on the distribution of the three types of solutions. While no significant difference was found between the High-Experimental and Low-Experimental, there was a significant difference between the Low-Experimental and the High-Control: χ2(2) = 11.74, p(χ2) <
0.003; and between the High-Experimental and High-Control: χ2(2) = 9.06, p(χ2) <
0.01. Qualitatively, the results appear to be: High-Experimental = Low-Experimental
> High-Control > Low-Control.
%solution

help_abuse
rocky
smooth

60
50
40

54.67
48.05

33.77

30

48.96
20

38.96

37.33

High Control

LowExperiment

13.54

42.86

10

37.50

27.27

8.00

9.09

0

Low Control

High Experiment

Fig. 2. Solution Percentage by Type

For a more quantitative measure, we used a smaller unit of analysis: individual
equations. We coded each correct equation entry in the solution logs with 3 features:
Relevance: The equation was labeled relevant or irrelevant based on whether it contributed to the problem solution.

610

M. Chi and K. VanLehn

Help: The equation was labeled “Help” if it was entered after the student asked for
help from Andes-physics. Otherwise, it was labeled “No-help”.
Content: The equation’s content was coded as either “a correct equation with new
physics content” or “others”.
We sought to find out how frequently students made progress toward solving a
problem without asking for any help from Andes. In terms of the three-feature coding
mentioned above, such a “desirable” equation would be coded as “Relevant”, “Nohelp”, and “Correct equation with new physics content”. We called them desirable
steps and defined the desirable steps ratio DSR:
DSR =

Desirable Steps in the solution
All Steps in the solution

DSR =

Desirable Steps in the solution
All Steps in the solution

DSR%
100
80
60
40
20

45.18

61.22

81.81

80.80

0
Low Control High Control
Low
Experimental

High
Experimental

Fig. 3. DSR on overall solutions

As shown in Figure 3, the Low-Experimental had significantly Higher DSR than
the Low-Control: t(169)= 7.50, p<0.0001. In fact, the former even made significantly
more progress than the High-Control: t(150)= 3.84, p< 0.001. While there is a significant difference between the Low-Control and High-Control groups: (t(171)=2.83, p<
0.01), there is no such difference between the two Experimental groups. In short, this
dynamic assessment showed that: High-Experimental = Low-Experimental > HighControl > Low-Control.
To summarize, both test scores and dynamic assessments show that the Low students catch up with the High ones in the Experimental condition but not in the Control
condition. On some measures, the Low-Experimental students even surpass the HighControl ones. Next, we’ll investigate what was transferred from probability to physics
that made the Low-Experimental students so successful?
Transferring the Two Cognitive Skills of the TVS
The TVS is BC problem-solving strategy [11]. That is, it solves problems backwards
from goals to givens. However, it differs from pure BC in that it requires students to
explicitly identify principles before applying them. As an illustration, Table 2 presents
the principle application part of a TVS solution.
Prior work on BC through equations required students to enter the equations alone [1].
Thus, they might only write the equations shown in the middle column of Table 2. Our

Eliminating the Gap between the High and Low Students

611

TVS strategy, however, also requires them to attend to the application of individual domain principles, as shown in the right column of Table 2. For example, instead of simply
entering an equation with one principle application each, students need to pick an unknown variable, select a principle that apply to the unknown variable, define all the variables appearing in its equation if undefined yet, write the equation, and finally remove
the “sought” mark and mark new unknown variables. Students were also asked various
questions on the characteristics of the principle. For example, in last row in Table 2, after
students pick the complement theorem, Pyrenees would ask: “… To apply the principle, you
must have noticed that there are a set of events that are mutually exclusive and collectively exhaustive. What are these events?” Students should answer: ~(A∩B) and (A∩B). Therefore, the

TVS is not only a BC strategy, but it draws students’ attention to the characteristics of
each individual domain principle, such as when it is applicable.
Table 2. Part of a TVS example solution
Problem: Given P(A)=1/3, P(B)=1/4, P(A ∩ B)=1/6, find: P(~A ∪ ~B).
Equations
Justification
Step
1
P(~A ∪ ~B)=P(~(A∩B))
To find P(~A ∪ ~B), apply De Morgan’s theorem.

2

P(A∩B) + P(~(A∩B))=1

Delete “sought” from P(~A ∪ ~B) and add
“sought” to P(~(A∩B))
To find P(~(A∩B)), apply the Complement theorem. Delete “sought” from P(~(A∩B))

In short, we argue that the TVS includes two main components: one is to solve
problems via BC from goals to givens, named the BC-strategy, and the other is to
focus attention to the domain principles, named the principle-emphasis skill. In order
to determine the BC-strategy usage, we analyzed students’ logs to see whether the
order of equations in their solutions follows the BC. For the principle-emphasis skill,
we used the single-principle problems as our litmus test because students who had
applied the BC-strategy would have no particular advantage on them because solving
these single-principle problems only need to apply one principle; while students who
had learned the idea of focusing on domains principles should show an advantage on
them.
Transfer the BC-Strategy: If students engaged in the BC-strategy, we expect they
would apply the BC-strategy when they had difficulties, that is, on rocky solutions.
Whereas on smooth solutions, students don’t have any difficulties since they may
solve problems mainly based on existing schemas [9]. Thus, we subcategorized each
desirable step in the logs as BC or non-BC, where non-BC included FC, combined
equations, and so on. We then defined BC% as the proportion of desirable steps that
were coded as BC. Figure 4 showed that on Rocky solutions the High-Experimental
group applied BC significantly more frequently than the other three groups:
t(40)=2.25, p=0.03 while the Low-Experimental group used the BC as frequently as
the two Control groups. Thus, apparently it was the High-Experimental group alone
who transferred the BC-Strategy to physics.

612

M. Chi and K. VanLehn

90

BC%

On Rocky solutions

60

30

53.42

56.20

58.58

74.00

Low Control

High Control

Low
Experiment

High
Experiment

0

Fig. 4. BC Usage on Rocky Solutions

Transfer of the Principle-Emphasis Skill: The Low-Experimental students scored just
as high as the High-Experimental ones even though they used the BC no more frequently
than two Control groups. Thus, they must have transferred something else of the TVS.
Our hypothesis is that they transferred the principle-emphasis skill. We divided both
post-tests into single-principle and multiple-principle problems. Furthermore, we divided
the multiple-principle problems into those that were isomorphic to a training problem and
those that were not. If the Low-Experimental group applied the principle-emphasis skill,
we expected them to out-perform the Low-Control group on all of them in both posttests. This turned out to be the case (see Figure 5). It suggests that the main effect of
teaching the TVS to the Low students was to get them to focus on the domain principles.
Further analysis showed no significant difference among the High-Control, the LowExperimental, and High-Experimental on any types of problems, which indicates that
High students may already have such skill.
1
0.93

0.8
0.6

Low-Control
Low-Experimental

0.70

0.89

0.48

0.70

0.93

0.91

0.45

0.44

0.80
0.60

0.4
0.19

0.2
0.23

Single

Multiple,
isomorphic

0.11

0.17

0

Multiple,
Nonisomorphic

Probability Post-test

Single

Multiple

Physics Pre-test

0.18

Single

Multiple,
isomorphic

Multiple,
Nonisomorphic

Physics Post-test

Fig. 5. Scores on Three Types of Problems in Both Tests

5 Conclusions
Overall, we found teaching students the TVS indeed exhibited an aptitude-treatment
interaction in deductive domains: the gap between High and Low students in the Experimental Condition seemed to be eliminated in both probability and physics. Although the two Experimental groups performed equally well in both physics pre- and
post-tests, the Low-Experimental group transferred the principle-emphasis skill to

Eliminating the Gap between the High and Low Students

613

physics while the High-Experimental apparently already possessed it and thus they
mainly transferred the BC-strategy.
These results suggest that it is not the BC-strategy that is most important to teach
Low learners. Instead, one should teach the meta-cognitive skill of focusing on individual principle applications. It could be that Low and High learners may have differed initially in that Low students lacked this "how to learn" meta-cognitive knowledge for a principle-based domain like probability or physics. Such results suggest
building an ITS that does not teach the TVS explicitly, but instead just teaches to
focus on principle applications in deductive domains. Perhaps it would be just as
effective as Pyrenees. Indeed, because its students need not learn all the complicated
bookkeeping of the BC-strategy, which may cause cognitive overload [9], it might
even be more effective than Pyrenees not only for an initial domain where the ITS
was used but subsequent domains where it is not used.

References
1. Bhaskar, Simon: Problem solving in semantically rich domains: An example from engineering thermodynamics. Cognitive Science 1, 193–215 (1977)
2. Bloom: The 2 sigma problem: The search for methods of group instruction as effective as
one-to-one tutoring. Educational Researcher 13, 4–16 (1984)
3. Chi, VanLehn: Accelerated future learning via explicit instruction of a problem solving
strategy. In: 13th International Conference on AIED, pp. 409–416 (2007)
4. Haywood, Tzuriel: Applications and challenges in dynamic assessment. Peabody Journal
of Education 77(2), 40–63 (2002)
5. Larkin, McDermott, Simon, Simon.: Expert and novice performance in solving physics
problems. Science 208, 1335–1342 (1980)
6. Priest, Lindsay: New Light On Novice-Expert Differences in Physics Problem-solving.
British Journal of Psychology 83, 389–405 (1992)
7. Russell, Norvig: Artificial Intelligence: A Modern Approach, 2nd edn. Prentice-Hall, Upper Saddle River (1995)
8. Scheines, Sieg: Computer environments for proof construction. Interactive Learning Environments 4(2), 159–169 (1994)
9. Sweller: Cognitive Technology: Some Procedure for Facilitating learning and Problemsolving in mathematics and Science. Journal of EdPsych 81(4), 81–84 (1989)
10. Trafton, Reiser: Providing natural representations to facilitate novices’ understanding in a
new domain: Forward and backward reasoning in programming. In: Proceedings of the
13th Annual Conference of the Cognitive Science Society, pp. 923–927 (1991)
11. VanLehn, et al.: Implicit versus explicit learning of strategies in a non-procedural cognitive skill. In: 7th Conference on ITS, Maceio, Brazil (2004)
12. VanLehn, et al.: The Andes physics tutoring system: Lessons learned. International Journal
of Artificial Intelligence and Education 15(3), 147–204 (2005)

Using HCI Task Modeling Techniques
to Measure How Deeply Students Model
Sylvie Girard, Lishan Zhang, Yoalli Hidalgo-Pontet, Kurt VanLehn,
Winslow Burleson, Maria Elena Chavez-Echeagaray, and Javier Gonzalez-Sanchez
Arizona State University, Computing, Informatics, and Decision Systems Engineering,
Tempe, AZ, 85281, U.S.A.
{sylvie.girard,lzhang90,yhidalgo,kurt.vanlehn,
winslow.burleson,helenchavez,javiergs}@asu.edu

Abstract. User modeling in AIED has been extended in the past decades to include affective and motivational aspects of learner’s interaction in intelligent tutoring systems. In order to study those factors, various detectors have been
created that classify episodes in log data as gaming, high/low effort on task, robust learning, etc. In this article, we present our method for creating a detector
of shallow modeling practices within a meta-tutor instructional system. The detector was defined using HCI (human-computer interaction) task modeling as
well as a coding scheme defined by human coders from past users’ screen recordings of software use. The detector produced classifications of student behavior that were highly similar to classifications produced by human coders with a
kappa of .925.
Keywords: intelligent tutoring system, shallow learning, robust learning, human-computer interaction, task modeling.

1

Introduction

Advances in student modeling in the past two decades enabled the detection of various cognitive [1], meta-cognitive [2], and affective [4] processes during learning
based on classification of episodes in log data. Steps have been taken toward detecting when learning occurs [1] and to predict how much of the acquired knowledge
students can apply to other situations [2]. However, an obstacle in such research is the
lack of generality of the detectors for tutoring systems involving problem solving
tasks, especially when trying to gain an understanding of the user’s cognitive or metacognitive processes while learning. While some of the indicators used in the literature
are common to any intelligent tutoring system, others are closely linked to the activities and pedagogical goals of a specific application. The adaptation of such indicators
from one application to another often necessitates a detailed analysis of the new domain and how the tutoring system guides learners to acquire its skills and knowledge.
We view the specificity of detectors as unavoidable, so the best solution is to develop
good methods for analyzing the new tutoring system and designing the detectors.
This short article describes our method and its application to AMT.
K. Yacef et al. (Eds.): AIED 2013, LNAI 7926, pp. 766–769, 2013.
© Springer-Verlag Berlin Heidelberg 2013

Using HCI Task Modeling Techniques to Measure How Deeply Students Model

767

AMT teaches students how to create and test a model of a dynamic system. The instruction is divided into three phases: (1) an introduction phase where students learn
basic concepts of dynamic system model construction and how to use the interface;
(2) a training phase where students are guided by a tutor and a meta-tutor to create
several models; and (3) a transfer phase where all scaffolding is removed from
software and students are free to model as they wish. The tutor gives feedback
and corrections on domain mistakes. The meta-tutor requires students to follow a
goal-reduction problem solving strategy, the Target Node Strategy [6], which decomposes the overall modeling problem into a series of “atomic” modeling problems
whose small scope encourages students to engage in deep modeling rather than shallow guess-based modeling strategies. To assess students, the project needed detectors
that detect shallow and deep modeling practices both with and without the meta-tutor.

2

Task Modeling: Analysis of User’s Actions on Software

A task model is a formal representation of the user’s activity in an interactive system.
It is represented by a hierarchical task tree to express all sub-activity that enables the
user to perform the planned activity. The tasks need to be achieved in a specific order,
defined in the task tree by the order operators. In AMT, every modeling activity follows the same procedure involving the same help features, task flow, and meta-tutor
interventions. With a single task model of a prototypical modeling task, it is therefore
possible to account for all of the user’s activity in software. The task modeling language K-MAD and its task model creation and simulation environment, K-MADe [3]
were chosen because they enable the creation and replay of scenarios of student’s
actions and they enable a formal verification of the model.
The task model developed with K-MADe was used to define the episode structure.
This established the unit of coding to be used in the next phase. Screen videos
representing the learners’ use of the AMT software with and without the meta-tutor
were recorded during an experimental study described in [6]. These videos were studied to determine how much shallow vs. deep modeling occurred and the contexts,
which tended to produce each type. A coding system was then created for video recordings of the learners’ behavior. Three iterations of design for this coding scheme
were performed, ending with a coding scheme that reached a multi-rater pairwise
kappa of .902. The final coding scheme mapped learners’ behavior to six classifications, which were implemented as the following depth detectors:
• GOOD_METHOD: The students followed a deep method in their modeling. They used the help tools appropriately, including the one for planning
each part of the model.
• VERIFY_INFO: Before checking their step for correctness, students
looked back at the problem description, the information provided by the instruction slides, or the meta-tutor agent.
• SINGLE_ANSWER: The student’s initial response for this step was
correct, and the student did not change it.

768

S. Girard et al.

• SEVERAL_ANSWERS: The student made more than one attempt at
completing the step. This includes guessing and gaming the system:
o The user guessed the answer, either by clicking on the correct answer by mistake or luck, or by entering a loop of click and guessing to find
the answer.
o The user “games the system” by using the immediate feedback given to guess the answer: series of checks on wrong answers that help deduce
the right answer.
• UNDO_GOOD_WORK: This action suggests a modeling misconception
on the students’ part. One example is when students try to run the model
when not all of the nodes are fully defined.
• GIVEUP: The student gave up on finding how to do a step and clicked on
the “give up” button.
Another detector was defined as a linear function of the six episode detectors. It was
intended to measure the overall depth of the students’ modeling, therefore providing
an outcome measure in the transfer phase in future experimental studies. It considered two measures (GOOD_ANSWER, VERIFY_INFO) to indicate deep modeling,
one measure (SINGLE_ANSWER) to be neutral, and three measures
(SEVERAL_ANSWERS, UNDO_GOOD_WORK, and GIVE_UP) to indicate shallow modeling.
Once the coding scheme reached a sufficient level of agreement between coders,
the task model was used to adapt the coding to students’ actions on the software. The
episodes that were coded for depth by human analysts in the sample video were analyzed by creating scenarios from the task model within K-MADe. The validation of
six detectors’ implementation involved three human coders, who watched a sample of
50 episodes, paying attention to the depth of modeling exhibited by the student’s actions, and chose the classification that best represented the depth of the learner modeling at the time of the detected value. A multi-rater and pairwise kappa was then
performed, reaching a level of inter-reliance of .925.

3

Conclusion and Future Work

In this paper, a method to create a detector of deep modeling within a meta-tutor using HCI task modeling and video coding schemes was described. The main outcome
of this process was the creation of detectors inferring the depth of students’ modeling
practices while they learn on a meta-tutoring system, reaching a multi-rater and pairwise kappa score of .925. One use of the detectors was to consider the proportion of
shallow versus deep learning as an outcome measure in the transfer phase. This was
used as a dependent measure of shallow learning in an experimental study investigating the effectiveness of the meta-tutor versus the original interface, described in [6].
The second use of the detectors was to help drive the behavior of an affective learning
companion in the current phase of the AMT project [5]. A limitation of the method
however is the applicability to different types of tutoring systems. In AMT, a single
task model was able to represent the entirety of a users’ learning activity. In tutoring

Using HCI Task Modeling Techniques to Measure How Deeply Students Model

769

systems that teach a set of skills through different pedagogical approaches for diverse
types of learning tasks, the creation of such task models might prove more costly and
may not be completely adapted to the creation of detectors that need to be adapted to
each task specifically.
Acknowledgements. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221. We would like to thank Sybille
Caffiau for consulting in the project and sharing her expertise in task modeling of
interactive systems.

References
1. Baker, R.S.J.d., Goldstein, A.B., Heffernan, N.T.: Detecting the moment of learning. In:
Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010, Part I. LNCS, vol. 6094, pp. 25–34.
Springer, Heidelberg (2010)
2. Baker, R.S.J.d., Gowda, S.M., Corbett, A.T., Ocumpaugh, J.: Towards automatically detecting whether student learning Is shallow. In: Cerri, S.A., Clancey, W.J., Papadourakis,
G., Panourgia, K. (eds.) ITS 2012. LNCS, vol. 7315, pp. 444–453. Springer, Heidelberg
(2012)
3. Caffiau, S., Scapin, D., Girard, P., Baron, M., Jambon, F.: Increasing the expressive power
of task analysis: Systematic comparison and empirical assessment of tool-supported task
models. Interacting with Computers 22(6), 569–593 (2010)
4. D’Mello, S.K., Lehman, B., Person, N.: Monitoring affect states during effortful problem
solving activities. International Journal of Artificial Intelligence in Education 20(4), 361–
389 (2010)
5. Girard, S., Chavez-Echeagaray, M.E., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., Zhang,
L., Burleson, W., VanLehn, K.: Defining the behavior of an affective learning companion
in the affective meta-tutor project. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P.
(eds.) AIED 2013. LNCS (LNAI), vol. 7926, pp. 21–30. Springer, Heidelberg (2013)
6. Zhang, L., Burleson, W., Chavez-Echeagaray, M.E., Girard, S., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., VanLehn, K.: Evaluation of a meta-tutor for constructing models of dynamic systems. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P. (eds.) AIED 2013.
LNCS (LNAI), vol. 7926, pp. 666–669. Springer, Heidelberg (2013)

XTutor: An Intelligent Tutor System for Science
and Math Based on Excel
Roxana Gheorghiu and Kurt VanLehn
Department of Computer Science, University of Pittsburgh
Sennott Square, Pittsburgh, PA 15260, USA
{roxana,vanlehn}@cs.pitt.edu

Abstract. VanLehn argued that an essential feature of many intelligent
tutoring systems (ITSs) is that they provide feedback and hints on every
step of a multi-step solution.But if step-based feedback and hints alone
suﬃce for strong learning gains, as Anderson et al. conjecture ([1]), then
perhaps a lightweight tutoring system that employ only feedback and
bottom-out hints would have advantages. This motivates the current
project.Using Excel there are some immediately advantages that can be
obtained: most people is familiar with its user interface and its notation
for mathematical expressions, Excel already contains facilities for solving
some systems of equations and it can be easy combined with many other
pieces of software, making it easier for instructors to include the tutor
in their course activities. Finally, web-based delivery is simple because
most students already have and use Excel.

1

Introduction

It was already tested on some well-kown ITS that providing feedback and hints
on every step of a multi-step solution is an essential feature. It is not clear yet
whether step-based feedback and hints alone suﬃce for strong learning gains, as
Anderson at al. conjecture ([1]).
We wonder if the spreadsheet program Excel would allow easy construction
of an ITS that would teach the same class of equation-based task domains as
Andes (VanLehn at al. [3]) and Pyrenees (VanLehn et al. [2]). In these task
domains, students solve a problem by deﬁning variables, writing equations and
ﬁnally solving the system of equations algebraically.

2

Using XTutor

As Fig. 1 illustrates, a problem in our Excel-based tutor, XTutor, is presented
as an Excel worksheet with the statement of the problem on the ﬁrst row of
the sheet. Rows three and four brieﬂy explain what should be entered in each
column and how. All the other rows are ﬁlled in by the student.
Figure 1 shows the worksheet after it has been ﬁlled out by the student. Each
row has:
B. Woolf et al. (Eds.): ITS 2008, LNCS 5091, pp. 749–751, 2008.
c Springer-Verlag Berlin Heidelberg 2008


750

R. Gheorghiu and K. VanLehn

Fig. 1. Example of a problem in XTutor

1. a variable name that is automatically ﬁlled in by the tutor when the student
selects its deﬁnition
2. a deﬁnition for the variable that is selected from a problem-speciﬁc menu of
all possible variables deﬁnitions.
3. an algebraic formula, typed in by the student, to compute the variables value
4. a numerical value for the variable, ﬁlled in by the tutor, as the result of the
previously introduced formula
5. units for the value, typed in by the student
6. the name of the principle that justiﬁes the algebraic expression used for the
variables value or other type of short justiﬁcation selected by the student
from a list of all possible justiﬁcations deﬁned for that particular problem.
XTutor provides feedback on each student entry. Variables used in a formula
that were not previously deﬁned are signaled by a #Name type of error in
the Value ﬁeld. Algebraic expressions, units and justiﬁcations are also given
immediate feedback. Moreover, if the student asks for a hint on a cell, the system
can give a sequence of hints, where the ﬁnal bottom out hint says exactly what
should be entered in the cell.
At the end, the < Done > button checks if the problem was correctly and
completely solved.

3

Authoring Domain Knowledge for XTutor

XTutor is an example-tracing type of tutor. That is, the domain knowledge of
the system consists of a set of examples, one per problem. The example contains
at least one and possibly several complete solutions to the problem.
The author enters an example exactly as a student would when solving the
problem, except that the author must enter all the cells by herself. This suﬃces
for giving immediate feedback and bottom-out hints. If the author wants any
hints besides the bottom out hint on a cell, then the hints must be entered for
that cell speciﬁcally.

XTutor: An Intelligent Tutor System for Science and Math Based on Excel

4

751

Positive and Negative Features of XTutor

The main advantage of XTutor is its simplicity, for both students and authors. It
takes only minutes to learn how to use it. A second feature is that once students
have seen how to use Excel in this fashion to solve problems, they can continue
to do so without the help of XTutor. Thus, we expect signiﬁcant transfer from
XTutor to unsupported problem solving using Excel.
However, there are certainly drawbacks to XTutor. In a solution, every variable appears in two or more equations. It is sometimes not clear to the student
which equation should be written in the row for a variable. Students often prefer to work forwards from the given values. Usually, this eliminates all choices.
However, even using this strategy, choices sometimes remain.
Take, for example, the following problem: How many liters of 70% alcohol solution must be added to 50 liters of 40% alcohol solution to produce 50% alcohol
solution? Suppose the student is working on a row for tot mix, which represents
quantity, in liters, of the mixture. There are two choices: tot mix=tot low+tot high
or tot mix=alc mix/pr mix, where tot low and tot high are the total quantity of
low and high concentration solutions, alc mix is the total quantity, in liters, of alcohol in the mixture solution and pr mix is the percentage of alcohol in the mixture solution. If the student picks the ﬁrst version then she must ﬁrst think what
formula to assign to tot high (or tot low ) since tot high=tot mix-tot low is not permitted anymore. Similar problems appear if the second formula is used to deﬁne
tot mix. When assigned to more skilled solvers, such problems may actually be
beneﬁcial, as they encourage students to plan their solutions before writing them
down. Thus, we see this feature as positive but with a potential for misuse.
The domain generality of XTutor has already been tested in a preliminary way
by authoring problems in three task domains: physics, math and chemistry. The
next step is to try it out on students, ﬁrst in pilot studies and then in comparison
to paper-and-pencil and an established tutoring system, such as Andes.

References
1. Anderson, J.R., Corbett, A.T., Koedinger, K.R., Pelletier, R.: Cognitive Tutors:
Lessons Learned. The Journal of the Learning Sciences 4(2), 167–207 (1995)
2. VanLehn, K., Bhembe, D., Chi, M., Lynch, C., Schulze, K., Shelby, R., et al.: Implicit vs. explicit learning of strategies in a non-procedural skill. In: Lester, J.C.,
Vicari, R.M., Paraguaca, F. (eds.) Intelligent Tutoring Systems: 7th International
Conference, pp. 521–530. Springer, Berlin (2004)
3. VanLehn, K., Lynch, C., Schultz, K., Shapiro, J.A., Shelby, R.H., Taylor, L., et
al.: The Andes physics tutoring system: Lessons learned. International Journal of
Artiﬁcial Intelligence and Education 15(3), 147–204 (2005)
4. Mitrovic, A., Suraweera, P., Martin, B.: Authoring constraint-based tutors in ASPIRE. In: Ikeda, M., Ashley, K., Chan, T.-W. (eds.) ITS 2006. LNCS, vol. 4053, pp.
41–50. Springer, Heidelberg (2006)

CHI 9 9 1 5 - 2 0

MAY

ACM ISBN: 1-58113-158-5

1999

Panels

Third Generation Computer Tutors:
Learn from or Ignore Human Tutors?
Albert Corbett

John Anderson

Art Graesser

Carnegie Mellon University
Pittsburgh, PA 15213
412-268-8808
corbett+@cmu.edu

Carnegie Mellon University
Pittsburgh, PA 15213
412-268-2788
ja+@cmu.edu

The University of Memphis
Memphis, TN 38152
901-678-2742
a-graesser@memphis.edu

Organizer and Moderator

Panelist

Panelist

Ken Koedinger

Kurt VanLehn

Carnegie Mellon University
Pittsburgh, PA 15213
412-268-7667
koedinger+ @cmu.edu

The University of Pittsburgh
Pittsburgh, PA 15260
412-624-7458
vanlehn+ @pitt.edu

Panelist

Panelist

ABSTRACT

Current "second generation or "intelligent" computer tutors
are approximately one-half as effective as human tutors.
How will we develop the next generation of computer
tutors that approaches human tutor effectiveness? Does
success lie in understanding and emulating the performance
of human tutors? If so, should we focus on natural
language dialog or human tutor pedagogy? Alternatively,
does computer technology afford effective instructional
interventions, unavailable to human tutors?
Can we
modify learning activities and monitor student problem
solving in ways that human tutors cannot.
Keywords

Intelligent Tutoring Systems,
Instructional Interface
Design, Cognitive Modeling, Education
INTRODUCTION

Individual human tutoring is perhaps the oldest form of
instruction. Countless millennia since its introduction, it
remains the most effective and most expensive form of
instruction. Thirty years ago "first generation" computer
tutors began to appear that presented text, asked questions
and provided feedback. With ensuing advances in interface
technology, these tutors have culminated in a variety of
hypertext and multimedia environments today.
Meta
analyses show that first generation tutors are as much as
1/4 as effective as human tutors.
Fifteen years ago "second generation" computer tutors
began to appear that incorporate artificial intelligence
problem solving technology. The most common form,
coached practice environments, reflect a "leaming by doing"

pedagogical theory. These tutors assign the students
a multi-step problem, such as solving a complex algebra
word problem, then coach the student as needed with
positive feedback, negative feedback or hints. We can
reliably develop intelligent coached practice environments
that are one-half as effective as human tutors.
TOPIC:

THIRD GENERATION

TUTORS

How will we develop "third generation" tutors that approach
the effectiveness of human tutors? The primary challenge
we face in improving learning outcomes is "shallow
learning", roughly speaking learning that is sufficient to
score satisfactorily on conventional tests, but does not
transfer to more realistic situations.
While the challenge of shallow learning is commonly
acknowledged, there is little agreement on
the most
effective and efficient line of attack on the problem. The
fundamental issue that divides researchers is whether or not
success lies in understanding and emulating human tutor
performance. The case for emulating human tutors is this:
We lack a proven general pedagogical theory and human
tutors remain the best model of an effective pedagogical
system. We need to learn what features of human tutor
behavior significantly affect learning outcomes to guide
computer tutor design. The counter argument is that
computational strengths of hardware and wetware remain
substantially different and that computer technology may
afford effective instructional interventions unavailable to
human tutors and vice versa.
PANEL

FORMAT

The panel is organized into three rounds. It will begin with
a brief overview by the moderator, followed by a 10-minute
statement by each panelist on one of the hypotheses above.
The second round will be more debate-like in character.
Each panelist will display a summary slide of his position
and will face rebuttal challenges from the other panelists.
The third round of discussion will be driven by audience
questions.

85

Panels

ACM ISBN: 1-58113.1 58-5

MODERATOR
A l b e r t C o r b e t t is co-director of the Pittsburgh Advanced

Cognitive Tutor Center at Carnegie Mellon University. He
has been developing intelligent tutors that have been in
continuous classroom use since 1985 and has completed
extensive empirical evaluations of intelligent tutor design.
PANELIST
POSITIONS
is professor
John Anderson

of Psychology and
Computer Science. His ACT theory of human cognition
inspired a series of successful computer tutors and he is
now exploring the relation between perception and
cognition in learning.
Standard tutoring methodology only has access to input
from the student every few seconds (at best) as a new item
is typed or selected from a menu. In contrast, human tutors
have a much richer perceptual access to the student, being
able to monitor expressions on the student's face, where the
student is looking, and any off-hand remarks that the
student might make. On the basis of this information
humans can tell when the student is paying attention, when
the student is confused, and what part of the problem the
student is trying to solve -- judgments which the standard
computer tutor often cannot make without intruding on the
student with a set of questions.
However, modem
technology for eye tracking, face recognition, and speech
recognition allow the possibility of high-density sensing giving the computer as much (or more) perceptual
information about the student as is available to a human
tutor. As an example, we will demonstrate how eye
movement data can enable a computer tutor to more
accurately diagnose what a student is doing than a human
tutor can. More generally, we will discuss the advantages
of high-density sensing for interpreting a student and the
kinds of instructional interventions it might permit.

CHI 9 9 1 5 - 2 0

MAY

1999

approximate estimate of student knowledge rather than
detailed student modeling. AutoTutor communicates via
printed text, synthesized speech, graphic displays,
animation, and facial expressions on a talking head.
Ken Koedinger is co-director of the Pittsburgh Advanced
Cognitive Tutor Center at Carnegie Mellon University. He
has been developing and evaluating intelligent tutoring
systems for mathematics and science since 1984.

A working definition of deep understanding is the creation
of multiple redundant representations of, or pathways to,
relevant knowledge. Shallow learning processes typically
result in perceptually-based knowledge representations.
Thus, one approach to deeper understanding may be helping
students to articulate their knowledge in verbal
representations. Here we expect students to not only know,
but also know what they know.
We have been
experimenting with this meta-cognitive approach in the
context of the PACT Geometry Tutor. We have found that
geometry students are often better able to provide numerical
answers for steps in a problem than they are able to
articulate the justifications or reasons for those steps. In
response to these observations, we designed an alternative
version of the Geometric Properties unit of the tutor. In
the new "Reason Tutor", students were asked to not only
provide numeric answers to steps, but also to provide
reasons for each step. In a comparative evaluation of these
tutor versions, we found students using the Reason Tutor
performed better at post-test in providing reasons for steps,
in providing answers, and in answering transfer items
designed to foil shallow reasoning.
Kurt VanLehn is director of CIRCLE, an NSF Center at
the University of Pittsburgh, and has been conducting
empirical studies of human tutors and developing computer
tutors for many years.

Art G r a e s s e r is co-director of the Institute for Intelligent

Systems at the University of Memphis. He has collected
extensive empirical data on human tutors and has applied
the lessons learned to the development of computer tutors.
A key feature of effective tutoring lies in generating
discourse contributions that assist learners in the active
construction of subjective explanations, elaborations, and
mental models of the material. Human tutors assist this
subjective construction of knowledge by delivering
collaborative discourse moves: questioning,
immediate
feedback, pumping, prompting, splicing, correcting,
hinting, elaborating, requestioning, and summarizing.
Similarly, intelligent tutors of the future should serve as a
"discourse prosthesis" that provides a collaborative
construction of knowledge. The Tutoring Research Group
at the University of Memphis has developed a fully
automated computer tutor (called AutoTutor) that generates
dialog moves that are pragmatically smooth and
pedagogically effective.
AutoTutor attempts
to
"comprehend" text that the learner types into the keyboard
and to formulate appropriate discourse contributions. The
generation of such dialog moves requires only an

86

It appears that micro-adaptation may be responsible for the
differences between human and computer tutors. Microadaptation is a feedback loop that occurs at the level of
conversational turns in tutorial dialog. If the tutor thinks
that the student isn't understanding the tutor's point, the
tutor often tries to make it in a different way. Although
higher level feedback loops exist in tutoring, they seem to
insure at best a shallow understanding of the material:
enough to prevent errors on these problems, but not enough
to prevent errors on transfer problems. Transcripts of both
human and computer tutors suggest that they implement
micro-adaptation very differently. Human tutors seem
much better at monitoring the students' comprehension and
adapting the remediation to the student. In part, this is just
the natural skill of anyone engaged in conversation. If
hearers don't understand, then speakers usually explain their
point again differently. This comprehension-feedback loop
may explain why inexperienced human tutors are somewhat
more effective than computer tutors. However, skilled
tutors seem to have a larger arsenal of tactics for getting
difficult points across, and they are better at recognizing
when students have confusions that require these tactics.

Minimally Invasive Tutoring of Complex Physics
Problem Solving
Kurt VanLehn1, Collin Lynch1, Linwood Taylor1, Anders Weinstein1,
Robert Shelby2, Kay Schulze2, Don Treacy2, and Mary Wintersgill2
1 LRDC, University of Pittsburgh, Pittsburgh, PA USA 15260
VanLehn@cs.pitt.edu, lht@lzri.com, {collinL,andersw}@pitt.edu
http://www.pitt.edu/~vanlehn/andes.html
2 US Naval Academy, Annapolis, MD, USA
{Shelby, Schulze, Treacy, MWinter}@usna.edu

Abstract. Solving complex physics problems requires some kind of knowledge
for selecting appropriate applications of physics principles. This knowledge is
tacit, in that it is not explicitly taught in textbooks, existing tutoring systems or
anywhere else. Experts seem to have acquired it via implicit learning and may
not be aware of it. Andes is a coach for physics problem solving that has had
good evaluations, but still does not teach complex problem solving as well as
we would like. The conventional ITS approach to increasing its effectiveness
requires teaching the tacit knowledge explicitly, and yet this would cause Andes
to be more invasive. In particular, the textbooks and instructors would have to
make space in an already packed curriculum for teaching the tacit knowledge.
This paper discusses our attempts to teach the tacit knowledge without making
Andes more invasive.

1 Objectives
The Andes project [1-3] began with three objectives [4]. The first was to improve the
learning of university physics students. This goal has been accomplished. In largescale field evaluations at the US Naval Academy over the last three years, students
who did their homework with Andes learned significantly more than students who did
similar homework on paper. These results are discussed below.
The second objective was to see if Andes could be minimally invasive. In particular, could students adopt Andes for doing their homework while virtually nothing else
in the physics course changed? The professors would give the same lectures, use the
same textbooks, assign the same homework problems and conduct the same labs and
recitations. We think that Andes will have a much wider impact if it can be used with
many kinds of teaching, both conventional and reformed. This goal has been difficult
to achieve, and our progress is discussed below.
The third objective was to test three help systems. The Conceptual Helper is called
when Andes decides that the student is unfamiliar with a specific principle of physics
or has a misconception. The Conceptual Helper uses “minilessons” that are adapted
to the context of the student’s problem solving. Students using Andes with the ConS.A. Cerri, G. Gouardères, and F. Paraguaçu (Eds.): ITS 2002, LNCS 2363, pp. 367–376, 2002.
© Springer-Verlag Berlin Heidelberg 2002

368

K. VanLehn et al.

ceptual Helper learned more than students using a version of Andes with ordinary hint
sequences [5, 6]. Recent work has replaced the expository minilessons of the Conceptual Helper with natural language dialogs run by Atlas, and has shown that this
results in an improvement in student understanding [7].
The Self-Explanation Coach coaches students as they study a solved physics problem (i.e., an example). In order to determine if the student has self-explained an example adequately, it monitors the location and latency of the student’s visual attention.
If it appears that the student fails to self-explain specific key aspects of the example,
the coach guides the student in doing so. Some students who used the SE Coach
learned more than students who studied the same examples without the coach [8, 9].
The Procedural Helper answers help requests while students are solving problems
[10]. In particular, if the student gets stuck and asks, “What do I do next?” the Procedural Helper will suggest a goal or action. If Andes marks a student entry wrong and
the student asks, “What’s wrong with that?” the Procedural Helper gives advice based
on determining whether the error is the result of either an incorrect inference or a
correct inference that does not lead to the goal. The Procedural Helper has been
evaluated repeatedly, as described below. Although it is improving, it is still not as
effective as we would like.
In short, although we have achieved many of our objectives, two remain: reducing
the invasiveness of Andes and increasing the effectiveness of the Procedural Helper.
This paper reviews our progress towards achieving those goals.

2 The Andes1 User Interface
This paper only discusses the part of Andes that coaches students as they solve problems. A typical physics problem and its solution on the Andes screen are shown in
Figure 1. Students read the problem (top of the upper left window), draw vectors and
coordinate axes (bottom of the upper left window), define variables (upper right window) and enter equations (lower right window). These are exactly the actions that
they should do when solving physics problems with pencil and paper. The main difference between Andes and paper are:
1. Andes gives immediate feedback by turning correct entries green and incorrect
entries red. If a red entry is the result of a low-level error (e.g., illegal algebraic syntax), an error message pops up saying so.
2. Andes answers “what should I do next?” and “what’s wrong with that?” help
requests. Most help requests are answered by canned-text and menu dialogues
in the tutor window (lower left). If Andes determines that the student has
flawed physics knowledge, it invokes either the Conceptual Helper, which
conducts a hypertext-based minilesson, or Atlas, which conducts a natural
language dialogue in the tutor window.
3. Andes will solve algebraically the equations that the student has entered, provided that student has entered enough correct ones.
4. Variables must be defined before they are used in equations, and the only way
to define certain variables is to draw vectors and/or coordinate axes.

Minimally Invasive Tutoring of Complex Physics Problem Solving

369

With the exception of the fourth point, which will be discussed later, Andes acts simply as an unobtrusive but helpful piece of paper. This should facilitate transfer to
unsupported, paper-based problem solving.

Fig. 1. The Andes screen.

3 The Andes1 Procedural Help System
Two major versions of Andes have been developed. From the student’s view, they
differed only in the kind of help they gave when asked “What’s next?” or “What’s
wrong?” That is, they had the same user interface but completely different Procedural
Help systems. This section briefly describes the first version of Andes, called Andes1.
Subsequent sections discuss its evaluation, then Andes2 and its evaluation.
When the student was stuck and asked Andes1 “What should I do next?”, it would
first select a target step then give a sequence of hints intended to suggest that action to
the student. The hint sequences were similar to those used by many intelligent tutoring systems, but the method of selecting a target step was unique. Andes1 precomputed all possible solutions to the problem and stored them as a Bayesian network
called the solution graph [11, 12]. The nodes in the solution graph represented goals,
facts and other propositions, as well as the inferences (rule applications) that con-

370

K. VanLehn et al.

nected the propositions. Andes1 searched the solution graph for a target step as follows [10]. Starting from the student’s most recent correct action, it found a goal in the
network that was likely to be the one dominating that action. It then searched for another action dominated by that goal that had not yet been entered and was probably
one that the student didn’t know how to do. This was selected as the target step.
If the student made an incorrect entry and asked Andes1 “What’s wrong with
that?”, it again selected a target step and gave a hint sequence. The target step was
selected by comparing the incorrect entry to all possible entries of that type and selecting the closest matching one [13]. The position of the target step in the solution
graph was not used.

4 Evaluations of Andes1
The first full-scale evaluation of Andes1 occurred during the fall of 1999 as part of the
regular US Naval Academy physics course. For about 6 weeks, 173 students used
Andes1 to do their homework, and 162 students did their homework using pencil and
paper. The Andes1 students scored significantly higher (t=2.2, p=.036) on a midterm
exam given at the end of the intervention period, and the effect size was a modest 0.21
standard deviations [14].
More detailed evaluations were undertaken to find ways to improve Andes1. Students at the University of Pittsburgh who had recently taken physics were asked to
solve problems on Andes while giving a verbal protocol. They were often totally lost
and seldom found Andes’ help useful.
In order to get a more formal evaluation at this level of detail, we extracted from
the log files 40 episodes where a student had asked Andes1 “what’s wrong?” with an
equation. We printed screen snapshots just before Andes1 gave its advice. Working
independently, the three USNA physicists wrote on the snapshots the advice that they
would give to the student. Often the advice was somewhat different, but on 21 snapshots their advice was the same. However, Andes’ advice was the same as the physicists’ advice on only 3 of the 21 snapshots.
Two patterns stood out in the physicists’ advice. One was that they often insisted
that students do any steps that they had skipped in the procedure for applying a principle. For instance, many students had trouble writing a correct equation for Newton’s
second law, and they had skipped drawing the force and acceleration vectors. Instead
of helping the students correct their equation, the physicists would usually ask the
students to draw the missing vectors.
The second pattern was that when the student was lost, the physicists would not just
select a target step and hint it as Andes would. Instead, they would help the students
infer a target step themselves via a dialogue such as the following:
Tutor:
What quantity is the problem seeking?
Student:
The acceleration of the car.
Tutor:
What principle should you use to find it?
Student:
Newton’s second law.

Minimally Invasive Tutoring of Complex Physics Problem Solving

371

If the student had not yet applied Newton’s second law, then the physicists would
coach the hypothetical student through the steps in its procedure. If the student had
already applied Newton’s second law, the physicists would say, “I see you’ve applied
it already as equation 4. What quantities in the equation are unknown?” When the
student answers, “the frictional force on the car,” the process repeats and the physicists ask for a principle to find that quantity, etc.
Lastly, we discovered that Andes would frequently give outlandish advice when
asked “What’s wrong?” with an incorrect equation. Often it would suggest replacing a
piece of the equation with a specific number or expression, creating an equation that
none of the physicists could recognize. This turned out to be due to its basic algorithm
for selecting a target step for “What’s wrong” help. Even if the student was just beginning to solve the problem, it would consider target equations from the very end of
the solution. Moreover, it would consider all possible algebraic combinations of correct equations even if that combination didn’t participate in the solution. Consequently, it would hint writing an equation that the student wasn’t intending to write
and should never write. Unfortunately, when students took Andes’ advice and entered
the suggested equation, it would turn green (correct). Students probably found this
terribly confusing.
The second full-scale evaluation occurred during fall of 2000 at the US Naval
Academy. The major goal was to increase the amount of physics covered, so the intervention lasted longer (10 weeks), covered more problems (60) and covered more
topics. We also made some limited changes to cure the problems found in the analyses of the 1999 log files: (1) Students were required to draw vectors rather than define
vector variables without drawing them. (2) The plan inference method described in
[10] was replaced with a simpler technique. These changes left Andes1 giving essentially the same feedback and help as the 1999 version. Nonetheless, the Andes1 students scored significantly higher on a midterm exam just after the intervention (t=7.74,
p<.00001), and this time the effect size was a satisfying 0.92 standard deviations [14].
However, Andes’ advice still differed substantially from the advice given by the
physicists, so it seemed that there was still room for improvement.

5 The Objectives of Andes2
The physicists clearly had a goal hierarchy in mind when they gave advice to the students. The lower levels of the goal hierarchy corresponded to well-known methods for
applying principles, which are printed in textbooks. For instance, the method for applying Newton’s second law to an object is to draw all the forces acting on the object,
draw its acceleration, draw coordinate axes and write an equation. The upper levels of
the goal hierarchy seemed to correspond to a search that starts at the sought quantity,
applies a principle containing the sought, and then recurses to find values for any
unknowns in the principle’s equation. This corresponds to a backwards search strategy
used by many expert physics problem solving systems [e.g., 15]. Although empirical
studies have not precisely identified the strategies used by human experts, it appears
that they use this backwards search some of the time, but more often hold principle

372

K. VanLehn et al.

applications in memory until they have planned out a complete solution, then they
write equations as they solve them [16]. Our physicists’ advice was consistent with
just one strategy, backwards search, so we decided to teach that, along with the wellknown procedures for applying principles.
Our first task was to invent a display that reified the goal hierarchies. When we implemented such a display (two of them, in fact), the physicists carefully considered
them but ultimately rejected them as too invasive. For students to use these displays,
they would have to be explicitly taught the backwards search algorithm, a vocabulary
for specifying goals and some tools for navigating and editing the goal hierarchies.
This seemed to the physicists to require too much class time. Even if they changed
their curriculum, they doubted that any of their colleagues would be willing to. In
short, the goal of minimal invasiveness was incompatible with the standard technique
of reifying goals and explicitly teaching a problem solving strategy.
Thus, we decided to have Andes teach the problem solving strategy in the same
way that the physicists seemed to teach it: as part of their advice when asked for help.
That is, when a student asks “What should I do next?” Andes should engage the student in the same question-answer dialogue that the physicists did.

6 The Implementation of Andes2
Many changes were required in order to implement Andes2. First, we had to restructure the physics knowledge base. Instead of a flat set of several hundred inference
rules as used in Andes1, the new knowledge base was organized as about 100 principles, each with its textbook method for applying it.
The solution graph was restructured as a bubble graph and a set of method graphs.
The bubble graph is composed of two kinds of nodes: nodes representing quantities
and nodes representing principle applications. A node representing a principle application is linked to the nodes representing the quantities that appear in the principle
application’s equation. Associated with each principle application node is a graph of
propositions similar to the ones used in Andes1. This graph, called the method graph,
represents how to actually accomplish the application of the principle.
When the student asks “What should I do next?”, Andes2 conducts a dialogue
based on the solution graph. It always starts by asking the student, “What quantity
does the problem seek?” It offers the student a hierarchical menu of all physics quantities. The student gets negative feedback and another chance if the student fails to
pick a quantity that is actually sought by the problem. Otherwise, the tutor asks, “What
principle should be used to find it?” and offers a hierarchical menu of all principle
applications. To evaluate the student’s selection, it starts at the sought quantity’s node
in the bubble graph and sees if the student’s selected principle application is indeed on
a solution path for this problem. If not, Andes2 says so and asks the student to try
again. If the selected principle application is appropriate and not yet finished, then
Andes2 enters the method graph for that principle. It locates the first unaccomplished
step, and composes a hint sequence for that step. These hints drive the rest of the dialogue. On the other hand, if the principle has already been applied, Andes2 says so

Minimally Invasive Tutoring of Complex Physics Problem Solving

373

and asks the student which unknowns in the equation should be determined next. The
student selects a quantity, and Andes2 uses the bubble graph to check that it is indeed
an unknown in the principle application. From this point on, the process recurses.
Eventually the tutor and the student end up at an unfinished principle application,
traverse its method graph, select a target step and accomplish it.
Since Andes2 locates a target step by asking a sequence of questions of the student,
it has no need to guess a target step based on probabilistic reasoning. Thus, Andes2
does not use a Bayesian network. We would revive it if Andes needed estimates of the
student’s mastery of principles in order to decide which problem to assign next and
whether to go on to the next chapter. However, having different students do different
homework was believed to be too invasive for the US Naval Academy. For instance,
students who were assigned more physics problems could justifiably complain that
Andes hurt their grades in their other courses.
We also revised the help that Andes gives when students ask, “What’s wrong with
that?” Instead of finding a closest matching target step then hinting the difference
between it and the student’s step, Andes2 has a set of error handlers. Each can recognize a specific type of error. For instance, there is an error handler for using the wrong
time specifier on a given quantity, which is usually due to a misreading of the problem statement. There is an error handler for failing to include a minus sign on a vector
component when the vector is parallel to the axis; this is usually due to a misunderstanding of vector algebra. Each error handler has hints and/or minilessons designed
to remedy the error and any misconceptions that might underlie it. If none of the error
handlers recognizes the student’s incorrect entry, then the student is advised to ask,
“What should I do next?”
Joel Shapiro, a physicist who visited our group for a year, implemented a fundamental change in the way Andes recognizes equations [17]. Andes1 precomputed a
table of all algebraic combinations of principle applications. This allowed it to recognize T_y-mg=m*a_y even though it is composed of several principle applications:
T_y+W_y=m*a_y and W_y = -W = -m*g. This precomputation became intractable as
Andes1 began to handle more complex problems. Andes2 recognized which primitive
equations have been combined to form the student’s equation by taking the gradient of
the student’s equation at a particular point, then seeing whether there is a set of principle applications such that the gradients of their equations sum to the gradient of the
student’s equation. This made it possible to handle problems with several hundred
principle applications.

7 Evaluations of Andes2
Andes2 was evaluated in fall 2001 at the US Naval Academy. The intervention lasted
around 12 weeks. Once again, the Andes students learned significantly more than the
control students (t=3.14, p=.0012). The 0.52 effect size was respectable but less than
Andes1 achieved in Fall 2000.
It was quite clear, both from log file analysis and from the comments of the students to the instructors, that Andes2 was simply young software. Although we had

374

K. VanLehn et al.

succeeded in removing bugs that would crash the system or cause obvious malfunctions, many pedagogical bugs remained. For instance, since all the hint sequences
were new, many of the hints were phrased in confusing ways. The error handlers
sometimes misrecognized errors. There were hundreds of these little “pedagogical
bugs.” Pilot testing would have uncovered them, but we could only run a few pilot
subjects before the Naval Academy semester began. In contrast, Andes1 was tuned
using log files from several hundred subjects by the time it reached the Fall 2000
evaluation, so it was much more mature software.
The instructors were happy to report that Andes2 did not suggest outlandish equations when students asked what was wrong with one of their equations. Moreover,
student acceptance of the tutor appears not to have been hurt, and may even have gone
up slightly. When asked at the end of the intervention if they would chose to continue
using Andes if they could, 33% of the Fall 2001 students reported that they would
versus 28% of the Fall 2000 students. Although this is good news, there is still room
for improvement.

8 Conclusions and Future Work
We are finally beginning to understand the problem faced by Andes. Physics knowledge can be divided into (1) principles and the multi-step methods used to applying
them, and (2) some kind of problem solving strategy, such as backwards search, that is
used to select a principle application. All the stakeholders agree that physics students
should learn principles and the methods for applying them. Indeed, recent textbooks
print the application method for Newton’s law, the application method for translational kinematics, and the application methods for many other major principles. However, there is no consensus on whether to explicitly teach a strategy for selecting principle applications. On the one hand, many successful tutoring systems have been built
around the common wisdom that one should first find out what tacit knowledge is
required for successful problem solving, then design a tutoring system that teaches that
knowledge explicitly [e.g., 18]. On the other hand, today’s experts probably acquired
their principle selection strategy via implicit learning. There are even computational
models of how such implicit learning could occur [19, 20]. Perhaps it would be best if
students were not explicitly taught a principle selection strategy, but instead learned it
implicitly. Moreover, explicit teaching of a principle selection strategy would be more
invasive than implicit teaching. It would require augmentation of the textbooks,
changes to the lectures, mastery of a notation for goals and reallocation of precious
student time from learning principles to learning strategies.
Since we do not know whether implicit or explicit learning is better for principle
selection knowledge, we designed Andes2 as a compromise between them. It offers
explicit teaching of the backwards search strategy for selecting principles, but only
when asked.
Clearly, the next major goal for the Andes project is an experiment that measures
the effectiveness of different amounts of explicit teaching of principle selection
knowledge. The current plan is to use 3 versions of Andes2. One has help turned off,

Minimally Invasive Tutoring of Complex Physics Problem Solving

375

so that students must learn principle selection knowledge implicitly. The second is the
current version, which teaches some but not all the necessary principle selection
knowledge. The third is a new version, which explicitly teaches principle selection
knowledge. It will reify goals and be accompanied by printed materials with copious
examples. A modified curriculum will be developed that inserts explicit teaching of
principle selection knowledge at key points (e.g., when students are able to solve single-principle problems easily, but have not yet begun to solve multiple-principle
problems). All three versions of Andes2, along with the printed instructional material,
should be thoroughly pilot tested in order to remove pesky pedagogical bugs like the
ones that plagued the Fall 2001 version.
Depending on the results of this experiment, we may move to the next obvious
study, which is seeing which versions of the tutor are acceptable to instructors and
students. That is, how much better do the more explicit tutors have to be in order to
justify their invasiveness?
At issue here is perhaps one of the oldest pieces of advice in the ITS literature: explicate tacit knowledge, then teach it. We hope to discover whether that advice is
correct for physics.
Acknowledgements. This research was supported by grant N00014-96-1-0260 from
ONR Cognitive Sciences.

References
1.
2.
3.
4.
5.

6.

7.
8.

Gertner, A.S. and K. VanLehn, Andes: A coached problem solving environment for physics, in Intelligent Tutoring Systems: 5th international Conference, ITS 2000, G. Gautheier,
C. Frasson, and K. VanLehn, Editors. 2000, Springer: New York. p. 133-142.
Schulze, K.G., et al., Andes: An intelligent tutor for classical physics. The Journal of
Electronic Publishing, 2000. 6(1).
Schulze, K.G., et al., Andes: An active learning intelligent tutoring system for Newtonian
physics, in THEMES in Education. 2000, Leader Books: Athens, Greece. p. 115-136.
VanLehn, K., Conceptual and meta learning during coached problem solving, in ITS96:
Proceeding of the Third International conference on Intelligent Tutoring Systems., C.
Frasson, G. Gauthier, and A. Lesgold, Editors. 1996, Springer-Verlag: New York.
Albacete, P.L. and K. VanLehn, The Conceptual Helper: An intelligent tutoring system for
teaching fundamental physics concepts, in Intelligent Tutoring Systems: 5th International
Conference, ITS 2000, G. Gauthier, C. Frasson, and K. VanLehn, Editors. 2000, Springer:
Berlin. p. 564-573.
Albacete, P.L. and K. VanLehn, Evaluation the effectiveness of a cognitive tutor for fundamental physics concepts, in Proceedings of the Twenty-Second Annual Conference of
the Cognitive Science Society, L.R. Gleitman and A.K. Joshi, Editors. 2000, Erlbaum:
Mahwah, NJ. p. 25-30.
Rose, C.P., et al., Interactive conceptual tutoring in Atlas-Andes, in Artificial Intelligence
in Education: AI-Ed in the Wired and Wireless future, J.D. Moore, C. Redfield, and W.L.
Johnson, Editors. 2001, IOS: Washington, DC. p. 256-266.
Conati, C. and K. VanLehn, Further results from the evaluation of an intelligent computer
tutor to coach self-explanation, in Intelligent Tutoring Systems: 5th International Confer-

376

9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.

K. VanLehn et al.
ence, ITS 2000, G. Gauthier, C. Frasson, and K. VanLehn, Editors. 2000, Springer: Berlin.
p. 304-313.
Conati, C. and K. VanLehn, Toward computer-based support of meta-cognitive skills: A
computational framework to coach self-explanation. International Journal of Artificial
Intelligence in Education, 2000. 11: p. 398-415.
Gertner, A., C. Conati, and K. VanLehn, Procedural help in Andes: Generating hints using
a Bayesian network student model., in Proceedings of the 15th national Conference on
Artificial Intelligence. 1998.
Conati, C., A. Gertner, and K. Vanlehn, Using Bayesian networks to manage uncertainty
in student modeling. User Modeling and User-Adapted Interactions, in press.
Conati, C., et al., On-line student modeling for coached problem solving using Bayesian
networks, in User Modeling: Proceedings of the Sixth International conference, UM97, A.
Jameson, C. Paris, and C. Tasso, Editors. 1997, Spring Wien: New York.
Gertner, A.S., Providing feedback to equation entries in an intelligent tutoring system for
Physics, in Intelligent Tutoring Systems: 4th International Conference, B.P. Goettl, et al.,
Editors. 1998, Springer: New York. p. 254-263.
Shelby, R.N., et al. The Andes Intelligent Tutor: an Evaluation. In Physics Education
Research Conference. 2001. Rochester, NY.
Bundy, A., et al., Solving mechanics problems using meta-level inference, in Proceedings
of the Sixth International Joint Conference on AI. 1979, Morgan Kaufmann: San Mateo,
CA. p. 1017-1027.
Priest, A.G. and R.O. Lindsay, New light on novice-expert differences in physics problem
solving. British Journal of Psychology, 1992. 83: p. 389-405.
Shapiro, J.A., Algebra subsystem for an intelligent tutoring system. International Journal
of Artificial Intelligence in Education, submitted.
Anderson, J.R., et al., Cognitive Tutors: Lessons Learned. The Journal of the Learning
Sciences, 1995. 4(2): p. 167-207.
VanLehn, K., R.M. Jones, and M.T.H. Chi, A model of the self-explanation effect. The
Journal of the Learning Sciences, 1992. 2(1): p. 1-59.
Elio, R. and P.B. Scharf, Modeling novice-to-expert shifts in problem-solving strategy and
knowledge organization. Cognitive Science, 1990. 14: p. 579-639.

Providing Adaptive Support to the Understanding
of Instructional Material
Cristina Conati

Kurt VanLehn

Department of Computer Science
University of British Columbia
2366 Main Mall
Vancouver, BC, V6T1Z4, CANADA,

Department of Computer Science
University of Pittsburgh
Pittsburgh, PA, 15260, USA,

vanlehn@pitt.edu

conati@cs.ubc.ca
To provide this guidance, the interface must draw the students’
attention to example parts that may be problematic for them and
must be able to trigger self-explanations even from those students
that do not have a tendency to self-explain. It is very important
that the interface interventions be generated only when the student
can benefit from the suggested self-explanations. Asking students
to always make their self-explanations explicit to the system
would burden the students who are natural self-explainers with
unnecessary work, possibly compromising their motivation to
deeply understand the instructional material. Thus, to determine
when to intervene, the SE-Coach interface relies on a probabilistic
student model (evolved from ideas proposed in [5]) that uses a
Bayesian network [13] to assess how well students understand the
instructional material by capturing both implicit self-explanation
and self-explanations that the students generate via the interface.

ABSTRACT
We present an adaptive interface designed to provide tailored
support for the understanding of written instructional material.
The interface relies on a user model based on a Bayesian network,
that assesses users’ understanding as users read the instructional
material and try to understand it by generating explanations to
themselves. The user model’s assessment is used by the interface
to generate tailored scaffolding of further user’s explanations that
can improve the user’s comprehension. After illustrating how the
Bayesian user model assesses understanding from the user’s
explanations and from latency data on the user’s attention, we
discuss initial results on the effectiveness of the interface’s
adaptive interventions.

Keywords
Adaptive tutoring, user modeling, Bayesian networks, modeling
attention.

Bayesian networks have been used in user modeling mostly to
perform assessment during problem solving [4, 9, 11, 15]. We
have extended their use to cover the sources of uncertainty
involved in assessing learning from instructional material, in
particular from example solutions. These sources of uncertainty
include monitoring users’ reading behavior through their focus of
attention and assessing learning from users’ explanations.
Although there has been increasing interest in using data on users’
attention to develop intelligent interfaces [7, 8], our model is one
of the first to actively use these data in an adaptive system. Also,
although other systems have been devised to improve learning by
triggering users’ explanations [1, 12], little research exists on how
to monitor these explanations to assess learning.

1. INTRODUCTION
Being able to provide tutoring tailored to the needs of individual
students has been identified as one on the main reasons for the
effectiveness of Intelligent Tutoring Systems [16]. Much research
has been devoted to provide adaptive guidance during problem
solving [16], but other instructional activities may benefit from
tailored support, because their effectiveness generally depends on
the students’ learning style and attitudes.
For instance, several cognitive science studies indicate that the
amount of learning from instructional material depends on
whether students spontaneously generate explanations to
themselves (i.e. self-explain) as they read the material [2].
However, most students do not self-explain spontaneously,
because they overestimate their understanding and/or do not use
their knowledge to elaborate what they read [14]. To help these
students, we have devised an educational environment, the SECoach, equipped with an interface providing the same adaptive
guidance for self-explanation that has proven highly beneficial
when administered by human tutors [2].

In the rest of the paper, we briefly describe the SE-Coach’s
interface (see [6] for a more detailed description of the interface
design and evaluation). We then illustrate how the student model
Bayesian network is created automatically for each example. Next,
we discuss how the SE-Coach’s interface is dynamically modified
by using the student model’s assessment to elicit further selfexplanation targeted to improve the user’s example understanding. We finally discuss initial results on the effectiveness of
the tailored support provided by the SE-Coach’s interface.

2. INTERFACE FOR EXAMPLE STUDY

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
IUI’01, January 14-17, 2001, Santa Fe, New Mexico, USA.
Copyright 2001 ACM 1-58113-325-1/01/0001…$5.00.

The SE-Coach interface allows students to read and self-explain
example solutions (like the physics example in Figure 1) under
the coach’s supervision. Since eye-tracking and natural language
understanding are still not powerful enough to reliably monitor
these tasks, the SE-Coach interface includes two alternative
mechanisms: a masking interface to track students attention and a
set of menu based tools that allow students to constructively

41

(a)

(b)

Figure 3: Rule Browser and a Template
selection. To explain more about a rule, the student can activate a
Rule Template, a dialog box containing a partial definition of the
rule that has blanks for the student to fill in (see Figure 3b). This
definition includes preconditions for and consequences of the
rule-application and reflects the rule definition in the SE-Coach
knowledge base. Clicking on a blank in the Template brings up a
menu of possible fillers. When the student submits a completed
template, the SE-Coach will give feedback on the submission
correctness. To explain step utility, the student can activate a Plan
Browser that displays a hierarchical tree representing an
example’s solution plan. The student can search the hierarchy for
the plan goal that most closely motivates the uncovered solution
step and receives feedback on the correctness of the selected goal.

Figure 1: Sample physics example problem

The student’s reading and self-explanation actions are used to
dynamically update the student model Bayesian network, that at any
time during the interaction assesses the student’s understanding of
the different example parts. If a student tries to close the example
when the student model indicates that there are still some parts that
are problematic for him, the interface generates a warning and
highlights the corresponding masking boxes (shown darker in
Figure 4). It also changes the “self-explain” button for each
highlighted line, to indicate what interface tools the student should
use to better self-explain the line (see Figure 4). As the student
performs new reading and self-explanation actions to follow the
interface suggestions, the boxes’ color and the related advice change
dynamically to reflect the updates in the student model probabilities,
as we will describe later.

Figure 2: Example 1 shown with the masking interface

3. THE SE-COACH’S STUDENT MODEL
3.1 Sources of Uncertainty

generate self-explanations. Figure 2 shows how the Newtonian
physics example in Figure 1 is presented with the masking
interface in the version of the SE-Coach implemented within the
Andes tutoring system for Newtonian physics [17]. To view an
example part, the student needs to move the mouse over the box
that covers it. When the student uncovers an example part, a “selfexplain” button appears, as a reminder to self-explain. Clicking on
this button activates more specific prompts that suggest two kinds
of self-explanations known to be highly effective for learning
[14]. They are: (1) explain an example solution step in terms of
domain principles (step correctness); (2) explain the role of a
solution step in the underlying solution plan (step utility).
The interface provides tools to help students generate these two
kinds of explanations. To explain step correctness, the student can
activate a Rule Browser (see Figure 3a), containing a hierarchy of

Several sources of uncertainty affect the SE-Coach’s student
model assessments and call for a probabilistic student model. The

Solution

(a)
Solution

(b)

rules that represent principles in the instructional domain and
reflect the content of the SE-Coach’s knowledge base. The
student can browse the rule hierarchy to find a rule that justifies
the uncovered part and receives feedback on the correctness of her

Figure 4: Adaptive prompts to self-explain
42

subgoal of applying Newton’s 2nd law, i.e. selecting a body to
which to apply the law.

first source stems from the fact that we do not want to force
spontaneous self-explainers to always make their self-explanations
explicit through the SE-Coach interface tools. Thus, the student
model must be able to detect self-explanations generated through
implicit reasoning. The only evidence that the student model can
use to do so is latency data on student’s attention and estimates of
student’s domain knowledge. If a student spends enough time
viewing a line of instruction and has sufficient knowledge to selfexplain that line, we want the model to assess that likely the
student self-explained the line correctly. But this adds uncertainty
to the assessment, because both student’s attention and student’s
knowledge cannot be unambiguously determined.

When a student opens an example, the student model Bayesian
network for the current session is automatically created from the
SE model and from a student’s long-term model. The structure of
the Bayesian network derives directly from the SE model. All
nodes in the initial network have binary values representing the
probability that the student knows rules, goals and facts in the
example solution and that she has explained the related
derivations. Rule node priors derive from estimates of a student’s
current domain knowledge stored in the long-term student model.
This model also contains a probability η representing a student’s
tendency to reason ahead, namely to derive the example solution
autonomously rather than reading it in the example. The
probability η is used to specify the conditional probability table
(CPT) for rule-application nodes, defined as a Noisy-AND. The
Noisy-AND models the following assumptions: (1) a student

The second source of uncertainty exists because students have
different studying styles. Some students prefer to generate
solution steps by themselves before reading them in the example,
while others rely completely on the example solution and start
reasoning about a solution step only when they read it [14].
Hence, depending on the student’s studying style, attention to a
solution step can indicate not only self-explanation for that step
but also derivation and self-explanation of subsequent steps.

(b ) SE M odel

(a) Student’s actions
Read - l1
Fin d force on Jake
Fin d Ten sion Force exerted
on Jake by ok
the rope
ok

The third source of uncertainty is due to the fact that sometime
examples leave out some solution steps (i.e., they have solution
gaps). A student needs to self-explain these steps to understand
the solution thoroughly. This adds an additional level of implicit
reasoning that the student model needs to assess from latency data
and knowledge estimates.

SE-action
Pb-choose-body
pb - choose- body

2

1.0

Finally, little research exists on how people learn from menu
selections and template filling. Thus, even when students generate
correct self-explanations through the interface tools described in
the previous section, there is uncertainty about how these selfexplanations reflect learning and understanding.

1

G -force -on -Jake

R - try - Newto n - 2law
0.95

RA - try - Newto n - 2law
0.91
0.

R - goal- choose- body

G -try - Newto n - 2law
0.9

0.91
R - b ody - by - force
0.5

Read - l2
We choose Jake as bod y…
long

RA - goal- choose- body
0.81
G - goal- choose - body
0.81

3

The SE-Coach student model evolved from the Andes’ student
model for problem solving [4], to formalize and handle in a
principled way these sources of uncertainty, as we describe in the
next sections.

0.95

4
5

RA - body - by - force
0.39
0.97 F -Jake - is - the - body

Figure 5: Segment of student model for example 1
cannot perform a derivation if she does not know the
corresponding rule and its preconditions; (2) there is a probability
(noise) that the student does not apply the rule even when she has
all the relevant knowledge. This is exactly the probability that a
student does not reason ahead, 1-η. Although η generally depends
on the student only, it can sometimes depend on the rule as well.
For instance, most students taking introductory physics
understand when they need to apply Newton’s 2nd law as soon as
they read a problem’s statement. Thus, the rule R-try-Newton2law in Figure 5b is generally associated with a large η in the
long-term student model. Fact and goal nodes (collectively called
proposition nodes) deterministically depend on their parent ruleapplication. Priors of root proposition nodes, representing the
example givens, are set to 0 until the student starts reading the
example text.

3.2 Generation of the Bayesian Network
The student model Bayesian network is built automatically for
each example. This is crucial to allow the extension of the SECoach’s set of examples with relatively limited effort, given that
the resulting networks are quite large and complex (in the order of
hundred of nodes). To add a new example, a problem solving
module builds a model of correct self-explanation (SE model) for
the example, starting from (i) a knowledge base of rules
describing domain principles and abstract planning steps; (ii) a
formal definition of the example givens. The resulting SE model
(see Figure 5b) is a dependency network that encodes how the
intermediate facts and goals (F- and G- nodes in Figure 5b) derive
from domain rules (R- nodes in Figure 5b) and from facts and
goals matching the rules’ preconditions. These derivations are
explicitly represented in the SE model by rule-application nodes
(RA- nodes in Figure 5b) and correspond exactly to the selfexplanations for step correctness and step utility that the SECoach targets. For instance, the node RA-body-by-force in Figure
5b encodes the explanation that Jake is chosen as the body
because a physics rule says that if we want to find a force on an
object, that object should be selected as the body to which to
apply Newton’s 2nd law. The node RA-goal-choose-body encodes
the explanation that choosing Jake as the body fulfils the first

As a student performs reading and self-explanation (SE) actions,
the initial Bayesian network is dynamically updated with nodes
representing these actions (see Figure 5a). In the next sections, we
describe the semantics of the nodes representing student’s actions
and how they influence the probabilities of nodes in the SE
model.

43

rule with only one precondition.) A student cannot self-explain a
derivation correctly if he does not have the necessary knowledge
(i.e., the rule and its preconditions), no matter for how long the
student attended to the derivation (these cases are grouped under
the otherwise entry in Table 2).

3.2.1 Read Nodes
Read nodes (nodes with prefix Read in Figure 5a) represent
viewing items in the masking interface. The values of read nodes
reflect the duration of viewing time and are set by comparing the
total time a student spent viewing an item (TVT) with the
minimum time necessary to read it (MRT). We currently compute
the MRT for each item by assuming the reading speed of an
average-speed reader (3.4 words/sec.,[10]). Depending upon the
result of the comparison, the value of a read node can be LOW
(TVT < MRT, time insufficient for reading), OK (TVT≈MRT,
time sufficient for reading only) or LONG (TVT >> MRT, time
sufficient for self-explanation).

Table 2: CPT for rule-application node after a student viewed
the related proposition node
Knows
RULE
T

Each read node connects to the SE model node reflecting the
semantic content of the viewed item (see link 1 and 5 in Fig. 5).
These links indicate that viewing time influences the probability
of knowing the related content. When a proposition node has
input from both a read node and a rule-application node (e.g., Fjake-is-the-body in Figure 5), then a student can acquire the
corresponding proposition either by reading it in the example or
by deriving it from rules and previous propositions. This
relationship is represented in the CPT for proposition nodes
whose content a student has viewed in the example solution (see
Table 1). If a rule-application node is TRUE (i.e., the student
explained the corresponding derivation), by definition the
proposition node is TRUE (i.e., known by the student).
Otherwise, the probability of knowing the proposition node is
small if reading time is LOW (probability p1 in Table 1) and high
when the time is OK or LONG (probabilities p2 and p3
respectively in Table 1) .

otherwise

RULEAPPL

LOW

OK

1.0

1.0

1.0

F

p1 <
0.5

p2>0.
9

p 3> p 2.

LOW

OK

LONG

p1 = η

p2= η

p3> max {p2, 0.9}

0

0

0

P(RA =T|Rule =T, Goal=T, Read∈{LOW,OK}) = η
as shown in Table 2. If viewing time is LONG, the probability
that self-explanation occurred (p3 in Table 2) is set to be at least
0.9.

3.2.2 SE Nodes

LONG

T

T

READ

If the student has all the necessary knowledge, the probability that
proper self-explanation occurred increases with viewing time. If
viewing time is LOW (i.e., insufficient for reading) or OK (i.e.
sufficient for reading only), self-explanation for this line could
only have occurred if a student reasoned forward from previous
lines. The probability that this happened (represented by
probabilities p1 and p2 in Table 2) is the probability η in the long
term student model, modeling the student’s tendency to derive
solution lines autonomously, before reading them in the example.
Thus,

Table 1: CPT for a proposition node the student viewed
READ

Knows
Goal/Fact

Nodes representing the student’s self-explanation actions (SE
nodes) are dynamically added to the Bayesian network to model
the occurrence of Rule Template filling and selection of goals in
the Plan Browser. Since Rule Template filling provides evidence
of the student’s understanding of the corresponding rule, the SE
node for a Template filling action is linked to the corresponding
rule node in the Bayesian network. Similarly, the SE nodes
encoding goal selection in the Plan Browser are linked to the rule
nodes establishing the corresponding goals in the SE model with
(see link 2 in Figure 5).

Some proposition nodes in the Bayesian network may not be
connected to a read node, even after the student has viewed all the
elements in the example solution. This happens when the solution
omits some of the steps, as it often happens in instructional
material. In Figure 5, for instance, the nodes G-try-Newton-2law
and G-goal-choose-body cannot have any read node pointing to
them, because these goals are not explicitly mentioned in the
example. A student can know unmentioned propositions only by
deriving them from rules and other example propositions.

Since SE actions such as menu selections and template filling
involve building self-explanations by composing given material,
they do not provide as strong evidence of rule understanding as
self-explanations generated verbally would. Even if a student has
little knowledge of a rule, he may still be able to generate a selfexplanation involving that rule by using the SE-Coach tools,
because of the scaffolding that these tools provide. However,
whether the student learns from the self-explanation action
depends upon how much constructive reasoning she performs
during the process. Given the lack of established results on how
people learn from menu selections and template filling, in the SECoach student model we prefer to be conservative. We assume
that each correct SE action increases the probability that a student
learns the corresponding rule, but if the student starts with low
rule knowledge we want evidence from more than one correct SE
action before assessing that the rule is mastered.

The fact that the student viewed an example item does not
necessarily mean that the student self-explained it. In particular, it
does not mean that the student self-explained the inferences
describing how the item derives from a domain principle (step
correctness) and what goal it achieves in the solution plan (step
utility). However, the longer the student viewed an example item,
the higher the probability that he self-explained it. This
relationship between viewing time and self-explanation is
encoded in the student model by linking a read node that
represents viewing an example item with the rule-application
nodes that represent self-explanation for correctness and utility for
that item (see, for instance, link 3 and 4 in Figure 5). The CPT for
these rule-application nodes is modified to take reading time into
account, as shown in Table 2 (For simplicity, this table shows a

44

R - g o a l - c h o o se - b o d y

R - b o d y - b y - f o rc e
0 .9 1

R A - b o d y - b y - f o rc e
0 .1 3

OK

(b )

R A - g o a l - c h o o se - b o d y
0 .2 5 1
G - g o a l - c h o o se - b o d y
0 .2 0

R e a d - l2
W e c h o o se th e w a g o n a s b o d y …

(a )

G - t ry - N e w t o n - 2 la w
0 .9

0 .9 1

0 .9 1

WW
a g o n -is-b o d y

Figure 6: Transfer of rule probabilities to a new example
suggest self-explanation with the Rule Browser (see Figure 4a).
Similarly, when the SE-Coach detects that the low probability of a
rule-application node is due to low probability of the closest
planning rule ancestor, it modifies the interface to suggests selfexplanation through the Plan Browser (see Figure 4b). When low
probability of a rule-application node is caused only by too short
reading time, the “self-explain” button for the related solution line
is turned into a hint suggesting to read more carefully.

To achieve this behavior without having to change the CPT of a
rule node every time a new SE action involving that rule occurs,
we direct the link from rule nodes to SE nodes (see Figure 5a).
The entry P(SE=T| Rule=F) in a SE node’s CPT represents the
probability that a student can complete a correct SE action
without knowing the corresponding rule. This probability can be
adjusted to vary the amount of evidence that a correct SE action
provides toward rule learning. For instance, the higher the number
of wrong attempts a student makes before generating a correct SE
action, the higher we set P(SE node=T| Rule=F) when we add the
corresponding SE node, because it becomes more likely that the
student achieved the correct action through random selection in
the interface tools, rather than through reasoning.

The student is not obligated to follow the interface suggestions.
When the student decides to close an example, the student model
Bayesian network is discarded, but the new rule nodes’
probabilities are used to update the long term student model.
These probabilities will become the new priors in the student
model for the next example study task and will influence the
system’s interventions accordingly. Let’s suppose, for instance,
that the student generates both the Plan Browser and Rule
Browser explanations for the segment of example in Figure 5 and
then opens a new example, shown in Figure 6a. The first line of
this example’s solution (“We choose the wagon as the body”) is
similar to the one for the previous example, as similar is the
corresponding segment of Bayesian network (shown in Figure
6b). However, the priors of the rule nodes involved in this part of
the model reflect the student’s previous interaction with the
system. If the student views the first example line for long
enough, the student model will predict that the student has
performed all the inferences required to self-explain this line,
because there is high probability that she has the knowledge
necessary to do so. Thus, the SE-Coach will never ask this student
to explicitly self-explain the selection of the wagon as the body by
using the interface tools. The only prompting that the interface
may generate on this example line would be caused by viewing
time too short for self-explanation. In this case, the Read node
modeling attention to the body-selection line would be set, for
instance, to OK, preventing the high probability of the rule nodes
R-goal-choose-body and R-body-by-force to propagate to the
corresponding rule-application nodes (see Figure 6b) and
resulting in the generation of a read-carefully prompt for the
body-selection line (see Figure 6a). If the student then reviews the
line for sufficient time, the student model will be dynamically
updated and the line will be turned back to an “explained” state
(light gray) when the student moves away from it.

4. USING THE STUDENT MODEL TO
GENERATE TAILORED SCAFFOLDING
At any time during the student’s interaction with the SE-Coach,
the probabilities in the Bayesian network assess how the student’s
domain knowledge and example understanding change through
the student’s interface actions. In particular, the probabilities
associated with rule-application nodes represent the probability
that the student self-explained the corresponding derivations.
Rule-application nodes with probability below a given threshold
become the target of the SE-Coach interventions.
Figure 5 shows the probabilities in the student model after a
student viewed the line “We want to find the tension force on
Jake” long enough for reading it, viewed the line “We choose Jake
as the body” quite longer and self-explained the latter with the
Plan Browser. The high probability of the node G-try-Newton2law, not mentioned in the example solution, and of its parent
rule-application node, show how the Bayesian network can model
forward reasoning and filling solution gaps. The high probabilities
of the nodes R-try-Newton-2law and G-force-on-Jake propagate
downward because of the large η assigned to the rule R-tryNewton-2law (as we described in the previous section), which
translates into a small noise (1-η) for the Noisy-AND CPT of the
rule-application node RA-try-Newton-2law.
In Figure 5, there is only one rule-application node that still has a
low probability, the non-shaded node RA-body-by-force. From
this node’s descendant, F-Jake-is-the-body, the SE-Coach infers
that the missing explanation relates to the first line in the example
solution. From the fact that the only input node with low
probability for RA-body-by-force is the rule R-body-by-force, the
SE Coach detects that the missing explanation relates to this rule.
Hence, it adds the first solution line among the lines to highlight
in the masking interface and modifies its self-explain button to

In summary, the SE-Coach interface prompts students to explicitly
generate self-explanations using its tools only when the student
model assesses that this can improve the student’s understanding.
In particular, the interface helps those students who do not selfexplain because they do not monitor their understanding, by
drawing the students’ attention to example parts that may be
45

interface for the three examples in the study. These are the
prompts the interface would generate if there was no student
model. (ii) The average number of prompts adaptively generated
by relying on the student model. (iii) The average percentage of
these prompts the students followed. After reporting these results
in [6], we computed the correlation between the percentages of
followed prompts and students’ post-test scores, controlling for
pretest scores. All three measures significantly (or nearly
significantly) correlate with post-test performance (p = 0.056 for
Rule Browser/Template prompts; p = 0.024 for Plan Browser
prompts; p = 0.016 for “Read more carefully” prompts). These
data provide an initial indication that the adaptive prompts based
on the student model effectively elicited self-explanations that
improve students’ learning, although further data should be
gathered to control for other variables that might have caused the
correlation, such as general academic ability or conscientiousness.

problematic for them and by providing specific scaffolding on
what knowledge these explanations should tap. Asking students to
always make their explanations explicit through the interface tools
would of course enable more accurate assessment of their
understanding, but would also burden the students who are natural
self-explainers with unnecessary work, possibly compromising the
effectivevess of their self-explanations and their motivation to use
the system.

5. EVALUATION OF THE INTERFACE
ADAPTIVE INTERVENTIONS
To evaluate the effectiveness of the SE-Coach, we conducted a
study in which 29 subjects studied Newton’s 2nd Law examples
with the complete SE-Coach (experimental condition), while 27
subjects studied the same examples with the masking interface
only and no coaching (control condition). All subjects took a
pretest and a posttest consisting of Newton’s 2nd Law problems.
At the time of the experiment, all subjects were taking
introductory physics in different colleges and had started studying
Newton’s laws in class. In this section, we present a more detailed
analysis of preliminary data on the usability of the SE-Coach
interface that we obtained from this study and reported in [6]. The
analysis focuses on the appropriateness of the interface adaptive
interventions by showing how they influenced the performance of
students in the experimental group (the only ones that had these
interventions). More general data on the difference between the
performance of the experimental and the control group can be
found in [3].

Table 3: SE-Coach prompts that students followed
Prompt Type

Max.

Generated

Followed

Use RuleBrowser/Templ.

43

22.6

38.6%

Use PlanBrowser

34

22.4

42.0%

Read mode carefully

43

7

34.0%

A second result that we obtained from further analysis of the
results presented in [6], indicates how accurately the student
model assesses knowledge changes from SE actions. We found an
interaction between the accuracy of this assessment and when
subjects had started studying Newton’s Laws in their classes. We
computed the correlation between posttest scores and the number
of rules that reached high probability in the student model. The
correlation is very low (r = −0.03) for subjects from classes that
had started the example topic more than a week before the study
(early-start subjects) and it is higher (= 0.33) for subjects from
classes that had started just a few days before (late-start subjects).
This indicates that the evidence that correct SE actions provide
toward rule knowledge in the student model reflects more
accurately how late-start subjects learned from these actions.
Since our data showed no significant differences in the two
groups’ initial knowledge or in how they used the interface tools,
we hypothesize that the difference in the correlation exists
because the SE-Coach examples were more challenging for latestart subjects and therefore they put more effort than the earlystart ones in reasoning and learning from the same SE actions.
These results suggest that the students’ learning stage should be
taken into account when modeling learning from SE actions. For
instance, in the CPT for SE nodes, the probability P(SE=T|
Rule=F) (e.g. the probability of generating a correct SE action
without knowing the corresponding rule), should be set to an
higher value if a student has been working on the example topic
for some time (like our early-start subjects), to account for the
possibility that a lower level of motivation may result in less
learning from SE actions.

5.1 Student Model Set Up
During the evaluation, we only used examples with limited gaps
in the presented solution. That is, the example solution mentioned
all the steps derived from physics rules (fact nodes in the student
model), and omitted only the most immediate goal that each step
fulfills. Thus, this study does not give us information on how well
the student model presented here handles the additional level of
uncertainty introduced by the higher degree of implicit reasoning
involved in studying examples with larger solution gaps. Also,
constraints on the study duration prevented us from initializing
the student model with data on the students’ initial knowledge and
studying style. Hence, we assigned to most rules a prior of 0.5,
and we assumed that students are very unlikely to reason forward
(η = 0.02) because other studies [14] show that this is how most
students usually behave. The threshold for considering rule nodes
as known was set to 0.9 and the conditional probabilities for SE
nodes were set in such a way that a correct self-explanation action
achieved at the first attempt would make the corresponding rule
node reach the threshold if the rule probability was 0.5 or higher.
Given the lack of formal theories on how people learn from menu
selection and template filling actions, this choice is not informed
but rather dictated by the rationale that when we do not have good
estimates of what initial knowledge students have (as the priors of
0.5 indicate), we prefer to be less conservative in assessing
student learning from correct SE actions, to compensate for
possible inaccuracies in the model set up.

6. CONCLUSIONS AND FUTURE WORK

5.2 Results

Providing interactive, tailored support to the understanding of
instructional material will become increasingly important as more
instruction will be delivered through computers and distant
learning on the Web. We have described a computational

We computed from log data how often students followed the
interface adaptive prompts to further self-explain. The results are
summarized in Table 3. For each type of prompt, the table reports:
(i) the maximum number of prompts that could appear in the
46

framework that provides tailored support to the understanding of
instructional material presenting example solutions. The
framework relies on a probabilistic student model that takes into
account the different sources of uncertainty embedded in
assessing understanding from latency data on attention, and from
explanations generated through menu selections and template
filling. Although there has been extensive research on monitoring
user’s attention, the work described in this paper is one of the first
attempts to actively use latency data to adapt the interaction to the
user’s behavior. To our knowledge, it is also the first attempt to
assess learning from students’ explanations.

[4] Conati, C., Gertner, A., VanLehn, K., and Druzdzel, M. Online student modeling for coached problem solving using
Bayesian networks, in User Modeling: Proc. of the Sixth Int.
Conf., A. Jameson, C. Paris, and C. Tasso, Editors. 1997,
Spring Wien: New York.
[5] Conati, C., J. Larkin, and K. VanLehn, A computer
framework to support self-explanation, in Proceedings of the
Eighth World Conf. of Artificial Intelligence in Education.
1997.
[6] Conati, C. and K. VanLehn. Teaching meta-cognitive skills:
implementation and evaluation of an tutoring system to guide
self-explanation while learning from examples. in AIED'99,
9th World Conf. of Artificial Intelligence and Education.
1999. Le Mans, France.
[7] Gluck, K.A., J.R. Anderson, and S.A. Douglass. Broader
Bandwidth in Student Modeling: What if ITS were "Eye"TS?
in ITS 2000, 5th Int. Conf. on Intelligent Tutoring Systems.
2000. Montreal, CA.
[8] Horvitz, E. Principles of Mixed-Initiative User Interfaces. in
CHI '99, ACM SIGCHI Conf. on Human Factors in
Computing Systems. 1999. Pittsburgh, PA.
[9] Jameson, A., Numerical uncertainty management in user and
student modeling: An overview of systems and issues. User
Modeling and User-Adapted Interaction, 1995. 5.
[10] Just, M. and P. Carpenter, The psychology of reading and
language comprehension, ed. A.a. Bacon. 1986, Boston.
[11] Luckin, R. and B. du Boulay. Capability, potential and
collaborative assistance. in UM '99: User Modeling. 1999.
Banff, Canada: SpringerWienNewYork.
[12] Okamoto, T. and T. Kasai. The collaborative system with
situated agents for activating observation learning. in
ITS2000, 5th Int. Conf. on Intelligent Tutoring Systems.
2000. Montreal, Canada.
[13] Pearl, J., Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. 1988, San Mateo, CA:
Morgan-Kaufmann.
[14] Renkl, A., Learning from worked-examples: A study on
individual differences. Cognitive Science, 1997. 21(1): p. 130.
[15] Reye, J. Two-phase updating of student models based on
dynamic belief networks. in 4th Int. Conf. on Intelligent
Tutoring Systems (ITS '98). 1998. S. Antonio, Texas.
[16] Shute, V.J. and J. Psotka, Intelligent tutoring systems: Past,
Present and Future, in Handbook of Research on Educational
Communications and Technology, D. Jonassen, Editor. 1996,
Scholastic Publications.
[17] VanLehn, K., Conceptual and meta learning during coached
problem solving, in ITS96: Proceeding of the Third Int.
Conf. on Intelligent Tutoring Systems., C. Frasson, G.
Gauthier, and A. Lesgold, Editors. 1996, Springer-Verlag:
New York.

We have discussed initial results on the effectiveness of the
adaptive support to example studying that the SE-Coach interface
provides by using the student model’s assessments. The results
indicate that, despite the lack of accurate initial parameters in the
student model, this support improved student’s learning. The
results also suggest how to improve the accuracy of the student
model by taking into account the students’ learning stage.
Although the probabilistic student model currently captures
attention through a masking interface and explanations through
menu based tools, its structure and assessment are independent
from the input modality. For example, as research on eye tracking
and natural language progresses, the model can be modified to
monitor latency data and explanations through these modalities.
This can be done by changing the conditional probabilities in the
model, to reflect the more accurate evidence that eye-tracking and
verbal explanations may provide on user’s attention and
understanding. Since eye tracking and natural language provide
less scaffolding for reading and self-explanation, we plan to
explore what types of learners benefit more from an interface that
relies on them and what learners benefit more from the current,
more constrained interface. This will allow us to explore how to
dynamically adapt the interaction mode to the users’ learning
style. We also plan to work on the automatic generation of
examples that tailor the number of solution gaps to the student’s
current knowledge, to provide adaptive support to the transition
from example studying to problem solving.

7. REFERENCES
[1] Aleven, V., K.R. Koedinger, and K. Cross. Tutoring answerexplanation fosters learning with understanding. in AIED
‘99, 9th World Conf. of Artificial Intelligence and
Education. 1999. Le Mans, France.
[2] Chi, M.T.H., Self-Explaining: The dual process of generating
inferences and repairing mental models. Advances in
Instructional Psychology, in press.
[3] Conati, C. and Vanlehn, K., Further Results from the
Evaluation of an Intelligent Computer Tutor to Coach SelfExplanation in Proc of the Ninth Int. Conf. on Intelligent
Tutoring Systems, ITS 2000, Montreal.

47

