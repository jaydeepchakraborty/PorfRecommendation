Building Faculty Expertise in Outcome-based
Education Curriculum Design
Srividya Bansal, Ashraf Gaffar

Odesma Dalrymple

School of Computing, Informatics, Decision Systems Engg.
Arizona State University, Mesa, AZ, USA
{srividya.bansal, ashraf.gaffar}@asu.edu

Shiley-Marcos School of Engineering
University of San Diego, San Diego, CA, USA
odesma@sandiego.edu

Abstract— An information technology (IT) tool that can guide
STEM educators through the complex task of course design
development, ensure tight alignment between various
components of an instructional module, and provide relevant
information about research-based pedagogical and assessment
strategies will be of great value. A team of researchers is engaged
in a User-Centered Design (UCD) approach to develop the
Instructional Module Development System (IMODS), i.e., a
software program that facilitates course design. In this paper the
authors present the high-level design of the IMODS and
demonstrate its use in the development of the curriculum for an
introductory software engineering course.
Keywords—instruction design; outcome-based
semantic web-based application; user-centered design

I.

education;

INTRODUCTION

Felder, Brent and Prince [1] have made a strong argument in
support of instructional development training for engineering
faculty. This argument, which cites among other reasons:
shortfalls in graduation rates, changing demographics and
attributes of the student body, and modifications in the
expectations of graduates; can be extended to encompass all
STEM fields. Furthermore, studies show that for 95% of new
faculty members, it takes four to five years, through trial and
error (the most common method of gaining expertise in
teaching), to deliver effective instruction [2]. While there are
a number of options available to faculty for receiving
instructional development training (i.e., training focused on
improving teaching and learning), most share similar format,
features, and shortcomings. For example: workshops, courses
and seminar series, the most common program structures, are
often offered at a cost to the institution, department or
individual attendee; delivered face-to-face at specified times;
and accessible to a restricted number of persons. Even when
interest is high, these factors can become obstacles to
participation.
Outcome-based Education (OBE) is a result-oriented approach
where the product defines the process. The learning outcomes
guide what is taught and assessed [3], [4]. This approach
contrasts the preceding “input-based” model that places
emphasis on what is included in the curriculum as opposed to
the result of instruction. There is a growing demand and
interest in faculty professional development in areas such as
OBE, curriculum design, and pedagogical and assessment
strategies.
In response to these challenges and needs, a group of faculty

researchers from two south-western universities have
undertaken a project to design and develop the Instructional
Module Development System (IMODS) that will facilitate
self-paced instructional development training while the user
creates his/her course design with the added benefits of being
free to all who are interested, accessible almost anywhere
through a web browser, and at any time that is convenient.
Additional key features of the IMODS are as follows:
1. Guides individual or collaborating users, step-by-step,
through an outcome-based education process as they
define learning objectives, select content to be covered,
develop an instruction and assessment plan, and define
the learning environment and context for their course(s).
2. Contains a repository of current best pedagogical and
assessment practices, and based on selections the user
makes when defining the learning objectives of the
course, the system will present options for assessment and
instruction that align with the type/level of student
learning desired.
3. Generates documentation of a course designs. In the same
manner that an architect’s blue-print articulates the plans
for a structure, the IMODs course design documentation
will present an unequivocal statement as to what to expect
when the course is delivered.
4. Provides just-in-time help to the user. The system will
provide explanations to the user on how to perform course
design tasks efficiently and accurately. When the user
explores a given functionality, related explanations will
be made available.
5. Provides feedback to the user on the fidelity of the course
design. This will be assessed in terms of the cohesiveness
of the alignment of the course design components (i.e.,
content, assessment, and pedagogy) around the defined
course objectives.
In this paper the authors present the high-level design of the
IMODS and demonstrate its use in the development of the
curriculum for an introductory software engineering course.
The rest of the paper is organized as follows. Background
material for this research project is presented in section 2.
Section 3 presents the high-level design of the IMODS
software system. Section 4 presents a case study that
demonstrates the use of the IMODS framework in the
development of an introductory software engineering course.
The paper concludes with future work and acknowledgements.

II.

BACKGROUND

A. Related Work
To justify the need for the development of IMODS we
conducted a competitive analysis to determine the strengths
and weaknesses of tools and approaches currently used to
support course design and related training. The tools and
approaches that were evaluated were categorized into five
groups based on primary functions and features.
Knowledge/Learning Management System (KMS/LMS):
This group contains a number of proprietary and open-source
solutions that are delivered either as desktop or web-based
applications. These tools mainly facilitate the administration
of training, through the (semi-) automation of tasks such as:
registering users, tracking courses in a catalog, recording data,
charting a user’s progress toward certification, and providing
reports to managers. These tools also serve as a platform to
deliver eLearning to students. In that context, their main
purpose is to assemble and deliver learning content,
personalize content and reuse it.
Examples: Blackboard, Moodle, Sakai, Canvas, WeBWorK,
and Olat
Educational Digital Libraries: These tools contain
collections of learning and educational resources in digital
format. They provide services that support the organization,
management, and dissemination of the digital content for the
education community.
Examples: National Engineering Education Delivery System
(NEEDS), National Science Digital Library (NSDL) and
Connexions
Personalized Learning Services: There are a number of elearning tools that leverage semantic web technologies to
support personalized learning services for their users with an
ontology based framework [5]. Some of these tools function
by initially profiling the learner and then, based on that
profile, identifying the best strategies for presenting resources
to them. They can also provide feedback to instructors on
student learning, so improvements to the content and structure
of the course can be incorporated. For many of these tools the
ontology framework is used to bridge learning content with
corresponding pedagogy; however, they seldom address
assessments and learning objectives. Examples: Content
Automated Design and Development Integrated Editor
(CADDIE), Intelligent Web Teacher (IWT), LOMster [6], and
LOCO-Analyst [7].
Understanding by Design Exchange (UbD Exchange): This
is a software framework based on Wiggins’s and McTighe’s
Backward Design principle [8] that is used for designing
curriculum, assessments, and instruction, and integrates K-12
state and provincial standards in the design of units [9]. It
provides a form-based user interface to fill in the details of the
course unit that is being designed.

Professional Development Workshops, Courses &
Seminars: Face-to-face training sessions in teaching and
learning that are facilitated by experts in the field of
instructional design.
Examples: National Effective Teaching Institute, Connect
Student Learning Outcomes to Teaching, Assessment, and
Curriculum, Content, Assessment ant Pedagogy [10].
Our search identified very few tools and approaches that
contained features or functionality that explicitly facilitated
the design of course curriculum. Of these tools few of them
facilitated the generation of design documentation and
feedback to the user on the fidelity of the design. These are
two key deficiencies that IMODS will address.
B. IMODS Framework – PC3 Model
The IMOD framework adheres strongly to the OBE approach
and treats the course objective as the spine of the structure.
New constructs (not included in the models previously
discussed) are incorporated to add further definition to the
objective. The work of Robert Mager [11] informs the IMOD
definition of the objective. Mager identifies three defining
characteristics of a learning objective: Performance –
description of what the learner is expected to be able to do;
Conditions – description of the conditions under which the
performance is expected to occur; and the Criterion – a
description of the level of competence that must be reached or
surpassed. For use in the IMOD framework an additional
characteristic was included, i.e., the Content to be learned –
description of the factual, procedural, conceptual or metacognitive knowledge; skill; or behavior related to the
discipline. The resulting IMOD definition of the objective is
referred to as the PC3 model [12].
The other course design elements (i.e., Content, Pedagogy,
and Assessment) are incorporated into the IMOD framework
through interactions with two of the PC3 characteristics.
Course-Content is linked to the content and condition
components of the objective. The condition component is
often stated in terms of pre-cursor disciplinary knowledge,
skills or behaviors. This information, together with the content
defined in the objective, can be used to generate or validate
the list of course topics. Course-Pedagogy is linked to the
performance and content components of the objective. The
types of instructional approaches or learning activities used in
a course should correspond to the level of learning expected
and the disciplinary knowledge, skills or behaviors to be
learned. The content and performance can be used to validate
pedagogical choices. Course-Assessment is linked to the
performance and criteria components of the objective. This
affiliation can be used to test the suitability of the assessment
strategies since an effective assessment, at the very least, must
be able to determine whether the learner’s performance
constitutes competency. Figure 1 shows a visual
representation of the IMOD framework. Learning domains
and domain categories defined by Bloom’s revised taxonomy
[11] are used to describe learner performance. Learning
domains are categorized into Cognitive, Affective, and

Psychomotor, which are further classified under various
Domain Categories (Remember, Understand, Apply, Analyze,
Evaluate, Create). Each Domain Category has performance
verbs associated to it. Learning objective in the PC3 model is
described in terms of Performance, Content, Condition, and
Criteria. Performance is described using an appropriate action
verb from revised Bloom’s taxonomy based on the learning
domain and domain category.
Criteria: Learning objective assessment criteria are
categorized as quality, quantity, speed, and accuracy. Criteria
for learning objectives are described in terms of one or more
of these categories with a criteria value defined or determined
later when the assessment is defined.
Knowledge Dimensions: The revised Bloom’s taxonomy
introduced an additional dimension called the knowledge
dimension that was categorized as Factual, Conceptual,
Procedural and Metacognitive.
Topic Prioritization: The IMODS framework uses a
prioritization framework that classifies topics and subtopics of
a particular course as one of the following:
• Critical
• Important
• Good to know
Achieving the right mix of the three levels of learning
(priorities) is essential to planning a good course.

enduring foundation of knowledge, skills and habits of mind
about curriculum development.
UCD is an emerging design method that focuses on both
operational and technical requirements by observing and
understanding user needs and wants, as well as by prototyping
and testing software throughout all phases of software
lifecycle. It enables the capturing and resolution of any
mismatches between users and software early on. The main
objective of UCD is to allow for a closer match between users
and the software, leading to a more intuitive interaction. As a
design process, it also has the objective of reaching that goal in
the most resource-efficient way, in terms of time and cost
through careful planning and execution [14].
The UCD process can be divided into five main phases: Plan,
User Research, Design, Develop, and Measure. Thus far, the
research team has completed the user research and design
phases of the UCD process. In this respect, 4 focus group
sessions were conducted with prospective users of IMODS to
gather insights on how faculty approach the task of designing a
course. At the beginning of each session all participants were
asked to fill an electronic background survey that collected
demographic information, primary areas of interest in teaching
and research, time spent on teaching, number of courses taught
per year (at both undergraduate and graduate levels), and
number of new courses developed (both at undergraduate and
graduate levels). Participants were also asked to fill an
electronic questionnaire about curriculum design tools that they
currently use to create and manage their courses (e.g. preparing
syllabi; communicating with students; developing teaching
materials; preparing, assigning, and delivering grades, etc.).
The results of this phase were published in ASEE 2014 and
FIE 2014 [15], [16]. The results from the focus group helped
identify the key features of the software system; an ontology
that defines the relevant terms of the domain and identifies
their specific meaning as well as the potential relationships
between them; and mental model, which is a tool to improve
understanding of the user needs and activities. Figure 2 shows
the ontological concepts and relationships between concepts as
a hierarchy. Figure 3 shows the mental model that depicts an
affinity diagram of similar activities organized sequentially into
9 towers in the upper half with detailed relevant activities in the
lower half.

Figure 1: IMODS Framework - PC3 model
C. User-centered design methodology
The IMODS system is being developed using a user-centered
design (UCD) methodology, as opposed to technology focused,
methodology [13]. This approach is well suited for the project
given the high cognitive nature of outcome-based course
design tasks, and the high levels of interactions required
between the user and the system to not only facilitate the
development of course designs, but to help users build an

Figure 2: IMODS Ontological Concepts and Relationships

Figure 5: IMODS Learning Objective Overview
Then as we planned and executed the testing, we were able to
directly ask the user to work on a “Criteria” within “learning
objectives”. Our dual purpose is to let the user know that those
are two concepts of the IMOD, as well as knowing the
structural relationship between them.

Figure 3: Mental Model
III.

HIGH-LEVEL DESIGN OF IMODS

One of the biggest challenges of software design is to make
sure the user has sufficient understanding to use the
application successfully and accomplish the required tasks.
Our User-centered design approach followed two main phases:
A. Conceptualization Phase
After the user research provided a relatively clear idea and
understanding of domain- and user needs, this initial design
phase provides a high-level design with concepts
identification, conceptual modeling and early prototyping.
During initial conceptualization and high-level design, we
focused on Brainstorming sessions and contextual analysis to
build an initial concept of the application. We gradually
consolidated it into a set of requirements on flip charts and
PowerPoint slides. The main goal of high-level design is to
plot down schematic ideas and steps into visual graphs and
models; an early blueprint. We started by investigating
different options and providing design alternatives to make
sure we have a broad view before identifying a good design.
Doing this early on, at high-level, sketchy, paper-based only,
and without going into details help provide several solution
alternatives at a very low cost.
The IMODS system is conceptualized such that a course
design is centered around the learning objectives of the course
defined by the instructor (user) as shown in Figure 4. Learning
objectives are directly associated with the course content,
assessments, and pedagogical activities as defined in the PC3
model (Figure 5).

Figure 4: IMODS System Overview
The Learning Objectives component of the IMODS system
was conceptualized with it various components based on the
PC3 model as shown in Figure 5.

B.
Development Phase
In order to validate that we were proceeding in the right
direction, we ran a series of usability tests on the application.
We chose between multiple good designs instead of focusing
on only one early on. The high-level design sketches were
discussed with the users to make sure what they said in
unstructured dialogs and vague ideas and imaginations can
now be concretely captured in design artifacts for further
validation and clarifications [17]. Our main goal was to
evaluate the simplicity and clarity of the application structure
to allow for an easy-to-recognize mental model. A mental
model can be loosely defined as the user perception of the
application. The opposite is the developers’ perception of the
application. While the latter one is the actual structure that
developers use to build the application, typically as their
interpretation of the requirements, the user mental model is
not necessarily the same. With the fact that users don’t
normally have access to the actual structure of the application,
or detailed and prolonged access to the application to know
any of it’s internal structure, they can only perceive what’s
exposed to them from the UI, and can build an “imagination:
of what the application structure might look like. In ideal
situations, this “imaginary” structure should match the actual
structure build by developers. In reality, though, it is rarely the
case. A discrepancy or vagueness on the user mental model
(we can call it a delta) is typically present and expected. The
problems arise when this delta is large, indicating an
application whose structure is not comprehensible by the user.
We have identified 2 tools that are most suitable for this
project in this phase.
C.
Tools
Navigation Model is one of the essential methods of design
that we used. A significant challenge in complex software is
not the contents of each screen, but how the user mentally
build a mental view of how all screens are connected (like a
city road map), and how to navigate between hundreds of
screens to accomplish their task. In this regard, we have
developed an effective technique, elastic prototyping, an
implementation of a participatory design to help designers and
users build a navigation model together, greatly reducing time
and effort needed. Figure 6 shows the navigation model for the
primary application. Figures 7 and 8 show the navigation
model for new user registration and user login. One of the

Figure 6: Primary IMODS Application - Navigation Model

Figure 7: New User Registration - Navigation
Model

Figure 8: User Login - Navigation Model

Figure 9: Course Overview - Navigation Model

Figure 10: Course Details - Navigation Model

Figure 11: Course Overview Mockup

Figure 12: Learning Objectives Mockup

main components of course design is describing course
overview information that includes data about course title,
description, schedule, instructors, course policies, etc. Figures
9 and 10 show the navigation model for course overview data
entry. In a similar manner navigation model for other screens
of IMODS that are used for design of Learning Objectives,
Content, Assessments, and Pedagogy were created.
Prototyping (PT) is extensively used in UCD to visualize and
validate all otherwise vague ideas and unclear expectations at
low cost and high effectiveness. We focused on three main
categories of prototyping: Paper (low-level) PT, low-fidelity
electronic (medium level) PT, and high-fidelity, detailed PT
[18]. Paper prototypes are very inexpensive and help us
capture several initial ideas and concepts, and validate them.
After explaining their needs, users often change their minds
when they see them on paper. Therefore multiple paper PT
sessions gives a head start in validating what users actually
mean and need. After initial concepts, design ideas and
directions were identified, we moved into a medium fidelity
prototyping stage where we provided a sketchy visualization
of key screens without contents and gradually validated them
and added initial contents. Figures 11 and 12 show the user
interface mockups of Course Overview and Learning
Objective components of the IMODS system
D. System Architecture
The development phase of the project included identifying
appropriate technologies to be used for the development of the
IMODS semantic web application, design of the back-end
database schema, installation and configuration of the serverside and client-side technologies, and development of the user
interface screens for login, registration, index, and creation of
an instructional module and the connectivity of these web
pages with the backend database. An Agile software
development methodology called Scrum is being used for the
development of this project. Scrum is an iterative and
incremental framework for managing product development.
The technologies chosen included Groovy on Grails, an open
source, full stack, web application framework for the Java
Virtual Machine. It takes advantage of the Groovy
programming language and convention over configuration to
provide a productive and streamlined development experience.
Grails uses Spring Model-View–Controller (MVC)
architecture as the underlying framework. MVC is a software
architecture pattern that separates the representation of
information from the user's interaction with it. PostgreSQL
was chosen as the database management system. It is a
powerful, open source object-relational database system with
more than 15 years of active development and a proven
architecture that has earned it a strong reputation for
reliability, data integrity, and correctness. Git was chosen for
source code version control. It is a distributed revision control
and source code management (SCM) system with an emphasis
on speed.

E. Testing
For the testing phase of the project, we opted to not have a
complete discovery test, where the user would be asked to do
a blind discovery of the application without any prior
knowledge [19]. That would be a simulation of a completely
novice user without any application background. Instead, we
decided to test for an average user with some level of
knowledge about the application structure. The reason is that
we already have a concrete navigation model, and we can
typically bring it to any user’s attention in few minutes to help
them in building a correct navigation model. That is one of the
main advantages of using the navigation modeling method.
Currently our research team is working on software
development and testing phases of the project.
IV.

CASE STUDY

The IMODS framework was applied to design an introductory
software engineering course titled “Software Enterprise I:
Personal Software Process” in B.S. in Software Engineering
program. This section describes the use of IMODS – PC3
model for course design.
A. About the Course
Software Enterprise I: Personal Software Process is a
sophomore course in the Software Engineering program that
introduces software engineering and object-oriented software
design principles using a modern programming language.
Students are introduced to Software Engineering, Software
Life Cycle models, Object-Oriented Programming, Personal
Software process, Effort estimation, effort tracking, defect
estimation and defect tracking. Students learn personal
software process for individual professionalism, time and
defect estimation; yield and productivity. A project-based
pedagogical model is used for delivery of all our courses in
Software Engineering program. Students in this course worked
on a game project using Java programming language.
B. Learning Objectives
Learning objectives of this course were defined using the PC3
model. The course has 6 objectives that are categorized under
Performance, Content, Condition, and Criteria as shown in the
Table 1. The objectives are as follows:
•

•

•

LO1: Design a software solution using Object-Oriented
Design principles of encapsulation, information hiding,
abstraction, inheritance, and polymorphism
LO2: Develop a software solution in an object-oriented
programming language employing standard naming
conventions and making appropriate use of advanced
features such as exception handling, I/O operations, and
simple GUI
LO3: Use object-oriented design tools such as UML class
diagrams to model problem solutions and express classes
and relationships such as inheritance, association,
aggregation, and composition

•

•

•

LO4: Use personal software process for individual
development productivity through time estimation and
tracking
LO5: Use personal software process for individual
development quality through defect estimation and
tracking
LO6: Demonstrate teamwork
Table 1: Learning Objectives based on PC3 Model

category, knowledge dimension, and criteria type that each
method is suitable for.
E. Instructional Activities
Pedagogical activities used in this course are listed in Table 4
along with the knowledge dimension and learning domain
category that they are suitable for. The list of activities
includes a mix of lectures, lab activities, Q&A discussions,
and problem solving activities.
Table 3: Course Assessments

** DPA Determined Per Assessment

C. Content
The list of Content topics and subtopics are listed in Table 2.
For each topic the knowledge dimension and topic priority is
defined. This information is used to find assessments and
instructional activities that best fit for delivering a topic.

F. Results
Software Enterprise I: Personal Software Process course in the
Software Engineering program in School of Computing,
Informatics, Decision Systems Engineering (CIDSE) at
Arizona State University was designed using the IMODS –
PC3 model and offered as a face-to-face section (with 82
students) as well as an online section (with 87 students) by the
same instructor (one of the co-authors). Using the IMODS
framework ensured the alignment between various course
elements and thereby ensuring high-quality course design.
Table 4: Course Pedagogical activities

Table 2: Content Topics based on PC3 Model

D. Assessments
Assessments chosen for this course include a mix of both
formative and summative assessments. The PC3 model aligns
assessments chosen for the course with the learning objectives
by checking compatibility of learning domains, performance,
and criteria requirements. Table 3 provides the list of
assessments with their corresponding learning domain

Alignment between various course components:
The framework supports the checking of alignment between
course assessments and learning objectives. The course
assessments are linked to the performance and criteria
elements of the learning objective as shown in Figure 1. The
framework supports the checking of alignment between course
instructional activities and learning objectives. The course
pedagogical activities are linked to the performance and
content of the learning objective as shown in Figure 1.
Topic Prioritization:
Use of the PC3 model ensured a balanced distribution of the
topics under Critical, Important, and Good to know as shown
in figure below.

V.

FUTURE WORK

Following the high-level design phase of the project, the next
step will be software development and testing of IMODS. We
will conduct usability testing of the prototype with instructors
and solicit feedback using surveys, observation, and user
interviews. The feedback will be incorporated into the iterative
software development lifecycle model. The scope of this
project will also include the evaluation of its novel approach to
self-guided web-based professional training in terms of: 1) user
satisfaction with the documentation of course designs
generated; and 2) impact on users’ knowledge of the outcomebased course design process.
ACKNOWLEDGMENT
The authors gratefully acknowledge the support for this project
under the National Science Foundation's Transforming
Undergraduate Education in Science, Technology, Engineering
and Mathematics (TUES) program Award No. DUE-1246139.
REFERENCES
[1] R. M. Felder, R. Brent, and M. J. Prince, “Engineering
Instructional Development: Programs, Best Practices, and
Recommendations.,” Journal of Engineering Education,
vol. 100, no. 1, pp. 89–122, Jan. 2011.
[2] R. Boice, Advice for new faculty members. Allyn &
Bacon, 2000.
[3] R. M. Harden, J. R. Crosby, M. H. Davis, and M.
Friedman, “AMEE Guide No. 14: Outcome-based
Education: Part 5--From Competency to MetaCompetency: A Model for the Specification of Learning
Outcomes.,” Medical Teacher, vol. 21, no. 6, pp. 546–
552, 1999.
[4] W. G. Spady and K. J. Marshall, “Beyond Traditional
Outcome-Based Education.,” Educational Leadership,
vol. 49, no. 2, pp. 67–72, 1991.
[5] G. Adorni, S. Battigelli, D. Brondo, N. Capuano, M.
Coccoli, S. Miranda, F. Orciuoli, L. Stanganelli, A. M.
Sugliano, and G. Vivanet, “CADDIE and IWT: two
different ontology-based approaches to Anytime,
Anywhere and Anybody Learning,” Journal of eLearning and Knowledge Society-English Version, vol. 6,
no. 2, 2010.
[6] S. Ternier, E. Duval, and P. Vandepitte, “LOMster: peerto-peer learning object metadata,” in Proceedings of
EdMedia, 2002, pp. 1942–1943.

[7] “LOCO-Analyst.” [Online]. Available:
http://jelenajovanovic.net/LOCO-Analyst/index.html.
[Accessed: 28-May-2012].
[8] G. P. Wiggins and J. McTighe, Understanding by design.
Association for Supervision & Curriculum Development,
2005.
[9] J. Warren, “Changing community and technical college
curricula to a learning outcomes approach,” Community
College Journal of Research &Practice, vol. 27, no. 8,
pp. 721–730, 2003.
[10] R. A. Streveler, K. A. Smith, and M. Pilotte, “Aligning
Course Content, Assessment, and Delivery: Creating a
Context for Outcome-Based Education,” K. Mohd Yusof,
S. Mohammad, N. Ahmad Azli, M. Noor Hassan, A.
Kosnin and S. K, Syed Yusof (Eds.), Outcome-Based
Education and Engineering Curriculum: Evaluation,
Assessment and Accreditation. Hershey, Pennsylvania:
IGI Global, 2012.
[11] R. F. Mager, “Preparing Instructional Objectives: A
critical tool in the development of effective instruction
3rd edition,” The Center for Effective Performance, Inc,
1997.
[12] K. Andhare, O. Dalrymple, and S. Bansal, “Learning
Objectives Feature for Instructional Module Development
System,” presented at the PSW American Society for
Engineering Education Conference, San Luis Obispo,
California, 2012.
[13] A. Gaffar, “Enumerating mobile enterprise complexity 21
complexity factors to enhance the design process,” in
Proceedings of the 2009 Conference of the Center for
Advanced Studies on Collaborative Research, 2009, pp.
270–282.
[14] A. Gaffar, N. Moha, and A. Seffah, “User-Centered
Design Practices Management and Communication,” in
Proceedings of HCII, 2005.
[15] S. Bansal, O. Dalrymple, A. Gaffar, and R. Taylor, “User
Research for the Instructional Module Development
(IMOD) System,” in American Society for Engineering
Education Annual Conference (ASEE), Indianapolis, IN,
2014.
[16] O. Dalrymple, S. Bansal, A. Gaffar, and R. Taylor,
“Instructional Module Development (IMOD) System: A
User Study on Curriculum Design Process,” in Frontiers
in Education (FIE), Madrid, Spain, 2014.
[17] J. Lazar, J. H. Feng, and H. Hochheiser, Research
methods in human-computer interaction. John Wiley &
Sons Inc, 2009.
[18] W. Lidwell, K. Holden, and J. Butler, Universal
principles of design: 125 ways to enhance usability,
influence perception, increase appeal, make better design
decisions, and teach through design. Rockport Pub, 2010.
[19] N. Moha, A. Gaffar, and G. Michel, “Remote usability
evaluation of web interfaces,” Human Computer
Interaction Research in Web Design and Evaluation. P.
Zaphiris and S. Kurniawan. Hershey, PA, Idea Group
Publishing, pp. 273–289, 2007.

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/261052310

Reputation-Based	Web	Service	Selection	for
Composition
Conference	Paper	·	July	2011
DOI:	10.1109/SERVICES.2011.96	·	Source:	DBLP

CITATIONS

READS

3

29

2	authors:
Srividya	Bansal

Ajay	Bansal

Arizona	State	University

Arizona	State	University

26	PUBLICATIONS			54	CITATIONS			

42	PUBLICATIONS			498	CITATIONS			

SEE	PROFILE

SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Srividya	Bansal	on	27	April	2015.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Reputation-based Web service selection for Composition
Srividya K Bansal
Department of Engineering
Arizona State University
Mesa, Arizona 85212, USA
Email: srividya.bansal@asu.edu

Abstract—The success and acceptance of Web service composition depends on computing solutions comprised of trustworthy services. In this paper, we extend our Web service Composition framework to include selection and ranking of services
based on their reputation score. With the increasing popularity
of Web-based Social Networks like Linkedin, Facebook, and
Twitter, there is great potential in determining the reputation
score of a particular service provider using Social Network
Analysis. We present a technique to calculate a reputation score
per service using centrality measure of Social Networks. We
use this score to produce composition solutions that consist of
services provided by trust-worthy and reputed providers.
Keywords-service composition; reputation; social networks;

Ajay Bansal
Department of Computer Science
Georgetown University
Washington, DC 20057, USA
Email: bansal@cs.georgetown.edu

rest of the network. So our rationale is that these central
figures who play a fundamental role in the network are
trusted by others in the network who are connected (directly
or indirectly) to them.
Our work investigates the following research issues: (i)
compute the reputation score of composition solutions based
on individual scores of service providers obtained using the
centrality measure of social networks (ii) set a threshold for
reputation that each and every Web service involved in the
composition has to satisfy. Failure to meet the threshold will
result in filtering out the Web service and it will not be used
in any composition solution.

I. I NTRODUCTION

II. C ENTRALITY M EASURE IN S OCIAL N ETWORKS :

The next milestone in the evolution of the World Wide
Web is making services ubiquitously available. We need
infrastructure that applications can use to automatically
discover, deploy, compose, and synthesize services. Along
with the functional attributes there is a need to consider
non-functional attributes (Quality of Service parameters) of
Web services in the process of building composite Web
services. The current challenge in automatic composition of
Web services also includes finding a composite Web service
that can be trusted by consumers before using it. In this
paper, we present our approach that uses analysis of Social
Networks to calculate a reputation score for each service
involved in the composition and further prune results based
on this score.
Web-based Social Networks have become increasingly
popular these days. Social Network Analysis is the process
of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups,
organizations, computers, or any knowledge entity. We propose to measure the reputation of a service by measuring
the centrality of a service provider and/or a service provider
organization in a well-known Social Network. We adopt our
idea of computing a reputation score using centrality measure based on the notion of centrality and prestige being key
in the study of social networks [1], [2]. The role of central
people (nodes with high centrality) in a network seems to
be fundamental as they adopt the innovation and help in
transportation and diffusion of information throughout the

Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds.
It involves measuring the formal and informal relationships
to understand information/knowldege flow that binds the
interacting units that could be a person, group, organization,
or any knowledge entity. In order to understand social
networks and their participants, the location of an actor in a
network is evaluated. The network location is measured in
terms of centrality of a node that gives an insight into the
various roles and groupings in a network.
Centrality gives a rough indication of the social power
of a node based on how well they “connect” the network.
The graph-theoretic conception of compactness has been
extended to the study of Social Networks and simply renamed “graph centrality” [1]. Their measures are all based
upon distances between points, and all define graphs as
centralized to the degree that their points are all close
together. Based on research on communication in Social
Networks, the centrality of an entire network should index
the tendency of a single point to be more central than all
other points in the network. Measures of a graph centrality
are based on differences between the centrality of the most
central point and that of all others. Thus, they are indexes
of the centralization of the network [4]. The three most
popular individual centrality measures are Degree Centrality,
Betweenness Centrality, and Closeness Centrality.
Degree Centrality: The network activity of a node can
be measured using the concept of degrees, i.e., the number

ServiceProvider
Provider A
Provider B
Provider C
Provider D
Provider E
Provider F
Provider G

Degree
2
3
1
8
3
4
5

ServiceProvider
Provider H
Provider I
Provider J
Provider K
Provider L
Provider M

Degree
3
1
2
4
1
1

Table I
D EGREE C ENTRALITY OF N ODES IN F IGURE 1

Figure 1.

A Social Network of Web service Providers

of direct connections a node has. In the example network
shown in figure 1 and table I, Provider D has the most direct
connections in the network, making it the most active node
in the network. In personal Social Networks, the common
thought is that “the more connections, the better”.
Betweenness Centrality: Though Provider D has many
direct ties, Provider H has fewer direct connections (close
to the average in the network). Yet, in many ways, Provider
H has one of the best locations in the network by playing
the role of a “broker” between two important components.
Closeness Centrality: Provider F and G have fewer connections than Provider D, yet the pattern of their direct and
indirect ties allow them to access all the nodes in the network
more quickly than anyone else. They have the shortest paths
to all other and hence are in an excellent position to have
the best visibility into what is happening in the network.
Individual network centralities provide insight into the individual’s location in the network. The relationship between
the centralities of all nodes can reveal much about the overall
network structure.

CD (sk ) =

n
X

a(si , sk )

i=0

where a(si , sk ) = 1 iff si and sk are connected
0 otherwise
As such it is a straightforward index of the extent to which
sk is a focus of activity. CD (sk ) is large if service provider
sk is adjacent to, or in direct contact with, a large number of
other service providers, and small if sk tends to be cut off
from such direct contact. CD (sk ) = 0 for a service provider
that is totally isolated from any other point. Our algorithm
filters out any services whose provider has a zero degree
centrality in a social network, i.e., such services will not be
used in building composition solutions. Reputation of the
entire composite service is computed as an average of the
individual reputation score of the services involved in the
composition.
We also need to set a reputation threshold and any service
with a reputation score that is below this threshold is not
used while generating composition solutions. In our initial
prototype implementation we set the reputation threshold
to zero, i.e., degree centrality of the service provider in
the network is zero. A service provider or service provider
organization that is not connected to any other nodes in
the Social network is not known to anyone else and is
an immediate reason to be pruned out from composition
solutions as the service cannot be trusted. Composition
solutions can be ranked such that solutions with highest
reputation score appear on top of the list.
IV. C ONCLUSIONS AND F UTURE W ORK
In this paper, we presented our approach to compute
reputation of services and use this score to select services
for composition. A reputation score is computed for every
service in the repository based on degree centrality of
the service provider in a well-known Web-based Social
Network. Our future work includes exploring other measures
of centrality such as betweenness centrality and closeness
centrality and analyzing the possibility of using a combination of all three measures of centrality to compute reputation
of a service and/or provider.
R EFERENCES

III. R EPUTATION - BASED W EB SERVICE SELECTION FOR
COMPOSITION

We extend our previous work on Web service composition
[3] (that uses both functional and non-functional parameters
to compute composition solutions) by using reputation
to filter services. The reputation score of each service
in a Web service repository is computed as a measure
of the degree centrality (CD ) of the social network to
which the service provider belongs. It is calculated as the
degree or count of the number of adjacencies for a node, sk :

View publication stats

[1] L. C. Freeman, Centrality in Social Networks Conceptual
Clarification, in Social Networks, Vol. 1, No. 3. (1979), pp.
215-239.
[2] S. Wasserman, and K. Faust, Social Network Analysis: Methods
and Applications, Cambridge: Cambridge Univ. Press, 1994.
[3] S. Kona, A. Bansal, M. Blake, and G. Gupta, Generalized
Semantics-based Service Composition in Proceedings of IEEE
Intl. Conference on Web Services (ICWS), September 2008.
[4] H. J. Leavitt, Some effects of communication patterns on group
performance in Journal of Abnormal and Social Psychology,
pp. 46:38-50, 1951.

Generalized Semantics-based Service Composition
Srividya Kona and Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

Abstract. Web services and Service-oriented computing is being widely used
and accepted. In order to effectively reuse existing services, we need to automatically compose Web services. The automatic composition techniques need to be
semantics-based rather than syntax-based, since only semantics can accurately
capture the task a service performs. In this paper we present general semanticsbased techniques for automatic service composition for Web services. Our general method takes as input (i) a repository, R, of semantic descriptions of Web services, (ii) a semantic description, Q, of the service that is desired, and produces a
composite service consisting of services taken from R that realizes Q. The composite service may combine services linearly as a chain or as a directed acyclic
graph, where in the most general case the combination may be conditional. The
composite service generated by our automatic composition method can be coded
as an OWL-S document. We present details of our initial implementation along
with performance results.

1 Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered and consumed. A Web service is an autonomous, platformindependent program accessible over the web that may effect some action or change in
the world (i.e., causes a side-effect). Examples of such side-effects include a web-base
being updated because of a plane reservation made over the Internet, a device being
controlled, etc. As automation increases, these services will be accessed directly by applications rather than by humans [7]. In this context, a Web service can be regarded as
a “programmatic interface” that makes application to application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced may have to satisfy certain
post-conditions. In order to make Web services more practical we need an infrastructure
that allows users to discover, deploy, synthesize and compose services automatically. To
make services ubiquitously available we need a semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their
discovery, composition, deployment, and synthesis [1]. Several efforts are underway to
build such an infrastructure [10–13].
Service Composition involves effectively combining and reusing independently developed component services. A composite service is a collection of services combined
together in some way to achieve a desired effect. Traditionally, the task of automatic

service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii)
Selection, and (iv) Execution [20]. Most efforts reported in the literature focus on one
or more of these four phases. The first phase involves generating a plan, i.e., all the
services and the order in which they are to be composed inorder to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The
second phase involves discovering services as per the plan. Depending on the approach,
often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from
the available potential solutions based on non-functional properties like QoS properties.
The last phase involves executing the services as per the plan and in case any of them
are not available, an alternate solution has to be used.
Note that one must make clear distinction between automatic service composition
and manual service composition. Services can be composed manually and the manually
specified composition encoded as a program/document in a language such as OWL-S
[2] or BPEL4WS [11]. Automatic service composition can be thought of as generating
this OWL-S or BPEL4WS description automatically. Thus, languages such as OWL-S
and BPEL4WS do not solve the automatic service composition problem—they merely
provide notations for specifying service composition.
In this paper we present a general approach for automatic service composition. Our
composition algorithm performs planning, discovery, and selection automatically, all at
once, in one single process. This is in contrast to most methods in the literature where
one of the phases (most frequently planning) is performed manually. Additionally, our
method generates most general compositions based on (conditional) directed acyclic
graphs. In our method, after a solution is obtained, the semantic description of the new
composite service is generated using the composition language OWL-S [2]. This description document can be registered in the repository and is thus available for future
searches. The composite service can now be discovered as a direct match instead of
having to look through the entire repository and build the composition solution again.
Our research makes the following novel contributions: (i) We present a generalized
composition algorithm based on generating conditional directed acyclic graphs; (ii) we
present an efficient and scalable algorithm for solving the composition problem that
takes semantics of services into account; our algorithm automatically discovers and
selects the individual services involved in composition for a given query, without the
need for manual intervention; (iii) we automatically generate OWL-S descriptions of
the new composite service obtained; and, (iv) we present a prototype implementation
based on constraint logic programming that works efficiently on large repositories.
The rest of the paper is organized as follows. In section 2 we present the related work
in the area of service composition and discuss their limitations. In section 3, we present
the service composition problem, the different kinds of composition with examples, and
our technique for automatic composition. In section 4, we discuss automatic generation
of OWL-S service descriptions. Section 5 presents our prototype implementation and
performance results. The last section presents the conclusions and future work.

2 Related Work
Composition of Web services has been active area of research recently [19, 20]. Most
of these approaches present techniques to solve one or more phases listed in section 1.

There are many approaches [14–16] that solve the first two phases of composition
namely planning and discovery. These are based on capturing the formal semantics of
the service using action description languages or some kind of logic (e.g., description
logic). The service composition problem is reduced to a planning problem where the
sub-services constitute atomic actions and the overall service desired is represented by
the goal to be achieved using some combination of atomic actions. A planner is then
used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are
usually not available. To the best of our knowledge, most of these approaches that
use planning are restricted to sequential compositions, rather than a directed acyclic
graph. In this paper we present a technique to automatically select atomic services
from a repository and produce compositions that are not only sequential but also nonsequential that can be represented in the form of a directed acyclic graph. The authors
in [14] present a composition technique by applying logical inferencing on pre-defined
plan templates. Given a goal description, they use the logic programming language
Golog to instantiate the appropriate plan for composing Web services. This approach
also relies on a user-defined plan template which is created manually. One of the main
objective of our work is to come up with a technique that can automatically produce the
composition without the need for any manual intervention.
There are industry solutions based on WSDL and BPEL4WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service by
composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when
an explicit flow is provided. In contrast, our technique automatically determines these
complex flows using semantic descriptions of atomic services.
A process-level composition solution based on OWL-S is proposed in [16]. In this
work the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the
descriptions of these individual services to produce a process-level description of the
composite service. They do not automatically discover/select the services involved in
the composition, but instead assume that they already have the list of atomic services.
In contrast, we present a technique that automatically find the services that are suitable
for composition based on the query requirements for the new composed service.
There are solutions such as [18] that solve the selection phase of composition. This
work uses pre-defined plans and discovered services provided in a matrix representation. Then the best composition plans are selected and ranked based on QoS parameters
like cost, time, and reputation. These criterion are measured using fuzzy numbers.
There has been a lot of work on composition languages such as WS-BPEL, WSML,
AO4BPEL, etc. which are useful during the execution phase. FuseJ is also once such
description language for unifying aspects and components. Though this language was
not designed for Web services, the authors present in [17] that it can be used for service
composition as well. It uses connectors to interconnect services. There is no centralized
process description, but instead is spread across the connectors. With FuseJ, the planning phase has to be performed manually wherein the connectors have to be written.
Similarly, OWL-S can also describe a composite service which is actually an abstract

service. Service grounding of OWL-S maps the abstract service to the concrete WSDL
specification. These languages are useful after the planning, discovery, and selection is
done. The new composite service can be described using one of these languages.
In this paper, we present a technique for automatically planning, discovering and
selecting services that are suitable for obtaining a composite service, based on the user
query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use human
input on what services would be suitable for composition. We also show different kinds
of compositions such as, non-sequential compositions (i.e., composition where there
can be more than one service involved at any stage, represented as a directed acyclic
graph of services), sequential composition (i.e, a linear chain of services) and composition with if-then-else conditions. We also automatically generate OWL-S descriptions
for the new composite service obtained.

3 Web service Composition
Informally, the Web service Composition problem can be defined as follows: given a
repository of service descriptions, and a query with the requirements of the requested
service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the
desired service. Figure 1 shows an example composite service made up of five services
0
0
1 to 5. In the figure, I and C I are the query input parameters and pre-conditions
0
0
respectively. O and C O are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes i and j indicates that outputs of i
constitute (some of) the inputs of j .

S S

S

S

S

S

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Fig. 1. Example of a Composite Service as a Directed Acyclic Graph

As mentioned in section 1, composition can be seen as four phases which include
(i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution. Most of the related work
presents techniques to solve one of these phases or a combination of them. We need a
technique that can automatically determine the services involved in composition without the need for any manual intervention or user-defined plans. In this section we present
the different kind of compositions and our technique for automatic composition which
performs the first three phases - planning, discovery, and selection simultaneously as
one phase.
Definition (Repository of Services): Repository is a set of Web services.
Definition (Service): A service is a 6-tuple of its pre-conditions, inputs, side-effect,
;
;
) is the repreaffected object, outputs and post-conditions. = ( ; ; ;
sentation of a service where
is the list of pre-conditions, is the input list, is the

CI

S CI I A AO O CO
I

A

AO

O

CO
Q CI I A AO O CO
A
AO

service’s side-effect,
is the affected object, is the output list, and
is the list
of post-conditions.
0
0
0
Definition (Query): The query service is defined as = ( 0; 0; 0;
;
;
)
0
0
0
0
where
is the pre-conditions, is the input list,
is the service affect,
is the
0
affected object, 0 is the output list, and
is the post-conditions. These are all the
parameters of the requested service.

CI

I

O

CO

3.1 Sequential Composition
A sequential composition is one which has a linear chain of services that form the composite service. When all nodes in the directed acyclic graph of figure 1 have not more
than one incoming edge and not more than one outgoing edge, the problem reduces to
a sequential composition problem.
Example 1: Suppose we are looking for a service to make travel arrangements, i.e.,
flight, hotel, and rental car reservations. The directory of services contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 1 shows the input/output parameters of the user-query and the three services ReserveFlight, ReserveHotel, and ReserveCar. For the sake of simplicity, the query and services have fewer input/output
parameters than the real-world services.
In this example, service ReserveFlight has to be executed first so that its output
ArrivalFlightNum can be used as input by ReserveHotel followed by the service ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service input/output parameters should be the same as the query
parameters or have the subsumption relation. This can be inferred using semantics from
the ontology provided. Figure 2 shows this example sequential composition as a directed acyclic graph.
Service
Query

Input Parameters
PassengerName, OriginAirport,Start
Date,DestinationAirport,ReturnDate
PassengerName, OriginAirport,Start
Date, DestinationAirport,ReturnDate
PassengerName, ArrivalFlightNum,
StartDate, ReturnDate
PassengerName,ArrivalDate
ArrivalFlightNum, HotelAddress

ReserveFlight
ReserveHotel
ReserveCar

Output Parameters
HotelConfirmationNum,
CarConfirmationNum
FlightConfirmationNum,
ArrivalFlightNum
HotelConfirmationNum,
HotelAddress
CarConfirmationNum

Table 1. Example Scenario for Sequential Composition

G VE
Q CI I A AO O CO
V

Definition (Sequential Composition): More generally, the Composition problem can
be defined as automatically finding a directed acyclic graph = ( ; ) of services
0
0
0
from repository , given query
= ( 0; 0; 0;
;
;
), where is the set
of vertices and is the set of edges of the graph. Each vertex in the graph represents
a service in the composition. Each outgoing edge of a node (service) represents the
outputs and post-conditions produced by the service. Each incoming edge of a node
represents the inputs and pre-conditions of the service. The following conditions should
, i
, i = ( i , i, i ,
hold on the nodes of the graph: i i
i,
i,
i)

R
E

8 S 2 V S 2 R S CI I A AO O CO

I wI O wI O wO
CI ) CI CO ) CI CO ) CO
v

0
1. 0
2, ..., n
1, 1
0
0
2.
1,
1
2 , ...,
n
The meaning of the is the subsumption (subsumes) relation and is the implication
relation. In other words, we are deriving a possible sequence of services where only
the provided input parameters are used for the services and at least the required output
parameters are provided as an output by the chain of services. The goal is to derive a
solution with minimal number of services. Also the post-conditions of a service in the
chain should imply the pre-conditions of the next service in the chain.

QueryInputs

ReserveFlight

)

QueryOutputs
ReserveHotel

ReserveCar

Fig. 2. Example of Sequential Composition as a Directed Acyclic Graph

3.2 Non-Sequential Composition
Let us consider an example where a non-sequential composition can be obtained using
the given repository of services. A non-sequential composition can have more than one
service involved at any stage of the composition task.
Example 2: Suppose we are looking for a service to make international travel arrangements and the directory of services contains ReserveFlight, ReserveHotel, ReserveCar,
and ProcessVisa services. In this scenario, we first need to apply for a visa and then
make the flight, hotel, and car reservations. Table 2 shows the input/output parameters
of the user-query and the four services.
In this example, service ProcessVisa has a post-condition VisaApproved. The services ReserveFlight and ReserveHotel have the pre-condition that the visa must be approved before making reservations. ProcessVisa has to be executed first followed by
ReserveFlight and ReserveHotel. The post-conditions of ProcessVisa must imply the
pre-conditions of ReserveFlight and similarly must imply the pre-conditions of ReserveHotel. The ReserveCar service needs inputs HotelAddress and ArrivalFlightNum.
Hence it is executed after both ReserveFlight and ReserveHotel are executed so that
their outputs can be used as inputs to ReserveCar. The semantic descriptions of the
service input/output parameters should be the same as the query parameters or have the
subsumption relation. This can be inferred using semantics from the ontology provided.
Figure 3 shows this non-sequential composition example as a directed acyclic graph.
Definition (Non-Sequential Composition): More generally, the Composition problem
can be defined as automatically finding a directed acyclic graph = ( ; ) of services
0
0
0
from repository , given query
= ( 0; 0; 0;
;
;
), where is the set
of vertices and is the set of edges of the graph. Each vertex in the graph represents
a service in the composition. Each outgoing edge of a node (service) represents the
outputs and post-conditions produced by the service. Each incoming edge of a node
represents the inputs and pre-conditions of the service. The following conditions should
hold on the nodes of the graph:
where i has exactly one incoming edge that represents the query inputs
1. i i
and pre-conditions, 0
, 0
i
i.
i i

G VE
Q CI I A AO O CO
V

R
E

8 S 2V

S

I wS I CI )^ CI

8 S 2 V where S has exactly
S one outgoing edge that represents the query outputs
and post-conditions, O v O , CO (^ CO .
3. 8 S 2 V where S has at least one incoming edge, let S 1, S 2, ..., S be the
S nodes
such that there is a directed edge from each of these nodes to S . Then v O [
,
( (CO 1^CO 2 ^ CO ^ ).
2.

i

i

i

0

i

i

i

0

i

i

i

i

i

i

im

i

I

0

CIi

i :::

i

im

CI

Ii

k

0

Service

PreInput Parameters
Conditions
Query
PassengerName, OriginAirport,
DestinationAirport,
StartDate, ReturnDate
Process
PassengerName, VisaType,
Visa
StartDate, ReturnDate
Reserve VisaPassengerName, OriginAirport,
Flight
Approved DestinationAirport,
StartDate, ReturnDate
Reserve VisaPassengerName,
Hotel
Approved StartDate, ReturnDate
Reserve
PassengerName, ArrivalDate,
Car
ArrivalFlightNum, HotelAddress

Output Parameters

ik

PostConditions

FlightConfirmationNum,
HotelConfirmationNum,
CarConfirmationNum
ConfirmationNum
VisaApproved
FlightConfirmationNum,
ArrivalFlightNum
HotelConfirmationNum,
HotelAddress
CarConfirmationNum

Table 2. Example Scenario for Non-Sequential Composition

v

)

The meaning of is the subsumption (subsumes) relation and is the implication
relation. In other words, a service at any stage in the composition can potentially have as
its inputs all the outputs from its predecessors as well as the query inputs. The services
in the first stage of composition can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition should contain all the outputs
that the query requires to be produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services in the next stage. Further
details on the formal description of the composition problem are in [5].

Condition:
VisaApproved

ReserveFlight

Parameter:
ArrivalFlightNum

QueryInputs

QueryOutputs
ProcessVisa
Condition:
VisaApproved

ReserveCar

ReserveHotel

Parameter:
HotelAddress

Fig. 3. Example of Non-Sequential Composition as a Directed Acyclic Graph

3.3 Non-Sequential Conditional Composition
Let us now consider an example of a non-sequential composition with if-then-else conditions, i.e., the composition flow varies depending on the result of the post-conditions
of a service.

Example 3: This example is similar to Example 2 but with more constraints. Suppose
we are looking for a service to make international travel arrangements. We first need
to make a tentative flight and hotel reservation and then apply for a visa. If the visa is
approved, we can buy the flight ticket and confirm the hotel reservation, else we will
have to cancel both the reservations. Also if the visa is approved, we need to make
a car reservation. The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel.
Table 3 shows the input/output parameters of the user-query and the services.

Service

PreConditions

Input Parameters

Output Parameters

Query

PasngrName,OriginArprt,Dest-,
Arpt,StartDate,ReturnDate
Reserve
PasngrName,OriginArprt,DestFlight
Arpt,StartDate,ReturnDate
Reserve
PasngrName, StartDate,
Hotel
ReturnDate
Process
PasngrName,VisaType,
Visa
FlightConfNum,HotelConfNum
ConfirmFlight VisaApproved FlightConfNum,CreditCardNum
ConfirmHotel VisaApproved HotelConfNum,CreditCardNum
CancelFlight VisaDenied
PasngrName,FlightConfNum
CancelHotel VisaDenied
PasngrName,HotelConfNum
Reserve
PasngrName,ArrivalDate
Car
ArrivalFlightNum

PostConditions

FlightConfNum,HotelConfNum,CarConfNum
FlightConfNum
HotelConfNum
ConfirmationNum

VisaApproved
_ VisaDenied

ArrivalFlightNum
ConfirmationNum
CancelCode
CancelCode
CarConfirmNum

Table 3. Example Scenario for Non-Sequential Conditional Composition

_ VisaDenied. The services ConfirmFlight and ConfirmHotel have the pre-condition
In this example, service ProcessVisa produces the post-condition VisaApproved

VisaApproved. In this case, one cannot determine if the post-conditions of service ProcessVisa implies the pre-conditions of services ConfirmFlight and ConfirmHotel until
the services are actually executed. In such a case, a condition can be generated which
will be evaluated at runtime and depending on the outcome of the condition, the corresponding services will be executed. The vertex for service ProcessVisa in the graph
is an OR node with the outgoing edges representing the generated conditions and the
VisaApproved and
outputs. In this case conditions (VisaApproved VisaDenied)
(VisaApproved VisaDenied)
VisaDenied are generated. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed.
Figure 4 shows this conditional composition example as an AND/OR directed acyclic
graph.
Definition (Non-Sequential Conditional Composition): More generally, the Composition problem can be defined as automatically finding an AND/OR directed acyclic
0
,
graph = ( ; ) of services from repository , given query = ( 0 ; 0; 0 ;
0
0
O ; C O ), where
is the set of vertices and is the set of edges of the graph. Each vertex in the graph is either an OR or AND vertex and represents a service in the composi-

_

G VE

_

)

V

E

R

)

Q CI I A AO

tion. Each outgoing edge of a node (service) represents the outputs and post-conditions
produced by the service. Each incoming edge of a node represents the inputs and preconditions of the service. The outgoing edges of an AND node have an arc around them
to differentiate from the OR node. The following conditions should hold on the nodes
of the graph:
1. i i
where i has exactly one incoming edge that represents the query inputs
and pre-conditions, 0
, 0
i
i.
i i
2. i i
where i has exactly one outgoing edge that represents the query outputs
0
and post-conditions, 0
,
i
i.
i
i
3. i i
where i is an AND vertex, let i1 , i2, ..., im be the nodes such that
there is a directed edge from i to each of these nodes. Then (Oi I 0 )
,
k ik
C Oi
( i1
i2 :::
im ).
4. i i
where i is an OR vertex, let i1, i2, ..., im be the nodes such that
there is a directed edge from i to each of these nodes. Then (Oi I 0 )
,
k ik
C Oi
( i1
i2 :::
im ).

8 S 2V S S
I w I CI )^ CI
8 S 2V S
O vS O CO (^ CO
8 S 2V
S
S S
S
) CI ^CI ^ CI
8 S 2V
S
S S
S
) CI _CI _ CI

S

S

Parameter:
ArrivalFlightNum
Condition:
VisaApproved

ReserveFlight
Query
Inputs

ProcessVisa
Condition:
VisaApproved

ReserveHotel

[ wS I
[ wS I

ConfirmHotel
QueryOutputs

ConfirmFlight
Parameter:
ArrivalFlightNum

ReserveCar

CancelFlight
CancelHotel

Fig. 4. Example of Conditional Composition as an AND/OR Directed Acyclic Graph

v

)

The meaning of the is the subsumption (subsumes) relation and
is the implication relation. In other words, a service at any stage in the composition can potentially
have as its inputs all the outputs from its predecessors as well as the query inputs. The
services in the first stage of composition can only use the query inputs. The union of the
outputs produced by the services in the last stage of composition should contain all the
outputs that the query requires to be produced. Also the post-conditions of services at
any stage in composition should imply the pre-conditions of services in the next stage.
When it cannot be determined at compile time whether the post-conditions imply the
pre-conditions or not, an OR node is created in the graph. Each outgoing edges represent the possible conditions which will be evaluated at run-time. Depending on the
condition that holds, the corresponding services are executed. That is, if a subservice
1 is composed with subservice 2 , then the postconditions
1 of 1 must imply the
preconditions 2 of 2 . The following conditions are evaluated at run-time:
if ( 1
2 ) then execute 1 ;
else if ( 1
2 ) then no-op;
else if ( 2) then execute 1 ;

S

CI S
CO ) CI
S
CO ) : CI
CI
S

S

CO S

3.4 Automatic Composition Algorithm
In order to produce the composite service which is the graph, as shown in the example
figure 1, we filter out services that are not useful for the composition at multiple stages.
Figure 5 shows the filtering technique for the particular instance shown in figure 1. The

composition routine starts with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. In figure 5,
C I ; I are the pre-conditions and the input parameters provided by the query.
1 and
2 are the services found after step 1. 1 is the union of all outputs produced by the
services at the first stage. For the next stage, the inputs available are the query input
parameters and all the outputs produced by the previous stage, i.e., 2 = 1 I . 2
is used to find services at the next stage, i.e., all those services that require a subset of
2. In order to make sure we do not end up in cycles, we get only those services which
require at least one parameter from the outputs produced in the previous stage. This
filtering continues until all the query output parameters are produced. At this point we
make another pass in the reverse direction to remove redundant services which do not
directly or indirectly contribute to the query output parameters. This is done starting
with the output parameters working our way backwards.

S

S

O

I O[ I

I

I=I
CI, I

1

S1
S2
.
.

O1

I=IUO
2

1

1

S
.
.

O

2

3

I=IUO
3

2

2

O
S

3

I=IUO
4

4

.
.

3

3

S

O

4

O

5

.
.

Fig. 5. Composite Service

4 Automatic Generation of OWL-S descriptions
After we have obtained a composition solution (sequential, non-sequential, or conditional), the next step is to produce a semantic description document for this new composite service. Then this document can be used for execution of the service and to
register the service in the repository, thereby allowing subsequent queries to result in
a direct match instead of performing the composition process all over again. We used
the existing language OWL-S [2] to describe composite services. OWL-S models services as processes and when used to describe composite services, it maintains the state
throughout the process. It provides control constructs such as Sequence, Split-Join, IfThen-Else and many more to describe composite services. These control constructs can
be used to describe the kind of composition. OWL-S also provides a property called
composedBy using which the services involved in the composition can be specified.
Below is the algorithm for generation of the OWL-S document when the composition
solution in the form of a graph is provided as the input.
Algorithm: GenerateServiceDescription (Input: G - Solution Graph)
1. Generate generic header constructs
2. Start Composite Service element
3. Start SequenceConstruct
4.
If Number(SourceVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each starting/source Vertex V
GenerateAtomicService
End For

5.

6.

7.
8.
9.

EndSplitJoinConstruct
End If
If Number(SinkVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each ending/sink Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct
End If
For Each remaining vertex V in G
If V is AND vertex with one outgoing edge
GenerateAtomicService
If V is AND vertex with more than one outgoing edge
GenerateSplitJoinConstruct
If V is OR vertex with one outgoing edge
GenerateAtomicService
If V is OR vertex with more than one outgoing edge
GenerateConditionalConstruct
End For
End SequenceConstruct
End Composite Service element
Generate generic footer constructs

A sequential composition can be described using the Sequence construct which indicates that all the services inside this construct have to invoked one after the other in
the same order. The non-sequential composition can be described in OWL-S using the
Split-Join construct which indicates that all the services inside this construct can be invoked concurrently. The process completes execution only when all the services in this
construct have completed their execution. The non-sequential conditional composition
can be described in OWL-S using the If-Then-Else construct which specifies the condition and the services that should be executed if the condition holds and also specifies
what happens when the condition does not hold. Conditions in OWL-S are described
using SWRL. The OWL-S description documents for the example composite services
in section 3 are shown in the Appendix.
There are other constructs such as looping constructs in OWL-S which can be used
to describe composite services with complex looping process flows. We are currently
investigating other kinds of compositions with iterations and repeat-until loops and their
OWL-S document generation. We are exploring the possibility of unfolding a loop into
a linear chain of services that are repeatedly executed. We are also analyzing our choice
of the composition language and looking at other possibilities as part of our future work.

5 Implementation
We implemented a prototype composition engine using Prolog with Constraint Logic
Programming over finite domain [8], referred to as CLP(FD) hereafter. In our current
implementation, we used semantic descriptions of web services written in the language
called USDL [4]. The repository of services contains one description document for
each service. USDL itself is used to specify the requirements of the service that an
application developer is seeking.

Each service description is converted into a tuple:
(Pre-Conditions, I, A, O, Post-Conditions).
I is the list of inputs and O is the list of outputs. Pre-Conditions are list of conditions
on the input parameters and Post-Conditions are the list of conditions on the output
parameters. A is the list of side-effects represented as affect-type(affected-object) where
the function symbol affect-type is the side-effect of the service and affected object is
the object that changed due to the side-effect. Services are converted to tuples so that
they can be treated as terms in first-order logic and specialized unification algorithms
can be applied to obtain exact, generic, specific, part and whole substitutions [6]. In
case conditions on a service are not provided, the Pre-Conditions and Post-Conditions
in the triple will be null. Similarly if the affect-type is not available, this module assigns
a generic affect to the service.
The composition engine consists of these modules: (i) Tuple Generator; (ii) Query
Reader; (iii) SemanticRelations Generator; (iv) Composition Query Processor; (v) OWLS Description Generator;
TupleGenerator converts each service in the repository into the tuple format. The
SemanticRelationsGenerator module extracts all the semantic relations and creates a
list of Prolog facts. In our current implementation, we use USDL service descriptions
which use OWL Wordnet Ontology [3] to specify the semantics. This module is generic
enough to be used with other domain-specific ontology as well to obtain semantic relations of concepts. The CompositionQueryProcessor module uses the repository of facts,
which contains all the services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition
engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs),
encodeParam(QueryOutputs, QO),
getExtInpList(QueryInputs, InpList),
encodeParam(InpList, QI),
performForwardTask(QI, QO, LF),
performBackwardTask(LF, QO, LR),
getMinSolution(LR, QI, QO, A), reverse(A, RevA),
confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The output of the query processor is the composition solution which is directed
acyclic graph of all the services involved in the composition. Our algorithm selects the
optimal solution with least composition length (i.e., the number of stages involved in
the composition). If there are any properties with respect to which the solutions can
be ranked, then setting up global constraints to get the optimal solution is relatively
easy with our constraint based approach. For example, if each service has an associated
cost, then the solutions with the minimal cost are returned. The next step is to produce
a description of the new composite service solution found. OWL-S DescriptionGenerator automatically generates the OWL-S description of the composite service using
constructs depending on the type of composition.
We tested our composition algorithm using repositories from WS-Challenge website[9], slightly modified to fit into USDL framework. They provide repositories of
various sizes (thousands of services). These repositories contain WSDL descriptions of

services. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema file. We evaluated
our approach on different size repositories and tabulated Pre-processing and Query Execution time. We noticed that there was a significant difference in pre-processing time
between the first and subsequent runs (after deleting all the previous pre-processed data)
on the same repository. What we found is that the repository was cached after the first
run and that explained the difference in the pre-processing time for subsequent runs.
Table 4 shows performance results for the different kind of compositions. The preprocessing time remains the same of all three kinds of composition whereas the query
execution time varies. The times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than
or equal to the wall clock time. The results are consistent with our expectations: for a
fixed repository size, the preprocessing time increases with the increase in number of
input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time
is directly proportional to the size of the repository. However, what is surprising is the
efficiency of service query processing, which is negligible (just 1 to 3 msecs) even for
complex queries with large repositories.
Repository
Size
(num of
services)
2000
2000
2000
2500
2500
2500
3000
3000
3000

Number
of I/O
param-eters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.5
45.8
57.8
47.7
58.7
71.6
56.8
77.1
88.2

QueryExec Time (msecs)
Sequential
Composition
1
1
2
1
1
2
1
1
3

NonSequential
Composition
1
1
2
1
2
2
1
2
3

Conditional
Composition
1
2
2
1
2
3
1
3
4

Table 4. Performance of Composition Algorithm

6 Conclusions and Future Work
To make Web services more practical we need an infrastructure that allows users to
discover, deploy, synthesize and compose services automatically. Our semantics-based
approach uses semantic description of Web services to find substitutable and composite services that best match the desired service. Given semantic description of Web
services, our engine produces optimal results (based on criteria like cost of services,
number of services in a composition, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential
or non-sequential composition that is possible for a given query and also automatically
generates OWL-S description of the composite service. This OWL-S description can
be used during the execution phase and subsequent searches for this composite service

will yield a direct match. We are able to apply many optimization techniques to our
system so that it works efficiently even on large repositories. Use of Constraint Logic
Programming helped greatly in obtaining an efficient implementation of this system.
Our future work includes extending our engine to support an external database to
save off pre-processed data. This will be particularly useful when service repositories
grow extremely large in size which can easily be the case in future. We are also investigating other kinds of compositions with loops such as repeat-until and iterations and
their OWL-S description generation. Analyzing the choice of the composition language
and exploring other language possibilities is also part of our future work.
Acknowledgments: We are grateful to our co-researcher Ajay Bansal for the helpful
discussions and comments.

References
1. S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16,
Issue 2, pp. 46-53, March 2001.
2. OWL-S www.daml.org/services/owl-s/1.0/owl-s.html.
3. OWL WordNet: Ontology-based information management system. http://taurus.
unine.ch/knowler/wordnet.html.
4. A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal ServiceSemantics Description Language. In ECOWS, pp. 214-225, 2005.
5. S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In
ICWS, 2007.
6. S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics
Description Language for Automatic Service Discovery and Composition. Tech. Report
UTDCS-18-06. www.utdallas.edu/˜sxk038200/USDL.pdf.
7. A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent
Services: A case study in chemical emergency response. In ICWS, pp.751-758, 2005.
8. K. Marriott and P. Stuckey. Prog. with Constraints: An Introduction. MIT Press, 1998.
9. WS Challenge 2006. http://insel.flp.cs.tu-berlin.de/wsc06.
10. U. Keller, R. Lara, H. Lausen, A. Polleres, and D. Fensel. Automatic Location of Services.
In European Semantic Web Conference, May 2005.
11. D. Mandell, S. McIlraith Adapting BPEL4WS for the Semantic Web: The Bottom-Up Approach to Web Service Interoperation. In ISWC, 2003.
12. M. Paolucci, T. Kawamura, T. Payne, and K. Sycara Semantic Matching of Web Service
Capabilities. In ISWC, pages 333-347, 2002.
13. S. Grimm, B. Motik, and C. Preist Variance in e-Business Service Discovery. In Semantic
Web Services Workshop at ISWC, November 2004.
14. S. McIlraith, T.C. Son Adapting golog for composition of semantic Web services. In KRR,
pages 482–493, 2002.
15. B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pp.467-477.
16. M. Pistore, P. Roberti, and P. Traverso Process-Level Composition of Executable Web Services In European Semantic Web Conference, pages 62-77, 2005.
17. D. Suvee, B. Fraine, and M. Cibran Evaluating FuseJ as a Web Service Composition Language In European Conference on Web Services, 2005.
18. D. Claro, P. Albers, and J. Hao Selecting Web services for Optimal Compositions In Workshop on Semantic Web Services and Web Service Composition, 2004.
19. J. Rao, X. Su. A Survey of Automated Web Service Composition Methods In Workshop on
Semantic Web Services and Web Process Composition(SWSWPC), 2004.
20. J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

Appendix
The sequential composite service shown in Example 1 and Figure 2 is described in
OWL-S as follows:
<rdf:RDF ...
<process:CompositeProcess rdf:ID="TravelReservation">
...
<process:composedOf><process:Sequence>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#ReserveFlight"/>
<process:AtomicProcess rdf:about="#ReserveHotel"/>
<process:AtomicProcess rdf:about="#ReserveCar"/>
</process:components></process:Sequence>
</process:composedOf>
</process:CompositeProcess></rdf:RDF>

The non-sequential composite service shown in Example 2 and Figure 3 is described in
OWL-S as follows:
<rdf:RDF ...
<process:CompositeProcess rdf:ID="TravelReservation">
...
<process:composedOf><process:Sequence>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#ProcessVisa"/>
<process:CompositeProcess rdf:about="#Stage1"/>
<process:AtomicProcess rdf:about="#ReserveCar"/>
</process:components>
</process:Sequence>
</process:composedOf>
</process:CompositeProcess>
<process:CompositeProcess rdf:ID="Stage1">
<process:composedOf><process:Split-Join>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#ReserveFlight"/>
<process:AtomicProcess rdf:about="#ReserveHotel"/>
</process:components></process:Split-Join>
</process:composedOf>
</process:CompositeProcess></rdf:RDF>

The conditional composite service shown in Example 3 and Figure 4 is described in
OWL-S as follows:
<rdf:RDF ...
<process:CompositeProcess rdf:ID="TravelReservation">
...
<process:composedOf><process:Sequence>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#Stage1"/>
<process:AtomicProcess rdf:about="#ProcessVisa"/>
<process:CompositeProcess rdf:about="#IfThenElseStage1"/>
</process:components></process:Sequence>
</process:composedOf>
</process:CompositeProcess>

<process:CompositeProcess rdf:ID="IfThenElseStage1">
...
<process:composedOf>
<process:If-Then-Else>
<process:ifCondition>
<expr:SWRL-Condition rdf:ID="VisaAccepted">
<expr:expressionLanguage rdf:resource="&expr;#SWRL"/>
<expr:expressionBody rdf:parseType="Literal">
<swrl:AtomList>
<rdf:first>
<swrl:IndividualPropertyAtom>
<swrlb:equal rdf:resource="#VisaAccepted"/>
<swrl:argument1 rdf:resource="#VisaAccepted"/>
<swrl:argument2 rdf:resource="&rdf;#true"/>
</swrl:IndividualPropertyAtom>
</rdf:first>
<rdf:rest rdf:resource="&rdf;#nil"/>
</swrl:AtomList>
</expr:expressionBody>
</expr:SWRL-Condition>
</process:ifCondition>
<process:then>
<process:Sequence>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#ConfirmFlight"/>
<process:CompositeProcess rdf:about="#Stage2"/>
</process:components>
</process:Sequence>
</process:then>
<process:else>
<process:Sequence>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#CancelFlight"/>
<process:AtomicProcess rdf:about="#CancelHotel"/>
</process:components></process:Sequence>
</process:else>
</process:If-Then-Else></process:composedOf>
</process:CompositeProcess>
<process:CompositeProcess rdf:ID="Stage1">
<process:composedOf><process:Split-Join>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#ReserveFlight"/>
<process:AtomicProcess rdf:about="#ReserveHotel"/>
</process:components></process:Split-Join>
</process:composedOf>
</process:CompositeProcess>
<process:CompositeProcess rdf:ID="Stage2">
<process:composedOf><process:Split-Join>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#ConfirmHotel"/>
<process:AtomicProcess rdf:about="#ReserveCar"/>
</process:components></process:Split-Join>
</process:composedOf>
</process:CompositeProcess></rdf:RDF>

Semantics-based Web Service Composition engine
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
The University of Texas at Dallas
Richardson, TX 75083

Abstract
Service-oriented computing is gaining wider acceptance. We need an infrastructure that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper we present an approach
for automatic service discovery and composition based on
semantic description of Web services. The implementation
will be used for the WS-Challenge 2007 [1].

1. Introduction
In order to make services ubiquitously available, we need
a semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis [6]. Informally, a service is characterized by its input
parameters, the outputs it produces, and the side-effect(s)
it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced
may have to satisfy certain post-conditions. For discovery and composition, one could take the syntactic approach
in which the services being sought in response to a query
simply have their inputs syntactically match those of the
query. Alternatively, one could take the semantic approach
in which the semantics of inputs and outputs, as well as a
semantic description of the side-effect is considered in the
matching process. Several efforts are underway to build an
infrastructure for service discovery, composition, etc. These
efforts include approaches based on the semantic web (such
as USDL [4], OWL-S [7], WSML [8], WSDL-S [9]) as well
as those based on XML, such as Web Services Description Language (WSDL [5]). Approaches such as WSDL are
purely syntactic in nature, that is, they only address the syntactical aspects of a Web service. In this paper we present
our approach for automatic service composition which is
an extension of our implementation that we used at WSChallenge 2006 [3].
In section 2 we present the formal definition of the Com-

Thomas D. Hite
Metallect Corp.
2400 Dallas Parkway
Plano, TX 75093

position problem. We describe our Service Composition algorithm in section 3. Section 4 presents the design of our
software with brief descriptions of the different components
of the system followed by conclusions and references.

2. Automated Web service Discovery and Composition
Discovery and Composition are two important tasks related to Web services. In this section we formally describe
these tasks. We also develop the requirements of an ideal
Discovery/Composition engine.

2.1. The Discovery Problem
Given a repository of Web services, and a query
requesting a service (we refer to it as the query service
in the rest of the text), automatically finding a service
from the repository that matches these requirements is the
Web service Discovery problem. Valid solutions to the
query satisfy the following conditions: (i) they produce
at least the query output parameters and satisfy the query
post-conditions; (ii) they use only from the provided input
parameters and satisfy the query pre-conditions; (iii) they
produce the query side-effects. Some of the solutions may
be over-qualified, but they are still considered valid as
long as they fulfill input and output parameters, pre/post
conditions, and side-effects requirements.

2.2. The Composition Problem
Given a repository of service descriptions, and a query
with the requirements of the requested service, if a matching service is not found, then the composition task can be
performed. The composition problem involves automatically finding a directed acyclic graph of services that can be
composed to obtain the desired service. Figure 1 shows an
example composite service made up of five services 1 to
0
0
5 . In the figure, I and C I are the query input parameters
and pre-conditions respectively. O0 and C O0 are the query

S

S

S

S

output parameters and post-conditions respectively. Informally, the directed arc between nodes i and j indicates
that outputs of i constitute (some of) the inputs of j .
Discovery and composition can be viewed as a single
problem. Discovery is a simple case of composition where
the number of services involved in composition is exactly
equal to one.

S

S

the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all
nodes in the graph have not more than one incoming edge
and not more than one outgoing edge, the problem reduces
to a sequential composition problem.

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
=( ; ; ;
;
;
) is the reprepost-conditions.
sentation of a service where
is the pre-conditions,
is the input list,
is the service’s side-effect,
is the
affected object,
is the output list, and
is the postconditions.
Definition (Repository of Services): Repository is a set of
Web services.
Definition (Query): The query service is defined as
0
0
0
0
= ( 0; 0; 0;
;
;
) where
is the pre0
0
conditions,
is the input list,
is the service affect,
0
0
is the affected object, 0 is the output list, and
is the post-conditions. These are all the parameters of the
requested service.
Definition (Composition): The Composition problem can
be defined as automatically finding a directed acyclic graph
= ( ; ) of services from repository , given query =
0
0
0
( 0; 0; 0;
;
;
), where is the set of vertices
and is the set of edges of the graph. Each vertex in the
graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of
a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:
where i has zero incoming edges,
1. i i
0
0
,
i
i.
i i
2. i i
where i has zero outgoing edges,
0
0
,
i
i.
i
i
3. i i
where i has at least one incoming edge,
let i1, i2 , ..., im be the nodes such that there is
a directed edge from each of these nodes to i . Then
0
0
Ii
( i1
C I ).
I , CI i
i2 :::
im
k ik
The meaning of the is the subsumption (subsumes) reis the implication relation. Figure 2 explains
lation and
one instance of the composition problem pictorially. When

S CI I A AO O CO
CI
A
AO
O
CO

Q CI I A AO O CO
I
A
AO
O
G VE
CI I A AO O CO
E

CI

V

R

8 SS 2 V
S
I w I CI )^ CI
8 SS 2 V
S
O v O CO (^ CO
8 S 2V
S
S S S
S
vS O [ ( CO ^CO ^CO ^
v
)

I

CO
Q

Figure 2. Composite Service

2.3

Requirements of an ideal Engine

The features of an ideal Discovery/Composition engine
are:
Correctness: One of the most important requirement for
an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the
requirements of the query. Also, the engine should be able
to find all services that satisfy the query requirements.
Small Query Execution Time: Querying a repository of
services for a requested service should take a reasonable
amount of (small) time, i.e., a few milliseconds. Here we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental Updates: Adding or updating a service to an
existing repository of services should take a small amount
of time. A good Discovery/Composition engine should not
pre-process the entire repository again, rather incrementally
update the pre-processed data (indexes, etc.) of the repository for this new service added.
Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition
engine should be able to give results based on requirements
(minimize, maximize, etc.) over the costs. We can extend
this to services having an attribute vector associated with
them and the engine should be able to give results based
on maximizing or minimizing functions over this attribute
vector.
These requirements have driven the design of our

semantics-based Composition engine described in the following sections.

3. A Multi-step Narrowing Solution

S1
Query
described
using
USDL
(S)

Sub-queries

Service Composition Algorithm: For service composition, the first step is finding the set of composable services.
The correct sequence of execution of these services can be
determined by the pre-conditions and post-conditions of the
individual services. That is, if a subservice 1 is composed
with subservice 2 , then the post-conditions of 1 must imply the pre-conditions of 2. The goal is to derive a single
solution, which is a directed acyclic graph of services that
can be composed together to produce the requested service
in the query. Figure 4 shows a pictorial representation of
our composition engine.
In order to produce the composite service which is represented by a graph as shown in figure 1, we filter out services that are not useful for the composition at multiple
stages. Figure 3 shows the filtering stages for the particular
instance shown in figure 1. The composition routine starts
with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. In figure 3, C I ; I are the pre-conditions and
the input parameters provided by the query. 1 and 2 are
the services found after step 1. 1 is the union of all outputs produced by the services at the first stage. For the next
stage, the inputs available are the query input parameters
and all the outputs produced by the previous stage, i.e., 2
= 1 I . 2 is used to find services at the next stage, i.e., all
those services that require a subset of 2. In order to make
sure we do not end up in cycles, we get only those services
which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all
the query output parameters are produced. At this point we
make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute
to the query output parameters. This is done starting with
the output parameters working our way backwards.

S

S

O[ I

I=I
CI, I

1

S

S

O

S

I

I

S1
S2
.
.

O1

I=IUO
2

1

1

S
.
.

O

2

3

I=IUO
3

2

2

O
S

3

I=IUO
4

3

4

.
.

Discovery Module
(Discovery Engine + Service
Directory + Term Generator)
S1

We assume that a directory of services has already been
compiled, and that this directory includes semantic descriptions for each service. In this section we describe our Service Composition algorithm.

S

.
.
.
Sn

Infer

3

S

O

4

O

5

.
.

Figure 3. Composite Service
Algorithm: Composition
Input: QI - QueryInputs, QO - QueryOutputs, QCI -

Pre-Cond(S)
S1

Post-Cond( S)1

S2

Sn

Composition Engine
(implemented using
Constraint Logic
Programming

Composed Service

Pre-Cond( S)1

..........................

................................. S n

Post-Cond( S)n
Post-Cond(S)

Pre-Cond( S)2

Figure 4. Composition Engine
Pre-Cond, QCO - Post-Cond
Output: Result - ListOfServices
1. L NarrowServiceList(QI, QCI);
2. O GetAllOutputParameters(L);
3. CO GetAllPostConditions(L);
4. While Not (O QO)
5.
I = QI O; CI QCI CO;
6.
L’ NarrowServiceList(I, CI);
7. End While;
8. Result
RemoveRedundantServices(QO, QCO);
9.Return Result;

[

w

^

4. Implementation
Our composition engine is implemented using Prolog
[10] with Constraint Logic Programming over finite domain
[11], referred to as CLP(FD) hereafter. In this section we
briefly describe our software system and its modules. The
details of the implementation along with performance results are shown in [2].
Triple Generator: The triple generator module converts each service description into a triple as follow:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to
the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output
parameters. Services are converted to triples so that they
can be treated as terms in first-order logic. In case conditions on a service are not provided, the Pre-Conditions and
Post-Conditions in the triple will be null. Similarly if the
affect-type is not available, this module assigns a generic
affect to the service.
Query Reader: This module reads a query file (in XML
format, possibly different from the XML format used for a

service) and converts it into a triple used for querying in our
engine.
Semantic Relations Generator: We obtain the semantic
relations from the provided ontology. This module extracts
all the semantic relations and creates a list of Prolog facts.
Composition Query Processor: The composition engine is
written using Prolog with CLP(FD) library. It uses a repository of facts, which contains list of services, their input and
output parameters and the semantic relations between the
parameters. The following is the code snippet of our composition engine:
composition(sol(Qname,A)) :dQuery(Qname,_,_),
minimize(compTask(Qname,A,SeqLen),SeqLen).
compTask(Qname, A, SeqLen) :dQuery(Qname,QI,QO), encodeParam(QO,OL),
narrowO(OL,SL), fd_set(SL,Sset),
fdset_member(S_Index,Sset),
getExtInpList(QI,InpList),
encodeParam(InpList,IL), list_to_fdset(IL,QIset),
serv(S_Index,SI,_), list_to_fdset(SI,SIset),
fdset_subtract(SIset,QIset,Iset),
comp(QIset,Iset,[S_Index],SA,CompLen),
SeqLen #= CompLen + 1, decodeS(SA,A).
comp(_, Iset, A, A, 0) :- empty_fdset(Iset),!.
comp(QIset, Iset, A, SA, SeqLen) :fdset_to_list(Iset,OL),
narrowO(OL,SL), fd_set(SL,Sset),
fdset_member(SO_Index,Sset), serv(SO_Index,SI,_),
list_to_fdset(SI,SIset),
fdset_subtract(SIset,QIset,DIset),
comp(QIset,DIset,[SO_Index|A],SA,CompLen),
SeqLen #= CompLen + 1.

The query is converted into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate “bagof” to return all possible ListOfServices that can be composed to get the requested
queryService.
Output Generator: After the Composition Query processor finds a matching service, or the graph of atomic
services for a composed service, the results are sent to
the output generator in the form of triples. This module
generates the output files in any desired XML format. For
the WS-Challenge, this module will produce output files in
the format provided [1].
For this year’s challenge, the software has to receive requests and return results via SOAP. Hence our software will
work as a Web service whose interface will accept the discovery/composition query.

5. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure
to publish services, document services and query repositories for matching services. We presented our approach for
Web service composition. Our composition engine can find
a graph of atomic services that can be composed to form the
desired service as opposed to simple sequential composition
in our previous work [3]. Given semantic description of
Web services, our solution produces accurate and quick results. We are able to apply many optimization techniques to
our system so that it works efficiently even on large repositories. The use of Constraint Logic Programming (CLP)
helped greatly in obtaining an efficient implementation of
this system. We used a number of built-in features such as
indexing, set operations, and constraints and hence did not
have to spend time coding these ourselves. These CLP(FD)
built-ins facilitated the fast execution of queries.

References
[1] WS Challenge 2007 http://ws-challenge.
org.
[2] S. Kona, A. Bansal, G. Gupta, and T. Hite. Efficient
Web Service Discovery and Composition using Constraint Logic Programming. In ALPSWS Workshop at
FLoC 2006.
[3] S. Kona, A. Bansal, G. Gupta, and T. Hite. Web
Service Discovery and Composition using USDL. In
CEC/EEE, June 2006.
[4] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005.
[5] Web Services Description Language. http://www.
w3.org/TR/wsdl.
[6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp.
46-53, March 2001.
[7] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[8] WSML: Web Service Modeling Language. www.
wsmo.org/wsml/.
[9] WSDL-S: Web Service Semantics. http://www.
w3.org/Submission/WSDL-S.
[10] L. Sterling and S. Shapiro. The Art of Prolog. MIT
Press, 1994.
[11] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/273793125

Integrating	Big	Data:	A	Semantic	ExtractTransform-Load	Framework
ARTICLE		in		COMPUTER	·	MARCH	2015
Impact	Factor:	1.44	·	DOI:	10.1109/MC.2015.76

READS

91

2	AUTHORS,	INCLUDING:
Srividya	Bansal
Arizona	State	University
12	PUBLICATIONS			25	CITATIONS			
SEE	PROFILE

All	in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate,
letting	you	access	and	read	them	immediately.

Available	from:	Srividya	Bansal
Retrieved	on:	26	January	2016

Semantic Extract-Transform-Load framework for Big Data Integration
Srividya K Bansal
Arizona State University, Mesa, AZ, USA
srividya.bansal@asu.edu

Abstract— Big Data researchers are dealing with the Variety of
data that includes various formats such as structured, numeric,
unstructured text data, email, video, and audio. The proposed
Semantic Extract-Transform-Load (ETL) framework that uses
semantic technologies to integrate and publish data from
multiple sources as open linked data provides an extensible
solution for effective data integration, facilitating the creation
of smart urban apps for smarter living. A case study that
integrates datasets, using the proposed framework, from
various Massive Open Online Courses and Household travel
data along with Fuel Economy data is presented.
Keywords—data integration; linked data; ontology engineering;
semantic technologies

Big Data comprises of data consisting of billions to trillions
of records of millions of people - all from different sources
(e.g. Web, customer contact center, social media, mobile
data, sales, etc.). The data is typically loosely structured and
is often incomplete and inaccessible. Big Data is
transforming science, engineering, medicine, healthcare,
finance, business, and ultimately society itself. Massive
amounts of data are available to be harvested for competitive
business advantage, government policies, and new insights
into a broad array of applications (including healthcare,
biomedicine, energy, smart cities, genomics, transportation,
etc.). Yet, most of this data is inaccessible to users, as we
need technology and tools to find, transform, analyze, and
visualize data in order to make it consumable for decisionmaking [1]. The research community also agrees that it is
important to engineer Big Data meaningfully [2].
Meaningful data integration in a schema-less, and complex
Big Data world of databases is a big open challenge. Big
Data research is usually discussed in the areas of 3V’s –
Volume (storage of massive amount of data streaming in
from social media, sensors, and machine-to-machine data
being collected), Velocity (reacting quickly enough to deal
with data in near-real time), and Variety (data is in various
formats such as structured, numeric, unstructured text data,
email, video, audio, stock ticker, etc.). Big Data challenges
are not only in storing and managing this variety of data but
also extracting and analyzing consistent information from it.
Researchers are working on creating a common conceptual
model for the integrated data [3].
The method of publishing and linking structured data on
the web is called Linked Data. This data is machinereadable, its meaning is explicitly defined, it is linked to
other external data sets, and it can be linked to from other
data sets as well. The Linked Open Data (LOD) community
effort has led to a huge data space, with 31 billion Resource

Sebastian Kagemann
Indiana University – Bloomington, IN, USA
sakagema@umail.iu.edu

Description Framework (RDF) [4] triples, and a W3C
specification for data interchange on the web [5]. LOD can
be used in a number of interesting Web and mobile
applications. Linking Open Government Data (LOGD)
project [6] investigates translating government-related data
using Semantic web technologies. LOD has gained
significant adoption and momentum, though the quality of
the interconnecting relationships remains questionable [7].
IBM Smarter City initiative aims at creating cities that are
vital and safe for its citizens and businesses. Their focus is
on building the infrastructure for fundamental services—
such as roadways, mass transit and utilities that make a city
desirable and livable. IEEE Smart Cities Initiative brings
together technology, government and society to enable smart
economy, mobility, environment, living, and governance.
Both these initiatives have to integrate and use information
from various data sources in addition to setting up the
required infrastructure. Government agencies are also
increasingly making their data accessible through initiatives
such as data.gov to promote transparency and economic
growth [8]. We need ways to organize variety of data such
that concepts with similar meaning are related through links,
while the concepts that are distinct are clearly represented as
well with semantic metadata. This will allow effective and
creative use of query engines and analytic tools for Big Data,
which is absolutely essential to create smart and sustainable
environments. Figure 1 shows the future vision of a web
portal with Linked Open Urban data integrated and
published from various sources and domains. The need to
integrate Big Data has been heightened in recent years due to
a growing demand and interest in mobile applications for
improving quality of life in urban cities. Here is an example
where various data sources can be used: a traffic jam that
emerges due to an unplanned protest may be captured
through a Twitter stream, but missed when examining
weather conditions, event databases, reported roadwork, etc.
Additionally, weather sensors in the city tend to miss
localized events such as flooding. These views of the city
combined however, can provide a richer and more complete
view of the state of the city, by merging traditional data
sources with messy and unreliable social media streams
thereby contributing to smart living, environment, economy,
mobility, and governance. Such applications rely on Big
Data available to the public via the cloud.
As outlined in the latest McKinsey Global Institute
report, we’re now seeing the global economy beginning to
operate in real time [9]. The total value generation for the
impact of new data technologies will be measured in trillions
of dollars globally according to this report. The National

Academies press published Visionary Grand Challenges of
Manufacturing for 2020 that included as one of the
challenges – ability to instantaneously transform information
gathered from a vast array of sources into useful knowledge
for making effective decisions. Work has been done on data
abstraction and visualization tools for Big Data [10]; analysis
of Big Data using various algorithms such as influence-based
module mining (IBMM) algorithm, online association rule
mining, graph analytics, and provenance analysis support
framework all of which are applicable after the data
integration phase. Use of semantic technologies has known
to improve user interaction with the system, simplified data
integration and extensibility, improved search, and improved
discovery of knowledge [11].

Figure 1: Linked Open Urban Data portal with integrated data
from various sources and domains

Traditionally Data Integration has been defined as the
problem of combining data residing at different sources, and
providing the user with a unified view of these datasets [1314]. A data integration system exposes to its users a schema
for posing queries. This schema is typically referred to as a
mediated schema (or global schema). To answer queries
using the various information sources the system needs
mappings that describe the semantic relationships between
the mediated schema and the schemas of the sources. Two
basic approaches have been proposed for this purpose. The
first approach, called global-as-view, requires that the global
schema be expressed in terms of the data sources. The
second approach, called local-as-view, requires the global
schema to be specified independently from the sources, and
the relationships between the global schema and the sources
are established by defining every source as a view over the
global schema [14-15]. These existing approaches were
applicable to relational data models and an integrated global
schema was determined manually. The proposed framework
in this paper is for integration of linked data that is available
on the web.

MOTIVATING SCENARIO:
Consider the following scenario where a typical driver, John
gets into his car in the morning and turns on the ignition. A
built-in innovative application in the car greets him and asks
him if he is going to work based on the time of the day.
John responds by saying “yes” and the app replies that the
vehicle performance has been optimized for the trip. The
built-in system uses the GIS system, road grade data, and
speed limits data to create an optimal velocity profile. As
John starts driving and is approaching Recker road to turn
left, the app informs John about a road repair on Recker
road for a 1-mile stretch on his route up to 3pm that day.
The app suggests that John should continue driving and take
the next left on Power road. John follows the suggestion
and answers a text message that he receives from his
collaborator. As he is answering the text message, his car
drifts into the neighboring lane. The app immediately
notifies John of the drift who quickly adjusts his driving. As
John approaches his workplace he drives towards Lot 1
where he usually parks. The app informs John that there are
only 2 parking spots open in Lot 1. As John is already
running late for a meeting, he decides to directly drive to the
next parking lot, Lot 2, to avoid spending the time looking
for the 2 empty spots in Lot 1. As John enters Lot 2 and is
driving towards one of the empty spots, he gets too close to
one of the parked cars. The app immediately warns John of
a collision. John quickly adjusts his car away from the
parked cars and parks in an empty spot. The app logs
tracking data about John’s style of driving on the server for
future use.
In order to build apps for automobiles, access to a
number of data sets from various sources is required. Some
of this is real-time data that is continuously being updated.
Data related to traffic, road repairs, emergencies, accidents,
driving habits, maps, parking, fuel economy data, household
data, diagnosis data, etc. would be required. Various
automotive apps could be built that focus on reducing energy
consumption, reducing emissions, provide fuel economy
guidance that is based on actual vehicle data, road
conditions, traffic, and most importantly personal driving
habits and ways to improve them. It is important to
effectively integrate data such that the data is tied to a
meaningful and rich data model that can be queried by these
innovative applications.
ETL PROCESS:
The Extract-Transform-Load (ETL) process in
computing has been in use for integration of data from
multiple sources or applications, possibly from different
domains. It refers to a process in data warehousing that
extracts data from outside sources, transforms it to fit
operational needs, which can include quality checks, and
loads it into the end target database, more specifically,
operational data store, data mart, or data warehouse. The
three phases of this process are described as follows:
• Extract: this is the first phase of the process that involves
data extraction from appropriate data sources. Data is

usually available in flat file formats such as csv, xls, and
txt or is available through a RESTful client.
• Transform: this phase involves the cleansing of data to
comply with the target schema. Some of the typical
transformation activities involve normalizing data,
removing duplicates, checking for integrity constraint
violations, filtering data based on some regular
expressions, sorting and grouping data, applying built-in
functions where necessary, etc. [12].
• Load: this phase involves the propagation of the data into a
data mart or a data warehouse that serves Big Data.
One of the popular approaches to data integration has been
ETL as shown in, which describe the taxonomy of activities
in ETL and a framework using a workflow approach to
design ETL activities. A declarative database programming
language called LDL was used to define the semantics of
ETL activities. Similarly, there have been other approaches
such as UML and data mapping diagrams for representing
ETL activities, quality metrics driven design for ETL, and
scheduling of ETL activities. A number of tools facilitate
the ETL process, namely IBM Infosphere, Oracle
Warehouse Builder, Microsoft SQL Server Integration
Services, and Informatica Powercenter for Enterprise Data
Integration. Talend Open Studio, Pentaho Kettle,
CloverETL are open source ETL products. The focus in
these existing approaches has been on the design of ETL
workflow and not about generating meaningful/semantic
data that is important in integration of data from variety of
sources. Semantic approaches to ETL technologies have
been proposed that use semantic technologies to further
enhance definitions of the ETL activities involved in the
process rather than the data itself. A tool that allowed semiautomatic definition of inter-attribute semantic mappings,
by identifying parts of data source schemas, which are
related to the data warehouse schema has been proposed by
the research community. This supported the extraction
phase of ETL. The use of semantics here was to facilitate
the extraction process and workflow generation with
semantic mappings.
SEMANTIC ETL FRAMEWORK:
The proposed semantic ETL framework generates a
semantic model of the dataset(s) under integration, and then
generates semantic linked data in compliance with the data
model. This generated semantic data is made available on
the web as linked data available for querying, analytics, or
used in data-driven innovative apps. The use of semantic
technologies is introduced in the Transform phase of an
ETL process to create a semantic data model and generate
semantic linked data (RDF triples) to be stored in a data
mart or warehouse. The transform phase will still continue
to perform other activities such as normalizing and
cleansing of data. Extract and Load phases of the ETL
process would remain the same. Figure 2 shows the
overview of activities in semantic ETL. Transform phase
will involve a manual process of analyzing the datasets, the

schema and their purpose. Based on the findings, the
schema will have to be mapped to an existing domainspecific ontology or ontology will have to be created from
scratch. If the data sources belong to disparate domains,
multiple ontologies will be required and alignment rules will
have to be specified for any common or related data fields.

Figure 2: Semantic ETL Framework

TECHNOLOGY STACK:
Semantic web technologies facilitate: the organization of
knowledge into conceptual spaces, based on their meanings;
extraction of new knowledge via querying; and maintenance
of knowledge by checking for inconsistencies. These
technologies can therefore support the construction of an
advanced knowledge management system. The following
Semantic technologies and tools are used as part of our
Semantic ETL framework:
• Uniform Resource Identifier (URI), a string of characters
used to identify a name or a web resource. Such
identification enables interaction with representations of
the web resource over a network (typically the Web)
using specific protocols.
• Resource Description Framework (RDF), a general
method for data interchange on the Web, which allows
the sharing and mixing of structured and semistructured data across various applications. As the name
suggests, RDF is a language for describing web
resources. It is used for representing information,
especially metadata, about web resources. RDF is
designed to be machine-readable so that it can be used

•

•

•

in software applications for intelligent processing of
information.
Web Ontology Language (OWL) is a markup language
that is used for publishing and sharing ontologies. OWL
is built upon RDF and an ontology created in OWL is
actually a RDF graph. Individuals with common
characteristics can be grouped together to form a class.
OWL provides different types of class descriptions that
can be used to describe an OWL class. OWL also
provides two types of properties: object properties and
data properties. Object properties are used to link
individuals to other individuals while data properties are
used to link individuals to data values. OWL enables
users to define concepts in a way that allows them to be
mixed and matched with other concepts for various uses
and applications.
SPARQL – a RDF Query Language, which is designed
to query, retrieve, and manipulate data stored in RDF
format. SPARQL allows for a query to consist of triple
patterns, conjunctions, disjunctions, and optional
patterns. SPARQL allows users to write queries against
data that can loosely be called "key-value" data, as it
follows the RDF specification of the W3C. The entire
database is thus a set of "subject-predicate-object"
triples.
Protégé Ontology Editor - Protégé is an open-source
ontology editor and framework for building intelligent
systems. It allows users to create ontologies in W3C’s
Web Ontology Language. It is used in our framework to
provide semantic metadata to the schema of datasets
from various sources.

CASE STUDY:
A case study was conducted to assess the effectiveness of
the proposed Semantic ETL framework using public
datasets on Educational data from Massive Open Online
courses (MOOC), National Household travel survey data
and EPA’s Fuel Economy data.
MOOC Course Data:
Data from 3 MOOC providers Coursera, edX, and Udacity
was integrated using the proposed framework for this study.
Coursera is the largest MOOC provider in the world with
7.1 million users in 641 courses from 108 institutions as of
April 2014. These courses span 25 categories including 4
subcategories of computer science. Course data is
retrievable via Coursera’s RESTful course catalog API and
returned in JSON. edX is a premier MOOC provider with
more than 2.5 million users and over 200 courses as of June
2014. edX courses are distributed among 29 categories,
many of which overlap with Coursera’s. edX does not
provide an API for accessing their course catalog, however,
edX’s entire platform is currently open-source. Udacity is a
vocational course-centric MOOC provider with 1.6 million
users in 12 full courses and 26 free courseware as of April
2014. The majority of Udacity courses are within the field
of computer science. Udacity does not provide an API for
accessing their course catalog data.

Household Travel Data:
National Household Travel survey (NHTS) data, published
by the U.S. Department of Transportation, has been used in
this study. This data was collected to assist transportation
planners and policy makers who needed transportation
patterns in the United States. The dataset consists of daily
travel data of trips taken in a 24-hour period with
information on the purpose of the trip (work, grocery
shopping, school dropoff, etc.), means of transportation
used (bus, car, walk, etc.), how long the trip took, day of
week when it took place, and additional information in case
a private vehicle was used. This dataset has been used by
research community to study relationships between
demographics and travel, correlation of the modes of
transportation, amount, or purpose of travel with the time of
the day and day of the week. This dataset was integrated
with Fuel Economy data to discover the performance of
private vehicles used by members of the household.
Fuel Economy Data:
The U.S. Environmental Protection Agency’s (EPA) data on
Fuel Economy of vehicles provides detailed information
about vehicles produced by all manufacturers (make) and
models. It provided detailed vehicle description, mileage
data for both city drive and highway drive, mileage for
different fuel types, emissions information, and fuel prices.
EPA obtained this data as a result of vehicle testing
conducted by EPA’s National Vehicle and Fuel Emissions
Lab and the manufacturers.
Semantic Data Model generation
A common semantic data model was designed and
developed to integrate Household Travel and Fuel Economy
data using Protégé for ontology engineering. This model
comprised of primary classes Person, Household, Vehicle at
the top most level. All the fields in the datasets were
modeled as classes, subclasses, or properties and connected
to the primary classes via suitable relationships. The
ontology had both object properties and data properties
depending on the type of data field being represented. Figure
3 shows the high-level semantic data model with the primary
classes and properties and their relationships. The semantic
data model for integrating MOOCs data was created by
extending Schema.org that has generic creative work
described and organized as a hierarchy of types, each
associated with a set of properties. Learning Resource
Metadata Initiative (LRMI) specification adds properties to
the CreativeWork object of Schema.org including the time it
takes to work through a learning resource (timeRequired),
the typical age range of the content’s intended audience
(typicalAgeRange), etc. This existing vocabulary for
CreativeWork was used as a base type extended to add
Course, Session, Category and their associated properties
drawn from MOOC datasets. Figure 4 shows the high-level
semantic data model. The complete ontology can be obtained
from the project website.

Figure 3: Semantic data model for household travel and fuel economy data

Figure 4: Semantic Data Model for MOOC data set

Semantic Instance Data generation
Household travel and fuel economy datasets were obtained
from in CSV format that was converted into RDF triples
using Oxygen XML editor. A web crawler was written using
Scrapy Crawler framework to obtain MOOC datasets in
JSON format. A total of 30,000 triples were generated that
can be accessed from the project website.
Semantic Querying
The integrated datasets were queried using RDF Query
language SPARQL. The Java open source framework for
building Semantic Web and Linked data applications,
Apache Jena, was used for executing SPARQL queries on
travel and fuel economy data. An application was built that
loads the ontologies and data, checks for conformity of data
with the vocabulary, and then run SPARQL queries against
the data. A sample list of queries is provided in Table 1.
Linked data generated from MOOCs was stored into a
stand-alone Fuseki server (a sub-project of Jena) that
provides an HTTP interface to RDF data. A sample
SPARQL query is shown below:
PREFIX mooc: <http://sebk.me/MOOC.owl#>
PREFIX schema: http://schema.org/
SELECT * WHERE {
?course rdf:type mooc:Course.
?course schema:name ?iname.

FILTER (regex(?iname, "calculus", "i")).
}

Table 1: Sample Queries for integrated household travel and
fuel economy data

A MOOC aggregator web application called MOOCLink
was developed that queries this integrated data set to
provide consolidated information about courses being
offered, upcoming courses, allows users to search for
courses, and compare courses. This web application is
currently under testing. A screenshot of a course detail page
of the web application is shown in Figure 5. Further details
of this ongoing project can be updated from the project
website.

Figure 5: Screenshot of Course detail page of the MOOC aggregator web application
Evaluation
Current tools that facilitate the ETL process focus in these
existing approaches has been on the design of ETL
workflow and not about generating meaningful/semantic

data that is important in integration of data from variety of
sources. The Semantic ETL framework focuses on
providing semantics to various data fields thereby
facilitating richer data integration.

LESSONS LEARNED:
Integration of data from various heterogeneous sources into
a meaningful data model that allows intelligent querying
and creation of innovative applications is an important open
issue in the area of Big Data. The case study conducted
showed great potential for the proposed Semantic ETL
framework in producing Linked Open Data that would
facilitate the creation of data-driven innovative apps for
smart living. The Semantic Data Model generation process
used in the case study comprises of manual analysis of data,
ontology engineering, creation of linked data (RDF triples).
One of the challenges of this project is ontology engineering
that needs a fairly good understanding of the data from
different sources. A human expert has to perform this step
and it is often a time-consuming process that involves a
deep understanding of the data sets under integration. There
is a need for algorithms that automatically (or with minimal
human intervention) generate semantic data models. Based
on this requirement the following are some future research
directions:
• Establishing a process to identify existing ontologies for
data sets under integration
• Extending an existing ontology with relevant properties,
classes, and relationships or creation of a new ontology
• Generate alignment rules between multiple ontologies that
might exist for individual data sets
• Generate alignment rules between the Semantic data
model that is created and existing well-known ontologies
such as FOAF, DBPedia, DublinCore, and Wordnet to
produce semantically rich Linked Open Data.
Semantic ETL process can be introduced into existing open
source ETL software tools such as CloverETL - an Open
Source Engine that is a Java library and does not come with
any User Interface components. It provides powerful data
transformation and ETL features that are used in the
commercial Clover edition. CloverETL is a rapid, end-toend data integration solution popular for its usability and
intuitive controls, along with its lightweight footprint,
flexibility, and processing speed.
Future directions for this project will also involve the
creation of interesting and innovative applications (web
and/or mobile) in various domains such as automotive,
aerospace, healthcare, education to list a few.
REFERENCES:
[1] E. Kandogan, M. Roth, C. Kieliszewski, F. Ozcan, B.
Schloss, and M.-T. Schmidt, “Data for All: A Systems
Approach to Accelerate the Path from Data to Insight,”
in 2013 IEEE International Congress on Big Data
(BigData Congress), 2013, pp. 427–428.
[2] C. Bizer, P. Boncz, M. L. Brodie, and O. Erling, “The
Meaningful Use of Big Data: Four Perspectives – Four
Challenges,” SIGMOD Rec., vol. 40, no. 4, pp. 56–60,
Jan. 2012.
[3] A. Azzini and P. Ceravolo, “Consistent Process Mining
over Big Data Triple Stores,” in 2013 IEEE

International Congress on Big Data (BigData
Congress), 2013, pp. 54–61.
[4] P. Hayes and B. McBride, “Resource description
framework (RDF),” 2004. [Online]. Available:
http://www.w3.org/TR/rdf-mt/. [Accessed: 28-Feb2014].
[5] C. Bizer, T. Heath, and T. Berners-Lee, “Linked datathe story so far,” International journal on semantic web
and information systems, vol. 5, no. 3, pp. 1–22, 2009.
[6] L. Ding, T. Lebo, J. S. Erickson, D. DiFranzo, G. T.
Williams, X. Li, J. Michaelis, A. Graves, J. G. Zheng,
Z. Shangguan, and others, “TWC LOGD: A portal for
linked open government data ecosystems,” Web
Semantics: Science, Services and Agents on the World
Wide Web, vol. 9, no. 3, pp. 325–333, 2011.
[7] S. Dastgheib, A. Mesbah, and K. Kochut, “mOntage:
Building Domain Ontologies from Linked Open Data,”
in 2013 IEEE Seventh International Conference on
Semantic Computing (ICSC), 2013, pp. 70–77.
[8] F. Lecue, S. Kotoulas, and P. Mac Aonghusa,
“Capturing the Pulse of Cities: Opportunity and
Research Challenges for Robust Stream Data
Reasoning,” in Workshops at the Twenty-Sixth AAAI
Conference on Artificial Intelligence, 2012.
[9] “Big data: The next frontier for innovation,
competition, and productivity | McKinsey &
Company.” [Online]. Available:
http://www.mckinsey.com/insights/business_technolog
y/big_data_the_next_frontier_for_innovation.
[Accessed: 12-Jul-2014].
[10] S. K. Bista, S. Nepal, and C. Paris, “Data Abstraction
and Visualisation in Next Step: Experiences from a
Government Services Delivery Trial,” in 2013 IEEE
International Congress on Big Data (BigData
Congress), 2013, pp. 263–270.
[11] C. C. Wang, D. A. Hecht, P. C. Sheu, and J. J. Tsai,
“Semantic Computing and Drug Discovery,” 2013.
[12] P. Vassiliadis, A. Simitsis, and E. Baikousi, “A
Taxonomy of ETL Activities,” in Proceedings of the
ACM Twelfth International Workshop on Data
Warehousing and OLAP, New York, NY, USA, 2009,
pp. 25–32.
[13] A. Halevy, A. Rajaraman, and J. Ordille, “Data
integration: the teenage years,” in Proceedings of the
32nd international conference on Very large data bases,
2006, pp. 9–16.
[14] M. Lenzerini, “Data integration: A theoretical
perspective,” in Proceedings of the twenty-first ACM
SIGMOD-SIGACT-SIGART symposium on Principles of
database systems, 2002, pp. 233–246.
[15] A. Calì, D. Calvanese, G. De Giacomo, and M.
Lenzerini, “Data integration under integrity constraints,”
in Seminal Contributions to Information Systems
Engineering, Springer, 2013, pp. 335–352.

Srividya Bansal is an Assistant Professor in Ira A. Fulton
Schools of Engineering at Arizona State University Her
primary research focuses on semantics-based approaches
for Big Data Integration, Web service description, discovery
& composition, and tools for outcome-based instruction
design in STEM education. She is also interested in
Software Engineering Education research that focuses on
experimenting various delivery models in project-centric
courses. She received a PhD in Computer Science from the

University of Texas at Dallas. She is a member of the IEEE
Computer Society. Contact her at srividya.bansal@asu.edu.
Sebastian Kagemann is a student in the department of
Computer Science at Indiana University – Bloomington
where he is pursuing B.S. in Computer Science. His
academic interests include mobile development, distributed
computing and cognitive science. Contact him at
sakagema@umail.iu.edu.

ScrumTutor: A Web-based Interactive Tutorial For
Scrum Software Development
Sindhura Potineni, Srividya K Bansal, Ashish Amresh
Department of Engineering
Arizona State University – Poly Campus
Mesa, AZ 85212 USA
{sindhura.potineni, srividya.bansal, amresh}@asu.edu
Abstract— In a traditional software engineering class,
students are typically engaged in theoretical lectures
followed by homework assignments or a project. Use of
hands-on training and laboratory activities using realworld projects is more likely to produce students with a
higher level of achievement and more confidence in the
course material. If every topic or technique introduced in
the course has a corresponding hands-on activity that
demonstrates an application or use of the concept in the
industry, students better understand the need for the
technique and the learning environment is more
interactive, engaging, and interesting to students. This
paper presents a project called ScrumTutor that aims at
providing an engaging learning experience of Scrum
Software development process through a web-based
Interactive tutorial. It is designed and developed for
undergraduate students in introductory Software
Engineering courses. This software tool introduces its
users to modern software engineering methodology used
these days in the software industry known as Agile
Software Development that includes the Scrum framework
for managing software projects or products.
Keywords— Software Engineering Education, Agile
Software Development, Scrum, Web-based interactive
tutorial.
I.

INTRODUCTION

Software Engineering courses are perceived as dry and boring.
Traditional software engineering classes mostly consist of
theoretical lectures and that do not engage students actively
thereby resulting in students not learning key concepts [2, 5].
Therefore application of those key concepts in real-world
scenarios is important for student learning. One can gain more
knowledge and retain it only when (s)he is able to implement it
or use it in a project that they might work on. An interactive
tutorial designed to engage a user in active learning of software
engineering concepts is of value. The requirements of this
tutorial would be: i) introduce user to a course topic through
the tutorial; ii) provide information on all the basic concepts
and key terms of the topic; iii) allow users to practice what they
have learnt in a user-friendly and engaging manner.
The goal of this project is the design and implementation of
a web-based interactive game to teach Scrum framework for

c
978-1-4673-6217-7/13/$31.00 2013
IEEE

managing software development. Interaction with the user is
divided in 3 significant phases where the user plays three
different roles in each phase. For example, in the Scrum
process there are multiple roles such as a product owner,
scrum master, developer, knowledge manager, etc. The first
phase is ‘Observation’; here the user will observe the Agile
process in action. The second phase is ‘Data collection’; here
the user will use the techniques observed in phase I and
contribute towards data collection part of the implementation
of the project. Finally the third phase is the ‘Development’
phase. Here the user performs a developer role on the team.
The user picks one of the available tasks for development and
implements it. The aim is to successfully implement all the
assigned tasks. After going through all three phases the student
would gain hands-on experience in executing a Scrum process.
The interactive tutorial covers all aspects of an Agile Scrum
process, describes the basic unit of Scrum called sprint,
describes a sprint cycle and its duration, and demonstrates how
a project is implemented in sprints.
This paper presents the prototype implementation of the
ScrumTutor as an interactive tutorial with 2 phases. In phase I
the user plays the role of an observer. The sprint is one weeklong and the tutorial depicts activities from day one to day five.
The tutorial involves four roles and uses a specific software
project that involves developing a website to manage music,
music albums, artists, and events using a Content Management
System such as Drupal [13]. The Scrum roles simulated in the
tutorial are a Product owner, a Scrum Master, and two team
members. In later phases of the tutorial the user gets involved
as a developer on the team. In the first phase, the team starts a
weekly sprint by discussing the product that they are going to
develop at a sprint Planning meeting led by the product owner.
Each day of the sprint, the team has a daily standup meeting to
discuss the tasks each member worked on yesterday, what they
are going to work on today and if they have any impediments
in completing their tasks. At midpoint during the weekly sprint,
the team also has a Scrum review meeting where they discuss
all the sprint tasks in detail, their status, and if any team
member needs help on any task. In phase II, the user plays the
role of a data collector and participates as one of the team
member. The user is involved in the process and daily Scrum
meetings where (s)he provides their status update and also
collects data required to handle certain testing tasks for the
team. The player gets involved in the project thereby gaining
better understanding of the Scrum process.

1884

 7KH UHPDLQGHU RI WKLV SDSHU LV RUJDQL]HG DV IROORZV
6HFWLRQ  SURYLGHV EDFNJURXQG PDWHULDO RQ $JLOH SURFHVVHV
6HFWLRQ  SUHVHQWV UHODWHG ZRUN LQ 6RIWZDUH (QJLQHHULQJ
(GXFDWLRQ 6HFWLRQ  SUHVHQWV WKH 6\VWHP GHVLJQ RI
6FUXP7XWRUIROORZHGE\LPSOHPHQWDWLRQLQVHFWLRQ6HFWLRQ
SUHVHQWVDTXDOLWDWLYHHYDOXDWLRQRIWKHVRIWZDUHWRRODQGLWV
UHVXOWV7KHODVWVHFWLRQSUHVHQWVVXPPDU\DQGIXWXUHZRUN
,,

%$&.*5281'

6RIWZDUH 'HYHORSPHQW /LIHF\FOH 6'/& LV D FRQFHSWXDO
PRGHO XVHG LQ SURMHFW PDQDJHPHQW WKDW GHVFULEHV WKH VWDJHV
LQYROYHGLQDQLQIRUPDWLRQV\VWHPGHYHORSPHQWSURMHFWIURP
DQ LQLWLDO IHDVLELOLW\ VWXG\ WKURXJK PDLQWHQDQFH RI WKH
FRPSOHWHG DSSOLFDWLRQ >@ 7KHUH DUH PDQ\ GLIIHUHQW W\SHV RI
6'/& PRGHOV DQG LQGXVWU\ XVHV RQH WKDW EHVW VXLWV WKHLU
QHHGV 7KH EDVLF DFWLYLWLHV LQ DQ\ 6'/& DUH WKH VDPH (DFK
VRIWZDUHOLIHF\FOHPRGHOVSHFLILHVSKDVHVRIWKHOLIHF\FOHDQG
WKH RUGHU LQ ZKLFK WKH\ ZLOO EH H[HFXWHG ,QLWLDOO\ JDWKHUHG
UHTXLUHPHQWVDUHFRQYHUWHGWREDVLFGHVLJQ&RGHLVSURGXFHG
LQ WKH SURFHVV RI LPSOHPHQWDWLRQ WKDW LV HYROYHG IURP WKH
GHVLJQ'HOLYHUDEOHVIURPWKHLPSOHPHQWDWLRQSKDVHDUHWHVWHG
DJDLQVW WKH UHTXLUHPHQWV 6RPH RI WKH WUDGLWLRQDO 6'/&
PRGHOV DUH  :DWHUIDOO ± D OLQHDUVHTXHQWLDO OLIHF\FOH PRGHO
WKDW LV YHU\ VLPSOH DQG HDV\ WR XQGHUVWDQG DQG XVH  9
6KDSHG  DQ H[WHQVLRQ RI WKH ZDWHUIDOO PRGHO ZKHUH WHVWLQJ
VWDUWV HDUO\RQ LQ WKH SURFHVV  6SLUDO  PRUH IDYRUDEOH IRU
ODUJH H[SHQVLYH DQG FRPSOLFDWHG SURMHFWV GXH WR LW H[WUD
HPSKDVLV RQ SODQQLQJ ULVN DVVHVVPHQW DQG GHVLJQ RI
SURWRW\SHV DQG VLPXODWLRQV  $JLOH ± DQ LWHUDWLYH DQG
LQFUHPHQWDO GHYHORSPHQW PHWKRGRORJ\ ZKHUH UHTXLUHPHQWV
DQG VROXWLRQV HYROYH WKURXJK FROODERUDWLRQ EHWZHHQ FURVV
IXQFWLRQDODQGVHOIRUJDQL]LQJWHDPV>@

WKDWSURYLGHVLQIRUPDWLRQRQZKDWLVWREHSURGXFHGZKHQLW
LVSURGXFHGDQGKRZPXFKLVSURGXFHG
• 6FUXPLVDSRSXODUIUDPHZRUNGHULYHGIURPDJLOHGHYHORSPHQW
WKDWSURYLGHVDIOH[LEOHDQGKROLVWLFVWUDWHJ\ZKHUHWKHHQWLUH
WHDPZRUNVDVDXQLWWRZDUGVRQHFRPPRQJRDO

% 6FUXP
6FUXP LV DQ LWHUDWLYH DQG LQFUHPHQWDO SURFHVV ZKHUH WKH
SURGXFWWKDWLVEHLQJGHYHORSHGNQRZQDVWKHSURGXFWEDFNORJ
LV GHVFULEHG DQG GLVFXVVHG DPRQJ WKH WHDP PHPEHUV 7KH
SURGXFWEDFNORJLVGLYLGHGLQWRVPDOOHUWDVNVWKDWDUHDVVLJQHG
WRWKHWHDPPHPEHUV7KHVHWRIWDVNVWKDWWKHWHDPZRUNVRQ
GXULQJDVSULQWLVFDOOHGWKHVSULQWEDFNORJ$SK\VLFDOERDUG
FDOOHGWKHVFUXPERDUGLVXVHGWRNHHSWUDFNRIDOOWKHVHWDVNV
DQG DVVLJQPHQWV 7KH UROHV LQ WKH 6FUXP IUDPHZRUN DUH DV
IROORZV
• 3URGXFW 2ZQHU LV UHVSRQVLEOH IRU WKH ZKROH SURGXFW LGHD
PDQDJHVWKHUHWXUQRQLQYHVWPHQW52,IRUWKHHIIRUWE\ WKH
WHDP NHHSV WUDFN RI SULRULWL]DWLRQ RI WKH SURGXFW EDFNORJ 	
UHOHDVHSODQVDQGFRXOGDOVRDFWDVDWHDPPHPEHU
• 6FUXP PDVWHU SURPRWHV 6FUXP SURFHVV VXSSRUWV WR UHVROYH
DQ\ LPSHGLPHQWV PDNHV D WHDP VHOIRUJDQL]HG NHHSV WKH
VSULQW EDFNORJ YLVLEOH SURWHFWV WKH WHDP IURP H[WHUQDO
LQWHUIHUHQFH DQG GLVWXUEDQFHV WR JHW DORQJ ZLWK WKH IORZ RI
ZRUNDQGKDVQRDXWKRULW\RQWKHWHDP
• 7HDP PHPEHU LV FURVVIXQFWLRQDO ZKR SRVVHVV VNLOOV RI D
WHVWHU EXVLQHVV DQDO\VW DQG QRW MXVW D GHYHORSHU DQG VWURQJO\
FROODERUDWHVZLWKRWKHUWHDPPHPEHUV




)LJXUH$JLOHSURFHVV


$ $JLOH3URFHVV
7KH $JLOH 0DQLIHVWR IRFXVHV RQ WKH IROORZLQJ LQGLYLGXDOV
DQG LQWHUDFWLRQV ZRUNLQJ VRIWZDUH FXVWRPHU FROODERUDWLRQ
DQGUHVSRQGLQJWRFKDQJH)LJXUHGHSLFWVDQ$JLOHSURFHVV
6RPHRIWKHYDULDWLRQVRIDQ$JLOHSURFHVVDUHDVIROORZV
• $JLOH 8QLILHG 3URFHVV $83 LV D VLPSOH DQG HDV\ WR
XQGHUVWDQGSURFHVVWRGHYHORSEXVLQHVVDSSOLFDWLRQVRIWZDUH
,WLQFOXGHVDJLOHWHFKQLTXHVVXFKDVWHVWGULYHQGHYHORSPHQW
DJLOH PRGHOLQJ DJLOH FKDQJH PDQDJHPHQW DQG GDWDEDVH
UHIDFWRULQJIRUKLJKHUSURGXFWLYLW\
• .DQEDQLVDMXVWLQWLPHGHOLYHU\SURFHVVWKDWGRHVQRW
RYHUORDGWKHGHYHORSHUV,WXVHVYLVXDOSURFHVVPDQDJHPHQW


)LJXUH6SULQW&\FOH


& 6SULQW
7KHVFUXPSURFHVVFRQVLVWVRIDOOPHPEHUVFRQWULEXWLQJWRWKH
GHYHORSPHQW DQG LPSOHPHQWDWLRQ RI WKH SURGXFW LQ D VSHFLILF
SHULRG RI WLPH FDOOHG D VSULQW 6SULQW LV WKH EDVLF XQLW RI
GHYHORSPHQW LQ 6FUXP ,W LV UHVWULFWHG WR D VSHFLILF GXUDWLRQ
WKDWFDQODVWDQ\ZKHUHIURPDZHHNWRDPRQWK$VSULQWVWDUWV
ZLWKDSODQQLQJPHHWLQJZKHUHSURGXFWXQGHUGHYHORSPHQWLV
GLVFXVVHG DQG WKH VSULQW WDVNV DUH GHULYHG 7KH\ DUH GLYLGHG
DQG DVVLJQHG WR HDFK PHPEHU RI WKH WHDP $ GDLO\ VFUXP
PHHWLQJ DOVR NQRZQ DV D VWDQGXS PHHWLQJ LV KHOG HYHU\ GD\
ZKHUHDOOPHPEHUVRIWKHWHDPSURYLGHDVWDWXVXSGDWHRIWKHLU
WDVNVWKDW ZHUHSHUIRUPHGRQWKHSUHYLRXVGD\WKHWDVNV WKDW
WKH\DUHFXUUHQWO\ZRUNLQJRQDQGDOVRLQIRUPWKHWHDPRIDQ\
LPSHGLPHQWV WKDW WKH\ DUH IDFLQJ LQ FRPSOHWLQJ WKHLU WDVNV
)LJXUHVKRZVDSLFWRULDOUHSUHVHQWDWLRQRIDVSULQWF\FOH

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1885

III.

RELATED WORK

The use of simulation games for education has become quite
widespread in the last few years [2-4]. Most of the related
work have established and published results that students
found simulation games for education useful in learning and
helped students in being more engaged in class. Although
games have been used to teach various concepts in a number
of fields, there are very few games that teach concepts in
Software engineering. In this section we present related work
on simulation games for teaching software engineering and
other related literature that provided inspiration for the design
and implementation of ScrumTutor.
Researchers have worked on simulation games to
teach software engineering processes through an experimental
card game that highlights process issues that were not
sufficiently addressed in lectures and projects [5]. This game
uses physical cards to reinforce software engineering concepts
unlike ScrumTutor that is a web-based interactive tutorial.
SimSE [6] is another game-based software
engineering simulation environment that provides a number of
games to teach various software development lifecycle models
such as waterfall model, incremental model, rapid prototyping
model, rational unified process, and extreme programming
model. SimSE does not teach Agile software development that
is the most recent and popular model. ScrumTutor addresses
Agile processes and draws inspiration from SimSE that creates
a game scenario with an office background and team members
working in an office thereby simulating a real-world
environment. ScrumTutor has used this aspect in its design
and addresses the area of Scrum that has not been addressed in
SimSE.
Students were introduced to Software Engineering
Interview process through a decision-based computer game in
a study at Rowan University [7]. Students learnt about
interview process that is needed in requirements analysis
phase of software engineering life cycle. By playing the game
multiple times students realized that gathering facts before
design and implementation is important for the success of the
project. Based on this work, ScrumTutor was designed to
have multiple phases so that concepts can be reinforced to
students to make sure they attain a high level of
understanding.
Researchers have also used the concept of games for
teaching various management concepts such as Risk
management and knowledge management [8, 9]. They
assessed the effectiveness in meeting learning objectives and
their findings clearly demonstrated the advantage of a
simulation game.
Other related work includes evaluation of objectoriented design patterns in game development [10] and seniors
rehabilitation using video game technology [11]. These
projects are close to education through interactive tools and
games but are in other domains. We believe ScrumTutor will
be a useful tool to the Software Engineering Education
community and can be customized to teach several other Agile
Process concepts.

1886

IV.

SCRUM TUTOR – DESIGN

The long-term goal of the project is to design Scrum Tutor as
a simulation game with game features such as engaging user
interaction, time factors, adjustability of player skill levels, replayability, scoring, multi-player, and providing score
statistics. The first version of ScrumTutor was designed with
this end goal in mind.
A. What is being taught and how?
The initial version of ScrumTutor provides hands-on practical
experience to students in a software engineering class.
Students learn Scrum process by engaging with this tool as an
observer and assimilating the flow of process and management
of software development. The tutorial takes its user through a
weeklong sprint with a Scrum team working on various
development activities distributed from day 1 to day 5. On day
1 the user observes the product owner describing the product
and the product backlog to the software team. The user is
introduced to various concepts such as the Scrum board that
displays the sprint backlog. User watches scrum task
allocation happening during a sprint planning meeting,
discussion of tasks and allocation among team members at the
scrum board, and discussion on status of various activities
through a daily standup meeting in front of the Scrum board.
The tutorial has a pre-defined simulated time assigned to each
day. When the pre-defined time for a day lapses, the user then
progresses to day 2 and observes a daily scrum meeting held
to discuss the activities of the team members performed on
that day and on previous day, to get the updates from the team,
and also discuss any impediment’s that a team member might
be facing. This process is repeated for the remainder of the
days. The user observes these daily standup meetings
happening at a fixed time everyday, example at 9:00am in the
morning. During the remainder of the day the user observes
the team working on their tasks in their cubicles and
interacting with each other. On day 4, a sprint review meeting
is held to discuss the progress made during the current sprint
and the team assesses if there is a need to make any changes to
their plan in order to successfully complete their tasks and
deliver the artifacts. In this simulation, the sprint review
meeting happens at around mid-point in the sprint at a fixed
time, example 11:00am on day 4.
In order to encourage active learning and engagement, as
the user is progressing from one day to the next, a number of
quiz-type questions are presented to the user to test their
understanding of the concepts. The user is allowed to process
to the next day only after answering these questions correctly.
If not, the user is redirected back to the previous step to
observe the process and understand it.
ScrumTutor is designed to have multiple phases so that
the user can more actively engage in the process and gain
hands-on experience as they are learning and getting better at
Scrum. This version of the tool is designed to have 2 phases.
Phase 1 is the observation phase that was presented so far.
Phase 2 is the data collection phase where the user is taken
through a new sprint with a new set of tasks in the sprint
backlog and the involvement of the user progressively

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

increases. The user is involved as one of the team members
participating in the meetings, providing inputs, and status
updates. Tasks are allocated to the user that has to be
completed for a successful completion of the sprint.
B. What product is being developed using the Scrum
framework in ScrumTutor?
This software tool is designed such that any project/product
can be used as an example and plugged in to demonstrate the
Scrum framework, sprints, and various activities. In this
version of the tool, the product being developed is a website to
manage music, music albums, artists, events, and venues. This
product is built using a Content Management System called
Drupal. An existing Software requirements specification and
product backlog of this product was used to demonstrate the
process in Scrum Tutor.
#
1
2

Table 1: Sample formative & summative assessment questions
Question
Answer Choices
(correct answers in bold)
Is the sprint backlog created
during the planning meeting?
How often do the teams
integrate their work and rerun
the regression tests?

3

What happens during a scrum
review meeting?

4

What is the duration of a daily
scrum meeting?

5

Who describes the product
backlog to the team?

6

When is a sprint retrospective
meeting be held?

7

During a sprint planning
meeting, how many sprints are
discussed and planned for?

8

Who attends the sprint
planning meeting?

9

How often is the sprint
retrospective meeting
conducted?

• Yes
• No
• Only at the end of
each sprint
• Once per day
• Continuously as things change;
potentially many times per day
• Discussion of sprint
progress
• Check if any member of the
team needs help to complete
their tasks
• Remove tasks from
sprint backlog
• Discuss a team outing
• As long as necessary
• 1 hour
• 15 minutes
• Product owner
• Scrum Master
• Knowledge Manager
• Quality Manager
• At the end of each
sprint
• It is not needed
• Few minutes before the sprint
planning meeting starts
• Review/Retrospective meeting is
the the same
• 4 sprints
• Current sprint only
• All sprints in the
project
• The scrum
development team
• Outside stakeholders
• Manager of the team
• Scrum Master
• Product owner
• Every day
• Every sprint
• Every project

C. What technology is used for implementation?
The long-terms goals of ScrumTutor is to provide education

through a fun and engaging environment, provide the thrill of
playing a game, and provide most features of a good game.
With these end goals in mind, HTML and Javascript was
chosen for UI development, C#.NET was chosen as the
server-side technology and Visual Studio as the Integrated
Development Environment (IDE).
D. Scoring/tests
ScrumTutor has a scoring mechanism through intermediate
quizzes to motivate the user to do better, replay and revisit
concepts that were not well understood, and gain further
understanding of the process by playing subsequent phases of
the game. A user is allowed to progress to the next phase only
after they have received a certain minimum score. The
quizzes are provided throughout the system in two ways:
formative assessment presented to the user during a sprint
while the scrum activities are happening and summative
assessment presented to the user at the end of a sprint.
Successful completion of the summative assessment allows
the user to move on to the next phase (and next sprint of the
project). Table 1 shows sample formative and summative
assessment questions.
V.

SCRUM TUTOR – IMPLEMENTATION

ScrumTutor was developed using C#.NET and the ModelView-Controller (MVC) architecture. Visual Studio 2010 was
used as the Integrated Development Environment (IDE), C#
.NET as the middleware and SQL Server 2008 R2 as the
database management system. User interface was designed
using HTML, JavaScript, and JQuery [12]. Technologies used,
database design and user interface design are further described
in the following sections.
A. Technologies
C#.NET is an object oriented programming language
developed by Microsoft; it is mostly implemented in the IDE
called Visual Studio, a product of Microsoft. Visual Studio
2010 was used as an IDE to write the functional code in C# to
access the database and the user interface.
SQL Server 2008 is a ‘Database Management System’
developed to allow definition, creation, querying, updating
and administrating an organized collection of data. SQL
Server Express 2008 R2 is the version used in this application
to maintain the assessment data, user scores, and business
logic data required for the simulation.
JavaScript is a client-side scripting language used to provide
interactions with the user. Client-side scripts can be written to
control, communicate and alter the content that needs to be
displayed.
JQuery, an open source software, is ‘a fast and concise
JavaScript library that simplifies HTML document traversals,
event handling, animations, and Ajax interactions for rapid
Web application development [12].
B. Database Design
The database of ScrumTutor consists of several tables to save

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1887

application data, user profile information, user scores, and
statistics. The detailed database design is shown in Figure 3.
Some of the important tables are listed and briefly described
below:
• Registration – This table is used to store user login
information. When a student or faculty registers for the first
time their data is saved in this table.
• Roledetails – This table tracks roles of the scrum team.
• FormativeQuestions – This table is used to store all the
formative assessment questions that are presented to the
user at the end of every day in a sprint with the answer
choices and the correct answer.
• SummativeQuestions – This table is used to store all the
summative assessment questions that are presented to the
user at the end of a sprint with the answer choices and
correct answer.
• Task – This table has detailed description of all the tasks
involved in a sprint.
• TeamMember – This table maintains the scrum team
member information and description.
• SprintStatus – This table stores data about a specific sprint,
status information and task assignments from all the
meetings.

VI.

SCRUM TUTOR – EVALUATION

ScrumTutor has been tested on multiple browsers like Mozilla
Firefox 19.0.2, Internet Explorer 10 and Google Chrome 26.0.
The functionality of the application was tested thoroughly to
ensure that an undergraduate student could go through the
complete tutorial without any issues. An evaluation study was
conducted at Arizona State University (ASU) to assess the
usefulness of the tool. Students from the Masters in Computer
Science Engineering program at ASU were surveyed prior to
their usage of ScrumTutor to assess their understanding of
Scrum. They were then allowed to use ScrumTutor and go
through phases 1 and 2 of the tutorial. A post-survey of these
same students was conducted after they used the tool and the
students were asked the same questions to assess improvement
in learning. Table 2 shows some of the questions on the preand post-survey. Unanimously, all of them felt that the tool
was useful, easy to follow and gave them a better
understanding of the Scrum process and the basic workflow
unit in Scrum called sprint. Many of them felt that going
through this tutorial provided them an industry perspective
and context of how this Software Engineering concept in used
in the industry on a project while working in teams. Some of
them said that they better understood the team dynamics and
various roles than earlier. Students also commented that they
felt this was an innovative and engaging pedagogical practice
for teaching software engineering processes.
#
1
2
3
4

Table 2: Pre- and Post-Survey Questions about ScrumTutor tool
Survey Question
What do you know about agile software development?
Describe Scrum framework?
Where do you think these concept can be used and applied?
Did ScrumTutor help in improving your knowledge of Scrum (Post)

VII. CONCLUSIONS AND FUTURE WORK

Figure 3: Database Design

C. User Interface Design
Figure 4 presents screenshots of the login, registration, and
home page of ScrumTutor tool. Figure 4 also shows
screenshots that introduce Scrum framework and values of
Scrum. It also shows screens that introduce the Scrum team,
Scrum board and terminology at the beginning of the tutorial.
Figure 5 shows screenshots of a sprint cycle going from day 15 in a week. Day 1 includes a discussion of the product, a
sprint planning, followed by assignment of tasks on the Scrum
board. Day 2 - 5 involve daily Scrum meetings that happen at
same time and place in the morning followed by the team
interacting and working on their tasks. In addition to the daily
Scrum meeting, on day 4 a scrum review meeting is held. Day
5 ends with a sprint retrospective meeting.

1888

ScrumTutor provides hands-on experience on Scrum
Framework to undergraduate students The interactive tutorial
has multiple learning phases, i.e., Observation and Data
Collection in this version and Development in the future
versions. An evaluation survey of students about this tool
provides preliminary results that using ScrumTutor enhanced
their knowledge of Scrum, provided hands-on training of
concepts learnt and provided a contextualized reinforcement
of Agile process via the music software product that was
managed and developed using ScrumTutor. Based on these
results we are confident that a full version of ScrumTutor that
is a game-based tool with multiple levels of learning through
phases will be a useful addition to the software engineering
classes. Future work on this project includes the
implementation of phase III where the user plays the role of a
‘team member’. The user gets to be part of the software team
and contributes as a developer. User picks a task and
implements it within the given timeframe of the sprint.
Furthermore, sprints can be customized based on the project
that is being implemented and managed. If the project is large,
a sprint can be anywhere from two to four weeks. A Mobile
app version of this game/tutorial for various platforms such as
Android, iOS, and Windows will be a great addition and is

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

Figure 4: User Interface Screenshots showing terminology and Scrum Process

Figure 5: User Interface screenshots showing sprint cycle in Phase 1

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1889

likely to be adopted by a larger audience. A pilot study will be
conducted in introductory Software Engineering classes at
sophomore level at ASU. Data will be collected on the
usability of the tool, student learning outcomes, student
reflection of the tool and faculty reflection of the tool. Phase I
and II of the tool can be further improvised with game-like
features by adding more user interaction. HTML5 may be
used to provide a rich user interface.

[5]

[6]

[7]

[8]

REFERENCES
[1]

[2]

[3]

[4]

I. Sommerville, “Software Engineering” (9th ed.), Chapter 2: Software
processes: Software process models and Chapter 3: Agile Software
Development, Boston, MA, 2011.
J. Pieper, “Learning Software Engineering Processes through Playing
Games,” IEEE International Workshop on Games and Software
Engineering (GAS) at Intl. Conf. on Software Engineering, June 2012.
N. Tillman, et al, “Pex4Fun: Teaching and Learning Computer Science
via Social Gaming”, IEEE Conf. on Software Engineering Education
and Training (CSEET) at Intl. Conf. on Software Engg., June 2012.
D. Ismailovic, et al, “Adaptive Serious Game Development”, IEEE
International Workshop on Games and Software Engineering (GAS) at
International Conference on Software Engineering, June 2012.

1890

[9]

[10]

[11]

[12]
[13]

A. Baker, E. O. Navarro, A. Hoek, “An experimental card game for
teaching software engineering processes”, Journal of Systems and
Software, IEEE, pp. 3–16, 2005.
E. O. Navarro, “SimSE: A software engineering simulation environment
for software process education”, Doctoral Dissertation, School of
Information & Computer Sciences, Univ. of California, Irvine, 2006.
A. Rusu, R. Russell, R. Cocco, “Simulating the Software Engineering
Interview Process using a Decision-based Serious Computer Game”,
IEEE International Conference on Computer Games (CGAMES), 2011.
A. Chua, “The Design and Implementation of a Simulation Game for
Teaching Knowledge Management”, Journal of the American Society
for Information Science and Technology , pp. 1207–1216, 2005.
G. Taran, “Using Games in Software Engineering Education to Teach
Risk Management”, IEEE Conf. on Software Engineering Education &
Training (CSEET), pp. 211-220, 2007.
A. Ampatzoglou, A. Chatzigeorgiou, “Evaluation of object-oriented
design patterns in game development”, Elsevier Informaiton and
Software Technology, Vol. 49, pp. 445-454, 2007.
D. Maggiorini, L. A. Ripamonti, E. Zanon, “Supporting Seniors
Rehabilitation through Videogame Technology”, IEEE Workshop on
Games & Software Engg. (GAS) at Intl. Conf. on Soft. Engg, June 2012.
“jQuery”[Online]. Available: http://jquery.com/ [Accessed: June-2013]
“Drupal”[Online]. Available:http://drupal.org/ [Accessed: June-2013]

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/4359251

Towards	a	General	Framework	for	Web	Service
Composition
Conference	Paper	·	August	2008
DOI:	10.1109/SCC.2008.134	·	Source:	IEEE	Xplore

CITATIONS

READS

2

27

4	authors,	including:
Ajay	Bansal

Gopal	Gupta

Arizona	State	University

University	of	Texas	at	Dallas

42	PUBLICATIONS			498	CITATIONS			

217	PUBLICATIONS			2,119	CITATIONS			

SEE	PROFILE

SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Ajay	Bansal	on	04	June	2014.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Towards a General Framework for Web Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

Abstract

S2
S5
CI’,I’

Service-oriented computing (SOC) has emerged as
the eminent market environment for sharing and reusing
service-centric capabilities. The underpinning for an organization’s use of SOC techniques is the ability to discover
and compose Web services. In this paper we present a generalized semantics-based technique for automatic service
composition that combines the rigor of process-oriented
composition with the descriptiveness of semantics. Our
generalized approach extends the common practice of linearly linked services by introducing the use of a conditional
directed acyclic graph (DAG) where complex interactions,
containing control flow, information flow and pre/post conditions, are effectively represented.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered,
and consumed. A composite service is a collection of services combined together in some way to achieve a desired
effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [2]. Most efforts
reported in the literature focus on one or more of these four
phases. The first phase involves generating a plan, i.e., all
the services and the order in which they are to be composed
in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The
second phase involves discovering services as per the plan.
Depending on the approach, often planning and discovery
are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting
the optimal solution from the available potential solutions
based on non-functional properties like QoS properties. The
last phase involves executing the services as per the plan and
in case any of them are not available, an alternate solution
has to be used.
In this paper we formalize the generalized composition
problem based on our conditional directed acyclic graph

CO’,O’

S3
S1

S4

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

representation. We present our approach that generates
most general compositions based on (conditional) directed
acyclic graphs (DAG). In our framework, the DAG representation of the composite service is reified as an OWL-S
description. This description document can be registered in
a repository and is thus available for future searches. The
composite service can now be discovered as a direct match
instead of having to look through the entire repository and
build the composition solution again.

2. Web service Composition
In this section we formalize the generalized composition
problem. In this generalization, we extend our previous
notion of composition [1] to handle non-sequential conditional composition (which we believe is the most general
case of composition). Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service
is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be
composed to obtain the desired service. Figure 1 shows an
example composite service made up of five services S1 to
S5 . In the figure, I 0 and CI 0 are the query input parameters
and pre-conditions respectively. O0 and CO0 are the query
output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and Sj indicates
that outputs of Si constitute (some of) the inputs of Sj .
Definition (Repository of Services): Repository (R) is a
set of Web services.

Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions,
I is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the list of
post-conditions. The pre- and post-conditions are ground
logical predicates.
Definition (Query): The query service is defined as Q
= (CI 0 , I 0 , A0 , AO0 , O0 , CO0 ) where CI 0 is the list of preconditions, I 0 is the input list, A0 is the service affect, AO0
is the affected object, O0 is the output list, and CO0 is the
list of post-conditions. These are all the parameters of the
requested service.
Definition (Generalized Composition): The generalized
Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI 0 , I 0 , A0 , AO0 , O0 , CO0 ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming
edge of a node represents the inputs and pre-conditions of
the service. The following conditions should hold on the
nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming edge
that S
represents the query inputs and pre-conditions,
I 0 w i I i , CI 0 ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing edge
that represents
the query outputs and post-conditions,
S
O0 v i Oi , CO0 ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has at
least one incoming edge, let Si1 , Si2 , ..., Sim be the
nodes such that there is a directed
S edge from each
of these nodes to Si . Then Ii v k Oik ∪ I 0 , CI i ⇐
(COi1 ∧COi2 ... ∧ COim ∧ CI 0 ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming
edge, let Sj be its immediate predecessor node such
that there is a directed edge from Sj to Si . Then the
inputs and pre-conditions at node Si are Ii = Oj ∪ I 0 ,
CI i = COj . The outgoing edges from Si represent
the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time.
The meaning of the v is the subsumption (subsumes) relation and ⇒ is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composi-

View publication stats

tion can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services
in the next stage. When it cannot be determined at compile
time whether the post-conditions imply the pre-conditions
or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible
conditions which will be evaluated at run-time. Depending
on the condition that holds, the corresponding services are
executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply
the preconditions CI 2 of S2 . The following conditions are
evaluated at run-time:
if (CO1 ⇒ CI 2 ) then execute S1 ;
else if (CO1 ⇒ ¬ CI 2 ) then no-op;
else if (CI 2 ) then execute S1 ;

3. Automatic Generation of Composite Services
In order to produce the composite service which is the
graph, we filter out services that are not useful for the composition at multiple stages. The composition routine starts
with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. For the next stage, the inputs available are
the query input parameters and all the outputs produced by
the previous stage, i.e., I2 = O1 ∪ I. I2 is used to find services at the next stage, i.e., all those services that require
a subset of I2 . In order to make sure we do not end up in
cycles, we get only those services which require at least one
parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.

4. Conclusions and Future Work
To make Web services more practical we need a general framework for composition of Web services. The generalized approach presented in this paper can handle nonsequential conditional composition that can be used in automatic workflow generation in a number of applications.

References
[1] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007.
[2] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

Automatic Composition of Semantic Web Services
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
The University of Texas at Dallas
Richardson, TX 75083

Abstract
Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services
automatically. For this automation to be effective, formal
semantic descriptions of Web services should be available.
In this paper we formally define the Web service discovery
and composition problem and present an approach for automatic service discovery and composition based on semantic
description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This
engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are
its scalability, i.e., its ability to handle very large service
repositories, and its extremely efficient processing times for
discovery and composition queries. We evaluate our engine
for automated discovery and composition on repositories of
different sizes and present the results.

1

Introduction

A Web service is a program accessible over the web that
may effect some action or change in the world (i.e., causes
a side-effect). Examples of such side-effects include a webbase being updated because of a plane reservation made
over the Internet, a device being controlled, etc. An important future milestone in the Web’s evolution is making services ubiquitously available. As automation increases, these
Web services will be accessed directly by the applications
rather than by humans [8]. In this context, a Web service
can be regarded as a “programmatic interface” that makes
application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize
and compose services automatically is needed in order to
make Web services more practical.
To make services ubiquitously available we need a

Thomas D. Hite
Metallect Corp.
2400 Dallas Parkway
Plano, TX 75093

semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis [3]. Informally, a service is characterized by its input
parameters, the outputs it produces, and the side-effect(s)
it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced
may have to satisfy certain post-conditions. For discovery,
composition, etc., one could take the syntactic approach in
which the services being sought in response to a query simply have their inputs syntactically match those of the query,
or, alternatively, one could take the semantic approach in
which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the
matching process. Several efforts are underway to build an
infrastructure [17, 23, 15] for service discovery, composition, etc. These efforts include approaches based on the
semantic web (such as USDL [1], OWL-S [4], WSML [5],
WSDL-S [6]) as well as those based on XML, such as Web
Services Description Language (WSDL [7]). Approaches
such as WSDL are purely syntactic in nature, that is, they
only address the syntactical aspects of a Web service [14].
Given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that
need can be automatically determined. This task is called
discovery. If the service is not found, the directory can be
searched for two or more services that can be composed to
synthesize the required service. This task is called composition. In this paper we present an approach for automatic
discovery and composition of Web services using their semantic descriptions.
Our research makes the following novel contributions:
(i) We formally define the discovery and composition problems; to the best of our knowledge, the formal description of
the generalized composition problem has been given for the
first time; (ii) We present efficient and scalable algorithms
for solving the discovery and composition problem that take
semantics of services into account; our algorithm automatically selects the individual services involved in composition
for a given query, without the need for manual intervention;

Q

and, (iii) we present a prototype implementation based on
constraint logic programming that works efficiently on large
repositories.
The rest of the paper is organized as follows. Section
2 describes the two major Web services tasks, namely, discovery and composition with their formal definitions. In
section 3 and 4, we present our multi-step narrowing solution and implementation for automatic service discovery
and composition. Finally we present our performance results, related work and conclusions.

2

Automated Web service Discovery and
Composition

Discovery and Composition are two important tasks related to Web services. In this section we formally describe
these tasks. We also develop the requirements of an ideal
Discovery/Composition engine.

2.1

The Discovery Problem

Given a repository of Web services, and a query requesting a service (we refer to it as the query service in
the rest of the text), automatically finding a service from
the repository that matches the query requirements is the
Web service Discovery problem. Valid solutions to the
query satisfy the following conditions: (i) they produce
at least the query output parameters and satisfy the query
post-conditions; (ii) they use only from the provided input
parameters and satisfy the query pre-conditions; (iii) they
produce the query side-effects. Some of the solutions may
be over-qualified, but they are still considered valid as
long as they fulfill input and output parameters, pre/post
conditions, and side-effects requirements.
Example 1: Say we are looking for a service to buy a book
and the directory of services contains services S1 and S2 .
The table 1 shows the input/output parameters of the query
and services S1 and S2.
In this example service S2 satisfies the query, but
S1 does not as it requires BookISBN as an input but
that is not provided by the query. Our query requires
ConfirmationNumber as the output and S2 produces ConfirmationNumber and TrackingNumber. The extra output
produced can be ignored. Also the semantic descriptions
of the service input/output parameters should be the same
as the query parameters or have the subsumption relation.
The discovery engine should be able to infer that the query
parameter BookTitle and input parameter BookName of
service S2 are semantically the same concepts. This can be
inferred using semantics from the ontology provided. The
query also has a pre-condition that the CreditCardNumber
is numeric which should logically imply the pre-condition
of the discovered service.

CI’,I’

CO,O

CI,I

S

where CI’ ==> CI,
I’
I,

CO’,O’

CO ==> CO’,
O
O’

Figure 1. Substitutable Service
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI ; I ; A; AO; O; CO) is the representation of a service where CI is the pre-conditions, I
is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the postconditions.
Definition (Repository of Services): Repository is a set of
Web services.
Definition (Query): The query service is defined as
Q = (CI 0; I 0; A0; AO0 ; O0; CO0 ) where CI 0 is the preconditions, I 0 is the input list, A0 is the service affect,
AO0 is the affected object, O0 is the output list, and CO0
is the post-conditions. These are all the parameters of the
requested service.
Definition (Discovery): Given a repository R and a query
Q, the Discovery problem can be defined as automatically
finding a set S of services from R such that S = fs j s =
(CI ; I ; A; AO ; O ; CO ), s 2 R, CI 0 ) CI , I v I 0 , A =
A0 , AO = AO0 , CO ) CO0 , O w O0 g. The meaning
of v is the subsumption (subsumes) relation and ) is the
implication relation. For example, say x and y are input
and output parameters respectively of a service. If a query
has (x > 5) as a pre-condition and (y > x) as postcondition, then a service with pre-condition (x > 0) and
post-condition (y > x) can satisfy the query as (x > 5) )
(x > 0) and (y > x) ) (y > x) since (x > 0).
In other words, the discovery problem involves finding
suitable services from the repository that match the query
requirements. Valid solutions have to produce at least those
output parameters specified in the query, satisfy the query
pre and post-conditions, use at most those input parameters that are provided by the query, and produce the same
side-effect as the query requirement. Figure 1 explains the
discovery problem pictorially.

2.2

The Composition Problem

Given a repository of service descriptions, and a query
with the requirements of the requested service, in case a
matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service.
Figure 2 shows an example composite service made up of

Service
Query

S1
S2

Input Parameters
BookTitle,CreditCardNumber,
AuthorName,CreditCardType
BookName,AuthorName
BookISBN,CreditCardNumber
BookName,
CreditCardNumber

Pre-conditions
IsNumeric(Credit
CardNumber)

Output Parameters
Post-Cond
ConfirmationNumber
ConfirmationNumber

IsNumeric(Credit
CardNumber)

ConfirmationNumber,
TrackingNumber

Table 1. Example Scenario for Discovery problem

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Figure 2. Example of a Composite Service as
a Directed Acyclic Graph

GetAvailability
BookISBN
BookName,
AuthorName
GetISBN

NumAvailable

BookISBN

ConfNumber
PurchaseBook

AuthCode
CreditCardNum

AuthorizeCreditCard

Figure 3. Example of a Composite Service
five services S1 to S5 . In the figure, I 0 and C I 0 are the query
input parameters and pre-conditions respectively. O0 and
0
C O are the query output parameters and post-conditions
respectively. Informally, the directed arc between nodes Si
and Sj indicates that outputs of Si constitute (some of) the
inputs of Sj .
Example 2: Suppose we are looking for a service to
buy a book and the directory of services contains services
GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. The table 2 shows the input/output parameters
of the query and these services.
Suppose the repository does not find a single service that
matches these criteria, then it synthesizes a composite service from among the set of services available in the repository. Figure 3 shows this composite service. The postconditions of the service GetAvailability should logically
imply the pre-conditions of service PurchaseBook.
Definition (Composition): The Composition problem can
be defined as automatically finding a directed acyclic graph
G = (V ; E ) of services from repository R, given query Q =
(CI 0; I 0; A0; AO0 ; O0; CO0 ), where V is the set of vertices
and E is the set of edges of the graph. Each vertex in the

graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of
a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:
1. 8i Si 2 V where Si has zero incoming edges,
I 0 w i I i , CI 0 )^i CI i .
2. 8i Si 2 V where Si has zero outgoing edges,
O0 v i Oi , CO0 (^iCO i .
3. 8i Si 2 V where Si has at least one incoming edge,
let Si1, Si2 , ..., Sim be the nodes such that there is
a directed edge from each of these nodes to Si . Then
0
0
Ii v k Oik [ I , C I i ( (CO i1 ^CO i2 ::: ^COim ^ C I ).
The meaning of the v is the subsumption (subsumes) relation and ) is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services
in the next stage.
Figure 4 explains one instance of the composition problem pictorially. When the number of nodes in the graph
is equal to one, the composition problem reduces to the
discovery problem. When all nodes in the graph have not
more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition
problem (i.e., the graph is a linear chain of services).

S
S

S

2.3

Requirements of an ideal Engine

Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition
where the number of services involved in composition is
exactly equal to one. The features of an ideal Discovery/Composition engine are:
Correctness: One of the most important requirements for
an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the

Service

Input Parameters

Pre-Conditions

Query

Output
Params
ConfNumber

PurchaseBook

BookISBN
NumAvailable NumAvailable > 0
AuthCode
AuthCode > 99^
AuthCode < 1000
ConfNumber

BookTitle,CreditCardNum,
AuthorName,CardType
GetISBN
BookName,AuthorName
GetAvailability
BookISBN
AuthorizeCreditCard CreditCardNum
BookISBN, NumAvailable, AuthCode

NumAvailable > 0

Post-Conditions

Table 2. Example Scenario for Composition problem
semantics-based Discovery and Composition engine described in the following sections.

2.4

Figure 4. Composite Service
requirements of the query. Also, the engine should be able
to find all services that satisfy the query requirements.
Small Query Execution Time: Querying a repository of
services for a requested service should take a reasonable
amount of (small) time, i.e., a few milliseconds. Here we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental Updates: Adding or updating a service to an
existing repository of services should take a small amount
of time. A good Discovery/Composition engine should not
pre-process the entire repository again, rather incrementally
update the pre-processed data (indexes, etc.) of the repository for this new service added.
Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition
engine should be able to give results based on requirements
(minimize, maximize, etc.) over the costs. We can extend
this to services having an attribute vector associated with
them and the engine should be able to give results based
on maximizing or minimizing functions over this attribute
vector.
These requirements have driven the design of our

Semantic Description of Web Services

A Web service is a software system designed to support
interoperable machine-to-machine interaction over a network. It has an interface that is described in a machineprocessible format so that other systems can interact with
the Web service through its interface using messages. The
automation of Web service tasks (discovery, composition,
etc.) can take place effectively only if formal semantic descriptions of Web services are available. Currently, there
are a number of approaches for describing the semantics of
Web services such as OWL-S [4], WSML [5], WSDL-S [6],
and USDL [1].

3

A Multi-step Narrowing Solution

With the formal definition of the Discovery and Composition problem, presented in the previous section, one can
see that there can be many approaches to solving the problem. Our approach is based on a multi-step narrowing of the
list of candidate services using various constraints at each
step. In this section we discuss our Composition algorithm
in detail. As mentioned earlier, discovery is a simple case of
Composition. When the number of services involved in the
composition is exactly equal to one, the problem reduces
to a discovery problem. Hence we use the same engine for
both discovery and composition. We assume that a directory of services has already been compiled, and that this
directory includes semantic descriptions for each service.

3.1

The Service Composition Algorithm

For service composition, the first step is finding the set
of composable services. Using the discovery engine, individual services that make up the composed service can be
selected. Part substitution techniques [2] can be used to find
the different parts of a whole task and the selected services
can be composed into one by applying the correct sequence
of their execution. The correct sequence of execution can be
determined by the pre-conditions and post-conditions of the

individual services. That is, if a subservice S1 is composed
with subservice S2 , then the post-conditions of S1 must imply the pre-conditions of S2. The goal is to derive a single
solution, which is a directed acyclic graph of services that
can be composed together to produce the requested service
in the query. Figure 6 shows a pictorial representation of
our composition engine.
In order to produce the composite service which is the
graph, as shown in the example figure 2, we filter out services that are not useful for the composition at multiple
stages. Figure 5 shows the filtering technique for the particular instance shown in figure 2. The composition routine starts with the query input parameters. It finds all those
services from the repository which require a subset of the
query input parameters. In figure 5, C I ; I are the preconditions and the input parameters provided by the query.
S1 and S2 are the services found after step 1. O1 is the
union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous stage, i.e., I2 = O1 [ I . I2 is used to find services at
the next stage, i.e., all those services that require a subset
of I2. In order to make sure we do not end up in cycles,
we get only those services which require at least one parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.

I=I
CI, I

1

S1
S2
.
.

O1

I=IUO
2

1

1

S
.
.

O

2

3

I=IUO
3

2

2

O
S

3

I=IUO
4

3

4

.
.

3

S

O

4

O

5

.
.

Figure 5. Composite Service
Algorithm: Composition
Input: QI - QueryInputs, QO - QueryOutputs, QCI - PreCond, QCO - Post-Cond
Output: Result - ListOfServices
1. L NarrowServiceList(QI, QCI);
2. O GetAllOutputParameters(L);
3. CO GetAllPostConditions(L);
4. While Not (O w QO)
5.
I = QI [ O; CI QCI ^ CO;
6.
L’ NarrowServiceList(I, CI);
7. End While;
8. Result
RemoveRedundantServices(QO, QCO);
9.Return Result;

S1
Query
described
using
USDL
(S)

Infer
Sub-queries

.
.
.
Sn

Discovery Module
(Discovery Engine + Service
Directory + Term Generator)
S1

Composed Service

Pre-Cond(S)
S1
Pre-Cond( S)1

Post-Cond( S)1
Pre-Cond( S)2

S2

..........................

Sn

Composition Engine
(implemented using
Constraint Logic
Programming

................................. S n

Post-Cond( S)n
Post-Cond(S)

Figure 6. Composition Engine

4

Implementation

Our discovery and composition engine is implemented
using Prolog [11] with Constraint Logic Programming over
finite domain [10], referred to as CLP(FD) hereafter. In
our current implementation, we used semantic descriptions
written in the language called USDL [1]. The repository of
services contains one USDL description document for each
service. USDL itself is used to specify the requirements of
the service that an application developer is seeking.
USDL is a language that service developers can use to
specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology
that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. USDL
uses WordNet [9] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the “meaning” of
input parameters, outputs, and the side-effect induced by
the service is given by mapping these syntactic terms to
concepts in WordNet (see [1] for details of the representation). Inclusion of USDL descriptions, thus makes services
directly “semantically” searchable. However, we still need
a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as
such a query language. A USDL description of the desired
service can be written, a query processor can then search
the service directory for a “matching” service. Due to lack
of space, we do not go into the details of the language in
this paper. They are available in our previous work [2].
These algorithms can be used with any other Semantic
Web service description language as well. It will involve
extending our implementation to work for other description
formats, and we are looking into that as part of our future
work.
The software system is made up of the following components.
Triple Generator: The triple generator module converts

each service description into a triple. In this case, USDL
descriptions are converted to triples like:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the
side-effect. I is the list of inputs and O is the list of outputs.
Pre-Conditions are the conditions on the input parameters
and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can
be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic,
specific, part and whole substitutions [2]. In case conditions on a service are not provided, the Pre-Conditions and
Post-Conditions in the triple will be null. Similarly if the
affect-type is not available, this module assigns a generic
affect to the service.
Query Reader: This module reads the query file and passes
it on to the Triple Generator. We use USDL itself as the
query language. A USDL description of the desired service
can be written, which is read by the query reader and converted to a triple. This module can be easily extended to
read descriptions written in other languages.
Semantic Relations Generator: We obtain the semantic
relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms,
meronyms, holonyms and many more. USDL descriptions
point to OWL WordNet for the meanings of concepts. A
theory of service substitution is described in detail in [2]
which uses the semantic relations between basic concepts of
WordNet, to derive the semantic relations between services.
This module extracts all the semantic relations and creates
a list of Prolog facts. We can also use any other domainspecific ontology to obtain semantic relations of concepts.
We are currently looking into making the parser in this module more generic to handle any other ontology written in
OWL.
The query is parsed and converted into a Prolog query that
looks as follows:
discovery(sol(queryService, ListOfSolutionServices).
The engine will try to find a list of SolutionServices that
match the queryService.
Composition Engine: The composition engine is written
using Prolog with CLP(FD) library. It uses a repository of
facts, which contains all the services, their input and output
parameters and the semantic relations between the parameters. The following is the code snippet of our composition
engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs),
encodeParam(QueryOutputs, QO),
getExtInpList(QueryInputs, InpList),
encodeParam(InpList, QI),
performForwardTask(QI, QO, LF),

performBackwardTask(LF, QO, LR),
getMinSolution(LR, QI, QO, A), reverse(A, RevA),
confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The query is converted into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate ’bagof’ to return all possible
ListOfServices that can be composed to get the requested
queryService.
Output Generator: After the Composition engine finds a
matching service, or the list of atomic services for a composed service, the results are sent to the output generator in
the form of triples. This module generates the output files
in any desired XML format.

5

Efficiency and Scalability Issues

In this section we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It
is because of these features, we decided on the multi-step
narrowing based approach to solve these problems and implemented it using constraint logic programming.
Correctness: Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions.
Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always
find a correct solution (if one exists) in the minimum possible steps. The formal proof of correctness and minimality
is beyond the scope of this paper.
Pre-processing: Our system initially pre-processes the
repository and converts all service descriptions into Prolog
terms. The semantic relations are also processed and loaded
as Prolog terms in memory. Once the pre-processing is
done, then discovery or composition queries are run against
all these Prolog terms and hence we obtain results quickly
and efficiently. The built-in indexing scheme and constraints in CLP(FD) facilitate the fast execution of queries.
During the pre-processing phase, we use the term representations of services to set up constraints on services and the
individual input and output parameters. This further helped
us in getting optimal results.
Execution Efficiency: The use of CLP(FD) helped significantly in rapidly obtaining answers to the discovery and
composition queries. We tabulated processing times for different size repositories and the results are shown in Section
6. As one can see, after pre-processing the repository, our
system is quite efficient in processing the query. The query
execution time is insignificant.

Programming Efficiency: The use of Constraint Logic
Programming helped us in coming up with a simple and
elegant code. We used a number of built-in features such as
indexing, set operations, and constraints and hence did not
have to spend time coding these ourselves. This made our
approach efficient in terms of programming time as well.
Not only the whole system is about 200 lines of code, but
we also managed to develop it in less than 2 weeks.
Scalability: Our system allows for incremental updates on
the repository, i.e., once the pre-processing of a repository is
done, adding a new service or updating an existing one will
not need re-execution of the entire pre-processing phase.
Instead we can easily update the existing list of CLP(FD)
terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will
be negligible, perhaps a few milliseconds. With real-world
services, it is likely that new services will get added often
or updates might be made on existing services. In such a
case, avoiding repeated pre-processing of the entire repository will definitely be needed and incremental update will
be of great practical use. The efficiency of the incremental
update operation makes our system highly scalable.
Use of external Database: In case the repository grows
extremely large in size, then saving off results from the preprocessing phase into some external database might be useful. This is part of our future work. With extremely large
repositories, holding all the results of pre-processing in the
main memory may not be feasible. In such a case we can
query a database where all the information is stored. Applying incremental updates to the database is easily possible
thus avoiding recomputation of pre-processed data .
Searching for Optimal Solution: If there are any properties with respect to which the solutions can be ranked,
then setting up global constraints to get the optimal solution is relatively easy with the constraint based approach.
For example, if each service has an associated cost, then the
discovery and the composition problem can be redefined to
find the solutions with the minimal cost. Our system can be
easily extended to take these global constraints into account.

6

Repository Size
(num of
services)
2000
2000
2000
2500
2500
2500
3000
3000
3000

Number
of I/O
parameters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.5
45.8
57.8
47.7
58.7
71.6
56.8
77.1
88.2

Query
Exec
Time
(msecs)
1
1
2
1
1
2
1
1
3

Incremental
update
(msecs)
18
23
28
19
23
29
19
26
29

Table 3. Performance on Discovery Queries

found is that the repository was cached after the first run
and that explained the difference in the pre-processing time
for subsequent runs. Table 3 shows performance results of
our Composition algorithm on discovery queries and table 4
shows results of our algorithm on composition queries. The
times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the
query should be less than or equal to the wall clock time.
The results are plotted in figure 8. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase
in number of input/output parameters. Similarly, for fixed
input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which
is negligible (just 1 to 3 msecs) even for complex queries
with large repositories.

Performance

We used repositories from WS-Challenge website[13],
slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services).
These repositories contain WSDL descriptions of services.
The queries and solutions are provided in an XML format. The semantic relations between various parameters
are provided in an XML Schema file. We evaluated our
approach on different size repositories and tabulated Preprocessing and Query Execution time. We noticed that there
was a significant difference in pre-processing time between
the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we

Figure 7. Performance on Discovery Queries

Figure 8.
Queries
Repository Size
(num of
services)
2000
2000
2000
3000
3000
3000
4000
4000
4000
Table 4.
Queries

7

Performance

Number
of I/O
parameters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.1
47.1
60.2
58.4
60.1
102.1
71.2
87.9
129.2

Performance

on

Composition

Query
Exec
Time
(msecs)
1
1
1
1
1
1
1
1
1
on

Incremental
update
(msecs)
18
23
30
19
20
34
18
22
32

Composition

Related Work

Composition of Web services has been active area of research recently [14, 15, 23, 18]. Most of these approaches
are based on capturing the formal semantics of the service
using an action description languages or some kind of logic
(e.g., description logic). The service composition problem
is reduced to a planning problem where the sub-services
constitute atomic actions and the overall service desired is
represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine
the combination of actions needed to reach the goal. With
this approach an explicit goal definition has to be provided,
whereas such explicit goals are usually not available [17].
To the best of our knowledge, most of these approaches that
use planning are restricted to sequential compositions (i.e.,

a linear chain of services), rather than a directed acyclic
graph. Our approach automatically selects atomic services
from a repository and produces the composition flow in the
form of a directed acyclic graph.
The authors in [19, 20] present a composition technique
by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for
composing Web services. This approach also relies on a
user-defined plan template which is created manually.
There are industry solutions based on WSDL and
BPEL4WS where the composition of the flow is obtained
manually. A comparison of the approaches based on AI
planning techniques and approach based on BPEL4WS is
presented in [17]. This work shows that in both these approaches, the flow of the composition is determined manually. They do not assemble complex flows of atomic services based on a search process. They select appropriate
services using a planner when an explicit flow is provided.
In contrast, we have shown a technique to automatically determine these complex flows using semantic descriptions of
atomic services.
A process-level composition solution based on OWL-S
is proposed in [21]. In this work the authors assume that
they already have the appropriate individual services involved in the composition, i.e., they are not automatically
discovered. They use the descriptions of these individual
services to produce a process-level description of the composite service. They do not automatically discover/select
the services involved in the composition, but instead assume
that they already have the list of atomic services. In contrast, we automatically find the services that are suitable for
composition based on the query requirements for the new
composed service.
In [22], a semi-automatic composition technique is presented in which atomic services are selected for each stage
of composition. This selection process involves decision
making by a human controller at each stage, i.e., the selection process requires some manual intervention.
Another related area of research involves message conversation constraints, also known as behavioral signatures
[16]. Behavior signature models do not stray far from the
explicit description of the lexical form of messages, they
expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while our approach deals with higher-level
real world concepts. However, both these approaches can
be regarded as complementary concepts when taken in the
context of real world service composition, and both technologies are currently being used in the development of a
commercial services integration tool [24].
Our most important, novel contribution in this paper is

our technique for automatically selecting the services that
are suitable for obtaining a composite service, based on the
user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use
human input on what services would be suitable for composition. Our technique also handles non-sequential compositions (i.e., composition where there can be more than
one service involved at any stage, represented as a directed
acyclic graph of services) rather than sequential composition (i.e, a linear chain of services) which is the case with
most of the existing approaches.

8

Conclusions and Future Work

To catalogue, search and compose Web services in a
semi-automatic to fully-automatic manner we need infrastructure to publish Web services, document them, and query
them for matching services. Our semantics-based approach
uses semantic description of Web services (example USDL
descriptions). Our composition engines find substitutable
and composite services that best match the desired service.
Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services,
number of services in a composition, etc.). The composition flow is determined automatically without the need for
any manual intervention. Our engine finds any sequential
or non-sequential composition that is possible for a given
query. We are able to apply many optimization techniques
to our system so that it works efficiently even on large
repositories. Use of Constraint Logic Programming helped
greatly in obtaining an efficient implementation of this system.
Our future work includes extending our engine to work
with other web services description languages like OWLS, WSML, WSDL-S, etc. This should be possible as long
as semantic relations between concepts are provided. It
will involve extending the TripleGenerator, QueryReader,
and SemanticRelationsGenerator modules. We would also
like to extend our engine to support an external database to
save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size
which can easily be the case in future. Future work also
includes developing an industrial-strength system based on
the research reported in this paper, in conjunction with a
system that allows (semi-) automatic generation of USDL
descriptions from code and documentation of a service [24].

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005.
[2] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and
T. Hite. USDL: A Service-Semantics Description Lan-

guage for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas.
edu/˜sxk038200/USDL.pdf.
[3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems, pp. 46-53, Mar. ’01.
[4] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[5] WSML: Web Service Modeling Language. www.
wsmo.org/wsml/.
[6] WSDL-S: Web Service Semantics. http://www.
w3.org/Submission/WSDL-S.
[7] WSDL: Web Services Description Language. http:
//www.w3.org/TR/wsdl.
[8] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services. In
ICWS, pp 751-758, ’05.
[9] OWL WordNet http://taurus.unine.ch/
knowler/wordnet.html.
[10] K. Marriott, P. Stuckey. Prog. with Constraints: An
Introduction. MIT Press, ’98.
[11] L. Sterling, S. Shapiro. The Art of Prolog. MIT Press.
[12] OWL: Web Ontology Language Reference. http:
//www.w3.org/TR/owl-ref.
[13] WS Challenge 2006. http://insel.flp.cs.
tu-berlin.de/wsc06.
[14] U. Keller, R. Lara, H. Lausen, A. Polleres, D. Fensel.
Automatic Location of Services. In ESWC, ’05.
[15] S. Grimm, B. Motik, and C. Preist Variance in eBusiness Service Discovery. In Workshop at ISWC, ’04.
[16] R. Hull and J. Su. Tools for design of composite Web
services. In SIGMOD, ’04.
[17] B. Srivastava, J. Koehler. Web Services Composition
- Current Solutions and Open Problems. In ICAPS, ’03.
[18] B. Srivastava. Automatic Web Services Composition
using planning. In KBCS, pp 467-477.
[19] S. McIlraith, T.C. Son Adapting golog for composition of Web services. In KRR, pp 482-493, ’02.
[20] S. McIlraith, S. Narayanan Simulation, verification
and automated composition of services. In WWW, ’02.
[21] M. Pistore, P. Roberti, P. Traverso Process-Level
Composition of Executable Services In ESWC, pp 6277, ’05.
[22] E. Sirin, J. Hendler, and B. Parsia Semi-automatic
Composition of Web Services using Semantic Descriptions In Workshop at ICEIS, ’02 .
[23] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara
Semantic Matching of Web Service Capabilities. In
ISWC, pp 333-347, ’02.
[24] Metallect IQ Server. http://metallect.com/
downloads/Metallect_Product_Brief_
IQServer.pdf

Annotating UDDI Registries to Support the Management of
Composite Services
M. Brian Blake, Michael F. Nowlan, Ajay Bansal, and Srividya Kona
Department of Computer Science
Georgetown University
Washington, DC
202 687-3084

{mb7,mfn3,ab683,sk558}@georgetown.edu
ABSTRACT
The future of service-centric environments suggests that
organizations will dynamically discover and utilize web services
for new business processes particularly those that span multiple
organizations. However, as service-oriented architectures mature,
it may be impractical for organizations to discover services and
orchestrate new business processes on a daily, case-by-case basis.
It is more likely that organizations will naturally aggregate
themselves into groups of collaborating partners that routinely
share services. In such cases, there is a requirement to maintain
an organizational memory with regards to the capabilities offered
by other enterprises and how they fit within relevant business
processes. As a result, registries must maintain information about
past business processes (i.e. relevant web services and their
performance, availability, and reliability). This paper discusses
and evaluates several hybrid approaches for incorporating
business process information into standards-based service
registries.

Categories and Subject Descriptors
D.3.3 [Programming Languages]: Language Contructs and
Features – abstract data types, polymorphism, control structures.

General Terms
Performance,
Design,
Standardization

Experimentation,

Security,

and

Keywords
Keywords are your own designated keywords.

1. INTRODUCTION
Service-oriented computing [10] promotes the development
of modular domain-specific capabilities that can be advertised to
and shared among collaborating organizations.

Moreover, the syntactic and semantic metadata that accompanies
these services [9][11] enable the discovery of these capabilities,
on-demand. Discovery, in this environment, largely depends on
the accessibility and capabilities of the repositories for which
these services are stored. Universal Description, Discovery, and
Integration (UDDI) is the leading specification for the
development of service-based repositories or registries. UDDI
registries of the future should facilitate fast search and discovery
of relevant web services akin to the performance currently
associated with resolving a domain name. Although, performance
and federation are two important aspects of UDDI, an additional
requirement for UDDI should be effective process-oriented
storage and retrieval. Currently, the abilities to browse and
discover independent services as characterized by their
overarching business name and/or their capability name are
important fundamental operations. However, as service-oriented
architectures mature, composite services (i.e. capabilities based on
the workflow composition of multiple atomic services) will also
be important to persist and manage. UDDI currently has limited
support for managing business processes [6]. Although not all
federated service registries will need business process annotations,
we suggest that a subset of registries frequently used by partnering
organizations would benefit from maintaining historical process
information.
Currently, there are numerous languages and protocols that
support the specification and execution [3][4][5][9]of composite
web services. Unfortunately, techniques for incorporating the
underlying process information into UDDI registries are limited.
In this work, we address several questions as listed below.
•
•
•

•

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SAC’09, March 8-12, 2009, Honolulu, Hawaii, U.S.A.
Copyright 2009 ACM 978-1-60558-166-8/09/03…$5.00.

What are the relevant descriptive attributes of composite web
services that must be represented in web service registries?
What are the relevant use cases for process-oriented service
registries?
What are the state-of-the-art methods for incorporating
process information into registries and the corresponding
challenges?
What are the most efficient and effective approaches, both
qualitatively and quantitatively, for process management of
services in UDDI registries?

This paper proceeds in the next section with a survey of related
work and a discussion of the state of the practice. The next
section formalizes a composite service by detailing the most
relevant descriptive attributes.
Next, we describe the
requirements of a process-oriented registry. We next introduce

several hybrid approaches for adding process information to web
service repositories. Finally, we describe a case study and
experimental evaluation of both approaches.

2. RELATED WORK
Universal Description, Discovery and Integration (UDDI)
describes the data model associated with a web-accessible registry
for the storage and management of web services. Three core
hierarchical objects specify the service provider (businessEntity),
the web service (businessService), and information about how the
service is binded (bindingTemplate). These foundational objects
can be extended with the use of technical models or tModels.
TModels facilitate the further description of businessEntities,
businessServices, and bindingTemplates through classification
based on metadata. Each tModel of a businessService represents
a certain behavior or classification system that the service must
implement. An example would be a web service that takes a state
abbreviation as input. This web service would probably choose to
reference the tModel that represents the US-State abbreviation
classification system.
A person looking at the service’s
bindingTemplate would then be able to see a reference to the USState System and know that a state should be entered with its
abbreviation. UDDI also supports other structures called
keyedReferences that allow previously mentioned core objects to
be associated to tModels. KeyedReferences consist of a
tModelKey, keyValue, and keyName. The tModelKey identifies
the referenced tModel. The keyValue allows a categorization of
the link between the core object, and the tModel and the keyName
is a text string readable for humans. In UDDI v3,
keyedReferences can be aggregated with keyedReferenceGroups.
The strength of the tModels and keyedReferences is that
further information about the main UDDI objects (i.e.
businessEntities, businessServices, and bindingTemplates) are not
populated in the repository. Tmodels merely point to webaccessible documents. This paradigm both reduces maintenance
of the registry and promotes overall robustness. However, this
paradigm also makes it difficult to associate web services that are
stored in the registry, which is a necessary requirement for
describing business processes within the registry.
The most common research projects tend to address the
problem of federating UDDI registries [1] [2][12], although, of
most relevance to our work, are the projects that directly address
the problem of business process annotations that associate
services. Perhaps the leading approach to inserting business
processes is the construction of a tModel classification system that
mirrors a particular taxonomy of business processes. These
tModels can then be used as pointers to the corresponding
business process description documents. In industry, several
OASIS technical reports [15] [17] describe high-level approaches
to integrating tModel classifications with ebXML and BPEL4WS
descriptions. Other research projects detail specialized domainspecific methods that leverage the same basic approach [6] [13]
These are reasonable approaches, but all business descriptions are
defined by external business process documents that are
decoupled from the individual services. In fact, since services
may be captured at individual locations, replacing services or even
discontinuing the offering of a particular business process
becomes difficult. In these cases, centralizing some vital part of
the process information can be valuable. Luo et al. [8] and
Srinivasan et al. [14] use semantic notations embedded in UDDI

tModels to associate services. This approach allows for more rich
definitions, but the underlying limitations caused by distribution
also apply. Other approaches look at the development of external,
integrated software mechanisms that run parallel with the UDDI
registry [18] [19]. These approaches tend to depart from the
essence of the SOA paradigm as they promote proprietary, less
standardized solutions.
In this paper, we experiment with a combination of external
process documentation and annotations that are embedded directly
in the UDDI registry. Prerequisite to any solution, it is important
to understand the required aspects of the web services-based
business process

3. ANATOMY OF WEB SERVICES-BASED
PROCESSES
Web-services based business processes also referred to as web
service workflows are similar to traditional processes that are
established between human stakeholders. Figure 1 illustrates the
metamodel of a web services-based business process using a
Unified Modeling Language (UML) class diagram. A difference
is, as opposed to human-managed tasks or steps, web services
enact the underlying steps. A web services-based business
process, BP, contains user data endpoints, DE , defined below.
DataPart
-DataPart_ID
-DPValue

UserDataEndpoint
-UserDataEndpointID
-InputID
-OutputID
*
1

<< has a>>

BusinessProcess
-ProcessID
-UserDataEndPointID
-TransitionID
-ServiceID
-Description
1

1

1

Transition

1

*

-TransitionID
-InputID
-OutputID
-CFlowType
-PreCondID
-PostCondID

*

<< has a>>
*

MessageContainer
1

-MessageContainerID
-DataPartID
*

*

QoS
Service
*

-ServiceID
-InputID
-OutputID
-QosID
-URI_ID

1
*

-QoSID
-SLOType
-SLOValue

<< has a>>

1
1

URI
*

-URI_ID
-URIValue

Figure 1. Metamodel of a Web Service-Based Business Process.
Definition (UserData EndPoint):
The user data end point is defined as a pair DE = (ID, OD) where ID
represents the input information of the business process provided
by the user and OD represents the output information, ultimately
generated by the completion of the business process. The business
process also has a sequence of tasks (realization of the steps) that
are implemented by a set of services, Ω = {S1, S2, S3, ….Sn}. Each
service Si has its own input, ISi, and output, OSi, information;
however the set of all input/output information of a service is less
relevant than the subset of inputs and outputs that are relevant to
the business process. In addition, a service can also be defined
with its quality of service information and its URI location.
Definition (Service):
A service is a tuple of its inputs, outputs, QoS parameters, and its
URI location. S = (IS , OS , QS , US) is the representation of a

service where IS is the input list, OS is the output list, QS is the list
of quality of service parameters and US represents the URI
location. Each step in the business process is defined by a
transition, T, that defines the shared information between the
output, OT, of the preceding step that connects to the input, IT , of
the subsequent step.
Definition (Transition):
A transition is represented as a tuple of its inputs, outputs, flowtype, pre-conditions, and post-conditions. T = (IT , OT ,FT , CPre ,
CPost) is the representation of the transition where IT is the input
list, OT is the output list, FT is the flow type, CPre represents the
pre-conditions of the transition and CPost represent the postconditions of the transition. Ultimately, the business process can
be formally defined as follows:
Definition (BusinessProcess):
A BusinessProcess is defined as a tuple BP = (DE, Ω, Γ) where DE
represents the user data endpoint, Ω is the set of services involved
in the business process, and Γ is the set of transitions in the
workflow .

DE = (ID, OD);
Ω = {S1, S2, ..., Sn} where Si = (ISi , OSi , QSi , USi),
for all i = 1 to n;
Γ = {T1, T2, .…, Tn} where Ti = (ITi , OTi ,FTi , CPrei ,
CPosti), for all i = 1 to n.
The following conditions should hold for a valid business
process:
For all Si
Ω and Ti
Γ, the inputs of Si are
1.
subsumed by the inputs of Ti , i.e., ISi C ITi
For all Si
Ω and Ti
Γ, the outputs of Si
2.
subsumes the outputs of Ti , i.e., OTi C OSi
For all Ti , Ti+1
Γ, the post-conditions of Ti
3.
imply the pre-conditions of Ti+1 , i.e., CPosti =>
CPre i+ 1
For all Ti , Ti+1
Γ, the outputs of Ti along with
4.
the user data inputs of the business process
subsume the inputs of Ti+1 , i.e., (OTi U ID) C IT i+ 1
The inputs of the business process subsume the
5.
inputs of the first transition, T1 (where T1
Γ ),
i.e., IT1 C ID
The outputs of the business process are subsumed
6.
by the outputs of the last transition Tn (where Tn
Γ), i.e., OD C OTn
The cardinality of data endpoints, services, and transitions vary
for each step, such that is necessary to develop containers to
aggregate the information into sets. The notion of containers is
central to business process languages, such as BPEL4WS and
BPML [3][4], for aggregating information related to subprocesses.

4. BUSINESS PROCESS AND SERVICE
REGISTRIES
In order for organizations to understand their business processes
defined with web services, it is important that their process
databases include relevant process information as defined in the

previous section. Organizations should be able to generally access
process information in addition to the service-specific details.

4.1 Potential Use Cases
There are several functions required by a registry that supports
composite services as business processes. Figure 2 illustrates the
functions of such a repository depicted as a UML use case
diagram. The basic registry actors follow the SOA paradigm (i.e.
service providers and consumers). Incorporating business process
metadata into the registry also supports the interaction of
intelligent software components or agents to autonomously
maintain the integrity of the information. Service providers should
be able to insert one or more services into the registry. Service
consumers should be able to either browse or explicitly search the
repository based on several attributes such as the name/type of
business, service, or process. Although only available using
specialized approaches, consumers may also want to search by
service/process message names. We focus on three major features
of such a repository.
•

Advertising a set of services aggregated as a process

When partnering organizations decide to share services, there
may be a predefined understanding for orchestration. As such,
these organizations must insert their relevant services into shared
UDDI registries annotated by process-based information (i.e. the
underlying control flow and data flow).
•

Discovering services associated with processes
discovering processes associated with services

and

Current UDDI registries facilitate browsing of services by
business name and by service name. A process-oriented registry
will also support browsing by process name or type. Consumers
should also be able to find all services associated with a particular
process.
• Managing process information by software agents
Once process information is annotated into a registry, intelligent
agents can regularly check the health of the underlying services.
Agents can look for indexing configurations that best support the
storage of the process information. In addition, agents can record
QoS information supplied by service consumers. This QoS
information can represent individual services or the process as a
whole.

4.2 Alternative Hybrid Approaches
By extending and leveraging the UDDI specification, we have
identified several approaches for annotating business processes
within service registries. Every service in a UDDI registry has a
bindingTemplate structure, which stores references to tModels.
TModels are “sources for determining compatibility of Web
services and keyed namespace references” Error! Reference
source not found.. This means that tModels identify how to
interact with a web service by describing the technologies it
implements. Although, the UDDI registry usually implements
default tModels, such as the State Abbreviation System, it is also
possible for an administrator of the registry to create tModels, ondemand. There are two approaches for using tModels to describe
web services-based business processes.

• Annotating business process information directly into the
UDDI registry
A new tModel is created for every business process that is
identified in the registry. In addition, a parent tModel is created
that simply classifies any tModel that annotates a business process
as such. In this way, when a new process chain is added or
identified in the registry, a tModel that points to the parent tModel
is created to represent the process. Furthermore, the categoryBag
element is used to store references to all the processes of which
the service is a part. Figure 3 shows an example tModel. Notice
that the keyName of the keyedReference contains the service
name and additional control flow information.
Also, the
keyValue maintains the sequence number of the service in the
process. Figure 4 demonstrates the actions taken by the registry to
identify and store the existence of a composite web service.
•

Defining business process information using external
markup documents

Figure 5 details the steps for annotating a business process within
the registry using the UDDI data structures. Perhaps the leading
approach in related work is the use of an external document (e.g.
BPEL4WS and ebXML) to store the process information. This
method involves simply adding an entry to each of the process
documents associated with the relevant services. The document
could exist centrally on a main server, or locally with each service
provider.
<tModel tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14">
<description>This tModel represents the process starting with
firstService and ending with thirdService. </description>
<categoryBag>
Advertise 1 or
More Services

Service Provider

<businessService>
<name>firstService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV=firstService, TRAN = Sequence"
keyValue="1"/>
</categoryBag>
</businessService>
<businessService>
<name>secondService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV = secondService, TRAN = Sequence"
keyValue="2"/>
</categoryBag>
</businessService>
<businessService>
<name>thirdService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV=thirdService, TRAN = Sequence"
keyValue="3"/>
</categoryBag>
</businessService>

Figure 3. Sample Process-Oriented TModel.

Manage and Update
Repository
<<includes>>

«extends»

Associate Services
with Processes

<keyedReference keyName="uddi:Process-Representing"
keyValue="categorization"
tModelKey="uddi:uddi.org:categorization:types"/>
</categoryBag>
</tModel>

Remove Nonfunctional
Services and Record QoS
Values

Repository Agent

Maintain Indexes
in Repository

Query by Service
Name or Type
Search Repository
«inherits»
Query by Process

«inherits»
<<includes>>

Query by Business
Name or Type

«inherits»

Filter and
Characterize Query

<<includes>>

Service Consumer

«inherits»
Browse Repository

Query by
Message/Part Names

Figure 2. Process-Oriented Service Repository Use Cases
(** Shaded areas are not currently well supported in the state-of-the-art)
Annotate( VS):
VS
GSFIRST
GSLAST
TM

Annotating Function
Vector of serviceKeys for services in chain
The first service in the vector
The last service in the vector
The newly created process-tModel

TMKEY
PM
CB
S
KR

The newly created tModel’s key
The tModel Process-Classification System
The CategoryBag of an individual structure
An individual service
KeyedReference – A pointer to a tModel

A(VS, TM)
Function to add a KR to CB of Services
MakeTM(GSFIRST , GSLAST ) Function to make a new TM
A(VS , TM)
KR = TMKEY
forAll S in VS
S.CB += KR
return VS
MakeTM(GSFIRST , GSLAST )
TM.CB += PM
TM.description = GSFIRST , GSLAST
return TM
Annotate(VS)
TM = MakeTM(GSFIRST , GSLAST )
return A(VS , TM)

Figure 4. Annotating a Business Process of Web Services within
UDDI Data Model.
Annotate(VS):
Annotating Function
MakeEntry(VS ) Function to make a new TM
A(VS , E)
Function to add entry to XML Doc.
VS
Vector of serviceKeys for services in chain
E
An entry containing a list of services
S
An individual service
S.KEY
Service key
D
A service’s description element
FS
A service’s XML document file
A(VS , E)
forAll S in VS
S. FS += E
return VS
MakeEntry(VS)
forAll S in VS
E += S.KEY
return E
Annotate(VS)
forAll S in VS
S.D = FS
E = MakeEntry(VS )
return A(VS , E)

Figure 5. Leveraging External Process Documents

4.3 Alternative Hybrid Approaches
Both of the hybrid approaches also have other functions relevant
to a business-oriented registry.
• Service Deletions or Changes when using UDDI Structure
Method
As the registry is updated when a process is identified, it will also
change when a service is removed or replaced. These actions
cause the process workflow chain to be incomplete. In this case,
all the services in the chain are affected and they must modify
their process annotation method (either XML document or
CategoryBag) by removing the process that is broken. When a
service is deleted, all the tModels that it points that represent
composite web services (excluding simple classifications
represented in the bindingTemplate) must be deleted. Any other
services in the registry that reference these tModels must remove
the reference from their categoryBags. In the future, registries
may be able to search for replacements services as opposed to
deleting the process.

• Service Deletions or Changes when Using External Process
Documentation
External process documents contain one entry for each process.
Irrespective of the process document notations, each entry will
define the serviceKeys of the services in the particular process. In
this approach, these serviceKeys are used to find all services that
share a process with the deleted service. The registry then edits
the relevant XML documents by removing the entry that
corresponds to the broken process.
• Query for Processes by Service Name using UDDI Data
Structure Method
The last action that can be taken on a registry, and perhaps the
most desired ability, is the ability to aggregate the information and
list of services for every process in which a particular service
plays a role. The serviceKey is used to get the service’s
categoryBag. The categoryBag contains references to all the
tModels, that represent all relevant processes. The registry is then
queried to find all services which point to any one of the tModels
in their categoryBags. The chain of services is then sorted by the
tModels and is returned as process.
• Query for Processes by Service Name using External Process
Documentation
The external document method is quicker because it does not
require querying the registry. The XML document for a service is
retrieved. In this document is a list of entries, each pertaining to
one composite web service. Each entry contains the serviceKey
for each service in that particular chain. These services can be
retrieved with a single call to the registry by compiling a vector of
the serviceKeys. The retrieved service information is then sorted
by the order of the entries in the initial service’s process document
and displayed.

5. CASE STUDY: A GENERAL UDDI
BUSINESS EXPLORER
Once process information is associated with or incorporated into a
UDDI registry, new graphical user interfaces can be created to
support enhanced service discovery and manipulation. As a part
of this work, we designed and experimented with a new user
interface front-end (i.e. UDDI-P) to the jUDDI registry Error!
Reference source not found.. A screenshot of the design of the
interface is shown in Figure 6. Using this new interface, a user
has the capability of browsing known process names as shown on
the left side of Figure 6. Once a process is identified, the
consumer can further decide to analyze the process to see if it
meets the organization need. The interface can display parameters
such as estimated delivery date and price range based on service
level objectives associated with the process stored in the registry.
By embedding process information into the registry, classification
of processes can occur to the point where they themselves
function as individual services. On the right side of the interface,
the user has the ability to browse each process meeting the search
criteria by price and delivery. This sort of interface would enable
a separate interface that allows the services to be executed.

An alternative approach is incorporating specific business process
information directly into the registry. We experimented to
characterize the performance of these approaches on our prototype
repository. The performance is illustrated in Table 1 and Figure 7
based on the use cases illustrated in Figure 2.

Service Request

UDDI-P
Directions:

Enter the necessary data on the left and click
Submit. Offers matching your criteria will be
displayed in the window on the right.

Specifications

Results

* = required

(Updated when Submit is pressed)

* Application

Viewing:

[Process]

Name:

1 of 5

* Delivery Date
Date:

Sort By:
Price

Business Chain

[Month]

dd

yyyy

FindBookByISBN

Consultation

SubmitPaymentInfo

Need Expert Help?
No, thanks.

ShipProduct

Price Range
From: $0

Reset

To:

$100

Ready by: mm/dd/yyyy
Price: $50.00

Choose

Submit

* Purchase on next page

Prev.

Next

Figure 6. UDDI-P: A Prototype User Interface for Process-Based
Service Management.

6. EXPERIMENTATION & EVALUATION
In previous sections, two distinct hybrid approaches for
aggregating UDDI services into business processes were
introduced. The leading approaches in published works suggest
using external mark-up documents to describe business processes.

In general, the difference in performance between adding
individual services, adding a chain, and annotating services with
business process annotations were only negligibly different
between the two approaches. However, deleting a service was
approximately three times faster when external business process
documents were used. Another variation in performance is
associated with retrieving all service IDs associated with a
particular process (or aggregating the services). The aggregation
time increased linearly with the increase in size of the UDDI
registry embedded process information. The main reason for
disparity between the approaches is due to the fact that the latter
approach requires a significant query within the repository. This
approach must retrieve the categoryBag for all services in the
repository and search those structures for a particular
keyedReference. The external business process document does not
require this step since the process information for a service is
stored in one location: the XML-based document. The retrieval of
this document is much faster when compared to the query that
must take place within the registry. Although the performance for
an external file is more favorable, having external BPEL4WS files
causes the duplication of process information (i.e. the same
process entry could appear in multiple files that may be attached
to the underlying services). Depending on the management of the
BPEL4WS files, centralizing the process within the repository
may be more advantageous. In such cases, embedding processes
with the registry represents an effective solution.

Table 1. Performance of External Document and Annotated UDDI Repository (milliseconds)

Repo
Size

Add
Serv.
(ms)

Add
Composite
(ms)

Annotate
(Note)
(ms)

Collect
Service IDs
by Process
(Aggregate)
(ms)

Ext.
Process
Doc

150
330
600
850

1573
1573
1573
1573

2500
2700
2600
2600

1500
1700
1600
1600

1500
1500
1500
1500

1900
1900
1900
1900

Internal
UDDI
Data
Structure

150
330
600
850

1573
1573
1573
1573

2800
3000
3000
3000

2100
2100
2100
2100

2600
4000
5700
7450

7790
7790
7790
7790

Del. Serv.
(ms)

9000
8000
7000
6000
5000
4000
3000
2000
1000
0

Add Service
Add Composite
Note
Aggregate

External XML Document

Repo=850

Repo=600

Repo=330

Repo=150

Repo=850

Repo=600

Repo=330

Repo=150

Delete Service

Annotated UDDI

Figure 7. Comparison of Storing Process Information in External Document versus Annotating the UDDI Repository.

7. CONCLUSIONS
An innovation in this paper is the formalized model for web
services-based business process and the relevant use cases for
using this information. In addition, we introduce the design of a
new interface for business-based UDDI interactions.
Our
experimentation evaluates the two leading approaches for
capturing process information in UDDI registries. Overall
performance information does not suggest a quantitative
advantage for embedding process information directly into the
repository. However, qualitatively, maintenance is less extensive
since process information is centralized in a potentially federated
registry. As future work, we plan to continue developing a
process-oriented UDDI explorer and experiment on new
approaches for interface design.

8. ACKNOWLEDGMENTS
We acknowledge fruitful conversations with Brian Schott and
Robert Graybill of the University of Southern California, ISI-East
and Suzy Tichenor of the Council of Competitiveness. This
material is based on research sponsored by DARPA under
agreement number FA8750-06-1-0240. This U.S. Government is
authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or implied, of
DARPA or the U.S. Government.

9. REFERENCES
[1] Al-Masri, E. and Mahmoud, Q.H., “Crawling Multiple UDDI
Business Registries”, Proceedings of the 16th International
Conference on the World Wide Web, Banff, Alberta,
Canada, 2007
[2] Blake, M.B., Sliva, A.L., zur Muehlen, M., and Nickerson, J.
“Binding Now or Binding Later: The Performance of UDDI
Registries”, IEEE Hawaii International Conference of
System Sciences (HICSS-2007), Track on Technology and
Strategies for Realizing Service-oriented Architectures with
Web services, January 2007
[3] WS-BPEL(2008 ):
http://www.ibm.com/developerworks/library/specification/w
s-bpel/

[4] BPML (2008): http://www.ebpml.org/bpml.htm (currently
moved to OMG)
[5] BPMN (2008): http://www.bpmn.org/
[6] Dogac, A., Tambag, Y., Pembecioglu, P, Pektas, S., Laleci,
G., Gokhan, K., Toprak, S., and Kabak, Y. “An ebXML
infrastructure implementation through UDDI registries and
RosettaNet PIPs” Proceedings of the 2002 ACM SIGMOD
Conference (SIGMOD 2002), Madison, Wisconsin, June
2002
[7] jUDDI (2008): http://ws.apache.org/juddi/
[8] Luo, J., Montrose, B., Kim, A., Khashnobish, A., Kang, M.
“Adding OWL-S Support to. the Existing UDDI
Infrastructure” Proceedings of the 4th International
Conference on Web Services (ICWS2006), Chicago, Ill,
November 2006.
[9] OWL-S(2008): http://www.daml.org/services/owl-s/
[10] Papazoglou, M. “Service-oriented computing: Concepts,
characteristics and directions. In Proc. of WISE ‘03
[11] RDF (2008): http://www.w3.org/RDF/
[12] Sivashanmugam, K., Verma, K., and Sheth, A. Discovery of
Web Services in a Federated Registry Environment,
Proceedings of 4th IEEE International Conference on Web
Services (ICWS), pp. 270-278, 2004.
[13] Spies, M., Schoning, H., and Swenson, K. “Publishing
Interoperable Services and Processes in UDDI” The 11th
Enterprise Computing Conference (EDOC 2007), Annapolis,
MD, October 2007
[14] Srinivasan, N., Paolucci, M. and Sycara, K. "Adding OWL-S
to UDDI, implementation and throughput," Proceedings of
First International Workshop on Semantic Web Services and
Web Process Composition (SWSWPC 2004), San Diego,
California, USA, 2004
[15] UDDI as the registry for ebXML Components, OASIS
Technical Note, February 2004, Accessed (2008):
http://www.oasis-open.org/committees/uddispec/doc/tn/uddi-spec-tc-tn-uddi-ebxml-20040219.htm
[16] Universal Description, Discovery, and Integration (UDDI)
(2008): http://www.uddi.org/pubs/uddi_v3.htm
[17] Using BPEL4WS in UDDI Registry, OASIS Technical Note,
July 2005 Accessed (2008): http://www.oasis-

open.org/committees/uddi-spec/doc/tn/uddi-spec-tc-tn-bpel20040725.htm
[18] Zhang, L-J., Zhou, Q., and Chao, T., “A Dynamic Services
Discovery Framework for Traversing Web Services
Representation Chain”, In Proceedings of the International
Conference on Web Services (ICWS 2004), 2004

[19] Zhang, M., Cheng, Z., Zhao, Y. Huang, J.Z. Yinsheng,
L., Zang, B. “ADDI: an agent-based extension to UDDI for
supply chain management” Proceedings of the Ninth Int
Conference on CSCW in Design, Shanghai, China, May
2005

MOOCLink: Building and Utilizing Linked Data
from Massive Open Online Courses
Sebastian Kagemann

Srividya Kona Bansal

Arizona State University—Polytechnic Campus
CRA-W Distributed Research Experience
Mesa, Arizona 85212, USA
sakagema@indana.edu

Arizona State University—Polytechnic Campus
Department of Engineering
Mesa, Arizona 85212, USA
srividya.bansal@asu.edu

Abstract—Linked Data is an emerging trend on the web with
top companies such as Google, Yahoo and Microsoft promoting
their own means of marking up data semantically. Despite the
increasing prevalence of Linked Data, there are a limited number
of applications that implement and take advantage of its
capabilities, particularly in the domain of education. We present
a project, MOOCLink, which aggregates online courses as
Linked Data and utilizes that data in a web application to
discover and compare open courseware.
Keywords—linked data; education; ontology engineering

Linked Data available for MOOCs or an ontology that denotes
properties unique to MOOCs.
In order to incorporate MOOC data into the Linked Data
cloud as well as demonstrate the potential of Linked Data when
applied to education, we propose to (i) build or extend an RDF
ontology that denotes MOOC properties and relationships (ii)
use our ontology to generate Linked Data from multiple
MOOC providers and (iii) implement this data in a practical
web application that allows users to discover courses across
different MOOC providers.

I. INTRODUCTION

II. BACKGROUND

Linked Data involves using the Web to create typed links
between data from different sources [1]. Source data may vary
in location, size, subject-matter and how congruously it is
structured. Typed links produced from this data create
uniformity and define properties explicitly in an effort to make
data easier to read for machines. This is opening up
opportunities for applications that were previously impractical
such as Berners-Lee’s intelligent agents [2].

A. Resource Description Framework
The most notable model for Linked Data is the Resource
Description Framework (RDF), which encodes data as subject,
predicate, object triples [7]. The subject and object of a triple
are both Uniform Resource Identifiers (URIs), while the
predicate specifies how the subject and object are related, also
using a URI. For the purposes of this paper, Linked Data is
presented as RDF/XML, an XML syntax for RDF, which is
also used in the implementation of our application.

Linked Data is relatively unexplored in the domain of
education. Although there are several data models for
structuring educational data as well as repositories adopting
these models [3], Linked Data-driven educational applications
are far and few between. As a result, initiatives such as the
LinkedUp Challenge have surfaced to encourage innovative
applications focused on open educational data [4].
Massive Open Online Courses or MOOCs are online
courses accessible to anyone on the web. Hundreds of
institutions have joined in an effort to make education more
accessible by teaming up with MOOC providers such as
Coursera and edX [5]. Delivering course content through
lecture videos as well as traditional materials such as readings
and problem sets, MOOCs encourage interactivity between
professors and students around the world by way of discussion
forums and graded assessments.
Coursera, a leading MOOC provider, offers a RESTful API
[6] for most information associated with their course catalog.
This includes properties such as a courses’s title, instructor,
and syllabus details. Although Coursera’s course catalog data
is easily accessible as JSON, there is no option to retrieve and
use it in a Linked Data format such as the Resource
Description Framework (RDF). Moreover, there is little to no

B. Simple Protocol and RDF Query Language
Simple Protocol and RDF Query Language or SPARQL is
an RDF query language that allows users to retrieve and
manipulate data stored as RDF [8]. SPARQL is used as our
application’s query language in order to retrieve data to
populate web pages with course information. A SPARQL
endpoint
for
our
data
can
be
accessed
at
http://sebk.me:3030/sparql.tpl.
C. Linked Data Principles
Tim Berners-Lee published a set of rules for publishing
data on the Web so that data becomes part of a global space in
which every resource is connected. The rules are as follows:
1.

Use URIs as names for things

2.

Use HTTP URIs so that people can look up those
names

3.

When somone looks up a URI, provide useful
information (RDF, SPARQL)

4.

Include links to other URIs, so that they can discover
more things

These principles provide a basis for contributing to a
Linked Data cloud in which a variety of datasets from different
fields of human knowledge are interconnected. Our project
aims to abide by these principles.
D. Linked Education Data
Many models have been devised for structuring educational
data, among the most popular are the IEEE Learning Object
Metadata (LOM) specification and Sharable Content Object
Reference model (SCORM). LOM is encoded in XML and
includes nine categories with sub-elements that hold data. An
RDF binding for LOM exists [9], however development is
halted at the time of this paper’s writing [10]. SCORM is an
extensive technical standard, typically encoded in XML, that
defines how educational content should be packaged, how it is
delivered, and how learners navigate between different parts of
an online course [11]. An RDF binding of SCORM has yet to
be developed. Rather than using the defunct LOM binding or
creating a new binding of SCORM to RDF, we chose to extend
a vocabulary meant for Linked Data, Schema.org, for which an
RDF mapping exists [12], to include properties unique to open
courseware.
In 2013, the Learning Resource Metadata Initiative (LRMI)
specification was incorporated into Schema.org’s vocabulary
for tagging educational content [13]. The properties added in
this adoption introduced fields for online course details
including the type of learning resource, time required, and so
on. While there is significant overlap between LRMI’s
additions to Schema.org, Schema.org’s Creative Work
properties and MOOC course details like those provided in
Coursera’s API, several crucial missing data fields such as
syllabus details, course difficulty, and predicates linking
courses to other objects, make it necessary to extend the
vocabulary for MOOC data.
E. Building Ontologies
After determining that there was no educational resource
ontology that denoted every property needed to create linked
MOOC data, we chose to extend Schema.org’s ontology. In
order to support uniformity in our data, we use RDF/XML
from ontology creation to final data generation.
Schema.rdfs.org hosts an RDF/XML version of Schema.org’s
ontology, which we imported into Stanford Protégé to extend
with additional types and properties.
III. MOOCLINK
MOOCLink is a web application which aggregates online
courses as Linked Data and utilizes that data to discover and
compare online courseware. This section of the paper outlines
our approach including choosing our providers, modeling the
data, data generation, and the development of our web
application.
A. MOOC Providers
Coursera is the largest MOOC provider in the world with
7.1 million users in 641 courses from 108 institutions as of
April 2014 [14]. These courses span 25 categories including 4
subcategories of computer science. All course details are
2014 CRA-W Distributed Research Experience for Undergraduates

retrievable by HTTP GET method using Coursera’s RESTful
course catalog API and returned in JSON.
edX is another premier MOOC provider with more than 2.5
million users and over 200 courses as of June 2014 [15]. edX
courses are distributed among 29 categories, many of which
overlap with Coursera’s. edX does not provide an API for
accessing their course catalog, however, as of June 2013,
edX’s entire platform is open-source.
Udacity, founded by Google VP, Sebastian Thrun, is a
vocational course-centric MOOC provider with 1.6 million
users in 12 full courses and 26 free courseware as of April
2014 [16]. The majority of Udacity courses are within the field
of computer science. Udacity does not provide an API for
accessing their course catalog data.
B. Data Model
Schema.org is organized as a hierarchy of types, each
associated with a set of properties. CreativeWork is
Schema.org’s type for generic creative work including books,
movies, and now educational resources. The LRMI
specification adds properties to CreativeWork including the
time it takes to work through a learning resource
(timeRequired), the typical age range of the content’s intended
audience (typicalAgeRange), as well as specifying properties
previously available in Schema.org’s CreativeWork type like
the subject of the content (about) and the publisher of the
resource (publisher) [17].
CreativeWork provides a base type for our ontology
extension, which adds types Course, Session, Category and
their associated properties drawn from MOOC data. The
extension is made in Stanford Protégé [18], which we use to
import the Schema.org vocabulary mapped to RDF at
Schema.RDFS.org. In the GUI of Protégé, types and
properties are added as well as sample individuals to be used
as a model for data generation. The final product is an
ontology in which classes are defined using OWL, the Web
Ontology Language [19], and in which data and object
properties are defined using RDFSchema, which provides
basic elements for the description of ontologies [20]. The
hierarchy of the ontology extension is outlined in the
Implementation section of this paper.
C. Data Generation
Coursera’s list of courses is accessible using their RESTful
API and JSON as the data exchange format. Using Requests, a
Python HTTP library [21], we retrieved a full list of courses,
universities, categories, instructors and course sessions in
JSON using the GET method.
edX and Udacity do not have an API therefore it became
necessary to use a scraper to obtain course data. We used
Scrapy, an open-source screen scraping and web crawling
framework for Python [22] to retrieve properties from edX
and Udacity course pages with XPath selectors. Scrapy
supports multiple formats for storing and serializing scraped
items; we export our data as JSON to maintain uniformity
with the data we retrieve from Coursera.

After collecting the data, we create Linked Data from
JSON using Apache Jena, an open source Semantic Web
framework for Java [23]. First, we import the ontology model
we created in Protégé to retrieve the types and properties to be
assigned. We use three methods, one for each course provider,
to read JSON specific to each provider (using Google Gson
[24]), map property names from JSON to our ontology’s
properties, and write each property to the RDF graph. The
RDF is output in a flat file, serialized as RDF/XML.
As recommended by Berners-Lee in his Linked Data
Principles, each property is assigned a URI. HTTP URIs are
used so these resources can be looked up. Because the
majority of our data points to URIs within the same RDF,
hash URIs are used to identify the local resources. More
details on the naming schemes of these URIs are available in
the Implementation section of this paper, where our RDF data
for Coursera, edX, and Udacity is discussed in detail.

IV. IMPLEMENTATION
This section outlines and provides relevant illustrations of
(i) our extension of Schema.org’s ontology to include MOOC
classes and properties, (ii) web crawling methods, (iii) RDF
data from three different MOOC providers and (iv) our web
application.
A. Ontology Schema
As mentioned in the previous section on our data model,
Schema.org is organized as a hierarchy of types. Much like
object-oriented programming, types inherit properties. Figure
1 displays relevant properties from CreativeWork and other
Schema.org types although our new types, Course, Session
and Category, inherit more properties that are not shown.
Sebastian’s personal domain, sebk.me, is used as a temporary
namespace for the ontology.

D. Web Application
To create an application that is appealing to the eye, we
used Bootstrap [25], a responsive HTML, CSS, and JavaScript
framework to create a UI consistent on both mobile and
desktop devices. Rather than designing each component of the
website, we repurposed a business template for displaying
online courses. The template incorporates many current web
design trends including parallax scrolling and image carousels,
which give it an up-to-date look and feel.
Fuseki is a sub-project of Jena that provides an HTTP
interface to RDF data. For the web component of our project,
we upload our data into a stand-alone Fuseki server from
which we can query and update our RDF by <what>. Fuseki
includes a built-in version of TDB (Tuple Data Base), which
indexes the contents of the file as opposed to storing RDF as a
flat file [26].
Google App Engine was initially explored as the web
development framework for its automatic scaling and Java
support. Unfortunately an incompatibility between App Engine
and Apache Jena was found as App Engine supports URL
Fetch to access web resources [27] while Jena’s SPARQL
processor, ARQ, implements their HTTP requests using their
own HTTP operation framework [28]. Additionally, App
Engine’s limits on request and response size (10 megabytes
and 32 megabytes respectively) as well as their maximum
deadline on request handlers (60 seconds) could potentially
cause problems if our queries executed slower than anticipated.
Because of App Engine’s constraints, we opted for a
combination of Apache Tomcat as our web server / servlet
container and JavaServer Pages (JSPs) and Java Servlets for
dynamic web page generation over App Engine’s similar Java
solution. Each JSP uses HTML, CSS and JS from the
Bootstrap template for the UI. Jena ARQ then queries our RDF
on the Fuseki server and writes the course details onto
respective pages. Screenshots of the application demo can be
found in the Implementation section of this paper.

Fig. 1. Diagram of Schema.org ontology extension.

B. Web Crawling
We retrieve Coursera’s course properties via their course
catalog API but use screen scrapers for edX and Udacity. This
section details the process of writing a Scrapy crawler for edX
in Python. After starting a new Scrapy project in the terminal,

we define the Item or container that will be loaded with
scraped data. This is done by creating a scrapy.Item class and
defining its attributes as scrapy.Field objects as shown in
Figure 2. These attributes, which are ultimately output as
JSON, are named slightly different than our RDF properties.
As a consequence, the naming scheme is mapped to the types
defined in our ontology later in our Jena methods, which
convert the collected JSON into RDF/XML.
class	
  EdxItem(Item):	
  
	
  	
  	
  	
  name	
  =	
  Field()	
  
	
  	
  	
  	
  about	
  =	
  Field()	
  
	
  	
  	
  	
  instructor	
  =	
  Field()	
  
	
  	
  	
  	
  school	
  =	
  Field()	
  
	
  	
  	
  	
  courseCode	
  =	
  Field()	
  
	
  	
  	
  	
  startDate	
  =	
  Field()	
  
	
  	
  	
  	
  url	
  =	
  Field()	
  
	
  	
  	
  	
  length	
  =	
  Field()	
  
	
  	
  	
  	
  effort	
  =	
  Field()	
  
	
  	
  	
  	
  prereqs	
  =	
  Field()	
  
	
  	
  	
  	
  video	
  =	
  Field()	
  
	
  	
  	
  	
  category	
  =	
  Field()
Fig. 2. Defining attributes for the EdXItem container

In
Figure
3
we
subclass
CrawlSpider
(scrapy.contrib.spiders.CrawlSpider), and define its name,
allowed domains, as well as a list of URLs for the crawler to
visit first. The subsequent lines of code open a list of edX
categories, iterate through those categories and append URLs
to start_urls. These appended URLs lead to paginated lists of
courses in each category. This method was chosen over using
edX’s “All Courses” listing as a start URL in order to gather
category names from each edX course. Category names are
currently not explicitly defined on each edX course page.
class	
  EdXSpider(CrawlSpider):	
  
	
  	
  	
  	
  name	
  =	
  "edx"	
  
	
  	
  	
  	
  allowed_domains	
  =	
  ["edx.org"]	
  
	
  	
  	
  	
  start_urls	
  =	
  []	
  
	
  	
  	
  	
  file	
  =	
  open('data/category_map',	
  'r')	
  
	
  	
  	
  	
  for	
  line	
  in	
  file:	
  
	
  	
  	
  	
  	
  	
  	
  	
  url="https://www.edx.org/course-­‐list/allschools/"	
  
+	
  line.strip()	
  +	
  "/allcourses"	
  
	
  	
  	
  	
  	
  	
  	
  	
  start_urls.append(url)
Fig. 3. Defining the spider and start_urls

Figure 4 shows our first parse method which takes a URL
from our list of start URLs defined in Figure 3, selects each
course URL listed on the page using the XPath selector
“//strong/a/@href”, then iterates through each site. For each
site, a new EdxItem is declared, “url” and “category” are
assigned, and a new scrapy.Request is made for the URL
selected, calling the parse_details method detailed in Figure 5.
The complete EdxItem is then assigned to the Request.meta
attribute, and the request is appended to a list of every request
made during the crawl.
def	
  parse_sites(self,	
  response):	
  
filename	
  =	
  response.url.split("/")[-­‐2]	
  
open(filename,	
  'wb').write(response.body)	
  
sel	
  =	
  Selector(response)	
  
sites	
  =	
  sel.xpath('//strong/a/@href').extract()	
  
sites.pop(0)	
  #	
  remove	
  home	
  directory	
  link	
  
requests	
  =	
  []	
  

for	
  site	
  in	
  sites:	
  
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  item	
  =	
  EdxItem()	
  
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  item['url']	
  =	
  site	
  
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  item['category']=response.request.url.split("/")[5]	
  
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  request=scrapy.Request(site,callback=self.parse_details)	
  
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  request.meta['item']	
  =	
  item	
  
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  requests.append(request)	
  
	
  return	
  requests

Fig. 4. Parsing start_urls

The next method, parse_details, shown in Figure 5, collects
attributes from individual course pages using XPath selectors.
The majority of these selectors are tailored to the HTML
classes provided by edX, which denote course properties
provided on the course page.
def	
  parse_details(self,	
  response):	
  
	
  	
  	
  	
  	
  	
  	
  	
  filename	
  =	
  response.url.split("/")[-­‐2]	
  
	
  	
  	
  	
  	
  	
  	
  	
  open(filename,	
  'wb').write(response.body)	
  
	
  	
  	
  	
  	
  	
  	
  	
  sel	
  =	
  Selector(response)	
  
	
  	
  	
  	
  	
  	
  	
  	
  item	
  =	
  response.meta['item']	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['name']	
  =	
  sel.xpath('//h2/span/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['about']=	
  
sel.xpath('//*[@itemprop="description"]').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['instructor']=sel.xpath('//*[@class="staff-­‐
title"]/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['school']=sel.xpath('//*[@class="course-­‐detail-­‐school	
  
item"]/a/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['courseCode']=sel.xpath('//*[@class="course-­‐detail-­‐
number	
  item"]/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['startDate']=sel.xpath('//*[@class="course-­‐detail-­‐
start	
  item"]/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['length']=sel.xpath('//*[@class="course-­‐detail-­‐length	
  
item"]/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['effort']=sel.xpath('//*[@class="course-­‐detail-­‐effort	
  
item"]/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['prereqs']=sel.xpath('//*[@class="course-­‐section	
  
course-­‐detail-­‐prerequisites-­‐full"]/p/text()').extract()	
  
	
  	
  	
  	
  	
  	
  	
  	
  item['video']=	
  
sel.xpath('/html/head/meta[@property="og:video"]/@content').extrac
t()	
  
	
  	
  	
  	
  	
  	
  	
  	
  return	
  item

Fig. 5. Parsing indiviudal course pages

Navigating to the Scrapy project folder in the terminal and
issuing the command “scrapy crawl edx –o items.json” yields a
JSON file containing all items scraped from edX. This works
for initializing our data although an item pipeline component
will need to be developed in the future to validate our scraped
data and check for duplicates.
C. RDF Data
The first section of our RDF (Fig. 6) invokes the
namespaces associated with (line 2) our ontology, (line 3)
RDF, (line 4) OWL, (line 5) XMLSchema, (line 6)
Schema.org and (line 7) RDFSchema. These namespaces are
used as prefixes to abbreviate URIs throughout the data.
<rdf:RDF xmlns="http://sebk.me/MOOC.owl#"
xml:base="http://sebk.me/MOOC.owl"
xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
xmlns:owl="http://www.w3.org/2002/07/owl#"
xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
xmlns:schema="http://schema.org"
xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntaxns#">
Fig. 6

RDF/XML namespace declarations

In Figure 7 line 1, Coursera subject URIs (rdf:about)
are set to “coursera_course_” concatenated with the course ID.
They are preceded by the ontology’s namespace explicitly.
Line 5 sets the type of resource to Course, a class provided by
our MOOC ontology. Data and object properties mapped from
Coursera
data
to
the
ontology
such
as
Recommended_Background are assigned values one by one.
These properties using a “schema” prefix where the property
is inherited from Schema.org and no prefix where the property
is borrowed from our ontology. Lines 4 6, and 8 link the
course to Coursera sessions, categories and instructors
respectively. These linked classes follow a similar subject URI
naming scheme to Coursera courses. They are also instantiated
in RDF/XML in the same way, but with their own properties.
<rdf:Description rdf:about="http://sebk.me/MOOC.owl#coursera_course_83">
<Course_Format>The class consists of lecture videos, 8 - 15 minutes in length. These
contain 2-3 integrated quiz questions per video. There are standalone quizzes each
week. Total lecture time is ~ 14 hours.&lt;br&gt;</Course_Format>
<Course_ID>83</Course_ID>
<hasSession>http://sebk.me/MOOC.owl#coursera_session_410</hasSession>
<rdf:type rdf:resource="http://sebk.me/MOOC.owl#Course"/>
<hasCategory>http://sebk.me/MOOC.owl#category_10</hasCategory>
<schema:name>Drugs and the Brain</schema:name>
<isTaughtBy>http://sebk.me/MOOC.owl#coursera_instructor_640696</isTaughtBy>
<schema:timeRequired>4-6 hours/week</schema:timeRequired>
<schema:inLanguage>en</schema:inLanguage>
<hasSession>http://sebk.me/MOOC.owl#coursera_session_971466</hasSession>
<Recommended_Background>Neuroscience, the most interdisciplinary science of the
21st century, receives inputs from many other fields of science, medicine, clinical
practice, and technology. Previous exposure to one or more of the subjects listed in
"Suggested Readings" will provide a good vantage point, as we introduce material
from these subjects.</Recommended_Background>
<hasCategory>http://sebk.me/MOOC.owl#category_3</hasCategory>
<schema:about>What happens in the body when a person smokes a cigarette? After
several weeks of smoking? When a person takes antidepressant or antipsychotic
medication? A drug for pain, migraine, or epilepsy? A recreational drug? Neuroscientists
are beginning to understand these processes. You’ll learn how drugs enter the brain, how
they act on receptors and ion channels, and how “molecular relay races” lead to changes
in nerve cells and neural circuits that far outlast the drugs themselves. “Drugs and the
Brain” also describes how scientists are gathering the knowledge required for the next
steps in preventing or alleviating Parkinson’s, Alzheimer’s, schizophrenia, and drug
abuse.</schema:about>
</rdf:Description>

Fig. 7. Sample of Coursera course data in RDF/XML

edX courses (Fig. 8) follow the same subject URI format as
Coursera’s with “edx_course_” appended to edX’s mixed
character and integer IDs. Note that on line 8 that the category
is in the 300s rather than the single or double-digit category
URIs seen in the previous RDF snippet. Although most of
edX’s courses are mapped to categories drawn from Coursera
JSON, some edX categories did not have an equivalent
category. As a result, additional edX categories were added to
our RDF with IDs starting at 300. Support to map relevant
courses from other providers to these new categories is
currently not implemented. An additional difference in this
data is that the session and instructor (lines 5 and 6) do not
have unique IDs. To accommodate this change, we append
course IDs to the subject URIs of the edX session and
instructor.
<rdf:Description rdf:about="http://sebk.me/MOOC.owl#edx_course_OEE101x">
<schema:name>Our Energetic Earth</schema:name>
<schema:video>http://www.youtube.com/v/lQc13bg2io?version=3&amp;amp;autohide=1</schema:video>
<schema:inLanguage>English</schema:inLanguage>
<hasSession>http://sebk.me/MOOC.owl#edx_session_OEE101x</hasSession>
<isTaughtBy>http://sebk.me/MOOC.owl#edx_instructor_OEE101x</isTaughtBy>
<schema:timeRequired>2-3 hours per week (6 weeks)</schema:timeRequired>

<hasCategory>http://sebk.me/MOOC.owl#category_308</hasCategory>
<Course_ID>OEE101x</Course_ID>
<Recommended_Background>None.</Recommended_Background>
<schema:about>&lt;span itemprop="description"&gt;&lt;h4&gt;
*Note - This is an Archived course*&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;This is a past/archived course. At this time, you can only
explore this course in a self-paced fashion. Certain features of this course may not be
active...</schema:about>
<rdf:type rdf:resource="http://sebk.me/MOOC.owl#Course"/>
</rdf:Description>

Fig. 8. Sample of edX course data in RDF/XML

Udacity follows the same subject URI scheme as
Coursera and edX but with the course ID provided by Udacity.
Similar to edX, instructors and sessions do not have their own
IDs. We once again use the course IDs in place of instructor
and session IDs for their subject URIs. We do not assign these
course IDs to Session_ID data properties however; this is to
avoid any conflicts in future SPARQL queries.
<rdf:Description rdf:about="http://sebk.me/MOOC.owl#udacity_course_cs259">
<Recommended_Background>&lt;p&gt;Basic knowledge of programming and Python
at the level of Udacity CS101 or better is required. Basic understanding of Objectoriented programming is helpful.&lt;/p&gt;</Recommended_Background>
<hasCategory>http://sebk.me/MOOC.owl#category_12</hasCategory>
<hasSession>http://sebk.me/MOOC.owl#udacity_session_cs259</hasSession>
<schema:name>Software_Debugging</schema:name>
<Course_ID>cs259</Course_ID>
<rdf:type rdf:resource="http://sebk.me/MOOC.owl#Course"/>
<schema:inLanguage>English</schema:inLanguage>
<schema:timeRequired>Assumes 6hr/wk</schema:timeRequired>
<schema:about>&lt;div&gt;&#xD;
&lt;p&gt;In this class you will learn how to debug programs systematically, how to
automate the debugging process and build several automated debugging tools in
Python.&lt;/p&gt;&#xD;
&lt;/div&gt;&lt;strong&gt;Why Take This Course?&lt;/strong&gt;&lt;div&gt;&#xD;
&lt;p&gt;At the end of this course you will have a solid understanding about systematic
debugging, will know how to automate debugging and will have built several functional
debugging tools in Python.&lt;/p&gt;&#xD;
&lt;/div&gt;&lt;strong&gt;Prerequisites
and
Requirements&lt;/strong&gt;&lt;div&gt;&#xD;
&lt;p&gt;Basic knowledge of programming and Python at the level of Udacity CS101 or
better is required. Basic understanding of Object-oriented programming is
helpful.&lt;/p&gt;&#xD;
&lt;/div&gt;&lt;p&gt;See the &lt;a href="https://www.udacity.com/tech-requirements"
target="_blank"&gt;Technology
Requirements&lt;/a&gt; for
using
Udacity&lt;/p&gt;</schema:about>
<schema:video>http://www.youtube.com/channel/UC0VOktSaVdm </schema:video>
<isTaughtBy>http://sebk.me/MOOC.owl#udacity_instructor_cs259</isTaughtBy>
</rdf:Description>

Fig. 9. Sample of Udacity course data in RDF/XML

D. UI Screenshots
MOOCLink’s home page, shown in Figure 10, is headed by
a navbar which contains links to “Providers”, “Subjects”, and
“Upcoming” pages. “Providers” drops down into a list of
Coursera, edX, and Udacity links when hovered over. Each of
these pages is a paginated list of courses from their respective
provider. “Subjects” leads to a table of links to our 36
categories. Clicking on one of these retrieves a paginated list of
courses in that category. “Upcoming” is an extension of the
starting soon table featured at the bottom of the home page,
listing courses starting within the next month. The last item on
the navbar is a search bar which retrieves courses by SPARQL
query, an example of which is shown in Figure 14.
The “Course Spotlight” is an image carousel which features
courses that might be popular with average users. Currently it
is a mix of introductory courses we have hand-picked. In the
future we hope to base these courses on statistics incorporated

into our RDF such as how many users have enrolled or
completed a specific course. This will depend on the future
availability of MOOC statistics via provider API or otherwise.
The course images in this carousel are retrieved from
YouTube. The IDs for these videos are taken from our RDF
which collects links to introductory videos. Where an ID is not
provided, a placeholder logo for the provider, such as the
Coursera logo in the rightmost course, is shown. Hovering over
a course prompts links to the MOOCLink course details page
and the introductory video. The course title, provider, start
date, as well as the beginning of the course summary are
provided below the course images.

Search results are shown in a 4-column image grid as
shown in Figure 11. The images of these courses are retrieved
by the same method as the home page carousel. Courses on the
search results page are filterable by course provider.
Checkboxes are provided below course titles to flag which
courses one would like to compare in more detail. In this
example, the bottom three courses, “Calculus: Single
Variable”, “Preparing for the AP Calculus AB and BC Exams”
and “Principles of Economics with Calculus” are flagged for
comparison.

Below the course spotlight, an image with a quote from
Daphne Koller is shown. On further scrolling (which creates a
parallax effect with the image) the “Starting Soon” section
follows with 7 courses starting in the next month. This is
followed by a link to our SPARQL endpoint and a footer
featuring a brief project description, links to related work as
well as our contact information.

Fig. 11.

Screenshot of search results

In Figure 12, the table for the comparison of courses is
shown. Information pertaining to each course is displayed as an
“accordion” in which clicking on a property opens the field for
each course being compared allowing for simple detail-bydetail comparison. Courses are once again filterable by course
provider and hovering over the course image brings up a link to
its corresponding MOOCLink “course details” page as well as
a link to enroll in the course.

Fig. 10.

Screenshot of MOOCLink home page

Fig 12.

Screenshot of course comparison table

A course page on MOOCLink, shown in Figure 13, is
headed by the course title and start date as well as a
breadcrumb for navigation. Next to the introductory video is
the same accordion found on the course comparison table,
listing relevant course properties. Below the video is the full
course summary and a link to enroll in the course on its
provider’s webpage.

syllabi details, we can perform a SPARQL query which
returns any course that mentions writing business plans
allowing the user to compare relevant courses side-by-side to
see that they enroll in the course that covers this topic most
extensively. We aim to incorporate more robust search such as
this example in future iterations of MOOCLink.
V. CONCLUSION AND FUTURE WORK
We have presented an extension of Schema.org’s Linked
Data vocabulary, which incorporates types and properties
found in online courses. The extended ontology allows for
assignment of data properties relevant to MOOCs as well as
object properties which link MOOC sessions, course details,
categories and instructors together.
Also presented is our approach to collecting and generating
Linked Data from three MOOC providers: Coursera, edX, and
Udacity. Using Coursera’s API and two Scrapy crawlers for
edX and Udacity, we collect MOOC data in JSON and convert
it to RDF with Apache Jena.

Fig 13.

Screenshot of course details page

In order to generate search results and retrieve course
properties from our RDF, SPARQL queries are performed.
These are called on our application server using Jena’s ARQ
API which queries the RDF from a separate Fuseki Server.
Currently we search for names in the RDF that contain the
words searched using regular expressions. Figure 14 illustrates
a search for “calculus”. All items in the RDF are selected
where they are of type mooc:Course and every course with a
schema:name or course title containing “calculus” (ignoring
case as defined by the regular expression filter) is returned.
PREFIX mooc: <http://sebk.me/MOOC.owl#>
PREFIX schema: <http://schema.org/>
SELECT * WHERE {
?course rdf:type mooc:Course.
?course schema:name ?iname.
FILTER (regex(?iname, "calculus", "i")).
}
Fig. 14.

Sample of a SPARQL query behind MOOCLink search

Although keyword search of course titles may yield
relevant results, there is more work to be done in order to take
advantage of the more extensive searches we can perform with
SPARQL. For example, we might want to know what courses
cover writing business plans. There are few courses which
concentrate around business plan writing or have “business
plan” in the title, although many business courses might have
lectures which cover it. Because our MOOC RDF contains

We describe a prototype implementation of MOOCLink, a
web application which utilizes the Linked MOOC Data to
allow users to discover and compare similar online courses. A
semantic web stack of Apache Tomcat as the web server and
servlet container, Fuseki as the SPARQL server, TDB as the
RDF store, and JavaServer Pages and Java Servlets to
dynamically create webpages is proposed to achieve this. The
functionality as well as the look and feel of the application are
highlighted with UI screenshots.
Our future work will focus on: incorporating demographic
data, reviews, developing an item pipeline for our crawlers,
automating website updates, enabling user profiles, course
tracks, natural language processing of syllabi and summaries
for more robust data and search.
ACKNOWLEDGMENTS
I would like to thank my mentor, Professor Srividya Bansal
of Arizona State University for her continual support. This
work is supported in part by the Distributed Research
Experience for Undergraduates (DREU) program, a joint
project of the CRA Committee on the Status of Women in
Computing Research (CRA-W) and the Coalition to Diversify
Computing (CDC).
REFERENCES
[1]

[2]
[3]

[4]
[5]
[6]

Bizer, C., Heath, T., & Berners-Lee, T. (2009). Linked data-the story so
far. International journal on semantic web and information systems,
5(3), 1-22. Chicago
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic
web. Scientific american, 284(5), 28-37.
Dietze, S., Yu, H. Q., Giordano, D., Kaldoudi, E., Dovrolis, N., & Taibi,
D. (2012, March). Linked Education: interlinking educational Resources
and the Web of Data. In Proceedings of the 27th Annual ACM
Symposium on Applied Computing (pp. 366-371). ACM.
Linked Up Challenge. (n.d.). LinkedUp Challenge. Retrieved July 22,
2014, from http://linkedup-challenge.org/
Pappano, L. (2012). The Year of the MOOC. The New York Times,
2(12), 2012. Chicago
Coursera Catalog API. (n.d.). - Coursera Technology. Retrieved July 22,
2014, from https://tech.coursera.org/app-platform/catalog/

[7]
[8]

[9]

[10]

[11]

[12]
[13]

[14]
[15]
[16]
[17]

Klyne, G., & Carroll, J. J. (2006). Resource description framework
(RDF): Concepts and abstract syntax.
Pérez, J., Arenas, M., & Gutierrez, C. (2006). Semantics and
Complexity of SPARQL. In The Semantic Web-ISWC 2006 (pp. 30-43).
Springer Berlin Heidelberg.
Nilsson, Mikael, Matthias Palmér, and Jan Brase. "The LOM RDF
binding: principles and implementation." Proceedings of the Third
Annual ARIADNE conference, Leuven Belgium, 2003. 2003.
IEEE Learning Object Metadata RDF binding. (n.d.). IEEE Learning
Object Metadata RDF Binding. Retrieved July 22, 2014, from
http://kmr.nada.kth.se/static/ims/md-lomrdf.html
Bohl, O., Scheuhase, J., Sengler, R., & Winand, U. (2002, December).
The sharable content object reference model (SCORM)-a critical review.
In Computers in education, 2002. proceedings. international conference
on (pp. 950-951). IEEE. Chicago
Schema.rdfs.org: a mapping of Schema.org to RDF. (n.d.). - Home.
Retrieved July 22, 2014, from http://schema.rdfs.org/
Learning Resource Metadata Initiative. (2013, April 9). :: World’s
Leading Search Engines Recognize LRMI as Education Metadata
Standard. Retrieved July 22, 2014, from http://www.lrmi.net/worldsleading-search-engines-recognize-lrmi-as-education-metadata-standard/
Coursera. (n.d.). Retrieved July 22, 2014, from http://coursera.org/
edX. (n.d.). Retrieved July 22, 2014, from http://edx.org/.
Udacity. (n.d.). Retrieved July 22, 2014, from http://udacity.com/
Learning Resource Metadata Initiative. (n.d.). :: The Specification.
Retrieved July 22, 2014, from http://www.lrmi.net/the-specification/

[18] A free, open-source ontology editor and framework for building
intelligent systems. (n.d.). protégé. Retrieved July 22, 2014, from
http://protege.stanford.edu/
[19] McGuinness, D. L., & Van Harmelen, F. (2004). OWL web ontology
language overview. W3C recommendation, 10(10), 2004.
[20] RDF Schema 1.1. (n.d.). RDF Schema 1.1. Retrieved July 23, 2014,
from http://www.w3.org/TR/rdf-schema/
[21] Requests: HTTP for Humans. (n.d.). Requests: HTTP for Humans —
Requests 2.3.0 documentation. Retrieved July 22, 2014, from
http://docs.python-requests.org/en/latest/
[22] Scrapy | An open source web scraping framework for Python. (n.d.).
Scrapy | An open source web scraping framework for Python. Retrieved
July 22, 2014, from http://scrapy.org/
[23] Apache Jena. (n.d.). Apache Jena. Retrieved July 22, 2014, from
https://jena.apache.org/
[24] google-gson - A Java library to convert JSON to Java objects and viceversa - Google Project Hosting. (n.d.). google-gson - A Java library to
convert JSON to Java objects and vice-versa - Google Project Hosting.
Retrieved July 22, 2014, from http://code.google.com/p/google-gson/
[25] Bootstrap. (n.d.). Bootstrap. Retrieved July 22, 2014, from
http://getbootstrap.com/
[26] TDB Architecture. (n.d.). Apache Jena. Retrieved July 22, 2014, from
https://jena.apache.org/documentation/tdb/architecture.html
[27] Google App Engine. (n.d.). URL Fetch API Overview. Retrieved July 22
2014, from https://developers.google.com/appengine/docs/java/urlfetch
[28] HTTP Authentication in ARQ. (n.d.). Apache Jena -. Retrieved July 22,
2014, from http://jena.apache.org/documentation/query/http-auth.html

IEEE Software

Vol. 16, no. 6, November/December 1999

The Guide to the
Software Engineering
Body of Knowledge

Pierre Bourque, Robert Dupuis, and Alain Abran,
University of Quebec at Montreal
James W. Moore, The MITRE Corporation
Leonard Tripp, The Boeing Company

. 1999 IEEE. Personal use of this material is permitted. However, permission to
reprint/republish this material for advertising or promotional purposes or for creating
new collective works for resale or redistribution to servers of lists, or to reuse any
copyrighted component of this work in other works must be obtained from the IEEE.

Repor ting on the SWEBOK project, the authors—who represent the
project’s editorial team—discuss the three-phase plan to characterize
a body of knowledge, a vital step toward developing soft ware
engineering as a profession.

The Guide to the
Software Engineering
Body of Knowledge
Pierre Bourque, Robert Dupuis, and Alain Abran,
University of Quebec at Montreal
James W. Moore, The MITRE Corporation
Leonard Tripp, The Boeing Company

he IEEE Computer Society and the Association for Computing
Machinery are working on a joint project to develop a guide to the
Software Engineering Body of Knowledge (SWEBOK). Articulating a
body of knowledge is an essential step toward developing a profession because it represents a broad consensus regarding the contents of the discipline. Without such a consensus, there is no way to validate a licensing examination, set a curriculum to prepare individuals for the examination, or formulate criteria
for accrediting the curriculum.
The SWEBOK project (http://www.swebok.org) is now nearing the end of the
second of its three phases. Here we summarize the results to date and provide an
overview of the project and its status.

T

0740-7459/99/$10.00 © 1999 IEEE

November/December 1999

IEEE Software

35

Table 1. The SWEBOK knowledge areas and their corresponding specialists.
Knowledge Area
Software configuration management
Software construction
Software design
Software engineering infrastructure
Software engineering management
Software engineering process
Software evolution and maintenance
Software quality analysis
Software requirements analysis
Software testing

Specialists
John A. Scott and David Nisse, Lawrence Livermore Laboratory, US
Terry Bollinger, The Mitre Corporation, US
Guy Tremblay, Université du Québec à Montréal, Canada
David Carrington, The University of Queensland, Australia
Stephen G. MacDonell and Andrew R. Gray, University of Otago, New Zealand
Khaled El Emam, National Research Council, Canada
Thomas M. Pigoski, Techsoft, US
Dolores Wallace and Larry Reeker, National Institute of Standards and Technology, US
Pete Sawyer and Gerald Kotonya, Lancaster University, UK
Antonia Bertolino, National Research Council, Italy

Table 2. Related Disciplines.
Cognitive sciences and human factors
Computer engineering
Computer science
Management and management science
Mathematics
Project management
Systems engineering

OBJECTIVES AND AUDIENCE
The SWEBOK project team established the project with five objectives:
1. Characterize the contents of the software engineering discipline.
2. Provide topical access to the software engineering body of knowledge.
3. Promote a consistent view of software engineering worldwide.
4. Clarify the place—and set the boundary—of
software engineering with respect to other disciplines such as computer science, project management, computer engineering, and mathematics.
5. Provide a foundation for curriculum development and individual certification material.
The product of the SWEBOK project will not be the
body of knowledge itself, but rather a guide to it.
The knowledge already exists; our goal is to gain
consensus on the core subset of knowledge characterizing the software engineering discipline.
To achieve these goals, we oriented the project
toward a variety of audiences. It aims to serve public and private organizations in need of a consistent
view of software engineering for defining education and training requirements, classifying jobs, and
developing performance evaluation policies. It also
addresses practicing software engineers and the officials responsible for making public policy regarding licensing and professional guidelines. In addi-

36

IEEE Software

November/December 1999

tion, professional societies and educators defining
the certification rules, accreditation policies for university curricula, and guidelines for professional
practice, as well as students learning the software
engineering profession, will benefit from SWEBOK.

THE GUIDE
The project comprises three phases: Strawman,
Stoneman, and Ironman. The Strawman guide, completed within nine months of project initiation, served
as a model for organizing the SWEBOK guide.1 Spring
2000 will see the completion of the Stoneman version, after which we’ll commence the Ironman phase,
which will continue for two or three years. Following
the principles of the Stoneman phase, Ironman will
benefit from more in-depth analyses, a broader review
process, and the experience gained from trial usage.
The SWEBOK Guide will organize the body of
knowledge into several Knowledge Areas. In its current draft, the Stoneman version of the Guide identifies
10 KAs—Table 1 lists the KA specialists responsible for
preparing each DA description. In addition, we’re considering seven related disciplines (see Table 2).
The distinction between KAs and related disciplines is important to the Guide’s purpose. The project will specify KAs—and topics within these KAs—
that are regarded as core knowledge for software
engineers. Software engineers should also know
material from the related disciplines, but the SWEBOK project will not attempt to specify that material. Instead, we’re leaving that to other efforts such
as those being coordinated by the Joint IEEE
Computer Society and ACM Software Engineering
Coordinating Committee, or the Working Group on
Software Engineering Education.2
As Figure 1 shows (and as the following sections explain), each KA description—which should
be around 10 pages—contains several important
components.

Hierarchical
breakdown of
topics

Hierarchical organization
The SWEBOK Guide will use a hierarchical organization to decompose each KA into a set of topics
with recognizable labels. A two- or three-level breakdown will provide a reasonable way for readers to
find topics of interest. The Guide will treat the selected topics in a manner compatible with major
schools of thought and with breakdowns generally
found in industry and in software engineering literature and standards. The breakdown of topics will
not presume particular application domains, business uses, management philosophies, development
methods, and so forth. The extent of each topic’s
description will be only that needed for the reader
to successfully find reference material. After all, the
Body of Knowledge is found in the reference materials, not in the Guide itself.
From the outset, the question arose as to the
depth of treatment the Guide should provide. After
substantial discussion, we adopted a concept of
generally accepted knowledge,3 which we had to distinguish from advanced and research knowledge
(on the grounds of maturity) and from specialized
knowledge (on the grounds of generality of application). The generally accepted knowledge applies
to most projects most of the time, and widespread
consensus validates its value and effectiveness.
However, generally accepted knowledge does
not imply that we should apply the designated
knowledge uniformly to all software engineering endeavors—each project’s needs determine that—but
it does imply that competent, capable software engineers should be equipped with this knowledge for
potential application. More precisely, generally accepted knowledge should be included in the study
material for a software engineering licensing examination that graduates would take after gaining four
years of work experience. Although this criterion is
specific to the US style of education and does not
necessarily apply to other countries, we deem it useful. However, both definitions of generally accepted
knowledge should be seen as complementary.
Additionally, the proposed breakdown must be
somewhat forward-looking—we’re considering not
only what is generally accepted today but also what
will be generally accepted in three to five years.
Reference materials and a matrix
The Guide will identify reference materials for
each KA. They might be book chapters, refereed papers, or any other well-recognized source of authoritative information—but the reference should

Matrix of topics
and reference
materials

Reference
materials

Topic
Classification Ratings References to
descriptions by Vincenti's by Bloom's
related
taxonomy
taxonomy
disciplines

Figure 1. The organization of a Knowledge
Area description.
be written in English and generally available. We
prefer material to which the IEEE Computer Society
or the ACM already has publication rights because
we want to make the references available on the
Internet without charge.
The Guide will also include a matrix that relates the
reference materials to the listed topics. Of course, a particular reference might apply to more than one topic.
Classification
To provide an alternative manner for viewing the
topics and connecting to other engineering disciplines, the Guide will classify the topics according
to the taxonomy of engineering design knowledge
that Walter Vincenti proposed in his 1990 history of
aeronautical engineering.4 The six categories of engineering design knowledge are fundamental design concepts, criteria and specifications, theoretical tools, quantitative data, practical considerations,
and approaches to problem solving.
Ratings
As an aid, notably to curriculum developers, the
Guide will also rate each topic with a set of pedagogical categories commonly attributed to Benjamin
Bloom. The concept is that educational objectives
can be classified into six categories representing increasing depth: knowledge, comprehension, application, analysis, synthesis, and evaluation (for Bloom’s
taxonomy, visit http://www.valdosta.peachnet.edu/
~whuitt/psy702/cogsys/bloom.html).
KAs from related disciplines
Each SWEBOK KA description will also identify
relevant KAs from related disciplines. Although these
KAs will be merely identified without additional description or references, they should aid curriculum
developers.

November/December 1999

IEEE Software

37

Guide to the Software Engineering Body of Knowledge

Software configuration
management
Management of the SCM
process
Software configuration
identification
Software configuration
control
Software configuration
status accounting
Software configuration
auditing

Software
construction
Linguistic construction
methods
Mathematical
construction methods
Visual construction
methods
Reduction of
complexity
Anticipation of diversity
Structuring for validation
Use of
external standards

Software release
management and delivery

(a)

Software
design

Software engineering
infrastructure

Basic concepts and
principles
Design quality and metrics

Software architecture

Design notations

Design strategies and
methods

(b)

(c)

Development methods
Heuristic methods
Formal methods
Prototyping methods
Software tools
Development and
maintenance tools
Supporting activities tools
Management tools
Workbenches: Integrated
CASE tools and software
engineering environments
Tool assessment techniques
Component integration
Component definition
Reference models
Reuse

(d)

Software engineering
management
Management
process
Coordination
Initiation and
scope definition
Planning
Execution
Review and
evaluation
Closure
Measurement

(e)

Figure 2. A mapping of the Guide to the Software Engineering Body of Knowledge.

THE KNOWLEDGE AREAS
The selection, titling, and descriptions of each
KA remain the subject of comment, review, and
amendment. Furthermore, some themes—such as
measurement, tools, and standards—cut across the
KAs and are currently treated separately in each.
These decisions will all be reviewed in subsequent
versions of the Guide. Here, in alphabetical order, we
describe the KAs as currently drafted. Figure 2 maps
out the 10 KAs and the important topics incorporated within them.
Software configuration management
We can define a system as a collection of components organized to accomplish a specific function or
set of functions. A system’s configuration is the function or physical characteristics of hardware, firmware,
software, or a combination thereof as set forth in
technical documentation and achieved in a product.
Configuration management, then, is the discipline
of identifying the configuration at discrete points in

38

IEEE Software

November/December 1999

time to systematically control its changes and to
maintain its integrity and traceability throughout the
system life cycle.
The concepts of configuration management
apply to all items requiring control, though there
are differences in implementation between hardware configuration management and software configuration management. The primary activities of
software configuration management are used as
the framework for organizing and describing the
topics of this KA. These primary activities are the
management of the software configuration management process; software configuration identification, control, status accounting, and auditing; and
software release management and delivery (see
Figure 2a).
Software construction
Software construction is a fundamental act of
software engineering; programmers must construct
working, meaningful software through coding, selfvalidation, and self-testing (unit testing). Far from

Software engineering
process
Basic concepts and definitions
Themes
Terminology
Process definition
Types of process definitions
Life cycle models
Life cycle process models
Notations for process
definitions
Process definition methods
Automation
Process evaluation
Methodology in process
measurement
Analytic paradigm
Benchmarking paradigm
Process implementation
and change
Paradigms for process
implementation and change
Infrastructure
Guidelines for process
implementation and change
Evaluating process
implementation and change
(f)

Software evolution
and maintenance
Maintenance concepts

Maintenance activities
and roles
Maintenance process

Organization aspect of
maintenance
Problems of software
maintenance

Software quality
analysis
Defining quality product
ISO 9126 quality
characteristics
Dependability
Quality facets related to
process and special
situations
Software quality analysis
Definition of quality analysis
Process plans
Activities and techniques
for quality analysis
Measurement in
software quality analysis

Software requirements
analysis

Software
testing

Requirements engineering
process

Basic concepts
and definitions

Requirements elicitation

Test levels

Requirements analysis

Test techniques

Requirements validation

Test-related measures

Requirements management

Organizing and
controlling
the test process
Automated testing

Maintenance cost and
maintenance cost estimation
Maintenance measurements

Tools and techniques
for maintenance

(g)

(h)

being a simple mechanistic translation of good design in working software, software construction burrows deeply into some of the most difficult issues
of software engineering.
The breakdown of topics for this KA adopts two
complementary views of software construction. The
first view comprises three major styles of software
construction interfaces: linguistic, mathematical,
and visual (see Figure 2b). For each style, topics are
listed according to four basic principles of organization that strongly affect the way software construction is performed: reducing complexity, anticipating diversity, structuring for validation, and
using external standards.
For example, the topics listed under anticipation
of diversity for linguistic software construction methods are information hiding, embedded documentation, complete and sufficient method sets, object-oriented class inheritance, creation of “glue” languages
for linking legacy components, table-driven software,
configuration files, and self-describing software and
hardware.

(i)

(j)

Software design
Design transforms requirements—typically stated
in terms relevant to the problem domain—into a description explaining how to solve the problem. It describes how the system is decomposed and organized into components, and it describes the interfaces
between these components. Design also refines the
description of these components into a level of detail suitable for initiating their construction.
Basic concepts and principles of software design
constitute the first subarea of this KA (see Figure 2c).
Design quality and metrics constitutes the second
subarea and is divided into quality attributes, quality
assurance, and metrics. Software architecture is the
next subarea and includes topics on structures and
viewpoints, architectural descriptions, patterns, and
object-oriented frameworks. It also includes a section
on architectural styles—an important notion in the
field of software architecture—which presents some
of the major styles various authors have identified.
The design notations subarea discusses notations for documenting a specific high-level design

November/December 1999

IEEE Software

39

or for producing a detailed system design. Design
strategies and methods constitute the last subarea,
and it contains four main topics: general strategies,
data-structure-centered design, function-oriented
design, and object-oriented design.
Software engineering infrastructure
This KA covers three subareas that cut across the
other KAs: development methods, software tools,
and component integration (see Figure 2d).
Development methods impose structure on software development and maintenance activity with
the goal of making the activity systematic and ultimately more successful. Methods usually provide a

Software engineering management
The software engineering management KA consists of both the management process and measurement subareas (see Figure 2e). While these two
areas are often regarded (and generally taught) as
being separate, and indeed they do possess many
mutually unique aspects, their close relationship motivates the combined treatment the Guide adopts.
In essence, management without measurement—
qualitative or quantitative—suggests a lack of rigor,
and measurement without management suggests
a lack of purpose or context.
The management process subarea considers the
notion of management “in the large”under the coordination topic, addressing issues such as
project selection, standards development
and implementation, project staffing, and
team development. It organizes the remaining topics according to stages in the
project development life cycle: initiation
and scope definition, planning (including schedule
and cost estimation and risk assessment), execution,
review and evaluation, and closure.
The measurement subarea addresses four topics: measurement program goals, measurement selection, data collection, and model development.
The first three topics are primarily concerned with
the theory and purpose behind measurement, and
they address issues such as measurement scales and
measure selection. Another issue included is the collection of measures, which involves both technical
issues (automated extraction) and human issues
(questionnaire design and responses to measurements being taken). The fourth topic (model development) is concerned with using both data and
knowledge to build models.

The emergence of software components as a
viable approach to software development
represents a maturing of the discipline.
notation and vocabulary, procedures for performing
identifiable tasks, and guidelines for checking both
the process and product. Development methods vary
widely in scope, from a single life-cycle phase to the
complete life cycle. The SWEBOK Guide will divide
this subarea into three nondisjointed main topics:
heuristic methods dealing with informal approaches,
formal methods dealing with mathematically based
approaches, and prototyping methods dealing with
approaches based on various forms of prototyping.
Software tools are the computer-based tools intended to assist the software engineering process.
Tools are often designed to support particular methods, reducing the administrative load associated
with applying the method manually. Like methods,
they are intended to make development more systematic, and they vary in scope from supporting individual tasks to encompassing the complete life
cycle. The top-level partitioning of the software tools
subarea distinguishes between development and
maintenance, supporting activities, and management tools. The remaining categories cover integrated tool sets (also known as software engineering environments) and tool assessment techniques.
The emergence of software components as a viable approach to software development represents
a maturing of the discipline to overcome the notinvented-here syndrome. The component integration subarea is partitioned into topics dealing with
individual components, reference models that describe how components can be combined, and the
more general topic of reuse.

40

IEEE Software

November/December 1999

Software engineering process
This KA covers the definition, implementation,
measurement, management, change, and improvement of software processes. The first subarea—basic
concepts and definitions—establishes the KA themes
and terminology (see Figure 2f ).
The purpose and methods for defining software
processes, as well as existing software process definitions and automated support, are described in
the process definition subarea. The topics of this
subarea are types of process definitions, life-cycle
models, life-cycle process models, notations for
process definitions, process definition methods, and
automation.
The process evaluation subarea describes the

approaches for the qualitative and quantitative
analysis of software processes. Measurement plays
an important role in process evaluation; therefore,
methodology in process measurement is this subarea’s first topic. Two general paradigms, analytic
and benchmarking, distinguish between types of
evaluations. The analytic paradigm relies on quantitative evidence to determine where improvements
are needed and whether an improvement initiative
has been successful. Under this paradigm falls qualitative evaluation, root-cause analysis, process simulation, orthogonal defect classification, experimental and observational studies, and personal
software process. The benchmarking paradigm depends on identifying an excellent organization in a
field and documenting its practices and tools.
Process assessment models and methods are the
two main topics listed under this paradigm.
The process implementation and change subarea describes the paradigms, infrastructure, and
critical success factors necessary for successful
process implementation and change. The topics of
this subarea are paradigms for process implementation and change, infrastructure, guidelines for
process implementation and change, and evaluating process implementation and change.
Software evolution and maintenance
Software maintenance is defined by IEEE Standard
1219-1998, IEEE Standard for Software Maintenance as
modifying a software product after delivery to correct
faults or improve performance or other
attributes, or to adapt the product to a
modified environment. However, software systems are rarely completed and
constantly evolve over time. Therefore,
this KA also includes topics relevant to
software evolution.
The maintenance concepts subarea defines maintenance, its basic concepts, and how the concept of
system evolution fits into software engineering (see
Figure 2g). It also explains the duties that maintainers perform. The maintenance activities and roles
subarea addresses the formal types of maintenance
and common activities. As with software development, the process is critical to the success and understanding of software evolution and maintenance.
The next subarea discusses standard maintenance
processes. Organizing maintenance might differ from
development; the subarea on organizational aspects
discusses the differences.
Software evolution and maintenance present

unique and different technical and managerial problems for software engineering, as addressed in the
problems of software maintenance subarea. Cost is
always a critical topic when discussing software evolution and maintenance. The subarea on maintenance cost and maintenance cost estimation concerns life-cycle costs as well as costs for individual
evolution and maintenance tasks. The maintenance
measurements subarea addresses the topics of quality and metrics. The final subarea, tools and techniques for maintenance, aggregates many subtopics
that the KA description otherwise fails to address.
Software quality analysis
Production of quality products is the key to customer satisfaction. Software without the requisite
features and degree of quality is an indicator of
failed (or at least flawed) software engineering.
However, even with the best software engineering
processes, requirement specifications can miss customer needs, code can fail to fulfill requirements,
and subtle errors can lie undetected until they cause
minor or major problems—even catastrophic failures. This KA therefore discusses the knowledge related to software quality assurance and software
verification and validation activities.
The goal of software engineering is a quality
product, but quality itself can mean different things.
Despite different terminology, there is some consensus about the attributes that define software
quality and dependability over a range of products.

Even with the best software engineering
processes, requirement specifications can
miss customer needs.
These definitions provide the base knowledge from
which individual quality products are planned, built,
analyzed, measured, and improved. The defining
quality products subarea discusses these definitions
(see Figure 2h).
Software quality assurance is a process designed
to assure a quality product; it is a planned and systematic pattern of all actions necessary to provide
adequate confidence that the product conforms to
specified technical requirements. Software verification and validation is a process that provides an
objective assessment of software products and
processes throughout the software life cycle; that is,
the verification and validation process lets management see into the product’s quality.

November/December 1999

IEEE Software

41

These two processes form the backbone of the software quality analysis subarea, which is divided into four
main topics: definition of quality analysis, process plans,
activities and techniques for quality analysis, and measurement in software quality analysis.
Software requirements analysis
The software requirements analysis KA is broken
down into five subareas that correspond approximately to process tasks that are often enacted concurrently and iteratively rather than sequentially
(see Figure 2i).
The requirements engineering process subarea
introduces the requirements engineering process,
orients the remaining four subareas, and shows how
requirements engineering dovetails with the overall software life cycle. This section also deals with
contractual and project organization issues.
The requirements elicitation subarea covers what
is sometimes termed requirements capture, discovery, or acquisition. It is concerned with where requirements come from and how they can be collected by the software engineer. Requirements
elicitation is the first stage in building an understanding of the problem the software must solve. It
is fundamentally a human activity, and it identifies
the stakeholders and establishes relationships between the development team and customer.
The requirements analysis subarea is concerned
with the process of analyzing requirements to detect and resolve conflicts between them, to discover
the boundaries of the system and how it must interact with its environment, and to elaborate user
requirements to software requirements.
The requirements validation subarea checks for
omissions, conflicts, and ambiguities and ensures
that the requirements follow prescribed quality
standards. The requirements should be necessary,
sufficient, and described in a way that leaves as little room as possible for misinterpretation.
The requirements management subarea spans
the whole software life cycle. It is fundamentally
about change management and maintaining the
requirements in a state that accurately mirrors the
software that will—or that has been—built.
Software testing
Software testing consists of dynamically verifying
a program’s behavior on a finite set of test cases—
suitably selected from the usually infinite domain of
executions—against the specified expected behavior. These and other basic concepts and definitions

42

IEEE Software

November/December 1999

constitute this KA’s first subarea (see Figure 2j).
This KA divides the test levels subarea into two
orthogonal breakdowns, the first of which is organized according to the traditional phases for testing
large software systems . The second breakdown concerns testing for specific conditions or properties.
The next subarea describes the knowledge relevant to several generally accepted test techniques.
It classifies these techniques as being either specification-based, code-based, fault-based, usage-based,
or specialized. The KA deals with test-related measures in their own subarea. The next subarea expands
on issues relative to organizing and controlling the
test process, including management concerns and
test activities. The automated testing subarea addresses existing tools and concepts related to automating the test process.

THE PROJECT
Since 1993, the IEEE Computer Society and the
ACM have cooperated in promoting the professionalization of software engineering through their joint
Software Engineering Coordinating Committee
(SWECC) (visit http://www.computer.org/tab/swecc).
The SWEBOK project’s scope, the variety of
communities involved, and the need for broad participation require full-time rather than volunteer
management. For this purpose, the SWECC contracted the Software Engineering Management
Research Laboratory at the University of Quebec
at Montreal to manage the effort. It operates
under SWECC supervision.
The project team developed two important principles for guiding the project: transparency and consensus. By transparency, we mean that the development process is itself documented, published,
and publicized so that important decisions and status are visible to all concerned parties. By consensus, we mean that the only practical method for legitimizing a statement of this kind is through broad
participation and agreement by all significant sectors of the relevant community. By the time we complete the Stoneman version of the Guide, literally
hundreds of contributors and reviewers will have
touched the product in some manner.
Project contributors
Like any software project, the SWEBOK project has
many stakeholders—some of which are formally represented. An Industrial Advisory Board, composed of

HOW

TO

CONTRIBUTE

TO THE

PROJECT

Those interested in participating in the third review cycle to be conducted on the entire Guide to the Software
Engineering Body of Knowledge can volunteer by signing up at the project’s Web site, http://www.swebok.org. The
transition from Stoneman to Ironman will be based primarily on feedback received from trial applications of the
Stoneman guide. Those interested in performing trial applications (contact Pierre Bourque at bourque.pierre@uqam.ca).

representatives from industry (Boeing, the National
Institute of Standards and Technology, the National
Research Council of Canada, Raytheon, and SAP LabsCanada) and professional societies (the IEEE Computer
Society and ACM), provides financial support for the
project. The IAB’s generous support permits us to
make the products of the SWEBOK project publicly
available without any charge (visit http://www.swebok.org). IAB membership is supplemented with related standards bodies (IEEE Software Engineering
Standards Committee and ISO/IEC JTC1/SC7) and related projects (the Computing Curricula 2001 initiative). The IAB reviews and approves the project plans,
oversees consensus building and review processes,
promotes the project, and lends credibility to the effort. In general, it ensures the relevance of the effort
to real-world needs.
We realize, however, that an implicit body of
knowledge already exists in textbooks on software
engineering. Thus, to ensure we correctly characterize the discipline, Steve McConnell, Roger Pressman,
and Ian Sommerville—the authors of three bestselling textbooks on software engineering—have
agreed to serve on a Panel of Experts, acting as a
voice of experience. In addition, the extensive review
process (described later) involves feedback from relevant communities. In all cases, we seek international
participation.
Normative literature
The project differs from previous efforts in its relationship to normative literature. Most of the software engineering literature provides information
useful to software engineers, but a relatively small
portion is normative. A normative document prescribes what an engineer should do rather than describing the variety of things that the engineer
might or can do. The normative literature is validated
by consensus formed among practitioners and is
concentrated in standards and related documents.
From the beginning, the SWEBOK project was

conceived as having a strong relationship to the normative literature of software engineering. The two
major standards bodies for software engineering are
represented in the project. In fact, a preliminary outline of KAs was based directly on the 17 processes
described in ISO/IEC 12207, Software Life Cycle
Processes. Ultimately, we hope that software engineering practice standards will contain principles
traceable to the SWEBOK Guide.
Reviews
We organized the development of the Stoneman
version into three public review cycles. The first review
cycle focused on the soundness and reasonableness of
the proposed breakdown of topics within each KA.
Thirty-four domain experts completed this review
cycle in April 1999. The reviewer comments, as well as
the identities of the reviewers, are available on the project’s Web site.
The second review cycle was organized around
the guidelines we originally gave to the KA specialists. A considerably larger group of professionals, organized into review viewpoints, answered a
detailed questionnaire for each KA description. The
viewpoints (for example, individual practitioners,
educators, and makers of public policy) were formulated to ensure relevance to the Guide’s various
intended audiences. The reviewer feedback collected in this review cycle, completed in October
1999, is also available on the project’s Web site. KA
specialists will document how reviewer feedback
was resolved in the KA descriptions.
The focus of the third review cycle will be on the
correctness and utility of the Guide. This review
cycle, currently scheduled to start in January 2000,
will be completed by individuals and organizations
representing a cross section of potential interest
groups (see the “How to contribute to the project”
sidebar). We’ve already recruited hundreds of professionals to review the entire Guide, and we’re soliciting more to fulfill our coverage objectives.

November/December 1999

IEEE Software

43

T

hroughout the project, the SWEBOK team has
ensured that there is always material available
to tangibly capture the project’s progress. Most of
this material is available publicly on the project’s
Web site. (Out of courtesy to the KA specialists, draft
material is withheld until completed.) The project
team is currently updating the KA descriptions
based on the results of the second review cycle. Early
in 2000, we’ll invite major professional societies and
the software engineering community to participate
in the third review cycle and comment on the entire
Guide. The completed Stoneman guide will then be
made available on the Web in Spring 2000, and, to
the extent possible, the cited reference materials will
also be made freely available.
Prior to developing the Ironman version of the
Guide, we’ll use the Stoneman guide in experimental application to provide feedback on its usability.
Although the extent of coverage is intended to be
identical, developing Ironman will involve a broader,
more exhaustive review process, based on feedback
❖
from trial usage of the Guide.

A CKNOWLEDGMENTS
The SWEBOK project team gratefully acknowledges the
support provided by the members of the Industrial Advisory
Board. Funding for this project is provided by the Association
for Computing Machinery, Boeing, the IEEE Computer
Society, the National Institute of Standards and Technology,
the National Research Council of Canada, Raytheon, and SAP
Labs (Canada). The team also appreciates the important
work performed by the KA specialists named in the article.
Finally, the team acknowledges the indispensable contribution of the reviewers who have participated so far.

R EFERENCES
1. P. Bourque et al., Guide to the Software Engineering Body of
Knowledge: A Strawman Version, University of Quebec at
Montreal, 1998; http://www.swebok.org (current Oct. 1999).
2. D.J. Bagert et al., Guidelines for Software Engineering Education,
Version 1.0, Software Eng. Inst., Carnegie Mellon Univ.,
Pittsburgh, Nov. 1999; http://www.sei.cmu.edu/collaborating/
ed/workgroup-ed.html (current Oct. 1999).
3. Project Management Institute, A Guide to the Project
Management Body of Knowledge, Upper Darby, Pa., 1996;
http://www.pmi.org/publictn/pmboktoc.htm (current Oct.
1999).
4. W. Vincenti, What Engineers Know and How They Know It:
Analytical Studies from Aeronautical History, The Johns Hopkins
Univ. Press, Baltimore, 1990.

44

IEEE Software

November/December 1999

About the Authors
Pierre Bourque is the director of applied
research at the Software Engineering
Management Research Laboratory at the
University of Quebec at Montreal. He is
also a coeditor of the SWEBOK project. He
has published and spoken internationally
on software measurement, functional
size measurement, project duration modeling, fundamental
principles of software engineering, software reengineering, IT
governance, and software engineering standards. He received
his MSc in mathematics (computer science) from the Université
de Sherbrooke. Contact him at bourque.pierre@uqam.ca.

Robert Dupuis is a professor and the director of four graduate programs, including the MSc program in software engineering at the University of Quebec at
Montreal. He is also a coeditor of the SWEBOK project. His teaching activities have
included software engineering, technology assessment, research methodology, computer ethics, and the diffusion of technology. His
main research interests include software engineering, the diffusion of end-user computing, and the diffusion and evaluation of
legal expert systems. He received his PhD in management sciences from the Université de Montpellier II, France. Contact him
at dupuis.robert@uqam.ca.

Alain Abran is a professor and the director of the Software Engineering
Management Research Laboratory at
the University of Quebec at Montreal. He
is also the coexecutive editor of the
SWEBOK project. His research interests
include functional size measurement,
software productivity and estimation
models, risk management, and software quality. He holds an
MS in management sciences and an MS in electrical engineering, both from the University of Ottawa, and a PhD in software
engineering from the École Polytechnique de Montréal. He is
actively involved in international software engineering standards and cochairs the Common Software Measurement
International Consortium. Contact him at abran.alain@uqam.ca.

James W. Moore’s biography appears in his article on page 57.

Leonard Tripp’s biography appears in the Guest Editors’
Introduction on page 18.

Readers can contact the authors in care of James W. Moore at
The MITRE Corp., 1820 Dolley Madison Blvd., W534, McLean,
VA 22102; james.w.moore@ieee.org.

USDL: A Service-Semantics Description Language for Automatic
Service Discovery and Composition.∗
Srividya Kona, Ajay Bansal, Luke Simon,
Ajay Mallya, and Gopal Gupta
University of Texas at Dallas
Richardson, TX 75083

Thomas D. Hite
Metallect Corp
2400 Dallas Parkway
Plano, TX 75093

Abstract
For web-services to become practical, an infrastructure needs to be supported that allows
users and applications to discover, deploy, compose, and synthesize services automatically. This
automation can take place only if a formal description of the web-services is available. In
this paper we present an infrastructure using USDL (Universal Service-Semantics Description
Language), a language for formally describing the semantics of web-services. USDL is based on
the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding
the meaning of services. USDL can be regarded as formal service documentation that will allow
sophisticated conceptual modeling and searching of available web-services, automated service
composition, and other forms of automated service integration. A theory of service substitution
using USDL is presented. The rationale behind the design of USDL along with its formal
specification in OWL is presented with examples. We also compare USDL with other approaches
like OWL-S, WSDL-S, and WSML and show that USDL is complementary to these approaches.

1

Introduction

A web-service is a program available on a web-site that “effects some action or change” in the world
(i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because
of a plane reservation made over the Internet, a device being controlled, etc. The next milestone
in the Web’s evolution is making services ubiquitously available. As automation increases, these
web-services will be accessed directly by the applications themselves rather than by humans. In
this context, a web-service can be regarded as a “programmatic interface” that makes application
to application communication possible. An infrastructure that allows users to discover, deploy,
synthesize and compose services automatically needs to be supported in order to make web-services
more practical.
To make services ubiquitously available we need a semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery,
deployment, composition and synthesis. Several efforts are underway to build such an infrastructure. These efforts include approaches based on the semantic web (such as OWL-S [5]) as well as
those based on XML, such as Web Services Description Language (WSDL [7]). Approaches such
∗

This is an expanded version of the paper ‘A Universal Service-Semantics Description Language’ that appeared
in European Conference On Web Services, 2005 [15] and received its best paper award.

1

as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. In
this paper we present an approach that is based on semantics. Our approach can be regarded as
providing semantics to WSDL statements. We present the design of a language called Universal
Service-Semantics Description Language (USDL) which service developers can use to specify formal semantics of web-services [14, 15]. Thus, if WSDL can be regarded as a language for formally
specifying the syntax of web services, USDL can be regarded as a language for formally specifying
their semantics. USDL can be thought of as formal service documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated composition, and
other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically
discover services. That is, given a formal description of the context in which a service is needed,
the service(s) that will precisely fulfill that need can be determined. The directory can then be
searched for the exact service, or two or more services that can be composed to synthesize the
required service, etc.
To provide formal semantics, a common denominator must be agreed upon that everybody can
use as a basis of understanding the meaning of services. This common conceptual ground must
also be somewhat coarse-grained so as to be tractable for use by both engineers and computers.
That is, semantics of services should not be given in terms of low-level concepts such as Turing
machines, first-order logic and their variants, since service description, discovery, and synthesis
then become tasks that are practically intractable and theoretically undecidable. Additionally,
the semantics should be given at a conceptual level that captures common real world concepts.
Furthermore, it is too impractical to expect disparate companies to standardize on application (or
domain) specific ontologies to formally define semantics of web-services, and instead a common
universal ontology must be agreed upon with additional constructors. Also, application specific
ontologies will be an impediment to automatic discovery of services since the application developer
will have to be aware of the specific ontology that has been used to describe the semantics of the
service in order to frame the query that will search for the service. The danger is that the service
may not be defined using the particular domain specific ontology that the application developer
uses to frame the query, however, it may be defined using some other domain specific ontology, and
so the application developer will be prevented from discovering the service even though it exists.
These reasons make an ontology based on OWL WordNet [2, 8] a suitable candidate for a universal
ontology of basic concepts upon which arbitrary meets and joins can be added in order to gain
tractable flexibility.
We describe the meaning of conceptual modeling and how it could be obtained via a common
universal ontology based on WordNet in the next section. Section 3, gives a brief overview of
how USDL attempts to semantically describe web-services. In section 4, we discuss precisely how
a WSDL document can be prescribed meaning in terms of WordNet ontology. Section 5 gives a
complete USDL annotation for a Hotel-Reservation service. In section 6 we present the theoretical
foundations of service description and substitution in USDL. Automatic discovery of web-services
using USDL is discussed in section 7. Composition of web-services using USDL is discussed in
section 8. Comparison of USDL with other approaches like OWL-S and WSML is discussed in
section 9. Section 10 shows related work. Finally, conclusions and future work are addressed in the
last section.

2

2

A Universal Ontology

To describe service semantics, we should agree on a common ground to model our concepts. We
can describe what any given web-service does from first principles using approaches based on logic.
This is the approach taken by frameworks such as dependent type systems and programming logics
prevalent in the field of software verification where a “formal understanding” of the service/software
is needed in order to verify it. However, such solutions are both low-level, tedious, and undecidable
to be of practical use. Instead, we are interested in modeling higher-level concepts. That is, we
are more interested in answering questions such as, what does a service do from the end user’s or
service integrator’s perspective, as opposed to the far more difficult questions, such as, what does the
service do from a computational view? We care more about real world concepts such as “customer”,
“bank account”, and “flight itinerary” as opposed to the data structures and algorithms used by
a service to model these concepts. The distinction is subtle, but is a distinction of granularity as
well as a distinction of scope.
In order to allow interoperability and machine-readability of our documents, a common conceptual ground must be agreed upon. The first step towards this common ground are standard
languages such as WSDL and OWL. However, these do not go far enough, as for any given type
of service there are numerous distinct representations in WSDL and for high-level concepts (e.g., a
ternary predicate), there are numerous disparate representations in terms of OWL, representations
that are distinct in terms of OWL’s formal semantics, yet equal in the actual concepts they model.
This is known as the semantic aliasing problem: distinct syntactic representations with distinct
formal semantics yet equal conceptual semantics. For the semantics to equate things that are conceptually equal, we need to standardize a sufficiently comprehensive set of basic concepts, i.e., a
universal ontology, along with a restricted set of connectives.
Industry specific ontologies along with OWL can also be used to formally describe web-services.
This is the approach taken by the OWL-S language [5]. The problem with this approach is that it
requires standardization and undue foresight. Standardization is a slow, bitter process, and industry
specific ontologies would require this process to be iterated for each specific industry. Furthermore,
reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing
is even more difficult. Undue foresight is required because many useful web services will address
innovative applications and industries that don’t currently exist. Standardizing an ontology for
travel and finances is easy, as these industries are well established, but new innovative services in
new upcoming industries also need be ascribed formal meaning. A universal ontology will have no
difficulty in describing such new services.
We need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual
level to common real world concepts. WordNet [8] is a sufficiently comprehensive ontology that
meets these criteria. As stated, part of the common ground involves standardized languages such as
OWL. For this reason, WordNet cannot be used directly, and instead we make use of an encoding of
WordNet as an OWL base ontology [2]. Using an OWL WordNet ontology allows our solution to use
a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which
we map web service messages and operations. As long as this mapping is precise and sufficiently
expressive, reasoning can be done within the realm of OWL by using automated inference systems
(such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes relationships between ontological
concepts, especially subsumption (hyponym-hypernym) and equivalence (synonym) relationships.

3

3

USDL: An Overview

As mentioned earlier, USDL can be regarded as a language to formally specify the semantics of
web-services. It is perhaps the first attempt to capture the semantics of web-services in a universal,
yet decidable manner. It is quite distinct from previous approaches such as WSDL and OWL-S
[5]. As mentioned earlier, WSDL only defines syntax of the service; USDL provides the missing
semantic component. USDL can be thought of as a formal language for service documentation.
Thus, instead of documenting the function of a service as comments in English, one can write USDL
statements that describe the function of that service. USDL is quite distinct from OWL-S, which
is designed for a similar purpose, and as we shall see the two are in fact complementary. OWL-S
primarily describes the states that exist before and after the service and how a service is composed
of other smaller sub-services (if any). Description of atomic services is left under-specified in OWLS. They have to be specified using domain specific ontologies; in contrast, atomic services are
completely specified in USDL, and USDL relies on a universal ontology (OWL WordNet Ontology)
to specify the semantics of atomic services. USDL and OWL-S are complementary in that OWL-S’s
strength lies in describing the structure of composite services, i.e., how various atomic services are
algorithmically combined to produce a new service, while USDL is good for fully describing atomic
services. Thus, OWL-S can be used for describing the structure of composite services that combine
atomic services described using USDL.
USDL describes a service in terms of portType and messages, similar to WSDL. The semantics
of a service is given using the OWL WordNet ontology: portType (operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly
negated) concepts in the OWL WordNet ontology. The semantics is given in terms of how a service
affects the external world. The present design of USDL assumes that each side-effect is one of
following four operations: create, update, delete, or find. A generic affects side-effect is used when
none of the four apply. An application that wishes to make use of a service automatically should
be able to reason with WordNet atoms using the OWL WordNet ontology.
We also define the formal semantics of USDL. As stated earlier, the syntactic terms describing
portType and messages are mapped to disjunctions of conjunctions of (possibly negated) OWL
WordNet ontological terms. A service is then formally defined as a function, labeled by the sideeffect. The main contribution of our work is the design of a universal service-semantics description
language (USDL), along with its formal semantics, and a theory of service substitution using it.

4

Design of USDL

The design of USDL rests on two formal languages: Web Services Description Language (WSDL)
[7] and Web Ontology Language (OWL) [6]. The Web Services Description Language (WSDL) [7]
is used to give a syntactic description of the name and parameters of a service. The description
is syntactic in the sense that it describes the formatting of services on a syntactic level of method
signatures, but is incapable of describing what concepts are involved in a service and what a service
actually does, i.e., the conceptual semantics of the service. Likewise, the Web Ontology Language
(OWL) [6], was developed as an extension to the Resource Description Framework (RDF) [3],
both standards are designed to allow formal conceptual modeling via logical ontologies, and these
languages also allow for the markup of existing web resources with semantic information from the
conceptual models. USDL employs WSDL and OWL in order to describe the syntax and semantics
4

of web-services. WSDL is used to describe message formats, types, and method prototypes, while a
specialized universal OWL ontology is used to formally describe what these messages and methods
mean, on a conceptual level.
USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL
documents contain two main constructs to which we want to ascribe conceptual meaning: messages
and portType. These constructs are aggregates of service components which will be directly ascribed meaning. Messages consist of typed parts and portType consists of operations parameterized
on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes,
which have properties with values in the OWL WordNet ontology.

4.1

Concept

USDL defines a generic class called Concept which is used to define the semantics of parts of
messages.
<owl:Class rdf:ID="Concept">
<rdfs:comment>Generic class of USDL Concept</rdfs:comment>
<owl:unionOf rdf:parseType="Collection">
<owl:Class rdf:about="#BasicConcept"/>
<owl:Class rdf:about="#QualifiedConcept"/>
<owl:Class rdf:about="#InvertedConcept"/>
<owl:Class rdf:about="#ConjunctiveConcept"/>
<owl:Class rdf:about="#DisjunctiveConcept"/>
</owl:unionOf>
</owl:Class>

The USDL Concept class denotes the conceptual objects constructed from the OWL WordNet
ontology. For most purposes, message parts and other WSDL constructs will be mapped to a
subclass of USDL Concept so that useful concepts can be modeled as set theoretic formulas of
union, intersection, and negation of basic concepts. These subclasses of Concept are BasicConcept,
QualifiedConcept, InvertedConcept, ConjunctiveConcept, and DisjunctiveConcept.
4.1.1

Basic Concept

An BasicConcept is the actual contact point between USDL and WordNet. This class acts as proxy
for WordNet lexical entities.
<owl:Class rdf:about="#BasicConcept">
<rdfs:subClassOf rdf:resource="#Concept"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#isA"/>
<owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">
1
</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

The property cardinality restrictions require all USDL BasicConcepts to have exactly one defining value for the isA property. An instance of BasicConcept is considered to be equated with a
WordNet lexical concept given by the isA property.

5

<owl:ObjectProperty rdf:ID="isA">
<rdfs:domain rdf:resource="#BasicConcept"/>
<rdfs:range rdf:resource="&wn;LexicalConcept"/>
</owl:ObjectProperty>

4.1.2

Qualified Concept

A QualifiedConcept is a concept classified by another lexical concept.
<owl:Class rdf:about="#QualifiedConcept">
<rdfs:subClassOf rdf:resource="#Concept"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#isA"/>
<owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">
1
</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#ofKind"/>
<owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">
1
</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

The property cardinality restrictions require all USDL QualifiedConcepts to have exactly one
defining value for the isA property, and exactly one value for the ofKind property. An instance of
QualifiedConcept is considered to be equated with a lexical concept given by the isA property and
classified by a lexical concept given by the optional ofKind property.
<owl:ObjectProperty rdf:ID="isA">
<rdfs:domain rdf:resource="#QualifiedConcept"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:ID="ofKind">
<rdfs:domain rdf:resource="#QualifiedConcept"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

4.1.3

Inverted Concept

In case of InvertedConcept the corresponding semantics are the complement of USDL concepts.
<owl:Class rdf:about="#InvertedConcept">
<rdfs:subClassOf rdf:resource="#Concept"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasConcept"/>
<owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">
1

6

</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
<owl:ObjectProperty rdf:ID="hasConcept">
<rdfs:domain rdf:resource="#Concept"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

4.1.4

Conjunctive and Disjunctive Concept

The ConjunctiveConcept and DisjunctiveConcept respectively denote the intersection and union of
USDL Concepts.
<owl:Class rdf:about="#ConjunctiveConcept">
<rdfs:subClassOf rdf:resource="#Concept"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasConcept"/>
<owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger">
2
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
<owl:Class rdf:about="#DisjunctiveConcept">
<rdfs:subClassOf rdf:resource="#Concept"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasConcept"/>
<owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger">
2
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

The property cardinality restrictions on ConjunctiveConcept and DisjunctiveConcept allow for
n-ary intersections and unions (where n ≥ 2) of USDL concepts. For generality, these concepts are
either BasicConcepts, QualifiedConcepts, ConjunctiveConcepts, DisjunctiveConcepts, or InvertedConcepts.

4.2

Affects

The affects property is specialized into four types of actions common to enterprise services: creates,
updates, deletes, and finds.
<owl:ObjectProperty rdf:ID="affects">
<rdfs:comment>
Generic class of USDL Affects
</rdfs:comment>
<rdfs:domain rdf:resource="#Operation"/>
<rdfs:range rdf:resource="#Concept"/>

7

</owl:ObjectProperty>
<owl:ObjectProperty rdf:about="#creates">
<rdfs:subPropertyOf rdf:resource="#affects"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:about="#updates">
<rdfs:subPropertyOf rdf:resource="#affects"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:about="#deletes">
<rdfs:subPropertyOf rdf:resource="#affects"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:about="#finds">
<rdfs:subPropertyOf rdf:resource="#affects"/>
</owl:ObjectProperty>

Note that each of these specializations inherits the domain and range of the affects property.
Most services can be described using one of these types of effects. For those services that cannot be
described in terms of these specializations, the parent affects property can be used instead which
is described as an USDL concept.

4.3

Conditions and Constraints

Services may have some external conditions (pre-conditions and post-conditions) specified on the
input or output parameters. Condition class is used to describe all such constraints. Conditions
are represented as conjunction or disjunction of binary predicates. Predicate is a trait or aspect of
the resource being described.
<owl:Class rdf:ID="Condition">
<rdfs:comment>
Generic class of USDL Condition
</rdfs:comment>
<owl:unionOf rdf:parseType="Collection">
<owl:Class rdf:about="#AtomicCondition"/>
<owl:Class rdf:about="#ConjunctiveCondition"/>
<owl:Class rdf:about="#DisjunctiveCondition"/>
</owl:unionOf>
</owl:Class>
<owl:Class rdf:about="#AtomicCondition">
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasConcept"/>
<owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">
1
</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#onPart"/>
<owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">

8

1
</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasValue"/>
<owl:maxCardinality rdf:datatype="&xsd;nonNegativeInteger">
1
</owl:maxCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

A condition has exactly one value for the onPart property and at most one value for the hasValue
property, each of which is of type USDL Concept.
<owl:ObjectProperty rdf:ID="onPart">
<rdfs:domain rdf:resource="#AtomicCondition"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:ID="hasValue">
<rdfs:domain rdf:resource="#AtomicCondition"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

4.3.1

Conjunctive and Disjunctive Conditions

The ConjunctiveCondition and DisjunctiveCondition respectively denote the conjunction and disjunction of USDL Conditions.
<owl:Class rdf:about="#ConjunctiveCondition">
<rdfs:subClassOf rdf:resource="#Condition"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasCondition"/>
<owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger">
2
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
<owl:Class rdf:about="#DisjunctiveCondition">
<rdfs:subClassOf rdf:resource="#Condition"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasCondition"/>
<owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger">
2
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

9

<owl:ObjectProperty rdf:ID="hasCondition">
<rdfs:domain rdf:resource="#Concept"/>
<rdfs:range rdf:resource="#Condition"/>
</owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveCondition and DisjunctiveCondition allow
for n-ary conjunctions and disjunctions (where n ≥ 2) of USDL conditions. In general any n-ary
condition can be written as a combination of conjunctions and disjunctions of binary conditions.

4.4

Messages

Services communicate by exchanging messages. As mentioned, messages are simple tuples of actual
data, called parts. Take for example, a flight reservation service similar to the SAP ABAP Workbench Interface Repository for flight reservations [4], which makes use of the following message.
<message name="#ReserveFlight_Request">
<part name="#CustomerName" type="xsd:string">
<part name="#FlightNumber" type="xsd:string">
<part name="#DepartureDate" type="xsd:date">
...
</message>

The USDL surrogate for a WSDL message is the Message class, which is a composite entity
with zero or more parts. Note that for generality, messages are allowed to contain zero parts.
<owl:Class rdf:ID="Message">
<rdfs:comment>
Generic class of USDL Message
</rdfs:comment>
<owl:unionOf rdf:parseType="Collection">
<owl:Class rdf:about="#Input"/>
<owl:Class rdf:about="#Output"/>
</owl:unionOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasPart"/>
<owl:minCardinality
rdf:datatype="&xsd;nonNegativeInteger">
0
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
<owl:Class rdf:about="#Input">
<rdfs:subClassOf rdf:resource="#Message"/>
</owl:Class>
<owl:Class rdf:about="#Output">
<rdfs:subClassOf rdf:resource="#Message"/>
</owl:Class>

Each part of a message is simply a USDL Concept, as defined by the hasPart property. Semantically messages are treated as tuples of concepts.

10

<owl:ObjectProperty rdf:ID="hasPart">
<rdfs:domain rdf:resource="#Message"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

Continuing our example flight reservation service, the ReserveF lightRequest message is given
semantics using USDL as follows, where &wn;customer and &wn;name are valid XML references to
WordNet lexical concepts.
<Message rdf:about="#ReserveFlight_Request">
<hasPart rdf:resource="#CustomerName"/>
<hasPart rdf:resource="#FlightNumber"/>
<hasPart rdf:resource="#DepartureDate"/>
</Message>
<QualifiedConcept rdf:about="#CustomerName">
<isA rdf:resource="#Name"/>
<ofKind rdf:resource="#Customer"/>
</QualifiedConcept>
<BasicConcept rdf:about="#Name">
<isA rdf:resource="&wn;name"/>
</BasicConcept>
<BasicConcept rdf:about="#Customer">
<isA rdf:resource="&wn;customer"/>
</BasicConcept>
<!-- Similarly concepts FlightNumber and DepartureDate are defined -->

4.5

PortType

A service consists of portType, which is a collection of procedures or operations that are parametric
on messages. Our example flight reservation service might contain a portType definition for a flight
reservation service that takes as input an itinerary and outputs a reservation receipt.
<portType rdf:about="#Flight_Reservation">
<hasOperation rdf:resource="#ReserveFlight">
</portType>
<operation rdf:about="#ReserveFlight">
<hasInput rdf:resource="#ReserveFlight_Request"/>
<hasOutput rdf:resource="#ReserveFlight_Response"/>
<creates rdf:resource="#FlightReservation" />
</operation>

The USDL surrogate is defined as the class portType which contains zero or more Operations
as values of the hasOperation property.
<owl:Class rdf:about="#portType">
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource=#hasOperation"/>
<owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger">
0
</owl:minCardinality>

11

</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
<owl:ObjectProperty rdf:ID="hasOperation">
<rdfs:domain rdf:resource="#portType"/>
<rdfs:range rdf:resource="#Operation"/>
</owl:ObjectProperty>

As with the case of messages, portTypes are not directly assigned meaning via the OWL WordNet ontology. Instead the individual Operations are described by their side-effects via an affects
property. Note that the parameters of an operation are already given meaning by ascribing meaning
to the messages that constitute the parameters.
<owl:Class rdf:about="#Operation">
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasInput"/>
<owl:minCardinality
rdf:datatype="&xsd;nonNegativeInteger">
0
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasOutput"/>
<owl:minCardinality
rdf:datatype="&xsd;nonNegativeInteger">
0
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#affects"/>
<owl:minCardinality
rdf:datatype="&xsd;nonNegativeInteger">
1
</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

An operation can have one or more values for the affects property, all of which are of type
USDL Concept, which is the target of the effect.
<owl:ObjectProperty rdf:ID="hasInput">
<rdfs:domain rdf:resource="#Operation"/>
<rdfs:range rdf:resource="#Input"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:ID="hasOutput">
<rdfs:domain rdf:resource="#Operation"/>
<rdfs:range rdf:resource="#Ouput"/>
</owl:ObjectProperty>

12

<owl:ObjectProperty rdf:ID="affects">
<rdfs:domain rdf:resource="#Operation"/>
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

5

Semantic Description of a Service

This section shows an example syntactic description of a web-service using WSDL and its corresponding semantic description using USDL.
Hotel Reservation Service: The service described here is a simplified hotel-reservation service
published in a web-service registry. This service can be treated as atomic: i.e., no interactions
between buying and selling agents are required, apart from invocation of the service and receipt
of its outputs by the buyer. Given certain inputs and pre-conditions, the service provides certain
outputs and has specific effects.
This service takes in a HotelChain, StartDate, NumNights, NumPersons, NumRooms, FirstName, and LastName as input parameters. It has a few input pre-conditions that NumNights,
NumRooms must be greater than zero and StartDate must be greater than today. This service
outputs a Reservation at the end of transaction.

5.1

WSDL definition

The following is WSDL definition of the service. This service provides a single operation called
ReserveHotel. The input and output messages are defined below. The conditions on the service
cannot be described using WSDL.
<definitions ...>
<portType name="ReserveHotel_Service">
<operation name="ReserveHotel">
<input message="ReserveHotel_Request"/>
<output message="ReserveHotel_Response"/>
</operation>
</portType>
<message name="ReserveHotel_Request">
<part name="HotelChain" type="xsd:string"/>
<part name="StartDate" type="xsd:date"/>
<part name="NumNights" type="xsd:integer"/>
<part name="NumPersons" type="xsd:integer"/>
<part name="NumRooms" type="xsd:integer"/>
<part name="FirstName" type="xsd:integer"/>
<part name="LastName" type="xsd:integer"/>
</message>
<message name="ReserveHotel_Response">
<part name="Reservation" type="xsd:string"/>
</message>
...
</definitions>

13

5.2

USDL annotation

The following is the complete USDL annotation
corresponding to the above mentioned WSDL
description. The input pre-condition and the
global constraint on the service are also described
semantically.
<definitions>
<portType rdf:about=
"#ReserveHotel_Service">
<hasOperation rdf:resource=
"#ReserveHotel"/>
</portType>

<QualifiedConcept rdf:about="#StartDate">
<isA rdf:resource="#Date"/>
<ofKind rdf:resource="#Start"/>
</QualifiedConcept>
<QualifiedConcept rdf:about="#TodaysDate">
<isA rdf:resource="#Date"/>
<ofKind rdf:resource="#Today"/>
</QualifiedConcept>
<!-- Similarly we can define Qualified
Concepts for #NumNights, #NumPersons,
#NumRooms, #FirstName and #LastName -->
<BasicConcept rdf:about="#Hotel">
<isA rdf:resource="&wn;hotel"/>
</BasicConcept>

<operation rdf:about="#ReserveHotel">
<hasInput rdf:resource=
"#ReserveHotel_Request"/>
<hasOutput rdf:resource=
"#ReserveHotel_Response"/>
<creates rdf:resource=
"#HotelReservation"/>
</operation>

<BasicConcept rdf:about="#Chain">
<isA rdf:resource="&wn;chain"/>
</BasicConcept>
<BasicConcept rdf:about="#Start">
<isA rdf:resource="&wn;start"/>
</BasicConcept>

<Message rdf:about=
"#ReserveHotel_Request">
<hasPart rdf:resource="#HotelChain"/>
<hasPart rdf:resource="#StartDate"/>
<hasPart rdf:resource="#NumNights"/>
<hasPart rdf:resource="#NumPersons"/>
<hasPart rdf:resource="#NumRooms"/>
<hasPart rdf:resource="#FirstName"/>
<hasPart rdf:resource="#LastName"/>
</Message>

<BasicConcept rdf:about="#Date">
<isA rdf:resource="&wn;date"/>
</BasicConcept>
<BasicConcept rdf:about="#greaterThan">
<isA rdf:resource="&wn;greater_than"/>
</BasicConcept>
<BasicConcept rdf:about="#Date">
<isA rdf:resource="&wn;date"/>
</BasicConcept>

<Message rdf:about=
"#ReserveHotel_Response">
<hasPart rdf:resource=
"#HotelReservation"/>
</Message>

<BasicConcept rdf:about="#Today">
<isA rdf:resource="&wn;today"/>
</BasicConcept>

<Condition rdf:about="#greaterThanToday">
<hasConcept rdf:resource="#greaterThan"/>
<onPart rdf:resource="#StartDate"/>
<hasValue rdf:resource="#TodaysDate"/>
</Condition>
<!-- Similarly we can define Condition
#greaterThanZero on parts #NumRooms
and #NumNights -->

<BasicConcept rdf:about="#Reservation">
<isA rdf:resource="&wn;reservation"/>
</BasicConcept>
<!-- Similarly we can define Basic
Concepts for #nights, #rooms,
#number, #persons, #name, etc. -->

</definitions>
<QualifiedConcept rdf:about="#HotelChain">
<isA rdf:resource="#Chain"/>
A Book-Buying Service example is presented
<ofKind rdf:resource="#Hotel"/>
in
[15].
</QualifiedConcept>

14

6

Theory of Substitution of Services

Next, we will investigate the theoretical aspects of USDL. This involves concepts from set theory.
From a systems integration perspective, an engineer is interested in finding (discovering) a service
that accomplishes some necessary task. Of course, such a service may not be present in any service
directory. In such a case the discovery software should return a set of services that can be used in a
context expecting a service that meets that description (of course, this set may be empty). To find
services that can be substituted for a given service that is not present in the directory, we need to
develop a theory of service substitutability. We develop such a theory in this section. Our theory
relates service substitutability to WordNet’s semantic relations.
In order to develop this theory, we must first formally define constructs such as USDL-described
concepts, affects, conditions and services, which we will also call concepts, affects, conditions and
services for short. While it is possible to work directly with the XML USDL syntax, doing so is
cumbersome and so we will instead opt for set theoretic notation.
Definition 1 (Set of WordNet Lexemes)
Let Ω be the set of WordNet lexemes. The following semantic relations exist on elements of Ω.
1. Synonym: A pair of WordNet Lexemes having the same or nearly the same meaning have
the synonym relation. Example, ‘purchase’ is a synonym of ‘buy’.
2. Antonym: A pair of WordNet Lexemes having the opposite meaning have the antonym
relation. Example, ‘start’ is an antonym of ‘end’.
3. Hyponym: A word that is more specific than a given word is called the subordinate or
hyponym of the other. Example, ‘car’ is a hyponym of ‘vehicle’.
4. Hypernym: A word that is more generic than a given word is called the super-ordinate or
hypernym of the other. Example, ‘vehicle’ is a hypernym of ‘car’.
5. Meronym: A word that names a part of a larger whole is a meronym of the whole. Example,
‘roof’ and ‘door’ are meronyms of ‘house’.
6. Holonym: A word that names the whole of which a given word is a part is a holonym of the
part. Example, ‘house’ is a holonym for ‘roof’ and ‘door’.
Definition 2 (Representation of USDL Concepts)
1. A Basic Concept c = x, where x is a WordNet lexeme, defines the values of isA property.
Example, customer is a Basic Concept and a WordNet lexeme.
2. A Qualified Concept c = (X, Y ), where X, Y are USDL concepts, defines the values of isA and
ofKind properties. Example, concept FlightNumber is a number of kind flight represented as
(number, flight).
3. An Inverted Concept c is represented as ¬X where X is an USDL concept. Example, concept
not a customer name can be represented as ¬(name, customer ).
4. Let X, Y be USDL Concepts.
(i) Conjunctive Concept c is represented as X ∧ Y . Example, concept EvenRationalNumber
is represented as (number, even) ∧ (number, rational ).
(ii) Disjunctive Concept c is represented as X∨Y . Example, concept OrderNumber/AvailabilityMessage is represented as (number, order ) ∨ (message, availability).
Definition 3 (Universe of USDL Concepts)
15

Let Θ be the set of USDL concepts. Θ can be inductively constructed as follows:
1. x ∈ Ω implies x ∈ Θ
2. X, Y ∈ Θ implies (X, Y ) ∈ Θ
3. X ∈ Θ implies ¬X ∈ Θ
4. X, Y ∈ Θ implies X ∨ Y ∈ Θ
5. X, Y ∈ Θ implies X ∧ Y ∈ Θ
Definition 4 (Semantic relations of Basic Concepts)
Semantic relations hold between two Basic concepts if their corresponding WordNet lexemes have
the same semantic relation in Ω. For example, Basic Concept Purchase is a synonym of Basic
Concept Buy.
Definition 5 (Synonym and Antonym relation of Qualified Concepts)
Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2 ∈ Θ.
1. C1 is synonym of C2 if X1 is recursively a synonym of X2 and Y1 is recursively a synonym of
Y2 .
2. If X1 = w1 and X2 = w2 where w1 , w2 ∈ Ω, then X1 is synonym of X2 if w1 and w2 have the
synonym relation in Ω.
For example, Qualified Concept (date, begin) is a synonym of Qualified Concept (date, start). Similarly we can determine the antonym relation between Qualified Concepts. For example, Qualified
Concept (date, begin) is a antonym of Qualified Concept (date, end ).
Definition 6 (Hyponym and Hypernym relation of Qualified Concepts)
Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2 ∈ Θ.
1. C1 is hypernym of C2 if any one of the following holds:
(i) X1 is recursively a hypernym of X2 and Y1 is recursively a hypernym of Y2 .
(ii) X1 is recursively a hypernym of X2 and Y1 is recursively a synonym of Y2 .
(iii) X1 is recursively a synonym of X2 and Y1 is recursively a hypernym of Y2 .
2. If X1 = w1 and X2 = w2 where w1 , w2 ∈ Ω, then X1 is hypernym of X2 if w1 and w2 have
the hypernym relation in Ω.
For example, Qualified Concept (number, vehicle) is a hypernym of Qualified Concept (number,
car ). Similarly we can determine the hyponym relation of Qualified Concepts. For example,
Concept (number, car ) is a hyponym of (number, vehicle).
Definition 7 (Holonym and Meronym relation of Qualified Concepts)
Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2 ∈ Θ.
1. C1 is meronym of C2 if any one of the following holds:
(i) X1 is recursively a meronym of X2 and Y1 is recursively a meronym of Y2 .
(ii) X1 is recursively a meronym of X2 and Y1 is recursively a synonym of Y2 .
(iii) X1 is recursively a synonym of X2 and Y1 is recursively a meronym of Y2 .
2. If X1 = w1 and X2 = w2 where w1 , w2 ∈ Ω, then X1 is meronym of X2 if w1 and w2 have
the meronym relation in Ω.

16

For example, Qualified Concept (door, brown) is a meronym of Qualified Concept (house, brown).
Similarly we can determine the holonym relation of Qualified Concepts. For example, Qualified
Concept (house, brown) is a holonym of Qualified Concept (door, brown).
Definition 8 (Semantic relations between Inverted Concepts)
Let C1 and C2 be two Inverted concepts where C1 = ¬X1 and C2 = ¬X2 .
1. C1 is a synonym of C2 if X1 and X2 are synonyms.
2. C1 is an antonym of C2 if X1 and X2 are antonyms.
3. C1 is a hypernym of C2 if X1 and X2 are hyponyms and vice versa.
4. C1 is a meronym of C2 if X1 and X2 are holonyms and vice versa.
For example, Inverted Concept ¬(date, begin) is a synonym of Inverted Concept ¬(date, start).
The synonym-antonym relation, hyponym-hypernym relation and meronym-holonym relation can
be extended to Conjunctive and Disjunctive concepts.
Definition 9 (Semantic relations between Conjunctive (resp., Disjunctive) Concepts)
Let C1 and C2 be two Conjunctive (resp., Disjunctive) concepts where C1 = X1 ∧ Y1 and C2 =
X2 ∧ Y2 .
1. C1 is a synonym of C2 if X1 is a synonym of X2 and Y1 is a synonym of Y2 OR X1 is a
synonym of Y2 and Y1 is a synonym of X2 .
2. C1 is a hypernym of C2 if one of the following holds:
(i) X1 is a hypernym of X2 and Y1 is a hypernym/synonym of Y2
(ii) X1 is a hypernym/synonym of X2 and Y1 is a hypernym of Y2
(iii) X1 is a hypernym of Y2 and Y1 is a hypernym/synonym of X2 .
(iv) X1 is a hypernym/synonym of Y2 and Y1 is a hypernym of X2 .
For example, Conjunctive Concept (vehicle, blue) ∧ (vehicle, automatic) is a hypernym of (car, blue)
∧ (car, automatic). Similar to the above defined hypernym relation, we can define the antonym,
hyponym, meronym, and holonym relations between Conjunctive (resp., Disjunctive) Concepts.
Definition 10 (Substitution of Concepts)
1. Exact Substitution: For any concepts C, C 0 ∈ Θ, if C is a synonym of C 0 , then C is
the exact substitutable of C 0 and C can safely be used in a context expecting concept C 0 .
Example, concept Purchase is an exact substitutable of concept Buy.
2. Generic Substitution: For any concepts C, C 0 ∈ Θ, if C is a hypernym of C 0 , then C is
the generic substitutable of C 0 and C can safely be used in a context expecting concept C 0
or a super-ordinate of C 0 . Example, concept (number, vehicle) is a generic substitutable of
concept (number, car ).
3. Specific Substitution: For any concepts C, C 0 ∈ Θ, if C is a hyponym of C 0 , then C is the
specific substitutable of C 0 and C can safely be used in a context expecting concept C 0 or
a sub-ordinate of C 0 . Example, concept (number, car ) is a specific substitutable of concept
(number, vehicle).
4. Part Substitution: For any concepts C, C 0 ∈ Θ, if C is a meronym of C 0 , then C is the part
substitutable of C 0 and C can safely be used in a context expecting a concept that is a part
of C 0 . Example, concept Roof is a part substitutable of concept House.

17

5. Whole Substitution: For any concepts C, C 0 ∈ Θ, if C is a holonym of C 0 , then C is the
whole substitutable of C 0 and C can safely be used in a context expecting a concept that is
a whole of C 0 . Example, concept House is a whole substitutable of concept Roof.
Definition 11 (Representation of Affects)
Let Γ = {(L, E) | L ∈ (Ψ∪Θ), E ∈ Θ} be the set of USDL side-effects, where Ψ = {creates, updates,
deletes, f inds}, L is the affect type and E is the affected object. The affect type could be one of
the pre-defined affects from Ψ or a generic effect which is described as a concept.
Definition 12 (Substitution of Affects)
USDL affect is represented as a pair where the first element is the affect type and second element
is the affected object. Both affect type and the affected object are described as USDL concepts.
Let A1 and A2 be two affects where A1 = (L1 , E1 ) and A2 = (L2 , E2 ). A1 can safely be used in
a context expecting affect A2 if all of the following hold:
1. Concept L1 is substitutable for L2
2. Concept E1 is substitutable for E2 .
These substitutables can be of kind Exact, Generic, Specific, Part, or Whole which also determines
the kind of substitution of the affect A1 in a context expecting A2 . Example, affect (finds, VehicleNumber ) is a generic substitution of affect (lookup, CarNumber ) as concept finds is an exact
substitutable of lookup and concept VehicleNumber is a generic substitutable of concept CarNumber.
Definition 13 (Representation of Conditions)
Let Φ = {(P, Arg1 , Arg2 ) | P, Arg1 , Arg2 ∈ Θ} be the set of USDL conditions. P is the constraint
which is either a binary or a unary predicate. Arg1 is the concept on which the predicate acts and
Arg2 is the concept which represents a value. Arg2 is an optional parameter.
Definition 14 (Substitution of Conditions)
USDL condition is represented as a tuple made up of the constraint or predicate and two arguments.
The constraint and the arguments are described as USDL concepts.
Let C1 and C2 be two conditions where C1 = (P1 , F irstArg1 , SecondArg1 ) and C2 = (P2 ,
F irstArg2 , SecondArg2 ). C1 can safely be used in a context expecting condition C2 if all of the
following hold:
1. Concept P1 is substitutable for P2
2. Concept F irstArg1 is substitutable for F irstArg2
3. Concept SecondArg1 is substitutable for SecondArg2 .
These substitutables can be of kind Exact, Generic, Specific, Part, or Whole which also determines the kind of substitution of the condition C1 in a context expecting C2 . Example, condition (greaterThan, NumberOfNights, 0) is an exact substitution of condition (moreThan, NumberOfNights, 0).
Definition 15 (Representation of a Web Service)

18

For any set S, let S ∗ = { |  ∈
/ S} ∪ {(x, y) | x ∈ S, y ∈ S ∗ } be the set of lists over S.
Let Σ be the set of USDL service descriptions represented in the form of terms. The USDL
description of a web service consists of
1. A list of Inputs, I ∈ Θ∗
2. A list of Outputs, O ∈ Θ∗
3. A list of Pre-Conditions, Pre-Condition ∈ Φ∗
4. A list of Post-Conditions, Post-Condition ∈ Φ∗
5. Side-effects, (affect-type, affected-object) ∈ Γ
USDL service description can be treated as a term of first-order logic [16]. The side-effect of
a service comprises of an affect type and the affected object. The service can be converted into a
triple as follows:
(Pre-Conditions, affect-type(affected-object, I, O), Post-Conditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that
changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions
are the conditions on the input parameters and Post-Conditions are the conditions on the output
parameters of the service.
We represent services as triples so that they can be treated as terms in first-order logic. The
first-order logic unification algorithm [16] then can be extended to specialized unifications for exact,
generic, specific, part and whole substitutions. This work is in progress [10].
Now that the formal definitions of concept, affects, conditions and service descriptions are
given, we would like to extend the theory of substitutability over Σ so that we can reason about
substitutability of services.
Definition 16 (Substitution of a Web Service)
Let σ and σ 0 be two services where
σ is represented as (Pre-Condition, affect-type(affected-object, I, O), Post-Condition) and
σ 0 is represented as (Pre-Condition0 , affect-type0 (affected-object0 , I 0 , O0 ), Post-Condition0 ).
σ can safely be used in a context expecting service σ 0 if all of the following hold:
1. Pre-Condition is substitutable for Pre-Condition 0
2. If the terms affect-type(affected-object, I, O) and affect-type0 (affected-object0 , I 0 , O0 ) can be
unified by applying an extended unification algorithm. The unification mechanism applied is
different based on the kind of substitution needed.
3. Post-Condition is substitutable for Post-Condition 0
Definition 17
For any services S1 , S2 ∈ Σ, we say S1  S2 if S1 is a substitutable of S2 based on one of the
WordNet semantic relations.
Thus far our notions of service substitutability are based on the six WordNet semantic relations
discussed earlier. However, one can define the notion of service substitutability independently
using the actual semantics (e.g., denotational semantics) of the program that realizes this service.
Consider a service S1 with inputs I1 and outputs O1 , and another service S2 with inputs I2 and
outputs O2 ; we ignore the side-effects of these services for the moment. The ideal conditions
under which service S1 can be substituted for service S2 is the following: I1 v I2 and O1 w O2 .
19

Essentially, the inputs needed by S1 must be present in the inputs being provided in anticipation
of availability of S2 . Likewise, the outputs produced by service S1 should contain the outputs
anticipated from service S2 . In such a case, S1 can be directly substituted for S2 . There can be
other types of general substitution relation defined. However, for these other types of substitutions,
the code of the service being used for substitution may have to be modified or wrappers placed
around it.
One can, however, develop a more general notion of substitutability based on denotational
semantics [17]. Let [[S1 ]] and [[S2 ]] be the semantic denotations of programs that implement
services S1 and S2 respectively (note that the side-effects of these services will be captured as the
state that becomes an argument in a denotational definition). Note that [[S1 ]] and [[S2 ]] can be
regarded as points in a complete partial order [17] that represents the space of all functions. Service
S1 can be substituted for S2 if [[S1 ]] and [[S2 ]] lie in the same chain in the complete partial order
(i.e., either [[S1 ]] v [[S2 ]] or [[S2 ]] v [[S1 ]] where v is the relation that induces the complete partial
order among denotations of the services).
Given the definition of substitutability based on denotational semantics, one can prove the
soundness and completeness of our notion of substitutability based on the WordNet semantic relations, i.e.,
S1  S2 ⇒ [[S1 ]] v [[S2 ]] (soundness)
[[S1 ]] v [[S2 ]] ⇒ S1  S2 (completeness)
Intuitively, one can see that these relationships hold, since the  relation is defined in terms
of subsumption of terms describing the service’s inputs and outputs and its effect (create, update,
delete, find and the generic affects). These soundness and completeness proofs are not included
here due to lack of space.

7

Service Discovery

Now that our theory of service substitutability has been developed, it can be used to build tools
for automatically discovering services as well as for automatically composing them. It can also be
used to build a service search engine that discovers matching services and ranks them.
We assume that a directory of services has already been compiled, and that this directory
includes a USDL description document for each service. Inclusion of the USDL description, makes
service directly “semantically” searchable. However, we still need a query language to search this
directory, i.e., we need a language to frame the requirements on the service that an application
developer is seeking. USDL itself can be used as such a query language. A USDL description of
the desired service can be written, a query processor can then search the service directory to look
for a “matching” service.
A discovery engine gets USDL descriptions from a service directory and converts them into
terms of logic. The terms corresponding to the USDL query can be compared with the terms
from the directory using an extended/special unification algorithm. Depending on the type of
match required, the unification mechanism could be different. That is, the matching or unification
algorithm used can look for an exact, generic, specific, part or a whole match depending on the
desire of the user. Part and Whole substitutions are not useful while looking for matching services,
but are very useful while selecting services for service composition. Also using Part or Whole
substitutions for discovery may produce undesired side-effects.

20

The discovery engine can also rank the various services discovered. In this scenario, the discovery engine returns a list of substitutable services after applying ranking based on the kind of
match obtained. Exact substitutables are assigned the highest rank among the different kind of
substitutables. The following is the default ranking order used for the different substitutions.
1. Exact Substitution: The matching service obtained is equivalent to the service in the query.
2. Generic Substitution: The matching service obtained subsumes the service in the query.
3. Specific Substitution: The matching service obtained is subsumed by the service in the query.
4. Whole Substitution: The matching service obtained is a composite service of the service in
the query and some other services.
5. Part Substitution: The matching service obtained is a part of a composite service that the
query describes.
The development of a service discovery engine based on these ideas is in progress.
With the USDL descriptions and query language in place, numerous applications become possible ranging from querying a database of services to rapid application development via automated
integration tools and even real-time service composition [12]. Take our flight reservation service
example. Assume that somebody wants to find a travel reservation service and that they query a
USDL database containing general purpose flight reservation services, bus reservation services, etc.
One could then form a USDL query consisting of a description of a travel reservation service and
the database could respond with a set of travel reservation services whether it be via flight, bus,
or some other means of travel. This flexibility of generalization and specialization is gained from
semantic information provided by USDL.

8

Service Composition

For service composition, the first step is finding the set of composable services. USDL itself can be
used to specify the requirements of the composed service that an application developer is seeking.
Using the discovery engine, individual services that make up the composed service can be selected.
Part substitution technique can be used to find the different parts of a whole task and the selected
services can be composed into one by applying the correct sequence of their execution. The correct
sequence of execution can be determined by the pre-conditions and post-conditions of the individual
services. That is, if a subservice S1 is composed with subservice S2 , then the postconditions of S1
must imply the preconditions of S2 .
In fact, the WordNet Universal ontology can also be helpful in automatically discovering services
that can be composed together to satisfy a service discovery query. To achieve this, the discovery engine looks at the USDL concepts that describe the service in the query. It then searches
the WordNet ontology to find out the meronymous components of that concept. The services
that exactly match the meronymous components are then discovered using the standard discovery
mechanism. Preconditions and postcondition consistency is then used to find the order in which
the meronymous components should be stitched together to produce the desired service.
A service composition engine of this kind is under development. Such an engine can also aid a
systems integrator in rapidly creating composite services, i.e., services consisting of the composition
of already existing services. In fact, such an engine can also be extended to automatically generate boilerplate code to manage the composite service, as well as menial inter-service data format
conversions needed to glue the meronymous components together.

21

9

Comparison with OWL-S, WSDL-S, and WSML

In this section we present a comparison of USDL with other popular approaches such as OWL-S
[5], WSML [1], and WSDL-S [24]. Our goal is to identify the similarities and differences of USDL
with these approaches. OWL-S is a service description language which attempts to address the
problem of semantic description via a highly detailed service ontology. But OWL-S also allows for
complicated combining forms, which seem to defeat the tractability and practicality of OWL-S.
The focus in the design of OWL-S is to describe the structure of a service in terms of how it
combines other sub-services (if any used). The description of atomic services in OWL-S is left
under-specified [9]. OWL-S includes the tags presents to describe the service profile, and the tag
describedBy to describe the service model. The profile describes the (possibly conditional) states
that exist before and after the service is executed. The service model describes how the service is
(algorithmically) constructed from other simpler services. What the service actually accomplishes
has to be inferred from these two descriptions in OWL-S. Given that OWL-S uses complicated
combining forms, inferring the task that a service actually performs is, in general, undecidable. In
contrast, in USDL, what the service actually does is directly described (via the verb affects and its
refinements create, update, delete, and find).
OWL-S recommends that atomic services be defined using domain specific ontologies. Thus,
OWL-S needs users describing the services and users using the services to know, understand and
agree on domain specific ontologies in which the services are described. Hence, annotating services
with OWL-S is a very time consuming, cumbersome, and invasive process. The complicated nature
of OWL-S’s combining forms, especially conditions and control constructs, seems to allow for the
aforementioned semantic aliasing problem [9]. Other recent approaches such as WSMO, WSML,
WSDL-S, etc., suffer from the same limitation [1]. In contrast, USDL uses the universal WordNet
ontology to solve this problem.
Note that USDL and OWL-S can be used together. A USDL description can be placed under
the describedBy tag for atomic processes, while OWL-S can be used to compose atomic USDL
services. Thus, USDL along with WordNet can be treated as the universal ontology that can make
an OWL-S description complete. USDL documents can be used to describe the semantics of atomic
services that OWL-S assumes will be described by domain specific ontologies and pointed to by
the OWL-S describedBy tag. In this respect, USDL and OWL-S are complementary: USDL can be
treated as an extension to OWL-S which makes OWL-S description easy to write and semantically
more complete.
OWL-S can also be regarded as the composition language for USDL. If a new service can be
built by composing a few already existing services, then this new service can be described in OWL-S
using the USDL descriptions of the existing services. Next, this new service can be automatically
generated from its OWL-S description. The control constructs like Sequence and If-Then-Else of
OWL-S allows us to achieve this. Note once a composite service has been defined using OWL-S
that uses atomic services described in USDL, a new USDL description must be written for this
composite service (automatic generation of this description is currently being investigated [10]).
This USDL description is the formal documentation of the new composite service and will make it
automatically searchable once the new service is placed in the directory service. It also allows this
composite service to be treated as an atomic service by some other application.
For example, the aforementioned ReserveFlight service which creates a flight reservation can be
viewed as a composite process of first getting the flight details, then checking the flight availabil-

22

ity and then booking the flight (creating the reservation). If we have these three atomic services
namely GetFlightDetails, CheckFlightAvailability and BookFlight then we can create our ReserveFlight service by composing these three services in sequence using the OWL-S Sequence construct.
The following is the OWL-S description of the composed ReserveFlight service.
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:process="http://www.daml.org/services/owl-s/1.0/Process.owl#">
<process:CompositeProcess rdf:ID="ReserveFlight">
<process:composedOf>
<process:Sequence>
<process:components rdf:parseType="Collection">
<process:AtomicProcess rdf:about="#GetFlightDetails"/>
<process:AtomicProcess rdf:about="#CheckFlightAvailability"/>
<process:AtomicProcess rdf:about="#BookFlight"/>
</process:components>
</process:Sequence>
</process:composedOf>
</process:CompositeProcess>
</rdf:RDF>

We can generate this composed ReserveFlight service automatically. The component services
can be discovered from existing services using their USDL descriptions. Once we have the component services, the OWL-S description can be used to generate the new composed service.

10

Related Work

Discovery and composition of web services has been active area of research recently [18, 19, 20, 21,
22, 23]. Most of these approaches are based on capturing the formal semantics of the service using an
action description languages or some kind of logic (e.g., description logic). The service composition
problem is reduced to a planning problem where the sub-services constitute atomic actions and the
overall service desired is represented by the goal to be achieved using some combination of atomic
actions. A planner is then used to determine the combination of actions needed to reach the goal.
In contrast, we rely more on WordNet (which we use as a universal ontology) and the meronymous
relationships of WordNet lexemes to achieve automatic composition. The approaches proposed by
others also rely on a domain specific ontology (specified on OWL/DAML), and thus suffer from the
problem mentioned earlier, namely, to discover/compose such services the discovery/composition
engine has to be aware of the domain specific ontology. Thus, completely general discovery and
composition engines cannot be built. Additionally, the domain specific ontology has to be quite
extensive in that any relationship that can possibly exist between two terms in the ontology must be
included in the ontology. In contrast, in our approach, the complex relationships (USDL concepts)
that might be used to describe services or their inputs and outputs are part of USDL descriptions
and not the ontology. Note that our approach is quite general, and it will work for domain specific
ontologies as well, as long as the synonym, antonym, hyponym, hypernym, meronym, and holonym
relations are defined between the various terms of the domain specific ontology.
Another related area of research involves message conversation constraints, also known as behavioral signatures [13]. Behavior signature models do not stray far from the explicit description
of the lexical form of messages, they expect the messages to be lexically and semantically correct
prior to verification via model checking. Hence behavior signatures deal with low-level functional
23

implementation constraints, while USDL deals with higher-level real world concepts. However,
USDL and behavioral signatures can be regarded as complementary concepts when taken in the
context of real world service composition and both technologies are currently being used in the
development of a commercial services integration tool [12].

11

Conclusions and Future Work

To reliably catalogue, search and compose services in a semi-automatic to fully-automatic manner we need standards to publish and document services. This requires language standards for
specifying not just the syntax, i.e., prototypes of service procedures and messages, but it also necessitates a standard formal, yet high-level means for specifying the semantics of service procedures
and messages. We have addressed these issues by defining a universal service-semantics description
language, its semantics, and we have proved some useful properties about this language. The current version of USDL incorporates current standards in a way to further aid markup of IT services
by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. This
approach is more practical and tractable than other approaches because description documents are
more easily created by humans and more easily processed by computers. USDL is currently being
used to formally describe web-services related to emergency response functions [11].
Our current and future work involves the application of USDL to formally describing commercial
service repositories (for example SAP Interface Repository and services listed in UDDI), as well
as to service discovery and rapid application development (RAD) in commercial environments
[12]. Current and future work also includes automatically generating USDL description from the
code/documentation of a service [12] as well developing tools that will allow automatic generation
of new services based on combining USDL descriptions of existing atomic services. The interesting
problem that arises then: can USDL description of such automatically generated services be also
automatically generated? This problem is also part of our current/future work.

References
[1] A conceptual comparison between WSMO and OWL-S. www.wsmo.org/TR/d4/d4.1/v0.1.
[2] Ontology-based information management system, wordnet OWL-Ontology. http://taurus.
unine.ch/knowler/wordnet.html.
[3] Resource Description Framework. http://www.w3.org/RDF.
[4] SAP Interface Repository. http://ifr.sap.com/catalog/query.asp.
[5] Semantic markup for web services. www.daml.org/services/owl-s/1.0/owl-s.html.
[6] Web Ontology Language Reference. http://www.w3.org/TR/owl-ref.
[7] Web Services Description Language. http://www.w3.org/TR/wsdl.
[8] WordNet: A Lexical Database for the English Language. www.cogsci.princeton.edu/~wn.
[9] S. Balzer, T. Liebig, and M. Wagner. Pitfalls of OWL-S - a practical semantic web use case.
In ICSOC, 2004.
[10] S. Kona, A. Bansal, G. Gupta, and T. Hite. Automatic Service Discovery and Composition
with USDL. Working paper, 2006.
24

[11] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. D. Harris, and J. C. Staves. Towards
Intelligent Services: A case study in chemical emergency response. In International Conference
on Web Services, pp. 751-758, 2005.
[12] T. Hite. Service Composition and Ranking: A strategic overview. Internal Report, Metallect
Inc., 2005.
[13] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004.
[14] L. Simon, A. Bansal, A. Mallya, S. Kona, G. Gupta, and T. Hite. Towards a Universal Service
Description Language. In Next Generation Web Services Practices, pp. 175-180, 2005.
[15] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal ServiceSemantics Description Language. In European Conference On Web Services, pp. 214-225,
2005.
[16] J.W. Lloyd. Foundations of Logic Programming. Springer-Verlag, 1987.
[17] D. Schmidt. Denotational Semantics: A Methodology for Language Development. 1986.
[18] B. Srivastava, J. Koehler. Web Services Composition - Current Solutions and Open Problems.
In ICAPS, 2003.
[19] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16,
Issue 2, pp. 46-53, Mar. 2001.
[20] S. McIlraith, T.C. Son Adapting golog for composition of semantic web services. In KRR,
pages 482–493, 2002.
[21] S. McIlraith, S. Narayanan Simulation, verification and automated composition of web services.
In World Wide Web Conference, 2002.
[22] G. Picinielli, et al. Web service interfaces for inter-orgranizational business processes - an
infrastructure for automated reconciliation. In EDOC, pages 285-292, 2002.
[23] B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pages 467–477.
[24] Web Service Semantics - WSDL-S http://www.w3.org/Submission/WSDL-S.

25

Preview
This document is a preview version
and not necessarily identical with
the original.
http://www.it-weise.de/

WSC-2009: A Quality of Service-Oriented Web Services Challenge
Srividya Kona

Ajay Bansal

M. Brian Blake

Georgetown University
Washington, DC 20057

Georgetown University
Washington, DC 20057

Georgetown University
Washington, DC 20057

kona@cs.georgetown.edu

bansal@cs.georgetown.edu

blakeb@cs.georgetown.edu

Steffen Bleul

Thomas Weise

Universityof Kassel
D-34121,Kassel Germany

Universityof Kassel
D-34121,Kassel Germany

bleul@vs.uni-kassel.de

weise@vs.uni-kassel.de

Abstract
With the growing acceptance of service-oriented
computing, an emerging area of research is the
investigation of technologies that will enable the
discovery and composition of web services. The Web
Services Challenge (WSC) is a forum where academic
and industry researchers can share experiences of
developing tools that automate the integration of web
services. In the fifth year (i.e. WSC-09) of the Web
Services Challenge, software platforms will address
several new composition challenges. Requests and
results will be transmitted within SOAP messages.
Semantics will be represented as ontologies written in
OWL, services will be represented in WSDL, and
service orchestrations will be represented in WSBPEL.
In addition, non-functional properties (Quality of
Service) of a service will be represented using WSLA
format.

1. Introduction
The annual Web Services Challenge has provided a
platform for researchers in the area of Web Service
Composition since 2005. Succeeding the EEE-05
[2][11], WSC-06, WSC-07, and WSC-08 Challenges
[3][4][5], the 2009 Web Services Challenge (WSC-09)
[6] is the fifth year of this SOA venue [1] that looks to
benchmark software applications that automatically
manage web services. WSC-09, held at the IEEE Joint
Conference on Electronic Commerce Technology
(CEC 2009) continues to provide a forum where
researchers can collaborate on approaches, methods
and algorithms in the domain of web service discovery
and automated composition. This forum provides
quantitative and qualitative evaluation results on the
performance of participating matchmaking and

automated composition software and facilitates the
dissemination of results that advance this field.
The WSC-09 represents the fifth event of the
matchmaking and composition challenge series. It
extends the original criteria of the first two
competitions which focused on service discovery and
service composition based on the syntactic matching of
WSDL part names. It also further extends the third
and fourth competition in 2006 and 2007 which
provided taxonomy of parameter types represented
using the natural hierarchies that are captured using
simple and complex types within XML documents.
WSC-08 further evolved with the adoption of
ontologies written in OWL to provide semantics.
The 2009 competition is a continuation of the
evolution of the challenge considering non-functional
properties of a web service. The Quality of Service of
a web service is expressed by its response time and
throughput. It has been the tradition of the WSC events
to adhere to technology-independent approaches to
semantics. In this way, the competition attempts to
circumvent debates on representations, such as
differences between approaches like OWL-S [8] [7]
and WSDL-S [12]. In 2006, the use of pure XMLbased semantics allowed for a bi-partisan approach.
In 2007, we have evolved the challenge by mirroring
the
XML-based
semantics
with
equivalent
representations using an OWL ontology. During the
previous three years, web service challenge focused on
optimizing the discovery and composition process
solely using abstractions from real-world situations.
The taxonomies of semantic concepts as well as the
involved data formats were purely artificial. In 2008,
we have further evolved the challenge by using data
formats and contest data based on OWL, WSDL, and
WSBPEL schemas for ontologies, services, and service
orchestrations.

Preview
This document is a preview version
and not necessarily identical with
the original.
http://www.it-weise.de/

In the WSC-08, concurrency was introduced into
the composition problem sets. The combination of
complex, workflow-based heuristics with semanticallyrich representations required participants to create new
software that is both robust and efficient. The problem
sets in WSC-09 will also are inherently concurrent.

2. Related Venues
Although WSC is perhaps the first venue, other
unique venues have been established to investigate the
need for solutions to the Service Composition
Problem. The Semantic Web Services Challenge [9] is
less of a competition and more of a challenge. Both
business cases and solution applications are the focus
of the venue. Participants are placed in a forum where
they can incrementally and collaboratively learn from
each other. While WSC venues are more driven by
application, the SWS venues concentrate more on the
environment. As such, the SWS venues place more
focus on semantics where the WSC favors applied,
short-term solutions.
Alternatively, the SOA Contest [10] held at the
International Conference on Services Computing
(SCC2006, SCC2007, and SCC2008) allows
participants to openly choose the problems that best
demonstrate their approach. The benefit of this venue
is that participants can show the best approach for a
particular domain-specific problem. In contrast, the
WSC venue attempts to set a common problem where
approaches can be evaluated side-by-side.
There is a unique niche for each forum and the
composition of the results from all the venues will
undoubtedly advance the state-of-the-art in serviceoriented computing.

3. The Challenge
In the competition, we adopt the idea of so-called
Semantic Web Services that represent Web Services
with a semantic description of the interface and its
characteristics. The task is to find a composition of
services that produces a set of queried output
parameters from a set of given input parameters. The
overall challenge procedure is shown in Figure 1.
The composer software of the contestants is placed on
the server side and started with a bootstrap procedure.
First of all, the system is provided with a path to a
WSDL file. The WSDL file contains a set of service
descriptions along with annotations of input- and
output parameters. The number of services will change
from challenge to challenge. Every service will have
an arbitrary number of parameters. Additional to the

WSDL file, we also provide the address of the OWL
file during the bootstrapping process. This file contains
the taxonomy of concepts used in this challenge in
OWL format. The bootstrapping process includes
loading the relevant information from these files. The
challenge task will then be sent to the composer via a
client-side GUI very similar to last year’s challenge.
After the bootstrapping on the server side is finished,
the GUI queries the composition system with the
challenge problem definition. The contestant’s
software must now compute a solution – one or more
service compositions – and answer in the solution
format which is a subset of the WSBPEL schema.
When the WSBPEL document is received by the GUI,
we will stop a time measurement and afterwards
evaluate the compositions themselves.
Bootstrap Phase

Challenge Server Side
WSDL
File

OWL
File

Challenge Client Side

WSLA
Files

Evaluation

WSDL of
Required
Service
Parse WSDL
Service Descriptions

Parse OWL
Ontology

Parse WSLA
QoS Description

Compute Service
Composition

WSDL of
Required
Service

Generate
WSBPEL

WSBPEL
File

Interface
Package

WSBPEL
File

Figure 1: The procedure of the Web Service
Challenge 2009.

3.1. What’s New
•
•

Document Formats: WSLA format
Quality of Service (QoS): Each
service will be annotated with its nonfunctional properties on response time and
throughput. The contestants do not have to
find the shortest or minimal composition
considering the amount of services. The
contestants should, instead, find the
composition with the least response time
and the highest possible throughput.

3.2. Semantics
Ontologies are usually expressed with OWL’s
XML format [13][14]. We use the OWL format in the

2009 challenge, but like in the previous years, we limit
semantic evaluation strictly to taxonomies consisting
of sub and super class relationship between semantic
concepts only. OWL is quite powerful. In addition to
semantic concepts (OWL-Classes), OWL allows to
specify instances of classes called individuals. While
we also distinguish between individuals and classes in
the competition, the possibility to express equivalence
relations between concepts is not used. In OWL, the
semantics are defined with statements consisting of
subject, predicate, and object, e.g. ISBN-10 is_a
ISBN (ISBN subsumes ISBN-10). Such statements
can be specified with simple triplets but also with
XML-Hierarchies
and
XML-References.
The
implementation of an OWL-Parser is hence not trivial.
In order to ease the development of the competition
contributions, we will stick to a fixed but valid OWLSchema.

3.3. Quality of Service
The Quality of Service for a service can be
specified using the Web Service Level Agreements
(WSLA) [15] language from IBM. In contrast to the
Web Service Agreements (WS-A) language, WSLA is
in its final version. Furthermore, WSLA offers more
specific information than WS-A. We can not only
specify the Service Level Objectives (SLO) of a
service and its service operations, but also the
measurement directives and measurement endpoints
for each quality dimension. WSLA represents a
configuration for a SLA management and monitoring
system. In contrast to WS-A, WSLA enables the
automated discovery and deployment of service
contracts inside SOAs. In the WSC-09, we define the
following quality dimensions for a Web Service. They
can be accessed in this document format and must be
calculated for a whole BPEL process.
• Response Time: In a data system, the
system response time is the interval between the
receipt of the end of transmission of an inquiry
message and the beginning of the transmission of a
response message to the station originating the
inquiry.1 Example: 200 milliseconds.
• Throughput: In communication networks
such as Ethernet or packet radio, throughput is the
average rate of successful message delivery over a
communication
channel.2
Example:
10.000
(successful) invocations per minute.
http://en.wikipedia.org/wiki/Response_time_(technology)
[accessed on 2009-05-12]
http://en.wikipedia.org/wiki/Throughput
2009-05-12]

[accessed

3.4. Evaluation
The Web Service Challenge awards the most
efficient system and also the best architectural
solution. The best architectural effort will be awarded
according to the contestant’s presentation and system
features. The evaluation of efficiency consists of two
parts as shown in Figure 2.
The BPEL checking software evaluates the results
of the participant’s composition system. The BPEL file
is examined for a solution path and its correctness
regarding the challenge task.
1. Three challenge sets are provided and each
composition system can achieve up to 18.
2. The time limit for solving a challenge is five
minutes. Every composition system replying with a
solution later than five minutes will receive 0 points
for the challenge set.
3. The task is to find the service composition
solution with the lowest response time. Additionally
the composition system that finds the service
WSDL of
available
Services

Compute Service
Composition
Interface
Package

Time
Measurement

WSBPEL
file

Composition
Evaluation

Generate
WSBPEL

Challenge Score

composition with the highest throughput in the fastest
time will be rewarded.
Figure 2: Evaluation in the WS-Challenge 2009.

4. Logistics of the Web Services Challenge

1

2

We define one WSLA document containing
the specification of the average response time in
milliseconds and the throughput (invocations per
minute) of each service in a challenge set. Metrics can
be omitted as they do not contain relevant information
for the service composition. They are interesting
nonetheless as they present the capabilities of WSLA.

on

WSC-09 has attracted international teams which
come from universities and research organizations in

countries including the Slovak Republic, China,
Vienna, Netherlands, South Korea, and the United
States. The organization of this event is divided into
two phases: The first phase focuses on evaluating the
technical viability of the methodologies proposed by
the participating teams. A four-page technical
description submitted from each team is peer-reviewed
and included in the proceedings of the conference CEC
2009. Once the reviewing and acceptance notification
phases have completed, teams that successfully
complete this first step are asked to submit a version of
their software for evaluation. Preliminary tests are
conducted using this evaluation version to ensure the
compatibility and applicability during the final
competition. The main objective is to avoid in advance
potential format related problems that may otherwise
occur when the competition takes place.
The second phase is the final competition which is
scheduled for two days at CEC-09. On the first day,
all the participating teams will present their approaches
in a specialized session of the conference. On the same
day, the participants are allotted times to install the
latest version of their software on the evaluations
services located onsite at the conference. On the
second day, the teams must execute their software
using a customized data set prepared specifically for
the competition. Participating software is measured for
performance during any indexing phases and during
the actual composition routine. Composition results
are evaluated against known solutions for correctness
and completeness. In 2009 there will be multiple sets
of correct answers with variable length chains.
Applications will be judged with weighted scores
based on the best solutions that they present.
The solution application with the best qualitative
and quantitative scores when run against several
datasets is awarded first place. The competition
typically has a winner and several runner-ups.

5. Acknowledgements
The authors would like to acknowledge the efforts of
Georgetown student Brian Miller who facilitated the
web site development for the challenge. Georgetown
graduate student, John Adams, organized the travel
logistics of the web service challenge. The authors also
acknowledge Hong Kong Baptist graduate student,
Kai-Kin Chan, for preparing the OWL representations.
The Web Service Challenge has been extensively
funded by the National Science Foundation under
award number 0548514 and 0723990. The Hewlett
Packard Corporation and Springer-Verlag have also
supported an award to the winners of the competition.

References
[1] Blake, M.B., Cheung, W., and Wombacher, A. “Web
Services Discovery and Composition Systems”
International Journal of Web Services Research, Vol. 4,
No. 1, pp iii – viii, January 2007
[2] Blake, M.B., Tsui, K.C., Cheung, W., “The EEE-05
Challenge: A New Web Service Discovery and
Composition Competition” Proc. of the IEEE Intl.
Conference on E-Technology, E-Commerce, and EServices (EEE-05), Hong Kong, March 2005.
[3] Blake, M.B., Cheung, W., Jaeger, M.C., and
Wombacher, A., “WSC-06: The Web Service
Challenge”, Joint Proceedings of the CEC/EEE 2006,
San Francisco, California, USA, June 2006.
[4] The
Web
Services
Challenge
(2007).
http://www.wschallenge.org/wsc07/
[5] The
Web
Services
Challenge
(2008).
http://cec2008.cs.georgetown.edu/wsc08/
[6] The Web Services Challenge (2009). http://www.wschallenge.org/wsc09
[7] Fensel, D. and Bussler, C. “The Web Service Modeling
Framework”, Electronic Commerce: Research and
Applications, 1(2): 113-137, 2002
[8] Martin, D. et al. “Bringing Semantics to Web Services:
The OWL-S Approach”, Proc. of the First Intl.
Workshop on Semantic Web Services and Web Process
Composition (SWSWPC-04), San Diego, USA, July ‘04.
[9] The Semantic Web Services Challenge (2007):
http://sws-challenge.org/wiki/index.php/Main_Page
[10] The
Services
Computing
Contest
(2007):
http://iscc.servicescomputing.org/2007/
[11] The Web Services Challenge at the IEEE Conference on
e-Business
Engineering
(ICEBE-05)
(2007):
http://www.comp.hkbu.edu.hk/simctr/wschallenge/
[12] WSDL-S
(2007):
http://www.w3.org/Submission/WSDL-S/
[13] Bechhofer, S., Harmelen, F., Hendler, J., Horrocks, I.,
McGuinness, D., Patel-Schneider, P., and Stein L.
OWL Web Ontology Language Reference. World Wide
Web Consortium (W3C), February 10, 2004. Online
available at http://www.w3.org/TR/2004/REC-owl-ref20040210/
[14] Bray, T., Paoli, J., Sperberg-McQueen, C., Maler, E.,
and Yergeau, F. Extensible Markup Language (XML)
1.0 (Fourth Edition). World Wide Web Consortium
(W3C), September 29, 2007. Online available at
http://www.w3.org/TR/2006/REC-xml-20060816
[15] Keller, A., Ludwig, H. The WSLA Framework:
Specifying and monitoring service level agreements for
Web services. Journal of Network System Management,
11(1), 2003.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

