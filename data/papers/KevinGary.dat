A Flexible Marker-Passer for Semantically Weak Search*
Kevin Gary )
Global Associates, Ltd.

Jennifer J. Elgot-Drapkin )
Arizona State University

Keywords: connectionist models, marker-passing, commonsense reasoning,
knowledge representation, search

Abstract
New challenges posed in many areas of AI research represent a
departure from domain-specific applications-to systems that can
more effectively cope with larger and more uncertain domains.
Such knowledge intensive applications require the easy and efficient
utilization of tremendous amounts of knowledge. The magnitude of
knowledge and often stringent response constraints that characterize
such applications poses a computationally prohibitive search problem.
A proposed technique for addressing this problem is a parallel
search technique known as marker-passing. Past work in markerpassers has shown they often return too much information, becoming
a bottleneck of the system in which they are embedded. This
paper presents the design of a flexible marker-passing mechanism
embedded in a commonsense reasoning model, RABIT (_Reasoning
About Beliefs I n Time), which overcomes this difficulty. The
unique design we present avoids traditional drawbacks in markerpassing implementations by emphasizing search over inference as
the goal of the marker-passing process. This marker-passing design
is powerful due to its separation of the marker-passing process from
the knowledge contained in the network itselL thus allowing for its
potential use not only in the area of commonsense reasoning, but also
in many other domains, including, but not limited to, natural language
processing, general-purpose planning, and robot navigation.

passing implementations by emphasizing search over inference 1 as
the goal of the marker-passing process.

Marker-passing is a parallel search technique based on the spreading activation theory of human semantic memory organization and
retrieval [15, 16, 3]. The technique assumes a (semantic) network representation of a knowledge base, with each node in the network being
assigned an individual processing element 2. Each node processes a
packet of information referred to as a marker, which it receives from
adjacent nodes and likewise passes onto other nodes when finished
processing; he.nee the label marker-passing. The assignment of.simple processing elements to each node in the network that may operate
in parallel greatly eases the computational burden of the system. As
such, marker-passing has been applied in several domains that require"
stores of general knowledge, including Planning [8], Natural Language Processing [2, 13, 19, 12] and Commonsense Reasoning [7].
A marker-passer is usually embedded in a system that includes some
pre- and post-processing steps. The system accepts some appropriate

input, initiates marker propagation, and collects the results of the
propagation process for further processing. Figure 1 depicts the flow
of processing in typical marker-passing systems.
Into-processing

(Extraction)

~Iarker-passing

)

(Search)

Pest-processing

)

(Evaluation)

Figure 1: Embedded marker-passing systems

1

Introduction

The identification of relevant information in a large knowledge base
is a search problem with wide applicability in AI. One proposed
technique for addressing this problem is a parallel search technique
known as marker-passing. In this paper we present the design of
a marker-passing mechanism which is embedded in a commonsense
reasoning model, RAB1T (.Reasoning About Beliefs I n Time). The
unique design we present avoids traditional drawbacks in marker-

*Supported in part by National Science Foundation under grant # IRI9210906 and by Arizona Stale University's Faculty-Grant-In-AidProgram.
)Global Associates, Ltd. 2300 Clarendon Blvd., Arlington, VA 22201;
internet:gary@enws320.eas.asu.edu
t Dept. of Computer Science and Eng., Arizona State Univ., Box 875406,
Tempe, AZ 85287-5406;interneudrapkin@asal.edu
Permission to copywithoutfee all o¢part of this material is granted provi4~lthat
the copies are not made or dk~'ibuted for direct commercial advantage., the ACM
copyright notice and the title of the publication and its date appeal, and notice is
given that copying is by permission of the Association for Computing Machinery.
To copy otherwise, or to republish, requires a fee and/or specific permission.
© 1994 ACM 089791-.647-6/94/0003 $3.50

The pre-processing phase extracts from the input information required
to initiate marker propagation in the network. Appropriate nodes in
the network initiate marker propagation concurrently. During the
marker-passing phase, the search is carried out by exhaustively
passing markers throughout the network 3. At the conclusion of this
propagation process, the marker-passer submits its results to a postprocessing phase for further evaluation. This post-processing phase
may perform such tasks as evaluating intersecting paths of two or
more markers, or topologically sorting the nodes for prioritization.
Traditional implementations of the marker-passing technique have
emphasized its use as a parallel inference mechanism. A drawback

IAIthough inferencing can be thought of as a search problem; we m a k e
a distinction here because of the notion of correztness associated with an
inference, which results in a semantically stronger search than we discuss
here.
2Though recent work has addressed the issue of constructing suitableparallelhardware for marker-passing [12, I0}, most implementations., including
ours. have been ,serialsimulations of the inherently paralleltechnique.
3"the semantic content of a marker and the rules that govern h o w it
is processed and propagated throughout the network are system*dependent.
What is important is that a marker can be an arbRrarily complex semantic
structure with complex rules associatexlwith its behavior, or can be a simple

~..rueture with few rules governing its behavior.

313

to using marker-passing in this way, however, is that often too
many potential inferences are returned irom the marker-passing
process [2, 13]. As Yu and Simmons [19] aptly point out, this
results in a bottleneck forming around the marker-passing and postprocessing phases. The efficiency gained by employing the parallet
marker-passing technique can thus be lost in the extra effort required
by the serial post-processing evaluation phase.
A recent trend [13, 19,12] in marker-passing irvplementations restricts
the propagation of markers in the network in an attempt to reduce
the number of inferences found (smart marker-passing; see [1, 8]).
However, in our system, RAB1T, w, take a different approach. The
search performed in RABIT is based on a weaker semantics than
inferencing, and requires no post-processing evaluation phase. This
allows us to adopt what is referred to as a dumb marker-passing design.
We have implemented a flexible marker-passing design in RABIT
that overcomes traditional difficulties in dumb marker-passers. Our
approach appears to be well-suited for search domains with weak
semantics.
The remainder of the paper is organized as follows. Section 2 briefly
describes the full architecture of RABIT, clari.fying the role of the
marker-passer in the system. Section 3 then details the design of the
marker-passer, while Section 4 gives a brief example to show how this
design is suitable for controlling marker-passing behavior. Section 5
provides a discussion of our approach compared to past work, and
Section 6 summarizes the work described here and suggests avenues
for future research.

the LTM retrieval process). The functioning of the RTM and ITM
modules is not relevant to our discussion here.
The beliefs in LTM are represented in a first-order clausal language 4.
STM is maintaLq~ ~ a small ~et of beliefs, Considered the "active"
beliefs. STM is updated by receiving beliefs from the processes
of observation, inferecce, and LTM retrieval. Observations are
observable phenomena representing information about the outside
world. The inference mechanism infers beliefs (through infercncs
rules such as n~cJus ponens) based solely on the current contents of
STM. LTM retrievalplaces relevant beliefs from LTM into STlVl. It
is this retrieval process that is implemented by the marker-passer.
The transfer o.%eliefs between LTM and STM is of criticalimportance
in determining the contents of STM at each cycle. Beliefs retrieved
from LTM and placed in STM represent the identification of relevant .
knowledge from a vast body of general knowledge (LTM) to the
current focus of reasoning (STM). Note that this is not an inference
process, but a computationaUy expensive search for relevant beliefs
~om an agent's store of general knowledge. We feel marker-passing
is a viable candidate for meeting these computational demands due to
its inherent parallelism. In addition, the weak semantics of the search
space allows us to adopt a flerAble dumb marker-passing design that
avoids the computational bottleneck between the marker-passer and
a post-processing step. Furthermore, this design is widely applicable
in domains with similar search characteristics: In the next section we
describe the design of the marker-passer implemented in RABIT.

3
2

The Architecture of R A B I T

RABIT is an extension to previous work on a memory model for
commonsensereasoning [4, 14, 7]. In this section we provide a brief
overview of the commonsense reasoning architecture of RABIT. A
complete discussion of this approach may be found in [6, 5].
The architecture of RAB1T is shown in Figure 2, RAB1T's archiepisodic.
.. memory

LTM

t

relevant

i fo

LTM
retrieval

ff

inference

I

observations
Figure 2: Architecture o f R A B I T
teeture has four distinct components: Short-Term Memory (STM),
Long-Term Memory (LTM), Relevant-Term Memory (RTM), and
Intermediafe-Term Memory (ITM). These components are based on
structures proposed in the field of cognitive psychology for human
memory. STM represents the agent's current focus of reasoning.
LTM represents the agent's total, store of knowledge. RTM stores
a notion of the current context of RABIT's reasoning. 1TM stores
a temporal record of past experience. RABIT proceeds forward in
discrete time steps or cycles. At each cycle there are various interacting processes that operate in parallel to update the contents of each
module. For example, observation, inference, and LTM retrieval all
work to update the contents of STM at each cycle. The focus of this
paper is the relationship between the STM and LTM modules (via

L T M and The Marker-Passing Design

The marker-passer in RABIT has been designed as a flexible process
that allows the search to be tailored to meet an application's requirements. The marker-passer uses the activation energy approach, a weak
semantics approach also referred to as dumb marker-passing [1, 8].
Dumb marker-passing has been advocated in previous work by Charnlak [1, 2] and Hendler [8]. The marker-passing implementation in
RA.BIT extends previous fcrraulations of dumb marker-passing systems by adding more flexible processing at each node to influence the
search. 'rials enhanced d e s i ~ overcomes some of the previous shortcomings of dumb marker-pass£qg by allowing more control over what
areas of the network become activated and how much information is
returned.
We first discuss how the LTM network is constructed and then turn
to the design of the marker-passing process that operates on it.

3.1

Constructing the LTM network

The LTM network is a finite, connected, bipartite graph. The nodes
of the graph are either beliefs or concepts; the arcs are unlabeled. The
belief nodes represent the relations between concepts. Each time a
belief that is not already stored in LTM enters STM, a sub-network
for the belief is created in the LTM network. This sub-network is
created by forming a concept node for each predicate and constant
term in the belief, and a belief node for storing the belief (if such
nodes do not already exist). All new and existing concept nodes
involved in the belief are linked to the (newly created) belief node.
As an example, consider the beliefs and network in Figure 3. The
four beliefs listed produce the network structure shown.
This network structure is similar to the network structure created in
Chamiak's WIMP [21 and Hendler's SCRAPS [81 models. The most
important ddisq"e:ence is that the existence of belief nodes provide
4A belief may be either a ground literal., referred to as a fact, or an
implication,where the antecedent; sa conjunction ofliterals and the consequent
is a single literal.

314

3.2.1 Marker Structure

®
®
®
(I) V x. penguin(x) ~ bird(x)
(2) V x. bird(x) ~ flies(x)
(3) bird(Opus)
(4) bird(Fred)

Figure 3: Simple Interconnected Network
an explicit repository for the beliefs stored in LTM, as opposed to
representing the belief itself as a piece of network structure. This
separation of the semantics of the knowledge representation scheme
from the operational semantics of the marker-passer is extremely
important. See Section 5 for a discussion of the advantages of this
separation.

3.2

Marker-passingDesign

The real power in RAB1T's marker-passing design comes from
its flexibility, derived from the individuality of processing at each
node. Table 1 shows the structure of each node created in the LTM
network. The fields act-function, prop-function, strength-function,
dist-function and links all play a role in the individual functionality
of each node. The four functions are uniquely instantiated at each
node. The links contain pointers to adjacent nodes with associated
strengths used in scaling activation levels passed to these nodes. The
functions together with the link strengths allow each node to process
and distribute activation values independently.
Field

Description

obj~t

A belief or concept.

method

How RAB1T obtained this information.

Belief nodes only.
cycle

When RAB1T obtained this information.

Belief nodes only.
zoreh
mark
act-function
prop-function
strength-function
dist-function
links

The current zorch value of the nodel
The most recent marker passed into' the
node.
Assigns a new zorch value to the zorch
field based on incoming zorch.
Determines a base zorch value to pass
out of a node.
Assigns strength values to newly created
outg.oing links.
Determines the final zorch values distributed over the outgoing links.
A slructure storing outgoing links and
their associated strengths.

Table 1: L T M N o d e Structure
A marker-passing system is characterized by the structure of the
markers, the operations that take place on an incoming marker at each
node, and the operations at each node that govern how that marker is
propagated to its neighbors. We discuss each of these factors in turn.

A market is represented as a 3-tuple, with fields for a zorch value, a
unique id, and a date. A zorch s value is a positive real number that
represents the activation level of the marker. The unique id prevents
marker propagation loops. The date is the current cycle number, also
used in detecting loops.
3.2,2 M a r k e r Processing
A node accepts a marker passed to it from an adjacent node, and
applies its own procedures for processing incoming zorch values.
First the id and date fields of the marker are compared against the
mark field of the node to ensure looping hasn't occurred (a marker
is only allowed one visit to a node). 15 looping hasn't occurred, then
processing, and potentially propagation, of the marker may continue.
The act.function, or activation function, accepts a zorch value from
the incoming marker and assigns anew zorch value to the node based
on the incoming value and the current zorch value at the node. This
function can be used to control how quickly a node's activation level
grows.
3.2.3 M a r k e r P r o p a g a t i o n
Marker-passing proceeds as follows. The set of beliefs added to STM
through the observation and inference processes are used to initiate
marker propagation in the network. These markers then propagate
in parallel throughout the network until thek zorch values fall below
an attenuation parameter or until further propagation would result in
looping. At the conclusion of this exhaustive propagation process, aU
belief nodes with zorch values above a given threshold are placed in
STM via LTM retrieval.
Research in cognitive psychology suggests that activation decays as
a function of both time and distance [3, 17, 18, 11]. Accordingly,
RABIT's marker-passing design incorporates two parameters, a cycle
decay and a level decay, for time and distance decay, respectively.
The cycle decay is applied at all nodes at the completion of each
cycle. The level decay is applied each time a marker traverses a link.
At each node, functions are invoked that determine how markers
are passed to their neighboring nodes. The prop, strength, and dist
functions determine the amount of zorch to pass over each link. The
prop.function computes a base zorch value as an inverse proportion
between the incoming zorch and the number of outgoing links at
the node. The base zorch value computed by the prop-function may
be scaled according to the strengths assigned to outgoing links. The
assignment and conversion of strength information iz performed by the
strength and dist. functions. The assignment of strength information
is made by the strength-function at the time a link is created and
stored in the links field of the node. The strength-function may assign
arbitrary symbolic information to.the link. This information can be
converted by the dist-function to a numeric weight at run-time.
Our marker-passing design has extended flexibility as compared to
previous dumb marker-passing formulations. The source of this
flexibility is in the functions stored at each node. These functions can
be used to let certain nodes become more prominent in the sprea d
of activation in the network. Different amoums of zorch may be
passed over a node's set of links by using the prop and strength
functions, allowing for activation to be "steered" to selected areas
of the network. Similarly, the act-function can be used to enhance
a node's receptability to incoming markers. In effect, this scheme
adds "smartness" to the marker-passing design, without adding the
overhead involved in smart marker-passing designs. We next provide
5We first saw the term zorch in [2].

a brief example that shows how this flexibility may be used to
overcome some potential difficulties in marker propagation.

4

Example

For this example we use the network shown in Figure 3. Let positive
integer z. be the initia! zorch value, and let all nodes initially have a
zorch value of zero. Let the act-function at each node be the sum of
the current zorch value of the node and the incoming Zorch value. Let
the prop-function at each node be the incoming zorch value to the
node divided by the number of outgoing links at the node. 6 We will
not use the strength- or dist functions. Level and cycle decay do not
have an impact on this example so we will not consider them here.
Suppose RABIT just observed Vz.per~guin(=) ~ bird(=). This
corresponds to node number (1) in the network. Marker-passing
will be initiated at this node with the value z. The resuks of the
marker-passing are shown in Table 2. lncoming refers to the zorch
being passed into the node7; assigned is the zorch value assigned to
the node; and outgoing is the zorch value passed to adjacent nodes.
The values in the outgoing column are derived by applying the propfunction to the value in the incoming column. This outgoing value,
of course, is the incoming value to those nodes to which it is linked.
Since all nodes had a previous zorch value of zero, the values in the
assigned column mirror those in the incoming column.
Node-"

I Incoming

(1)
(2)
(3)

z
z/8
z/8

(4)

z/8
z/2

bird
penguin

Opus
Fred

flies

z/2
z/16
z/16
zli6

Assigned

Outgoing

z
z/8
z/8
z/8

z/2
z/16
z/16
z/16

z/2

~/8

z
z/16

What is clearly needed here is a way to effectively steer zorch values
through dense areas of the network so as to be more discriminating in
the search. RAB1T's design aceormnodates this through the use of the
strength- and dist- functions. These functions can be used to derive
link strengths that direct zoreh to Certain areas of the network. For
example, we might use a laealristie that says we would like to favor
nodes that store implications (nodes (1) and (2) h: our example) over
nodes that stores facts (nodes (3) and (4)). We can do this by having
the strength- and dist- fenctions in the conceptnodes weight their
outgoing links to belief nodes storing implications higher than those
links to belief nodes with eacts. The result is higher zoreh values in the
nodes storing implications, and a greater likelihood these nodes will
be candidates for LTM retrieval. An alternative method for handling
promiscuity is to use a different act-func,'ion at belief nodes storing
implications, thereby allowing them to be moi'e receptive to incoming
zorch. In [6] we provide more comple x ways to use these functions
for controlling the behavior.
This example has illustrated how the flexibility of our marker-passer
may be used to overcome the 6ifficulties in marker-passing design.
We contend that this approach is feasible in any search space. While
this example is on a network of triviaI size and centent, we believe it
shows the inherent flexibility of RAB1T's marker-passer.

5

Discussion

The recent trend [13, 19, 12] in marker-passing is toward smart
designs that use complex marker structures and complex processes
at each node that operate on these markers. By contrast, dumb
marker-passers, such as the one implemented in RAB1T, are based
on a much weaker semantics than their smart counterparts. Marker
propagation usually involves computationaUy trivial processes that
separate marker propagation from the semantics of the network. The
lack of a strong semantics has often resulted in the marker-passer
returning too much information [2. 13]. However. the design of
the marker-passer in RAB1T overcomes this difficulty and shows
that dumb marker-passing may still be a viable approach in many
application domains.

o , .

z/16

z/16

T a b l e 2: R ~ t t l t s o f m a r k e r - p a s s i n g
At the conclusion of the marker-passing process, each node will have
the zorch value indicated under assigned. Note that aU nodes that
are the same distance (number of links) from the source of marker
propagation receive the same zorch value. This produces an all or
none retrieval effect, where the nodes a given distance from the source
node are either all retrieved (the zorch value is above the threshold)
or none are (the zorch value is below the threshold).
This situation can be troublesome in networks that have numerous

promiscuous nodes. Charnlak [1] coined the term promiscuous nodes
to refer to nodes with high branching factors. These nodes can
be particularly troublesome to marker-passers since they connect a
relatively high number of nodes together, making it diffieuk for the
marker-passer to filter information in these dense areas of the network.
In a marker-passing scheme such as the one used in this example, the
result is that either ail of the nodes a given distance away from the
source of propagation are candidates for LTM retrieval, or none of
them are. In a network with promiscuous nodes, this can cause the
post-processing phase to be overburdened with returned information,
or starved for more information, respectively.

6This is the same function used by WIMP [2] and SCRAPS [8].
7Recall that looping is cheel~ed for; a node will receive zorch only once
from a given source.

The decision on whether to use a smart or dumb marker-passing
design inherently depends on the purpose for which it is intended.
In domains that require an inference mechanism, it may well be
most beneficial to use the smart marker-passing approach. However,
the smart marker-passing approach suffers from several drawbacks.
First, as Hendier [9] points out, the complex processes that operate on
markers at each node are computationally expensive compared to the
trivial processes that take place at nodes in a dumb marker-passing
scheme. Second, the complexity of the relationship between the
knowledge representation scheme and the rules that govern marker
traffic in the network make smart marker-passing systems extremely
difficult to design. Determining an appropriate set of processes at each
node that produce the correct global behavior requires a great deal of
insight into how these loeaUy-applied processes affect the outcome
of the process as a whole. This has resulted in smart marker-passers
being implemented in specialized domains. It is not clear that these
designs can be scaled to wider domains.
Dumb marker-passing; on the other hand, does not suffer from these
drawbacks. The trivial computation that takes place at each node
resuks in faster execution. More importantly, dumb marker-passing
designs are easier to construct and may be more generally applied
to a number of domains. Evidence of this can be found in the
similarity in marker-passing design between Chamiak's WIMP [2]
(Natural Language), Hendler's SCRAPS [8] (Planning), and RABIT
(Commonsense Reasoning).Furthermore, the marker-passing design

316

[2] E. Chamiak. A neat theory of marker passing. In Proceedings
ofAAA186, pages 584-588, 1986.

we have presented here overcomes the traditional difficulty of too
much marker propagation by controlling the process through its
extended ftmetionality. 'lTnis opens the door for application in larger
domains.
Hendier [8] points out that rather than drawing a strict distinction
between smart and dumb marker-passers, the two should be thought
of as lying at different ends of ~ spectrum. It is apparent that
the inherently dumb marker-passing design implemented in RAB1T
has incorporated some measure of "smartness" through its increased"
functionality at each node. However, the approach we have taken does
not depend on a strong semantic relationship between the markers
and the network relations. This allows us to study the distribution
of activation values across the network as a whole, and adjust the
functionahty of key nodes to steer activation to areas of the network
for improved performance. In this way we can control both how many
as well as exactly what beliefs are returned by the marker-passer.

6

Conclusions and Future Work

We have presented the design of a flexible marker-passing system
embedded in RAB1T, an architecture for commonsense reasoning,
which allows for greatly enhanced capabilities over previous dumb
marker-passing designs. This is achieved by adding more functionality to each node in the network without tying this functionality to
the semantics of the knowledge base. The major advantage to this
approach is that the marker-passer may be considered an entity separate from the knowledge representation scheme, thereby al.towing the
dynamics of the marker-passing process to be globally controlled.

[3] A. Collins and E. Loftus. A spreading activation theory of
semantic processing. The Psychological Review, 82(6):407 ..
428, 1975.
.[4] J. Elgot-Drapkin, M. Miller, and D. Perlis. Life on a desert
island: Ongoing work on real-time reasoning. In Proceedings
of the 1987 Workshop on the Frame Problem, pages 349--357,
1987.
[5] J. Elgot-Drapkin, M. Miller, and D. Perils. Memory, reason,
and time: the step-logic approach. In R.Cummins and J.PoUock,
editors, Philosophy in AI: Essays at the Interface. M1T Press,
Cambridge, ]vIA, 1991.
[6] K. Gary, RAB1T: A spreading activation approach to realtime commonsense reasoning. Master's thesis, Arizona State
University, Tempe, Arizona, 1993.
[7] K. Gary and J. Elgot-Drapkin. Ongoing work on a memory
model for real-time commonsense reasoning. Technical Report TR-92006, Computer Science Department, Arizona State
University, 1992.
[8] J. Hendler. Integrating Marker-Passing and Problem Solv-

ing: A Spreading Activation Approach to Improved Choice in
Planning. Lawrence Erlbaum Associates, Hillsdale, NJ, 1988.
[9] L Hendier. The design and implementation of marker-passing
systems. ConnectionScience, 1(1): 17--40, 1989.

There are several avenues along which to extend this work. First, we
contend that the marker-passing design presented may be applied to
domains that have similar search requirements. It is our intuition that
our design sub sumes the designs given ha WIMP [2] and SCRAPS [8].
A rigorous investigation of our intuition is needed. Second, it is
reasonable to investigate whether the global dynamics of the markerpassing process may be control.led to some extent. Our use of a
dumb marker-passing design based on activation energy enables us
to study the distribution of activation in the network. The impact
of marker-passing parameters, including the functions at each node,
may be determined with respect to the topology of the network.
An empirical study is performed in [6] which lends insight into the
behavior of the marker-passer and what factors affect its behavior.
We intend to continue this study and use the resuks to allow RAB1T
to autonomously monitor the behavior of the marker-passer and
dynamicaUy adjust parameter values to ensure high performance.
This will aid us in our efforts to make marker-passing scalable
to larger domains. Third, and on a similar theme, we wish to
investigate whether a symbolic network using marker-passing can
team better behavior through "training" in a manner similar to
distributed connectionist representations. Finally, we have yet to
uncover the fuU power in our chosen marker-passing design. The
issue of weight assignment to links is in general an unresolved issue.
Unresolved-in our specific design is how to take full advantage
of the functionality of each node to ensure appropriate relevant
information is retrieved. [6] presents several heuristics useful in the
area of commonsense reasoning. We feel more general heuristics are
attainable through our study of the behavior of the marker-passer.

[10] T. Higuchi, H. Kitano, T. Furuya, K. Handa, N. Takahashi,
and A. Kokubu. 1XM2: A parallel associative processor for
knowledge processing. In ProceedingsofAAAI-91, pages 296-303, 1991.

References

[ 19] Y. Yu and R. Simmons. Truly parallel understanding of text. In
Proceedings ofAAA190, pages 996--1001, 1990.

[11] D. King and J. Anderson. Long-term memory search: An
intersecting activation process. Journal of VerbalLearning and
Verbal Behavior, 15:587--605, 1976.
[12] W. Lee and D, Moldovan. The design of a marker passing
architecture for knowledge processing. In Proceedingsof AAA1
90, pages 59--64, 1990.
[13] P. Norvig. Marker passing as a weak method for text inferencing.
Cognitive Science, 13:569--620, 1989.
[14] S. Paul. Marker passing in a memory based inference system. Master's thesis, Universky of Maryland, CoUege Park,
Maryland, 1988.
[15] M. R. Qoilli~n.

Semantic memory. In M. Minsky, editor,

Semantic Information Processing, pages 227--270. MIT Press,
Cambridge, MA, 1968.
[16] M. R. Quilli~n. The teachable language comprehender: A
simulation program and theory of language. Computational
Linguistics, 12(8):459--476, August 1969.
[17] R. Ratcliff and G. McKoon, Does activation really spread? The
Psychological Review, 88(5):454--462,1981.
[18] S. Yantis and D. E. Meyer. Dynamics of actN~tion in semantic
and episodic memory. Journal of Experimental Psychology:
General, 117(2): 130--147, 1988.

[1] E. Chamiak. Passing markers: A theory of contextual influence
in language comprehension. Cognitive Science, 7:171--190,
1983.
317.

COMPUTING EDUCATION

Project-Based
Learning
Kevin Gary, Arizona State University

Repeated, sustained team interactions on
scalable complex problems that require
constant synthesis and the application of core
computing concepts throughout the curriculum
place students on a trajectory toward becoming
professional software engineers.

T

oday’s aspiring software developer has a plethora of easily accessible and entertaining educational options including massive open online
courses (MOOCs), step-by-step tutorials, weekend hackathons, maker events, visual programming tools,
and easy-to-assemble app builders. This “kit-oriented”
skill-building style is far more palatable than 15 weeks of
insomnia-curing university lectures combined with long
nights in the computing lab learning what it takes to do
real programming.
Not that these resources and learning techniques are
bad, mind you; they can build some level of mastery. But
invariably something gets skipped. Epiphanies at 2 a.m.
over Coke and pizza aside, computing educators face a real
challenge: battling the perception that complex problem-­
solving skills can be acquired through intense, short-term
learning activities.

PROJECT-BASED LEARNING

Over my summer break, I enrolled in a self-paced MOOC
on data science. I was surprised that the instructor didn’t
98	

r9edu.indd 98

CO M PUTE R P U B LISH ED BY TH E I EEE COMP UTER SOCI E T Y

require prior knowledge of calculus. The course was excellent and
helped me understand a number of high-level concepts, but at
each point where a derivation of
the details was required, it was
glossed over or I was sent to an
external resource.
This happens in many areas of
computing. For example, I teach
mobile and Web system development, and most of my
upper-division students have some prior knowledge, typically from one of the above sources. But they don’t r­ eally
understand how things work or fit together, and because
they have a false sense of security about their skills, maneuvering students around their shortcomings and onto
a path toward deeper learning can be more challenging
than working with a “greenfield” student.
In The Engineer of 2020,1 the National Academy of Engineering underlines the importance of a well-rounded skill
set. James Duderstadt2 argues that purely technical skill
sets aren’t sufficient to translate intellectual output to practical products and services. Put another way, intense, shortterm learning activities may rapidly build technical skills,
but these skills have a short half-life, while more durable
skills are shortchanged. (Hackathons and maker faires are
possible exceptions, as their relentless give-and-take over a
short period can build unique communication skills.)
Further, short-term, skills-focused learning activities
usually do an inadequate job of placing those skills in context: a student might master skill X but is unable to assess
0018-9162/15/$31.00 © 2015 IEEE

8/21/15 11:00 AM

EDITOR ANN E.K. SOBEL

Miami University; sobelae@muohio.edu

tools and techniques that address the
same problem space differently. The
professional software developer is often contextually challenged; he or she
works for a “Java shop,” a “.NET shop,”
or a “Ruby on Rails shop.” And although
the numerous reasons for these technology stack decisions go beyond the
scope of this article, undeniably one
of the main barriers to innovation is
engineers’ inability to see outside the
boundaries of their own skill set. As
computing educators we give students
a mighty hammer, but the nails keep
changing.
Project-based learning (PBL) is an
approach particularly well suited to
achieve more durable, contextual outcomes for computing students. Sally
Fincher and Daniel Knox 3 describe PBL
as engaging “students in sustained,
collaborative focus on a specific project, often organized around a ‘driving
question.’” These aspects are critical:
sustained collaboration over a deep interaction requires learners to establish
a rhythm and working process to solve
a complex problem.

PROCESS, PROCESS,
PROCESS

Software engineering is inherently
frenetic. No two projects are exactly
alike, and managing evolving requirements, expectations, and technologies
has led to unique innovations such as
agile practices. Although traditional
engineering disciplines are promoting “makers” as a new means of understanding the design process, software engineers have long recognized
the need to fluidly transition between
analysis and design, requirements
and validation, and, more recently,
implementation and deployment (for
example, DevOps). Computing educators must teach across the spectrum
of these software engineering process
activities so that students understand
where they are at all times.

Today’s computing educator also
needs to teach not only how things
work “under the hood,” but the process
by which a software engineer arrives at
a solution. How do I adapt and translate my experiences to the problem
at hand? How does what I’ve learned
apply to this situation? This emphasis on process requires the aspiring
developer to understand the context
of the problem and its potential solutions. Having rote knowledge of a use
case’s structure isn’t as important as
understanding how effective a use
case−driven approach might be in a
particular scenario. Too often, universities outsource this practical aspect
of a student’s education through internships or industry-sponsored projects that are supposedly “real-world”
without defining what that means. A
PBL approach provides “real-worldedness” through a situated learning
process that forces students to identify
key decision-­making factors and draw
upon prior experiences. Sustained interaction over time ensures that students continuously apply and evaluate
these experiences.
Distinguishing a project process
from a learning process is likewise critical and often gives students trouble.
I’ve had many complain about earning a grade lower than they expected
even though the product their team
produced was superior to those of their
peers. These students have a hard time
understanding that the ends don’t
justify the means. In PBL, fidelity to a
project process enables a cooperative
learning process.4 Students that heroically complete a product the night before it’s due haven’t had a meaningful
learning experience. Ultimately, the
software they produce is less valuable
than the contextualized learning they
acquire through numerous adjustments and decisions made over the
project’s duration. Students are naturally bursty, and PBL compensates for

this by ensuring sustained, long-term
participation.

PBL AS THE CORE OF
A COMPUTING DEGREE

Most software engineering degree programs leverage PBL for various purposes. At the program level, a senior
capstone project focusing on synthesis
is required for accreditation. Many degree programs offer a freshman project
as a form of orientation and introduction to the design process; as an engineering colleague told me, the project
gives freshmen time to take all the
physics and math they need to know before engineering gets hard. Some individual faculty might take the initiative
to use PBL in a topic-specific course, but
few programs require project experience throughout the curriculum.
Sheri Sheppard and her colleagues5
suggest a layered, integrative degree
structure centered on a professional
spine. A PBL spine would enable students to contextualize new knowledge. Learning to communicate in
context, learning to problem solve in
context, and practicing a design process in context lead to professionally
prepared students.
Computing educators need to teach
to the gaps. For example, software development programs typically offer
separate courses in requirements engineering, verification and validation,
or analysis and design to teach the
relevant techniques of each life-cycle
phase. However, integrating across
these boundaries requires repeated experiences in a project context. Further,
these integrative experiences should
apply broadly outside of specializations.
PBL as a pathway to sustained engagement provides a vehicle to synthesize
computing science foundations—­data
structures, algorithms, architecture,
theory—with professional engineering
principles and specialized topics in the
upper division.

	 SEPTEMBER 2015

r9edu.indd 99

99

8/21/15 11:00 AM

COMPUTING EDUCATION

PBL AT ASU

For the past decade at Arizona State
University (ASU), we’ve implemented
PBL as the central feature of the Bachelor of Science in Software Engineering.
In this curricular design, students engage in semester- and year-long projects from sophomore through senior
years. The PBL integrates software
engineering material with computing
fundamentals appropriate to the year
of the major: sophomores learn individual professional skills through the
Personal Software Process6 while designing and implementing a semester-­
long project emphasizing data structures and algorithms; juniors design
and implement a year-long project
synthesizing concepts taken from a
focus application area (such as Web,
mobile, or game development) they select at the start of the year; and seniors
conduct an industry-sponsored yearlong capstone project to synthesize
advanced concepts from their upper-­
division coursework.
Since implementing PBL, we’ve
observed significant changes in our
students. Our industry partners note
that graduates are better prepared
for professional work—specifically,
in terms of adaptability—than peers
from other programs. We attribute
this to the contextualized cooperative
learning experience. Surveys indicate
that student satisfaction and efficacy
are up as well. For example, for the
just completed academic year, 22 juniors reported positive attitudes toward PBL, while only 1 reported a negative attitude (8 students remained
neutral); similarly, 27 of 30 students
indicated PBL was pedagogically
useful, with no negative responses (3
were neutral). The mean for both survey responses was also higher than
any other teaching method utilized,
including lectures, reflection, and
problem-based labs.

ADOPTION CHALLENGES

There are, of course, myriad challenges to PBL adoption, both at the
program and the individual class level.
100	

r9edu.indd 100

COMPUTER 

First, PBL benefits from a physical
learning environment that supports
collaboration and creativity, yet space
availability on most campuses is a constant tug-of-war with administration.
In addition, PBL can be difficult to assess, both formatively and summatively,
due to the ingenuity some students
show in hiding behind others’ work.
Moreover, teaching in a PBL approach requires the right amount of
guidance toward learning outcomes.
No two projects or project teams are
the same, so nobody has the same
experience—yet everyone needs to
achieve the same outcomes.
Scaling PBL is also difficult both in
terms of student head count and integration across the degree program.
The economics of PBL don’t always
make sense to administration, especially if it needs to be combined with
recent innovations such as online degree programs.
Finally, PBL can be exhausting for
students and instructors alike. PBL
requires sustained scaffolding and
mentorship over a significant period
of time: you can’t disengage from the
process as easily as you can walk out of
a classroom when class is over.

P

roject-based learning isn’t a new
idea, nor does it conform to consistent definitions of what constitutes a project-based experience.
Although a body of research supports
PBL in general and its application to
STEM fields in particular, there are
few systematic studies presenting evidence of greater learning outcomes in
computing disciplines. Yet, PBL is typically included in recommended sets of
best practices for computing curricula
such as the ACM/IEEE’s CS2013.7
Repeated, sustained team interactions on scalable complex problems
that require constant synthesis and
the application of core computing
concepts throughout the curriculum
place students on a trajectory toward
becoming professional software engineers. In higher education, we have

the responsibility not only to equip
our students with the technical skills
they need to start a career but the ability to apply, evolve, and practice those
skills throughout their lifetime. While
many forms of learning have their
place, sustained PBL provides students
with these more durable benefits.

REFERENCES
1.	 Nat’l Academy of Eng., The Engineer
of 2020: Visions of Engineering in the
New Century, The Nat’l Academies
Press, 2004.
2.	 J.J. Duderstadt, “Engineering for a
Changing World: A Roadmap to the
Future of American Engineering
Practice, Research, and Education,”
Holistic Engineering Education: Beyond
Technology, D. Grasso and M.B. Burkins, eds., Springer, 2010, pp. 17−35.
3.	 S. Fincher and D. Knox, “The Porous
Classroom: Professional Practices in
the Computing Curriculum,” Computer, vol. 46, no. 9, 2013, pp. 44−51.
4.	 K.A. Smith, “Cooperative Learning: Lessons and Insights from
Thirty Years of Championing a
Research-Based Innovative Practice,”
Proc. 41st ASEE/IEEE Frontiers in
Education Conf. (FIE 11), 2011,
pp. T1A-1−T1A-7.
5.	 S.D. Sheppard et al., Educating Engineers: Designing for the Future of the
Field, Jossey-Bass, 2008.
6.	 W.S. Humphrey, PSP: A Self-Improvement Process for Software Engineers,
Addison-Wesley Professional, 2005.
7.	 ACM/IEEE Computer Society Joint
Task Force on Computing Curricula,
Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate
Degree Programs in Computer Science,
20 Dec. 2013; www.acm.org
/education/CS2013-final-report.pdf.

KEVIN GARY is an associate professor in the School of Computing,
Informatics, and Decision Systems
Engineering at the Ira A. Fulton
Schools of Engineering, Arizona
State University. Contact him at
kgary@asu.edu.

W W W.CO M P U T E R .O R G /CO M P U T E R

8/21/15 11:01 AM

Work in Progress: Constructing a Multidisciplinary
Design Project for First-Year Engineering and
Computing Students
Traffic Simulation Engineering Design Challenge

Micah Lande, Benjamin Ruddell, Darryl Morrell, Robert Grondin,
Ramon Anguamea Lara, Richard Whitehouse and Kevin Gary
Department of Engineering
College of Technology and Innovation
Arizona State University
Mesa, Arizona, USA
micah.lande@asu.edu
Abstract— Teaching first-year students is a balance between
introducing technical content and providing perspective for
students to forge connections to the context of what engineers do.
Making the possible impact of engineers real and visible to
introductory students in an authentic manner can be challenging.
This paper describes the implementation and impact for a Traffic
Simulation engineering design challenge in a freshman level
“Introduction to Engineering Design” course comprised of both
Engineering and Computing students at Arizona State
University.
Keywords-first-year engineering, multidisciplinary, engineering
design, programming, problem based learning, projects

I.

INTRODUCTION

First-year engineering students are, no doubt, well aware of
cars, roadways, traffic lights, etc. They may not have a
perspective on the engineering efforts behind these systems
however. Posing an engineering design challenge in a real
world context can engage and motivate the learning of
technical knowledge necessary to address this type of problem
and to gain a broader context of understanding [1]. This paper
describes a Traffic Simulation engineering design challenge
given to freshmen multidisciplinary engineering students to
introduce shared skills needed for their engineering success.
The civil engineering context focuses on analysis of a
transportation network and the optimization of transportation
control systems after collecting data for a system involving
randomness, decision making, and human users of the system
in an environment with time pressure. The civil engineering
nature of project theme was a guise under which a baseline of
engineering skills and approaches was presented. These
specific project goals are implemented within the context of a
course with learning outcomes including the use of a design
process, teaming skills, and communication skills. In addition,
by using the Python object-oriented programming [2] language
as a tool, students were introduced to engineering modeling,

978-1-4673-1352-0/12/$31.00 ©2012 IEEE

incorporating computational modeling and algorithmic
constructs, to complete the project-based learning course [3].
This first-year course was conceived and designed as a
fusion of first-year engineering and computing students [4, 5].
Multidisciplinary teams of diverse students were tasked with
applying their new-found computing skills to a scenario to
model, prototype and test a system of stoplight traffic controls.
The aim was to manage vehicle traffic in a simple, scaled-down
road network. Students were asked to maximize the total
throughput of traffic, measured as total trips between
designated stations on a playing surface installed in a
classroom space. The road network featured roadways, 4-way
traffic lights and traffic sensors. A hardware interface with
Arduino controlled the traffic lights and read optical traffic
sensors. In addition to optimizing traffic throughput and
controlling the traffic lights, teams of students remotely
navigated a RC car through the road network with the goal of
maximizing their car’s trips between a pair of designated
stations.
II.

PROJECT-BASED LEARNING AS CORE CURRICULA

In Arizona State University’s College of Technology and
Innovation, the Department of Engineering houses both an
undergraduate engineering program and multiple computing
degree programs. In academic years 2010-2011 and 2011-2012,
a common set of courses – Engineering 101/102 Introduction to
Engineering Design I/II –with project-based learning
experiences [3] was shared in the fall and spring semesters for
both engineering and computing students. These courses built
upon the pedagogical framework of a hands-on project spine in
the undergraduate program in the Engineering Department. The
programs have a core, project course each semester throughout
the student’s tenure in the program from freshman to senior
years. Within this multidisciplinary undergraduate engineering
major there are opportunities for students to concentrate in civil
and land development, electrical systems and mechanical

systems areas. Reference [4] and [5] describe the context of the
undergraduate multidisciplinary engineering program, its
origins, objectives and structure. Computing students can major
in software engineering, applied computer science and
computer systems. Reference [6] provide pedagogical
descriptions of the software engineering program.
The freshman year provides some foundation in
engineering process, an array of technical competencies, and
practice in tackling engineering projects [7]. Table 1 lists the
relevant Engineering Program learning outcomes connected to
the Engineering 101 Introduction to Engineering Design
course. Fig. 1 shows the matrix of core classes in the
undergraduate program’s first year mapped to specific learning
outcomes.
TABLE I.

ENGINEERING 101 COURSE LEARNING OBJECTIVES

Design
(Level 1)
Professionalism
(Level 1)
Engineering
Practice (Level 1)

Recites the steps and information flow in the
engineering design process and uses at least one
organizational or technical tool in each step
Exhibits professionally appropriate behavior patterns,
appreciates engineering as a learned profession and
possesses daily success skills
Describes the essential elements of engineering
practice including teaming
from Engineering Program learning outcomes

Figure 1. Program learning outcomes aligned to first-year courses.

computing content within an interdisciplinary problem scoped
appropriately for first-year science, technology, engineering
and math (STEM) students. The learning objectives of the
Engineering 101 course state that students should: (1) be aware
of how team member behaviors affect teaming, (2) use design
process steps and tools, (3) perform engineering modeling, (4)
design solutions to problems incorporating computational and
algorithmic constructs, and (5) gain experience in the objectoriented programming paradigm.
Students were given the following design brief for the
Traffic Simulation engineering design challenge:
Your team will design a traffic control computing system to
manage vehicle traffic in a simple road network. You are
asked to maximize the total throughput of traffic, measured
as total trips between designated stations. The road
network features roadways, 4-way traffic lights and traffic
sensors. A hardware interface controls the traffic lights and
can read optical traffic sensors. In addition to optimizing
traffic throughput, your team will remotely navigate a RC
car through the road network to maximize your trips
between designated stations.
The project requires effective teamwork and application of
the software and engineering design processes (course
objectives 1 and 2: teaming and design process) to perform
physical analysis of the system to set light timing and create
a path-finding strategy to maximize your car’s trips
between stations (course objectives 3 and 4: engineering
modeling and incorporating computational and algorithmic
constructs) to create a user interface (UI) for the traffic
light controls and program a finite state machine to
manage the traffic lights (course objective 5: gain
experience in the object-oriented programming paradigm).
The project’s basic criteria and constraints were also
outlined and detailed, along with basic documentation of the
hardware and software interfaces that to be used, and an
explanation of the deliverables, rubrics, and rules of the project.

Number indicated levels 1-4 within each outcome the associated course aims to meet.
Some Engineering Program learning outcomes are listed in detail in Table 1.

III.

TRAFFIC SIMULATION PROJECT

Traffic flow and optimization was selected as an interesting
application area for first-year engineering and computing
students. The variety of electro-mechanical elements in the
system as well as the mapping of a traffic control algorithm to
the concept of a finite state machine in computing made this
scenario a useful context for teaching computing outcomes in a
problem based environment. Additionally, traffic flow control
is an aspect of civil engineering involving logic, experimental
data collection, human-centered systems, and transportation
network design. This project balances engineering and

A. Teaming
Each section was divided into 10 teams of four students
each. The Comprehensive Assessment of Team Member
Effectiveness (CATME) [8, 9] team formation and team
feedback tool were used to create diverse teams (by discipline,
experience, gender, etc.). Teams remained the same over the
entire project, spanning a standard four-month semester.
B. Design Process
As part of the pre-project preparation in the course, students
were given exercises and information about a general design
process and design language to be used in the class and the
project. As the class was made up of students who would be
advancing towards either a multidisciplinary engineering major
or computing major (software engineering, applied computer
science or computer systems), effort was make to include and
discuss both an engineering design process and a slightly
differentiated software development process. The concepts and
processes introduced were the same but the differentiated
language used was disciplinary specific. Table 2 lists both the

engineering design process and software development process
used.
TABLE II.

ENGINEERING DESIGN PROCESS AND SOFTWARE
DEVELOPMENT PROCESS

Engineering Design Process
Problem Definition
Criteria & Constraints
Specifications
Concept Generation
Concept Selection
Architectural Design
Prototype Implementation
Prototype Evaluation
Communicate Solution

controlled intersection light. The most successful algorithm for
automatic control allowed the most total scoring trips by all
four driving teams on the roadway, as averaged over multiple
timed runs.

Software Development Process
Planning
Requirements
Specifications
Analysis & Design
Implementation
Testing & Evaluation
Documenting

To help students practice their design process steps and
introduce the application area of traffic light design, an initial
assignment was given to students to explore the front end of the
design process. Students were asked to:

Figure 2. Representation of one intersection and its traffic lights

Discuss why a traffic engineer might select a “4-way stop
sign” concept over a “stoplight” concept for an
intersection. Engineers use numbers; what are some
numbers this engineer would consider?
For individual student responses, a grading metric was
used to reward (1) multiple criteria for concept decisions
described and (2) a quantitative example of numbers an
engineer could use as supporting evidence for a decision..
As teams developed models of a finite state machine for
one traffic intersection, teams were asked to write-up an initial
report and then create a poster on how they would solve the
larger project at hand. Teams were asked to include a problem
statement, criteria and constraints, a Finite State Machine
design, a reflection on their concept generation and selection, a
storyboard of the interactions at a traffic light and information
on how they would implement a finite state machine in code.
Formal formative feedback on student communication of
their design process was given by faculty at each project
“checkpoint”, and student teams communicated their final
design and experimental results. This documentation also
included their graphical user interface design and final Python
language algorithm code.
C. Modeling:Automated Control with a Finite State Machine
Students were required to first develop a Finite State
Machine that would control one four-way stoplight
intersection, and then second control a five intersection system
as pictured in Fig. 2 and Fig. 3. Sensor inputs to the algorithm
included sensors at each approach to an intersection, and the
algorithm’s outputs are the red-yellow-green lights for the
traffic lights.
Students were asked to create a Finite State Machine that
managed traffic flow in a single intersection to meet specified
design criteria as determined by their RC Car modeling and
GUI modeling. The Finite State Machine algorithm optimally
managed a single intersection light using only sensor input and
no human intervention. Ideally, this algorithm was capable of
generating more trips than would be possible using a manually

Figure 3. Schematic representation of the five-intersection traffic netowork
designed for four teams (teams are A, B, C, D) to make “trips” traversing the
network between a pair of “bases” assigned to their team.

D. Modeling: GUI for Human Operation of Lights
Student teams were also required to develop Graphical User
Interface (GUI) control scheme to interface with their code and
allow manual intersection control. A simplified example of the
GUI is pictured in Fig. 4. During the running of the cars on
“Race Days”, the expectation was that an operator could use
manual control of a light mediated by a Finite State Machine
that delivers traffic sensor and light states to the human
intersection control operator in an easy-to-use manner and
without reference to the actual traffic lights or RC car status..
The most successful GUI was the one whose human controllers
allow the most total scoring trips by all four driving teams on
the roadway, as averaged over multiple timed runs. In this test
human operators were selected from among the members of the
team that built the GUI. Also, the human operator is turned
away from the roadway is to prevent the operator from
obtaining visual information that is not provided by the GUI.

Figure 4. Simplified traffic light control GUI example coded in Python

Bonus points were given to the student teams that
successfully integrated their Finite State Machine and user
interface systems so that all five intersections were controlled
from a single computer, using a single automatic algorithm,
and which in separate runs a single GUI with which a single
human controller runs all intersections. Students that were able
to achieve this level of proficiency were then challenged with a
stretch goal to demonstrate efficiency through integrated traffic
management. By optimizing their automatic algorithm already
implemented, students were asked to show that their integrated
automatic algorithm produces better trip-scores for a network
of traffic lights than the basic Finite State Machine that controls
a single intersection independent of information about the other
four intersections’ states, using quantitative evidence that the
integrated algorithm boosted trip counts.
E. Modeling: Human Operated RC Car Performance
Students were also required to conduct an engineering
experiment to analyze the kinematic performance and reaction
time of the human operated RC cars used for driving around
the course and model that performance to choose optimal
traffic light timing and sequence. A companion how-to video
was produced by one of the course instructors to walk student
through the necessary steps. This video can be viewed on
YouTube at http://bit.ly/egr101cars [10]. Students captured
video of the RC cars stopping and used Tracker Video Analysis
and Modeling Tool, a physics-based freeware tracking software
to do analysis [11].
Fig. 5 shows the general setup to capture video of the RC
car. Lines with blue tape are spaced 12” apart next to the
Traffic Simulation roadway as the test track. Three people are
needed to do this experiment: a driver, starter and
videographer. The car’s driver begins driving at maximum
speed when the starter provides a signal, and then brings the car
to a complete stop when the starter provides a signal. The
starter’s signal and the entire path of the car must be visible in
the video (a signal can be provided by lifting and dropping a
foot, for example). It is important that the starter provide a
“random” or surprising start and stop signal that cannot be
anticipated by the driver, so that the driver’s reaction time may
be accurately interpreted from the video evidence. It is
suggested that each member of the team understand and
experience each role, and that several data collection runs be
conducted with different members in different roles to generate
a statistically meaningful sample of kinematic data.

Figure 5. General setup to capture video of the RC car

The analysis tool used was Tracker Video Analysis and
Modeling Tool version 3.10 [4]. It was the basis for recording
the car’s travel. Fig. 6 shows the Tracker software screenshot.
Notice the blue tape lines and the red target marks capturing
the RC car’s travel trajectory at equal distances along the
course.
Additional tools required in preparation were a custom
Python code script to correct the effect of camera perspective
on the car position data. Collected video was processed through
Tracker. Axes were added and in clip settings set to advance 5
video frames at a time. Using the Point Mass Track feature of
the Tracker software, students were able to map the travel of
the RC car over time and create a track of pointed for its
forward movement, stopping and reverse movement. This is
shown in Fig. 6 by the red target marks. The collected data was
then run through the Python code script to generate corrected
data.

Figure 6. Tracker software screenshot

Student teams were asked to use Tracker and then generate
plots in MS Excel of distance, velocity and acceleration over
time. Braking time and reaction time may be calculated as the
length in time between when the starter’s visual signal is issued
and when the car’s acceleration changes. The results of all
teams’ analyses for rate of braking, acceleration, maximum
velocity, and operator response time are shared with the class,
providing a statistically meaningful sample of performance for
the diversity of RC car operators in the class. Student teams
then generated short reports analyzing the class’s kinematic
data, presenting estimates for acceleration and braking rates,
peak velocities, and operator reaction times for the class. This
information is utilized to choose optimal traffic light timings
given the specific geometry of the traffic networks’ roads and
lights.
F. Learning Object-Oriented Programming
The programming basis for this class was Python 3.2. An
online textbook, How to Think Like a Computer Scientist [2],
was used in support of the programming content. A summary
overview of concepts applied in this class are listed in Fig. 7.

Python types/objects:

numbers (integers and floating point)

characters

lists/arrays/maps/dictionaries

user defined
Statements:

programs are composed of modules

modules contain statements

statements contain expressions

expressions create and process objects

statements we have covered so far:
o
assignment, if/if-else/if-elif-else, while/for, return, def,
import, global

statements are used to provide program execution flow control
o
sequence, selection, iteration
Functions:

we use functions to:
o
to maximize code reuse, to minimize redundancy, to
help in proceedural decomposition

functions provide scope

functions have arguments/parameters
Modules:

organize components into a system; think about the modules we've
used (tkinter, time, math, isec)

aid code reuse

provide namespace partitioning

provide shared services

allocated to the teams with the highest trip scores in the
manual-GUI light control, automatic light control, and RC car
driving categories, to incentivize effort toward efficiency and
performance.
Following the project, student teams were asked to reflect
on their design process and communicate project results in a
written report. Students were asked to: (1) address each portion
of the design process in some part of the report, (2) include
schematics of their finite state machine and figures detailing
results of analyses, (3) pre-competition testing and estimation
of vehicle speed, stopping time, (4) pre-competition estimates
of optimal light cycle times and light control algorithms, (5)
pre-competition estimates of total trips and individual team
trips, (6) pre-competition testing to validate estimates, (7)
comparison of competition data with estimates based on precompetition analysis and (8) suggestion of specific
improvements to their control algorithms based on competition
results.

Figure 7. Overview of course programming concepts

G. Design Solutions
Based on their finite state machine modeling and RC car
kinematics analysis, students teams applied an array of
strategies to optimize transit from one corner of the traffic
simulation course to the other. Fig. 8 shows the name of the
project as given to students to highlight the real-world
application of the project. Fig. 9 shows the race course setup in
the classroom.

Figure 9. Race course setup in classroom; blue tape delineates lane
boundaries, striped tape delineates entries to the “bases”, and black tape
delineates intersection stopping locations.

IV.
Figure 8. Name of project as given to students. Attempt to highlight the
application area of traffic simulation and to ground it in students’ experience.

On “race days” at the culmination of the project, several
pools containing four teams each took turns racing the RC cars
using the automatic and GUI-driven manual FSM traffic
control algorithms. Total trips between opposing “bases”
completed by each of the four teams during the timed duration
of each “race” was recorded. A “traffic cop” enforced timed
penalties on any RC car driver who left the delineated lane
boundaries, initiated collision with another driver’s car, or ran a
red light. Right turning on red lights was allowed in these
races.
Following “race day”, students analyzed the resulting trip
data to generate empirical evidence that their automated and
manual-GUI control algorithms performed optimally relative to
the project’s criteria. A modest number of “bonus” points were

DISCUSSION: INTEGRATED ENGINEERING SYSTEMS

Students were asked to reflect on their experience in a fused
engineering and computing classroom experience. The analysis
of their responses and emerging thematic analysis is still
ongoing but there seems to be at least two surprising learning
outcomes. The first is that even for those engineering students
who were surprised at the amount of computing content, a
confidence in thinking computationally seems to have emerged
and fear of programming, debugging, and other computing
activities has lessened. The other is a realization at how
integrated computing and engineering disciplines are in
embedded systems.
Another class project that is electro-mechanical in nature –
to build an automatically functioning elevator – is being
implemented in the spring semester freshman engineering
project course. Future work may include comparison of this
second project’s added learning outcomes with those of the
first project.

REFERENCES
[1]

[2]

[3]
[4]

[5]

National Academy of Engineering (2004). The engineer of 2020: visions
of engineering in the new century, Washington D.C.: National Academy
Press.
Wentworth, P., J. Elkner, A. Downey and C. Meyers. How to think like
a computer scientist. http://openbookproject.net/thinkcs/, accessed
February 12, 2012.
Prince, M. (2004). Does active learning work? A review of the research.
Journal of Engineering Education, 93, 223-232.
Morrell, D., et al. (2005). A flexible curriculum for a multidisciplinary
undergraduate engineering degree. In proceedings of the Frontiers in
Education. Indianapolis, IN: IEEE.
Roberts, C., et al. (2007). An update on the implementation of a new
multidisciplinary engineering program. In proceedings of the American
Society for Engineering Education Annual Conference & Exposition.
Honolulu, HI.

[6]

Gary, K., et al. (2006). The software enterprise: facilitating the industry
preparedness of software engineers. In proceedings of the American
Society for Engineering Education Annual Conference & Exposition.
Chicago, IL.
[7] Sheppard, S., et al. (1993). Examples of freshman design education.
International Journal of Engineering Education, 13:4, 248–261.
[8] Ohland, M., et al. (2006). W., The comprehensive assessment of team
member effectiveness: a new peer evaluation instrument. In proceedings
of the American Society for Engineering Education Annual Conference
& Exposition. Chicago, IL.
[9] Comprehensive assessment of team member effectiveness (CATME),
from http://www.CATME.org, accessed February 12, 2012.
[10] How to characterize your rc car, from http://bit.ly/egr101cars (or
http://www.youtube.com/watch?v=SznWeL8R2As), accessed February
12, 2012.
[11] Tracker video analysis and modeling tool version 3.10, from
http://www.cabrillo.edu/~dbrown/tracker/, accessed February 12, 2012.

Agile Learning Through Continuous Assessment
Kevin A. Gary and Suhas Xavier
Computing, Informatics, & Decision Systems Engineering
Ira A. Fulton Schools of Engineering, ASU
Mesa, AZ, USA
{kgary,sxavier2}@asu.edu

Abstract—This work explores the impact of teaching and
learning if the rate of learner engagement outside the
classroom is continuously measured and available to the
instructor and students. We describe an ongoing
implementation of a monitoring tool built within a
software engineering continuous integration and testing
(CI & Test) platform that integrates multiple streams of
student activity and performance on yearlong junior
software engineering projects. The CI & Test platform
allows for continuous and instantaneous feedback, which
we will use to inform student behavior change. In the
work-in-progress we describe the technology, its impact on
the teaching process for the instructor, and preliminary
results observing impacts on student engagement
behavior.
Keywords—agile; project learning; continuous assessment

I. INTRODUCTION
This work-in-progress is an application of agile process
concepts from software engineering to learning assessment.
Agile is the overarching theme, taken in the software
engineering context (and derived from industrial engineering)
of empirical process control [6]. In a forward-engineered,
defined process model, the steps of a process are elaborated
and arranged using tools such as task activity networks.
Participants in the process execute the scheduled tasks, with
expected process output if the tasks are completed within the
specified constraints. Most courses are planned in this
predictive, forward-engineered way. Learning outcomes are
supported by learning activities (whether active or passive),
and typically laid out on a course calendar or syllabus, which
guides both the teacher and learner through the process. In
contrast, empirical process control suggests that complex
processes are not amenable to a predefined set of scheduled
steps, as the environment is too fluid. Instead, the process
should be driven by a feedback loop where small adjustments
are constantly made based on continuous feedback.
The popular agile software engineering process models are
based on this empirical process principle. In prevailing agile
software engineering practice, processes are instrumented with
continuous and visible feedback mechanisms. For example,
daily standups are short (less then 5 minute) meetings to share
individual progress and hurdles to success. Scrumboards are
This work is supported by a grant from the State Farm Foundation.

978-1-4799-8454-1/15/$31.00 ©2015 IEEE

information radiators [1] meant to provide a highly visible,
continually updated representation of project requirements and
associated tasks. Continuous Integeration and Test (CI & Test)
tools provide an instanteous view of product quality, and
reduce the risk associated with long gaps of development
without validating system integration (“big-bang integration”).
There are several other practices and extensions to mainstream
agile practices (such as Kanban) that are beyond the scope of
this work. Suffice to say this project is motivated by the
potential for engineering an instrumented learning process for
the purpose of providing visible continuous feedback to
students and instructors. The hoped for result is behavior
change in the manner in which students conduct long-term
(semester or yearlong) projects.
II. BACKGROUND
A. Continuous Assessment
Continuous assessment refers to a field in educational
research devoted to the practice of performing frequent
assessments in a course context [6]. The concept has become
somewhat popular with the rising popularity of constructivist
approaches to learning, as frequent assessment provides a
feedback mechanism ensuring students are properly aligned
with a scaffolded learning process. It remains an active area of
research, with an ongoing debate surrounding the utility of
formative versus summative continuous assessment [2][6].
From a practice perspective, frequent assessments are
becoming more common with the support of technology.
Badges and micro-certificates used in some MOOCs are, in our
view, examples relying on finer-grained and more frequent
feedback mechanisms.
We do not present a new validated vehicle for continuous
assessment nor do we have a specific perspective to advance in
the debate over formative versus summative assessment. This
work instead offers a specific instance of continuous
assessment in the context of a project-centric student
experience with the support technology-based tools.
B. Embedded Assessment
Embedded assessment is a concept related to continuous
assessment, with an emphasis on the context in which the
assessment is delivered. Typically the embedded assessment
happens as part of the learning process, not as a discrete

external activity. Some embedded assessment techniques obey
similar principles to continuous assessment. Classroom
assessment techniques and technologies (clickers, in-class
polls, 1-minute papers, tablet-based activity monitoring etc.)
support frequent assessment to give the instructor feedback on
learner absorption of material. Automated online assessments
not only ease grading workloads, they also offer the
opportunity for instant feedback, remediation, and
personalization of self-study outside a classroom. Embedded
assessments within e-content or newer print textbooks give the
student immediate feedback on the content just consumed.
C. Continuous Assessment from an Agile Perspective
Despite the rise in popularity of continuous and embedded
assessment, the prevalent delivery model is to push students
through a predefined amount of content and assess the student
at the conclusion of that activity. Even descriptions of
continuous or embedded assessments tend to follow a
scaffolded, discrete event learning process, albeit at a much
higher rate of frequency than traditional course-grained
learning processes. What if we drive the time of assessment to
time of learning gap limit to zero?
As discussed in the introduction section, agile software
engineering methods derive from empirical process control, or
simply put the ability to enact an heavily instrumented process
so continual minor adjustments (“tweaks”) keep the process
within desired control parameters. Active research and practice
in agile methods attempts to drive the feedback loop to zero,
thereby eliminating the need for process scaffolding such as
frequent short iterations (or “sprints”).

Arizona
State
University.
The
Enterprise is curently
in its 10th year, and
details
of
the
pedagogy have been
presented elsewhere
[1][4]. In the context
of this work, a short
summary
of
the
approach is needed,
which we do in the
context of the junior
year
project
experience, as this is
where have piloted
continuous assessment
practices.

Fig. 1. Enterprise undergraduate spine

B. The Software Enterprise
In a Software Enterprise course, software engineering
concepts are broken into discrete modules and sequenced over
the course of a semester to synch up with project activities. In
this just-in-time approach, students are exposed to a concept,
practice it, apply it on a scalable project, and then evaluate the
applied technique all within a three-week project iteration (see
Fig. 2). The goal here is to avoid situations where students
understand a solution in the small, and instead understand
problems and their solutions in context.

Our work conveniently overloads the term continuous from
both the educational and agile software engineering
perspectives. Conceptually, our work is an example instance of
continuous assessment. In practice, it applies mechanisms from
agile software engineering, specifically continuous integration
and testing, to engineer a learning process resulting in greater
engagement in a yearlong project experience. This specific
instance and its mechanics are described next.
III. PROJECT-CENTERED LEARNING AND THE SOFTWARE
ENTERPRISE
A. Projects as a Professional Spine
Sheppard et al. [6] proposed an integrated development
thread in undergraduate engineering programs called a
professional spine. The professional spine is realized by the
Software Enterprise as a project spine in the context of
undergraduate and graduate software engineering degree
programs [5] (see Fig. 1). Instead of traditional dedicated
courses in each phase of the software engineering lifecycle
(requirements, design, verification & validation), the
Enterprise-as-spine promotes learning each of these phases as
themes in a integrative project context.
The Enterprise pedagogy combines project-centric and
traditional models for course delivery resulting in a just-intime, contextual learning experience. Software Enterprise
courses are a required part of the degree programs from the
sophomore year through 1st year of the graduate program at

Fig. 2. Software Enterprise Pedagogical Model

Example modules in the junior year spring semester course
include source code control, unit testing, static analysis, code
reviews, metrics, refactoring, and defensive programming.
Additionally, agile concepts such as sprint planning with user
storiesm continuous integration and testing, and burndown
charts are covered. Again, for each of these concepts, students
iterate through preparation (typically a reading), discussion (in
a traditional lecture setting), practice (a lab or discovery
activity), learning-in-context (put in practice in a scalable
project), and reflection. The key to the Software Enterprise
approach is that all of these stages happen at the point of
discovery in a single three-week sprint.
C. Challenges and an Opportunity for Assessment
Perhaps the most challenging aspect of implementing the
Software Enterprise pedagogy is the expectation that the

student constantly the integrate software engineering concepts
(in the form of current modules) into their team’s project
workflow. This is antithetic to how students typically work on
project coursework; projects are deadline-driven, with the
majority of the work coming just before the project due date.
There are a variety of reasons for this, from student time
pressures to a simple inability to properly plan and estime work
over a long time period. Simple mitigation strategies include
defining intermediate milestones, though this typically results
in a microcosm of the larger problem; students do not pay
attention to the milestone until it is almost due.
It is imperative in any project course that students make a
consistent effort throughout the semester on the project.
Students that defer work to the last minute usually shortchange
the concepts they are to discover and apply in favor of just
“getting it done”. In agile software engineering projects, it is
even more important to make consistent progress, the very
nature of agile processes suggest developing a constant rhythm
so constant feedback is available; without constant activity no
work is completed, no feedback can be generated, so no
adjustments can be made. The nature of empirical process
control breaks down. Finally, in the Software Enterprise is it
critical to have consistent activity so that concepts may be
integrated in the project at the point of learning.
In the Enterprise, students are given the directive to work
on their projects for 8 hours per week, and save 1 hour out of
class for preparation activities. Students are asked not to go
beyond 8 hours even if their project is falling behind, as the
intent is to have the students rely on a team process, not heroic
efforts, to complete the sprint goal. Students are assessed each
sprint (4 times per semester) on their process-related activities,
code-related activity, integration of modules into the project,
and team process activities. Individual process activities
include updating a scrumboard and participating in standup
meetings (3 per week). Code activities relate to individual
velocity measured by frequency and significance of commits to
the source code repository. Module integration is determined
by a student journaling evidence and presenting it as part of a
sprint individual reflection. Team process activities are based
on adherence to Scrum principles such as user story
development, estimation and tracking on burndown charts, and
sprint reviews and retrospectives. A student’s final grade is the
aggregation of each of the 4 sprint gardes plus 20% reserved
for the end-of-semester product deliverables.
IV. CONTINUOUS ASSESSMENT IN THE ENTERPRISE
A significant hurdle in the Software Enterprise is getting
students to work in a constant rhythm. A phenomenon we
observed is that students tend to do a burst of work at the
beginning of a sprint and at the end of a sprint and not inbetween. The burst at the beginning of a sprint aligned with
when students received their prior sprint grade, while the end
of the sprint is the milestone-driven mentality discusssed
above. Anecdotally, it seemed the students were more
motivated just after receiving feedback (a partial course grade),
and just before, but in-between fell into the same bad habits.
The premise for this project then, is that continuous
assessment, where the student always has visibility into her/his
project grade, will motivate students to adhere to the

“consistent work rhythm” expectation, thereby increasing the
student’s ability to learn in context. The Software Enterprise
provides a good vehicle for continuous assessment due to its
highly modularized curricular organization, emphasis on highly
iterative software development and pedagogical processes, and
expectation of continuous, consistent work on the project.
We borrow from the concept of continuous integration and
testing (CI & Test) in agile software engineering. CI & Test
promotes continual integration and verification of software by
performing a software build and regression test for each
commit to a software repository, or at least on a nightly basis.
This practice avoids the problem of infrequent integration
testing and long build processes. CI & Test dashboards in tools
such as TravisCI (travisci.org), Cdash (cdash.org), or Jenkins
(jenkins-ci.org, see Fig. 3) provide pervasive visibility of
product quality status. Used in conjunction with a scrumboard
and a mature source code control platform like GitHub
(github.com), the project team and all stakeholders have full
visibility into the rate of product development, project activity
toward requirements, and product quality.

Fig. 3. Example Jenkins dashboard. The traffic light and weather metaphors
on the left provide a simple short-term and long-term feedback mechanism.

Our goal in this project is to create a dashboard similar to
CI & Test platforms showing the continuous assessment view
of a student’s grade components – individual process activity,
code activity, module integration, and team process activity.
We have created a web-based tool that dynamically displays
three of these perspectives (all but module integration) and
made it available to Software Enterprise students in the Spring
2015 semester. The views this tool provides are shown Fig. 4.
The top chart in Fig. 4 shows team process progress. It
measures, as a stacked area chart, the number of To-Do, In
Progress, To Test, and Done tasks per day on the team’s
scrumboard. This particular chart shows good team process
over the sprint, as the tasks are getting moved across the states
of the scrumboard at a fairly consistent rate. The middle chart
is an view of individual code activity. The grouped bar charts
show frequency and significance of commits normalized to a
common visual scale with a double y-axis chart. This particular
student shows decent behavior, though in several cases there a
number of insignificant commits, as shown by the blue bar
being higher than the orange (commits outweighing code
addition, meaning several trivial commits performed).

students indicated it helped, while 8 said it did not help and 4
were neutral. For the bottom (radial) chart, 7 said it helped, 5
said it did not, and surprisingly 10 were neutral. While the
results overall were positive, particularly for the top chart, it
was disappointing the radial chart did not generate stronger
opinions among the students. Of course this is a small ad hoc
survey, and we did not in fact have time yet to determine if
availability of this data on a daily basis during semester-long
project performance would incur the desired behavior change.
Of course, generating reports is useful but alone is of
limited utility. We are more interested in behavior change;
getting students to pay attention to their rate of progress and
seek to work in a consistent manner so the learning process,
guided by the pedagogy, has its maximum effect. To this end
our technical design goal is to integrate rich views into daily
activity through 1) automating assessment data gathering, 2)
rendering views in the CI & Test project dashboard and/or in
the LMS, 3) adding more and detailed views, and 4) using
email, SMS, and/or mobile notifications to remind students as
soon as they falli behind on their project expectations.
This project is in a preliminary stage as we prepare to
mature the tools for use in the coming academic year. From a
theoretical perspective, this project is one data point in
agreement with Trotter [9] in that continuous feedback is a
tremendous amount of work and sometimes stressful for
students and instructors, but is on the whole worth it. Our
motivation is to motivate behavior change in students toward
consistent project activity, and prepare as instructors for larger
project class sizes and a conversion of the Enterprise classes to
online delivery. Automated continuous assessment tools is one
way, we hope, to address these coming challenges.
Fig. 4. Continuous assessment of team and individual project performance

The most important chart however, is the bottom chart, as it
shows a view of the four components of the sprint grade, laid
out on a radial chart. Further, the individual student’s
component scores (shown in the small dark area) are overload
with the team’s average component scores, and then the
component scores of the entire class. The student is able to see,
immediately, her/his performance with respect to the class.
These charts are worthwhile in that they show a rate of
progress (the top two) and instant comparison to cohorts within
the class. They are easily interpreted by the student, and are
rendered in one place. This is better than going to the LMS
gradebook for textual averages, a scrumboard, or a GitHub
pulse diagram. We pull data from the source tools of each
component, namely Scrumwise for the scrumboard and GitHub
for the code activity, on a daily basis – and are working toward
continuous streaming and integration of component data.
The web tool was deployed in Sprint 4 (the last full project
sprint), so it was not available in time to use for multiple
sprints. Students were asked, in the context of a larger course
survey given by the instructor, how useful each of these three
views are in the context of team projects. 22 students
responded to the survey questions. For the top chart of Fig. 4,
15 students indicated the tool helped them understand team
progress, while 1 said it did not help and the other 6 remained
neutral. For the code activity (middle) chart in Fig. 4, 10

REFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]
[8]

[9]

Cockburn, A. (Online), “Information Radiator”, Available at
http://alistair.cockburn.us/Information+radiator, (last accessed April 27,
2015) June 19, 2008.
Crisp, B.R. “Is it worth the effort? How feedback influences students’
subsequent submission of assessable work”, Assessment & Evaluation in
Higher Education vol. 32, no. 5, October 2007, pp. 571-581.
Gary, K. “The Software Enterprise: Practicing Best Practices in
Software Engineering Education”, The International Journal of
Engineering Education Special Issue on Trends in Software Engineering
Education, Volume 24, Number 4, July 2008, pp. 705-716.
Gary, K., “The Software Enterprise: Preparing Industry-ready Software
Engineers” Software Engineering: Effective Teaching and Learning
Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea
Group Publishing. October 2008.
Gary, K., Lindquist, T., Bansal, S., and Ghazarian, A. “A Project Spine
for Software Engineering Curricular Design”, Proceedings of the 26th
Conference on Software Engineering Education & Training (CSEET
2013), Co-located with ICSE 2013, San Francisco, CA, May 2013.
Ghiatău, R., Diac, G., and Curelaru, V. “Interaction between summative
and formative in higher education assessment: students’ perception”,
Social and Behavioral Sciences vol. 11, 2011, pp. 220–224
Schwaber, K. and Beedle, M., Agile Software Development with Scrum,
Prentice-Hall 2001.
Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M.
Educating Engineers: Designing for the Future of the Field, JosseyBass, San Francisco, 2008.
Trotter, E. “Student perceptions of continuous summative assessment”,
Assessment & Evaluation in Higher Education, vol. 31, no. 5, October
2006, pp. 505-521.

2009 Sixth International Conference on Information Technology: New Generations

A Case Study: Open Source Community and the Commercial Enterprise
Kevin Gary, Harry Koehnemann
Division of Computing Studies
Arizona State University
Mesa AZ, 85212

John Blakley
CEO
Unicon Inc.
Chandler, AZ 85225

full-fledged business model dependence on developing,
supporting, and servicing the open source community.

Abstract
Open source opens new ground for software
development houses and other IT shops. New business
models that range from support services to pure product
development now consider open source a key component
of their core business plan. This has led to interesting
relationships between open source communities and
vendors who must co-exist for each other’s survival. In
this paper we share a case study experience of a
relatively small software development and support shop
whose business model relies on open source software.
This experience describes the advantages and
disadvantages, discussing the blending of business
objectives, cultures, people, and software practices
across an open source community and software vendor.
Lessons learned and perspectives on how such
arrangements may fail or succeed are shared.

1. Introduction
Years ago, GNU’s founder Richard Stallman had a
visionary idea combining the efforts of software
developers with contributions of equipment and other
donations from industry to produce a freely available set
of UNIX tools [1]. Today, there is an increase in both
the number of developers who engage in open source
development as well as companies that use open source
development as a part of their corporate strategy [2].
The popularity of open source is reflected in its
growing rate of adoption, which in turn has led to
increasing attention from the academic research
community to understand the genesis and impact of this
movement. A proliferation of new business models
demonstrates that the market has yet to fully converge
on ways to bring open source technology to market.
One unique business model pattern involves commercial
enterprises working side-by-side with an active open
source development community to provide products and
services based on open source.
This paper presents a case study of a small company
that worked with an open source community to
transition themselves into a leading player in the vertical
market space of Higher Education. This company
converted from open source technology adopter to a
978-0-7695-3596-8/09 $25.00 © 2009 IEEE
DOI 10.1109/ITNG.2009.313

Cheryl Goar, Holly Mann, Al Kagan
The Morrison School
Arizona State University
Mesa AZ, 85212

940

2. Open Source Business Models
Open source projects often begin with a small group
of developers solving a common need and collaborating
to produce a solution. The solutions commonly target
some core technology, for example an operating system,
an application server, a CORBA ORB, leaving several
business opportunities to fill gaps. This section discusses
the gaps left and the business models that have been
created around open source projects.

2.1 Cost and Risk Sharing
In this model, businesses share residual work that
creates common technology. Businesses do not view the
results coupled with required maintenance as a
competitive advantage, and choose to share the work as
a catalyst for others. Participating businesses receive
many benefits. They reduce costs by leveraging
community efforts. They reduce risk of modifications
being lost or not maintained by the original authors.
[3] provides an excellent analogy between software
property and the common laws that govern them. It
concludes the author of a patch has two choices, "to sit
on it or throw it in the pool for free”. The first helps no
one, neither the author nor the community. The second,
while appearing altruistic, ultimately encourages
reciprocal giving which ultimately benefits the original
giver and the entire community.

2.2 Service and Support
Perhaps the most common business model in open
source is providing support and services for open source
products. We present some examples in this section.
One large gap left by open source is the training,
contracting, and consulting work to its customers. JBoss
was founded as an open source solution to provide an
option to the expensive licensing of commercial
J2EE/EJB containers. The developers are professional
open source developers, making their entire living from
the services work around JBoss. In [4] Marc Fleury, the

JBoss founder, describes the opportunities around what
he calls, Professional Open Source. In the JBoss
organization, each developer works half time on JBoss
code development and half time with professional client
engagements. This balance allows them to maintain a
connection with their customer base and at the same
time provide those customers with an extremely
competent consultant. On the negative side, Fleury also
describes the substantial effort to “get the ball rolling”.
There are no services to drive income without the
product. In the case of JBoss it took a handful of
talented, passionate individuals several years for the
JBoss product to reach a level of sophistication to
compete with commercial products.
In contrast to JBoss, Red Hat leveraged an already
existing open source platform, Linux. The availability
of the open source Linux kernel only served a subset of
the market that could afford the risk of an unsupported
operating system. Red Hat not only provides supported
builds of Linux on specific platforms, it also provides
training, service contracts and software tools. The roots
of the differences between Red Hat’s model and the
JBoss model derived from how their respective
communities evolved. JBoss exerted control over its
own community because it decided early on to manage
the evolution of the source code carefully. JBoss did not
allow community participants to contribute critical path
code, instead open sourcing the code meant customers
could download support it themselves if they chose. In
contrast, Linux kernels predated the RedHat distribution,
and so today there remain a proliferation of Linux
variants. Red Hat eventually succumbed to the
economics of community support pressures and forked
their own codebase into Red Hat Enterprise Linux and
Fedora Core Linux. Despite the distinct paths of the
technologies and respective communities, Red Hat
eventually acquired JBoss, as the services-oriented
business model was compatible.
Other organizations work the open source services
and support side without first making an entry with an
open source product. SpikeSource burst on the open
source scene in 2004 as the first major player to create a
business model solely based on the support of open
source technologies it did not itself create (though
eventually SpikeSource decided to promote branded
offerings of open source solutions). Optaros’ business
model relies solely on professional services in support of
open source business solutions. To the authors’
knowledge Optaros has no branded variant of an open
source project, but instead prefers to integrate existing
open source technologies or build customized
functionality for their clients. To create goodwill among
the open source community, Optaros claims to have
developers spend 10-20% of their time participating in
open source communities.

941

2.3 Market Position
Vendors can create open source applications as a
means to create a market position, as either as version of
their proprietary software or to simply position another
vendor out of that market. By placing some useful
portion of a software product into the open source
community, the vendor leverages an entire community
to provide enhancements and create a marketing name.
If the open source project is successful, the vendor also
locks other out of products in that market space.
Netscape first tried this approach when they moved
Mozilla into the open source community to compete
with Microsoft Internet Explorer (IE). Netscape moved
its browser into the open source community to retain
market share, and while this did not happen for the
Netscape browser, the Mozilla family of browsers is
only slightly behind IE in WWW usage1.
IBM was more successful when it placed Eclipse
into the open source community as a direct competitor to
Sun’s Netbeans. IBM’s purpose of placing Eclipse into
the open source was to make the Sun’s Netbeans
offering insignificant (as in “Eclipse the Sun”). Further,
by developing an open source product and creating
market share, IBM has since been able to leverage it into
a branded commercial offering named Jazz (jazz.net).

2.4 Reduced Development Costs
Leveraging open source products to reduce
development time and costs is another important
business case for open source. Including open source
results allows vendors to focus their efforts on other
value added features and not fund the creation of
components not core to their business. An increasingly
common assumption is that there are recognizable
benefits associated with using an open source
development model to build software [5]. While some
vendors simply include the open source work in their
products, others are more active in the open source
community serving on advisory boards and providing
resources for the code development. Currently there is a
need in the community to produce empirical data to
confirm assumptions made by many with regard to the
success of open source development as model [6].
A recent significant trend has been the public
adoption and pledges of support to the open source
community by major vendors such as IBM, Oracle,
BEA, Sun Microsystems, and Microsoft. Interestingly,
the community is accepting this support warily, ever

1

Usage statistics as of the end of October 2008, taken from
http://www.w3schools.com/browsers/browsers_stats.asp

concerned with maintaining the principles of open
source over perceived selfish interests of major players.
For example, Oracle “acquired” Berkeley DB and
promotes “unbreakable Linux”. Sun’s acquisition of
MySQL and promotion of Java as open source has been
confusing and faces complaints Sun is not relinquishing
enough control. Microsoft has been tepid in its support
in the media for open source, while at the same time
supporting the CodePlex open source repository.

3. A Case Study: Unicon Inc. and JA-SIG
Unicon Inc. is a small software company
(approximately 85 employees) headquartered in
Chandler Arizona. For the past twelve years Unicon has
developed and support eLearning software products and
services. Unicon has built almost all of its software
solutions on top of open source technology.
The Java Architectures Special Interest Group (JASIG) is a collaborative group facilitating the use of open
source solutions for Higher Education. JA-SIG extends
an invitation to both educational institutions and
commercial enterprises to participate in the open source
community they have nurtured for almost a decade.
Unicon leverages open source to support its business
model in several ways. Unicon has throughout its history
incorporated open source software as a technology base
upon which to develop solutions. Second, Unicon’s
collaborative groupware product, Academus, is built
directly within uPortal. Third, Unicon acquired and
enhanced a professional services offering around uPortal
through mergers and a partnership with JA-SIG. Finally,
Unicon replicated its professional services successes for
the Sakai, Zimbra, and CAS projects.

3.1 Building Product on Open Source:Academus
Academus is Unicon’s enterprise portal product.
Unicon markets two offerings: Academus Collaborative
Groupware (CG) and Academus Portal, to higher
education institutions. Academus CG is a collection of
portlets built in a proprietary container that embedded
into uPortal as a single portlet. Academus Portal is a
collection of ready-to-use portlets and custom uPortal
extensions that provide groupware, community building,
and integration functionality on top of uPortal.
Unicon found uPortal in the intersection of
innovation, market demands, and business constraints.
Deep penetration in the higher education market by
WebCT and Blackboard meant Unicon was forced to
find an innovative yet low-cost platform upon which to
develop and market a solution. Unicon’s approach was
to build groupware inside a portal to present an
innovative dashboard user interface that starkly contrasts
the long click paths in WebCT and Blackboard. The

942

portal also provides a natural integration platform as
opposed to making a single component, the Course
Management System, the center of a systems
architecture. Finally, as a J2EE shop Unicon found it
natural to work with a J2EE portal such as uPortal.
Academus CG was a gateway project for Unicon.
Unicon quickly found there was a lack of market
penetration in Higher Education for portal technology,
and so quickly followed up with a campus portal
offering. Though market feedback was the principal
driver behind creating a portal offering, the success of
the research and development of the Academus CG
product introduced to Unicon a model that could work
for product development. The development to release
cycle for Academus CMS 1.0 was very short (under one
year) compared to typical product efforts. Unicon was
convinced it could do this again for a portal offering.
Academus technology was so dependent on uPortal,
it caused challenges from a number of perspectives:
• Unicon did not control release cycles of the uPortal
framework, so release and configuration
management must deal with a “moving target”.
• Functionality planned for releases of uPortal was
often delayed or simply not completed due to
constraints on the contributors of the project
(typically developers in University IT units).
• uPortal, like many open source projects, does not
provide an end-to-end solution. Documentation is
sparse, and deployment processes are not
competitive with commercial portal offerings.
• Again, like many open source projects, uPortal did
not have a rigorous or formal testing process.
• The community and JA-SIG control requirements
for future releases, though Unicon does get to
participate in the process.

3.2 Professional Services for Open Source
At the same time Unicon marketed Academus portal,
market feedback indicated a sufficient number of
institutions desired “rolling their own” on uPortal.
“Sufficient number” is defined in terms of market size,
meaning Unicon believed there a profitable market for
professional services on uPortal in addition to
Academus. Unicon merged with Interactive Business
Solutions (IBS), at the time the sole provider of uPortal
professional services. This merger also facilitated the
introduction of Unicon to the JA-SIG community, as
IBS employed the uPortal Chief Architect and Project
Manager, and their co-founder had a longstanding
relationship with the JA-SIG collaborative board.
The result for Unicon has been a growing
professional services offering that includes custom
portlet development, custom integration, hosting,

deployment, call center, and training services. This
model has been successful for several reasons:
• Higher Education has natural ties to open source
from a cost and “philosophy” standpoint. Open
source is often invoked in project charters as
supporting the mission of the institution.
• Higher Education has business needs suited to open
source. Development and technology acquisition
costs must remain low. Institutions often desire a lot
of customization to satisfy institutional branding,
privacy, and security requirements.
• The need for professional services to compliment
the open source offering. Institutions with large
development staffs tend to roll their own out of a
desire to take control of the solution and because
they have the staff to support ownership. However,
even these institutions use Unicon professional
services for mentoring and training as part of rampup. Smaller institutions need the full range of
professional services Unicon offers because they do
not have the staff and expertise, to deploy, support,
and build custom solutions on their own.
• The willingness of the open source sponsor, in this
case JA-SIG, to create and nurture relationships
with vendors to build products and provide services
for the open source project. JA-SIG anticipated and
facilitated the evolution of their technology into the
marketplace by considering how vendors may be
included in the process at the outset.
This model was so successful for Unicon and JASIG, that Unicon became an initial Sakai Commercial
Affiliate. Sakai is a multi-institution initiative to
distribute open source tools and platforms for Higher
Education. The involvement of JA-SIG has clearly
influenced the Sakai project’s model with SCAs, and
enabled Unicon to easily transition to this community.
Although Unicon is not unique in offering products
and services based on open source, their success is
unique in terms of the market and relationships that help
make this model successful. The higher education
market is clearly one ready to accept open source, and
tends to accept some of the trials and tribulations of
open source that make it “open” but not “free”. The
realities of IT budgets in higher education also make
open source a highly competitive alternative. The
presence of an organization such as JA-SIG to drive the
open source project and guide its technology adoption in
the marketplace is perhaps the key distinguishing factor
in this success story.

3.3 Lessons Learned
Unicon learned many lessons from its initial open
source experience, and not all of the news is good.

943

Unicon’s product offering had a heavy reliance on the
underlying uPortal technology, yet little control over its
direction. Open source requires participation in a
community, and the decisions are made as part of the
community. Companies are not willing to accept this
constraint should consider whether building a solution
on open source is appropriate for them. Furthermore,
there is an expectation that companies will donate
software and services back to the community.
Credibility is “earned’ through participation for
individuals, institutions, and companies alike. Finally,
perhaps the most significant drawback to leveraging
open source is the instability of the product release
cycle. Open source products are often released with
insufficient documentation, insufficient testing, poor
deployment support, and unsatisfied requirements.
Supporting product release cycles and professional
services offerings in the face of this instability is
difficult. For this reason Unicon deferred evolution of its
product and services offering by at least one release
cycle behind uPortal. Furthermore, Unicon was forced to
fork from the uPortal codebase, and despite several
substantial efforts, was never able to fully reconcile the
forked changes with the uPortal main codeline. Despite
these challenges, Unicon was able to rapidly deliver a
branded commercial offering to the marketplace since
uPortal provided an existing infrastructure.
Unicon also discovered an unexpected benefit - it
was a relatively simple to determine the market for the
product. If there were no demand for the technology,
uPortal almost certainly would not have been developed
in the first place. Additionally, marketing groups are
often attracted to the fact that typically the application is
in use at a number of in-market organizations. This
allows the marketing and sales groups to avoid the
challenge of explaining to potential customers that no
one has yet adopted a new product being marketed. By
the time Unicon entered the uPortal community, there
were more than two hundred adopters of uPortal
worldwide and twice-a-year conferences.

3.4 Discussion
A successful business plan based on open source
technology must address a variety of challenges
associated with commercially supporting an open source
application. The plan must allow for flexibility in areas
such as release dates for the software as well as
requirements that are implemented per release.
The operational plan will have to include time and
resources to perform a variety of testing, covering both
the functional and non-functional aspects of the
software. In many cases, this testing can be difficult, as
there are often few requirements documents to create
test cases against. There must additionally be testing for

performance and scalability, in addition to other
environmental issues such as the various databases and
other platforms the product will be required to support.
The plan must include time and resources to create
and maintain a variety of documentation related to the
application from a variety of perspectives.
Unfortunately, it is often the case that documentation for
open source applications can be somewhat sparse or
non-existent. Customers that pay for support have higher
expectations about the content and quality of the
documentation they receive. This is a huge opportunity
for the vendor to establish its expertise and its
willingness to support the application.
One aspect that is often overlooked in open source
projects is the installation or deployment of the
application itself. This is a major source of aggravation
for many would be users of these applications. Due to
the fact that these applications can be difficult to install,
many users simply decide to purchase a product with a
more familiar “install wizard.” This is a huge
opportunity for a vendor to offer a value-add that all
most potential users would be willing to pay for.
Unicon’s most commonly requested service is
supporting initial deployment and rollouts.
A business plan surrounding the commercial support
of open source software must address many issues, and
the company considering the offering must be willing to
adapt to a differences from internally developed
software products. There are considerable savings; both
in terms of cost and time, and these can be significant
drives of adoption of this model. Since to some extent
the target market for these offerings is well understood,
this type of solution can and has been highly successful
and may well be the preferred model of the future.

4. Conclusions
Unicon recently discontinued commercial product
offerings for Academus, but continues offering
commercial support. Academus products were released
as open source called “Open Toro Portal”, and Unicon
spends “bench time” maintaining the source with a
priority scheme based on community vote. In a recent
interview with Unicon CEO John Blakley, he offered the
following reasons for this decision.
• Product licensing in the higher education market is
difficult. Recent budget constraints forced Unicon
to lower its pricing to encourage adoption. While
this did result in more initial users, it was not a
sustainable revenue model.
• Surprisingly, support licensing was also not
profitable. One would expect a support model to
have reasonable predictable costs. Unicon’s CEO
suspects this may have been a result of poor
execution, ironically citing the community-oriented

944

spirit of open source lead for-hire developers to do
extra work not covered by the support contract.
• Professional services can be profitable with good
execution, as in other markets and non-open source
offerings. This was a primary driver behind support
of Academus as an open source product.
• Academus earned a brand name in the market and
Unicon did not want to retire the brand or associate
it with an unsuccessful connotation.
Unicon
believed Academus would be successful, and
perhaps should have focused on services and
support execution from the beginning.
Another interesting comment is that Blakley noted
that preparing the codebase to be released as open
source resulted in a nontrivial cost. “Decommissioning”
software as open source is not a matter of simply posting
it on Sourceforge; the code may have proprietary pieces
to be removed or other artifacts (such as copyright
notices, documentation conventions, and coupling to
proprietary internal libraries) that must be removed.
Furthermore, the Unicon names stands behind the Open
Toro portal, and the company has had to expend bench
time to maintain the software for the community.

5. Assessing the Future
Unicon believes the future for their company lies in
being the main provider of open source professional
services for higher education. After decommissioning
Academus, the company has re-focused and expanded
their services offerings to include not only support for
uPortal, but also for open source projects Sakai,
Shibboleth, Zimbra, and CAS. To assess the future
viability of Unicon’s transformation to a services-only
open source company, we conducted external factors
and SWOT (Strength, Weakness, Opportunity, Threat)
analyses, and present the results in this section.

5.1 External Factors Evaluation
The external factor evaluation matrix (EFE):
External Factors
Weight Rating Score
1. Open source more than education
2. Global market potential in 3rd world
3. Community based technical support
4. Expanding demand
5. Lower cost
6. Quality
7. Licensing that permits freedom

.08
.09
.06
.08
.09
.08
.06

4
4
3
4
4
3
3

.32
.36
.18
.32
.36
.24
.18

.09
.08
.07
.04
.08

4
3
2
2
3

.36
.24
.14
.08
.24

Threats
8. Declining economy
9. Proprietary software
10. Patents
11. Exploited OS vulnerability
12. Hackers

13. Denial of service
14. Exploited unknown application

.04
.06

1
2

.04
.12

Total
1.00
3.18
The total weighted score is 3.18, above the industry
average of 2.5, suggesting Unicon is doing well in the
open source software industry for higher education.

5.2 SWOT Analysis
Unicon has many internal strengths. They include:
1. Highly experienced employees with a good company
culture– good people who like to help people
2. Open Source is where the action is
3. Award Winning Company which mirrors the industry
- slow and steady
4. Service-oriented philosophy
5. Experts in a narrow field – higher education only
6. Diversification of Product Lines
7. Match sales staff to client - CIO’s have changed
8. Salary paid to their employees is competitive and
Phoenix is a good market for engineers
9. Experienced corporate board that meets quarterly

Unicon’s attempt to market a product by creating a
branded offering of uPortal and a 3.18
proprietary enhanced
version of the portal for course management was not
successful. During the development and go-to-market
cycle they did have concerns, mainly the market did not
grow and smaller schools did not have the budget to
purchase the software. They also found it was very
expensive for their company to keep up the software and
make sure it was current. In 2005 Unicon went back to
its roots. They transformed into a technology delivery
company only with open source software. They believe
that open source holds the future of the technology
industry. The two greatest strengths of open source
software are that there are no reoccurring costs and you
can build whatever the client wants. As a result of
changing their strategy, Unicon has been recognized as a
leading Arizona company by Phoenix Business Journal
and the Chandler Chamber of Commerce.

6. Acknowledgements
The authors acknowledge the help of Jimmy Layne,
Unicon’s VP of Sales at Unicon, and John Heidenreich,
former ASU graduate student working in open source.

Unicon’s internal weaknesses include:
1. Narrow field of higher ed degree granting institutions
2. Only 175 customers out of a complete market segment
of 2000 universities.
3. Leads all come from viral marketing - can’t have any
mistakes since universities “talk”

7. References
[1] Stallman, Richard, “The GNU Manifesto”,

http://www.gnu.org/gnu/manifesto.html

Unicon’s external threats include:
1. Barrier to open source software just began fading a
year or two ago
2. The cost for a university to make a change in their
software is very expensive
3. Economy is contracting—need to offer same services
but at a lower cost

[2] Goldman, R. and Gabriel, R.P. Innovation happens

elsewhere: open source as business strategy.
Morgan Kaufmann, San Francisco, 2005.
[3] Raymond, Eric, “The Magic Cauldron”,

http://www.catb.org/~esr/writings/magiccauldron/magic-cauldron.html
[4] Fleury, Marc, “Why I Love Open Source”,

Unicon’s external opportunities include:
1. Open source more than education. Govt., finances,
and life sciences are all areas that could possibly
benefit from open source software for example.
2. Global market potential for 3rd world countries.
Pakistan, Africa, even Asia.
3. Community based technical support. “Large company
abilities, small company values.”
4. Lack of competition.
5. Increasing knowledge and experience.
6. uPortal enhancements. Unicon has developed and
contributed over 40 new uPortal enhancements.
7. Cooperative support program. Unicon has 34
academic institutions involved in this program.
8. Implementation, installation, configuration, and
branding strategies are key components to Unicon’s
commitment to open source through project planning.

http://www.jboss.org/modules/html/white3.pdf
[5] Spinellis D. and Szyperski C. “How is open source

affecting software development?” IEEE Software,
21(1):28{33, January 2004.
[6] Stewart, K. J., and Ammeter, T. “An Exploratory

Study of Factors Influencing the Level of Vitality
and Popularity of Open Source Projects” Proc. of
the 23rd International Conference on Information
Systems, 2002.

945

C O V E R

F E A T U R E

IGSTK: An Open Source
Software Toolkit for
Image-Guided Surgery
Kevin Gary, Arizona State University
Luis Ibáñez and Stephen Aylward, Kitware Inc.
David Gobbi, Atamai Inc.
M. Brian Blake and Kevin Cleary, Georgetown University

Reliable software is a critical component of image-guided surgical applications, yet
costly expertise and technology infrastructure barriers hamper current research and
commercialization efforts.The Image-Guided Software Toolkit applies open source
methods to provide a safe, inexpensive, robust, shareable, and reusable infrastructure.

S

urgeons increasingly rely on dynamic, threedimensional medical images for instrument
guidance and clinical decision making during
minimally invasive procedures. Using imageguided, or computer-aided, surgical systems
results in less trauma for the patient. These systems are
particularly useful for brain, spinal, orthopaedic,
ear/nose/throat, and other operative procedures that
require extreme precision or involve sensitive structures.
As Figure 1 shows, these systems provide physicians
with a virtual, real-time display of the anatomy located
in a surgical instrument’s region.1 The typical display
combines images acquired before the intervention with
graphical representations of the instruments; it also can
include intraoperative image updates of the anatomy.
Tracking systems indicate the location of surgical instruments relative to the patient’s body; this helps guide the
physician to the specific anatomical target and provides
an X-ray-like view of what lies beneath an instrument
before moving it.
Software is the most critical technology in imageguided surgery systems, which have tightly coupled components, complex mathematical processes, and
demanding synchronization constraints. The software
must integrate information from tracking systems, correlate this data with the relevant images, and display
real-time updates of the instruments and patient

46

Computer

anatomy. Moreover, because it is used in life-critical
applications, the software must be carefully designed to
ensure ease of use, robustness, and stability.
Traditional software engineering techniques do not
provide cost-effective, nonintrusive methods for validating image-guided surgery systems for clinical use,
presenting a barrier for those trying to develop innovative technologies. Open source software has the potential to meet these needs; however, it remains largely
unexplored because many academic research groups
and small businesses lack the software engineering
infrastructure and expertise to carefully design and test
robust software.
These considerations led to creation of the ImageGuided Software Toolkit (www.igstk.org) by a multidisciplinary team of software engineers and medicalimaging scientists. IGSTK contains the basic software
components for developing an image-guided surgery
system, including a component for controlling the
tracker as well as a display component for overlaying
images of patient anatomy and surgical instruments.

TECHNICAL REQUIREMENTS
To determine IGSTK’s technical requirements, we analyzed several specific surgical procedures. For example,
radio frequency ablation (RFA) is a minimally invasive
treatment of cancerous liver lesions. In this procedure,

Published by the IEEE Computer Society

0018-9162/06/$20.00 © 2006 IEEE

a radiologist first locates the lesion
using X-ray computed tomography
(CT) or magnetic resonance imaging
(MRI), then advances the RFA probe (a
needle) through the skin and into the
lesion using ultrasound imaging, and
finally connects the probe to a generator that sends RF energy into the tumor.
To ensure cell death, the treatment generates temperatures up to 100°C at the
tip of the probe lasting 10 to 30 minutes, causing burns as large as 7 cm.
Allowing for a 5-cm spherical lesion
with a surrounding 1-cm “negative
margin” burn, the radiologist must
position the RFA probe within 5 mm of
its intended location.
Figure 1. Image-guided system for brain surgery. An optical tracker (top center)
We then compared the procedures to reveals the relative location of surgical instruments and the patient’s anatomy.
determine both unique and common Photo courtesy of Dr. Richard Bucholz, St. Louis University School of Medicine.
components. For example, the RFA
application requires a component that provides 3D the anatomy beneath these instruments. A four-quadtracking of a 2D ultrasound probe so that 2D ultrasound rant view—axial, sagittal, coronal, and 3D—is typical,
images can be organized into a 3D volume. This com- but many variations exist; for example, oblique reforponent addresses the challenging task of reconstructing matting of the images at any angle can provide better
a valid liver image despite liver movement due to respi- image guidance.
ration, cardiac motion, and probe pressure during acquisition of a sequence of 2D ultrasound images.
COMPONENT-BASED ARCHITECTURE
Another component uses image-based registration to
IGSTK supports safety-critical surgical applications
account for organ movement. This component tracks in which software errors can lead to catastrophic results.
the ultrasound probe and patient to initialize the regis- To minimize the risk of patient harm from possible mistration of the intraoperative 3D ultrasound data and the use of the toolkit, IGSTK uses a component-based archipreoperative CT or MRI data. It then refines that initial tecture. Every component has a well-defined set of
registration using details in the images to account for features governed by a state machine, and strongly typed
the intraoperative displacement of a designated organ. interfaces enforce interaction contracts between comThe RFA application overlays the tracked needle onto ponents. State machines are encapsulated within comfused ultrasound/CT/MRI data to intraoperatively guide ponents to prevent clients from manipulating state
the surgical procedure. Figure 2 shows one slice of coreg- outside the contract specified by the interface—this
ensures that a component’s state is explicit and always
istered MRI and 3D ultrasound liver images.
Based on these component-level analyses, we determined
that a typical image-guided surgery system combines
• a control computer;
• software for image processing, control, and the user
interface; and
• a tracker for localizing instruments and patient
anatomy in 3D space.
The surgeon first imports a preoperative CT or MRI
scan into the computer and attaches a reference target to
the anatomy to compensate for any inadvertent motion
by the camera or patient. The computer then registers
fiducials—small markers placed on the anatomy prior
to the scan—in the image and tracking space and computes a transformation matrix between these two coordinate systems. This enables the system to track surgical
instruments, including probes or pointers, and display

Figure 2. Radio frequency ablation of liver cells.This image
shows a slice of fused 3D ultrasound and MRI liver data.
April 2006

47

InitialState
SpatialObjectValidInput
NonTrackedState
TrackingEnabledInput

TrackingDisabledInput

TrackedState
TrackingLostInput TrackingRestoredInput
TrackingDisabledInput
TrackedLostState

Figure 3. State machine for IGSTK spatial object component.
A state machine is defined by a set of states (black), a set of
inputs, and a set of directed transitions (blue) between states.

known and that all transitions between states are valid
and meaningful.

State machines
A state machine is a model of behavior defined by a set
of states, a set of inputs, and a set of directed transitions
between states. Transitions in a state machine change the
current state in response to some stimulus or input. A
behavior can execute during a transition or while entering or exiting a state. Figure 3 shows the state machine
implementation for an IGSTK spatial object component.
The formal semantics of state machines,2,3 coupled
with a large body of established research, influenced our
decision to adopt this trusted architectural pattern. State
machines in IGSTK provide
• safety and reliability. State machines ensure that
component behavior is deterministic and that all
components are in a known and error-free state at
any given moment.
• a cleaner design. Because developers must anticipate
all possible inputs, states, and transitions, state
machines encourage and enforce a cleaner and more
robust design without untested assumptions.
• API simplicity. A focused, clearly expressed application programming interface is essential for supporting
robustness and reliability. In the context of surgical
guidance, flexibility and abundance of features are
undesirable because they create more opportunities
for things to go wrong during an intervention.
• a consistent integration pattern. IGSTK’s value as it
matures will be tied to the incorporation of additional
functionality at the component level, often in the form
of reusable code from existing toolkits. State machines
provide a consistent pattern for integrating this functionality while adhering to the safety-first principles
necessitated by the application domain.
• quality control. State machines facilitate code cov48

Computer

erage in terms of lines of code tested, as well as path
coverage on a per-component basis. Using code not
based on state machines can result in applications
that, at runtime, enter into any number of untold
states unexplored by the developer, leading to error
conditions that may not become visible to users.
Most component-based architectures do not exploit
explicit state machines to represent component state. Such
relaxed programming practices lead to components with
poorly defined and understood states—this can cause the
components to behave unpredictably, which is unacceptable in image-guided surgery. In contrast, IGSTK encourages explicit knowledge of component state and
determines whether a given behavior can be executed
while in that state. The toolkit thus combines reliability
with dynamic management of available behavior.4

Components
Component-based computing is prevalent in modern
computing architectures. Strict contract-based interfaces
govern component boundaries within IGSTK, and component realizations encapsulated behind these boundaries make it possible to manage the complexity required
in a safety-critical domain.
IGSTK exploits components in several ways. First,
rigid component boundaries with well-understood and
easily visualized interaction patterns enable rigorous
testing and the creation of component-level “safe
zones.” In addition, by mapping specialized implementations of behaviors to concrete realizations of base
interfaces, the framework supports controlled extensibility via bounded component behavior. Moreover,
IGSTK safely and predictably integrates third-party
component implementations such as the National
Library of Medicine Insight Segmentation and Registration Toolkit (ITK; www.itk.org) and the open source
Visualization Toolkit (VTK; www.vtk.org). Finally, from
a development process perspective, team members can
focus on individual components’ capabilities rather than
on component interactions.
These advantages do not come without costs. Extra
method incursions due to wrapping third-party interfaces, as well as overhead associated with dynamic binding in a polymorphic language such as C++, incur
potential performance penalties. As with any object-oriented software system that relies on encapsulation, specialization, and loose coupling, abstraction layers
obscure linear views of realized functionality, making it
difficult to unwind component interactions to troubleshoot issues during development.
Despite these concerns, we decided to leverage a component-based architecture to meet IGSTK’s high-priority
safety requirements. As Figure 4 shows, this architecture
consists of four main components: a tracker, spatial
objects, spatial object representations, and viewers.

Group SpatialObject

PulseGenerator

View2D-axial

Tracker-1

Hardware
Tick

Needle
representations

CT image SpatialObject

Pushing

Tick

Pulling
Pulling

Pulling

PulseGenerator

View2D-sagittal
View2S-sagittal
Pulling
IGSTK
application

CT image
representations

Needle SpatialObject

Tools

Fluoroscopy
representations

Fluoroscopy SpatialObject

Position and
orientation from
the tracker

Spatial objects

Tracker

View2D-coronal
Clinician
View3D

Viewers

Spatial object
representations

Figure 4. IGSTK component architecture. Strict contract-based interfaces govern component boundaries, making it possible to
manage the complexity required in a safety-critical domain.

Tracker. This component, which directly incorporates
a state machine into code donated by Atamai Inc., provides an object-oriented representation of tracking
devices, tracked tools, and all relevant static and
dynamic information associated with tracking such as
position and orientation. It also manages communication with tracker hardware and updates internal state
information at the required frequency rate; spatial
objects then pull information about their corresponding tools from the tracker at the desired rendering rate.
Spatial objects. Central to maintaining the types and
physical locations of objects in a surgical environment,
these components are used to hold the geometrical information of a surgical scene. IGSTK spatial object classes
encapsulate ITK spatial objects inside a restricted API
subject to state machine control. In this way, they retain
the functionality of ITK classes without exposing all
their flexibility.
Spatial object representations. While spatial objects
provide a geometric description of surgical instruments,
spatial object representations specify how they should
appear on the screen. Each spatial object representation
is responsible for displaying its associated spatial objects
within a specific context, which is typically either a 3D
view of the patient anatomy or a 2D tomographic or
projective image through the patient anatomy.
Viewers. These components present renderings of surgical scenes and are the clinician’s main source of information. Viewers are built using VTK classes encapsulated
into a restrictive API and are subject to state machine control. They limit interactions to a touch screen, as a keyboard or mouse is not a convenient input device in the
surgical environment.

Viewers aggregate spatial object representations,
which in turn derive their geometric information from
the spatial objects. Spatial objects query the tracker to
determine current position and orientation. To synchronize scene generation and provide an acceptable
rendering frequency, IGSTK uses a PulseGenerator
object to generate “ticks” and a real-time clock mechanism to check timing intervals for validity of scene information. The current IGSTK implementation runs on
common operating platforms such as Linux, Mac OS,
and Windows, but continuing work aims to extend
IGSTK for real-time OS services.

Component interaction and state machines
In Figure 4, components request information or services from components to their right; these components
move information or services left via events.4 In keeping with the principle of safety by design, this interaction
occurs under the purview of state machines. By translating events to state machine input, IGSTK avoids errorprone if/else conditions on return values.
In the simple example shown in Figure 5, Client is a
component requesting some service and Service is the
called component. Initially, Client is performing some
behavior while in a well-known state (1). Client then
requests service (2) via a public method invocation on
Service. Service accepts the request (3) and translates it
to an input to its state machine (4). Based on the transition, a private method invocation on component Service
also can occur (5). Upon successful completion of the
computation (6), Service generates an event (7) containing the requested information and dispatches the
event to Client (8). Client translates the event into an
April 2006

49

these include nonfunctional
requirements, especially
safety. This definition includes completeness, emS1
S3
bracing traditional verifiRequestX
input 4
cation and validation.
2
Client
Service
makeRequest
3
1
Given the IGSTK develperform
Attempt
someBehavior
+ RequestForX
SomeBehavior
opment
team’s distributed
ToDoX
5
– performX
+ eventHandler
nature, the complexity of
invokeEvent
6
8
9
7
Xsuccess
the algorithms impleS2
S4
mented, and our limited
access to subject-matter
experts (SMEs), we decided
early in the project against
adopting a “heavy” softFigure 5. Example of state-machine-managed interaction between components. Beside each
ware process. Instead,
component is the relevant part of its private state machine.
IGSTK supports correctness
via a collection of best pracinput to its state machine (9), thereby avoiding error- tices woven into an agile process framework.
prone conditional logic on return values.
Our top 10 best practices for safety-critical developTo a conventional programmer, this sequence may ment with agile methods are:
seem awkward compared to simply calling a method
1. Recognize that people are the most important
and checking its return value, but it has several salient mechanism available for ensuring high-quality software.
features that robustly enforce safety. First, a component The IGSTK team consists of developers with considermaking a request initially transitions to a state indicat- able expertise in the application domain, supporting
ing it is busy processing; by not performing a computa- software, and tools. Their collective judgment outweighs
tion as an action associated with a transition, the state process mandates.
machine avoids situations in which a component is
2. Facilitate constant communication. To prevent dis“between” known states (IGSTK state machines are tributed team members who are working on decoupled
Moore machines).2
components from becoming too isolated, IGSTK memIn addition, IGSTK state machines use fully populated bers participate in a weekly teleconference and meet in
state transition tables—no matter what input is given, a person twice per year. IGSTK also employs a mailing list,
state machine will take a known and verified transition. instant messaging, and a wiki for online collaboration.
Further, public request methods delegate the decision
3. Produce iterative releases. IGSTK’s development
making to the state machine, which is the only one cycle includes twice-yearly external releases. We conallowed to trigger private behaviors in response to the sidered six months too long a horizon to manage develrequests—in other words, IGSTK executes a behavior opment, so internal releases are broken down into
only when it is certain that doing so is safe. Finally, approximately two-month iterations. At the end of an
returning information via events translated to state iteration, team members perform quality reviews and
machine inputs avoids error-prone conditional logic. move code considered stable to the main repository.
Instead, the logic is embedded in a component’s state
4. Manage source code from a quality perspective.
machine and can be visualized and verified explicitly.
IGSTK defines different configuration management policies to satisfy different quality criteria. Codelines with
ROBUST SOFTWARE DEVELOPMENT
separate policies—for example, a main repository and
We believe that surgical applications’ complex re- a sandbox—let developers collaborate on code that
quirements, unique development team composition, might not yet meet stringent requirements. Exploiting
reliance on preexisting software packages, and high-qual- configuration management approaches early in a proity standards are a natural fit for an open source approach. ject helps document and track quality progress.
IGSTK incorporates several quality-management best
5. Focus on 100 percent code and path coverage at
practices and requirements techniques that augment typ- the component level. Unit tests ensure complete code
ical agile methods to provide a uniquely robust process.
coverage across all platforms. We are also developing
customized visualization and validation tool machines
Best practices
to guarantee that correctness properties within all
IGSTK quality-management processes focus on cor- IGSTK state machines are verified with every nightly
rectness—that is, ensuring that the software fulfills all build. In addition, dynamic analysis tools prevent memof, and only, the requirements in an error-free manner; ory leaks and access violations.
50

Computer

In review

Conceptualized

Post to Wiki/∧Code.
CheckInSandbox

Abort

Defined

Discuss
as group

Reviewed

Modifications
requested

Revise

Defined

Reject

Move to log area
of Wiki/∧Code.
CheckInSandbox
Reopen

Accept

Accepted
Enter into PHP BugTracker/
CheckInMainBranch

∧Code.

REQ open for
implementation

Development

Implemented

Code review and
inspections/∧Code.
RunNightlyDashboard

Reviewed

REQ verified

Logged

Nightly
generation as
LaTeX/PDF files

Continuous
REQ archival

Figure 6. IGSTK requirements management process as a UML state diagram. New requirements undergo an iterative process in
which team members review, discuss, potentially modify, and ultimately accept or reject the requirements.

6. Emphasize continuous builds and testing. IGSTK
uses the open source Dart tool (http://public.kitware.
com/Dart/HTML/Index.shtml) to produce a nightly
dashboard of build and unit test results across all supported platforms.
7. Support the development process with robust tools.
In addition to Dart, IGSTK employs the CMake open
source cross-platform build solution (www.cmake.org),
KWStyle (http://public.kitware.com/KWStyle) for sourcecode style checking, and Doxygen, an open source documentation system (www.stack.nl/~dimitri/doxygen). Best
practices for coding and documentation posted on the
wiki augment these tools.
8. Manage requirements iteratively in lockstep with
code management. As requirements evolve and the code
matures, adopting flexible yet defined processes for managing requirements becomes necessary.
9. Focus on meeting exactly the current set of requirements. Traceability is needed in safety-critical domains,
particularly in surgical applications that need government approval, and implies heavy process structures—
large documents and invasive tools. IGSTK addresses
this problem with continuous requirements review, lightweight tools, and codeline policies.
10. Evolve the development process. Through constant communication, IGSTK members recognize when
to approach the complexities they face within the current process framework, when “tweaks” are required, or
when to adopt entirely new practices.
These best practices encompass IGSTK’s “lightweight”
approach to robust software development. We do not
equate lightweight with process “ignorance”; we readily
recognize that simply applying a collection of good prac-

tices without structure will not lead to software quality.
Our best practices include process structures—namely,
people, managed requirements, continuous testing, and
iterative development. Nor does lightweight mean
“optional” process execution; IGSTK developers are not
free to simply ignore best practices when convenient.

Lightweight requirements management
Because significant expertise is needed to develop effective image-guided surgical applications, IGSTK tightly
integrates iterative involvement of SMEs with code management. To accomplish this, we introduced a customized requirements management process that extends
traditional agile methods.
Agile requirements management. IGSTK developers categorize new requirements as architecture, style,
or application requirements. We derived these classifications from earlier work on component-based product
line analysis and development.5 Team members introduce new architecture and style requirements as components are developed, while application requirements
evolve through interaction with clinicians. Reviewing,
implementing, validating, and archiving these requirements is a collaborative process closely integrated with
application development.
Figure 6 illustrates the IGSTK requirements management process as a Unified Modeling Language (UML)
state diagram. First, a developer who identifies a new
potential requirement posts a description on the wiki
and, at the same time, might enter initial code that fulfills the requirement into a sandbox repository. The
requirement then undergoes an iterative process in which
team members review, discuss, potentially modify, and
April 2006

51

Patient’s body

IGS software application

Tracking hardware

Guidewire tool

Initialize application

Initialize hardware

Initialize tools

Fiducials positioned on body

Preoperative scan taken

Load Dicom

Initial configuration of display

Confirm tracking and IGS in sync

Initiate tracking

Select fiducial on IGS

Select actual fiduciation

Enable image overlay

Evaluate registration

Load 3D view

Configure software view

Record preop display

Inspect entry target/angle

Record postop display

Perform needle placement

Document procedure

Figure 7. UML activity diagram illustrating radio frequency ablation application flows. Such diagrams facilitate collaboration
between IGSTK software engineers and subject-matter experts.

ultimately accept or reject the requirement. If accepted,
the requirement is entered into an online database and
marked as “open”; once the supporting software is
implemented and its functionality confirmed, the requirement is marked as “verified.” The team automatically
archives all verified requirements after nightly builds.
Conceptualizing requirements. The disparity of
knowledge between software engineers and SMEs, who
communicate in totally different terms, creates a barrier
to conceptualizing requirements. IGSTK software engineers collaborate with clinical SMEs by documenting
surgical applications workflow using UML activity diagrams. Figure 7 is a UML diagram of the RFA application described earlier. We created several such diagrams
52

Computer

during the initial application requirements gathering
phase to help identify the required functionality for
IGSTK components.
Defining requirements. Safety-critical applications
demand that clinicians understand the functionality of
the underlying software they use. To create robust software, IGSTK developers leverage the agile methods
approach to define requirements at each level. In addition, UML activity diagrams provide a visual communication medium for software engineers and clinicians. This
tight interaction between SMEs and software engineers
and the close traceability between requirements and software ensure that software is well defined, managed, and
understood by those who will ultimately use it.

I

mage-guided surgery applies leading-edge technology
and clinical practices to provide better quality of life
to patients who can benefit from minimally invasive
procedures. Reliable software is a critical component of
image-guided surgical applications, yet costly expertise
and technology infrastructure barriers hamper current
research and commercialization efforts in this area.
IGSTK applies the open source development and delivery model to this problem. Agile and component-based
software engineering principles reduce the costs and risks
associated with adopting this new technology, resulting
in a safe, inexpensive, robust, shareable, and reusable
software infrastructure. ■

Acknowledgments
This research is supported by the National Institute
of Biomedical Imaging and Bioengineering at the
National Institutes of Health (NIH) under grant
R42EB000374, and by US Army grant W81XWH-041-0078. The article’s content does not necessarily reflect
the position or policy of the US government. We thank
the IGSTK project team, including Rick Avila, Patrick
Cheng, Andinet Enquobahrie, Julien Jomier, Hee-Su
Kim, Sohan Ranjan, and James Zhang. We also thank
the IGSTK advisory board for their advice throughout
the project: Will Schroeder of Kitware Inc.; Ivo Wolf of
the University of Heidelberg; Peter Kazanzides and
Anton Deguet of Johns Hopkins University; and Ingmar
Bitter, Matt McAuliffee, and Terry Yoo of the NIH.

References
1. R.L. Galloway Jr., “The Process and Development of ImageGuided Surgical Procedures,” Ann. Rev. Biomedical Eng., vol.
3, 2001, pp. 83-108.
2. J.E. Hopcroft, R. Motwani, and J.D. Ullman, Introduction to
Automata Theory, Languages, and Computation, 2nd ed.,
Addison-Wesley, 2000.
3. A.M.K. Cheng, Real-Time Systems: Scheduling, Analysis, and
Verification, Wiley, 2002.
4. E. Gamma et al., Design Patterns: Elements of Reusable
Object-Oriented Software, Addison-Wesley, 1995.
5. M.B. Blake et al., “Use Case-Driven Component Specification: A Medical Applications Perspective to Product Line
Development,” Proc. 2005 ACM Symp. Applied Computing,
ACM Press, 2005, pp. 1470-1477.

Kevin Gary is an assistant professor in the Division of Computing Studies at the College of Science and Technology at
Arizona State University’s Polytechnic campus. His research
focuses on software engineering, particularly software

process. Gary received a PhD in computer science from Arizona State University. He is a member of the IEEE. Contact him at kgary@asu.edu.

Luis Ibáñez is a senior research engineer at Kitware Inc.,
which develops visualization, graphics, and image-processing software solutions. His primary research interest is
open source software for medical image analysis. Ibáñez
received a PhD in image processing from the Université de
Rennes I, France. He is a member of the IEEE Engineering
in Medicine and Biology Society, the IEEE Computer Society, the Medical Image Computing and Computer Assisted
Intervention Society, and the American Association for the
Advancement of Science. Contact him at luis.ibanez@kitware.com.

Stephen Aylward is chief medical scientist at Kitware Inc.
and president of the nonprofit Insight Software Consortium, which promotes open source software for medical
image analysis. His research interests include model-based
image registration and vascular network segmentation and
characterization. Aylward received a PhD in computer science from the University of North Carolina at Chapel Hill.
He is an associate member of the IEEE and an associate
editor of IEEE Transactions on Medical Imaging. Contact
him at stephen.aylward@kitware.com.

David Gobbi is the vice president, CEO, and lead software
designer of Atamai Inc., which develops image-analysis,
visualization, and control software for medical research
applications. His specialties are image processing and virtual
toolkit development. Gobbi received a PhD in medical biophysics from the University of Western Ontario, Canada.
Contact him at dgobbi@atamai.com.

M. Brian Blake is an associate professor in the Department
of Computer Science at Georgetown University. His
research focuses on service-oriented computing, component-based software engineering, and workflow modeling.
Blake received a PhD in information and software engineering from George Mason University. He is a senior member of the IEEE. Contact him at blakeb@cs.georgetown.
edu.

Kevin Cleary is an associate professor in the Department
of Radiology’s Imaging Science and Information Systems
Center at Georgetown University Medical Center. His
research focuses on image-guided surgery and medical
robotics. Cleary received a PhD in mechanical engineering
from the University of Texas at Austin. He is a member of
the IEEE. Contact him at cleary@georgetown.edu.
April 2006

53

Distributed Version Control for
Curricular Content Management
Srikesh Mandala

Kevin A. Gary (Author)

Cisco Systems Inc.
Austin TX, USA
reddysrikesh@gmail.com

Department of Engineering
Arizona State University
Mesa AZ, USA
kgary@asu.edu

Abstract—Educators have at their disposal many digital
content sources, from textbook publishers to open
courseware repositories (OCRs) to specialized collections
to shared resources from a network of peers. It is rare one
needs to go create new lecture materials, instead one can
download and adapt to fit their needs. The proliferation of
such resources is expected to result in great productivity
for educators, particularly those in higher education where
time demands relegate content development to the backburner. But are we seeing such productivity? Are
courseware repositories spawning heavy reuse? Or are
issues integrating content to courses causing a loss in
productivity and greater frustration? These are the
questions being investigated by the Distributed Version
Control for Curricular Content Management project. This
work-in-progress project has conducted a local faculty
survey of curricular content development and used the
results to drive the initial implementation of a distributed
version control tool for curricular content management.
Keywords—curriculum development, software tools

I.
INTRODUCTION
At the 2013 ACM Special Interest Group for Computer
Science Education conference (SIGCSE 2013), Peter Norvig
led a Google session on Massively Open Online Courses
(MOOCs). In concluding his presentation, he indicated that
(paraphrase) we have to start thinking of courseware
development as an engineering process, much like software
development. We agree with this perspective, as we believe
there has been a significant shift in courseware requirements
driven by continuous change, change in learners’ content
consumption behaviors, and the disruption caused by online
technologies. Linear (waterfall?) workflows need revamping in
the face of these forces; in fact we contend this is already
taking place and it is the understanding of the engineered
process with associated tool support that will drive innovation.
The Distributed Version Control for Curricular Content
Management (DVC4CCM) project is a first step at applying a
software development best practice to the curricular content
development process. The problem of curricular content
management is similar to an open source community source

978-1-4673-5261-1/13/$31.00 ©2013 IEEE

code control problem. In open source, the ability to take and
customize the source code to meet domain requirements is
central to the value proposition. The problem of an instructor
obtaining, customizing, and integrating curricular content from
multiple sources is not unlike the open source problem. The
instructor must 1) identify resources (or subsets of resources),
2) ensure the resources are available for use in the target
context (licensing), 3) adopt the resources into the learning
context (formats and modalities), and 4) ensure the content
flows in a seamless fashion to support learning outcomes
(integration and customization). These activities are laborious
and frustrating, as the time to locate, assemble, and integrate
content from available sources can be as burdensome as the
time it takes to create content from scratch.
The larger research question the DVC4CCM project aims
to address is the understanding and improvement of curricular
content management patterns for higher education instructors.
The methodology uses a survey instrument to understand
curricular reuse, and based on the results to identifies viable
version control workflows from the software engineering
community. The initial technology solution is a customized
wrapper around the popular Git distributed version control
system to facilitate content integration and reuse. This work-inprogress paper frames the research problem, describes the
survey vehicle, and presents the initial tool implementation.
II.

RESEARCH METHODOLOGY

Source code control and configuration management (CM)
are established competency areas in software engineering. CM
has largely evolved through client-server tools like the
Concurrent Versions System (CVS) or subversion (SVN)
which employ centralized repositories for software artifacts and
an optimistic approach to managing change. To generalize, the
centralized repository maintains the baseline copy of artifacts
(the mainline), developers work on local copies of artifacts, and
the client tool maintains metadata to describe the deltas against
the mainline. The most common and accepted ([6]) workflow
is a branch-and-merge, where developers work on shared
branches, and merge changes into the mainline. The prevailing
mantra is to ‘check in early and check in often’ to reduce the
probability of long, painful, and error-prone merges.
Techniques for managing long-running branches range from
creating sandboxes to maintaining private branches.

In a distributed version control system (DVCS), change is
managed among multiple distributed repositories, usually by
creating complete copies of artifacts in each repository
(though this is not always the case). The presumed centralized
repository to many endpoints model is replaced by what is
effectively a peer-to-peer repository approach.
The value proposition for DVCS over the traditional
centralized model is that change is allowed to proliferate, and
consumers can manage what changes they want by determining
which changes they pull from a peer repository. This is an
important improvement over traditional CM, which implicitly
tries to minimize change through the ‘check in early, check in
often’ mantra. In practical terms, the flexibility to define
advanced workflows opens the possibility for supporting new
collaborations between artifact creators. For our initial
exploration, we evaluated workflow patterns evolving in
DVCS practice, surveyed department faculty to determine
typical curricular creation patterns, and implemented a tool to
support a workflow for curricular content management.
A. CM Patterns for DVCS
Software houses are rapidly adopting DVCSs and evolving
best practices such as identifying appropriate workflows. We
reviewed workflows for software development with an eye
toward curricular content management, and summarize our
thoughts on some of the potentially applicable ones here.
1. Branch per developer – In this pattern, a coach is
responsible for the stability of the code base. Developers
maintain a cloned local repository in which to work, and a
public repository with read access to other members of the
team. When a developer is done with implementation s/he
commits changes to the local repository. When the developer
is ready to submit changes to the mainline, s/he pushes the
changes to the public repository and notifies the coach. The
coach pulls changes from the developer’s public repository to a
local repository and verifies they meet quality standards. If
they do, the coach publishes these changes to the mainline
repository and notifies other team members [1]. This
configuration pattern ensures the stability of codebase. It also
helps in maintaining releasable version of the project at all
times. This pattern is suitable for small agile teams, but the
setup would be extremely complex if the team is large, and
thus this pattern does not suit for medium to large teams [1].
2. Feature separation through named branches - In this
pattern [2], developers create named public branches in their
own repositories for working on distinct independent features.
Adopters select which features to assemble into an application
or system by determining what named features they wish to
pull and integrate from the various public options available.
The mapping of this workflow to curricular content could go
like this: an educator may get materials from one of the
repositories. When s/he makes changes a new named branch
from the repository is created. When another educator wants to
create a curricular module, s/he can pick from any named
branch based on features desired. If there is a central repository
maintainer, that person may choose to maintain and evolve the
mainline by merging in the named branches, but there is no
requirement on adopters to only use the mainline version.

This pattern helps to develop features collaboratively and
also allows the user to check for which feature a given change
was added. It is best suited for large to medium agile teams but
does not scale up well if the project has a lot of small features.

Figure 1: Feature separation through named branches

3. Working off named stable bases - Software developers
often work on a project’s mainline so that they get all the latest
changes and do not have to deal with huge merge conflicts
when committing their code. However, the mainline often has
unstable and untested code, which can cause rework problems
if changesets must be rolled back, potentially impacting all
changesets committed after the problematic changeset. The
‘Working off Named Stable Bases’ pattern solves this problem
by allowing developers to work off a stable tested revision
instead of potentially unstable mainline. When the revision is
stable it will be tagged as stable and other developers may start
working off this revision.

Figure 2: Working off named stable bases

This pattern is most suited for large projects where
developers work on unrelated tasks. This approach might not
be suitable for projects where many developers are working on
the same or related tasks [3].
4. Other configuration management workflow patterns We reviewed several other CM patterns. These were not
selected as they were either too close to the workflows
described above or did not fit the workflow requirements.
•
•
•
•
•
•
•
•
•
•
•

Episodic Collaboration [3]
Local Branching [1]
Private Versioning Workflow [4]
Incremental Workflow [4]
Independent Development Workflow [4]
Staged integration lines Workflow [1]
CVS-like Workflow [2]
One-off patch submission Workflow [2]
Centralized Workflow [5]
Integration-Manager Workflow [5]
Dictator and Lieutenants Workflow [5]

B. Faculty Survey
In order to figure out the process used by educators perform
collaborative courseware development, we created a survey for

the faculty of our department The survey consists of 26
questions about the sources of course materials and how one
incorporates change. 15 faculty completed the survey, though
not all answered every question. Survey result highlights:
•
•
•
•
•
•
•
•
•
•

87% said 61-100% of materials are in electronic form.
87% said 21- 60% of course material changes per year.
67% said they have revisited older versions of materials.
60% said adapting outside materials is inconvenient.
67% said adapting outside materials takes about 1-4 hours.
50% said they thought of sharing course materials with
others, but could not find the right platform/mechanism.
80% said they have incorporated changes to their notes
based on contributions from others.
40% said they reuse course materials from colleagues.
75% said they use email to share updated materials back
with the originator.
All (100%) said they construct their current course
materials using their last year's course materials.

Of course this is a small single-source sample, and the
results expected. It is not a stretch to think this is representative
of most places (certainly broader study is needed).
C. Workflow selection and implementation
We used these results to determine which of the workflows
we reviewed might address the most significant change
management issues. We determined that a combination of
Feature separation through named branches and Working off
named stable bases is the best candidate solution. The former
allows adopters to maintain a separate space for material, so
they can make available their own materials when needed and
make changes at the same time. However, this might work for
only a small number of materials. It will be difficult to manage
if there are many borrowers and if all borrowers submit back to
the originator with updated materials. The latter workflow will
help solve these problems. Adopters can go ahead and submit
their files to the repository and inform the originator, who can
tag it as stable for future adopters. Figures 4 and 5 depict the
customized workflows implemented in DVC4CCM.

can create a repository or download an existing repository. A
user who creates a repository is the owner of that repository
and can submit content to it. When a user submits content to
the repository, s/he will be asked whether to mark the
repository as stable. Only repositories which are marked as
stable are available for others to download. Adopters get a copy
of the materials in the repository by using the tool to indicate
intended use, including whether a new feature will be added. If
a new feature is intended, the backend creates a new branch.
Then later, once the user submits content and marks it as stable,
the named branch will be available for others to download.
After the feature owner submits content to the repository, both
feature owner and the original author have an option of
merging the named branch. If the feature owner merges, the
merged changes will go to the branch, but if the author merges,
they will go to main trunk (master). For now, both the branches
are compared against each other and names of the changed files
are displayed to the user. The user can download both the
repositories and do a manual merge and submit the updated
repository. Going forward, DVC4CCM will provide auto
merge feature and present merge conflicts to user.
VI. SUMMARY AND FUTURE WORK
The aim of this project is to create a change management
system which reduces the amount of effort needed by educators
to build course materials. This system helps educators share
their course materials with other educators. Similar to open
source software development, educators can also participate in
collaborative course development which ultimately results in
better course materials. The approach is to provide a tool
(DVC4CCM) that hides the complexity of DVCS from
curriculum authors and adopters, and to enforce workflows to
enable an innovative yet managed curricular evolution process.
There is significant work required to evolve this approach.
Study of existing curricular development processes needs to be
expanded, and perhaps include textbook publishing processes.
The initial analysis and definition of our customized workflow
should be re-evaluated in light of additional study, and data
collected on the utility of the workflow in managing curricular
change. The tool requires refinement and evolution based on
end user study. In short, as Norvig stated, the engineering of a
curricular content management solution is required, and as in
software, is a process that requires maintenance and evolution.
REFERENCES

Figure 3: Custom workflow if author merges branches

[1]
[2]
[3]
[4]

Figure 4: Custom workflow if feature owner merges branches

DVC4CCM is implemented as web application enforcing
these custom workflows. The tool wraps the open source
Javagit codebase (javagit.sf.net) and allows end users to
perform actions based on the state of the workflow and a
defined role with respect to that workflow. Users of the tool

[5]
[6]

D. Arve, “Branching Strategies with Distributed Version Control in
Agile Project”, 2010.
A. Babenhauserheide. (2011, September 14).Mercurial Workflows.
[Online]. Available: http://mercurial.selenic.com/wiki/Workflows
P. Rigby, E. Barr, C. Bird, D. German, P. Devanbu, “ Collaboration and
Governance with Distributed Version Control” , ACM Transactions on
Software Engineering and Methodology, Vol. V, No. N, 20YY
S. Berczuk, “Configuration Management Patterns”, Third Annual
Conference on Pattern Languages of Programs, Monticello, IL, 1996.
S. Chacon. (2011, September 14).ProGit. [Online]. Available:
http://progit.org/book/ch5-1.html
L. Wingerd and C. Seiwald. “High-Level Best Practices in Software
Configuration Management” Proceedings European Conference on
Object-Oriented Programming (ECCOP), pp. 57-66, 1998.

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu
Abstract
Software engineering education is a technologically challenging, rapidly evolving
discipline. Like all STEM educators, software engineering educators are bombarded with
a constant stream of new tools and techniques (MOOCs! Active learning! Inverted
classrooms!) while under national pressure to produce outstanding STEM graduates.
Software engineering educators are also pressured on the discipline side; a constant
evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the
technology, guidance on the adoption of project-centric curricula is needed. This paper
focuses on vertical integration of project experiences in undergraduate software
engineering degree programs or course sequences. The Software Enterprise, now in its
9 th year, has grown from an upper-division course sequence to a vertical integration
program feature. The Software Enterprise is presented as an implementation of a project
spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those
in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software
engineering and computer science education focus on content taxonomies and bodies of
knowledge. This is not a bad thing, but taken in isolation may lead educators to believe
content coverage is more important than applied learning experiences. There is literature
on project-based learning within computing as a means to learn soft skills and complex
technical competencies. However, project experiences tend to be disjoint [5]; there may
be a freshman project or a capstone project or a semester project assigned by an
individual instructor. Yearlong capstone projects are offered at most institutions as a
synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do
it all the time?
Project experiences, while pervasive in computing programs, are not a central
integrating feature. Sheppard et al. [6] suggests that engineering curricular design should
move away from a linear, deductive model and move instead toward a networked model:
“The ideal learning trajectory is a spiral, with all components revisited at increasing
levels of sophistication and interconnection” ([6] p. 191). The general engineering degree
program at Arizona State University (ASU) was designed from its inception in 2005 [7]
to be a flexible, project-centric curriculum that embodied such integration (even before
[6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division
course sequence to integrate contextualized project experiences with software engineering
fundamental concepts. The computing and engineering programs at ASU’s Polytechnic
campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

299
c 2013 IEEE
978-1-4673-5140-9/13/$31.00 

CSEE&T 2013, San Francisco, CA, USA

Board of Regents (ABOR) approved a new Bachelor’s degree in software engineering
(BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo
accreditation review shortly thereafter.
At the course level the Software Enterprise defines a delivery structure integrating
established learning techniques around a project-based contextualized learning
experience. At the degree program level, the Enterprise weaves project experiences
throughout the BS SE degree program, integrating program outcomes at each year of the
major. There are several publications on the manner in which the Software Enterprise is
conducted within a project course (for example, [8][9]]), and we summarize this in-course
integration pedagogy in section 2. The intent of this work-in-progress paper is to describe
extending the Enterprise as a spiral curricular design feature we refer to as the project
spine, and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a
student’s competencies from understanding to comprehension to applied knowledge by
co-locating preparation, discussion, practice, reflection, and contextualized learning
activities in time. In this model, learners prepare for a module by doing readings,
tutorials, or research before a class meeting time. The class discusses the module’s
concepts, in a lecture or seminar-style setting. The students then practice with a tool or
technique that reinforces the concepts in the next class meeting. At this point students
reflect to internalize the concepts and elicit student expectations, or hypotheses, for the
utility of the concept. Then, students apply the concept in the context of a team-oriented,
scalable project, and finally reflect again to (in)validate their earlier hypotheses. These
activities take place in a single three-week sprint, resulting is a highly iterative
methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right)
The Software Enterprise represents an innovation derived from existing scholarship in
that it assembles best practices such as preparation, reflection, practice (labs), and
project-centered learning in a rapid integration model that accelerates applied learning.
Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle
[10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on
maturing the delivery process, creating new or packaging existing learning materials to fit
the delivery model, and to explore ways to assess project-centered learning.

300

3. The Software Enterprise Project Spine
An innovation in the new BS in Software
Engineering at ASU has been the vertical adoption of
the Software Enterprise. Enterprise courses are now
required from the sophomore to senior years. This
innovation represents what [6] calls a professional
spine, as the Enterprise serves as an integrator of
learning outcomes for a given year in the major. We
refer to our project-centered realization as a project
spine, where foundational concepts are tied to project
work throughout the undergraduate program. There is
significant
computing
literature
on
projects
(embedded, mobile, gaming, etc.) to achieve learning
or retention outcomes. However, computing lacks a
framework for integrating concepts in a project spine.
The Enterprise is an implementation that moves
students from basic comprehension to applied
Figure 2. ASU Project Spine
knowledge to critical analysis outcomes. In the BS SE
at ASU, program outcomes are described at 4 levels: describe, apply, select, and
internalize. Students must achieve level 3 (select between alternatives) in at least 1
outcome and achieve level 2 (apply) in all others. The program outcomes for the BS SE
include Design, Computing Practice, Critical Thinking, Professionalism, Perspective,
Problem Solving, Communication, and Technical Competence. An example leveled
outcome description for Perspective is given in Table 1. The Enterprise accelerates level
3 outcomes by providing contextualized integrated experiences fostering decision-making
in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes.
Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in
business, global, economic, environmental, and societal contexts.
Level 1. Understands technological change and development have both positive & negative effects.
Level 2. Identifies and evaluates the assumptions made by others in their description of the role and
impact of engineering and computing on the world.
Level 3. Selects from different scenarios for the future and appropriately adapts them to match current
technical, social, economic and political concerns.
Level 4. Has formed a constructive model for the future of our society, and makes life and
career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical
competencies by assigning projects inclusive of the technical material covered in the
regular computing courses. So for example, junior projects (Software Enterprise III and
IV) emphasize technical complexities in Networks, Distributed Computing, and
Databases, while senior projects emphasize technical complexities in Web and Mobile
computing. The technical “focus area” courses are chosen more based on faculty expertise
and recruitment goals than software engineering outcomes; one can envision many
different areas represented by upper division courses here. These do help address the
concern that an accredited software engineering degree has an application area. A risk we
have not yet addressed is if the technical area impacts the software engineering process,
such as with a soon-to-be-introduced embedded systems focus area.

301

There are 2 additional aspects of integration to the project spine. As summarized in
section 2, the Enterprise integrates software engineering concepts throughout the project
experiences. Students in the sophomore year learn the Personal Software Process [11] as a
means to build individual understanding of time management, defect management, and
estimation skills. They then focus on Quality, including but not limited to testing. In the
junior year Enterprise students focus on Design (human-centered and system design
principles) followed by best practices in software construction, taken primarily from
eXtreme Programming. In the senior year students focus on Requirements Engineering
then Process and Project Management. The final aspect of integration is with soft-skill
outcomes such as Communication, Teamwork, and Professionalism (see Table 1).
Throughout the spine the project experiences are crafted to ensure variations on pedagogy
to address these outcomes. For example, in the freshman year students receive explicit
instruction in teamwork. In the senior year the emphasis is on formal documentation as a
means of communication. In the junior year, students work on service learning projects of
high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of
program adoption. There are examples of program design and lessons learned [5][12][13],
or reflections and recommendations on the software engineering education landscape
[14][15][16][17][18]. These are worthwhile guides but do not offer examples on
evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on
“Program Implementation and Assessment” which discusses a number of key factors in
program adoption, but is geared toward accreditation and not evaluation instruments. A
survey instrument is presented in [19] but is designed for comparison of a large number
of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate
programs in software engineering but more as an aggregate counting exercise in
knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software
Engineering project conducted a survey of graduate degree programs [20] and then
produced a comparison report [21] of graduate programs to the GSwE2009 reference
model, which includes data on program characteristics and in-depth profiles from 3
institutions. A recent study is Conry’s [23] survey of accredited software engineering
degree programs. Conry summarizes institutional, administrative, and curricular
(knowledge area) aspects in describing the 19 accredited programs as of October 2009.
Certainly program adoption measures from other engineering programs are also relevant,
though software engineering programs are unique due to the forces discussed in section 1.
Our next steps for the Enterprise-as-project-spine involve defining measures for
adoption impact, and determining how this concept fits with established patterns for
curricular maps in software engineering programs. We plan to use quantitative and
qualitative instruments to evaluate adoption. Quantitative data, such as program size,
institution type, faculty and student backgrounds, can be collected via available resources
(departmental archives or online) and direct surveys. Qualitative data can be collected
through survey instruments and interviews of all stakeholders (faculty participants,
administrators, and advisors). Different instruments may be used at different times to
evaluate “in-stream” attitudes versus post-adoption reflections. Defining and validating
these instruments is a significant area of work going forward.
The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in
software engineering. Taxonomies are useful and the sign of an emerging discipline. We

302

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas,
and plan to elaborate on these mappings. Specifically, we intend to produce CS2013
course exemplars. Further, the SE2004 report includes a section on program curricular
patterns, and we will propose new patterns based on the project spine concept, which we
hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the
New Century. The National Academies Press, Washington D.C., 2005.
[2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of
Knowledge (SWEBOK). Los Alamitos, CA, 2004.
[3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society
Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at
http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013.
[4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society.
Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software
Engineering. Joint Task Force on Computing Curricula, 2004.
[5] Shepard, T. “An Efficient Set of Software Degree Programs for One Domain.” In Proceedings of the
International Conference on Software Engineering (ICSE) 2001.
[6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the
Future of the Field, Jossey-Bass, San Francisco, 2008.
[7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. “A Flexible
Curriculum for a Multi-disciplinary Undergraduate Engineering Degree.” Proceedings of the Frontiers in
Education Conference 2005.
[8] Gary, K. “The Software Enterprise: Practicing Best Practices in Software Engineering Education”, The
International Journal of Engineering Education Special Issue on Trends in Software Engineering Education,
Volume 24, Number 4, July 2008, pp. 705-716.
[9] Gary, K., “The Software Enterprise: Preparing Industry-ready Software Engineers” Software Engineering:
Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group
Publishing. October 2008.
[10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984.
[11] Humphrey, W.S. Introduction to the Personal Software Process, Addison-Wesley, Boston, 1997.
[12] Lutz, M. and Naveda, J.F. “The Road Less Traveled: A Baccalaureate Degree in Software Engineering.”
Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997.
[13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor’s Program.
IEEE Software November/December 2006.
[14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. “Guidance for the
development of software engineering education programs.” The Journal of Systems and Software,
49(1999):163-169. 1999.
[15] Ghezzi, C. and Mandrioli. “The Challenges of Software Engineering Education.” In Proceedings of the
International Conference on Software Engineering (ICSE) 2006.
[16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. “Improving software practice through
education: Challenges and future trends.” Proceedings of the Future of Software Engineering Conference, 2007.
[17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the
Future of Software Engineering, Limerick Ireland, 2000.
[18] Mead, N. (2009). Software Engineering Education: How far We’ve Come and How far We Have to Go.
Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009.
[19] Modesitt, K., Bagert, D.J., and Werth, L. “Academic Software Engineering: What is it and What Could it be?
Results of the First International Survey for SE Programs.” Proceedings of the International Conference on
Software Engineering (ICSE) 2001.
[20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering
Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008.
[21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master’s Programs in
Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013.
[22] Bagert, D.J. & Chenoweth, S.V. “Future Growth of Software Engineering Baccalaureate Programs in the United
States”, Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005.
[23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of
the American Society for Engineering Education, Louisville, KY, 2010.
[24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). “Revision of the SE2004
Curriculum Model.” Panel at the ACM Conference of the Special Interest Group on Computer Science
Education (SIGCSE), Denver, CO, 2013.

303

Automated Process Support for Organizational and Personal
Processes
Kevin Gary, Tim Lindquist, Harry Koehnemann. Ly Sauer
Arizona State University
Computer Science Department
Mail Stop 5406
Tempe, AZ 85287-5406
yfppg@asu.edu
units. Current technology is growing at a rate that can be
difficult to track. Industry investments in desktop tools and
groupware must be leveraged against growth in local and
wide-area networks (LANs and WANs). In particular, the
growth of the Internet makes it possible to envision computer support of global, decentralized, business processes.

ABSTRACT
We propose two views on process: an organizational view
and a personal process view. Information technology applies
Automated Worktiow technology to define, execute, and
track an organization’s automated business processes. Calendaring tools provide a form of personal process view
through scheduled work items. However, the personal, or
individual, view of the process space has largely been
ignored. We maintain that as organizations become increasingly decentralized, a single organization’s process space is
becoming difficult to recognize. Individuals of the organization are asked to do work that spans organizational, functional, and even geographic boundaries. An integrated view
of organizational worktiows and personal processes is
needed to address these new demands. In this paper we
argue for the need to integrate organizational and personal
processes. We then propose a component-based process
modeling approach and supporting process architecture that
integrates these process spaces. Finally, we describe our
recent efforts at developing Java prototype process tools that
realize the proposed modeling technique and supporting
architecture.

These changes in industry and technology escalate the pressures put on information technology research. Providing
computer support for widely distributed organizations using
new technologies such as the Internet, Groupware, Calendar
Management, and Automated Workflow is at least an IT systems analyst’s headache. Determining the best way to integrate these tools to ensure maximum productivity is at best
an IT manager’s nightmare. In our view, the ability to
define, execute, and track business processes is central to the
ability to integrate these technologies in a widely distributed
setting and make their use productive. Therefore in our
research we focus on automated process support. In the
business domain, automating business processes is known
as Automated Workflow.
Workilow is the study of modeling and enacting business
processes by human and computer agents. Automated
Workflow adds an emphasis on applying current computer
and information technology in a workflow environment,
with the desire of automating parts of worktlows, or supporting entire workflows. ’

Keywords: Workilow, Personal Process, Components

1.0 Introduction
Recent changes in industry and technology are imposing
more demanding technical requirements on information
technology. In industry, organizations are downsizing and
becoming increasingly decentralized, often causing projects
to be managed across multiple organizations or functional

Automated Workflow has traditionally focused on defining
and automating business processes from the organization’s
standpoint. Little regard is given to managing overlapping
workflows in an individual workspace, or for even considering the personal processes of an individual when considering the productivity of the organization. The current solution
is to drop a set of personal productivity tools, such as calendaring tools, in the lap of the individual and let her/him
work it out.

Permission to mnke digitalhard
copies ol’nll or part of this material for
personnl or clnwoom
use is granted without fee provided that the copies
nre no1 mnde ar distributed for profit or commercial advantage, the copyright nolice, the title ofthe publication and its date appear, and notice is
given thl copyright is by pennissiou of the ACM, Inc. To copy othenvise,
10 republish, to post on servers or to redistribute
to lists, requires specific
permission nnd/or fee.
GROUP
Copyright

97 Phoenix Arizona
1997 ACM

049791-897-5/97/l

US.4
l...S3.50

221

tectural elements of workflow systems and the interactions
between those elements. The Process Interchange Format
(PIF) Working Group was formed to explore the potentinl to
provide automatic translations between process representation formalisms[l5]. Finally, Microsoft is pushing their
Messaging API (MAPI) as a defacto standard for implementing workflow systems. Microsoft has recently teamed
with Wang to develop the MAPI-WF specification[l7], an
extension of MAP1 for supporting workflow-specific services.

In order to achieve greater productivity from both workflow
and personal productivity tools, a more integrated view of
organizational and personal processes must be considered.
An integrated view allows individuals the ability to develop
their own productive work practices in support of an organization’s processes, and allows for a more natural handling
of processes spanning multiple organizations and individuals. We are in the beginning stages of our research into the
utility of providing such an integrated view. In this paper we
propose an open architecture for integrating organizational
workflows and personal productivity processes. We motivate the need for an integrated approach, and present a component-based approach to process modeling that provides
the interoperability required to achieve the integration. We
also present a suite of tools being developed at Arizona
State University that realize this architecture.

The WfMC is presently the most significant of the efforts
attempting to standardize workflow systems. The WfMC
Reference Model (Figure 1) identifies the basic architectural
components of a workflow environment. At the center of the
model is a Workflow Enactment Service (WES), comprised
of one or more Workflow Engines. A WES provides services through the WAPIs to workflow-related tools. These
include Process Definition Tools for defining processes,
Workflow Client Applications for handling user requests for
work, Third-party Applications that need to communicate
data and operations to the WES, other WESs for providing
interoperability between enactment services, and Administration and Monitoring Tools for data gathering for process
improvement activities.

The rest of this paper is organized as follows. Section 2.0
discusses relevant issues in current workflow and calendaring technology. Section 3.0 argues for an integrated view of
organizational and personal process spaces, presents a component-based approach to process modeling, and proposes a
general process support architecture. Section 4.0 presents
tool prototypes realizing this architecture that were recently
developed at Arizona State University. We conclude in
Section 5.0 with a summary and discuss future avenues for
our research.

The WfMC Reference Model identifies common workflow
system components and interfaces. The WAPI interface
specifications define a set of low-level protocols for synchronously and asynchronously exchanging workflow data
between the tools and the WES. Our basic problem with this
approach is that these protocols are too low-level: they
imply a restrictive workflow model. Workflow representations that cannot easily convert their process data to conform with this underlying model cannot obtain conformance
with the model. This is one of the issues our research
addresses.

2.0 Background
Approaches to developing workflow systems have both
commercial and academic origins. Commercial systems
have evolved from work on forms-based image processing
systems and groupware[l4]. The line between workflow and
other types of systems is often blurred, with groupware,
scheduling, database, and email tools providing some workflow functionality. In addition, several commercial products
that advertise workllow capabilities fall far short of providing full-fledged support for defining and enacting business
processes. Academic research has focused mainly on process modeling and database transaction issues[9]. Process
modeling research has led to the development of workflow
representations based on a variety of formalisms. Database
transaction research focuses on extending traditional,transaction semantics to support long duration[2][9] and/or
cooperative transaction[5][10] models. The result is aproliferation of approaches and issues relating to workHow.

Recent standardization efforts also address the area of calendaring protocols. One popular calendaring protocol
(adopted by Netscape’s Calendar Server[l8]) is the vCalendar protocol[l2]. In the vcalendar protocol, calendaring and
scheduling entities, called events, are transported between
applications that can understand the protocol. This approach
is similar to the effort of the WfMC protocols in that it
defines a low-level data interchange format that tools must
understand to conform to the protocol. Other, more industry-wide standardization efforts are being sponsored by the
Internet Engineering Task Force (IETF) based in part on the
vcalendar specification. The IETF has recently sponsored
the development of three separate calendaring protocols, the
Calendaring Interoperability Protocol (CIP), the Core
Object Specification (COS), and the Internet Calendar

Current efforts are attempting to get researchers and vendors to converge on a common foundation for workflow,
The Workflow Management Coalition (WfMC) was formed
in August 1993 to promote workflow technology. The
WfMC has proposed a reference mode1[22] and a set of
interfaces,
Called WAPIS’ based on that
mode1[24][25][26][27] as an attempt at standardizing archi-

1. For Workflow API and Interchange

222

3.1 Organizationvs. Personal ProcessSpace
Automated Worktlow is the specification and execution of a
business process of an organization[9]. Workflows are modeled as a collection of process steps, or tasks, assigned to
individuals taking on particular roles. Many modem workflow systems work in this way; the process is considered
from a single organization’s viewpoint. This viewpoint is
illustrated in Figure 2.
Organization A

FIGURE 1. WfMC Reference Model ([23])
Access Protocol (ICAP). These protocols specify interface
and other requirements on calendaring systems exchanging
calendaring data.
The standardization efforts in both workflow and calendaring focus on low-level data interchange and protocols for
exchanging such data in a client-server environment. While
this is a widely accepted standardization approach, we fear
that a stable data format is difficult to obtain due to the
maturing of the underlying models in each domain. This is
especially true in worktlow. In the calendaring domain, a
problematic issue is that calendaring formats and tools support only rudimentary dependencies between tasks. These
issues are compounded when integrating workllow and calendaring systems. Workflow systems can write events to
calendar tools, but are not aware of the personal views of the
process of the participating individuals. Likewise, calendaring systems provide a personalized view of work, but do not
possess sophisticated enough models to negotiate with
workllow systems over the ability to do assigned work.

Organization B
FIGURE 2. Organizational Process Perspective

Figure2 shows the process space of two organizations,
generically labeled A and B. These organizations share two
workflows: Workflow 3 and Workflow 4. Interoperability of
the underlying process models and process support architecture is required to allow these organizations to share these
workflows.
The workllow systems we have experienced or seen in the
literature take this organization-centered approach to automating business processes. For example, Action Workllow
from Action Technologies[l] operates on a cyclical model
where workflow units interoperate to produce customer satisfaction. Different participants are viewed as customers,
performers, or observers at each workflow stage of the
cycle. While Action Workflow provides client-side fimctionality to obtain task lists for individuals, it does not provide a structured way for individuals to define personal
processes and integrate them into the scope of organizational processes. Another example is the application of
groupware-oriented tools such as Lotus Notes to workflow[20]. Notes provides much of the needed infrastructure
for managing data and transactions within a worktlow.
However, again there is no structured way to define personal
processes and integrate them into organizational processes.
Instead, the approach is again organization-centered, where
workflows are defined at the organizational scope, and per-

3.0 Integrated Process Support
We advocate an integrated view of an organization’s process
space and the personal process spaces of its individual
workers, In this view, the organization’s workflows are integrated with individual personal process spaces. Section 3.1
discusses this idea in more detail. To support this integrated
view, we advocate a component-based approach to process
modeling that avoids a reliance on low-level data interchange formats. This approach is called Open Process Components, and is described in Section 3.2. Finally, we propose
a generic architecture in Section 3.3 that derives from our
integrated view of process. In Section 4.0 we present some
Java prototype tools based on our ideas.

223

works for Organization B. Jill participates in Organization
A’s workflow 1 and 3. Bob participates in Organization B’s
workflows 3 and 5. In order to accomplish tasks in workflow
1, Jill employs her Personal Process 1. Likewise, Bob
employs his Personal Process 3 in carrying out tasks relevant to Workflow 5. In addition, Bob employs Personal Process 3 to carry out similar tasks in the shared Workflow 3,
Jill does not have a relevant personal process defined for her
assigned tasks in Workflow 3. Finally, each individual may
have personal processes defined that are outside the scope of
an explicit workllow for either organization. These may be
processes defined solely by the individual’s personal productivity initiative.
OrganizationA’s Space
Jill
t PersonalSpace
/
Y
ersonal
1
Process 1
1 Personal
.,
1%

sonal tasks derived from the workflow model. Still other
workflow platforms, such as InConcert[l6], emphasize collaborative aspects of workllow execution. Collaborative
work is closer in spirit to the idea of integrated process
spaces, but differs in that the emphasis is on mechanisms
supporting shared access to data. Users still act on tasks delegated to them by the organizational workflow model.
To keep pace with industry trends and technology impacts,
this organization-centered viewpoint will have to change in
at least the following ways:
l

Interoperability between worktlows developed across
business functional units and/or organizations must be
supported.

l

The potential for wide-area-distributed participation
must be supported.

l

Individuals must have the ability to define, execute, and
track the personal processes they perform to be productive within the context of &I organization’s business processes and goals.

LJ

@z!g&yI
o&f&v3--------

------

The work of the Workflow Management Coalition as well as
research efforts such as our Open Process Components
Framework (see Section 3.2) address the first two issues
directly. However, there has not been a lot of consideration
for the last issue. At best, current workflow systems notify
individuals of new work items through email or custom client applications. Some even have the ability to write to personal calendaring software through interfaces such as
Microsoft and Wang’s MAPI-WF[l7]. But the viewpoint
still originates with the organizational process. An agentcentered viewpoint, showing the distribution of workflows
an individual participates in, and the set of personal prdcesses an individual employs, is not considered.

I

,% ’P
Worktlow 5

Personal
Process 4

Personal

OrganizationB’s Space

Bob

PersonalSpace

FIGURE 3. Personal Process Perspective
There are several reasons for arguing for an integrated view
of organizational and personal prodesses. Figure 3 shows
the overlap of the personal and organizational process
space. Defining and executing business processes is motivated in part by the need to ensure business goals are
achieved. Workilows are largely assumed to be static, repetitive processes that involve rote decision-making in support
of well-defined business goals[9]‘. To expand the scope of
processes automated workflow systems can support, more
dynamic workflows that include personal processes should
be considered. Another motivating reason comes from the
diverse set of relationships in which both organizations and
individuals participate. Individual workers, particularly at
highly skilled levels, perform in a wide variety of diverse
business functions. Downsizing and decentralization of

The need for supporting the personal process view is just
beginning to be recognized in more dynamic process areas
such as Software Engineering[ll]. In the software process
domain, the work of the software developer is considered
dynamic in the sense that the developer must be creative in
seeking the solutions to design, implementation, and maintenance dilemmas[4]. As workfloi extends to more complex and skilled tasks, automated workflow systems will be
required to encompass more than just the straightforward
document-routing capabilities of image processing systems.
Future demands will include the ability to support more of
the skilled, or knowledge work, that people perform in the
organization. In order to do this, workflow systems must
relax the prescriptive constraints it places on performers of
the worktlow, and allow these workers to perform their own
./
personal processes to carry out the work
1..
Figure 3 shows an agent-centered viewpoint of the process
space. Jill is an agent working for Organization A, Bob

1. We refer to Georgakopoulos, Homick, and Sheth’s[9]
trade press characterization of administrative andproduction workflows. Our research is closer to ad hoc
workflows, though our point is they can be better understood through an integrated view of the process space.

224

organizations coupled with increasing outsourcing of work
makes it unrealistic to take the single organization approach.
The business processes of multiple organizations must be
integrated with the personal processes of the participants.
In order to accomplish this integration, we propose a corn- ’
ponent-based approach to process modeling and an open
architecture for supporting personal and organizational process spaces.

meaning management must decide how to map organizational roles to process-specific roles. This mapping is the
relationship between Roles and Agents. The me&model
described briefly here is adopted from the PCIS LCPS metamodel[7]. However, the concepts are similar in a variety of
general descriptions’ of process in the literature[5][9][15][22]. In the OPC Framework, this set of process entities and relationships form the basis for meaningful
component interactions.
The second important aspect of the OPC Framework is a
state-based encapsulation of execution interfaces. By this
we mean each component in a process model possesses a
process state, and this state is manipulable by a set of interfaces to the component that are available during various
stages of executing the process model. Example interfaces
include start, suspend, resume, abort, completeWithFailure
and completewithsuccess. Each component maintains an
explicit, independent state during execution of the process
model, and the state of process execution at any point in
time is the combination of states of the components
involved in the process.

3.2 Component-based Process Modeling
Organizations developing standards in workflow and calendaring focus on low-level data interchange protocols to be
applied in a client-server environment. The development of
such protocols, particularly the protocols related to workflow definition interchange’, are too restrictive to ensure
widespread adoption. Instead, we propose an object-oriented component-based approach to process modeling and
execution. In our research we are developing a componentbased framework for process modeling called the Open Process Components (OPC) Framework. It is not the focus of
this paper to delve into the details of the OPC Framework,
but we do provide a brief discussion relevant to the process
support architecture presented in Section 3.3. Further details
may be found in [8].

The final salient feature of the OPC Framework is a threetiered object-oriented class hierarchy for defining components. An object-oriented methodology provides several
advantages: encapsulation of heterogeneous process representations, an economy of representation through inheritance, and the ability to specialize component definitions
through subclassing. From a process modeling perspective,
one major advantage of the hierarchy is its ability to be
extended. New component definitions and abstractions can
be added within the framework without modifying preexisting definitions. A second important advantage is that
specialized component definitions allow heterogeneous process modeling formalisms to interoperate with one another.
For example, a Petri-net based process model fragment can ‘
interoperate with a process model fragment developed in a
scripting language by encapsulating each as a component
under the framework. This is especially beneficial in the
organizational/personal context of processes we consider in
this paper since it should not be assumed that homogeneous
process models are generated across these contexts.

There is a need for a unifying framework for representing
and manipulating worktlow abstractions. We take an objectoriented approach we call Open Process Components. Entities of the workflow domain are represented as objects, with
manipulations of those objects defined as object behaviors.
The approach is component-based, from the perspective that
interfaces are well-defined so that components interact in
meaningful ways. The OPC Framework provides a foundation for constructing component-based process models in an
extendable fashion.
~
There are three important aspects to the OPC Framework
that allow it to support component-based process modeling.
The first is a me&model that identifies basic process entities and relationships between entities. Basic process entities include Process, Activity, Product, Role, and Agent. A
Process is a decomposable entity into subprocesses and subactivities. This allows development of process models in a
top-down fashion. An Activity is an executable fragment of
a process model; it represents a refinement of a portion of a
process model down to an executable state. A Prodcrct is a
work artifact that is either consumed as input by an Activity
or produced as output. A Role is a process-specific definition of the skill set required to perform an Activity. A Role
is process-specific as opposed to organization-specific,

As a brief example, consider the ad hoc workilow depicted
in Figure4, taken from [9]. This workflow represents a
paper review process. In a component-based process model,
each task in the workilow is represented as an activity component. Interactions between the components is governed
by the set of interfaces each component supports. The benefit is that the implementation of each component is separated from these interfaces. Different process modeling and
enactment services can be used to define and execute the
details of each task. This differs from existing systems
where homogeneous models and supporting services are
employed.

1. More specifically, the Workflow Process Definition Language proposed in WAPI 1[24].

225

port for the personal process space. Figure 6 shows the
proposed general architecture.

The workflow in Figure 4 is a relevant example of the utility
of integrated organizational and personal process spaces,
Consider for example the “Review” tasks in the workflow.
These are assigned to separate persons fulfilling the role of
Reviewer. However, there is not sufficient detail in this definition to automate the support of review activities for each
reviewer. Furthermore, it is not appropriate to believe that
this organizational workflow should provide such detail.
Instead, it is more natural that each reviewer perform a personalized review process that meets the requirements of the
organizational workflow. Therefore, if Jill and Bob were
Reviewers in this workflow, each would carry outthe review
according to her/his own personal process for reviewing
papers, employing familiar tools and methods for producing
the needed results.

FIGURE6. ProcessSupportArchitecture
The architecture in Figure 6 integrates organizations’ workflow servers and personal process servers with calendaring
technology to produce a time-oriented view of work for the
end-user. Arcs indicate the bidirectional flow of components
over the architecture. This architecture extends traditional
workflow architectures, such as the Workflow Management
Coalition’s Reference Model[22], to include the end user’s
personal process space. The components of this architecture
are:
Process Definition Tools

FIGURE 4. Exam le Workfiow

Process Definition Tools are used to create componentbased process models. These tools may query Personal
Process and Workflow servers in order to reuse existing
process component definitions.

(taken from ::9)

Component-based process modeling is at the heart of our
research and relevant to the topics discussed in the rest of
this paper. However, the elements of organizational versus
personal process spaces and process architecture we discuss
do not necessarily rely on a component-based approach.
One can readily envision modifications to existing tools
such as Action Workflow or Lotus 3Jotes discussed earlier
that would address process space integration. We encourage
the reader to consider process modeling approaches and
process space integration issues as independently as possible.

WorkflowServers

One or more servers create the organizational process
space(s). These servers manage component-based workflow models created for organizational units by Process
Definition Tools.
Personal Process Servers

Similar to a Workflow Server, a Personal Process Server
manages process definitions for individuals, created
from components by Process Definition Tools.

3.3 A Process Support Architecture

Calendar Manager

The Calendar Manager is the organizer of an individual’s
process space. The Calendar Manager manages
instances of process models from the individual’s perspective.

To support the integration of the organizational and personal
process spaces, we propose an architecture that extends traditional worktlow client-server architectures to include sup-

226

l

process models. Component-based process modeling allows
for easier integration of organizational and personal process
spaces in the Process Definition Tools and Calendar Managers. Without components, there would be a push on each
tool to support low-level protocols allowing for heterogeneous process models to be integrated. This is just the type
of interoperability that is deficient in cm-rent worktlow systems, and a major motivating force behind the componentbased approach to process modeling described in
Section 3.2.

WorklistHandler/Calendar Tool

This is a client-side tool that presents the individual with
her/his work to do. This may be in the form of a task list,
or may be a time-oriented view depending on process
constraints and personal scheduling preferences.
This general architecture clearly shows the separation and
integration of organizational and personal process spaces.
The distinct servers manage personal and organization processes, This distinction is a logical one; in practice a single
implemented server may include the functionality to manage both process spaces. Integration of the spaces comes
from the Process Definition Tools and the Calendar Manager. A Process Definition Tool creates component-based
process models. By accessing the process definitions on
both servers, the tool is able to create and reuse organizational process that utilizes process specifications of relevant
individuals. The Calendar Manager integrates instances of
organizational and personal processes from the individual’s
perspective. The Calendar Manager has the ability to accept
or decline work requests from process servers, or manage
changes to the individual’s process space when forced to do
so. This tool is the focal point of the individual’s process
space, Finally, the Worklist Handler/Calendar Tool is a
combination of a workflow client and a personal calendaring tool. This client-side tool has the ability to host process
components and support the enactment of such components
in order to carry out the actual work.

We have developed a set of Java tools realizing the proposed
architecture. In the next section we present our progress
with this project.

4.0 The Current Prototype
The YFPPG Research Group at Arizona State University
has sponsored a series of Master’s projects during the
Spring 1997 semester for developing a toolset in Java for
component-based process modeling and enactment. This
toolset conforms closely to the general architecture presented in Section 3.3. The specific architecture is shown in
Figure 7.

Process

Component
Repository

The architecture we propose is an integration of current
worktlow architectures such as the WfMC’s Reference
Model[22] and calendaring environments such as
Netscape’s Calendar Server[l8]. However, current architectures do not take such an integrated view. We know of no
tool that allows for process models to be created that integrate a workflow model and a personal process model. The
proposed process definition tool allows for this integration.
We know of no environment that provides a componentized
personal view of process like the proposed Calendar Manager. One can envision workflow servers writing to an individual’s calendar through an interface such as Microsoft and
Wang’s MAPI-WF interface[l7]. However, this requires
that the workflow server have explicit knowledge and access
rights to individuals’ calendars, The proposed Calendar
Manager explicitly manages an individual’s workspace,
negotiating between servers and individual preferences to
present the personal process space to the end user. The
existence of such a tool enables a component-based architecture that does not require Personal Process and Worktlow
Servers to communicate directly to negotiate over rights to
assign work to an individual.

FIGURE 7. OPC Support Architecture
The components of this architecture are:
l

Process Component Repository

This is implemented as a Java RMI[22] server that uses
Java Serialization facilities to distribute process objects
to client tools. The repository stores component-based
process definitions and distributed components for
enactment. Multiple named repositories, each storing
multiple process models, can be managed by a single
server.

The proposed architecture is process model independent. It
does not favor any particular representation of process.
However, we again advocate the use of component-based

l

227

Calendar Manager Server

Java RMI and CORBA’ versions of this server exist.
This server stores time-oriented appointments as well as
task lists for individuals.
j/Connected to plglet

Repository Browser

~

The Repository Browser is a process administration and
management tool that allows users to browse through the
current objects in a repository. This is implemented as a
Java RMI client.

32 Submlt Request Enactable

Components Editor

rcl

The Components Editor is another Java RMI client. It
allows users to graphically create component-based process models through component creation and reuse.
Figure 8 shows the Components Editor GUI with our
example process definition from Figure 4.

Working Items:

,I

FIGURE 9. Worklist Handler
At this point in the development of our toolset we have yet
to implement the full envisioned functionality of the Calendar Manager Server. The overlap of the organizational and
personal process space occurs in the Calendar Client, which
is responsible for providing the integrated view of the two
spaces. The next step is to implement the full negotiation
between the two servers, as we discuss in the next section,

WorklistHandler

This client-side tool obtains work items for a user from a
repository. The work items are actually Java objects that
are serialized and obtained through Java RMI calls.
Once the Worklist Handler obtains these objects, it can
execute them, changing the state of the process model
and invoking tools on work products. Figure 9 shows a
Worklist Handler for Bob.

We have already learned several lessons during the development and use of this toolset. On the plus side, these tools
successfully demonstrate the integration of organizational
and personal process spaces. These tools are also demonstrations of forward-looking component distribution technologies such as Java RMI[22] and CORBA[19]. Finally,
these tools demonstrate the ntility of component-based process modeling. There have been some hiccups however.
Managing migrating components in a distributed environment is a difficult configuration management problem. It
has proven troublesome to track distributed process components’ states and synchronize updates to process models
stored in the repository. Despite these problems, we are
excited by the possibilities of distributed, component-based
process modeling, and are initiating a new set of projects to
update the current environment. Readers interested in
obtaining the prototypes or tracking progress of this project
may visit the .YPPPG website at http:I&vww.eas.asu.edt~

Calendar Client

The Calendar Client obtains the appointments and task
lists for an’individual from a Calendar Manager Server.
In addition, the Calendar Client can bring up a Worklist
Handler to access the Process Component Repository.
Java RMI and CORBA versions of this tool exist.

stome!Ser\rlce Request

-Y&P&

5.0 Summary and Future Work
In this paper we have advocated an integrated view of organizational workflows and personal process spaces. In this
view, both the perspective of the organization and the perspective of the individual are considered when integrating
process spaces. This view allows organizational goals to be
pursued while allowing individual workers the flexibility to
define how to sccomplish such goals. Such flexibility will
be required in the not-too-distant future due to the increasing demands on current workflow systems and the current
pace of technology.

FIGURE 8. Components @itor

1. Iona Technologies’ OrbiiWeb[l3] was used to implement the CORBA-enabled calendar server and client.

228

Athens, GA, May, 1996.

In this paper we proposed a generic architecture for process
support that logically integrates functionality needed for
both perspectives. We suggest a component-based process
modeling approach to further reduce the dependencies
between workflow and calendaring systems by avoiding the
need for low-level, brittle data interchange protocols.
Finally, we described a set of prototype tools based on component-based process modeling that realizes the generic
architecture. Despite the success or failure of our efforts, we
hope that the argument for integrated organizational and
personal process spaces will have an effect on future considerations in the converging areas of workflow and groupware
research.

P.1 Conradi, R., Liu, C., and Hagaseth, M. Planning Support for Cooperating Transactions in EPOS. Information Sciences, vol. 20, no. 4, pp. 317’336. 1995.

F-1 Curtis, B., Keller, M., and Over, J. Process Model-

ing. Communications of the ACM, vol. 35, no. 9, pp.
75-90, September, 1992.

l7.1

Demiame, J.C. Life Cycle Process Support in PCIS.
Proceedings of the PCTE ‘94 Conference. 1994.

P-1 Gary, K., Lindquist, T., and Koehnemann, H. Compo-

nent-based Process Modeling. Technical Report TR97-022, Computer Science Department, Arizona
State University. May, 1997.

Given the relatively early stage of this research, there are
several avenues we intend to pursue in this area. First, further research is needed to fully understand the nature of the
negotiation between organizational and personal process
spaces that takes place in the Calendar Manager. We are
pursuing research in this area under the topic Process Component Brokering, where such negotiation is carried out by
having the Calendar Manager provide a brokering service
that identifies personal process components that meet organizational process requirements. Second, we are looking at
ways to integrate automated planning and scheduling techniques for workflow and personal processes. The result will
be enhanced Calendar Managers that negotiate with organizations Workflow Servers to optimize the overlap between
organizational and personal process execution. Finally, we
plan to validate the proposed architecture by employing our
tools in real workflow settings, and extending our work into
more dynamic process areas. Specifically, we are looking at
ways to support Personal Software Processes and Distributed Learning processes between mentors and students.

Georgakopoulos, D., Hornick, M., and Sheth, A. An
Overview of Workflow Management: From Process
Modeling to Workflow Automation Infrastructure.
Distributed and Parallel Databases, vol. 3, pp. 119153.1995.
Godart, C., Canals, G., Charoy, F., and Molli, P. An
Introduction to Cooperative Software Development
in COO. International Conference on System Integration, 1994.
Humphrey, W. The Personal Process in Software Engineering. Proceedings of. the Third International
Conference on the Software Process (ICSP-3). IEEE
Press. October, 1994.
12.1 Internet Mail Consortium. vcalendar V1.0 Specification. Available at http://ivww.imc.org/pdi/pdiproddev.html

13.1 Iona Technologies. OrbixWeb 2.0 Programming
Guide. November 1995.
[14.] Khoshafian, S., and Buckiewicz, M. Introduction to
Groupware, Workflow, and Workgroup Computing.

6.0 References
[I.]

Action Technologies, Inc. Coordination Software:
Enabling the Horizontal Corporation. Action Technologies, Inc. White Paper. July, 1994.

[2,]

Alonso, G. and Schek, H. Research Issues in Large
Workflow Management Systems. Proceedings of the
NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996.

[3.]

[4.]

J. Wiley and Sons, New York. 1995.
[ 15.1 . Lee, J. Gnmiger, M., Jin, Y., Malone, T., and Yost, G.
The PlF Process Interchange Format and Framework.
Available at http:/3vwwaiai.edac.u~pi@uZex.html.
May 24,1996.
[16.] McCarthy, D. and Sarin, S. Workflow and Transactions in InConcert. IEEE Bulletin of the Technical
y;pee
on Data Engineering, vol. 16 no. 2. June

Armitage, J. and Kellner, M. A Conceptual Schema
for Process Definitions and Models. Proceedings of
the Third International Conference on the Software
Process (ICSP3), pp. 153-165, Reston, VA. October,
1994.

[17.] Microsoft Corporation and Wang Laboratories, Inc.
Microsoft MAP1 Workflow Framework Concepts and
Facilities . (White Paper). Available at http://
www.wang.com/sb&vP602210.htmn. February 21,
1996.

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study
into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow
and Process Automation in Information Systems,

[18.] Netscape Communications Corporation. Netscape
Calendar Server 1.0 and 2.0. Available at http://
home.netscape.com/comprooYserver-central/prod-

229

uct/calendar/calenakr2_datahtml.

[lg.]

January 1995.
[24.] The Workflow Management Coalition. Interface 1:
Process Definition Interchange. WfMC Document
Number TC-1016, Version 1.0 Beta. May 29,1996.
L
[25.] The Workflow Management Coalition. Interface 2
Specification. WfMC Document Number TC-1009,
Version 1.0. November 20, 1995.

Object Management Group. Corba 2.0 Specification.
~;g~lable at http:/..www,omg.org/corbaskhtm. July
.

l

[20.] Reinwald, B and Mohan, C. Structured Workflow
Management with Lotus Notes release 4. Proceedings
of the 41st IEEE CompCon digest of papers, pp.451457, Santa Clara, CA. February, 1996.

[26.] The Workflow Management Coalition. Interoperability Abstract Specification. WfMC Document Number
TC-1012, Version l.O..October 20,1996.

[21.] Riddle, W. E. Fundamental Process Modeling Concepts. Proceedings of the NSF Workshop on Workflow and Process Automation in Information
Systems. May, 1996.

[27.] The Workflow Management Coalition. Draft Audit
Specification. WfMC Document Number TC-1015.
August 14,1996.

[22.] Sun Microsystems, Inc. Remote Method Invocation
Specification.
Available
at
http://www.javasof.com:80/product~jdWI.l/docs/guidekmiLspec/
rmiTOC.doc.html

[23.] The Workflow Management Coalition. The Reference Model. WfMC Document Number TCOO-1003,

230

Contextual Requirements Experiences within the Software Enterprise
Kevin A. Gary
Department of Engineering
Arizona State University at the Polytechnic Campus
Mesa, AZ 85212
kgary@asu.edu
Abstract
This paper describes a pedagogical process
applied to requirements engineering education in the
Software Enterprise, a multi-year software
engineering project-based course sequence at
Arizona State University. The Software Enterprise
emphasizes contextual and accelerated learning
through a modular, iterative pedagogical pattern.
One course within the Enterprise is specifically
dedicated to requirements engineering, with an
emphasis on elicitation and quality. This paper
presents the Enterprise pedagogical model, describes
its application in the requirements engineering
course, and discusses how these concepts differ from
the other courses within the Enterprise. An example
of Enterprise pedagogy in a requirements
engineering module is given. Finally, conclusions are
drawn regarding the suitability of this approach
versus
more
classically-oriented
software
engineering courses.

1. Introduction
Requirements engineering (RE) is constantly
evolving and maturing. There is no shortage of
techniques, tools, and methods introduced to
researchers and practitioners alike as the community
strives to better capture, analyze, and manage
stakeholder expectations. Innovations include better
knowledge capture to communication-oriented
techniques, quickly becoming popular in an agile
world. But despite the evolution of the field,
requirements engineering pedagogy in higher
education is not evolving rapidly; for example
software engineering texts continue to emphasize
modeling instead of elicitation and quality concepts.
The Software Enterprise at Arizona State
University is a multiple course sequence emphasizing
contextual teaching and learning of software
engineering through a modular curriculum combined
with problem-based learning and industry projects. A
requirements course is taught to seniors and first-year
graduate students that emphasizes elicitation and
quality aspects of the RE process. This course applies

the Enterprise’s modular pedagogy together with
student engagement of industry sponsors for capstone
projects. The result is an iterative exposure to RE
tools and techniques, and an immediate context in
which to apply these techniques and evaluate their
efficacy in different situations.
This paper presents the Enterprise model and its
instantiation in the RE course. It presents example
modules, and describes the role of the capstone
project. The paper concludes with a discussion of RE
course differentiators and some preliminary
conclusions by the principal investigator.

2. The Software Enterprise
Space limitations preclude a complete
presentation of the Software Enterprise pedagogy,
interested readers are referred to [1] and [2]. This
section gives a brief summary.

2.1 Enterprise Pedagogy
The Software Enterprise defines a new
pedagogical approach to software engineering
education focused on preparing students for industry
upon graduation. The pedagogy iterates over
preparation, dissemination, practice, project, and
reflection activities as shown in Figure 1.

Figure
1.
The
Software
Enterprise
pedagogical model. For each module,
students prepare by reading before a lecture
session; then practice on lab exercises,
apply concepts in a project setting, and
reflect on the experience.

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

Software engineering concepts are broken into
discrete modules and sequenced over the semester to
synch up with project activities. In this just-in-time
approach, students are exposed to a concept, practice
it, apply it on a scalable project, and then evaluate the
applied technique all within a three-week project
iteration. The goal is to avoid having students
understand a solution in the small, and instead
understand problems and solutions in context.
This modularized iterative pedagogy is applied
throughout a four-course sequence, beginning in the
fall semester of students’ junior year. The first course
focuses on tools and process concepts. The second
(spring semester, junior year), focuses on
implementation and deployment concepts. The third
is the RE course, while the final course (spring
semester, senior year) is on project management.
Contact time is one lecture, one lab per week, and
students spend nine hours per week outside class on
the project.

2.2 Requirements Engineering Course
Software Enterprise III: Inception and
Elaboration is the third course in the Enterprise
sequence, offered each fall to seniors and first-year
graduates. The course is named for the Rational
Unified Process (RUP) [3] phases, though while RUP
is used as the meta-model, the specific requirements
engineering processes are not limited to RUP-only
workflows and tools. Graduate students are allowed
to register for the RE course, though enrollment of
graduate students is less than the number of seniors.
The structure of the RE course organizes
modules around four concept categories: elicitation,
analysis, quality, and management. Elicitation
modules include interviewing, group techniques,
surveys, and observation. Analysis modules include
structuring and interpreting language, prototyping,
and various modeling formalisms (mainly UMLbased models). Quality is driven by the IEEE8301998 specification [4], though additional concepts
such as usability are also covered. Requirements
management covers metadata attributes describing
requirements’ state, traceability, and change
management analysis.
While all of these categories are covered, there is
a particular emphasis on elicitation and quality
activities. These areas are undernourished in most
software engineering texts and courses. While
analysis and management areas might be easier to
teach in the sense that there are concrete methods,
tools and formalisms that can be covered, one can
argue that the ability to understand stakeholder
problems and priorities is most important.

The textbook for the course is Leffingwell and
Widrig [5], which in fact is not really a textbook at
all. This book has excellent coverage of elicitation
and scoping activities in particular compared to
traditional software engineering texts. Other content
is drawn from various sources, including the IBM
University Initiative, swenet.org, and the instructor’s
professional contacts. It is not the content that makes
the course unique, but the emphasis on elicitation and
quality combined with the Enterprise pedagogical
approach.
The RE course is unique compared to the other
courses in the Enterprise. The emphasis on soft-skills
requires different teaching and assessment methods.
It is also the case that students universally have no
prior exposure to these concepts, nor does the course
subject matter represent an incremental intellectual
leap such as is the case say, with a programming
course. One interesting result is that some students
that have gone through three years of classes within a
cohort, intimidated by fellow students’ coding
prowess, find that they are in fact more talented than
such pure techies at these soft-skill concepts.

2.3 Software Enterprise Projects
Enterprise projects are executed by student
teams in conjunction with an industry sponsor.
Sponsors are identified through industry advisory
boards and individual faculty contacts. Sponsors are
almost always industry sponsors, and preferably not
developers but business stakeholders, so that they do
not confuse expectations as stakeholders with a
desire to mentor teams on complex technologies.
Candidate sponsors submit project ideas as a single
paragraph to the instructor about two weeks before
the start of the fall semester. The instructor typically
engages in either a phone conversation or email
exchange to vet the sponsor and the project idea. This
is important, as the objective is to have sponsors with
a solid business domain understanding but only a
vaguely conceived idea. It is the purpose of the teams
to elicit, capture and formulate the idea into a
specification by the end of the semester. It is also
helpful to have industry sponsors with multiple
stakeholders and end user representatives. Sponsors
that appear to have too canned a project or in some
way view the student team as a mini “body shop” are
politely declined.
In the fall, teams are created from juniors in the
first course (“Tools and Process”), and the
seniors/graduates in Software Enterprise III. In this
way juniors get some exposure to customer
interaction and requirements engineering, though
their project tasks focus mainly on helping to develop
a prototype and process infrastructure, such as

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

creating the configuration management plan. Seniors
and graduate students “own” the projects, and are
responsible for managing customer expectations and
overall project delivery. At the end of the fall
semester, each project team is expected to complete
four customer deliverables:
1. Vision document – captures business drivers,
features, and the relationship of this project to a
product roadmap.
2. Software Requirements Specification (SRS) –
classic SRS that follows either a RUP use case
template (such as the one provided by [5]) or one
from the appendices in [4], or a custom format if
previously approved.
3. Prototype – Teams must deliver a prototype
capturing the user-facing interactions with the
system. Most projects are web applications, as such
projects use an iRise instance donated by the
vendor (http://www.irise.com).
4. Software Development Plan – The SDP asks
student teams to map out an iteration plan for the
spring semester, indicating what requirements are
slated for which iteration. Teams justify their plans
based on requirement priority, complexity, and
other quality attribute values.
As three of the four of these main deliverables (and
several secondary deliverables) require significant
writing skills, the course is approved for fulfilling
students’ literacy requirement on their program of
study.
Sponsors are asked to make themselves available
to the team for a minimum of 10 hours over the
course of the semester; however, most sponsors
spend significantly more time (by choice) with their
teams.

3. RE Module Example
In order to explain Enterprise pedagogy as
applied within this RE course, this section presents
an example module in the area of Requirements
Elicitation, specifically group techniques for eliciting
requirements. Materials associated with this module
are submitted with this paper.
Group elicitation techniques are well suited for
the Enterprise pedagogical approach. There is a
wealth of academic and industry reading on grouporiented topics, and it is straightforward to compose a
slide deck summarizing various approaches. But to
elevate understanding, one must practice techniques
in the small, and apply them in a real setting.
Students must come to realize no single technique is
a silver bullet, some techniques work better than
others in certain situations, and group techniques may

be combined with others to get a complete picture of
a problem.
The Enterprise pedagogy asks students to do all
of these things. Students read about group techniques
and benefits before class. Then a 1.5 hour lecture
summarizes the approaches, their benefits, and their
drawbacks. Then the lab exercise asks them to put the
technique into practice. Students then choose which,
when, and where to incorporate one or more
techniques, and then reflect on its utility in the realworld setting. It is not important that they think the
techniques are good or bad, but that they better
understand the nuances of applying the techniques in
particular settings.

4. Assessment
Assessment of Enterprise pedagogy as it relates
to RE is incomplete. The principal evidence is an
anonymous survey of student perceptions of the
importance of Software Enterprise concepts.
Specifically, at the conclusion of the academic year,
seniors are asked a small number of questions on an
anonymous survey, including “How much will each
topic benefit you professionally?” which they answer
on a seven-point Likert scale for each of the twentytwo topics taught in the senior year of the Enterprise
sequence. Four of the top seven topics are
“Requirements
Engineering”,
“Requirements
Analysis Process”, “Requirements Quality”, and
“Requirements Elicitation”. While these survey
results are not statistically significant and only
measure student perception of importance, it is still
compelling to find that students appreciate RE
concepts.
Evidence of student success comes from industry
sponsor evaluation of team and project performance.
Sponsors provide qualitative feedback on their teams
twice a semester, and the instructor often engages
with sponsors conversationally outside the university
environment. Anecdotally, the instructor finds that
sponsor feedback is often exceedingly positive, to the
point of surprise, at how useful the RE deliverables
listed above are to the sponsor.

5. Discussion
Work is underway to provide rigorous
assessment of the validity of Enterprise pedagogy as
it applies to all the software engineering concepts
covered in the four courses. In this paper, the concern
is with respect to RE. This section makes some
observations based on the evidence available.
One observation is that the nearness in time from
concepts to lab to project activity to reflection is

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

necessary for accelerating applied learning. Classic
approaches teach students RE within a specific RE
course, and then later ask the students to apply the
techniques in a capstone project.
As a corollary, often the RE component of the
capstone project is considered insignificant compared
to design and implementation activities. Yet
understanding the true customer need remains the
most significant obstacle to software project success
today. Software Enterprise projects disproportionally
emphasize RE elicitation and quality activities in
comparison with classic work distribution models in
the lifecycle process in order to stress the importance
of these activities to students.
The project component is critical, as is the
reflection component. Methods, tools, and techniques
evolve, and it is unlikely that university students will
engage in customer engagement processes
immediately out of school. In other words, they
won’t immediately use many of the tools and
methods taught in an RE course (except perhaps as a
low-level participant), and when they do, they will
probably need training in a specific methodology
employed by their organization. The real value of the
project is in providing a context in which to apply the
RE methods and tools, and understand why some
work in one situation yet do not work in another. To
state it in a simpler way, whether a student learns use
cases, user stories, or P-specs is not as important as
learning why communication breaks down, how
change impacts a project, and how to interact with
stakeholders to meet objectives. These learning
outcomes are best achieved by employing techniques
in a real project context, and then discussing what
worked, what didn’t, and why.
Despite this paper’s emphasis on elicitation and
quality, analysis and management are important RE
concepts as well, and they are difficult to teach to
college students. This RE course once included
analysis , but it was difficult to get through analysis
methods (RUP and SAM were covered) in sufficient
depth to have teams make rationale choices. Further,
customers did not find the analysis deliverables
(document and architectural prototype) useful,
echoing an industry trend de-emphasizing analysis in
the SDLC. Requirements management is a an
unwieldy topic, and the Enterprise approach does not
help. Management requires continuity and a strong
toolset. The Enterprise attempts to do management in
an ad hoc manner through Wikis and spreadsheets,
but the results (like many organizations) are
unsatisfactory. We are moving to the IBM Jazz
platform and hoping its repository will give us a de
facto management environment, but this will take
time to mature.

6. References
1. K. Gary, “The Software Enterprise: Preparing Industryready Software Engineers”, in Software Engineering:
Effective Teaching and Learning Approaches and
Practices, Ellis, H. (ed.). IGI Global Press 2008.
2. K. Gary, “The Software Enterprise: Practicing Best
Practices in Software Engineering Education”, The
International Journal of Engineering Education Special
Issue on Trends in Software Engineering Education, July
2008.
3. IEEE-CS, IEEE Recommended Practice for Software
Requirements Specifications. (IEEE standard 830-1998).
New York, NY, 1998.
4. Leffingwell, D. & Widrig, D.. Managing Software
Requirements: A Use Case Approach (2nd edition)
Addison-Wesley, Boston, 2003.
5. P. Kruchten, P. The Rational Unified Process – An
Introduction. (2nd edition), Addison-Wesley, Boston,
2000.

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

7. Appendix: RE Module
The RE module artifacts presented in this appendix
are 1) a group elicitation (idea generation/reduction)
lab, 2) student group responses to the lab, 3) student
group discussions on the lab, 4) Project team excerpts
on the utility of this technique in the context of their
projects, and 5) an exam question on elicitation.
Student and sponsor identities have been removed
from the artifacts presented here.

7.1 Lab Handout
Software Enterprise 3: Inception & Elaboration
Lab: Requirements Elicitation using Group
Techniques
In this exercise you will conduct idea generation and
idea reduction activities using group-oriented
techniques. 2 related techniques will be used,
“Structured Brainstorming” and “Affinity Process”.
Activity 1: Structured Brainstorming
Consider an application that suggests free elective
courses to students.
Follow this process:
1. Select a group Facilitator and a group Recorder

idea, it is removed. If anyone does not agree, the
idea stays.
- Look for ways to merge related ideas into one,
but do not force it.
- If 2 or more ideas are merged, remove the
original ideas and replace with the new one.
Mark the merged sticky with a small “M” in the
upper-right corner.
- Note that there is not time for long debate or
discussion. If the decision to remove or merge is
not clearly unanimous, leave it alone and move
on.
6. After idea reduction is prioritization. This task
will take 10 minutes.
- For each idea, have the facilitator read the idea
aloud in turn (perhaps with a short reminder
from the team as to what the idea is). The
facilitator should ask for a show of hands for 2
questions: 1) How desirable is this feature? A)
Must have b) Nice to have; and 2) How hard
would this feature be to implement? A) Easy B)
Hard.
- The show of hands phase is not a time for
discussion, nor should there be any
positive/negative comments made.
- Log the tally of hands in a table as shown below:

2. Make sure each person has at least a dozen large
sticky notes
3. Gather around a common wallspace away from
other teams
4. For 8 minutes, team members should volunteer
features for the application.
- Each time an idea is generated, that team
member should say it out loud, the Recorder
write it down on a large sticky note and hand it
to the Facilitator. The Facilitator is responsible
for ensuring there is not a shouting match.
- The Facilitator should post it on open wallspace.
Do not try to organize ideas yet.
- No ideas are off-limits, and no ideas are bad.
No negative comments are allowed (positive
ones are). Every suggestion is included; none are
thrown out at this point.
- You should go for the full 8 minutes, even if
there are lulls in the generation.

FEATUREs

Feature 1
Feature 2
Feature 3

Desirability
Must
Nice
have
to
have
XXXX
XX
X
XXX
XXX
XX

Implementation
Easy
Hard

XXXX
XX
XXXXX

XX
XXXX
X

This table represents a 6-person team. Note that for
desirability, not everyone even voted that Features 2
or 3 are valuable at all!
In this table, Feature 1 has more value and is
considered easier to implement than Feature 2, so it
would be implemented first. But what about Feature
3? Would you put it before/after Feature 1?
Before/after Feature 2?

5. At the conclusion of idea generation, idea
reduction begins. This task should last 12
minutes.

Discussion Questions: For your tables, identify the
features that are most desirable and the ones that are
easiest to implement. Prioritize the order in which
you would go about addressing these features and
provide a justification.

- The facilitator visits each sticky note in turn. If
the group unanimously agrees to throw out an

Activity 2: Affinity Process

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

Consider an application that tells you the best gas
station at which to do your next fill up.
Follow this process:
1. Each team member should consider the question:
“What features would I add to this application?”
2. Each team member should write down, on small
stickies, as many feature ideas as s/he can think
of. This is done silently, no discussion with
other team members. Descriptions should be no
more than a short phrase.
3. After 8 minutes, everyone places all stickies on a
common wall area. Again, no talking!!!
4. The entire team should gather around the wall
and organize the stickies into naturally-forming
groups. This is done by physically arranging the
stickies; again there is still no talking! These
clusters represent related features.
5. After the clusters of stickies are defined on the
wall, the group takes a new sticky and comes up
with a single-word name for each cluster. You
may now discuss with each other, however, there
is no moving a sticky to another group at this
point.
6. Voting. Each team member gets 2 sets of 10
votes to distribute across all groups (you might
want blue/black pens).
- Visit each cluster in turn. Examine the features
listed in the cluster. Assign 0 to 10 votes for the
cluster based on perceived value of the feature
set to the end user, with the sum of all of your
votes across all clusters being 10.
- Repeat the previous step, visiting each cluster in
turn. This time assign 0 to 10 votes for the
cluster based on perceived complexity/risk in
implementing the feature set, with the sum of all
of your votes across all clusters being 10.
7. Enter your clusters (feature sets) into a table with
their votes. Calculate the sum of the votes and
rank the features.

Note how the sum of the votes of Clusters 1, 2, and 3
equals 60 for both the business value and complexity
columns. This is assuming 6 people per group, 10
votes per cluster X {value, complexity}. The 3rd and
5th columns are the percent of the total vote for that
cluster. For example Feature Set 1 gets 50% of the
Business Value vote because 30 votes is half of the
total votes for all clusters.
According to this table, Feature Set 1 is most
desirable, Feature Set 2 is next, followed by Feature
Set 3. But, Feature Set 2 is easier to implement while
Feature Set 1 is next, and Feature Set 3 is hardest.
One would have to wonder if Feature Set 3 is worth
including in the feature list at all, given it is the
hardest to do yet contributes the least business value.
Discussion Questions: For your set of features, which
feature sets are most valuable? Most complex?
Which order would you have your implementation
team do the requirements in?
POSTING TO SAKAI: Describe the similarities
and differences between the Brainstorming activity
and the Affinity activity. Name and discuss (couple
sentences) 2 ways in which Brainstorming is better
than Affinity and why. Name and discuss 2 ways in
which Affinity is better than Brainstorming. Finally,
discuss when group techniques may be better to use
than individual interviews.

7.2 Student Responses to Lab
These responses were posted to a Wiki area made
available to the lab groups.
GROUP EXERCISE DISCUSSION
Each individual is required to go over to the forums
tool and post a response to each of the three
discussion questions.
TEAM ONE RESULTS

I would expect a table to look like:
CLUSTER
Feature Set
1
Feature Set
2
Feature Set
3

Biz
Value
Votes
= 30
Votes
= 20
Votes
= 10

Biz
%
50%

Complexity

%

Votes = 20

33%

33%

Votes = 30

50%

17%

Votes = 10

17%

Team One please post your Brainstorming and
Affinity prioritization results here. Use the table
feature in the Wiki.
Implementati
on

Desirability
Features

M.H.

Web based

XXXXXX

Send suggestions
XXX
through myASU

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

N.T.H.

Easy

Hard

XXXXXX
XXX

XX

XXXX

Email class
listings

XXX

XXX

XX

Ability to search XXXXXX

XXXXXXX

Weighted
suggestions

X

XXXXXX XXX

Course
suggestions

XXXXXX

User Reports

XXXX

XXXX

XX

XXXXXX

Status

XXXXX

X

XXXXXX

Ability to
Comment on
classes

X

XXXXX

XXXXXX

Class Rating

X

XXXXX

XXXXXX

Categories

Business
Value

Complexity

Application

4

4

Extras

10

13

Fuel

8

6

Cost

21

9

Location

19

19

Comp

2

2

Total

64

53

XXX

“After experiencing both techniques first hand, I
would have to say that the Affinity process is
more enjoyable. Working in a team and not being
able to communicate verbally is a nice challenge
that makes the technique a bit more fun. It's
interesting to watch how your team works
together without speaking to form the clusters of
similar items on the wall. In the end you learn
new inventive ways to work with your teammates.
This method as feels a lot more informal than
brainstorming since you a pretty much just
writing down ideas on sticky notes and posting
them on the wall. Since it is less formal, it makes
it a bit less stressful for getting ideas.”
“I found the brainstorming technique to be more
enjoyable because it gave more room for open
collaboration and I felt more involved.”
2. “Which technique is more effective (produces
better feature lists)?”

Difficulty

Must
Have

Nice to
Have

Easy

Difficult

Know Student's Major Before
Suggesting Classes

XXXX

X

X

XXXXproduce

Class Cart

XXX

XX

XXX

XX

Robust Search Tool

XXXXX

X

XXXX

XXXXX XXXX

County Wide/Multiple Schools

X

Admin Add/Remove Courses

XXXX

Detailed Course Description

XXXX

XX

Student Reviews

XXXX
X

XXXXX

XXXXX XXXXX

Cluster

Business
Percentage Complexity Percentage
Value

Directions

18

36

19

38

Feedback

4

8

2

4

Prices

8

16

11

22

Extra Services

9

18

7

14

Operating Hours 5

10

3

6

Miscellaneous

4

2

4

2

12

1. “Which technique was more enjoyable?”

Desire

Subscription for Future Free
Courses

6

Students were required by the lab to individually post
responses to questions put on the course management
system’s forums tool. Below are the questions and a
sampling of student responses.

TEAM TWO RESULTS
Team Two please post your Brainstorming and
Affinity prioritization results here. Use the table
feature in the Wiki.
Features

8

7.3 Student Discussion

XXXXXX

Categorization

4

“I think that the brainstorming approach will
better results as it gives more room for a
group effort and then narrows the results to the
most viable.”

“Affinity seemed to help create a larger set of ideas
since no one would contest the ideas, so people
XXX could feel less insecure about their ideas and just
throw them out there in the beginning.”
X

3. “Which technique mitigates social imbalance and
social issues better?”
“I think that the affinity process was better in this
realm because the affinity process doesn't allow for
open collaboration/open discuss, there by eliminating
room for snide remarks or comments about what
another person added as a requirement.”
“I think this issue is relative and you have to really
look at what is meant by social issues. I think
ultimately, it depends on team members and their

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

personality. I personally like brainstorming and I find
it much more interesting, but the problem usually
comes when people ignore the "no idea is a bad idea"
line and that's where the tension kicks in. On the
other hand, people can be very difficult explaining
themselves on paper and much rather discuss it out in
the open. You could say that affinity process works
better, because it allows those with shy personalities
to put an idea down, there's no need for a coordinator
so everyone gets a fair chance as well. But again, it
really depends on the team.”

concessions had to be made, but on the whole, was a
very rewarding experience, as it helped to show how
people tend to work in groups.”

7.5 Exam Question
Asking questions on elicitation can be difficult in a
traditional exam setting. Below is a question from the
Fall semester 2008 midterm on elicitation:
Requirements Elicitation. Consider the following
scenario:

7.4 Reflection
After performing the lab, teams are expected to apply
techniques immediately on their projects. This means
that teams should, if they feel it is applicable,
perform a group elicitation activity with their
industry sponsor. After doing so, they are then
required to reflect on the experience on their team’s
project Wiki. A sampling of relevant entries on group
elicitation from team project Wikis:
This team did not have a positive experience:
“We decided to make our first meeting with <our
sponsor> an interview session. We made up an
agenda and we sent a copy for <sponsor> to review.
We ran into a few snags though. We did not follow
the order of the agenda. This made the flow of the
meeting and interview jump all over the place and it
felt chaotic. Next time we try this, we will have to
stick the agenda more.
We tried a brainstorming exercise. We called a team
meeting and sat down and our goal was to come up
with things we would want in a collaboration site.
<STUDENT 1> and <STUDENT 2> came up with a
few ideas, but no one else would even give
suggestions. I do not know if there was intimidation
or what may have caused no ideas to flow of other
members in the group. We are thinking of doing a
survey to obtain these results now.”

“Your customer wishes to build a new web-based
test-taking tool for colleges and universities. The tool
will have to support a wide range of subjects
(chemistry, english, nursing, religious studies, etc.)
and state-specific assessment regulations for
privacy.”
For each technique indicate whether you think it
would be useful to apply this technique for this
domain. Note that options continue on the next page.
One-on-One Interviews

Questionnaires/Surveys

Ethnography/Observation

Independent Research

Unstructured Group Brainstorming

Structured Group Workshops

Prototyping

This team had a bit more success:
“The in class group exercises were very helpful, and
taught me some great techniques in our meetings
with <our sponsor>. The first interview activity was
helpful in learning some good one-on-one techniques
and basic requirements elicitation skills. The group
activity however, seemed to be the more helpful of
the two. Having to work with 4 other people and
create features and requirements as a group was very
interesting indeed. There was a lot of clashes and

2009 Fourth International Workshop on Requirements Engineering Education and Training (REET'09)
978-0-7695-4103-7/10 $26.00 © 2010

JMaPSS: Spreading Activation Search for the Semantic Web
Kevin Gary✝, Bradley Szabo, Lavanya Vijayan,
Braden Chapman, Jayavarshini Radhakrishnan, Aishwarya Sivaraman
Division of Computing Studies
Arizona State University at the Polytechnic Campus
7001 East Williams Field Road
Mesa, Arizona 85212
kgary@asu.edu
Abstract

was anticipated as the next great revolution in computing.
As processor efficiency rose dramatically, the revolution
sputtered, and the emphasis on parallel algorithms in AI
waned. Second, the research communities in both
cognitive psychology and AI never seemed to agree on a
foundation for the theory and practice; instead a number
of variants were experimented with (to some success) but
momentum never sustained.
The latter reason is especially prescient. AI
techniques, including marker-passing, were evaluated
more on a formal basis as a means to proving correctness
of a (limited) reasoning agent’s capabilities.
The
imprecise, informal, and unpredictable nature of the
marker-passing algorithm did not fit well (though some
researchers tried). Of course, this was also an era before
widespread use of the Internet. The utility of a search
technique that naturally identifies relevant information
was not understood at the time.
The Internet is an intractable web of knowledge that
must be sifted through to find needed information. This
is eerily analogous to the issues encountered by limited
reasoning agents, who must draw conclusions based on
incomplete knowledge and limited computational
capabilities. Search engines are the prevalent method for
finding information on the Internet. Most engines are
based on some type of parsing, indexing, and ranking
algorithm that returns repeatable search results. Given
the similarities to limited reasoning agents, we believe it
is worthwhile to revisit techniques such as markerpassing to see if they have some utility for performing
web-based search. The Java Marker-passing Search
System (JMaPSS) is an experiment in this area.
In this paper we present JMaPSS, a work-in-progress
in exploring marker-passing for Internet search. We
present an overview of the JMaPSS engine and describe
its application to the semantic web. We discuss our initial
explorations and compare it to related work, and
conclude by suggesting avenues for future work.

The semantic web augments search by providing
meta-information to structure knowledge. Challenges
associated with search technology, such as accessing a
large knowledge base with limited processing capability,
may be addressed by AI techniques that provide greater
flexibility albeit with less precision. In this paper we
present JMaPSS, which applies a parallel search
algorithm known as marker-passing to improve search
relevancy results. We describe an instantiation of
JMaPSS implemented specifically for semantic web
search. Our investigations suggest that such techniques,
using an expanded notion of recall emphasizing
relevance, deserve additional exploration.

1. Introduction
Marker-passing is a parallel search technique with
foundations in spreading activation theories[1][2] of
cognitive psychology. Broadly speaking, spreading
activation theories assume human memory is organized
as a semantic network, with nodes representing symbolic
or sub-symbolic chunks of knowledge and links represent
relationships between chunks. External stimuli “excites”
nodes, causing a cascading excitation in a neighborhood
of the originating nodes in the network. Marker-passing
implements the spreading activation theory.
A
knowledge base is structured as a semantic network, and
tokens, or markers, initiate a parallel search process in
the repository. At the conclusion of the propagation
process, nodes that have been visited by one or more
markers are candidates for further processing.
For over a decade marker-passing enjoyed some
popularity in AI, including applications in planning [3],
natural language understanding[4][5][6], and knowledge
representation and inferencing[7][8]. Popularity waned
for several reasons.
First, marker-passing became
popular in an era where massively parallel computation

1-4244-1500-4/07/$25.00 ©2007 IEEE

104

2. JMaPSS Overview

network using a JUNG data structure. The markerpassing algorithm operates over this data structure. When
a user provides a search query, markers (tokens extracted
from the query) are created and injected into the network.
The markers propagate from matching nodes in parallel
throughout the network until their excitation level falls
below a threshold (attenuates). At the termination of this
propagation process, nodes with excitation levels above a
threshold are returned as the results of the search.
The expectation is that if search terms are related via
some common (intersecting) concept, then markers will
converge on that concept, excite that node, and cause that
node to be returned by the search process. Search results
imply relevance; nodes are returned because they are
related (recall), not because they “match” (precision).

JMaPSS applies a heuristic marker-passing search
algorithm to generic web-based search and semantic web
search. In this section we present the basic features and
operation of JMaPSS. The following sections describe
JMaPSS application to these two types of searches.
The JMaPSS system has following features:
• Web-based Interface. Users are provided with a webbased interface for creating a searchable graph
structure, modifying the elements of that structure, and
executing various tasks (see Table 1).
• Web Service Interface. An optional web services
interface may be deployed that exposes WSDL for
each major JMaPSS operation.
• Document Retrieval and Indexing. JMaPSS uses a web
spider to collect documents and then parse and index
them for use in searching.
• Creating Graph Structures from Indexed Documents.
The system uses the indexed documents to build a
graph structure in memory.
• Modifiability of Graph Elements and Functionality.
The system enables the user to retrieve the state and
functionality of individual graph elements and also to
update those settings.
• Search Requests, Execution, and Results. Search is
performed through user interfaces (UIs) that enable
terms to be specified as search primers. The UIs handle
receiving search requests, conducting the search
process, and presenting search results.
JMaPSS functions are accessed through these tools:

Figure 1. Graph Viewer Tool

3. Semantic Web Search

Table 1. JMaPSS tools
Function
Search query
Search results
Function editor
Node editor
Graph viewer
Node selector
Reset editor
Web spider

The standard JMaPSS engine creates a semantic
network from web (HTML) pages. The semantic network
consists of Document and Term nodes that store specific
data related to each type of node. Links between nodes
are represented as separate objects that correspond to
relationships between connected nodes. For example,
when a HTML file is parsed and indexed, the terms are
stored in “Term” nodes and the HTML document in
which all the terms appear is stored in a “Document”
node. Links are created between Term nodes and their
corresponding Document node. If a term exists in more
than one HTML document, links are created to all the
document nodes that contain the specific term.
This implementation had mixed success. Interestingly,
it often finds relevant information at the intersection of
two or more search terms that might otherwise not be
found. However, the algorithm has proven difficult to
tune as it is sensitive to the topology characteristics of the
semantic network. Although the implementation provides
for per-node customization of marker propagation
behavior, there is no basis for utilizing this behavior. Put
another way, there is no semantic information that tells

Description
The user enters a set of search terms.
Results are returned based on the
excitation of nodes in the graph.
The power user may modify markerpassing characteristics at any node.
The power user may manipulate state
(attribute values) at any given node.
The user may navigate the semantic
network and inspect nodes (Figure 1).
Users may filter returned nodes by
excitation level.
The power user may reset system state
Seeds the semantic network.

JMaPSS leverages existing open source technologies
such as Lucene[9], JUNG[10], and Apache Commons
Digester[11] to create the underlying search framework.
A web spider utility extracts files from various websites.
These files are parsed and indexed using Lucene. Lucene
separates the terms and indexes them with their
associated web addresses and massages it into a semantic

105

the algorithm when one Term node should be favored
when markers propagate through a Document node. This
means any term appearing in the document is considered
as important as any other term. Furthermore, there is no
effective way to normalize propagation for documents
with varying numbers of terms. We are currently
exploring the use of link analysis techniques in traditional
ranking engines for application within JMaPSS.
We are also exploring stronger heuristics in the
construction of our semantic network. In this exploration
we use the semantic web to provide richer type
information that can then be used to direct marker
propagation. To understand how the semantic web can
assist with marker propagation issues, we first describe
the nature of an ontology.

semantic network. This was done in JMaPSS by
modifying the implementation in the following ways:
1. OWL-Lite files are parsed and elements extracted.
2. Elements are indexed and stored using Lucene.
3. Various indexed terms in Lucene are mapped to a
JUNG-implemented graph.
4. The JUNG graph implementation was modified to
support multiple types of nodes.
A graph structure for a portion of the publishing
example from Knouf[14] is shown in Figure 2. Oval
nodes represent classes, rounded rectangles represent
properties, and octagonal nodes represent individuals.
Type information on the links between nodes indicates
the nature of the relationship between nodes, i.e. whether
a class is a subclass of another class, a property is a
subtype of another property, or a property describes a
class (DataProperty) or an individual (ObjectProperty).
The additional typing information provided by an
OWL-Lite ontology serves as input to the markerpasser’s propagation algorithm. The marker-passer can
now define how much excitation to distribute from one
type of node to another based on the type of relationship.
For example, markers originating at Property nodes
excite Class nodes at the intersection of those properties.

3.1. OWL-Lite ontologies
Ontologies define terms used to describe and represent
an area of knowledge. To support the sharing and reuse
of formally represented knowledge a common vocabulary
is needed. An ontology is a representational vocabulary
for a shared domain of discourse providing definitions for
classes, relations, functions, and other objects[12]. The
Web Ontology Language (OWL)[13] is a W3C standard
widely used for specifying ontologies for information on
the Web. OWL-Lite is a subset of the OWL language that
is commonly used to describe simple ontologies. The
following elements are part of the OWL-Lite format
relevant to JMaPSS: {Class, subClassOf, Property,
subPropertyOf, domain, range, Individual}.
Individuals that share properties is described by the
element Class. An Individual is an instance of a Class,
and a Property can be used to state relationships between
individuals (ObjectProperty) or between individuals to
data values (DataProperty). Class hierarchies can be
created by using the syntax element subClassOf. In the
example given in [14], Entry is a Class with subclasses
Article, Publisher, and Book, while elements prefixed by
“has” are properties. DoubleDayPublisher and The
DaVinci Code are Individuals of Publisher and Book.
Semantic typing information in OWL-Lite provides
useful metadata for guiding heuristic search. One can
envision users’ providing intelligent queries based on the
ontological structure, and defining heuristics to direct
relevance-oriented search engines such as JMaPSS.

Figure 2. Sample OWL-Lite Graph

3.2. Mapping OWL-Lite to a semantic network
To apply the marker-passing algorithm, an OWL-Lite
file must be converted to a graph structure resembling a

106

The semantic network construction algorithm for the
sample OWL-Lite ontology is given in Figure 3.

Beaches” then JMaPSS based semantic search should
give results only related to island.
For this scenario, two OWL-Lite files were created and
deployed in JMaPSS, one describing an analog still
picture camera (top half of Figure 4) and one which
describes a digital video camera (Figure 6). We presented
JMaPSS with the following queries:
1. Camera that has Compression
2. Camera with CCD
3. Camera with MPEG
There are two expected outcomes to this experiment:
(1) the result should be an intersection of the query terms
(2) if a concept is searched using the properties of the
concept (keyword), the search should resolve in the
concept itself even though the keyword or concept is not
part of the query. In this example, we expected digital
video camera as a result even though the query was
formed without using keywords digital or video.

1. For each Class A, create a node for A.
Example: Entry, Book, Article, Publisher
2. For each subclass B of A, link B to A
Example: Article to Entry, Book to Entry
3. For each Property P, create a node for P
Example: hasJournal, humanCreator
4. For each DatatypeProperty DP, connect DP to
Class A if DP is a propertyOf A (A is the
domain of DP). Only connect DP at the highest
level of the Class hierarchy.
Example: hasName
5. For each ObjectProperty OP, connect OP to
its domain and range Classes.
Example:hasPublisher,dom:Book,range:Publish
6. For each subProperty S of Property P,
connect S to P.
7. Create a node for each Instance
Example: “The Davinci Code”, “Doubleday”
8.For each Instance X of Class A link X->A
Example: link “The Davinci Code”->“Doubleday”
9. If Class A is the range of some
ObjectProperty P on another Class B (the
domain), then connect the Instance X of Class
A to the Instance Y of Class B.
Example: “The Davinci Code” is linked to
“Doubleday” as the XML the value of the Book’s
publisher is the Publisher instance Doubleday.

Figure 3. Semantic Network Algorithm

3.3. JMaPSS Validation Scenarios
Heuristic search based on semantic web information
may be applied to resolve semantic ambiguity scenarios.
One scenario is where a search engine attempts to resolve
queries by identifying concepts relevant to other
concepts. Other scenarios are derived from the problem
of ontology matching, where two reasoning agents must
negotiate common understanding in the face of distinct
ontologies that may or may not partially overlap.
To evaluate the behavior of JMaPSS search, three
scenarios were constructed. The first scenario simply
verified that JMaPSS returned relevant information when
presented with search queries of a single term on a single
instance of JMaPSS. This scenario is used more to verify
the proper implementation of the engine and tune
propagation behavior and so is not presented here.
3.3.1. Scenario 1: Concept ambiguity. Searching for a
concept (keyword) having different meanings and
represented by different ontologies should result in a term
that is an intersection of the meanings of the query terms.
For example, “Java” has three different meanings: coffee,
computer language, and island. If we search for Java, we
will get results related to all the three meanings. But if we
qualify the search with a description like “Java with

Figure 4. Digital camera ontology schematic

107

The result of the query is shown in Figure 5. JMaPSS’s
result matched the expected outcomes. We can see the
terms Camera and MPEG are two hops away from the
term “DigitalVideoCamera” yet excites the term
“DigitalVideoCamera”. DigitalVideoCamera is an
intersection of Camera and MPEG. Also, we can see that
even though the search query did not have terms digital
or video, the result of the search yielded
“DigitalVideoCamera” as one of the outputs.

Figure 6. Digital video camera ontology
Results for queres 1 and 2 are given in Figures 7 and 8.
Figure 5. Scenario 1 query results
3.3.2. Scenario 2: Partially overlapping ontologies.
The goal of this experiment is to map an ontology, which
is a subset of another ontology of a given domain, and
check whether search is able to resolve queries pertaining
to each of these ontologies.
For this experiment, we created four ontologies for
analog still picture camera (ASPC), analog video camera,
(AVC), digital still picture camera (DSPC) and (DVC)
digital video camera such that ASPC ∪ AVC, DSPC, and
DVC; DSPC ∪ DVC; and AVC ∪ DVC. Two OWLLite files were created and deployed in JMaPSS (see full
Figure 4 and Figure 6). The following queries were given
using the search interface:
1. Camera with shutterspeed 200
2. DigitalCamera with FlashMemory
A search for "camera" should get results related to
ASPC and perhaps other elements. A search for “digital
camera” using digital camera properties we should get
results related to DSPC and not the other ontologies.
Figure 7. Results for scenario 2, query 1

108

5. Future Work
JMaPSS embodies the good and bad of spreading
activation theories and implementations. The JMaPSS
implementation for the semantic web shows that the
additional meta-information included in OWL-Lite
documents may be used to direct excitation down
“relevant” pathways in the semantic network. The result
is a flexible, intuitive algorithm that may find interesting
information for the user that might otherwise not be
returned using a more conventional search engine.
The flexibility also leads to inexplicable connections
and a lack of repeatable search results. While the former
may be accepted in exchange for interesting results, the
latter violates a fundamental assumption of most modern
search engines. Whether this assumption is appropriate
is a topic outside the scope of this paper.
Figure 8. Results for scenario 2, query 2

8. References

The results were as expected. Search for camera
yielded all terms related to camera and not digital
camera. Search for digital camera yielded all terms
related to digital camera and not camera.

[1] M.R. Quillian, “Semantic Memory”, in M. Minsky (ed.) Semantic
[2]

4. Related Work

[3]

A few other examples of spreading activation
implementations for the semantic web have recently
appeared in the literature. In [15], Rocha et. al. present a
hybrid approach to implementing a spreading activation
model that relies on domain experts to set numeric
weights on relationships defined by the ontology. The
same effect is achieved in JMaPSS by assigning different
propagation functions to different node types. Rocha also
allows different initial propagation values to be assigned
based on the relevance of the input token (an instance, or
Individual as described here for OWL-Lite) to a specified
task. The paper also discusses how marker propagation
must be constrained based on network topology, a
common issue as discussed above.
In [16], the author describes another hybrid search
strategy, but this time does not assign weights a priori
based on human intervention. Instead, the algorithm
starts with an initial estimate based on the documents
stored in the network, and uses a feedback process to
adjust weights over time. Given an assumption of a
growing knowledge base and the imprecision in
attempting to manually assign weights, a feedback or
training process should help with tuning search results.

[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]

109

Information Processing, MIT Press, Cambridge, MA. pp. 227270, 1968.
A. Collins and E. Loftus, “A spreading activation theory of
semantic processing”, The Psychological Review, 82(6):407-428,
1975.
J. Hendler, Integrating marker-passing and problem solving: A
spreading activation approach to improved choice in planning,
Lawrence Erlbaum Associates, Hillsdale, NJ, 1988.
E. Charniak, “Passing markers: A theory of contextual influence
in language comprehension”, Cognitive Science, 7:171-190, 1983.
E. Charniak, “A neat theory of marker-passing”, Proceedings of
AAAI-86, pp. 584-588, 1986.
P. Norvig, “Marker passing as a weak method for text
inferencing”, Cognitive Science, 13:569-620, 1989.
S. Fahlman, NETL: A system for representing and using realworld knowledge, MIT Press, Cambridge, MA, 1979.
W. Lee and D. Moldovan, “The design of a marker passing
architecture for knowledge processing”, Proceedings of AAAI-90,
pp. 59-64, 1990.
Apache Lucene Overview [Online]. Available:
http://lucene.apache.org/java/docs.
JUNG. [Online]. Available:
http://jung.sourceforge.net/doc/index.html.
O. Gospodnetic. Parsing, indexing and searching XML with
Digester. Available: http://www128.ibm.com/developerworks/library/j-lucene/index.html.
T. R. Gruber. “A translation approach to portable ontologies”,
Knowledge Acquisition, p. 199-220 1993
World-Wide Web Consortium (W3C), Web Ontology Language
(OWL), [Online]. Available : http://www.w3.org/2004/OWL/
N. Knouf. bibTeX Definition in Web Ontology Language (OWL).
[Online] Available: http://visus.mit.edu/bibtex/0.1/
C. Rocha, D. Schwabe, and M.P. de Aragao, “A hybrid approach
for searching in the semantic web”, Proceedings WWW 2004,
May 2004.
M.M. Hasan, “A spreading activation framework for ontologyenhanced adaptive information access within organizations”,
International Symposium on Agent Mediated Knowledge
Management, pp. 288-296, Stanford, CA, March 2003.

Pre-Conference Workshop:
Agile Teaching and Learning
Kevin A. Gary, Sohum Sohoni, and Suhas Xavier
School of Computer, Informatics, and Decision Systems Engineering (CIDSE)
The Ira A. Fulton Schools of Engineering
Mesa, AZ 85281 USA
kgary@asu.edu*, ssohoni@asu.edu, suhas.xavier@asu.edu
Abstract— Agile methods are the fastest rising software
lifecycle process methods in software engineering. Educators are
converting traditional and project-base courses to agile in
response, but this is a daunting task with few structured teaching
resources methods available to reduce the burden on the
educator. In professional practice, agile methods have been
particularly effective in empowering experienced software
engineers through its focus on empirical process control and
constant feedback loop. These process traits are difficult to
simulate in an academic setting, as student developers are
inexperienced, synchronous meeting times are few and far
between, and obtaining meaningful constant feedback a laborious
undertaking. This workshop will present a comprehensive
approach to teaching Agile methods that is itself agile, employing
a highly iterative, continuous feedback-driven process.
Pedagogical and assessment strategies will be shared, and the
presenter will facilitate a best practices interactive discussion to
draw out lessons learned from workshop participants. Specific
agile practices with supporting labs from the popular Scrum and
eXtreme Programming (XP) process models will be presented.
The workshop will also encourage interaction amongst
participants to share best practices and lessons learned. Research
directions related to the application of agile principles to teaching
and learning will be discussed.
Keywords—agile; software engineering; software process;
assessment; integration

I.

INTRODUCTION

Agile methods are the fastest rising software lifecycle
process methods in software engineering, yet they present
unique challenges to educators. Many educators are familiar
with agile methods but have not practiced them in an
industrialized setting. Further, most textbooks have introduced
agile only in later additions with little supporting material, and
curricular resources are either scarcely available or geared
toward the industry practitioner. This workshop will convey
instructional resources to higher ed instructors, and provide the
scaffolding to conduct an agile project course. Participants will
receive materials including Eclipse-based open source labs,
project scripts, and pedagogical scripts to adapt to their
classrooms. Additionally, the workshop will include strategies
and instructions for setting up Scrum, and supporting XP
developer practices including Continuous Integration and
Testing using Jenkins, Source Code Control using Git, unit
This work is supported by a grant from the State Farm Foundation.
*contact author

978-1-4799-8454-1/15/$31.00 ©2015 IEEE

testing, static analysis, and more. Post-workshop support will
be provided through a website hosted by the presenter.
The goals of the workshop are to:
1.
2.
3.

Provide participants with ready-to-implement materials
and practices for teaching agile methods in the classroom.
Identify a community of interest in applying agile methods
to project-based learning.
Discuss research potential in applying these software
engineering principles to teaching and learning.
II.

INTENDED AUDIENCE

Post-secondary Computer Science educators who are
interested in Agile methods applied to teaching and learning,
particularly in a hands-on, project-centered course. Teachers
from other disciplines can also benefit through the presentation
of Agile principles and considerations for adoption in other
disciplines. Engineering education researchers may find the
potential research directions of agile and teaching and learning
of interest.
III.

PRESENTER BIOGRAPHIES

Kevin Gary is an Associate Professor in the School of
Computing, Informatics, and Decision Systems Engineering
(CIDSE) within the Ira A. Fulton Schools of Engineering at
Arizona State University. Kevin is a founding program faculty
member for the new B.S. and M.S. degrees in Software
Engineering at ASU. He created the Software Enterprise, a
highly iterative, agile pedagogy in 2004, which has served as
the project spine of 8 courses in the Software Engineering
degrees. Contact him at kgary@asu.edu
Sohum Sohoni is an Assistant Professor in the School of
Computing, Informatics, and Decision Systems Engineering
(CIDSE) within the Ira A. Fulton Schools of Engineering at
Arizona State University. Prior to ASU, he was an Assistant
Professor at Oklahoma State University. He has published over
25 peer-reviewed papers in journals and conferences including
papers in ACM SIGMETRICS, IEEE Transactions on
Computers, the International Journal of Engineering Education,
and Advances in Engineering Education. His research interests
are broadly in the areas of computer architecture and
performance analysis, and in engineering and computer science

education. He has received many teaching awards including the
Regents Distinguished Teaching Award in 2010 at OSU. He
has received a best paper award for his work in computer
engineering from the IETE Technical Review journal, and two
best paper awards for his work in engineering education from
the ASEE Midwest Section. He received the B.E. degree in
Electrical Engineering from Pune University, India, in 1998
and a PhD in Computer Engineering from the University of
Cincinnati, Cincinnati, Ohio, in 2004.
Suhas Xavier is a graduate student in the Master of Science
in Software Engineering program at Arizona State University.
Suhas’ interests are in big data, learning analytics, and software
engineering. Prior to enrolling at ASU, Suhas worked in
industry as a software engineering for Tech Mahindra Private
Limited in India. Suhas holds a Bachelor of Engineering in
Information Science (First Class with Distinction) from the
Visvesvaraya Technological University
IV.

MATERIALS PROVIDED

Each participant will receive printed handouts of slides
presented at the workshop. Additionally, a custom Eclipse
install with requisite tools will be available for installation
during the workshop via USB stick and the Web. The presenter
will record and coalesce information gathered from
collaborative sessions on the agenda and make the record
available to participants on his university website. Instructions
for server-based installations and trial accounts will be
distributed to participants.
V.
1.

2.

AGENDA

Topic one: Introduction to Agile Methods, (20 minutes).
a. Quick recap of Agile, including Scrum and
eXtreme Programming (XP), for those somewhat
unfamiliar with these methods.
Topic two: Integrating Agile Projects into a semester
course (50 minutes)
a. Interactive exploration: Challenges teaching agile
(table discussions)
b. Scheduling Scrum activities during the semester

3.

4.

5.

c. Setting student expectations
Topic three: Establishing an Agile Rhythm (45 minutes)
a. What does rhythm mean and why is it important?
b. Encouraging continuous project participation using
Scrumboards.
c. Continuous project assessment
d. The role of tools and scaffolding
Utilizing Agile development best practices in an Agile
project context (45 minutes)
a. Agile best practices labs including source code
control with Git, continuous integration and testing
using Jenkins.
b. Immediate synthesis of best practices into agile
course project
Wrap-up (20 minutes)
a. Interactive: Eliciting best practices and lessons
learned
b. Agile recap
VI.

REQUIREMENTS

A. Audio/Visual and Computer Requirements:
It is strongly recommended participants bring a laptop to
the workshop, though it is not strictly required. Laptop users
will be able to browse/experiment with lab materials and
remote server tools. However non-laptop users will still be able
to follow along the pedagogical and curricular discussions and
participate in the interactive sessions. Wireless Internet is
recommended to access remote server tools, though the
presenters will also demo these up front. Software will be
provided that will be distributed on a website or by USB stick
for those without Internet access.
B. Laptop Required:
Laptops are highly recommended. Tablets will be of only
limited utility.
C. Space and Enrollment Restrictions:
None. No fee required for materials or expenses.

2005 ACM Symposium on Applied Computing

Use Case-Driven Component Specification: A Medical
Applications Perspective to Product Line Development
M. Brian Blake

Kevin Cleary , Sohan R. Ranjan

Department of
Computer Science
Georgetown University
Washington, DC, USA

Imaging Sciences and Information Systems
(ISIS) Center, Dept of Radiology
Georgetown University
Washington, DC, USA

Luis Ibanez

Kevin Gary

Kitware
Division of
Incorporated
Computing Studies
Clifton Park, New Arizona State Univ.
York, USA
Mesa, Arizona, USA
blakeb@cs.georgetown.edu {cleary,ranjan}@isis.imac.georgetown.edu luis.ibanez@kitware kgary@asu.edu
.com
level software programs with well-defined interfaces.
Encapsulation of functionality within such components can
assure the autonomy of these components, thus facilitating their
reuse. Furthermore, the composition of various components can
lead to the development of higher-level composite services.

ABSTRACT
Modular and flexible software components can be useful for reuse
across a class of domain-specific applications or product lines.
By varying the composition of components suited to a particular
product line, an assortment of applications can be developed to
support differing operational needs. A top-down approach to the
design components for a specific application may be effective,
however a more evolutionary approach is needed to support the
specification of components suited for a class of applications. In
addition, such evolutionary approaches require support for the
knowledge transfer that must occur from domain experts, who are
not software experts, to skilled software engineers.
By
combining concepts from Software Product Line Development
(SPLD) and other evolutionary design techniques, a new, use
case-driven approach has been created called Component-Based
Product Line Analysis and Design (C-PLAD). This approach was
used to develop components in the domain of image-guided
surgery applications.

An open problem is how to define such components to maximize
their ability to be reused in other software development
environments. The focus of this work is designing components
suited for a specific class of applications (i.e. image-guided
surgery applications).
Even within a specific class of
applications, there are several problems that arise in specifying
appropriate components that support reuse and later composition.
This paper addresses two specific problems:

Categories and Subject Descriptors
D.2.2 [Software]: Design Tools and Techniques

1.

Given a list of functional requirements within a class of
applications, what methods can be used to determine
the appropriate grouping of functions to create the
independent components?

2.

What development process supports component
specification considering the collaboration that must
occur between domain experts and software engineers?

Relating to problem 1, components require the grouping of
functional requirements or features [1][11].
Setting these
boundaries is a difficult problem for software engineers. If
components are specified to fulfill an unreasonable amount of
features then the components become monolithic and inherently
too specific. Components scoped too aggressively become
elementary and lose their usefulness.

General Terms
Design, Experimentation, Standardization, and Languages

Keywords
Component Specifications, Generation of Component-Based
Systems, Medical Domain, Software Lifecycle.

Problem 2 further extends problem 1. Generating component
specifications requires the collaboration of domain experts and
software engineers. Experienced software engineers are trained to
understand various analysis, design, and development processes
such as the Rational Unified Process (RUP) [9] or Agile Software
Development [2][7].
However, these processes and their
underlying methodologies are typically not apparent to domain
experts. Nevertheless, the specification of appropriate system
features requires the input and iterative feedback of the domain
experts.

1. INTRODUCTION
Many development methodologies are adopting practices to
increase the modularity of the software that they create. Objectoriented principles [3] and new approaches to component-based
software development [8] support the development of higher-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SAC’05, March 13-17, 2005, Santa Fe, New Mexico, U.S.A.
Copyright 2005 ACM 1-58113-964-0/05/0003...$5.00

In this paper, an approach is introduced to address the
aforementioned problems. By incorporating concepts from
Software Product Line Development (SPLD) (to be discussed in
Section 2) and developing a customized analysis, design and
development process, a new approach has been created and used
to develop components in the domain of image-guided surgery

1470

applications. The following Section 2 describes the customization
of SPLD towards component development in this context.
Sections 3 and 4 present a new process for component
specification and the underlying use case analysis technique.
Section 5 details our experience in using this approach to develop
components suitable for the composition of applications in the
image-guided surgery domain.

ESAPS Reference Model
Legacy Code
Domain Expertise

Feedback/ Adapatation / Reverse Architecting

Domain
Analysis

Domain
Design
*

Domain
Engineering

*
Domain-Specific

Domain
Implementation
*

Reference
*
Architecture

Reusable Components
*
*

Requirements

Architecture

Application
Requirements

Application
Design

Components

Application
Engineering

2. USE CASE-DRIVEN PRODUCT LINE
ANALYSIS
The idea of product lines is not new in the manufacturing domain.
Particularly in the automotive industry, product lines have been
used to reduce cost by exploiting the commonalities of products.
SPLD can be used similarly to promote cross-application reuse.
The major concept of this approach is to use the knowledge of
domain experts to help understand the commonalities of a class of
applications. Once these commonalities are determined, a
software framework can be developed to support a common
baseline for relevant applications or products [12].

New Requirements

Business
Requirements
Modeling

Analysis
and Design

Application
Coding

Implementation

Test

Use-Case Driven Rational Unified Process

SPLD has a natural relationship to component software principles.
Commonalities discovered in SPLD can be applied to the
development of components. In our work, component-based
development processes require the combination of top-down
principles in addition to bottom-up, iterative techniques. More
specifically, a successful component must support the system
features specified when analyzing a specific domain in addition to
having the modularity and flexibility to support the utilization or
composition during implementation. In this paper, there is a
combination of a well-known evolutionary development process,
RUP, and the principles of SPLD. The Engineering Software
Architectures, Processes and Platforms for System-Families
(ESAPS) reference process [10] is a SPLD process where the
domain analysis and development is executed in parallel to the
application-level development, though there are several integrated
sections.
This approach is just one representative SPLD
technique, and the authors recognize that there are many other
frameworks [12][15]. However, the ESAPS reference process
provides the best foundation for our approach, since there is a
strong focus toward component development.
The work
presented in this paper extends the ESAPS approach by focusing
on its relation to an evolutionary development process (i.e. the
Rational Unified Process (RUP)).

Figure 1. Aligning RUP with the ESAPS Reference Model.

3. COMPONENT-BASED PRODUCT LINE
ANALYSIS AND DESIGN
In this paper, a new development process is created based on a
customization of RUP. The major difference is that this process
jointly supports domain engineering and application engineering
within one analysis, design and development process. This new
approach is called Component-Based Product Line Analysis and
Design (C-PLAD). C-PLAD is divided into six high-level phases.
These phases are Specification, Requirements, High-Level Use
Cases, Component-Level Use Cases, Software Design and
Development, and Testing. The software design and development
phase and the testing phase are iterative phases. The C-PLAD
process is illustrated in Figure 2. Each of the phases can be
defined in more detailed as follows.

In our work, we combine the domain engineering process and the
application engineering process defined in ESAPS into a
customized RUP. The requirements/analysis, design, and
implementation/coding phases of the ESAPS dual processes are in
the same sequence as the sub-process phases of RUP. This
relation is illustrated in Figure 1. RUP supports a use case-driven
approach. Since use cases can define the business-level and
system-level requirements of the system, they can be applied
throughout the software development lifecycle for analysis,
design, development, and testing. Though RUP is consistent with
both the domain engineering and application engineering
processes in ESAPS, it does not specifically support an integrated
approach (in its current form). A major innovation of this work is
to extend RUP with processes to support integrative domain
analysis and application analysis using use case-driven
approaches. This approach is discussed in detail in the following
section.

1471

•

Specification. The C-PLAD approach takes a similar
approach to system conceptualization as the Object
Modeling Technique (OMT) [14]. The initial system
description is written in the form of a problem statement.
This problem statement is written jointly by the domain
expert and the software engineers. The problem statement
represents the initial high-level understanding of the problem
domain or class of applications.

•

Requirements. In the requirements phase, software engineers
again consult with the domain experts to draft a set of written
requirements which scopes the project. The focus of the
software engineers should be to separate system features
from software requirements. In context of a class of
applications, this represents the separation of functional
concerns from nonfunctional concerns.

•

High-Level Use Cases. The first two phases are similar to
both the RUP and OMT development phases. Though the
high-level use case phase is also related, it is in this phase
that specific C-PLAD use case templates are employed to
characterize the specific product. Based on the initial
problem statement and requirements, software engineers
draft an initial set of use cases in this phase. These high-

•

level use cases characterize the sample applications in this
particular product line in addition to the separation of
functional and nonfunctional use cases.
•

•

Component-Level Use Cases. This component-level use case
phase represents an innovation in the C-PLAD approach.
This phase consists of a step-wise iterative process for
extracting component-level use cases from the high-level use
cases in the previous phase. Component-level use cases are
specified where actors typically are other components. In
addition to developing the C-PLAD-specific use case
templates, software engineers must collaborate with domain
experts to assure accuracy.

Similar to RUP, C-PLAD relies heavily on analysis based on
use case-driven techniques. A major innovation in this
approach is a specific step-wise process for creating use
cases that support both application-level and domain-level
analysis. This approach introduces new specific use case
templates to assist this step-wise process. This step-wise
process ultimately results in component-based use cases.
The component-based use cases can then follow the original
RUP techniques for implementation. The following section
describes this step-wise process and the C-PLAD use case
templates supporting the approach.

Software Design and Development. Using standard RUP
techniques, the component-based use cases are used to
continue the design and implementation process. In this
phase, structural and dynamic models (class and sequence
diagrams, respectively) are created to represent each
component. In this phase, software engineers focus on
designing clean interfaces to the component to support
composition.

S p e c ific a tio n R e q u ire m e n ts

H ig h -L e v e l
U se C ases

Testing. The testing phase is an iterative phase with the
software design and development phase. In this phase,
components are composed and tested for the set of sample
applications as modeled in the high-level and componentlevel use cases. Problems discovered in the testing phase are
used to adapt the component-based design. To support
traceability, use case component-level models are updated
based on changes in this phase.

C o m p o n e n tLevel U se
C ases

A p p lic a tio n Level

A p p lic a tio n
S p e c ific a ito n

T e s tin g

Ite ra tiv e D e s ig n , D e v e lo p m e n t, a n d T e s tin g

S y s te m
F e a tu re s
Com ponent
1 ..n

S o ftw a re
R e q u ire m e n ts

S o ftw a re D e s ig n a n d
D e v e lo p m e n t

C la s s D ia g ra m s a n d
S e q u e n c e D ia g ra m s

Test
S c rip ts

F u n c tio n a l
In fra s tru c tu re

A p p lic a tio n -L e v e l D o c u m e n ta tio n :
S p e c ific a tio n , R e q u ire m e n ts a n d S y s te m -L e v e l U s e C a s e s

Com ponent
1 ..n

C o m p o n e n t D o c u m e n ta tio n :
U s e C a s e s , C la s s D ia g ra m s , S e q u e n c e D ia g ra m s ,
a n d S o ftw a re A P Is fo r e a c h c o m p o n e n t

Figure 2. C-PLAD Approach.

4. SYSTEMATIC USE CASE-DRIVEN
SPECIFICATION PROCESS

4.1 C-PLAD High-Level Template
The C-PLAD step-wise approach is highly dependent on the
initial phases of specification and requirements. The problem
statement, system features, and software requirements represent
the necessary artifacts for constructing the initial high-level use
case diagram. The C-PLAD approach introduces a specific
template for the high-level use case model. This template and its
relation to other diagrams are shown in Figure 3. As a written
document, the problem statement typically describes both the
domain of the system to be designed in addition to an initial
explanation of its features. An experienced software engineer can
craft this document based on collaboration with a domain expert.

The major innovation of the C-PLAD approach is the
development of the component-based use cases using a step-wise
approach. This step-wise approach encompasses both high-level
use case and component-level use case phases. In this section,
there is a discussion of the creation of the high-level use cases and
their artifacts. The subsequent section contains details on how the
component-level use cases are formed.

1472

acknowledge that the application domain is more accurately the
intersection of all the specific applications.

Subsequently, a requirements document can be created to clarify
the system features and software requirements. In the C-PLAD
approach, the high-level template is a one-page view of the
system. As shown in Figure 3, the application layer in the upper
portion of the high-level template contains a list of domain-related
applications (SpecificApplication1..n) which are derived into a
class of applications or a product line (Application
Domain/Product Line Applications).
Though the use of the
hollow-tip arrow is a generalization relationship, the authors

The lower portion of the template contains a grouping of
functional capabilities and nonfunctional capabilities. These
capabilities can be derived from the features and software
requirements of the written requirements. This is accurately
shown with the hollow-tip arrow. It is these capabilities that will
be further grouped into components.

C - P L A D H ig h -L e v e l
T e m p la te

P r o b le m S ta te m e n t
Specific Application_2

W r itte n
Text

Specific Applications_1

Specific Application_n

D e riv e d C la s s
o f A p p lic a to n s
Application D omain/Product
Application Layer
Line A pplications
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Component General Functional ity

<<uses>>

C omponent Framework
<<include>>
<<uses>>

W r itte n
R e q u ir e m e n ts
Functional Capabilities

F e a tu re s

Nonfunctional C apabilties

D e riv e d
F u n c tio n a l
R e q u ire m e n ts
General Capability_1

S o ftw a r e

General Capability_n

C ommon Service_n
C ommon Services_1

General Capability_2

D e riv e d
N o n fu n c tio n a l
R e q u ire m e n ts

Figure 3. C-PLAD High Level Template.
As an example of this approach, consider a class of applications
that require software that controls an audio tape recorder. At the
application layer, the product line is the class of tape recorder
control software. This software may be specialized to applications
such as a personal radio, telephone answering machine, and
personal dictation recorder.
A software engineer could
collaborate with a domain expert to determine that a feasible
initial set of general component functions may be record, play,
and insert cassette, while nonfunctional capabilities might include
error-handling and speed control as shown in Figure 4.

Personal Radio
(Walkman)

Telephone Answering
Machine
Personal Dictation
Recorder

Tape Recorder
Application Layer
Control
-----------------------------------------------------------------------------------------------------------------------...
<<uses>>
Component General Functionality

Component
<<include>> Framework

Functional
Capabilities

4.2 Iterative Analysis Process
The C-PLAD high-level template serves both as the high-level
view and as the roadmap to an iterative analysis process. Once
the initial high-level template is created, a reasonable assumption
may be that the general capabilities (GeneralCapability1..n)
represent potential components.

Record

<<uses>>

Nonfunctional
Capabilties

Cassette Insertion
Play

Speed Control
Error-Handling

Figure 4. High-Level Template for a Class of Applications
Containing Tape Recorders.

1473

component use cases to develop components that are relevant
across a family of applications.
Likewise, component
functionality specific to a particular application are discovered.
Figure 7 demonstrates this approach. Similar to the approach in
step 2 of Figure 5, iterative creation of application-level use cases
also helps to determine functional intersections. In Figure 7, Write
to Tape is an intersection of the Dictation Machine and
Answering Machine functionality.

However, in our work in the medical domain of image-guided
surgery, it was discovered that a mixture of the underlying
functions represent a more logical component when abstracting
functions for a class of applications. As such, an iterative process
was created to help discover the most efficient grouping of
functions for creating software components. The process consists
of four steps as illustrated in Figure 5. In the first step, the
software engineer begins to further describe each of the General
Capabilities, one at a time. As in step 2, each time the software
engineer moves to a new capability, new joint capabilities may be
discovered.
In addition, new common services may be
discovered. Once all capabilities have been detailed with help
from the domain experts, in step 3, the software engineer should
create application-level use case diagrams using the General
Capabilities use cases as building blocks.
Revisit earlier use
cases

1. Specialize
Each
General
Capability,
one at a time

«uses»
Record Speaker

Listen for Record
Event

4. Use all
applicationlevel use
cases to
discover
common
functionality

Specialized
Component

Specialized
Component

The Imaging Science and Information Systems (ISIS) group is a
multi-disciplinary research group in the Department of Radiology
within Georgetown University’s Medical Center. One of the
research group’s major concentrations is the development of
prototype systems in the image-guided surgery domain. The CPLAD approach was used to develop a component framework to
support rapid application development of these prototype systems
[5][6]. Image-guided surgery is a rapidly developing field, since
these procedures mean substantially less trauma for the patient.
Image guidance was originally developed for neurosurgical
applications since navigation in the brain requires great precision.
This technology allows the physician to use pre-operative
computed tomography (CT) or magnetic resonance imaging
(MRI) scans to guide minimally invasive procedures.

<<play>>

Play
«uses»

«uses»Check Tape Inserted
Write to Tape

Common
Component

5. IMAGE-GUIDED SURGERY: A C-PLAD
EXPERIENCE REPORT

«uses»

«uses»

Access Phone
Receiver

Figure 7. Deciphering Common and Specialized Component
Functional Use Cases.

«uses»

Translate Sound to
Tape Format

«uses»
Write to Tape

Dynamic
Sound-Level Adjust

In addition, in step 3, software engineers may discover
functionality to add to the General Capability and the underlying
lower-level functions. An example, based on the earlier highlevel template, is shown in Figure 6. A software engineer may
propose initial general component functionality of Record and
Play for the tape recorder product line. As in the first step shown
in Figure 5, the Record use case may be further defined with user
cases such as Translate Sound to Tape Format and Write to Tape.
A similar approach is used to specialize the play use case. During
this process of further detailing the general use cases, several
discovered use cases may be common to both record and play.
The authors acknowledge that this is an intuitive example,
however additional examples will be shown in the following
sections.

Record

«uses»

Phone Ringing
Listener

3. Use lowlevel use
cases to
compose each
application,
one at a time

<<record>>

«uses»

«uses»
Access External
Microphone

Figure 5. Iterative Use Case Creation Process

«uses»

Record Phone
Caller Message

«uses»

«uses»

Add new low-level use
cases discovered

2. As you move to each
General Capability, look
for situations where one
General Capability uses
another and, also be
aware of the need for
new Common Services

<<Answering Machine>>

<<Dictation Machine>>

Translate Tape
«uses» Format to Sound

The domain of image-guided surgery systems incorporating an
electromagnetic tracking device is illustrated in Figure 8. This
diagram is based on a subset of the C-PLAD high-level template
of this domain mapped to several exemplar requirements.
In the scope of this paper, it is not possible to describe the entire
domain of image-guided surgery using C-PLAD. However, the
functionality required for registration demonstrates the richness of
the use case models that have been created using the C-PLAD
approach. In image-guided surgery, registration is the process of
mapping the pre-operative imaging data set (typically computed
tomography (CT) or magnetic resonance imaging (MRI)) with an
intraoperative tracking device. The final use case showing the
registration component is illustrated in Figure 9. The stereotypes
<<standardviz>>,
<<commonservices>>,
and
<<humanactions>>, show how common functionality is
discovered using the C-PLAD approach.

Assure Tape Not
Ended

New Common
Services

Figure 6. Common Use Cases among General Functionality.
Finally, in step 4, common component use cases and specialized
component use cases are extracted from the application-level use
cases. This extraction is performed via an inspection of the
resulting use case diagrams. Software designers use the common

1474

IGBiopsy

Guidewire Tracking

Bone Screw Placement

<<is a>>

Radio Frequency
Ablation

Clinician/Doc IGSTK Application
tor
Application Layer

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

General Functionality <<uses>>

Component Framework
Medical App
<<include>>
Developer

<<commoninterfaces>>

Common Interfaces
<<instantiates>>
Functional Capabilities
<<magtrack>>

Non-Functional
Capabilities

<<commonservices>>

Common Services

Magnetic Tracking
Interface
<<readerinterface>>
<<registration>>

<<segviz>>

Registration

Segmentation
Visualization

Image Reader Interface

<<standardviz>>

Standardized
Visualization

Figure 8. System Features and C-PLAD High-Level Use Case Model.

<<standardviz>>

Copied and further defined
from Main Use Case
Diagram

<<standardviz>>

Manage Current Manage Final
Transform
Transform
<<instantiates>>

<<include>>

<<include>>

<<human action>>

Touch Fiducials
<<registration>>

<<registration>>

<<registration>>

Managing Interface to
Registration
Managing User Actions
StandardVisualization
from GUI
<<include>>
<<include>>
<<instantiates>>
<<registration>>

<<registration>>

<<human action>>

Translate Fiducials to
Mag Space

Managing Connection to
Managing Image Reader
Common Services
Connection
<<include>> <<include>>

<<human action>>

Touch Fiducials in CT
Space

<<consists of>>

<<instantiates>>

<<standardviz>>

<<human action>>

Capture User Input

Start Optimization

<<registration>>

<<registration>>
<<consists of>>
Configure Moving Image<<commonservices>>
Configure Fixed Image
Manage Optimizer
Parameters
Process Actions
<<human action>>

Define Stopping Criteria
Place Fiducials Place Catheter

Figure 9. Final Registration Use Case Diagram.

1475

there were 20 components considered initially.
After
incorporating the first application, the total number of
components decreased to 14. Eight components were removed,
and two components were added. This result was as anticipated
since it was the expectation that only common components would
remain. However, there was an increase of 4 components when
incorporating the second application. This increase consisted
mostly of the addition of common cross-cutting components.
Also, other components were decomposed into multiple
components.

6. EVALUATION
To evaluate the effectiveness of the C-PLAD approach, metrics
were recorded for each iterative phase where a new application
was composed of low-level use cases. The ISIS team initially
identified four applications for the image-guided surgery
components. These applications are shown in Application Layer
of Figure 8. It is not in the scope of this paper to describe each
application in detail. However, it is important to understand that
all applications fit within the image-guided surgery product line.
The ISIS team agreed that the registration component represented
the component that was most understood by the software
engineers and the domain experts. In addition, this component
also represented the best candidate for development into a
component in the opinion of the team based on its perceived use
in multiple applications. The number of use cases representing
the registration component, the total number of components, and
the number of components removed/added at a particular iteration
were recorded. At the time of this paper, C-PLAD was only
evaluated using the first two applications. The recorded metrics
are listed in Table 1. It was anticipated that the number of use
cases for the registration component would decrease as the
additional applications were included. Also, a decrease in the
total number of components would occur as functionality was
grouped into components.

7.

Table 1. Metrics using C-PLAD for Image-Guided Surgery.
Design
Iteration
Problem
Statement and
Requirements
Liver
Biopsy
Guidewire
Placement

Number of
Use Cases for
Registration

This problem is exacerbated in component-based product line
development. The need to identify components early, produce
them at an appropriate level of generality, and leverage existing
components in the product line impose constraints that could be
addressed if component interactions and dependencies were better
understood earlier in the development process. RUP suggests that
component diagrams should begin as part of the deployment core
workflow by this time, but these diagrams are better suited for
deployment as they identify packaging and location constraints
between components. RUP also includes the nebulous notion of a
“service package”, naming a collection of service-oriented
features so dependencies may be created to these services in
analysis packages. However, little guidance is given as to how
one arrives at a service package. UML 2.0 introduces notation for
reusing sequence diagrams called “Interaction Occurrences”. This
new element allows a modeler to model primitive message
interactions for reuse by higher-order, complex interactions. In
other words, sequence diagrams now have modular reuse. The CPLAD approach pushes the modularity upstream into the use
cases itself, so explicit requirements exist for the modular
components.

Number of Components
All
Removed
Added

6

20

Initial

Initial

12

14

8

2

7

18

1

5

DISCUSSION

C-PLAD addresses a gap in the RUP Core Workflows [15]. RUP
advocates use-case driven requirements analysis, producing an
analysis model that is used as input to the architecture description
definition process. The use case model is inherently vertical; it
drills down through vertical slices of functionality supported by
the system as a whole. It is then the architect’s job to take this
model, identify commonalities, and begin sketching core objects
and components of the system using tools such as analysis classes
and component diagrams. In other words, the architect is solely
responsible for translation of a vertical model (the use case
model) to a potentially horizontal one (the architecture
description). This is natural given the evolution of RUP,
combining a use-case driven process with architecture-centric
notations.

In evaluating the C-PLAD approach, there were several
significant observations. The first observation was the variation
in the number of use cases (registration component) for each of
the C-PLAD iterations. The research team anticipated a decrease
in the number of use cases for the registration component during
the first iteration. The team predicted that the number of use
cases would increase as only the most common use cases would
remain. However, after incorporating the first application (i.e.
Liver Biopsy), the number of use cases increased from 6 to 12.
The increase in use cases was based on additional unanticipated
functionality that became obvious in the second step of the CPLAD approach where use cases for one component are used to
complete other components. This increase in use cases is a
logical result that shows the ability of the C-PLAD approach to
help identify unsuspected requirements. Furthermore, after
incorporating the second application (i.e. Guidewire Placement),
five use cases were determined to be too specific and removed
from the components functionality. In this case, the C-PLAD
approach assisted in maintaining the robustness of the registration
component.

C-PLAD offers an alternative. C-PLAD suggests
extending use cases to capture component interactions and
dependencies as part of the analysis model by having components
serve as first-class objects in the use case model. The step-wise
iterative process allows C-PLAD to be used as a tool in the gap
area between requirements analysis and architecture, while the
High Level Template (see Figure 3) supports traceability between
high-level domain concepts and application engineering concepts
that leverage components. C-PLAD has the additional benefit
that such models may be discussed with customers as they are
represented in a familiar notation (use cases). Finally, the CPLAD step can be inserted into the RUP Core Workflows
seamlessly; for example, the development of detailed component

A second observation was made based on the change in the
number of components during C-PLAD iterations. Through the
collaboration of the domain specialists and software engineers,

1476

Government. We also acknowledge the fruitful conversations
with Will Schroeder of Kitware Incorporated.

diagrams downstream should reconcile with the component-level
use cases identified during step-wise refinement.
Another major benefit of the C-PLAD approach is the processbased approach to discovering the most robust components for
development while identifying common components. Most of the
components added in incorporating both applications were
common nonfunctional components and through the
decomposition of components. In initial cases, it is common for
the software engineers to focus on the functional aspects of the
components, while the tendency is to neglect the crosscutting
aspects, such as logging, data management, exception-handling,
and user communication. The C-PLAD approach was effective
for discovering common components for such nonfunctional
concerns. The C-PLAD approach also assisted in determining
components that encapsulated too much functionality. The CPLAD approach results in detailed use case diagrams. Use cases
at the bottom-level of such diagrams can be effectively translated
into system functions as in related work [4] and into sequence
diagrams as in UML 2.0. In this paper, the main focus is on
conceiving the use cases, but, in future work, we plan to describe
the translation of the use cases into concrete representations.

10. REFERENCES
[1] Batory, D., Johnson, C., MacDonald, B. and von Heeder, D.
"Achieving Extensibility Through Product-Lines and
Domain-Specific Languages: A Case Study", ACM
Transactions on Software Engineering and Methodology,
April 2002
[2] Beck, K. “Extreme Programming Explained, Embrace
Change”, Addison-Wesley Professional, Boston, Ma, 2000
[3] Booch, G. Rumbaugh, J, and Jacobson, I. “The Unified
Modeling Language User Guide”, Addison Wesley, Reading,
MA 1999
[4] Bruegge, B. and Dutoit, A.H. Object-Oriented Software
Engineering Using UML, Patterns, and Java. 2nd Edition
Prentice Hall, Englewood Cliffs, NJ 2003
[5] Cleary, K., Clifford, M., Stoianovici, D., Freedman, M.T.,
Mun, S.K., Watson, V. “Technology improvements for
image-guided and minimally invasive spine procedures”
IEEE Transactions on Information Technology in
Biomedicine Vol. 6, No. 4, pp 249-261, 2002

The C-PLAD approach has a different perspective than current
approaches to refactoring. Using the C-PLAD approach, reuse is
built into components while refactoring promotes the change of
software designs with evolving requirements. The C-PLAD
approach is most effective for projects that are aware of at least a
subset of target applications. In future work, we plan to extend CPLAD to allow for the necessary refactoring with the introduction
of new unanticipated applications.

[6] Cleary, K., Ibanez, L., Ranjan, S.R., and Blake, M.B.
“IGSTK: A Software Toolkit for Image-Guided Surgery
Applications”, Proceedings of the 18th International
Conference on Computer-Assisted Radiology (CARS2004),
Chicago, IL, June 2004, Elsevier, 473-479.
[7] Cockburn, A. Agile Software Development. Addison-Wesley
2002

8. CONCLUSIONS
In this paper, a new software engineering process is introduced to
support component-based software engineering for a specific
class of applications. Using a use case-driven approach, a major
innovation in this work is a principled approach to identifying
requirements to support the development of general components
for a specific product line. The C-PLAD approach varies from
standard product line analysis because it addresses projects that
are aware of a significant subset of target applications. In these
cases, it is valuable if reuse can be incorporated in the design to
help prevent the need for large-scale refactoring later. Although
this work has been useful in the development of a componentbased framework for image-guided surgery, there is one major
area of future work. In the scope of the initial studies, only a few
components were developed and a subset of the identified
applications. In the future, the C-PLAD analysis needs to
incorporate other applications within the image-guided surgery
product line. In addition, we plan to build new applications
consisting mainly of the components designed using the C-PLAD
approach. The ability of the C-PLAD-generated components to
fulfill new applications will help evaluate the accuracy of the
approach while identifying areas for improvement.

[8] Heineman, G.T. and Councill, W.T. Editors, ComponentBased Software Engineering: Putting the Pieces Together,
Addison-Wesley, Boston, MA June 2001

9. ACKNOWLEDGMENTS

[14] Rumbaugh, J., Blaha, M., Premerlani, W., Eddy, F. ,and
Lorensen, W. Object-Oriented Modeling and Design.
Prentice Hall 1991

[9] Kruchten, P. (2000). The Rational Unified Process—An
Introduction, Second Edition, Addison-Wesley
[10] Lerchundi, R. (Ed.) (2003-05-18). System Family Process
Frameworks,http://www.esi.es/en/Projects/esaps/publicpdf/CWD212-21-02-01.pdf SEI CMM (2003-04-25).
Capability
Maturity
Model
for
Software,
http://www.sei.cmu.edu/cmm/
[11] Li, H., Krishnamurthi,, S., and Fisler, K. ”Verifying CrossCutting Features as Open Systems”, Proceedings of the
International Conference on Foundations of Software
Engineering, Charleston, S.C. 2002
[12] M. Eriksson (2003): An Introduction to Software Product
Line Development, Proceedings of Umeå's Seventh Student
Conference in Computing Science, UMINF 03.05, ISSN0348-0542, pp. 26-37.
[13] Rosenberg, D. (2001) Use Case Driven Modeling with UML:
A Practical Approach, Addison-Wesley

This research is supported by the National Institute of Biomedical
Imaging and Bioengineering (NIBIB) at the National Institute of
Health (NIH) under grant R41 EB000374-01A1 and by U.S.
Army grant DAMD17-99-1-9022. The content of this manuscript
does not necessarily reflect the position or policy of the U.S.

[15] SEI PLP (2003). Framework for Product Line Practice,
http://www.sei.cmu.edu/plp/

1477

Software Enterprise Pedagogy for Project-Based Courses
Kevin Gary, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, srividya.bansal, arbi.ghazarian} @ asu.edu

Abstract
The Software Enterprise is a pedagogical model combining traditional lecture with
project-based learning. The Enterprise model leads students through a modular series
of lessons that combine foundational concepts with skills-based competencies. In this
tutorial, software engineering educators in higher education or industry will learn the
methodology, get exposed to assessment techniques such as e-portfolios and concept
maps, and become familiarized with the open resources available to educators that
adopt the pedagogy. This tutorial should be of interest to any educator interested in
making their project-based courses more engaging and more relevant to students
needing to be ready to practice the profession from the first day they exit the Enterprise
environment.

1. Overview of the Software Enterprise
The Software Enterprise has been in use at Arizona State University since 2004, and
now forms the "project spine" that runs the full four years of the new B.S. in software
engineering degree program at ASU and the two-year M.S. in Computing Studies
graduate program. The primary goal of the Enterprise is to move students rapidly from
foundational concepts to industry best practices, so students completing the degree
program are prepared from day one to meaningfully contribute to the profession. The
Enterprise pedagogy takes students from introduction of a concept to scalable practice
in a real project in the span of a 3-week "sprint." This is in contrast to typical degree
programs that introduce a concept with toy problems in one course and then expect
students to synthesize the multiple concepts in a capstone project in a different course
(perhaps years later).
In the Enterprise model, students are first exposed to concepts via traditional
dissemination activities; lectures, readings, and discussion. They then put the skill into
practice using an industry-accepted tool in a mentored lab exercise. The practice is then
adapted and incorporated into the current iteration of the lifecycle process model on a
scalable project. Students complete the learning cycle by reflecting on the lessons
learned while applying the technique on their project. The key is these activities take
place closely in time as opposed to over a semester (or more).
As an example, consider the topic configuration management (CM). CM concepts
such as quality thresholds for codelines, good and bad branching patterns, and change
management are presented and discussed in class meetings. At the end of the same
week, CM is grounded via a half-day problem-centered lab in a CM tool. The following
week, CM is incorporated into the project. At the end of the three-week project
iteration, teams are required to write in their team journal (a Wiki), a rationale for how

368
c 2013 IEEE
978-1-4673-5140-9/13/$31.00 

CSEE&T 2013, San Francisco, CA, USA

they applied the technique into the project and an evaluation of how well the approach
worked. Finally, individual students are also required to log an individual journal entry
on her/his thoughts on the utility of the technique at the end of the iteration
The Software Enterprise is a flexible model for delivery, and as such allows a wide
variety of techniques to be incorporated into the approach. The facilitators have
experimented with peer mentoring, entrepreneurial projects, outsourcing, and other
variations. The study of what teaching and learning innovations may be incorporated
into a project-centric framework such as the Software Enterprise is a significant area of
future research for the team. Further, existing and freely available course materials
(from the Enterprise website, textbooks, or 3 rd party sites such as SWENET) may easily
be utilized in the framework. The primary desired outcomes for attendees of this
tutorial are to 1) understand how the instructor-as-facilitator can re-orient their existing
approaches to one that is project-centered and contextualized, and 2) communicate
lessons learned from a near-decade of experience, 3) suggest alternative forms of
assessment for project-centered learning, and 4) promote a project-centered form of
curricular design for the future.

2. Presenters
Kevin Gary, Associate Professor, Department of Engineering, Arizona State University
kgary@asu.edu, http://dcs.asu.edu/faculty/KevinGary
Dr. Gary designed and created the Software Enterprise in 2004. He has received over $200K
in support funding and mentored over 40 student team projects for industry. Prior to joining
ASU, Dr. Gary spent several years in industry as an enterprise systems architect. His
motivation is to promote the education of software engineering through rigorous pedagogy
and close attention to current best practices.
Srividya Bansal, Assistant Professor, Department of Engineering, Arizona State University
Srividya.bansal@asu.edu, http://dcs.asu.edu/faculty/skbansal
Dr. Bansal teaches the sophomore year of the Software Enterprise, and mentors senior
Enterprise projects. Dr. Bansal's research interests are in the area of semantic web services.
Before joining the ASU faculty in 2010, Dr. Bansal was a Visiting Professor at Georgetown
University and earned her Ph.D. in Computer Science from the University of Texas-Dallas.
Arbi Ghazarian, Assistant Professor, Arizona State University
Arbi.ghazarian@asu.edu, http://technology.asu.edu/directory/1627371
Dr. Ghazarian teaches the senior year Enterprise capstone project sequence, leading and
mentoring several industry projects. His research interests include software requirements and
software design. Prior to joining the ASU faculty in 2009, Dr. Ghazarian held a postdoctoral
appointment at the University of Toronto, and spent several years doing industry consulting.

3. Intended Audience and Pre/Post-Conference Expectations
This tutorial is primarily intended for faculty at 2- or 4-year colleges and universities,
which offer software engineering degrees, minors, certificates, or courses. Industry
professionals involved in corporate training will also benefit from the presentation as the
modular approach of the Software Enterprise translates to this environment. However, the

369

project aspects of the tutorial will not be as beneficial for these attendees, though professional
experience is a viable substitute for project-based learning as a contextualizing experience.
Prior to the conference attendees will be asked to review materials on the Software
Enterprise community site, perform a simple assessment exercise, and take a short online
survey. Total anticipated preparation time pre-conference is 30-60 minutes. After the
conference attendees will be asked to provide feedback via an online quantitative and
qualitative survey. Educators interested in pursuing deeper collaboration will be provided
instruction on how to engage the larger Software Enterprise community in a variety of
potential roles.

4. Agenda
The requested amount of time for the tutorial is 120 minutes. The suggested agenda is as
follows:
1. Introductions and objectives of participants (10 minutes)
2. Overview of the Software Enterprise model and website (10 minutes)
3. The sequence of a software engineering module in the Software Enterprise
(interactive, 25 minutes)
4. Assessment in the Software Enterprise (interactive, 20 minutes)
5. Incorporating pedagogical innovations in the Enterprise framework (20 minutes)
6. Pitfalls and lessons learned in project-centered teaching (10 minutes)
7. Resources available to the Software Enterprise community (20 minutes)
8. Future Directions and Wrap-up (5 minutes)

5. About this Tutorial
A recently concluded NSF CCLI Phase I grant, an IBM Jazz Innovations Award, a
Kaufmann Pathways to Entrepreneurship Grant (PEG), and an Arizona Board of
Regents Learner-Centered Education (LCE) award have supported this work. This
tutorial was previously presented at the Software Engineering and Applications (SEA)
conference in November 2012, and feedback from that presentation has been used to
focus this proposal on the most beneficial areas for attendees. Specifically, an insession assessment piece has been refactored into the pre-conference activity, as it was
too time consuming during the session. Time was added at the end to allow for
discussion and input from the attendees on project-centered software engineering
education. Finally, an activity was added (#5) to discuss how variations on Enterprise
delivery have been (or we anticipate will be) integrated in the delivery model. These
variations include cross-year mentoring, service learning, team formation,
entrepreneurial projects, software process models, and several more.

370

SOFTWARE – PRACTICE AND EXPERIENCE
Softw. Pract. Exper. 2011; 41:945–962
Published online 25 April 2011 in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/spe.1075

Agile methods for open source safety-critical software
Kevin Gary1, ∗, † , Andinet Enquobahrie2 , Luis Ibanez2 , Patrick Cheng3 , Ziv Yaniv3 ,
Kevin Cleary4 , Shylaja Kokoori1 , Benjamin Muffih1 and John Heidenreich1
1 Department

of Engineering, Arizona State University, Mesa, AZ 85212, U.S.A.
2 Kitware Inc., Clifton Park, NY 12065, U.S.A.
3 Imaging Science and Information Systems (ISIS) Center, Department of Radiology,
Georgetown University Medical Center, Washington, DC 20007, U.S.A.
4 The Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Medical Center,
Washington, DC 20010, U.S.A.

SUMMARY
The introduction of software technology in a life-dependent environment requires the development team to
execute a process that ensures a high level of software reliability and correctness. Despite their popularity,
agile methods are generally assumed to be inappropriate as a process family in these environments due
to their lack of emphasis on documentation, traceability, and other formal techniques. Agile methods,
notably Scrum, favor empirical process control, or small constant adjustments in a tight feedback loop.
This paper challenges the assumption that agile methods are inappropriate for safety-critical software
development. Agile methods are flexible enough to encourage the right amount of ceremony; therefore if
safety-critical systems require greater emphasis on activities, such as formal specification and requirements
management, then an agile process will include these as necessary activities. Furthermore, agile methods
focus more on continuous process management and code-level quality than classic software engineering
process models. We present our experiences on the image-guided surgical toolkit (IGSTK) project as a
backdrop. IGSTK is an open source software project employing agile practices since 2004. We started
with the assumption that a lighter process is better, focused on evolving code, and only adding process
elements as the need arose. IGSTK has been adopted by teaching hospitals and research labs, and used
for clinical trials. Agile methods have matured since the academic community suggested almost a decade
ago that they were not suitable for safety-critical systems; we present our experiences as a case study for
renewing the discussion. Copyright q 2011 John Wiley & Sons, Ltd.
Received 10 September 2009; Revised 15 December 2010; Accepted 31 January 2011
KEY WORDS:

agile methods; software process; open source; safety

1. INTRODUCTION
It seems to be a universally accepted maxim that agile development methods are not suitable
for safety-critical domains. Agile methods, as the argument goes, do not encourage formal,
document-centric activities needed to satisfy robust process requirements, such as documented
design, requirements management, and other forms of traceability. Although the Agile Manifesto
[1] was introduced almost a decade ago, only in the past couple of years has the maturity of agile
methods become apparent through industry adoption, availability of reference materials, certification processes for individuals, increased scholarly activity, and convergence on a focused set
of agile process models. This recent evidence suggests that now is a good time to step back and
reconsider the broader implications of agile methods for safety-critical software.
∗ Correspondence
†

to: Kevin Gary, Department of Engineering, Arizona State University, Mesa, AZ 85212, U.S.A.
E-mail: kgary@asu.edu

Copyright q

2011 John Wiley & Sons, Ltd.

946

K. GARY ET AL.

Open source is an increasingly popular development and distribution model for software. Open
source teams often employ agile methods, as the focus is on concurrent development and fast
production (sprints) over gated production (milestones). However, unlike some of the core principles
of agile methods, many open source projects rely on dedicated and highly skilled volunteers
(or part-time supporters) in distributed development teams with no singularly available customer.
The image-guided surgical toolkit (IGSTK) is an open source project that relies on the collaboration of a skilled distributed development team to construct a cross-platform application framework
in a safety-critical domain. IGSTK provides features for image-guided surgery (IGS), including
DICOM image import, image display, registration, and segmentation. It provides advanced functionality such as device tracking support and scene graph manipulation, and non-functional features
such as portability across a variety of operating systems. The goal of the project is to facilitate developing research and commercial applications faster by providing a reusable application
framework. Researchers and entrepreneurs developing surgical applications can quickly build functionality by using core IGSTK components or modifying them to fit their needs. Framework usage
reduces the need to be concerned about the intricate details in developing a safety-critical surgical
application [2]. IGSTK is a rare intersection of agile and open source processes in a safety-critical
domain.
Extreme care is needed in the design and development of safety-critical applications, because
the occurrence of an error could result in loss of life [3]. The software engineering research
community suggested shortly after agile methods first started becoming popular that they were not
suited for building safety-critical systems (particularly Boehm [4], which we discuss later in this
paper). The reasons range from a perceived lack of documentation, formal specification, to detailed
planning. Agile methods have matured past the hype phase into a maturely defined, understood,
and executed family of process models. We argue that agile methods can contribute to safetycritical software development, particularly in the areas of process management and implementation
quality. Specifically, Scrum process management, eXtreme Programming (XP), and open source
development principles can enhance traditional safety activities. The IGSTK team has enhanced the
process with a tailored set of best practices augmenting common agile and open source methods
for the express purpose of delivering safety-critical software. In this paper we make an argument
for using these methods based on our experience with IGSTK since 2004.

2. BACKGROUND
Software safety is a necessary quality attribute in certain classes of systems because of the impact
it has on life or property. Software safety deals with minimizing threats or risks to the system and
mitigating loss in the event of failures or adverse events. Leveson [5] defines risk as ‘a function of
the probability of a hazardous state occurring in a system, the probability of the hazardous state
leading to a mishap, and the perceived severity of the worst potential mishap that could result from
the hazard’. Consequently, while engineering for safety in safety-critical systems, high levels of
assurance are needed (not solely based on testing) that will determine whether a system can and
should be used [5, 6]. Leveson also argues that safety should not only prevent malicious actions
from happening in general, but should also be concerned with inadvertent actions happening.
Are agile methods appropriate for safety-critical systems? In [4] Boehm performs a comparative
study of agile methods vs plan-driven or traditional methods in developing software development
methods and asserts that it is important to know which method is applicable to what type of project.
Boehm suggests that life critical systems need stable requirements and that an agile approach
might not be suitable for such applications. Further, Boehm suggests that more thorough planning
will reduce risk in developing such systems.
We question the underlying assumptions of Boehm’s argument. First, there is an implication
that agile methods do insufficient planning and that more thorough planning eliminates risk. We
disagree; agile methods, particularly Scrum, do plan while focusing on empirical process control
[7] as a constant oversight and management mechanism. Scrum plans by managing the product
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

AGILE METHODS

947

backlog, which may include activities related to risk reduction, HA (hazard analysis), FTA (faulttree analysis), FMEA (failure mode effects analysis), and formal specification (where warranted).
Further, Scrum advocates pervasive process management through sprints, daily standups, and
visibility (report dashboards and related tools). Second, Scrum recognizes that knowledge is incomplete, change happens, and provides a framework for dealing with it, instead of ad hoc workarounds
outside the boundaries of a plan-oriented process model. Finally, Boehm’s discussion does not
consider the impact of the expertise on the team. The omission implies that team members are
interchangeable parts, whereas Cockburn [8] argues for their inclusion in the process. Formal
specifications, models, tools, and processes reduce risk in the application domain, but software,
perhaps more than any engineered discipline, is the output of a human-oriented process. The
expertise of the team, the communication patterns it utilizes, and the human capacity to reason
with incomplete knowledge are important factors in determining that output (the software). The
IGSTK team consists of experts specialized in the image-guided surgical domain and from other
fields, such as software engineering and robotics. The best practices presented later in this paper
reinforce the role of the experts in this process.
In an eWorkshop report [9], 18 agile experts from around the world evaluate Boehm’s statement
regarding life critical systems and agile methods. They contend that when performance requirements
and plans about level of testing for the project are made at an early stage in development, agile
methods are ideal for safety-critical systems because in an agile approach the customer is available
throughout the development process to obtain and clarify requirements. In [10] Gelowitz et al.
acknowledge this claim after performing a step-by-step comparison of XP process against a
traditional waterfall method considering the concept, requirements, design, implementation, test,
and maintenance phases theoretically. They say XP performs all the phases better, except it
does not create elaborate design document during the design phase. However, they also say
that not creating elaborate design documents provides flexibility with respect to accommodating
changes.
Several industry experience reports suggest that adopting an agile approach has improved the
efficiency and reliability of their project. Spence [11] discusses how an organization had to adopt
an agile methodology due to the shortcomings of traditional plan-based methods in handling
frequently changing requirements. The author claims that the team reviewed agile methodology in
detail and is confident that it can be used to develop safety-critical software. Similar to IGSTK, their
approach in becoming agile was incremental, trying and adapting agile principles on a constant
basis. Van Schooenderwoert [12] says their team benefited by adapting XP in an embedded project.
The author claims that XP is ideal when requirements are not concrete at the beginning of the
project, asserting that agile principles work best for volatile or ambiguous requirements. In [13] van
Schooenderwoert explains the importance of agile testing in embedded projects. The project team
performed unit testing on the software and used mock object simulation to test the hardware on the
embedded project. The author asserts that the project displayed very few bugs at any given point
of time because of this testing approach. Another industry report by Manhart et al. [14] describes
the development of embedded software for Daimler Chrysler including safety-oriented functions
like Automatic Breaking System (ABS). The authors point out how the team, fundamentally
oriented towards plan-driven development in order to mitigate risks, had to adopt agile practices
to manage changes to their requirements efficiently at a later stage of development. Grenning [15]
and Bowers et al. [16] describe how they have successfully used agile principles on their large
mission critical projects. They claim that adopting agile principles, such as pair programming,
iterative development, refactoring, and automated testing, has helped to improve the quality of the
code and productivity of the team.
Agile methods and open source are suitable for safety-critical systems because these methods
are synergistic with safety principles, not orthogonal to them. Agile methods bring strong practices
in the area of process management and software construction, while having a philosophy that
allows for traditional safety-oriented practices to the extent they are warranted. Understood in the
proper way, agile methods reinforce Boehm’s argument in [4] that thorough project management
is required for safety-critical software development. Agile methods can minimize risk by reducing
the probability of loss through their best practices. Yet they allow for traditional analysis methods
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

948

K. GARY ET AL.

that help determine the magnitude of the loss, thereby allowing the team to determine the risk
exposure.
The following section presents an overview of IGSTK as a point of reference in our position,
and describes some of the unique challenges it presents.

3. AN OVERVIEW OF IGSTK
IGS presents interesting design challenges for software and system implementers. A typical IGS
environment is shown below.
Figure 1 depicts several challenges. Physical devices, such as robotic arms and tracking devices,
need to be integrated in a controlled environment. The software must be usable by specialized
users, such as surgeons and surgical assistants. The software system must have fail–safe mechanisms, provide correctness and soft real-time performance, and ensure safety. IGSTK must exhibit
these traits, and also address additional architectural qualities, including portability (hardware and
operating systems), reusability, maintainability, and openness.
IGSTK is an open source framework for creating surgical applications. IGSTK is distributed
under a BSD-like license that allows for dual-use between academic research labs and commercial
entities. IGS involves the use of preoperative medical images to provide image overlay and instrument guidance during procedures. The toolkit contains the basic software components to construct
an image-guided system, including a tracker and a four-quadrant view incorporating image overlay,
as shown in Figure 2.
The remainder of this section describes the challenges that IGSTK presents from software
process and architecture perspectives.
3.1. Software process challenges
IGSTK development presents interesting challenges from a process perspective. The first challenge
derives from the nature of the framework-level requirements, which is difficult to completely
understand before applications are constructed upon it. Waterfall-style methodologies [17] that
attempt to define requirements completely before development begins are not considered suitable,
as the range of possible behaviors to be supported on the framework is necessarily incomplete.
Similarly, use-case analysis modeling [18] is selectively applied, as one cannot assume requirements
derived from a set of applications today represent a complete set of requirements for the future.
Given the complexities of the requirements process and application domain, we discuss IGSTK’s
approach in Section 4.
The second challenge is the makeup of the team, comprising academic and commercial collaborators in a distributed setting. All of the team members have other demands on their time.
These factors create challenges for setting project deliverables and expectations over medium- and

Figure 1. Components of an image-guided surgical environment.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

949

AGILE METHODS

Figure 2. 4-up GUI for a robotic needle driver application.

Figure 3. IGSTK software dependencies.

long-term horizons. Fortunately, most developers are deeply familiar with the domain and have
significant exposure to agile methods. Their expertise combined with the shorter horizons (Scrum
sprints) for delivering software mitigates this issue.
Another challenge, the high quality standards of the application domain, suggests that both agile
and traditional practices should be used. Safety-critical software should undergo code review [5, 6],
which IGSTK does at the end of each sprint. The agile practice of comprehensive unit testing
and full code coverage ensures that each component is extensively tested [19]. Additionally, the
open source community exercises the released code at an early stage, finding defects and usability
issues to evolve a stable codebase. Finally, as discussed in section 2, there are no constraints in
the agile process against performing various safety analysis techniques. However, the IGSTK has
not performed these safety techniques as they are more appropriately applied to specific surgical
applications, not at the framework level.
3.2. Architecture challenges
Figure 3 shows IGSTK’s dependencies and its role in support of an IGS application. The top layer
corresponds to IGS applications built on top of IGSTK. IGSTK also interfaces with third-party tools
like ITK and VTK to provide image-related functionality. FLTK performs user interface-related
tasks; other tools like Qt could be used for this purpose as well. The bottom layer, the operating
system, forms the base of this entire architecture. As shown in Figure 3, the IGS applications
interact with the lower layers only using IGSTK APIs.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

950

K. GARY ET AL.

A significant challenge is providing a safe platform when there are many third-party dependencies. The IGSTK layer is a safety region, wrapping underlying library functionality, and decorating
them with safety attributes. The state machine is a significant abstraction in this approach, and is
described in detail in Section 4.3.
The distributed nature of the core development team is not only a process challenge but also
an architecture challenge. The lack of good communication practices and strong open source
evolutionary principles could lead to a fragmented architecture. The IGSTK strong communication
patterns, coupled with a strong central design pattern (the state machine), resulted in a stable and
consistent architecture.
Another challenge is maintaining proper safe configurations of the software when components
may be assembled based on varying application requirements. IGSTK does this in two main ways.
First, run-time configurability of the software is kept at a minimum. The only configuration possible
is scoped internally to tracking components. Component connectors are determined at compiletime and verified through strong typing mechanisms enforced by the toolkit; no configuration files
exist to wire components at run-time. Second, each compiled configuration is verified through a
continuous integration and build process.

4. AGILE METHODS AND IGSTK
Lutz [20] proposes six key areas to consider when engineering for safety. These include:
1. Hazard analysis: identification and analysis of hazards in terms of their severity of effects
and likelihood of occurrence.
2. Safety requirements specification/analysis: specify requirements in formal notation, allowing
formal analysis to investigate whether certain properties are perceived.
3. Designing for safety: focus on consequences to avoid in the general system context.
4. Testing: tests should demonstrate that the software responds appropriately to some anticipated
environment.
5. Certification and standards: this involves assessing it against certain criteria.
6. Resources: utilizing resources and books for good software safety engineering [21].
IGSTK focuses on 2, 3, 4, and 5 from an agile perspective. Classic safety analysis techniques
(#1), such as HA, FTA, and FMEA, are not the issue in our research. These techniques have been
shown to be useful for software safety, and there is nothing about an agile process that suggests a
project should omit or de-emphasize them. Resources (#6) are not in conflict with agile methods;
this best practice is software life cycle model agnostic. To reiterate, the overriding principle is the
‘right amount of ceremony’, therefore these techniques should be applied to the extent they are
required, no more no less.
Formal methods (#2) enhance the safety and reliability of safety-critical systems [22, 23] by
providing models and model transformations that can be analyzed for completeness and correctness.
Models are constructed in a language with well-defined semantics; then, a correctness-preserving
transformation turns the model into an executing program, supported by an environment that guarantees certain runtime properties. If the model, through formal analysis and/or simulation, is accepted
as correct, then the executing code is accepted as correct. However, the IGSTK team has adopted
an agile approach in developing the framework, synergistic with the practices of open source tool
development but orthogonal to traditional approaches [2]. Agile software development practices
emphasize working software as the most important measure for progress. This means developers
usually implement the software ‘by hand’, with no formal design specifications or modeling
tools, or the use of automatic code generators. Thus, developers have to constantly communicate,
re-design, re-code, and re-test software throughout the entire life cycle of the project. Agilists
argue that this approach results in better low-level code quality and adherence to user/customer
intent, and that formal modeling may result in constrained requirements that do not meet customer
needs.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

AGILE METHODS

951

The IGSTK team believes that formal models are useful, but does not want to change from an
agile process model to leverage their benefits. The model-driven community espouses forward,
waterfall-like flow to guide the development safe software. Agilists prefer to work in short fluid
iterations (‘sprints’). IGSTK attempts to leverage the best of both worlds by using an agile process
while incorporating formal checks integrated into that process via a validation toolset as described
in Section 4.5.
IGSTK is designed for safety (#3). From the very beginnings of the project the team adopted an
architectural style based on layered components governed by state machines. This safety-by-design
principle is pervasive to IGSTK, and is presented in Section 4.3.
IGSTK employs extensive testing (#4), particularly at the component level. A continuous testing
and integration dashboard reports on automated unit tests run around the globe on a variety of
platforms. Application-specific testing, other than the sample applications included in the source
tree, does not apply, as IGSTK is a framework, and not a specific application. This agile testing
methodology, presented in detail in Section 4.4, adheres to the recommendation of Leveson and
Turner [19] to test extensively at the component and system levels.
Software engineered for safety often must meet liability laws and standards set forth by government agencies or licensing bureaus (#5). Unfortunately, the current models do not guarantee that
builders of safety-critical systems meet the requirements of the regulatory institutions [5]. Software safety should ensure that the system executes properly without unacceptable risk. The risk
that is considered acceptable usually involves other factors (economical, political, and moral)
defined outside the realm of software engineering. IGSTK, as an open source framework for
IGS, must deal with FDA approval, IRB review, and open source licensing issues. IRB review
for surgical applications within a hospital setting is based on the surgical procedure, and not
on a specific technology. IGSTK has elicited requirements from surgeons for example surgical
applications, constructed activity models for these procedures, and received IRB approval for
clinical trials at a major research university hospital [24]. FDA approval is challenging as software is treated as a medical device [19, 25], therefore design and traceability are important. In
response, IGSTK has adopted a lightweight but integrated requirements management process
described in Section 4.2, and the safety-by-design approach presented in Section 4.3. Finally,
as an open source framework, IGSTK must deal with unique licensing issues, both in terms of
what it relies upon and who uses it. IGSTK relies on other open source platforms (see Section 3)
and must be disseminated as open source. This restricts which third-party software may be integrated, which is sometimes an issue for certain algorithms or commercially available tracker
devices.
In the remainder of this section we describe how agile methods have been applied, and in some
cases augmented, to meet IGSTK’s safety-critical needs.
4.1. Best practices
Early on the team recognized the need to establish a collaborative set of principles, or agile culture,
on the project. These were expressed as a set of best practices [2] that take precedence over
dogmatic adherence to scripted processes. An abridged version is given below.
Best Practice #1. Recognize that people are the most important mechanism available for ensuring
high quality software [8]. The IGSTK team comprises developers with a high degree of training
and experience with the application domain, supporting software, and tools. Their collective
judgment is weighted over any high-level process mandate.
Best Practice #2. Promote constant communication. This is difficult in open source projects with
distributed and part-time teams. IGSTK members constantly communicate through weekly teleconferences, biyearly meetings, mailing lists, and an active Wiki.
Best Practice #3. Produce iterative releases. IGSTK’s external releases coincide with IGSTK
yearly user group meetings. Internally, releases are broken down into approximately two month
‘sprints’. At the end of a sprint, the team can stop, assess and review progress, and determine
what code is considered stable enough to move to the main code repository.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

952

K. GARY ET AL.

Best Practice #4. Manage source code carefully. Require 100% mainline code coverage. Use sandboxes for evolving code with different check-in policies to allow developers to share experimental
code without sacrificing quality policies in the mainline.
Best Practice #5. Augment the validation process with reverse engineering tools that support
complex white-box testing. This best practice is a nod to the specialty of the domain.
Best Practice #6. Emphasize continuous builds and testing. IGSTK uses the open source CDash
tool to produce a nightly dashboard of builds and unit tests across all platforms. Developers are
required to ensure that code coverage stays as close as possible to 100%, that their source code
builds on all supported platforms, and that all unit tests pass.
Best Practice #7. Support the process with open tools. IGSTK uses CDash, a test dashboard,
CMake, a cross-platform build solution, and Doxygen, a documentation system.
Best Practice #8. Emphasize requirements management in lockstep with code management. As
requirements evolve and code matures, it is necessary to adopt flexible processes for managing
requirements and source code. The organization and tracking of requirements is a complex
process for a project such as IGSTK, and thus is detailed in the following section.
Best Practice #9. Focus on meeting exactly the current set of requirements. This is one of the
most important benefits of an agile approach. The team focuses on only the current backlog,
and not burdening itself on designs not realized in the current code.
Best Practice #10. Allow the process to evolve. Through constant communication, IGSTK
members recognize when the complexities they face can be addressed within the current
process, when ‘tweaks’ are required, or when new practices should be adopted.
These best practices are not new to agile and open source practitioners. However, in a safetycritical domain, following these practices alone is insufficient. In the spirit of doing the right
amount of ceremony, the IGSTK team augments these practices. To illustrate, we describe how
the IGSTK team performs lightweight requirements management (#8), safety-by-design (#9),
continuous integration and testing (#6), and architecture validation (#5).
4.2. Requirements management
Requirements management is described in best practice #8, traceability of requirements at development time. Developers introduce new requirements into the product backlog. The team then
selects the subset of the product backlog suitable for implementation in the current sprint. The
team employs a collaborative process for reviewing, implementing, validating, and archiving these
requirements, and it is integrated with application development.
This process is illustrated as a UML state diagram in Figure 4. When a developer identifies a
new potential requirement (Initialized), s/he will post a description (Defined) on the Wiki. At the
same time, the initial code that fulfills the requirement is entered into the sandbox. The requirement
undergoes an iterative review process where team members review, discuss, and potentially modify
the requirement. Based on the team’s decision, requirements can be Rejected/Aborted or Accepted.
Rejected requirements are archived on the Wiki (Logged) so that they can be reopened later if
necessary. Accepted requirements are included in the product backlog. Once the supporting software
is implemented and its functionality confirmed, the requirement is marked as Verified. As nightly
builds take place, all Verified requirements are automatically extracted into Latex and PDF files
and archived.
This synchronization between requirements management and code management gives a more
evolutionary process feel to backlog management.
4.3. Safety-by-design
IGSTK’s layered component-based architecture is shown in Figure 5. Each component has a
strongly typed interface that accepts request events and returns event responses. Events are translated into inputs to an internal state machine that determines whether the request can be satisfied
in the component’s current state.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

953

AGILE METHODS

Figure 4. Requirements Management process in IGSTK.

Figure 5. IGSTK components and connectors.

State machines are the principal mechanism for the safety-by-design approach. State machines
govern each component instance, restricting the allowable behaviors that a component exhibits at
a given point in time based on the state of the component. Strong encapsulation of state machines
inside components means that they cannot be manipulated by the outside world other than through
interaction with the specified interfaces. State machines have fully populated transition tables to
ensure that any possible input has a defined response. State machines provide a reliable medium
for high levels of assurance needed that will determine whether a system can and should be used in
a safety-critical environment [26]. Safety-critical component-based systems like those used in the
aeronautical, medical, and defense industries are often engineered using state machines in order
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

954

K. GARY ET AL.

Figure 6. State machine for the Spatial Object component type.

to meet the safety regulations and standards set forth by liability laws and government agencies
or licensing bureaus [5]. An example state machine is shown in Figure 6.
The inputs define valid transitions between states. Inputs are generated by transduction from
requests (events) to the component. The state machine determines if a component is in a state
where it can process that request (input). State machines were an early architectural decision that
has persisted over the lifetime of IGSTK, resulting in a very stable architecture. The absence of
architectural shift and rework is evidence of the platform’s safety.
The safety-by-design approach in IGSTK supports Leveson’s notion of intrinsic safety [5],
wherein no component can be in an unexpected state. The U.S. Food and Drug Administration
(FDA) as a necessary attribute for certified medical devices (software is classified as a medical
device) require design safety [25]. IGSTK documents the critical elements of component design,
including its state machine catalog, on the Wiki, and conducts manual inspections and automated
validation of these design implementations.
4.4. Continuous integration and testing
Testing is essential to ensure software quality. It involves generating test cases, executing the
application against the test cases, and comparing the results against expected results. Testing helps
ensure that the software meets the functional and non-functional requirements specified for the
project. Automating the test process is beneficial as it can generate and run a large number of test
scripts and provide results in a faster and reliable manner.
IGSTK relies heavily on the agile practice of continuous and extensive unit testing with automated tool support. IGSTK requires 100% code coverage from its unit tests. Automation is achieved
through the CTest tool, which posts results to a CDash dashboard (Figure 7). Dashboards are
powerful tools for agile methods as they provide transparency into the quality of the software.
IGSTK automates tests from computers at sites around the world.
An important contributing aspect of open source methods to the agile perspective is the leveraging
of the community to evolve the software to a stable point. An open source community creates
a ready population of early adopters that identify defects near to the time they are injected, and
exercises a framework like IGSTK in ways that the development team often cannot anticipate.
Figure 8 shows a bar ‘stack’ chart tracking the identification and resolution of the major and minor
defects in IGSTK since 2006 on a quarterly basis.
Comparing the left and right bars in each pairing, particularly the lower part of each stack, shows
that the vast majority of defects are resolved within the same quarter. The community-driven open
source process helps to identify the defects, and the highly iterative agile sprints ensure defect
resolution before the next release. The chart also shows the impact of dedicated stabilization sprints
(the two spikes). The reduction in the total and major defects from quarter 11 on coincides with
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

955

AGILE METHODS

Figure 7. CDash dashboard displaying IGSTK nightly build and test results.

Figure 8. Quarterly defect tracker activity for IGSTK since 2006.

the release of IGSTK 2.0, supporting the hypothesis that early participation by the community
leads to later stabilization of the software.
4.5. Agile architecture validation in IGSTK
Advances in model-checking tools should be leveraged in IGSTK, but there are no formal models
created in the design process. To address this problem, the team constructed reverse engineering
tools that extract models from the code, validates them through a simulation-based testing methodology, and posts the results to the dashboard. In this way the design is validated with the code as a
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

956

K. GARY ET AL.

part of the agile continuous testing process. The validation toolset is described elsewhere [24, 27],
here we discuss the key features with respect to an agile process.
The center of IGSTK’s validation tool suite is a simulator that accepts descriptions of state
machines in SCXML [28] reverse engineered from the source code, and event sequences defined by
a variety of sources, and ‘runs’ the events as a simulation. Observers of a simulation may provide
a number of useful functions, such as constraint checking, execution visualization (animation),
playback and record, and so on. The key is in determining an appropriate set of events to give to
the simulator. The validation suite currently supports two kinds of event generators, coverage and
record-and-playback. Record-and-playback allows developers to trace an execution of an IGSTK
application and capture state machine inputs from a generated logfile. The inputs can then be
replayed through a visual animation application or through automated testing components.
Coverage measures the percentage of requirements satisfied when a test criterion is applied
[23, 29, 30]. Traditional code coverage mechanisms include statement coverage, branch coverage,
and path coverage. Translated into state machines, the types are state, transition, and path coverage.
State coverage involves checking that each state in the state machine of an IGSTK component
is reachable from the initial state [31]. Transition coverage verifies that each pairwise transition
between states occurs correctly as a result of processing a set of inputs. Path coverage is more
involved, as (i) there exist expected path sequences for (functionally) correct component executions,
and other sequences for unexpected and potentially catastrophic sequences and (ii) the state machine
is an explicit representation of object state, which is conceptually long-lasting and repetitive—how
many iterations over a path should a test conduct? Our coverage algorithms align with Watson and
McCabe’s work on cyclomatic complexity and basis paths [32]. Specifically, since exhaustive path
coverage of state machines is not feasible, we employ structural- and domain-related heuristics
(cf. [24]) to eliminate superfluous paths, reducing the total number of paths generated. This helps
in prioritizing the paths of interest, providing a sophisticated testing mechanism for IGSTK.
We present a brief example to give a better idea about these heuristics. Consider the state
machine for a spatial object shown in Figure 6. The dashed arcs in the figure represent a minimal
set of inputs to process to achieve state coverage, yet this sequence has no reflection on the object’s
expected use. A structural (domain-independent) path heuristic might suggest that the next node in
a recurring choice state (such as TrackedState) be the least recently visited. Thus if in the previous
iteration of the path the TrackingDisabledInput was processed, then this time the TrackingLostInput
is processed. Note that this decision is based on the structure and history of the path, and not on
any information about what the state machine itself represents. This is different from a domain
heuristic, which is implemented based on expectations of component usage. For example, a domain
heuristic might cause the TrackingLostInput to be processed 3% of the time based on empirical
evidence that the specific tracking device used in a clinical environment has a data loss rate of 3
frames per every 100. This may then be combined with a projected number of times around the
loops present in this state machine based on the estimated lifetime of the spatial object during the
given clinical procedure.
The need for continuous architecture validation motivated integration with the CDash dashboard.
This integration requires formalized test statements, capturing results of test executions in the
simulator, and posting results to the dashboard. Formalized test statements are constructed using a
freely available business rules engine, Drools‡ . Test statements specified constraints on the global
state of the system, expressed as the union of states at a given time in each component state
machine throughout the system. In this way the global state of the system can be validated. Rules
are evaluated after the simulator processes each state machine input from a test input stream
(generated from a replay or coverage algorithm), and results indicate whether test conditions pass
or fail. These results are transmitted to IGSTK’s dashboard. The significance of this process is that
it shows (i) how activities grounded in formal methods can be incorporated on an agile project
with the proper tool support and (ii) that such methods can be applied after implementation as a

‡

http://jboss.org/drools.

Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

957

AGILE METHODS

focused part of validation, instead of as an unbounded (heavy) task in the beginning of the process
cycle—again, just the right amount of ceremony.
Before creating our own tool we reviewed the existing tools, such as SPIN [33], UPAAL
[34], LTSA [35], and commercial offerings RoseRT and Rhapsody. These tools were either too
burdensome, solved the wrong problem, or were not license compatible with IGSTK. IGSTK
follows an agile approach. The IGSTK team does designs—on its Wiki. These designs are peer
reviewed and a prototype implementation presented. IGSTK constructs models—state machine
models to be exact—directly in the source code. The validation suite of tools checks the fidelity
of the model’s representation and execution semantics.

5. IGSTK AND SAFETY
Are agile methods appropriate for safety-critical systems? We claim yes, or at least that agile
practices can contribute to a software process that results in safer software. This is a case study,
but we are encouraged by our experiences since 2004. IGSTK is in use in 27 hospitals and
research centers worldwide. IGSTK has been used in clinical trials for transthoracic lung biopsy
approved by the IRB of a major university research hospital (for details see [36]), and received
a determination of non-significant risk from the FDA. Finally, IGSTK is a principle component
of the LUTi platform (http://www.luti.com.ar), a commercial image-guided surgical navigation
product used in ten neurosurgery cases in Argentina.
Assessing the safety of IGSTK is difficult because it is a framework and not a specific application.
One can evaluate a particular application such as the one described above as the surgical procedure
has exact requirements and domain-specific constraints. The application software and how it uses
IGSTK could be evaluated with traditional safety techniques, but to our knowledge no IGSTK user
has conducted such an analysis. Instead we evaluate the architecture using a modified Architecture
Tradeoff Analysis Method (ATAM [37]). This is appropriate as IGSTK is effectively an off-the-shelf
architecture for surgical applications.
The modified process examined IGSTK documentation from all sources (Wiki, code, mailing
lists, book, papers, etc.) to extract the key quality attributes and characterizations, and construct
a quality utility tree, as shown in Table I. Attribute characterizations include two ratings, one for
the importance of the item and the second the relative difficulty in achieving that quality attribute
level. The table essentially describes risks and scenarios in a way that is compatible with Boehm’s
[4] notion of risk and loss.
We define architectural approach descriptions to map how the IGSTK architecture addresses the
scenarios. Scenario descriptions for three of the four safety scenarios are included in the appendix
Table I. Quality Attribute Utility Tree for IGSTK.
Attributes

Quality sub-factors

Attribute characterizations

Safety

Framework misuse

(S1) H, H—Prevent framework misuse and ensure IGS
applications access to a basic unified layer
(S2) H, H—IGSTK classes won’t throw exceptions in order
to curb misuse
(S3) H, H—Ensure that the surgical view is up to date
(S4) H, M—Provide logging when component failure
occurs and provide failure message to user
H, M—Provide logging when lower level component
failure occurs and provide failure measure to user
H, H—Create a set of predictable deterministic behaviors
with a high level of code coverage 90%
H, M—Create a system of testing that uses sandboxing to
test and prototype all release
M,M—Response Time for visualization reduced to smallest
possible delay

Visual/instrumentation failure
Component failure
Testability

Error detection

Code incorrectness detectability
Usability

Copyright q

Latency

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

958

K. GARY ET AL.

of this paper. The fourth (S4) is not included as logging occurs automatically in all scenarios in
IGSTK, and the failure messages provided to end users are actually application-level requirements
based on the surgical procedure.
The modified assessment process adds some rigor to the evaluation of how IGSTK addresses
quality attributes, and in particular safety. But it is not a quantitative or formal analysis of failure
risks and loss. A more traditional safety analysis using techniques, such as FTA or FMEA, should
be performed on specific system instances that include IGSTK.

6. DISCUSSION
Leveson and Turner [19] recommend guidelines for safety-critical systems in their review of the
Therac-25 medical system, which the authors note failed due to coding, and were not requirements
errors. IGSTK follows these recommendations through agile and open source practices.
1. Documentation should not be an afterthought: documentation is created as the code is being
developed through (a) a Wiki that is constantly updated, (b) automated online documentation
created from the source via Doxygen, and (c) a freely available book which is created from
the source tree via the same build process as the source code.
2. Software quality assurance practices and standards should be established: IGSTK has established practices and standards for quality. The practices have been described throughout this
paper, from traditional techniques like code reviews to agile techniques such as pervasive
unit testing.
3. Designs should be kept simple: IGSTK’s design is simple; in fact simplicity is an overriding
agile principle. Component designs share a common architecture pattern (the state machine),
and design principles are enforced in code through strongly typed interfaces and specific
macros. Designs are documented and discussed on the Wiki.
4. Ways to get information about errors—for example, software audit trails—should be designed
into the software from the beginning: IGSTK’s logging facility records each request and
response in the system, providing a full audit trail. The lightweight requirements management
process ensures lockstep requirement and code changes.
5. The software should be subjected to extensive testing and formal analysis at the module and
software level; system testing alone is not adequate: IGSTK is rigorously tested at the unit
level on a continuous basis. Formal analysis is incorporated into the agile process through a
validation suite that reports results directly to the dashboard.
The IGSTK team has adopted an agile methodology and tailored it to their needs due to the nature
of the project. IGSTK’s agile approach is a combination of Scrum management practices combined
with XP coding practices with the support of an open source community. IGSTK team members
include experts from different fields, such as software engineering, computer graphics, imaging, and
robotics. Software development is based on a set of best practices iteratively applied with continuous
automated unit testing and 100% code coverage to ensure software quality. This ‘best practices’
approach is augmented by the right amount of ‘heavier’ practices taken from traditional approaches
to safety-critical systems. But even these practices, as shown with requirements management,
continuous integration and testing, and architecture validation, are integrated into the process
in such a way as to reinforce, and not obstruct, the agile culture. While an agile purist might
object that these activities are not agile, we believe the overriding principle of ‘just the right
amount of ceremony’ justifies the selected application of heavier practices in areas critical to
ensuring system safety.
We acknowledge that IGSTK’s agile approach is neither as rigorous nor as complete as it could
be for a safety-critical domain. A single case study, no matter how deep the experience, makes
‘the right amount of ceremony’ seem like a platitude, but we believe that many traditional process
models (or instances of those models by software development organizations) forget the obvious
and instead conduct a ‘checklist execution’ of the process. The tale of IGSTK’s agile evolution, we
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

959

AGILE METHODS

think, offers lessons and hope for applying agile methods to safety-critical domains. A more agile
process, augmented with key process elements and faithfully followed by the people that execute
it, is a way to achieve safety. Agile is not the absence of process, and lightweight does not mean
‘skip’. Agile means the right amount, adapting to change. Our position is supported only by our
experience, and we acknowledge that the debate is too broad and nuanced for us to start to outline
in this paper. The community assumes that agile methods cannot make a contribution in safetycritical domains, and that document-centric process models are the only option. The debate should
be reopened.

APPENDIX: ARCHITECTURAL APPROACH DESCRIPTIONS
Scenario: S1 Prevent framework misuse and ensure IGS applications access to a basic unified layer
Attribute: Safety
Environment: Normal / Strained Use
Stimulus: Developer Misuse
Response: Classes that inherit lower level component behavior are restricted to basic functionality.
Architectural Decisions
Risk
Sensitivity
Tradeoff
Encapsulation of toolkit functionalities
T1
Layered architecture
R1
Medium sized objects
T2
Reasoning:
Encapsulation of toolkit functionalities, preventing developers from directly manipulating objects without
passing first safeguards. Here there is a choice between flexibility and managing safety. By using safe
encapsulation to restrict functionality the IGSTK can better manage lower level APIs forcing all API calls
through safe checks managed via tactics such as a state machine implementation.
By implementing an architecture that depends on other toolkits/APIs not maintained by IGSTK developers,
the reliability of the IGSTK is limited to the APIs that it depends on. While safety may be managed from
the IGSTK layer, reliability can only be maintained to the extent to which it can be tested.
Medium sized objects resulting in reduced functionality again sacrifice flexibility in order to achieve safety.
A limited set of function calls allow IGSTK to manage complexity that could threaten safety.
Architectural Diagram:

Scenario: S2 Reduce the IGS Application’s ability to miss potentially harmful errors.
Attribute: Safety
Environment: Normal Use / Strained Use
Stimulus: An error is thrown by any element of the IGSTK or underlying layer of components.
Response: IGSTK classes will not throw exception in order to curb misuse
Architectural decisions
Risk
Sensitivity
Tradeoff
Limited use case.
T3
State Machine
T4
Reasoning:
By limiting the number of choices (functions) available to IGS developers the IGSTK developers can
ensure a limited number of use cases that can cover nearly the entire set of possible scenarios for which
to test resulting in a high level of code coverage.

Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

960

K. GARY ET AL.

The implementation of a state machine forces all events to traverse through the IGSTK model for handling
states, transitions and actions. The developer sacrifices flexibility for the convenience and safety that the
IGSTK has to offer.
Architectural Diagram:

Separation Between Public and Private Interface of IGSTK Components
Scenario: S3 Ensure that the surgical view is up to date
Attribute: Safety
Environment: Normal use
Stimulus: System may experience stress conditions
Response: Synchronicity is maintained through an event observer pattern. Through a series of pulses a
tracker class will query the actual hardware tracker device and will get from it information about the
position of the tracked instruments in the operating room.
Architectural decisions
Risk
Sensitivity
Tradeoff
Event observer pattern to facilitate updated surgical views
S1
Reasoning:
- Event observer pattern used to track and manage visual and physical representations over time
through a series of steady pulses. Expired views are not displayed and visual indicators are displayed.
If this management fails serious damage could result from a mismanaged physical and logical views
Architectural

Diagram
UML Sequence Diagram of the IGSTK Timing Collaboration

ACKNOWLEDGEMENTS

This work was funded by NIBIB/NIH grant R01 EB007195. This paper does not necessarily reflect the
position or policy of the U.S. Government.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

AGILE METHODS

961

REFERENCES
1. Beck K, Beedle M, van Bennekum A, Coburn A, Cunningham W, Fowler M, Grenning J, Highsmith J, Hunt A,
Jefferies R, Kern, Schwaber K, Sutherland J, Thomas D. Manifesto for Agile Software Development, November
2001. Available at: http://agilemanifesto.org,last [10 December 2010].
2. Gary K, Ibanez L, Aylward S, Gobbi D, Blake MB, Cleary K. IGSTK: An open source software toolkit for
image-guided surgery. IEEE Computer 2006; 39(4):46–53.
3. Bowen JP, Stavridou V. Safety-critical systems, formal methods and standards. IEE/BCS Software Engineering
1993; 8(4):189–209.
4. Boehm B. Get ready for agile methods, with care. IEEE Computer 2002; 35(1):64–69.
5. Leveson NG. Software safety: Why, what, and how. ACM Computing Surveys (CSUR) 1986; 18(2):125–163.
6. Parnas D, van Schouwen AJ, Kwan SP. Evaluation of safety-critical software. Communications of the ACM 1990;
33(6):636–648.
7. Schwaber, K, Beedle M. Agile Software Development with Scrum (1st edn). Prentice Hall PTR: Upper Saddle
River, NJ, 2001.
8. Cockburn A. Characterizing people as non-linear, first-order components in software development. Fourth
International Multi-Conference on Systems, Cybernetics and Informatics, Orlando, FL, 2000.
9. Lindvall M, Basili V, Boehm B, Costa P, Dangle K, Shull F, Tesoriero R, Williams L, Zelkowitz M. Empirical
findings in agile methods. Proceedings of the Second XP Universe and First Agile Universe Conference on Extreme
Programming and Agile Methods—Xp/Agile Universe 2002 (Lecture Notes in Computer Science, vol. 2418),
Wells D, Williams LA (eds.). Springer: London, 2002; 197–207.
10. Gelowitz C, Sloman I, Benedicenti L, Paranjape R. Real-Time Extreme Programming (Lecture Notes in Computer
Science, vol. 2675). Springer: Berlin, 2003.
11. Spence JW. There has to be a better way! [software development]. Proceedings of the Agile Development
Conference, ADC. IEEE Computer Society: Washington, DC, 2005; 272–278.
12. Van Schooenderwoert N. Embedded extreme programming: An experience report. Embedded Systems Conference,
Boston, 2004.
13. Van Schoownderwoert N, Morsicato R. Taming the embedded tiger agile test techniques for embedded software.
Agile Development Conference, Salt Lake City, 2004.
14. Manhart P, Schneider K. Breaking the ice for agile development of embedded software: An industry experience
report. Proceedings of the 26th International Conference on Software Engineering. IEEE Computer Society:
Washington, DC, 2004; 378–386.
15. Grenning J. Launching extreme programming at a process-intensive company. IEEE Software 2001; 18(6):27–33.
16. Bowers J, May J, Melander E, Baarman M, Ayoob A. Tailoring XP for large system mission critical software
development. Proceedings of the Second XP Universe and First Agile Universe Conference on Extreme
Programming and Agile Methods—Xp/Agile Universe 2002 (Lecture Notes in Computer Science, vol. 2418),
Wells D, Williams LA (eds.). Springer: London, 2002; 100–111.
17. Royce WW. Managing the development of large software systems: Concepts and techniques. Proceedings of
IEEE WestCon, Los Angeles, 1970.
18. Kruchten P. The Rational Unified Process—An Introduction. Addison-Wesley: Reading, MA, 2000.
19. Leveson NG, Turner CS. An investigation of the Therac-25 accidents. IEEE Computer 1993; 26(7):18–41.
20. Lutz RR. Software engineering for safety: A roadmap. Proceedings of the International Conference on the Future
of Software Engineering, Limerick, Ireland, 2000; 213–226.
21. Storey N. Safety-Critical Computer Systems. Addison-Wesley Longman: Harlow, England, 1996.
22. Bowen JP. Ethics of safety-critical systems. Communications of the ACM 2000; 43(4):91–97.
23. Broy M, Jonsson B, Katoen JP, Leucker M, Pretschner A. Model-Based Testing of Reactive Systems (Lecture
Notes in Computer Science, vol. 3472). Springer: Berlin, 2005.
24. Cleary K, Cheng P, Enquobahrie A, Yaniv Z. IGSTK: The Book (2nd edn). The ISIS Center, Georgetown
University, 2009.
25. U.S. Food and Drug Administration. Code of Federal Regulations, Title 21, Chapter 1, Subchapter H, Part 820
Medical Device Quality System Regulation, 1996.
26. Raheja D. Assurance Technologies: Principles and Practices. McGraw-Hill: New York, 1991.
27. Gary K, Kokoori S, David B, Otoom M, Cleary K. Architecture validation in open source software. Proceedings
of the Third Workshop on the Role of Software Architecture for Testing and Analysis (ROSATEA’07), Boston,
MA, 2007.
28. W3C. State Chart XML (SCXML): State Machine Notation for Control Abstraction. State Chart XML (SCXML):
State Machine Notation for Control Abstraction 1.0, 2005. Available at: http://www.w3.org/TR/2005/WD-scxml20050705 [19 January 2011].
29. Harris IG. Fault models and test generation for hardware–software covalidation. IEEE Design and Test of
Computers 2003; 20(4):40–47.
30. Offutt J, Liu S, Abdurazik A, Ammann P. Generating test data from state-based specifications. The Journal of
Software Testing, Verification and Reliability 2003; 13(1):25–53.
31. Hune TS. Analyzing Real-Time Systems: Theory and Tools. PhD Dissertation, Basic Research in Computer
Science, University of Aarhus, Denmark, 2001.
Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

962

K. GARY ET AL.

32. Watson AH, McCabe TJ. Structured Testing: a Testing Methodology Using the Cyclomatic Complexity Metric.
NIST Special Publication 500-235, 1996.
33. Holzmann G. The SPIN Model Checker. Addison-Wesley: Boston, MA, 2003.
34. Bengtsson J, Larsen K, Larsson F, Pettersson P, Yi W. UPPAAL—A tool suite for automatic verification of
real-time systems. Proceedings of Hybrid Systems III. Springer: Berlin, 1996.
35. Magee J, Kramer J. Concurrency: State Models and Java Programs (2nd edn). Wiley: Hoboken, 2006.
36. Yaniv Z, Cheng P, Wilson E, Popa T, Lindisch D, Campos-Nanez E, Abeledo H, Watson V, Cleary K. Needlebased interventions with the image-guided surgery toolkit (IGSTK): From phantoms to clinical trials. IEEE
Transactions on Biomedical Engineering 2010; 57(4):922–933
37. Kazman R, Klein M, Clements P. ATAM: Method for architecture evaluation. Technical Report CMU/SEI-2000TR-004, (ADA382629), Software Engineering Institute, Carnegie Mellon University, 2000.

Copyright q

2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2011; 41:945–962
DOI: 10.1002/spe

2009 Sixth International Conference on Information Technology: New Generations

3D VQI: 3D Visual Query Interface
Subhash Uppalapati, John C. Femiani, Anshuman Razdan, and Kevin Gary.
Division of Computing Studies
Arizona State University at the Polytechnic Campus
Suppala1@asu.edu
more powerful than was possible when vessel shape was
given a mathematical treatment by G. Birkhoff [1].
Therefore physical measurements and other interactions
using an Online, interactive digitized 3D version of a
specimen will facilitate new techniques for research and
education by archaeologists.
This report describes the methodology used in the
development of the VQI web application on content-based
retrieval and 3D interactive display of archaeological
vessels. This report uses vessels from the Classic Period (A.
D. 1250 – 1450) of the prehistoric Hohokam culture area of
the Southwest (Salt/Gila River Valleys) near present-day
Phoenix, Arizona. The main components of this work are a
visual query applet, which allows archaeologists to form a
shape-based query, and a 3D Display component, which
provides a 3D interactive platform to inspect and make
measurements on 3D vessels. Obtaining the geometric
features involves obtaining shape information from the
scanned three-dimensional data of archaeological vessels,
using 2D and 3D geometric models to represent scanned
vessels, extracting features from geometric models and
storing the feature information in database for Web-based
retrieval.
In a typical workflow Figure 1, digital samples are
acquired from physical specimens using 3D data acquisition
devices and the resulting point set is wrapped by a 3D
surface, cleaned of spurious digitization artifacts. It is
segmented and annotated by a domain expert to produce 3D
content with added semantic information. The compressed,
encrypted, or watermarked geometric and semantic
information is part of the database, so that it can be used in
searching documents that include descriptions and other
structured domain specific data.

Abstract
The 3D Visual Query Interface (3D VQI) is a web-based
database application that allows for shape-based searches
in addition to more traditional text-based queries of the 3D
vessel dataset. This interface allows researchers remote
access to vessel collection that can be analyzed from
anywhere in the world. The interface is divided into two
sections with right half supporting the ability to draw profile
curve both symmetric and asymmetric. This query can be
further refined using the left half which provides contextual
based information. Using a curve-matching algorithm the
software then searches the database and returns a series of
potential matches sorted by the percentage of similarity
between the curves. Each match can then be selected for
more detailed viewing of the 3D model and data or for
further analysis.

1

Introduction

The 3D Visual Query Interface (VQI) is a web
application which allows shape-based matches in addition to
text-based search into a 3D vessel database. The application
focuses on providing the ability for the user to draw the
profile curve as one of the main input criteria for shapebased matching in addition to text-based search. Based on
the search conditions given, the application executes a shape
matching algorithm and contextual-based search and
displays all the matching results. The application also needs
to display the 3D view of the vessel in a more robust way
reducing the burden on the users to install software. It also
needs to have cross-browser and cross-platform capabilities
and provides help, demo and feedback features for the users
to effectively use the web interface.

The contributions of the project are as follows:

3D knowledge often plays an important role in
archaeology. Archaeologists characterize the development
of Native American cultures by studying the 3D form of
pottery. Access to a large repository of 3D vessels via the
web can provide a means to study pottery when access is
limited by geography or other barriers. Quantitative methods
of reasoning about the shape of a vessel are becoming far

978-0-7695-3596-8/09 $25.00 © 2009 IEEE
DOI 10.1109/ITNG.2009.331

• Provide access to 3D content in the form of a database of
digitized pottery samples.
• Demonstrate a visual query interface based on profile
curves, silhouettes, or cross-sections of 3D objects as a
simple cost-effective and intuitive way to formulate 3D

1347

and other ill-defined 3D models. Their approach is to use the
rendered appearance only of the model as the basis for shape
similarity comparison. Their method removes scale and
positional degrees-of-freedom by using normalization and
the three rotational degrees of freedom by using a
combination of discrete sampling of solid angles and a
rotation- invariant 2D image similarity comparison
algorithm.
Most of the online websites that are available are still in
experimental condition, query methods are difficult to use
and not easy enough to understand the working mechanism.
This gives scope for efficiently finding the 3d models on the
web, the different query interfaces to use and the efficiency
and effectiveness of the matching and 3D display of the
object.
For the researchers in the field of Archaeology, to find
the pottery based on textual data or of a particular shape, we
developed a web based application. The system provides
query interface to search based on both contextual data and
2D shape. The interface is developed keeping in mind the
usability and simplicity. We used the latest efficient and
effective shape matching algorithm for finding the similar
objects (as demonstrated in [2]). The contribution of this
report is an investigation of the design and implementation
trade-offs in building such a 3D model search engine based
on these methods.

queries into a shape database when laser scanners or other
3D digitization tools are not available.
• Provide a cross-platform, portable, cross-browser viewer
that allows interaction and measurement of the pottery
samples.

2

Prior Art

Several systems exist that provide different ways to
form a query, retrieve matching objects, and/or display the
3D content on the web. One of sketch-based query model
algorithms is the pseudo-3D sketching [16]. In pseudo-3D
sketching, as the name implies the user inputs the crosssection of desired shape and also mentions the size in the
still undefined dimension. In the interactive 3D model
sketching based on the user's hand movements from
consecutive frames will be extracted. One other example is
Princeton search engine [6] which expects outline sketches
up to three views and matches to pre computed sketches of a
3D model using Fourier descriptors of rendered images. The
Purdue search engine [21] matches 2D views using 2.5D
spherical harmonic descriptors.
Some of the shape matching algorithms and their
problems in shape retrieval, shape recognition and
classification, shape alignment and registration and shape
approximation and simplification were discussed by [3]. To
find the available 3D models on the web Thomas
Funkhouser [17] developed a focused crawler for 3D
models. Their system provides a wide variety of query
interfaces based on text, 3D shape, and 2D shape, which
were designed to be easy to use, and hide parameters of the
underlying matching methods.
This report is an extension of the earlier work done in
the domain-specific query interface. Rowe et al. [18]
describe a system where the user can draw a 2D outline of a
ceramic vessel as a shape query into a database of 3D
models of such vessels. For example, the web site of the
State Hermitage Museum in St. Petersburg uses the “Query
by Image Content” method to allow users to search
paintings by drawing simple colored sketches [19].
A detailed study of different content-based 3D shape
retrieval methods were discussed by Johan W.H. Tangelder
[21]. For the shape-based queries, Thomas Funkhouser [17]
has developed a new matching algorithm that uses spherical
harmonics to compute discriminating similarity measures
without requiring repair of model degeneracy or alignment
of orientations. It provides 46–245% better performance
than related shape matching methods during precision-recall
experiments. The most significant feature of [20] 3D shape
similarity comparison method is that it accepts polygon soup

3

System Design and Architecture

Once acquired, the aim is to make 3D digitized pottery
samples accessible to the researchers. Archaeologists must
be able to retrieve an annotated 3D model from the
collection based on contextual or semantic information as
well as by the shape or geometric similarities. Figure 1
describes the flow chart of the Web-based VQI. The query
process in VQI combines a sketch-based interface which
allows archaeologist's to draw the profile curve of a pot, in
addition to traditional text and metric data. A free-form
profile sketch can be created in the interface window with
options to draw asymmetric curve and to smooth the curve.
The user can also upload image of the pot that he has, and
draw outline in the applet.
One of the most difficult tasks in forming a 3D shapebased query is identifying an example shape. Techniques
include using an existing 3D model such as a laser scan,
providing a query interface with basic 3D modeling tools to
build an example, or conducting a query based on sketches
from multiple 2D points of view [4]. Pottery is often radial
symmetric, so query is formulated based on a profile curve.
A perfect match in the data base would be a surface of
revolution generated from the profile curve itself.

1348

Figure 1: Flow Diagram

3.1

identified by zero crossings of the curvature plot. In addition
wherever the cosine of the tangent angle crosses zero
identifies points of vertical tangency. The number and
position of these landmark points, along with the profiles
endpoints, are used to categorize the overall shape of each
pot.

Acquiring Digital 3D Content

After digitizing archaeological vessel via a 3D laser
scanner (Cyberware 3030), a triangle mesh is obtained that
is constituted of triangular faces, edges and vertices. The
triangle mesh is used as a raw data for further analysis. This
raw mesh captures the shape of the vessel but, but not in a
way that is easy to interpret or work with in order to
formulate a query. At this stage a domain expert identifies
meaningful features [1] [1, 8]. The first step is to identify the
salient cross-sections in a vessel, so that later a 2D query can
be formulated by sketching a profile curve. In order to get a
2D profile curve from a vessel, archaeologists interactively
select a cutting plane that intersects the triangle mesh of the
vessel. If a triangle intersects the plane, then the intersection
is a line segment connecting two points. The intersecting
segments form several connected poly lines represented by
2D chain codes. The longest one is extracted as the shapes
profile curve. Non-uniform rational B-spline (NURBS)
curves are fit to the points of each chain code to form least
squares approximation. Curvature, or turning angle in
discrete point sets, provides useful information such as
convexity, smoothness, and the locations of inflection points
of the curve needed by vessel analysis. NURBS curves
reduce noise that may be present in a dense profile curve
and allow thereby reduce the impact of digitization errors or
irrelevant details in the profile curve. Details are in [4].
Curvatures or angles in space don't really have a sign, but
when shapes are planer one can convert curvatures or angles
into signed values based on the right hand rule [4,5], see
Figure 1. By the threshold of the magnitude of curvature
plots from above corner points and inflection points are

3.2

User Interface Design

The user interface is developed keeping in view of the
users of the application. It is developed so that the
navigation from one page to another is smooth. It is also
developed with the aim to keep unique look and feel
throughout the website, and user friendliness. The only
requirement for the website is to have Java Runtime
Environment (JRE) in the machine used to access the
website.
3.2.1
Visual Query Interface (VQI)
The query interface supports either text-based search or
Visual Query based search or search based on both text and
visual based search. The query input page is shown in
Figure 2.
The text-based search works on the contextual data. The
user can enter the time period a particular pot belongs to or
the type of the pot or the archaeological site it belongs to.
The user needs to provide one of these major criteria to
effectively search for related pots in the database. Optional
criteria that can be entered by the user include rim diameter,
vessel volume, maximum diameter, and vessel height.
Visual query applet enables the users to draw profile
curve of the pot as the search input. The applet supports
asymmetric drawing of the curve by checking the

1349

asymmetric checkbox. Which means the user can draw
profile curve differently to the left and right of the middle
line. Once a basic outline of the curve has been drawn, user
can smooth the curve by checking the smooth checkbox. To
aid the user in better drawing the profile curves, there is
provision for uploading the image of the pot to the applet.
By uploading the image to the applet, user can draw on the
outline of the image and he can remove the image from the
applet by using remove button to see how the profile curve
looks. To completely erase the drawing window and remove
the image, user has the option to click on Clear button.

Figure 3: Search Results Page
3.2.3
Interactive 3D Display
The final presentation of 3D objects, the 3D display
interface, makes the artifacts more accessible because it
allows interactive exploration of the surface geometry as if
the object was in the same room. In order to be accessible a
viewer must be portable, it must be simple or automatic to
install, users must be able to use it without admin rights on
the machine, and it should be capable of launching and
loading surface geometry quickly.
There are several options for deploying 3D content on
the web, including the option of using an existing viewers
such as JavaView [5]. Initially we chose to build a custom
viewer. Java's WebStart Technology [7], which is bundled
with recent versions of the Java Runtime Environment,
allows digitally signed extensions such as JOGL or Java3D
[13] to be deployed with a standalone application. After
further research we found the JNLPApplet launcher [Add]
which enables the applets embed within a web page without
requiring the applet to be signed or performing any manual
installation on the client’s computer. This means that
embedded 3D viewers can leverage the full power of Java
for transmitting data and controlling behavior of 3D content
available through a web page.
The viewer allows data to be encrypted, and it allows
the rendered images watermarked. Intellectual property is
protected by restricting the amount or type of data that can
be saved for each sample.

Figure 2: Query Input Page
3.2.2

Search Results
Once the user submits the query, search will be
performed based on the criteria provided by the user. If the
user provides the visual query then Shape matching
algorithm will be used to match the pots from the database
to the input profile. The matching of each pot will be
represented in percentages from zero to hundred with
hundred means exact match. Shape matching algorithm
ranks the query results by descriptive and spatial similarity
to the query image. Query response information is presented
sequentially over several screens, each providing an
additional level of information about the selected objects.
The query results are displayed in a table with its detail and
can be ordered based on the curve matching percentage,
specimen number or vessel type. Figure 3 shows the first
result screen, which displays thumbnail images and brief
descriptions of the top search results. Also presented is a
large image of the pot of the selected row, selectable using
radio button, along with more detailed descriptive and
calculated information.

1350

resulting in Stanford polygon file format (PLY) files. The
PLY files were then edited to remove the markers used for
scanning and save the 3D scan in the OBJ format. The OBJ
files are saved in the MySQL database. The PLY files are
studied by an archaeology domain expert and he selects the
cutting plane for obtaining the profile curve. And features of
the pot were extracted from the pot from the study of the
expert. Then the contextual data, the thumbnail image,
profile curve and all the computed data is saved in the
database.
The application interface provides the ability to do either
a text-based, visual query based or both text and visual
query based search for the matching of pots from the pots
database. Each request comes to Tomcat 5.5 web server
which runs on Windows 2003 server. At the server side the
shape matching algorithm pre computes the geometries of
the profile curves from their profile curve descriptions
chosen by the expert. Based on the contextual criteria
provided and the shape drawn in 2D, the matching results
will be computed and will be displayed in an order based on
the matching percentage.
The application also supports the 3D display of the object
inside the browser. For this, Java 3D is used to create a
JApplet and by using JNLP Applet launcher it is possible to
display the 3D applet inside the browser. This requires
minimum install from the user and makes the 3D display
hassle free unlike other viewers of 3D objects. This also
gives the flexibility of the 3D display to work on cross
browser and cross platform.

Figure 4: View Complete Details Page
3.2.4
Feedback Page
The feedback page allows the users to input their
comments and problem experienced in using the website.
This helps the developers to understand the user’s needs and
problems and helps in better building of website to make it
more useful for the users.

3.3.1

Database Server
All the information related to the pots is stored in the
database. The DBMS used is MySQL 5.x which has new
features like triggers, stored procedures and views included.
3.3.2
Application Server
The application server used to serve the web
application is Tomcat 5. It handles all the user requests
coming from the browser, maintains sessions, communicates
with the database to retrieve records, responds to the user.

3.4
Figure 5: Feedback Page

3.3

Shape matching Algorithm

Shapes are compared using normalized correlation of
the coordinates of the two curves. The cross correlation is
maximized over all possible translations, rotations and a
finite rage of scales. The curves are aligned at the centers
and a range of intervals extending from the middle 75% to
the entire length of the longer curve are compared with the
shorter curves. The rigid body motion that maximizes the
normalized correlation between the two curves is identified
using the closed form method presented by Horn [22].
Normalized correlations generally fall in the range from
minus one to one, but in our case results will be positive. A
correlation of one is a 100% match; a correlation of zero is
the worst match possible. Score of minus one means that the
curves are identical but the points were listed in opposite

System Architecture

The whole system containing the visual query and textbased query, search results, the shape matching algorithm
and the 3D display is implemented in Java. The complete
application comes as a Web Application Archive (WAR)
and can be deployed on any webserver which supports J2EE
1.4. The application is platform independent and has cross
browser support.
The original pots from the Archaeological Research
Institute (ARI) were laser scanned using Cyberware 3030

1351

The palette provided by the IDE is used for developing the
user interfaces. Debugger tool is used to debug the code to
fix the issues. Profiler is used to understand the hot spots in
the application, the memory performance and CPU
performance. Unit test cases are written to carry out method
level testing.

order. That case is not possible since Horn’s method would
have flipped the curve to make the correlation positive.

3.5

Caching of vessel geometry calculations

Every time user submits the visual query, the input
curve is matched with the profile curves stored in the
database using the shape matching algorithm. One way of
doing it is for every request, accessing the database for the
profile curves, computing the geometries based on profile
curve and then matching it with the input curve.
Theoretically the performance can be improved by
caching the computed geometries of the profile curves in the
application. When the web application is accessed for the
first time, a connection will be made to the database and all
the profile curves will be fetched from the database. Then
using the application logic, all the required calculations will
be made on the profile curves and the objects are stored in a
HashMap.
Now, for every new request the application accesses
HashMap to see if the calculated object is available. If it is
available the shape matching algorithm is executed to find
the matching percentage. If not the profile curve is queried
from the database and after calculations, the object is added
to the HashMap for future access.

3.6

3.6.5

TOAD
Tool for Application Developers (TOAD) makes
database and application development faster and easier and
simplifies day-to-day administration tasks [15].
TOAD data modeler has been used to model the
database and ER diagrams. From the data modeler the SQL
file to create the database can be directly exported. It also
gives the option to specify the foreign key constraints. Once
the database is modeled, it is created using the TOAD for
MySQL software which can be used to administer the
MySQL database. By using this freeware, the database
administrators are connected to the DB server from remote
machine to administer the data. Efficient queries are created
using the query builder and the results can be immediately
checked by executing the queries.

3.7

Project Management

The web application started with the requirements
phase, continued with the design, development and testing.
For managing the website Trac [10] is used which is Project
management and bug/issue tracking system and SVN is used
for code versioning.

Technologies and Tools

During the development of the web application
several technologies and tools are used. The major
components are described below.

3.7.1
Trac
Trac is used for project management by following
systematic development. The project has several milestones
created with each milestone having requirements as tickets.
All activities will be tracked as ticket system. It has
integrated wiki which is used to describe project
terminology, all necessary information. It also supports the
integration of SVN versioning system so the code can be
browsed through the website. All the executables required to
run this web application are available to download from the
Downloads section of the website which can be accessed by
registered users. Meeting minutes of weekly meetings is also
maintained on this website.

3.6.1

Java Server Faces (JSF)
The framework followed for the development of the
web application is Java Server Faces (JSF) [11] [12]. It is
built on top of Model-View-Controller (MVC) architecture.
User interfaces are developed using JSF technology.
3.6.2

Java 3D
Java 3D is a scene graph based technology useful for
programming for the Java. It is built on top of OpenGL for
displaying graphics. In this web application, 3D viewer for
the pot display is developed using Java 3D inspired by one
of the example PrintCanvas3D available at [13].
3.6.3

SQL
Structured Query Language (SQL) has been used
extensively to retrieve and manage data available in the
vessel database.
Based on the contextual data provided by the user,
search query will be formed programmatically. Over the
results obtained after contextual search, shape-based
matching will be performed to obtain the matching
percentage.

3.7.2
Version Control
In this project Subversion, popularly known as SVN is
used for version control. Versioning helps in saving the
work with the ability to revert back to previous versions. It
also helps in team oriented development where multiple
developers are working on the same code base at the same
time.

4

3.6.4

NetBeans IDE
NetBeans [14] Integrated Development Environment
has been used for the development of the web application.

Experimental Results

Several experiments have been performed on the shape
matching algorithm on how best the similar pots are getting
retrieved from the database.

1352

4.1

Experiment 1

Vessel ID

In this experiment, the input is a bowl shaped pot and
the results are observed based on the percentage of matches
for different types of pots. Blue color line is for the input
drawn using the visual query applet provided by the web
application. Read color lines indicate the profile curve of the
vessels present in the database. When the shape matching
algorithm is applied on these two curves the matching
percentages in the range from zero to hundred are shown in
the table. The results are shown in Figure 6.

Vessel
ID
1950.0
01.001
62

Matc
hing
%
89.15

1963.0
02.000
04

88.21

1950.0
01.001
47

87.75

1950.001.
00158

1950.001.
00154

87.21

1959.053.
00003

86.46

1950.001.
00147

75.40

Figure 7: Experiment 2 Results

Future Work

3D VQI is a robust web application to aid researchers
at remote location to access the pots database. It provides
the ability to view the 3D display of the pot to study its
features and geometries. The researcher can perform search
on the pots database by providing a 2D profile curve-based
search. Some of the features that can be added in the future
are given below.
The Visual query applet can be enhanced to provide
sample profile curves which the user can select and modify.
The smooth functionality can be improved and by getting
feedback from the users on the usability of the curve
drawing, changes can be implemented.
The shape matching algorithm which forms the core of
the shape matching can be enhanced with the results it is
giving and with the feedback from the domain experts.
The 3D display applet which will display the 3D pot
inside the browser depends on the requirement that the user

85.00

Figure 6: Experiment 1 results

4.2

Curves

Curves

5
1950.0
01.001
58

Matching
%
90.70

Experiment 2:

In this experiment the input curve is a complex pot with
one inflection point. Blue curve indicates the curve drawn
using the visual query applet and the red curve is the profile
curve of the actual pot available in the pots database. The
results are shown in Figure 7.

1353

[16] Konstantinos Moustakas., Georgios Nikolakis and
Dimitrios Tzovaras, 3D content-based search using
sketches., Springer-Verlag London Limited 2007
[17] Thomas Funkhouser and Patrick Min. A Search Engine
for 3D Models, ACM 0730-0301, 2002
[18] Jeremy Rowe, Anshuman Razdan and Arleyn Simon,
Acquisition, Representation, Query and Analysis of
Spatial Data: A Demonstration 3D Digital Library.
IEEE, 2003
[19] C. Faloutsos and R. Barber. Efficient and Effective
Querying by Image Content, Journal of Intelligent
Information Systems, 231-262 (1994)
[20] Ryutarou Ohbuchi, Masatoshi Nakazawa and Tsuyoshi
Takei. Retrieving 3D Shapes Based On Their
Appearance, ACM SIGMM, 2003.
[21] Johan W.H. Tangelder and Remco C. Veltkamp. A
Survey of Content Based 3D Shape Retrieval Methods,
Proceedings of the Shape Modeling International 2004

has JRE installed in his machine. A message can be shown
in cases where the OBJ file is not present or the JRE is
unavailable.

6

Acknowledgements

We would like to thank Dr. Arleyn Simon and Dr. Stephen
Savage from Archeological Research Institute at ASU for
providing requirements and domain knowledge related to
the application.

References
[1] Birkhoff, G. Aesthetic Measure. Harvard University
Press, 1933
[2] Berthold K.P. Horn, Closed-form solution of absolute
orientation using unit quaternions, Journal of the
Optical Society of America, 1987
[3] Remco C. Veltkamp, Shape matching: Similarity
measures and algorithms, 2001.
[4] Bae, M. Curvature and Analysis of Archaeological
Shapes. MS Thesis, Arizona State University, 1999
[5] Java View – Interactive 3D geometry and visualization,
Nov 2008
Available: http://www.javaview.de/
[6] Princeton 3D Model Search Engine, Nov 2008
Available: http://shape.cs.princeton.edu/search.html
[7] Java Web Start – Frequently Asked Questions, Nov
2008
[8] A. Razdan, D. Liu, M. Bae, M. Zhu, G. Farin, A.
Simon, and M. Henderson. Using geometric modelling
for archiving and searching 3d archaeological vessels.
Proc.Conf.on Imaging Science, Systems and
Technology, Las Vegas, June, 2001.
[9] Razdan, A. and Henderson, M. Feature Based
Neighborhood Isolation Techniques for Automated
Finite Element Meshing, Geometric Modeling for
Product Engineering, Editors Josh Turner, Ken Press,
North Holland, 1989
[10] TRAC integrated SCM & Project management, Nov
2008
[11] Chris Schalk (Author), Ed Burns (Author) and James
Holmes (Author), JavaServer Faces: The Complete
Reference, McGraw-Hill Osborne Media; 1 edition
(August 25, 2006), ISBN-10: 0072262400
[12] Developing Web Applications with Java Server Faces,
Nov 2008
Available: http://tinyurl.com/3vedtv
[13] Java 3D Web start example programs, Nov 2008
Available: http://tinyurl.com/b6zdxz
[14] NetBeans Integrated Development Environment, Nov
2008
Available: http://www.netbeans.org/
[15] Tool for Application Developers (TOAD), Nov 2008
Available: http://www.toadsoft.com/

1354

Designing a Mobile Application to Support the Indicated
Prevention and Early Intervention of Childhood Anxiety
Mandar Patwardhan

Ryan Stoll

Derek B. Hamel

Arizona State University
School of Computing, Informatics and
Decision Systems Engineering
Ira A. Fulton Schools of Engineering
Tempe, AZ USA

Arizona State University
Department of Psychology
Tempe, AZ USA

Arizona State University
School of Computing, Informatics and
Decision Systems Engineering
Ira A. Fulton Schools of Engineering
Tempe, AZ USA

Ryan.Stoll@asu.edu

mpatward@asu.edu

dbhamel@asu.edu
Kevin A. Gary

Ashish Amresh

Arizona State University
Arizona State University
School of Computing, Informatics and School of Computing, Informatics and
Decision Systems Engineering
Decision Systems Engineering
Ira A. Fulton Schools of Engineering Ira A. Fulton Schools of Engineering
Tempe, AZ USA
Tempe, AZ USA

amresh@asu.edu

Armando Pina
Arizona State University
Department of Psychology
Tempe, AZ USA

Armando.Pina@asu.edu

kgary@asu.edu

ABSTRACT

1. INTRODUCTION

This paper presents the design of an mHealth application for
prevention and early intervention of childhood anxiety. The
application is based on REACH, a preventative-early intervention
protocol for childhood anxiety. This paper describes the
multidisciplinary design process, sharing lessons learned in
developing an effective mHealth application. This mHealth
application is unique due to participant age, preventive-early
intervention focus, and utilization of mobile technology in a
situated manner. A design process inspired by user-centered
leveraging key informant interviews was used to identify
application features, including game based strategies and an
animated motivational avatar. Validation was performed through
external review and a usability study performed with target end
users of the application. Results suggest overall satisfaction, ease
of use, and increased motivation.

Mobile health applications (mHealth apps) span a wide spectrum
of health-related issues and treatment approaches, such as health
monitoring (physiological or self-reported), protocol adherence
through reminder communications, and (psycho)education [15].
Interestingly, the ubiquitous and familiar nature of smartphone
devices creates the potential for mobile health (mHealth)
applications targeted to youth “at risk” for anxiety disorders or
meeting criteria for anxiety disorder diagnoses. In fact, mHealth
for anxiety disorders may be of unique importance because most
parents do not seek help for their anxious youth, effect sizes from
anxiety programs are generally modest and need to be potentiated,
and there is a pressing need for sustainable and streamlined
intervention efforts that have “real world” utility [2][3][13]. In
addition, targeting anxiety disorders is of public health
significance because these are among the most prevalent
psychiatric problems in children with rates ranging from 5% to
10% and as high as 25% in adolescents. Anxiety disorders also
cause significant impairment, typically fail to spontaneously
remit, and are prospectively linked to clinical depression and
problematic substance use for some youth [13].

Categories and Subject Descriptors
D.2.2 [Software Engineering]: Design Tools and Techniques –
Evolutionary prototyping, and user interfaces.

General Terms
Design, Human Factors, Verification

Keywords
Youth Anxiety Prevention, mHealth, User-Centered Design.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee.
Wireless Health ‘15, October 14–16, 2015, Bethesda, MD, USA.
Copyright 2015 ACM 978-1-4503-3160-9 …$15.00.
http://dx.doi.org/10.1145/2811780.2811954

Although the popularity of mHealth apps is exploding, few
lessons have been shared regarding the user experience design for
such innovations. Building on randomized control trial (RCTs)
studies and theory, this research focuses on the design process for
adapting aspects of an empirically informed child anxiety disorder
intervention to a smartphone platform. Thus, this work is
significant due to the domain (anxiety), the nature of the
intervention (preventative-early intervention), the use of an app to
increase protocol efficiency, and the integration of concepts from
innovative design technology (gaming, notifications, user
experience design) to improve outcomes.
Focusing on the anxiety protocol, it is important to note that
considerable strides have been made to develop evidence-based
treatment and prevention armamentaria targeting youth anxiety
with almost every protocol employing the same cognitive and
behavioral procedures (Fisak et al., 2011; Silverman et al., 2008) .

REACH for Success (REACH hereafter) is a school-based
cognitive-behavioral protocol designed for 4th and 5th graders for
the indicated prevention and early intervention of childhood
anxiety and related problems. REACH uses procedures found to
be efficacious in RCTs, including in our own 3 RCT [8][9][12];
however, there are several features that set REACH apart. Most
relevant to this paper is data suggesting that the classic design of
evidence-based prevention programs (including programs like
FRIENDS [1]) is simply not feasible or sustainable in schools
(e.g., there are too many sessions, sessions are too long, manuals
are too cumbersome and not organized for real worldimplementation, too much training is required, and preparation is
too time consuming). In contrast, REACH was created from our
evidence-based exposure-based cognitive-behavioral protocols as
a practical intervention that can build a foundation for sustainable
large-scale diffusion. That is, REACH was streamlined into 6
sessions (instead of the typical 12-15), each 20-30 minutes in
length (rather than the typical 60 to 90 minutes), and uses an easyto-follow manual (each session is condensed into one page front
and back while FRIENDS, for example, has an 89 page manual).
One concern with REACH, however, is that such a streamlined
protocol may result in a lower dosage of the active change
ingredients and fewer opportunities for youth to practice coping
skills because there are fewer sessions and less practitioner
feedback time. This concern is justified as a recent child anxiety
treatment study evaluating an 8 session adaptation of the 16 to 20
session Coping Cat program yielded lower youth response rates
suggesting that difficulty practicing the skills was a major
impediment to recovery [11].
A purpose of this research was to design an mHealth platform to
accompany the REACH 6 session school-based preventative early
intervention protocol. Specifically, the goal was to develop an
mHealth app that: (a) provides on-demand opportunities for skill
practice, (b) uses notifications relevant to skill practice to improve
compliance, (c) offers tools for personalizing and tailoring the
protocol, (d) increases opportunities for corrective feedback based
on user data amenable to creating personalized reports of youth
weekly practice and response, and (e) yields high user ratings
along core validated usability dimensions relevant to technology
innovation efforts. Herein, the REACH protocol, the app design
process, and the app implementation are described. Results from
an empirical study in a usability context are presented. To set
domain context, the face-to-face protocol is described followed by
a discussion on design, implementation, and usability.

2. THE REACH PROTOCOL
REACH for Personal and Academic Success is an indicated
prevention and early intervention program targeting anxiety
disorders and related problems in youth. The protocol is
administered in a group format (five to seven children per group).
Each session (S) in the manual is organized in terms of Overview,
Content (didactic, games), Review/Closing, and After the Session
(homework). Self-evaluation of emotion expressiveness is
embedded in every session. The protocol focuses on broad-based
exposure and problem solving skills, which have a wide reach for
the range of anxiety disorders targeted. Unique session content is
as follows. S1: Introduction (group name, rules, and
confidentiality), Learn about emotions, and Relaxation. S2-3:
Define worries, Learn cognitive self-control, and Practice
cognitive self-control (Worryheads game). S4: Define social skills
and Learn about conversation skills (starting and managing
conversations). Practice conversations (make-believe game). S5:
Learn about assertiveness and Practice assertiveness (stand-up!

game). S6: Learn to face situations and Engage in behavioral
exposures to mild-moderate anxiety-provoking situations. Core
skill acquisition and practice tools include the use of Daily
Diaries, Guided Relaxation, STOP acronym, and STIC acronym.
Relevant to the REACH app, Daily Diaries are used to facilitate
self-evaluation of emotion expressiveness. Youth self-monitor and
describe in writing the anxiety or fear provoking situations that
occurred during the week. Youth also rate using a 0-8 feelings
thermometer the severity of anxiety/fear associated with the
situation. Lastly, youth describe in writing thoughts that occurred
before/during/after the situation (e.g., worries) and actions that
resulted (e.g., avoidance behaviors). In terms of Guided
Relaxation, youth are provided with pre-recorded standardized
step-by-step procedures designed to improve self-regulation of
anxiety related physiological hyperarousal via breathing exercises,
muscle tension/release exercises, and imagery. When it comes to
cognitive self-control, a four-step coping plan is introduced via
the “STOP” acronym where S = Scared? T = Thoughts, O= Other
[thoughts], P = Praise. STOP is first practiced via the Worryheads
game by using pre-written emotionally ambiguous and anxiety
provoking scenarios along with an accompanying “worry
thought”. Youth are then asked to change the “worry thought” for
a more realistic and alternative solution to the scenario provided.
In the game, successful resolution of the worry thought results in
advances toward a common goal for each player (reaching the end
to win the game). Subsequently, with basic knowledge of STOP,
youth engage in prospectively applying the technique to situations
that emerge as anxiety or fear provoking for them during the
course of each week. Lastly, behavioral exposures are introduced
via STIC jobs (STIC = Show That I Can. STICs are provided in
the form of a pre-written or prepopulated Fear Hierarchies based
on modules from the Anxiety Disorders Interview Schedule for
Children where each avoidance behavior has been pre-populated
for the child as individual exposures.
The REACH protocol has been implemented using a paper-andpencil approach. The protocol, while effective, encountered some
common limitations in practice, notably protocol compliance.
Specifically, subjects did not practice skills between sessions or
were not diligent in recording practice activity and outcomes.
Further, as noted in section 1, lower dosage in the related Coping
Cat tool resulted in lower response rates. Data capture with paperand-pencil methods is also time consuming and subject to human
coding errors or oversights. The psychology researchers believed
mobile and gaming technologies could effectively address the
limitations, improve compliance and data capture, thereby
reducing dosage while increasing effectiveness. They teamed with
software engineering researchers to conduct a multidisciplinary
design and development process to construct the app.

3. DESIGN PROCESS
The multidisciplinary team embarked on a highly iterative design
process focused on the capabilities and context of end users. The
researchers aspired to use a user-centered design (UCD)
approach, but in practice the designers did not have direct access
to end users during the design process and as such relied on
subject matter experts (SMEs) as proxies. The SMEs were the
psychologists who developed the REACH protocol and had
deployed it 56 times to youth over 6 months. Section 5 describes
external validation via design review by a school advisory board
and a usability study with independent youth end users (n=22).

3.1 Gap Analysis
REACH is a pre-existing protocol, so the first design activity was
to review program materials and workflow, seeking opportunities
to effectively translate existing steps, and later innovating on
smartphone-specific solutions to achieve the domain objectives
for increased dosage, engagement, and feedback (see Section 1).
To better understand the domain of the app, the SMEs shared the
provider manual of the REACH protocol to the designers and the
materials for delivering the protocol (board games, handouts,
MP3s). The manual describes how the sessions, each conducted
consecutively over the course of six weeks, employ specific
practice worksheets, information gathering forms, and interactive
exercises designed to train youth in the preventive and coping
skills. The main activities defined in the manual were Daily Diary,
Relaxation, S.T.O.P, Worryheads board game, and STICs. Table 1
summarizes the protocol component steps and highlights
challenges in porting these steps to the mobile environment.
Table 1: REACH protocol components and gap analysis
REACH

Component Description / Design Challenges

Daily Diary

Self-monitoring
engagement; daily compliance; rich data entry

Relaxation

Pre-recorded audio exercises
media porting and translation

S.T.I.C.

Behavioral exposures with adult feedback
preserving steps; rewards; feedback

S.T.O.P.
Worryheads

Self-application of cognitive self-control plan
encouraging tool engagement through positive UX
Learn and practice cognitive self-control plan with
provided scenarios
detailed alternatives; increasing dosage; feedback

A round of stakeholder interviews involving the SMEs followed
the domain research of the REACH protocol. These included
working sessions between the design team leads and the SMEs,
visits by the SMEs to the design team’s lab, and synchronous
question-answer sessions over email and videoconferencing. This
step of the process addressed difficulties relating to understanding
the protocol and assumptions on both sides regarding
implementation objectives. This step took longer than expected,
with a result of inconsistent understanding of implementation
outcomes. The design team conducted an internal review to
identify root causes and come up with design process alternatives.
The causes identified included:
1. New terminology.
2. Gaps in understanding by the design team with respect to the
protocol.
3. Assumptions of the designers based on past implementations
of mHealth apps in non-preventative domains.
4. Ad hoc communications patterns between SMEs and the
design team, and within the design team itself.
5. A lack of understanding of the end user context.
Together, these issues are not uncommon in design processes, and
some were addressed (1, 3, 4) through simple awareness of the
issue in the team review. For example, improving ad hoc
communication patterns was improved through more frequent
design team meetings, clarifying the lines of communication with
SMEs, and reiterating design team understanding of requirements
back to the SMEs for validation.

Issues #2 and #5 were more significant. Issue #2 represents a
“blind spot” in design, due to factors such as missing information
implicitly understood by the SMEs but not apparent to the design
team. Issue #5 was a recognition that the design team did not
understand who would be using the app and in what context. At
this point the design team realized a more patient-centric approach
was required to overcome these design obstacles.

3.2 A Patient-centric Design Process
The design process described in the previous section focused on
translating a field manual; it is not surprising that the translation
had gaps derived from implicit knowledge assumed by the manual
authors and not understood by the designers. The software
engineering researchers suggested a more user-centric approach,
where the needs of the end user, in this case the patients, is the
focus of the design process. The gold standard for such a design
process is User-Centered Design (UCD), originally credited to
Norman and Draper [7]. UCD assumes a participatory design
process with end users, but for this research we prefer the more
inclusive definition of UCD as “the active involvement of users
for a clear understanding of user and task requirements, iterative
design and evaluation, and a multi-disciplinary approach.” [14].
ISO 9241-210 [4] identifies 6 principles to UCD (quote):
1. The design is based upon an explicit understanding of users,
tasks and environments.
2. Users are involved throughout design and development.
3. The design is driven and refined by user-centered evaluation.
4. The process is iterative.
5. The design addresses the whole user experience.
6. The design team includes multidisciplinary skills and
perspectives.
These principles were especially attractive to the design team due
to the uniqueness of the domain and protocol, and identified
issues understanding the end user context. The team realized the
app would not be a direct translation of the paper-based REACH
protocol, and needed to focus on context and end user experience.
There is a wide range of practices supporting UCD; the design
team utilized personas, prototyping with iterative feedback,
participatory design, and end user validation. The SMEs served
as participatory designers, eliminating the back-and-forth ad hoc
aspects of the initial process. They also served as proxies for the
end users during design as gaining access to youth (4th-5th grade
users for an extended time for intense design activities was not
possible). Access to end users would have certainly been
preferable during the design process but was not possible at the
time. However end user validation was emphasized before
approving the app for protocol trial; these results are reported in
section 5. Fortunately, prior domain research and SME interviews
from the gap analysis proved useful in the context of the UCD.

3.2.1 Personas
The design team started the UCD process by developing personas,
or proxies for categories of end users, and inviting the SMEs to
review them. The SMEs were not familiar with personas, and after
overcoming initial confusion about the technique, gained
enthusiasm and effectively provided useful feedback. The
personas shared with the SMEs are presented in Table 2.
Iterating over these personas led to several design insights that
were previously not understood by the design team. For example,
the design team came to understand subjects in this domain have a
higher need for re-assurance; respond well to attention and
approval, and are highly compliant (persona 2). Discussion of the

personas with the SMEs further revealed that in community
samples girls are more likely identified as “anxious” than boys,
and anxious youth fear the evaluative nature of social situations
(personas 3 and 4). After capturing a clearer idea about end user
context through discussing the personas created with the SME, the
design team started a phase of rapid prototyping to ensure the
SMEs provided frequent feedback on each design decision.
Table 2: REACH protocol components and gap analysis
Persona 1 Jacob is 10 years old, and is currently being raised by
his single mother. He was held back for behavior
problems as he tends to lash out when stressed. When
confronted with even minor change he shuts down,
and becomes irritable. His goal is to do as little as
possible, or just enough so he doesn't get in trouble.
Persona 2 Jessie is 9 years old and very shy. In larger groups of
10 or more people she panics, and is dangerously on
edge. She has a strong recognition of her symptoms,
and works very hard at overcoming them. Her goal is
to be free from required effort as soon as possible.
Persona 3 Mike is 12 years old. He finds it difficult interact in
groups. He thinks that everyone has prying eyes on
him and judging his every move. He loves to read
books and is distracted by day dreaming. He gets very
anxious and nervous in social situations.
Persona 4 Elizabeth is 10 years old. She is relatively overweight
and is embarrassed in evaluative situations. When her
classmates tease her, she cries and withdraws from
interacting with peers. This typically happens during
physical education and school games.

3.2.2 Rapid Prototyping
Rapid prototyping is an iterative design technique refining the
details of interaction models and overall user experience. Early
prototypes, or storyboards, focus on task sequences, or the
mapping of task workflows to interface screens. This leads to user
interaction modeling; the identification of user input actions
effecting transitions between screens or for the capture of critical
information. Later iterations refine these models and also layer in
thematic elements, until a final design is converged upon.
Iterations are meant to be short, frequent, and focused on
answering specific questions regarding the user experience.

3.2.2.1 Storyboarding and Clickthrough Prototypes
The design team used the freely available Pencil prototyping tool
to construct screen and clickthrough mockups. Clickthroughs take
simple screen mockups and overlay “hot regions” that advance the
mock to a new screen, simulating a user interaction. One
drawback is the tool runs its simulations in a web browser so tap
and swipe gestures are not supported; however, the tool does
support mobile UI “skins” to promote a look-and-feel consistent
with the mobile user experience. Figure 2 shows an example of an
early mockup created for S.T.O.P. activity.
The team created mockups of different scenarios in the app. Each
mockup was peer-reviewed within the design team, validated
against the documented protocol, and then presented to the SMEs
for feedback. The design was iteratively refined until the scenario
interactions were adequately captured, and the design team felt
comfortable moving to implementation on the Android platform.

Figure 1: S.T.O.P. Mockup in Pencil

3.2.2.2 Translating Protocol Components
As identified in the gap analysis (section 3.1), some protocol
components are a fairly straightforward translation, or port, to the
mobile app, while others are not. For example, the Relaxation
audio components were a straightforward port of the media to the
device wrapped with a simple consistent interaction metaphor. Of
course this component also requires the least user interaction of
any of the components. On the contrary, the Worryheads game is
a multiplayer board game involving cards. The app required
limiting the game experience to a single user compared to the
multiplayer board game. The design team replaced the physical
cards in the board game with preset “Situations” and “Thoughts”
screens. The user was then presented with a choice of four of
“Other Thoughts” options to choose from. Once the user selects a
choice from possible options a praise message was showed on the
screen to appreciate the correct answer. Screens depicting
Worryheads are shown in section 4.
A design concern in translating the protocol was the significant
amount of text a child is asked to input during activities such as
the Daily Diary and S.T.O.P. The mobile device is not suited for
textual input that goes beyond instant messaging or social media
apps, and further the end users are at an age where they are often
mobile-aware, but not proficient mobile typists. The fear was that
textual input would be skipped or significantly limited, or in the
worst-case cause frustration of the app to the extent children
would abandon it. The design team identified speech capture input
as a means to facilitate better information capture.

3.2.3 Injecting Innovations in the Mobile Experience
A challenge in applying mHealth concepts to existing clinical
protocols is the desire to innovate versus leveraging validated
protocol steps. For this project, the mobile platform provided the
means for increasing dosage by virtue of the device being everpresent. However, ubiquity is not enough, end users must be
motivated to practice the protocol. Engagement was addressed
through innovative features introduced in the mobile platform
including thematic and age-appropriate media, game strategies
(e.g. progressive reward incentives), and mobile notifications.

3.2.3.1 Designing an Appropriate Theme
A user interface theme refers to the consistent application of
stylistic elements such as images, fonts, audio or video media, and
user interface widgets (buttons, menus, taps, etc.). To gain
acceptance of the app amongst users familiar with the paper
protocol, the design team used the same theme used in the paper
protocol. The team ensured that color codes and the fonts used in
paper based protocol and the fonts used in the app are same. To

design the features of the app, the team studied the paper-based
versions of the activities to be performed by youth to get a better
idea of how to replicate the activities in the application. The team
followed the same nomenclature of the existing activities in the
screen designs reduce confusion and gain rapid acceptance.
The user experience required a gender-neutral, age-appropriate
proxy for the human guide who assists in the existing REACH
protocol. This proxy personifies the guide, providing instruction
and feedback to the end user through the mobile interface. Initial
ideas focused on themes such as “feed your pet” or “grow your
plant” but were rejected as being either too “babyish” for the
target age range or gender-biased.
The design team came up with the idea of an animated
motivational character in the form of a blob. The design team
referred to the character as “Bob the Blob” (Figure 3), but the
male name is never used in the app itself. Based on game design
concepts, “Bob” presents an age-appropriate, gender-neutral
proxy for protocol guidance and feedback [6][8].

3.2.3.2 Progressive Reward Incentives
While one of the goals of the REACH protocol is to empower
youth to be intrinsically motivated to enact the protocol, at the
training stage it is imperative to repeat the dosage faithfully in
order to attain this intrinsic motivation. A common gamification
technique is to employ leveled rewards as an extrinsic motivator
for performing a targeted behavior. Therefore a simple
progressive (leveled) set of rewards for extrinsic motivation
included in the app design. When an end user completes a task
from the REACH protocol they get a reward in the form of Bob’s
abilities/tricks. This way the user is motivated to follow the
protocol and completing the tasks (dosage) so s/he can unlock
more complicated tricks for Bob.
One concern SMEs raised during the design process was the
potential to inadvertently punish the child for not performing a
task. Given the domain, a design invariant was specified to keep
all interactions with the child positive; therefore, all language and
emotive expressions of Bob throughout the app were scrubbed to
ensure there were no negative connotations. For progressive
rewards, a setting in the app was designed to unlock new tricks
twice every week. The presence of these tricks also served as
extrinsic motivation for engagement.

3.2.3.3 Smartphone Notifications
Mobile platforms offer an “always on” communications channel
between service providers and end users. Most categories of
mHealth apps emphasize the communications channel between
clinicians and patients, or between patients and automated big
data platforms on the cloud. This project is unique in that it does
not leverage the mobile device as a communications channel. In
this generation of the app, the focus is on leveraging the device as
an information collector and dosage vehicle for the protocol. In
this sense the device serves more as a Personal Digital Assistant
(PDA) than as a connected mobile phone.
In this modality it is still important to present to the end user a
feeling of connectedness. The personification of Bob the Blob as a
proxy guide is one way the design provides this connectedness.
As a second design concept, the design team wanted to make use
of mobile notifications, but without relying on cloud-based push
notifications as these would require a persistent network
connection. Therefore the design supports local notifications
presented to the end user in both fixed and adaptive schedules.

Fixed schedules are daily time-based notifications, such as for the
Daily Diary, to complete a regular interval task. Adaptive
notifications require tracking end user interactions with the app
and dynamically determining whether to issue a notification to
engage with Bob the Blob again. The designers are concerned
with the notion of alarm fatigue through over-notification, though
currently the mobile device is given to the end users as a locked
down tool for practicing the protocol, and not as a generalpurpose smartphone for personal use.

3.2.3.4 Security and Privacy
Any mHealth app needs to be concerned with how user data is
stored, transmitted, and identified. These concerns can become
overbearing nonfunctional requirements on the app and down to
the underlying mobile operating system providing the
communication and storage services. At this stage of the app’s
development, it made more sense to de-identify data and work in a
locked-down, disconnected mode. There were several simplifying
assumptions the design team was able to make:
1. The emphasis on increased dosage over remote monitoring of
compliance or personal health measurements puts this project
in a different class of mHealth apps. Such apps push data to
remote providers (often via a cloud-based service) and
support human or automated communication reminders.
2. The relatively small number of participants in planned early
studies meant the devices, with a specific chosen version of
the mobile operating system, could be purchased and
distributed to end users. The design team selected a Motorola
phone running Android API version 19 (KitKat).
3. The relatively small number of participants makes it easier to
de-identify the data and manage it external to the app. A
secret user interaction combined with a password protects
access to functionality that supports exporting user
interaction and task completion data (see above).
Of course these assumptions will have to change in future
generations of the platform to facilitate broader adoption. But as a
dosage augmentation platform, the design team leveraged the
weekly visits with the psychologists combined with the
computational sophistication of modern smartphone platforms to
provide a self-contained solution.

4. APP IMPLEMENTATION
The Android platform was selected to support the app. The
openness of the Android platform, the availability of low-cost
devices, the ease of the Google Speech API, and the ability to
deploy the app without the involvement of an app store were the
deciding factors for the first generation of the app. This section
briefly describes the implementation on the Android platform.
The final user interaction model combined with scheduled
interactions per protocol rules is shown in Figure 2.
This timeline in Figure 2 is based on weeks one to six of the
REACH training program. Daily Diary, as the name suggests
needs to be made available daily for all the six weeks whereas the
Worryheads needs to be made available only in third, fourth and
fifth week of the training program.

When the user selects the app from the Android home screen, a
landing page is shown allowing the user to select from 5 available
activities (see Figure 3, upper left). At any time only activities that
are available can be selected from the landing page. Further,
activities that are overdue are highlighted by a soft gold pulsing
glow around the button (not shown) to provide a further visual cue
to the end user to perform an activity.
The S.T.I.C activity is shown in the upper right in Figure 3. In this
activity end users are encouraged to do a task they would
normally avoid due to their anxiety. In the paper protocol, once a
child completes the activity s/he receives a physical stamp from
an adult (usually a teacher or parent). In the app this was
implemented as a secret code entered by the adult, who could then
provide an electronic stamp of approval.

Figure 2: REACH App intervention Timeline

Landing page

S.T.I.C.

The S.T.O.P. activity (Figure 3, mid-left) asks the child to provide
responses to a set of questions (see section 2). Each response is
stored in a SQLite database on the device. Figure 3, mid-right
shows the “O” (Other Thoughts) step of the Worryheads game.
This is basically a variant of the S.T.O.P. activity with preselected “S” and “T”s. The child has to consider the given “S” and
“T” and select an appropriate “O” and “P” to complete the
simulation. At the conclusion of these activities Bob the Blob
praises the child (Figure 3, bottom right).
The Daily Diary (Figure 3, bottom left) is a scheduled activity
available to the child each day. The activity is available during
school hours but notifications are only given after school hours.
As described in section 2, the Daily Diary asks the child to reflect
on potentially anxiety-provoking events from her/his day, and
inquires about thoughts that came to mind in that situation. Youth
also rate how s/he handled and felt about the situation. This
embedded diary is part of the organizational framework of
REACH emphasizing the need to identify and confront anxietyprovoking situations that are threatening but manageable.
In addition to the 5 protocol activities available from the landing
page, the end user also can tap directly on Bob the Blob and be
taken to a table-oriented layout of “tricks” Bob can perform. The
tricks (animations) available at any time are based on the protocol
schedule as described in section 3.2.3.2.
Additional features were provided by the app to support research
outcomes (section 2). An on-device database stores all end user
responses, and tracks each user action. The latter will be used
after trials to answer research questions such as whether alarm
fatigue occurred, or end users were not sufficiently motivated to
engage with the app. A data export feature provided only to
interventionists allows data to be offloaded as csv files.

S.T.O.P.

Worryheads

Finally, in the face-to-face protocol trial, interventionists can
personalize dosage schedules or tailor training activities during
weekly visits. To support this in the app, a hidden feature was
embedded only for the interventionist role. A specific multi-tap
sequence combined with a secret PIN unlocks this feature so
interventionists can decide if a protocol component should be
enabled/disabled or otherwise modify the planned dosage for that
week. Additional settings include selecting the start date of the
protocol, notification time windows and frequency, the schedule
trick release, changing the teacher PIN, and exporting data.

5. VALIDATION
Daily Diary

Positive Reinforcement

Figure 3: REACH App Interaction Screens

The highly iterative participatory design process described in
section 3 enabled continuous feedback during app evolution. After
completing the initial candidate release version, the design team
and psychologists conducted two types of external validation. The
first was two feedback sessions with external SMEs from a school

advisory board (SAB). The second was a usability study
conducted with actual youth end users in the schools.

5.1 Advisory Board Feedback
The SAB consisted of two school psychologists with experience
delivering REACH, and two school district administrators who
oversee student services and prevention efforts for 47 K-8
schools. Based on their experience with youth, the SAB
considered the developmental appropriateness of the design and
program tools included (e.g., during the face to face sessions,
youth wanted to utilize Relaxation and play Worryheads ondemand, so those activities were selected for inclusion in the app).
From the SAB feedback, three issues emerged:
1. Safety and security - would youth have access to texting and
Internet on the devices?
2. Cost: would parents be responsible for the devices, if lost?
3. Flexibility - would versions of the app be available for the
iPhone, smartboards, and tablets?
The first issue was addressed by adding security software
SureLock to every device. The second was addressed by applying
procedures used by the school relevant to laptop computers where
parents are financially responsible. For flexibility, it was
determined that preliminary data is necessary prior to investing in
additional versions of the technology for different devices.

5.2 Usability Study
5.2.1 Participants
With parental consent (and assent from child), 22 youth (Mean
age = 9.67 years, 12 girls, 12 Hispanic/Latino, 5 White, 1 Black, 1
Asian, 3 “other”) from public schools participated in the ‘system
usefulness, satisfaction, and ease’ aspect of this research. The
median household income was about $39,000 and most youth
were recruited from the same zip code and class grades. In
addition, 77% reported knowing how to use an Android
smartphone and 54.5% reported playing games using a
smartphone “all the time”.

5.2.2 Measures
System usefulness, satisfaction, and ease were assessed via 22items from the Usefulness, Satisfaction, and Ease of Use
Questionnaire [4] modified for children and adolescents. Youth
responded to each item using a 10-point rating scale (1= “not at
all” to 10 = “very much”). System ease of use (SYSUSE) was
measured via 11 items (e.g., it is easy to use; it is simple to use),
quality of support information (INFOQUAL) was measured via 3
items (e.g., instructions and messages are easy to understand;
messages to fix problems are clear), system ease of learning
(SYSEASE) was measured via 4 items (e.g., I easily remember
how to use it; I quickly became good at it), and system
satisfaction (SYSSATIS) was measured via 4 items (e.g., I am
happy with this app; I would tell a friend about this app).
Consistent with the original measure, alpha reliabilities were
excellent: system ease of use (α = 0.92), quality of support
information (α = 0.83), system ease of learning (α = 0.92), system
satisfaction (α = 0.88), and stigma (α = 0.81) scale scores, and
overall usability score (α = 0.95).

5.2.3 Procedures
Parents (primary caregivers, legal guardians) received a letter
from the research team describing the nature of the study and the
timeframe for participation (within the next 7 to 10 days). From
those contacted, 26% provided child consent and every child
provided assent (n=22). Youth with consent/assent provided data

at a university laboratory or at their school. At the beginning of
the study, each youth was provided with an envelope that
contained a device and a questionnaire. After receiving the study
materials, three phases (1-Listen to the Relaxation; play
Worryheads game; 2-Write a daily-dairy or S.T.O.P. entry; 3-Play
with the Blob) were implemented by trained research assistants.
For a phase, each prescribed interactions with the app was 2minutes and responding to the survey lasted about 5 minutes. At
the end, youth were thanked for their participation in the study,
which lasted a total of 20 to 30 minutes. Parents of participant
youth were provided with $15.00 at the end of the study.

5.2.4 Results
Descriptive statistics and correlations for the focal variables are
given in Table 3. There were no missing data and some variables
exceeded conventional cutoffs of |2| for skewness and |7| for
kurtosis [16]: System Ease of Use (-3.04 skewness, 10.39
kurtosis), System Ease of Learning (-2.15 skewness; 3.9 kurtosis),
and System Satisfaction (-2.23 skewness; 4.53 kurtosis).
Moreover, statistically significant Shapiro-Wilks test values were
found for these indicators and thus subsequent tests were
conducted via non-parametric approaches. Specifically,
Wilcoxon-Mann-Whitney tests were conducted to estimate any
sex (boys vs. girls) or ethnicity/race (Hispanic/Latino vs. NonHispanic/Latino) variations in terms of: system ease of use,
quality of support information, system ease of learning, and
system satisfaction. No statistically significant mean differences
were found suggesting robustness across sex and ethnicity/race.
Table 3. Usability Study Results
Mean

sd

Median

Overall Usability 35.69

19.84

38.23

1. SYSUSE

8.94

1.48

9.24

2. INFOQUAL

9.13

1.28

9.67

3. SYSEASE

8.72

2.03

9.41

4. SYSSATIS

8.90

1.70

9.75

1

2

-- .61**
--

3

4

.92**

.47*

.80**

.53*

--

.48*
--

Note: Ranges from 0 to 40 for Overall Usability, 0 to 10 for other
variables; SYSUSE = system ease of use; INFOQUAL = quality of
support information; SYSEASE = system ease of learning; SYSSATIS =
system satisfaction; *p< .05; **p< .01

Given these findings, mean estimates for the total sample were
calculated and results showed that the REACH app system was
highly and positively rated, for the most part, along the four
dimensions of interest: system ease of use, quality of support
information, system ease of learning, and system satisfaction with
means ranging from 8.72 to 9.13. Also, as shown in Table 3,
statistically significant correlations were found among the four
dimensions with correlation coefficients ranging from .47 to .80
(p < .05). Lastly, transforming SUSE-Y overall total scores into a
traditional “grade” scale, analyses showed that the REACH app
system earned an “A” grade from 55% of youth, “A-” from 14%,
“B+” from 9%, “B” from 9%, and failing grades of “C-” or less
from 13% (or 3 youth). Focusing those youth who rated the
system with a “C-” grade or less, data showed that all three youth
reported no knowledge of Android operating system. One of the
three youth did not know how to connect the earbuds to the
phone, had trouble placing earbuds in his ears, asked what he is
supposed to press during the Worryheads, asked what the word
“respond” means, and did not know what to press during the
STOP task. Another seemed “lost” during Worryheads and the
third youth was distracted by SureLock pop-ups during testing.

6. DISCUSSION
Our multidisciplinary, collaborative efforts resulted in a
smartphone app to potentiate the prevention and early intervention
of childhood anxiety disorders and related problems. To our
knowledge this is the first research-based child anxiety prevention
and early intervention app with known usability ratings. The
FRIENDS for Life Program released an app for Android, but there
is no research relevant to the technology developed. In child
anxiety treatment, SmartCAT is a promising mhealth platform for
ecological momentary intervention, used as an adjunct to the
Coping Cat treatment program [11]. The REACH prevention app
appears to be more similar than different to SmartCAT whereas
the FRIENDS app is mostly psychoeducational. Focusing on
prevention, for example, REACH and FRIENDS provide ondemand opportunities for skill practice but REACH explicitly
focuses on reducing problematic anxiety at the indicated and early
intervention level as it includes focused and direct features
relevant to engaging youths in self-monitoring, in-vivo exposures,
and cognitive self-control. In addition, REACH is capable of
deploying notifications relevant to skill practice, offers tools for
personalizing and tailoring the protocol (e.g., increase
notifications, activate new tools based on performance, activate
tools parallel to the weekly focal module), and allows for
opportunities for corrective feedback based on user data amenable
to creating personalized reports of youth weekly practice and
response. When it comes to contrasting the SmartCAT treatment
app with the REACH prevention app, both yielded high “ease of
use” ratings. Moreover, as found in this research, the REACH
prevention app yielded overall high ratings along additional
dimensions not examined for FRIENDS or SmartCAT. That is,
REACH showed high ratings for quality of support information,
system ease of learning, and system satisfaction. Also, this
research found no significant differences between boys and girls
or between Hispanic/Latino and non-Hispanic/Latino youth on
any of the usability dimensions examined.
The REACH app appears promising and has the potential to study
questions not only relevant to potentiating program response and
refining aspects of the technology, but about large scale diffusion,
personalized care, and bridging the gap in health disparities when
it comes to affective problems and its related disease outcomes.
The version of the app described in this paper was designed and
created through a multidisciplinary process that is user-centered in
the broad interpretation of the process. Our subsequent plans for
the REACH app include incorporating patients, caregivers, and
interventionists directly into the design process, and broadening
its applicability to minority populations, populations with sleep
disorders, and studying the potential for positive remedies for
negative outcomes of anxiety, notably drug abuse.

7. REFERENCES
[1] Barrett, P., and Turner, C. 2001. Prevention of anxiety
symptoms in primary school children: preliminary results
from a universal school-based trial. Br J Clin Psychol, 40(Pt
4), 399-410.

[4] International Organization for Standardization, 2008.
Ergonomics of human system interaction-Part 210: Humancentred design for interactive systems (formerly known as
13407).
[5] Lund, M. 2001. Measuring usability with the USE
questionnaire.
http://ww2.stcsig.org/usability/newsletter/0110_measuring_
with_use.html
[6] Murray, T., Hardy, D., Spruijt-Metz, D., Hekler, E., and Raij,
A. 2013. Avatar interfaces for biobehavioral
feedback. Design, User Experience, and Usability. Health,
Learning, Playing, Cultural, and Cross-Cultural User
Experience Berlin Heidelberg: Springer
[7] Norman, D. A., and Draper, S. W. 1986. User-Centered
System Design: New Perspectives on Human Computer
Interacti. Hillsdale N.J. : Lawrence Erlbaum Associates.
[8] Pina, A. A., Silverman, W. K., Fuentes, R. M., Kurtines, W.
M., and Weems, C. F. 2003. Exposure-based cognitivebehavioral treatment for phobic and anxiety disorders:
Treatment effects and maintenance for Hispanic/Latino
relative to European-American youths. Journal of the
American Academy of Child & Adolescent
Psychiatry, 42(10), 1179-1187.
[9] Pina, A. A., Zerr, A. A., Villalta, I. K., and Gonzales, N. A.
(2012). Indicated prevention and early intervention for
childhood anxiety: A randomized trial with Caucasian and
Hispanic/Latino youth. Journal of consulting and clinical
psychology, 80(5), 940.
[10] Pinto, M. D., Greenblatt, A. M., Hickman, R. L., Rice, H.
M., Thomas, T. L., and Clochesy, J. M. 2015. Assessing the
Critical Parameters of eSMART-MH: A Promising AvatarBased Digital Therapeutic Intervention to Reduce Depressive
Symptoms. Perspectives in Psychiatric Care, n/a-n/a.
[11] Pramana, G., Parmanto, B., Kendall, P. C., and Silk, J. S.
2014. The SmartCAT: An m- Health Platform for Ecological
Momentary Intervention in Child Anxiety Treatment.
Telemedicine and E-Health, 20(5), 419-427.
[12] Silverman, W. K., Kurtines, W. M., Jaccard, J., and Pina, A.
A. (2009). Directionality of change in youth anxiety
treatment involving parents: an initial examination. Journal
of Consulting and Clinical Psychology, 77(3), 474.
[13] Silverman, W. K., Pina, A. A., and Viswesvaran, C. 2008.
Evidence-based psychosocial treatments for phobic and
anxiety disorders in children and adolescents. J Clin Child
Adolesc Psychol, 37(1), 105-130.
[14] Vredenburg, K., Mao, J., Smith, P. W., and Carey, T. 2002. A
Survey of User-Centered design Practice. Paper presented at
the Proceedings of the 2002 ACM Symposium on ComputerHuman Interaction (CHI 2002), Minneapolis, MN, April
2002., Minneapolis, MN.

[2] Chavira, D. A., Stein, M. B., Bailey, K., and Stein, M. T.
2003. Parental opinions regarding treatment for social
anxiety disorder in youth. J Dev Behav Pediatr, 24(5), 315322.

[15] Wang, J. T., Wang, Y. Y., Wei, C. L., Yao, N. L., Yuan, A.,
Shan, Y. Y., and Yuan, C. R. 2014. Smartphone
Interventions for Long-Term Health Management of Chronic
Diseases: An Integrative Review. Telemedicine and EHealth, 20(6), 570-583.

[3] Fisak, B. J., Richard, D., and Mann, A. 2011. The prevention
of child and adolescent anxiety: a meta-analytic review. Prev
Sci, 12(3), 255-268.

[16] West, S. G., Finch, J. F., and Curran, P. J. 1995. Structural
equation models with nonnormal variables: Problems and
remedies.

2009 Sixth International Conference on Information Technology: New Generations

WERCCS: A Client-side Workflow Enactment Service Using AJAX
Mahin Jeyachandran, Kevin Gary
Department of Engineering
Arizona State University
Mesa, AZ 85212, USA

Abstract

In this paper we present WERCCS (WorkflowEnabled Components on the Client-Side), a framework for
creating reusable client-side components for embedded
workflows in web applications. The goal of WERCCS is
to provide client-side embedded workflow that integrates
user responsiveness with a lightweight workflow
enactment engine. WERCCS leverages AJAX to build a
workflow enactment service that dynamically downloads
process definitions from the server based on user
interaction with a web application.

Embedded workflow engine map actions to sequences of
web screens. Client-side technologies such as AJAX
enhance the user experience of web applications. In this
paper we present WERCCS, a framework for Workflow
Enabled Reusable Components on the Client-Side.
WERCCS combines the best of server-side embedded
workflow ideas with AJAX client-side processing to
distribute lightweight process capabilities to the browser.
Key Words: Web framework, User experience

3. WERCCS Architecture Components

1. Introduction

Figure 1 shows the components of the WERCCS
framework and the sequence in which they get loaded.

The evolution of web application development
technologies has led to the recent popularity in server-side
embedded workflow frameworks. These frameworks
control the sequence of screens displayed to the end user
based on externalized process logic captured in a
configuration file. An example of such a technology is
Spring WebFlow [1], which defines screens and
transitions between screens in an XML file separated
from Java source code. In our view, this XML is
essentially a process definition file, and the Spring
WebFlow runtime a process enactment engine embedded
within the web application space.
While this type of server-side workflow evolution is a
welcome enhancement, it can become bogged down in
tedious coding details due to the variety of options
provided a user, not all of which are important to the
process flow. Furthermore, server-side frameworks,
despite separating process logic into files separate from
the code, often do not achieve true decoupling from web
application and presentation layer code due to the need to
read and write process variables from HTML elements.
These are problems historically encountered in
workflow environments on the desktop. The advent of
“fat client” technology with the PC allowed client
applications to shoulder some of the processing, such as
input verification, custom usability elements for different
users, and restricted view access to workflow relevant
data (you only see what you need to see). A client-side
technology for the web that could facilitate what the PC
did for the desktop client nearly two decades ago would
be a welcome complement to server-side technologies.
AJAX (Asynchronous Javascript and XML) represents
such a technology.

978-0-7695-3596-8/09 $25.00 © 2009 IEEE
DOI 10.1109/ITNG.2009.312

Figure 1. WERCCS components
1. HTML Document: structures a page and is dynamically
updated by scripts using data from the server.
2. Boot.js: a lightweight workflow enactment service that
handles user events and directs them to event handlers.
1597

3. Flow Index file: an XML file that contains a list of all
the interaction flow files used by the enactment service.
4. Flow files: resemble classic workflow definition files
by modeling user interaction with a process.
5. Javascript and Servlet: share conversational state with
respect to the executing process.
The interaction flow XML files (#4) are especially
important in that they define workflow steps and provide
handlers to user actions in the workflow context. These
files provide for 4 types of workflow state:

state to be executed (state chaining). If no states are
returned, the controller waits for a user event.

4. Related Work and Discussion
The “buzz” factor around AJAX is fostering new efforts
to create developer toolkits to ease the integration of the
technology into web applications [1][2].
The Google Web Toolkit (GWT) [3] and Echo2 [4]
focus on user responsiveness and alleviating developer
complexity by enabling UI code to be written entirely in
Java, and having the toolkit generate the needed scripts or
XML files. This addresses Javascript complexity issues,
and indirectly provides reuse by allowing Java developers
to code UIs using a familiar programming model.
Another project with the same goal but emphasizing
different technologies is the ZK project [5]. ZK uses the
XML User Interface Language (XUL) to describe the web
user interface and the corresponding event model. Events
are handled by the toolkit in a way resembling more
classic HTML and Javascript without using Javascript.
None of these client-side toolkits addresses the need for
embedded workflow to sequence web pages presented to
the end user. While some server-side toolkits, such as
Spring WebFlow address flow issues, they are inherently
bound on the server-side and are thus limited in user
responsiveness. While the combination of a server-side
embedded workflow technology with AJAX-supported
client-side processing seems reasonable, the authors are
unaware of other efforts to make this reality.
The framework described in this paper focuses on
client-side process-enabled reusable components with
server-side support to enable lightweight workflow
enactment. This provides several advantages, most
notably that flows can be easily chained and reused across
web applications. WERCCS main disadvantages stem
from its immaturity, resulting in performance issues if
flow XML files grow large, a lack of design best practices
on components, and an inability to support concurrent
execution between user actions and state processing.

1. Action state: used when user events can be handled in
the client-side without requesting the server for data.
2. Request state: makes a request to the server for data.
3. Decision state: This state makes decision regarding the
state to which control must be forwarded.
4. Load state: loads the next flow XML file from the
server, thereby concatenating 2 interaction flows.
The other component critical to the architecture is the
boot.js controller, which acts as a client-side workflow
enactment service. Figure 2 shows the controller design.

5. References
[1] SpringSource WebFlow, January 2009,
http://www.springsource.org/webflow
[2] L.D. Paulson, "Building Rich Web Applications with
AJAX," IEEE Computer, October 2005, 38(10):14-17.
[3] T. Noda and S. Helwig, "Rich Internet Applications"
November 2005, http://www.uwebc.org/opinionpapers
[4] Google Inc. The Google Web Toolkit. December
2008, http://code.google.com/webtoolkit
[5] NextApp Inc. The Echo2 Platform. December 2008,
http://echo.nextapp.com/site
[6] Potix Corporation. The ZK framework. December
2008, http://http://zkoss.org

Figure 2. Controller Architecture
The controller has handlers for each of the 4 states.
When the controller gets loaded, it loads the flow index
file, which in turn causes the first flow XML file to be
loaded from the server. Each of the states in the flow
XML file is parsed and loaded as state objects. Once state
objects get loaded, the controller starts executing the first
state by using the corresponding state handlers. The
controller executes states as long as handlers return a next

1598

The Software Factory: Combining Undergraduate
Computer Science and Software Engineering Education
John D. Tvedt
Roseanne Tesoriero
Kevin A. Gary
The Catholic University of America (CUA)
CUA and
UNICON, Inc.
3140 N. Arizona Ave.
Department of EE & Contputer Science Fraunhofer Centerfor
201 Pangbom Hall
Experimental Software Engineering
Suite 1 I3
Washington DC 20064 USA
College Park, MD 20740 USA Chandler, AZ 85225 USA
+I 480 926 2368
+ I 202 319 5299
+ I 301 403 8937
tvedt @ cua.edu
tesoriero @ cucr.edu
garyk@cua.edu
tvedt@orangebunny.com
rtesoriero @fc-md.umd.edu
garyk@ unicon.net
technology becomes obsolete, so does the students’
knowledge. Industry complains that current university
curricula fail to address the practical issues of real
software development 141 [51 [91 [151 [161. With current,
undergraduate, computer science curricula, it is rare for
students to encounter large-scale development requiring
teamwork, written and oral communication skills,
maintenance, management, and quality activities.
Additionally, traditional computer science courses do not
expose students to the non-technical issues that often drive
decision making in a real development environment.
Universities have attempted to address industry
complaints with curricula in computer science. Assigning
team projects in semester courses [61 1111 [7] [E], team
projects that span multiple semesters [8] [13], and
encouraging internships [14] are a few of the ways
universities have attempted to include practical experience
along with traditional coursework. However, project
courses and internships often begin late in a student’s
career and are typically short in duration. The exposure
does not provide the depth of experience to appreciate the
responsibilities of the roles and the implications of their
decisions on future development. Nor, do the students
have the opportunity to learn from their mistakes and
apply their experience to future projects.
The Catholic University of America (CUA) is a small,
private institution in the heart of Washington D.C.,which
has one of the nation’s fastest growing technology sectors.
This growth is fueled by an established government
services
market
and
a
rapidly
expanding
telecommunications market. Of course, much of the
technology developed for these sectors has a$ its core,
complex software.
Like many higher educational
institutions these days, CUA has difficulty retaining and
motivating students who see better immediate
opportunities in these markets. The challenge facing CUA
today is one echoing through the halls of Computer

Abstract
Industry ofren complains that current university
curricula fail to address the practical issues of real
s o f i a r e development. This paper outlines a proposal for
an innovative core curriculum for a Bachelor of Science in
Computer Science.
The proposed core curriculum
contains elements of traditional computer science
programs combined with s o f i a r e engineering via a teamoriented, hands-on approach to large-scale software
development.
In
addition
to
traditional
lecture/project/exam courses, students are required to take
an eight-semester sequence of “Software Factory ’’
courses. Software Factory courses put the students’ newly
acquired skills to work in a real sofnvare organization
staffed and managed by all students in the program.
Students from all courses in the Sojiware Factory
sequence meet simultaneously to fuljZl their roles in the
s o f i a r e organization. We expect the students will be
better-prepared software engineering practitioners afer
completing a curriculum that combines tradiiional courses
with practical Software Factory experience.

-1. Introduction
With the explosive growth of the Internet and the
permeation of software into nearly every aspect of our
lives, the need for qualified software developers to build
quality software is apparent. Industry would like
educational institutions to train future employees in the
latest technology.
However, the value system of
universities emphasizes long term education rather than
training in short term skills. While it may be tempting to
focus on the latest technologies to satisfy industry
demand, from a pedagogical perspective, doing so would
not serve students in the long term. Once the latest

633
0-7695-1050-7/01 $10.00 0 2001 IEEE

students in the Computer Science program. Software
Factory courses are facilitated by an instructor, but they
emphasize learning through real work-experience. These
classes meet twice each week. One class meeting lasts
one hour and is led by the instructorI”consu1tant.” During
this meeting, the “consultant” introduces concepts that are
relevant to the current work being performed in the factory
and addresses problems faced by the students at the
factory. The second class meeting lasts two hours.
During this second meeting, all Software Factory cla9ses
meet simultaneously in one location, thus fully staffing the
factory.
During this session, the instructor is a
“facilitator” who does not decide right or wrong (as in
traditional courses), but instead facilitates learning the
pitfalls and peaks in development processes. The
facilitator may perform various roles external to the
organization, such as “customer”, “patent agent”, “enduser”, “certification agent” (health-case, aviation, etc.),
and so forth.
The purpose of the Software Factory is to provide
students with practical experience in software
development.
The students should gain business
experience, as well as technical experience. It is important
for the students to be exposed to the constraints that
business decisions place on technological decisions. The
classroom space for the Software Factory simulates a real
working environment with cubicles, meeting rooms and
office equipment.
The projects for the Software Factory will be chosen by
the managers (senior computer science students). These
projects may reflect current trends in industry. For
example, the Internet has created unique opportunities for
students in the Software Factory to gain experience in both
new technologies and entrepreneurial areas. The students
could create e-commerce web sites to provide services and
software. One potential project for the Software Factory
would be to build and run an online auction site. The site
could be targeted at students, allowing them to auction
used textbooks, or other small items. Revenue from the
web site could be used to support the Software Factory.
Additional projects might be related to ongoing
research at the university. The students could negotiate
with faculty to develop software that would support their
research projects, thus, encouraging multidisciplinary
collaboration. The Software Factory could negotiate with
industry
for
projects
that
would
promote
universityhndustry partnerships.

Science departments throughout the country (if not the
world), “How can we make our curriculum meaningful in
today’s technology-driven world without compromising
the essential knowledge and training a student in computer
science must receive?”
The purpose of this paper is to describe CUA’s answer
to this challenge, the Software Factory. The Software
Factory combines traditional computer science
coursework with experience-based learning in which
students participate in the development of a real-world
software project. Each student gains experience in every
participatory role in the software lifecycle, from serving as
a requirements analyst to a software tester to a project
manager, and all roles in between. In this paper, we
present the Software Factory curriculum.
The main objectives of the Software Factory
curriculum are the following:
1. meet industry needs for producing computer scientists
familiar with today’s technology and processes;
2. ensure computer science students are given a solid
and lasting foundation in computer science by
providing an accredited computer science program;
3. attract and retain high quality students;
4. conduct empirical software engineering research; and
5 . encourage multidisciplinary collaboration.

2.

The Software Factory

The Software Factory concept combines traditional
computer science coursework with Software Factory
courses. The traditional courses cover the fundamental
topics of computer science (see Appendix A). Software
Factory courses expose students to large-scale, teamoriented development in a software development
organization staffed and managed by students under the
guidance of faculty. There are eight Software Factory
courses that combine to form a software development
organization. Each course represents a specific, software
engineering role or job within the development
organization. The eight semester sequence progresses the
students through the following roles: (1) Software Factory
process and tools trainee, (2) software system tester, (3 &
4) software developer and maintainer, (5) requirements
analyst and test planner, (6) software designer and (7 & 8)
software project manager. Students from all courses in the
Software Factory sequence meet simultaneously to fulfill
their roles in the software organization. The enrollment in
the program allows for multiple teams within the
organization.
Software Factory courses are hands-on courses that
require student participation in the Software Factory
throughout their undergraduate career. The Software
Factory is a software organization staffed and managed by

Software Factory Course SequencelDescription
Each course in the eight-semester sequence through the
Software Factory is described below.
1” Semester:
CSC 151: Software Development Process and Tools (3)
This course introduces students to the software
development process and the tools that support it.

634

Students learn about software processes, in general, as
well as the process in use at the factory. These students
learn about the roles and activities of the members of the
factory, such as developers, testers, QA, and management.
Finally, the students learn about and use the tools that
support the roles and activities at each different stage in
the software development process.

This course is a continuation of CSC451.

3. Discussion
We believe that the Software Factory concept meets the
objectives listed in Section 1. This section describes how
the design of the Software Factory meets each objective.

2ndSemester:

Meet the needs of industry
The needs of industry addressed by the Software
Factory design include: exposing students to new
technologies, teamwork, large-scale development,
management activities, maintenance activities, quality
activities and written and oral communication. By having
the students meet in one location simultaneously, we
simulate a real world, development organization. By
participating in the different roles of the Software Factory,
students are learning the skills needed by industry as well
as gaining an appreciation for their use in industry. For
example, configuration management can be taught in a
traditional course, but when a student is confronted with a
large-scale system, the need for configuration
management is better appreciated. The structure of the
Software Factory allows the course to be flexible and
adaptive to new technology.

CSC 152: Software System Testing (3)
In this course, students are put in the role of software
testers. Student responsibilities include: writing test plans
that test the requirements, writing test cases, running test
cases, documenting test results, and following the
documented process and standards.
j r dSemester:

CSC 251: Software Development and Maintenance I
(Code and Unit Test) (3)
In this course, students are put in the role of software
developers. Student responsibilities include: writing
software that adheres to the design, unit testing,
integration testing, documenting, performing peer reviews,
and following the documented process and standards.
41hSemester:

CSC 252: Software Development and Maintenance II
(Code and Unit Test) (3)
This course is a continuation of CSC251.

Create an accredited program
The coursework designed for the Software Factory
follows guidelines for software engineering education
[2][10][1] and meets the ABET computer science
curriculum requirements [3]. A mapping from the
requirements to the coursework is given in Appendix B.

ShSemester:
CSC 351: Software Requirements and Test Planning (3)
In this course, students are put in the role of software
requirements analysts and test planners.
Student
responsibilities include: meeting with customers,
analyzing customer requirements, writing a requirements
specification document, writing test plans, performing
peer reviews, and following the documented process and
standards.

Attract and retain quality students
Students often complain that they do not get enough
exposure to coursework in their major until later in their
academic careers.
The Software Factory concept
immediately immerses students into their area of academic
interest. By offering a unique program that gives students
an opportunity to have early exposure in their area of
concentration, we can attract quality students. With
exposure to the latest technologies and interesting projects
and opportunities, we hope to be able to retain those
students.

61hSemester:

CSC 352: Software Design (3)
In this course, students are put in the role of a software
designer. Student responsibilities include: writing a
design document that meets the requirements
specification, performing peer reviews, and following the
documented process and standards.

Conduct empirical software engineering research
The Software Factory will benefit the computer science
faculty as well. With a fully staffed Software Factory,
there will be multiple teams working within the
organization. Those teams could work independently to
develop distinct versions of the same product. This setup
would provide an opportunity to conduct empirical
software engineering studies. For example, an experiment
could be run to test the benefits of implementing software
inspections. Some teams in the Software Factory could
act as a control group while others could develop the same
product utilizing inspections.

YhSemester:
CSC 451: Software Project Management I (3)
In this course, students are put in the role of a software
project manager. Student responsibilities include: project
planning, resource allocation, project estimation, project
tracking, risk analysis/mitigation, personnel management,
SQA, and planning the future direction of the Software
f h

Semester:

CSC 452: Software Project Management II (3)

635

Factory will accept during the first few weeks of the
Fall semester. Upon acceptance, those projects will
undergo requirements analysis, test planning and design
by the junior class during the academic year. The
managers will be responsible for guiding and mea9uring
the development and testing of the projects for which they
wrote the requirements and design in the previous year.
The development and maintenance will be performed by
the sophomore class. The testing will be performed by the
freshmen class (second semester). The trainees (first
semester freshmen) will be assigned to one of the
developers as a mentor to learn about the process and the
tools used in the Software Factory. The trainees will be
exposed to all of the different software engineering roles.
Due to the length of the cycle time, other life cycle
models are being considered. However, we do not expect
to implement them initially.

Encourage multidisciplinary collaboration
Faculty are confronted often with the need for software
developers when working on and applying for research
grants. As with industry, often the supply does not meet
the demand. The Software Factory could provide
resources to support those grants. The students would be
exposed to problems from various disciplines. The end
result is an environment that encourages collaboration
among the departments of the university.

4. Implementation plan
Implementing the Software Factory concept requires
special consideration. In this section, we describe
alternatives for implementing the Software Factory and
give a concrete example of how we plan to implement it in
our environment.
The Catholic University of America is a small, private
university that uses the semester system. There is an
existing computer science program in place.
The
enrollment goal of the computer science program is to
have 25 new, freshmen students each Fall. Once we
achieve that goal, the Software Factory will have five
teams, each consisting of twenty students. These numbers
do not take into account attrition, however a small attrition
rate should not greatly impact the success of a team.
Attrition is one of the biggest problems in industry. It’s
another real world problem that the students must face.

Grading
The Software Factory courses are meant to be
participatory classes.
The management will be
responsible for writing performance reviews for each team
member, each semester. These performance evaluations
will be the basis for the students’ course grades assigned
by the faculty member. The managers are better-suited to
evaluate the team members than the faculty due to their
working relationship. By evaluating their team members,
managers will gain valuable experience in interpersonal
communication skills and writing performance
evaluations. The performance review will give the
managers leverage to motivate the team members.
Managers’ grades will be based on evaluations from
team members. This grading method provides checks and
balances between the management and team members.

Physical Environment
Each team will have its own laboratory session in
which the students on the team will staff the Software
Factory. The physical location for the Software Factory
laboratory must be able to accommodate 20 people. The
laboratory should be set up like an office space with
meeting rooms, cubicles equipped with computers and
miscellaneous office equipment and supplies.

Implementation Alternatives
Many universities already have an existing computer
science program in place. Therefore, it is important to
analyze the existing situation and choose an alternative
that provides the most benefits and least disruption to the
university and students. We have considered three
alternative approaches for implementing the Software
Factory in our environment: the incremental, big-bang and
sandwich approaches. The incremental approach is
defined as introducing the Software Factory in incremental
stages, starting with the freshmen factory courses
(Processmools and Software Testing). Each additional
year, more courses are offered until all of the courses for
the Software Factory are offered. The big-bang approach
is the opposite of the incremental approach. In this case,
all four years of Software Factory courses are offered
immediately. The sandwich approach is a compromise
between the incremental and big-bang approaches. With
this approach, in the first year, only the freshmen and the
senior (Management I and Management 11) courses are
introduced. In the second year, the full set of Software

Flow of Work
In our instance of the Software Factory, we will start
with a waterfall-like process model. Each project will
have a two-year cycle time (See Figure 1).
Year 1

Year 2

1

Tools.
Pr-s/

lTest

I

Figure 1. Two-year project work flow

636

,

Factory courses are offered. Each alternative has its own
set of advantages and disadvantages.

one can't expect full participation from the existing
students, some students will have to participate in order to
staff the SoftwareFactory fully.

Incremental Approach
Some of the advantages to implementing the Software
Factory courses incrementally include a reduced course
load and the possibility of smaller lab space requirements.
By only offering the freshmen courses in the first year,
only two additional courses are added instead of the full
eight courses required with full implementation of the
Software Factory. Additionally, since only the freshmen
would be involved, the university could incrementally
allocate resources and equipment to set up the lab.
Although there are some advantages to an incremental
approach, there are also several disadvantages. The most
serious concern is the amount of time it will take to be
able to evaluate the full benefits and drawbacks of having
the Software Factory. This approach would require four
years to fully staff the Software Factory. This approach
also delays using the Software Factory for
experimentation and research purposes. A more practical
issue is that although only two courses are introduced, the
work products for the freshmen to use for the testing
course in the second semester of the first year would have
to be developed or obtained. And, in each additional year,
the work products for all of the courses that have not yet
been introduced would have to be developed or obtained.
Implementing the incremental approach starting with the
senior year and working backward to the freshman year or
using one of the alternative approaches eliminates this
need.

'

Sandwich Approach
The sandwich approach, while reducing the initial
course load that is required by the big-bang approach, still
has many of the problems of the incremental approach. If
there are no developers in the first semester of the
Software Factory, a work product for the second semester
testers will have to be developed or obtained by the
faculty. This approach reduces by half the time to have a
fully operational Software Factory when compared to the
incremental approach, but it would still take two full
years. In addition, the faculty would have to develop or
provide work products for the testers in the second
semester of the first year and design documents for the
developers in the second year.
After considering the three alternatives, the big-bang
approach was considered to be the most desirable for our
situation. Using this approach will require some students
to incorporate some of the new courses into their existing
programs. We plan to count the Software Factory courses
as fulfilling current computer science electives. Students
entering into the Software Factory course sequence will
enter at a level based on a recommendation from their
academic advisor. Transfer students will be handled in
the same way as existing, computer science students.

5.

Evaluation Plan

We plan to use our objectives in Section 1 to guide our
evaluation plan. Based on our objectives, we would like
to answer the following questions:

Big Bang Approach
Introducing all of the new Software Factory courses
has several advantages over the incremental approach.
During the first semester, the students will be producing
the work products required for the second semester
courses. With this approach, the only work product that
would have to be developed would be a design document
for the developers to use for the first semester. Many
universities already offer courses in which a design
document is used or developed. These existing design
documents could be used for the developers in the first
year. Another advantage is that the second semester
courses can use the work products from the first semester.
This approach provides quick feedback since the Software
Factory will be staffed at all levels in the first year. This
approach also makes the Software Factory available for
experimentation and research more quickly than other
approaches.
There are some drawbacks to this approach. The most
serious is that introducing eight new courses may be
difficult for existing faculty to cover. One solution would
be to rely on adjuncts and lecturers to teach some of the
courses. This approach also assumes that current students
will want to participate in the Software Factory. While

Meet the needs of industry
1. Is industry interested in collaborating with the
Software Factory?
We will keep track of the number of inquiries from
industry. We will monitor the number of projects and
experiments completed for industry in the Software
Factory.
Create an accredited program
1. Does the new cumculum satisfy ABET curriculum
requirements for accreditation?
We have mapped the courses for the new curriculum to
the ABET curriculum requirements. The Software
Factory curriculum meets these requirements.
See
Appendix B.
2. Is industry happy with our graduates?
To address this question, we need to get feedback from
the employers and potential employers of our graduates.
3. Are our graduates trained in the technologies that
industry is interested in?

637

quality students. Finally, industry will benefit by gaining
access to students that have experience in a real world,
software development organization upon graduation.
The new curriculum was approved by the Department
of Electrical Engineering and Computer Science at The
Catholic University of America during the Spring of 2000.
We are working with other colleges and universities to
help them with the implementation of the Software
Factory curriculum.

To address this question, we will compare the
technologies used in the Software Factory with those that
are in-demand in industry.
Attract and retain quality students
1. Have the number of applicants in computer science

increased?
We will look at the total number of computer science
applicants each year over the 5 years prior to the
introduction of the Software Factory and compare it to the
total number of computer applicants each year since the
introduction of the Software Factory.
2. Have the GPA and SAT scores of applicants in
computer science increased?
We will look at the GPA, SAT scores and quality
ratings of computer science students for the 5 years prior
to the introduction of the Software Factory. We will
compare these scores and ratings to the GPA, SAT scores
and quality ratings of applicants since the introduction of
the Software Factory.
3. Has the retention rate been maintained or improved?
We will look at the retention rate of computer science
studen$ for the 5 years prior to the introduction of the
Software Factory. We will look for the same or better rate
of retention in each year since the introduction of the new
curriculum.

7. References
[l] Bagert D. J., A model for the software engineering
component of a software engineering curriculum.
Information and Software Technology,40,4, 195 - 201.
[2] Bagert D.J., Hilbum T.B., Hislop G., Lutz M., McCracken
M., and Mengel S. Guidelines for software engineering
education. Version 1.O. Technical Report CMWSEI-99-TR032, October 1999.
[3] Computing Accreditation Commission Acreditation B o d
for Engineering and Technology (ABET), 2000-2001
criteria for accrediting computing programs. November 1,
2000.
[4] Coulter N. and Dammann J. Current practices, culture
changes, and software engineering education. Computer
Science Education, 5, 2, 1994.21 1 - 227.
[5] Dawson RJ., Newsham, R.W. and Kemdge, R.S.
Introducing new software engineering graduates to the ‘real
world’ at the GPT company. Software Engineering Journal,
7, 3, (1992), 171 - 176.
[6] Dawson, R. and Newsham, R. Introducing software
engineers to the real world. IEEE Software
(Novembermecember 1997). 37 - 43.
[7] Easterbrook S.M. and Arvanitis T.N. Preparing students for
software engineering. In Proc. of the Third International
Workshop on Software Engineering Education (I WSEE3),
(Berlin, Germany, March 1996).
[8] Garlan, D., Glutch D.P. and Tomayko, J.E. Agents of
change: Educating software engineering leaders. IEEE
Computer, 30, 11,59 - 65.
[9] Gibbs N. The SEI education program: The challenge of
teaching future software engineers, Comm. of the ACM,
(May 1989), 594 - 605.
[ 101 IEEE/ACM Software Engineering Coordinating Committee,
Accreditation criteria for software engineering. Sep. 1998.
h ttp://computer.org/tab/Accred
10.html.
[ 113 Hilbum, T.B. Software engineering education: A modest
Proposal. IEEE Sofiare (Novmec 1997), 44 - 48.
[12] Homing, J.J. and Wortman D.B. Software hut: A computer
program engineering project in the form of a game. IEEE
Trans. on Software Engineering, SE-3, 7, (July 1997), 325 330.
[ 131 Moore, M. and Potts, C. Leaming by doing: Goals and
experiences of two software engineering project courses. In
Proceedings of the Seventh Software engineering Institute
Conference on Software Engineering Education, (San
Antonio, Texas, January 1994).

Conduct empirical software engineering research
1. Has the factory been used to conduct empirical

software engineering research?
The number of experiments completed in the Software
Factory will be monitored. We will also keep track of the
number of papers generated from work done in the
factory.

Encourage multidisciplinary collaboration
1. Are other departments interested in the factory?
We will keep track of the number of grants supporting
the factory, the dollar amount of funding for the factory
and the number of departments utilizing the factory.

6. Summary and Future Work
With current, undergraduate, computer science
curricula, students graduate with technical skills, but lack
practical software engineering skills needed in industry.
We believe the Software Factory concept will benefit the
students, the faculty, the university and industry. Students
will leam more and retain more by putting their newly
acquired skills to use in a real software development
organization. Faculty will have the opportunity to contract
out software to students running the Software Factory and
conduct software engineering experimentation. The
university may be able to attract a greater number of

638

[14] Powell, G.M., Diu-Herrera, J.L., and Tumer D.J.
Achieving synergy in collaborative education. IEEE
Sofrware (NovemberDecember 1997), 58 - 65.
[15] Shaw M. Prospects for an engineering discipline of
software. IEEE Sofirware (November 1990), 15 - 24.
[16] Wasserman A.I. Toward a discipline of software
engineering. IEEE Sofhvare (November 1996), 23 - 3 1.

xxx: Required Elective
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC 451: Software Project Management I (3)

t?" Semester:
xxx: Required Elective
xxx: Required Elective
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC 452: Software Project Management I1 (3)

Appendix A Course Sequence/Description
I"' Semester:
xxx: Required Elective
xxx: Required Elective
MATH 121: Calculus I(4)
CSC 131: Computer Science I (Java) (3)
CSC 15 1: Software Development Process and Tools (3)

Required electives: (44)
Science:
SCI xxx: 1" semester of a lab course (4)
SCI xxx: 2"dsemester of a lab course (4)
SCI xxx: Science Elective (3)
SCI xxx: Science Elective (3)

Semester:
xxx: Required Elective
xxx: Required Elective
MATH 122: Calculus II(4)
CSC 132: Computer Science II (Java) (3)
CSC 152: Software System Testing (3)

English:
ENG 101 (or 105,103,104): English Composition (3)
Religion:
REL 201 (or 203): Christian Difference (201 cannot be
taken 1'' semester freshman year) (3)
REL xxx: Religion Elective (3)
REL xxx: Religion Elective (3)

YdSemester:
xxx: Required Elective
xxx: Required Elective
CSC 21 1: Discrete Structures (3)
CSC 23 1: Data Structures (3)
CSC 251: Software Development and Maintenance I
(Code and Unit Test) (3)

Philosophy:
PHIL 362: Professional Ethics in Engineering (3)
Liberal Studies:
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)

4thSemester:

xxx: Required Elective
xxx: Required Elective
CSC 212: Theoretical Computer Science (3)
CSC 222: Computer Organization and Assembly
Language (3)
CSC 252: Software Development and Maintenance I1
(Code and Unit Test) (3)

Additional requirements
1. To be accepted as a major, a student must have
completed CSC 131, CSC 132, CSC 211, and CSC
231 with a minimum G.P.A. of 2.5 in these three
courses.
2. To ensure competence in the core material, all core
math and computer science courses must be passed
with a grade of C or better to satisfy the requirements
of the degree.
3. To ensure breadth in the choice of the six computer
science electives, at least one course, and no more
than two courses, must be taken from each of the
following areas: Computer Science Foundations (CSC
xlx), Computer Systems (CSC x ~ x ) , Software
Systems (CSC x ~ x ) ,and Computing Methodologies
(CSC x4x).

5""Semester:
xxx: Required Elective
MATH 501: Linear Algebra (3)
CSC 311: Design and Analysis of Algorithms (3)
CSC 331: Programming Languages (3)
CSC 351: Software Requirements and Test Planning (3)
6IhSemester:

xxx: Required Elective
MATH 531: Probability and Statistics (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC 352: Software Design (3)

ThSemester:
xxx: Required Elective

639

Appendix B Mapping to ABET curriculum requirements [3]
The table below contains section IV from the ABET requirements for accreditation. The standards are given on the
left and the evidence of the Software Factory curriculum on the right.

IV. Curriculum
Intent: The curriculum is consistent with program's
documented objectives. It combines technical
requirements with general education requirements and
electives to prepare students for a professional career in
the computer field, for further study in computer
science, and for functioning in modem society. The
technical requirements include up-to-date coverage of
basic and advanced topics in computer science as well
as an emphasis on science and mathematics.
Standards:
General
IV-1. The curriculum must include at least 40 semester
hours of up-to-date study in computer science topics.

~~

~~

Evidence:

63 semester hours
CSC 131: Computer Science I (Java) (3)
CSC 151: Software Development Process and Tools (3)
CSC 132: Computer Science I1 (Java) (3)
CSC 152: Software System Testing (3)
CSC 231: Data Structures (3)
CSC 251: Software Development and Maintenance I (Code
and Unit Test) (3)
CSC 212: Theoretical Computer Science (3)
CSC 222: Computer Organization and Assembly Language
(3)
CSC 252: Software Development and Maintenance 11 (Code
and Unit Test) (3)
CSC 31 1: Design and Analysis of Algorithms (3)
CSC 331: Programming Languages (3)
CSC 351: Software Requirements and Test Planning (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC 352: Software Design (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
'CSC 45 1: Software Project Management I (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC 452: Software Project Management I1 (3)
IV-2. The curriculum must contain at least30 semester 31 semester hours
hours of study in mathematics and science as specified Math:
below under Mathematics and Science.
MATH 121: Calculus I (4)
MATH 122: Calculus II(4)
CSC 21 1: Discrete Structures (3)
MATH 501: Linear Algebra (3)
MATH 531: Probability and Statistics (3)
Science:
SCI xxx: 1st semester of a lab course (4)
SCI xxx: 2nd semester of a lab course (4)
SCI xxx: Science Elective (3)
SCI xxx: Science Elective (3)

640

IV-3. The curriculum must include at least 30 semester
hours of study in humanities, social sciences, arts and
other disciplines that serve to broaden the background
of the student.

IV-4. The curriculum must be consistent with the
documented objectives of the program.
Computer Science
IV-5. All students must take a broad-based core of
fundamental computer science material consisting of at
least 16 semester hours.

IV-6. The core materials must provide basic coverage
of algorithms, data structures, software design,
concepts of programming languages, and computer
organization and architecture.

IV-7. Theoretical foundations, problem analysis, and
solution design must be stressed within the program’s
core materials.

N-8.
Students must be exposed to a variety of
programming languages and systems and must become
proficient in at least one higher-level language.
N-9. All students must take at least 16 semester hours
of advanced course work in computer science that

30 semester hours
English:
ENG 101 (or 105,103,104): English Composition (3)
Religion:
REL 201 (or 203): Christian Difference (201 cannot be taken
1st semester freshman year) (3)
REL xxx: Religion Elective (3)
REL xxx: Religion Elective (3)
Philosophy:
PHIL 362: Professional Ethics in Engineering (3)
Liberal Studies:
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)
LS xxx: Liberal Studies Elective (3)

45 semester hours
CSC 131: Computer Science I (Java) (3)
CSC 151: Software Development Process and Tools (3)
CSC 132: Computer Science 11(Java) (3)
CSC 152: Software System Testing (3)
CSC 231: Data Structures (3)
CSC 251: Software Development and Maintenance I (Code
and Unit Test) (3)
CSC 212: Theoretical Computer Science (3)
CSC 222: Computer Organization and Assembly Language
(3)
CSC 252: Software Development and Maintenance 11(Code
and Unit Test) (3)
CSC 3 11: Design and Analysis of Algorithms (3)
CSC 331: Programming Languages (3)
CSC 351: Software Requirements and Test Planning (3)
CSC 352: Software Design (3)
CSC 451: Software Project Management I (3)
CSC 452: Software Project Management II (3)
CSC 31 1: Design and Analysis of Algorithms (3)
CSC 231: Data Structures (3)
CSC 352: Software Design (3)
CSC 33 1: Programming Languages (3)
CSC 222: Computer Organization and Assembly Language
(3)
CSC 212: Theoretical Computer Science (3)
CSC 3 11: Design and Analysis of Algorithms (3)
CSC 351: Software Requirements and Test Planning (3)
CSC 352: Software Design (3)
CSC 331: Programming Languages (3)
CSC 131: Computer Science I (Java) (3)
CSC 132: Computer Science 11(Java) (3)
18semester hours
CSC xxx: Computer Science Elective (3)

64 1

provides breadth and builds on the core to provide
depth.

CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
CSC xxx: Computer Science Elective (3)
To ensure breadth in the choice of the six computer science
electives, at least one course, and no more than two courses,
must be taken from each of the following areas: Computer
Science Foundations (CSC xlx), Computer Systems (CSC
x ~ x )Software
,
Systems (CSC x ~ x ) and
, Computing
Methodologies (CSC x4x).

Mathematics and Science
IV-10. The curriculum must include at least 15
semester hours of mathematics.

17 semester hours
MATH 121: Calculus I (4)
MATH 122: Calculus II (4)
CSC 21 1: Discrete Structures (3)
MATH 501: Linear Algebra (3)
I MATH 531: Probability and Statistics (3)
IV-11. Course work in mathematics must include
I CSC 21 1: Discrete Structures (3)
discrete mathematics, differential and integral calculus, MATH 121: Calculus I (4)
MATH 122: Calculus II (4)
and probability and statistics.
MATH 531: Probability and Statistics (3)
IV-12. Thecurriculum must include at least 12
14 semester hours
SCI xxx: 1" semester of a lab course (4)
semester hours of science.
SCI xxx: 2"d semester of a lab course (4) .
SCI xxx: Science Elective (3)
SCI xxx: Science Elective (3)
IV-13. Course work in science must include the
SCI xxx: 1" semester of a lab course (4)
equivalent of a two-semester sequence in a laboratory
SCI xxx: 2"d semester of a lab course (4)
science for science or engineering majors.
IV-14. Science course work additional to that specified SCI xxx: Science Elective (3)
in Standard IV-13 must be in science courses or
SCI xxx: Science Elective (3)
courses that enhance the student's ability to apply the
scientific method.
Additional Areas of Study
IV-15.The oral communications skills of the student
ENG 101 (or 105,103,104): English Composition (3)
must be developed and applied in the program.
CSC 451: Software Project Management I (3)
CSC 452: Software Project Management II (3)
IV-16. The written communications skills of the
ENG 101 (or 105,103,104): English Composition (3)
student must be developed and applied in the program. CSC 352: Software Design (3)
CSC 351: Software Requirements and Test Planning (3)
CSC 451: Software Project Management I (3)
CSC 452: Software Project Management II (3)
IV-17. There must be sufficient coverage of social and PHIL 362: Professional Ethics in Engineering (3)
ethical implications of computing to give students an
understanding of a broad range of issues in this area.

642

Cooperating Process Components
Kevin A. Gary
Electrical Engineering and Computer Science Department
The Catholic University of America
Washington, D.C. 20064
garyk@cua.edu
Timothy E. Lindquist
Computer Science and Engineering Department
Arizona State University
Tempe, AZ 85287-5406
lindquist@ asu.edu

Abstract

cuting process models, analyzing them, and evolving
them. The proposed framework does not solve all of these
problems in and of itself, but instead provides a platform
by which automated process support issues may be
addressed in further depth. This research is part of a larger
vision of how processes are constructed, executed, monitored, and analyzed. This is just the first step, providing an
infrastructure for realizing this vision by considering
trends in software engineering that will effect process
technology in the foreseeable future.
The recent boom in technologies such as the Internet,
Java, Visual Basic, and CORBA has given rise to new
buzzphrases like “reusable software component”, “distributed object middleware”, “n-tier architecture”, and “ubiquitous client”. The implications (once you get past the
hype) these technologies hold for the future of software
development and deployment includes “plug and play”
software, “rent per use” software and “open marketplaces
for distributed components”. Current approaches to automated process support do not lend themselves to these
trends. Automated process support systems are large,
monolithic creatures that often require extensive knowledge of the users, tools, and data types in their environment. The systems are usually tied to some underlying
representation of process, and in that representation exist
dependencies that make componentization of the process
models themselves difficult. The research community has
begun to recognize and propose solutions to these problems. It is sufficient to state that current automated process
support systems are not the open systems that characterize
prevailing trends in software development.
Open Process Components (OPC)is a first step
toward realizing a component-based environment for auto-

Modern automated process support systems can be
considered monolithic in three ways. First, they model
processes top-down, usually from a single perspective,
such as the organizational perspective. Second, they are
process-centered, in that they often require extensive
knowledge of the users, data types, and applications in
their environment. Third, they tie process implementation
to a specific representation, making reuse and interoperability between process models difficult to achieve. This
paper describes the application of component-based techniques to process modeling across overlapping process
spaces. This approach, the Open Process Components’
approach, encapsulates process fragments as interoperable
and reusable process components. This paper motivates a
vision of cooperating components for automated process
support, presents an overview of the Open Process Components approach, and shows the application of this technique to Humphrey’s Personal Software Process.

1. Introduction
Automated process support is the application of computer technology to assist and automate work on behalf of
users. This includes applications of automated workflow[ 151 and automated software process support[9]. This
paper proposes a component-based framework for supporting process activities: building process models, exe-

1. This work was supported in part by the Office of Naval Research
under award number N00014-97-1-0872.

218
0-7695-0368-3/99 $10.00 0 1999 EEE

mated process support. It is an exercise in modularization
and object-orientation applied in the extreme to process.
OPC modularizes process models in order to define
boundaries between process fragments. It enumerates
objects in the process domain in order to encapsulate
information and behaviors in the process space. It “cornponentizes” processes by capturing collections of objects
that act together as a cohesive entity, and interact with
other such collections in well-defined ways. By componentizing process, the environment in which the process
acts is shielded from the complexity of specific process
representations. Dependencies between processes are not
eliminated, but moved to the boundaries between components where they are easily recognized and managed. In a
component-based environment, there is hope ideas such as
“plug and play processes”, “rent per use processes”, and
“open marketplaces for distributed process components”
can be realized. The Open Process Components vision is
of an environment of distributed process components,
where components can be located, adapted, and optimized
on demand with respect to a set of process requirements.
This paper motivates the need for reusable, interoperable process support that spans multiple process spaces,
and suggests a solution based on component-based process
support. Section 2 and Section 3 provide background and
motivation for a new approach to process automation. Section 3 overviews the component-based approach, suggesting that this technique is useful for realizing the vision
discussed in Section 3. Section 4 demonstrates this
approach on the Personal Software Process, and Section 5
provides some conclusions.

OPC represents a fundamental departure from traditional approaches in distributed, interoperable process
support. OPC promotes a fundamental shift from monolithic process-centered environments, to flexible processenabled environments. OPC is centered around the building blocks of enactable processes: work, people, and artifacts. Participants use their own tool sets and services to
do the work. Process participants accept work items in the
form of components, manipulate these components, and
pass them along to the next participant or activate subsequent components. Strict adherence to prescriptive process
definitions and tight integration of tools is not required.
From the user’s perspective, work items, or process components, are received from any number of sources and
integrated into the user’s personal work space.

3. Cooperating Components
In order to understand component-based process support, one must understand what it means to be a process
component, and how the traditional activities of process
modeling and enactment are performed in a componentbased environment. OPC offers the following definitions:
Process Component - a process component is an encapsulation of process information and behaviors at a
given level of granularity.
Component-based Process Model - a component-based
process model is a collection of process components
that interact in meaningful ways. A component-based
process model is itself a process component.
Component-based Process Enactment - componentbased process enactment refers to the creation of a
component, the dynamic evolution of the component,
its interaction with other process components, and its
interaction with users and an environment in which the
component is situated.
These definitions are elaborated on in this section.

2. Background
Research in providing distributed, interoperable process support focuses on (1) describing low-level infrastructure support for distribution, such as distributed
architectures[1][8][ 13][141 and distributed transactions[13,
or (2) distributed process control integration protocols[l][l1][13][17], or (3) distributed process data interchange protocols, usually in the form of a common
language for translation[7][161. In the context of current
research, OPC contributes new ways of performing control and data integration through component-based design.
The OPC approach is to support top-down process decomposition and bottom up synthesis via interacting components. Control integration is achieved by viewing a
process model as a collection of interacting components.
Data dependencies are reduced by encapsulating components behind well-defined interfaces, eliminating the need
for translating between process languages.

3.1 Process Components
A process component is an encapsulation of process
information and behaviors at a given level of granularity.
The process information is expressed in whatever formalism or language is used to represent the semantics of the
component. OPC separates this information three ways:
process schema, process states and transitions, and process implementation (see Figure 1). The process schema
defines a vocabulary for the component, identifying entities and relationships that have meaning for a particular
component. OPC defines a minimal, extendable, common
schema for components. Process state and transitions

219

sponding views. Additional meta-roles and views can be
defined on a per component basis.

3.2 Component-based Process Models

FIGURE 1. A process component
between states are represented as a finite-state machine.
The set of process states and valid transitions are defined
per component, with a common set of meta-states identified within OPC. The process implementation is the
underlying representation of the component and the
machinery which interprets this representation. This may
be a petri net, a process programming language, or whatever other means are used to represent process semantics.
It may also be a process tool that is wrapped to provide the
process implementation. The implementation of a component is encapsulated in the component, so that components
interact without requiring homogeneous representations.
A process component provides two additional
abstractions. One is a process type, which is a means of
categorizing processes. The schema, states and transitions,
and implementation define the object implementation of a
component, but they do not determine the type of process
a particular component represents. Two components with
the same exact schema, states and transitions, and implementation may represent two distinct processes, for example, a peer review process and a software development
process. Likewise, two components with different schemas, states and transitions, and underlying implementations may both be software inspection processes. In OPC,
a process type is defined for each component that indicates
what type of process the component represents.
The second additional abstraction is meta-views.
Meta-views define the allowable ways in which meta-roles
expect to interact with process components. A meta-role
identifies how the component is currently being used; for
definition, enactment, ownership, or observation. A view
is provided for each of the meta-roles that defines the way
a given meta-role interacts with the component. OPC
defines the four meta-roles just listed and their corre-

220

A component-based process model is a collection of
process components that interact in meaningful ways.
OPC provides mechanisms for component interaction
through well-defined interfaces, process events, and process messages. These mechanisms define a common con-?
tract by which all components abide. However, in order to
prevent from prescribing a contract that is too rigid, these
mechanisms are extendable and open.
A process model realized as a set of cooperating components is a step toward an environment of interoperable,
reusable process fragments. Components are combined in
an aggregation tree to support various levels of process
granularity (see Figure 2). A component-based process
model is itself a process component. OPC supports both
top-down modeling and bottom-up reuse. A top-down process tree is constructed by decomposing process components at coarser levels to finer-grained process parts.
Bottom-up reuse is supported by realizing a relation
between two or more process parts as components. A subprocess component serves as part of the implementation of
its parent process component. In terms of process granularity, a process component captures an abstraction of the
process space at a particular level. The fact that the component may be composed of subprocess components is not
exposed to the outside world.

3.3 Component-based Enactment
Component-based process enactment refers to component creation, dynamic component evolution, component interactions with other process components and with
its environment (including end users}. A process component in OPC is an active, ‘‘living’’ entity. It encapsulates its
own process schema, state, and implementation, and
allows dynamic modification of itself based on its own
implementation. For example, a process component may
extend its schema instance during enactment, or a compo-

FIGURE 2. A Process Component Tree

component defines the boundaries of process information,
while meta-views define ways in which components interact with the external world of users, tools, and middleware
platforms. In the sense that components interact with each
other in meaningful ways that define a contract between
components, meta-roles and meta-views define a contract
between process components and the outside world. A
component-supporting architecture provides the services
components require but cannot provide for themselves.
For example, components require support for registering
interest in, and receiving notification of, categories of
events. Components also require a repository for locating,
naming, and storing process components. While these can
largely be considered implementation issues, they do force
one to reconsider the traditional view of process support
architectures like the Workflow Reference Model [15] in
the context of component-based process support. The OPC
framework provides these types of services in support of
distributed process components.
This section has provided a brief overview of the general approach of OPC. Details of the framework support
for these concepts is beyond the scope of this paper. Further information may be found in [3].

FIGURE 3. Components and Contexts
nent may dynamically change its set of process states and
transitions. Dynamic component modifications are managed by the component itself, reducing reliance on other
components or global process information.
In OPC’s component-based approach, there is not a
strict separation between process model and process
instance. In that a component defines its own schema,
states and transitions, and implementation, it defines its
own process fragment. Process components evolve as they
are enacted, activating subprocess components and instantiating itself as binding information becomes available.
Hence each process component runs the gamut from creation to instantiation[2] during its active lifetime.
Process components interact with other components,
users, and an environment during enactment. Interactions
between components are provided through well-defined
interfaces, events, and messages. Components also define
interactions based on their relationship in the process tree.
For example, in OPC a subprocess component notifies its
parent process component when it changes state through a
process event. Parent processes request subprocesses to
perform actions through process messages.
Process components interact with users and the external environment through meta-views. As shown in Figure
3, a user interacts with a component by assuming a metarole. The user is part of an environment that defines a context in which the component is activated. The environment
provides the component with situation-specific information the component requires when interacted with under a
given meta-view. For example, tool invocation may be
required during process component enactment. The process component may determine what type of tool to invoke
and when to invoke it, but asks the environment for a specific tool to bind to at the given time.
Meta-views and environments are the bridge between
abstract process components and an architecture for distributing and interacting with real components. A process

4. Example: The Personal Software Process
The Personal Software Process (PSP)[4][6], developed by Watts Humphrey at the Software Engineering
Institute (SEI), is the application of a disciplined software
process at the individual level in software engineering.
The PSP is an entire discipline that structures the way
individuals develop software, monitor their work, and
improve their methods of performing tasks. The PSP
includes defined processes for guiding how developers
perform the PSP. This section introduces the PSP, and
applies the component-based approach to produce an
implementation of the PSP in OPC.

4.1 PSP Specification
Although the PSP discusses general process improvement principles for any individual process domain, it is the
Personal Sofrware Process, and as such recommends practices specific to software process development. These
practices include a process script for guiding the sequence
of individual activities.
The PSP introduces a fair amount of overhead into the
daily activities of software developers by adding estimating and recording activities. “Overhead” does not imply
these activities are not worth doing; they are precisely the
activities which provide the value-added benefits of the
PSP. However, they are activities that software developers
do not usually perform on a daily basis. Humphrey

221

implementation of the PSP supports these products as part
of the schema of the components in Figure 4.
A component-supporting environment for OPC has
been implemented in Java. This environment includes
mechanisms for component creation, modification, extension, enactment, monitoring, reuse, and interoperability.
The components in Figure FIGURE 5. are defined within
this environment to provide PSP support. The PSP process
tree in the OPC environment is shown in Figure 5.

acknowledges the extra time requirements and tediousness
of these activities, but argues that in the end performing
these activities actually saves time and increases quality.

4.2 Automating the PSP
The time consuming and tedious tasks the PSP adds to
a personal software development process are ideal candidates for process automation. Humphrey realized as much
and created a specification for automating the PSP [ 5 ] .
The specification is quite large and detailed, specifying a
dataflow between forms presented in [6]. The implementation described here is based on the process scripts specified in [4]. Process automation can reduce the time spent
on these tasks by automatically recording the various
kinds of tracking data the PSP requires. More importantly,
automated process support can ensure that the individual
consistently and faithfully follows the PSP. This section
presents the implementation of the PSP as a set of process
components based on the process script presented in [4].
The PSP is modeled as a set of cooperating components in
OPC that form a process tree as shown in Figure 4.
The PSP specifies a number of forms for collecting
process-related data during process enactment. These
forms are associated with PSP activities tracking time and
defects, and provide input for the PSP Project Plan Summary. PSP enactment also involves the usual products of a
software development process such as requirements documents, design documents, and source code. The OPC

FIGURE 5. PSP process tree in OPC

pl
(ordered)

1

I

planning
Phase
(parallel,
enactable)

I

I

Design
Phase

Coding
Phase

(ordered)

(ordered)

I

I
'0 e
R~v~ew
Phase
(ordered)

I

I

Compile
Phase
(ordered)

Testing
Phase
(ordered)

\.Fix..,
FIGURE 4. PSP process tree

222

1

Postmortem
Phase
(parallel,
non-enactj

ified March 5 1996.

5. Summary
Open Process Components provides a framework for
component-based support for automated software processes. Component-based process support addresses some
of the restrictive characteristics of existing process environments. It allows processes to be viewed as dynamically
interacting components that can be reused across process
models. These components are also interoperable in the
sense they allow process fragments created in heterogeneous process tools to interact with each other without
requiring a translation from one process representation to
another. The result is an open, process-enabled environment as opposed to a closed, monolithic one.
This paper has presented the vision Open Process
Components, provided an overview of the foundation of
the approach, and described an implementation of components for the Personal Software Process. OPC has been
implemented in Java, providing capabilities for creating
dynamic, reusable process components. Our plan to use
this environment to implement repositories of components
for a variety of software processes. For example, in addition to the PSP, the ISPW-6 Software Process Scenario has
also been implemented in OPC [3].
The Open Process Components framework is the first
step toward a vision of distributed, dynamic process components. Powerful new functionality, such as dynamic
lookup of components (brokering) and dynamic scheduling of components can be investigated using this framework. Research is already under way on applying
brokering capabilities for process components [ 121.

16.1

Humphrey, W. A Discipline for Sofnvare Engineering.
Addison-Wesley,Reading MA. 1995.

17.1

Lee, J., Gruniger, M., Jin, Y., Malone, T., Tate, A., Yost,
G., and the PIF Working Group, The PIF Process Interchange Format v.1.2. Available at http://ccs.mit.edu/pif,
December 8 1997. Also published in The Knowledge‘Engineering Review, wol13(1), pp. 91-120. CambridgeUniversity Press. March 1998.

v.1

Miller, J.A., Sheth, A., Kochut, K.J., and Wang, X.Corbabased Run-timeArchitectures for Workflow Management
Systems. Journal of Database Management,Vol. 7, pp.
16-27. 1996.

19.1

Osterweil, L. Software Processes Are SoftwareToo. Proceedings of the 9th International Conferenceon Software
Engineering (ICSE 9). IEEE Computer Society Press.
Monterey, CA. 1987.

6. References

113.1 Swenson, K. SimpleWorkflow Access Protocol (SWAP).
Internet Draft Standard of the Internet Engineering Task
Force (IETF). Available at http://www.aiim.org/wfmc.
August7 1998.

11.1

12.1

111.1 PCIS2Working Group. PCIS2Architecture Specification,
Version 1.0.Lindquist, T. (4.). Available at http://
pcis2.nosc.mil. January 1998.
112.1 Sauer, L. Brokering Process Components.Ph.D. Dissertation Proposal, Department of ComputerScience, Arizona
State University. December, 1997.

Ben-Shaul, I. and Kaiser, G. An Architecture for Federation of Process-Centered Environments.Journal of Automated Software Engineering, vol. 5 , no. 1, pp. 97-132.
January 1998.

114.1 Wallnau, K., Long, F., and Earl, A. Toward a Distributed,
Mediated Architecture for Workflow Management. Proceedings of the NSF Workshop on Workflow and Process
Automation in Information Systems. May 1996.

Conradi, R. Femstrom, C., Fuggetta, A. and Snowdon,R.
Towards a Reference Framework for Process Concepts.
Proc. of the 2nd European Workshopon SoftwareProcess
Technology (EWSPT’92),pp. 3-17, Trondheim, Norway.
September 1992.

13.1

Gary, K. Open Process Components.Ph.D. dissertation,
Department of Computer Science, Arizona State University. January 7, 1999.

14.1

Humphrey, W. Introduction to the Personal Sofhvare
Process. Addison-Wesley,Reading MA. 1997

15.1

110.1 Osterweil, L. Software Processes Are Software Too, Revisited An Invited Talk on the Most Influential Paper of
ICSE 9. Proceedings of the 19th International Cnoference
on SoftwareEngineering. pp. 540-548. Boston, MA. May
1997.

115.1 The Workflow Management Coalition. The Reference
Model. WfMC Document Number TC00-1003, January
1995.
116.1 The Workflow Management Coalition. Interface 1: Process Definition InterchangeProcess Model. WfMC Document Number TC-1016-P, Version 7.05 Beta. August 5
1998.
~ 7 . 1The Workflow Management Coalition. Interoperability
Abstract Specification. WfMC Document Number TC1012,Version 1.0. October 20 1996.

Humphrey,W. A Specificationfor Automated Supportfor
the PSP. Available at http://www.sei.cmu.edu. Last mod-

223

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann
Computer Science and Engineering Department
Arizona State University
Tempe, Arizona 85287-5406
Jean-Claude Derniame
Laboratoire lorrain de Recherche en Informatique et Applications
LORIA : Bd des Aiguillettes
BP 239 54 506 Vandoeuvre Cedex
Abstract
Only recently has the research community started to consider how to make software process models interoperable
and reusable. The task is difficult. Software processes are
inherently creative and dynamic, difficult to define and
repeat at an enactable level of detail. Additionally,
interoperability and reusability have not been considered
important issues. Recent interoperability and reusability
solutions advocate the development of standard process
model representations based on common concepts or
generic schemas, which are used as a basis for translating
between heterogeneous process representations. In this
paper we propose an alternative approach through the
development of process-based components. We present the
Open Process Components Framework, a componentbased framework for software process modeling. In this
approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of
process representations, an explicit representation of process state, and an extendable set of class relationships.

1.0 Introduction
Since Osterweil’s proposal[12] for automating the software process a decade ago, there has been significant
debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets
[9], rule-based formalisms [1,8,13], process programming
languages [15], event-based representations [3,6,10],
object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research
under Award number N00014-97-1-0872

their benefits, systems based on these formalisms create
enactable process models which are not interoperable nor
reusable with one another. The prevailing solution is to
advocate an intermediary standard process representation
and provide translations for interoperability and reuse. We
do not believe this approach is scalable and defeats the
purpose of using heterogeneous process representations.
We advocate an object-oriented, component-based philosophy for providing software process interoperability and
reuse. This paper presents Open Process Components, a
component-based framework for software process definition and enactment. In this framework, components are
well-encapsulated representations of process entities that
interact in meaningful ways. The framework is solidly
founded on mature concepts in the software process field,
and yet is extendable so that process models may be customized in a particular domain. A componentized view of
process representations results in easier process definition,
modularized process enactment, natural interoperability,
and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable.
Instances vary according to constantly changing demands
of specific projects. Fully elaborating a software process
model to an enactable level of granularity is often too
tedious, time-consuming, and costly[4].
Motivated by the need for interoperability and reuse, we
advocate applying component-based techniques to software process modeling. Constructing software process
models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined
components.

A component-based approach:

2.1.2 Process Component States

•
•
•
•

The elements of the meta-model appear in most process
models. Each model requires a different enactment service
to interpret the representation and execute the process.
Regardless of the formalism employed and the interpreter
used, all models define actions on the entities within the
process domain, which effect the states of those entities.
The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes
finite state machines as part of its basic abstractions.
Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for
process modeling is not unique[7,11]. The OPC framework
defines a basic set of states and transitions for Process and
Activity components. These include states such as executing, suspended, and aborted, with corresponding transitions between states defined by actions such as
startProcess, suspendProcess, and abortProcess.
Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object’s unique behavior when
interacting with other components within the framework.
The current class definition for state transition graphs
include operations to add and remove states and allowable
transitions between states, making a component’s state and
behaviors affecting state explicit and manipulable.

avoids deep integration of semantic models
handles the natural complexity of software processes,
responds to dynamic software processes, and
facilitates reuse, minimizing one-shot process models.
Component-based process modeling requires a framework
for developing components. The framework must identify
process entities, define meaningful interactions between
entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides
a foundation for developing, integrating, maintaining, and
reusing a variety of process representations. The framework defines basic abstractions of the problem space that
can be specialized. Yet, the framework must make some
commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon
three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a
per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this
meta-model as a basis for translation between process
models, we use it as a foundation for identifying elements
of the process space for componentization, and for defining
meaningful ways in which process components interact.
has_sub

role

activity
assigned_to

can_perform

has_input

product

has_output
consists_of

has_version
has_variant

agent

process

has_sub

FIGURE 1. The Open Process Components Framework
Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and
Agent), and artifacts (Product). The meta-model defines
the “rules of engagement” for components. It identifies
what component types interact with what other component
types under what relationships. These relationships are not
static; process components and component relationships
are highly dynamic during the course of the component’s
life cycle.

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a
layered, three-tier software architecture (Figure 2). The
Framework Layer defines classes and interfaces modeling
process entities derived from the OPC meta-model. The
Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component
Layer extends representations to particular domains. It is
from this layer that actual Process component objects are
instantiated. A process model in the OPC framework is a
set of components, realized as objects of Component Layer
classes, and a set of relations between those components,
created under the constraints of the Framework Layer,
implemented using Representation Layer semantics.
Figure 2 shows example classes at each layer of abstraction
for the meta-model element Process. The Framework
Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation
Layer is comprised of class definitions for specific process
representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets
(PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any
other representations we wish to encapsulate. The Compo-

Process

Framework
Layer

Process
Representation
Layer

PDLProcess

ECAProcess

OrderedProcess
PetriNetProcess

Component
Layer
Bug Fix

RuleProcess

Code Module
Code Module

Integration Test

Design

Peer Review
Stress Test

FIGURE 2. Object-oriented class diagram for Process
components

nent Layer contains type definitions for actual process
types. The dashed lines between layers in Figure 2 denotes
that the Representation and Component Layers in fact can
have many levels. This allows for multiple ways in which
to extend and specialize the framework.
The first step identifies a base set of classes and interfaces
at the Framework Layer. The next step is to construct
encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are
encapsulated behind the interfaces inherited from the
Framework Layer. For example, the implementation may
come from a COTS process tool. Finally, components
defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used
since inheritance would tie the component’s type to its
implementation. Component Layer objects are configurable. Component Layer classes represent generic process
models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product,
Role, or Agent) fully specified and bound is part of an
instantiated process model[5].
Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given
representations. Delegating Representation Layer classes
provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between
components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi’s levels of process specialization[5].

3.0 PCIS2 Process Services
The Open Process Components framework is currently
used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

software development support. PCIS2 services include
Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA.
The process support services in PCIS2 are based on the
OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification,
known as jFlow[11], submitted to the OMG. The jFlow
specification is largely an “object-ization” of existing
WfMC interfaces[16]. This is not a drawback, but one of
the strengths of the OMG’s approach to adopting and
adapting existing technology. The jFlow specification
improves upon the original WAPI specifications by defining appropriate interactions between objects to gain
interoperability and maintainability of workflow systems.
The PCIS2 specification is object-oriented from the ground
up, but has borrowed some of the jFlow concepts in order
to maintain compliance with emerging standards.
PCIS2 and the jFlow specification differ in three areas.
First, PCIS2 supports dynamic processes through ad hoc
process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared
and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2
incorporates support for the metaprocess, by defining
views on its services for controlling, defining, performing,
and monitoring processes. jFlow only defines interfaces for
performing (enacting) and monitoring workflows.
It should also be pointed out that jFlow identifies concepts
not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these
specifications, they are largely complementary and both
provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The
authors propose the construction of software process components for producing process artifacts. A “software process component” is essentially a process model fragment
written in some Process Modeling Language (PML). Components are dynamically combined to construct complete
process models through interface types and their respective
“connectors ports”. The authors correctly motivate the
need to eliminate monolithic process systems and instead
provide reuse and integration capabilities for process representations. However, the approach lacks adherence to
foundational concepts, such as those used in OPC (see
Section 2.1). The three-tier layering of the OPC framework
provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite
their differences, the Pynode component approach is simi-

lar in philosophy and motivation to the OPC framework,
and appears to be at roughly the same level of maturity.
Results of these two experiments will be very useful to the
software process modeling community.
A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14].
The authors view the object space and the process space at
different levels. The object space is data-oriented, whereas
the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that
brings together the object level and the process level
together. Shams-Aliee and Warboys[14] also advocate
modeling a process as a collection of objects or components. However, we find the distinction between the object
level and the process level unnecessary. In particular, we
do not agree that the object level is a data-oriented model.
In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of
these objects as defined by their interfaces. OPC merges
objects and processes into components through an explicit
representation of process state contained in the component.
We propose a full object-oriented framework that includes
class definitions, inheritance, and rules for component
interaction. This merging of objects and processes into a
complete component-based model allows OPC the full
potential to achieve interoperability and reuse by being
independent of any process modeling formalism.

Software Process (ICSP4). December, 1996.
[3.]

Ben-Shaul, I. and Kaiser, G. An Interoperability Model for
Process-Centered Software Engineering Environments
and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995.

[4.]

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T.,
Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the
NSF Workshop on Workflow and Process Automation in
Information Systems, Athens, GA, May, 1996.

[5.]

Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R.
Towards a Reference Framework for Process Concepts.
Proc. of EWSPT’92, pp. 3-17, Trondheim, Norway. September 1992.

[6.]

Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling
and Technology, A. Finklestein, J. Kramer, and
B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994.

[7.]

Derniame, J.C. Life Cycle Process Support in PCIS. Proc.
of the PCTE ‘94 Conference. 1994.

[8.]

Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994.

[9.]

Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net
Based Software Process Modeling Language. Proc. of the
6th International Workshop on Software Specification and
Design, Como, Italy. September 1991.

[10.]

Melo, W.L. and Belkhatir, N. TEMPO: A Support for the
Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers,
North Holland. 1994.

[11.]

Object Management Group. jFlow Joint Submission.
OMG Document Number bom/98-06-07. July 4, 1998.

[12.]

Osterweil, L. Software Processes are Software Too. Proc.
of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987.

[13.]

Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the
14th International Conference on Software Engineering,
pp. 262-279. May, 1992.

[14.]

Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings
of the First World Conference on Design and Process
Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for
Design and Process Science, Austin, TX. December 1995.

[15.]

Sutton, S., Heimbigner, D., and Osterweil, L. Language
Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium
on Software Development Environments, Irvine, CA. December 1990.

[16.]

Workflow Management Coalition. The Reference Model.
WfMC Document Number TC00-1003, January 1995.

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software
processes. This framework identifies common concepts in
the research community and defines an object-oriented
framework for applying these concepts. This framework is
currently employed in the construction of a software architecture for support distributed software development.
This approach, together with related efforts in the field of
workflow, makes the important contribution that the software process automation field is maturing to the point that
efforts such as the one described herein can be attempted.
Despite whether the reader agrees with the design of this
framework, providing interoperability and reusability will
overcome one of the serious hurdles preventing wide scale
deployment of software process automation technology.

6.0 References
[1.]

Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G.
PEACE: Describing and Managing Evolving Knowledge
in Software Process. Proc. of EWSPT ‘92, Trondheim,
Norway. September, 1992.

[2.]

Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified
Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

2012
2012 Ninth
Ninth International
International Conference
Conference on
on Information
Information Technology
Technology-- New
New Generations
Generations

A Tool for Teaching Risk
Santosh Rajendran, Kevin Gary

Harry Koehnemann

Department of Engineering
College of Technology & Innovation, ASU
Mesa, AZ USA
{sthenmoz, kgary*}@asu.edu

321 Gang Inc.
Scottsdale, AZ USA
harry@321gang.com

Abstract— Students tend to think optimistically about the
software they construct. They believe the software will be
defect free, and underestimate apparent risks to the
development process. In the Software Enterprise, a 4-course
upper division project sequence, student team failures to
predict and prevent these risks lead to various problems
like schedule delays, frustration, and dissatisfaction from
external customer sponsors. The Enterprise uses the IBM
Rational Jazz platform, but it does not have a native risk
management capability. Instead, project teams were
recording risks
associated with their projects on paper
reports. To facilitate maintaining and managing the risks
associated with their projects, we developed a risk
management component in the Jazz environment. This
component complements Jazz by providing features of the
risk management process like risk control and monitoring. The
risk management component was used and evaluated by
student capstone project teams.

status report. Software risk management, as a vital practice
must be updated more frequently so teams can view their
project’s status and address risks daily. But the creation of
paper based status reports involved a considerable amount
of effort put into it which the team can’t afford with the
given time span for complete project development.
II.

Software Enterprise pedagogy is a hybrid project-based
method where students work through modules dedicated to
core concepts in software engineering. Students prepare,
discuss, practice, scale and reflect on each module in a
relatively short period of time (3 weeks). The prepare and
discuss activities most closely resemble traditional
classroom activities. Students may, for example, read a
paper or conduct some preliminary discovery exercise and
then come to class to discuss the concept. There is a postprepare but pre-discuss online assessment task. Practice is
essentially a lab session, and at the end of the lab meeting
students are asked to reflect on whether practice matches the
expectations formed during practice and discuss, and form
expectations for incorporating the technique in their
projects. Prepare, discuss, and practice all take place
typically within one calendar week. Then, in the 3-week
project sprint, students must incorporate the technique in a
complex industry-sponsored project (scale and context) and
then reflect on whether the application of the technique in
context matched their expectations stated after practice.
The Software Enterprise module on risk management
asks the student to perform the prepare activity by reading
and taking an online assessment on Boehm’s 1991 paper
"Software Risk Management: Principles and Practices" in
IEEE Software [2]. In this paper he explains the importance
of risk management in the software project and steps
involved in it. Some of the steps and concepts mentioned in
his paper are described in the next section. This module is
presented as the first module of the fourth course in the
Software Enterprise sequence titled Software Product and
Process Management. Students are required to identify and
address risks through to the completion of their project.
Fidelity to the process is assessed as part of course grading.
Based on Boehm’s risk management process, Rand
Corporation developed a “Guide for the Management of
Expert Systems Development” [12]. This guide describes
risk related activities for each phase but did not recommend
any tools. The Software Engineering Institute designed a

Keywords- risk, tools, software engineering education

I.

INTRODUCTION

The Software Enterprise [9] is a four semester sequence
of courses designed to expose students to the real world
challenges in software development. In the "Project
and Process” course, students learn and practice project
management techniques like project tracking through Earn
Value Analysis (EVA) and Burn down Analysis (BDA),
project estimation, defect management and tracking, and
risk management through Boehm's model [2]. The Software
Enterprise project teams are using IBM Rational Jazz [11],
a team collaboration tool for their project management
purposes. The project teams in the previous semesters
where producing the status report manually every week.
The status report contains Earn Value Chart, Burn down
chart, risk identification and activity summary.
Every software project will have some unfavorable
event that delays or stops the progress of the project. Such
events or scenarios are called risks in software development.
A good project management process considers risks during
its estimation phase and manages it well [13].
Managing software risks with paper based status reports
is time consuming for Software Enterprise project teams.
There are two parts to this problem that needs to be
addressed: 1) the teams should find an effective way to
record and maintain risks associated with their projects, 2)
teams should be able to view their project’s progress based
on risk on a daily basis.
During previous semesters, teams were reviewing
project status once per week by creating a paper based

978-0-7695-4654-4/12 $26.00 © 2012 IEEE
DOI 10.1109/ITNG.2012.172

BACKGROUND

349

course for risk management named “OCTAVE Allegro”
[16] based on Boehm’s model. In this course the teams use
spreadsheets, graphs and tables to manage and map risks.
Another flavor of risk management was proposed by
Richard Fairley based on his experience in organizations
[6]. In his paper, he discusses various steps needed for risk
management but there is no recommendation of any tools.
There are a few variations of risk management
methodologies and a few variations of risk analysis tools
like Boehm’s decision tree [2], risk management table [17]
and spreadsheets available in the market.
The most recent version of Jazz (version 3, our work is
based on version 2) does include a new risk work item type
and risk actions that can be linked to the work item. These
are available by default in the formal project management
template but must be explicitly enabled in a customized
flavor of the Scrum/Agile or other project templates. The
Jazz model is fairly similar to the Boehm model, adding a
precision metadata element to represent the level of
confidence the manager has in the risk assessment.
Although these methodologies and tools assist risk
management, they are not integrated into an IDE. The
Software Enterprise teams are using Jazz tool for project
management and team collaboration and all the data are
recorded and stored in the Jazz repository. One of the
advantages of our risk management component is that it is
integrated into Jazz, so that teams can use a single tool for
team collaboration and risk management.
III.

construct a decision tree. The decision tree shows the
possible outcomes of the risk with each branch summarizing
a given alternative’s risk exposure in that project. Risk
Exposure (RE) is the product of the probability of the risk
occurrence and its impact. Every project should have a
threshold for risk exposure and when the total RE crosses
the threshold then the project is considered to be in risk.
Below is a decision tree constructed by one of the
Software Enterprise teams.

Figure 1: Decision Tree

The decision tree has two branches; the top branch
summarizes the total risk exposure if the mitigation plan is
followed and the bottom branch summarizes the risk
exposure if the mitigation plan is not followed. The team
identified that there might be a 25% chance of issues while
packaging and deploying their application. Their mitigation
plan for this risk was that they would spend eight hours of
their time in developing a detailed deployment document. If
this mitigation plan is followed the team estimated that the
risk will reduce from 25% to 5%. From the decision tree, it
is evident that the combined risk exposure of implementing
the mitigation plan is less than the combined risk exposure
of not implementing the plan. So the team proceeded with
implementing the mitigation plan.
Software Enterprise project teams previously produced
paper status reports manually every week. The status report
contained an earn value chart, a burn down chart, risk
management data, and an activity summary. Although all
the teams understood the value of the status reports, they
felt generating these status reports every week consumed a
considerable amount of time. In addition, it was hard for
teams to visualize status of their projects, especially risk
management, which was represented in simple tabular form.
The implemented solution was to automate the risk
management process and integrate it into Jazz platform. The
advantage of automating the risk management process is
that the team can record, analyze, monitor and manage
the risks effectively. Another advantage in automating this
process is that the data is stored in the Jazz repository that

RISK MANAGEMENT

According to Boehm [2], risk management involves
two steps, Risk Assessment and Risk Control. Risk
assessment involves risk identification, analysis and
prioritization and Risk Control involves activities like risk
management planning, risk resolution and risk monitoring.
Our solution supports mainly the Risk Control activities.
Risk identification is the process of identifying project
specific risk items affecting the success of the project. Some
risk
identification
techniques
include
checklists,
examination of decision drivers, assumption analysis, and
decomposition. Student teams use a combination of these
methods to create a risk list and enter it into the tool. The
teams, again using a variety of techniques, perform risk
analysis and risk prioritization. For example, teams might
conduct a Delphi-style convergence exercise to prioritize
risks across projects. For risk control, teams have two types
of plans – contingency or mitigation plans. Mitigation plans
try to reduce the probability of the occurrence of risk for
e.g. a team that is not familiar with a technology is
identified as a potential risk, asking the team to spend
some time weekly to learn the technology is an example of a
mitigation plan. Contingency plans concentrate on reducing
the loss for the parties after the risk has occurred.
Once plans are created, the team analyzes them to
choose which one is best. The fundamental approach is to

350

facilitates the creation of trending reports based on the risk
progress for each project. This allows the team to view their
project status on a daily basis instead of a weekly basis as
they were doing previously. The daily project status report
increases the team’s understanding of where they stand
with respect to the project deadline.
IV.

following features:
1. Teams should be able to record risks for their project
in Jazz with loss and probability values.
2. Teams should be able to create mitigation and
contingency plans and should be able to link the plans
with the corresponding risks.
3. Teams should be able to monitor the risk and the risk
exposure throughout the project.

A RISK MANAGEMENT TOOL IN JAZZ

Jazz is IBM’s new platform for collaborative software
project development. Jazz, and the client Team Concert, are
extendable as they are built on the OSGi framework [7]. The
Jazz integration architecture is designed such that it gives
the flexibility for the project teams to put together their own
software delivery environment, using preferred tools.

The risk management component was designed such that
the client plugin provides a facility for users to enter risk
specific details in the newly created “Risk” work item type
and evaluate the data that were entered. Server side service
plugins were designed such that they get triggered when a
risk type work item was saved. These services check the
current state of the risk work item and validate the
transitions based on the state model of the risk work
item. The biggest challenge in implementing the design was
finding the correct extension points in Jazz.
A. Implementing Feature I
Jazz allows the users to customize the process model of
the project. The process enables and holds control over the
components. Basically components are process neutral. For
teams to record risk, there is be a “Risk” work item with
custom attributes “Risk Probability in %”, “Risk Impact”
and “Risk Exposure”. Risk Exposure is calculated as
the probability of the risk times the risk impact.

Figure 2: Jazz Architecture [4]

For the risk management solution two server side
plugins were developed. In addition, the Scrum process
model [15] used by the software enterprise teams within
Jazz was customized to add some risk related data. Apart
from this, a few client side plugins were developed using
Jazz client side extensions. A trending report was created
based on the data recorded in the Jazz repository.
Five plugins for different purposes were developed
for this risk management component solution. Some plugins
are to be deployed in the RTC client and some of them are
to be deployed in the Jazz server and some are common, so
that they have to be deployed on both client and the server.
Table 1 describes the plugins and where they are deployed.

RE = Probability of the risk occurrence * Risk Impact
The state model, shown in figure 3, for the risk work item
was designed based on the Boehm’s model [2].

TABLE 1: Risk Management Component Plugins
Plugin
Probability
presentation
Risk Exposure
presentation
Link Type extension
Follow up action

Pre-condition

Description
Customized editor
representation
Customized editor
representation
Customized link types
Sets initial risk
probability and risk
impact
Ensures update to risk
probability and
impact values

Deploying
place
Client
Client
Figure 3: Risk Work item state model

Client and
Server
Server

The state model can be added into Jazz process template
through provided “Workflows” interface as shown in the
Figure 4. The “Workflows” interface provides the users to
enter the possible states for the work item with the actions
that trigger the state change and the respective target states.

Server

The risk management component should have the

351

of the “Risk Exposure”.

Figure 5: Screenshot of Risk Exposure editor presentation

Figure 4: Jazz interface for defining state model

The user interface should track and show the initial risk
probability and the risk impact in the client UI along with
the current risk probability and impact. The users in the
analysis phase will create either mitigation or contingency
plans with the help of decision trees discussed in Section
II. This will reduce either the risk probability or the risk
impact. This follow up action gets triggered after the work
item is saved. This plugin checks whether the initial risk
probability and impact values are empty, if so, the follow up
action assigns the current risk probability and the risk
impact values to the initial values. Since in the client
user interface these initial value representations are read
only, the user cannot modify the values later. In addition
this follow up action updates the values of the custom
attributes into the Jazz repository. The risk attributes values
can be updated if the team has identified mitigation or
contingency plans and associate the plans with the
respective risks, or i f the team feels that the initial
estimation of the values might not be accurate.

The risk work item client UI provides users an interface
to enter risk-probability as a percentage and risk impact in
any unit; the unit depends solely on what the team decides,
the unit can be either the loss in dollars or it can be the extra
hours the team has to put into, if the risk comes to reality.
A plugin was developed by extending the extension
point “com.ibm.team.workitem.ide.ui.editorPresentations”.
This plugin creates a new editor representation for “long”
attribute in the RTC client. When this custom representation
is selected for long attribute, then whenever the users enter
values greater than 100, the representation will evaluate the
value and show a small red error mark near the value and if
you hover over the error mark, it will display a detailed
message. This plugin even validates whether the entered
value is an integer or not.
The work item UI helps the users to notice how much
they have reduced the risk exposure from the project
inception until that day. This process of tracking and
displaying the initial values is done by creating a new
follow up action which gets triggered after the work item is
saved. A follow-up action [1] runs after a process enabled
operation has been invoked. The follow-up action can
be
implemented
by
extending
the
“com.ibm.team.process.service.operationParticipants”
extension point. “Risk Exposure Calculation” plugin was
developed extending the above extension point and this
plugin was defined such that it will be a follow up action for
the work item save operation by mentioning the operation
id “com.ibm.team.workitem.operation.workItemSave” in the
“plugin.xml” file. Once deployed into the Jazz server, the
plugin is available in the project configuration page, so that
the team can choose this as one of their follow-up actions
for the work item save operation.
The Risk Exposure plugin is an extension of the editor
presentation for “long” attribute. This risk exposure
representation calculates the product of the “Risk
probability in %” and “Risk Impact”. This representation
listens for the value change in the above mentioned
attributes and triggers the product calculations whenever
any one of the value changes. Figure 5 shows a screenshot

B. Implementing Feature 2
This user story defines an aspect of users able to create
contingency and mitigation plans and link them to the
associated risk work item. This process falls under the risk
analysis phase of the risk management process. Contingency
and mitigation plans [2] are simple unit of work. The team
members can use “Task” work item type to define the
plans. But there should be a way to associate these plans
with the respective risks. The Links user interface provided
in the RTC client establishes relationships between work
items. The work items can be linked with other work items
with different relationships like parent –child, work item
with blocking work item, work item with duplicate, etc. For
the risk management component a client side plugin was
developed to extend these link types. This plugin
extends the link type list by adding few more options for
the users to select. The plugin provides options to add
contingency and mitigation plan from the risk work items
and options to add risk work items from the plans as well.
The work items client UI (Figure 6) was customized to
show the work items linked to it by the new link types in the

352

overview tab. This configuration can be set using the quick
information section option of process configuration page.

both the rich client and the web user interface. The reports
component can be extended by creating user-defined reports
based on the data in the Jazz data warehouse. The Data
Warehouse in the Jazz repository is the one that contains all
the historical data. The Jazz reports component collects the
needed data related to work items, builds and source
control to visualize in a report. A Snapshot is a
collection of facts and dimensions that are related.
Examples of snapshots
include
the
Work
Items
snapshot, the Build snapshot, and the Source Control
snapshot. The Work items snapshot [14] is relevant to us, as
it contains the needed data to generate risk trending report.
BIRT [3] is an Eclipse-based open source reporting
tool for applications based on Java. The report designer
component of BIRT tool was used to design the risk report.
BIRT has rich variety of reports that can be used as needed
such as lists, charts, crosstabs, letters and documents. Data
can be easily understood if shown visually as charts. BIRT
has variety of charts to use as per the requirement;
some of them are pie charts, line and bar charts. A BIRT
report was designed with data from the Jazz data warehouse
that groups all the active risk work items in the project on a
daily basis. The risk work items are considered as active if
they are not in the “Invalid” or “Resolved” state. The data
set then calculates the number of risk work items and the
sum of the risk exposures of the active risk work items.

Figure 6: Screenshot of customized link types

The main purpose of adding plans to the risk is to
reduce either the risk probability or to reduce the loss
caused by the risk. The plans are added to the risk work
item when the risk is in “Analyzing” state. The plugin has
to make sure that whenever the risk work item is saved in
the “Analyzing” state, the value for the risk probability
or the value for the risk impact should be updated. This
can be achieved by developing a plugin that checks for the
change in the risk custom attribute values whenever the
risk work item is saved in the “Analyzing” state. This plugin
should be deployed in the server as a pre-condition.
A pre-condition [1] is run before the process enabled
operation to determine whether the operation should be
executed. If a pre-condition finds an error or a problem, the
operation will not be executed and the problem will be
reported to the user for potential remediation. This precondition is triggered whenever the work item is saved.
Once the plugin is deployed into the Jazz server, it will
be listed as the available pre-conditions for the work item
save operation. The team can select this pre-condition to
incorporate this feature into their project. This precondition plugin gets triggered whenever a work item is
saved. This plugin checks whether the work item is of type
“Risk” and the target state is “Analyzing” state. If the
condition holds true then the plugin compares the risk
probability and risk impact values entered in the work item
with the values stored in the repository. If both were same
then the precondition generates an error message to the user
advising them to update the values of either risk probability
or risk impact before saving the work item.

Figure 7: Risk Area Chart of a software enterprise project team

Figure 7 is one of the Software Enterprise project
team’s Risk Area Charts. The risk chart plots the number
of risk work items and the sum of the risk exposures of
those work items on a daily basis. Based on the chart, we
can interpret that the team started its risk identification
phase during the third week of February and continued it
until the first week of March. Then there was a period of
inactivity from first week of March until the third week of
March, followed by a spike in the risk work item count and
the risk exposure values for a short span of time. The
reason for the spike is that the team identified and assessed
a set of risks on the same day. The data in the Jazz
repository shows that after a few days they invalidated and
resolved few risk work items; this is the reason behind the
sudden drop of the risk exposure during the first week of
April. During the second week of April, we can see that the
team identified few risks and they started to resolve the risk

C. Implementing Feature 3
The main use of this component is that team members
can review their project’s risk on a daily basis. This can be
achieved by having a report that visualizes the trend of the
total risk work items and the total risk exposure in the
project on a daily basis.
The reports component in Jazz provides a rich library of
reports, a data warehouse, and integrated report viewing in

353

work items. By seeing the report, we can say that the team
had actively participated in the risk management process
other than the two weeks of inactivity. The team has done a
good job during the month of April by reducing the risk
exposure of the project to a considerable extent.
V.

evaluated by Software Enterprise project teams. The
component has two parts to it, the risk work item plugin and
the Risk Area Chart. The risk work item plugin helps to
record the identified risk and risk specific attributes like
probability of risk, risk impact, and risk exposure. The
plugin helps in risk analysis by providing features to link
contingency and mitigation plans to the risk work item.
Future development of this plugin could be incorporating
the Boehm’s decision trees [2].
The current version of the risk work item plugin
supports Jazz RTC client only. Another nice to have feature
would be to develop a web user interface for the risk plugin.
Jazz’s extensible architecture is easy to learn and RTC
is built on eclipse which supports extensions through plugins. Jazz is an open source product and it has a very good
support through its forums in Jazz.net.

VALIDATION

The intended users for this risk management
component are Software Enterprise project teams. A team
uses this component to review and manage their risks with
less effort. The instructor’s aim of this tool is that both the
team and the instructor should be able to view the project’s
risk status on a daily basis, so that both of them can
understand the status of the project. Based on the data
collected, there is evidence that the software enterprise
project teams preferred using this risk management tool for
recording and managing their risks.
An anonymous survey was conducted to validate this
component. The 20 participants were Software Enterprise
project team members. The survey was structured such that
it validates the risk management component in two different
perspectives. The two different parts of the risk management
component were 1) risk work item plugin and 2) Risk Area
Chart. The two different perspectives were 1) how usable
is the tool and 2) how useful is the tool. Table 2 shows the
survey questions and results.

REFERENCES
[1]
[2]

[3]
[4]

TABLE 2: Survey Questions and Results
[5]
Survey Question
In general, Risk management process is a
must for a project.

Agree Neutra Disagree
90%
0l
10%

[6]

50%

30%

20%

[7]

50%

40%

10%

[8]

The Risk Area Chart in Jazz clearly
depicts the project's status with respect to
risks on a daily basis, which helped us to
manage the project better.

10%

60%

30%

[9]

In comparison with the weekly paper
based status reports, the Jazz based
automated reports saved us more time.
As a whole, the Risk management
component integrated into Jazz was very
useful for us in project risk management.

90%

0

10%

[10]

60%

40%

0

The Risk work item plug-in for Jazz is
easy to learn and use.
Risk work item plug-in helped us to track
and manage the risks effectively.

[11]

[12]

Based on the survey results we can claim that 90% of
the participants accepted that risk management process is
important for the project management. The results for
usability and effectiveness were positive but not as high as
we had hoped. When going through the individual survey
results we found out that 60% of them who agreed that the
plugin is easy to use also agreed that the plugin is useful.
The risk area chart did not appear to benefit the teams, which
is disappointing. In general though, the teams viewed the
tool as an improvement over the paper-based status reports.

[13]
[14]
[15]

[16]

VI. SUMMARY

[17]

The risk management component for Jazz was used and

354

Andrews M. “Precondition & Follow-up Creation”, Nov 2009.
https://jazz.net/wiki/bin/view/Main/PreconditionFollowupCreation
Barry W. Boehm, "Software Risk Management: Principles and
Practices," IEEE Software, vol. 8, no. 1, pp. 32-41,
Jan./Feb. 1991, doi:10.1109/52.62930
BIRT http://www.eclipse.org/birt/phoenix/intro/
Carrie S. “How to use Team Concert alone or integrated with
IBM Rational Clear
Quest”, June 2008. http://jazzlab.net/
jazz/?mid=tip&search_target=tag&search_keyword=RTC&document
_srl=2161
Clayberg E, Rubel D. “Eclipse: Building Commercial-Quality PlugIns”, Pearson Higher Education, 2004
Fairley R. "Risk Management for Software Projects," IEEE
Software, May 1994, pp. 57-67.
Frost R. 2007. “Jazz and the eclipse way of collaboration”,
IEEE Software 24(06): 114–117.
Gamma, E and Beck, K. “Contributing to Eclipse: Principles,
Patterns, and Plug-Ins”. Addison-Wesley, 2004.
Gary K. “The Software Enterprise: Practicing Best Practices in
Software Engineering Education”, International Journal of
Engineering Education Special Issue on Trends in Software
Engineering Education, 24( 4), July 2008, pp. 705-716.
IBM. “Team Process Developer
Guide”, June 2009.
https://jazz.net/wiki/bin/view/Main/TeamProcessDeveloperGuide
Jazz team. “Jazz Platform Technical Overview”, June 2008.
http://jazz.net/library/LearnItem.jsp?href=content/docs/platformoverview/index.html#team_process
Kameny I, Khan U, Paul J, Taylor D. “Guide for the
Management of Expert Systems Development”, Rand Corporation,
Santa Monica Ca, July 1989.
Molt,
George.
“Risk
Management
Fundamentals
in
Software Development”, CROSSTALK Aug. 2000.
Moody J. “Data Warehouse Snapshot Schemas”, June 2009.
https://jazz.net/wiki/bin/view/Main/DataWarehouseSnapshotSchemas
Schwaber K. “SCRUM Development Process”, Workshop on
Business Object Design and Implementation. 10th Conf. on ObjectOriented Programming Systems, Languages, and Applications, 1995.
Software Engineering Institute, Carnegie Mellon, “OCTAVE
Allegro Speeds up Risk Assessment” [Online]. Available:
http://www.sei.cmu.edu/solutions/risk/octave-allegro.cfm
Roger S. Pressman. “Software Engineering, A Practitioner’s
Approach”, McGraw-Hill, 2001.

