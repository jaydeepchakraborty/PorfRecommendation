Proceedings of ?he 2002 Winrer Simulation Conference
E. Yuceson, C:H. Chen, J. L. Snowdon, and J. M. Charnes, eds.

PARAMETERIZATION OF FAST AND ACCURATE
SIMULATIONS FOR COMPLEX SUPPLY NETWORKS
Brett Marc Duarte
John W. Fowler
Kraig Knutson
Esma Gel
Dan Shunk
Department of Industrial Engineering
PO Box 875906
Arizona State University
Tempe, AZ 85287-5906, U.S.A.

ABSTRACT

1999). The first dimension is functional integration, involving decisions about purchasing, manufacturing and distribution activities within the company and between the company
and its suppliers and customers. The second dimension is
geographical integration of these functions across physical
facilities located in one or several continents. The third dimension is integration of strategic, tactical and operational
supply chain decisions. Supply Chains (SC) as described by
Stevens (1989) is: “A system whose constituent parts include material suppliers, production fac
services and customers linked together via the feed forward
flow of materials and the feedback flow of information”.
With globalization of the market, optimization of supply
chains becomes more and more important. Synchronizing
this complex supply chain network, and making it respond to
demand fluctuation is not trivial, hut how well companies’
react to rapidly changing customer demands becomes a very
important factor in their ability to dominate their markets.
Typically, a supply chain is a multi-echelon system where
each “node” in the supply chain may have several disparate
suppliers. A simple supply chain in the semiconductor industry that was adapted from Godding and Kempf (2001) is
shown in Figure 1.

More efficient and effective control of supply networks is
conservatively worth billions of dollars to the world economy. Adopting an approach by which the basic disciplines
of Industrial Engineering, Control Engineering, System
Simulation and Business Re-Engineering are integrated
into one comprehensive system has been known to produce
impressive results. This paper discusses a modular approach to develop a discrete event simulation model that
has the appropriate level of abstraction to capture the inherent complexities that exist in a supply chain and is yet
simple, fast and produces results of high fidelity. It discusses a method to parameterize each module by finetuning a few parameters to make it represent an entire factory, a warehouse or a transportation link.

1

INTRODUCTION

High value products that quickly become obsolete! A vast
manufacturing network! Rapidly declining prices! A demanding customer base! The supply chain challenges facing the semiconductor industry are complex and difficult!
In today’s globally competitive business world, the network of companies that band together to create an end
product or service are not restricted to a sub-continent. The
world is moving from single enterprise mass production to
multiple enterprise customizations. Why? The strengths
strategic alliances have to offer, which include higher margins, shorter development cycles, higher quality, lower
overall costs, and the ability to meet demand on a single. customer basis. The key to gaining this competitive advantage is integrating decisions across the supply network.
across geographically dispersed facilities, and across time.
The essence of Supply Chain Management is integrated
planning, which has three important dimensions (Shirodkar

c..
P
!“.I:

-

5

L!a,
-&+
AD,,

f.biS.*,

_I

, - ,

!

-

“.I;.

3

.

I;

-.

-

!“,S.

&

,-,i

rUIt“l

1082

&-.
““I

,”.I

.~

e.*>

Figure 1: Simple Model of a Supply Chain

1327

Duane, Fowler, Knutson, Gel, and Shunk
One major obstacle in creating a seamless supply
chain is uncertainty. In order to deal with this issue, it is
imperative to identify and understand the cause of uncertainty and determine how it affects other activities up and
down the supply chain. The complexity described above
causes the semiconductor industry to experience erratic
changes in demand and this makes it difficult to decipher
the true demand from normal fluctuations, (Shirdokar
1999). What often appears as small random ripple variations in sales at the market place are amplified dramatically
at each level in the chain, so that upstream companies or
facilities experience the classical "boom-bust" effect,
(Towill 1996). In particular the variance in orders tends to
be larger than that of sales and this distortion tends to increase upstream. (Lee er a1 1997) describes this phenomenon termed the "bullwhip" effect and attributes its cause to
demand forecast updating, order batching, price fluctuations, and rationing and shortage gaming.
Computer simulation, because it can be applied to operational problems that are too difficult to model and solve
analytically, is an especially effective tool to help analyze
supply chain logistical issues. Currently, tools for understanding uncertainty are limited to traditional mathematical
formulas that do not account for variability. However,
simulation is one of the best means for analyzing supply
chains because of its capability to handle variability, (Towill 1996). Obviously, experimenting with an actual supply
chain could be detrimental, as the profit at risk is prohibitively high. Useful results have been obtained by adopting
an approach in which the basic disciplines of industrial engineering, control engineering, system simulation and
Business Reengineering are integrated into one comprehensive system, (Forrester 1961).

2

SII"LATIO*

""NllA,.,

Ik..,

Figure 2: Simulation Accuracy Versus Run Time

3

THE BASIC ATOMIC MODULE

T o model the material flow in the physical system a module was developed by Shirodkar (1999) that can be used to
represent a factory, a transportation link, or a warehouse.
This hybrid module is made up of three sub-modules: a capacity sub-module, a delay sub-module and a yield submodule as shown in the Figure 3.

-+
' T I

'

T I The time a lot gpends in the ',e,
T2 +The processing time of the lot.
T3 3 Total cycle time of the lot
Figure 3: The Basic Atomic Module (Adapted
from Shirodkar 1999)
The production units that arrive at the capacity submodule sit in a queue. A sample is then drawn from a
probability distribution, which represents the capacity of
the module. This occurs at a predetermined time interval
based on the chosen sampling rate; we use once a day in
our experimentation. The value of capacity drawn from the
distribution is then compared to the number of lots sitting
in the queue and the lesser of the two is picked and that
number of lots are released from the capacity sub-module
into the delay sub-module. The delay sub-module has an
infinite number of servers and each lot that enters this
module is allotted a processing time from a user specified
probability distribution. The queue in the capacity module
thus represents the time spent waiting in front of the capacity for it to become available and the delay sub-module
represents the time spent in processing once the capacity
has become available. The lots that finish processing proceed to the "yield sub-module" where the good lots m
separated from the defective ones.

PROBLEM STATEMENT

Traditionally, simulation models used in supply chains
have either been detailed discrete event simulation (DES)
models that track every individual lot that is processed at
every workstation or high-level, continuous simulation
models that do not track each lot but consider the gross
output and cycle time performance of each factory in the
chain. The first approach produces results that are very accurate but it generally takes a long time to build the model
and the execution time of such a model is often extremely
slow. Building models of the second type is generally
much easier and their execution time is much faster, but
the data produced is often far from accurate. Little work
has been done to combine these approaches to develop a
model that has an appropriate level of abstraction to capture the inherent complexities that exist in a supply chain
and is yet simple, fast and produces results of high fidelity.
The schematic diagram shown in Figure 2, illustrates the
objective.

1328

Duane, Fowler, Knutson, Gel, and Shunk
4

PARAMETERIZATION OF MODULE

Among the various sources of error that make the output
from a simulation less valid, is the modeling the “wrong” distribution for various input quantities, for example the arrival
times of jobs to a job shop or the service time of machines. A
commonly encountered problem in simulation modeling is
the specification of a suitable input distribution for the observed data. The data is a specific realization of some underlying distribution that can be regarded as the ‘ h e ” distribution, (Shankar and Kelton 1999). A prevalent practice is to
approximate this ‘ h e ” distribution with a standard family,
for example an exponential distribution or a uniform distribution. In many situations, this approximation may not adequately represent the observed data, and may introduce significant error in the input that may adversely affect the
validity of the output. In general there are three methods of
specifying an input distribution, (Shankar and Kelton 1999).
1.

2.

3.

eterize the atomic module is shown in Figure 4. In this a p
proach, we use data taken from the real factory or from a
detailed discrete event simulation (DES) of a factory to develop the capacity and cycle time parameters. Determining
the capacity distribution is relatively simple. Since the predetermined sampling rate at the capacity sub-module is
chosen to be once a day, the capacity parameters are determined by fitting the daily throughput of a simulation of
a fully loaded factory to an empirical distribution or by fitting the daily throughput of the real factory divided by the
utilization of the bottleneck to an empirical distribution.

Figure 4: Parameterization Methodology

Use a “standard” parametric distribution: These
include distributions such as uniform, exponential,
weibull.
Use an empirical distribution: Here the observed
data itself is used in some way to come up with a
distribution function. Empirical distributions have
flexibility, which is much desired.
Use a Flexible parametric family: Such a parametric family supplies a flexible distribution function
that is an approximation of the true distribution
function. This alternative can be viewed as a compromise between the first two approaches and is
both generalizable and flexible.

Matching the cycle time is not as straightforward because the cycle time distribution depends on the loading of
the factory. It is important to use the cycle times of individual lots in ascertaining the distribution at each capacity
loading. By using individual cycle times the reduction in
variance caused by the averaging affect of lots coupled into
daily or weekly time buckets is eliminated.
Figure 5 shows the (sanitized, for confidentiality) distribution of cycle times for lots coming out of a real Intel
factory. Figure 6 shows a similar cycle time distribution of
lots coming out of the detailed discrete event simulation
(DES) model. The model used was dataset #I from the
MASM Lab at Arizona State University (www. eas.asu.
edu/-masmlab). The dataset was modified to consist of
a single product with a release rate of 12 lotslday, which
corresponds to a factory loading of 97%, each lot containing 48 wafers. The modified model produces no scrap and
has 83 tool groups with 265 tools. The model also has 32
operator groups with 90 operators. The detailed simulation
was run for 200 days with 10 replicates and the first 65
days of each replicate was truncated. The remaining lot
cycle times were combined into one file and data from
10,000 lots were used to plot the histogram.

Output measures of performance can indeed be sensitive to the particular input distribution. Using standard
two-parameter distributions for which only the first two
moments are captured in many cases is not sufficient,
unless the system is running at relatively high traffic intensity. It may be necessary to use at least five moments for
systems with low traffic intensity, (Gross and Juttijudatta
1997), even though the lower order moments are the ones
that actually dominate.
It is apparent that the problem of input distribution selection is inherent to simulation modeling. A point in favor
of the empirical distribution is that their performance is
consistent. This cannot be said about the standard distributions whose performance quality depends more critically
upon the underlying true distribution. This robustness of an
approximating method is an important issue in input distribution specification.
Results from the previous research inferred that the
model produces data that is qualitatively correct. The next
step would therefore be to develop an approach by which
the model could produce throughput and cycle time data
that is (nearly) quantitatively correct and thus consistent
with data from a real factory. The approach used to param-

Figure 5: Total Cycle Time of Lots From
a Real Factory

1329

Duane, Fowler, Knurson, Gel, and Shurtk
results of both simulations. Therefore, subtracting the additional queue time ‘TI’ would render results that are very accurate, as we would expect since the resulting cycle time is
simply a sample from the actual cycle time distribution.

Figure 6: Total Cycle Time of the Detailed DES
The cycle time distribution in Figure 5 and Figure 6
above look fairly similar. Note that the average cycle time
of the DES model is 34.65 days. A simulation was then run
using our atomic module (coded in EXTEND”). The capacity distribution for this model was obtained from the
100% factory loading detailed DES and the cycle time distribution was obtained from the 97% factory loading detailed DES by fitting the cycle time of the 10,000 lots to an
empirical distribution. Figure 7 shows the cycle time distribution from this simulation run.

Figure 8: Average Cycle Time versus Capacity Loading
Estimating this additional queue time ‘TI’ can be either done on an individual entity basis or by evaluating the
average queue time at a particular capacity loading using
analytical methods. The latter approach proves beneficial
later in the section, as a graph of the average queue time
versus the capacity loading is a good characteristic approximation of what the cycle time curve would look like
qualitatively.

Analytical Approach to
Estimate Queue Time

4.1

The model under consideration can be thought of as an inventory system that has a deterministic supply but a random demand. It is assumed that any demand that cannot be
satisfied for the day is lost.

Figure 7: Total Cycle Time from Our
Model

Y.

Once again the distribution looks similar’to that of the
DES. The average cycle time however is 40.87 days, which
is higher that that of the DES and is attributed to the additional time the lots spend at the capacity sub-module. The
breakdown of the overall cycle time of a lot through the
atomic module is shown in Figure 3. The value ‘TI’, which
is the amount of time a lot spends in the queue, is determined by the capacity sub-module and will be negligible
when the factory is lightly loaded, but will increase as the
factory loading increases. ‘T2‘ for each lot is simply a
sample from the given cycle time distribution and does not
depend on the factory load. ‘T3’ is the total cycle time of a
lot and is the sum of ‘TI’ and ‘T2’. Matching the cycle
time distribution of the detailed simulation with that from
our module for a given loading by running a detailed simulation for each loading of interest is easy.
A simple experiment shown in Figure 8 show the results
obtained by using this approach. As mentioned above the
difference in cycle times between the two simulations is
purely due to the time a lot spends at the capacity submodule ‘TI”. As the system is more heavily loaded this
queue time increases and so also the difference between the

X,
a

The demand on day n
The inventory available to satisfy the demand on day n.
Deterministic start rate (Lotdday), of the
lots that arrive at the beginning of the day
and are available to satisfy the demand for
the day.

The stochastic process (X, n 2 0) possesses the
Markovian property which states that if the present state of
the system is known, the future of the system is independent of its past, (Kulkami 1995). Stated another way, the
present state of the system contains all the relevant information needed to predict the future in a probabilistic sense..
Hence the stochastic process (X, n 201 can be modeled as
a Discrete-Time Markov Chain (DTMC). The steady state
distribution of the process can be found by solving for the
following set of equations.

1330

Duane, Fowler, Knurson, Gel. and Shunk
where lim

. ... .......
... ....,. ,.
._
........
..............
._.- ._......
........
."......,..."
._._._
..........
..............
..............
..............
..............
........
....
......
........
._
.
._
..
............
.....
............
..
........
....
.............

ei= R ~ is,the long-run average fraction of

time that the system stays at state j and Py represents the
transitional probability of moving from state 'i' to state
as shown in Figure 9 and each state is defined as the inventory left at the end of the day.

.
I

Y

Y

Y

.
I
.I.IY
Y

Y
.
I

Y.I.

Y

Y

"-1

... ... .,. ...
... ... ... ... . .
.. .. ,. ..
.. .. ... ...
.-.
......
.......
. -.
........
._
........
._
._
._
..
-.".
..
.
.
I
........
....
.......
....,,
....._
."._
. .....
".,,"

"

"+DD

Y

"

.... ..

Y . .

".,
I).

.I

\

.

Figure 11: Transitional Probability Matrix
(TPM) with Finite State Space
Figure 9: Markov Chain with Transition
Probabilities
The Transitional Probability Matrix (TPM) that represents this Markov chain is set up by evaluating the probability of a having a certain number of lots waiting in inventory at the end of the day, after the capacity of the
system has been set for that day
The example shown in Figure 10 is for a start rate of 5
lotdday. Poo would therefore be the transition probability
of having zero lots at the end of a day on which five lots
entered the system with an initial inventory of zero. This
would occur if the capacity for the day were greater than or
equal to 5. The probability of achieving this based on the
capacity distribution is 0.85. Similarly, Pol is the transition
probability of having one lot at the end of a day on which
five lots entered the system with an initial inventory of
zero. This would occur if the capacity for the day were
four. The probability of achieving this based on the capacity distribution is 0.03. Similarly the rest of the probabilities in the transitional probability matrix (TPM) can be
evaluated. The TPM for the model is that of an irreducible
Markov chain with infinite state space. The matrix is symmetrical with an upper and lower triangle of zeros.
A quick way of solving this matrix is by approximating the TPM with a finite state space. We truncated the
TPM as shown in Figure 11, and then solved it using the
Grassmann, Taksar and Heyman (GTH) algorithm (Grassmann et al 1985).
The GTH Algorithm is a state reduction algorithm.
Recursively, a Markov chain with one state less is consuuctttd from the previous one. The algorithm begins with
the n row and column and performs a series of iteration
and computation, working its way up the matrix. In the

process it calculates the steady state probability vector
n'=
with low relative error. The GTH algorithm was coded in Matlab. The expected queue length (not
including jobs in processing) 'LY'can then be calculated as
L , =

n

j

'

j

j=o

From Little's Law we have:

L

=,x*w,

Where h denotes the start rate and W, denotes the waiting
time in the queue. Therefore for a given start rate 'L',the
waiting time in queue W,can easily be calculated.
One question that arises is how big should the truncated transitional probability matrix be? Naturally we expect that the bigger the matrix, the more accurate the result
will be. Figure 12 show the trade-off between the size of
the matrix and the average queue time obtained. The experiment was carried out using the matrix corresponding to
a 97% loading.

Figure 12: Trade-off between the
Size of the Matrix and the Average
Queue Time for a 97% Capacity
Loading
Results obtained by using the analytical method of estimating queue time were also compared to that of the
simulation to check the validity of the method. The comparison is shown in Table 1.
Therefore, if we can estimate the additional queue
time analytically, it is relatively easy to match the cycle
time distribution of the detailed simulation with that from
our module for given start rates by running a detailed simu-

Figure IO: Transitional Probability Matrix
(TPM) with infinite state space

1331
I

2

Duane, Fowler, Knurson, Gel. and Shirnk
asymptote corresponds to a lightly loaded factory where
the cycle time is almost equal to the raw processing time.
The second asymptote represents a heavily loaded factory
where the traffic intensity approaches the capacity of the
system. The cycle time for such loadings approaches infinity due to the ever-increasing queue.
As a next step, to achieve the cycle time at various
loadings, we linearly interpolated the average cycle times
or each segment of the curve based on the load percent.
Developing linear equations to represent each segment requires two reference points for each equation. The question
is how do we pick the reference points? A graph of the average queue time versus the capacity loading is a good
characteristic approximation of what the cycle time curve
would look like qualitatively. Since obtaining the average
queue time using analytical methods is efficient, we use the
queue time versus capacity loading graph to choose which
capacity loadings to tun the DES to best represent each cycle time segment. The mean and standard deviation of the
cycle time at each reference point is noted and linear equations for both parameters are set.

Table I: Comparison of the Queue Time Achieved
from the Analytical Approach to that of the Simulatinn R i m

lation for each loading of interest, using empirical distributions for the capacity and delay and subtracting the additional queue. However, our goal is to specify a small numher of capacity and cycle time parameters that will give
reasonable estimates of cycle time distributions over a
range of factory loadings.
As indicated earlier, using a single cycle time distribution to statistically match data at different capacities would
be ideal. In order to see how well a single cycle time distribution would work, the module was tun using the empirical cycle time distribution that was built using data that
corresponded to a 40% capacity load of the detailed DES.
It was assumed that the effect of queuing is insignificant at
this loading. The capacity distribution supplied to the
module was the throughput distribution from a detailed
DES at 100% capacity load. The module was then run at
different start rates (40%. 8l%, 90%. 97%) to check if the
output matched that of the detailed DES. The results of this
experiment are shown in Figure 13. Notice that the average
cycle time from our module significantly underestimated
the average cycle time from the detailed DES for all loadings. The same experiment was repeated using the 80% cycle time distribution of the detailed DES as the delay distribution in our module. As shown in Figure 13, using this
distribution led to a significant overestimate of average cycle time for a lightly loaded factory and a significant underestimate for a heavily loaded factory.
The cycle time characteristic curve of a system, with
no batching policies, can be represented by a monotonically increasing curve (Fowler and Park 2001). For these
systems, the cycle time curve can be broken up into three
principal segments, two asymptotes and a knee. The first

!

L.W

Figure 1 4 Choosing the Three Segments
Based on the assumption of Rose (1999).the cycle time
distributions at higher capacity loadings can be assumed to
be normally distributed. Using the equations for the mean
and the standard deviation we can set the parameters of'the
normal distribution. Intuitively the average cycle time using
our module will still he overestimated due to the fact that a
non-linear curve has been replaced by a linear one for the
purpose of estimation and due to the additional queue time
'TI' in the model. We propose to use the analytical method
to estimate 'TI' and subtract this estimate from the cycle
time of each lot so as to eliminate the later problem.
To verify this approach, the simulation model was run
for a period of 3400 days at different start rates. An initial
bias of 5000 lots were considered and eliminated from the
statistics. Eight replicates were performed at each capacity
loading. The results obtained are shown in Figure 15 and in
Table 2.
Figure 16 is a plot of the average daily cycle time versus the elapsed time for the system run at 89% capacity
loading which corresponds to a start rate of 11 lotdday.
Data from day 400 through day 3400 has been plotted. The
average cycle time for the DES is 25.20 days with a standard deviation of 1.78 days while our model has a mean
cycle time of 25.58 days with a standard deviation of 1.51

I

Figure 13: Cycle Time Versus Capacity
Loading Using a Single Delay Distribution

1332

Duane, Fowler, Knutson, Gel, and Shunk

10

so

.o
5.p.m"L...l"

/ '

so

100,

__*--

...10

,
/

!I ,I
!I

/ I
IO
,111

so

I

Figure 15: Cycle Time Versus Capacity
Loading

Figure 17: Effect of a Deterministic/Stochastic Cycle Time Distribution

Table 2: Statistical Comparison of DES with

,

.
I
0."

.1~,~I.1I..~l.l,,.",~",o

.,.,

--,

Figure 18: Effect the Delay Distribution
h& on the Throughput
trates the extent to which this "cross-jumping" of lots effect the variance in throughput. When the delay distribution is deterministic, the standard deviation of the throughput for our model matches that of the DES, however, as the
width of the cycle time distribution increases, the standard
deviation of the throughput decreases till it eventually
reaches a steady state.

-"RI
81-m
I
I I
Figure 1 6 Cycle Timemhroughput Versus Elapsed
Time at 89.1% Capacity Loading

I

5

days. Similar experiments were run to validate the model
at different capacity loadings.
As far as throughput goes, the first 400 days of data
has been truncated and data for the next 6M) days has been
plotted. Notice that while the average throughput of our
model is consistent with that of the DES, it does not have
as much variability. The average throughput for the DES is
11.03 unitslday while that for our model is 11.02 unitdday.
The standard deviation for the DES is 6.27 unitslday while
that for OUT model, however, is 3.36 unitdday. This is attributed to the interaction between the capacity distribution
and the delay distribution.
The schematic diagrams in Figure 17 illustrate the effect of the interaction between the capacity and delay distribution. The system is analogous to a conveyor on which
the delay distribution sprays lots. When the delay distribution is deterministic, the lots that enter the delay submodule fall into the same time bucket and the variability in
throughput is preserved. For this example the processing
time is 4 days. With the advance of the time clock the lots
move one day closer to completion as a result the throughput at the end of days four, five, six, seven and eight would
be five, two, zero, five, and six.
However, variability in the delay distribution causes
lots to jump into different time buckets and in the process
reduces the variability in the throughput. Figure 18 illus-

EXECUTION TIMES

As far as accuracy goes, sufficient evidence has been put
forth to illustrate the credibility of our model, speed on the
other hand is a critical issue. Figure 19 is a plot of the
simulation run time for the DES compared to our model.
The model was run at different capacity loadings and the
simulation run time was recorded. The experiments were
run on a Pentium II, 333 MHz machine. The results show
that our model is much faster than the DES.

When modeling complex, supply networks, which
consists of several manufacturing, assembly and distribution facilities, the speed of our model would be even more
apparent. With its low run time and accuracy the model
should be a useful tool.

1333

Duarfe, Fowler, Knutson. Gel, and Shunk

6

REFERENCES

CONCLUSIONS

In manufacturing, common performance measures used to
evaluate a system are Cycle Time (CT), Throughput (TH)
and Work in Process (WIP). Changes to operating policies
can he evaluated by examining the impact on these three
performance metrics. Due to the complexities of manufacturing systems in the semiconductor industry, a simulationbased approach becomes a viable choice.
As stated earlier, detailed discrete event simulators
(DES) track each individual lot that is processed at every
workstation. As a result such models produce results that are
very accurate but they generally take a long time to execute.
Our model on the other hand aims at having the right level
of abstraction to capture the inherent complexities that exist
in a supply chain and yet is simple, fast and produces results
of high fidelity. By means of a simple model, we intend to
foster a basic understanding of the behavior of manufacturing units. If the simple modeling approach mimics the full
factory accurately, then these models can be used to model
complex supply networks. As far as accuracy goes, sufficient evidence has been put forth to prove its credibility.
Speed on the other hand is a critical issue. Run-time experiments carried out on a Pentium LI 333 MHz machine
show that our model is much faster than the detailed discrete
event simulator (DES) when modeling a single manufacturing unit. It is believed that the speed of the model would he
even more impressive when modeling a complex supply
network consisting of multiple factories, assembly facilities,
transportation centers and component warehouses.
Currently the model is set up to accommodate one
generalized product family, however an important next
step would he to accommodate multiple product groups.
This would lead to a more intuitive understanding of factory dynamics based on product prioritization coupled with
various dispatching policies. Future research in this area
would he aimed at attaining output parameters. namely cycle time and throughput that are statically indistinguishable
from data obtained from a real factory. The present model
produces results that are very encouraging, however, interaction between the capacity and delay distribution tends to
squeeze the variability in the throughput.
Each module can further be embellished to make it
look more like a factory, a transportation link or a component warehouse. Yield loss can be incorporated into the
model to give it a more realistic flavor.
As the capacity loading of the system increases the effect of auto-correlation in cycle time becomes more apparent. Future work in this area could entail comparing several correlation scenarios with respect to their ability to
mimic real factory data.

Forrester, J.W.,1961 Industrial Dynamics. MIT Press,
Cambridge, MA.
Fowler, W. J., Park, S., 2001, Efficient Cycle TimeThroughput Curve Generation Using Fixed Sample
Size Procedure. International Journal of Production
Research, Vol. 39, No.12,2595-2613.
Godding, G., Kempf, K., August 11-14, 2001, A Modular,
Scalable Approach To Modeling And Analysis Of
Semiconductor Manufacturing Supply Chains. Proceedings of IVSlMPOI/POMS 2001, GuarujUSP-Brazila.
Grassmann, W. K., Taksar, M. I., Heyman, D. P.,1985,
Regenerative Analysis and Steady State distributions
for Markov Chains. Operations Research, Vol. 33,
No. 5, 1107-1116.
Gross, DJuttijudatta, M., Dec.1997, Sensitivity Of Output
Performance Measures To Input Distributions in
Queuing Simulation Models. Winter Simulation
Conference, pp 296-302.
Hopp. W. J., Spearman, M. L, 2001, Fuctory Physics. Second Edition.
Ingalls, R., Kasales, C., Dec.1999, CSCAT: The Compaq
Supply Chain Analysis Tool. Winter Simulation
Conference, Vol. 1, pp 1201-1206.
Jain, S., Lim, C.C., Gan, B.P., Low, Y.K., Dec 1999, Criticality Of Detailed Modeling In Semiconductor Supply
Chain Simulation. Winter Simulation Conference,
Vol.1, pp 888-896.
Kempf , K., Knutson. K., Fowler J. W., Armbmster, B.,
Duane, B. M., Babu, P., April 24-25, 2001, Fast And
Accurate Simulations Of Physical Flow In Demand
Networks. Proceeding of International Conference on
Semiconductor Manufacturing Operational Modelling
an Simulation, Seattle. WA, pp 11 1-1 16.
Kitagawa, T., Maruta, T., Ikkai, Y., Komoda, N., Aug 2426, 2000, A Description Language Based On Multifunctional Modeling And A Supply Chain Simulation
Tool. 4'* IEEE International Workshop, pp 71-78.
Kulkarni, G. V.,1995, Modelling And Annlysis Of Stochastic Systems. 1995 Edition.
Law, A. M., Kelton, D. W.,1991, Simulation Modelling
And Analysis. Second Edition.
Lee, H. L., Padmanabhan, V., and Whang, S., April 1997,
Information Distortion In A Supply Chain: .The Bullwhip Effect. Management Science, Vol. 43, No. 4, pp

546-558.
Leemis, L., Dec 2000, Input Modeling. Winter Simulation
Conference, Vol. I , pp 17-25.
Maltz, A. B., Grenoble, W. I., Rogers, D. S., Baseman, R.
S . , Grey, W. and Katircioglu, K .K.,Lessons From The
Semiconductor Industry. Retrieved March 10, 2001
from the World Wide Web: http://www.rnan
ufacturing.net/scl/lessons/james.htrnl

ACKNOWLEDGMENTS
This research has been partially supported by a grant from
Intel Corporation.

1334

D u a m , Fowler, Knurson, Gel, and Shunk
Maruta, T., Ikkai, Y., Komoda, N., May 1999, Simulation
Tool Of Supply Chain Model With Various Structure
And Decision Making Processes. ThIEEE Conference,
Vol. 2 , pp 1443-1449.
Maskell, B., 2001, The Age Of Agile Manufacturing. Supply Chain Management: An International Journal,
Vol. 6, ISSN 1359-8546.
Ramberg, J. S., Dudewicz, E. J., Tadikamalla, P. R.,
Mykytka, E. F., May1978,
A Probability Distribution And Its Uses In Fitting Data. ASQC Chemical Division Technical Conference.
Rose, 0..Jan 1999, Estimation Of The Cycle Time Distribution Of A Wafer Fab By A Simple Simulation
Model. In Proceedings of rhe SMOMS ‘99 (1999
WMC), pp. 133-138.
Schunk, D., Dec 2000, Using Simulation To Analysis Supply Chains. Winrer Simularion Conference, Vol. 2, pp
1095-1100.
Shankar, A., Kelton, W., Dec.1999, Emperical Input Distributions: An Alternative To Standard Input Distributions In Simulation Modeling. Winrer Simulation Conference, pp 978-985.
Shirodkar, S., Dec.1999, A Modular Approach For Modeling And Simulating Semiconductor Supply Chains.
Masters thesis at Arizona State University.
Stevens, J., 1989, Integrating The Supply Chain. International Journal of Physical Distribution & Material
Managemenr, Vol. 19, pp 3-8.
Towill, D., 1996, Industrial Dynamics Modeling Of Supply
Chains. International Journal Of Physical Disrriburion
And Logistics Managemenr, Vol. 26, No.2, pp 23-42.
Towill, D. R., 1995, Time Compression And Supply Chain
Dynamics. Logistics Internarional, Sterling publicarions, London, pp 43-7.
Turner, S., Gan, P., 2000, Adapting A Supply Chain Simulation For HLA. 4Ih IEEE Internarional Workshop, pp
71-78, Aug. 24-26.

at Advanced Micro Devices. His research interests include
modeling, analysis, and control of semiconductor manufacturing systems. Dr. Fowler is the co-director of the Modeling and Analysis of Semiconductor Manufacturing Laboratory at ASU. The lab has had research contracts with NSF,
SRC, SEMATECH, Infineon Technologies, Intel, Motorola, ST Microelectronics, and Tefen, Ltd. He is a member of ASEE, IIE, IEEE, INFORMS, POMS, and SCS. His
emailaddress is <john.fowler@asu.edu>

KRAIG KNUTSON is an assistant professor in the Del E.
Webb School of Construction at Arizona State University.
He holds a bachelor’s and master’s degree in construction
and a Ph.D. in industrial engineering from Arizona State
University. His research interests are related to the design,
simulation and optimization of manufacturing systems and
construction processes. He is a member of IIE, INFORMS,
AACE, AIC and ASCE.
ESMA GEL is currently Assistant Professor of Industrial
Engineering at Arizona State University. Her research interests are stochastic modeling and control of manufacturing systems and her current work is on agile workforce
policies in various production environments. She is a
member of INFORMS, IIE and ASEE. She completed her
Ph.D. studies in 1999, at the Department of Industrial Engineering and Management Sciences of Northwestern University where she also received her M.S. degree in 1996.
She earned her B.S. degree in Industrial Engineering from
Om Dogu Technical University, Ankara, Turkey and was
awarded the Walter P. Murphy Fellowship by Northwestern University for graduate study in 1994.

DAN SHUNK is a Full Professor of Industrial Engineering
at Arizona State University and former Director of the
CIM Systems Research Center. He is currently pursuing
research into global new product development, modelbased enterprises and global supply chain. His latest book
is Inteerated Process Desien and Development. an Irwin
publication. Dr. Shunk studied at Purdue where he received
his Ph.D. in Industrial Engineering in 1976. He is cofounder of the USAF Integrated Computer Aided Manufacturing (ICAM) Program where he launched such industry standards as IDEF and IGES, former manager of Industrial Engineering at Rockwell, former manager of
manufacturing systems at International Harvester, and
former VP-GM of the multi-million dollar Integrated Systems Division of GCA Corporation. Dr. Shunk has served
on the Board of Advisors of CASA of the Society of
Manufacturing Engineers, and chaired CASA in 1993. He
helped Motorola conceive Motorola University and has
served on their faculty since 1984. He is on the Editorial
Board of the Aeilitv and Global ComDetition Journal and
the International Journal of Flexible Automation and Integrated Manufacturing, He is an active member of the Inter-

AUTHOR BIOGRAPHIES
BRETT MARC DUARTE received his Master’s degree
in Industrial Engineering from Arizona State University in
May 2002. He has a specialization in manufacture of
semiconductors, and his interests lie in simulation and
modeling, with an emphasis on supply chain management
and integration. His email address is <Brett.
Duarte@asu.edu>

JOHN W. FOWLER is an Associate Professor in the Industrial Engineering Department at Arizona State University. Prior to his current position, he was a Senior Member
of Technical Staff in the Modeling, CAD, and Statistical
Methods Division of SEMATECH. He received his Ph.D.
in Industrial Engineering from Texas A&M University and
spent the last 1.5 years of his doctoral studies as an intern

1335

Duarte, Fowler, Knurson, Gel, and Shunk
national Federation of Information Processors (IF'IP) Committee 5.3 on CIM. He is a senior member of SME and
IIE. He won the 1996 SME International Award for
Education, the 1999 and 1991 Industrial Engineering Faculty of the Year award, the 1989 SME Region VI1 Educator of the Year award, chaired AutoFact in 1985, and won
the 1982 SME Outstanding Young Engineer award. For the
year 2000 he has been nominated as the US Alternate to
the Intelligent Manufacturing Systems project.

1336

1022

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Petri Net Translation Patterns for the Analysis of
eBusiness Collaboration Messaging Protocols
Andrew L. Feller, Teresa Wu, Dan L. Shunk, and John Fowler

Abstract—Electronic messaging protocols such as RosettaNet
(RN) automate the asynchronous exchange of business documents between collaborating trading partners over the Internet.
Such protocols commonly employ mechanisms such as retries,
time-outs, and fault handling to overcome uncertainty in the
timing and reliability of message transmission, receipt, and
processing. To determine the reliability and performance of such
a protocol under varying network and message processing conditions, we have developed reusable patterns for Petri net modeling of these common mechanisms and used them to assemble
a timed Petri net simulation that represents the RN standard’s
behavior. The reusable patterns are derived through translation
of the abstract representations provided in the RN standard into
an executable model, paving the way for a multimodeling approach for supply chain communications. The resulting stochastic
Petri net is simulated to generate performance curves guiding
improved protocol reliability.
Index Terms—Communication system fault tolerance, message
systems, modeling, multimodeling, Petri nets, protocols, simulation, supply chain.

I. I NTRODUCTION

P

ETRI net models have a good track record for performance
validation of time-dependent concurrent processes, such as
communication and messaging protocols, as demonstrated in
[1]–[3]. One challenge in using this approach, however, is the
need for the analyst to generate complex Petri nets from scratch,
which accurately represent a protocol’s behavioral rules and
implied event sequencing. To make this modeling process easier
and more reliable, we have developed preset patterns for model
generation. Our purpose in presenting this research is to provide
a reusable set of such patterns that can improve the analysis,
design, and implementation of messaging protocols used to
integrate business processes across company boundaries. This
paper focused on modeling the popular RosettaNet (RN) supply
chain collaboration standard; however, the same patterns will
be useful for modeling other communication and messaging
protocols. This research contributes to the practice of model
generation and presents analysis results characterizing the robustness of an RN Partner Interface Process (PIP) under varying
Manuscript received June 15, 2007; revised July 11, 2008. First published
August 14, 2009; current version published August 21, 2009. This work
was supported in part by a grant from Intel Corporation. This paper was
recommended by Associate Editor M. P. Fanti.
A. L. Feller is with the Arizona State University, Tempe, AZ 85282 USA, and
also with American Express Technologies, Phoenix, AZ 85027 USA (e-mail:
andrew.feller@asu.edu).
T. Wu, D. L. Shunk, and J. Fowler are with the Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail:
teresa.wu@asu.edu; dan.shunk@asu.edu; john.fowler@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TSMCA.2009.2025022

message timing and loss rate conditions. In particular, we used a
timed Petri net (TPN) simulation derived from the RN protocol
to determine the rates at which a single-action PIP will fail
when individual messages are lost or delayed by the network,
or when a trading partner’s acknowledgment response is delayed. This question is important because of the number of RN
transactions that are processed by firms on a continuous basis.
Reviewing RN log files at a prominent semiconductor firm
showed over 400 000 PIPs processed in a month with a failure
rate of approximately 3% [4]. It is of considerable importance
to determine whether the protocol’s dynamic behavior could be
part of the problem.
Our research was conducted in the context of developing
methods for translating supply chain models between levels
of abstraction. Supply chain models fall into two paradigms:
conceptual models that represent the supply chain in ways
that are readily accessible for practitioners to comprehend
and analysis or execution models based on mathematical and
computer representations used to simulate and analyze system
performance. Conceptual models are easy to understand and
have face validity but do not allow for detailed model execution
and analysis of system-wide behavior. Formal analysis and
execution models allow system behavior and performance to be
analyzed or executed in a simulation environment but require
knowledge of the methodology being used and significant
expertise to build. As supply chains grow in complexity and
interdependence, more expressive and complete methods are
needed for translating and integrating conceptual and analytical
modeling approaches to enable the analyses of the integrated
flows of information and materials that determine a supply
chain’s performance.
Our approach uses a multimodeling method developed using
conceptual graphs, which is a knowledge representation method
from the field of artificial intelligence (AI) [5]. The approach
builds on systems theory [6] to generate translation patterns
using conceptual graphs that capture all required behaviors
specified in the RN protocol. The patterns are then used to
generate an executable TPN.
There are three contributions made by this research. First, we
define a set of patterns for translating RN PIPs into Petri nets
which accurately represent the messaging behavior and event
sequencing that occurs under the RN implementation framework (RNIF). Next, we apply these patterns to generate a complete Petri net model representing the asynchronous behavior of
a single-action PIP including all possible state/event paths that
the protocol can lead to, including successful completion, failure processing, and recovery. Finally, we use the derived Petri
net to analyze the performance of a PIP to determine robust

1083-4427/$26.00 © 2009 IEEE

FELLER et al.: PETRI NET TRANSLATION PATTERNS

thresholds for message timing and loss rates. In particular, we
answer the question “what are the network thresholds below
which RN’s performance becomes unacceptably degraded?”
The following sections are organized as follows. In
Section II, we provide background on multiparadigm modeling,
RN, and the elements of Petri nets and conceptual graphs relevant to our approach. We then present our translation patterns in
Section III, followed by the generation of a complete Petri net
representing a single-action PIP in Section IV. In Section V, we
present analytical results for the PIP’s performance, followed
by conclusions and further research in supply chain multimodeling in Section VI.
II. BACKGROUND
This section discusses prior works in multiparadigm modeling and introduces RN PIPs and RNIF along with the elements
of Petri nets and conceptual graphs used in our approach to
model translation and analysis.
A. Multimodeling
This paper builds on research and practice in multimodeling
frameworks. Multimodeling entered mainstream system analysis in the mid 1990s with a number of candidate approaches
resolving into the Unified Modeling Language (UML) standard,
which is an object-oriented system-modeling method that has
gained wide practical acceptance and use [7], [8]. UML’s
success is due in large part to the successful balancing of
the conflicting goals of human comprehension versus rigorous
analysis [9]. UML diagrams represent systems using succinct
diagrams that aid human understanding; however, the syntax
and semantics do not directly enable formal analysis and execution. UML behavior diagrams are used to represent RN PIPs;
however, an approach for model translation and simulation to
analyze RN’s capabilities had not been accomplished.
To our knowledge, few works have been done to translate
between abstraction levels in supply chain models; however,
multiparadigm model translation is an emerging field with new
methodologies under development. Zeigler first developed a
formal representation for discrete event simulation and discussed abstraction techniques [10]. Fishwick’s early work in
simulating complex processes over multiple levels of abstraction in 1986 [11] began a stream of research into multimodeling
[12], [13]. Fishwick and Zeigler reported on a multimodeling
methodology used to bridge levels of abstraction in qualitative
models [14], yet the proposed framework did not solve the
translation between multiparadigm models. As Fishwick and
Ziegler [14] stated:
The problem of semiautomating the process of taking
a conceptual nonexecutable object-based model of the
system and converting it into an executable model remains
a very hard problem that has taken form in AI, simulation,
and software engineering.
Subsequent work by Fishwick and others has developed
multimodeling methods for real-time simulation [15] and general multimodeling software frameworks [16]. More recently,
Vangheluwe et al. [17] describe the emerging field of multi-

1023

paradigm modeling, asserting it can assist with the following:
1) transformation between models described in different formalisms; 2) clarifying relationships between models at different levels of abstraction; and 3) describing models in formal
specifications or metamodeling.
A number of multiformalism metamodels have been developed for translating between models described in different
formalisms [18]; however, the focus has been on linking analytical models, such as in the hybrid environments surveyed by
Barton and Lee [19], which link discrete event simulation with
differential and algebraic equations, and not with linking across
levels of abstraction.
Mapping between levels of abstraction in simulation frameworks is beginning to be addressed at simulation conferences
[17], [20]–[22]; however, model translation in the context of
supply chains has not been addressed. Current research works
reported in the IEEE T RANSACTIONS ON S YSTEMS , M AN ,
AND C YBERNETICS —PART A, ACM Transactions on Modeling and Computer Simulation, and IEEE T RANSACTIONS
ON C ONTROL S YSTEM T ECHNOLOGY [18], [23], [24] focus
on modeling imbedded control systems and translating between
analytical formalisms. Kim et al. [25] use hybrid modeling
to represent traffic networks; however, specific techniques for
translating supply chain models are not considered. Still, some
of the conclusions reached for analyzing these systems are
relevant to the supply chain domain. For example, Lee and Hsu
[26] translate between UML diagrams and Petri nets for model
execution and conclude that UML diagrams lack the precise
semantics needed for qualitative and quantitative analyses of
system behavior. Lin and Jeng also transform UML diagrams
into Petri nets to verify the logical correctness and dynamic
behavior of a system framework [27], and Du et al. [28]
extend message sequence charts using Petri nets to analyze the
soundness of cooperative workflows. Similar to our approach,
these researchers all use Petri nets for analysis. In their works,
however, they provide no formal mechanism or patterns for
translation, relying on the modeler’s expertise to perform the
mapping. Researchers have translated UML diagrams into several variants of Petri nets, such as object coordination nets [29],
[30], colored or high-level Petri nets [31]–[33], generalized
stochastic Petri nets [34], [35], and other approaches including
generalized semi-Markov processes [36], queuing networks
based on execution graph/machinery models [37], and firstorder temporal logic [38]. These studies, however, did not focus
on patterns for model translation or on messaging protocols.
Similar to this paper, prior UML diagram translations have
focused on the behavioral diagrams that come closer to defining
a finite state machine for an object’s behavior. The addition of
rigor for execution is required due to problems in the formal
tractability of statecharts for specifying and verifying designs
[39]. The UML diagrams and derived executable models share
properties that allow the modeling primitives to be correlated, leading to partial mappings of one modeling approach
to another. Graph grammars provide a compact method for
translation between graphical system representations but lack
the semantic richness needed to map from abstract conceptual
models to an executable representation. This leaves a gap in
the current research—translation patterns between conceptual

1024

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Fig. 2.

Fig. 1. UML diagrams representing single-action PIP 4A1: “Notify of Strategic Forecast.”

and executable models needed in the supply chain domain.
Our method for model translation uses system theory and
conceptual graphs to fill this gap. We build on Fishwick’s
framework [13] for classifying models using system theory to
organize our approach and provide a formal basis for model
translation.
B. Conceptual Model: RN
RN PIPs are specialized system-to-system eXtended Markup
Language (XML)-based dialogs used to execute business
processes between trading partners [40]. Each PIP specification
includes a business document with XML vocabulary and a
business process with the choreography of the message dialog.
Several hundred different PIPs are used to automate standard
information flows between supply chain participants, such as
exchanging forecasts and transmitting orders, and thousands
of partner connections have been implemented in industry.
It is not unusual for a trading partner to execute millions
of PIP transactions in a year, yet the conceptual model for
most exchanges is quite simple. Every PIP specification uses
a UML sequence diagram to document a prespecified twoway transaction pattern for the intercompany business process
and a modified UML statechart diagram to further define the
PIP functional behavior (see Fig. 1) [41]. In this example, the
success path has one trading partner sending a business message
containing a forecast notification. Once the message format is
verified, the receiving partner sends an acknowledgment.
In addition to these diagrams, PIP specifications contain
tables and text specifying trading partner roles and performance
controls, such as retry and time-out parameters.
The general behavior for fault management required of all
PIPs is specified in the RNIF. RN uses a two-stage process
for validating and processing business messages, which first
determines if it is readable and well formatted. If it is, an ac-

Two-stage validation and processing of an RN message.

knowledgment signal is returned to the sender. If not readable at
all, the receiver waits for a resend. If readable but not correctly
formatted, an exception signal is sent, and the originator of
the PIP aborts processing. After a correctly formatted message
is acknowledged, the receiver then processes the message’s
business content. This second stage can complete normally or
find a business rule violation in the content, leading to a subsequent error condition. Depending on whether the PIP would
have been completed by this message and acknowledgment
signal or would have required a subsequent responding business
message, the failure to process the business content results
in either an exception signal message or the initiation of a
Notification of Failure (NoF)—PIP 0A1. The invocation of the
NoF PIP begins another pattern of PIP execution with a twostage process for validating and processing the NoF message
and its own sequence of retries and message processing. This
general pattern for the two-stage process of handling a received
RN message is shown in Fig. 2. The RNIF specification provides flowcharts and descriptive text defining these message
validation and error handling processes, but no methods or
guidelines for analysis.
We have chosen RN as our conceptual model for three reasons. First, it provides a structured starting point for developing
model translations for supply chain messaging.
Second is our interest in determining causes for observed
failure rates in RN transactions. Finally, the patterns derived
from RN can be used to analyze other protocols, such as the
European Internet Electronic Data Interchange (EDI) guidelines [42].
C. Translating to an Executable Model: Petri Nets
A Petri net is a bipartite directed graph of alternating place
and transition nodes connected by arrows [43]. States are
represented by places shown as circles on the graph. Events are
represented by transitions that are shown as bars. The places can
hold tokens represented by dots within a circle. Finally, a transition rule specifies how tokens migrate through the network.
The basic rule is that all of the places preceding a transition
must hold a token in order for the transition to fire. Firing the
transition then removes a token from each of the transition’s
input places and puts tokens in each of the transition’s output
places. Formally specified, a Petri net structure “S” is as a
four-tuple: S = (P, T, I, O), where P = {p1 , p2 , p3 , . . . , pn }
is a finite set of places, T = {t1 , t2 , t3 , . . . , tm } is a finite set

FELLER et al.: PETRI NET TRANSLATION PATTERNS

of transitions, the set of places and transitions is disjoint, I is
the input function that maps places to transitions, and O is the
output function that maps transitions to places.
Sometimes the I and O functions are combined into a single
flow function F . The state of a system is represented by tokens
using a marking vector M with an integer valued number of
tokens (≥ 0) at each place in P .
A number of Petri net variants have been applied to supply chain and communication protocol modeling. This paper
focused on timed place transition (P/T) nets. A P/T net adds
integer valued arc weights and places capacities to the net
definition to achieve a more concise representation. TPNs allow
the representation of temporal behavior by adding a duration
parameter to the transitions of a Petri net. This allows the
execution of a modular form of discrete event simulation that
can be used to characterize system behavior using traditional
simulation techniques. The duration or delay timings can also
be represented as random variables. The assignment of exponentially distributed random variables to the duration periods
in a TPN defines a stochastic Petri net. Petri nets have been
used for modeling and analysis of the quality of Web services
by Xiong et al. [44] and combined with network simulation
tools by Ye and MacGregor to enable formal verification of the
correctness of a network protocol [45].
We build upon this background to develop Petri net translation patterns that accurately represent the behavior of the RN
messaging protocol and the RNIF. To document the patterns,
we use conceptual graphs. Conceptual graphs are a synthesis
of Peirce’s existential graphs [5] with dependence graphs and
the semantic networks of AI and can be translated to logically
equivalent expressions in predicate calculus [5], [46]. A conceptual graph is a connected bipartite graph made of concept
nodes and conceptual relation nodes. Every relation node has
one or more arcs, each of which must be connected to some
concept. Concepts are mapped into a set of type labels, and
the type labels are arranged into a hierarchical lattice called a
type hierarchy which imposes a partial ordering based on the
meaning of the concept types. Conceptual relations are also
mapped into a hierarchy with relations of the same type having
the same number of arcs.
Conceptual graphs have been used in the development of
expert systems, semantic database retrieval, inference capabilities, and natural language processing. Feller and Rucker used
conceptual graphs to extend structured analysis modeling in
the analysis of shop floor system requirements, developing a
metamodel for system analysis that generated a TPN [47], [48].
Willumsen [49] developed a method for generating executable
prototypes from conceptual models using rule-based languages,
but this was a general approach for model execution focused
on information system design and not tailored for supply chain
modeling. Baresi and Pezze [31] used graph grammars to
develop customization rules for translating UML models into
Petri nets; however, the graph grammars lack the richness of
conceptual graphs’ ability to ground the mappings semantically
and the extensibility to develop new and varying translations. In
our use of conceptual graphs to generate Petri net patterns for
RN, we limit our type hierarchy of concepts to two fundamental
types: states (Q in system theory notation) and transitions (δ in

1025

Fig. 3. Example of a conceptual graph translation pattern.

system theory notation). The states and transitions are labeled
to provide semantic relevance for the translation patterns.
A simple example for translating a portion of the RNIF
flowchart for sending a message that also enables retries into
a Petri net is shown in Fig. 3.
III. T RANSLATION PATTERNS
Translation patterns for generating Petri nets for RN PIPs
were derived by first analyzing PIP and Petri net representations
using the system theory framework to characterize the required
mapping. This clarified underlying patterns of abstraction,
specification, and representational capability that the modeling
approaches provide. This was followed by conceptual modeling
of the constructs needed to span the gap from the RN model to
an executable Petri net.
A. System Theoretic Analysis
The initial step uses the seven elements of system theory
to analyze RN PIP and RNIF specifications, including text,
tables, and diagrams. Our analysis of the expressive capability
of the RN model from a knowledge representation standpoint
provides a formal basis for correlating modeling elements from
RN to the states and events of a Petri net.
To demonstrate, consider an example PIP: 4A1—Notify of
Strategic Forecast and system theory’s time set variable T . In
the RN conceptual model, time is captured by numbering steps
in a UML sequence diagram in a fashion similar to a flowchart.
The business operational view for a PIP specification provides
the modified statechart diagram shown in Fig. 1 [41]. Start and
end state conditions are given in the PIP specification along
with a 2-h time limit to acknowledge receipt and a maximum
retry count of three. An RN UML sequence diagram for this
interaction set specified in the functional service view of the
PIP specification is also shown in Fig. 1. This diagram shows
the sequencing of the forecast notification and acknowledgment, which is a simple two-step handshake. Based on these
views, the time set for this PIP can be described as an index
variable representing the time at which the system reaches one
of four possible states. The possible states are the initial or
START state, a “Request Sent” intermediate state, and either
a successful “Acknowledge Received” END state or a FAILED
state. The examination of the intermediate request sent state,
however, shows that there are elements of the state set Q that
are not represented. This state may last for up to 8 h (up to three
retries with 2-h maximum duration each for acknowledgment);

1026

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

TABLE I
A NALYSIS OF RN AND P ETRI N ET M ODELS U SING S YSTEM T HEORY E LEMENTS

however, the UML diagram does not represent the number of
requests sent or how long the state might endure—these are implicit and result from other information provided in tables in the
PIP specification and the rules for retries specified in the RNIF.
Comparing the time set for the target executable model, a
TPN formulation for simulating this sequence would require a
place initialized with a token count to explicitly represent the
allowed number of retries, a counter place for the number of
attempted retries, a failure transition that requires three retry
tokens, and a timed retry transition. This indicates the need for
additional modeling primitives in the executable representation
that the RN diagrams do not contain. As demonstrated using
this example, the time set T and state set Q are modeling
elements that must be augmented when translating an RN PIP
into a Petri net.
In similar fashion, our analysis focused on identifying all differences in the seven system modeling elements in the RN specifications and the required elements needed in a Petri net model.
We cataloged the modeling capabilities of the RN specifications
and the required Petri net, and their respective placement in
the system theory framework. The application of this approach
identified five key differences. These were the following.

end may be stateless. The Petri net, on the other hand,
must include explicit places for the intermediate states of
each trading partner and must represent the dependence
structures that determine timing and sequencing for each
of these states.
4) The transition function δ. Petri net transitions and execution rules are required to allow net execution, and a complete transition function between all states is required.
The state changes represented on RN PIP diagrams are
not fully formed since the trading partner roles have
no internally defined states and, therefore, no transition
function.
5) The output function λ. Finally, the RN specification
has no formal output analysis functions such as reachability analysis. This is an obvious difference between
conceptual and executable models: the lack of a formal
capability for output analysis of system behavior. A Petri
net allows analysis for specific properties of the overall
system. Various output functions are needed, depending
on the desired analysis, for example, analysis for possible
deadlock states, or simulation results to characterize RN’s
performance.

1) The time set T . As described, the representation of explicit time sequences is needed for intermediate states
and events. Some information about timing is provided
in text, tables, and flowcharts in the PIP specification but
is not included in the specification in a structured format.
2) The output set Y . RN PIPs do not have a fully defined output function. The sequence diagrams show a
return response, but the RN specifications call for failure
processing to be specified in individual trading partner
agreements, allowing the possibility of an ambiguous end
state. The Petri net will require specific places to hold
tokens representing the end state of both trading partners.
An additional specification of places is needed for representing all messages that retries and error processing may
require.
3) The state set Q. RN does not specify a complete set
of internal trading partner states. The diagrams for the
PIP show that the internal conditions other than start and

These results are summarized in Table I. Our analysis exposed a lack of system theoretic elements in the RN PIP and
RNIF specifications needed to analyze a PIP’s behavior.
In particular, RN specifies a limited time set, state set, output
set, and transition function and has no output function for analytical results. RN does specify a more thorough input set and
a detailed definition of the allowed content for each message.
The gaps we identified pinpoint the model elements that need
to be added to an RN conceptual model to make it executable.
In particular, a Petri net model for a PIP must add places to
represent all possible messages, including exception signals and
notifications of failure, and requires a sequential state/execution
model that reflects the implied time set, including retries,
acknowledgment time-out, and exception processing, to derive
the complete set of possible states, i.e., the output set.
The next step in developing general patterns for RN PIPs was
recognizing that partner roles interact solely by the exchange of
interface messages in specific sequences and time frames and

FELLER et al.: PETRI NET TRANSLATION PATTERNS

1027

Fig. 4. All possible interface messages for a single-action PIP success or
failure.

listing these interface messages as states with transitions for
sending and receiving them. For the single-action PIP patterns,
there are eight possible interface messages:
1) single-action PIP messages
a) initial message: Msg;
b) acknowledgment: Ack;
c) exception: Excp;
2) NoF messages
a) sender NoF: S-NoF;
b) acknowledgment of sender NoF: S-NoF Ack;
c) responder NoF: R-NoF;
d) acknowledgment of responder NoF: R-NoF Ack;
3) out of band messages
a) manual reset.
These messages are organized for generating the Petri net, as
shown in Fig. 4, with the interface messages in the middle
and send/receipt transitions on either side representing the
trading partner’s actions. The success path for this type of
PIP is represented by the top two rows of two message places
and two send/receive transition pairs. All other messages and
send/receive transitions are used for failure processing.
Next, a set of patterns for modeling the internal states of the
trading partners connected to the interface messages is specified
using conceptual graphs.
B. Conceptual Graph Pattern Mapping
RN is a message passing protocol; thus, we begin by focusing
on the business and signal (acknowledgment and exception)
messages that can be sent and the process specifications for
sending and receiving them.
RNIF provides flowcharts to define the process for message retries, receipt, acknowledgment, validation, and failure
processing; however, the flowcharts cannot be directly translated due to ambiguous states and processes in the conceptual
model. To execute properly, two patterns were formulated to
represent the messaging structure. The first, shown in Fig. 5,
handles retries beginning with a start state and the initiating
“send message” event. This is followed by two states: a sent
message and the enablement of retries. Subsequent states and
events capture the retries and the possible resulting states, in-

Fig. 5. (a) Conceptual graph: Send message with retries. (b) Resulting Petri
net pattern for retry mechanism.

cluding a retry failure, an acknowledgment leading to success,
or receipt of an exception signal leading to an exception failure.
Note that there is a general pattern that can be used to model
retries with a transition that initiates retries (A), and output for
retries (B), retry failure (C), and transitions that halt retries due
to success or failure events (D).
The second pattern follows from Fig. 2, represents the receipt, validation, and processing of a business message by the
message recipient, and signals results to the sender. This pattern
begins with the message-sent state from the prior graph and
allows the recipient to handle unreadable or invalid messages.
As specified in the RNIF, this pattern also enables resending
an acknowledgment or exception signal if a subsequent retry
message is received. These patterns are shown in Fig. 6.
The other significant pattern required for the analysis of
an RN PIP behavior is processing a failure notice: RN PIP
0A1—NoF. RN has defined this special PIP for handling failure
notification when a failure arises after the normal processing of
a PIP may be completed by the other trading partner. This requires special handling to ensure that the status of the business
process being conducted remains synchronized between both
parties. When a NoF is received, the recipient must abort the
current process and prevent subsequent processing no matter
what state he/she is in. This requires an interrupt scheme and
the capability to clear any possible state occurring when the
NoF is received. In addition, the NoF receipt must be acknowledged to alert the trading partner when (and whether) the
process state has been synchronized; otherwise, an out of band

1028

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Fig. 6. Conceptual graph and resulting Petri net pattern for RNIF compliant message receipt, validation, processing, and signaling of success or failure.

communication is required. The conceptual graph pattern and
resulting Petri net structure for processing a failure notice are
shown in Fig. 7. This graph starts with the failure notification
state and includes a general pattern of linkages to the potential
state and event sets that must be cleared or aborted when a NoF
message is received. Note that, for clearing an undetermined
state, the model requires an intermediate state for processing
the NoF and a series of events (labeled δA , δB , δC , etc.) for
clearing any potential state that the trading partner might be in.
The output of this graph is a failure state for the PIP initiator
and a closed incomplete state for the PIP recipient. This pattern
must be combined with the first pattern for sending an action

message, so that retries are generated for the NoF as well. As
specified in the RNIF, a retry failure for the NoF does not
generate another NoF but goes to an out of band communication
(e.g., a phone call) to reset the process.
The translation patterns shown in Figs. 5–7 augment the RN
model for analysis. The time set is specified using timed transitions, the state and output sets now contain all intermediate
states and possible outcomes, and the transition function is
well defined based on the constraint of alternating state/event
nodes and the use of Petri net firing rules. Output functions
are also available using Petri net analysis capabilities, such as
reachability graphs and simulations.

FELLER et al.: PETRI NET TRANSLATION PATTERNS

1029

TABLE II
R EGRESSION T ESTING PATHS FOR A S INGLE -ACTION PIP

Fig. 7. Conceptual graph and resulting Petri net structure for processing a
NoF—PIP 0A1.

The following conventions have been used to simplify the
conceptual graphs. State concepts are labeled with Q and
subscripted with a state label. Event or transition concepts
are labeled with δ and subscripted with an event label, and
timed events have a bold outline. Arrows without a conceptual
relation (circle) imply a functional flow relation linking states
and events. In one instance, an increment action is used. Conceptual graphs use a diamond for actors that can perform an
algorithm, for example, incrementing a counter for the number
of retries.
The only remaining element needed for model execution is
the input set Ω. Tokens are initially required in the set {QStart ,
QRetriesLeft (3), QNo Readable Msg , and QNo NoF Rcvd }.
Adding the initial marking completes the executable model.
IV. A PPLICATION —S INGLE -ACTION PIP
The patterns we developed were combined to compose a
complete Petri net for a single-action PIP. The first step was to
link the patterns to the business and signal interface messages
that can occur. For a single-action PIP, there is one business
message and two potential NoF messages that require retry
mechanisms. The business message can be responded to by

an acknowledgment or an exception, and the NoF can be
acknowledged or not—in which case, an out of band reset
message is required. These messages are represented by sent
message states at the interface between the trading partners.
Transitions are added to send, receive, and possibly lose each
of these interface messages. A composition process was used
to combine the patterns to generate the complete Petri net.
One key feature of the net is a processing gate with an
initialized token indicating that no NoF has been received. This
place is used as an input and an output for all processing steps
other than those used to process a failure notification. This
freezes processing regardless of state when the trading partner
receives a failure notice.
It should be noted that earlier attempts to develop a Petri
net representing RN PIP behavior without these patterns were
considerably more difficult and time consuming, yet resulted
in a network that was overspecified and had process anomalies
that were only discovered later during testing. This motivated
our development of reusable patterns representing common
building blocks of protocol behavior.
Regression testing of the pathways that the process can take
was initially used to determine the correctness of the result. This
regression testing exercised all of the contingencies identified
in Table II. Through structured token game exercises with
the model, we were able to validate correct processing and
address unforeseen contingencies when augmenting the model
for continuous simulation.
The resulting Petri net structure is shown in Fig. 8. Surprisingly, the resulting network for a simple single-action PIP has
238 562 possible states, making the analysis of the reachability tree cumbersome. For this reason, the resulting structure
was additionally tested using linear temporal logic (LTL) with
Büchi Automota using the TPN analyzer (TINA) from the

1030

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Fig. 8. Complete Petri net for a single-action PIP.

LAA–CNRS [50] to assure that the only output (deadlock)
states correspond to the synchronized outputs specified by the
protocol (END or FAIL) and that one of these final states is
always achieved.
LTL statements for checking this are shown in (1) and (2)
using LTL symbols [ ] (box) for always and  (diamond) for
eventually
[ ] (QStart ⇒   ((QEnd ∧ QComplete )
∨(QFailed ∧ QClosedIncomplete ))) .

(1)

Equation (1) indicates that the start state always eventually
leads to either the End/Complete or the Failed/Closed Incomplete state. To assure that these are the only end states, the LTL
checker is used to test (2). This tests that the End/Complete
and Failed/Closed Incomplete states are the only cases that
deadlock
¬ ((QEnd ∧ QComplete ) ∨ (QFailed ∧ QClosedIncomplete ))
⇒ ¬DEAD.

(2)

Results from these tests were true, indicating that the model
allows only synchronized end states. Additional verification
using reachability and simulation ensured that the model’s
behavior matched the RN specification.

We augmented the Petri net with additional structure to
reset initial conditions and enable continuous processing of the
modeled protocol with variable random choice of process paths,
message loss rates, and variable random delay timings. This
allowed running the model for a 100 000 iterations in a few
minutes, allowing all paths to be exercised and verifying that the
frequency of intermediate states conforms to what is expected
and allowable.
This also allowed controlled experimentation for performance analysis. Results from these experiments are reported
in the following section.
V. P ERFORMANCE A NALYSIS
The Petri net structure was created and analyzed using the
set of publicly available research tools shown in Fig. 9. The
Platform Independent Petri net Editor [51], [52] was used to
construct the net and perform visual token game simulations,
invariant analysis, and model testing. This editor generates a
standard Petri Net Markup Language file that is readily input
into the TINA tool mentioned earlier for LTL model checking
and formatting. A textual output from TINA was then modified
manually to generate an input file for a TPN tool [53] for
running simulations. To simplify the generation of multiple
runs, additional patterns were devised to reset token markings to

FELLER et al.: PETRI NET TRANSLATION PATTERNS

Fig. 9.

1031

Petri net tools used for model construction and analysis.

the original start state and capture protocol success and failure
counts for output analysis.
The augmented Petri net for TPN simulation requires no
transient since each run starts in a known state and can run
many thousands of iterations in less than a minute. The TPN
tools allow probabilities to be defined for the transitions following free-choice places. This allows network message loss
rates to be specified using transitions for lost messages that
were added to each interface message. The tools also allow
for deterministic or stochastic (exponentially distributed) timed
transitions. Deterministic transitions were used for the time-out
of retries and stochastic transitions for message transmission
and acknowledgment processing times. C language functions
for pseudorandom sampling imbedded in the tool generate the
delay. The TPN tools were operated with a script to enable input
parameters to be modified for each set of runs. One hundred
thousand runs were simulated for each setting to generate a ratio
of protocol successes versus failures.
Two research questions were considered: 1) What is the impact of network conditions on RN’s success rate and 2) what is
the impact of a trading partner’s delay to acknowledge a message on the success rate? The network condition question focuses on how robust the protocol is to network-caused message
losses and stochastic message delivery timing. In other words,
what are the network performance thresholds below which
the RN protocol’s performance begins to degrade and become
unacceptable? The second question considers how sensitive the
protocol is to acknowledgment processing and response time. In
other words, how long is “too long” for a trading partner to send
a response. Full factorial experiments were run to answer these
questions, running the Petri net over a wide range of reasonable
parameter values.
A. Sensitivity to Network Conditions
Experiments were run to determine the protocol’s sensitivity
to message transmission speed and loss rate. With variable conditions on the Internet and trading partners that span the globe,
messages take varying amounts of time to get to their destination and may be lost with varying frequency. For our analysis,
we varied random message loss rates in 1% increments from
0.1% to 10% and varied the average message speed using an
exponentially distributed processing time in increments from
6 to 24 min. Acknowledgment time was pegged at twice the
message transmission time to allow for reasonable messageacknowledgment turnaround. These experiments showed that

Fig. 10. Protocol sensitivity to network conditions.

the RN protocol for a single-action PIP achieves highly reliable
communications (> 99%) as long as the message loss rate
is below 6% for very fast (i.e., 6 min) message transmission
and below 3.5% for very slow (i.e., 24 min) transmission. For
message loss rates above 4%–5%, the performance degrades,
losing about 0.4% in yield for each percentage increase in individual message loss. These results are summarized graphically
in Fig. 10, where transmission times were adjusted to provide a
visible spread in the curves. These results are not tractable analytically due to repeated interactions between retried messages
and probabilistic message loss and delay timing.
B. Sensitivity to Acknowledgment Processing Time
Additional experiments were run to determine the protocol’s
sensitivity to delays in acknowledgment response time. Using
an exponential random variable for delay, the average delay
time was varied in regular increments from 1.2 min to 15 h. The
protocol is very robust to acknowledgment delays as long as the
acknowledgment time is less than an hour and then degrades
quickly, losing nearly 10% for each additional hour of average
delay. This is not surprising; the protocol is asynchronous and
should be able to handle reasonable delays in response.
Still, it shows that average downtimes longer than an hour
in a trading partner’s responding system will begin to create
failed PIPs that need to be handled. These results are shown
graphically in Fig. 11.
After completing our simulation analysis, we determined
that the result for exponentially distributed delay times might
be determined analytically by combining three cumulative

1032

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

VI. C ONCLUSION AND F UTURE R ESEARCH

Fig. 11. Protocol sensitivity to acknowledgment delays—simulated and analytical results.

exponential distributions. If the probability of success for a
single retry is governed by the cumulative exponential distribution in
P(success) = 1−e−(Time−out/(ProcessingDelay+AckTransmitDelay))
(3)
then assuming independence with three retries, the protocol
yields a success/delay curve according to




Y ield = 1− 1−P(success) 1−P(success) 1−P(success) . (4)
Graphing the analytical versus simulated results (Fig. 11)
verifies the Petri net simulation; however, the simulation can
be used to determine performance parameters that analytical
solutions cannot such as cycle time. Cycle time analysis performed using our model showed potential business impacts in
scenarios where RN is used to transmit time-sensitive data such
as for just-in-time inventory replenishment.
We found that, when degraded network performance and
response delays cause PIPs to fail between 0.025% and 1.5% of
the time, the successful PIPs complete in less than 2 h between
68% and 82% of the time and less than 4 h over 94% of the time.
The PIPs that do fail under these conditions, however, take
more than 6 h to fail and reset between 55% and 94% of the
time. Even with rapid partner response, network failure rates
that exceed 3% begin to cause failed PIPs to take over 6 h more
than 13% of the time, indicating that using RN for just-in-time
operations that must complete the same day may not be robust
unless network conditions are adequate.
The prediction of failure rates for the protocol based on
trading partners’ ability to respond might be used to assess the
costs of manually resetting failed transactions in a high volume
transaction environment. We have observed RN transaction log
files [54] that indicate a preponderance of failed PIPs come
from relatively few specific trading partners. This analytical
result supports the establishing of an objective benchmark for
acknowledgment response times in the 5-min range to improve
protocol performance.

We conclude with several comments about our work and a
discussion on future work to generalize and automate the use of
our patterns through the use of a composition algorithm.
This research has been validated through testing and analysis
augmented with comparisons to industry logs of RN transactions. We find the model consistent with the RN protocol
specification and our observations of the protocol’s behavior
in practice, but we were unable to characterize the log files
statistically for comparison with our results since network
delays, loss rates, and acknowledgment delays could not be
extracted from the logs and we were limited to viewing the logs
on site due to proprietary restrictions.
We also recognize a number of needed extensions, such as
analyzing the RN two-action PIP and other protocols such
as the GS1 EDI over the Internet Transport Communication
Guidelines [42] which allow for configurable retry schedules.
For the GS1 guidelines, it would be useful to determine how
tuning the retry mechanism can improve robustness under
degraded network conditions and processing delays.
An area of interest considered for future work is the generalization and automation of model translation using the messaging behavior patterns we developed. We have looked into
composition rules for using these patterns and have some useful
results. First, the protocol to model must conform to the same
rules as RN, such as timed retries, a two-stage process for
validation and processing of messages, and a rule that aborts
processing when a failure message is received. An algorithm
for composing a model using our patterns would then begin
by classifying the interface messages that are sent between the
parties into categories that commonly occur in communication
protocols. In RN, we have the following four: 1) messageWithRetry; 2) acknowledgmentSignal; 3) exceptionSignal; and
4) failMessageWithRetry.
Each message is represented by a place with transitions
attached to send, receive, and possibly lose the message. The
patterns attach to the appropriate message send and receive
transitions: retry patterns to send the messages with retry;
receipt pattern with validation, processing, and signaling of
results to the receipt of these messages; and failure processing
to the receipt of exception and fail signals. One key to making
an algorithmic process work is attributing the places and transitions in the patterns to indicate the states that must be cleared
and events that must be prevented when an exception or failure
notice is received. Then, the necessary “clear state” transitions
can be added and linked to a “Process Failure Notice” state
place to pull a token from each possible internal state place, and
internal event transitions can be linked to a “No-NoF Received”
place to prevent internal events when the process aborts.
We performed this algorithm manually when composing the
Petri net for our study; however, when extending the patterns
to handle a greater number of interactions such as in RN’s twoaction PIP, we found that considerable additional complexity
was introduced due to a proliferating number of potential
state combinations that occur across the trading partner boundary and the addition of a time-out clock maintained by the
PIP originator. Time-out is an additional failure mechanism

FELLER et al.: PETRI NET TRANSLATION PATTERNS

implemented in the RNIF for two-action PIP that is still being
considered in our research.
For example, whether an exception occurs during the initial
validation of message format or later during processing, the
message requires two different exception states. This is because, for the latter, the acknowledgment of message format
will deactivate the retry timer. Thus, one type of exception deactivates a timer, but the other does not. Adding the possibility
of a time-out failure for the PIP originator requires the addition
of new message places to handle exception signals that may be
sent after an acknowledgment signal has deactivated the timeout mechanism.
When we consider the challenge of supply chain message
modeling more generally, we note that the creation and application of models that accurately represent the behavior of supply
networks are challenging for at least two reasons.
One is the inherent complexity of the environment being
modeled. For example, we were surprised in the course of our
work that a simple single-action message/handshake protocol
could explode into nearly a quarter of a million possible states!
A second reason for difficulty in applying models of supply
chain performance is that analytical models are often difficult to
comprehend by practitioners in industry, precisely because they
are driven by the high level of complexity in the environment.
Our research is directed at building bridges to span this division
by combining tools and approaches from software engineering,
AI, and simulation.
We believe that this research represents an important step
toward developing a rigorous approach for creating multiparadigm modeling frameworks in the supply chain domain. Future
work will be directed at making the analysis of supply chain
messaging and collaboration more accessible while increasing
the use of analytical methods in practice. To this end, we plan to
develop translations for other conceptual models of the supply
chain into executable simulation models.
ACKNOWLEDGMENT
A. L. Feller would like to thank W. Zuberek for the assistance
with the TPN tool and the RN staff at Intel for their support.
The authors would like to thank the reviewers and editor for
their comments that have helped to improve this paper.
R EFERENCES
[1] K. Garg, “An approach to performance specification of communication
protocols using timed Petri nets,” IEEE Trans. Softw. Eng., vol. SE-11,
no. 10, pp. 1216–1225, Oct. 1985.
[2] B. Berthomieu and M. Diaz, “Modeling and verification of time dependent
systems using time Petri nets,” IEEE Trans. Softw. Eng., vol. 17, no. 3,
pp. 259–273, Mar. 1991.
[3] A. Yakovlev, S. Furber, R. Krenz, and A. Bystrov, “Design and analysis
of a self-timed duplex communication system,” IEEE Trans. Comput.,
vol. 53, no. 7, pp. 798–814, Jul. 2004.
[4] Intel Corporation, Intel RosettaNet Transaction Logs, Chandler, AZ, 2006.
[5] J. Sowa, Conceptual Structures: Information Processing in Mind and
Machine. Reading, MA: Addison-Wesley, 1984.
[6] L. Padulo and M. A. Arbib, Systems Theory: A Unified State Space Approach to Continuous and Discrete Systems. Philadelphia, PA: Saunders,
1974.
[7] Introduction to OMG’s Unified Modeling Language (UML). [Online].
Available: http://www.omg.org/gettingstarted/what_is_uml.htm
[8] OMG,
UML Summary, 1997. ver. 1. [Online]. Available:
http://www.omg.org/cgi-bin/doc?ad/97-08-11

1033

[9] OMG UML Specification, Mar. 2003. ver. 1.5 formal/03-03-01. [Online].
Available: http://www.omg.org/docs/formal/03-03-01.pdf
[10] B. Zeigler, Theory of Modeling and Simulation. New York: Wiley, 1976.
[11] P. A. Fishwick, “Hierarchical reasoning, simulating complex processes
over multiple levels of abstraction,” Ph.D. dissertation, Univ.
Pennsylvania, Pittsburgh, PA, 1986.
[12] P. A. Fishwick, “The role of process abstraction in simulation,” IEEE
Trans. Syst., Man, Cybern., vol. 18, no. 1, pp. 18–39, Jan./Feb. 1988.
[13] P. A. Fishwick, “An integrated approach to system modeling using a
synthesis of artificial intelligence, software engineering and simulation
methodologies,” ACM Trans. Model. Comput. Simul., vol. 2, no. 4,
pp. 307–330, Oct. 1992.
[14] P. A. Fishwick and B. P. Zeigler, “A multimodel methodology for qualitative model engineering,” ACM Trans. Model. Comput. Simul., vol. 2,
no. 1, pp. 52–81, Jan. 1992.
[15] K. Lee and P. A. Fishwick, “OOPM/RT: A multimodeling methodology
for real-time simulation,” ACM Trans. Model. Comput. Simul., vol. 9,
no. 2, pp. 141–170, Apr. 1999.
[16] R. M. Cubert and P. A. Fishwick, “A framework for distributed objectoriented multimodeling and simulation,” in Proc. IEEE Winter Simul.
Conf., 1997, pp. 1315–1322.
[17] H. Vangheluwe, J. de Lara, and P. Mosterman, “An introduction to multiparadigm modeling and simulation,” in Proc. AIS Conf. (AI, Simul. Planning High Autonomy Syst.), 2002, pp. 9–20.
[18] P. J. Mosterman and H. Vangheluwe, “Guest editorial: Special issue
on computer automated multi-paradigm modeling,” ACM Trans. Model.
Comput. Simul., vol. 12, no. 4, pp. 249–255, Oct. 2002.
[19] P. I. Barton and C. K. Lee, “Modeling, simulation, sensitivity analysis,
and optimization of hybrid systems,” ACM Trans. Model. Comput. Simul.,
vol. 12, no. 4, pp. 256–289, Oct. 2002.
[20] H. Vangheluwe and J. de Lara, “Metamodels are models too,” in Proc.
IEEE Winter Simul. Conf., 2002, pp. 597–605.
[21] H. Vangheluwe and J. de Lara, “Computer automated multi-paradigm
modeling: Meta modeling and graph transformation,” in Proc. IEEE Winter Simul. Conf., 2003, pp. 595–603.
[22] M. Traore, “A meta-theoretic approach to modeling and simulation,” in
Proc. IEEE Winter Simul. Conf., 2003, pp. 604–612.
[23] J.-S. Lee, M. C. Zhou, and P.-L. Hsu, “Multiparadigm modeling for
hybrid dynamic systems using a Petri net framework,” IEEE Trans.
Syst., Man, Cybern. A, Syst., Humans, vol. 38, no. 2, pp. 493–498,
Mar. 2008.
[24] P. J. Mosterman, J. Sztipanovits, and S. Engell, “Special issue on computer
automated multi-paradigm modeling in control systems,” IEEE Trans.
Control Syst. Technol., vol. 12, no. 2, pp. 223–234, Mar. 2004.
[25] Y. W. Kim, T. Kato, S. Okuma, and T. Narikiyo, “Traffic network control
based on hybrid dynamical system modeling and mixed integer nonlinear
programming with convexity analysis,” IEEE Trans. Syst., Man, Cybern.
A, Syst., Humans, vol. 38, no. 2, pp. 346–357, Mar. 2008.
[26] J. Lee and P. Hsu, “Design and implementation of the SNMP agents for
remote monitoring and control via UML and Petri nets,” IEEE Trans.
Control Syst. Technol., vol. 12, no. 2, pp. 293–302, Mar. 2004.
[27] C.-P. Lin and M. D. Jeng, “An expanded SEMATECH CIM framework for
heterogeneous applications integration,” IEEE Trans. Syst., Man, Cybern.
A, Syst., Humans, vol. 36, no. 1, pp. 76–90, Jan. 2006.
[28] Y. Y. Du, C. J. Jiang, and M. C. Zhou, “Modeling and analysis of real-time
cooperative systems using Petri nets,” IEEE Trans. Syst., Man, Cybern. A,
Syst., Humans, vol. 37, no. 5, pp. 643–654, Sep. 2007.
[29] H. Giese and G. Wirtz, “Visual modeling,” J. Vis. Lang. Comput., vol. 12,
no. 2, pp. 183–202, Apr. 2001.
[30] H. Giese, J. Graf, and G. Wirtz, “Closing the gap between object-oriented
modeling of structure and behavior,” in Proc. 2nd Int. Conf. UML—
Beyond the Standard, Fort Collins, CO, Oct. 28–30, 1999, pp. 534–549.
[31] L. Baresi and M. Pezze, “On formalizing UML with high-level Petri nets,”
in Concurrent Object Oriented Programming and Petri Nets: Advances in
Petri Nets, vol. 2001, Lecture Notes in Computer Science. New York:
Springer-Verlag, 2001, pp. 276–304.
[32] J. Garrido and M. Gea, “A coloured Petri net formalisation for a UMLbased notation applied to cooperative system modeling,” in Proc. 9th
Int. Workshop Interactive Syst. Des., Specification, Verification, 2002,
pp. 16–28.
[33] J. Xu and J. Kuusela, “Modeling execution architecture of software system
using colored Petri nets,” in Proc. 1st Int. Workshop Softw. Perform.,
Santa Fe, NM, 1998, pp. 70–75.
[34] J. Merseguer, J. Campos, S. Bernardi, and S. Donatelli, “A compositional
semantics for UML state machines aimed at performance evaluation,” in
Proc. 6th IEEE Int. WODES, 2002, pp. 295–302.
[35] S. Bernardi, S. Donatelli, and J. Merseguer, “From UML sequence diagrams and statecharts to analyzable Petri net models,”

1034

[36]

[37]
[38]

[39]
[40]
[41]
[42]
[43]
[44]
[45]
[46]
[47]

[48]
[49]
[50]
[51]
[52]
[53]
[54]

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

in Proc. 3rd Int. Workshop Softw. Perform., Rome, Italy, 2002,
pp. 35–45.
C. Lindemann, A. Thümmler, A. Klemm, M. Lohmann, and O. Waldhorst,
“Performance analysis of time-enhanced UML diagrams based on stochastic processes,” in Proc. 3rd Int. Workshop Softw. Perform., Rome,
Italy, 2002, pp. 25–34.
V. Cortellessa and R. Mirandola, “Deriving a queuing network based
performance model from UML diagrams,” in Proc. 2nd Int. Workshop
Softw. Perform., Ottawa, ON, Canada, 2000, pp. 58–70.
L. Lavazza, G. Quaroni, and M. Venturelli, “Combining UML and formal
notations for modeling real-time systems,” in Proc. 8th Eur. Softw. Eng.
Conf. Held Jointly With 9th ACM SIGSOFT Int. Symp. Found. Softw. Eng.,
2001, pp. 196–206.
A. Simons, “On the compositional properties of UML statechart diagrams,” in Proc. 3rd Workshop Rigorous Object Oriented Methods, 2000,
pp. 4.1–4.19.
RosettaNet Implementation Framework: Core Specification: RosettaNet,
Jul. 13, 2001. [Online]. Available: http://rosettanet.org
RosettaNet PIP Specification, Cluster 4: Inventory Management, Segment A: Collaborative Forecasting, PIP4A1: Notify of Strategic Forecast:
RosettaNet2002. [Online]. Available: www.RosettaNet.org
Issue 1 EDIINT AS1 and AS2 Transport Communication Guidelines:
GS1, Feb. 2006. [Online]. Available: http://www.gs1.org/docs/gsmp/
EDIINT_AS1_AS2_Transport_Comm_Guide_i1.pdf
J. L. Peterson, Petri Net Theory and the Modeling of Systems.
Englewood Cliffs, NJ: Prentice-Hall, 1981.
P. C. Xiong, Y. S. Fan, and M. C. Zhou, “QoS-aware web service configuration,” IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 38, no. 4,
pp. 888–895, Jul. 2008.
Q. Ye and M. H. MacGregor, “Combining Petri nets and ns-2: A hybrid
method for analysis and simulation,” in Proc. 4th Annu. CNSR Conf.,
May 24–25, 2006, pp. 139–148.
J. Sowa, “Relating diagrams to logic,” in Proc. 1st ICCS, Conceptual
Graphs Knowl. Represent., Quebec City, QC, Canada, Aug. 4–7, 1993,
pp. 1–35.
A. Feller and R. Rucker, “Extending structured analysis modeling with
A.I.: An application to MRPII profiles and SFC data communications
requirements specifications,” in Optimization of Manufacturing Systems
Design. Amsterdam, The Netherlands: North Holland, 1990.
A. Feller and R. Rucker, “Meta-modeling systems analysis primitives,” in
Conceptual Structures: Current Research and Practice. New York: Ellis
Horwood, 1992, ch. 10, pp. 201–220.
G. Willumsen, “Executable conceptual models in information systems engineering,” Ph.D. dissertation, Norwegian Inst. Technol. Univ.
Trondheim, Trondheim, Norway, 1993.
B. Berthomieu, P.-O. Ribet, and F. Vernadat, “The tool
TINA—Construction of abstract state spaces for Petri nets and time
Petri nets,” Int. J. Prod. Res., vol. 42, no. 14, pp. 2741–2756, Jul. 2004.
J. D. Bloom, “Pipe—A platform independent Petri net editor,” M.S. thesis,
Imperial College, London, U.K., 2003.
ver. 2.0 Platform Independent Petri Net Editor (PIPE), The Home Page
of PIPE (Platform Independent Petri Net Editor). [Online]. Available:
http://pipe2.sourceforge.net
W. Zuberek, “Timed Petri nets—Definitions, properties and applications,”
Microelectron. Reliab., vol. 31, no. 4, pp. 627–644, 1991.
Intel Corporation, Intel RosettaNet BCRN Indicators—Transaction Summary Report, Chandler, AZ, 2006.

Andrew L. Feller received the B.S. and M.S. degrees in industrial engineering from Arizona State
University (ASU), Tempe, where he is currently
working toward the Ph.D. degree.
He is also working with American Express Technologies, Phoenix, AZ, leading modeling and simulation projects focused on improving information
technology capacity planning and service process
performance. During the past several years, he has
led consulting practices as a Vice President and the
Director at two Arizona firms, improving operational
efficiency and financial performance for clients such as Honeywell and MD
Helicopters. He coauthored two book chapters on advanced system analysis
methods and architectures. His research interests include modeling and simulation in manufacturing, supply chain, and service management.
Mr. Feller is a member of the Society of Manufacturing Engineers and the
Institute of Industrial Engineers. He is the recipient of the alumni of the year
and academic distinction from the ASU Corporate Leaders program.

Teresa Wu received the Ph.D. degree in industrial
engineering from The University of Iowa, Iowa City,
in 2001.
She is currently an Associate Professor of industrial engineering with the Department of Industrial
Engineering, Arizona State University, Tempe. Her
research interests include distributed decision support, distributed information systems, supply chain
modeling, and disruption management. She has over
25 articles published in journals, such as the International Journal of Production Research, Omega, Data
and Knowledge Engineering, and American Society of Mechanical Engineer:
Journal of Computing and Information Science in Engineering, and the IEEE
T RANSACTIONS ON E NGINEERING M ANAGEMENT. She serves on the Editorial Review Board for the International Journal of Production Research, IEEE
T RANSACTIONS ON E NGINEERING M ANAGEMENT, Computer Standards and
Interfaces, and the International Journal of Electronic Business Management.

Dan L. Shunk received the Ph.D. degree in industrial engineering from Purdue University, West
Lafayette, IN, in 1976.
He is currently an Avnet Professor of supply network integration in industrial engineering with the
Department of Industrial Engineering, Arizona State
University, Tempe, where he is pursuing research
into collaborative commerce, global new product
development, model-based enterprises, and global
supply network integration.
Dr. Shunk won a Fulbright Award in 2002–2003,
the 1996 Society of Manufacturing Engineers (SME) International Award for
Education, the 1991 and 1999 I&MSE Faculty of the Year award, the 1989
SME Region VII Educator of the Year award, and the 1982 SME Outstanding
Young Engineer award. He was the Chair of AutoFact in 1985.

John Fowler received the Ph.D. degree in industrial
engineering from Texas A&M University, College
Station, in 1990.
He is currently a Professor of industrial engineering with the Department of Industrial Engineering,
Arizona State University, Tempe, and he was the
Center Director for the Factory Operations Research
Center that was jointly funded by International SEMATECH and the Semiconductor Research Corporation. His research interests include modeling,
analysis, and control of semiconductor manufacturing systems. He is an Area Editor for SIMULATION: Transactions of the
Society for Modeling and Simulation International (SCS).
Dr. Fowler is a member of the Institute of Industrial Engineers (IIE), Institute
for Operations Research and the Management Sciences (INFORMS), and SCS.
He is an IIE Fellow, the Vice President of Chapters/Fora for INFORMS, and
the Treasurer of Omega Rho and is on the Winter Simulation Conference
Board of Directors. He is an Associate Editor of the IEEE T RANSACTIONS
ON S EMICONDUCTOR M ANUFACTURING .

The Journal of Systems and Software 59 (2001) 259±270

www.elsevier.com/locate/jss

Behavioral characterization: ®nding and using the in¯uential
factors in software process simulation models
Dan X. Houston a,*, Susan Ferreira b,1, James S. Collofello c,2,
Douglas C. Montgomery d,3, Gerald T. Mackulak d,4, Dan L. Shunk d,5
a

b

Honeywell Inc., 817 W. Northview Avenue, Phoenix, AZ 85021-8042, USA
Arizona State University and Motorola IISG, 8201 E. McDowell Rd, MD H8175, Scottsdale, AZ 85257, USA
c
Computer Science & Engineering Department, Arizona State University, Tempe, AZ 85287-5406, USA
d
Industrial Engineering Department, Arizona State University, Tempe, AZ 85287-5906, USA
Received 12 July 2000; received in revised form 1 December 2000; accepted 22 March 2001

Abstract
Most software process simulation work has focused on the roles and uses of software process simulators, on the scope of models,
and on simulation approaches. Consequently, the literature re¯ects a growing body of models that have recently been characterized
by modeling purpose, scope, key result variables, and simulation method. While the software process simulation arena is maturing,
little eort appears to have been given to statistical evaluation of model behavior through sensitivity analysis. Rather, most of
software process simulation experimentation has examined selected factors for the sake of understanding their eects with regard to
particular issues, such as the economics of quality assurance or the impact of inspections practice. In a broad sense, sensitivity
analysis assesses the eect of each input on model outputs. Here, we discuss its use for behaviorally characterizing software process
simulators. This paper discusses the bene®ts of using sensitivity analysis to characterize model behavior; the use of experimental
design for this purpose; our procedure for using designed experiments to analyze deterministic simulation models; the application of
this procedure to four published software process simulators; the results of our analysis; and the merits of this approach. Ó 2001
Elsevier Science Inc. All rights reserved.
Keywords: Sensitivity analysis; Design of experiments; Model characterization; Software process modeling

1. Introduction
The ®eld of software process simulation has been
growing, particularly since the 1991 publication of Abdel-Hamid and Madnick's Software Project Dynamics:
An Integrated Approach (Abdel-Hamid and Madnick,
1991) focused attention on the application of dynamic
simulation to software project management. Most of the

*

Corresponding author. Tel.: +1-602-313-4231.
E-mail addresses: dxlsh@uswest.net (D.X. Houston), susanf1@
uswest.net (S. Ferreira), collofello@asu.edu (J.S. Collofello), doug.
montgomery@asu.edu (D.C. Montgomery), mackulak@asu.edu
(G.T. Mackulak), dan.shunk@asu.edu (D.L. Shunk).
1
Tel.: +1-480-441-2139.
2
Tel.: +1-480-965-3733.
3
Tel.: +1-480-965-3836.
4
Tel.: +1-480-965-6904.
5
Tel.: +1-480-965-6330.

work in this developing ®eld has focused on the roles
and uses of software process simulators, on the scope of
models, and on simulation approaches. Consequently,
the literature re¯ects a growing body of models that
have recently been characterized by modeling purpose,
scope, key result variables, and simulation method
(Kellner et al., 1999).
While the software process simulation arena is maturing, little eort appears to have been given to statistical evaluation of model behavior through sensitivity
analysis. (Perhaps this lack of interest re¯ects the same
lack in the ®eld of systems dynamics modeling (Clemson
et al., 1995), the venue in which a large stream of software process simulation research was initiated.) Rather,
most of software process simulation experimentation
has examined selected factors for the sake of understanding their eects with regard to particular issues,
such as the economics of quality assurance (Abdel-Hamid and Madnick, 1991) or the impact of inspections
practice (Madachy, 1994).

0164-1212/01/$ - see front matter Ó 2001 Elsevier Science Inc. All rights reserved.
PII: S 0 1 6 4 - 1 2 1 2 ( 0 1 ) 0 0 0 6 7 - X

260

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

In a broad sense, sensitivity analysis assesses the eect
of each input on model outputs. Here, we discuss its use
for behaviorally characterizing software process simulators. A behavioral characterization provides basic information about a simulation model, information as
important as the static characteristics of purpose, scope,
and so forth.
The remainder of this paper discusses the bene®ts of
using sensitivity analysis to characterize model behavior;
the use of experimental design for this purpose; our
procedure for using designed experiments to analyze
deterministic simulation models; the application of this
procedure to four published software process simulators; the results of our analysis; and the merits of this
approach.

2. Bene®ts of sensitivity analysis for characterizing model
behavior
With the continuing maturity of software process
simulation, modelers are able to re¯ect more on the
modeling process, improving it through evaluation of its
models. Sensitivity analysis is one of the better-known
statistical techniques for evaluating a simulation model.
For the purpose of this discussion, it involves the systematic variation of a model's exogenous parameters,
and analysis of the outputs of interest so as to estimate
the eects of each parameter. Most systems exhibit a
sparsity of eects, meaning their behavior is dominated
by only some (often a minority) of the factors. The
identi®cation of these in¯uential factors in a model
provides a characterization of the model's behavior,
which can be used to improve the modeling process, and
to aid in the selection of a model, either for a particular
usage or for enhancement.
 Improved modeling. A major trend in software
process modeling has been the development of system
dynamics models (SDM) that are validated against the
data of an actual software project (hereafter referred to
as a ``base case''). In a single experiment, sensitivity
analysis can readily identify the signi®cant factors for a
base case, thereby determining which ones have to be
modeled correctly. A modeler may then make informed
decisions regarding further level of detail in modeling,
degree of eort spent in data collection, and speci®cation of parameter values (Law and Kelton, 2000).
 Model selection. Rather than investing in the construction of a new simulation model, prospective model
users may want to choose from among various existing
software process simulators. An understanding of a
model's behavior with respect to project outcomes (cost,
quality, and schedule) provides an additional criterion
for selecting a model either for use as-is or for enhancement. Later in this paper, we discuss examples of

using behavioral characterization to support model selection for these purposes.
3. A procedure for sensitivity analysis
A variety of techniques, including many from design
of experiments (DOE), are available for designing a set
of input values such that the output values can be analyzed for their sensitivity to the inputs. Although DOE
was developed in applications to physical systems, many
of its techniques, due to their eciency, have been applied to the sensitivity analysis of computer models.
Comparisons of input selection techniques for computer
model sensitivity analysis have been oered by McKay
et al. (1979), Morris (1991), and Clemson et al. (1995).
Among the techniques available are 2-level fractional
factorial designs, widely used for screening in¯uential
from unimportant factors (Montgomery, 1997).
A fractional factorial design is fraction of a full factorial design. In a two-level factorial design, designated
2k , k  number of factors, every combination of two
values for each input is run. An advantage of full factorials is that factor eects and the eects of all the interactions between factors can be measured. With
fractional factorials, the number of experimental runs is
traded-o with the ability to distinguish factor interactions. As a factorial design is more highly fractionated
(1/2 fraction, 1/4 fraction, . . .), more interactions are
aliased by (measured with) main factor eects. The degree of aliasing, or resolution, allows an analyst to
choose an experimental design that provides an optimal
trade-o between the cost of experimentation and the
resolution required. If interactions are not expected to
be signi®cant, then a low resolution design is usually
sucient.
Another bene®t of fractional factorial designs is the
ability to accommodate any number of factors. In most
physical experiments, the number of factors is on the
order of 1±10 factors, but in computer experiments,
having 10±100 factors is not uncommon. Consequently,
highly fractionated two-level factorials (designated 2k p ,
where p is the power of the 1/2 fraction or 1=2p ) provide
an ecient means of identifying signi®cant factors. If
interactions are indicated and cannot be clearly identi®ed, an additional fraction of the full factorial can be
run to ``de-alias'' eects. Factorial designs can also be
sequenced, selecting fewer factors in successive experiments as insigni®cant factors are eliminated.
Analysis of a factorial experiment produces leastsquares estimates of the coecients of a linear model of
the form
y  bx  e;
where e is a random error term, ordinarily estimated
from replications of the experiment. However, when

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

working with deterministic computer models, the error
term is eliminated, 6 so the usual parametric statistics do
not apply. Nonetheless, the partitioned sums of squares
provide a convenient means for measuring the relative
contribution of each factor to variations in the response
variables.
The steps for performing a 2k p experiment are:
1. Choose the factors.
2. Choose the response variable(s).
3. Choose the levels of the factors.
4. Generate the experimental design.
5. Run the experiment using the combinations in the design and measuring the responses.
6. Analyze the eects of the factors. For each response
variable, calculate each factor's contribution to the
total variation in the response: divide each factor's
sum of squares by the total sum of squares.
4. Case studies
4.1. Selection of models
The above procedure was applied to four published
simulation models of software development (AbdelHamid and Madnick, 1991; Madachy, 1994; Sycamore,
1996; Tvedt, 1996). These models were selected because
(a) the documentation and source code was readily
available for them; (b) they are dierent models with
dierent purposes and scopes, but employ some of the
same modeling constructs, thereby allowing comparisons between experimental results; (c) they have been
validated (albeit to varying degrees); and (d) the availability of an ithinkâ implementation of each model facilitated experimentation. Even though these four are
system dynamics models, the procedure is applicable to
any simulation model.
It is helpful to summarize the emphasis of each
model.
· Abdel-Hamid and Madnick (1991) (A-H&M) modeled an integrated approach to project management
of software development for the purpose of learning
about the management process. In addition to production, the scope of his model includes project staing, personnel ¯ow, eort allocation, project planning
and control, and an error ¯ow.
· Madachy's model (1994) focused on the production
stream with particular attention to work product inspections. He introduced inputs from COCOMO
for risk assessment. He includes stang, calculated

from COCOMO, and an error stream, ignoring the
managerial aspects.
· Tvedt's model (1996) encompassed the scope of AH&M, reusing many of its modeling constructs in
an incremental development process. It elaborated
work product inspections in signi®cant detail.
· Sycamore (1996) also modeled an incremental development process, but in a very reduced scope. He does
not have a production ¯ow and instead models eort
consumption for the sake of studying trade-os between cost and duration. 7
4.2. Applying the procedure
The exogenous factors in each model were selected by
identifying any element having a modi®able value: inputs, constants, and tables. 8 To facilitate experimentation, inputs were added for manipulating the values of
constants and table outputs.
Sensitivity analysis, though de®ned variously across
disciplines, is commonly understood as the investigation
of model responses either to extreme input values or to
drastic changes in input values (Kleijnen, 1995). This
de®nition usually presumes that the analysis is for the
purpose of model validation, optimization, or related
activity. However, for purpose of characterizing model
behavior around a base case, we focused the analysis
(local sensitivity analysis) using marginal variations in
parameter values. Speci®cally, factor levels were selected
by choosing values 10% above and below the values
speci®ed for each model's base case. Local sensitivity
analysis and the degree of common factor variation were
chosen for a number of reasons.
· Job size and stang were expected to dominate, particularly for duration and eort, if all factors were
varied across normal operational ranges. By varying
all factors the same degree from values in a validated
baseline case, the relative signi®cance of the factors in
the baseline case could be assessed.
· It is often dicult to identify normal operating ranges
for many factors, especially in a computer model that
is highly abstracted from the system it represents. The
use of a ®xed percent variation, about baseline case
values, provides an objective and consistent means
of choosing factor levels.
· Using 10% provided adequate input variation to
measure signi®cant eects on responses. Varying
some factors to larger degrees (say, 15% or 20%)
7

6
Least-squares analysis of deterministic output does produce a sum
of squares for error, but this represents linear model bias rather than
experimental variance. It is desirable to reduce this bias to an
insigni®cant level. A footnote in the next section explains further.

261

Although the Tvedt and Sycamore models are incremental, only the
®rst increment was used for experimentation so that all experiments
would re¯ect a simple waterfall process.
8
An exception to this approach was the COCOMO calculations in
two of the models: A-H&M and Madachy. These equations were
considered integral to the structure of these models, therefore they
were not treated as experimental factors.

262

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

Table 1
Experimental designs
Model
A-H&M
Madachy
Tvedt
Sycamore

No. of factors
65
21
103
30

Table 2
Number and percent of in¯uential factors
Design
65 57

2
221 15
2103 95
230 24

No. of runs
256
64
256
64

may create extreme values that would not allow model runs to ®nish. (In fact, this was found to be the case
for the one model on which it was tried.)
· This approach would better accommodate serial experimentation. If factors are to be screened for subsequent experiments, screening of locally insigni®cant
factors is much less likely to produce errors than
screening of globally insigni®cant factors. In other
words, a factor may be insigni®cant globally but locally in¯uential, while the converse (globally signi®cant but locally insigni®cant) is improbable.
· Using the same variation to choose levels for all factors in all models facilitates comparison of results
across models.
A 2k p design of Resolution IV was produced for each
of the four models (Table 1) using SAS PROC FACTEX. Resolution IV designs were chosen because twoway interactions could be clearly distinguished from
main eects. 9 Response variables included measures of
cost (total eort expended on the project), of project
duration, and of quality (number of defects in the last
project phase).
Each experiment was performed by copying the factor values from the design in a spreadsheet and pasting
them into the ithinkâ Sensitivity-Spec.
SAS PROC GLM was used to calculate partitioned
sums of squares, though a spreadsheet program also
proved very workable for calculating them from formulas provided by Montgomery (1997). For each response variable, the percent contribution to its variation
was calculated, as described earlier, and the model factors were sorted by this percentage. 10 Each list of sorted
factors was categorized into very in¯uential, marginally
in¯uential, and non-in¯uential factors. The distinction
between very in¯uential and marginally in¯uential was
usually 10% contribution to response variation, though
9

In many cases, Resolution IV designs also have the advantage of
minimizing the bias error (Myers and Montgomery, 1995). In these
experiments, the use of Resolution IV designs reduced the percentage
of the error sum of squares to insigni®cant levels, well below 1%.
10
The calculation of percentage contribution to variation from sums
of squares is not commonly used in DOE, so it was employed in
conjunction with normal probability plots, a widely accepted technique
for analyzing unreplicated factorial experiments. In every experiment
discussed here, the two techniques produced the same ordering of
factors. The former is used here because it oers the advantage of
quanti®ed results.

Model

No. of factors
having P1%
in¯uence

Percentage
of factors
tested

Percent
variation
explained

A-H&M
Madachy
Tvedt
Sycamore

15
9
21
16

23
43
20
53

58±79
97±99
87±88
95±96

in one case 15% provided a better line of demarcation.
The distinction between marginally in¯uential and nonin¯uential factors was arbitrarily chosen at 1%.
Appendix A contains the lists of very in¯uential and
marginally in¯uential factors for each of the four models
studied. These two categories of in¯uence capture most
of the variation in responses (see Table 2). Interactions
were found to be signi®cant only in the A-H&M model.
4.3. Results
Table 2 shows the number of factors that exhibited a
base case in¯uence P1% of the variation in any of the
three response variables. This table also shows the percentage of in¯uential factors, and the percent of variation in the responses explained by the in¯uential factors.
(The ``percent variation explained'' is a range for the
multiple responses.)
The A-H&M and Tvedt models are each dominated
by roughly 20% of their exogenous factors, whereas the
Madachy and Sycamore models appear to be much
more ecient. In all but the A-H&M model, the in¯uential factors explain a very high percentage of the
variation in all responses; for A-H&M, approximately
40% of the variation in two of the responses is diused
across many factors.
The results of each experiment can be used to characterize the behavior of the model within the context of
a given job size (10%) and stang (10%). In order to
re®ne the behavioral characterization of each model, the
results in Appendix A were summarized into factor
categories: each factor listed in Appendix A was categorized using the factor categories listed in Table 3 (due
to model dierences, the categories are similar but not
the same for each model). For example, the factors listed
for A-H&M in Table 4 were summarized into four
categories: error generation/regeneration, stang level,
schedule, and QA eort. The percentage contribution to
outcome variation for each factor (shown in parentheses
in Tables 4±7 of Appendix A) was summed for the respective category. Again using the A-H&M model as an
example, the total contribution to variation in Project
Duration by the Error generation/regeneration factors
(Nominal errors per KDSI, Error multiplier for schedule
pressure, Time to smooth active error density, Multi-

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270
Table 3
Percent contribution to variation of in¯uential factor categories
Model
Factor category
A-H&M
Error generation/
regeneration
Stang level
Schedule
QA eort
Madachy
Stang/schedule/
productivitya
Job size
Inspection eectiveness
Inspection practice
Error generation
Tvedt
Work rate/
productivity
Rework
Inspection eort
Job size
Inspection eectiveness
Technical risk
Error generation/
regeneration
Stang level (by
interval)
Inspection bene®ts

Response
Duration

Cost

Quality

49

45

54

7
2
3

5
2
3

88

52

0

12
0
0
0

44
1
0
0

16
40
31
11

39

26

0

16
13
11
0
8
0

20
18
13
0
10
0

0
0
31
49
1.5
4

20
8
1.5

0

1.5

0

0

0

2

61
10
14
3
8
0
0

54
19
4
12
1.4
3
2

b

Sycamore
Work rate/productivity
Budget
Schedule
Reviews eectiveness
Stang level
Error ®x cost
Error generation

a
These factors are aggregated in the model into two inputs from
COCOMO, a schedule constraint multiplier and a constant calibrated
to the organization.
b
Sycamore modeled eort consumption rather than production and
error processing, therefore, it has no response variable for quality.

plier to active error regeneration rate due to error density, and Fraction of escaping errors that will be active)
is 49% (21  20  3:7  2  1:8).
Verbal descriptions of behavioral characterizations,
based on both Appendix A and Table 3, are presented in
this section, followed by a graphical display of the behavioral characterizations.
4.3.1. A-H&M model
Two error generation factors account for approximately 40% of the variation in all three of the major
response variables for cost, duration, and quality. This
dominance of two factors in the same function, error
generation, across all responses may be interpreted as
both a lack of behavioral discrimination and a very tight

263

integration within the model. Speci®cally, this dominance is explained by the biological modeling of error
regeneration, a unique construct of this simulator. (In
fact, the error regeneration time, ``time to smooth active
error density'', contributes 4±5% variation for each response.) Aside from the model's focus on error generation, the remainder of the behavior is quite diused.
Only a stang factor, willingness to change workforce,
contributes a variation (to duration) on the same order
as the two error generation factors. Another 13 factors
account for an additional 12±20% of variation in responses, leaving 22±46% of variation accounted for by
the remaining 49 factors.
4.3.2. Madachy model
This model exhibits discrimination of factor in¯uence
across the responses. The COCOMO schedule constraint dominates the duration; both COCOMO inputs,
plus the job size, dominate the cost; and the quality
response is governed by a combination of job size, inspection eort, inspection eectiveness, and error generation. Factor in¯uence is undiused: one or two
factors account for 70% or more of variation in each
response. The simulation model is very focused, due
largely to the COCOMO-based rule engine that serves
as its front end with inputs to stang, productivity, and
schedule. The model is sensitive to small changes in job
size: it is a dominant factor for eort and in¯uential in
the other two responses. The dominance of the inspection factors for quality clearly re¯ects one of the author's two purposes for the model; the dominance of the
COCOMO factors re¯ects the other purpose, risk assessment.
4.3.3. Tvedt model
This model exhibits a moderate degree of factor in¯uence diusion. No single factor accounts for the majority of variation in a response, but 87% of the
variation for each response is produced by 20 of the 103
factors. Re¯ecting its attention to detail in the inspection
process, the most in¯uential factor for duration and cost
is the percent of work needing only minor changes after
inspection. However, factors for productivity, job size,
and inspection eort are signi®cant in¯uences on duration and cost. This model shares two behavioral patterns
with Madachy's model:
1. all three responses are sensitive to job size;
2. quality is most sensitive to the fraction of errors
found during inspections, congruent with research
that supports the bene®ts of inspections by reducing
defect leakage.
(This is the only one of the four models that counts
defects left in the product after testing.) This model also
contains a heuristic technical risk factor (not calibrated
to any of the usual development metrics), in¯uential for

264

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

all responses because it directly modi®es productivity
and error generation.
4.3.4. Sycamore model
Sycamore modeled eort consumption rather than
production and error processing, therefore, it has no
response variable for quality. This model also exhibits a
moderate degree of factor in¯uence diusion. No single
factor accounts for a majority of the variation in either
cost or duration, but 8 of the 30 factors contribute approximately 85% of the variation for both responses.
Two work rate/productivity factors, nominal work rate
and communication overhead (a productivity modi®er),
account for 50% of each response's variation. Due to the
model's eort consumption approach, the eort budgeted for each development phase is a factor. These
budgets are marginally in¯uential, with one exception:
the eort budgeted for design is signi®cant for project
cost.
4.3.5. Graphical display
In addition to the verbal descriptions above, the data
in Table 3 can be plotted to display a graphical characterization of each model's base case behavior. These are
presented in Figs. 1(a)±(d). This ®gure clearly displays the
contrasts between the behavioral characterizations of the
four models. (Each factor category is colored the same in
all four plots.) Because all four models are intended to
represent the same process (waterfall software develop-

ment) and have very similar scopes, one might expect the
behavioral characterizations to be similar. Some agreements are apparent. Madachy and Tvedt agree that QA
eectiveness is important for Quality; Tvedt and Sycamore agree that work rate/productivity is important for
duration and cost; Madachy and Tvedt agree that job size
is important for all three response variables. Yet the
patterns for the four models are so dierent that one
might wonder whether they represent the same process,
or whether the modeling dierences account for these
dierent behavior patterns.

5. Uses of behavioral characterizations
5.1. Improved modeling
Simulation modeling is an expensive undertaking,
particularly for modeling the abstractions of human
organizations, such as software development projects.
Extensive analysis of the target system is required to
ascertain its structure and represent it in a model. A
substantial amount of eort is usually necessary to
identify, specify, and collect data for model parameterization. Veri®cation and validation of the simulation
model also take a large amount of work. Given these
high costs, eciency in the modeling process is very
desirable. Once a tentative model has been constructed
and populated with values, either estimated or for a

Fig. 1. Percentage contribution of each factor category to variation in response variables for four models (%).

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

known case, local sensitivity analysis can be used for
behavioral characterization. If the characterization
matches observations of the real system, then the modeling process can proceed, but a mismatch may indicate
the need to revisit the system analysis. This early assessment may prevent wasted eort in the modeling
process while providing con®dence in intermediate results.
The use of behavioral characterization in speci®cation of parameter values and in data collection and
analysis is straightforward. Knowing which parameters
must be speci®ed with a degree of precision, a modeler
can direct the expense of data collection and analysis
accordingly. On the other hand, insigni®cant factors do
not warrant the expense of data collection and analysis;
they can be speci®ed with rough estimates. For example, most of the delays common in system dynamics
models fall into this latter category (only one delay
factor is listed in Appendix A). As another example,
despite the detailed modeling of exhaustion due to
overwork in both A-H&M and Tvedt, only one of the

265

six factors in this construct is listed in Tables 4±7 of
Appendix A (for A-H&M, exhaustion depletion delay
time is among the marginally in¯uential for project
eort), and with a very low percent contribution to
variation (1.3). If either of these two models were to be
reused to simulate projects on the order of the base
cases, only cursory eort would be warranted for calibrating these factors.
Taking a longer view, behavioral characterization
also supports modeling evolution. Whether one starts
with a simple model and adds detail (top±down approach), or starts with a detailed model and simpli®es it
(bottom±up approach), behavioral characterizations can
guide or explain modeling decisions over generations of
a model.
The model of A-H&M provides a bottom±up example in which behavioral characterization explains a lack
of construct reuse. The base case behavior of this model
is dominated by its error generation and regeneration
factors. This may explain why its biological modeling of
error reproduction has not been widely reused. This

Table 4
A-H&M model
Project duration

Project eort (cost)

Product qualitya

Very in¯uential
(>10% contribution
to variation)

 Nominal errors per KDSI (21)b
 Error multiplier for schedule
pressure (20)b
 Willingness to change
workforce (14)

 Nominal errors per KDSI (19)b
 Error multiplier for schedule
pressure (18)b

 Nominal errors per KDSI (23)b
 Error multiplier for schedule
pressure (22)b

Marginally
in¯uential (1±10%
contribution to
variation)

 Schedule compression factor
(8.2)
 Time to smooth active error
density (3.7)
 Nominal fraction of man-day on
project (2.9)

 Time to smooth active error
density (3.7)
 Nominal fraction of man-day
on project (3.6)
 Fraction of escaping errors that
will be active (2.2)

 Initial understang factor (2.5)

 Schedule compression factor
(1.7)
 Initial understang factor (1.7)

 Time to smooth active error
density (5.4)
 Nominal fraction of man-day on
project (2.4)
 Multiplier to active error
regeneration rate due to error
density (2.4)
 Schedule compression factor
(2.3)
 Initial understang factor (2.1)

 Multiplier to active error
regeneration rate due to error
density (2.0)
 Fraction of escaping errors that
will be active (1.8)
 Planned fraction of
manpower for QA (1.5)

 Multiplier to active error
regeneration rate due to error
density (1.6)
 Testing manpower needed per
error (1.5)

 Real job size (1.2)

 Willing to change workforce (1.4)
 Exhaustion depletion delay time
(1.3)
 Nominal QA manpower needed
to detect average
error (1.1)
 Multiplier to productivity weight
due to development (1.1)

a

 Fraction of escaping errors that
will be active (1.6)
 Nominal QA manpower needed
to detect average
error (1.5)
 Planned fraction of
manpower for QA (1.1)

Measured by the number of errors ®xed in test.
The interaction between these two factors was signi®cant and it's percentage is allocated between them. A number of other interactions appeared in
the marginally in¯uential list, but they are ignored in this analysis because each was <3%.
b

266

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

Table 5
Madachy model
Very in¯uential
(>15% contribution
to variation)

Marginally
in¯uential (1±15%
contribution to
variation)

a

Project duration

Project eort (cost)

Product qualitya

 Relative schedule constraint
eort multiplier from
COCOMO (79)

 Job size (44)

 Inspection eciency (40)

 Relative schedule constraint
eort multiplier from
COCOMO (28)
 Calibrated organizational
COCOMO constant (24)

 Fraction of code tasks that are
inspected (28)

 Inspection eciency (1.1)

 Code error density (7.0)
 Fraction of design tasks that
are inspected (3.1)
 Design error density (1.9)
 Design error to code errors
ampli®cation (1.7)

 Job size (12)
 Calibrated organizational
COCOMO constant (8.1)

 Job size (16)

Measured by number of errors ®xed in test.

Table 6
Sycamore modela
Project duration

Project eort (cost)

Very in¯uential (>10%
contribution to variation)

 Nominal work rate (27)
 Communication overhead (22)
 Scheduled completion date (13.6)






Communication overhead (37)
Nominal work rate (17)
Eort budgeted for design (14)
Fraction of errors found in reviews (12)

Marginally in¯uential (1±10%
contribution to variation)

















Scheduled completion date (3.5)
Eort budgeted for coding (2.7)
Eort budgeted for integration (2.3)
Errors generated per hour (2.1)
Time to ®x an error (1.7)
Number of level 2 engineers (1.4)
Relative cost of ®xing errors found after the
integration (1.3)

Productivity for each level 2 engineers (8.1)
Eort budgeted for design (6.9)
Number of level 2 engineer (4.8)
Fraction of errors found in reviews (3.3)
Productivity for each level 1 engineers (2.5)
Number of level 3 engineers (2.2)
Eort budgeted for coding (1.6)

 Eort budgeted for integration (1.6)
 Productivity for each level 3 engineers (1.6)
 Number of level 1 engineers (1.0)
a

This model has no error stream, so it has no response for quality.

modeling construct does not support a behavioral
characterization that matches those of many modeled
software production environments. (Lack of reuse of a
modeling construct may re¯ect process advances in
software engineering. The last 15 years has seen a
surging interest in defect prevention, exhibited particularly in an emphasis on inspections and other forms of
peer review.)
Taking a top±down approach, on the other hand,
behavioral characterization provides information for
making decisions about which aspects of a model warrant more detailed consideration. For example, Tvedt
modeled technical risk as a factor aecting productivity
and error generation. This factor simply uses an ordinal
scale (0±2). However, it makes an important contribution to the base case variation in the cost (10%) and
duration (8%) responses, suggesting that a technical risk
factor warrants more detailed modeling, such as development of an interval scale.

5.2. Model selection
Someone who wants to use a software process simulator, but lacks either the expertise or the resources to
develop a model, may want to select an existing model
for use. After ascertaining the requirements for such a
model, the acquirer must decide which model best suits
the intended use. The behavioral characterization, if
available, can be considered in addition to each model's
scope, purpose, responses of interest, and implementation. Following are examples that draw upon the preceding base case behavioral characterizations.
5.2.1. Training
When selecting a model for training new project
managers in software project trade-os, one might
choose a simple model, one that is relatively easy to
understand, but represents basic concepts such as staing, experience mix, production cost, error management

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

267

Table 7
Tvedt model
Project duration

Project eort (Cost)

Product qualitya

Very in¯uential
(>10% contribution
to variation)

 Percent of work needing
only minor changes after
inspection (16)
 Daily manpower per engineer
(15.5)
 Potential productivity for
experienced engineers (12)
 Initial perceived size of the
development phase (11)

 Percent of work needing
only minor changes after
inspection (20)
 Potential productivity for
experienced engineers (15)
 Initial perceived size of the
development phase (13)
 Planned fraction of daily
eort for review (11)
 Level of technical risk (10)

 Fraction of errors found during
inspection (41)

Marginally
in¯uential (1±10%
contribution to
variation)

 Planned fraction of daily
manpower for reviews (8.8)
 Level of technical risk (7.9)

 Inspection team size (5.4)

 Fraction of a man-day spent on
the project (work rate) (7.0)
 Inspection team size (4.3)
 Typical level of work
intensity (2.5)
 Maximum level of potential
work intensity (2.4)

 Fraction of a man-day spent on
the project (work rate)
(3.3)
 Typical level of work
intensity (3.1)
 Maximum level of potential
work intensity (3)
 Number of experienced
engineers working during
last development interval
(1.5)
 Inspection meeting eort
(1.2)
 Communication overhead
(1.1)

a

 Initial perceived size of the
development phase (22)

 Initial perceived size of test
phase (8.7)
 Percentage of defects found
during test (8)
 Defect regeneration (2.6)
 Error generation rate for
experienced (1.7)
 Level of technical risk (1.5)

 Inspection eectiveness in
decreasing future error
generation (1.2)
 Maximum decrease in future
error generation due to
inspections (1.1)

Measured by the number of defects left in product after testing and ®xing.

cost, and project duration. Sycamore's model addresses
these factors and its behavioral characterization supports its use for training. Work rate and productivity
factors dominate the trade-os between cost and
schedule, but these are modi®ed by the budgeting of
eort for each phase, the schedule, the eectiveness of
reviews, and stang levels. In the base case, error generation and management play a small role in the outcomes, but armed with this characterization knowledge,
trainers could make the cost/schedule trade-o more
interesting by increasing both the error generation rates
and the error ®nd and ®x costs.
5.2.2. Risk analysis
A model used for risk analysis should incorporate a
variety of factors. In Madachy's model, inputs produced
by a COCOMO-based rule engine are used to propagate
measures of risk into the simulator. Since the COCOMO
inputs dominate the cost and duration outcomes, the
model is very well suited to its stated purpose of risk
analysis.
5.2.3. Project planning
Project managers often face stang issues regarding
manpower build-up and the levels of experience. Tvedt's

model takes into account both timing and experience of
stang by providing separate stang pro®le inputs for
two levels of experience. The combinations of experience
aect error generation and productivity, including effects of learning, communication overhead, work rate,
and training. The dominance of work rate/productivity
and rework factors, for cost and duration, indicate that
it responds well to ``what-if'' simulation analysis of these
stang issues.
5.2.4. Process improvement
Tvedt modeled an inspections improvement that includes factors for representing the bene®ts of inspections
when supplemented with causal analysis of defects
(``inspection eectiveness in decreasing future error
generation'' and ``maximum decrease in future error
generation due to inspections''). The behavioral characterization shows that these factors have a low degree
of in¯uence on quality as calibrated for the base case
(though May et al., 1990 report a signi®cant in¯uence in
practice). Nonetheless, one can reasonably expect the
model to illustrate the trade-os for this improvement
once it is calibrated to include the additional eort for
causal analysis and the likely inspection bene®ts for a
target organization.

268

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

5.3. Model enhancement
Someone lacking the resources for developing a new
model may also select an existing one for the purpose of
enhancing it. This situation can easily occur when an
organization is considering improvements to its development process. Again, the static characteristics of
models will play an important role in such a decision, but
the behavioral characterization may be as important.
Consider, for example, a process quality manager
who wants to use simulation to study the potential effects of two improvements to work product inspections.
The ®rst improvement is a policy requiring all design
and code to be inspected in an environment where inspections are often neglected as schedule pressure increases. The second improvement is the assignment of
product-oriented roles in addition to process roles.
Madachy and Tvedt both modeled inspection processes. Madachy's model takes constant inputs for
specifying how much inspections are used, but these
inputs could be enhanced as variables dependent on
schedule pressure (the schedule pressure calculation
would need to be added). This enhancement would allow the manager to compare the simulated quality
outcome for runs in which inspection practice is a
function of schedule pressure against those for which a
project adheres to a 100% inspection. The behavioral
characterization for this model indicates he could certainly see the eect of inspection practice on defect
leakage. However, the characterization also shows that
the model is not designed to show the eects of inspection practice on cost and duration. If the manager
wanted to see this trade-o between inspection practice
and cost or duration, he would either have to modify the
model further or turn to another model.
Tvedt modeled inspections in much more detail. The
behavioral characterization of this model shows that
cost and duration are sensitive to inspection eort, while
the quality metric is sensitive to the eectiveness of inspections. Thus this model is a good candidate for
studying the aforementioned trade-o.
For the same reason, Tvedt's model is also a good
candidate for enhancement to study the second improvement, assignment of product-oriented roles in addition to process roles for inspections. In this case, the
required enhancement is minor, adding curves to the
model to represent training and learning costs, and increasing inspection eectiveness. Again, the characterization shows that the eects of these changes would be
readily discernible in the simulated project outcomes.
6. Summary and conclusions
This paper has examined the use of sensitivity analysis in characterizing model behavior and has explained

how fractional factorial experiments can be used for
ecient sensitivity analysis. The procedure was applied
to local sensitivity analysis of four published software
process simulators. Behavioral characterizations for the
base cases were produced from the experimental results.
These characterizations were applied in discussions of
improving modeling and of selecting a model, either for
unaltered use or for enhancement.
Though the experimentation described herein was
originally undertaken with the idea that it might reveal
something about the software production systems
modeled, the results do not support conclusions about
software development. Therefore, we refrained from
making inferences about software development and
drew conclusions only about the models. Since our
®ndings pertain only to the models, no particular level
of model validation has been assumed. In fact, behavioral characterization might be used as a validation tool:
if a new model can reproduce the behavioral characterization of a highly validated model, then the characterization builds con®dence in the new model.
The experimental results reported here apply only to
the base case for each model. However, it is not unreasonable to expect that each model's characterization
may also apply well beyond the base case to the degree
that a characterization may be explained by model
structure. For example, the A-H&M characterization is
readily explained by the biological modeling of errors, a
distinct feature of this model. As another example, the
Calibrated COCOMO constant is a unique feature of
Madachy's model that can be expected to dominate its
behavior in all cases. One who wishes to employ a model
for a set of operational values signi®cantly dierent
from those of the base case should consider characterizing its behavior in the region of interest.
Systematic knowledge of a model's behavior is requisite for its appropriate use, but also guides subsequent
modeling decisions. Thus, sensitivity analysis for behavioral characterization can be expected both to enhance current model characterizations and to contribute
to the maturing of software process modeling practices.
Therefore, the authors encourage its use when reporting
work on software process models.
Software process simulation is a young, emerging
technology. At this time it provides more bene®ts for
process understanding than for process prediction and
design. But, just as the use of simulation in manufacturing matured into a process design tool, simulation of
software processes will also. To achieve this level of
maturity, software process simulators must continue to
explore, test, and validate the signi®cant factors in various software processes. As software process models
evolve, they can be expected to become more accurate
(in the correctly modeling the most important factors),
more balanced in their treatment of factors, and more
diverse (representing software process diversity).

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

7. Further research
The experiments described in this paper were limited
to varying the exogenous factors about the base case for
each model. One may reasonably question the validity
of these results for other cases within the scope of each
model. Would the same behavioral characterizations be
exhibited for other experiments, such as larger projects?
The authors expect the same behavior would be demonstrated for each model insofar as the behavioral
characterization can be explained by the model's structure. However, further experimentation is required to
validate these characterizations beyond the base cases.
This paper did not discuss ``pruning'' of insigni®cant
factors from models because the reported experiments
were limited to sensitivity analysis about each model's
base case. However, further experimentation and sensitivity over the complete ranges of factors will provide
data for identifying those factors that can be eliminated
without signi®cantly aecting modeled project outcomes. More ecient modeling would be accompanied
by the bene®ts of making a model easier to understand,
simplifying training in its use, and improving model
maintainability and performance.
The four models in this study all represent software
projects, but their behavioral characterizations are quite
dierent. This raises questions about the nature of
software development processes and their representations. Do the characterizations dier due system dierences or due to modeling dierences? To the extent that
there exist ``natural laws'' governing the development of
software, commonality in behavioral characterizations
should aid in their discovery and de®nition. Statistical
analysis of more software process models may help to
distinguish systemic variations from representational
dierences.
Sensitivity analysis is one of a number of statistical
techniques available for evaluating software process
simulation models. Other techniques that can be applied
in future research include response surface analysis for
process optimization, uncertainty analysis for quantifying output probabilities based on uncertain inputs, and
decision analysis for evaluating and ranking potential
model improvements.
Appendix A. Tables of base case in¯uential factors for
each model studied, listed by response variable
See Tables 4±7.
References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics An
Integrated Approach. Prentice-Hall, Englewood Clis, NJ.

269

Clemson, B., Tang, Y., Pyne, J., Unal, R., 1995. Ecient methods for
sensitivity analysis. System Dynamics Review 11 (1), 31±49.
Kellner, M.I., Madachy, R.J., Rao, D.M., 1999. Software process
simulation modeling: Why? What? How? Journal of Systems and
Software 46, 91±105.
Kleijnen, J.P.C., 1995. Sensitivity analysis and optimization of system
dynamics models: regression analysis and statistical design of
experiments. System Dynamics Review 11 (4), 275±288.
Law, A.M., Kelton, W.D., 2000. Simulation Modeling and Analysis,
third ed. McGraw-Hill, Boston, MA.
Madachy, R.J., 1994. A software project dynamics model for process
cost, schedule, and risk assessment. Ph.D. Dissertation, University
of Southern California.
May, R.G., Jones, C.L., Hathaway, G.J., Studinski, D.P., 1990.
Experiences with defect prevention. IBM Systems Journal 29 (1), 4±
32.
McKay, M.D., Beckman, R.J., Conover, W.J., 1979. A comparison of
three methods for selecting values of input variables in the
analysis of output from a computer code. Technometrics 21 (2),
239±245.
Montgomery, D.C., 1997. Design and Analysis of Experiments, fourth
ed. Wiley, New York.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments. Technometrics 33 (2), 161±174.
Myers, R.H., Montgomery, D.C., 1995. Response Surface Methodology. Wiley, New York.
Sycamore, D.M., 1996. Improving software project management
through system dynamics modeling. Master of Science Thesis,
Arizona State University, Tempe, Arizona.
Tvedt, J., 1996. An extensible model for evaluating the impact of
process improvements on software development cycle time. Ph.D.
Dissertation, Arizona State University, Tempe, Arizona.
Dan X. Houston received Ph.D. and M.S. degrees in Industrial Engineering at Arizona State University, Master of Divinity at St. Thomas
University (Houston, Texas), and B.S. Mechanical Engineering at the
University of Texas at Austin. He leads a global software development
team at Honeywell. His research interests include software project
management and economics, software risk and quality management,
and statistical modeling of software processes. He is a member of the
IEEE Computer Society, the Association for Computing Machinery,
and the American Society for Quality.
Susan Ferreira works as a systems engineer in the engineering group at
Motorola's Integrated Systems Division while completing her doctoral
studies. Her previous work experience includes full time positions in
both information systems and manufacturing. Susan earned her
Master of Science in Industrial Engineering from Arizona Statue
University in 1990 after receiving a fellowship to Motorola's Industrial
Fellows program and while working part-time. She also holds a
Bachelor of Science in Industrial Engineering from ASU. Her current
research interests include requirements engineering, software systems
engineering process improvement, and software process modeling.
Susan is a member of IEEE-Computer Society and Institute of Industrial Engineers.
James S. Collofello is a professor in the Department of Computer
Science and Engineering at Arizona State University. He received his
Doctor of Philosophy degree in Computer Science from Northwestern
University, Master of Science in Mathematics and Computer Science
from Northern Illinois University, and Bachelors of Science in
Mathematics and Computer Science from Northern Illinois University.
He is a member of the IEEE Computer Society and Association for
Computing Machinery. His research and teaching interests are in the
software engineering area with an emphasis on software project
management, software process improvement and software quality assurance.
Douglas C. Montgomery is Professor of Engineering at Arizona State
University. He is an author of 12 books and over 100 technical papers.
He is a recipient of the Shewhart Medal, the Brumbaugh Award, the
Hunter Award, and the Shewell Award from the American Society for
Quality Control. He is also a recipient of the Ellis R. Ott Award. He is

270

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

the one of the Chief Editors of Quality & Reliability Engineering International and a former Editor of the Journal of Quality Technology.
He is a member of the editorial boards of JQT, Quality Engineering,
and the International Journal of Production Research. Dr. Montgomery is a Fellow of the American Statistical Association, a Fellow of
the American Society for Quality Control, a Fellow of the Royal
Statistical Society, and a Fellow of the Institute of Industrial Engineers. He also serves on the Technical Advisory Board of the US Golf
Association.
Gerald T. Mackulak received his B.Sc., M.Sc., and Ph.D. degrees from
the Department of Industrial Engineering at Purdue University. He is
currently the Executive Associate Chair Undergraduate Studies in the
Department of Industrial Engineering at Arizona State University. Dr.
Mackulak has held positions with US Steel, Burroughs Corporation,
and Pritsker and Associates as well as consulting for over 75 national
companies. His primary areas of research interest are in the modeling

of manufacturing systems (particularly semiconductor AMHS), simulation methodology and production control.
Dan L. Shunk is an Associate Professor of Industrial Engineering at
Arizona State University and former Director of the CIM Systems
Research Center. He received his Ph.D. in Industrial Engineering from
Purdue. He is former manager of Industrial Engineering at Rockwell,
former manager of manufacturing systems at International Harvester,
and former VP-GM of the multi-million dollar Integrated Systems
Division of GCA Corporation. He is on the Editorial Board of the
Agility and Global Competition Journal and the International Journal
of Flexible Automation and Integrated Manufacturing. He is an active
member of the International Federation of Information Processors
(IFIP) Committee 5.3 on CIM and is a senior member of SME and
IIE. He is currently pursuing research into global new product development, model-based enterprises and global supply chain. His latest
book is Integrated Process Design and Development.

192

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 50, NO. 2, MAY 2003

A Hybrid Approach Using the Analytic Hierarchy
Process and Integer Programming to Screen
Weapon Systems Projects
Michael A. Greiner, John W. Fowler, Member, IEEE, Dan L. Shunk, W. Matthew Carlyle, and Ross T. McNutt

Abstract—Screening weapon systems development projects
is a complex, multicriteria decision problem that must be accomplished within a constrained resource environment. This
paper presents a hybrid decision support methodology for use
in the screening of weapon systems development projects. The
hybrid methodology integrates the analytic hierarchy process
(AHP) with a 0–1 integer portfolio optimization model. An AHP
component allows the decision maker to incorporate qualitative
and intangible criteria into the decision-making process and use
the priority rankings of the AHP to represent a measure of value
in the 0–1 integer model objective function. The hybrid methodology is extremely flexible and the decision maker can tailor both
the AHP and the 0–1 integer optimization model to represent a
specific decision-making situation. To fully appreciate the added
value of implementing the hybrid screening methodology, a group
of Air Force decision makers applied the hybrid methodology
to a realistic weapon-systems project screening activity. Results
indicate that decision support provided by the methodology could
lead to substantial improvements in a weapon-systems portfolio
value.
Index Terms—Analytic hierarchy process, integer programming, research and development (R&D) management, R&D
project selection, weapon systems.

I. INTRODUCTION

W

ITH AN ANNUAL research, development, test, and
evaluation (RDT&E) budget of $37.9 billion, and an
additional $60.3 billion allocated toward weapon systems
procurement in fiscal year 2001, the Department of Defense
(DoD) spends more on new product development (NPD) than
any other organization in the world [17]. Of this $98.2 billion,
$9.7 billion in RDT&E funds and $14.1 billion in procurement
funds were appropriated specifically for Air Force weapon
systems development and procurement. The screening of new
weapon systems initiatives is a critical activity within the NPD
process. The ability to make more informed decisions and
eventually transform resources and development proposals
into successfully fielded new or modified weapon systems can

Manuscript received June 1, 2001. Review of this manuscript was arranged
by Department Editor R. Balachandra.
M. A. Greiner is with the Air Force Institute of Technology, Wright-Patterson
AFB, OH 45433 USA (e-mail: michael.greiner@afit.edu).
J. W. Fowler, D. L. Shunk are with the Industrial Engineering Department,
Arizona State University, Tempe, AZ 85287 USA.
W. M. Carlyle is with the Operations Research Department, Naval Postgraduate School, Monterey, CA 93943 USA.
R. T. McNutt is with the Air Command and Staff College, Maxwell AFB, AL
36112 USA.
Digital Object Identifier 10.1109/TEM.2003.810827

have a profound impact on a nation’s ability to defend national
interests, both at home and abroad.
In the weapon-systems development environment, the operation of a sound development portfolio management process is
imperative and is an endeavor that comes with challenges unique
relative to those found in the commercial sector. For example,
economic measures including increased profits, revenue generation, increased market share, and increased shareholder value
represent some of the more common measures of NPD success
in commercial industry. However, success in weapon systems
development cannot easily be measured in economic terms and
success criteria are often more qualitative in nature.
The Planning, Programming, and Budgeting System (PPBS)
is the structure by which all DoD agencies conduct resource
allocation activities. The PPBS is a cyclic process that provides
for decision making on future programs and permits decisions
to be examined and analyzed from the viewpoint of the current
environment—threat, political, economic, technological, and
resource. Of particular interest in this paper is the multifunctional Air Force Corporate Structure (AFCS), which is charged
with making decisions regarding resource allocation relative to
all Air Force activities including weapon-systems acquisition
programs. Fig. 1 provides insight into the flow of the AFCS and
how this decision-making body fits within the PPBS process.
While these activities appear very structured, past research has
uncovered a decision-making process where a multitude of
weapon-systems development projects are evaluated, selected,
and resources allocated, all within an environment that does not
take advantage of a structured decision support methodology
[18].
This research proposes a hybrid approach, integrating an
analytic hierarchy process (AHP) with a 0–1 integer portfolio
optimization model, for supporting decision-making activities within the AFCS that provides a powerful approach to
screening new weapon systems development projects. The
AHP component of this screening process allows the decision
maker to develop a stakeholder relationship by establishing
evaluation criteria and deriving criteria weights, then evaluating
alternative projects against the established criteria. Air Force
decision makers must consider more projects than available
resources can support. Integrating the results of the AHP
evaluation effort with a mathematical programming model
allows decision makers to optimize the portfolio given a set of
constrained resources.
The remainder of this paper is organized as follows. In the
next section, discussion focuses on reviewing the research and

0018-9391/03$17.00 © 2003 IEEE

GREINER et al.: HYBRID APPROACH USING THE ANALYTIC HIERARCHY PROCESS AND INTEGER PROGRAMMING

193

Fig. 1. AFCS decision-making process flow.

development (R&D) project screening environment. With opportunities to advance current project screening practices clearly
identified, a comprehensive description of the hybrid methodology is presented in Section III. To fully appreciate the added
value of implementing this hybrid screening methodology, a
group of Air Force decision makers applied the optimized AHP
model to an actual weapon systems project screening activity in
Section IV. Actual data was used and methodology results were
compared to historical portfolio recommendations. In the final
section, model extensions are presented, the measured improvements provided by applying a hybrid screening methodology are
addressed, and concluding remarks offered.
II. R&D PROJECT SCREENING ENVIRONMENT
Screening weapon-systems development projects is a
critical task in determining whether a systems development
effort will be successful and eventually provide the increased
warfighting capability the user had originally envisioned.
Air Force decision makers must conduct tradeoffs between
improved warfighting capability, development costs, likelihood
of success, availability of resources, and the time required to
develop weapon systems. Not surprisingly, commercial industry decision makers face many of the same decisions when
conducting project screening activities. In either environment,
a “Go” decision at the screening point authorizes the project
to enter the development process and for the allocation of a
potentially significant amount of resources.
In their study of more than 120 manufacturing organizations, Cooper and Kleinschmidt [14] discovered that project
screening activities were conducted by decision makers 92% of

the time during the NPD process, and that among 12 other NPD
activities, project screening exhibited the highest correlation
with new product performance. Conversely, results from their
research also point out that decision makers consistently rate
project screening as the weakest area of all NPD activities
[11], [15], [16]. Scott’s [35] research into high-tech companies
reaffirmed these findings, with new product project selection
ranked second among 24 technology management issues
considered to pose problems and contribute to decreased effectiveness in the NPD process. Finally, in his review of more than
300 Air Force weapon systems development projects, McNutt
[27] concluded that an ineffective project screening process
was a contributing factor in weapon systems schedule overruns.
A decision to terminate a development effort prior to entry
into full-scale production results in sunk resources, a strategic
deficiency in not being able to support a mission requirement,
and often increased scrutiny from Congressional leaders. For
this reason, it is not surprising that past research has found it
often difficult for decision makers from both commercial and
DoD sectors to make tough “Kill” decisions once an NPD
project is underway [3]–[5], [18], [33].
III. HYBRID METHODOLOGY
A. Case for a Hybrid Approach
During the last four decades, the literature of R&D project
selection has expanded. By the 1960s, hundreds of techniques
had been proposed and reported in the literature [2], [10].
In addition to the mathematically-based linear, nonlinear,
dynamic, and integer programming methodologies originally
introduced in the 1960s, new R&D project selection techniques

194

Fig. 2.

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 50, NO. 2, MAY 2003

Proposed hybrid project screening decision support methodology.

have been developed. They include, but are not limited to,
the following: financial models, option pricing, strategic
approaches, scoring models, analytical hierarchy processes,
and mapping approaches. Henriksen and Traynor [20] cluster
the range of R&D project selection methods into the following
categories:
1) unstructured peer review;
2) scoring;
3) mathematical programming—including linear (LP), integer (IP), nonlinear (NLP), goal (GP), and dynamic programming (DP);
4) economic models—such as internal rate of return (IRR),
net present value (NPV), return on investment (ROI),
cost–benefit analysis, and option pricing theory;
5) decision analysis—including multiattribute utility theory
(MAUT), decision trees, risk analysis, and AHP;
6) interactive methods—such as Delphi, -sort, behavioral
decision aids (BDA), and decentralized hierarchical
models (DHM);
7) artificial systems (AI)—including expert systems and
fuzzy sets;
8) portfolio optimization.
The successful implementation of these techniques often
depends upon the type of decision being made, availability
of project information, resource availability during the decision-making process, a decision maker’s understanding of the
technique, and a decision maker’s perception that a rigorous,
quantitative approach may lead to eliminating pet projects. In
many instances, these techniques are presented as stand-alone
processes where project information is processed by the

proposed technique and a subset of best projects are reported.
For examples, see [7], [19], [21], and [22]. Researchers have
begun to question this approach since, even though successfully
presented in the literature, many of these project selection
techniques have yet to attract a large constituency among
practitioners [11], [25], [26], [34], [37]; with the results of
Cabral-Cardoso and Payne’s [8] research indicating an implementation percentage as low as 10%–15%. In a study of more
than 200 U.S. companies, Cooper et al. [12], [13] provided
empirical evidence that showed benchmark firms implemented
multiple project selection techniques within their NPD portfolio
management process. “Using multiple methods—the notion of
a hybrid approach to portfolio management—appears to be the
right answer” [13, p. 346].
Our proposed hybrid project screening decision support
methodology builds upon this notion. Fig. 2 depicts the hybrid
approach with the involvement of the decision maker at the
center of the process. Keeping the decision maker actively
engaged during the entire screening decision-making process
is the foundation with which all other activities interact.
Decision makers must define the decision hierarchy, define
criteria, derive criteria weights, and evaluate projects against
the criteria. These activities will be addressed in more detail
in the following sections. Once the priorities for each project
alternative have been determined, they are incorporated into the
portfolio optimization component of the methodology where
they represent value coefficients for a linear objective function.
The ability to conduct sensitivity analysis is designed into the
methodology and provides the decision maker with additional
insight regarding the robustness of a portfolio.

GREINER et al.: HYBRID APPROACH USING THE ANALYTIC HIERARCHY PROCESS AND INTEGER PROGRAMMING

Fig. 3. Decision making as a hierarchical structure.

B. Analytic Hierarchy Process
The AHP is a multicriteria decision support methodology
introduced by Saaty [29], which has been widely used by
both practitioners and researchers in addressing complex
decisions. Its application as a powerful management science
tool has been diverse. Examples include project management
[1], environmental policy [36], information systems [24], risk
assessment [28], and project screening [9]. The AHP allows
decision makers to model a complex problem as a hierarchical
structure that shows the relationship between the goal, primary
criteria, subcriteria, and alternatives (see Fig. 3).
One of the key factors behind AHP’s broad acceptance, and
a primary reason why it was selected as the lead component of
our hybrid methodology, is the value AHP places on a decision
maker’s input and the crucial role these inputs play in the decision-making process. Additionally, AHP is capable of integrating both qualitative and quantitative criteria into the decision-making process. Finally, through the pairwise comparison
process, AHP decomposes large, complex decisions and allows
the decision maker to focus there attention on each criterion.
While Fig. 3 portrays only two levels of decision criteria, decision makers can decompose a particular problem down to a
granularity level that corresponds to the decision-making environment by increasing the levels of subcriteria in a hierarchical
manner. The foundations of the AHP are based on the ability
of decision makers to integrate information and experience and
measure relative magnitudes through a process of pairwise comparisons [32]. Pairwise comparisons enable decision makers to
derive weights as opposed to arbitrarily assigning them, which
in the case of large-scale decisions, could increase the level of
complexity facing the decision maker thus leading to inaccuracies [39].
In general terms, the following steps describe how to apply
the AHP to a decision-making situation [1], [30], [39].
1) Define the problem, determine the desired goals, list all
alternatives to be evaluated, and establish primary criteria
and subcriteria which will be used to evaluate the alternatives.
2) Decompose the problem into a decision hierarchy starting
with the goal, through to the primary criteria and sub-

195

criteria levels, down eventually to the lowest level which
consists of the set of alternatives.
3) Collect input data from the decision makers by pairwise
comparison matrices, size
or
, where
is the number of primary criteria elements and
is
the number subcriteria elements under each parental primary criteria element. Matrices are developed for each element with respect to the level immediately above in the
decision hierarchy. Primary criteria are evaluated based
on their importance to the goal and subcriteria evaluated
based on their importance to their parental primary criteria element, while alternatives are compared in terms of
their importance against each subcriteria element.
4) Solving the eigenvalue problem for each matrix constructed in Step 3 gives the relative weights of the
primary criteria and subcriteria.
5) Hierarchical synthesis aggregates the weights of the primary criteria and subcriteria and a composite priority at
each criterion and at each level is obtained. The result
of this process is an overall priority vector for the lowest
level of the decision hierarchy—the set of all alternatives.
While this process may seem complex, commercially available software (i.e., Expert Choice) is capable of performing
all matrix calculations and solving all eigenvalue problems
extremely efficiently. What is of critical importance is the
involvement of the decision maker during Steps 1, 2, and
3. For decision makers to value the outcome of an AHP
solution, they must agree that the priorities for each criterion
generated through the pairwise comparison process accurately
represent their real-world environment. This is directly related
to developing a proper hierarchical structuring of the problem
and conducting a realistic evaluation during the pairwise
comparisons.
C. Optimizing the Portfolio
As a stand-alone technique, the AHP is capable of determining priority measures for each of the development projects
under consideration. It is not, however, capable of determining
the optimal mix of those development projects in light of a set
of resource constraints or other constraints (i.e., strategic constraints and/or mandated constraints). Incorporating a 0–1 integer programming model provides such a capability [6], [23],
[25], [38]. These types of 0–1 integer programming models,
commonly referred to as knapsack problems, are based on the
premise that the decision maker wants to define a portfolio that
provides optimal value while meeting a specific constraint, a
budgetary constraint in our case. The optimization model can
be automated and solved efficiently using the Solver function
within Microsoft Excel.
The general formulation for the optimization component is as
follows:
Parameters
individual weapon systems development project alternatives;
number of alternative weapon systems development
projects;

196

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 50, NO. 2, MAY 2003

TABLE I
C4I DEVELOPMENT PROJECTS INFORMATION

the set of alternative weapon systems development
projects;
priorities for weapon systems development projects, as
defined during the AHP component;
cost of development project ;
maximum available budget;
decision variable must take on 0–1 values, where 0 indicates project was not selected for portfolio inclusion,
and 1 indicates project was selected for portfolio inclusion.
Maximize portfolio value
(1)
subject to the following constraints:
1) budget constraint
(2)
2) ranges on decision variables
(3)
IV. APPLICATION OF THE METHODOLOGY:
AN ILLUSTRATIVE EXAMPLE
A. Participants and Project Data
To illustrate the potential for improving the weapon-systems
project screening process, an actual screening situation was
addressed. The Air Force headquarters-level office responsible
for the management oversight of all Command, Control,
Communications, Computer, and Intelligence (C4I) projects
agreed to participate in this facet of the research. A group of
three Air Force officers and three Air Force civilians, who
have been actively involved with the AFCS project selection
decision-making process, assumed the role of decision makers

in this example. A set of 15 (historical) candidate development
projects was identified based on three criteria: 1) whether
the decision maker had an understanding of each project;
2) whether accurate required funding data existed for each
project; and 3) whether each project had been reviewed by
the AFCS during the previous budget cycle evaluation. This
last criterion is important as it will allow for the comparison
between the portfolio recommendations provided by this
proposed hybrid methodology and the portfolio recommended
by the AFCS process during the previous budget cycle. For
security reasons, each project was coded by project type:
communications (COMM), command and control (C&C), and
computer (COMP). Additional information for each project is
provided in Table I.
B. Developing the Hierarchy, Deriving Criteria Weights, and
Rating Alternatives
Initial information regarding which primary and secondary
criteria would be used to evaluate the set of projects was collected during an initial brainstorming session with the six decision makers. Potential criteria were first listed by the decision
makers without regard to redundancy or correlation. Once an exhaustive list had been established, a process of clustering similar
and supporting criteria and eliminating redundant criteria was
completed. In a relatively short amount of time, the decision
makers had developed a decision hierarchy consisting of seven
primary criteria level elements, 22 secondary criteria level elements, and an overarching goal of selecting the best C4I portfolio as measured by the portfolio’s value. The complete hierarchy is shown in Fig. 4. It should be highlighted that, like other
decision-making techniques that involve a human element, the
results of an AHP hierarchy development exercise are sensitive
to the personnel makeup of the decision-making group. Preferably, this sensitivity is minimized by taking advantage of the
element of structure provided by AHP. This structure provides

GREINER et al.: HYBRID APPROACH USING THE ANALYTIC HIERARCHY PROCESS AND INTEGER PROGRAMMING

197

Fig. 4. AHP decision hierarchy for C4I development project selection example.

a medium that, in our application example, helps align AHP criteria with higher Air Force strategic objectives. Therefore, it is
reasonable to assume that Air Force decision makers are versed
in these objectives, and that the AHP hierarchy developed for
our 15 C4I projects would be similar for groups other Air Force
decision makers.
The next step in the methodology involved deriving the priority weights for each of the primary and secondary criteria
elements. This called for the decision makers to conduct pairwise comparisons at each level of the decision hierarchy. In this
process, the decision maker is first asked to rate the relative importance of each primary criteria element in achieving the overarching goal of selecting the best C4I portfolio as compared to
other elements at that level. Once this is completed for all primary criteria, the same process is conducted for the secondary
criteria elements. For example, the decision maker must evaluate whether external factors are more important than industrial
base in achieving the overarching goal, and if so, by how much.
Expert Choice software uses the nine-point fundamental scale
described by Saaty [30], [32]. The decision maker must deter-

mine on a scale of 1–9 the importance, preference, or likelihood
of each element when compared to other elements at the same
level. In the case of the example just described, if the decision
maker believes these two primary criteria elements are equally
important, then a “1” is entered in the pairwise comparison matrix. Conversely, if the decision maker believes external factors
to be extremely more important than industrial base, then a “9”
is entered in the pairwise comparison matrix. Ratings between
the two extremes indicate moderate importance “3,” strong importance “5,” and very strong importance “7,” while ratings of
“2,” “4,” “6,” or “8” describe intermediate rankings between the
two adjacent judgments. For the C4I portfolio case, each decision maker conducted individual assessments from which the
geometric mean was calculated and used as the group input for
the model [31]. Table II shows the pairwise comparison matrix
for the primary criteria elements of the decision hierarchy.
Once the pairwise comparisons have been completed for
both the primary and secondary criteria, the final step in the
AHP process is to evaluate the C4I development projects
against the 22 secondary criteria elements. For large-scale

198

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 50, NO. 2, MAY 2003

TABLE II
PAIRWISE COMPARISON MATRIX FOR PRIMARY CRITERIA ELEMENTS

decisions such as the C4I portfolio screening exercise, Saaty
proposes [30], [32] developing absolute intensity ratings for
each secondary criterion and evaluating each project alternative
against these ratings instead of conducting pairwise comparisons. For example, a three-point ratings scale was developed
for the secondary criterion element, Warfighter Capability. The
decision maker was asked to evaluate the potential for each
project alternative to improve Warfighter Capability and rate
this potential as strong, moderate, or weak, with the responses
carrying weights of 1.00, 0.405, and 0.164, respectively. This
absolute ratings method significantly reduces the number of
, to 330,
evaluations necessary from 2310,
, where represents the number of C4I development
represents the total number of
project alternatives and
secondary criteria elements. Again, each decision maker conducted individual assessments from which the geometric mean
was calculated and used as the group input for the model [31].
C. AHP Model Results
With all pairwise comparisons and ratings entered into
the AHP model, Expert Choice software quickly solves
the eigenvalue problems and synthesizes the global priority
weights for the primary criteria, secondary criteria, and the
C4I development project alternatives. In addition, the AHP
measures inconsistency within the decision hierarchy and
calculates an inconsistency index. For large decision problems
like the C4I portfolio example, index measures of 0.10 or less
indicate consistency by the decision makers throughout the
hierarchy [31], [32]. In the C4I portfolio example, an overall
inconsistency index of 0.06 was calculated.
Fig. 5 shows the primary criteria, secondary criteria, and
the C4I development project alternatives along with their
respective priority weights. At the primary criteria level, user’s
needs (0.372), capability (0.232), and funding (0.181) were the
three highest weighted elements. An order of magnitude below
those were risk (0.069), standards (0.063), external factors
(0.059), and industrial base (0.026). At the secondary level,
defense planning guidance/integrated priority list (0.199),
funding source (0.127), improve warfighter capability (0.125),
and operational requirements document (0.091) were the
four highest weighted elements. Finally, the bottom of Fig. 5
shows the priority weights for the C4I development project

alternatives with COMM4 (0.092) the highest weighted C4I
development project.
D. Portfolio Optimization Results
The final step of the hybrid methodology is to consider the
effects of constrained resources when selecting the final C4I
development project portfolio. The optimization component of
the hybrid methodology accomplishes this goal. In optimizing
the portfolio, the 0–1 integer program described in Section III
is solved. The priority weights, , derived in the AHP model
represent a measure of value in the linear objective function.
Table I lists project costs in addition to information regarding
the level of funding allocation that was recommended during
the previous budget cycle. Since the recommended funding level
for this portfolio of projects was $196.46 million, the maximum
available budget is set at this value for the C4I portfolio example.
Table III presents the results of the portfolio optimization
component of the hybrid methodology. In this table, four solutions are presented. First, the Air Force recommended solution shows which projects, and at what level of funding commitment, were recommended during the previous budget cycle.
A “1” indicates the project was fully funded, while a “0” indicates the project was not. Projects C&C2 and COMP1 were
recommended for partial funding. In order to measure the portfolio value for this solution, the priority weights derived from
the AHP component were applied. A project that was selected
was assigned its respective priority weight. For those projects
that were recommended for partial funding, we assume a linear
reduction of the full priority weight (i.e., C&C2 was partially
funded at 0.721, therefore, its value measure is
). In measuring the utility of each priority weight, it should
be noted that the potential exists for a nonlinear relationship;
likely contingent upon such factors as cost or the point in time
within a weapon system’s acquisition lifecycle. Since the primary focus of this research pertains to the project screening
process, in which go/no-go decisions are being made early in the
acquisition lifecycle, this linear assumption seems appropriate.
A total portfolio value of 0.581 was realized with $196.46 million in funding required.
In the hybrid methodology (optimized) solution, the C4I
development project alternatives were either selected for

GREINER et al.: HYBRID APPROACH USING THE ANALYTIC HIERARCHY PROCESS AND INTEGER PROGRAMMING

199

Fig. 5. Priority weights for primary criteria, secondary criteria, and C4I alternatives.

full funding or were not included in the recommended C4I
portfolio. For this solution, a total portfolio value of 0.878
was realized (a 51.1% improvement over the Air Force recommended solution) with $190.28 million in funding required. By
selecting projects based on their priority weights, highest values
first, then selecting projects in descending order until available

)
funds were exhausted, the hybrid methodology (Greedy
solution was developed. For this solution, a total portfolio value
of 0.649 was realized (a 11.7% improvement over the Air Force
solution) with $193.76 million in funding required. Finally, by
selecting projects based on the ratio of their priority weights
versus costs, highest values first, then selecting projects in

200

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 50, NO. 2, MAY 2003

TABLE III
C4I PORTFOLIO OPTIMIZATION RESULTS

descending order until available funds were exhausted, the
) solution was developed.
hybrid methodology (Greedy
For this solution, a total portfolio value of 0.857 was realized (a
47.5% improvement over the Air Force solution) with $166.28
million in funding required. These heuristics are useful as a
quick measure, but in this example fall short of the optimal
and the Greedy
portfolio value where the Greedy
heuristic’s solutions represent 73.9% and 97.6% of the optimal
portfolio value, respectively.
V. METHODOLOGY EXTENSIONS
The previous example showed the benefits, in increased portfolio value, of implementing the optimized AHP methodology.
Of additional interest to the decision maker is determining how
the methodology can be extended to more accurately model a
particular decision-making environment. In this section we explore several different extensions to the model defined in the
C4I portfolio example. The three specific cases include partial funding of projects, mandatory project selection, and a balanced portfolio. This section concludes with some discussion
regarding the modeling of budget uncertainties.
In the previous C4I portfolio example, the optimized solution, while greatly improving the current portfolio value, still
leaves $6.18 million in unallocated funds. In many NPD environments, it may not be desirable to select a development portfolio without allocating all available resources. Allowing for the
partial funding of projects circumvents this situation. To model
the partial funding of projects simply requires modifying the
constraint within the optimization component to
instead allow for continuous values of . Table IV shows that
if we allow C4I projects to be partially funded, the portfolio
value increases to 0.921 by funding COMM1 at 87.5% of its
required amount. COMM1 being funded over C&C1 is logical since COMM1 has a higher priority weight (0.073) than

C&C1 (0.070). By definition, the solution to the modified linear
optimization process guarantees that all projects will be fully
funding, with the exception of those projects for which sufficient funds do not exist (COMM1 and C&C1 in this case).
In the weapon systems development environment, there are
many instances where senior leadership (i.e., Secretary of Defense or Congress) mandates that certain projects will be funded
and development will begin. The ability to model the effects on
the entire portfolio of development projects again can be modeled by modifying the optimization component of the methodology. If it were mandated that, in the C4I portfolio example, all
four communications and both command and control development projects were to be fully funded and included in the portfolio, then adding the following constraints models this scenario
COMM
(4)
C&C
(5)
where COMM and C&C represent the set of communications
and command and control projects, respectively. For this
methodology extension, a total portfolio value of 0.814 was realized with $193.78 million in funding required (see Table IV).
The final methodology extension explores situations where
the decision maker may be interested in selecting a portfolio
of projects that are balanced as opposed to strictly optimizing
the portfolio value. In the C4I portfolio example, the original
optimized solution recommended selecting all of the communications projects, eight of the nine computer projects, and only
one of the command and control projects. In this recommended
portfolio, communication projects account for 45.5% of the
allocated budget, while computer and command and control
projects account for 47.2% and 7.3%, respectively. To provide
a more balanced portfolio, the decision maker may decide to
control the level of funding allocated to be more representative
of the relative required funding (as presented in Table I).
For example, suppose the decision maker desires command

GREINER et al.: HYBRID APPROACH USING THE ANALYTIC HIERARCHY PROCESS AND INTEGER PROGRAMMING

201

TABLE IV
HYBRID METHODOLOGY EXTENSION RESULTS

and control projects represent at least 24% of the allocated
budget, and limit the funding level of computer projects and
communications projects to 41% and 35%, respectively, of the
allocated budget. Adding the following constraints models the
decision maker’s goal:
(6)
(7)
(8)
For this methodology extension, a total portfolio value of 0.855
was realized with $179.98 million in funding required (see
Table IV).
Similarly, the portfolio can be balanced with regards to the
project horizon (near-term versus long-term). In the C4I portfolio example, the original optimized solution recommended
selecting eight of the nine near-term projects and five of the six
long-term projects. In this recommended portfolio, near-term
projects account for 49.4% of the allocated budget, while
long-term projects account for 50.6%. Again, the decision
maker may wish to control the level of funding allocated to
be more representative of the relative required funding (as
presented in Table I). In this extension, the decision maker
desires Near-term projects represent no more than 43% of the
allocated budget. Adding the following constraint models the
decision maker’s goal:
(9)
For this methodology extension, a total portfolio value of 0.855
was realized with $179.98 million in funding required (see
Table IV).

Tables III and IV provide the decision maker with additional
insight regarding the robustness of recommended portfolio options. Considering the fully funded, partially funded, and both
balanced portfolio scenarios, 11 of the 15 C4I development
project alternatives are common among each portfolio scenario.
However, of particular interest is the fact that three of the four
project alternatives not included in the recommended portfolios
for each scenario are three of the four highest cost projects.
The priority weights ( ) for COMM1, C&C1, and COMP6
are ranked in the top five for all C4I development project
alternatives. However, the ratio of priority weights versus costs
) ranks these three projects at the bottom.
(
In the 0–1 integer program, the primary constraint is the maximum budget available to fund the C4I development project
alternatives. In the C4I portfolio example, it is assumed that
the maximum budget available is known with certainty to be
$196.46 million. In a simple spreadsheet, the decision maker can
conduct sensitivity analysis that models the change in portfolio
value as the available budget is increased or decreased. Fig. 6
shows the results of such an exercise when applied to the C4I
portfolio example for the fully funded, partially funded, mandated, and both balanced portfolio scenarios.
VI. DISCUSSION AND CONCLUSION
In this paper, we have presented a hybrid methodology to support the decision maker in selecting weapon-systems development projects. The hybrid methodology integrates an AHP component with a 0–1 integer program to provide the decision maker
with an effective and efficient decision support process that accurately models today’s constrained resource environment. One
of the unique aspects of military weapon systems development
is the need to define a measure of value for each system. Unlike
the traditional economic measures commonly found in commercial portfolio screening practices, weapon system value is more

202

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 50, NO. 2, MAY 2003

Fig. 6. Portfolio value versus increased budget.

difficult to ascertain and often involves more intangible success
criteria. One of the benefits of the AHP is its ability to provide
structure to the decision-making process and provide a measure
of value for development processes that involve such intangible
success criteria.
Implementing the hybrid methodology in an actual weapon
systems portfolio screening decision provided an opportunity to
measure process improvement. Results of this exercise indicate
that decision support provided by our hybrid methodology could
lead to substantial improvements in a weapon systems portfolio
value. Of equal importance in assessing process improvement,
those involved with the C4I portfolio exercise found the hybrid
methodology provided additional insight into the relative importance of decision-making criteria, provided a means for addressing and systematically evaluating intangible and qualitative criteria, and overall, believed the hybrid methodology could
improve the weapon system portfolio screening process.
Results of this exercise also point out an interesting fact, that
in many of the portfolio scenarios, those projects with some of
the highest costs are not included in the recommended portfolio.
While ranked high in the AHP process, their priorities are, however, not overwhelming when compared to the rest of the C4I
development project alternatives. Methodology extensions provided several options for dealing with this situation—balancing
the portfolio, mandating certain projects be included, or using
a greedy heuristic approach and selecting based on highest priority weights first. Other alternatives that warrant further investigation include determining whether projects that have differences in costs that are orders of magnitude should be separated
for evaluation and the effect these projects may have on the assumption of a linear marginal utility function, developing relative priorities that take into consideration project cost (versus
absolute priorities that do not), or scaling priority weights to
take project costs into consideration.

Although the hybrid methodology may appear time consuming and complex, Expert Choice software and spreadsheet
software like Excel allow for complete automation of the
process. Developing the decision hierarchy and populating the
model with the decision makers’ pairwise comparison inputs
and assessments took less than four hours. With an automated
process, once the initial personnel capital investment in conducting these comparisons and assessments was completed,
the model could be updated as conditions warranted and future
portfolio screening exercises could be completed in a fraction
of the time.
While this paper was primarily focused on screening weapon
systems development projects, the hybrid methodology’s usefulness as a decision support process is more universal. Several methodological extensions were offered that provided the
decision maker with examples of how the AHP/portfolio optimization hybrid methodology could be adapted to address a
variety of decision-making environments; in the DoD and potentially commercial industry. Decision-making environments
that must take into account both qualitative and intangible measures of success are prime candidates for the implementation of
an AHP/portfolio optimization hybrid methodology.

REFERENCES
[1] K. M. Al-Subhi Al-Harbi, “Application of the AHP in project management,” Int. J. Prod. Manage., vol. 19, pp. 19–27, 2001.
[2] N. R. Baker and W. H. Pound, “R&D project selection: Where we stand,”
IEEE Trans. Eng. Manage., vol. EM-11, pp. 124–134, Nov. 1964.
[3] R. Balachandra, “Critical signals for making go/nogo decisions in new
product development,” J. Prod. Innov. Manage., vol. 2, pp. 92–100,
1984.
[4]
, “A comparison of R&D project termination factors in four industrial nations,” IEEE Trans. Eng. Manage., vol. 43, pp. 88–96, Feb. 1996.
[5] J. F. Bard, R. Balachandra, and P. E. Kaufmann, “An interactive approach to R&D project selection and termination,” IEEE Trans. Eng.
Manage., vol. 35, pp. 139–146, Aug. 1988.
[6] G. J. Beaujon, S. P. Marin, and G. C. McDonald, “Balancing and optimizing a portfolio of R&D projects,” Nav. Res. Logist., vol. 48, no. 1,
pp. 18–40, 2001.
[7] D. Bertsimas, C. Darnell, and R. Soucy, “Portfolio construction through
mixed-integer programming at Grantham, Mayo, Van Otterloo, and
Company,” Interfaces, vol. 29, no. 1, pp. 49–66, 1999.
[8] C. Cabral-Cardoso and R. L. Payne, “Instrumental and supportive use of
formal selection methods in R&D project selection,” IEEE Trans. Eng.
Manage., vol. 43, pp. 402–410, Nov. 1996.
[9] R. J. Calantone, C. A. Di Benedetto, and J. B. Schmidt, “Using the
analytic hierarchy process in new product screening,” J. Prod. Innov.
Manage., vol. 16, pp. 65–76, 1999.
[10] M. J. Cetron, J. Martino, and L. Roepcke, “The selection of R&D
program content—Survey of quantitative methods,” IEEE Trans. Eng.
Manage., vol. EM-14, pp. 4–13, Feb. 1967.
[11] R. G. Cooper, S. J. Edgett, and E. J. Kleinschmidt, Portfolio Management
for New Products. Cambridge, MA: Perseus, 1998.
[12]
, “Best practices for managing R&D portfolios,” Res. Technol.
Manage., vol. 41, no. 4, pp. 20–33, 1998.
[13]
, “New product portfolio management: Practices and performance,”
J. Prod. Innov. Manage., vol. 16, pp. 333–351, 1999.
[14] R. G. Cooper and E. J. Kleinschmidt, “An investigation into the new
product process: Steps, deficiencies, and impact,” J. Prod. Innov.
Manage., vol. 3, pp. 371–385, 1986.
[15]
, “Benchmarking firm’s new product performance and practices,”
Eng. Manage. Rev., vol. 23, no. 3, pp. 112–120, 1995.
[16]
, “Winning businesses in product development: Critical success factors,” Res. Technol. Manage., vol. 39, no. 4, pp. 18–29, 1996.
[17] Department of Defense, “Annual report to the President and the Congress,” Office of the Secretary of Defense, Washington, DC, 2000.

GREINER et al.: HYBRID APPROACH USING THE ANALYTIC HIERARCHY PROCESS AND INTEGER PROGRAMMING

[18] M. A. Greiner, K. J. Dooley, D. L. Shunk, and R. T. McNutt, “An assessment of Air Force development portfolio management practices,” Acq.
Rev. Quart., vol. 9, no. 2, pp. 117–142, 2002.
[19] K. Heidenberger, “Dynamic project selection and funding under risk: A
decision tree based MILP approach,” Euro. J. Oper. Res., vol. 95, no. 2,
pp. 284–298, 1996.
[20] A. D. Henriksen and A. J. Traynor, “A practical R&D project-selection
scoring tool,” IEEE Trans. Eng. Manage., vol. 46, pp. 158–170, May
1999.
[21] J. A. Jackson, J. M. Kloeber, Jr., B. E. Ralston, and R. F. Deckro, “Selecting a portfolio of technologies: An application of decision analysis,”
Decision Sci., vol. 30, no. 1, pp. 217–238, 1999.
[22] R. N. Kostoff, “Evaluation of proposed and existing accelerated research
programs by the Office of Naval Research,” IEEE Trans. Eng. Manage.,
vol. 35, pp. 271–279, Nov. 1988.
[23] G. J. Kyparisis, S. K. Gupta, and C. Ip, “Project selection with discounted returns and multiple constraints,” Eur. J. Oper. Res., vol. 94,
no. 1, 1996.
[24] J. W. Lee and S. H. Kim, “An integrated approach for interdependent
information system project selection,” Int. J. Prod. Manage., vol. 19,
pp. 111–118, 2001.
[25] M. J. Liberatore and G. J. Titus, “The practice of management science in
R&D project management,” Manage. Sci., vol. 29, no. 8, pp. 962–974,
1983.
[26] C. H. Loch, M. T. Pich, C. T. Terwiesch, and M. Urbschat, “Selecting
R&D projects at BMW: A case study of adopting mathematical programming models,” IEEE Trans. Eng. Manage., vol. 48, pp. 70–80, Feb.
2001.
[27] R. T. McNutt, “Reducing DoD product development time: The role of
the schedule development process,” Ph.D. dissertation, Massachusetts
Institute of Technology, Cambridge, 1998.
[28] M. A. Mustafa and J. F. Al-Bahar, “Project risk assessment using the analytic hierarchy process,” IEEE Trans. Eng. Manage., vol. 38, pp. 46–52,
Feb. 1991.
[29] T. L. Saaty, The Analytic Hierarchy Process. New York: McGraw-Hill,
1980.
, Decision Making for Leaders: The Analytic Hierarchy Process for
[30]
Decisions in a Complex World. Pittsburgh, PA: RWS, 1990.
[31]
, “How to make a decision: The analytic hierarchy process,” Interfaces, vol. 24, no. 6, pp. 19–43, 1994.
[32]
, Fundamentals of Decision Making and Priority Theory With the
Analytic Hierarchy Process. Pittsburgh, PA: RWS, 2000.
[33] J. B. Schmidt and R. J. Calantone, “Are really new product development
projects harder to shut down?,” J. Prod. Innov. Manage., vol. 15, pp.
111–123, 1998.
[34] R. L. Schmidt and J. R. Freeland, “Recent progress in modeling R&D
project-selection processes,” IEEE Trans. Eng. Manage., vol. 39, pp.
189–201, May 1992.
[35] G. M. Scott, “Critical technology management issues of new product
development in high-tech companies,” J. Prod. Innov. Manage., vol. 17,
pp. 57–77, 2000.
[36] M. Z. Siddiqui, J. W. Everett, and B. E. Vieux, “Landfill siting using
geographic information systems: A demonstration,” J. Environ. Eng.,
vol. 122, no. 6, pp. 515–523, 1996.
[37] K. M. Watts and J. C. Higgins, “The use of advanced management techniques in R&D,” Omega, vol. 15, no. 1, pp. 221–229, 1987.
[38] L. A. Wolsey, Integer Programming. New York: Wiley, 1998.
[39] F. Zahedi, “The analytic hierarchy process—A survey of the method and
its applications,” Interfaces, vol. 14, no. 4, pp. 96–108, 1986.

Michael A. Greiner received the Ph.D. degree in industrial engineering from Arizona State University,
Tempe.
He is currently an Assistant Professor of Acquisition Management and the Director of the
Graduate Cost Analysis Program, Air Force Institute
of Technology, Wright-Patterson AFB, OH. His
research interests include acquisition reform and
its relationship with cost management, new product
development, and risk modeling and mitigation
planning. He holds the Society of Cost Estimating
and Analysis Certified Cost Estimator/Analyst Professional Certification.
Dr. Greiner is a member of the Institute of Operations Research and the Management Sciences, the Decision Sciences Institute, and the Society for Cost Estimating and Analysis.

203

John W. Fowler (M’87) received the Ph.D. degree in
industrial engineering from Texas A&M University,
College Station, and spent the last year and a half of
his doctoral studies as an Intern with Advanced Micro
Devices, Austin, TX.
He is currently an Associate Professor with the
Industrial Engineering Department, Arizona State
University (ASU), Tempe. He is also the Co-Director
of the Modeling and Analysis of Semiconductor
Manufacturing Laboratory, ASU, which has had
research contracts with the National Science
Foundation, Semiconductor Research Corporation (SRC), SEMATECH,
Infineon Technologies, Intel, Motorola, ST Microelectronics, and Tefen, Ltd.
He is also an Area Editor for SIMULATION: Transactions of the Society For
Modeling and Simulation International—Applications. Prior to his current
position, he was a Senior Member of the Technical Staff in the Modeling,
CAD, and Statistical Methods Division, SEMATECH, Austin,TX. His current
research interests include modeling, analysis, and control of semiconductor
manufacturing systems.
Dr. Fowler is an Associate Editor of IEEE TRANSACTIONS ON ELECTRONICS
PACKAGING MANUFACTURING. He is a Member of the American Society for
Engineering Education, the Institute of Industrial Engineers, Institute of Operations Research and the Management Sciences, the Production and Operations
Management Society, and the Society for Computer Simulation.

Dan L. Shunk received the B.S., M.S., and Ph.D. degrees from Purdue University, West Lafayette, IN.
He began his career in the United States Air Force
at Wright-Patterson AFB, OH, as the Co-Founder
of the Integrated Computer Aided Manufacturing
(ICAM) Program. He has previously held positions
in industry with GCA, International Harvester, and
Rockwell. He is currently the Avnet Chair of Supply
Network Integration and an Associate Professor of
Industrial Engineering at Arizona State University,
Tempe.

W. Matthew Carlyle received the B.S. degree in information and computer science from the Georgia Institute of Technology, Atlanta, in 1992, and the Ph.D.
degree in operations research from Stanford University, Standford, CA, in 1997.
He is currently an Associate Professor with the
Operations Research Department, Naval Postgraduate School, Monterey, CA. He joined the faculty in
2002 after working as an Assistant Professor with the
Department of Industrial Engineering, Arizona State
University, Tempe. His research interests include
effective models and solution procedures for large combinatorial optimization
problems. Applications of this research have included modeling and analysis
of Navy combat logistics force size and structure, sensor mix and deployment
for the Army’s objective force, communications network diversion, workforce
planning, underground mining operations, printed circuit-card assembly
systems, and semiconductor manufacturing operations.

Ross T. McNutt received the Ph.D. degree in technology, management, and policy in defense product
development from Massachusetts Institute of Technology, Cambridge.
He is currently on the faculty at Air Command
and Staff College, Maxwell AFB, AL. Previously,
he was the Director of the Air Force Cycle Time
Reduction Program in the Air Force Assistant
Secretary (Acquisition) Office, Washington, DC.
He directed Air Force efforts to reduce acquisition
response time—the time required to develop and
field new and modified weapon systems.

European Journal of Operational Research 238 (2014) 270–280

Contents lists available at ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Decision Support

An intelligent decomposition of pairwise comparison matrices
for large-scale decisions
Eugene Rex Jalao b,⇑, Teresa Wu a, Dan Shunk a
a
b

School of Computing, Informatics and Decision Systems Engineering, Arizona State University, 699 S. Mill Ave., Tempe, AZ 85281, United States
Department of Industrial Engineering and Operations Research, University of the Philippines, Diliman 1101, Philippines

a r t i c l e

i n f o

Article history:
Received 22 February 2013
Accepted 21 March 2014
Available online 2 April 2014
Keywords:
AHP
ANP
Pairwise comparison matrices
Inconsistency
Binary integer programming

a b s t r a c t
A Pairwise Comparison Matrix (PCM) has been used to compute for relative priorities of elements and are
integral components in widely applied decision making tools: the Analytic Hierarchy Process (AHP) and
its generalized form, the Analytic Network Process (ANP). However, PCMs suffer from several issues limiting their applications to large-scale decision problems. These limitations can be attributed to the curse
of dimensionality, that is, a large number of pairwise comparisons need to be elicited from a decision
maker. This issue results to inconsistent preferences due to the limited cognitive powers of decision makers. To address these limitations, this research proposes a PCM decomposition methodology that reduces
the elicited pairwise comparisons. A binary integer program is proposed to intelligently decompose a
PCM into several smaller subsets using interdependence scores among elements. Since the subsets are
disjoint, the most independent pivot element is identiﬁed to connect all subsets to derive the global
weights of the elements from the original PCM. As a result, the number of pairwise comparison is reduced
and consistency is of the comparisons is improved. The proposed decomposition methodology is applied
to both AHP and ANP to demonstrate its advantages.
Published by Elsevier B.V.

1. Introduction
An m  m pairwise comparison matrix (PCM) denoted by A is a
reciprocal matrix which is composed of pairwise comparisons
aij 2 ½1=9; 9 that represent the scaled relative importance scores
of element i as compared to element j. These elements could be criteria or alternatives within a PCM. Typically, a PCM is generated
from pairwise comparisons elicited from a decision maker (DM)
to estimate element priorities for any decision problem. One of
the most widely used multiple criteria decision making (MCDM)
methodology that use PCMs is the Analytic Hierarchy Process
(AHP) developed by Saaty (1977). Currently, there are several successful applications of the AHP in a wide-range of MCDM problems
(Ishizaka & Labib, 2011). Conversely, the AHP fails to account for
the interdependencies of the criteria and alternatives, and hence
it assumes that all criteria and alternatives are independent. If left
unchecked, any DM using the AHP would then provide inaccurate
decisions. To address this issue, the Analytic Network Process
⇑ Corresponding author. Contact Address: Melchor Hall Room 402, Department of
Industrial Engineering and Operations Research, College of Engineering, University
of the Philippines Diliman, Quezon City 1101, Philippines. Tel.: +632 981 8500 loc
3128.
E-mail address: eljalao@up.edu.ph (E.R. Jalao).
http://dx.doi.org/10.1016/j.ejor.2014.03.032
0377-2217/Published by Elsevier B.V.

(ANP) has been developed by Saaty and Takizawa (1986) as a generalization of the AHP. The ANP requires additional pairwise comparisons to estimate the inner and outer dependencies of the
criteria and alternatives. Although it addresses the limitations of
the AHP, the ANP still use PCMs which are faced with the following
issues: (1) numerous pairwise comparisons elicited from the DM
are required for the ANP to work and (2) inconsistent pairwise
comparisons are obtained when numerous pairwise comparisons
are elicited from the DM are large.
The ﬁrst limitation is attributed to the fact that both methodologies suffer from the curse of dimensionality. Consider a PCM of m
criteria. A total of mðm  1Þ=2 pairwise comparisons are needed to
obtain the priorities. Additionally, for the ANP, m2 comparisons are
needed to estimate the inner dependencies of the criteria. This
would be impractical when m is large. Saaty (1977) argues that
the redundancy of the questioning process provides weights which
are much less sensitive to biases and judgement errors. In a case
study by Lin, Wang, and Yu (2008), it took two and a half hours
on average to complete a three-level AHP decision problem per
DM and a total of 380 man-hours to complete all pairwise comparisons. This could be greater for the case of the ANP. On the other
hand, there are generally three reasons why a DM is reluctant to
complete the required comparisons speciﬁcally: (1) there is insufﬁcient time to complete all comparisons; (2) the DM is unwilling to

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

make direct comparisons of two alternatives and (3) the DM is
unsure of some of the comparisons (Harker, 1987a). In a MonteCarlo simulation study (Carmone, Kara, & Zanakis, 1997), comparisons are deleted from large matrices and results show that at most
50% of the comparisons can be deleted without signiﬁcantly affecting the weights of the criteria. Furthermore, to obtain a reasonable
and consistent PCM, Saaty (1977) recommends that the number of
criteria or alternatives within a PCM should only be at most seven.
Hence, any PCM with eight or more elements is considered large.
Unfortunately, lots of decision problems far exceed this maximum
threshold. There exist several articles that attempt to address the
issue of numerous pairwise comparisons, one of which are called
decomposition methodologies (Ishizaka, 2008; Ishizaka, 2012;
Shen, Hoerl, & McConnell, 1992; Triantaphyllou, 1995). When a
PCM A of n alternatives is decomposed into k subsets, pairwise
comparisons are elicited only in those subsets. Since the dimensions of the decomposed matrices are smaller than A, a reduction
in the number of pairwise comparisons is realized. Unfortunately,
these methods are not without any disadvantages. Firstly, these
methodologies focus on decomposing PCMs for alternatives. Lots
of pairwise comparisons can be saved when these methodologies
are extended to decompose the criteria PCMs since both the AHP
and ANP can have multiple criteria PCMs within the hierarchy or
network respectively. Secondly, when a PCM is decomposed into
subsets, the obtained relative weights of the elements are valid
only within those subsets and the problem arises when aggregating the results back to its global counterpart. As a result, a pivot
element is selected arbitrarily and assigned to all subsets and is
used as a basis for comparing the elements across all disjoint subsets. The global weights can then be estimated (Shen et al., 1992;
Ishizaka, 2008). Please note that pivot element selection is a challenging issue as the decisions should consider reducing the number and inconsistency of the pairwise comparisons, as well as
reducing the amount of dependence present among elements
within each subset. Thirdly, these methodologies lack guidelines
to assign criteria or alternatives to respective subsets since they
are done arbitrarily. This does not guarantee that the number of
pairwise comparisons elicited is reduced optimally.
The second limitation related to the curse of dimensionality is
attributed to the consistency of the pairwise comparisons elicited
from the DM when the number of alternatives or criteria is large.
As the number of pairwise comparisons increases, the consistency
of these comparisons is expected to be less reliable and results to
inconsistent decisions (Weiss & Rao, 1987). A performance metric
called consistency index (CI) is generally used to estimate the
inconsistency of a PCM A (Saaty, 1977). The CI is computed by
obtaining the eigenvalue of the pairwise comparison matrix using
Eq. (1):

CIðAÞ ¼

kmax  m
m1

ð1Þ

where m is the dimension of the PCM A and kmax is the maximal
eigenvalue of matrix A. The consistency ratio (CR) is the ratio of
CI and RI and is computed using Eq. (2):

CRðAÞ ¼

CIðAÞ
RIðmÞ

ð2Þ

where RIðmÞ is the random index obtained from the average CI of
500 randomly ﬁlled matrices and is dependent on the value of the
m. According to Saaty (1977), if a PCM A has CR < 10%, then A is considered to have an acceptable level of consistency. Nevertheless,
DMs that use the PCMs are faced with the issues on bounded rationality (Simon, 1972). With this, due to their limited cognitive processing powers, the DMs are not expected to provide consistent
pairwise comparisons all throughout the pairwise comparison elicitation process especially when the number of pairwise comparisons

271

is large. Therefore, a methodology that reduces the pairwise comparisons elicited from the DM would lead to improved consistency levels since only a handful of pairwise comparisons are elicited and
would not be cognitively taxing to the DM.
This research proposes the PCM Decomposition Methodology
(PDM) to address the limitations of PCMs when used in either
the AHP or the ANP methodology. The contributions of the PDM
are twofold: (1) The PDM seeks to reduce the number of pairwise
comparisons elicited from the DM thereby increasing its consistency. A binary integer programming (BIP) model is proposed to
accomplish this by intelligently decomposing PCMs into smaller
and manageable subsets. Only pairwise comparisons within those
subsets are elicited from the DM. The BIP uses the inner dependence comparisons of the elements to assign these elements into
mutually exclusive subsets. Hence, interdependent elements are
separated as much as possible thereby reducing the amount of
interdependencies among subsets. (2) Since the subsets are disjoint, a pivot element is optimally selected and is used to connect
all pairwise comparison matrices within each PCM. The pivot is
selected that minimizes the interdependencies of the elements.
Using the pivot element and the local weights, the global weights
of the elements of the PCM are then calculated.
The rest of this paper is organized as follows. Section 2 reviews
existing literature that tries to solve the aforementioned problems.
Section 3 illustrates the steps of the proposed PDM, while Section 4
describes the application of the PDM in reducing the number of
pairwise comparisons in an AHP problem. On the other hand, Section 5 presents the application of the PDM in reducing the number
of pairwise comparisons in an ANP problem. Finally, Section 6 concludes the paper and proposes further research areas.
2. Review of related literature
There exist methodologies that address the limitations for the
PCMs, in terms of the numerous required pairwise comparisons
and its inconsistency, can be mainly classiﬁed as: (1) optimization
methods and (2) heuristic methods. Each category is reviewed as
follows.
2.1. Optimization methods
Optimization models start with a handful of pairwise comparisons only. The remaining pairwise comparisons are estimated
using optimization algorithms by taking advantage of the matrix
properties of A. Starting with a of minimum m  1 comparisons,
a gradient descent method is proposed to select the next pairwise
comparison that would have the biggest information gain (Harker,
1987b). Additionally, stopping rules are provided for terminating
the pairwise comparison elicitation process. The methodology by
Bozoki, Fulop, and Ronyai (2009) uses nonlinear optimization with
exponential scaling to estimate the missing pairwise comparisons
from available ones. However, all possible combinations of connecting paths must be considered. The number of combinations
exponentially grows as the number of missing comparisons
increases and thus would be inefﬁcient to solve. A linear programming formulation by Triantaphyllou (1995) is used to estimate the
missing pairwise comparisons of A by considering two arbitrary
subsets s1 and s2 of the criteria PCM where s1 [ s2 – ;. By solving
the linear programming problem, the global weights of the m criteria of the PCM can be estimated. Nevertheless, the algorithm only
focuses on dividing the PCM A into two subsets. If m is large, then
the two subsets are still large. Moreover, the error rates of estimating the missing comparisons are dependent on the number of
common elements of subsets s1 and s2 . The smaller the s1 [ s2 ,
the estimation of the missing comparisons is expected to be less

272

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

accurate and deviation error increases signiﬁcantly. The issue of
pairwise elicitation is addressed by Despotis and Derpanis (2008)
which is composed of a min–max goal programming formulation
to derive the priorities from an interval. Zhang and Chen (2009)
develop a nonlinear programming model to improve the consistency of a PCM that uses a genetic simulated annealing algorithm.
In general, the challenge of optimization based approach lies to the
scalability of the problem. That is, as most decision problems tend
to have large number of alternatives and criteria, these analytical
approaches may suffer.
2.2. Heuristic methods
There are several types of heuristic methodologies but all of
propose to reduce the number of pairwise comparisons elicited
from the DM. Saaty (1990) proposes the idea to group alternatives
into subsets according to a common decision criterion. Ishizaka
(2012) propose to assign the alternatives into k subsets based on
a subjective absolute scale in which alternatives that have close
‘‘magnitudes’’ are grouped together. By using several pivot alternatives that is common to at most two subsets, the global priorities of
the alternatives are then obtained. Note, the deﬁnition of close
‘‘magnitudes’’ is not well deﬁned and is highly subjective. Furthermore, no guidelines are provided to determine which alternatives
are assigned to which subset. Shen et al. (1992) propose an arbitrary decomposition of the alternative PCM into k subsets such that
these k subsets have one common pivot alternative. Pairwise comparisons are ﬁrst performed on each subset and local priorities are
calculated. The global priority is then derived by using common
pivot alternative and local priorities of each subset. Ishizaka
(2012) applies the same decomposition algorithm on supplier
selection. There exist models that use the concept of a balanced
incomplete block designs in which subsets of the PCM A are
assigned to different DMs treated as replicates in contrast to having all DMs focus on the large PCM (Takahashi, 1990; Weiss &
Rao, 1987). The computation of the global alternative weights is
done by using the geometric mean. To the best of our knowledge,
no methodology has tried to optimally assign PCM elements to
subsets that minimize the number of pairwise comparisons, the
amount of dependence among criteria and the consistency of the
pairwise comparisons. Any methodology that reduces the required
number of pairwise comparisons for a PCM is fruitful for wider
adoption of the AHP and ANP methodologies (Ishizaka & Labib,
2011). Triantaphyllou (2000) develops a method to reduce the
number of pairwise comparisons via the duality approach when
the number of alternatives is greater than the number of criteria
plus one. Islam and Abdullah (2006) consider reducing the number
decision criteria by the nominal group technique. The decision criteria that have insigniﬁcant weights are eliminated from future
pairwise comparison elicitation process.
There are several articles (Benítez, Delgado-Galván, Izquierdo, &
Pérez-García, 2011b; Cao, Leung, & Law, 2008; Ergu, Kou, Peng, &
Shi, 2011; Saaty, 2003) that focus on reducing the inconsistency
of a given PCM without reducing the number of pairwise comparisons. Benítez et al. (2011b) propose a linearization heuristic that
provides the closest consistent PCM which is later extended to balance the consistency and the preferences of the DM (Benítez,
Delgado-Galván, Gutiérrez, & Izquierdo, 2011a). Still, upon applying these methodologies, the original pairwise comparisons significantly deviate from the resulting consistent pairwise comparisons
and thus would not reﬂect the actual preferences of the DM. These
methods focus on improving a given PCM which could be large. If
the number of decision criteria or alternatives is large, these
heuristics would still take a lot of time to complete and are more
subject to human error when providing the initial pairwise
comparisons.

We present Table 1 as an overview of the reviewed existing
related literature and their corresponding methodologies on
addressing PCM limitations.
In summary based on the analysis from Table 1, the following
four gaps in literature can be gleaned from the review as follows:
(1) existing decomposition methodologies focus on decomposing
the alternative PCMs with alternative elements. Additional pairwise comparisons can be saved when decomposing algorithms
are extended to criteria PCMs. (2) These methodologies lack guidelines to assign elements to subsets of the PCMs that minimize the
number of pairwise comparisons elicited, as well as the independence of elements within each subset. (3) These methodologies
select the pivot element arbitrarily and no rules are provided in
literature. (4) To the best of our knowledge, there is a lack of methodologies that try to reduce the number of pairwise comparisons of
a PCM for the ANP but not for the AHP. Any methodology that can
simplify the ANP would be beneﬁcial for any decision with interdependent criteria and alternatives.
3. Proposed PCM decomposition methodology
This section outlines the proposed PDM decision making framework. Fig. 1 presents a high level overview of the proposed PDM.
We illustrate the decomposition of a PCM A with m elements. In
step 1, the m elements are collected and a value of the number of
subsets k 2 ½2; m  1 is elicited from the DM. The pairwise comparisons that measure the inner dependencies of the elements are
qualitatively elicited or quantitatively gathered. Quantitative pairwise comparisons are direct observations from the attributes of
alternatives, while qualitative comparisons are elicited from the
DM to quantify the degree of preference between any two
elements. This is completed in step 2 (see Section 3.1). Let these
0
comparisons be R ¼ frii0 ji; i ¼ 1; 2; . . . ; mg. A symmetric interdependence matrix for the elements is derived from the R scores.
Using the obtained interdependence matrices, the m elements
are decomposed into k mutually exclusive subsets sl 2 S using
the proposed BIP decomposition methodology (see Section 3.1).
Additionally, in step 4, the pivot element is selected by choosing
the most independent one and assigned to all subsets (see Section 3.2). In step 5, local pairwise comparisons aij are then elicited
for all subsets of S and local weights are calculated (see Section 3.3).
In step 6, global priorities are estimated from the local pairwise
comparisons for all elements (see Section 3.4).
3.1. Decompose PCM into subsets
We start with the elicited m2 inner dependence pairwise comparisons denoted by matrix R as follows:

2

r 11
6
6r
6 12
R¼6
6 ..
4 .

r 21

...

r m1
..
.
..
.

r 22
..
.

...
..
.

r 1m

...

. . . rmm

3
7
7
7
7
7
5

ð3Þ

According to Saaty and Takizawa (1986) if all elements are
independent, then R ¼ Im where Im is an identity matrix of size
m. Otherwise, a score r ij is used to denote the dependence of element i to element j. Since it is unintuitive to partition the elements
using a directed graph, we transform the directed graph into a
symmetric undirected graph as follows:
T

e ¼RþR
R
2

ð4Þ

e with scores ~r ij 2 R
e where
We then obtain a symmetric matrix R
~rij ¼ ~rji . Given this, independent elements are intelligently assigned
into the k subsets using the proposed BIP formulation as follows:

273

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280
Table 1
Summary analysis of existing research.
Papers

Methodology

Limitations

Bozoki et al. (2009), Harker (1987b), Despotis and
Derpanis (2008)
Ishizaka (2012), Saaty (1990), Shen et al. (1992)

Estimate Missing Comparisons Using LP

Cao et al. (2008), Saaty (2003), Benítez et al. (2011b),
Ergu et al. (2011)

Changes Pairwise Comparisons to be
More Consistent

Arbitrary Subsets, Not Scalable, Number of pairwise comparisons
not minimized
Arbitrary Assignment, Alternatives only, Number of pairwise
comparisons not minimized
Large Number of Pairwise Comparisons Elicited, Values differ from
preferences of DM

Group Alternatives into Subsets

Fig. 1. The proposed PCM decomposition methodology.

Let the decision variables be:

have at least 1 element, then the kth subset has m  k elements
assigned.

0

~rii0 :¼ dependence of element i to i

1 if element i is assigned to subset with index l
xil ¼
0 otherwise
(
0
1 if both elements i and i is assigned to l
yii0 l :¼ xil xi0 l
0 otherwise

3.2. Select pivot element

Objective Function:

max

k X
m X
X
~r ii0 yii0 l

ð5Þ

l¼1 i¼1 i0 <i

Constraints:

xil  yii0 l P 0;

i 2 ½1; m; l 2 ½1; k

xi0 l  yii0 l P 0;

i 2 ½1; m; l 2 ½1; k

0

xi0 l þ xil  yii0 l 6 1;

0

i; i 2 ½1; m; l 2 ½1; k

k
X
xil ¼ 1 i 2 ½1; m

ð6Þ
ð7Þ
ð8Þ
ð9Þ

l¼1
m
X

xil P 1 l 2 ½1; k

When the PCM elements are decomposed into k subsets, the
elements are disjoint since there are no pairwise comparisons
across subsets. In order to determine the relative priorities of the
elements across subsets, a pivot element is selected and assigned
to all subsets and is used as a basis for the global weights. To select
the best pivot element, we select the element that has the least
sum of interdependencies ~r ii0 across among elements within the
PCM. Hence we shall choose the element that minimizes the following function:

ð10Þ

Piv ot Element i ¼ argmini

X
~r ii0

!
ð12Þ

i0

Eq. (12) selects the least interdependent element as compared to all
other elements within the PCM, and the selected element is then
assigned to all k subsets.

i¼1

yii0 l 2 BmC2k ;

xi0 l 2 Bmk

0

i; i 2 ½1; m; l 2 ½1; k

ð11Þ

The output of the BIP is a mutually exclusive assignment of the
m elements to subsets S ¼ fsl jl ¼ 1; 2; . . . ; kg. Eq. (5) describes the
objective function of minimizing the inner dependencies of the elements to be assigned in each subset sl . The BIP formulation would
work hard to assign two elements to two different subsets if
~r ii0 > 0. Constraint sets (6)–(8) are constraints that linearize the
quadratic constraint yii0 l :¼ xil xi0 l . Constraint set (9) forces each element to be a member of a subset while constraint set (10) forces all
subsets to have at least one element. Eq. (11) deﬁnes xil as binary
integer variables.
Given the BIP formulation, the following properties can be realized: (1) the minimum number of elements assigned to a given
subset is one and (2) the maximum number of elements assigned
to a given subset is m  k. The minimum number of elements
assigned follows from the BIP formulation; speciﬁcally constraint
set (10) forces the number of elements assigned to subsets to be
at least 1. In terms of the maximum, since the k  1 subsets would

3.3. Elicit local pairwise comparisons and calculate local weights
After decomposition, local pairwise comparisons are elicited
from the DM for all subsets after the elements are assigned to subsets. The local pairwise comparisons for elements subset sl are
illustrated in matrix form Al as shown in Eq. (13) as follows:

2

1

6 a2;1
6
Al ¼ 6
6 ..
4 .
aml ;1

a1;2
1
..
.
aml ;2

   a1;ml

3

   a2;ml 7
7
7
..
. 7;
.
.
. 5

1

8s l 2 S

ð13Þ

A new performance measure is proposed to keep track of the
consistency of the pairwise comparisons. The original deﬁnition
of the CR of matrix A is no longer applicable since the m elements
are assigned into k subsets. With this, a new deﬁnition of consistency is proposed as follows:

274

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

Deﬁnition 1. The Average Consistency Ratio (ACR) performance
measure of an AHP problem decomposed into k subsets is deﬁned
as:

ACR ¼

k
X
1
ml CRðAl Þ
m þ k  1 l¼1

ð14Þ

where CRðAl Þ is the consistency ratio of the pairwise comparison
matrix Al of subset l.
In simple terms, the ACR is the weighted average of the CR of
each of the local pairwise comparison matrices. The ACR is used
to estimate the overall CR of the pairwise comparisons across all
subsets.
Given the assignment of m elements into k subsets and the
addition of the pivot element in all subsets, we deﬁne a quantity
to determine the reduction in the number of pairwise comparisons
elicited from the DM as follows:
Deﬁnition 2. Given the set of the assigned elements into the k
subsets with one pivot element, the total number of required
pairwise comparisons needed to obtain the local priorities of the
elements denoted by dðAk Þ is computed as:

dðAk Þ ¼


k 
X
ml
l¼1

2

¼

k
X
ml ðml  1Þ
l¼1

2

ð15Þ

Remark: The maximum number of elements assigned to k subsets
after including the pivot elements is:
k
X
ml 6 m þ k  1

DðAk Þ ¼

m



2

 dðAk Þ ¼

The total number of pairwise comparisons after decomposition
is dependent on the distribution of the assignment of elements to
the k subsets and as such is shown using Eq. (15). If k ¼ 1, no
decomposition is performed, then all the required mðm  1Þ=2
pairwise comparisons are elicited. On the other hand when
k ¼ m  1, then the pivot element is compared to all other elements with a total of only m  1 pairwise comparisons. The following propositions can be drawn with regards to the number of
pairwise comparisons.
Proposition 1. Given values of k, function dðAk Þ is maximized when
ml ¼ m þ ðk  1Þ for l 2 S and ml0 ¼ 2; 8l 2 S n flg

$
%



kþ2
ðk  1Þðm  1Þ2
k
6 DðA Þ 6
ðk  1Þ m 
2
2k

R Z

Proof. See Appendix. h

~
a
j¼1 1j

6 m
6
..
wðAl Þ ¼ 6
6
4 Pm.l

j¼1

m

Deﬁnition 3. The difference between the original number of
pairwise comparisons of matrix A and number of pairwise comparisons after decomposition into k subsets denoted by DðAk Þ is
given by:

~m j
a
l

3

wð1; lÞ

3

7 6 wð2; lÞ
7 6
7¼6
..
7 6
5 4
.

7
7
7;
7
5

2

l ¼ 1; 2;    ; k

ð19Þ

wðml ; lÞ

3.4. Calculate global weights of the PCM
This subsection describes the methodology to compute global
weights of the elements of the decomposed PCM from the local
subsets using a pivot element. Given values of k there will be
m þ k  1 instances of wði; lÞ. The local element weight in subset l
is divided by the weight of pivot element in that subset and is
~ l Þ be the vector
repeated for all subsets. To illustrate this, let wðA
~ lÞ 2 wðA
~ l Þ is computed using
of normalized weights where each wði;
Eq. (20).

1
½wði; lÞ;
wði ¼ cp ; lÞ

8i 2 sl ; 8sl 2 S

ð20Þ

Given this, the normalized pivot element weight in each subset has
a value equal to one. Since all normalized pivot element weights
have a value equal to one, all the other elements in the other subsets can be compared to the pivot elements. For the computation
of the global weights, let w0 ðAÞ be the vector of global weights
where w0 ðiÞ 2 w0 ðAÞ is computed using Eq. (21).

w0 ðiÞ ¼ Pk P
l¼1

Deﬁnition 3 illustrates the difference between the original total
number of pairwise comparisons mðm  1Þ=2 and the amount of
pairwise comparisons needed, DðAk Þ when the elements are
assigned to subsets.

ð18Þ

Proof. See Appendix. h
Theorem 1 provides the pessimistic and the optimistic estimates of the reduction of the required mðm  1Þ=2 pairwise comparisons of A. This performance metric would be a good yardstick
to determine the amount time saved by the DM when making a
complex decision using the proposed PDM.
After eliciting local pairwise comparisons for all subsets Al we
now deﬁne the local element weights computed for each subset.
Let wðAl Þ be the vector of local weights from Al where
wði; lÞ 2 wðAl Þ is the local weight of element i. The original eigenvector methodology is used to calculate the local element weights
as follows:

~ lÞ ¼
wði;

 ml ¼ mþðk1Þ
for l 2 S if mþðk1Þ
2 Z or
k
j k
k
l
m
0
mþðk1Þ
for some l 2 S and ml0 ¼ mþðk1Þ
for some l if
 ml ¼
k
k

ð17Þ

Theorem 1. Given m elements grouped into k subsets, the total
number of required local pairwise comparison saved is bounded by:

Proof. See Appendix. h
Proposition 2. Given values of k, function dðAk Þ is minimized when:


k 
ml
mðm  1Þ X

2
2
l¼1

Given Propositions 1 and 2, the amount of time saved by the DM
in terms of the reduction of the number of pairwise comparisons
can be generalized in terms of Theorem 1.

2 Pm l
ð16Þ

l¼1

mþðk1Þ
k



1
~ lÞ;
wði;
~
wði;
lÞ  k þ 1
i2sl

8i 2 ½1; m

ð21Þ

4. Decomposing PCMs for the AHP methodology
This section illustrates the application of the PDM on an AHP
decision. The AHP assumes that the criteria and alternatives are
independent and hence, we seek an alternative way to measure
the interdependence of the elements. Speciﬁcally, the correlation
of the alternative scores rij is used to estimate the inner dependencies of the criteria. The correlation of the alternative scores could

275

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

act as an alternative to estimate inner dependencies of the criteria.
The alternative scores rij represent the raw rating score of alternative j on criterion i. We formally deﬁne the correlation denoted by
Rðci ; ci0 Þ as follows:
Deﬁnition 4. Let Rðci ; ci0 Þ be the correlation between criterion ci
and criterion ci0 which is calculated using (22) as follows





Pn



0
0


j¼1 ðr ij  r ij Þðr i j  r i j Þ
0
Rðci ; ci Þ ¼ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

P
P

n
2
n
2

0
0

j¼1 ðr ij  r ij Þ
j¼1 ðr i j  r i j Þ 

ð22Þ

These correlation coefﬁcients will be used in the proposed BIP
to group uncorrelated criteria. A value Rðci ; ci0 Þ ¼ 1 means that
the criterion ci is positively or negatively correlated to criterion
ci0 . Hence, the PDM can be applied to decompose the lowest level
criteria PCM, directly above the alternatives within an AHP decision problem. Otherwise, we decompose all other AHP PCMs arbitrarily as is done in existing literature. Fig. 2 illustrates the
proposed PDM as applied to a 3-level AHP problem.
In step 1, the DM decides on the m criteria and n alternatives
and the three-level decision hierarchy (see Section 4.1). Step 2 is
the alternative scores pairwise comparison elicitation stage and
the corresponding correlation matrix is computed (see Section 4.2).
The PDM is applied to decompose the criteria PCM and the global
weights of the m criteria are calculated in step 3 (see Section 4.3).
Lastly, step 4 computes the weighted scores of the n alternatives for
decision making (see Section 4.4). A peer reviewed AHP dataset
from existing literature is used to illustrate the application of the
PDM for AHP. Önüt, Efendigil, and Kara (2010) use a fuzzy AHP
model for shopping center site selection and is illustrated in this
subsection.

Fig. 3. AHP Hierarchy structure for the dataset of Önüt et al. (2010).

Table 2
Raw data of the scores of each alternative on each criterion
Alternative

Criterion

A
B
C
D
E
F

1

2

3

4

5

6

7

8

5
7
5
5
7
5

7
7
5
5
9
5

5
5
7
8
5
7

9
7
5
3
5
3

5
7
5
5
7
5

5
7
5
5
7
5

3
5
9
9
3
9

3
5
7
5
3
5

Table 3
Correlation matrix computed from Table 2.

4.1. Initialize AHP hierarchy

Criterion

1

2

3

4

5

6

7

8

The m criteria and n alternatives are identiﬁed and arranged
into a three-level decision hierarchy. In terms of the dataset from
Önüt et al. (2010), the goal, eight criteria and six alternatives are
setup as a hierarchy as is done in the traditional AHP methodology.
Fig. 3 illustrates the proposed three-level AHP hierarchy structure.

1
2
3
4
5
6
7
8

1.000
0.791
0.680
0.221
1.000
1.000
0.600
0.343

0.791
1.000
0.860
0.489
0.791
0.791
0.922
0.759

0.680
0.860
1.000
0.794
0.680
0.680
0.933
0.633

0.221
0.489
0.794
1.000
0.221
0.221
0.758
0.417

1.000
0.791
0.680
0.221
1.000
1.000
0.600
0.343

1.000
0.791
0.680
0.221
1.000
1.000
0.600
0.343

0.600
0.922
0.933
0.758
0.600
0.600
1.000
0.824

0.343
0.759
0.633
0.417
0.343
0.343
0.824
1.000

4.2. Compute correlations of criteria
Table 2 presents the most likely scores of the six site alternatives over the eight selection criteria. Additionally, the corresponding 8  8 correlation matrix is computed and is presented in
Table 3.

criteria. Speciﬁcally, the criteria PCM is decomposed into two subsets (k ¼ 2), then a pivot criterion is selected. Furthermore, the
local pairwise comparisons are collected for each subset and the
corresponding global criteria PCM weights are calculated.

4.3. Apply PCM to the criteria PCM
This subsection illustrates the decomposition of the criteria
PCM and the calculation of the global weights of the eight decision

4.3.1. Decompose criteria PCM
The proposed BIP methodology from Section 3.1 is applied to
the criteria PCM using the correlation scores from Table 3. After

Fig. 2. Application of the PDM for a 3-level AHP problem.

276

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

executing the proposed BIP methodology, criteria 1, 4, 6 and 7 are
assigned to subset s1 while criteria 2, 3, 5 and 8 are assigned to
subset s2 .

Table 5
PCM for subset 1.

1
4
6
7

4.3.2. Select pivot element
The optimal pivot criterion is selected by applying Eq. (12) and
Table 4 presents the results of the sum of the individual correlations of criterion i to all other criteria. It is evident from Table 4
that criterion 4 is the least independent criterion. And thus criterion 4 is assigned to all subsets. Hence, subsets s1 and s2 now have
1, 4, 6 and 7 and 2, 3, 4, 5 and 8 criteria respectively.

k
X
1
4
5
ml CRðAl Þ ¼ 0:0547 þ 0:119 ¼ 0:0904
m þ k  1 l¼1
9
9

4

6

7

Local priority (%)

1.00
3.00
7.00
0.67

0.33
1.00
1.00
0.33

0.14
1.00
1.00
0.20

1.50
3.00
5.00
1.00

10.31
33.65
46.99
9.05

Table 6
PCM for subset 2.

4.3.3. Elicit local pairwise comparisons
To illustrate the eliciting of local pairwise comparisons, the original most likely fuzzy values are used from the original 8  8
fuzzy AHP criteria PCM. Tables 5 and 6 summarize the local pairwise comparison matrices for the two subsets respectively.
A total of 4C2 þ 5C2 ¼ 16 pairwise comparisons are elicited in
this setup, which is a reduction of DðAk Þ ¼ 12, as compared to
the original 28 required pairwise comparisons when the original
AHP methodology is used. The average consistency of these two
priority matrices is computed using Eq. (14) as follows:

ACR ¼

1

2
3
4
5
8

2

3

4

5

8

Local priority (%)

1.00
3.00
5.00
3.00
1.00

0.33
1.00
0.33
3.00
0.33

0.20
3.00
1.00
1.00
0.33

0.33
0.33
1.00
1.00
0.20

1.00
3.00
3.00
5.00
1.00

7.43
26.39
24.24
34.94
7.00

Table 7
Results of the PDM as compared to the original AHP criteria PCM.

ð23Þ

The original 8  8 AHP matrix has a CR of 12.96% which is highly
inconsistent while the decomposed matrix has an ACR of only
9.04%. Hence, in this setup, several inconsistent pairwise comparisons are excluded from the decision making. Furthermore the subset PCMs have a smaller dimensions (dimðs1 Þ ¼ 4; dimðs2 Þ ¼ 5) thus
making the elicitation of pairwise comparisons less taxing for the
DM.

Criterion

Original criteria PCM weights

PDM weights

5
6
4
3
1
2
8
7

26.09%
21.95%
15.69%
15.36%
7.52%
5.24%
4.46%
3.69%

23.64%
22.90%
17.86%
16.40%
5.027%
5.025%
4.736%
4.409%

DðA2 Þ
ACR

None

12

12.96%

9.63%

from the alternatives. Hence, the application of the PDM for AHP
problems is limited. Therefore, we illustrate the full potential of
the PDM in terms of an ANP network in Section 5.

4.3.4. Calculate global weights of the criteria PCM
After calculating the priorities of the local criteria, the corresponding global priorities of the criteria need to be calculated.
Using the local weights from subset 1 and subset 2 and Eq. (20),
all local weights are divided by the local weight of pivot criterion
4. And as such, we obtain normalized weights for subset
~ 1 Þ ¼ ½0:31; 1:00; 1:40; 0:27T and subset 2: wðA
~ 2Þ ¼
1: wðA
½0:31; 1:00; 1:00; 1:44; 0:29T . Using the normalized weights and
using Eq. (21) we sum the normalized weights and we obtain the
following global weights as summarized in Table 7.
Based on the results, the weights obtained from the proposed
PDM is relatively similar to the value of the original weights with
a lower average consistency ratio. We observe from Table 7 that we
have saved 12 pairwise comparisons for the criteria PCM while
having similar weights. This savings is attributed to the decomposition of the PCM into two subsets in which the pairwise comparisons of the criteria across subsets are not elicited. Furthermore, a
reduction of the CR is observed from 12.96% to 9.63%. In this setup,
lots of inconsistent pairwise comparisons are omitted. Furthermore, the PCMs for the subsets are smaller well below Saaty’s
threshold of seven elements.
However, the PDM can only be applied to the special case of a
three-level AHP problem. Furthermore, by deﬁnition of the correlation, we only measure the linear interdependencies of the criteria

4.4. Compute weights of alternatives
After computing for the global weights of the eight criteria, we
now compute the weights of the six alternatives. Table 8 presents
the results of the weighted scores by multiplying the scores from
Table 2 with the obtained global weights from Table 7. Hence,
alternative B is selected since it has the highest weighted score followed by alternative E, then A, C, D and E.

5. Decomposing PCMs for the ANP methodology
This section illustrates the application of the PDM on an ANP
problem. Since all PCMs that form clusters within the ANP network
require inner dependencies to measure the interdependence of elements, these inner dependencies are then used to decompose all
PCMs within the network. Hence, all PCMs in the network can be
decomposed using the PDM which leads to larger pairwise comparison savings. We illustrate the decomposition of the eight decision criteria into three subsets using Fig. 4 as follows.

Table 4
Sum of correlation.
Criterion

Correlation

1

2

3

4

5

6

7

8

5.6346

6.402

6.2592

4.1196

5.6346

5.6346

6.2365

4.6615

277

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280
Table 8
Weighted scores of the six alternatives.
Alternative

Weighted score

A
B
C
D
E
F

5.6319
6.4889
5.5989
5.3110
6.0493
5.1470

In step 1, the ANP network structure is developed with the n
alternatives and m criteria (see Section 5.1). In Step 2, the inner
dependencies of all the elements of each PCM are elicited (see
Section 5.2). Furthermore, we apply the PDM to all PCMs of the
network in step 3 (see Section 5.3). Finally, step 4, we calculate
the limiting weights of the alternatives for decision making (see
Section 5.4). A dataset from existing literature is used to demonstrate the application of the PDM for the ANP. Cheng and Li
(2004) propose an ANP methodology for contractor selection. This
dataset is selected since the eight decision criteria are considered
simultaneously in a single cluster. Furthermore, the decision
focuses on three alternatives.

Fig. 5. The ANP network structure for the dataset of Cheng and Li (2004).

Table 9
Inner dependency scores for the eight criteria.

5.1. Initialize ANP Network
The goal, eight criteria and four alternatives are setup as a
network as is done in the traditional ANP methodology. Fig. 5
illustrates the proposed three-cluster ANP network structure.
5.2. Elicit inner dependence
Inner dependencies are elicited from the DM to estimate the
interdependencies of the criteria. Table 9 summarizes the elicited
inner dependency scores of all eight decision criteria which are
elicited from the DM from Cheng and Li (2004). We then compute
the corresponding symmetric matrix using Eq. (4). Table 10
presents the symmetric matrix.
5.3. Apply PDM to all PCMs
This subsection illustrates the application of the PDM to decompose all PCMs. In our example, we illustrate the decomposition of
the criteria cluster into three subsets. Furthermore, the optimal
pivot criterion is selected and is used to link all three subsets.
The local pairwise comparisons are then elicited and local weights
are calculated. The corresponding global criteria PCM weights are
computed from the local weights.
5.3.1. Decompose criteria PCM
The proposed BIP methodology is applied from Section 3.1 on
the symmetric inner dependency scores from Table 10. Hence,

Criterion

1

2

3

4

5

6

7

8

1
2
3
4
5
6
7
8

0.00
0.14
0.20
0.11
0.20
0.11
0.11
0.11

0.56
0.00
0.06
0.06
0.06
0.06
0.06
0.12

0.33
0.37
0.00
0.06
0.06
0.06
0.06
0.06

0.31
0.32
0.20
0.00
0.04
0.04
0.06
0.03

0.21
0.24
0.21
0.18
0.00
0.09
0.04
0.04

0.20
0.26
0.18
0.18
0.07
0.00
0.03
0.07

0.17
0.17
0.17
0.09
0.17
0.17
0.00
0.04

0.18
0.14
0.17
0.16
0.23
0.03
0.10
0.00

Table 10
Symmetric inner dependency scores for the eight criteria.
Criterion

1

2

3

4

5

6

7

8

1
2
3
4
5
6
7
8

0.00
0.35
0.27
0.21
0.21
0.16
0.14
0.15

0.35
0.00
0.22
0.19
0.15
0.16
0.12
0.13

0.27
0.22
0.00
0.13
0.14
0.12
0.12
0.12

0.21
0.19
0.13
0.00
0.11
0.11
0.08
0.10

0.21
0.15
0.14
0.11
0.00
0.08
0.11
0.14

0.16
0.16
0.12
0.11
0.08
0.00
0.10
0.05

0.14
0.12
0.12
0.08
0.11
0.10
0.00
0.07

0.15
0.13
0.12
0.10
0.14
0.05
0.07
0.00

criteria 2, 5, and 7 are assigned to subset s1 while criteria 1, 6
and 8 are assigned to subset s2 and lastly criteria 3 and 4 are
assigned to subset s3 .

Fig. 4. Application of the PDM for an ANP problem.

278

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

5.3.2. Select pivot element
The optimal pivot criterion is selected by applying Eq. (12) and
Table 11 presents the results of the sum of the individual inner
dependencies of criterion i to all other criteria.
It is evident from Table 11 that criterion 7 is the least independent criterion. And thus criterion 7 is assigned to all subsets.
Hence, subset s1 has criteria 2, 5 and 7, subset s2 has criteria 1, 6,
7 and 8 and subset s3 has 3, 4 and 7.
5.3.3. Elicit local pairwise comparisons
Local pairwise comparisons are elicited for all subsets of the criteria cluster. Tables 12–14 show the local priorities of the decomposed AHP obtained from the dataset of Cheng and Li (2004).
A total of 3C2 þ 4C2 þ 3C2 ¼ 10 pairwise comparisons are elicited in this setup. A reduction of DðAk Þ ¼ 18 pairwise comparisons
is realized from the original 28 required pairwise comparisons. The
average consistency of these three PCMs is computed using Eq.
(14) as follows:

ACR ¼
¼

1
mþk1

k
X
ml CRðAl Þ

Table 12
PCM for subset 1.

2
5
7

2

5

7

Local priority (%)

1
1/6
1/7

6
1
1

7
1
1

76.38
12.11
11.51

Table 13
PCM for subset 2.

1
6
7
8

1

6

7

8

Local priority (%)

1
1/9
1/9
1/9

9
1
2
2

9
1/2
1
2

9
1/2
1/2
1

73.28
6.01
8.79
11.93

Table 14
PCM for Subset 3.
3

4

7

Local priority (%)

1
1/3
1/3

3
1
1/2

3
2
1

58.89
25.19
15.93

l¼1

3
4
3
0:0042 þ
0:0847 þ
0:0607 ¼ 0:05335
10
10
10

ð24Þ

The original 8  8 ANP cluster has a CR of 9.44% which is around the
threshold of 10% while the decomposed matrix has an ACR of only
5.335%. Again we observe a reduction of the inconsistency of the
PCMs. Several inconsistent pairwise comparisons are not elicited
in this setup and the dimensions of the three subsets are considerably less than the original criteria PCM.
5.3.4. Calculate global weights of the criteria PCM
Again using the local weights from subset 1, 2 and 3 and
Eq. (20), all local weights are divided by the local weight of pivot
criterion 7. And as such we obtain normalized weights for subset
~ 1 Þ¼ ½6:64;1:00;1:00T , subset 2: wðA
~ 2 Þ¼ ½8:64;0:68;1:00;1:36T
1: wðA
~
and subset 3: wðA3 Þ ¼ ½3:70;1:58;1:00T Using the normalized
weights and using Eq. (21) we sum the normalized weights and
we obtain the following global weights as summarized in Table 15.
The weights obtained from the proposed PDM is relatively similar to the value of the original weights with a lower average consistency ratio. Additionally, we observe from Table 15 that a
reduction of 18 pairwise comparisons is realized for the criteria
PCM while having similar weights. This savings is again attributed
to the decomposition of the criteria PCM into three subsets in
which the pairwise comparisons of the criteria across subsets are
not elicited. Furthermore, a reduction of the CR is observed from
9.44% to 5.34%. In this setup, lots of inconsistent pairwise comparisons are omitted. Furthermore, the three subset PCMs are smaller
which are within Saaty’s threshold of seven elements.
We further test the PDM by changing the number of subsets.
The same dataset from Cheng and Li (2004) is used in terms of
the decomposition of the 8 criteria into k 2 ½2; 7 subsets. The traditional ANP is used and the results are presented in Table 16.
It is observed from Table 16 that as the value of k increases, we
generally observe an increase in the number of pairwise comparisons saved. This is attributed to smaller subset PCMs when we
increase the number of subsets. Furthermore, we observe a gradual

3
4
7

Table 15
Results of the PDM as compared to the original ANP criteria PCM.
Criterion

Original criteria PCM weights

PDM weights

1
2
3
4
8
5
7
6

33.9%
27.3%
14.8%
9.1%
3.4%
3.2%
3.8%
4.5%

33.3%
27.3%
15.2%
6.5%
5.6%
4.3%
4.1%
2.8%

DðA2 Þ
ACR

None

18

9.44%

5.34%

Table 16
Performance of the PDM.
Metric

Methodology
No Decomposition

k

DðA Þ
ACR (%)

Proposed PDM
k¼2

k¼3

k¼4

k¼5

k¼6

k¼7

0

12

16

18

19

20

21

9.44

8.99

5.34

5.24

5.17

4.82

0.00

Table 17
Weighted scores of the three alternatives.
Alternative

Weighted score

A
B
C

0.47
0.27
0.26

Table 11
Sum of inner dependencies.
Criterion

Sum of Inner dependence

1

2

3

4

5

6

7

8

1.47

1.31

1.095

0.92

0.92

0.775

0.72

0.74

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

decrease on the average CR of the PCMs. However, lots of pairwise
comparisons are not elicited for values of k P 4 hence, the redundancy advantage of PCMs is reduced and thus the preferences of
the DM would be subject to more biases and judgement errors. A
trade-off mechanism must be developed to address this which is
the subject of our future research.

After computing the weights of the criteria cluster for the eight
decision criteria, we now calculate the weights of the three alternatives using the traditional ANP methodology. Table 17 presents
the results of the three alternatives. Hence, alternative A is selected
since it has the highest limiting score followed by alternative B
then alternative C.
6. Conclusions and future work
A Pairwise Comparison Matrix (PCM) is an integral component
of decision making methodologies: Analytic Hierarchy Process
(AHP) and Analytic Network Process (ANP). These PCMs are used
to determine relative weights of criteria and alternatives. However,
a PCM suffers from the curse of dimensionality and hence the issue
of inconsistent pairwise comparisons when elicited from a decision
maker (DM). The proposed PCM Decomposition Methodology
(PDM) addresses these disadvantages. The PDM decomposes a
PCM into smaller manageable subsets using binary integer programming. As a result, the number and the inconsistency of pairwise comparisons elicited are reduced. Since the subsets are
disjoint, the most independent pivot element is selected to connect
all disjoint subsets. Hence the inner dependencies of the elements
are minimized within each subset. Using local priorities and the
pivot element, global priorities are then estimated for the elements
of the PCM.
The PDM is applied to a three-level AHP problem to decompose
the criteria PCM. Correlation of the criteria from alternative scores
is used as an alternative to estimate the interdependencies of the
criteria. The proposed methodology does indeed reduce the number of pairwise comparisons and the consistency ratio. Nevertheless, more pairwise comparisons is saved when the PDM is
applied to the ANP methodology. The PDM can be applied to all
cluster PCMs within the network since inner dependencies of the
elements are elicited for each PCM.
The authors plan to extend the framework by determining the
optimal number of subsets k for each ANP cluster by balancing
the (1) time savings by reducing pairwise comparisons, (2) the
amount of inner dependency among criteria and alternatives (3)
the level of consistency, and (4) the accuracy of the global weights.
Furthermore, multiple pivot elements are to be studied to further
improve the estimation of the global weights.
Appendix A. Proof of propositions
A.1. Proof of Proposition 1
Maximizing the value of dðAk Þ is determined by
P
Pk
max dðAk Þ ¼ kl¼1 ml ðm2l 1Þ subject to
l¼1 ml 6 m þ ðk  1Þ; l 2 ½1; k;
ml P 2; l 2 ½1; k and ml 2 Zk ; l 2 ½1; k. It is obvious that the integer
program tends to assign all elements to a single subset while minimizing the assignment to other subsets. Therefore, ml P 2 would
be a binding lower bound. And thus, the solution is
0

ml ¼ m þ ðk  1Þ for l 2 S and ml0 ¼ 2; 8l 2 S n flg maximizes dðAk Þ
where:

ðm  k þ 1Þðm  kÞ
þk1
2

A.2. Proof of Proposition 2
The proof for Proposition 2 is similar to the proof of Proposition
1 but the IP problem is set to minimize and the other extreme
point is obtained. However, by solving the IP problem, if
mþðk1Þ
k

2 Z, then the solution of the IP problem would be equal to

the LP relaxation problem where ml ¼ mþðk1Þ
in which all elements
k
are equally distributed among the k subsets. However, when

5.4. Calculate weights of alternatives

dðAk Þ ¼

279

ðA:1Þ

mþðk1Þ
R Z, then the solution of the LP relation of the IP problem
k
is different and thus the solution would require round off values
j
k
l
m
0
for some l 2 S and ml0 ¼ mþðk1Þ
for all l 2 S n flg.
of ml ¼ mþðk1Þ
k
k

A.3. Proof of Theorem 1
Part 1 (Lower Limit): Given that there are mðm  1Þ=2 required
pairwise comparisons for an original PCM, the minimum number
of pairwise comparisons reduced is bounded by:

DðAk Þ P

mðm  1Þ
 max dðAk Þ
2

ðA:2Þ

Based on Proposition 1, by substituting the values ml ¼ m þ ðk  1Þ
0
for l 2 S and ml0 ¼ 2; 8l 2 S n flg that maximizes dðAk Þ, we have:

DðAk Þ P

	


mðm  1Þ
ðm  k þ 1Þðm  kÞ

þk1
2
2

ðA:3Þ

Simplifying Eq. (A.3) we have:




kþ2
DðAk Þ P ðk  1Þ m 
2

ðA:4Þ

Part 2 (Upper Limit): Given that there are mðm  1Þ=2 required
pairwise comparisons for the original PCM, the maximum number
of pairwise comparisons reduced is bounded by:

DðAk Þ P

mðm  1Þ
 min dðAk Þ
2

ðA:5Þ

Based on Proposition 1, by substituting the values ml ¼ mþðk1Þ
if
k
2 Z, we have:

mþðk1Þ
k

DðAk Þ 6

mðm  1Þ k

2
2

	



m þ ðk  1Þ m þ ðk  1Þ
1
k
k

ðA:6Þ

Simplifying Eq. (A.6) we have:

$
DðAk Þ 6

ðk  1Þðm  1Þ2
2k

%
ðA:7Þ

References
Benítez, J., Delgado-Galván, X., Gutiérrez, J., & Izquierdo, J. (2011a). Balancing
consistency and expert judgment in AHP. Mathematical and Computer Modelling,
54, 1785–1790.
Benítez, J., Delgado-Galván, X., Izquierdo, J., & Pérez-García, R. (2011b). Achieving
matrix consistency in AHP through linearization. Applied Mathematical
Modelling.
Bozoki, S., Fulop, J., & Ronyai, L. (2009). Incomplete pairwise comparison matrices in
multi-attribute decision making. In IEEE international conference on industrial
engineering and engineering management, 2009. IEEM 2009 (pp. 2256–2260).
IEEE.
Cao, D., Leung, L., & Law, J. (2008). Modifying inconsistent comparison matrix in
analytic hierarchy process: A heuristic approach. Decision Support Systems, 44,
944–953.
Carmone, F. J., Kara, A., & Zanakis, S. H. (1997). A monte carlo investigation of
incomplete pairwise comparison matrices in AHP. European Journal of
Operational Research, 102, 538–553.
Cheng, E. W. L., & Li, H. (2004). Contractor selection using the analytic network
process. Construction Management and Economics, 22, 1021–1032.
Despotis, D. K., & Derpanis, D. (2008). A min–max goal programming approach to
priority derivation in ahp with interval judgements. International Journal of
Information Technology and Decision Making, 7, 175–182.

280

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

Ergu, D., Kou, G., Peng, Y., & Shi, Y. (2011). A simple method to improve the
consistency ratio of the pair-wise comparison matrix in ANP. European Journal
of Operational Research, 213, 246–259.
Harker, P. T. (1987a). Alternative modes of questioning in the analytic hierarchy
process. Mathematical Modelling, 9, 353–360.
Harker, P. T. (1987b). Shortening the comparison process in the AHP. Mathematical
Modelling, 8, 139–141.
Ishizaka, A. (2008). A multicriteria approach with AHP and clusters for supplier
selection. In 15th international annual EurOMA conference. Groningen.
Ishizaka, A. (2012). Clusters and pivots for evaluating a large number of alternatives
in AHP. Brazilian Operations Research Society, 31.
Ishizaka, A., & Labib, A. (2011). Review of the main developments of the analytic
hierarchy process. Expert Systems with Applications, 38, 14336–14345.
Islam, R., & Abdullah, N. A. (2006). Management decision-making by the analytic
hierarchy process: A proposed modiﬁcation for large-scale problems. Journal for
International Business and Entrepreneurship Development, 3, 18–40.
Lin, C. C., Wang, W. C., & Yu, W. D. (2008). Improving AHP for construction with an
adaptive AHP approach (A3). Automation in Construction, 17, 180–187.
Önüt, S., Efendigil, T., & Kara, S. S. (2010). A combined fuzzy MCDM approach for
selecting shopping center site: An example from istanbul, turkey. Expert Systems
with Applications, 37, 1973–1980.
Saaty, T. L. (1977). A scaling method for priorities in hierarchical structures. Journal
of Mathematical Psychology, 15, 234–281.

Saaty, T. L. (1990). How to make a decision: The analytic hierarchy process.
European Journal of Operational Research, 48, 9–26.
Saaty, T. L. (2003). Decision-making with the AHP: Why is the principal eigenvector
necessary. European Journal of Operational Research, 145, 85–91.
Saaty, T. L., & Takizawa, M. (1986). Dependence and independence: From linear
hierarchies to nonlinear networks. European Journal of Operational Research, 26,
229–237.
Shen, Y., Hoerl, A., & McConnell, W. (1992). An incomplete design in the analytic
hierarchy process. Mathematical and Computer Modelling, 16, 121–129.
Simon, H. A. (1972). Theories of bounded rationality. Decision and Organization, 1,
161–176.
Takahashi, I. (1990). Analysis of AHP by BIBD. Journal of the Operations Research
Society of Japan, 33, 12–21.
Triantaphyllou, E. (1995). Linear programming based decomposition approach in
evaluating priorities from pairwise comparisons and error analysis. Journal of
Optimization Theory and Applications, 84, 207–234.
Triantaphyllou, E. (2000). Multi-criteria decision making methods: A comparative
study (vol. 44). Springer.
Weiss, E. N., & Rao, V. R. (1987). AHP design issues for large-scale systems. Decision
Sciences, 18, 43–57.
Zhang, G., & Chen, Y. (2009). A new method for improving the consistency of
the comparison matrix in AHP. Mathematics in Practice and Theory, 23,
140–146.

Proceedings of the 3rd Annual
IEEE Conference on Automation Science and Engineering
Scottsdale, AZ, USA, Sept 22-25, 2007

SuRP-A03.6

Decision Paradigms in the Semiconductor Supply Chain: A Survey
and Analysis
Yang Sun, Andrew Feller, Dan Shunk, John Fowler, Member, IEEE, Thomas Callarman, and Brett Duarte

Abstract— An online questionnaire was used to survey the
current state of decision making paradigms for the
semiconductor supply chain and to identify future trends and
needs. A logistic regression analysis on the survey data clearly
shows that decision makers tend to use optimization techniques
in the front-end planning decisions, and to make them in a slow
clockspeed fashion. As products move downstream in the
supply chain, it is more likely that decision makers will use
their tacit knowledge to make back-end configuration and
allocation decisions quickly and frequently. Survey results also
showed that executives want optimization methods to be used
across the supply chain, creating a need to develop heuristicbased methods that will set the stage for optimizing final
allocation decisions.

back-end allocation decisions. Future research is
recommended to investigate the effectiveness of using
heuristic-based techniques in configuring and shipping
finished goods to customers and to set the stage for
optimizing the final allocation decisions.
The rest of the paper is organized under the following
headings: (II) Crucial Decisions in the Semiconductor
Supply Chain; (III) Research Methodology; (IV) Survey
Results and Analysis; and (V) Conclusion, Discussion, and
Future Research.

I. INTRODUCTION

While making a semiconductor integrated-circuit chip is
one of the world’s most complex processes, the
manufacturing-centered semiconductor supply chain can be
roughly divided into three major stages: 1) wafer fabrication
(W/F), which also includes the probe (initial test) process, 2)
assembly and test (A/T), and 3) configuration and shipment
(C/S). Global logistics are involved since facilities and
customers often have different geographic locations. The
W/F stage, in which integrated-circuit dies are produced on
silicon wafers and initially tested, is often referred to as
“front-end” manufacturing. In the A/T stage, the “backend”, dies are sawed out of the wafers and put into a
package; packaged semiconductor devices are then fully
tested. Functional semiconductor devices are configured and
delivered to the customers in the C/S stage. See Sun et al.
(2007a) for details on semiconductor supply chain flows.
In each of these three stages a number of decisions critical
to achieving operational excellence are made. In the W/F
stage, the expected or forecasted demand drives the
replenishment of raw semiconductor wafer inventory and
the release of semiconductor wafer lots into the wafer
fabrication facility. Based on these forecasts, an appropriate
mix and quantity of semiconductor wafer lots are processed
and sent into die-bank inventory. The goal of this stage is, in
essence, to build the right stock. In the A/T stage, decisions
are made by a cross-functional team to determine how dies
in the die-bank inventory will be allocated to planned or
anticipated customer orders. At this point dies are committed
to a specific package and are assembled and tested to make
the final product. In this stage, fabricated wafer lots are
loosely allocated to anticipated incoming orders or shortterm forecasts in a rough-cut fashion. In the C/S stage the
final allocation decisions are made to assign finished goods
to firm orders and finally determine “who gets what”.

F

AST clockspeed high-technology industries, such as
semiconductors and computers, are facing a new
challenge. The dilemma executives have is that markets
have become more competitive while the industry itself has
become more commoditized (Cullen 2004). Increasing
customer expectations call for customer-driven supply chain
management that involves planning and managing the flow
of materials, information, knowledge, and cash through
multiple stages of manufacturing, transportation, and
distribution to ensure that the right products can reach
customers effectively and efficiently (Chong et al. 2004).
Tools and methodologies that can help make good decisions
across the supply chain are of critical importance (Chwif et
al. 2002). In this research, industrial experts from leading
semiconductor and electronics firms are surveyed using an
online questionnaire to determine how supply chain
decisions are made today and to explore what needs to be
done for tomorrow.
A logistical regression analysis on the survey results
shows that while most firms utilize optimization techniques
for front-end planning, they mainly use their experts’ tacit
knowledge to make back-end allocation decisions. The
survey also shows that top management wants optimization
throughout the entire supply chain. This raises the question
of whether or not it is worthwhile to use optimization in
Yang Sun, Andrew Feller, Dan Shunk, John Fowler, and Brett Duarte are
with the Department of Industrial Engineering at Arizona State University,
PO Box 875906, Tempe, AZ 85287-5906, USA (phone: 480-965-3185;
fax: 480-965-8692; e-mails: Yang.Sun@asu.edu, Andrew.Feller@asu.edu,
Dan.Shunk@asu.edu, John.Fowler@asu.edu, Brett.Duarte@asu.edu).
Thomas Callarman is with China Europe International Business School
(CEIBS), 699 Hongfeng Road, Pudong, Shanghai 201206, P.R.China.
tecallarman@ceibs.edu.

1-4244-1154-8/07/$25.00 ©2007 IEEE.

II. CRUCIAL DECISIONS IN THE SEMICONDUCTOR SUPPLY
CHAIN

106

SuRP-A03.6

Finished products are then configured per customer
requirements and shipped to the customers’ site.
III. RESEARCH METHODOLOGY
Our online questionnaire surveys the structural reasoning
paradigms – dynamic business rules – applied to make the
crucial decisions outlined in section II. Dynamic business
rules are the explicit and implicit policies used to guide the
decision process. The questionnaire was designed to explore
the following three dimensions of the dynamic business
rules: 1) decision technique, 2) timing and frequency, and 3)
degree of flexibility (allowance for making changes in
decision parameters).
The questionnaire is designed orthogonally, in the
language of statistical experimental design. A set of
decisions-to-make are listed for each of the supply chain
stages (W/F, A/T, or C/S). These questions are highly
correlated across the decision sets. For each decision-tomake, the survey participant needs to determine the three
dimensions of the business rules (i.e., technique,
timing/frequency, and flexibility). A few open-ended
questions give participants the opportunity to comment on
how decision paradigms differ as the viewpoint of the
decision maker changes. The detailed questionnaire is
available online at http://enpub.fulton.asu.edu/supplynetwork/ .
Decision Set #1 addresses the wafer launch decisions at
the W/F stage, i.e., the quantity and type of wafers released
into the wafer fabrication facility to meet the expected
demand. These decisions are:
z (Inventory) How much raw material inventory to
hold?
z (Release) What category/family of products to
release? How many wafers to release? (This builds the right
stock.)
z (Priority) What priorities are assigned to wafer lots
in the process?
z (Logistics) Which assembly and test facility to ship
to?
Decision Set #2 covers the rough-cut, loose lot allocation
decisions at the A/T stage. Decisions in this set are made by
a cross-functional team dealing with planned/anticipated
customer orders. Semiconductor dies from the die-bank
inventory are committed to specific packages and are
assembled and tested to make the final products. These
decisions are:
z (Inventory) How much die inventory should be held?
How
much
packaging
material
inventory
should be held?
z (Priority) What priorities are assigned to jobs?
z (Allocation) What portions of in-process inventory
are committed to anticipated orders? (i.e., rough-cut
allocation.)
Decision Set #3 considers the final allocation decisions
made to configure and allocate finished goods to firm orders
in the C/S stage. Final products are then labeled and shipped
to the customers to determine “who gets what”. These
decisions are:

z (Inventory) How much finished goods inventory
should be held?
z (Priority) What is the final prioritization of firm
customer orders?
z (Allocation) What is the quantity and mix of final
products assigned to firm customer orders? (i.e., “who gets
what?”)
z (Logistics) What quantity and mix of products are
shipped from which assembly and test facility to
which customer?
Definitions of the three dynamic business rule dimensions
are as follows:
1) Decision Technique:
z Optimization: maximizing or minimizing an
objective function using various searching algorithms.
Besides algorithms for solving mathematical
programming problems (e.g., simplex method,
interior-point methods, branch-and-bound methods,
etc.), meta-heuristics such as generic algorithms,
simulated annealing, and tabu search are also
considered searching techniques in an optimization
context.
z Heuristics: intuitive or rule based algorithms.
As an example, the Weighted Shortest Processing
Time (WSPT) rule is a commonly used heuristic for
making scheduling decisions. (It happens to be
optimal for a single machine scheduling problem with
the objective of minimizing weighted flow time. See
Pinedo 2002, Theorem 3.1.1.) In the language of
algorithms, heuristics in this category are constructive
heuristics, while improving heuristics belong to the
category of optimization.
z Tacit Knowledge: using the experience of
experts in the organization to make a decision. There
still can be “rules” for decision making; however,
these rules are generally uncodified (while codified
rules should be put in the heuristics category).
2) Decision Timing and frequency:
z How often the decisions are made;
z How long it takes to make the decisions.
3) Degree of Flexibility (DoF):
z Can the quantity be changed? If so, how much
can it be changed?
z Can the time of order release or delivery be
changed? If so, by how much can it be changed?
z Can the type of products released or delivered
be changed? If so, which products can be used?
z Can the price of the products be adjusted? If so,
by how much can they be adjusted?
For each specific decision listed above, the following
questions are asked. This makes the design of the survey
“orthogonal”.
z What kind of technique is used? (Choose from
Optimization (Opt), Heuristics (Heu), and Tacit
Knowledge (Tac).)
z How often is the decision made? (Choose from
Hourly (H), Daily (D), Weekly (W), and Monthly
(Mo).)

107

SuRP-A03.6

z How long does it take to make the decision?
(Choose from Minutes (Mi), Hours (Hr), Days (Dy)
and Weeks (Wk).)
z What DoF is allowed in altering the decision
made? (1 through 5, where 1 represents low DoF, 3
represents medium DoF, and 5 represents high DoF)
 Product Type
1 2 3 4 5
 Time of Action
1 2 3 4 5
 Quantity
1 2 3 4 5
 Price
1 2 3 4 5
IV. SURVEY RESULTS AND ANALYSIS
Thirty-one industry experts and leaders that include
CEO’s, CIO’s, senior VP’s, and middle-level managers from
leading firms participated in the survey. Their average
managerial experience is 11 years. The categorical data
analysis is based on logistic regression. (See Myers et al.
2002 for logistic regression.)
From the participant’s demographic information, the
following five factors are identified as independent variables
(regressors): 1) whether or not the participant is a top
management person; 2) whether the participant is an
operations management expert or an information systems
expert; 3) participant’s years of experience in his/her current
role; 4) whether the participant comes from a
semiconductor/electronics component manufacturer (ECM),
an electronics distributor, an original equipment
manufacturer (OEM), an original design manufacturer
(ODM), an electronics manufacturing services (EMS)
provider, or a company of another type; and 5) whether or
not the company of the participant is an industry leader.
We also have the following independent variables based
on the categories of the questions: 6) whether the question is
for Decision Set #1 (W/F), Set #2 (A/T), or Set #3 (C/S);
and 7) whether the question is for an inventory decision, an
allocation/release decision, a priority decision, or a logistics
decision.
Answers to the questions led to the following categorical
dependent variables (responses): A) decision technique used
(Opt, Heu, or Tac); B) decision frequency (H, D, W, or Mo);
C) decision timing (Mi, Hr, Dy, or Wk); and D) DoF
allowed in altering product type, release time, release
quantity, and product price in decisions made.
Note that there are a lot of factors (regressors) and
factor interactions that may contribute to changes in
response (answers). A stepwise procedure (see Myers et al.
2002) was implemented to screen important factors that are
statistically significant. The following four factors appear to
be significant:
• Whether the participant is in top management or a
middle-level manager.
• Whether the participant is an operations expert or an
information systems expert.
• What type of firm the participant comes from: ECM,
Distributor, or others.
• Which Decision Set the question belongs to: Set #1
(W/F), Set #2 (A/T), or Set #3 (C/S).

It is interesting that time in position does NOT make a
difference. People new in a position and people having a lot
of experience generally think the same way. People from
leading firms and non-leader firms think the same, as well.
The type of the decision (inventory, release/allocation,
priority, or logistics) also does NOT make a significant
difference. In addition, participants have a common opinion
on the DoF for all decisions. They agreed that a medium
DoF (level 3) is in place for altering all decisions. In other
words, none of the DoF factors are statistically significant to
make the response different.
Survey responses indicated that semiconductor
manufacturers want more optimization used in their
decisions, however they spend more time on their decisions
and make them less frequently. This makes intuitive sense
since the semiconductor firms are positioned upstream in the
overall electronics supply chain and have a relatively slower
clockspeed (Fine, 1998). The distributors want even more
optimization in their approach to decision making since they
operate on a lower margin and their value proposition is
limited. Based on the logistic regression odds ratios (see
Myers et al. 2002 for details on odds ratios in logistic
regression), semiconductor/component manufacturers are
approximately 8 times more likely to choose Optimization
over Tacit Knowledge, while electronics distributors are
about 60 times more likely to choose Optimization over
Tacit Knowledge in making decisions. Interestingly, top
management people uniformly want more optimization in
the decision making throughout the supply chain.
Compared with middle-level management, top management
is approximately 14 times more likely to choose
Optimization over Heuristics/Tacit Knowledge.
Decision Timing and Frequency are strongly correlated
with the Decision Technique response. If a longer time is
allowed for decision making and the decision is made less
frequently, it is more likely that optimization is used. On the
other hand, if the decision makers are under pressure and
need to make a determination quickly, they tend to use their
tacit knowledge.
We believe that the most important result from the
analysis is as follows. According to the odds ratio, when the
decisions move downstream from one stage in the supply
chain to the next stage (i.e., moving from W/F to A/T or
moving from A/T to C/S), it is 4 times less likely that
managers will use Optimization over Tacit Knowledge (see
Fig. 1), and 2 times less likely that they will choose
Heuristics over Tacit Knowledge.
Logistic Regression Estimation of Probability
of Using the Corresponding Decision Tech

Optimization
Heuristics
Tacit Knowledge

108

W/F
.45
.33
.22

A/T
.35
.27
.38

C/S
.20
.29
.51
Odds Ratio ≈0.25

Fig. 1. Decision Paradigms in the Semiconductor
Supply Chain

SuRP-A03.6

V. CONFIRMATION FROM THE LITERATURE
In a make-to-order (pull) environment (e.g., a third-party
semiconductor foundry), lot allocation is determined at the
W/F stage. The lot-to-order allocation problem in wafer fabs
is often referred to as a “pegging” problem. There are
generally two types of pegging policies. “Hard” pegging
commits a set of semiconductor wafer lots to a certain order
and the constructed lot-to-order assignment is fixed. “Soft”
pegging allows modifications in the lot-to-order assignment
to improve the solutions while the lots are being processed.
Pegging problems are often considered combinatorial
optimization problems with different objectives. Wu (2003)
presents a mixed pegging algorithm based on a hybrid of
hard and soft pegging for semiconductor foundry business
with multiple fabs and high-volume, high-mix products. The
mixed pegging method helps reduce WIP level and achieve
a better supply and demand matching with fewer unsatisfied
orders and unpegged lots. Bang et al. (2005) propose a repegging (soft pegging) method based on pairwise
interchange of pegged lots between orders (an improving
heuristic) to reduce the number of tardy orders. Kim et al.
(2007) propose re-pegging algorithms to reduce total
tardiness of jobs.
Many articles have also been published to address other
allocation problems in the semiconductor manufacturing
domain. Singh et al. (1988), Avram and Wein (1992), and
Seshadri and Shanthikumar (1997) study a specific problem
in semiconductor wafer fabs where a wafer can contain
different chips and a set of different chips are allocated to
wafers for production. Toktay and Uzsoy (1998) study an
equipment capacity allocation problem for a semiconductor
wafer fab with tooling constraints, setup considerations, and
differences in machine capabilities. A network flow model
with integer side constraints is formulated to maximize
throughput and minimize deviation from predetermined
production goals. Akcali et al. (2005) extend the work in
Toktay and Uzsoy (1998) by formulating a generalized case
as a degree-constrained network flow problem and
providing additional heuristics. Kim et al. (2002) study the
problem of finding the optimal stepper allocations using a
mixed integer programming model and show that a linear
relaxation works well in approaching the optimal solution.
Mallik and Harker (2004) and Karabuk and Wu (2005)
study the problem of allocating restrictive semiconductor
factory capacity to a set of different product lines. Game
theoretic models show that it is optimal for factory managers
to deflate their capacity forecasts and for product managers
to magnify their demand forecasts. A central coordinator
should make effort to induce truth-telling.
Some semiconductor manufacturers also adopt a pushpull strategy with a die-bank sitting between the front-end
and the back-end as buffer inventory (inventory/order
interface) to store wafers with semiconductor chips
produced on them (see Brown et al., 2000 for an example).
Knutson et al. (1999) introduce the problem of assigning
fabricated semiconductor wafer lots, available in die-bank
inventory, to customer orders. It is known as the lot-toorder-matching (LTOM) problem in the A/T stage. In order

to maximize back-end factory utilization and on-time
delivery and to minimize excess products, the problem is
formulated as a nonlinear integer program, decomposed into
two linear integer programs: a knapsack model dealing with
releasing orders to the factory with a limited capacity, and a
generalized bin-covering model dealing with assigning
wafer lots to released orders. Fowler et al. (2000) analyze
rule-based heuristics for the LTOM problem using an
experimental design with representative data sets, and derive
conditions under which certain heuristics work well. Carlyle
et al. (2001) specifically focus on the mathematical
properties of the bin-covering problem (the second stage) in
the LTOM problem. Boushell et al. (2007) is a recent
extension of the LTOM problem addressing classconstrained cases.
A pegging problem is a lot-to-order allocation problem at
the W/F stage, while the LTOM problem is a lot-to-order
allocation problem at the A/T stage. To the best of our
knowledge, there are no articles in open literature that
specifically address the lot-to-order allocation problem at the
final C/S stage in semiconductor supply chains where the
traditional push (make-to-stock) strategy is adopted and the
inventory/order interface is held at finished goods inventory.
This emphasizes our survey results that many optimization
tools (or at least optimization-oriented heuristics tools) are
available for front-end planning, a few exist for back-end
allocation, and no such tools for the final allocation at the
C/S stage.
VI. CONCLUSION, DISCUSSION, AND FUTURE RESEARCH
This survey result shows that when the viewpoint in the
supply chain changes, the decision paradigms change.
Decision makers tend to use optimization techniques in the
front-end stage of the semiconductor supply chain where the
major concern is to build the appropriate inventory based on
an uncertain forecast. As the products move to downstream
stages and the managers begin to face real customers, it is
more likely that experts’ tacit knowledge is used to make the
decisions in a fast clockspeed fashion..
In our review of the literature we noted that there are
numerous optimization models developed to help production
planning and control in the front-end. Heuristics are also
reported that help the A/T facility allocate fabricated wafer
lots available in die-bank inventory to anticipated customer
orders and to release these lots for A/T. We find this to be in
stark contrast to a gap in the literature deriving models that
address the final allocation decisions at the C/S stage. We
believe that the history of successful deployment of modelbased lot release and allocation applications in the first two
stages of semiconductor manufacturing should encourage
the development of optimization models to allocate final
products from finished goods inventory to firm customer
orders to determine “who gets what”. This conclusion
correlates well with top management’s perspective that all
supply chain decisions should be optimized. The major
implementation barrier for using optimization in this arena is
the computational complexity of the product allocation

109

SuRP-A03.6

problem in the semiconductor supply chain (Sun et al.,
2007b). Therefore we believe that it is necessary to construct
heuristics to find satisfactory solutions in a reasonable time
to support optimizing the final allocation. As some survey
participants pointed out, 100% customer satisfaction is
infeasible, so some product allocation will be required.
Since some firms are beginning to make their final
allocation decisions on a nearly real-time (daily) basis, it is
of crucial importance to develop allocation models to help
determine “who gets what” quickly and effectively. The
formulation of the final allocation problem, as well as the
development of efficient and effective heuristics is being
conducted as future research.
ACKNOWLEDGMENT
The authors would like to thank Intel, Corp. for
sponsoring this research.
REFERENCES
Akcali, E., A. Ungor, and R. Uzsoy, “Short-term capacity
allocation problem with tool and setup constraints”, Naval
Research Logistics, Vol. 52, pp754-764, 2005.
Avram, F., and L.M. Wein, “A product design problem in
semiconductor manufacturing”, Operations Research, Vol. 40, No.
5, pp986-998, 1992.
Bang, J.Y., K.Y. An, Y.D. Kim, and S.K.Lim, “A due-date
based algorithm for order-lot pegging in a semiconductor wafer
fabrication facility”, Proceedings of the International Conference
on Modeling and Analysis of Semiconductor Manufacturing
(MASM ’05), Singapore, October, pp175-180, 2005.
Boushell, T.G, Fowler, J.W., Keha, A., Knutson, K., and
Montgomery, D.C., "Evaluation of Heuristics for a Classconstrained Lot-to-Order Matching Problem in Semiconductor
Manufacturing", International Journal of Production Research, to
appear, 2007.
Brown, A.O., H.L. Lee, and R. Petrakian, “Xilinx improves its
semiconductor supply chain using product and process
postponement”, Interfaces, Vol. 30, No. 4, pp65-80, 2000.
Carlyle, W.M., K. Knutson, and J.W. Fowler, “Bin covering
algorithms in the second stage of the lot-to-order matching
problem”, Journal of the Operational Research Society, Vol. 52,
No. 11, pp1232-1243, 2001.
Chong, C.S., P. Lendermann, B.P. Gan, B.M. Duarte, J.W.
Fowler, and T.E. Callarman, “Analysis of a customer demand
driven semiconductor supply chain in a distributed simulation test
bed”, Proceedings of the 2004 Winter Simulation Conference,
pp1902-1909, 2004.
Chwif, L., M.R.P. Barretto, and E. Saliby, “Supply chain
analysis: spreadsheet or simulation?” Proceedings of the 2002
Winter Simulation Conference, pp59-66, 2002.
Cullen, S., “Semiconductor market outlook”, In-Stat/MDR
Analyst Report, April, 2004.
Fine, C.H., Clockspeed: Winning Industry Control in the Age
of Temporary Advantage, Perseus Books, 1998.
Fowler, J.W., K. Knutson, and W.M. Carlyle, “Comparison
and evaluation of lot-to-order matching policies for a
semiconductor assembly and test facility”, International Journal of
Production Research, Vol. 38, No. 8, pp1841-1853, 2000.
Karabuk, S., S. D. Wu. “Incentive schemes for semiconductor
capacity allocation: a game theoretic analysis”, Production and
Operations Management, Vol. 14, No. 2, pp175-188, 2005

Kim, S., S. Yea, and B. Kim, “Shift scheduling for steppers in
the semiconductor wafer fabrication process”, IIE Transactions,
Vol. 34, No. 2, pp167-177, 2002.
Kim, Y.D., J.Y. Bang, K.Y. An, and S.K. Lim, “A due-date
based algorithm for lot-order assignment in a semiconductor wafer
fabrication facility”, working paper, Korea Advanced Institute of
Science and Technology, 2007.
Knutson, K., K. Kempf, J.W. Fowler, and W.M. Carlyle,
“Lot-to-order matching for a semiconductor assembly and test
facility”, IIE Transactions, Vol. 31, No. 11, pp1103-1111, 1999.
Mallik, S., and P.T. Harker, “Coordinating supply chains with
competition: capacity allocation in semiconductor manufacturing”,
European Journal of Operational Research, Vol. 159, No. 2,
pp330-347, 2004.
Myers, R.H., D.C. Montgomery, and G.G Vining, Generalized
Linear Models with Applications in Engineering and the Sciences,
John Wiley & Sons, New York, 2002.
Pinedo, M.. Scheduling: Theory, Algorithms, and Systems.
Prentice-Hall, 2002.
Seshadri, S., and J.G. Shanthikumar, “Allocation of chips to
wafers in a production problem of semiconductor kits”, Operations
Research, Vol. 45, No. 2, pp315-321, 1997.
Singh, M.R., C.T. Abraham, and R. Akella, “Planning for
production of a set of components when yield is random”, Fifth
IEEE CHMT International Electronic Manufacturing Technology
Symposium Proceedings, Lake Buena Vista, FL, USA, 1988.
Sun, Y., D.L. Shunk, J.W. Fowler, and E.S. Gel, “Strategic
factor-driven supply
chain design for semiconductor
manufacturing”, working paper, Arizona State University, 2007a.
Sun, Y., J.W. Fowler, D.L. Shunk, E.S. Gel, and C.W.
Kirkwood, “The generalized allocation problem in semiconductor
supply chains”, working paper, Arizona State University, 2007b.
Toktay, L.B., and R. Uzsoy, “A capacity allocation problem
with integer side constraints”, European Journal of Operational
Research, Vol. 109, No. 1, pp170-182, 1998.
Wu, T.W., “Modular demand and supply pegging mechanism
for semiconductor foundry,” Proceedings of the 2003 IEEE
International Symposium on Semiconductor Manufacturing (ISSM
’03), pp325-328, 2003.

110

The Journal of Systems and Software 82 (2009) 1568–1577

Contents lists available at ScienceDirect

The Journal of Systems and Software
journal homepage: www.elsevier.com/locate/jss

Understanding the effects of requirements volatility in software engineering by
using analytical modeling and software process simulation
Susan Ferreira a,*, James Collofello b, Dan Shunk c, Gerald Mackulak c
a

Industrial and Manufacturing Systems Engineering Department, The University of Texas at Arlington, Arlington, TX 76019, USA
Computer Science and Engineering Department, Arizona State University, Tempe, AZ 76019, USA
c
Industrial Engineering Department, Arizona State University, Tempe, AZ 76019, USA
b

a r t i c l e

i n f o

Article history:
Available online 19 March 2009
Keywords:
Requirements volatility
Software process modeling
Requirements engineering risk

a b s t r a c t
This paper introduces an executable system dynamics simulation model developed to help project
managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The
paper discusses detailed results from two cases that show signiﬁcant cost, schedule, and quality impacts
as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the
complex set of factor relationships and effects related to requirements volatility.
Ó 2009 Elsevier Inc. All rights reserved.

1. Requirements volatility introduction
Requirements volatility refers to growth or changes in requirements during a project’s development lifecycle. There are multiple
aliases commonly associated with or related to the phenomenon of
requirements volatility. These terms include requirements change,
requirements creep, scope creep, requirements instability, and
requirements churn among others. Costello (1994) provides a relatively detailed set of metrics for requirements volatility. Other
simple metrics for requirements volatility deﬁne it as the number
of additions, deletions, and modiﬁcations made to the requirements set per time unit of interest (per week, month, phase,
etc.). Requirements volatility, in its various forms, surfaces as a frequent and high impact risk in numerous empirical studies performed to identify risk factors or to understand variables leading
to a project’s success or failure (examples include Boehm, 1991;
Curtis et al., 1988; Houston, 2000; Jones, 1994; Känsälä, 1997;
Moynihan, 1997; Ropponen, 1999; Ropponen and Lyytinen, 2000;
Schmidt et al., 2001; The Standish Group, 1995; Tirwana and Keil,
2006).
Changes to a set of requirements can occur at multiple points
during the development process (Kotonya and Sommerville,
1998). These changes can take place ‘‘while the requirements are
being elicited, analyzed and validated and after the system has
gone into service”. Past philosophy dictated that requirements
had to be ﬁrm by the completion of the requirements phase and
* Corresponding author. Tel.: +1 817 272 1332; fax: +1 817 272 3406.
E-mail addresses: ferreira@uta.edu (S. Ferreira), collofello@asu.edu (J. Collofello),
dan.shunk@asu.edu (D. Shunk), mackulak@asu.edu (G. Mackulak).
0164-1212/$ - see front matter Ó 2009 Elsevier Inc. All rights reserved.
doi:10.1016/j.jss.2009.03.014

that requirements should not change after this time. This view is
now understood to be unrealistic (Reifer, 2000). Kotonya and Sommerville (1998) discuss that requirements change is unavoidable.
They also indicate that requirements changes do not necessarily
imply that poor requirements engineering practice was utilized
as requirements changes could be the result of a combination of
factors. The term ‘‘requirements engineering” refers to the processes required to generate and maintain the software requirements throughout the duration of the project.
Concern for the effects of requirements volatility is not usually
associated with the front end of the process, for example, during
requirements deﬁnition. Volatility during the requirements deﬁnition phase is expected because this is when requirements are being
created. However, once the design process begins, the impact of
requirements change is progressively greater due to the additional
investment in time and effort as the project continues to generate
artifacts and complete required tasks. Additions or modiﬁcations
may need to be made to previously generated or in process project
artifacts and additional time investment or scrapped effort can
result. Due to the additional unplanned effort, severe consequences
can potentially occur, including signiﬁcant cost and schedule overruns, and at times, cancelled projects. The impact of changing
requirements during later phases of a project and approaches for
assessing the impacts of these changes has been well documented
(Yau et al., 1978, 1986, 1988; Yau and Kishimoto, 1987; Yau and
Liu, 1988). In an agile software development environment, changes
are welcomed throughout the development process (Beck et al.,
2001). The target of this article is not agile type projects but more
traditional development type projects.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

2. The need for a requirements volatility assessment tool
The effects of requirements volatility have been discussed in the
literature for some time. However, little empirical research has
been carried out on the topic of requirements volatility that considered the factors involved and the integrated quantitative effects
of requirements volatility on factors related to key project management indicators (cost, schedule, and quality). A relatively small
number of studies consider requirements volatility and its associated effects, especially in a manner integrated with other software
project management factors. These existing studies primarily fall
into a few major research method categories: survey or software
assessment based research (Jones, 1998, 1994; Lane, 1998; Nidumolu, 1996; Zowghi et al., 2000; Zowghi and Nurmuliani, 2002),
interviews and case studies (Javed et al., 2004; Loconsole and Börstler, 2005, 2007; Nurmuliani et al., 2004; Zowghi and Nurmuliani,
1998), regression analysis (Stark et al., 1999), reliability growth
model (Malaiya and Denton, 1999), analytic hierarchy process
analysis (Finnie et al., 1993), and simulation models (Houston,
2000; Lin and Levary, 1989; Lin et al., 1997; Madachy et al.,
2007; Madachy and Tarbet, 2000; Pfahl and Lebsanft, 2000; Smith
et al., 1993).
The existing simulation models discussed in the literature were
developed and tailored for one organization, have a limited view of
requirements volatility or requirements engineering, or do not include requirements engineering processes considered in concert
with the rest of the lifecycle or other critical project factors. A paucity of the literature exists on process modeling and simulation
work performed in requirements engineering, an area now receiving more focused attention because of the impact that it has on the
rest of the systems and software engineering lifecycle.
The limited research and relative importance of requirements
volatility as a risk and the relatively sparse level of requirements
engineering process modeling led the researchers to more analysis
and examination of these areas. A system dynamics process model

1569

simulator, the Software Project Management Simulator (SPMS)
that includes data which is stochastically based on industry survey
data distributions, was then developed as part of a doctoral dissertation (Ferreira, 2002). SPMS illustrates a software business model
that considers the effects of requirements volatility on a software
project’s key management parameters: cost, schedule, and quality.
SPMS presents a more comprehensive and detailed view of the
researched areas than previous models. The development of the
SPMS simulator and associated results developed in this paper
are discussed in this journal article.
3. Requirements volatility tool development process
This section of the paper brieﬂy discusses the research method
used to develop the simulation model. Key research questions addressed in the initial research study include: (1) Which software
factors are affected by requirements volatility? (2) How can these
factors and the uncertainty associated to these factors be modeled?
and (3) What is the project management impact of requirements
volatility? Fig. 1 provides a summary view of the processes used
during the research effort. Starting with the ﬁgure’s top left and
top right sides and ﬂowing down, the ﬁgure illustrates that two efforts (one per ﬁgure side) were initiated concurrently and these efforts ﬂowed into the development of the software process
simulator discussed in this paper.
A rigorous review of the requirements engineering and requirements volatility related literature was performed. Various process
and information models were created to represent and assist in
analysis and synthesis of the knowledge gained during the literature review. Requirements engineering process models and workﬂows, an information model, and a causal model were developed
prior to the simulator development. Relevant factors and associated relationships were identiﬁed based on analysis of the literature review material and discussions with software engineering
experts. Further analysis of the captured information led to the

Fig. 1. Research method.

1570

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

development of a causal model (Fig. 2). The causal model is important because it illustrates the cause and effect relationships between software development factors related to requirements
volatility and includes hypothesized relationships between factors.
The causal model was iteratively developed based on fundamental
factor relationships (for example, job size and overall project effort), researcher industry and academic experience, and hypothesized relationships. The ﬁgure highlights (blue shading) factors
and associated relationships (blue lines) that were further explored
during the research effort. This causal model is expected to evolve
over time as more relationships are explored and understood.
One of the proposed solutions to address the identiﬁed research questions was to develop a software process simulator.
A simulator was selected because it provides a tool for software
project managers and researchers to perform ‘‘what if” analyses,
and enables users to examine the risk of various levels of
requirements volatility and determine project outcomes. A simulator can represent the complexity of relationships between large
quantities of interrelated factors and effectively illustrates the effects and impact of requirements volatility. A simulator with a
graphical icon format was chosen to represent project factors
and relationships. The previously developed causal model was
used to develop the simulator. A subset of the previously generated causal model relationships and factors were selected and
modeled. A subset of the causal model relationships and factors
was chosen because follow-on work needed to be limited in
length to allow key factor data to be collected or these areas
were already modeled in the pre-existing simulator that was extended to create the new simulator, SPMS. Concepts and constructs from all of the generated process and information
models contributed to the creation of the simulator’s workﬂows
and other model sectors.
Joint research was performed with Daniel Houston to characterize four deterministic software process simulators using statistical
design of experiments (Houston et al., 2001). Results of this experimentation work fed into Dan Houston’s development of the Software Process Actualized Risk Simulator (SPARS) (Houston, 2000).
The SPMS research simulation model evolved from Houston’s
SPARS model. The SPARS model also represents an adaptation, as
it reuses or modiﬁes large portions from Abdel-Hamid and Madnick’s model (1991) and Tvedt’s model (1996) and then extends
the consolidated model to incorporate effects of a number of risk
factors. The SPARS model was selected because of its comprehensive software project management scope and updates for more
modern development practices. Reusing components from SPARS
facilitated the development of the SPMS model in that common
constructs did not need to be recreated and the model used previously validated simulator components. As part of the research effort, the SPARS model was modiﬁed to eliminate some
unnecessary factors and extended to create the research model,
SPMS.
As the initial simulator sector designs were generated, walkthroughs were conducted with an initial set of individuals who
were familiar with the research. Once an initial version of the model containing the key constructs was completed, a secondary walkthrough of the model was held with four reviewers outside of the
research group. These reviewers included representatives from
industry and academia that were currently performing research
in requirements engineering and/or software process modeling
and simulation. Following the model walkthroughs, the simulator
was modiﬁed to incorporate reviewer comments and suggestions.
The model was then ready to include quantitative data.
Many of the model variables required data that was not available in the literature. In order to populate these model parameters,
a survey was developed and administered to collect the needed
data. The Project Management Institute’s Information Systems

Speciﬁc Interest Group (PMI-ISSIG) sponsored the survey by providing the host site for the web-based survey and sending notiﬁcations about the survey to its members. Although PMI-ISSIG was the
primary target population for the survey, one mailing was sent to
individual Software Engineering Institute (SEI) Software Process
Improvement Network (SPIN) group contacts within the United
States and to individual professional contacts. Three hundred
twelve software project managers and other software development
personnel submitted responses for the survey.
Survey results indicated that 78% of the respondents experienced some level of requirements volatility on their project. The
survey ﬁndings highlight that requirements volatility can increase
the job size dramatically, extend the project duration, cause major
rework, and affect other project variables such as morale, schedule
pressure, productivity, and requirements error generation. Fig. 3
shows an example of requirements volatility related data from
the survey. The histogram in the ﬁgure depicts how requirements
change affected the job size as a percent of the original job size. In
the vast majority of cases, requirements volatility increased the job
size. However, one can see that there were also cases where there
was no change or a net decrease in the job size. Survey respondents
had an average of 32.4% requirements volatility related job size increases. Other captured volatility effects included signiﬁcant increases in project rework and reduced team morale. The survey
also captured effects from schedule pressure. As the resource effort
increases due to requirements volatility (to address job size additions and rework), schedule pressure also increases. The survey
data showed increases in requirements error generation as the
schedule pressure increases. These effects cause consequences
leading to impacts on key project management indicators such as
cost, schedule, and quality. More details on the survey ﬁndings
showing primary and secondary effects of requirements volatility
are addressed in Ferreira (2002).
Statistical analysis of the survey responses allowed the generation of stochastic distributions for many of the simulation model’s
requirements volatility and requirements engineering factors and
relationships. The model’s stochastic inputs are primarily generated using either empirical discrete distributions derived from analyzing histograms of the data or are generated using the inverse
transform method (Abramowitz and Stegun, 1964). The selection
of the type of distribution depended on the survey analysis results.
Using these stochastic inputs, the simulation model’s random variates use a random number as an input to generate the desired distribution. Based on the sampling frequency, the distributions are
sampled once per run or are sampled continuously throughout a
run to be drawn as necessary.
Once the derived stochastic factor distributions and relationships were added to the simulator, veriﬁcation and validation
(V&V) exercises were performed. Model validation determines
whether a model is a ‘‘useful or reasonable representation of the
system” (Pritsker et al., 1997). Veriﬁcation checks that the simulation model runs as intended. Matko et al. (1992) indicate that modeling work is not an exact science since the real system is never
completely known. Given this situation, the validation and veriﬁcation effort primarily focused on building conﬁdence in the model
as a reasonable representation of the system and in its usefulness
in the provision of results. The overall approach for veriﬁcation and
validation included tests of the structure and behavior of the model. This strategy follows a framework of guidelines presented by
Richardson and Pugh (1981) that builds conﬁdence in the model
and its results. The tests focus on suitability and consistency
checks. The model veriﬁcation and validation activities were performed by the model developer, and various software process
and project experts.
Additional veriﬁcation work was performed in order to understand differences between the SPMS and SPARS model when the

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

1571

Fig. 2. Causal model.

risk factors are not actualized. Differences between the models in
this case were relatively small. More information about this veriﬁcation work is discussed in Ferreira et al. (2003).
4. Assessment tool capability and use
SPMS illustrates researched effects of requirements volatility
and includes requirements engineering extensions to the SPARS

software project management model. The SPARS model was modiﬁed to eliminate some unnecessary factors and extended to create
the research model. Major additions to the base model include the
researched results for the effects of requirements volatility and signiﬁcant extensions to add and support the requirements engineering portions of the software development lifecycle. SPMS
encompasses the requirements engineering through test phases
of the software development lifecycle. Requirements volatility

1572

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Fig. 3. Distribution of requirements volatility related percent change in job size.

starts and is stochastically simulated during the project’s development and test phases. The model workﬂows also cover the entry of
change requests, change request analysis and review activities, and
their disposition (accepted, rejected/deferred).

Table 1
Model classiﬁcation.
Purpose
Scope
Model
approach

Planning, understanding, process improvement
Medium to large size project, short to long duration, one product/
project team
Continuous, mixed mode (stochastic and deterministic variables),
iThinkTM simulation tool

SPMS demonstrates causal model effects of requirements volatility. Survey ﬁndings showing the impact of requirements volatility on increasing software project job size (this occurs a majority of
the time), increased rework, and lowered staff morale are represented in the model using stochastic relationships derived during
the survey data analysis. Over 50 new parameters that used distributions derived from the survey data were added to the model. In
addition to these survey drawn distributions, a signiﬁcant number
of distributions were reused from other sources, or parameters
were modeled using single point inputs that could be changed by
a user. The effects of lowered morale on requirements engineering
productivity and requirements error generation are represented in
the model. Schedule pressure effects on requirements error generation were also studied as part of the survey data analysis and were
added to pre-existing schedule pressure effects in the model.
Among other survey data used in the simulator, the model includes
requirements defect detection effectiveness for various software
development activities or milestones and relative work rates of
requirements volatility related activities compared to their normal
work rate. Other model contributions include the addition of a
requirements engineering staff type and requirements engineering
support activities which were added to pre-existing simulator
development and test personnel and activities.
Table 1 presents a view of the model’s classiﬁcation, according
to the characterization framework from Kellner et al. (1999). The
typical model audience is expected to be software development
project managers or researchers seeking to gain an understanding
of requirements engineering and requirements volatility and its effects integrated with other software project risks. The model is relatively complex, given its purpose, and assumes a sophisticated

Table 2
Model sector descriptions.
Sector name

Description

Change request work ﬂow

Stochastic change request entry over 10 intervals. Incoming change request (CR) analysis, change request control board (CCB) review,
and disposition
Requirements generation and review process including requirements error and defect rework. Stochastic entry of requirements
volatility related project scope changes and rework over 10 intervals
Work product ﬂow through the development lifecycle, from design and code through testing and rework of design and code errors.
Work is pulled from development and test activities for requirements volatility related rework and reduction and for rework of
requirements defects
A support sector that calculates the amount of product to add to the product cycle for requirements volatility additions based on
survey data
A support sector that calculates the amount of product to pull from the development and test work for requirements volatility related
rework based on survey data
A support sector that calculates the amount of product to pull from development and testing work ﬂow activities for requirements
related reductions. Includes policy choice that deﬁnes where to remove product
Entry and summation of requirements engineering, developer, and tester planned stafﬁng proﬁle information
Allocation of requirements engineer effort to requirements engineering activities based on activity priority

Requirements work ﬂow
Development and test work ﬂow

Requirements change additions
Requirements change rework
Requirements change reductions
Planned stafﬁng
Requirements engineer effort
allocation
Developer and tester effort
allocation
Requirements quality management
Development and test quality
management
Actual stafﬁng
Attrition and replacement
Planning
Control
Adjustment of job effort
Productivity inputs
Productivity
Progress measure
Senior management commitment

Allocation of developer and tester effort to project activities based on activity priority
Generation and detection of requirements errors and defects
Generation and detection of design and code errors and defects
Entry and exit of staff based on planned stafﬁng proﬁles, assimilation of new staff, attrition, and replacements. Entry of contract
personnel and organizationally experienced personnel handled separately for the different staff groups
Attrition (including attrition due to low morale) and replacement calculations for requirements engineers and the grouped set of
developers and testers
Calculation of requirements engineer and the grouped developer and tester work force levels needed based on willingness to hire
Calculation of effort perceived still needed to complete project, effort shortages, schedule pressure, and assumed requirements
engineer, developer, tester productivity. Calculations to determine start of development and testing
Adjustment of job effort based on requirements volatility related additions, rework, and reductions as well as from underestimation
Productivity inputs and calculation of project activity productivity based on productivity multipliers
Work rate modiﬁcation due to effort remaining, effort required, and staff exhaustion. Generation of two staff type productivity
multipliers (including learning, communication overhead, staff experience, and morale)
Calculation of requirements engineering and development and testing progress. Modiﬁcation of currently perceived job size based on
requirements volatility and discovered work due to underestimation
Adjustment of stafﬁng and schedule multipliers based on senior management commitment. This is the only sector that was not
modiﬁed from the original SPARS model

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

user that is educated on the use of simulation models and software
development project management. The model is practically based,
relying on a signiﬁcant and proven foundation of software project
management and simulation research.
The model is segmented into 20 sectors. The sectors are organized into convenient and logical groupings of related factors. Table 2 provides an overview of the model sectors with a brief
description of each in order to give the reader an introduction to
the model’s scope. An example of one sector excerpted from the
model is shown in Fig. 4. The view illustrates the requirements
engineering work ﬂow sector. The connections on the right of the
sector ﬂow into or out of the development and test work ﬂow sector (sector not shown).
The requirements work ﬂow sector encompasses a normal
product work ﬂow. Requirements are generated and reviewed during the normal product work ﬂow. The normal requirements work

1573

ﬂow begins at the initiation of the requirements engineering phase,
at the To_Be_Worked stock. The To_Be_Worked stock is initially
populated with the estimated starting job size. The work then
ﬂows through the Generated_Reqts, Reqts_Awaiting_Review,
Reviewed_Reqts stocks, and then into the development process
as an inﬂow. The requirements generation activities are aggregated, encompassing requirements elicitation, analysis, negotiation, and initial requirements management. Once reviewed, the
requirements product is dispositioned and defective product containing requirements errors is removed from the normal work ﬂow
and becomes part of the requirements error rework work ﬂow, to
be reworked before ﬂowing into the development and test activities. Requirements defects caught post the requirements phase
come back into the requirements defect rework work ﬂow to be reworked. The reworked product then ﬂows back into the development and test activities. Additions to job size due to volatility

Fig. 4. SPMS simulator requirements engineering work ﬂow sector.

1574

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

and underestimation ﬂow into the normal work ﬂow through the
course of the project. The lower half of the sector includes the
requirements change rework work ﬂow. Rework due to requirements volatility is drawn from the development and test work
ﬂows and is worked though the requirements change rework work
ﬂow. This work ﬂow contains separate requirements error related
activities.
The model allows the user to enter detailed inputs related to
their project. Other data is automatically extracted from the stochastic distributions derived from the survey data. Table 3 provides
a listing of a subset of the new inputs added to the model to provide a ﬂavor for the types of data required from the user. These inputs can be input per run or a set of runs. Table 4 identiﬁes the new

Table 3
New factor user inputs (subset only).
Factor

Input quantity

Quantity of experienced requirements
engineers (REs), per interval
Quantity of new requirements
engineers, per interval
Quantity of experienced personnel allocated
to training new reqts engineers (%)
RE transfer in day (days)
Experienced organization RE transfer quantity
Time delay to transfer REs off project (days)
Max quantity of new RE hires per experienced
staff (staff/staff)
Time for organization experienced REs (but not
project experienced) to be productive (days)
Project day that person in org is scheduled to
come onto project (day)
Quantity of RE staff experienced in the organization who
are to be transferred on project (staff)
Requirements review adjustment policy – adjusts
review effort (boolean switch)
Reworked requirements review adjustment policy
(boolean switch)
Requirement error bad ﬁx fraction (%)
Requirements defect bad ﬁx fraction (%)
Nominal change request analysis productivity
(function points/person-day)
Nominal requirements generation productivity
(function points/person-day)
Nominal requirements review productivity
(function points/person-day)
Requirements volatility (boolean switch)

10 (1 for each of 10
intervals)
10 (1 for each of 10
intervals)
1
1
1
1
1
1
1
1
1 Selection
1 Selection
1
1
1
1
1
1 Selection

survey distributions added to the model. Data from the survey is
extracted from stochastic distributions with timing as deﬁned in
Table 4. ‘‘1 Selection per run” means that a value is pulled from
the stochastic distribution one time per run and used throughout
the run. While the data listed in Tables 3 and 4 do not include all
the new or modiﬁed factors in the model it does provide a perspective of the type of data that the user can enter and use.
5. Assessment tool results
The simulator was used to run two cases for the purpose of
comparing them. Each case was setup for 100 runs apiece. The
set of 100 runs for each case was selected for convenience as the
modeling tool allows additional runs, if desired. The two cases
are as follows: (1) A baseline case without the requirements volatility actualized [baseline] and (2) a case with the requirements
volatility related factors actualized [reqts volatility]. The data in
the square brackets corresponds to the case identiﬁer in later ﬁgures. Actualizing the requirements volatility risk for the second
case allows the model to represent the stochastic researched effects of requirement volatility. These include survey-based effects
related to requirements additions, changes, and modiﬁcations as
well as entering change requests. Also included, among the other
effects, are morale, and related schedule pressure effects on morale. The initial job size was estimated to be 400 function points.
The original schedule estimate (planned duration) was deﬁned to
be 408 days.
Figs. 5–8 depict the differences, at the completion of the projects, between the baseline runs (no requirements volatility considered) and runs with the other case where the requirements
volatility risk is actualized for various summary outputs. The model allows additional detailed outputs, if desired. Box plots were
used to represent the data because the simulation results were
positively skewed given the tendency for higher project size, cost,
duration, and released defects (among other outputs) with the
actualization of the requirements volatility risk. The box plots provide a graphical display of the center and the variation of the data,
allowing one to see the symmetry of the data set (Ott, 1988). With
the exception of the ﬁnal project size (Fig. 5), the baseline results
show a small level of variability because some of the model parameters (e.g. requirements engineering process factors) were modeled
stochastically. Therefore, even when the requirements volatility
risk is not actualized, some variability in results will appear.

Table 4
New survey data distributions.
Factor

Selection frequency

Quantity of requirements change requests, per interval (intervals 1–10)
Change request time span with requirements volatility (ratio of duration)
Determination of change request acceptance/deferral due to schedule pressure (ratio)
Relative requirements defect rework productivity (ratio)
Relative requirements generation rework productivity (ratio)
Relative requirements change error rework productivity (ratio)
Relative requirements change requirement review productivity (ratio)
Relative design and code requirements defect rework productivity (ratio)
Percentage of perceived job size increased due to requirements volatility, per interval (%)
Percentage of perceived job size reworked due to requirements volatility, per interval (%)
Percentage of perceived job size reduced due to requirements volatility, per interval (%)
Requirements review effectiveness (%)
Design and code requirement defect detection effectiveness (%)
Design and code review requirement defect detection effectiveness (%)
Test requirement defect detection effectiveness (%)
Reworked requirements review effectiveness (%)
Reworked requirements design and code defect detect effectiveness (%)
Requirements error multiplier for schedule pressure (ratio)
Requirements error multiplier for morale (ratio)

10 Selections per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
10 Selections per run
10 Selections per run
10 Selections per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run

(1 selection for each interval)

(1 selection for each interval)
(1 selection for each interval)
(1 selection for each interval)

1575

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Fig. 5. Project size box plots.

Fig. 7. Project duration box plots.

Fig. 8. Project released defect density box plots.
Fig. 6. Project cost box plots.

Fig. 5 illustrates the difference in project size between the baseline case and the requirements volatility case. The baseline size is
400 function points. The average size for the requirements volatility case is 499.9 function points with a standard deviation of 82.0,
and a range from 382.0 to 788.3 function points over the 100 runs.
The runs that have values below the baseline 400 function points
(e.g. 382.0) can be explained by the fact that requirements changes
do not always add additional project scope, but can be made to remove scope or unnecessary requirements. However, 93 of the 100
runs had a ﬁnal project size greater than the base case of 400 functions points with relatively large scope added in the course of the
project. This scope addition has a signiﬁcant negative ripple effect
on the project cost (effort in person-days) and project duration. Table 5 contains the detailed statistics for each case including average, median, standard deviation calculations and the minimum
and maximum run values.
Fig. 6 presents the results for cost. Cost is considered to be human effort consumed during the project in person-days. This cost is
equivalent to effort and does not include other project costs. The
effort or cost presented in the ﬁgure encompasses the cumulative
effort for the requirements engineering through test activities.

Table 5
Case study simulation results.
Output/Case

Median

Std. Dev.

Minimum

Maximum

Size (function points)
Baseline
400
Reqts volatility
499.9

Average

400
485.7

0
82.0

400
382.0

400
788.3

Cost (effort in person days)
Baseline
4107.6
Reqts volatility
6208.2

4119.0
5968.9

35.9
1398.1

3969.5
3988.5

4155.5
11196.1

Duration (days)
Baseline
Reqts volatility

452.0
608.5

4.3
132.8

442.0
444.0

461.0
1101.0

Released defect density (defects per function point)
Baseline
0.753
0.753
0.013
Reqts volatility
0.894
0.886
0.075

0.726
0.747

0.792
1.170

452.4
634.0

The detailed statistics are presented in Table 5. As mentioned
earlier, the baseline results do show a small level of variability because some of the model parameters are represented stochastically. The average for the requirements volatility case is over
2000 person-days more than the average for the baseline case.

1576

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Considering that the baseline cost is an average of 4107.6 person
days, this is a very large number for a program manager to justify
as it represents more than a 50% increase in resource costs. The
cost range for the requirements volatility case is very large as well.
The requirements volatility case has a maximum of 11,196.1 person-days. This represents a very signiﬁcant difference from the
baseline maximum.
Fig. 7 displays the results for project duration in days. The duration
encompasses the requirements engineering through test activities for
the project. The detailed statistics for the two cases are shown in Table
5. It takes an average of 452.4 days to complete the project for the
baseline case. The average for the requirements volatility case is signiﬁcantly higher at an average of 634.0 days. As in the case of the cost,
the range for the requirements volatility case has a wide span.
Fig. 8 presents the project released defect density box plots. The
data represents the defects released post the test process and is in
units of defects per function point. The quantity of released defects
is also signiﬁcantly higher for the requirements volatility case. The
baseline case has an average of 0.753 defects per function point.
The average for the requirements volatility case is 0.894. The range
span for this case is also relatively large.
For the case that include the risk of requirements volatility, the
box plots for each parameter have a wide span. The wide span in outcomes indicates a large potential for unpredictable results as well as
considerable impact on project results. A comparison of the model
results for projects with no requirements volatility and those with
requirements volatility show signiﬁcant differences in the box plots.
One can clearly see the potential impact of requirements volatility
on key project management parameters such as cost (effort), schedule, and quality. Differences between the baseline and requirements volatility case results can be attributed to the addition (or
reduction) in project scope caused by requirements volatility, cause
and effect relationships between factors, and the stochastic representation of a number of the factors.

6. Summary and conclusions
Key research questions were addressed as part of the research. These questions include: (1) Which software factors are
affected by requirements volatility? (2) How can these factors
and the uncertainty associated to these factors be modeled?
and (3) What is the project management impact of requirements
volatility? A causal model was developed and used to evaluate
which software factors are affected by requirements volatility.
A survey was administered to collect information for a subset
of the factors identiﬁed in the causal model. This allowed the
researchers to verify the relationships and quantify the level of
the relationships. A simulator was chosen as the means to model
how the factors relate to other project factors and to represent
the uncertainty associated to the factors using stochastic distributions derived from survey data. The SPMS model assists in
understanding and evaluating the program management impact
of requirements volatility.
This simulator can help an interested user to better understand
the requirements engineering process and the impact of requirements volatility via its process-oriented workﬂows and comprehensive scope. The factor relationships represented in the model
represent a signiﬁcant contribution. This work expands our understanding of the requirements engineering process and the effects of
requirements volatility. The rigorous research to both understand
and leverage the previous foundation of knowledge in requirements engineering and requirements volatility and the thorough
nature of the survey and analysis of this valuable data to assess factor relationships and populate the simulator with empirical data is
a signiﬁcant contribution.

In addition to simulating the effects of requirements volatility,
SPMS offers the ability to simulate various project scenario combinations starting with the requirements engineering portion of the
lifecycle. The model offers a robust set of parameters that capture
various facets of the project including requirements errors and defects, requirements defect density, defect containment effectiveness for various milestones, and other parameters integrated
with a very comprehensive development and test related set of factors and relationships. Each of the model distributions can be easily calibrated and tailored to a speciﬁc organization’s environment.
All the other factors are also setup to be readily modiﬁable to reﬂect an organization’s historical data.
Survey data used in the simulator captures information on factors and relationships not previously available from a wide population in the software industry and allows modeling variables
stochastically given the large quantity of survey responses. Many
of the model variables were modeled stochastically to allow the
user to assess project outcomes probabilistically. The model results
quantitatively illustrate the potential for signiﬁcant cost, schedule,
and quality impacts related to requirements volatility.
Software project managers can use this tool to better understand the effects of requirements volatility and this risk in concert
with other common risks. The simulator can be used to assess potential courses of action to address the risk of requirements
volatility.

7. Future research
Additional research in the area of requirements engineering, as
it continues to evolve, is expected to provide a continuous stream
of new ideas, perspectives, and information that can allow for the
development of richer models and that can represent different facets of understanding in this under-represented yet critical research
area. More work needs to be done to model the impact of requirements engineering related processes and policies so that project
managers and software development personnel are more aware
of the impact of the decisions they make during this phase and
how the rest of the lifecycle may be affected by their choices. This
work can lead to the further identiﬁcation of best practices and
generate insights valuable to managing software development projects in the future.
As this and other simulators that incorporate requirements
engineering processes and relationships continue to evolve, additional experimentation with the models may prove valuable in
identifying common factors and relationships. The research model
presented in this paper is relatively large and complex. Sensitivity
analysis can assist in the identiﬁcation of inﬂuential factors in this
and other models. The results of further experimentation may be
used to ‘‘prune” insigniﬁcant factors from the model so that a more
efﬁcient model can result (Houston et al., 2001). One of the beneﬁts
of a simpler model includes a shortened training and learning
ramp-up time. Core concepts that make the most difference in results can be emphasized. Since the model is less complex it becomes easier to understand and may perhaps be more popular
given the reduced time to understand and then populate the model
with organizational and project speciﬁc data. Maintenance time
may also be reduced because the model is smaller and it is easier
to ﬁnd and ﬁx problems.
Agile processes which welcome requirements changes present
another valuable area to study. As these processes continue to mature and more quantitative results are available, simulating different types of agile processes, (e.g. XP, Scrum, etc.) would be of
interest to determine the project management ramiﬁcations related to cost, schedule, and quality as compared to more traditional
approaches.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics: An Integrated
Approach. Prentice-Hall, Englewood Cliffs, NJ.
Abramowitz, M., Stegun, I.A. (Eds.), 1964. Handbook of Mathematical Functions,
Applied Mathematics Series 55. National Bureau of Standards, Washington, DC.
Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M.,
Grenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R.,
Mellor, S., Schwaber, K., Sutherland, J., Thomas, D., 2001. Principles Behind the
Agile Manifesto. Retrieved 11.6.2008 from: <http://www.agilemanifesto.org/
principles.html>.
Boehm, B.W., 1991. Software risk management: principles and practices. IEEE
Software 8 (1), 32–41.
Costello, R.J., 1994. Metrics for Requirements Engineering. Master of Science Thesis,
California State University, Long Beach.
Curtis, B., Krasner, H., Iscoe, N., 1988. A ﬁeld study of the software design process for
large systems. Communications of the ACM 31 (11), 1268–1287.
Ferreira, S., 2002. Measuring the Effects of Requirements Volatility on Software
Development Projects, Ph.D. Dissertation, Arizona State University.
Ferreira, S., Collofello, J., Shunk, D., Mackulak, G., Wolfe, P., 2003. Utilization of
Process Modeling and Simulation in Understanding the Effects of Requirements
Volatility in Software Development. In: Proceedings of the 2003 Process
Simulation Workshop (ProSim 2003).
Finnie, G.R., Witting, G.E., Petkov, D.I., 1993. Prioritizing software development
productivity factors using the analytic hierarchy process. Journal of Systems
and Software 22 (2), 129–139.
Houston, D.X., 2000. A Software Project Simulation Model for Risk Management,
Ph.D. Dissertation, Arizona State University.
Houston, D.X., Ferreira, S., Collofello, J.S., Montgomery, D.C., Mackulak, G.T., Shunk,
D.L., 2001. Behavioral characterization: ﬁnding and using the inﬂuential factors
in software process simulation models. Journal of Systems and Software 59 (3),
259–270.
Javed, T., Maqsood, M., Durrani, Q.S., 2004. A study to investigate the impact of
requirements instability on software defects. ACM SIGSOFT Software
Engineering Notes 29 (3), 7.
Jones, C., 1994. Assessment and Control of Software Risks. PTR Prentice-Hall, Inc.,
Englewood Cliffs, NJ.
Jones, C., 1998. Estimating Software Costs. McGraw-Hill, New York.
Känsälä, K., 1997. Integrating risk assessment with cost estimation. IEEE Software
14 (3), 61–67.
Kellner, M.I., Madachy, R., Raffo, D.M., 1999. Software process modeling: why?
what? how? Journal of Systems and Software 46 (2–3), 91–105.
Kotonya, G., Sommerville, I., 1998. Requirements Engineering: Processes and
Techniques. John Wiley and Sons, Ltd..
Lane, M.S., 1998. Enhancing software development productivity in Australian ﬁrms.
In: Proceedings of the Ninth Australasian Conference on Information Systems
(ACIS ’98), vol. 1, pp. 337–349.
Lin, C.Y., Abdel-Hamid, T., Sherif, J.S., 1997. Software-engineering process
simulation model (SEPS). Journal of Systems and Software 38 (3), 263–277.
Lin, C.Y., Levary, R.R., 1989. Computer aided software development process design.
IEEE Transactions on Software Engineering 15 (9), 1025–1037.
Loconsole, A., Börstler, J., 2005. An industrial case study on requirements volatility
measures. In: Proceedings of the 12th Asia-Paciﬁc Software Engineering
Conference (APSEC ’05), 8 p.
Loconsole, A., Börstler, J., 2007. Are size measures better than expert judgment? An
industrial case study on requirements volatility. In: Proceedings of the 14th
Asia-Paciﬁc Software Engineering Conference (APSEC ’07), pp. 238–245.
Madachy, R., Tarbet, D., 2000. Initial experiences in software process modeling.
Software Quality Professional 2 (3), 1–13.
Madachy, R., Boehm, B., Lane, J., 2007. Assessing hybrid incremental processes for
SISOS development. Software Process: Improvement and Practice 12 (5), 461–
473.
Malaiya, Y.K., Denton, J., 1999. Requirements volatility and defect density. In:
Proceedings of the 10th International Symposium on Software Reliability
Engineering, pp. 285–294.
Matko, D., Zupancic, B., Karba, R., 1992. Simulation and Modeling of Continuous
Systems: A Case Study Approach. Prentice-Hall International Ltd., Great Britain.
Moynihan, T., 1997. How experienced project managers assess risk. IEEE Software
14 (3), 35–41.
Nidumolu, S.R., 1996. Standardization, requirements uncertainty and software
project performance. Information and Management 31 (3), 135–150.
Nurmuliani, N., Zowghi, D., Fowell, S., 2004. Analysis of requirements volatility
during software development life cycle. In: Proceedings of the 2004 Australian
Software Engineering Conference (ASWEC ’04), pp. 28–37.
Ott, L., 1988. An Introduction to Statistical Methods and Data Analysis. PWS-KENT
Publishing Company.
Pfahl, D., Lebsanft, K., 2000. Using simulation to analyze the impact of software
requirements volatility on project performance. Information and Software
Technology 42 (14), 1001–1008.
Pritsker, A. Alan B., O’Reilly, Jean J., LaVal, David K., 1997. Simulation with Visual
SLAM and AweSim. System Publishing Company, West Lafayette, IN.
Reifer, D.J., 2000. Requirements management: the search for Nirvana. IEEE Software
17 (3), 45–47.
Richardson, George, P., Alexander, L., Pugh III, 1981. Introduction to System
Dynamics Modeling with DYNAMO. The MIT Press, Cambridge, MA.

1577

Ropponen, J., 1999. Risk assessment and management practices in software
development. Chapter 8 in Beyond the IT Productivity Paradox. John Wiley
and Sons. pp. 247–266.
Ropponen, J., Lyytinen, K., 2000. Components of software development risk: how to
address them? A project manager survey. IEEE Transactions on Software
Engineering 26 (2), 98–112.
Schmidt, R., Lyytinen, K., Keil, M., Culle, P., 2001. Identifying software project risks:
an international Delphi study. Journal of Management Information Systems 17
(4), 5–36.
Smith, B.J., Nguyen, N., Vidale, R.F., 1993. Death of a software manager: how to avoid
career suicide through dynamic software process modeling. American
Programmer 6 (5), 10–17.
The Standish Group, 1995. The Chaos Report. Obtained from <http://www.
standishgroup.com/chaos.html>.
Stark, G.E., Oman, P., Skillicorn, A., Ameele, A., 1999. An examination of the effects of
requirements changes on software maintenance releases. Journal of Software
Maintenance: Research and Practice 11 (5), 293–309.
Tirwana, A., Keil, M., 2006. Functionality risk in information systems development:
an empirical investigation. IEEE Transactions on Engineering Management 53
(3), 412–425.
Tvedt, J.D., 1996. An Extensible Model for Evaluating the Impact of Process
Improvements on Software Development Cycle Time. Ph.D. Dissertation,
Arizona State University.
Yau, S.S., Collofello, J.S., MacGregor, T., 1978. Ripple effect analysis of software
maintenance. In: Proceedings of the IEEE Computer Society’s Second International
Computer Software and Applications Conference (COMPSAC ’78), pp. 60–65.
Yau, S.S., Kishimoto, Z., 1987. A method for revalidating modiﬁed programs in the
maintenance phase. In: Proceedings of 11th IEEE International Computer
Software and Applications Conference (COMPSAC ‘87), pp. 272–277.
Yau, S.S., Liu, C.S., 1988. An approach to software requirement speciﬁcations. In:
Proceedings of 12th International Computer Software and Applications
Conference (COMPSAC ‘88), pp. 83–88.
Yau, S.S., Nicholl, R.A., Tsai, J.P., 1986. An evolution model for software maintenance.
In: Proceedings of 10th IEEE International Computer Software and Applications
Conference (COMPSAC ‘86), pp. 440–446.
Yau, S.S., Nicholl, R.A., Tsai, J.P., Liu, S.S., 1988. An integrated life-cycle model for software
maintenance. IEEE Transactions on Software Engineering 14 (8), 1128–1144.
Zowghi, D., Nurmuliani, 1998. Investigating requirements volatility during software
development: research in progress. In: Proceeding of the Third Australian
Conference on Requirements Engineering (ACRE98), pp. 38–48.
Zowghi, D., Nurmuliani, 2002. A study of the impact of requirements volatility on
software project performance. In: Proceedings of the Ninth Asia-Paciﬁc
Software Engineering Conference (ASPEC ’02), pp. 3–11.
Zowghi, D., Offen, R., Nurmuliani, 2000. The impact of requirements volatility on the
software development lifecycle. In: Proceedings of the International Conference
on Software Theory and Practice (IFIP World Computer Congress), pp. 19–27.

Susan Ferreira is an Assistant Professor and the founding Director of the Systems
Engineering Research Center (SERC) at The University of Texas, Arlington (UTA).
Before joining UTA, Dr. Ferreira worked as a systems engineer in the Defense
industry on complex software intensive systems. Her industry background includes
work for Lockheed Martin, General Dynamics/Motorola, and Northrop Corporation.
Her teaching and research interests are related to systems engineering (SE) and
include requirements engineering, SE process modeling and simulation, lean SE, SE
return on investment, SE cost estimation, and system of systems engineering.
James Collofello is currently Associate Dean for the Engineering School and Professor of Computer Science and Engineering at Arizona State University. He received
his Ph.D. in Computer Science from Northwestern University. His teaching and
research interests lie in the software engineering area with an emphasis on software project management, software quality assurance and software process modeling. In addition to his academic activities, he has also been involved in applied
research projects, training and consulting with many large corporations over the
last 25 years.
Dan Shunk is the Avnet Professor of Supply Network Integration in Industrial Engineering at Arizona State University. He is currently pursuing research into collaborative commerce, global new product development, model-based enterprises and
global supply network integration. He won a Fulbright Award in 2002-2003, the 1996
SME International Award for Education, the 1991 and 1999 I&MSE Faculty of the Year
award, the 1989 SME Region VII Educator of the Year award, chaired AutoFact in 1985,
and won the 1982 SME Outstanding Young Engineer award. Dr. Shunk studied at
Purdue where he received his Ph.D. in Industrial Engineering in 1976.
Gerald Mackulak is an Associate Professor of Engineering in the Department of
Industrial, Systems and Operations Engineering at Arizona State University. He is a
graduate of Purdue University receiving his B.Sc., M.Sc., and Ph.D. degrees in the
area of Industrial Engineering. His primary area of research is simulation methodology with a focus on model abstraction, execution speed and output analysis. He
has authored over 75 articles, served as an associate editor for two simulation
journals and continues to guide research in simulation efﬁciency.

Computers in Industry 53 (2004) 153–164

Matching indirect procurement process with different
B2B e-procurement systems
Joong-In Kima,*, Dan L. Shunkb
a

Department of Management Information Systems, Hongik University, Jochiwon, Choongnam 339-701, South Korea
b
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287-5906, USA
Received 25 April 2003; accepted 23 July 2003

Abstract
A single information system cannot meet all the business requirements, and not all e-procurement systems for the
procurement of indirect goods are equally suitable for supporting different business processes throughout their distinct phases.
Companies need to adopt different e-procurement systems or business models in a hybrid and seamless manner. Existing
literature addressing e-procurement systems or business models have focused on the indirect procurement at high-level
granularities only or at a low-level granularity only, either within a single e-procurement system or without matching between
procurement processes and different e-procurement systems. In this paper, we provide the matching between indirect
procurement process and different e-procurement systems, at both the high-level and low-level activities, for identifying
and articulating the areas where various e-procurement systems can be utilized in a hybrid and seamless manner. The output of
this paper can help companies adopt various e-procurement systems or business models and redesign their e-procurement
processes.
# 2003 Elsevier B.V. All rights reserved.
Keywords: E-procurement; Indirect procurement; Procurement process

1. Introduction
Corporate procurement can be broadly divided into
the procurement of direct goods (i.e. direct procurement) and that of indirect goods (i.e. indirect procurements). Direct goods are the materials that are used
in the production of manufactured goods. Indirect
goods are the supplies a company uses in day-today operations, but not in the manufacturing of
goods. These include office supplies and equipment,
MRO (maintenance, repair and operation), computers,
*
Corresponding author. Tel.: þ82-41-860-2526;
fax: þ82-41-862-2974.
E-mail addresses: jokim@hongik.ac.kr (J.-I. Kim),
dan.shunk@asu.edu (D.L. Shunk).

software and other IT equipment, marketing kits and
services, travel reservations and other services, as well
as capital goods.
Traditional emphasis of the procurement function
has been on direct procurement [6,9,17,26,55,57].
Considered of strategic relevance, many efforts have
been made to streamline the inflow of direct goods
onto the manufacturing floor and to increase the
efficiency of structured procurement processes. Direct
procurement can be scheduled in a timely manner to
meet demand, provided sufficient information about
demand is available, and sources of materials are
secure and reliable. Besides establishing routine buying procedures, IT (information technology) has long
been utilized in this context. In particular at large

0166-3615/$ – see front matter # 2003 Elsevier B.V. All rights reserved.
doi:10.1016/j.compind.2003.07.002

154

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

organizations, EDI (electronic data interchange)
applications have provided the basis for JIT (justin-time) arrangements and automated replenishment
systems. Buying operations are typically triggered
through demand forecasting, and stipulated in
BOM (bill of materials), and manifested in MRP
(material requirements planning) or ERP (enterprise
resource planning). The production and distribution
of direct goods in collaboration with suppliers and
business partners is termed SCM (supply chain
management). Many computer applications have
been developed for supply chain and production
management.
Compared to direct procurement, indirect procurement had received far less attention on an organizational level and the use of IT, resulting in little process
standardization and a majority of paper-based activities. Historically, indirect goods have been procured
manually via phone, fax, and traditional mail. Only
with the advent of affordable, often Internet-based
applications, did companies become aware of the time
and cost saving opportunities in this area, with the
result of numerous reengineering and e-procurement
projects [5,17,20,22].
In simplest terms, e-procurement is commonly
defined as an organization’s procurement using the
Internet technologies. In contrast to direct procurement systems, e-procurement systems concentrate on
indirect procurement. E-procurement systems are well
suited to support and automate indirect procurement in
new and many ways, which can yield significant
efficiencies, time and cost savings at all levels across
an enterprise, resulting in self-service transactions for
end-user purchasing and empowerment, and reduced
maverick buying.
There is a tremendous amount of literature on
e-procurement systems or business models. Among
them, the literature addressing e-procurement systems
along with procurement processes have focused on the
indirect procurement at high-level granularities only
(e.g. [2,4,7,8,15,16,19,23–25,27,35,37,39,43,44,46–
50,54,60]) or at a low-level granularity only (e.g.
[3,9,17,30,31,36,41,42]), either within a single eprocurement system/business model or without matching between procurement processes and different
e-procurement systems.
However, a single information system cannot
meet all the business requirements, and not all

e-procurement systems or business models for indirect
procurement are equally suitable for supporting different business processes. That is, one size does not
fit all. Therefore, a buying company needs to adopt
various e-procurement systems or business models
in a hybrid and seamless manner. But one of the
difficulties is to find, deploy, and utilize the right
solutions to the right places. In this paper, we provide
the matching between indirect procurement process
and different e-procurement systems, at the highlevel and low-level activities, in order to identify
and articulate the areas where various e-procurement
systems can be utilized in a hybrid and seamless
manner.

2. Procurement processes
While there is generally consensus on what a business transaction is all about and whom it involves,
the approaches to delineate their sequence show some
variety. As with any definition, the task largely
depends on the research objective and perspective that
is taken. In the literature, procurement transaction
processes have been generally defined along two
different ways.
Many academic researchers provided similar process models at higher-level granularities as shown in
Fig. 1 (e.g. [2–4,7,8,15,16,19,23–25,27,35,37,39,43,
44,46–50,54,60]). Among them, the most representative model is a four-phase model (information, negotiation, settlement, and after-sales) extended from the
three-phase model provided by Schmid [48]. Gebauer
and Scharl [16] reviewed the literature on the transaction process models, and depicted a figure showing an
overview and contrasts between them (from 1 to 6
in Fig. 1). In this paper, we extend the Gebauer and
Scharl’s figure by adding the last two models (i.e. 7
and 8 in Fig. 1) and with more references than those
by Gebauer and Scharl.
Those models can be well suited to conceptualize
the transaction processes between the buyers and suppliers of e-marketplaces as well as the internal activities of the buyers at a high-level granularity.
However, they do not make a distinction between
contracted buying and off-contracted buying processes, and need additional work for the process
decomposition or matching their distinct phases

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

155

Fig. 1. Procurement process models (extended from [16,34]).

consistently with the lower-level activities actually
performed by the purchasers (i.e. end-users and purchasing personnel).
The activities performed in each phases need to be
coordinated. In a general sense, there are two possible
ways to coordinate between a selling and buying
organization: bilateral (one-to-one, direct, or end-toend) contact between buyers and sellers or multilateral
(or many-to-many) contact using intermediaries like
e-marketplaces. With transactions typically involving
a large amount of information processing and communication, procurement is well suited for information technology support and automation throughout
all the steps of a transaction. However, prior to the
advent of the Internet, available IT systems either
supported the information phase or automated operational activities during the settlement phase. Systems
were often proprietary and not very interactive. Little
IT supports can be found for the negotiation phase
as well as for capital and maverick procurement
[11,14,56]. Internet and web-based systems provide
support for the fields throughout all the steps of a
transaction.
On the other hand, most business practitioners in the
area of commercial e-procurement systems (with the
narrow meaning of the e-procurement systems building at the buying organizations) and other general
purchasing literature focused on the buyer’s internal

activities or scenarios at a lower-level granularity only
(e.g. [3,9,17,30,31,36,41,42]). In this paper, we make
a distinction between contracted buying and off-contracted buying processes, and also decompose the
high-level procurement processes or match their distinct phases consistently with the lower-level activities
through the effort for matching various e-procurement
systems with procurement processes in Section 4.

3. E-procurement systems or business models
for indirect goods
In a narrow sense, e-procurement systems can be
defined as the web-based systems building at the
buying organizations, i.e. buyer-centric (buy-side,
buyer-managed, buyer-focused, buyer-specific, or
buyer-oriented) e-procurement systems such as intranet (internal, desktop, or end-user’s) e-procurement
systems and buy-centric private e-marketplaces managed by a single buyer. However, e-procurement is
commonly defined as an organization’s indirect procurement using the Internet as mentioned earlier,
and procurement is the concept closely inter-related
with the supplier’s selling activities. Therefore, we
consider e-procurement systems as various Internetbased B2B (business-to-business) commerce (trading
or buying-and-selling) systems, which are located at

156

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

the buyer, the supplier or the third party, with the
following taxonomy:
 buyer-centric e-procurement systems, e.g. intranet
e-procurement systems, buy-side private e-marketplaces, and buy-side consortium e-marketplaces;
 supplier-centric e-procurement systems, e.g. e-storefronts (sell-side private e-marketplaces, web storefronts, virtual storefronts, online shops, or merchant
servers) and sell-side consortium e-marketplaces;
 neutral e-marketplaces, e.g. independent, thirdparty e-marketplaces;
 end-to-end electronic document/message exchange
systems, e.g. Internet-enabled EDI, XML/EDI,
extranet, standard-based messaging systems (i.e.
XML-based e-business framework such as ebXML
and RosettaNet), and e-mail.
In addition to the above categories, other supporting
systems such as intelligent search agents and independent workflow applications will be considered in
the alignment of procurement processes with e-procurement systems. In particular, there is an amount of
confusion about what the rapidly and continuously
evolving e-marketplace business models are, where
they add most value, and how profitable and defensible
are they likely to be. There are also wide variety of
synonyms for e-marketplaces: electronic marketplace,
electronic market, electronic intermediary, cybermediary, net market, net marketplace, digital marketplace, virtual marketplace, procurement marketplace,
market maker, exchange, trading exchange, digital
exchange, e-hub, trading hub, market site, market
space, trading networks, fat butterflies, vortex, and
market tone. Moreover, there exist many different
categories, taxonomies, classifications, typologies, or
types of the e-marketplace and e-procurement business
models, based on different viewpoints, criteria, or
dimensions. It is becoming increasingly difficult to
force them into a single category. This taxonomical
issue on business models is outside the scope of this
paper.
In this paper, we use the term e-marketplace focusing on trading or commerce rather than focusing on
collaboration or collaborative supply-chain management for direct goods via e-marketplaces (i.e. collaborative commerce, c-commerce, or collaborative emarketplaces) [13]. In this section, we briefly examine
the pros and cons of various e-procurement systems

from the buyer’s perspective and with the focus on
their usages for indirect procurement, but our examination is not intended to be exhaustive.
3.1. Buyer-centric e-procurement systems
With buyer-centric e-procurement systems, requisitioners or purchasers access multi-vendor catalogs in a
buyer-specific format from a web browser, select the
items, and initiate the requisition that is processed for
approval (if required) through workflow. Approved
requisitions are turned into purchase orders flowing
directly to the supplier via EDI, extranet, XML, e-mail
or fax, or being passed to the legacy purchasing system
(e.g. ERP), or being transmitted to the buy-side private
or consortium e-marketplaces through which they
will be sent to the appropriate suppliers in specified
individual formats. Some systems assist with RFQ
(request for quotation), bidding, and reverse auction
processes to select a preferred supplier(s). As these
applications empower end-users to perform individual
buying operations in accordance with corporate buying rules, they allow the purchasing department to
reduce their administrative workloads and to focus on
strategic activities [17,18,26,52].
In addition to the limitation of setting up and
maintaining a separate communication linkage to each
supplier, the major challenge is associated with managing the content of multi-vendor catalogs [5]. Similar
to traditional EDI applications, early e-procurement
projects have been undertaken by large organizations.
But, differently form the EDI that are not integrated
with electronic catalogs, large buyers are often the
ones that take the initiative to set up multi-vendor
catalogs based on their preferred specifications.
Unfortunately, no common standard has yet emerged
for web catalogs, which is made more complicated by
the absence of standard taxonomy. Small suppliers
often end up having to provide and regularly update
catalog data in a number of different formats to meet
each buyer’s specifications. Whereas this approach is
satisfactory for small numbers of buyers or suppliers,
it is not scaleable to many buyers or suppliers. With a
large company, there may be hundreds or thousands
of suppliers, or in the case of a supplier, hundreds or
thousands of customers. Each supplier may have
thousands of catalog items. There will be equivalent
goods from alternative suppliers that need to be

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

described consistently to simplify sourcing decisions.
Such volume and complexity becomes impossible to
manage [26]. The onus of the regular maintenance of
multi-vendor catalogs is divided between the suppliers
(who need to keep sending the latest data feeds in
the buyer-specific format) and the buyer (who takes
responsibility of loading the supplier feeds into the
intranet database). This introduces problems of data
integrity, when buyer organizations lag in loading the
latest feeds from the suppliers [59].
The specialized service providers can offer the
buyers an outsourcing agreement for the multi-vendor
catalog management or catalog hosting. A third party
could host the catalogs of approved suppliers, and
display to the buyer only those products on the
approved list. The third party would not necessarily
mirror supplier’s catalogs, but would provide the
meta-catalog service that brings together the approved
suppliers’ products so that they are easily retrievable
by a buyer. For example, a buyer typing the keywords
on the screen would be presented with a set of
suppliers along with a summary of their offerings.
These would in turn point to the catalogs of the actual
suppliers [38].
3.2. Supplier-centric e-procurement systems
An alternative for catalog management is the distributed catalogs (or punch-out enabled catalogs) whereby
suppliers host their own catalogs, sometimes in a buyerspecific format, on their own sell-side e-procurement
systems. The suppliers may have them searched from
the buyer-centric e-procurement systems, some large
buyers’ ERP systems, third-party e-marketplaces,
vendor/dealer extranets, trading partner networks,
or a central search engine provided by a specialized
third-party service provider. The entries (or a list of
approved suppliers) in the buyer’s e-procurement systems would be hyper-linked to the catalogs of endorsed
suppliers. Suppliers must be able to recognize and
validate buyers, and having done so, display only those
products that form part of the pre-established contract
with that buyer along with the pre-negotiated prices.
This method requires a supplier to have many different personalities (or customizations) on the web
[12,32,33,38,40].
The main advantage for the buyers lies in the
easy access to the suppliers’ catalogs without major

157

investments in a specific solution for their own purposes. Buyers can independently check on an order or
an invoice, confirm pricing, and access technical
support or product configuration service. Sometimes,
auctions are provided for excess or surplus inventories
[32,33,40]. However, buyers have to create multiple
accounts and passwords, navigate through numerous
different interfaces, and take a significant amount of
time to perform a function that was not really their
primary work. Additional drawbacks are no catalog
standard, no easy comparative shopping, no or limited
integration with internal master data (e.g. human
resources and finance), relying on suppliers for
reports, and limited ability to control where requisitioners go to make purchases [5].
3.3. Third-party e-marketplaces
Third-party e-marketplaces are the neutral, independent places between buyers and suppliers. The
economic leverage provided by competition has been
most beneficial in the traditional markets as well as
e-marketplaces where the products or services are
commodities or near commodities, which means
that the goods are well standardized or consistently
specified, e.g. indirect goods.
E-marketplaces can create value by two fundamentally different transaction mechanisms: aggregation
(or static pricing) and matching (or dynamic pricing)
[7,8,10,23,24,28,29,51]. The aggregation mechanism
(e.g. catalog aggregation and demand aggregation) is
static in nature because prices are pre-negotiated and
fixed. The matching mechanism (e.g. auction, reverse
auction, bid, exchange, and one-to-one negotiation)
brings buyers and sellers together to negotiate prices
on a dynamic and real-time basis.
There are also several synonyms for the transaction
mechanism: market mechanism, market-making
mechanism, match-making mechanism, pricing
mechanism, price-setting mechanism, price-negotiation mechanism, price-discovery mechanism, (buyer–
seller) interaction mechanism, (market-based) trading
mechanism, and bargaining mechanism. In some
literature, the term ‘‘mechanism’’ was replaced by
the term ‘‘model’’.
The third-party catalog enables huge scale economies in catalog management because there is only one
catalog to manage rather than a catalog in every buyer

158

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

and supplier organization. Catalog content and format
can be normalized to assist buyers in their product
comparison. Catalogs can be updated daily or more
frequently with little overhead. The thirty-party
e-marketplace enables the processes, such as price
and availability checks and daily content changes, that
would otherwise be impractical in a many-to-many
environment where the buyers’ catalogs were updated
over a network from each supplier [26].
However, static prices based on catalogs are often
referred to as sticky because they are typically slow to
adjust to market conditions, thereby causing a gap
between the price that is charged for a good and its
actual market value. Considering the proliferation of
different product SKUs (stock keeping units), the use
of static pricing is not surprising because a tremendous
amount of effort is required to determine how to price
each item and how to keep pricing catalogs current.
But static pricing provides only a glimpse of the
overall market demand, conveying only whether or
not there was a buyer at a given price. As a result,
businesses spend a substantial amount of money on
market research to help them understand and forecast
demand. Enabled by the Internet, dynamic pricing
solves the dilemma of sticky prices by providing
virtually instantaneous knowledge about market
demand, such as how many bidders exist, who they
are, how motivated they are, and the prices and
quantities they desire [1,21].
3.4. Electronic document or message
exchange systems
In the initial phase of e-commerce from the mid1990s on, leading companies established extranet
connections with their suppliers and customers. An
extranet uses Internet protocols and is a securityenabled replica of the closed, proprietary EDI
network. Whereas EDI automated paperwork, extranets went considerably further, providing a secure
private electronic environment for real-time communication up and down the supply chain. Many
companies now run more than one extranet and a
recent development is the so-called enterprise portal,
which combines extranets so as to provide one integrated entry point for interaction between large companies and their partners, either for buying or selling
[45].

In order to overcome the proprietary nature of EDI
and extranet, XML (eXtensible markup language)
is rapidly becoming a common standard for the
exchange of B2B messages or documents. There
are several XML-based standard development efforts
resulting in e-business or B2B frameworks—generic
templates that provide interoperability enabling
businesses to communicate efficiently over the Internet: XML/EDI, ebXML (electronic business XML),
RosettaNet, BizTalk, eCo, UDDI (universal description discovery and integration of business for the
web), OBI (open buying on the Internet), OTA (open
travel alliance), and cXML (commerce XML)
[53,58].
Those electronic document or message exchange
systems are established for the ordering of direct and
indirect goods under negotiated contractual arrangements. Those systems are also preferred to fax, e-mail,
and traditional mail for sending purchase orders,
invoices and payments as well as for the RFQ/RFP/
RFI/RFB (request for quotation/proposal/information/
bid) exchanges for direct bilateral negotiations or
bidding with pre-established suppliers in a secure
end-to-end environment.
3.5. Other supporting systems
Intelligent search agents are able to help buyers
extend information search and gathering capabilities.
They can independently search suppliers’ web sites
and e-marketplaces, gather the information on suppliers, products, pricing and offerings (e.g. auctions), and
report changes to them with no human intervention.
This greatly improves the buyer’s ability to react
to pricing and availability changes as they occur,
regardless of where in the world those changes occur.
However, particularly at the current development
stage of electronic catalogs, the automatic search of
multiple catalogs by intelligent search agents is a
difficult problem because of the absence of a common
standard for electronic catalogs.
Workflow systems support the automatic approval
processes for purchase requisitions and purchase
orders based on the organization’s policy and business
rules. They are usually either embedded in the buyside e-procurement systems and back-office purchasing systems, or provided as a separate application such
as Intranet and groupware.

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

4. Matching indirect procurement process
with e-procurement systems
In this section, we present the matching between
procurement processes and various e-procurement
systems in Table 1 for identifying and articulating
the areas where various e-procurement systems can be
utilized in a hybrid manner, which is followed by their
descriptions. In Table 1, we not only make a distinction between contracted buying and off-contracted
buying processes, but also decompose the high-level
procurement processes or match their distinct phases
consistently with the lower-level activities as pointed
out earlier.

159

4.1. Information phase
4.1.1. Item search
To purchase indirect goods or services using eprocurement systems, a purchaser (usually, an enduser employee) begins with pre-contracted catalogs,
which is preferred for the strategic procurement. That
is, he or she accesses a buy-side e-procurement system
(i.e. an intranet e-procurement system, a buy-side
private e-marketplace, or a buy-side consortium
e-marketplace) in order to browse buy-side catalogs
or sell-side catalogs hyper-linked from the buy-side
e-procurement system. If the purchaser finds the
required items in the pre-contracted catalogs, he or

Table 1
Matching indirect procurement process with different e-procurement systems
High-level process

Low-level process

E-procurement system

Information

Pre-contracted item search

Buy-side e-procurement systems (intranet e-procurement system, buyside private e-marketplace, buy-side consortium e-marketplace),
hyper-linked sell-side e-procurement systems (e-storefronts, sell-side
private e-marketplaces, sell-side consortium e-marketplaces)
Third-party e-marketplaces, content and community portals,
suppliers’ advertising web sites
Buy-side e-procurement systems (shopping cart), sell-side
e-procurement systems (shopping cart)
Buy-side e-procurement system (workflow), back-office purchasing
system (workflow), e-mail, Intranet, PDA
Buy-side e-procurement system (workflow), back-office purchasing
system (workflow), e-mail, Intranet, PDA

Off-contracted item search
Item selection
Purchase requisition
Requisition approval (for a
non-standard requisition)
Negotiation for off-contracted
buying (if required)

Sell-side catalog, auction,
group buying
Bilateral negotiation
Exchange
Reverse auction, bid
Offer-to-buy posting

Settlement

PO generation
PO placement

Order tracking and receiving

Invoicing and payment
After-sales

Transaction analysis

Sell-side e-procurement systems, third-party e-marketplaces, buy-side
consortium e-marketplaces (group buying only)
Electronic document/message exchange systems (Internet-enabled
EDI, extranet, XML, standard-based messaging systems, e-mail)
Third-party e-marketplaces
Buy-side e-procurement systems, third-party e-marketplaces
Third-party e-marketplaces, content and community portals
Buy-side e-procurement systems (workflow), back-office
purchasing systems (workflow)
Buy-side e-procurement systems, sell-side e-procurement systems,
third-party e-marketplaces, electronic document/message exchange
systems
Buy-side e-procurement systems (order tracking), sell-side
e-procurement systems (order tracking), e-logistics, third-party
e-marketplaces
Buy-side e-procurement systems (e-payment), back-office
purchasing system
Buy-side e-procurement systems, back-office purchasing system,
e-mail, helpdesks, e-manuals, data warehousing

160

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

she passes them into the shopping cart of the internal
(buy-side) or external (sell-side) e-procurement system
(i.e. e-storefronts, sell-side private e-marketplaces,
sell-side consortium e-marketplaces). Otherwise, the
purchaser relies on off-contracted catalogs at the thirdparty e-marketplaces, portals, or suppliers’ advertising
web sites, sometimes with the help of intelligent search
engines.
4.1.2. Item selection
If the items are found from the buy-side or sell-side
catalogs, the purchaser determines the items to buy
within the shopping cart at either buy-side or sell-side
e-procurement systems. This contains a list of selected
items, the company-specific pricing per item, and the
total cost of the order. The purchaser is then presented
with the checking-out of the shopping cart along with
a pre-filled form containing standard profile information such as name, shipping instructions, account
authorization, and possibly payment mechanism. In
the case of using the sell-side e-procurement system, it
is processed through a series of transactions involving
the supplier’s merchant server, catalog server, backend accounting system, and inventory system. Else if
the items are selected from the other sources, the
purchaser goes directly to the purchase-requisition
process of the buy-side e-procurement system.
4.1.3. Purchase requisition
When the shopping cart is checked out, any needed
approvals for purchase requisitions are processed
using the workflow of the buy-side e-procurement
system or the back-office purchasing system (e.g.
ERP), e-mail, or intranet. Even remote-site employees
can place requisitions quickly and easily. Remote
ordering can also be extended to employees literally
in the field, allowing employees using hand-held
devices like PDA (personal digital assistants) and
wireless transmission to request orders and specify
delivery location. This technology provides tremendous opportunity to improve both the buying organization’s ability to serve employees and the employee’s
ability to serve customers.
4.1.4. Requisition approval
If any requested items either exist in the pre-contracted catalogs, but fall outside pre-approved categories, or are selected from the off-contracted sources,

then the requisition (i.e. a non-standard requisition)
is routed for approval. Otherwise, the requisition (i.e.
a standard requisition) is immediately turned into a
purchase order or multiple purchase orders. The
approvals are based on user or cost-center limits, at
the order level or in aggregate.
The buy-side e-procurement system can provide the
automatic requisition approval based on authorization
rules. A rule-based user profile contains the definitions
of the type of goods being ordered, supplier profiles and
other variables. This automated requisition-approval
process affects the number of purchasing staff dedicated to review and approve employee requisitions to be
greatly reduced, and requisitions approval becomes one
of exception management. With the human intervention
required only for managing the approvals that are
exceptions to established rules, purchasing departments
are increasingly able to address strategic issues.
4.2. Negotiation phase for off-contracted buying
(if required)
The standard requisition (i.e. the pre-approved requisition for pre-contracted items) or the approved, nonstandard requisition for pre-contracted items is directly
turned into the purchase order in the e-procurement
systems or in the back-office purchasing system (e.g.
ERP). For the approved, non-standard requisition for
off-contracted items, the purchasing personnel (or
purchasing agents) may choose one or many of the
following negotiation mechanisms for the best price:
 online shopping such as sell-side catalog-based
shopping with price comparisons, auctions (forward
auction or English auction), and group buying
(buyer aggregation, demand aggregation, or power
buying) at the sell-side e-procurement systems or
the third-party e-marketplaces;
 negotiation with pre-established partners through
bilateral direct negotiation on price using electronic
document or message exchange systems;
 exchange service (mutual auction, two-way auction, double auction, bid-and-ask, or mutual
exchanges of offers-to-buy and offers-to-sell) at
the third-party e-marketplaces;
 other dynamic pricing mechanisms such as reverse
auction (competitive bidding, open bidding, or
Dutch auction) and bidding (RFQ, sealed bidding,

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

or tendering) at the buy-side e-procurement systems
or the third-party e-marketplaces;
 the offer-to-buy posting (post-and-browse, trade
directory, or bulletin board) services at the thirdparty e-marketplaces or the content and community
portals.
4.3. Settlement phase
4.3.1. Purchase order generation and placement
If the items requested should be supplied from
multiple suppliers, the purchasing requisition is split
into several purchase orders with the buy-side eprocurement system or the back-office purchasing
system. The buy-side e-procurement system can automatically verify and approve the purchase order for the
standard requisition or the approved, non-standard
requisition for pre-contracted items. It verifies product
availability and prices by checking against the catalogs. If a product is not available or cannot be delivered by a preferred supplier, the system automatically
checks the catalogs of other pre-approved suppliers,
locates the product, verifies that pricing is within the
purchasers’ stated parameters and that delivery terms
are acceptable. After verification and approval, the
system can automatically place a purchase order with
the supplier for fulfillment.
For those suppliers who have a sell-side e-procurement system, the purchasing personnel can directly
place orders at the suppliers’ web sites. If the buyer and
the suppliers are subscribing the same third-party
e-marketplace providing automatic settlement functionality, he or she can place orders through the
e-marketplace. For the other suppliers, orders can be
sent in a format that is acceptable to these suppliers via
electronic document or message exchange systems.
4.3.2. Order tracking and items receiving
Once an order is placed, the supplier receives the
approved order and fulfils the order. Both buy-side and
sell-side e-procurement systems can track the order
status and notify the purchaser of any delays in
delivery until receipt of an invoice from the supplier.
The purchaser is able to monitor progress of the order
by the notification from the e-procurement systems, by
accessing the appropriate web pages of the e-procurement systems, or by querying on-line the shipping
company used via e-logistics systems.

161

4.3.3. Invoicing and payment
Integration with a company’s back-office purchasing system (financial system or ERP) allows supplier
invoices to be automatically matched against purchase
orders, which replace manual checking of documents
with a much smaller exception report. The exception
report includes only those invoices for which no
purchase order can be identified. All the other successfully matched invoices are paid automatically.
Payment can be executed with traditional ways or
e-payment systems such as electronic checks, p-card
(procurement or purchasing card), EFT (electronic
funds transfer), and electronic monthly invoice, with
off-line payment, as per normal account-based procurement.
4.4. After-sales phase
After a transaction has taken place, the buy-side eprocurement system or the back-office purchasing
system stores the transaction data to accumulate buying history, assess supplier performance, analyze
internal buying pattern, provide the basis for consolidating corporate buys, better leverage price negotiation with suppliers who provide greater service levels,
improve future bargaining positions with suppliers,
and gather information for management reports.
At the buyer side, the information flow is often split.
While the purchase data is stored with central procurement, the end-user keeps the product-related documentation. In case of unexpected irregularities, it is
often the end-user who contacts the supplier (e.g. to
request a repair). Without proper access to the transaction file, communication problems and delays can
occur. Capturing, storing, and managing data are vital
at this point. The electronic support of after-sales
activities ranges from simple e-mail services to automated helpdesks and sophisticated electronic maintenance manuals. Data warehousing applications
support the storing, accessing and processing of large
amounts of data.

5. Conclusion
E-commerce is not something that can be instantly
plugged into existing workplace, and implementing
e-procurement is not a simple matter. The new solution

162

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

will require changes, updates, replacements, and adaptations throughout the infrastructure. But changes go
beyond the IT environment to impact the entire enterprise. Successful e-procurement is often more attributable to the fundamental procurement aspects than
to the electronic aspects. Plans for new ways of doing
business, particularly when expressed using technological jargon, are generally not well received. It may
well require changes to the way people work as well as
the strategies of the organization. In most cases,
e-procurement systems or business models that will
be embedded into the procurement processes and
existing IT infrastructure are not well perceived by
the purchasers (i.e. end-users and purchasing personnel). Many reports have found that general business
employees other than technology division and lots of
SMEs (small and medium sized enterprises) are technologically illiterate.
Moreover, there is so much confusion about the
wide variety of e-procurement systems, business models, business process models, taxonomies, synonyms,
and standards due to the revolutionary and evolutionary nature of e-commerce. Therefore, it is necessary to
clarify the several e-procurement aspects in order for a
company to find, deploy, and utilize the right solutions
to the right places. In an attempt to help a buying
company make a right decision to realize and implement e-procurement systems and strategies, we have
tried to examine the e-procurement aspects from the
buying firm’s perspective. Then we have provided the
matching between indirect procurement process and
different e-procurement systems, at the high-level and
low-level activities, for identifying and articulating the
areas where various e-procurement systems can be
utilized in a hybrid and seamless manner. The output
of this paper can be customized and further extended.
This paper intends to help companies adopt various eprocurement systems or business models, redesign
their e-procurement processes and strategies, reshape
buyer–seller integration or relationships, or capture
requirements at the initial phase of e-procurement
implementation.
References
[1] K. Appell, C. Brousseau, Business-to-business dynamic
commerce, Montomery Research Ascet 2, 15 April 2000.
http://www.ascet.com.

[2] D. Chen, J.Y. Chung, OBI&XML standard based business to
business electronic business solution, in: Proceedings of the
International Symposium on Government and E-commerce
Development (ISGED), Ningbo, China, 23–24 April 2001,
pp. 35–41.
[3] P. Buxmann, J. Gebauer, Internet-based intermediaries—the
case of the real estate market, in: Proceedings of the Sixth
European Conference on Information Systems (ECIS’98),
Aiz-en-Provence, France, 4–6 June 1998.
[4] R. Clarke, EDI is but one element of electronic commerce, in:
Proceedings of the Sixth International EDI Conference, Bled,
Slovenia, June 1993.
[5] Compaq Inc., E-procurement: sharpening the competitive
edge through implementation of an e-procurement solution,
White Paper, 31 August 2001.
[6] P.D. Cousins, Supply base rationalization—myth or reality?
European Journal of Purchasing and Supply Management 5
(3–4) (1999) 143–155.
[7] Q. Dai, R. Kauffman, To be or not to B2B? An evaluative
model for e-procurement channel adoption, in: Proceedings of
the Fifth INFORMS Conference on Information Systems and
Technology (CIST 2000), San Antonio, TX, November 2000.
[8] Q. Dai, R. Kauffman, Business models for Internet-based
e-procurement systems and B2B electronic markets: an
exploratory assessment, in: Proceedings of the 34th Hawaii
International Conference on System Sciences (HICSS-34),
Maui, Hawaii, January 2001.
[9] D.W. Dobler, D.N. Burt, Purchasing and Supply Management, McGraw-Hill, New York, 1996.
[10] O.A. El Sawy, Identifying structural models of B2B
procurement exchanges, Final Technical Report, External
Acquisition Research Program, University of Southern
California, 30 August 2001.
[11] A. Foroughi, A survey of the user of computer support for
negotiation, Journal of Applied Business Research, Spring
(1995) 121–134.
[12] Forrester Research, Sizing Intercompany Commerce, Forrester Report on Business Trade & Technology, July 1997.
[13] R. Ganeshan, R.K. Marath, Web-enabling the supply chain: an
exploratory case study, in: T. Boone, R. Ganeshan (Eds.), New
Directions in Supply-Chain Management: Technology, Strategy
and Implementation, AMACOM Press, 2002, pp. 73–90.
[14] J. Gebauer, C. Beam, A. Segev, Impact of the Internet on procurement, Acquisition Review Quarterly 5 (2) (1998) 167–180.
[15] J. Gebauer, C. Beam, A. Segev, The use of emerging
technologies in procurement—state of the art and a look into
the future, in: Proceedings of the Ninth International
Conference of the Information Resources Management
Association (IRMA 1998), Boston, MA, 17 May 1998.
[16] J. Gebaur, A. Scharl, Between flexibility and automation: an
evaluation of web technology from a business process
perspective, Journal of Computer-Mediated Communication
(JCMC) 5 (2) (1999). Available at http://www.ascusc.org/
jcmc/vol5/issue2/gebauer.html.
[17] J. Gebauer, A. Segev, Changing shapes of supply chains—
how the Internet could lead to a more integrated procurement
function, Supply Chain Forum 2 (1) (2001) 2–9.

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164
[18] J. Gebauer, M. Zagler, Assessing the status quo and future of
B2B e-commerce, in: World Market Research Center’s
Business Briefing Series: Global Purchasing & Supply Chain
Strategies, December 2000, pp. 100–104.
[19] A. Goodchild, C. Herring, Z. Milosevic, Business contracts
for B2B, in: Proceedings of the CAISE’00 Workshop on
Infrastructure for Dynamic Business-to-Business Service
Outsourcing, Stockholm, Sweden, 5–6 June 2000.
[20] Web procurement 2000—status and directions: a case study
analysis, Granada Research Report, 2000.
[21] B. Gressens, C. Brousseau, The value propositions of
dynamic pricing in business-to-business e-commerce, Montgomery Research CRM Project 1, 15 January 1999. http://
www.crmproject.com/documents.asp?d_ID¼733#.
[22] P.L. Grieco, MRO Purchasing, Pt Publications, West Palm
Beach, Florida, USA, 1997.
[23] M. Grieger, Introducing electronic marketplaces for electronic supply chain management, in: Proceedings of the Sixth
European Logistics Association (ELA) Doctoral Workshop,
2001.
[24] M. Grieger, Electronic marketplaces: a literature review and a
call for supply chain management research, European Journal
of Operational Research 144 (2) (2003) 280–294.
[25] F. Griffel, M. Boger, H. Weinreich, W. Lamersdorf, M. Merz,
Electronic contracting with COSMOS—how to establish,
negotiate and execute electronic contracts on the Internet, in:
Proceedings of the Second International Enterprise Distributed Object Computing Workshop (EDOC’98), San Diego,
CA, 3–5 November 1998.
[26] D. Hughes, Assessing the value of electronic marketplaces
for business to business trading, in: Proceedings of the Second
International Conference of Electronic Trade in the CIS and
Eastern Europe Countries: Together to XXI Century (ET
2000), Minsk, Belarus, Palace of Republic, 18–20 October
2000.
[27] M. Janssen, H.K. Sol, Evaluating the role of intermediaries in
the electronic value chain, Internet Research: Electronic
Networking Applications and Policy 10 (5) (2000) 406–417.
[28] M.S. Jordan, The digital exchange: structure and methods,
White Paper, Trade Dynamics Corporation, 2001.
[29] S. Kaplan, M. Sawhney, E-hubs: the new B2B marketplaces,
Harvard Business Review May–June (2000) 97–102.
[30] G.E. Kersten, S. Szpakowicz, Modelling business negotiations for electronic commerce, Interim Report, IIASA
(Internal Institute for Applied Systems Analysis), March
1998.
[31] R. Koom, D. Smith, C. Muller, E-procurement and online
marketplaces, White Paper, Compact Inc., January 2001.
[32] S. Koushik, Understanding B2B e-business solutions, White
Paper, IBM Global Services, 2000.
[33] W. Kruger, The evolution of netsourcing business models,
White Paper, EIC-Partner Inc., 2001.
[34] R. Kraut, C. Steinfield, A. Chan, B. Butler, A. Hoag,
Coordination and virtualization: the role of electronic
networks and personal relationships, Journal of Computer
Mediated Communication 3 (4) (1998). Available at http://
www.ascusc.org/jcmc/vol3/issue4/kraut.html.

163

[35] U. Lechner, B. Schmid, Logic for media: the computational
media metaphor, in: Proceedings of the 32nd Annual Hawaii
International Conference on Systems Science (HICSS-32),
Maui, Hawaii, 7–10 January 1999.
[36] J. Leech, Entering the dynamic new e-procurement marketplace: a procurement director’s guide, White Paper, EJiva—
An iGATE Corporation Company, 2002.
[37] M.A. Lindemann, B.F. Schmid, Framework for specifying,
building, and operating electronic markets, International
Journal of Electronic Commerce 3 (2) (1998) 7–21.
[38] P. McCrea, Trends in electronic procurement, CSIRO
Mathematical and Information Sciences Report, December
1997.
[39] M. Merz, F. Griffel, T. Tu, S. Müller-Wilken, H. Weinreich,
M. Boger, W. Lamersdorf, Supporting electronic commerce
transactions with contracting services, International Journal
on Cooperative Information Systems 7 (4) (1998) 249–274.
[40] P. Nakache, What you need to know about purchasing online,
Harvard Management Update, July 1998.
[41] M.E. Nissen, The commerce model for electronic redesign,
Journal of Internet Purchasing 1 (2) (1997). Available at
http://www.arraydev.com/commerce/JIP/9702-01.htm.
[42] Osprey Systems Inc., E-procurement: how it works, what it
offers and how to begin, Plain Talk White Paper, 2000.
[43] J. Peiro, P. Steiger, Making electronic commerce easier to use
with novel user interfaces, Electronic Markets 8 (3) (1998) 8–
12.
[44] A.W. Rohm, G. Pernul, COPS: a model and infrastructure for
secure and fair electronic markets, in: Proceedings of the
32nd Hawaii International Conference on System Sciences
(HICSS-32), 1999.
[45] P. Rosson, Electronic trading hubs: review and research
questions, in: Proceedings of the 16th Annual Industrial
Marketing and Purchasing (IMP) Conference, University of
Bath, UK, 7–9 September 2000.
[46] A. Scharl, Evolutionary Web Development (Applied Computing), Springer, Berlin, 2000.
[47] A. Scharl, J. Gebauer, C. Bauer, Matching process requirements with information technology to assess the efficiency of
web information systems, Information Technology and
Management 2 (2) (2001) 193–210.
[48] B.F. Schmid, Electronic markets, Wirtschaftsinformatik 35
(5) (1993) 465–480.
[49] B.F. Schmid, Trends in electronic markets, in: Proceedings of
the International Symposium on Government and E-commerce Development (IGSED), Ningbo, China, 23–24 April
2001.
[50] B.F. Schmid, M.A. Lindemann, Elements of a reference
model for electronic markets, in: Proceedings of the 31st
Annual Hawaii International Conference on Systems Science
(HICSS-31), 6–9 January 1998, pp. 193–201.
[51] A. Segev, C. Beam, Brokering strategies in electronic
commerce markets, in: Proceedings of the First ACM
Conference on Electronic Commerce, 1999, pp. 167–176.
[52] A. Segev, J. Gebauer, F. Färber, The market for Internet-based
procurement systems, CITM Research Report WP-1040,
University of California, Berkeley, CA, 2000.

164

J.-I. Kim, D.L. Shunk / Computers in Industry 53 (2004) 153–164

[53] S.S.Y. Shim, V.S. Pendyala, M. Sundaram, J.Z. Gao,
Business-to-business e-commerce frameworks, IEEE Computer October (2000) 40–47.
[54] T. Skjott-Larson, H. Kotzab, M. Grieger, Electronic marketplaces and supply chain relationships, Industrial Marketing
Management 32 (3) (2003) 199–210.
[55] P.K. Sokol, EDI: The Competitive Edge, McGraw-Hill, New
York, 1989.
[56] P.K. Sokol, From EDI to Electronic Commerce: A Business
Initiative, McGraw-Hill, New York, 1995.
[57] R.J. Trent, R.M. Monczka, Purchasing and supply management:
trends and changes throughout the 1990s, International Journal
of Purchasing and Materials Management 34 (4) (1998) 2–11.
[58] F. Willaert, XML-based frameworks and standards for
B2B e-commerce, Thesis, Catholic University of Leuven,
Belgium, May 2001.
[59] Vinimaya Inc., Rapid integration of suppliers to various
e-procurement platforms, White Paper, 2001.
[60] H.D. Zimmermann, Business media: a new approach to
overcome current problems of electronic commerce, in:
Proceedings of the Fourth Americas Conference on Information Systems, AIS’98 (Association for Information Systems),
Baltimore, MD, 14–16 August 1998.
Joong-In Kim, PhD, is an associate
professor of Management Information
Systems at Hongik University, South
Korea. He received PhD in industrial
engineering at Arizona State University,
USA, in 1995, and BS and MS in
industrial engineering at Hanyang
University, South Korea. He worked
as a senior researcher at ETRI (Electronics and Telecommunications Research

Institute), South Korea. His research interests include e-business,
electronic commerce, supply chain management, and information
systems modeling and development.

Dan L. Shunk, PhD, is the Avnet chair
of Supply Network Integration and a full
professor of Industrial Engineering at
Arizona State University, USA. During
2002–2003 he is on a fulbright scholarship to the University College Cork in
Ireland. He was the former director of
the CIM Systems Research Center at
ASU. He is currently pursuing research
into global new product development,
model-based enterprises and global supply network integration. He
studied at Purdue University where he received his BS, MS and
PhD in industrial engineering in 1976. He was co-founder of the
US Air Force ICAM Program where he launched such industry
standards as IDEF and IGES, former manager of Industrial
Engineering at Rockwell, former manager of Manufacturing
Systems at International Harvester, and former vice president and
general manager of the multi-million dollar Integrated Systems
Division of GCA Corporation. He is a senior member of SME and
IIE. He won the 1996 SME International Award for Education, the
1999 and 1991 Industrial Engineering Faculty of the Year award,
the 1989 SME Region VII Educator of the Year award, served as
the CASA/SME chair in 1987, chaired AutoFact in 1985, and won
the 1982 SME Outstanding Young Engineer award. He has served
on the Manufacturing Studies Board of the NRC and has chaired
two National Research Council panels. Recently he participated in
the creation of the Next Generation Manufacturing Project. He is
the US Delegate to the global Intelligent Manufacturing Systems
Project.

Computers & Industrial Engineering 45 (2003) 167–193
www.elsevier.com/locate/dsw

The application of an integrated enterprise modeling
methodology—FIDO—to supply chain integration modelingq
Dan L. Shunka,*, Joong-In Kimb, Hee Yerl Namc
a

Department of Industrial Engineering, Goldwater Research Center, Arizona State University, Room 524,
Tempe, AZ 85287-5906, USA
b
Department of MIS, Hongik University, Yeongi-Gun, Chochiwon 339-701, Choongnam, South Korea
c
Motorola, Inc., Phoenix, AZ, USA
Accepted 20 February 2003

Abstract
Existing multi-view enterprise-modeling methodologies have been utilized for the modeling and integration of a
single company or within an enterprise, but they do not specifically address the techniques for inter-enterprise
modeling and integration. This paper presents the function, information, dynamics, and organization (FIDO)
multi-view enterprise-modeling methodology developed to support the understanding, analysis, and design of
business processes aligned with information systems for inter-enterprise integration between companies as well as
for a single company. The FIDO methodology includes an integrated modeling framework and a suite of modeling
techniques for enterprise modeling. We present the application of the FIDO methodology to analyze the
interactions and to establish the supply-chain process integration between primes and subs in the defense
electronics industry.
q 2003 Elsevier Science Ltd. All rights reserved.
Keywords: Enterprise modeling; Modeling methodology; Supply chain integration

1. Introduction
Large companies are forcing their subcontractors in the integrated product and process development
(IPPD) process occurring in supply-chain integration to be much more responsive and attempting to push
them toward a commodity provider status that is no risk, fixed cost, and low cost with instantaneous
delivery. On the other hand, small firms must somehow carve out a larger niche of a smaller pie to
q
This manuscript was processed by Area Editor Philip M. Wolfe.
* Corresponding author. Tel.: þ1-480-965-6330; fax: þ1-480-965-8692.
E-mail addresses: dan.shunk@asu.edu, d.shunk@ucc.ie (D.L. Shunk).

0360-8352/03/$ - see front matter q 2003 Elsevier Science Ltd. All rights reserved.
doi:10.1016/S0360-8352(03)00024-X

168

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

survive. They must position themselves in the IPPD process away from simply being a commodity
provider into a unique or strategic role to insure their position with their key customers. They wish to be
known for the knowledge they provide early in the design phase, allowing them to compete on more than
cost and reputation.
Clearly the supply-chain integration model for the IPPD needed for the agile business practices must
be able to represent prime-sub relationships, it must be capable of demonstrating how the functions and
information must be integrated, and it must show metrics of time, etc. Closer relationships between
partners need models that support processes which communicate across organizational boundaries.
These must complement traditional support for internal business processes. However, only a few
organizations that ‘go digital’ seem to recognize what effects this transition will have on their business
processes and on the organization as a whole. Since business processes are the heartbeats of the business,
it is important for organizations to understand them. One means of doing this is to create enterprise
models based on business processes (Wangler, Persson, & Söderström, 2001).
There is much in the literature regarding efforts and methodologies to improve enterprise performance
through the modeling and design of business processes: business process reengineering (BPR),
enterprise modeling (Liles & Presley, 1996; Sgegheo & Andersen, 1999; Whitman & Huff, 2001),
enterprise engineering (Liles & Presley, 1996; Presley, 1997; Presley et al., 1993), enterprise integration
modeling (Petrie, 1992; Presley, 1997), and integrated enterprise modeling (IEM). These efforts all
concern themselves with the configuration of processes to achieve enterprise goals, albeit with slightly
different focuses. A common thread through these approaches is the use of process models, as an aid to
understanding and design (Presley, 1997).
IEM is synonymous with multi-view enterprise modeling (Liles & Presley, 1996; Presley, 1997),
holon-based or holonic enterprise modeling (Presley & Liles, 2001), and general-purpose modeling
methodologies. This requires that enterprise and inter-enterprise relationships be viewed or modeled
from several perspectives if it is to be fully described and understood (Barnett, Presley, Johnson, & Liles,
1994; Liles & Presley, 1996; Presley, 1997; Whitman, Huff, & Presley, 1998a,b).
Traditional process modeling methodologies typically emphasize one aspect or view of an enterprise
over others. Each aspect is modeled from vastly different conceptual perspectives. Separation of the
aspects of a business process into a number of separate models is an unnatural representation method
(Liles & Presley, 1996). A process can be looked at from various perspectives depending on the kind of
information required. Usually this will represent what activities comprise the process, what work is
going to be done, who is performing these activities, how and why are they executed, when and where
are these activities performed, and what data elements they manipulate. Modeling techniques differ in
the extent to which their constructs highlight the information that answers these questions. Hence,
previous research defines a number of such different views: integration definition (IDEF) (Menzel &
Mayer, 1997; Shunk, Sullivan, & Cahill, 1986), computer integrated manufacturing-open systems
architecture (CIM-OSA) (Bruno & Torchiano, 1999; Vernadat, 1992, 1996), Zachman framework
(Zachman, 1987) and its extension by Sowa (Sowa & Zachman, 1992), Curtis, Kellner, and Over (1992),
architecture of integrated information systems (ARIS) (Scheer, 1998, 1999), enterprise engineering
framework or ARRI (Liles & Presley, 1996; Presley, 1997; Presley et al., 1993), and Lin, Fan, and Wu
(1999).
According to the previous research, a process has multiple views or perspectives such as function (or
activity), information, behavior (or dynamics), organization (or structure), resource (or control),
business rules, decisions, goals, and actors (or people) views. Among them, the most common views are

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

169

function, information, dynamics and organization views (Curtis et al., 1992; Giaglis, 2001). However,
the existing methodologies that have been utilized for the modeling and integration of a single enterprise
or within an enterprise do not specifically address the techniques for inter-enterprise modeling and
integration.
In this paper, we present the FIDO (pronounced as ‘faido’), which stands for Function– Information–
Dynamics –Organization, IEM methodology developed to support for the understanding, analysis, and
design of business processes aligned with information systems for inter-enterprise integration between
companies as well as for a single company. The FIDO methodology includes an integrated modeling
framework and a suite of modeling techniques for enterprise modeling. The FIDO integrated modeling
framework and methodology provide a systematic means to simultaneously achieve three kinds of
integration: model integration of the different views, paradigm integration of the traditional and objectoriented (OO) paradigms, and the integration of the systems development life cycle (SDLC).
2. An overview of the FIDO modeling methodology
2.1. FIDO integrated modeling framework
There is a need for a totally integrated modeling framework that users and developers can agree upon,
that is robust enough to capture the very essence of the system, yet is simple enough that all can analyze
without being afraid of the complexity (Shunk, 1990). Fig. 1 presents the FIDO integrated modeling
framework.
The framework consists of three dimensions for integration: model integration, paradigm integration,
and integration of the SDLC. The model integration encompasses function, information, dynamics, and

Fig. 1. FIDO integrated modeling framework.

170

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Fig. 2. Current coverage of the FIDO methodology.

organization models. Since the model integration maintains consistencies between different models, the
effects of changes of one model can be identified on the other. The paradigm integration includes
structured and OO paradigms. Through the paradigm integration, we can take advantages of the benefits
from both of them. The SDLC integration is comprised of analysis, design, and programming. The
seamless integration of the SDLC allows systems analysts, designers, and programmers to easily work
together.
The FIDO methodology aims to completely integrate the function and information models in a unified
OO paradigm. Although it additionally integrates the structured dynamics and organization views with
the function and information models, the OO dynamics and OO organization views are not covered in
this methodology. Fig. 2 highlights those areas that remain as future work. In addition, the structured
design and programming are not included in the FIDO methodology because they are considered as
mature techniques to OO design and programming.
2.2. Objectives of the FIDO methodology
The FIDO methodology has four primary objectives. It is developed:
† To support for the understanding, analysis, design, and implementation of business processes aligned
with information systems for a single company or intra-enterprise integration as well as interenterprise integration between companies.
† To provide a comprehensive enterprise modeling methodology that integrates function, information,
dynamics (or simulation), and organization views. The methodology also attempts to overcome the
shortcomings of the existing methodologies while keeping the capabilities and characteristics of the
different views. The methodology intends to include a set of integrated modeling techniques. Each of
them will be comprised of graphical notations, step-by-step modeling processes, and abstraction

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

171

mechanisms that manage the complexity of large systems.
† To integrate the structured and OO paradigms by the integration of the IDEF0 function model and the
OO information model in such a way that will enhance and complement the values and ranges of the
applications of both paradigms. An integration of the robust IDEF0 function model and the OO
information model will provide a model-based alignment or coupling of business process analysis and
OO information systems design. Such a synthetic approach will provide a user with the best of both
worlds, so that both top– down decomposition and bottom– up refinement/aggregation approaches
may be followed in practice. An approach is not a panacea for all applications. Through the
integration of the IDEF0 and the OO approaches, they will complement each other.
† To cover the SDLC in an integrated fashion. However, this methodology currently focuses on the
modeling, that is, analysis and design. The set of modeling techniques included in the methodology
are used for analysis and design phases. Programming and implementation remain as future work.
2.3. Comparison of the FIDO and CIM-OSA frameworks
At first glance, the FIDO integrated modeling framework appears to be similar to the CIM-OSA
modeling framework (Bruno & Torchiano, 1999; Vernadat, 1992, 1996), which is shown in Fig. 3.
However, they are different. A brief description of the CIM-OSA modeling framework is presented
below, and is followed by the differences between the CIM-OSA modeling framework and the FIDO
integrated modeling framework.
The purpose of the CIM-OSA modeling framework (or CIM-OSA cube) is to provide a conceptual
framework to assist CIM users in developing a particular enterprise model. ‘Generic Architecture’ is
applicable to all manufacturing enterprises. ‘Partial Architecture’ aims at a specific industry segment
such as automotive manufacture and aerospace. ‘Particular Architecture’ models the structure and
design of a particular enterprise. These architectures support different phases of a CIM system life cycle.
It begins with analysis modeling of the system requirements defined by the business requirements. From

Fig. 3. CIM-OSA modeling framework.

172

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Table 1
Differences between FIDO framework and CIM-OSA framework
Different aspects

FIDO framework

CIM-OSA framework

Difference in the view
Difference in the dimension

Dynamics view
Dimension of the paradigm
integration (structured and OO)
All of them

Resource view
Dimension of the genericity
(generic and partial)
Not defined

Intra-enterprise (i.e. within the
enterprise) modeling

Intra-enterprise and inter-enterprise modeling

Integration aspects: model integration
paradigm integration SDLC integration
Modeling scope

these requirements, the system design, representing a method of implementation, is derived. The design
proceeds through components-buy or build, installation, verification, qualification, and release-to the
description of the implemented CIM system that produces an executable model.
The CIM-OSA framework provides four different modeling perspectives: function, information,
resource, and organization perspectives. It attempts to allow the user to view and optimize the different
aspects of the enterprise operation. The framework was accepted as a European pre-standard and a
general framework. However, the CIM-OSA framework does not support the inter-enterprise modeling.
It focuses on the multi-view enterprise modeling within an enterprise. Table 1 shows the differences
between FIDO framework and CIM-OSA framework.

3. Description of the FIDO modeling methodology
A model that is associated with a particular view should provide a medium for communication
between people with different professional backgrounds. Therefore a corresponding modeling language
should provide intuitive concepts that are, simultaneously suited to structure the problem domain in a
meaningful way. While those individual preferences are hard to identify-and may vary in a wide range, it
is a good idea to (re-) use existing concepts that have already proved themselves. Those concepts can be
found in specific terminologies that are common within a particular view (Frank, 2002).
Hence, the FIDO methodology incorporates existing robust, but independent, models such as the
IDEF0 function model, the OO information model, the SLAMII dynamics model, and the organization
chart into a comprehensive set of integrated modeling techniques with more syntax and semantics. The
new suite of integrated modeling techniques of the FIDO methodology consists of:
(1) FIDO1 (Extended IDEF0) modeling that integrates function and organization models for crossorganizational (i.e. intra- and inter-enterprise) business process analysis,
(2) FIDO2 (OOIDEF0: object-oriented IDEF0) modeling that integrates function, information, and
organization models for cross-organizational information systems analysis,
(3) FIDO3 (OOIDEF0 specification) that specifies the objects for the design of information systems
such as databases, repositories, and applications in a distributed client/server architecture, and
(4) FIDO4 (Dynamic IDEF0) modeling that integrates function and simulation models for performance
analysis and evaluation of the business processes.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

173

The FIDO methodology encompasses the required components of a generic modeling methodology
such as graphical/symbolic notations and representations, modeling processes (modeling steps), and
abstraction mechanisms (problem-partitioning mechanisms, complexity management) (Doumeingts &
Chen, 1992; Monarchi & Puhr, 1992). Additionally, it provides consistencies between different models
that are essential in an integrated modeling methodology.
The FIDO methodology uses a unified OO paradigm throughout the SDLC. It can be used with
any systems development model such as the traditional waterfall model, OO spiral model, and OO
fountain/incremental model. The methodology starts with the FIDO1 (Extended IDEF0) modeling for
the requirements description of the SDLC as well as business process analysis for BPR efforts; it is
followed by the FIDO2 (OOIDEF0 for OO analysis) models and then the FIDO3 (OOIDEF0
specifications for OO Design) in an integrated fashion. For BPR projects, after identifying As-Is and
To-Be business processes with the FIDO1 models, FIDO4 (Dynamic IDEF0) models translate the
FIDO1 models into the consistent SLAMII simulation models through a set of conversion
mechanisms in order to evaluate the performances. However, this does not mean that all these FIDO
models must be created. Models are developed to answer specific questions about the enterprise. If
the information modeled in a particular model is unnecessary to answer these questions, it may not
be necessary to create the model.
Abstraction mechanisms are generally performed via top – down decomposition and bottom– up
aggregation. The FIDO methodology incorporates both approaches. Additionally, it provides
consistencies between different models that are essential in an integrated modeling methodology.
3.1. FIDO1 (Extended IDEF0) for business process analysis
3.1.1. Graphical notations
FIDO1 consists of development of the Extended IDEF0 model of the companies and processes
involved, which is an extension of the classical IDEF0 methodology. System development starts with a
building block being the extended function box as shown in Fig. 4.
The extension of the traditional IDEF0 model is added in the following two ways: First, in addition to
the general input, control, output, mechanism (ICOM) elements of the IDEF0 methodology, FIDO1

Fig. 4. Generic building block of the FIDO1 model.

174

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

incorporates the following elements within the building blocks to describe the functions from the OO
information, organization and dynamic perspectives:
† ICOM objects. Traditional IDEF0 identifies the ICOMs as data, groups of data, and policies.
However, the ICOMs of the FIDO1 represent object classes (i.e. business objects) such as business
forms, documents, design drawings, materials, and products. For example, request for quote, purchase
order, blueprint, invoice, and product become the business objects. Therefore, FIDO1 modelers
should ask business people about the documents that include the data and policies, and should change
the data and policies into the business objects. This is not a difficult task, and some of the existing
modeling methodologies proposed the identification of the inputs and outputs of the functions as the
objects (Bailin, 1989; Barnett et al., 1994; Berard, 1990; Berzins, 1991; Bonney, Barson, Head, &
Huang, 1992). This approach for identifying ICOM flows as objects was easily applied and validated
through three real-world case studies that will be described in Section 4.
† Function processing time. Each processing time can be defined with either a deterministic value or a
probability distribution.
† Object owners. They represent the departments or persons that have the authorities and
responsibilities for the ICOM objects.
† Function owners. They represent the departments or persons performing the functions.
Second, in inter-enterprise modeling each diagonal of the FIDO1 model represents the business
processes of a company and has interfaces with other diagonals representing the business processes of
the related companies (Fig. 5). We call this the extended inter-enterprise IDEF0 model.
3.1.2. Modeling process and abstraction mechanism
The FIDO1 modeling process and abstraction mechanism allow for both the top – down
decomposition and bottom– up refinement approaches. The modeling starts with the context diagram

Fig. 5. Extended inter-enterprise IDEF0 model.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

175

or the top-level functions such as core processes (Batini, Lenzerini, & Navathe, 1986) through
interviews with business people and system users. The functional decomposition process continues until
the desired level of detail is identified. Because it is difficult to identify all exchanges between activities
at the higher levels of the model, lower level exchanges should be added to the parent activity in a
bottom– up fashion until all exchanges are included on all levels (Shunk et al., 1986). After individual
models have been combined and exchanges are consistent between the levels, the FIDO1 model should
be verified by the companies involved.
3.2. FIDO2 (OOIDEF0) for information systems analysis
3.2.1. Graphical notations
Although OO methodologies have different modeling activities during the analysis and design phases,
a consensus exists on the definitions of the analysis and design. Analysis is characterized by defining
what has to be done, and design defines how to specify the systems in the implementable forms.
According to the consensus, the FIDO methodology distinguishes OO analysis and design: OO analysis
(FIDO2 modeling) identifies objects, structures, and relationships between objects. A more broad
definition of the analysis may include the requirements description phase (FIDO1 modeling) for
information systems development. Structures and relationships are not actually implemented into
systems. They merely provide the base knowledge for specifying the objects in the implementable
forms. OO design (FIDO3 modeling) specifies objects in terms of attributes, class methods, and message
passing, which are implemented by OO languages.
Fig. 6 shows a generic building block of the FIDO2 (OOIDEF0) model for OO information analysis.
The FIDO2 modeling depicts ‘local’ OO analysis sub-models within the functions of the FIDO1 models
for business process analysis. The term ‘local models’ means the sub-models for problem sub-domain as

Fig. 6. Generic building block of the FIDO2 model.

176

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

compared to a global, enterprise-wide model or a whole problem-domain. Each function in the FIDO1
model becomes the scope (or subject, schema, subsystem) of each local OO analysis sub-model. For
example, the function ‘Design Product’ becomes the scope of the ‘computer-aided design (CAD)’
information system, and the function ‘Manufacture Product’ corresponds to the ‘computer-aided
manufacturing (CAM)’ information system. This scheme is similar to the ‘packages’ or ‘subsystems’ in
the unified modeling language (UML) (Rumbaugh, Jacobson, & Booch, 1999) model.
One of the advantages of the FIDO2 model is the ability to represent the interfaces between
local information models. These interfaces are consistent with the interfaces between function
objects of the FIDO1 models. The local information models are aggregated bottom– up into a
global information model following the function– object hierarchy. The local information models
can be used for the development of the distributed information systems. The global information
model can be used for the development of a centralized database system as well as for the
distributed systems.
In Fig. 6, (I), (C), (O), and (M), respectively, represent input, control, output, and mechanism
objects. In this paper, the term ‘object’ means the ‘class’ which includes different ‘instances’ of an
object. The FIDO2 methodology has such a flexibility that any graphical notations of the existing
OO methodologies (e.g. UML, OMT, and Coad and Yourdon’s OOA) can be used for the class
diagram. Fig. 6 demonstrates the use of the Coad and Yourdon’s (1991) notations.
Fig. 7 depicts the information and material exchanges (i.e. ICOM flows) between functions and also
between local OO analysis models that were represented as the same flows in the FIDO1 models. If the
FIDO1 modeling is done by using the extended inter-enterprise IDEF0 modeling for inter-enterprise
integration, then the corresponding FIDO2 modeling is also done by the inter-enterprise OOIDEF0
modeling. Since the FIDO1 models and the FIDO2 models have the same functions, the same ICOM
object flows between functions, and the same dynamic and organizational elements, they are completely
consistent and one-to-one mapped.
3.2.2. Modeling process and abstraction mechanism
The OOIDEF0 modeling uses bottom – up aggregation as shown in Fig. 8. It starts with defining
the local OO analysis models for the bottom-level functions, and aggregates the information submodels into the top-level functions along the functional hierarchy. Finally, it models a global
information model for an enterprise-wide level or a whole problem-domain.
During the bottom – up aggregation, redundant objects are removed, and additional structures and
relationships are identified. For example, an object coming out from one function and coming into
another function appears within both functions. After these functions are aggregated into a higher-level
function, the object will appear, without redundancy, in the higher-level function. Additional structures
and relationships will be identified because different objects within different functions at a certain level
may have structures and relationships between them, but they cannot appear at this level. They will
appear in a higher-level information model after aggregation. In addition, some objects are hidden within
functions because the ICOM objects represent only the visible interfaces between functions. Those
objects can be identified in the process of defining the additional structures and relationships. Those
objects are called ‘hidden objects’, in contrast to the visible ICOM objects, and are represented in the
model with a notation ‘(H)’ that means ‘Hidden’.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

177

Fig. 7. Consistencies between FIDO1 and FIDO2 models.

3.3. FIDO3 (OOIDEF0 specification) for information systems design
3.3.1. Graphical notations
The FIDO3 (OOIDEF0 specification) defines the specifications of the ICOM objects (in terms of
attributes and class methods) and their message passing for the design of information systems. A
seamless transition from analysis to design is accomplished by using the same ICOM objects, structures,
and relationships as the FIDO2 modeling.
In addition, functions of the FIDO1 models (and the FIDO2 models) can become a kind of object, that
is, the ‘function object’. The FIDO3 defines the specifications of the function objects as well as the
ICOM objects. The function objects contain information represented in the FIDO2 models and

178

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Fig. 8. Transitions from FIDO1 models to FIDO2 models.

the FIDO4 (Dynamic IDEF0) models. The FIDO3 that will be programmed into information systems is
expected to provide significant advantages to BPR and model management because they incorporate OO
benefits such as reusability, extensibility, and maintainability into the functions as well as into the ICOM
objects.
The ICOM objects, their structures, and relationships are already defined through the business process
analysis (FIDO1) and information systems analysis (FIDO2). Therefore, the OO design specifies the
ICOM objects in terms of their attributes (or data), class methods (or data procedure), and message
passing between objects. These specifications are performed from the global information model of the
FIDO2 modeling. In this paper, we omit the detailed description of the specification of the ICOM objects
because it is not different from other OO analysis modeling methodologies.
For the function objects, the function name represents object identity. The structures between function
objects are ‘composition’ structures because the function objects are usually decomposed into lowerlevel function objects (or sub-functions). There is no ‘inheritance’ structure between them. The
relationships between function objects are one-to-one relationships because all function objects are
‘abstract classes’ that do not have any object instance.
The function objects are specified in terms of attributes, class methods, and message passing as in the
ICOM objects. Attributes and class methods are specified by the bottom– up aggregation along the
functional hierarchy of the FIDO1 models. Going upward along the functional hierarchy, the function
objects at a higher level have more attributes and class methods than those at a lower level because a
higher-level function is aggregated from several lower-level ones.
However, it is necessary to clarify the differences between functions and class methods in order to
avoid confusion. A function (or function object) is the activity or transformation associating with
multiple ICOM objects. It usually requires several data (i.e. attribute values) and data procedures (i.e.
class methods) from different ICOM objects. A class method is the data procedure that changes the data
of only one object because it is encapsulated into an object. Thus, a function consists of the several class
methods scattered over different ICOM objects.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

179

The same kinds of class methods as the ICOM objects are used for the function objects. For
example, ‘create’ new outputs, ‘connect’ to other functions, ‘access’ inputs or controls, ‘update’
inputs, and ‘delete’ objects become the class methods of the function objects. However, they deal
with the ICOM objects instead of the data. Actual data processing for these class methods occur
through the corresponding class methods of the ICOM objects. The class methods of the function
objects just send messages to the corresponding ICOM objects to perform the related data
processing. The class methods of the function objects and those of the ICOM objects are basically
the same because both of them change the same data of the ICOM objects. However, the class
methods of a function object are represented as the aggregated set of the class methods of the
associated ICOM objects.
The information and material exchanges (that is, the ICOM object flows) between functions
can be thought of as the responses to the message passing from one function to another. The
sender functions request the required information and materials to the receiver functions. Then the
receiver functions respond to the requests (or messages). They pass the required information
and materials to the sender functions. Thus, the messages between the functions are passed in
reverse direction to the information and material exchanges between the functions in the FIDO1
models.
3.3.2. Relationships between ICOM objects and function objects
Fig. 9 shows the structures, relationships, and message passing between the function objects and the
ICOM objects. The composition structure exists between them because a function object is composed of
many associated ICOM objects. The relationships between them are many-to-many because a function
object is associated with many ICOM objects and, vice versa, an ICOM object may be related to different
function objects. Messages are passed in both directions. For example, the ‘create outputs’ class method
of the function object requests the output object to activate the ‘create instances’ class method, and vice
versa.

Fig. 9. Relationships between ICOM objects and function Objects.

180

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

3.4. FIDO4 (Dynamic IDEF0) for business performance analysis
3.4.1. Graphical notations
Before moving into the To-Be information systems design phase, it is necessary to evaluate As-Is and
To-Be business processes and information systems in order to get approval and commitment from top
management. Simulation can be used as a tool for that purpose (Ardhaldjian & Fahner, 1994). It can be
used for As-Is modeling as a tool for understanding, and for To-be alternatives as a tool for comparison.
We can evaluate the performances of the As-Is and To-Be business processes by the integration of the
function and dynamics models. This is possible because the functions of the dynamic models can be
found in the function models of the same level (van Slooten & Brinkkemper, 1993). The FIDO4
(Dynamic IDEF0) modeling translates the FIDO1 (Extended IDEF0) models into the consistent SLAMII
simulation models through a set of conversion mechanisms developed in this methodology.
The generic building block of the FIDO4 model shown in Fig. 10 is an additional extension of the
FIDO1 building block. It includes more dynamic parameters needed for simulations than the FIDO1
building block.
Within the function boxes of the FIDO1 model, the activity cost for activity based costing and the
error rate for quality measurement may be specified as well as the function– processing time. The flow
time and branching conditions of the input, output, and control flows are attached next to the flows.
Different types of orders (for example, customer orders, job orders, and so on) belong to control flows. If
they represent the simulation entities flowing through the whole system with attribute values to be
collected at the end of simulation, they are depicted as heavy lines that are separate from other control
flows. If material inputs represent the simulation entities, they are also represented as heavy lines to
distinguish them from other input flows. Output flows become either simulation entities (that is, the order
flows) or material flows. Functions are not always activated if all inputs, controls, and resources are
present. Some functions are processed only at a predetermined time. These periodic functions are
activated by the periodic condition or the arrival time-event, which is shown as a dotted line.

Fig. 10. Generic building block of the FIDO4 model.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

181

The resources for the functions are specifically represented in terms of the resource type and resource
capacity.
A precise conversion mechanism must be defined to translate the FIDO4 model into a consistent
SLAMII network simulation model. The mechanism should be simple, easy to understand, and accurate.
The proposed conversion mechanism consists of the nine types of generic building blocks of the FIDO4
model and corresponding SLAMII network model. In this paper, however, we omit the details of the
conversion mechanism due to the length of the paper.
3.4.2. Modeling process and abstraction mechanism
As-Is and To-Be simulation models can be developed at any level of abstraction according to the
functional hierarchy of the FIDO1 (Extended IDEF0) models. If we need to simulate the business
processes at all the different levels of abstractions, then bottom –up simulations will be performed.
Simulations for a lower-level business process provide more accurate results than the simulations at
higher levels. However, the higher-level simulations provide broader overviews than the lower-level
ones. For example, a simulation at the strategic, higher level processes (e.g. core processes) will give an
overview of the business processes with less accurate data for simulation. A decision depends on the
scope and objectives of the modeling projects or the nature of the problem domain to be modeled. This
really draws upon the initial model assumptions of purpose, viewpoint and context.
3.5. Consistencies between different FIDO models
Fig. 11 summarizes the consistencies between different FIDO models. The FIDO methodology
provides consistencies between different models by using shared (or common) modeling elements:
† The FIDO1 models and the FIDO2 models are consistent because they have the same functions, the
same ICOM object flows between functions, and the same dynamic and organizational elements.
† The FIDO4 models and the SLAMII simulation models are consistent because they include the same
functions, the same sequence of the functions, and the same dynamic parameters. Due to

Fig. 11. Consistencies between different FIDO models.

182

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

the consistency, we can simulate and evaluate the performances of the business processes from the
function models.
† The FIDO1 models maintain consistencies with the organization chart by the modeling of the function
owners and the object owners. The modeling of them exposes cross-organization interdependencies of
the functions as well as of the authorities and responsibilities.

4. Case studies
4.1. An application to inter-enterprise modeling
The FIDO methodology was applied to analyze the interactions and to establish the supply-chain
IPPD process integration between primes and subs in the defense electronics industry. The project
included two committed prime contractors (Motorola and Hughes Missile Systems Company) and two
key subcontractors (Precision Technologies and Catalina Manufacturing). Motorola and Hughes
Missiles had both been actively involved in the development of electronic data interchange (EDI)
systems, linking them with their two suppliers. The key to the common processes that both Motorola and
Hughes Missiles had been developing, was the use of IPPD systems centered around a common product
data management (PDM) system and common electronic communications tools such as EDI, e-mail,
white board, and video conferencing. The IPPD process ranged from the design and engineering, bid,
process planning, fabrication, assembly, and delivery of precision prototype machine parts. As a
minimum the following agile business practices in the supply-chain IPPD process were detailed and
developed:
† The usage of electronic communication tools to eliminate all time delays associated with on-site
product design reviews.
† The usage of electronic communication tools to address the needs for proximity in the notion of IPPD
process.
† The creation of a trust-based relationship that use a singular PDM as the technological hub.
Through a set of interviews with the companies involved and reviews on the organization charts, the
FIDO1 models (specifically, the extended inter-enterprise IDEF0 models) were created to map the crossorganizational interfaces, that is, both intra- and inter-enterprise interactions, through top – down
decomposition and bottom– up refinements. The context diagram (first level FIDO1) and the second
level FIDO1 model between Motorola and suppliers are shown, respectively, in Figs. 12 and 13. The
more detailed third level FIDO1 model was created. Then the FIDO2 model (specifically, the interenterprise OOIDEF0 model) corresponding to the third level FIDO1 models was developed. The models
were printed out on large sheets of paper and were hung on walls in order to enable participating people
to look at and effectively communicate around the models.
Due to the size of the third level FIDO1 and corresponding FIDO2 models and proprietary
information contained on them, these models are not presented in this paper. Instead, an excerpt from the
full FIDO1 and FIDO2 models showing the consistency between the FIDO1 and FIDO2 models of the
Motorola project are presented in Fig. 14. We take two functions out of one of the top– down FIDO1

183

Fig. 12. Context diagram—1st level FIDO1 model.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Fig. 13. 2nd Level FIDO1 model.

184

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

185

Fig. 14. An excerpt from the full FIDO1 and FIDO2 models.

models (the left portion of the figure) and show the corresponding FIDO2 model (the right portion of the
figure).
By the FIDO1 models that represent the interactions between the participating companies, it was
possible to identify the process redesign and improvement opportunities to reduce the time to
produce prototype parts such as non-value added activities and time-consuming activities, e.g. onsite product design reviews, that were the bottlenecks in the way these companies did agile
business practices. The non-value added activities were reconsidered to be eliminated. The timedelayed activities and the activities that data/documents interchange between companies occurred
were the opportunities for using the electronic communication tools. This conformed to the purpose
of addressing the needs for geographical and organizational proximity in the IPPD process.
In addition, participating people from different professional backgrounds and companies attached
different meanings to artifacts, and therefore used them in different ways and definitions for the
concepts, e.g. ‘process’, ‘activity’, ‘task’, ‘function’, and ‘work flow’. This diversity was a hurdle
for effective communications and eventually the process integration. Their respective terminologies
need to be aligned or at least understood. This could be done through the common FIDO1 interenterprise modeling, resulting in a description of the shared terminology.
The FIDO2 models consistent with the FIDO1 allowed business people (system users) and technical
people (system developers) to work together more easily than by using the independent, inconsistent
models. Business people preferred the easily understandable business process models (FIDO1) to the
OO information models (FIDO2) of which they were not familiar. They also became less reluctant to the
changes through the reviews on the FIDO1 models, shared understanding and communications between
the parties via the FIDO1 models. Technical people also took advantage of the combined use of the
FIDO1 and FIDO2 for the requirements capture and analysis for the development of a common PDM

186

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Fig. 15. Mappings between a common PDM architecture and FIDO models.

system than pure OO methodologies because of the shared understanding and easy communications with
business people and with system developers in other participating companies.
Although the design specifications for the PDM system by the FIDO3 was not finished during the
project experiments, a client/server architecture for the PDM system was prepared by the model-based
alignment or coupling of the FIDO models (i.e. FIDO1, FIDO2 and FIDO3) and PDM system
development. It is shown in Fig. 15.
The FIDO4 model was not developed because the time frame and goals of the project did
not require the sophisticated evaluation through simulation. Even though the impact of the
electronic communication tools could be quantitatively measured by using the FIDO4, it was
sufficient for the project to evaluate the performance by point-estimation. In order to provide
an example of the FIDO4 from the FIDO1 in this paper, we will present another case study in
Section 4.2.
4.2. An application to the enterprise modeling within a company
One of the other applications of the FIDO methodology was the development of enterprise-wide
integrated information systems of a manufacturing company. The company was originally
modeled using the IDEF0 function modeling and the IDEF1x information modeling by the teams
of graduate students. However, the integration of the function and information models was not
originally covered during the modeling because the FIDO methodology was not developed at
that time. In this project, the original IDEF0 models are converted into the FIDO1 models and
then the IDEF1x models are converted into the FIDO2 models. This project demonstrated the
feasibility of the conversion of the existing IDEF0 and IDEF1x models to the corresponding FIDO
models. In addition, for the applications of the other FIDO models, the FIDO4 models are
also derived from the FIDO1 models, and the FIDO3 specifications are defined from the FIDO2
models.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

187

Fig. 16. An example of the FIDO4 model.

In this case example we will focus only on showing the FIDO4 model, although full FIDO models
were developed, as mentioned in the previous case example. Fig. 16 presents the As-Is FIDO4 model,
derived from the FIDO1 model of the company, with more elements such as function processing time
and material and information flow time than the FIDO1. But, the other additional elements such as
resources, function owners and object owners are not included in the figure because the FIDO1 model
was converted from the original IDEF0 model that did not include those elements. The company has a
mixed form of the three types of order fulfillment: make-to-stock (MTS), make-to-order (MTO), and
assemble-to-order (ATO). The corresponding SLAMII simulation models translated from the FIDO4
model are shown through Figs. 17– 19.

188

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Fig. 17. (Continued) SLAMII simulation model translated from the FIDO model.

5. Conclusions and value proposition of the FIDO methodology
Existing multi-view enterprise-modeling methodologies have been utilized for the modeling and
integration of a single enterprise or within an enterprise, but they do not specifically address the
techniques for inter-enterprise modeling and integration. The FIDO modeling framework and
methodology is developed to support the understanding, analysis, and design of business processes
aligned with information systems for inter-enterprise integration between companies as well as for

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

189

Fig. 18. (Continued) SLAMII simulation model translated from the FIDO model.

a single company. The FIDO methodology can be applied to such areas as intra- and inter-enterprise
integration, information systems development, BPR, alignment or coupling of BPR and information
systems development, and quantitative evaluations of the business processes.
In particular, inter-enterprise integration requires that the goals and processes of an enterprise
must be managed and integrated along with those of different enterprises in a supply chain, virtual
enterprise, extended enterprise, and business-to-business (B2B) integration (Presley, 1997). Often,
enterprise integration refers to the technical issues such as automation of business processes,

190

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Fig. 19. (Continued) SLAMII simulation model translated from the FIDO model.

integration (more precisely, interoperability) of computer and information systems, enterprise
application integration (EAI), and B2B integration (B2Bi). Even if XML and messaging
standards for interoperability are flexible and extensible, it is not sufficient by itself for integrating
business processes within an enterprise or across multiple enterprises (Baum, Dessaux, & Talukdar,
2000; Bubenko, 1993; Nilsson, Tolis, & Nellborn, 1999; Wangler et al., 2001; Whitman et al.,
1998a).
B2B companies often have problems in making their internal IT systems communicate with
those of their suppliers, customers, and partners. These deficiencies may be partly caused by poor
understanding of the underlying business processes and their needs. Therefore, additional enterprise
models, e.g. FIDO1, focusing on relevant aspects of business processes are needed. The task
of developing and implementing effective and flexible solutions to enterprise integration requires a
high degree of understanding of the business operations in question and for the consequences
of integration. This understanding and modeling through FIDO1 and FIDO2 are of course
important to those developing the supporting software and information infrastructure. Enterprise
integration modeling can effectively be used to achieve such common and comprehensive
understanding since it is considered to provide a knowledge development process for all
participants.
In addition, the FIDO integrated modeling framework and methodology provide a systematic means
to simultaneously achieve three kinds of integration: model integration of the different models, paradigm
integration of the structured and OO paradigms, and the integration of the SDLC.
Since the model integration maintains consistencies between different models, the effects of any
changes of one model can be identified on the other. This will improve the time, cost, and quality
efforts required to maintain and modify different but inter-related models. The model integration
and consistency allow business people (or system users) and technical people (or system
developers) to work together more easily than by using the independent, inconsistent models. A
model that is associated with a particular view should provide a medium for communication
between people with different professional backgrounds. Therefore a corresponding modeling
language should provide intuitive concepts that are, at the same time, suited to structure the
problem domain in a meaningful way. While those individual preferences are hard to identify-and
may vary in a wide range, it is a good idea to (re-) use existing concepts that have already proved
themselves. In this context, the FIDO methodology incorporates, modifies, or extends the graphical
notations and modeling processes of the existing robust models such as the IDEF0 function model,
the OO information model, the SLAMII dynamics model, and the organization chart into a
comprehensive set of integrated modeling techniques.

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

191

The paradigm integration complements the structured and OO paradigms by the coupling of the
IDEF0 function model and the OO information model. Through the paradigm integration, we can take
advantages of the benefits from both of them. Especially in the requirements description phase of
the SDLC, the IDEF0 models are enhanced into the FIDO1 (Extended IDEF0) models, an OO extension
of the IDEF0. In the other phases of the SDLC, the FIDO methodology uses the proposed OO
information modeling approaches (that is, OOA, OOD, and OOP) that are integrated with the FIDO1
models. The paradigm integration also facilitates teamwork between business people and system
developers with different purposes and backgrounds, such as the function modelers, the simulation
modelers, and the OO information modelers. Furthermore, the paradigm integration allows business
people and system developers to build on the base of skill and understanding already established in
applying traditional approaches instead of discarding hard-won knowledge and ability for a newer set of
OO techniques.
In particular, OO multi-view modeling languages, such as UML and open modeling language
(Firesmith, Henderson-Sellers, Graham, & Page-Jones, 1996; Prasse, 1998), are designed mainly for
software development/software engineering, although the extension for business modeling has also
been developed (Frank, 2002; Giaglis, 2001). Furthermore, some may argue that they are heavily
based on the OO paradigm and hence may not be applicable in situations where the modelers want
to follow a more traditional modeling approach (Giaglis, 2001). In software systems development,
the full UML modeling process (e.g. rational unified process) could be used on its own. However,
it has been found that the UML needs extra support in the users’ requirements capture or
description stage. The requirements capture, that is mainly done with use case diagrams, needs the
designer to have a previous good understanding of the requirements for the system. Using the
information modeled with traditional function models, this understanding can be provided (Dorador
& Young, 2000).
Additionally, the UML models need a higher level of expertise to be able to understand them, which
makes communication with potential system users significantly more difficult. The initial use of
traditional process models, e.g. IDEF0 or FIDO1 (Extended IDEF0), is a better way for the understanding
and communication with collaborating companies than UML models. While the preferences of the people
with different professional backgrounds are hard to identify-and may vary in a wide range, it is a good idea
to (re-) use intuitive, existing concepts that have already proved themselves (Dorador & Young, 2000;
Frank, 2002). Furthermore, a collaborative approach allows system developers to build on the base of skill
and understanding already established in applying traditional approaches instead of discarding hard-won
knowledge together with the ability for a newer set of OO techniques.
Through the integration of the SDLC, a seamless transition from requirements description, analysis,
design and, finally, to programming is achieved. This allows systems analysts, designers, and
programmers to easily work together. In addition, using the same objects throughout the SDLC provides
the general benefits of the OO paradigm.

References
Ardhaldjian, R., & Fahner, M. (1994). Using simulation in the business process reengineering effort. Industrial Engineering,
July, 60 – 61.
Bailin, S. C. (1989). An object-oriented requirements specification method. Communications of the ACM, 32(4), 608– 623.

192

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

Barnett, W. D., Presley, A., Johnson, M. M., & Liles, D. H. (1994). An architecture for the virtual enterprise. IEEE International
Conference on Systems, Man, and Cybernetics, 506– 511.
Batini, C., Lenzerini, M., & Navathe, S. B. (1986). A comparative analysis of methodologies for database schema integration.
ACM Computing Surveys, 18(4), 323–364.
Baum, D., Dessaux, C., & Talukdar, N. (2000). e-Business integration. Oracle Magazine, September.
Berard, E. V. (1990). Object-oriented requirements analysis. Hotline on Object-Oriented Technology, 1(8), 9 – 11.
Berzins, V. (1991). Software engineering with abstraction. Reading: Addison-Wesley.
Bonney, M. C., Barson, R. J., Head, M. A., & Huang, N. (1992). UNISON—a tool for enterprise integration. First International
Conference on Enterprise Integration Modeling, 239– 248.
Bruno, G., & Torchiano, M. (1999). Making CIMOSA operational: the experience with the PrimeObjects tool. Computers in
Industry, 40(2 – 3).
Bubenko, J. A (1993). Extending the scope of information modelling. Fourth International Workshop on the Deductive
Approach to Information Systems and Databases, Lloret, Costa Brava (Catalonia), September 1993
Coad, P., & Yourdon, E. (1991). Object-oriented analysis. Englewood Cliffs: Prentice-Hall.
Curtis, W., Kellner, M. I., & Over, J. (1992). Process modeling. Communications of the ACM, 35(9), 75 –90.
Dorador, J. M., & Young, I. M. (2000). Application of IDEF0, IDEF3 and UML methodologies in the creation of information
models. International Journal of Computer Integrated Manufacturing, 13(5), 430– 445.
Doumeingts, G., & Chen, D. (1992). State-of-the-art on models, architecture and methods for CIM system design. Human
aspects in computer integrated manufacturing, North-Holland: Elsevier, pp. 27 –40.
Firesmith, D., Henderson-Sellers, B., Graham, I., & Page-Jones, M (1996). OPEN Modeling Language (OML) reference
manual, December 1996
Frank, U. (2002). Multi-perspective enterprise modeling (MEMO)—conceptual framework and modeling languages. HICSS
2002: 35th International Conference on System Science, Big Island, Hawaii.
Giaglis, G. M. (2001). A taxonomy of business process modeling and information systems modeling techniques. International
Journal of Flexible Manufacturing Systems, 13(2), 209– 228.
Liles, D. H., & Presley, A. R. (1996). Enterprise modeling within an enterprise engineering framework. Winter Simulation
Conference, San Diego, CA, December.
Lin, H., Fan, Y., & Wu, C (1999). The research of integrated enterprise modeling method based on workflow model. Seventh
IEEE International Conference on EFTA, Barcelona, October 1999 (pp. 187– 193)
Menzel, C., & Mayer, R. (1997). The IDEF-family of languages. International handbook on information systems architecture,
Berlin: Springer.
Monarchi, D. E., & Puhr, G. I. (1992). A research typology for object-oriented analysis and design. Communications of the
ACM, 35(9), 35 – 47.
Nilsson, A. G., Tolis, C., & Nellborn, C (Eds.), (1999). Perspectives on business modelling: Understanding and changing
organisations. Berlin: Springer
Petrie, C. (Ed.), (1992). Introduction. First International Conference on Enterprise Integration Modeling, Austin, TX.
Cambridge, MA: MIT Press.
Prasse, M. (1998). Evaluation of object-oriented modelling languages: a comparison between OML and UML. The unified
modeling language—technical aspects and applications (pp. 58 – 78). Heidelberg: Physica.
Presley, A. R (1997). A representation method to support enterprise engineering. PhD Dissertation, Industrial and
Manufacturing Systems Engineering, University of Texas at Arlington
Presley, A., Huff, B., et al (1993). A comprehensive enterprise model for small manufacturers. Second Industrial Engineering
Research Conference, Los Angeles, CA
Presley, A. R., & Liles, D. H. (2001). A holon-based process modeling methodology. International Journal of Operations and
Production Management, 21(5/6), 565–581.
Rumbaugh, J., Jacobson, I., & Booch, G. (1999). The unified modeling language reference manual. Reading: Addison-Wesley.
Scheer, A. W. (1998). Business Process Engineering: Reference Models for Industrial Enterprises. Berlin: Springer.
Scheer, A. W. (1999). ARIS—business process engineering. Berlin: Springer.
Sgegheo, O., & Andersen, B (1999). Modeling the extended enterprise: a comparison of different modeling approaches.
International Conference on Enterprise Modeling, Verdal

D.L. Shunk et al. / Computers & Industrial Engineering 45 (2003) 167–193

193

Shunk, D. L. (1990). Creating an integrated, useful systems definition technique. Optimization of manufacturing systems
design, North-Holland: Elsevier, pp. 221– 225.
Shunk, D. L., Sullivan, B., & Cahill, J. (1986). Making the most of IDEF modeling—the triple diagonal concept. CIM Review,
Fall, 12 – 17.
van Slooten, K., & Brinkkemper, S. (1993). A method engineering approach to information systems development. IFIP WG 8.1
Working Conference on Information System Development Process (A-30), North-Holland: Elsevier, pp. 167– 186.
Sowa, J. F., & Zachman, J. A. (1992). Extending and formalizing framework for information systems architecture. IBM Systems
Journal, 31(3).
Vernadat, F. B. (1992). CIMOSA—a European development for enterprise integration. Part 2. Enterprise modelling. First
International Conference on Enterprise Integration Modeling, Austin, TX, Cambridge, MA: MIT Press.
Vernadat, F. B. (1996). Enterprise modeling and integration: principles and applications. London: Chapman & Hall.
Wangler, B., Persson, A., & Söderström, E. (2001). Enterprise modeling for B2B integration. SSGRR : International
Conference on Advances in Infrastructure for Electronic Business, Science, and Education on the Internet, L’Aquila, Italy,
August, 2001.
Whitman, L., & Huff, B. (2001). On the use of enterprise models. International Journal of Flexible Manufacturing Systems,
13(2), 195– 208.
Whitman, L., Huff, B., & Presley, A (1998). The needs and issues associated with representing and integrating multiple views of
the enterprise. DIISM’98: International Conference on Design of Information Infrastructure Systems for manufacturing,
Fort Worth, Texas, May 18 –20, 1998
Whitman, L., Huff, B., & Presley, A (1998). Issues encountered between model views. Flexible Automation and Integrated
manufacturing Conference, New York, NY
Zachman, J. A. (1987). A framework for information systems architecture. IBM Systems Journal, 26(3), 276–292.

