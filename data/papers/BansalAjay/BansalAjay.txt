Web Service Discovery and Composition using USDL
Srividya Kona, Ajay Bansal, Gopal Gupta Department of Computer Science University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp. 2400 Dallas Parkway Plano, TX 75093

Abstract
For web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper, we present the design of an automatic service discovery and composition engine using USDL (Universal Service-Semantics Description Language) [1, 2], a language for formally describing the semantics of web-services. The implementation will be used for the WS-Challenge 2006 [3].

1. Introduction
A web-service is a program available on a website that "effects some action or change" in the world (i.e., causes a side-effect). The next milestone in the Web's evolution is making services ubiquitously available. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. To achieve this, we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery and composition. In this paper we present the design of a software system for automatic service discovery and composition. This system uses web service descriptions written in USDL [1, 2]. In section 2 we present a brief overview of USDL. Section 3 shows the design of our software with brief descriptions of the different components of the system followed by conclusions and references.

This is an ongoing project with Metallect Corp. The design and formal specification in OWL was published in European Conference On Web Services, 2005 [1]. The WSDL [4] syntax and USDL semantics of web services can be published in a directory which applications can access to discover services. To provide formal semantics, a common denominator must be agreed upon that everybody can use as a basis of understanding the meaning of services. Additionally, the semantics should be given at a conceptual level that captures common real world concepts. USDL uses an ontology based on OWL WordNet [5] for a universal ontology of basic concepts upon which arbitrary meets and joins can be added in order to gain tractable flexibility. USDL describes a service in terms of portType and messages, similar to WSDL. The semantics of a service is given using the OWL WordNet ontology: portType (operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly negated) concepts in the OWL WordNet ontology. Additional semantics is given in terms of how a service affects the external world. Formal semantic description of any external conditions (pre-conditions or post-conditions) acting on the service can also be provided using USDL.

3. Design
Our discovery and composition engine is written using Prolog [6]. Our software system for the WSChallenge [3] comprises of the following components shown in Figure 1.

3.1. USDL Generator

2. Overview of USDL
USDL is a language that provides formal semantics of web-services thus allowing sophisticated conceptual modeling and searching of available services, automated composition, and automated service integration. This module parses all WSDL descriptions from the given repository and converts them into corresponding USDL descriptions. For the WS-Challenge, this module maps part names of a service to their corresponding type definitions from the XML Schema file, as opposed

3.3. Semantic Relations Generator
For the semantic part of the challenge, we have to match the parmater types. The XML Schema will provide type hierarchy. We will obtain the semantic relations from the XML Schema file provided instead of OWL WordNet ontology. Complex types will be described in the schema file using simple data types, thus providing a type hierarchy. A supertype will subsume its subtype which is nothing but the hyponym and hypernym relation. A hyponym is a word that is more specific than a given word, also called the sub-ordinate. A hypernym is a word that is more generic than a given word, also called the super-ordinate. This module will extract the type definitions from the XML Schema file and create the semantic relations (hypernym, hyponym, etc.) [7] between the different types in a format that the discovery and composition query processors will understand. For example, if the XML Schema has a type p66a9128258 which is a supertype of p56a4809967, then this module will add the prolog fact hyponym(p56a4809967,p66a9128258). to the list of facts. to pointing them to concepts in OWL WordNet ontology. Our approach does not need a separate query language. It parses the query file and converts each query for the desired service also into a USDL description.

3.4. Discovery Query Processor
This module compares the discovery query with all the services in the repository. It uses an extended/special unification algorithm to find a matching term. The unification mechanism is different depending on the type of match (Exact, Specific, Generic, Part or Whole ) required. The processor works as follows: 1. On the input parts of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a type with hypernym relation [7], i.e., a generic substitutable. 2. On the output parts of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a type with hyponym relation [7], i.e., a specific substitutable. The discovery engine is written using prolog (a logic programming language) [6]. It uses a repository of facts, which contains a list of all the services and the semantic relations. The query is converted into a prolog query that looks as follows: discoverServices(queryService, solutionService). The engine will try to find a list of solutionService s that match the queryService. Our code for the engine will have various rules to solve the discoverServices query.

3.2. Triple Generator
The triple generator module converts each service description into a triple as follows: (Pre-Conditions, affect-type(affected-object, I, O), Post-Conditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters of the service. Services are converted to triples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [7]. For the WS-Challenge, conditions on a service will not be provided and hence the Pre-Conditions and PostConditions in the triple will be null. The affect-type will also not be available and so this module will assign a generic affect to all services.

3.5. Composition Query Processor
For service composition, the first step is finding the set of composable services. If a subservice S1 is composed with subservice S2 , then the output parts of S1 must be the input parts of S2 . Thus the processor has to find a set of services such that the outputs of the first service are inputs to the next service and so on. These services are then stitched together to produce the desired service. If the complete semantics of a web service are described using USDL and OWL WordNet ontology, Part substitution technique [7] can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the individual services. For the challenge, we do not have with mappings to OWL WordNet Ontology and description of conditions. So we will be using only Exact, Generic, and Specific substitution [7] techniques to find the list of composable services. Similar to the discovery engine, composition engine is also written using prolog. The query is converted into a prolog query that looks as follows: composeServices(queryService, listOfServices). The engine will try to find a listOfServices that can be composed into the requested queryService. Our code for the engine will have various rules to solve the composeServices query. Our prolog code will be setup in such a way that all possible listofServices that can be used for composition will be returned.

substitutable services that best match the desired service. Our solution will produce very good results when semantic descriptions of web services are provided using USDL. While matching real-world services, our discovery and composition engine will look at OWL WordNet ontology for the meanings. For the challenge, type hierarchy is the only semantics provided and hence we will not be able to use all the semantic relations available in WordNet. We apply some optimization techniques to our system so that it is efficient on the WS-Challenge repositories. We use constraint-logic programming [8] for better pruning of the search space. We have put in efforts for testing the efficiency of our system and identifying the correct optimization strategies.

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005. [2] S. Kona, A. Bansal, G. Gupta, and T. Hite. USDL - Formal Definitions in OWL. Internal Report, University of Texas, Dallas, 2006. Available at http://www.utdallas.edu/~srividya.kona/ USDLFormalDefinitions.pdf. [3] WS Challenge 2006. tu-berlin.de/wsc06. http://insel.flp.cs.

3.6. Output Generator
After the Discovery/Composition Query processor finds a matching service, or the list of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files using the specified XML format for the WS-Challenge[3].

[4] Web Services Description Language. http://www. w3.org/TR/wsdl. [5] Ontology-based information management system, wordnet OWL-Ontology. http://taurus.unine. ch/knowler/wordnet.html. [6] L. Sterling and S. Shapiro. The Art of Prolog. MIT Press, 1994. [7] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics Description Language for Automatic Service Discovery and Composition. Technical Report UTDCS-1806, University of Texas, Dallas, 2006. Submitted to JWSR. Available at http://www.utdallas.edu/ ~srividya.kona/USDL.pdf. [8] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

4. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure to publish services, document services and query repositories for matching services. Our approach uses USDL to formally document the semantics of services and our discovery and composition engines find

A Universal Service-Semantics Description Language
Luke Simon, Ajay Bansal, Ajay Mallya, and Gopal Gupta Department of Computer Science University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp 2400 Dallas Parkway Plano, TX 75093

Abstract
To fully utilize web-services, users and applications should be able to discover, deploy, compose and synthesize services automatically. This automation can take place only if a formal semantic description of the web-services is available. In this paper we present the design of USDL (Universal Service-Semantics Description Language), a language for formally describing the semantics of web-services. USDL is based on the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding the meaning of services. USDL can be regarded as formal program documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated service composition, and other forms of automated service integration. The design of USDL is presented, along with examples, and its formal semantics given. A theory of service composition for USDL is presented and proved sound and complete.

can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis. Several efforts are underway to build this infrastructure. These efforts include approaches based on the semantic web (such as OWL-S [4]) as well as those based on XML, such as Web Services Description Language (WSDL). Approaches such as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. In this paper we present an approach that is based on semantics. Our approach can be regarded as providing semantics to WSDL statements. We present the design of a language called Universal Service-Semantics Description Language (USDL) which service developers can use to specify formal semantics of web-services. Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL can be thought of as formal program documentation that will allow sophisticated conceptual modeling and searching of available web services, automated composition, and other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be determined. The directory can then be searched to check if this exact service is available, and if not available, then whether it can be synthesized by composing two or more services listed in this (or another) directory. To provide formal semantics, a common denominator must be agreed upon that everybody can use as a basis of understanding the meaning of services. This common conceptual ground must also be somewhat coarse-grained so as to be tractable for use by both engineers and computers. That is, semantics of services should not be given in terms of low-level concepts such 1

1

Introduction

The next milestone in the Web's evolution is making services ubiquitously available. A web service is a program available on a web-site that "effects some action or change" in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. In this context, a web-service can be regarded as a "programmatic interface" that makes application to application communication possible. For web-services to become practical, an infrastructure needs to be supported that allows users to discover, deploy, compose, and synthesize services automatically. Such an infrastructure must be semantics based so that applications

as Turing machines, first-order logic and their variants, since service description, discovery, and synthesis then become tasks that are practically intractable and theoretically undecidable. Additionally, the semantics should be given at a conceptual level that captures common real world concepts. Furthermore, it is too impractical to expect disparate companies to standardize on application (or domain) specific ontologies to formally define semantics of web-services, and instead a common universal ontology must be agreed upon with additional constructors. Also, application specific ontologies will be an impediment to automatic discovery of services since the application developer will have to be aware of the specific ontology that has been used to describe the semantics of the service in order to frame the query that will search for the service. The danger is that the service may not be defined using the particular domain specific ontology that the application developer uses to frame the query, however, it may be defined using some other domain specific ontology, and so the application developer will be prevented from discovering the service even though it exists. These reasons make an ontology based on WordNet OWL a suitable candidate for a universal ontology of atomic concepts upon which arbitrary meets and joins can be added in order to gain tractable flexibility. In the next section we describe what is meant by conceptual modeling and how such a thing could be obtained via a common universal ontology based on WordNet. Section 3, gives a brief overview of how USDL attempts to semantically describe web services. In section 4, we discuss precisely how a WSDL document can be prescribed meaning in terms of WordNet ontology, and in addition, a short example WSDL service is annotated, so that a concrete example of instances of the various OWL classes and properties can be seen. Section 5 gives a complete USDL annotation for a BookBuying service. Automatic discovery and composition of web services using USDL is discussed in section 6. In section 7 we explore some of the theoretical aspects of service description in USDL. Comparison of USDL with other approaches like OWL-S and WSML is discussed in section 8. Finally, other related work and conclusion are given in the remaining sections.

2

A Universal Ontology

We can describe what any given computer program does in a logical manner, from first-principles. This is the approach taken by frameworks such as dependent type systems and programming logics prevalent in the field of software verification where a "formal under2

standing" of the software is needed in order to verify it. However, such solutions are both too low-level and too tedious (and not to mention, undecidable) to be of practical use. Instead, we are interested in modeling higher-level concepts. That is, we are more interested in answering questions such as, what does a service do from the point of the end user or integrator of the service, as opposed to the far more difficult questions, such as, what does the program do from a computational view? The distinction is subtle, but is a distinction of granularity as well as a distinction of scope: we care more about real world concepts such as "customer", "bank account", and "flight itinerary" as opposed to the data structures and algorithms used by a program to model these concepts. At some point, a common denominator must be agreed upon in order to allow interoperability and machine-readability of our documents. The first step towards this common ground are standard languages such as WSDL and OWL. However, these do not go far enough, as for any given type of service there are numerous distinct representations in WSDL and for high-level concepts (e.g. a tertiary predicate), there are numerous disparate representations in terms of OWL, representations that are distinct in terms of OWL's formal semantics, yet equal in the actual concepts they model. This is known as the semantic aliasing problem: distinct syntactic representations with distinct formal semantics yet equal conceptual semantics. This problem can be overcome by standardizing on a sufficiently comprehensive set of atomic concepts, i.e. a universal ontology, along with restricted connectives, such that the semantics always equate things that are conceptually equal. Another approach would be to use industry specific ontologies along with OWL (this is the approach taken by the OWL-S language [4]). The problem with this approach is that it requires standardization and undue foresight. Standardization is a slow, bitter process, and industry specific ontologies would require this process to be iterated for each specific industry. Furthermore, reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing is even more difficult. Undue foresight is required because many useful web services will address innovative applications and industries that don't currently exist. Standardizing an ontology for travel and finances is easy, as these industries are well established, but then how will new innovative services in new upcoming industries be ascribed formal meaning? Of course, a universal ontology will have no difficulty in describing such new services. Thus, our common conceptual ground must be

somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. Currently there is only one sufficiently comprehensive ontology that meets these criteria: WordNet [7]. As stated, part of the common ground involves standardized languages such as OWL. For this reason, WordNet cannot be used directly, and instead we make use of an encoding of WordNet as an OWL base ontology [1]. Using an OWL WordNet ontology allows for our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which we map web service messages and operations. As long as this mapping is precise and sufficiently expressive, reasoning can be done within the realm of OWL by using an automated inference systems (such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological concepts, especially subsumption (hyponym) and equivalence (synonym) relationships. Finally, USDL restricts its conceptual constructors, in order to address the problems of semantic aliasing and tractability.

3

USDL: An Overview

Like WSDL, USDL describes a service in terms of ports and messages. The semantics of the service is given using the WordNet OWL ontology. USDL maps ports (operations provided by the service) and messages (operation parameters) to disjunctions of conjunctions of (possibly negated) concepts in the WordNet OWL ontology. The semantics is given in terms of how a service affects the external world. The present design of USDL assumes that each side-effect is one of the following operations: create, update, delete, or find, but also allows for a generic affects side-effect when none of the others apply. An application that wishes to make use of a service automatically should be able to reason with WordNet atoms using the WordNet OWL ontology. USDL is perhaps the first language that attempts to capture the semantics of web-services in a universal, yet decidable manner. It is quite distinct from previous approaches such as WSDL and OWL-S [4]. As mentioned earlier, WSDL only defines syntax of the service; USDL can be thought of as providing the missing semantic component. USDL can be thought of as a formal language for program documentation. Thus instead of documenting the function of a service as comments in English, one writes USDL statements that describe the function of that service. USDL is quite distinct from OWL-S, which is designed for a similar purpose, and as 3

we shall see the two are in fact complimentary. OWL-S primarily describes the states that exists before and after the service and how a service is composed of other smaller sub-services (if any). Description of atomic services is left underspecified in OWL-S. They have to be specified using domain specific ontologies; in contrast atomic services are completely specified in USDL, and USDL relies on a universal ontology (OWL WordNet Ontology) to specify the semantics of atomic services. USDL and OWL-S are complimentary in that OWL-S's strength lies in describing the structure of composite services, i.e., how various atomic services are algorithmically combined to produce a new service, while USDL is good for fully describing atomic services. Thus, OWL-S can be used for describing the structure of composite services that combine atomic services that are described using USDL. In order to develop a theory for composing services, we also define the formal semantics of USDL. The syntactic terms describing ports and messages are mapped to disjunctions and conjunctions of (possibly negated) OWL WordNet ontological terms. These disjunctions and conjunctions are represented by points in the lattice obtained from the WordNet ontology with regards to the OWL subsumption relation. A service is then formally defined as a function, labeled with zero or more side-effects, between points in this lattice. The main contribution of our work is the design of a universal service- semantics description language USDL, along with its formal semantics, and soundness and completeness proofs for a theory of service composition with USDL.

4

Design of USDL

The design of USDL rests on two formal languages: Web Services Description Language (WSDL) [6] and Web Ontology Language (OWL) [5]. The Web Services Description Language (WSDL) [6], is used to give a syntactic description of the name and parameters of a service. The description is syntactic in the sense that it describes the formatting of services on a syntactic level of method signatures, but is incapable of describing what concepts are involved in a service and what a service actually does, i.e. the conceptual semantics of the service. Likewise, the Web Ontology Language (OWL) [5], was developed as an extension to the Resource Description Framework (RDF) [2], both standards are designed to allow formal conceptual modeling via logical ontologies, and these languages also allow for the markup of existing web resources with semantic information from the conceptual models. USDL employs WSDL and OWL in order to describe the syntax

and semantics of web services. WSDL is used to describe message formats, types, and method prototypes, while a specialized universal OWL ontology is used to formally describe what these messages and methods mean, on a conceptual level. As mentioned earlier, USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL documents contain two main constructs to which we want to ascribe conceptual meaning: messages and ports. These constructs are actually aggregates of service components which will actually be directly ascribed meaning. Messages consist of typed parts and ports consist of operations parameterized on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes, which have properties with values in the OWL WordNet ontology.

be modeled as set theoretic formulas of union, intersection, and negation of atomic concepts. These subclasses of Concept are  AtomicConcept  InvertedConcept  ConjunctiveConcept  DisjunctiveConcept 4.1.1 Atomic Concept

AtomicConcept is the actual contact point between USDL and WordNet. This class acts as proxy for WordNet lexical concepts.
<owl:Class rdf:about="#AtomicConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isA"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#ofKind"/> <owl:mincardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:mincardinality> </owl:Restriction> </rdfs:subClassOf> ... </owl:Class>

4.1

Concept

USDL defines a generic class called Concept which is used to define the semantics of parts of messages. Semantically, instances of Concept form a complete lattice, which will be covered in section 7.
<owl:Class rdf:ID="Concept"> <rdfs:comment> Generic class of USDL Concept </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#AtomicConcept"/> <owl:Class rdf:about="#InvertedConcept"/> <owl:Class rdf:about="#ConjunctiveConcept"/> <owl:Class rdf:about="#DisjunctiveConcept"/> </owl:unionOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:mincardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:mincardinality> </owl:Restriction> </rdfs:subClassOf> ... </owl:Class> <owl:ObjectProperty rdf:ID="hasCondition"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Condition"/> </owl:ObjectProperty>

The property cardinality restrictions require all USDL AtomicConcept s to have exactly one defining value for the isA property, and zero or more values for the ofKind property. An instance of AtomicConcept is considered to be equated with the WordNet lexical concept given by the isA property and classified by the lexical concept given by the optional ofKind property.
<owl:ObjectProperty rdf:ID="isA"> <rdfs:domain rdf:resource="#AtomicConcept"/> <rdfs:range rdf:resource="&wn;LexicalConcept"/> ... </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="ofKind"> <rdfs:domain rdf:resource="#AtomicConcept"/> <rdfs:range rdf:resource="&wn;LexicalConcept"/> ... </owl:ObjectProperty>

The USDL Concept class denotes the top element of the lattice of conceptual objects constructed from the OWL WordNet ontology. For most purposes, message parts and other WSDL constructs will be mapped to a subclass of USDL Concept so that useful concepts can 4

4.1.2

Inverted Concept

In the case of InvertedConcept the corresponding semantics are the complement of the WordNet lexical concepts.
<owl:Class rdf:about="#InvertedConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype= "&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasConcept"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

intersections and unions(where n  2) of USDL concepts. For generality, these concepts are either AtomicConcepts, ConjunctiveConcepts, DisjunctiveConcepts, or InvertedConcepts .

4.2

Affects

The affects property is specialized into four types of actions common to enterprise services: creates, updates, deletes, and finds.
<owl:ObjectProperty rdf:ID="affects"> <rdfs:comment> Generic class of USDL Affects </rdfs:comment> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/> ... </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#creates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#updates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#deletes"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#finds"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty>

4.1.3

Conjunctive and Disjunctive Concept

The ConjunctiveConcept and DisjunctiveConcept respectively denote the intersection and union of USDL Concept s.
<owl:Class rdf:about="#ConjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions on ConjunctiveConcept and DisjunctiveConcept allow for n-ary 5

Note that each of these specializations inherits the domain and range of the affects property. Most services can be described as a conjunction of these types of effects. For those services that cannot be described in terms of a combination of these specializations, the parent affects property can be used instead, or the property can be omitted entirely when the meaning of the operation parameter messages are enough for conceptual reasoning. The purpose of limiting the types of services as opposed to allowing the creation of new arbitrary side-effect types, for example via OWL-DL, is to: (i) make USDL more structured and therefore easier to create documents in, (ii) make USDL computationally more tractable for programs that process large volumes of USDL documents, and (iii) help prevent the semantic aliasing problem mentioned in section 2.

4.3

Conditions and Constraints

Services may have some external conditions specified on the input or output parameters. Condition class is used to describe all such constraints. Conditions are represented as conjunction or disjunction of binary predicates. Predicate is a trait or aspect of the resource being described.
<owl:Class rdf:ID="Condition"> <rdfs:comment> Generic class of USDL Condition </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#AtomicCondition"/> <owl:Class rdf:about="#ConjunctiveCondition"/> <owl:Class rdf:about="#DisjunctiveCondition"/> </owl:unionOf> ... </owl:Class> <owl:Class rdf:about="#AtomicCondition"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype= "&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#onPart"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasValue"/> <owl:maxCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:maxCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

<owl:ObjectProperty rdf:ID="onPart"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="hasValue"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.3.1

Conjunctive and Disjunctive Conditions

The ConjunctiveCondition and DisjunctiveCondition respectively denote the conjunction and disjunction of USDL Condition s.
<owl:Class rdf:about="#ConjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasCondition"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Condition"/> </owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveCondition and DisjunctiveCondition allow for nary conjunctions and disjunctions (where n  2) of USDL conditions. In general any n-ary condition can be written as a combination of conjunctions and disjunctions of binary conditions.

4.4
A condition has exactly one value for the onPart property and atmost one value for the hasValue property, each of which is of type USDL Concept. 6

Messages

Services communicate by exchanging messages. As mentioned, messages are simple tuples of actual data,

called parts. Take for example, a flight reservation service similar to the SAP ABAP Workbench Interface Repository for flight reservations [3], which makes use of the following message.
<message name="&flight;ReserveFlight_Request"> <part name="&flight;CustomerName" type="xsd:string"> <part name="&flight;FlightNumber" type="xsd:string"> <part name="&flight;DepartureDate" type="xsd:date"> ... </message>

<AtomicConcept rdf:about="&flight;FlightNumber"> <isA rdf:resource="&wn;number"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;flight"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasPart> ... </Message>

4.5

Ports

The USDL surrogate for a WSDL message is the Message class, which is a composite entity with zero or more parts. Note that for generality, messages are allowed to contain zero parts.
<owl:Class rdf:about="#Message"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasPart"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

Services consist of ports, which are collections of procedures or operations that are parametric on messages. Our example flight reservation service might contain a port definition for a flight reservation service that takes as input an itinerary and outputs a reservation receipt.
<portType name="&flight;Flight_Reservation"> <operation name="&flight;ReserveFlight"> <input message= "&flight;ReserveFlight_Request"/> <output message= "&flight;ReserveFlight_Response"/> </operation> ... </portType>

Each part of a message is simply a USDL Concept, as defined by the hasPart property. Semantically messages are treated as tuples of concepts.
<owl:ObjectProperty rdf:ID="hasPart"> <rdfs:domain rdf:resource="#Message"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

The USDL surrogate is defined as the class Port which contains zero or more Operation s as values of the hasOperation property.
<owl:Class rdf:about="#Port"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource=#hasOperation"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> ... </owl:Class> <owl:ObjectProperty rdf:ID="hasOperation"> <rdfs:domain rdf:resource="#Port"/> <rdfs:range rdf:resource="#Operation"/> </owl:ObjectProperty>

Continuing our example flight reservation service, the Itinerary message is given semantics using USDL as follows, where &wn;customer and &wn;name are valid XML references to WordNet lexical concepts.
<Message rdf:about= "&flight;ReserveFlight_Request"> <hasPart> <AtomicConcept rdf:about="&flight;CustomerName"> <isA rdf:resource="&wn;name"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;customer"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasPart> <hasPart>

As with the case of messages, ports are not directly assigned meaning via the OWL WordNet ontology. Instead the individual Operation s of a port are described by their side-effects via an affects property. Note that the parameters of an operation are already given meaning by ascribing meaning to the messages that constitute the parameters. 7

<owl:Class rdf:about="#Operation"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#affects"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The service has an input pre-condition that a User Identifier for the buyer must exist before invoking the service. It also has a global constraint that a valid credit card number for the buyer must exist.

5.1

WSDL definition

An operation can have multiple or no values for the affects property, all of which are of type USDL Concept, which is the target of the effect.
<owl:ObjectProperty rdf:ID="affects"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

The following is WSDL definition of the service. This service provides a single operation called BookBuying. The input and output messages are defined below. The conditions on the service cannot be described using WSDL.
<definitions> ... <portType name="BookBuying_Service"> <operation name="BookBuying"> <input message="BuyBook_Request"/> <output message="BuyBook_Response"/> </operation> </portType> <message name="BuyBook_Request"> <part name="BookISBN" type="xsd:string"/> <part name="UserIdentifier" type="xsd:string"/> <part name="Password" type="xsd:string"/> </message> <message name="BuyBook_Response"> <part name="OrderNumber/Availability" type="xsd:string"/> </message> ... </definitions>

Finishing our flight reservation service example, we can describe the main side-effect of invoking the reserve operation in USDL as follows.
<Port rdf:about="&flight;Flight_Reservation"> <hasOperation> <Operation rdf:about="&flight;ReserveFlight"> <creates> <AtomicConcept rdf:about="flight_reservation"> <isA rdf:resource="&wn;reservation"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;flight"/> </AtomicConcept> </ofKind> </AtomicConcept> </creates> </Operation> </hasOperation> ... </Port>

5.2

USDL annotation

Again, note that it is not necessary to annotate the operation with regards to its input and output parameters, as these are already annotated by Message surrogates.

The following is the complete USDL annotation corresponding to the above mentioned WSDL description. The input pre-condition and the global constraint on the service are also described semantically.
<definitions> ... <port rdf:about="#BookBuying_Service"> <hasOperation> <operation name="BuyBook"> <creates> <AtomicConcept rdf:about="#BookOrder"> <isA rdf:resource="&wn;order"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;book"/> </AtomicConcept> </ofKind> <hasCondition>

5

Semantic Description of a Service

A simple Book Buying Service: The service described here is a simplified book buying service published in a web service registry. This service can be treated as atomic: i.e., no interactions between buying and selling agents are required, apart from invocation of the service and receipt of its outputs by the buyer. Given certain inputs and preconditions, the service provides certain outputs and has specific effects. 8

<Condition rdf:about="#exists"> <AtomicConcept> <isA rdf:resource="&wn;exists"/> </AtomicConcept> <onPart rdf:about="#CreditCard"> <AtomicConcept> <isA rdf:resource="&wn;card"/> <ofKind> <AtomicConcept> <isA rdf:resource= "&wn;credit"/> </AtomicConcept> </ofKind> </AtomicConcept> </onPart> </Condition> </hasCondition> </AtomicConcept> </creates> </operation> </hasOperation> </port> <portType name="BookBuying_Service"> <operation name="BuyBook"> <input message="BuyBook_Request"/> <output message="BuyBook_Response"/> </operation> </portType> <Message rdf:about="#BuyBook_Request"> <hasPart> <AtomicConcept rdf:about="#BookISBN"> <isA rdf:resource="&wn;identifier"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;book"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasPart> <hasPart> <AtomicConcept rdf:about= "#UserIdentifier"> <isA rdf:resource="&wn;identifier"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;user"/> </AtomicConcept> </ofKind> <hasCondition> <Condition rdf:about="#exists"> <AtomicConcept> <isA rdf:resource="&wn;exists"/> </AtomicConcept> <onPart rdf:resource= "#UserIdentifier"/> </Condition> </hasCondition> </AtomicConcept> </hasPart> <hasPart>

<AtomicConcept> <isA rdf:resource="&wn;Password"/> </AtomicConcept> </hasPart> </Message> <Message rdf:about="#BuyBook_Response"> <hasPart> <DisjunctiveConcept rdf:about="#OrderNumber/Availability"> <hasConcept> <AtomicConcept rdf:about= "#OrderNumber"> <isA rdf:resource="&wn;number"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;order"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasConcept> <hasConcept> <InvertedConcept rdf:about= "#NotAvailable"> <AtomicConcept> <isA rdf:resource="&wn;available"/> </AtomicConcept> </InvertedConcept> </hasConcept> </DisjunctiveConcept> </hasPart> </Message> ... </definitions>

6

Service Discovery and Composition

Note that given a directory of services, a USDL description could be included for each service, making the directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. Note that USDL itself could be such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory to look for a "matching" service. For matching we could treat USDL descriptions as well as the USDL query as terms, and perhaps use some kind of extended unification to check for a match. This work is currently in progress [9]. With the USDL descriptions and query language in place, numerous applications become possible ranging from querying a database of services to rapid application development via automated integration tools and even real-time service composition [14]. Take our flight reservation service example. Assume that some9

body wants to find a travel reservation service and that they query a USDL database containing general purpose flight reservation services, bus reservation services, etc. One could then form a USDL query consisting of a description of a travel reservation service and the database could respond with a set of travel reservation services whether it be via flight, bus, or some other means of travel. This flexibility of generalization and specialization is gained from USDL's subtyping relation covered in section 7. Furthermore, with a pool of USDL services at one's disposal, rapid application development (RAD) tools could be used to aid a systems integrator with the task of creating composite services, i.e. services consisting of the composition of already existing services. The service designer could use such a RAD tool by describing the desired service via a USDL document, and then the tool would query the pool of services for composable sets of services that can be used to accomplish the task as well as automatically generate boilerplate code for managing the composite service, as well as menial inter-service data format conversions and other glue. Of course these additional RAD steps would require other technologies already being researched and developed [10, 11, 16, 13, 15]. At present, we presume only four specific operations (find, create, delete, update); this set of basic operations may be extended as more experience is gained with USDL. In practice, most services, however, deal with manipulating databases and for such services these four operations are sufficient. As stated, one of the reasons for limiting the side-effect types is to safeguard against the semantic aliasing problem described in section 2. This is also one of the main reasons for restricting the combing forms in USDL to conjunction, disjunction, and negated atoms. As discussed in the next section, this allows USDL descriptions to be put into a type of disjunctive normal form, from which a sound and complete notion of subtyping is created.

of this section. In order to prove these theorems, we must first formally define constructs such as USDL described objects and services, which we will also call objects and services for short. While it is possible to work directly with the XML USDL syntax, doing so is cumbersome and so we will instead opt for set theoretic notation. Definition 1. Let  be the set of WordNet nouns and  be the OWL subsumption relation on . Definition 2. Let  be the least set of objects such that: 1. x   implies x, 촼   2. x, y   implies x  y , x  y   Hence  is simply the set of objects described by USDL concepts. Definition 3. Let  = {(L, E ) | L  , E  } be the set of USDL side-effects, where  = {creates, updates, deletes, f inds} L is the affect type and E is the effected object. Definition 4. For any set S , let S = { |  / S }  S  {(x, y ) | x  S, y  S  } be the set of lists over S . Let  = {(A, I, O) | A  2 , I   , O   } be the set of USDL service descriptions, where A is the set of side-effects, I is the list of input parameters, and O is the list of output parameters of a particular service. Now that the formal definitions of object and service descriptions are out of the way, we would like to define a subsumption relation  over  so that we can reason about composability of services, but this will, in turn, require a subsumption relation  over . The proof of correctness of  is covered by the principle of safe substitution below. Definition 5. Assuming without loss of generality that all objects are expressed in disjunctive normal form, then let  be the ordering relation defined on objects such that: 1. x  y for x, y   and x  y 2. 촼  촽 if and only if y  x 3.
i wi  j xj if and only if for all wj = k yk there exists some xj = l zl such that for every zl there exists some yk  zl .

7

Theory of Service Description

In this section, we will investigate the theoretical aspects of service description via USDL. This involves concepts from set theory, lattice theory, and type theory. From a systems integration perspective, an engineer is interested in finding a set of composable services that accomplish some necessary task. Therefore service description should allow the engineer to describe a service in USDL and receive in return a set of services that can be used in a context expecting a service that meets that description. We prove that this is possible in the soundness and completeness theorems at the end 10

Definition 6. Let (A, I, O)  (A , I , O ) if and only if (L, E )  A .(L, E )  A .E  and I  I and O  O , where  is the element-wise extension of  to lists of objects.

Note that (,  ) and (,  ) respectively form a complete lattice. Given a description of a service   , we can now define the set of composables C ( ), which in practice corresponds to a query against a database of services , for services that satisfy description  . Notice that the definition is contravariant for inputs and covariant for outputs [17]. This contravariance is also seen in the field of type theory with regards to polymorphic subtyping [17], and will be covered in the proof of the principle of safe substitution below. Definition 7. Let C be the set of composables parametric over services be a function mapping  to a subset of services such that C ( ) = { |    }. In order to be able to prove the soundness and completeness of C , we will first need to prove the following lemma known as the "Principle of Safe Substitution." Lemma 1. For any services ,   , if    then  can safely be used in a context expecting service  . Proof. The proof follows by establishing the principle of safe substitution for objects, which is then used to prove the principle of safe substitution for services. Assume for sake of contradiction that for some objects x and y that x  y such that object x can not be safely used in a context expecting object y . Then under the set-theoretic interpretation of x and y as the sets sx and sy of all objects satisfying respective descriptions, sx contains an element o  / sy . Since o is described by some   x, and so by assumption that x is incompatible with y , it must be true that there is no   y that describes x. However, this contradicts the definition of  that requires that there exists a conjunctive concept   y that describes x. Therefore the principle of safe substitution holds for the lattice (,  ). Now assume that there exists a  such that    , but  can not be used in a context expecting  , where  = (A, I, O) and  = (A , I , O ). Clearly the context expects a service that can accept input of the type described by I , and since  accepts a more general input type, every input of type I is also an input of type I . This explains the contravariance of input in the definition of  . Now, since the expected service will output objects of type O , the context can handle a service that outputs objects of a more specific kind O by the principle of safe substitution for objects. Therefore  can safely be used in place of  . Theorem 1. (Soundness of Composables) For any services  and  , if service  can not be safely used in a context expecting service  then   / C ( ), that is the set of composables for a service does not contain any incompatible services. 11

Proof. Assume the existence of services  and  such that  can not be safely used in a context expecting service  . By the principle of safe substitution it is not true that    , and hence by the principle of extensionality,   / C ( ). Therefore the set of composables only contains correct, i.e. compatible services. Theorem 2. (Completeness of Composables) For any service  , if there exists a service  that can safely be used in a context expecting service  then   C ( ), that is the set of composables contains all services compatible with a given description. Proof. Assume there exists such a service  , then by the principle of safe substitution    . So by the definition of C ( ),   C ( ), and therefore the set of composables is complete for arbitrary   .

8

Comparison with OWL-S and WSML

OWL-S is another service description language [4], which attempts to address the problem of semantic description via a highly detailed service ontology. But OWL-S also allows for complicated combining forms, which seem to defeat the tractability and practicality of OWL-S. The focus in the design of OWL-S is to describe the structure of a service in terms of how it combines other sub-services (if any used). The description of atomic services in OWL-S is left underspecified. OWL-S includes the tags presents to describe the service profile, and the tag describedBy to describe the service model. The profile describes the (possibly conditional) states that exist before and after the service is executed. The service model describes how the service is (algorithmically) constructed from other simpler services. What the service actually accomplishes has to be inferred from these two descriptions in OWL-S. Given that OWL-S uses complicated combining forms, inferring the task that a service actually performs is, in general, undecidable. In contrast, in USDL, what the service actually does is directly described (via the verb affects and its refinements create, update, delete, and find). OWL-S recommends that atomic services be defined using domain specific ontologies. Thus, OWL-S needs users describing the services and users using the services to know, understand and agree on domain specific ontologies in which the services are described. Thus annotating services with OWL-S is a very time consuming, cumbersome, and invasive process. The complicated nature of OWL-S's combining forms, especially conditions and control constructs, seems to allow for

the aforementioned semantic aliasing problem. The other recent approaches like WSMO,WSML etc also suffer from the same limitation. In contrast, USDL uses the universal WordNet ontology to take care of this problem. Note that USDL and OWL-S can be used together. A USDL description can be placed under the describedBy tag for atomic processes, while OWL-S can be used to compose atomic USDL services. Thus, USDL along with WordNet can be treated as the universal ontology that can make an OWL-S description complete. USDL documents can be used to describe the semantics of atomic services that OWL-S assumes will be described by domain specific ontologies and pointed to by the OWL-S `describedBy' tag. In this respect, USDL and OWL-S are complimentary. Thus USDL can be treated as an extension to OWL-S which makes OWL-S description easy to write and more complete. Also, OWL-S can be regarded as the composition language for USDL. If a new service can be build by composing a few already existing services, then this new service can be described in OWL-S using the USDL descriptions of the existing services. Then this new service can be automatically generated from its OWL-S description. The control constructs like Sequence and If-Then-Else of OWL-S allows us to achieve this. Note once a composite service has been defined using OWL-S that uses atomic services described in USDL, a new USDL description must be written for this composite service. This USDL description is the formal documentation of the new composite service and will make it automatically searchable once the new service is placed in the directory service. The USDL description also allows this composite service to be treated as an atomic service by some other application. For example: The aforementioned reserve service which creates a flight reservation can be viewed as a composite process of first getting the flight details, then checking the flight availability and then booking the flight(creating the reservation). If we have these three atomic services namely GetFlightDetails, CheckFlightAvailability and BookFlight then we can create our reserve service by composing these three services in sequence using the OWL-S Sequence construct. The following is the OWL-S description of the composed reserve service.
<rdf:RDF xmlns:rdf="http://www.w3.org/1999 /02/22-rdf-syntax-ns#" xmlns:process="http://www.daml.org/services /owl-s/1.0/Process.owl#"> <process:CompositeProcess rdf:ID="reserve"> <process:composedOf>

<process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="&GetFlightDetails"/> <process:AtomicProcess rdf:about="&CheckFlightAvailability"/> <process:AtomicProcess rdf:about="&BookFlight"/> </process:components> </process:Sequence> </process:composedOf> </process:CompositeProcess> </rdf:RDF>

We can generate this composed reserve service automatically by using the USDL descriptions of the component services for discovering them from the existing services. Once we have the component services, the OWL-S description can be used to generate the new composed service.

9

Other Related Work

Another related area of research involves message conversation constraints, also known as behavioral signatures [12, 13, 15]. Behavior signature models do not stray far from the explicit description of the lexical form of messages, they expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while USDL deals with higher-level real world concepts. However, USDL and behavioral signatures can be regarded as complimentary concepts when taken in the context of real world service composition and both technologies are currently being used in the development of a commercial services integration tool [14].

10

Conclusion

In order to address the mounting complexity of information technology services integration, standards must be used so that services can be published and documented so that they can be reliably cataloged, searched, and composed in a semi-automatic to fullyautomatic manner. This requires language standards for specifying not just the syntax, i.e. prototypes, of service procedures and messages, but it also necessitates a standard formal, yet high-level means for specifying the semantics of service procedures and messages. We have addressed these issues by defining a service semantics description language, its semantics, and we have proved some useful properties about this language. The current version of USDL incorporates 12

current standards in a way to further aid markup of IT services by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. This approach is more practical and tractable than other approaches because description documents are more easily created by humans and more easily processed by computers. USDL is currently being used to formally describe web-services related to emergency response functions [8]. Future work involves the application of USDL to formally describing commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as to service discovery and rapid application development (RAD) in commercial environments [14]. Future work also includes developing tools that will allow automatic generation of new services based on combining USDL descriptions of existing atomic services. The interesting problem to be addressed is: can USDL description of such automatically generated services be also automatically generated?

[14] T. Hite. Service composition and ranking: A strategic overview. Metallect Inc., 2005. [15] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004. [16] S. Melnik, E. Rahm, and P.Bernstein. Rondo: A programming platform for generic model management. In SIGMOD, 2003. [17] B. C. Pierce. Types and Programming Languages. MIT Press, Cambridge, MA, 2002.

References
[1] Ontology-based information management system, wordnet owl-ontology. http://taurus.unine.ch/ knowler/wordnet.html. [2] Resource description framework. http://www.w3. org/RDF. [3] Sap interface repository. http://ifr.sap.com/ catalog/query.asp. [4] Semantic markup for web services. http://www.daml. org/services/owl-s/1.0/owl-s.html. [5] Web ontology language reference. http://www.w3. org/TR/owl-ref. [6] Web services description language. http://www.w3. org/TR/wsdl. [7] Wordnet: a lexical database for the english language. http://www.cogsci.princeton.edu/~wn. [8] A. Bansal, K. Patel, and G. G. et al. Towards intelligent services: A case study in chemical emergency response. In ICWS, 2005. [9] A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. Automatic querying and composite services generation with usdl. Working paper, 2005. [10] P. Bernstein. Generic model management - a database infrastructure for schema manipulation. In CoopIS, 2001. [11] P. A. Bernstein. Applying model management to classical meta data problems. In CIDR, pages 209220, 2003. [12] Z. Dang, O. H. Ibarra, and J. Sun. Composability of infinite-state activity automata. In ISAAC, 2004. [13] C. E. Gerede, R. Hull, O. H. Ibarra, and J. Su. Automated composition of e-services:lookaheads. In ICSOC, 2004.

13

Black Hole Artificial Bee Colony Algorithm
Artificial bee colony (ABC) is an efficient methodology to solve optimization problems. Here, in this article a modified variant of ABC, namely Black Hole ABC algorithm (BHABC) is proposed which is based on the natural space black hole (BH) phenomenon. In BHABC, the implementation of BH gives a high exploration ability while maintaining the original exploitation ability of the ABC algorithm. The suggested algorithm is judged against 12 benchmark test functions and accessed with original ABC and its two modifications, that are Best So Far ABC (BSFABC) and Modified ABC (MABC). The results reveals that BHABC is a competitive variant of ABC.

A Low-Power Integrated x86-64 and Graphics Processor for Mobile Computing Devices
:
80 stream processors, a media accelerator, an integrated NorthBridge (NB), integrated DisplayPort, LVDS, and VGA display interfaces, a PCIe짰혻Gen1 or Gen2 I/O interface, and a single 64-bit memory channel at up to DDR3-1066 on a single die implemented in a 40 nm bulk CMOS process

Artificial intelligence applications in Permanent Magnet Brushless DC motor drives
Permanent Magnet Brushless DC (PMBLDC) machines are more popular due its simple structure and low cost. Improvements in permanent magnetic materials and power electronic devices have resulted in reliable, cost effective PMBLDC drives, for many applications. Advances in artificial intelligent applications like neural network, fuzzy logic, Genetic algorithm etc. have made tremendous impact on electric motor drives. The brushless DC motor is a multivariable and non-linear system. In conventional PMBLDC drives speed and position sensing of brushless DC motors require high degree of accuracy. Unfortunately, traditional methods of control require detailed modelling of all the motor parameters to achieve this. The Intelligent control techniques like, fuzzy logic control/Neural network control etc. uses heuristic input뱋utput relations to deal with vague and complex situations. This paper presents a literature survey on the intelligent control techniques for PMBLDC motor drives. Various AI혻techniques for PMBLDC motor drives are described. Attempt is made to provide a guideline and quick reference for the researchers and practicing engineers those are working in the area of PMBLDC motor drives.
Weaving Functional and Non-Functional Attributes for Dynamic Web Service Composition
Given the numerous, potentially reusable, Web services available on the Internet, search and composition techniques that efficiently discover viable services will be a strong requirement. A major challenge for dynamic Web service composition will be the ability to measure the quality or reliability of services that are delivered. In this paper, we present a solution for dynamic Web service composition that leverages non-functional attributes provided in the form of Service-Level Agreements (SLA's). The objective of our work is to understand the most efficient algorithms for discovering and composing web services into capabilities with predictable quality. As such, we analyze different approaches to composition when web service composition routines must take into account functional and nonfunctional information. We present our algorithm, a prototype implementation, and experimental results obtained from the various approaches to "weaving" attributes (of different dimensions) as a part of the composition process.
Continuing the Web Services Challenge
:
The capabilities of organizations can be openly exposed, easily searched and discovered, and made readily-accessible to humans and particularly to machines, using service-oriented computing approaches. Artificial intelligence and software engineering researchers alike are tantalized by the promise of ubiquitously discovering and incorporating services into their own business processes (i.e. composition and orchestration). With growing acceptance of service-oriented computing, an emerging area of research is the investigation of technologies that will enable the discovery and composition of web services. The Web Services Challenge (WSC) is a forum where academic and industry researchers can share experiences of developing tools that automate the integration of Web services. In the fourth year (i.e. WSC-08) of the Web Services Challenge, software platforms will address several new composition challenges. Requests and results will be transmitted within SOAP messages. In addition, semantics will be represented as ontologies written in OWL, services will be represented in WSDL, and service orchestrations will be represented in WS-BPEL.

An Agent-Based Approach for Composition of Semantic Web Services
:
The paradigm of Service-oriented computing (SOC) introduces emerging concepts for distributed- and e-business processing enabling the sharing and reuse of service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services. Leading industry approaches rely heavily on syntactical approaches for managing service-based business processes. As such, these approaches are limited since the true functionality of ambiguous capabilities (i.e. web service operations) cannot be inferred. We introduce approaches that disambiguate services by interleaving process-based control with semantic annotations. In this paper, we introduce a generalized architecture where intelligent software agents control process-oriented composition that leverages the descriptiveness of semantics. An outcome of this work is the specification of a multiple agent system where a query agent interacts with multiple repository agents to perform business-oriented service composition.

Co-Logic Programming: Extending Logic Programming with Coinduction
In this paper we present the theory and practice of혻co-logic programming혻(co-LP for brevity), a paradigm that combines both inductive and coinductive logic programming. Co-LP is a natural generalization of logic programming and coinductive logic programming, which in turn generalizes other extensions of logic programming, such as infinite trees, lazy predicates, and concurrent communicating predicates. Co-LP has applications to rational trees, verifying infinitary properties, lazy evaluation, concurrent LP, model checking, bisimilarity proofs, etc.

Coinductive Logic Programming
We extend logic programming셲 semantics with the semantic dual of traditional Herbrand semantics by using greatest fixed-points in place of least fixed-points. Executing a logic program then involves using혻coinduction혻to check inclusion in the greatest fixed-point. The resulting혻coinductive logic programming language혻is syntactically identical to, yet semantically subsumes logic programming with rational terms and lazy evaluation. We present a novel formal operational semantics that is based on혻synthesizing a coinductive hypothesis혻for this coinductive logic programming language. We prove that this new operational semantics is equivalent to the declarative semantics. Our operational semantics lends itself to an elegant and efficient goal directed proof search in the presence of rational terms and proofs. We describe a prototype implementation of this operational semantics along with applications of coinductive logic programming.




SOCA (2016) 10:111133 DOI 10.1007/s11761-014-0167-5

ORIGINAL RESEARCH PAPER

Generalized semantic Web service composition
Srividya Bansal  Ajay Bansal  Gopal Gupta  M. Brian Blake

Received: 29 January 2014 / Revised: 31 August 2014 / Accepted: 23 October 2014 / Published online: 8 November 2014  Springer-Verlag London 2014

Abstract With the increasing popularity of Web Services and Service-Oriented Architecture, we need infrastructure to discover and compose Web services. In this paper, we present a generalized semantics-based technique for automatic service composition that combines the rigor of processoriented composition with the descriptiveness of semantics. Our generalized approach presented in this paper introduces the use of a conditional directed acyclic graph where complex interactions, containing control flow, information flow, and pre-/post-conditions are effectively represented. Composition solution obtained is represented semantically as OWL-S documents. Web service composition will gain wider acceptance only when users know that the solutions obtained are comprised of trustworthy services. We present a framework that not only uses functional and non-functional attributes provided by the Web service description document but also filters and ranks solutions based on their trust rating that is computed using Centrality Measure of Social Networks. Our contributions are applied for automatic workflow generation in context of the currently important bioinformatics domain. We evaluate our engine for automatic workflow generation of a phylogenetic inference task. We also evaluate our engine
S. Bansal (B)  A. Bansal Arizona State University, Mesa, AZ, USA e-mail: srividya.bansal@asu.edu A. Bansal e-mail: ajay.bansal@asu.edu G. Gupta The University of Texas at Dallas, Richardson, TX, USA e-mail: gupta@utdallas.edu M. B. Blake University of Miami, Miami, FL, USA e-mail: m.brian.blake@miami.edu

for automated discovery and composition on repositories of different sizes and present the results. Keywords Service composition  Service discovery  Semantic Web  Ontology  Workflow generation

1 Introduction The next milestone in the evolution of the World Wide Web is making services ubiquitously available. As automation increases, Web services will be accessed directly by the applications themselves rather than by humans [1,2]. In this context, a Web service can be regarded as a "programmatic interface" that makes application-to-application communication possible. To make services ubiquitously available, we need infrastructure that applications can use to automatically discover, deploy, compose, and synthesize services. A Web service is an autonomous, platform-independent program accessible over the web that may affect some action or change in the world. Sample of Web services include common plane, hotel, rental car reservation services or device controls like sensors or satellites. A Web service can be regarded as a "programmatic interface" that makes application-to-application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the actions that it initiates. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order to make Web services more practical, we need an infrastructure that allows users to discover, deploy, synthesize, and compose services automatically. To make services ubiquitously available, we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, composition,

123

112

SOCA (2016) 10:111133

deployment, and synthesis [3]. Several efforts are underway to build such an infrastructure [46]. With regard to service composition, a composite service is a collection of services combined together in some way to achieve a desired effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [7]. Most efforts reported in the literature focus on one or more of these four phases. The first phase involves generating a plan, i.e., all the services and the order in which they are to be composed in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions based on non-functional properties like QoS properties. The last phase involves executing the services as per the plan and in case any of them are not available, an alternate solution has to be used. In this paper, we present a general approach for automatic service composition. Our composition algorithm performs planning, discovery, and selection automatically, all at once, in one single process. This is in contrast to most methods in the literature where one of the phases (most frequently planning) is performed manually. Additionally, our method generates most general compositions based on (conditional) directed acyclic graphs (DAG). Note that service discovery is a special case of composition of n services, i.e., when n = 1. Thus, we mainly study the general problem of automatically composing n services to satisfy the demand for a particular service, posed as a query by the user. In our framework, the DAG representation of the composite service is reified as an OWL-S description. This description document can be registered in a repository and is thus available for future searches. The composite service can now be discovered as a direct match instead of having to look through the entire repository and build the composition solution again. We show how service composition can be applied to a Bioinformatics analysis application, for automatic workflow generation in the field of Phylogenetics [8]. One of the current challenges in automatic composition of Web services also includes finding a composite Web service that can be trusted by consumers before using it. Our approach uses analysis of Social Networks to calculate a trust rating for each Web service involved in the composition and further prune results based on this rating. Web-based Social Networks have become increasingly popular these days. Social Network Analysis is the process of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups, organizations, computers, or any knowledge entity. We propose to measure

the trust factor of a service by measuring the centrality of a service provider and/or a service provider organization in a well-known Social Network. The three level indices that can be applied to measure centrality are degree, betweenness, and closeness [9]. We adopt our idea of computing trust using centrality measure based on the notion of centrality and prestige being key in the study of social networks [9,10]. The role of central people (nodes with high centrality) in a network seems to be fundamental as they adopt the innovation and help in transportation and diffusion of information throughout the rest of the network. So our rationale is that these central figures who play a fundamental role in the network are trusted by others in the network who are connected (directly or indirectly) to them. A simple use case scenario Jane is a researcher in the field of Evolutionary Genetics. One evening she is examining the evolution of crab species and needs to build a Phylogenetic tree for various crab species using protein sequence data. In order to complete this task, she will have to go to her lab and access the computer with necessary software and perform multiple computations using various algorithms. She uses the well-known Molecular Evolutionary Genetics Analysis software program (MEGA5) [11] that is an integrated tool for conducting automatic and manual sequence alignment, inferring phylogenetic trees, mining web-based databases, estimating rates of molecular evolution, inferring ancestral sequences, and testing evolutionary hypotheses. Jane has to use this software to first align the sequence data using one of several algorithms (such as Clustal W [12], MUSCLE [13], etc.) provided by MEGA5 for this purpose. Next, she wants to compute and compare the evolutionary distances for sequences from crab species using various algorithms. In order to do this she must first compute relevant models for crab species. The next step is to compute evolutionary distances using Jukes쵣antor model followed by Tamura쵳ei model and compare them. She is also interested in the evolutionary distance computed based on proportion of amino acid differences. Finally, she is interested in building a Phylogenetic tree from the aligned sequence data. She will have to pick one of the algorithms/methods provided by MEGA5 that include Maximum Likelihood, Minimum Evolution, Maximum Parsimony, Neighbor-Joining, etc. Currently, there is no easy way to perform this analysis and she will have to use the software tools manually and go through this step-by-step process and wait while computation for each of the steps is being performed. Jane will have to go through this laborintensive process in order to find an answer to her research question. Imagine a Software-as-a-Service (SaaS) platform [14] available on the cloud and Jane has access to it from any computer or mobile device. With a few simple clicks, Jane provides a query request that includes input parameters to this workflow process and expected final outputs. The software

123

SOCA (2016) 10:111133

113

platform built upon our composition engine produces multiple possible workflows using different combinations of algorithms/methods (available as Web services) for each of the tasks such as sequence alignment, model computation, distance computation, and generation of phylogeny. Jane picks a workflow that is most suited for her research analysis and possibly even edits the workflow by adding a service to compute the diversity in the subpopulation of crabs. She saves off this workflow to her profile for future use. She initiates the workflow execution and on the following day, analyses the output results that were produced and saved off in her account. This software platform is able to save Jane a significant amount of time--not only in performing the computations for analysis, but also with configuring workflows and using interesting workflows already created by her colleagues. This is just one simple example of the potential of Web service discovery and composition in various disciplines. This paper presents the underlying composition engine that is needed in order to build such a software platform. This paper extends our previous work in the area of Web service composition [15] by conducting a case study on automatic workflow generation for Phylogenetic Inference tasks in Bioinformatics using our composition engine and introducing the computation of a trust rating of each Web service, based on Centrality measure in Social Network analysis, and using this trust rating in filtering and ranking services. This work would support the development of a SaaS platform that supports domain-specific workflow generation. Our research makes the following novel contributions: (i) Formalization of the generalized composition problem based on our conditional directed acyclic graph representation; (ii) Computation of trust rating of composition solutions based on individual ratings of service providers obtained using the Centrality measure of Social Networks; (iii) Efficient and scalable algorithm for solving the composition problem that takes semantics of services into account; our algorithm automatically discovers and selects the individual services involved in composition for a given query, without the need for manual intervention; (iv) Automatic generation of OWL-S descriptions of the new composite service obtained; (v) Case study of our generalized composition engine to automatically generate workflows in the field of Bioinformatics for Phylogenetic Inference tasks. The rest of the paper is organized as follows. In Sect. 2, we present the related work in the area of Web service discovery and composition and discuss their limitations. In Sect. 3, we formalize the generalized Web service composition problem. We present our multi-step narrowing technique for automatic Web service composition and automatic generation of OWLS service description in Sect. 4. We present the implementation and experimental results in Sect. 5. Section 6 presents an application of our generalized composition engine to automatically generate workflows for Bioinformatics analy-

sis tasks. The last section presents conclusions and future work.

2 Related work Composition of Web services has been active area of research [7,16,17]. Most of these approaches present techniques to solve one or more phases of composition as listed in Sect. 1. There are many approaches [6,18,19] that solve the first two phases of composition namely planning and discovery. These are based on capturing the formal semantics of the service using action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available. To the best of our knowledge, most of these approaches that use planning are restricted to sequential compositions, rather than a directed acyclic graph. In this paper, we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential but also non-sequential that can be represented in the form of a directed acyclic graph. The authors in [18] present a composition technique by applying logical inferencing on predefined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also relies on a user-defined plan template, which is created manually. One of the main objectives of our work is to come up with a technique that can automatically produce composition without the need for any manual intervention. Boustil et al. [20] present an approach that uses an intermediate ontology built using OWL-DL and SWRL rules to define the affected object and their relationships. Their selection strategy considers relationships between services by looking at object values of affected objects. They use a custom intermediate ontology that is built within their framework using OWL-DL. Our approach focuses on the semantics of the parameters as well as constraints represented as pre- and post-conditions. Also our approach is generic and can be used with any domain ontology to provide semantics. There are industry solutions based on WSDL and BPEL4 WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service by composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when an explicit flow is provided. In contrast, our technique auto-

123

114

SOCA (2016) 10:111133

matically determines these complex flows using semantic descriptions of atomic services. A process-level composition solution based on OWL-S is proposed in [19]. In this work, the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but instead assume that they already have the list of atomic services. In contrast, we present a technique that automatically finds the services that are suitable for composition based on the query requirements for the new composed service. There are solutions such as [21] that solve the selection phase of composition. This work uses pre-defined plans and discovered services provided in a matrix representation. Then, the best composition plans are selected and ranked based on QoS parameters like cost, time, and reputation. These criterions are measured using fuzzy numbers. There has been a lot of work on composition languages such as WS-BPEL, FuseJ, AO4BPEL, etc. which are useful only during the execution phase. FuseJ is a description language for unifying aspects and components [22]. Though this language was not designed for Web services, the authors contend that it can be used for service composition as well. It uses connectors to interconnect services. We believe that there is no centralized process description, but instead information about services is spread across the connectors. With FuseJ, the planning phase has to be performed manually that is the connectors have to be written by the developer. Similarly, OWL-S also describes a composite service but does not automatically find the services involved in the composition. So these languages are only useful for execution which happens after the planning, discovery, and selection of services is done. Service grounding of OWL-S maps that describe abstract services to the concrete WSDL specification helps in executing the service. In contrast, our approach automatically generates the composite service. This new composite service generated can then be described using one of these composition languages. QoS-aware composition has also been active area of research [6,21]. Research on a QoS-aware composition [23 25] consider applying SLA's to workflow compositions or Web service compositions, although they do not perform dynamic composition. They use one of the existing composition languages to create the composite service manually or create a template that is later used to select appropriate services for each stage of composition. After obtaining composition solutions manually or semi-automatically, these approaches present a QoS model and apply the nonfunctional attributes on the potential solutions to confirm that they comply with the pre-defined agreements. Thus, the solutions are pruned based on SLA compliance. Work

on workflow Composition of service-level agreements [26] presents a set of SLA measures and principles that best support QoS-based Composition. A model and representation of SLA attributes were introduced and an approach to compose SLA's associated with a workflow of Web services was presented. The research on creating a QoS-Aware middleware for Web service Composition in [27] is similar to our work as they identify services that can fit into a useful composition based on QoS measures. They use two approaches for selection: one based on local (task-level) selection of services and the second is based on a global allocation of tasks to services. They also use a template for composition; in this case, a state chart that has the generic service tasks defined. Finding a composite service involves finding concrete services that fit into the template. In contrast, we do not use any template but instead find the composition solution automatically. The work presented in [28] combine semantic annotations and SLA's thereby providing better approach to specification of SLA's. Researchers have looked into a fuzzy linguistic preference model to provide preference relations on various QoS dimensions [29]. They use a specific weighting procedure to provide numeric weights to preference relations, and then use a hybrid evolutionary algorithm to find skyline solutions efficiently. Their algorithm is designed on the basis of Pareto-dominance and weighted Tchebycheff distance. In this approach, the authors assume that they have candidate services for composition. Their algorithm helps identify best solution based on their SLA's. Feng's research group proposed an approach to composition that associated QoS attributes to service dependencies and showed their approach could model real-life services and perform effective QoS constraint satisfaction and optimization [30]. The attributes taken into consideration by this study are Response time, Cost, Reliability, Availability, and Reputation. They consider that QoS values of a service may be dependent not only on the service itself but also some other services in the workflow. They propose 3 types of QoS for each attribute namely: default QoS, partially dependent QoS, and totally dependent QoS. Default QoS applies no matter what the preceding service is in a workflow, just like the conventional QoS. Partially dependent QoS applies if and only if some of the inputs of a service are provided by the outputs of another service. Totally dependent QoS applies if and only if all inputs of a service are provided by the outputs of another service. Formal modeling of QoS attributes is provided in OWL-S [27]. Work by Wen et al. [32] presents an approach to obtaining probabilistic top-K dominating services with uncertain QoS. QoS values tend to fluctuate at run-time and hence this approach uses probabilistic characteristics of service instances to identify dominating service abilities for better selection. A detailed survey of approaches for a reliable dynamic Web service composition is presented by Immonen

123

SOCA (2016) 10:111133

115

and Pakkala [33]. They discuss various approaches that use Reliability ontology to manage and achieve reliable composition. They address the lack of formalization to handle reliability of composition, whereas the focus of our approach is the formalization of a generalized composition that uses functional attributes to compose a solution and non-functional attributes help in further filtering and ranking solutions. A number approaches focus on trust and reputation QoS criteria for service selection. Mehdi et al.'s [34] approach assigns trust scores to Web services and only services with highest scores are selected for composition. They use Bayesian networks to learn the structure of composition. The approach presented by Kutler et al. [35] considers social trust in Web service composition. They compute trust based on similarity measures over ratings of users added into a system. They use the correlation between trust and overall similarity measures in online communities. On the contrary, we use the centrality measure in a Web-based Social Network. In this paper, we present a technique for automatically planning, discovering, and selecting services that are suitable for obtaining a composite service based on user-query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about services involved or use human input on what services would be suitable for composition. This work is an extension of our earlier work [15] that introduced a generalized Web service composition engine. In this paper, we use the trust rating of a Web service in addition to the functional and non-functional attributes of a service in filtering and ranking solutions. In this paper, we evaluate our composition the engine using a case study from the bioinformatics domain for Phylogenetic inference tasks to show that this engine can be used for automatic workflow generation. The case study uses example workflows that would be generated as a sequential composition, non-sequential composition as well as non-sequential conditional composition.

tasks and develop the requirements of an ideal discovery/composition engine. 3.1 The discovery problem Given a repository of Web services, and a query requesting a service (hereafter query service), automatically finding a service from the repository that matches these requirements is the Web service discovery problem. Only those services that produce at least the requested output parameters that satisfy the post-conditions and use only from the provided input parameters that satisfy the pre-conditions and produce the same side effects can be valid solutions to the query. Some of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre-/post-conditions, and side effect requirements. This activity is best illustrated using an example: Example (Discovery) A buyer is looking for a service to buy a book and the directory of services contains services S1 and S2 . Table 1 shows the input/output parameters of the query and services S1 and S2 . In this example service S2 satisfies the query; however, S1 does not as it requires BookISBN as an input and it is not provided by the query. Our query requires ConfirmationNumber as the output and S2 produces ConfirmationNumber and TrackingNumber. The extra output produced can be ignored. Also the semantic descriptions of service input/output parameters should be same as the query parameters or satisfy the subsumption relation. The discovery engine should be able to infer that the query parameter BookTitle and input parameter BookName of service S2 are semantically the same concepts. This can be inferred using semantics from the annotation of the service and the ontology (e.g., OWL WordNet ontology) provided. The query also has a pre-condition that the CreditCardNumber is numeric, which should logically imply pre-conditions of the discovered service. Definition (Service) A service is a 6-tuple of its preconditions, inputs, side effect, affected object, outputs and post-conditions. S = (CI, I , A , AO, O , CO) is the representation of a service where CI is the list of pre-conditions, I is the input list,

3 Automated Web service discovery and composition Discovery and composition are two important tasks related to Web services. In this section, we formally describe these
Table 1 Discovery--example Service Query S1 S2 Input parameters BookTitle, CreditCardNumber, AuthorName, CreditCardType BookName, AuthorName, BooklSBN, CreditCardNumber BookName, CreditCardNumber Pre-conditions

Output parameters ConfirmationNumber ConfirmationNumber

Post-conditions

lsNumeric(CreditCard Number)

lsNumeric(CreditCard Number)

ConfirmationNumber, TrackingNumber

123

116

SOCA (2016) 10:111133

A is the service's side effect, AO is the affected object, O is the output list, and CO is the list of post-conditions. The preand post-conditions are ground logical predicates. Definition (Repository of Services) Repository ( R ) is a set of Web services. Definition (Query) The query service is defined as Q = (CI , I , A , AO , O , CO ) where CI is the list of preconditions, I is the input list, A is the service affect, AO is the affected object, O is the output list, and CO is the list of post-conditions. These are all the parameters of the requested service. Definition (Discovery) Given a repository R and a query Q , the discovery problem can be defined as automatically finding a set S of services from R such that S = {s | s = I, A = (CI, I , A , AO, O , CO), s R , CI  CI, I A , AO = AO , CO  CO , O  O }. The meaning of is the subsumption (subsumes) relation and  is the implication relation. For example, say x and y are input and output parameters, respectively, of a service. If a query has (x > 5) as a pre-condition and ( y > -x ) as post-condition, then a service with pre-condition (x > 0) and post-condition ( y > x ) can satisfy the query as (x > 5)  (x > 0) and

( y > x )  ( y > -x ) since (x > 0). Figure 1 shows the substitution rules for the discovery problem. 3.2 The composition problem Given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 2 shows an example composite service made up of five services S1 to S5 . In figure, I and CI are the query input parameters and pre-conditions, respectively. O and CO are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and S j indicates that outputs of Si constitute (some of) the inputs of S j . Example (Sequential Composition) Suppose we are looking for a service to make travel arrangements, i.e., flight, hotel, and rental car reservations. The directory of services contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 2 shows the input/output parameters of the user query and the three services ReserveFlight, ReserveHotel, and ReserveCar. For the sake of simplicity, the query and services have fewer input/output parameters than the realworld services. In this example, service ReserveFlight has to be executed first so that its output ArrivalFlightNum can be used as input by ReserveHotel followed by the service ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Figure 3 shows this example sequential composition as a directed acyclic graph. Definition (Sequential Composition) The sequential Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R , given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges

Fig. 1 Substitutable service

Fig. 2 Composite service represented as a directed acyclic graph

Table 2 Sequential composition example Service Query ReserveFlight ReserveHotel ReserveCar Input parameters PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, ArrivalFlightNum, StartDate, ReturnDate PassengerName, ArrivalDate, ArrivalFlightNum, HotelAddress Pre-conditions Output parameters HotelConfirmationNum, CarConfirmationNum FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress CarConfirmationNum Post-conditions

123

SOCA (2016) 10:111133

117

Fig. 3 Sequential composition example

Fig. 5 Non-sequential composition example

Fig. 4 Sequential composition

of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: i Si V , Si R , Si = (CIi , Ii , Ai , AOi , Oi , COi ) 1. I I 1 , O1 I 2 , . . . , On O 2. CI  CI1 , CO1  CI2 , . . . , COn  CO The meaning of the is the subsumption (subsumes) relation, and  is the implication relation. In other words, we are deriving a possible sequence of services where only the provided input parameters are used for the services and at least the required output parameters are provided as an output by the chain of services. The goal is to derive a solution with minimal number of services. Also, the post-conditions of a service in the chain should imply the pre-conditions of the next service in the chain. Figure 4 depicts an instance of sequential composition. Example (Non-sequential composition) Suppose we are looking for a service to buy a book and the directory of services contains services GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. Table 3 shows the input/output parameters of the query and the four services in the repository. Suppose a single matching service is not
Table 3 Non-sequential composition example Service Query GetlSBN GetAvailability Authorize CreditCard PurchaseBook Input parameters BookTitle, CreditCardNum, AuthorName, CardType BookName, AuthorName BooklSBN CreditCardNum BooklSBN, NumAvailable, AuthCode

found in the repository, a solution is synthesized from among the set of services available in the repository. Figure 5 shows this composite service. The post-conditions of the service GetAvailability should logically imply the pre-conditions of service PurchaseBook. Definition (Non-sequential composition) More generally, the Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R , given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: 1. i Si V where Si has exactly one incoming edge that represents the query inputs and pre-conditions, I i Ii , CI  i CIi . 2. i Si V where Si has exactly one outgoing edge that represents the query outputs and post-conditions, O i Oi , CO  i COi . 3. i Si V where Si has at least one incoming edge, let Si 1 , Si 2 , . . . , Sim be the nodes such that there is a directed edge from each of these nodes to Si . Then, Ii k Oik  I , CIi  (COi 1  COi 2 . . .  COim  CI ). Figure 6 depicts an instance of non sequential composition.

Pre-conditions

Output parameters ConfNumber ConfNumber NumAvailable AuthCode

Post-conditions

NumAvailable > 0 AuthCode > 99  AuthCode < 1000

NumAvailable > 0

ConfNumber

123

118

SOCA (2016) 10:111133

Fig. 6 Non-sequential composition

Example (Non-sequential conditional composition) Nonsequential conditional composition consists of if-then-else conditions, i.e., the composition flow varies depending on the result of the post-conditions of a service. Suppose we are looking for a service to make international travel arrangements. We first need to make a tentative flight and hotel reservation and then apply for a visa. If the visa is approved, we can buy the flight ticket and confirm the hotel reservation, else we will have to cancel both the reservations. Also, if the visa is approved, we need to make a car reservation. The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table 4 shows the input/output parameters of the user query and services. In this example, service ProcessVisa produces the post-condition VisaApproved  VisaDenied. The services ConfirmFlight and ConfirmHotel have the pre-condition VisaApproved. In this case, one cannot determine whether the post-conditions of
Table 4 Non sequential conditional composition example Service Query Pre-conditions Input parameters

service ProcessVisa implies the pre-conditions of services ConfirmFlight and ConfirmHotel until the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and depending on the outcome of the condition, the corresponding services will be executed. The vertex for service ProcessVisa in the graph is followed by a condition node which represents the postcondition of service ProcessVisa. This node has two outgoing edges one representing the case if the condition is satisfied at run-time and other edge for the case where the condition is not satisfied. In other words, these edges represent the generated conditions which in this case are, (VisaApproved  VisaDenied)  VisaApproved and VisaApproved  VisaDenied)  VisaDenied. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed. Figure 7 shows this conditional composition example as a directed acyclic graph.

Definition (Generalized Composition) The generalized Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R , given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph either represents a service involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can be determined only after the execution of the service. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph:

Output parameter FlightConfirmationNum, HotelConfirmationNum, CarConfirmationNum FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress ConfirmationNum

Post-conditions

PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, ArrivalFlightNum, StartDate, ReturnDate PassengerName, VisaType, FlightConfirmationNum, HotelConfirmationNum VisaApproved VisaApproved VisaDenied VisaDenied FlightConfirmationNum, CreditCardNum HotelConfirmationNum, CreditCardNum FlightConfirmationNum, PassengerName HotelConfirmationNum, PassengerName PassengerName, ArrivalDate, HotelAddress, ArrivalFlightNum

ReserveFlight ReserveHotel ProcessVisa

VisaApproved V VisaDenied

ConfirmFlight ConfirmHotel CancelFlight Cancel Hotel ReserveCar

FlightConfirmationNum HotelConfirmationNum CancelCode CancelCode CarConfirmationNum

123

SOCA (2016) 10:111133

119

Fig. 7 Non-sequential conditional composition

1. i Si V where Si has exactly one incoming edge that represents the query inputs and pre-conditions, I i Ii , CI  i CIi . 2. i Si V where Si has exactly one outgoing edge that represents the query outputs and post-conditions, O i Oi , CO  i COi . 3. i Si V where Si represents a service and has at least one incoming edge, let Si 1 , Si 2 , ..., Sim be the nodes such that there is a directed edge from each of these nodes to Si . Then, Ii k Oik  I , C Ii  (COi 1  COi 2 . . .  COim  CI ). 4. i Si V where Si represents a condition that is evaluated at run-time and has exactly one incoming edge, let S j be its immediate predecessor node such that there is a directed edge from S j to Si . Then, the inputs and preconditions at node Si are Ii = O j  I ; CIi = CO j . The outgoing edges from Si represent the outputs that are same as the inputs Ii and the post-conditions that are the result of the condition evaluation at run-time.

post-conditions CO1 of S1 must imply the preconditions CI2 of S2 . The following conditions are evaluated at run-time: if (CO1  CI2 ) then execute S1; else if (CO1   CI2 then no-op}; else if (CI2 then execute S1 }; When the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem. Further details and examples are available in our prior work [15]. 3.3 Requirements of an ideal engine The features of an ideal discovery/composition engine are as follows: Correctness One of the most important requirements for an ideal engine is to produce correct results, i.e., the services discovered and composed by it should satisfy all the requirements of the query. Also, the engine should be able to find all services that satisfy the query requirements. Minimal query execution time Querying a repository of services for a requested service should take a reasonable amount of (minimal) time, i.e., a few milliseconds. Here, we assume that the repository of services may be pre-processed (indexing, change in format, etc.) and is ready for querying. In case services are not added incrementally, then time for pre-processing a service repository is a one-time effort that takes considerable amount of time, but gets amortized over a large number of queries. Incremental updates Adding or updating a service to an existing repository of services should take minimal time. An ideal discovery and composition engine should not preprocess the entire repository again; rather incrementally update pre-processed data (indexes, etc.) with data for the new service.

The meaning of the is the subsumption (subsumes) relation and  is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. When it cannot be determined at compile time whether the post-conditions imply the pre-conditions or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible conditions that will be evaluated at run-time. Depending on the condition that holds, the corresponding services are executed. That is, if a subservice S1 is composed with subservice S2 , then the

123

120

SOCA (2016) 10:111133 Table 5 Degree centrality of nodes in Fig. 10 Service provider Provider A Provider B Provider C Provider D Provider E Provider F Provider G Provider H Provider I Provider J Provider K Provider L Provider M Degree 2 3 1 8 3 4 5 3 1 2 4 1

Cost function If there are costs associated with every service in the repository, then an ideal discovery and composition engine should be able to provide results based on requirements (minimize, maximize, etc.) over the costs. We can extend this to services having an associated attribute vector, and the engine should be able to provide results based on maximizing or minimizing functions over the attribute vector. These requirements have driven the design of our semanticsbased discovery and composition engine described in this paper. 3.4 Centrality measure in social networks Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds. It involves measuring the formal and informal relationships to understand information/knowledge flow that binds the interacting units that could be a person, group, organization, or any knowledge entity. Social Network Analysis has an increasing application in social sciences that has been applied to diverse areas such as psychology, health, electronic communications, and business organization. In order to understand social networks and their participants, the location of an actor in a network is evaluated. The network location is measured in terms of centrality of a node that gives an insight into the various roles and groupings in a network. Centrality gives a rough indication of the social power of a node based on how well they "connect" the network. There has been extensive discussion in the Social Network community regarding the meaning of the term centrality when it is applied to Social Networks. One view stems directly from graph theory [9]. The graph-theoretic conception of compactness has been extended to the study of Social Networks and simply renamed "graph centrality". Their measures are all based upon distances between points, and all define graphs as centralized to the degree that their points are all close together. The alternative view emerged from substantive research on communication in Social Networks. From this perspective, the centrality of an entire network should index the tendency of a single point to be more central than all other points in the network. Measures of a graph centrality of this type are based on differences between the centrality of the most central point and that of all others. Thus, they are indexes of the centralization of the network [36]. The three most popular individual centrality measures are Degree, Betweenness, and Closeness Centrality.  Degree centrality The network activity of a node can be measured using the concept of degrees, i.e., the number of direct connections a node has. In the example, network shown in Fig. 10 and Table 5, Provider D has the most direct connections in the network, making it the most

active node in the network. In personal Social Networks, the common thought is that "the more connections, the better".  Betweenness centrality Though Provider D has many direct ties, Provider H has fewer direct connections (close to the average in the network). Yet, in many ways, Provider H has one of the best locations in the network by playing the role of a "broker" between two important components. A node with high betweenness has greater influence over what flows and does not in the network.  Closeness centrality Provider F and G have fewer connections than Provider D, yet the pattern of their direct and indirect ties allow them to access all the nodes in the network more quickly than anyone else. They have the shortest paths to all others, i.e., they are close to everyone else. They are in an excellent position and have the best visibility into what is happening in the network.

Individual network centralities provide insight into the individual's location in the network. The relationship between the centralities of all nodes can reveal much about the overall network structure. 3.5 Trust rating of a service and trust threshold The trust rating of each service in the repository is computed as a measure of the degree centrality (CD) of the social network to which the service provider belongs. It is calculated as the degree or count of the number of adjacencies for a node, sk : C D (s k ) =
n i -0

a (si , sk )

123

SOCA (2016) 10:111133

121

where a (si sk ) = 1 iff si and sk are connected by a line 0 otherwise As such it is a straightforward index of the extent to which sk is a focus of activity [9]. C D (sk ) is large if service provider sk is adjacent to, or in direct contact with, a large number of other service providers, and small if sk tends to be cut off from such direct contact. C D (sk ) = 0 for a service provider that is totally isolated from any other point. Our algorithm filters out any services whose provider has a zero degree centrality in a social network, i.e., such services will not be used in building composition solutions. Trust rating of the entire composite service is computed as an average of the individual trust ratings of the services involved in the composition. We also need to set a Trust Threshold and any service with a Trust rating that is below this threshold is not used while generating composition solutions. In our initial prototype implementation, we set the Trust threshold to zero, i.e., degree centrality of the service provider in the network is zero. A service provider or service provider organization that is not connected to any other nodes in the Social network is not known to anyone else and is an immediate reason to be pruned out from composition solutions, as the service cannot be trusted. Composition solutions can be ranked such that solutions with highest trust rating appear on top of the list. 4 Dynamic Web service composition: methodology In this section, we describe our methodology for automatic Web service composition that produces a general directed acyclic graph. The composition solution produced is for the generalized composition problem presented in Sect. 3. We also present our algorithm for automatic generation of OWLS descriptions for the new composite service produced. 4.1 Algorithms for Web service discovery and composition Our approach is based on a multi-step narrowing of the list of candidate services using various constraints at each step. As mentioned earlier, discovery is a simple case of Composition. When the number of services involved in the composition is exactly equal to one, the problem reduces to a discovery problem. Hence, we use the same engine for both discovery and composition. We assume that a directory of services has already been compiled, and that this directory includes semantic descriptions for each service. In our implementation, we use semantic descriptions written in USDL [37], although the algorithms are general enough that they will work with any semantic annotation language. The repository of services contains one USDL description document for each service. However, we still need a query language to

search this directory, i.e., we need a language to frame the requirements of the service that an application developer is seeking. USDL itself can be used as such as a query language. A USDL description of the desired service can be written (with tool assistance), a query processor can then search the service directory for a "matching" service. For service composition, the first step is finding the set of composable services. USDL itself is used to specify the requirements of the composed service that an application developer is seeking. Using the discovery engine, individual services that make up the composed service can be selected. Part substitution techniques [38] can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the individual services. That is, if a subservice S1 is composed with subservice S2 , then the post-conditions of S1 must imply the pre-conditions of S2 . The goal is to derive a single solution, which is a directed acyclic graph of services that can be composed together to produce the requested service in the query. Figure 8 shows a pictorial representation of our composition engine. 4.2 Multi-step narrowing solution To produce a composite service, as shown in the example Fig. 2, our algorithm filters services that are not useful for the composition at multiple stages. Figure 9 shows the filtering technique for the particular instance graph represented in Fig. 2. The composition routine begins with the query input parameters and finds all those services from the repository that require a subset of the query input parameters. In Fig. 9, CI , I are the pre-conditions and the input parameters provided by the query. S1 and S2 are the services found after step 1. O1 is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e., I2 = O1  I. I2 is used to find services at the next stage, i.e., all those services that require a subset of I2 . To make sure we do not end up in cycles, we get only those services that require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point, we make another pass in the reverse direction to remove redundant services that do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters and working our way backwards. Next, another level of filtering is performed using the trust ratings of services. Table 6 shows the algorithm and we have prototype implementation of this algorithm implemented using Prolog [39] with Constraint Logic Programming over finite domain (CLP(FD)) [40]. The rationale behind the choice of

123

122 Fig. 8 Composition engine--design

SOCA (2016) 10:111133

Fig. 9 Multi-step narrowing solution Table 6 Algorithm for multi-step narrowing Multi-step Narrowing Algorithm Algorithm: Composition (Input QI - QueryInputs, QO - QueryOutputs, QCI - Pre-Cond, QCO - Post-Cond, T - TrustThreshold) (Output: Result - ListOfServices) 1. L  NarrowServiceList(QI, QCI); 2. O  GetAllOutputParameters(L); 3. CO  GetAllPostConditions(L); 4. While Not (O QO) 5. I = QI  O; CI  QCI  CO; 6. L  NarrowServiceList(I, CI); 7. End While; 8. IntResult  RemoveRedundantServices(QO, QCO); 9. Result  RemoveRedundantServices(T, IntResult); 10. Return Result;

CLP(FD), implementation details, and experimental results are available [15]. 4.3 Automatic generation of OWL-S descriptions After obtaining a composition solution (sequential, nonsequential, or conditional), the next step is to produce a semantic description document for this new composite service. This document can be used for execution of the service

and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of performing the composition process all over again. We used the existing language OWL-S [31] to describe composite services. OWL-S models services as processes and when used to describe composite services, it maintains the state throughout the process. It provides control constructs such as Sequence, Split-Join, If-Then-Else and many more to describe composite services. These control constructs can be used to describe

123

SOCA (2016) 10:111133 Table 7 Generation of composite service description Generation of Composite Service Description Algorithm: GenerateCompositeServiceDescription (Input: G - CompositionSolutionGraph) (Output: D - CompositeServiceDescription) 1. Generate generic header constructs 2. Start Composite Service element 3. Start SequenceConstruct 4. If Number(SourceVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each starting/source Vertex V GenerateAtomicService End For EndSplitJoinConstruct End If 5. If Number(SinkVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each ending/sink Vertex V GenerateAtomicService End For EndSplitJoinConstruct End If 6. For Each remaining vertex V in G If V is AND vertex with one outgoing edge GenerateAtomicService If V is AND vertex with > 1 outgoing edge GenerateSplitJoinConstruct If V is OR vertex with one outgoing edge GenerateAtomicService qquad If V is OR vertex with > 1 outgoing edge GenerateConditionalConstruct End For 7. End SequenceConstruct 8. End Composite Service element 9. Generate generic footer constructs

123

rently. The process completes execution only when all the services in this construct have completed their execution. The non-sequential conditional composition can be described in OWL-S using the If-Then-Else construct which specifies the condition and the services that should be executed if the condition holds and also specifies what happens when the condition does not hold. Conditions in OWL-S are described using SWRL. There are other constructs such as looping constructs in OWL-S that can be used to describe composite services with complex looping process flows. We are currently investigating other kinds of compositions with iterations and repeat-until loops and their OWL-S document generation. We are exploring the possibility of unfolding a loop into a linear chain of services that are repeatedly executed. We are also analyzing our choice of the composition language and looking at other possibilities as part of our future work. 5 Implementation and experimental results This section presents implementation details of the composition engine. We also analyze the performance and present experimental results. 5.1 Implementation Our discovery and composition engine is implemented using Prolog [39] with Constraint Logic Programming over finite domain [40], referred to as CLP(FD) hereafter. In our current implementation, we used semantic descriptions written in the language called Universal Semantics-Service Description Language (USDL) [37]. The repository of services contains one USDL description document for each service. USDL itself is used to specify the requirements of the service that an application developer is seeking. USDL is a language that service developers can use to specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real-world concepts. USDL uses WordNet [41] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the "meaning" of input parameters, outputs, and the side effect induced by the service is given by mapping these syntactic terms to concepts in WordNet [38] for details of the representation. Inclusion of USDL descriptions thus makes services directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory for a "matching" service. These algorithms can be used with any other Semantic Web service descrip-

the kind of composition. OWL-S also provides a property called composedBy using which the services involved in the composition can be specified. Table 7 shows the algorithm for generation of the OWL-S document when the composition solution in the form of a graph is provided as the input. A sequential composition can be described using the Sequence construct that indicates that all the services inside this construct have to be invoked one after the other in the same order. The non-sequential composition can be described in OWL-S using the Split-Join construct which indicates that all the services inside this construct can be invoked concur-

123

124

SOCA (2016) 10:111133

A USDL description of the desired service can be written, which is read by the query reader and converted to a triple. This module can be easily extended to read descriptions written in other languages. (iii) Semantic relations generator We obtain the semantic relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms, meronyms, holonyms and many more. USDL descriptions point to OWL WordNet for the meanings of concepts. A theory of service substitution is described in detail in [38] which uses the semantic relations between basic concepts of WordNet to derive the semantic relations between services. This module extracts all the semantic relations and creates a list of Prolog facts. We can also use any other domain-specific ontology to obtain semantic relations of concepts. We are currently looking into making the parser in this module more generic to handle any other ontology written in OWL. (iv) Discovery query processor This module compares the discovery query with all the services in the repository. The processor works as follows: 1. On the output parameters of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a parameter with hyponym relation [38], i.e., a specific substitutable. 2. On the input parameters of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a parameter with hypernym relation [38], i.e., a generic substitutable. The discovery engine, written using Prolog with CLP(FD) library, uses a repository of facts, which contains a list of all services, their input and output parameters and semantic relations between parameters. The code snippet of our engine is shown in Table 8. The query is parsed and converted into a Prolog query that looks as follows: discovery(sol(queryService, ListOfSolutionServices). The engine will try to find a list of SolutionServices that match the queryService. (v) Composition engine (ii) Query reader This module reads the query file and passes it on to the Triple Generator. We use USDL itself as the query language. The composition engine is written using Prolog with CLP(FD) library. It uses a repository of facts, which contains all the services, their input and output parameters and the semantic

Fig. 10 A social network of Web service providers

tion language as well. It will involve extending our implementation to work for other description formats, and we are looking into that as part of our future work. The parsing of all the USDL description documents and the universal ontology is written in Java. The parsing is done via SAXReader library of Java and after the parsing, prolog engine is instantiated to run the Composition query processor. The complete discovery and composition engine is implemented as a Web service in Java using Apache Tomcat. The Web service in turn invokes Prolog to do all the processing [4244]. The high-level design of the Discovery and Composition engines is shown in Fig. 10. The software system is made up of the following components: (i) Triple generator The triple generator module converts each service description into a triple. In this case, USDL descriptions are converted to triples like: (Pre-Conditions, affect-type(affected-object, I, O), PostConditions) The function symbol affect-type is the side effect of the service and affected object} is the object that changed due to the side effect. I is the list of inputs, and O is the list of outputs. Pre-Conditions are the conditions on the input parameters, and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [38]. In case conditions on a service are not provided, the Pre-Conditions and PostConditions in the triple will be null. Similarly, if the affecttype is not available, this module assigns a generic affect to the service.

123

SOCA (2016) 10:111133 Table 8 Discovery Algorithm--Code Snippet Discovery Algorithm discovery(sol(Qname,A)) :dQuery(Qname,I,O), encodeParam(O,OL), /* Narrow candidate services(S) using output list(OL) */ narrowO(OL,S), fdset(S,FDs), fdsettolist(FDs,SL), /* Expand InputList(I) using semantic relations */ getExtInpList(I, ExtInpList), encodeParam(ExtInpList,IL), /* Narrow candidate services(SL) using input list (IL) */ narrowI(IL,SL,SA), decodeS(SA,A).

125

step narrowing-based approach to solve these problems and implemented it using constraint logic programming. (i) Correctness Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions. Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always find a correct solution (if one exists) in the minimum possible number of steps. (ii) Pre-processing Our system initially pre-processes the repository and converts all service descriptions into Prolog terms. The semantic relations are also processed and loaded as Prolog terms in memory. Once the pre-processing is done, then discovery or composition queries are run against all these Prolog terms and, hence, we obtain results quickly and efficiently. The built-in indexing scheme and constraints in CLP (FD) facilitate the fast execution of queries. During the pre-processing phase, we use the term representations of services to set up constraints on services and the individual input and output parameters. This further helped us in getting optimal results. (iii) Execution efficiency The use of CLP (FD) helped significantly in rapidly obtaining answers to the discovery and composition queries. We tabulated processing times for different size repositories, and the results are shown in the next section. As one can see, after pre-processing the repository, our system is quite efficient in processing the query. The query execution time is insignificant. (iv) Programming efficiency The use of Constraint Logic Programming helped us in coming up with a simple and elegant code. We used a number of built-in features such as indexing, set operations, and constraints and, hence, did not have to spend time coding these ourselves. This made our approach efficient in terms of programming time as well. Not only the whole system is about 200 lines of code, but we also managed to develop it in less than 2 weeks. (v) Scalability Our system allows for incremental updates on the repository, i.e., once the pre-processing of a repository is done, adding

Table 9 Composition Algorithm--Code Snippet Composition Algorithm composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs), encodeParam(QueryOutputs, QO), getExtInpList(QueryInputs, InpList), encodeParam(InpList, QI), performForwardTask(QI, QO, LF), performBackwardTask(LF, QO, LR), getMinSolution(LR, QI, QO, A), reverse(A, RevA), confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

relations between the parameters. A code snippet of our composition engine is shown in Table 9. The query is converted into a Prolog query that looks as follows: composition(queryService, ListOfServices). The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the built-in, higher order predicate "bagof" to return all possible ListOfServices that can be composed to get the requested queryService. (vi) Output generator After the Composition engine finds a matching service, or the list of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files in any desired XML format. 5.2 Efficiency and scalability issues In this section, we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It is because of these features that we decided on the multi-

123

126

SOCA (2016) 10:111133 Table 10 Sample service interface description Sample WSDL description <message name ="InputName"> <part name = "part0" type = "Name"/> </message > <message name ="OutputAddress"> <part name = "part0" type = "US-Address" /> </message> <portType name = "AdressConverter"> <operation name ="Convert" > <input message ="InputName" /> <output message = "OutputAddress"/> </operation > </ portType >

a new service or updating an existing one will not need reexecution of the entire pre-processing phase. Instead, we can easily update the existing list of CLP (FD) terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will be negligible, perhaps a few milliseconds. With real-world services, it is likely that new services will get added often or updates might be made on existing services. In such a case, avoiding repeated preprocessing of the entire repository will definitely be needed and incremental updates will be of great practical use. The efficiency of the incremental update operation makes our system highly scalable. (vi) Use of external database In case the repository grows extremely large in size, then saving off results from the pre-processing phase into some external database might be useful. This is part of our future work. With extremely large repositories, holding all the results of pre-processing in the main memory may not be feasible. In such a case, we can query a database where all the information is stored. Applying incremental updates to the database is easily possible, thus avoiding recomputation of preprocessed data. (vii) Searching for optimal solution If there are any properties with respect to which the solutions can be ranked, then setting up global constraints to get the optimal solution is relatively easy with the constraint-based approach. For example, if each service has an associated cost, then the discovery and the composition problem can be redefined to find the solutions with the minimal cost. Our system can be easily extended to take these global constraints into account. 5.3 Performance and experimental results To conduct our experiments, we looked at various benchmarks and Web services challenge (WSC) datasets [42,43] best suited our needs and fit well into the overall architecture with minimal changes. They provided semantics through XML schema, provided queries and corresponding solutions. We used repositories from WSC website [42,43], slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The input and output messages of the services may contain multiple parameters. Each parameter is annotated with a semantic concept stored in the attribute type. Table 10 shows a service AddressConverter with one operation named Convert. It can be invoked with an input message (InputName) and produces a response message (OutputAddress). The value of

the attribute message represents a reference to a message element. Each message has a set of part elements as children, which represent the service parameters, annotated with concepts referenced by the type-attribute. The Convert operation in this example requires a parameter of the type Name and returns an instance of US-Address. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema format. Concepts were treated as data types and taxonomies encoded as hierarchies of such data types in XSD schemas. The subsumes relation between two semantic concepts can be compared to the subclass relationship in Object-oriented programming. Table 11 shows a sample XSD schema defining the data types Address and US-Address inheriting from Address. In the context of the WSC, this schema would be interpreted as a taxonomy introducing the concepts Address and US-Address with subsumes(Address, US-Address). We evaluated our approach on different size repositories and tabulated Pre-processing, Query Execution, and Incremental update time. We noticed that there was a significant difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we found is that the repository was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 12 shows performance results of our Composition algorithm on discovery queries, and Table 13 shows the results of our algorithm on composition queries. The times shown in tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than or equal to the wall clock time. The experiments were conducted on different repository sizes as well as varying number of input and output parameters for each service in the repository. The results are plotted in Figs. 11 and 12 where

123

SOCA (2016) 10:111133 Table 11 Sample XSD Schema providing semantics Sample XSD Schema <complexType name =" Address " > <sequence > <element name = "name" type = "string" minOccurs = "0"/> <element name = "street" type = "string" /> <element name = "city" type = "string" /> </ sequence> </complexType> <complexType name = "US - Address"> <complexContent > <extension base = "Address" > <sequence> <element name = "state" type = "US - State"/> <element name = "zip" type = "positiveInteger"/> </sequence> </extension> </complexContent> </complexType> Fig. 11 High-level design on composition engine

127

Table 12 Performance on discovery queries
Repository size Number of I/O parameters 48 1620 3236 48 1620 3236 48 1620 3236 Pre-processing time (ms) Incremental Query execution update (ms) time (ms) 1 1 2 1 1 2 1 1 3 18 23 28 19 23 29 19 26 29

2,000 2,000 2,000 2,500 2,500 2,500 3,000 3,000 3,000

36.5 45.8 57.8 47.7 58.7 71.6 56.8 77.1 88.2

the numbers of I/O parameters were 48, 1620, and 3236, respectively. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the pre-processing time increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which is negligible (just 13 ms) even for complex queries with large repositories. Discussion We evaluated our approach for correctness, efficiency, and scalability of the software. The correctness of the algorithm has been described in Sect. 4. In the experiments conducted, discovery and composition solutions obtained were checked against a pre-defined set of solutions produced for the WSChallenge datasets [43]. This was the first and most important criterion for evaluation of the engine. Our second criterion was query efficiency, i.e., rapidly obtaining answers to discovery and composition queries. The results show that irrespective of the repository size the query execution time was always 12 ms. This was because the repositories were preprocessed, and the queries were run against pre-processed data. Different repository sizes were used along with varying the number of input and output parameters of the Web services. The software also scaled well with varying the size of the repositories. These three criterion are important for the successful adoption of the composition engine to produce a Software-as-a-Service (SaaS) platform for automatic workflow generation in a specific domain as described in the sample scenario in Sect. 1. An important part of scalability is the ability to sustained performance even if pre-processed

Table 13 Performance on composition queries
Repository size Number of I/O parameters 48 1620 3236 48 1620 3236 48 1620 3236 Pre-processing I/O time (ms) 36.1 47.1 60.2 58.4 60.1 102.1 71.2 87.9 129.2 Incremental Query execution update (ms) time (ms) 1 1 1 1 1 1 1 1 1 18 23 30 19 20 34 18 22 32

2,000 2,000 2,000 3,000 3,000 3,000 4,000 4,000 4,000

123

128

SOCA (2016) 10:111133

Fig. 12 Performance on discovery queries

repositories change in size, new services are added, existing services are removed, etc. To evaluate this aspect, we studied the time taken for incremental updates to the repository. The results obtained were all less than one second for different repository sizes and well as varying number of input and output parameters. A network of service providers was introduced synthetically into the experimental datasets. Trust rating of services was computed based on this network. Trust ratings generator helps with filtering services based on the trust threshold and helps in ranking. The performance of the engine on discovery and composition queries still remains the same.

development, etc. In order to perform these tasks, scientists use a number of software tools and programs already available. They use them by putting them together in a particular order (i.e., a workflow) to get their desired results. That is, it involves execution of a sequence of steps using various programs or software tools. These tools use different data formats, and hence translating from one data format to another becomes necessary. 6.2 Automatic workflow generation The software tools and programs created for specific phylogenetic tasks use different data formats for their input and output parameters. The data description language Nexus [46,47] is used as a universal language for representation of these bioinformatics related data. There are translator programs that convert different formats into Nexus and vice versa. For example, one could use the BLAST program to get a sequence set of genes. Once the sequence set is obtained, the sequences can be aligned using the CLUSTAL program. But the output from BLAST cannot be directly fed to CLUSTAL, as their data formats are different. The translator can be used to convert the BLAST format to Nexus and then the Nexus format to CLUSTAL. In order to perform an inferencing task, one has to manually pick all the appropriate programs and corresponding format translators and put them in the correct order to produce a workflow. We show how Web service composition can be directly applied to automate this task of producing a workflow. MyGrid [48] has a wealth of bioinformatics resources and data that provides opportunities for research. It has hundreds of biological Web services and their WSDL descriptions, provided by various research groups around the world. We illustrate our generalized framework for Web service composition by applying it to these services to generate workflows automatically that are practically useful for this field. Example Workflow Generation (Non-sequential Composition) Suppose we are looking for a service that takes a GeneInput and produces its corresponding AccessionNumbers, AGI, and GeneIdentifier as output. The directory of services contains CreateMobyData, MOBYSHoundGetGen-

6 Application to bioinformatics We illustrate the practicality of our general framework for automatically composing services by applying it to phylogenetics, a subfield of bioinformatics, for automatic generation of workflows. In this section, we present a brief description of the field of Phylogenetics [8] followed by an example of a workflow generation problem that can be mapped to a non-sequential conditional composition problem (the most general case of the composition problem) and can be solved using our generalized composition engine. 6.1 Phylogenetics Phylogenetic inferencing involves an attempt to estimate the evolutionary history of a collection of organisms (taxa) or a family of genes [45]. The two major components of this task are the estimation of the evolutionary tree (branching order), then using the estimated trees (phylogenies) as analytical framework for further evolutionary study and finally performing the traditional role of systematics and classification. Using this study, a number of interesting facts can be discovered, for example, who are the closest living relatives of humans, who are whales related to, etc. Different studies can be conducted, for example, studying dynamics of microbial communities, predicting evolution of influenza viruses and other applications such as Drug Discovery, and vaccine

123

SOCA (2016) 10:111133 Table 14 Bioinformatics application--non-sequential composition example Service Query CreateMoby Data MOBYSHoundGet GenBankWbatev erSequence MlPSBlastBetterE13 Extract Accession ExtractBestHit Format (MobyData) = NCBI Pre-conditions Input parameters Genelnput Genelnput MobyData Output parameters AccessionN umbers, AGIj Geneldentity MobyData GeneSequence

129

Post-conditions

Format (MobyData) = NCBI

GeneSequence GeneSequence WUGeneSequence

WUGene Sequence AccessionNumbers AGI, Geneldentity

Fig. 13 Performance on composition queries

Bank WhateverSequence, ExtractAccession, ExtractBestHit, MIPSBlastBetterE13 services. In this scenario, the GeneInput first needs to be converted to NCBI data format and then its corresponding GeneSequence is further passed to ExtractAccession and ExtractBestHit to obtain the AccessionNumbers, AGI, and GeneIdentity respectively. Table 14 shows the input/output parameters of the user query and the services. Figure 13 shows this non-sequential composition example as a directed acyclic graph. In this example:  Service CreateMobyData has a post-condition on its output parameter MobyData that the format is NCBI and service MOBYSHoundGetGenBankWhateverSequence has a pre-condition that its input parameter MobyData has to be in NCBI format for service execution. The postcondition of CreateMobyData must imply pre-condition of MOBYSHoundGetGenBankWhateverSequence service.  Both services ExtractAccession and ExtractBestHit have to be executed to obtain the query outputs.  The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Example Workflow Generation (Non-sequential Conditional Composition) Suppose we are looking for a service that takes

a GeneSequence and produces an EvolutionTree and EvolutionDistance after performing a phylogenetic analysis. Also the service should satisfy the post-condition that the EvolutionTree produced is in the Newick format. This involves producing a sequence set first, followed by aligning the sequence set and then producing the evolution tree and evolution distance. Also any necessary intermediate data format translations have to be performed. Table 15 lists the services in the repository and their corresponding input/output parameters and user query. For the sake of simplicity, the query and services have fewer input/output parameters than the realworld services. In this example, service BLAST has to be executed first so that its output BLASTSequenceSet can be used as input by CLUSTAL after the data format has been translated using BLASTNexus and NexusCLUSTAL. The service BLASTNexus has a post-condition that the format of the output parameter NexusSequenceSet is Nexus which is the pre-condition of the next service NexusCLUSTAL. Similarly the service NexusCLUSTAL has a post-condition that the format of the output parameter ClustalSequenceSet is Clustal that is the pre-condition of the next service CLUSTAL. At every step of composition, the post-conditions of a service should imply the pre-conditions of the following service. The post-condition of the service CLUSTAL is that the output parameter AlignedSequenceSet has either Paup or Phylip format. Depending on which one of these two conditions hold,

123

130 Table 15 Bioinformatics application--non-sequential conditional composition example Service Query Pre-conditions Input parameter Sequence, OrganismType, Word Size, DatabaseName Sequence, Organism Type, DatabaseName Format (Clustal SequenceSet) = Clustal Clustal SequenceSet Outpur parameter EvolutionTree, EvolutionDistance BlastSequenceSet AlignedSequenceSet

SOCA (2016) 10:111133

Post-conditions Format (EvolutionTree) = Newick

BLAST CLUSTAL

Format (AlignedSequence Set) = Paup  Forrmat(AlignedSequence Set) = Phylip Format (NexusSequenceSet) = Nexus Forma(ClustalSequenceSet) = Clustal Format (EvolutionTree) = Newick Format (EvolutionTree) = Newick

BLASTNexus NexusCLUSTAL PAUP PHYLIP MEGA Format (Nexus SequenceSet) = Nexus Format (Aligned SequenceSet) = Paup Format (Aligned SequenceSet) = Phylip Format (Aligned Sequenced) = Paup

BlastSequenceSet NexusSeqtienceSet AlignedSequenceSet, Word Size AlignetBeciuenceSet AlignedSequenceSet

NexusSequenceSet ClustalSequenceSet EvolutionTree EvokiSonTree EvaluationDistance

Fig. 14 Bioinformatics application--non sequential composition as a directed acyclic graph

the next service for composition is chosen. In this case, one cannot determine whether the post-conditions of the service CLUSTAL imply pre-conditions of PAUP or PHYLIP until the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and depending on the outcome of the condition, corresponding services will be executed. The vertex for service CLUSTAL in Fig. 14 has an outgoing edge to a conditional node. The outgoing edge represents the outputs and post-conditions of the service. The conditional node has multiple outgoing edges that represent the generated conditions that are evaluated at run-time. In this case, the following conditions are generated:  (Format(AlignedSequenceSet) = Paup  Format(AlignedSequenceSet) = Phylip)  (Format(AlignedSequenceSet) = Paup)  (Format(AlignedSequenceSet) = Paup  Format(AlignedSequenceSet) = Phylip)  (Format(AlignedSequenceSet) = Phylip)

Depending on the condition that holds, the corresponding services PAUP and MEGA or PHYLIP are executed respectively. The outputs EvolutionTree and EvolutionDistance are produced in both the cases along with the post-condition that the format of the evolution tree is Newick. Figure 14 shows this non-sequential conditional composition example as a conditional directed acyclic graph. 6.3 Implementation In order to apply service composition to obtain the sequence of tasks automatically, these programs have to be made available as Web services and their descriptions should be provided using one of the Web services description languages like WSDL (Web Services Description Language) or USDL (Universal Service-Semantics Description Language). The translator programs also have to be available as Web services. Then, we can write a query specifying the input parameters provided and the output parameters that have to be obtained. The Composition engine then looks up the repository of available services and finds the solution, i.e., a set

123

SOCA (2016) 10:111133 Fig. 15 Bioinformatics application--non sequential conditional composition as a directed acyclic graph

131

Fig. 16 Composition Solution 1 for query in Table 15

of services that can be executed to obtain the requested output parameters. These set of services obtained will need only those input parameters that are provided by the query and their execution produces the output parameters specified by the query. We tested our Composition engine on a repository of services that had descriptions of Web services corresponding to the following programs:

Table 16 Bioinformatics application--workflow query Service Query1 Input parameters Sequence, DatabaseName, OrganismType, SelectionOptions, MaxTargetSequences, ExpectedThreshold, UserTransversion Parsimony, UseThresholdParsimony, WordSize Output parameters NewickEvoIutionTree

1. BLAST: This service compares protein sequences to sequence databases and calculates the statistical significance of matches. It query's a public database of generic information like GenBank and GSDB and produces a molecular sequence. It takes in Sequence, DatabaseName, OrganismType, SelectionOptions, MaxTargetSequences, ExpectedThreshold, WordSize as input parameters and produces BlastSequenceSet as the output. 2. CLUSTAL: This service produces multiple sequence alignment for DNA or proteins. It takes in ClustalSequenceSet as input parameter and produces ClustalAlignedSequenceSet as the output. 3. PHYLIP: This service is used for inferring phylogenies. It analyzes molecular sequences and infers phylogenetic information. It takes in PhylipAlignedSequenceSet, UseThresholdParsimony, UseTransversionParsimony as input parameters and produces NewickEvolutionTree as the output. 4. PAUP: This service is used for inferring phylogentic trees. It analyzes molecular sequences and infers phylogenetic information. It takes in PaupAlignedSequenceSet as input parameter and produces NewickEvolutionTree as the output. 5. BLASTNexus: This service takes input in BLAST format and converts it into Nexus format. It takes in BLAST-

SequenceSet as input and produces NexusSequence Set. 6. NexusCLUSTAL: This service takes input in Nexus format and converts it into CLUSTAL format. It takes in NexusSequenceSet as input parameter and produces CLUSTALSequenceSet as the output. 7. CLUSTALNexus: This service takes input in CLUSTAL format and converts it into Nexus format. It takes in CLUSTALAlignedSequenceSet as input parameter and produces NexusAlignedSequenceSet as the output. 8. NexusPAUP: This service takes input in Nexus format and converts it into PAUP format. It takes in NexusAlignedSequenceSet as input parameter and produces PaupAlignedSequenceSet as the output. 9. NexusPHYLIP: This service takes input in Nexus format and converts it into PHYLIP format. It takes in NexusAlignedSequenceSet as input parameter and produces PhylipAlignedSequenceSet} as the output. 10. MEGA: This service is used for Molecular Evolutionary Genetics Analysis. It takes in MEGAInp as input parameter and produces MEGASequence as the output.

123

132 Fig. 17 Composition Solution 2 for query in Table 15

SOCA (2016) 10:111133

11. KEPLER: This service provides scientific workflows. It takes in KEPLERData as input parameter and produces KEPLERSequence as the output. The composition engine can automatically discover a complex workflow based on the query requirements, from a large repository without having to analyze all the programs manually. Figure 15 shows the non-sequential conditional composition for query specified in Table 16 as a directed cyclic graph. Figures 16 and 17 show the solutions obtained for the query specified in Table 16. A task that had to be performed manually by biologists whenever they had to make phylogenetic inferences can now be done automatically with Web services Composition. The programs have to be made available as Web services and their descriptions provided. Once we have a repository of such services, the composition engine can be used as shown above to automatically generate workflows. 7 Conclusions and future work Due to the growing number of services on the Web, we need automatic and dynamic Web service composition in order to utilize and reuse existing services effectively. It is also important that the composition solutions obtained can be trusted. Our semantics-based approach uses semantic description of Web services to find substitutable and composite services that best match the desired service. Given semantic description of Web services, our engine produces optimal results (based on number of services in the composition). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential, nonsequential or non-sequential conditional composition that is possible for a given query and also automatically generates OWL-S description of the composite service. This OWLS description can be used during the execution phase and subsequent searches for this composite service will yield a direct match. A trust rating is computed for every service in the repository based on the degree centrality of the service provider in a known social network. Currently, we are in the process of testing the trust-based dynamic Web service composition engine in a complete operational setting and running experiments to measure the quality of composition results obtained. We will also explore the other measures of centrality such as betweenness centrality and closeness centrality and analyze the possibility of using a combination of

all three measures of centrality to compute trust rating of a service provider. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. The strengths of this engine is the minimal query execution time that is achieved through pre-processing of repositories and incremental updates to the pre-processed data whenever a service is added, removed, or modified. Use of Constraint Logic Programming helped greatly in obtaining an efficient implementation of this system and made it easy to incorporate non-functional parameters for ranking of results. The limitations of the engine include trust aspect of the Web services involved in a composition solution. A model that provides a trust rating to services or service providers would improve confidence in the generated solutions. Also, when working with domains such as Bioinformatics where software systems involved in a workflow need to be converted into services, generation of semantics of the inputs and outputs of the Web services is a challenge. They have to be manually assigned semantics by a domain expert. Our future work includes investigating other kinds of compositions with loops such as repeat-until and iterations and their OWL-S description generation. Analyzing the choice of the composition language (e.g., BioPerl [49] for phylogenetic workflows) and exploring other language possibilities is also part of our future work. We are also exploring combining technologies of automated service composition and domain-specific languages to develop a framework for problem solving and software engineering.

References
1. Castagna G, Gesbert N, Padovani L (2008) A theory of contracts for web services. ACM SIGPLAN Not 43:261272 2. Bansal A, Patel K, Gupta G, Raghavachari B, Harris ED, Staves JC (2005) Towards intelligent services: a case study in chemical emergency response. In: IEEE International conference on web services (ICWS) 3. McIlraith SA, Son TC, Zeng H (2001) Semantic web services. IEEE Intell Syst 16(2):4653 4. Mandell DJ, McIlraith SA (2003) Adapting BPEL4WS for the semantic web: the bottom-up approach to web service interoperation. In: The semantic web-ISWC. Springer 2003, pp 227241 5. Paolucci M, Kawamura T, Payne TR, Sycara K (2002) Semantic matching of web services capabilities. In: The semantic Web ISWC. Springer 2002, pp 333347 6. Rao J, Dimitrov D, Hofmann P, Sadeh N (2006) A mixed initiative approach to semantic web service discovery and composition: SAP's guided procedures framework. In: International conference on web services. ICWS'06, 2006, pp 401410

123

SOCA (2016) 10:111133 7. Cardoso J, Sheth AP (2006) Semantic web services, processes and applications. Springer, Berlin 8. Edwards AWF, Cavalli-Sforza LL (1964) Reconstruction of evolutionary trees, systematics association publication number 6, No. Phenetic and Phylogenetic Classification, pp 6776 9. Freeman LC (1979) Centrality in social networks conceptual clarification. Social Netw 1(3):215239 10. Wasserman SF (1994) Social network analysis: methods and applications. Cambridge University Press, Cambridge 11. Tamura K, Peterson D, Peterson N, Stecher G, Nei M, Kumar S (2011) MEGA5: molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods. Mol Biol Evol 28(10):27312739 12. Thompson JD, Higgins DG, Gibson TJ (1994) CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice. Nucleic Acids Res 22(22):46734680 13. Edgar RC (2004) MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Res 32(5):1792 1797 14. Badr Y, Caplat G (2010) Software-as-a-service and versionology: towards innovative service differentiation. In: 24th IEEE international conference on advanced information networking and applications (AINA), 2010, pp 237243 15. Kona S, Bansal A, Blake MB, Gupta G (2008) Generalized semantics-based service composition. In: IEEE international conference on web services (ICWS), pp 219227 16. Rao J, Su X (2005) A survey of automated web service composition methods. In: Cardoso J, Sheth A (eds) Semantic web services and web process composition. Springer, Berlin, pp 4354 17. Srivastava B, Koehler J (2003) Web service composition-current solutions and open problems. In: ICAPS 2003 workshop on planning for web services, vol 35, pp 2835 18. McIlraith S, Son TC (2002) Adapting golog for composition of semantic web services. KR 2:482493 19. Pistore M, Roberti P, Traverso P (2005) Process-level composition of executable web services: on-the-fly versus once-for-all composition. In: Gomez-Perez A, Euzenat J (eds) The semantic web: research and applications. Springer, Berlin, pp 6277 20. Boustil A, Maamri R, Sahnoun Z (2013) A semantic selection approach for composite web services using OWL-DL and rules. Serv Oriented Comput Appl 8:118 21. Claro DB, Albers P, Hao JK (2005) Selecting web services for optimal composition. In ICWS international workshop on semantic and dynamic web processes, Orlando-USA 22. Suvee D, De Fraine B, Cibrn MA, Verheecke B, Joncheere N, Vanderperren W (2005) Evaluating FuseJ as a web service composition language. In: Third IEEE European conference on web services (ECOWS) 23. Dong W, Jiao L (2008) QoS-aware Web service composition based on SLA. In: Fourth international conference on natural computation (ICNC) vol 5, pp 247251 24. Yan J, Kowalczyk R, Lin J, Chhetri MB, Goh SK, Zhang J (2007) Autonomous service level agreement negotiation for service composition provision. Future Gener Comput Syst 23(6):748759 25. Wada H, Champrasert P, Suzuki J, Oba K (2008) Multiobjective optimization of SLA-aware service composition. In: IEEE congress on services-Part I, pp 368375 26. Blake MB (2007) Decomposing composition: service-oriented software engineers. IEEE Softw 24(6):6877 27. Zeng L, Benatallah B, Ngu AH, Dumas M, Kalagnanam J, Chang H (2004) QoS-aware middleware for web services composition. IEEE Trans Softw Eng 30(5):311327

133 28. Cardoso J, Sheth A, Miller J, Arnold J, Kochut K (2004) Quality of service for workflows and web service processes. Web Semant 1(3):281308 29. Zhao X, Shen LW, Peng X, Zhao W (2013) Finding preferred skyline solutions for SLA-constrained service composition. In: 2013 IEEE 20th international conference on web services (ICWS), pp 195202 30. Feng Y, Ngan LD, Kanagasabai R (2013) Dynamic service composition with service-dependent QoS attributes. In: 2013 IEEE 20th international conference on web services (ICWS), pp 1017 31. "OWL-S". [Online]. http://www.w3.org/Submission/OWL-S/. (Accessed: 22-Jan-2014) 32. Wen S, Tang C, Li Q, Chiu DK, Liu A, Han X (2014) Probabilistic top-K dominating services composition with uncertain QoS. Serv Oriented Comput Appl 8(1):91103 33. Immonen A, Pakkala D (2014) A survey of methods and approaches for reliable dynamic service compositions. Serv Oriented Comput Appl 8(2):129158 34. Mehdi M, Bouguila N, Bentahar J (2013) A QoS-based trust approach for service selection and composition via Bayesian networks. In: 2013 IEEE 20th international conference on web services (ICWS), pp 211218 35. Kuter U, Golbeck J (2009) Semantic web service composition in social environments. Springer, Berlin 36. Leavitt HJ (1951) Some effects of certain communication patterns on group performance. J Abnorm Soc Psychol 46(1):38 37. Bansal A, Kona S, Simon L, Mallya A, Gupta G, Hite TD (2005) A universal service-semantics description language. In: Third IEEE European conference on web services (ECOWS), pp 214225 38. Kona S, Bansal A, Simon L, Mallya A, Gupta G (2009) USDL: a service-semantics description language for automatic service discovery and composition. Int J Web Serv Res 6(1):2048 39. Sterling L, Shapiro EY, Warren DH (1986) The art of Prolog: advanced programming techniques. MIT Press, Cambridge 40. Marriott K, Stuckey PJ (1998) Programming with constraints: an introduction. MIT Press, Cambridge 41. "RDF/OWL Representation of WordNet". [Online]. http://www. w3.org/TR/wordnet-rdf/. (Accessed: 23-Jan-2014) 42. Blake MB, Cheung W, Jaeger MC, Wombacher A (2006) WSC-06: the web service challenge. In: The 3rd IEEE international conference on E-commerce technology, 2006. The 8th IEEE international conference on enterprise computing, E-commerce, and E-services, pp 6262 43. Blake MB, Cheung WKW, Jaeger MC, Wombacher A (2007) WSC-07: Evolving the web services challenge. In: The 9th IEEE international conference on E-commerce technology and the 4th IEEE international conference on enterprise computing, E-commerce, and E-services, 2007. CEC/EEE 2007, pp 505508 44. Kona S, Bansal A, Gupta G, Hite D (2007) Automatic composition of semantic web services. In: International conference on web services (ICWS), vol 7, pp 150158 45. Felsenstein J (2004) Inferring phylogenies, vol 2. Sinauer Associates, Sunderland 46. Iglesias JR, Gupta G, Pontelli E, Ranjan D, Milligan B (2001) Interoperability between bioinformatics tools: a logic programming approach. In: Practical aspects of declarative languages. Springer, Berlin, pp 153168 47. Maddison DR, Swofford DL, Maddison WP (1997) NEXUS: an extensible file format for systematic information. Syst Biol 46(4):590621 48. "myGrid". [Online]. http://www.mygrid.org.uk/ 49. "BioPerl". [Online]. http://www.bioperl.org/wiki/Main_Page

123

Annotating UDDI Registries to Support the Management of Composite Services
M. Brian Blake, Michael F. Nowlan, Ajay Bansal, and Srividya Kona
Department of Computer Science Georgetown University Washington, DC 202 687-3084

{mb7,mfn3,ab683,sk558}@georgetown.edu ABSTRACT
The future of service-centric environments suggests that organizations will dynamically discover and utilize web services for new business processes particularly those that span multiple organizations. However, as service-oriented architectures mature, it may be impractical for organizations to discover services and orchestrate new business processes on a daily, case-by-case basis. It is more likely that organizations will naturally aggregate themselves into groups of collaborating partners that routinely share services. In such cases, there is a requirement to maintain an organizational memory with regards to the capabilities offered by other enterprises and how they fit within relevant business processes. As a result, registries must maintain information about past business processes (i.e. relevant web services and their performance, availability, and reliability). This paper discusses and evaluates several hybrid approaches for incorporating business process information into standards-based service registries. Moreover, the syntactic and semantic metadata that accompanies these services [9][11] enable the discovery of these capabilities, on-demand. Discovery, in this environment, largely depends on the accessibility and capabilities of the repositories for which these services are stored. Universal Description, Discovery, and Integration (UDDI) is the leading specification for the development of service-based repositories or registries. UDDI registries of the future should facilitate fast search and discovery of relevant web services akin to the performance currently associated with resolving a domain name. Although, performance and federation are two important aspects of UDDI, an additional requirement for UDDI should be effective process-oriented storage and retrieval. Currently, the abilities to browse and discover independent services as characterized by their overarching business name and/or their capability name are important fundamental operations. However, as service-oriented architectures mature, composite services (i.e. capabilities based on the workflow composition of multiple atomic services) will also be important to persist and manage. UDDI currently has limited support for managing business processes [6]. Although not all federated service registries will need business process annotations, we suggest that a subset of registries frequently used by partnering organizations would benefit from maintaining historical process information. Currently, there are numerous languages and protocols that support the specification and execution [3][4][5][9]of composite web services. Unfortunately, techniques for incorporating the underlying process information into UDDI registries are limited. In this work, we address several questions as listed below.    What are the relevant descriptive attributes of composite web services that must be represented in web service registries? What are the relevant use cases for process-oriented service registries? What are the state-of-the-art methods for incorporating process information into registries and the corresponding challenges? What are the most efficient and effective approaches, both qualitatively and quantitatively, for process management of services in UDDI registries?

Categories and Subject Descriptors
D.3.3 [Programming Languages]: Language Contructs and Features  abstract data types, polymorphism, control structures.

General Terms
Performance, Design, Standardization Experimentation, Security, and

Keywords
Keywords are your own designated keywords.

1. INTRODUCTION
Service-oriented computing [10] promotes the development of modular domain-specific capabilities that can be advertised to and shared among collaborating organizations.



Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SAC'09, March 8-12, 2009, Honolulu, Hawaii, U.S.A. Copyright 2009 ACM 978-1-60558-166-8/09/03...$5.00.

This paper proceeds in the next section with a survey of related work and a discussion of the state of the practice. The next section formalizes a composite service by detailing the most relevant descriptive attributes. Next, we describe the requirements of a process-oriented registry. We next introduce

several hybrid approaches for adding process information to web service repositories. Finally, we describe a case study and experimental evaluation of both approaches.

2. RELATED WORK
Universal Description, Discovery and Integration (UDDI) describes the data model associated with a web-accessible registry for the storage and management of web services. Three core hierarchical objects specify the service provider (businessEntity), the web service (businessService), and information about how the service is binded (bindingTemplate). These foundational objects can be extended with the use of technical models or tModels. TModels facilitate the further description of businessEntities, businessServices, and bindingTemplates through classification based on metadata. Each tModel of a businessService represents a certain behavior or classification system that the service must implement. An example would be a web service that takes a state abbreviation as input. This web service would probably choose to reference the tModel that represents the US-State abbreviation classification system. A person looking at the service's bindingTemplate would then be able to see a reference to the USState System and know that a state should be entered with its abbreviation. UDDI also supports other structures called keyedReferences that allow previously mentioned core objects to be associated to tModels. KeyedReferences consist of a tModelKey, keyValue, and keyName. The tModelKey identifies the referenced tModel. The keyValue allows a categorization of the link between the core object, and the tModel and the keyName is a text string readable for humans. In UDDI v3, keyedReferences can be aggregated with keyedReferenceGroups. The strength of the tModels and keyedReferences is that further information about the main UDDI objects (i.e. businessEntities, businessServices, and bindingTemplates) are not populated in the repository. Tmodels merely point to webaccessible documents. This paradigm both reduces maintenance of the registry and promotes overall robustness. However, this paradigm also makes it difficult to associate web services that are stored in the registry, which is a necessary requirement for describing business processes within the registry. The most common research projects tend to address the problem of federating UDDI registries [1] [2][12], although, of most relevance to our work, are the projects that directly address the problem of business process annotations that associate services. Perhaps the leading approach to inserting business processes is the construction of a tModel classification system that mirrors a particular taxonomy of business processes. These tModels can then be used as pointers to the corresponding business process description documents. In industry, several OASIS technical reports [15] [17] describe high-level approaches to integrating tModel classifications with ebXML and BPEL4WS descriptions. Other research projects detail specialized domainspecific methods that leverage the same basic approach [6] [13] These are reasonable approaches, but all business descriptions are defined by external business process documents that are decoupled from the individual services. In fact, since services may be captured at individual locations, replacing services or even discontinuing the offering of a particular business process becomes difficult. In these cases, centralizing some vital part of the process information can be valuable. Luo et al. [8] and Srinivasan et al. [14] use semantic notations embedded in UDDI

tModels to associate services. This approach allows for more rich definitions, but the underlying limitations caused by distribution also apply. Other approaches look at the development of external, integrated software mechanisms that run parallel with the UDDI registry [18] [19]. These approaches tend to depart from the essence of the SOA paradigm as they promote proprietary, less standardized solutions. In this paper, we experiment with a combination of external process documentation and annotations that are embedded directly in the UDDI registry. Prerequisite to any solution, it is important to understand the required aspects of the web services-based business process

3. ANATOMY OF WEB SERVICES-BASED PROCESSES
Web-services based business processes also referred to as web service workflows are similar to traditional processes that are established between human stakeholders. Figure 1 illustrates the metamodel of a web services-based business process using a Unified Modeling Language (UML) class diagram. A difference is, as opposed to human-managed tasks or steps, web services enact the underlying steps. A web services-based business process, BP, contains user data endpoints, DE , defined below.
DataPart UserDataEndpoint -DataPart_ID -DPValue

<< has a>>

-UserDataEndpointID -InputID -OutputID * 1

<< has a>>
*

* 1

BusinessProcess -ProcessID -UserDataEndPointID -TransitionID -ServiceID -Description 1

1

Transition -TransitionID -InputID -OutputID -CFlowType -PreCondID -PostCondID

MessageContainer 1 * -MessageContainerID -DataPartID * QoS

1

*

Service * -ServiceID -InputID -OutputID -QosID -URI_ID

1 * 1 1

-QoSID -SLOType -SLOValue

<< has a>>
URI * -URI_ID -URIValue

Figure 1. Metamodel of a Web Service-Based Business Process. Definition (UserData EndPoint): The user data end point is defined as a pair DE = (ID, OD) where ID represents the input information of the business process provided by the user and OD represents the output information, ultimately generated by the completion of the business process. The business process also has a sequence of tasks (realization of the steps) that are implemented by a set of services,  = {S1, S2, S3, ....Sn}. Each service Si has its own input, ISi, and output, OSi, information; however the set of all input/output information of a service is less relevant than the subset of inputs and outputs that are relevant to the business process. In addition, a service can also be defined with its quality of service information and its URI location. Definition (Service): A service is a tuple of its inputs, outputs, QoS parameters, and its URI location. S = (IS , OS , QS , US) is the representation of a

service where IS is the input list, OS is the output list, QS is the list of quality of service parameters and US represents the URI location. Each step in the business process is defined by a transition, T, that defines the shared information between the output, OT, of the preceding step that connects to the input, IT , of the subsequent step. Definition (Transition): A transition is represented as a tuple of its inputs, outputs, flowtype, pre-conditions, and post-conditions. T = (IT , OT ,FT , CPre , CPost) is the representation of the transition where IT is the input list, OT is the output list, FT is the flow type, CPre represents the pre-conditions of the transition and CPost represent the postconditions of the transition. Ultimately, the business process can be formally defined as follows: Definition (BusinessProcess): A BusinessProcess is defined as a tuple BP = (DE, , ) where DE represents the user data endpoint,  is the set of services involved in the business process, and  is the set of transitions in the workflow .

previous section. Organizations should be able to generally access process information in addition to the service-specific details.

4.1 Potential Use Cases
There are several functions required by a registry that supports composite services as business processes. Figure 2 illustrates the functions of such a repository depicted as a UML use case diagram. The basic registry actors follow the SOA paradigm (i.e. service providers and consumers). Incorporating business process metadata into the registry also supports the interaction of intelligent software components or agents to autonomously maintain the integrity of the information. Service providers should be able to insert one or more services into the registry. Service consumers should be able to either browse or explicitly search the repository based on several attributes such as the name/type of business, service, or process. Although only available using specialized approaches, consumers may also want to search by service/process message names. We focus on three major features of such a repository.  Advertising a set of services aggregated as a process

DE = (ID, OD);  = {S1, S2, ..., Sn} where Si = (ISi , OSi , QSi , USi), for all i = 1 to n;  = {T1, T2, ...., Tn} where Ti = (ITi , OTi ,FTi , CPrei , CPosti), for all i = 1 to n. The following conditions should hold for a valid business process: For all Si  and Ti , the inputs of Si are 1. subsumed by the inputs of Ti , i.e., ISi C ITi For all Si  and Ti , the outputs of Si 2. subsumes the outputs of Ti , i.e., OTi C OSi For all Ti , Ti+1 , the post-conditions of Ti 3. imply the pre-conditions of Ti+1 , i.e., CPosti => CPre i+ 1 For all Ti , Ti+1 , the outputs of Ti along with 4. the user data inputs of the business process subsume the inputs of Ti+1 , i.e., (OTi U ID) C IT i+ 1 The inputs of the business process subsume the 5. inputs of the first transition, T1 (where T1  ), i.e., IT1 C ID The outputs of the business process are subsumed 6. by the outputs of the last transition Tn (where Tn ), i.e., OD C OTn
The cardinality of data endpoints, services, and transitions vary for each step, such that is necessary to develop containers to aggregate the information into sets. The notion of containers is central to business process languages, such as BPEL4WS and BPML [3][4], for aggregating information related to subprocesses.

When partnering organizations decide to share services, there may be a predefined understanding for orchestration. As such, these organizations must insert their relevant services into shared UDDI registries annotated by process-based information (i.e. the underlying control flow and data flow).  Discovering services associated with processes discovering processes associated with services and

Current UDDI registries facilitate browsing of services by business name and by service name. A process-oriented registry will also support browsing by process name or type. Consumers should also be able to find all services associated with a particular process.  Managing process information by software agents Once process information is annotated into a registry, intelligent agents can regularly check the health of the underlying services. Agents can look for indexing configurations that best support the storage of the process information. In addition, agents can record QoS information supplied by service consumers. This QoS information can represent individual services or the process as a whole.

4.2 Alternative Hybrid Approaches
By extending and leveraging the UDDI specification, we have identified several approaches for annotating business processes within service registries. Every service in a UDDI registry has a bindingTemplate structure, which stores references to tModels. TModels are "sources for determining compatibility of Web services and keyed namespace references" Error! Reference source not found.. This means that tModels identify how to interact with a web service by describing the technologies it implements. Although, the UDDI registry usually implements default tModels, such as the State Abbreviation System, it is also possible for an administrator of the registry to create tModels, ondemand. There are two approaches for using tModels to describe web services-based business processes.

4. BUSINESS PROCESS AND SERVICE REGISTRIES
In order for organizations to understand their business processes defined with web services, it is important that their process databases include relevant process information as defined in the

 Annotating business process information directly into the UDDI registry A new tModel is created for every business process that is identified in the registry. In addition, a parent tModel is created that simply classifies any tModel that annotates a business process as such. In this way, when a new process chain is added or identified in the registry, a tModel that points to the parent tModel is created to represent the process. Furthermore, the categoryBag element is used to store references to all the processes of which the service is a part. Figure 3 shows an example tModel. Notice that the keyName of the keyedReference contains the service name and additional control flow information. Also, the keyValue maintains the sequence number of the service in the process. Figure 4 demonstrates the actions taken by the registry to identify and store the existence of a composite web service.  Defining business process information using external markup documents

<keyedReference keyName="uddi:Process-Representing" keyValue="categorization" tModelKey="uddi:uddi.org:categorization:types"/> </categoryBag> </tModel> <businessService> <name>firstService</name> <categoryBag> <keyedReference tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14" keyName="SERV=firstService, TRAN = Sequence" keyValue="1"/> </categoryBag> </businessService> <businessService> <name>secondService</name> <categoryBag> <keyedReference tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14" keyName="SERV = secondService, TRAN = Sequence" keyValue="2"/> </categoryBag> </businessService> <businessService> <name>thirdService</name> <categoryBag> <keyedReference tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14" keyName="SERV=thirdService, TRAN = Sequence" keyValue="3"/> </categoryBag> </businessService>

Figure 5 details the steps for annotating a business process within the registry using the UDDI data structures. Perhaps the leading approach in related work is the use of an external document (e.g. BPEL4WS and ebXML) to store the process information. This method involves simply adding an entry to each of the process documents associated with the relevant services. The document could exist centrally on a main server, or locally with each service provider.
<tModel tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"> <description>This tModel represents the process starting with firstService and ending with thirdService. </description> <categoryBag>
Advertise 1 or More Services

Figure 3. Sample Process-Oriented TModel.

Manage and Update Repository <<includes>>

Service Provider

첿xtends

Repository Agent

Associate Services with Processes

Remove Nonfunctional Services and Record QoS Values

Maintain Indexes in Repository

Query by Service Name or Type Search Repository 쳃nherits Query by Process 쳃nherits <<includes>> Query by Business Name or Type 쳃nherits Filter and Characterize Query <<includes>>

Service Consumer

쳃nherits Query by Message/Part Names Browse Repository

Figure 2. Process-Oriented Service Repository Use Cases (** Shaded areas are not currently well supported in the state-of-the-art)
Annotate( VS): VS GSFIRST GSLAST TM Annotating Function Vector of serviceKeys for services in chain The first service in the vector The last service in the vector The newly created process-tModel TMKEY PM CB S KR The newly created tModel's key The tModel Process-Classification System The CategoryBag of an individual structure An individual service KeyedReference  A pointer to a tModel

A(VS, TM) Function to add a KR to CB of Services MakeTM(GSFIRST , GSLAST ) Function to make a new TM A(VS , TM) KR = TMKEY forAll S in VS S.CB += KR return VS MakeTM(GSFIRST , GSLAST ) TM.CB += PM TM.description = GSFIRST , GSLAST return TM Annotate(VS) TM = MakeTM(GSFIRST , GSLAST ) return A(VS , TM)

 Service Deletions or Changes when Using External Process Documentation External process documents contain one entry for each process. Irrespective of the process document notations, each entry will define the serviceKeys of the services in the particular process. In this approach, these serviceKeys are used to find all services that share a process with the deleted service. The registry then edits the relevant XML documents by removing the entry that corresponds to the broken process.  Query for Processes by Service Name using UDDI Data Structure Method The last action that can be taken on a registry, and perhaps the most desired ability, is the ability to aggregate the information and list of services for every process in which a particular service plays a role. The serviceKey is used to get the service's categoryBag. The categoryBag contains references to all the tModels, that represent all relevant processes. The registry is then queried to find all services which point to any one of the tModels in their categoryBags. The chain of services is then sorted by the tModels and is returned as process.  Query for Processes by Service Name using External Process Documentation The external document method is quicker because it does not require querying the registry. The XML document for a service is retrieved. In this document is a list of entries, each pertaining to one composite web service. Each entry contains the serviceKey for each service in that particular chain. These services can be retrieved with a single call to the registry by compiling a vector of the serviceKeys. The retrieved service information is then sorted by the order of the entries in the initial service's process document and displayed.

Figure 4. Annotating a Business Process of Web Services within UDDI Data Model.
Annotate(VS): Annotating Function MakeEntry(VS ) Function to make a new TM A(VS , E) Function to add entry to XML Doc. VS Vector of serviceKeys for services in chain E An entry containing a list of services S An individual service S.KEY Service key D A service's description element FS A service's XML document file A(VS , E) forAll S in VS S. FS += E return VS MakeEntry(VS) forAll S in VS E += S.KEY return E Annotate(VS) forAll S in VS S.D = FS E = MakeEntry(VS ) return A(VS , E)

Figure 5. Leveraging External Process Documents

5. CASE STUDY: A GENERAL UDDI BUSINESS EXPLORER
Once process information is associated with or incorporated into a UDDI registry, new graphical user interfaces can be created to support enhanced service discovery and manipulation. As a part of this work, we designed and experimented with a new user interface front-end (i.e. UDDI-P) to the jUDDI registry Error! Reference source not found.. A screenshot of the design of the interface is shown in Figure 6. Using this new interface, a user has the capability of browsing known process names as shown on the left side of Figure 6. Once a process is identified, the consumer can further decide to analyze the process to see if it meets the organization need. The interface can display parameters such as estimated delivery date and price range based on service level objectives associated with the process stored in the registry. By embedding process information into the registry, classification of processes can occur to the point where they themselves function as individual services. On the right side of the interface, the user has the ability to browse each process meeting the search criteria by price and delivery. This sort of interface would enable a separate interface that allows the services to be executed.

4.3 Alternative Hybrid Approaches
Both of the hybrid approaches also have other functions relevant to a business-oriented registry.  Service Deletions or Changes when using UDDI Structure Method As the registry is updated when a process is identified, it will also change when a service is removed or replaced. These actions cause the process workflow chain to be incomplete. In this case, all the services in the chain are affected and they must modify their process annotation method (either XML document or CategoryBag) by removing the process that is broken. When a service is deleted, all the tModels that it points that represent composite web services (excluding simple classifications represented in the bindingTemplate) must be deleted. Any other services in the registry that reference these tModels must remove the reference from their categoryBags. In the future, registries may be able to search for replacements services as opposed to deleting the process.

UDDI-P Directions:

Service Request

Enter the necessary data on the left and click Submit. Offers matching your criteria will be displayed in the window on the right.

An alternative approach is incorporating specific business process information directly into the registry. We experimented to characterize the performance of these approaches on our prototype repository. The performance is illustrated in Table 1 and Figure 7 based on the use cases illustrated in Figure 2. In general, the difference in performance between adding individual services, adding a chain, and annotating services with business process annotations were only negligibly different between the two approaches. However, deleting a service was approximately three times faster when external business process documents were used. Another variation in performance is associated with retrieving all service IDs associated with a particular process (or aggregating the services). The aggregation time increased linearly with the increase in size of the UDDI registry embedded process information. The main reason for disparity between the approaches is due to the fact that the latter approach requires a significant query within the repository. This approach must retrieve the categoryBag for all services in the repository and search those structures for a particular keyedReference. The external business process document does not require this step since the process information for a service is stored in one location: the XML-based document. The retrieval of this document is much faster when compared to the query that must take place within the registry. Although the performance for an external file is more favorable, having external BPEL4WS files causes the duplication of process information (i.e. the same process entry could appear in multiple files that may be attached to the underlying services). Depending on the management of the BPEL4WS files, centralizing the process within the repository may be more advantageous. In such cases, embedding processes with the registry represents an effective solution.

Specifications
* = required

Results
(Updated when Submit is pressed)

* Application
Name:

[Process]

Viewing:
1 of 5

Sort By:
Price

* Delivery Date
Date:

Business Chain
dd yyyy

[Month]

FindBookByISBN SubmitPaymentInfo ShipProduct

Consultation
Need Expert Help? No, thanks.

Price Range
From: $0 To:

$100

Ready by: mm/dd/yyyy Price: $50.00

Reset

Submit Prev.

Choose
* Purchase on next page

Next

Figure 6. UDDI-P: A Prototype User Interface for Process-Based Service Management.

6. EXPERIMENTATION & EVALUATION
In previous sections, two distinct hybrid approaches for aggregating UDDI services into business processes were introduced. The leading approaches in published works suggest using external mark-up documents to describe business processes.

Table 1. Performance of External Document and Annotated UDDI Repository (milliseconds) Collect Service IDs by Process (Aggregate) (ms)
1500 1500 1500 1500 2600 4000 5700 7450

Repo Size
Ext. Process Doc 150 330 600 850 150 330 600 850

Add Serv. (ms)
1573 1573 1573 1573 1573 1573 1573 1573

Add Composite (ms)
2500 2700 2600 2600 2800 3000 3000 3000

Annotate (Note) (ms)
1500 1700 1600 1600 2100 2100 2100 2100

Del. Serv. (ms)
1900 1900 1900 1900 7790 7790 7790 7790

Internal UDDI Data Structure

9000 8000 7000 6000 5000 4000 3000 2000 1000 0

Add Service Add Composite Note Aggregate Delete Service

Repo=150

Repo=330

Repo=600

Repo=850

Repo=150

Repo=330

Repo=600

External XML Document

Annotated UDDI

Figure 7. Comparison of Storing Process Information in External Document versus Annotating the UDDI Repository.

7. CONCLUSIONS
An innovation in this paper is the formalized model for web services-based business process and the relevant use cases for using this information. In addition, we introduce the design of a new interface for business-based UDDI interactions. Our experimentation evaluates the two leading approaches for capturing process information in UDDI registries. Overall performance information does not suggest a quantitative advantage for embedding process information directly into the repository. However, qualitatively, maintenance is less extensive since process information is centralized in a potentially federated registry. As future work, we plan to continue developing a process-oriented UDDI explorer and experiment on new approaches for interface design.

[4] BPML (2008): http://www.ebpml.org/bpml.htm (currently moved to OMG) [5] BPMN (2008): http://www.bpmn.org/ [6] Dogac, A., Tambag, Y., Pembecioglu, P, Pektas, S., Laleci, G., Gokhan, K., Toprak, S., and Kabak, Y. "An ebXML infrastructure implementation through UDDI registries and RosettaNet PIPs" Proceedings of the 2002 ACM SIGMOD Conference (SIGMOD 2002), Madison, Wisconsin, June 2002 [7] jUDDI (2008): http://ws.apache.org/juddi/ [8] Luo, J., Montrose, B., Kim, A., Khashnobish, A., Kang, M. "Adding OWL-S Support to. the Existing UDDI Infrastructure" Proceedings of the 4th International Conference on Web Services (ICWS2006), Chicago, Ill, November 2006. [9] OWL-S(2008): http://www.daml.org/services/owl-s/ [10] Papazoglou, M. "Service-oriented computing: Concepts, characteristics and directions. In Proc. of WISE `03 [11] RDF (2008): http://www.w3.org/RDF/ [12] Sivashanmugam, K., Verma, K., and Sheth, A. Discovery of Web Services in a Federated Registry Environment, Proceedings of 4th IEEE International Conference on Web Services (ICWS), pp. 270-278, 2004. [13] Spies, M., Schoning, H., and Swenson, K. "Publishing Interoperable Services and Processes in UDDI" The 11th Enterprise Computing Conference (EDOC 2007), Annapolis, MD, October 2007 [14] Srinivasan, N., Paolucci, M. and Sycara, K. "Adding OWL-S to UDDI, implementation and throughput," Proceedings of First International Workshop on Semantic Web Services and Web Process Composition (SWSWPC 2004), San Diego, California, USA, 2004 [15] UDDI as the registry for ebXML Components, OASIS Technical Note, February 2004, Accessed (2008): http://www.oasis-open.org/committees/uddispec/doc/tn/uddi-spec-tc-tn-uddi-ebxml-20040219.htm [16] Universal Description, Discovery, and Integration (UDDI) (2008): http://www.uddi.org/pubs/uddi_v3.htm [17] Using BPEL4WS in UDDI Registry, OASIS Technical Note, July 2005 Accessed (2008): http://www.oasis-

8. ACKNOWLEDGMENTS
We acknowledge fruitful conversations with Brian Schott and Robert Graybill of the University of Southern California, ISI-East and Suzy Tichenor of the Council of Competitiveness. This material is based on research sponsored by DARPA under agreement number FA8750-06-1-0240. This U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.

9. REFERENCES
[1] Al-Masri, E. and Mahmoud, Q.H., "Crawling Multiple UDDI Business Registries", Proceedings of the 16th International Conference on the World Wide Web, Banff, Alberta, Canada, 2007 [2] Blake, M.B., Sliva, A.L., zur Muehlen, M., and Nickerson, J. "Binding Now or Binding Later: The Performance of UDDI Registries", IEEE Hawaii International Conference of System Sciences (HICSS-2007), Track on Technology and Strategies for Realizing Service-oriented Architectures with Web services, January 2007 [3] WS-BPEL(2008 ): http://www.ibm.com/developerworks/library/specification/w s-bpel/

Repo=850

open.org/committees/uddi-spec/doc/tn/uddi-spec-tc-tn-bpel20040725.htm [18] Zhang, L-J., Zhou, Q., and Chao, T., "A Dynamic Services Discovery Framework for Traversing Web Services Representation Chain", In Proceedings of the International Conference on Web Services (ICWS 2004), 2004

[19] Zhang, M., Cheng, Z., Zhao, Y. Huang, J.Z. Yinsheng, L., Zang, B. "ADDI: an agent-based extension to UDDI for supply chain management" Proceedings of the Ninth Int Conference on CSCW in Design, Shanghai, China, May 2005

Automatic Composition of Semantic Web Services
Srividya Kona, Ajay Bansal, Gopal Gupta Department of Computer Science The University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp. 2400 Dallas Parkway Plano, TX 75093

Abstract
Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. For this automation to be effective, formal semantic descriptions of Web services should be available. In this paper we formally define the Web service discovery and composition problem and present an approach for automatic service discovery and composition based on semantic description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are its scalability, i.e., its ability to handle very large service repositories, and its extremely efficient processing times for discovery and composition queries. We evaluate our engine for automated discovery and composition on repositories of different sizes and present the results.

1

Introduction

A Web service is a program accessible over the web that may effect some action or change in the world (i.e., causes a side-effect). Examples of such side-effects include a webbase being updated because of a plane reservation made over the Internet, a device being controlled, etc. An important future milestone in the Web's evolution is making services ubiquitously available. As automation increases, these Web services will be accessed directly by the applications rather than by humans [8]. In this context, a Web service can be regarded as a "programmatic interface" that makes application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize and compose services automatically is needed in order to make Web services more practical. To make services ubiquitously available we need a

semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis [3]. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. For discovery, composition, etc., one could take the syntactic approach in which the services being sought in response to a query simply have their inputs syntactically match those of the query, or, alternatively, one could take the semantic approach in which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the matching process. Several efforts are underway to build an infrastructure [17, 23, 15] for service discovery, composition, etc. These efforts include approaches based on the semantic web (such as USDL [1], OWL-S [4], WSML [5], WSDL-S [6]) as well as those based on XML, such as Web Services Description Language (WSDL [7]). Approaches such as WSDL are purely syntactic in nature, that is, they only address the syntactical aspects of a Web service [14]. Given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be automatically determined. This task is called discovery. If the service is not found, the directory can be searched for two or more services that can be composed to synthesize the required service. This task is called composition. In this paper we present an approach for automatic discovery and composition of Web services using their semantic descriptions. Our research makes the following novel contributions: (i) We formally define the discovery and composition problems; to the best of our knowledge, the formal description of the generalized composition problem has been given for the first time; (ii) We present efficient and scalable algorithms for solving the discovery and composition problem that take semantics of services into account; our algorithm automatically selects the individual services involved in composition for a given query, without the need for manual intervention;

and, (iii) we present a prototype implementation based on constraint logic programming that works efficiently on large repositories. The rest of the paper is organized as follows. Section 2 describes the two major Web services tasks, namely, discovery and composition with their formal definitions. In section 3 and 4, we present our multi-step narrowing solution and implementation for automatic service discovery and composition. Finally we present our performance results, related work and conclusions.

Q

CI',I'

CI,I

CO,O S

CO',O'

where CI' ==> CI, I' I,

CO ==> CO', O O'

Figure 1. Substitutable Service Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and   ) is the reprepost-conditions.  = (   sentation of a service where  is the pre-conditions,  is the input list, is the service's side-effect,  is the affected object,  is the output list, and  is the postconditions. Definition (Repository of Services): Repository is a set of Web services. Definition (Query): The query service is defined as  = (      퓬 퓬 퓬 ) where   is the preconditions,   is the input list,  is the service affect, 퓬 is the affected object, 퓬 is the output list, and 퓬 is the post-conditions. These are all the parameters of the requested service. Definition (Discovery): Given a repository  and a query , the Discovery problem can be defined as automatically finding a set  of services from  such that  =        풩, s  ,     ,    ,  ,  퓬 ,   퓬 ,   퓬 . The meaning of  is the subsumption (subsumes) relation and  is the implication relation. For example, say  and  are input and output parameters respectively of a service. If a query has 단  as a pre-condition and 닫  昞 as postcondition, then a service with pre-condition 단 섧 and post-condition 닫 昞 can satisfy the query as 단 도 단 섧 and 닫 昞  닫  昞 since 단 섧. In other words, the discovery problem involves finding suitable services from the repository that match the query requirements. Valid solutions have to produce at least those output parameters specified in the query, satisfy the query pre and post-conditions, use at most those input parameters that are provided by the query, and produce the same side-effect as the query requirement. Figure 1 explains the discovery problem pictorially.

2

Automated Web service Discovery and Composition

Discovery and Composition are two important tasks related to Web services. In this section we formally describe these tasks. We also develop the requirements of an ideal Discovery/Composition engine.

2.1

The Discovery Problem

Given a repository of Web services, and a query requesting a service (we refer to it as the query service in the rest of the text), automatically finding a service from the repository that matches the query requirements is the Web service Discovery problem. Valid solutions to the query satisfy the following conditions: (i) they produce at least the query output parameters and satisfy the query post-conditions; (ii) they use only from the provided input parameters and satisfy the query pre-conditions; (iii) they produce the query side-effects. Some of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre/post conditions, and side-effects requirements. Example 1: Say we are looking for a service to buy a book and the directory of services contains services 降 and 鱇 . The table 1 shows the input/output parameters of the query and services 降 and 鱇. In this example service 鱇 satisfies the query, but 降 does not as it requires BookISBN as an input but that is not provided by the query. Our query requires ConfirmationNumber as the output and 鱇 produces ConfirmationNumber and TrackingNumber. The extra output produced can be ignored. Also the semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. The discovery engine should be able to infer that the query parameter BookTitle and input parameter BookName of service 鱇 are semantically the same concepts. This can be inferred using semantics from the ontology provided. The query also has a pre-condition that the CreditCardNumber is numeric which should logically imply the pre-condition of the discovered service.

2.2

The Composition Problem

Given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 2 shows an example composite service made up of

Service Query

降 鱇

Input Parameters BookTitle,CreditCardNumber, AuthorName,CreditCardType BookName,AuthorName BookISBN,CreditCardNumber BookName, CreditCardNumber

Pre-conditions IsNumeric(Credit CardNumber)

Output Parameters Post-Cond ConfirmationNumber ConfirmationNumber

IsNumeric(Credit CardNumber)

ConfirmationNumber, TrackingNumber

Table 1. Example Scenario for Discovery problem graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph:    where  has zero incoming edges, 1. 좁   , 좁   . 2.    where  has zero outgoing edges, 퓬   , 퓬   .    where  has at least one incoming edge, 3. let  ,   , ...,   be the nodes such that there is a directed edge from each of these nodes to  . Then    ,   (        ).   The meaning of the  is the subsumption (subsumes) relation and  is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. Figure 4 explains one instance of the composition problem pictorially. When the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem (i.e., the graph is a linear chain of services).

S2 S5 CI',I' S1 S3 S4 CO',O'

Figure 2. Example of a Composite Service as a Directed Acyclic Graph

 

GetAvailability BookISBN BookName, AuthorName GetISBN NumAvailable ConfNumber PurchaseBook

BookISBN



AuthCode CreditCardNum AuthorizeCreditCard

Figure 3. Example of a Composite Service five services 降 to  . In the figure,   and   are the query input parameters and pre-conditions respectively. 퓬 and   are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes  and  indicates that outputs of  constitute (some of) the inputs of  . Example 2: Suppose we are looking for a service to buy a book and the directory of services contains services GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. The table 2 shows the input/output parameters of the query and these services. Suppose the repository does not find a single service that matches these criteria, then it synthesizes a composite service from among the set of services available in the repository. Figure 3 shows this composite service. The postconditions of the service GetAvailability should logically imply the pre-conditions of service PurchaseBook. Definition (Composition): The Composition problem can be defined as automatically finding a directed acyclic graph = 늴  of services from repository , given query  = (      퓬 퓬 퓬 ), where  is the set of vertices and is the set of edges of the graph. Each vertex in the

2.3

Requirements of an ideal Engine

Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition where the number of services involved in composition is exactly equal to one. The features of an ideal Discovery/Composition engine are: Correctness: One of the most important requirements for an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the

Service Query

Input Parameters

Pre-Conditions

BookTitle,CreditCardNum, AuthorName,CardType GetISBN BookName,AuthorName GetAvailability BookISBN AuthorizeCreditCard CreditCardNum PurchaseBook BookISBN, NumAvailable, AuthCode NumAvailable


Output Params ConfNumber

Post-Conditions

BookISBN NumAvailable NumAvailable  AuthCode AuthCode AuthCode 슨세 ConfNumber

Table 2. Example Scenario for Composition problem semantics-based Discovery and Composition engine described in the following sections.

2.4

Semantic Description of Web Services

Figure 4. Composite Service requirements of the query. Also, the engine should be able to find all services that satisfy the query requirements. Small Query Execution Time: Querying a repository of services for a requested service should take a reasonable amount of (small) time, i.e., a few milliseconds. Here we assume that the repository of services may be pre-processed (indexing, change in format, etc.) and is ready for querying. In case services are not added incrementally, then time for pre-processing a service repository is a one-time effort that takes considerable amount of time, but gets amortized over a large number of queries. Incremental Updates: Adding or updating a service to an existing repository of services should take a small amount of time. A good Discovery/Composition engine should not pre-process the entire repository again, rather incrementally update the pre-processed data (indexes, etc.) of the repository for this new service added. Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition engine should be able to give results based on requirements (minimize, maximize, etc.) over the costs. We can extend this to services having an attribute vector associated with them and the engine should be able to give results based on maximizing or minimizing functions over this attribute vector. These requirements have driven the design of our

A Web service is a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface that is described in a machineprocessible format so that other systems can interact with the Web service through its interface using messages. The automation of Web service tasks (discovery, composition, etc.) can take place effectively only if formal semantic descriptions of Web services are available. Currently, there are a number of approaches for describing the semantics of Web services such as OWL-S [4], WSML [5], WSDL-S [6], and USDL [1].

3

A Multi-step Narrowing Solution

With the formal definition of the Discovery and Composition problem, presented in the previous section, one can see that there can be many approaches to solving the problem. Our approach is based on a multi-step narrowing of the list of candidate services using various constraints at each step. In this section we discuss our Composition algorithm in detail. As mentioned earlier, discovery is a simple case of Composition. When the number of services involved in the composition is exactly equal to one, the problem reduces to a discovery problem. Hence we use the same engine for both discovery and composition. We assume that a directory of services has already been compiled, and that this directory includes semantic descriptions for each service.

3.1

The Service Composition Algorithm

For service composition, the first step is finding the set of composable services. Using the discovery engine, individual services that make up the composed service can be selected. Part substitution techniques [2] can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the

individual services. That is, if a subservice 降 is composed with subservice 鱇 , then the post-conditions of 降 must imply the pre-conditions of 鱇. The goal is to derive a single solution, which is a directed acyclic graph of services that can be composed together to produce the requested service in the query. Figure 6 shows a pictorial representation of our composition engine. In order to produce the composite service which is the graph, as shown in the example figure 2, we filter out services that are not useful for the composition at multiple stages. Figure 5 shows the filtering technique for the particular instance shown in figure 2. The composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. In figure 5,   are the preconditions and the input parameters provided by the query. 降 and 鱇 are the services found after step 1. 퓰 is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e., 종 = 퓰  . 종 is used to find services at the next stage, i.e., all those services that require a subset of 종. In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

S1 Query described using USDL (S) Infer Sub-queries . . . Sn

Discovery Module (Discovery Engine + Service Directory + Term Generator) S1 Sn

.......................... Composition Engine (implemented using Constraint Logic Programming

Composed Service

Pre-Cond(S) S1 Pre-Cond( S) 1

Post-Cond( S) 1 Pre-Cond( S) 2

S2

................................. S n

Post-Cond( S) n Post-Cond(S)

Figure 6. Composition Engine

4

Implementation

I=I CI, I
1

S1 S2 . .

O1

I=IUO
2 1

1

S . .

O
3

2

I=IUO
3 2

2

O S . .
4

3

I=IUO
4 3

3

S . .

O
5

4

O

Figure 5. Composite Service Algorithm: Composition Input: QI - QueryInputs, QO - QueryOutputs, QCI - PreCond, QCO - Post-Cond Output: Result - ListOfServices 1. L NarrowServiceList(QI, QCI); 2. O GetAllOutputParameters(L); 3. CO GetAllPostConditions(L); 4. While Not (O  QO) 5. I = QI O; CI QCI CO; 6. L' NarrowServiceList(I, CI); 7. End While; 8. Result RemoveRedundantServices(QO, QCO); 9.Return Result;

Our discovery and composition engine is implemented using Prolog [11] with Constraint Logic Programming over finite domain [10], referred to as CLP(FD) hereafter. In our current implementation, we used semantic descriptions written in the language called USDL [1]. The repository of services contains one USDL description document for each service. USDL itself is used to specify the requirements of the service that an application developer is seeking. USDL is a language that service developers can use to specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. USDL uses WordNet [9] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the "meaning" of input parameters, outputs, and the side-effect induced by the service is given by mapping these syntactic terms to concepts in WordNet (see [1] for details of the representation). Inclusion of USDL descriptions, thus makes services directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory for a "matching" service. Due to lack of space, we do not go into the details of the language in this paper. They are available in our previous work [2]. These algorithms can be used with any other Semantic Web service description language as well. It will involve extending our implementation to work for other description formats, and we are looking into that as part of our future work. The software system is made up of the following components. Triple Generator: The triple generator module converts

each service description into a triple. In this case, USDL descriptions are converted to triples like: (Pre-Conditions, affect-type(affected-object, I, O), PostConditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [2]. In case conditions on a service are not provided, the Pre-Conditions and Post-Conditions in the triple will be null. Similarly if the affect-type is not available, this module assigns a generic affect to the service. Query Reader: This module reads the query file and passes it on to the Triple Generator. We use USDL itself as the query language. A USDL description of the desired service can be written, which is read by the query reader and converted to a triple. This module can be easily extended to read descriptions written in other languages. Semantic Relations Generator: We obtain the semantic relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms, meronyms, holonyms and many more. USDL descriptions point to OWL WordNet for the meanings of concepts. A theory of service substitution is described in detail in [2] which uses the semantic relations between basic concepts of WordNet, to derive the semantic relations between services. This module extracts all the semantic relations and creates a list of Prolog facts. We can also use any other domainspecific ontology to obtain semantic relations of concepts. We are currently looking into making the parser in this module more generic to handle any other ontology written in OWL. The query is parsed and converted into a Prolog query that looks as follows: discovery(sol(queryService, ListOfSolutionServices). The engine will try to find a list of SolutionServices that match the queryService. Composition Engine: The composition engine is written using Prolog with CLP(FD) library. It uses a repository of facts, which contains all the services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs), encodeParam(QueryOutputs, QO), getExtInpList(QueryInputs, InpList), encodeParam(InpList, QI), performForwardTask(QI, QO, LF),

performBackwardTask(LF, QO, LR), getMinSolution(LR, QI, QO, A), reverse(A, RevA), confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The query is converted into a Prolog query that looks as follows: composition(queryService, ListOfServices). The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the built-in, higher order predicate 'bagof' to return all possible ListOfServices that can be composed to get the requested queryService. Output Generator: After the Composition engine finds a matching service, or the list of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files in any desired XML format.

5

Efficiency and Scalability Issues

In this section we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It is because of these features, we decided on the multi-step narrowing based approach to solve these problems and implemented it using constraint logic programming. Correctness: Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions. Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always find a correct solution (if one exists) in the minimum possible steps. The formal proof of correctness and minimality is beyond the scope of this paper. Pre-processing: Our system initially pre-processes the repository and converts all service descriptions into Prolog terms. The semantic relations are also processed and loaded as Prolog terms in memory. Once the pre-processing is done, then discovery or composition queries are run against all these Prolog terms and hence we obtain results quickly and efficiently. The built-in indexing scheme and constraints in CLP(FD) facilitate the fast execution of queries. During the pre-processing phase, we use the term representations of services to set up constraints on services and the individual input and output parameters. This further helped us in getting optimal results. Execution Efficiency: The use of CLP(FD) helped significantly in rapidly obtaining answers to the discovery and composition queries. We tabulated processing times for different size repositories and the results are shown in Section 6. As one can see, after pre-processing the repository, our system is quite efficient in processing the query. The query execution time is insignificant.

Programming Efficiency: The use of Constraint Logic Programming helped us in coming up with a simple and elegant code. We used a number of built-in features such as indexing, set operations, and constraints and hence did not have to spend time coding these ourselves. This made our approach efficient in terms of programming time as well. Not only the whole system is about 200 lines of code, but we also managed to develop it in less than 2 weeks. Scalability: Our system allows for incremental updates on the repository, i.e., once the pre-processing of a repository is done, adding a new service or updating an existing one will not need re-execution of the entire pre-processing phase. Instead we can easily update the existing list of CLP(FD) terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will be negligible, perhaps a few milliseconds. With real-world services, it is likely that new services will get added often or updates might be made on existing services. In such a case, avoiding repeated pre-processing of the entire repository will definitely be needed and incremental update will be of great practical use. The efficiency of the incremental update operation makes our system highly scalable. Use of external Database: In case the repository grows extremely large in size, then saving off results from the preprocessing phase into some external database might be useful. This is part of our future work. With extremely large repositories, holding all the results of pre-processing in the main memory may not be feasible. In such a case we can query a database where all the information is stored. Applying incremental updates to the database is easily possible thus avoiding recomputation of pre-processed data . Searching for Optimal Solution: If there are any properties with respect to which the solutions can be ranked, then setting up global constraints to get the optimal solution is relatively easy with the constraint based approach. For example, if each service has an associated cost, then the discovery and the composition problem can be redefined to find the solutions with the minimal cost. Our system can be easily extended to take these global constraints into account.

Repository Size (num of services) 2000 2000 2000 2500 2500 2500 3000 3000 3000

Number of I/O parameters 4-8 16-20 32-36 4-8 16-20 32-36 4-8 16-20 32-36

PreProcessing Time (secs) 36.5 45.8 57.8 47.7 58.7 71.6 56.8 77.1 88.2

Query Exec Time (msecs) 1 1 2 1 1 2 1 1 3

Incremental update (msecs) 18 23 28 19 23 29 19 26 29

Table 3. Performance on Discovery Queries

found is that the repository was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 3 shows performance results of our Composition algorithm on discovery queries and table 4 shows results of our algorithm on composition queries. The times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than or equal to the wall clock time. The results are plotted in figure 8. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which is negligible (just 1 to 3 msecs) even for complex queries with large repositories.

6

Performance

We used repositories from WS-Challenge website[13], slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema file. We evaluated our approach on different size repositories and tabulated Preprocessing and Query Execution time. We noticed that there was a significant difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we

Figure 7. Performance on Discovery Queries

Figure 8. Queries Repository Size (num of services) 2000 2000 2000 3000 3000 3000 4000 4000 4000 Table 4. Queries

Performance

on

Composition

Number of I/O parameters 4-8 16-20 32-36 4-8 16-20 32-36 4-8 16-20 32-36

PreProcessing Time (secs) 36.1 47.1 60.2 58.4 60.1 102.1 71.2 87.9 129.2

Query Exec Time (msecs) 1 1 1 1 1 1 1 1 1 on

Incremental update (msecs) 18 23 30 19 20 34 18 22 32

Performance

Composition

7

Related Work

Composition of Web services has been active area of research recently [14, 15, 23, 18]. Most of these approaches are based on capturing the formal semantics of the service using an action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available [17]. To the best of our knowledge, most of these approaches that use planning are restricted to sequential compositions (i.e.,

a linear chain of services), rather than a directed acyclic graph. Our approach automatically selects atomic services from a repository and produces the composition flow in the form of a directed acyclic graph. The authors in [19, 20] present a composition technique by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also relies on a user-defined plan template which is created manually. There are industry solutions based on WSDL and BPEL4WS where the composition of the flow is obtained manually. A comparison of the approaches based on AI planning techniques and approach based on BPEL4WS is presented in [17]. This work shows that in both these approaches, the flow of the composition is determined manually. They do not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when an explicit flow is provided. In contrast, we have shown a technique to automatically determine these complex flows using semantic descriptions of atomic services. A process-level composition solution based on OWL-S is proposed in [21]. In this work the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but instead assume that they already have the list of atomic services. In contrast, we automatically find the services that are suitable for composition based on the query requirements for the new composed service. In [22], a semi-automatic composition technique is presented in which atomic services are selected for each stage of composition. This selection process involves decision making by a human controller at each stage, i.e., the selection process requires some manual intervention. Another related area of research involves message conversation constraints, also known as behavioral signatures [16]. Behavior signature models do not stray far from the explicit description of the lexical form of messages, they expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while our approach deals with higher-level real world concepts. However, both these approaches can be regarded as complementary concepts when taken in the context of real world service composition, and both technologies are currently being used in the development of a commercial services integration tool [24]. Our most important, novel contribution in this paper is

our technique for automatically selecting the services that are suitable for obtaining a composite service, based on the user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use human input on what services would be suitable for composition. Our technique also handles non-sequential compositions (i.e., composition where there can be more than one service involved at any stage, represented as a directed acyclic graph of services) rather than sequential composition (i.e, a linear chain of services) which is the case with most of the existing approaches.

8

Conclusions and Future Work

To catalogue, search and compose Web services in a semi-automatic to fully-automatic manner we need infrastructure to publish Web services, document them, and query them for matching services. Our semantics-based approach uses semantic description of Web services (example USDL descriptions). Our composition engines find substitutable and composite services that best match the desired service. Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services, number of services in a composition, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential or non-sequential composition that is possible for a given query. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. Use of Constraint Logic Programming helped greatly in obtaining an efficient implementation of this system. Our future work includes extending our engine to work with other web services description languages like OWLS, WSML, WSDL-S, etc. This should be possible as long as semantic relations between concepts are provided. It will involve extending the TripleGenerator, QueryReader, and SemanticRelationsGenerator modules. We would also like to extend our engine to support an external database to save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size which can easily be the case in future. Future work also includes developing an industrial-strength system based on the research reported in this paper, in conjunction with a system that allows (semi-) automatic generation of USDL descriptions from code and documentation of a service [24].

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005. [2] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics Description Lan-

guage for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas. edu/~sxk038200/USDL.pdf. [3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems, pp. 46-53, Mar. '01. [4] OWL-S www.daml.org/services/owl-s/1. 0/owl-s.html. [5] WSML: Web Service Modeling Language. www. wsmo.org/wsml/. [6] WSDL-S: Web Service Semantics. http://www. w3.org/Submission/WSDL-S. [7] WSDL: Web Services Description Language. http: //www.w3.org/TR/wsdl. [8] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services. In ICWS, pp 751-758, '05. [9] OWL WordNet http://taurus.unine.ch/ knowler/wordnet.html. [10] K. Marriott, P. Stuckey. Prog. with Constraints: An Introduction. MIT Press, '98. [11] L. Sterling, S. Shapiro. The Art of Prolog. MIT Press. [12] OWL: Web Ontology Language Reference. http: //www.w3.org/TR/owl-ref. [13] WS Challenge 2006. http://insel.flp.cs. tu-berlin.de/wsc06. [14] U. Keller, R. Lara, H. Lausen, A. Polleres, D. Fensel. Automatic Location of Services. In ESWC, '05. [15] S. Grimm, B. Motik, and C. Preist Variance in eBusiness Service Discovery. In Workshop at ISWC, '04. [16] R. Hull and J. Su. Tools for design of composite Web services. In SIGMOD, '04. [17] B. Srivastava, J. Koehler. Web Services Composition - Current Solutions and Open Problems. In ICAPS, '03. [18] B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pp 467-477. [19] S. McIlraith, T.C. Son Adapting golog for composition of Web services. In KRR, pp 482-493, '02. [20] S. McIlraith, S. Narayanan Simulation, verification and automated composition of services. In WWW, '02. [21] M. Pistore, P. Roberti, P. Traverso Process-Level Composition of Executable Services In ESWC, pp 6277, '05. [22] E. Sirin, J. Hendler, and B. Parsia Semi-automatic Composition of Web Services using Semantic Descriptions In Workshop at ICEIS, '02 . [23] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara Semantic Matching of Web Service Capabilities. In ISWC, pp 333-347, '02. [24] Metallect IQ Server. http://metallect.com/ downloads/Metallect_Product_Brief_ IQServer.pdf

International Journal of Computer Mathematics Vol. 89, No. 16, November 2012, 21232142

Mathematical analysis and simulation of multiple keys and S-Boxes in a multinode network for secure transmission
Ajay Kakkara *, M.L. Singhb and P.K. Bansalc
a Department of Electronics and Communication Engineering, Thapar University, Patiala, Punjab, India; b Department of Electronics Technology, Guru Nanak Dev University, Amritsar, Punjab, India; c M.I.M.I.T.,

Malout, India
(Received 15 September 2010; revised version received 1 June 2011; second revision received 2 September 2011; third revision received 1 June 2012; fourth revision received 11 June 2012; accepted 13 June 2012) The requirement of data security is an important parameter for all organizations for their survival in the world. Cryptography is the best method to avoid unauthorized access to data. It involves an encryption algorithm and the keys that are being used by the users. Multiple keys provide a more secure cryptographic model with a minimum number of overheads. There are various factors that affect the security pattern such as the number of keys and their length, encryption algorithm, latency, key shifting time, and users. In this paper, a new approach is proposed for generating keys from the available data. The analysis of various times, such as encryption, decryption, key setup, processing, and key shifting times, has been done. The model takes minimum time to replace the faulty keys with the fresh keys. In this paper, we consider all the above-mentioned factors and suggest an optimized way of using them. Keywords: encryption; S-Boxes; keys; key shifting time; hacking 2010 AMS Subject Classification: 94A60

1.

Introduction

Security attacks against networks are increasing significantly with time using latest software. Over the years, a number of techniques and approaches have been developed to ensure data confidentiality, integrity, and availability. The techniques used for data security include multiple passwords, cryptography, and biometrics. Cryptography is a technique used to avoid unauthorized access to data. It includes an encryption algorithm and keys. The basic problem that is concerned with keys is that their strength gets degraded with time. Using powerful software packages, it is quite easy to determine the keys. It has been observed that a single key does not provide the required secure model [4,26]. The key length and number of keys and their mutual arrangement provide better security; for the above, it is mandatory to use them in an optimized manner to avoid overheads. If we use 128 key lengths to encrypt 8-bit data, then it will consume more power and also more processing time. Therefore, the best method is to use multiple keys of short length to encrypt
*Corresponding author. Email: kakkar_ajay29@rediffmail.com  Current address: Department of Electronics and Communication Engineering, Thapar University, Patiala, Punjab, India.

ISSN 0020-7160 print/ISSN 1029-0265 online  2012 Taylor & Francis http://dx.doi.org/10.1080/00207160.2012.704022 http://www.tandfonline.com

2124

A. Kakkar et al.

the data [12,25]. The main limitations in this scheme are that the short keys are more sensitive and get attacked by the hacker easily. To tackle the problem, first we determine the failure rate of the keys and calculate the time for which their security level remains in the higher level [23]. Another problem with keys is that how these are transported from the sender to the receiver. Therefore, this can be avoided by generating the keys from the available data with the help of an encryption algorithm. Using the information regarding timing and power consumption by a device during the execution of a cryptographic algorithm, cryptanalysts can break the model [15]. Therefore, the main purpose of a secure encryption algorithm is to protect the interests of parties communicating in the presence of adversaries [14]. The modelling of the behaviour of cyber attackers is difficult and determining the appropriate level of attack is very important from the security point of view. We are aware of the fact that in a multinode network (MN), security decreases with an increase in the number of nodes [13]. It is clear that the flaws in the key designing account for 30 45% of security problems, and architectural risk analysis plays an important role in any secure program [7,14,19,24]. In view of this, multiple keys are used to provide resistance against the virtual and real attacks made by hackers. The following section describes the work of various researchers in the area of data security in MNs. Data Encryption Standard is one of the most widely accepted, publicly available cryptographic systems. It was developed by IBM in the 1970s but was later adopted by the US government as a National Standard. In 1990, International Data Encryption Algorithm (IDEA) was originally developed as the Proposed Encrypted Standard, and in 1992, it was renamed as IDEA. It is a block cipher that uses 64-bit data blocks and a 128-bit key. Aiello and Venkatesan [1] described that nselected plaintexts in an MN can be distinguished by the hacker with the numbers from a random function. This means that it is possible to hack the model with a determined probability. Banerjee et al. [3] gave an overview of signalling enhancement and recovery techniques used in an MN. Such techniques are useful to determine the security of a model. Eschenauer and Gligor [11] proposed a random key establishment technique for wireless sensor networks. Lee and Griffith [17] presented a hierarchical approach to resolve multiple failures in an MN in which various security levels have been proposed for different types of attacks, and a recovery mechanism can be selected on the basis of these security levels. Chan et al. [8] extended the technique of nrandom key establishment that enables two neighbouring nodes to establish a secure communication only when they share n common keys (where n  2). Du et al. [10] developed two similar random key pre-distribution techniques that use the multi-space key pool to improve network resilience and memory usage efficiency [18]. Hundessa and DomingoPascual [16] presented a protection mechanism packed with multiple key(s) to handle multiple link/node failures. Furthermore, Backes and Pfitzmann [2] presented the relating symbolic and cryptographic secrecy technique for an MN. Bertino et al. [4] discussed an efficient time-bound hierarchical key management scheme for secure broadcasting. There are numerous cryptographic algorithms for data encryption and authentication techniques for an MN. Using encryption, an efficient generic solution for an MN was proposed by Naor et al. [22]. Naor's model was not compatible with multiple keys having different failure rates. Hundessa and Domingo-Pascual [16] provided data-gathering strategies over all the possible network routes. Blake and Kolesnikov [5,6] did not provide any practical ways to achieve secure re-routing schemes.

2.

Motivation from the literature survey

From the literature survey, the following observations have been drawn:  A single key with a fixed length cannot be used to provide secure communication in an MN. By knowing the data and key length, the hacker is able to generate side-channel and middle-line attacks.

International Journal of Computer Mathematics

2125

 If the key length is short (11024 bits), it is very easy for the hacker to get the hold of the key by using various permutations and combinations. On the other hand, if large key lengths are used, it results in complexity, which increases the probability of error.  A single key with a variable length provides little bit more secure communication than a single key having a fixed length. The technique is preferred only for short data streams in an MN having less number of nodes.  Multiple keys having different failure rates can be achieved by varying the key length. They are always preferred for encrypting the data in an MN having a large number of nodes. Multiple keys have different failure rates: (i) if the length of the keys is of different order, (ii) if different polynomials are used for the encryption, and (iii) if the size of the data block varies.  In case of node failure, the algorithm immediately generates new keys for the corresponding node. It has been found that for an efficient and reliable model, keys should be generated from the available data. Key recovery mechanisms should be available in the model in order to take care of the failure situation. There is a need to minimize the key shifting time () from the first key to the second key in the case of a multiple key encryption-based system. Keeping in mind the importance of multiple keys for secure data transmission, this work incorporated the use of multiple keys. Multiple keys were generated from the available data to reduce the overheads such as the need of sending additional bits along with the data. Eight to 16 S-Boxes were used to perform random round functions for the generation of multiple keys. There is a design and development procedure of an optimized encryption algorithm that is based on an efficient key management scheme in order to provide secure data transmission in an MN. For better security, multiple keys having a minimum key shifting time were used. The failure rate of multiple keys was evaluated and analysed mathematically to make the model secure. The analysis shows that the failure rate plays a vital role in reducing the time available to the hackers for the various attempts made to destroy the model. In the encryption process, a slight increase in the processing time was observed at various nodes, but it is acceptable because it is very small in comparison with that consumed by the existing encryption algorithms. The objective of this work is to develop an optimized efficient key management technique(s) in order to     generate random key(s) from the data by the algorithm, determine the failure rate of multiple key(s) used by various S-Boxes, reduce the time available for the hacker for making attempts to destroy the model, and minimize the key shifting time () from the first key to the second key and so on.

3.

Proposed work

Modern cryptography involves the use of keys for data signing, encoding, and decoding. Some keys are distributed privately between the parties, while others allow the parties to use public keys that can be broadcast openly [8,9]. The level of protection is varied for every situation and also dependent upon the work and technique used; some encryption techniques provide a virtually unbreakable barrier to information theft; others just require a determined attacker with moderate resources to be broken. One way of comparing the techniques on this level is to estimate how much CPU time would be required on a machine of a given processing speed to iterate through all the possible keys to the encoded data, based upon the permutation [27]. System-wide security is always required to make sure that the data are safe; data are safe for some time while using

2126

A. Kakkar et al.

Figure 1.

Conversion of alphabets into their equivalent codes.

Figure 2. Various stations in a model with buffers.

security techniques, but overall system is not safe; using the information about timing, power consumption, and radiation of a device when it executes a cryptographic algorithm, cryptanalysts have been able to break the system. Keeping in mind the importance of multiple keys for secure data transmission, this work incorporated the use of multiple keys. Multiple keys were generated from the available data to reduce the overheads such as the need of sending additional bits along with the data. The data were encoded using Figure 1 in which all the alphabets and the number having their weight were further converted into binary (0/1), for example, 25 = 010101, 55 = 101101, and so on. All the binary data were encoded with the help of a key and further passed through S-Boxes (having a different key for each round) as shown in Figure 2. During the transmission of data if any station fails, due to the attacks made by the hacker or by other means (atmospheric conditions), then it makes the overall model weak. To maintain the security over a model, all the parameters such as nodes, key generation mechanism, and latency need continuous attention; they must be upgraded with time. Each station is required to be packed up by the recovery mechanism. It is important to calculate the latency time concerned with a particular station:
N

D(S , K ) =
i =1

d (Si ) + d (ki ),

International Journal of Computer Mathematics

2127

Figure 3.

Different stations in a model.

where D(S ) and D(K ) are the delay caused by various stations due to the logical effort in the encryption process and delay caused by the various keys to get generated from the data, respectively. In case of failure of a station, let us take a case in which station S2 is hacked by the hacker, then the data of the same station are moved to the neighbouring station S3 ; in such a case, the buffers g and h available at S3 also result in a delay (Figure 3). The total delay is calculated as
N

D(S , K ) =
i =1

d (Si ) + d (ki ) + d (gi ) + d (hi ).

User A wishes to transmit the data in an MN having nodes (S1 , S2 , . . . , S8 ). Multiple paths are provided to send the data over the path P1 , whereas encrypted packets are transmitted by A. Similarly, one can select the other paths P2 and P3 by considering the delay and reliability. For a secure system, all the intermediate nodes should be under the control of master node A. Failure notification of a node is immediately forwarded to the nearest node, preferably the neighbouring node, in order to reduce the latency and congestion. Fast reroute methods are employed in such situations, which deal with the change of path and are known as dynamic routing for a network. Dynamic re-routing is effective when a model has less number of nodes. Whenever the security level of a node falls below a certain level, then master node A has the power to immediately change the path and re-encrypt the data using another key. In the above exercise, we also make sure that there is no faulty node in the new path and determine the failure rates of S-Boxes: Path P1 : A - S1  (K1 ) - S3  (K1 , K1 ) - S4 - S5 - S7 (K1 , K1 , K1 ) - S8 - B. In the above equation, if S3 is the weak station, then either change the key for the encryption Path P1 : A - S1  (K2 ) - S3  (K2 , K2 ) - S4 - S5 - S7 (K2 , K2 , K2 ) - S8 - B or change the path (i) Path P1 : A  S2  (K1 )  S3  (K1 , K1 )  S4  S5  S7 (K1 , K1 , K1 )  S8  B, (ii) Path P2 : A  S2  (K2 )  S3  (K2 , K2 )  S4  S5  S7 (K2 , K2 , K2 )  S8  B.

2128

A. Kakkar et al.

For a highly secure system, transmission is done from A  S4 . Note: All the stations have the power to change the key and encrypt the data with it only iff A permits: If X1 = inputs, S1 = weak stations, S1 = strong stations. Now, we assume that the S-Boxes are under threat due to a high failure rate of keys. The probability of recovering the data and the latency time and making the system reliable by the help of a shifting key is determined in the following section. 3.1 Case 1: when two stations are under attacks made by the hacker (failures of two S-Boxes in a given model)

In this case, it is required to change the key for particular stations, but this will be not treated as a reliable method, so it will be preferred to change the path; for this, it is required to determine the input data as those concerning weak stations: X1 - -  S1 , X2 - -  S2 & X1,2 - -  S1 & S2 . We know that S1 = X1  X1,2 , On the other hand, S1  S2 = (X1  X2 )  X1,2 . If xi = Pr (Xi ), xi,j = Pr (Xi,j ). Complement Pr (XiC ) = 1 - Pr (Xi ),
N Key Key Key

S2 = X2  X1,2 .

(1)

(2)

(3) (4)

P(Xi ) = 1.
i =1

Complement
N N

Pr (XiC )
i =1

=
i =1

1 - Pr (Xi ) = N - 1.

(5)

Similarly, we can determine the probabilities of S-Boxes as si = Pr (Si ), Using Equation (2), we can write s1 = x1 + x1,2 - x1  x1,2 , s2 = x2 + x1,2 - x2  x1,2 , s1,2 = x1  x2 + x1,2 - x1  x2  x1,2 . (7) si,j = Pr (Si,j ). (6)

International Journal of Computer Mathematics

2129

Rearrange the equation in order to get the probability of x1 , x2 , and x1,2 : s1 - x1,2 , 1 - x1,2 s2 - x1,2 x2 = , 1 - x1,2 s1,2 - x1  x2 x1,2 = . 1 - x1 - x2 - x1,2 x1 =

(8)

Similarly, for four weak stations in a model, it is required to determine the single failure Si , double failures Si,i , triple failures Si,j,k , and quadrate failures Si,j,k ,m . For strong stations Ri , the probabilities are determined using complements (Equation (3)). Pr (Ri ) = Pr (XiC ) or ri = Pr (Ri ) = 1 - xi . In terms of strong stations, Pr (S1 ) = 1 - x1 = r1  r1,2  r1,3  r1,4  r1,2,3  r1,3,4  r1,2,4  r1,2,3,4 , Pr (S2 ) = 1 - x2 = r2  r1,2  r2,3  r2,4  r1,2,3  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S3 ) = 1 - x3 = r3  r1,3  r2,3  r3,4  r1,2,3  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S4 ) = 1 - x4 = r4  r1,4  r2,4  r3,4  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 . Also, Pr (S1  S2 ) = Pr (S1  S2 ) = 1 - s1 - s2 + s1,2 = r1  r2  r1,2  r1,3  r2,3  r1,4  r2,4  r1,2,3  r1,3,4  r1,2,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S1  S3 ) = Pr (S1  S3 ) = 1 - s1 - s3 + s1,3 = r1  r3  r1,2  r1,3  r2,3  r1,4  r3,4  r1,2,3  r1,3,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S1  S4 ) = Pr (S1  S4 ) = 1 - s1 - s4 + s1,4 r1  r4  r1,2  r1,4  r1,3  r2,4  r3,4  r2,4  r1,2,3  r1,2,4  r1,3,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S2  S3 ) = Pr (S2  S3 ) = 1 - s2 - s3 + s2,3 = r2  r3  r1,2  r1,3  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S2  S4 ) = Pr (S2  S4 ) = 1 - s2 - s4 + s2,4 = r2  r4  r1,2  r1,4  r2,3  r2,4  r2,4  r3,4  r1,2,3  r1,2,4  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S3  S4 ) = Pr (S3  S4 ) = 1 - s3 - s4 + s3,4 = r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S1  S2  S3 ) = Pr (S1  S2  S3 ) = 1 - s1 - s2 - s3 + s1,2 + s1,3 + s2,3 - s1,2,3 = r1  r2  r3  r1,2  r1,3  r2,3  r1,4  r2,4  r3,4  r1,2,3  r1,3,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S1  S2  S4 ) = Pr (S1  S2  S4 ) = 1 - s1 - s2 - s4 + s1,2 + s1,4 + s2,4 - s1,2,4 = r1  r2  r4  r1,2  r1,4  r1,3  r2,3  r2,4  r1,4  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , (9)

2130

A. Kakkar et al.

Pr (S1  S3  S4 ) = Pr (S1  S3  S4 ) = 1 - s1 - s3 - s4 + s1,3 + s1,4 + s2,4 - s1,2,4 = r1  r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S2  S3  S4 ) = Pr (S2  S3  S4 ) = 1 - s2 - s3 - s4 + s1,2 + s2,3 + s2,4 - s1,2,4 = r2  r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S1  S2  S3  S4 ) = Pr (S1  S2  S3  S4 ) = 1 - s1 - s2 - s3 - s4 + s1,2 + s1,3 + s1,4 + s2,3 + s2,4 + s3,4 - s1,2,3 - s1,2,4 - s1,3,4 - s2,3,4 + s1,2,3,4 = r1  r2  r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 . Now, determine the values of xi , xi,j , xi,j,k , xi,j,k ,l : x1 = 1 - x2 = 1 - x3 = 1 - x4 = 1 - P r (S 1  S 2  S 3  S 4 ) , Pr (S2  S3  S4 ) P r (S 1  S 2  S 3  S 4 ) , Pr (S1  S3  S4 ) P r (S 1  S 2  S 3  S 4 ) , Pr (S 1  S 2  S 4 ) P r (S 1  S 2  S 3  S 4 ) , Pr (S1  S2  S3 ) Pr (S1  S3  S4 )  Pr (S2  S3  S4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 3  S 4 ) P r (S 1  S 2  S 4 )  P r (S 2  S 3  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 2  S 4 ) Pr (S1  S2  S3 )  Pr (S2  S3  S4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 2  S 3 ) P r (S 1  S 2  S 4 )  P r (S 1  S 3  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 1  S 4 ) P r (S 1  S 2  S 3 )  P r (S 1  S 3  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 1  S 4 ) P r ( S 1  S 2  S 3 )  P r ( S1  S 2  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 1  S 2 ) P r ( S 1  S 4 )  P r ( S 2  S 4 )  P r ( S 3  S4 )  P r ( S1  S 2  S 3  S 4 ) , Pr (S4 )  (Pr (S1  S2  S4 ))(Pr (S1  S3  S4 ))(Pr (S2  S3  S4 )) Pr (S1  S3 )  Pr (S2  S3 )  Pr (S3  S4 )  Pr (S1  S2  S3  S4 ) , Pr (S3 )  (Pr (S1  S2  S3 ))(Pr (S1  S3  S4 ))(Pr (S2  S3  S4 ))

x1,2 = 1 - x1,3 = 1 - x1,4 = 1 - x2,3 = 1 - x2,4 = 1 - x3,4 = 1 -

x1,2,3 = 1 - x1,2,4 = 1 -

International Journal of Computer Mathematics Table 1. Round functions with key size in encryption process. Key size and data length after encryption 36 4.5 2.25 1.125 0.5625 1296 162 81 40.5 20.25 46656 5832 2916 1458 729

2131

Round functions 8 16 32 64

x1,3,4 = 1 - x2,3,4 = 1 - x1,2,3,4 = 1 - 

Pr (S1  S2 )  Pr (S2  S3 )  Pr (S2  S4 )  Pr (S1  S2  S3  S4 ) , Pr (S2 )  (Pr (S1  S2  S3 ))(Pr (S1  S2  S4 ))(Pr (S1  S3  S4 )) Pr (S1  S2 )  Pr (S1  S3 )  Pr (S1  S4 )  Pr (S1  S2  S3  S4 ) , Pr (S1 )  (Pr (S1  S2  S3 ))(Pr (S1  S2  S4 ))(Pr (S1  S3  S4 )) Pr (S1 )  Pr (S2 )  Pr (S3 )  Pr (S4 )  Pr (S1  S2  S3 ) (Pr (S1  S2  S3  S4 ))  (Pr (S1  S2 ))  (Pr (S1  S3 ))

Pr (S1  S2  S4 )  Pr (S1  S3  S4 )  Pr (S2  S3  S4 ) (Pr (S1  S4 ))  (Pr (S2  S3 ))  (Pr (S2  S4 ))  (Pr (S3  S4 ))

Using the above expressions, all the values of xi , xi,j , xi,j,k , xi,j,k ,l can be evaluated. The cryptographic key pair needs maintenance; in order to keep up with the increasing processing power available for breaking the keys, the keys need to be replaced periodically [5,20]. This will also limit the possibility of damage in a situation where somebody has managed to steal a copy of the secret key. Buffers g and h (Figure 1) were provided to each station to store the incoming data and the key used to re-encrypt the data. The probability of an optimized linear expression was calculated by |Pl - 0.5|  2NS  |Po - 0.5|NS . (10)

The size of the key and the data length after the encryption process provide the flexibility to choose the desired round functions as shown in Table 1.

4.

Latency and energy consumption

The latency and energy consumption for encryption and decryption processes were evaluated in order to get the information about the time which will be available for the hacker during the channel; if this time is observed to be large, then one has to make more number of attempts to break the model; there is the possibility that during this exercise, the hacker would fail to break it but cause harm to it [20]. Using the experimental results, the smallest value of latency was calculated (Table 2). Moreover, the efficiency for the 8-bit processors (Motorola 6811) and for AMD 4600 was calculated (Figure 4). 4.1 Generation of a symmetric key

If C is the arrangement of characters in a set and the nth alphabet (A) in a set is A = {A0 , A1 , A2 , . . . , An-1 }, then the function is given by F = {f (A0 ), f (A1 ), . . . , f (An-1 )}. One-to-one mapping was done for each character of A w.r.t. C .

2132 Table 2.

A. Kakkar et al. Latency in the encryption and decryption process. Operation Encryption Key size (bits) 8 8 16 16 Round functions 8 16 8 16 Latency (s) 22.32 34.23 15.55 21.25 Energy consumption (J) 24.54 32.55 23.43 44.45

Decryption Key size (bits) Round functions 8 8 8 16 16 8 16 16

24.18 36.67 19.10 22.10

23.50 27.57 19.09 42.10

Figure 4. Various factors for different processors.

Encryption: Ek (M ) = f (m0 )f (m1 )    f (mn-1 ). For substitution cipher, N= log2 S  d. D (11)

Using Vernam cipher Ci = (mi + ki ) mod 2, Equation (11) becomes Ek (M ) = (m1 + k1 )  (m2 + k2 ). The  operation with the same key Ek (M ) = Mi results in a symmetric key [ ki  ki = 0]. For an asymmetric key, different operations are required: (i) X-OR and use of S-Boxes and (ii) generation of keys from the data. 4.2 Generation of keys from the data Let us assume a key set K = (kn ) having a security parameter n (Figure 5). If xi is equal to the input data, then yi is equal to the input data, 1  i  N , B is the Boolean function, and An is the adversary A.

International Journal of Computer Mathematics

2133

Figure 5.

Key generation process.

Then, Pr [B(yn ) = 1 : k  Kn , {yi  Ai (xi )}1iN ] < For any positive polynomial P and k , Pi = Pr [B(yn ) = 1 : {yi  Ai (xi )}1iN ] is the conditional probability of success of A for a fixed i, and we know that Pr k Kn 1 . > pi 2p(k ) Pr [k ]  Pk
k  Kn

1 . p(k )

 Pr [B(yn ) = 1 : k  Kn , {yi  Ai (xi )}1iN ] = =
Pr <1/2(p(k ))

Pr [k ]  Pk +
Pr >1/2(p(k ))

Pr [k ]  Pk <
Pr <1/2(p(k ))

Pr [k ] 

1 2P(k )

+
Pr >1/2(p(k ))

Pr [k ]  1, Kn 1  pi 2P(k ) Kn 1  pi 2P(k )   1 + Pr 2P(k ) 1 + Pr 2P(k ) i i In 1 > Pi 2P(k ) In 1 > Pi 2P(k ) < < 1 1 + , 2P(k ) 2P(k ) 1 . P(k )

Pr Pr

k k

For M and R positive polynomials and k  , 1 > Pr [B(yn ) = 1 : k  Kn , {yi  Ai (xi )}1iN ], M (k )R(k ) Pr [k ]  Pk 
k  KN Pi >1/M (k )

Pr [k ]  Pk , Pr [i]  Pi .

Pr [i]  Pi 
i  IN Pi >1/R(k )

From the above calculations, we can obtain the asymmetrical keys used for the S-Boxes.

5.

Cryptographic algorithm used for the generation of keys from the data

The proposed algorithm uses the available data to generate keys and also avoids the need of transmitting additional bits along with the cipher text. It improves the bandwidth and performance of the model, which enhances the data rate. The key generation mechanism was used to know both the parties (sender and receiver), so that the correct combination of keys can be used for retrieving the data. The key generation process also depends upon the input data stream, and Table 3 was used to generate the keys. It also represents the various conditions for the operation

2134

A. Kakkar et al.

Table 3. Algorithm to generate the keys from the available data with S-Boxes. Operation performed by S-Boxes A=B A-B AB  +B A  -B A  B A  A+B  A-B  AB  B A AB  AB

Data type Alphabets

Conditions A>B A=B A<B A>B A=B A<B A>B A=B A<B A>B A=B A<B

Key length (KL) An 8-bit KL is used if the input data stream is  16 bits; otherwise a 16-bit KL is used An 8-bit KL is used if the input data stream is  8 bits; otherwise a 16-bit KL is used An 8-bit KL is used if the input data stream is  16 bits; otherwise a 16-bit KL is used An 8-bit KL is used if the input data stream is  32 bits; otherwise a 16-bit KL is used

Round functions (RFs) 8 RFs are used if the input data stream is  16 bits; otherwise 16 RFs are used 8 RFs are used if the input data stream is  8 bits; otherwise 16 RFs are used 8 RFs are used if the input data stream is  16 bits; otherwise 16 RFs are used 8 RFs are used if the input data stream is  32 bits; otherwise 16 RFs are used

Number

Alphanumeric

Hybrid

to be performed by the S-Boxes in order to design an MN having single or multiple keys of a fixed/variable length. If the key size is small and has a fixed length, then it is discarded due to its poor security response. There is a need to determine the failure rate of all the keys in an MN for secure data transmission. The algorithm checks the input data stream, and further key generation process was used to design the multiple keys using S-Boxes. The input data streams are broadly classified into four categories, namely (i) alphabets, (ii) numeric, (iii) alphanumeric, and (iv) hybrid (includes the combination of alphabets, numbers, and special characters). The input data streams were processed using Figure 1 into numeric values and then further converted into binary strings. This binary data string was used for the random key generation process specified by Table 3. Once the key was generated, the encryption was carried out using the fixed and variable length keys. The variable length key was preferred due to less overheads, and it also provided a more secure model. The padding overheads were very less in this case. The failure rate of a key was checked using mathematical tools; for weak nodes, the re-encryption was done using the second key, which undergoes a procedure the same as that undergone by the first key. The second key is required if there is a node failure or the input data sequence is very large. The key strength is very high in the case of hybrid data sequences because more combinations are available for the generation of keys.

6. Analysis and simulation results for the failure rate of single and multiple keys having variable lengths for each node Multiple keys were generated from the available data sequences using Table 3. Initially, data were placed in a pool and divided into nearly two sections. Both the sections were compared, and further based upon the conditions, proper operations were performed. For example, if A > B and the data are in the terms of alphabets only, then the A + B operation can be performed in the initial phase and is named as the first round function. The output of this operation is used by the second round operation, which requires another operation for its working. These operations are randomly selected and they provide different outputs even if the pool has the same data for multiple keys. This procedure continues for 8 and 16 iterations depending upon the required security level for a given MN. For A > B and data stream greater than 16 bits, a 16-bit key can be used for the encryption of data. The first key was generated using the following method: assume

International Journal of Computer Mathematics

2135

that A = 1000001011010001 and B = 0010100000101110 are the two data streams available in the pool, then the first round function uses the first operation, which is given as follows: A = 1000001011010001, B = 0010100000101110, C = 1010101011111111. C is the output of the first round function, which is further used by the second round function, which is based upon the left shifting of the stream by 1 bit and is given as C = 1010101011111111, D = 0101010111111110. The output of the second round function is further used by the next S-Box and the process continues for 8 or 16 iterations. Similarly, the second key was generated by keeping an eye on the length and type of input data. 6.1 Case 1: when the input data are in the form of Alphabets

When the data are in terms of only alphabets, then the following steps are used for the generation of keys: (a) (b) (c) (d) (e) Converting the data (text) into a number using Figure 1. Checking the conditions based upon the data evaluated using Table 3. Performing the specified operation mentioned in Table 3. Converting a number into a binary format. Using S-Boxes in order to perform round functions for the generation of keys. Checking the next input if it is still in terms of alphabets and then following the same procedure; otherwise, switching to case 2, 3, or 4 as required. Generation of symmetrical and unsymmetrical keys

6.2

This section involves the analysis of symmetrical and unsymmetrical key generation mechanisms from the data streams. In the proposed algorithm, unsymmetrical keys were preferred because it eliminates the key transportation problem. For a small MN, the use of symmetrical keys is also required; that is why the symmetrical keys were been also considered. The nth alphabet A in a set is given as A = {A0 , A1 , A2 , . . . An-1 }. Then, the function for the same can be expressed as F = {f (A0 ), f (A1 ), . . . , f (An-1 )}. One-to-one encryption was done for each character of A w.r.t. the key. As a result, the encryption for the messages was achieved, and it is expressed as Ek (M ) = f (m0 )f (m1 )    f (mn-1 ), where m is the total number of messages. For a symmetrical key, the valid condition is f (1) = f (0). The second key function is given as f (x ) = 0 in [0,1]; then f (1) = f (0) leads to an asymmetrical key.

2136

A. Kakkar et al.

Proof f (1) - f (0) = f (x ), 1-0 which is not equal to 0: f (1) = f (0).

6.3 (a) (b) (c) (d) 6.4

Case 2: when the input data are in the form of Number Converting a number into a binary format. Checking the conditions based upon the data evaluated using Table 3. Performing the specified operation mentioned in Table 3. Using S-Boxes in order to perform round functions for the generation of keys. Case 3: when the input data are in the form of Alphanumeric

It has been observed that all the alphabets have their own frequency, that is, the occurrence of a particular alphabet in the given information is not the same for a given message. If all the common alphabets are processed in one step, then it will reduce the overheads of the system. For a given message M in a set of Y0 , Y1 , . . . , Yn-1 , the probability is defined as
n -1

P(Yi ) = 1.
i =0

The conditional probability of message X in a given message Y is PY (X ), which can also be written as P(X /Y ). The joint probability messages X and Y are given as P(X , Y ) = PY (X )P(Y ). The entropy is calculated as HY (X ) =
X ,Y

P(X , Y ) log2

1 , PY (X )

HY (X ) = -
X ,Y

P(X , Y ) log2 PY (X )

or HY (X ) =
Y

P(Y )
X

PY (X ) log2 PX (Y ) log2
Y

1 , PY (X ) 1 . PX (Y )

HX (Y ) =
X

P(X )

The entropy of the key is expressed as
n -1

Pi =
i =0

M-

1 n

2

.

The probability of the occurrence of an event always lies in the interval 0  Pi  1. For n  , the chance of getting the exact alphabet reduces to Pi  = 0; it means that an increase in the bits

International Journal of Computer Mathematics

2137

of the given information in a processing unit always increases the security of the model. It would be preferable to increase the number of bits and the round functions at the transmitter in order to provide an equal number of bits for A and B. If a system has nearly equal length sequences for the data and key, then the padding time is reduced, which results in fast processing. For a system having 26 alphabets and 09 numeric digits, the encryption of data takes place with the help of multiple keys designed by the S-Boxes. The analysis for the probability of finding the correct message when the input is in terms of alphanumeric is given as
35

Pi =
i =0 35

M-

1 36

2

,
35

Pi =
i =0

(M )2 - 2/36
i =0

(M )2 + 36

1 36

2

.

For identical messages M , the second data are the same as the first data, that is,
35

35 2 i=0 (M )

= 1.

 Pi =
i =0

(M )2 -

2 1 + , 36 36

35

Pi =
i =0

(M )2 - 0.084.

The above example is applicable if a model contains less number of nodes (230, the results were verified on TMS 320 ADP6713). For larger stages, the probability of obtaining the correct message PM is given as PM = S (k ) log2 d ! = , D D

where S (k ) is the number of stages, D is the total data handled by the model, and d is the data of an individual node. For D   and large data for a stage d  n, the equation changes to PM = log2 n! , D

where n is the number of nodes used in the network. Using the above expression, one can obtain the desired message that would provide the information about the number of bits used for the encryption process. The probability of obtaining the correct message also depends upon the number of nodes used in the model. It is very clear that if D increases rapidly, then the decryption process takes more time to decrypt the cipher text. The processing time for each node depends upon the number of keys used for the encryption process, which clearly indicates that the probability of obtaining the correct message is indirectly related to the keys. If the number of nodes is increased, then it suggests that the number of keys in a network also increases; therefore, more time would be required to get the correct message. The following steps are used for the encryption of data: (a) (b) (c) (d) (e) Converting the data (text and number) into a numeric format using Table 1. Checking the conditions based upon the data evaluated using Table 3. Performing the specified operation mentioned in Table 3. Converting a number into a binary format. Using S-Boxes in order to perform round functions for the generation of keys.

2138

A. Kakkar et al. Table 4. Coding for the special character used in the hybrid technique. ! 121 ( 211 " 321 . 411 @ 122 ) 212 ; 322 / 412 # 123 { 213 ` 323 ` 413 $ 124 } 214 ' 324  414 % 125 [ 215 < 325
-

415

^ 126 ] 216 > 326 + 416

& 127 : 217 ? 327
-

417

* 128 " 218 , 328 = 418

Figure 6.

Evaluation of the failure rate of S-Boxes with two keys.

6.5

Case 4: when the input data are in the form of a hybrid

For the hybrid structure, the alphanumeric numbers were processed using case 3, and the special symbols were processed using Table 4. The key strength is very high in the case of hybrid structure. It offers more resistance to the hacker, and as a result, the model remains secure for more time. To protect the data from intruders, powerful encryption algorithms with multiple keys were used. After the encryption process, it is desirable to transmit the cipher text over the channel. The secure model was examined on the basis of its design, mode of transmission of data, and number of nodes. With an increase in the number of nodes, key length, number of keys, and data length, the model consumes more power and takes more time to generate keys from the available data. A new approach in which keys are generated and processed in the cryptographic model with the help of S-Boxes in order to reduce the processing time has been proposed. MATLAB 7.3 was used to determine the failure rate of various keys in an MN (Figure 6). The model is designed in such a way that it comprises multiple keys and S-Boxes and enables the higher classes to retrieve the encrypted data related to the lower classes. The lower classes do not have the power to access the data concerned with the higher classes. A key management scheme was used to provide such kind of facility to the higher classes. Once a key is exchanged,

International Journal of Computer Mathematics

2139

Figure 7.

Determination of the failure rate of S-Boxes with three keys.

the bit string of the key becomes known to the receiver. In such cases, it is highly desirable to reencrypt the same data with the replaced key. This has been adopted only in cases where the failure rate of the previous key exceeds a predefined value. The behaviour of the keys is unpredictable in a real-time environment; there is always a difference between the characteristics of ideal keys and those of the real keys. In order to achieve a secure model, one should determine the extent to which several security patterns are robust to the known categories of attacks. Various classes were created to represent the number of attacks in a given interval of time. Figure 7 shows that if multiple keys are used, a number of attacks (varies from 200 to 9000) are not able to break the system even after 100 min. A total of five S-Boxes were used to design the key using round functions. The S-Boxes result in variable key lengths used for the encryption of data in five stages. This work is focused on the determination of the failure rate of both types of keys (single and multiple) for each node in a specified interval (90 min for the first key and 55 min for the second key). The first key was used to encrypt the data, and it provided only 20 min for the hacker to make the attacks. After 20 min, the first key was replaced by the second key, which remained active for 50 min. This combination can handle 1250 attacks in 100 min without collapsing. Similarly, one can calculate the failure rate of the other S-Boxes. The following observations have been drawn from Figures 6 and 7: (a) The strength of the keys decreases with time; therefore, the keys are used to encrypt the data in a short interval. For the same parameters, if the number of keys is increased, a better secure model is achieved. (b) S-Boxes are used to design the keys using round functions. The encryption of data with multiple keys always provides a better security level than that of data with a single key. (c) Variable key lengths make the hacking process tougher and cause congestion and complexity.

2140

A. Kakkar et al.

Figure 8.

Single key having a variable length designed by eight S-Boxes.

Figure 9.

Multiple keys having variable failure rates.

International Journal of Computer Mathematics

2141

The key shifting time was reduced by starting the generation procedure of the second key whenever the failure rate of the first key increased from 34%. This work includes the response of the nodes having encrypted data with multiple keys. Multiple keys were used for the encryption of data having the key shifting time (0.01 ns) and a better response was achieved and this is shown in Figure 8. Eight S-Boxes were used to design a single key of a variable length for the encryption of 8, 16, 32, 64, 128, 256, 512, and 1024 data sequences. Figure 8 shows that the response of multiple nodes having encrypted data with a single key (8-bit key length) is not acceptable from a security point of view. The failure rate of the key is fixed due to its length and slightly varies in accordance with the data streams. For higher data streams such as 1024 bits, the security level of the node is much poorer than the security level achieved when an 8-bit key is used to encrypt the 8-bit data. For the same parameters, if keys having a low failure rate are used, then the response of the model for a particular node (a4 and b5 in Figure 9) falls below the danger level. Whenever the failure rate of the second key is more than that of the first key, the system reliability tends to decrease. Recovery mechanisms are required to get a smooth response; conversely, if the first key fails, then it does not affect the model much because the second key is used to encrypt the data in that situation.

7.

Conclusion and future work

Secure and timely transmission of data is always an important aspect for an organization. An efficient encryption algorithm should consist of two factors: (i) fast response and (ii) reduced complexity. Key selection techniques and analysis for security provision of an MN were used in this study. The failure rate of multiple keys was calculated by considering the multiple failures in the model, and it has been analytically shown in the paper. The security also increases if the key size is increased and the key shifting time ( ) is reduced; the above combination may be adopted for secure transmission. This work can be extended if more number of S-Boxes (64 and 128) are used for the same task, and the key length would be reduced with nominal processing time. References
[1] W. Aiello and R. Venkatesan, Foiling birthday attacks in length-doubling transformations, in Advances in Cryptology  EUROCRYPT '96, U. Maurer, ed., Lecture Notes in Computer Science Vol. 1070, Springer-Verlag, Berlin, 1996, pp. 307320. [2] M. Backes and B. Pfitzmann, Relating symbolic and cryptographic secrecy, IEEE Trans. Dependable Secure Comput. 2(2) (2005), pp. 109123. [3] A. Banerjee, L. Drake, L. Lang, B. Turner, D. Awduche, L. Berger, K. Kompella, and Y. Rekhter, Generalized multiprotocol label switching: An overview of signaling enhancements and recovery techniques, IEEE Commun. Mag. 39(7) (2001), pp. 144151. [4] E. Bertino, N. Shang, and S.S. Wagstaff Jr., An efficient time-bound hierarchical key management scheme for secure broadcasting, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 6570. [5] I.F. Blake and V. Kolesnikov, Strong conditional oblivious transfer and computing on intervals, Proceedings of Advances in Cryptology  ASIACRYPT 2004, 10th International Conference on the Theory and Application of Cryptology and Information Security, Jeju Island, Korea, December 59, 2004, Lecture Notes in Computer Science Vol. 3329, Springer-Verlag, Berlin, Heidelberg, Germany, 2004, pp. 515529. [6] I.F. Blake and V. Kolesnikov, Conditional encrypted mapping and comparing encrypted numbers, Proceedings of the 10th International conference on Financial Cryptography and Data Security, Springer-Verlag, Berlin, Heidelberg, 2006, pp. 206220. [7] A. Bobbio and K.S. Trivedi, Computing cumulative measures of stiff Markov chains using aggregation, IEEE Trans. Comput. 39(10) (1990), pp. 12911297. [8] H. Chan, A. Perrig, and D. Song, Random key predistribution schemes for sensor networks, Proceedings of IEEE Symposium on Security and Privacy (S & P '03), IEEE Computer Society, Washington, DC, USA, 2003, pp. 197213. [9] G. Ciardo, R. Marmorstein and R. Siminiceanu, Saturation unbound, Proceedings of the 9th international conference on Tools and algorithms for the construction and analysis of systems, Springer-Verlag, Berlin, Heidelberg, 2003, pp. 379393.

2142

A. Kakkar et al.

[10] W. Du, J. Deng, Y. S. Han, P. K. Varshney, J. Katz and A. Khalili, A pairwise key predistribution scheme for wireless sensor networks, Proceedings of the 10th ACM conference on Computer and communications security (CCS'03), ACM, New York, NY, USA, 2003, pp. 4251. [11] L. Eschenauer, V. D. Gligor, A key-management scheme for distributed sensor networks, Proceedings of the 9th ACM conference on Computer and communications security (CCS'02), ACM, New York, NY, USA, 2002, pp. 4147. [12] M. Fischlin, A Cost-Effective Pay-Per-Multiplication Comparison Method for Millionaires, Proceedings of the 2001 Conference on Topics in Cryptology: The Cryptographer's Track at RSA, Springer-Verlag, London, UK, 2001, pp. 457472. [13] Z. Fu, H. Luo, P. Zerfos, S. Lu, and L. Zhang, The impact of Multihop wireless channel on TCP performance, IEEE Trans. Mobile Comput. 4(2) (2005), pp. 209221. [14] S.T. Halkidis, N. Tsantalis, A. Chatzigeorgiou, and G. Stephanides, Architectural risk analysis of software systems based on security patterns, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 129142. [15] X. He, M. Zhang, and Q.K. Yang, SPEK: A storage performance evaluation kernel module for block-level storage systems under faulty conditions, IEEE Trans. Dependable Secure Comput. 2(2) (2005), pp. 138149. [16] L. Hundessa, Optimal and guaranteed alternative LSP for multiple failures, Proceedings of 13th International Conference on Computer Communications and Networks, IEEE Conference, Illinois, Chicago, 2004, pp. 5964. [17] S.K. Lee, C. Kim, and D. Griffith, Hierarchical Restoration Scheme for Multiple Failures in GMPLS Networks, Proceedings of International Conference on Parallel Processing Workshops (ICPPW'02), IEEE Computer Society, Vancouver, BC, Canada, 2002, pp. 177182. [18] D. Liu and P. Ning, Establishing pairwise keys in distributed sensor networks, Proceedings of the 10th ACM conference on Computer and communications security (CCS'03), ACM, New York, NY, USA, 2003, pp. 5261. [19] V.B. Livshits and M.S. Lam, Finding security vulnerabilities in java applications with static analysis, Proceedings of the 14th conference on USENIX Security Symposium (SSYM'05), USENIX Association, Berkeley, CA, USA, 2005, pp. 1936. [20] B.B. Madan, K. Goseva-Popstojanova, K. Vaidyanathan, and K.S. Trivedi, A method for modeling and quantifying the security attributes of intrusion tolerant systems, Perform. Eval. 56(1) (2004), pp. 167186. [21] J. Muppala, M. Malhotra, and K. Trivedi, Stiffness-tolerant methods for transient analysis of stiff Markov chains, Microelectronics Reliab. 34(11) (1994), pp. 18251841. [22] M. Naor, B. Pinkas, and R. Sumner, Privacy preserving auctions and mechanism design, EC'99, ACM Press, New York, 1999, pp. 129139. [23] P. Paillier, Public-key cryptosystems based on composite degree residuosity classes, Proceedings of the 17th International conference on Theory and application of cryptographic techniques, Springer-Verlag, Berlin, Heidelberg, 1999, pp. 223238. [24] P. Papadimitratos and Z.J. Haas, Secure message transmission in mobile ad hoc networks, Ad Hoc Networks 1(1) (2003), pp. 193209. [25] J.T. Park, J.W. Nah, and W.H. Lee, Dynamic path management with resilience constraints under multiple link failures in MPLS/GMPLS networks, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 143154. [26] A. Reibman and K.S. Trivedi, Numerical transient analysis of Markov models, Comput. Oper. Res. 15(1) (1988), pp. 1936. [27] J. Ren and L. Harn, Generalized ring signatures, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 153164.

Copyright of International Journal of Computer Mathematics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.

Goal-Directed Execution of Answer Set Programs
Kyle Marple
University of Texas at Dallas 800 W. Campbell Road Richardson, TX, USA kbm072000@utdallas.edu

Ajay Bansal
Arizona State University 7231 E. Sonoran Arroyo Mall Mesa, Arizona, USA Ajay.Bansal@asu.edu

Richard Min
University of Texas at Dallas 800 W. Campbell Road Richardson, TX, USA min75243@hotmail.com

Gopal Gupta
University of Texas at Dallas 800 W. Campbell Road Richardson, TX, USA gupta@utdallas.edu

Abstract
Answer Set Programming (ASP) represents an elegant way of introducing non-monotonic reasoning into logic programming. ASP has gained popularity due to its applications to planning, default reasoning and other areas of AI. However, none of the approaches and current implementations for ASP are goal-directed. In this paper we present a technique based on coinduction that can be employed to design SLD resolution-style, goal-directed methods for executing answer set programs. We also discuss advantages and applications of such goal-directed execution of answer set programs, and report results from our implementation. Categories and Subject Descriptors D.1.6 [Programming Techniques]: Logic Programming General Terms Algorithms Keywords Answer set programming, goal-directed execution, coinduction

1.

Introduction

Answer Set Programming (ASP) is an elegant way of developing non-monotonic reasoning applications. ASP has gained wide acceptance, and considerable research has been done in developing the paradigm as well as its implementations and applications. ASP has been applied to important areas such as planning, scheduling, default reasoning, reasoning about actions [3], etc. Numerous implementations of ASP have been developed, ranging from DLV
c ACM 2012. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Proceedings of the 14th symposium on Principles and Practice of Declarative Programming (PPDP '12), pages 35-44, http: //dx.doi.org/10.1145/2370776.2370782.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PPDP'12, September 1921, 2012, Leuven, Belgium. Copyright c 2012 ACM 978-1-4503-1522-7/12/09. . . $10.00

([17]) and smodels [22] to SAT-based solvers such as cmodels [13] and the conflict-driven solver clasp [10]. However, these implementations compute the whole answer set: i.e., they are not goaldirected in the fashion of Prolog. Given an answer set program and a query goal Q, a goal-directed execution will systematically enumerate--via SLD style call expansions and backtracking--all answer sets that contain the propositions/predicates in Q. Other efforts have been made to realize goal-directed implementations (e.g., [5]), however, these approaches can handle only a limited class of programs and/or queries. In this paper we describe a goal-directed execution method that works for any answer set program as well as for any query. The method relies on coinductive logic programming (co-LP) [14]. CoLP can be regarded as providing an operational semantics, termed co-SLD resolution, for computing greatest fixed points (gfp) of logic programs. Co-SLD resolution systematically computes elements of the gfp of a program via backtracking [14, 27]. Additionally, calls are allowed to coinductively succeed if they unify with one of their ancestor calls [27]. To permit this, each call is stored in the coinductive hypothesis set (CHS) as the call is made. A more detailed introduction to co-LP and co-SLD resolution can be found in Appendix A. A goal-directed method for executing answer set programs is analogous to top-down, SLD style resolution for Prolog, while current popular methods for ASP are analogous to bottom-up methods that have been used for evaluating Prolog (and Datalog) programs [25]. A goal-directed execution method for answering queries for an answer set program has several advantages. The main advantage is that it paves the way to lifting the restriction to finitely groundable programs, and allows realization of ASP with full first-order predicates [20]. In the rest of the paper we develop a goal-directed strategy for executing answer set programs, and prove that it is sound and complete with respect to the method of Gelfond and Lifschitz. We restrict ourselves to only propositional (grounded) answer set programs in this paper; work is in progress to extend our goaldirected method to predicate answer set programs [20, 21]. Note that the design of a top-down goal-directed execution strategy for answer set programs has been regarded as quite a challenging problem [3]. As pointed out in [6], the difficulty in designing a goal-directed method for ASP comes about due to the absence of a relevance property in stable model semantics, on which answer set

programming is based [6, 23, 24]. We will introduce a modified relevance property that holds for our goal-directed method and guarantees that partial answer sets computed by our method can be extended to complete answer sets.

(p  not q  not not p). Such rules are termed ordinary rules. Rule P1.b is also an ordinary rule, since if used for expanding the call to q, it will lead to a recursive call to q through two negations. For simplicity of presentation, all non-cyclical rules will also be classified as ordinary rules. 2. Cyclical rules which when used to expand a call to subgoal G lead to a recursive call to G that is in the scope of an odd number of negations. Such recursive calls are known as odd loops over negation (OLONs). For example, given the program P2 below: p :- q, not p, r. . . . Rule P2.a a call to p using Rule P2.a will eventually lead to a call to not p. Under ordinary logic programming execution, this will lead to non-termination. Under ASP, however, the program consisting of Rule P2.a has {not p, not q, not r} as its answer set. For brevity, we refer to rules containing OLONs as OLON rules. Note that a rule can be both an ordinary rule and an OLON rule, since given a subgoal G, its expansion can lead to a recursive call to G through both even and odd numbers of negations along different expansion paths. For example in program P3 below, Rule P3.a is both an ordinary rule and an OLON rule. p :- q, not r. r :- not p. q :- t, not p. . . . Rule P3.a . . . Rule P3.b . . . Rule P3.c

2.

Answer Set Programming (ASP)

Answer Set Programming (ASP) [12] (A-Prolog [11] or AnsProlog [3]) is a declarative logic programming paradigm which encapsulates non-monotonic or common sense reasoning. The rules in an ASP program are of the form: p :- q1 , ..., qm , not r1 , ..., not rn . where m  0 and n  0. Each of p and qi (i  m) is a literal, each not rj (j  n) is a naf-literal (not is a logical connective called negation as failure (naf) or default negation). The semantics of an Answer Set program P is given via the Gelfond-Lifschitz method [3] in terms of the answer sets of the program ground(P), obtained by grounding the variables in the program P. Gelfond-Lifschitz Transform (GLT) Given a grounded Answer Set program P and a candidate answer set A, a residual program R is obtained by applying the following transformation rules: for all literals L  A, 1. Delete all rules in P which have not L in their body. 2. Delete all the remaining naf-literals (of the form not M) from the bodies of the remaining rules. The least fixed-point (say, F) of the residual program R is next computed. If F = A, then A is a stable model or an answer set of P. ASP can also have rules of the form: :- q1 , ..., qm , not r1 , ..., not rn . p :- q1 , ..., qm , not r1 , ..., not rn , not p. These rules capture the non-monotonic aspect of Answer Set Programming. Consider an example rule: p :- q, not p. Following the Gelfond-Lifschitz method (GL method) outlined above, this rule restricts q (and p) to not be in the answer set (unless p happens to be in the answer set via other rules, in which case due to presence of not p this rule will be removed while generating the residual program). Note that even though an answer set program can have other rules to establish that q is in the answer set, adding the rule above forces q to not be in the answer set unless p succeeds through another rule, thus making ASP non-monotonic.

Our top-down method requires that we properly identify and handle both ordinary and OLON rules. We will look at each type of rule in turn, followed by the steps taken to ensure that our method remains faithful to the GL method. 3.1 Ordinary Rules

Ordinary rules such as rules P1.a and P1.b in program P1 above exemplify the cyclical reasoning in ASP. The rules in the example force p and q to be mutually exclusive, i.e., either p is true or q is true, but not both. One can argue the reasoning presented in such rules is cyclical: If p is in the answer set, then q cannot be in the answer set, and if q is not in the answer set, then p must be in the answer set. Given a goal, G, and an answer set program comprised of only ordinary rules, G can be executed in a top-down manner using coinduction, through the following steps:
 Record each call in the CHS. The recorded calls constitute the

3.

Goal-directed ASP Issues

Any normal logic program can also be viewed as an answer set program. However, ASP adds complexity to a normal logic program in two ways. In addition to the standard Prolog rules, it allows: 1. Cyclical rules which when used to expand a call to a subgoal G lead to a recursive call to G through an even (but non-zero) number of negations. For example, given the program P1 below: p :- not q. q :- not p. . . . Rule P1.a . . . Rule P1.b

coinductive hypothesis set, which is the potential answer set.
 If at the time of the call, the call is already found in the CHS, it

succeeds coinductively and finishes.
 If the current call is not in the CHS, then expand it in the style

of ordinary SLD resolution (recording the call in the CHS prior to expansion).
 Simplify not not p to p, whenever possible, where p is a

proposition occurring in the program.
 If success is achieved with no goals left to expand, then the

Ordinary logic programming execution for the query ?- p. (or ?- q.) will lead to non-termination. However, ASP will produce two answer sets: {p, not q} and {q, not p} . Expanding the call p using Rule P1.a in the style of SLD resolution will lead to a recursive call to p that is in scope of two negations
Note that we will list all literals that are true in a given answer set. Conventionally, an answer set is specified by listing only the positive literals that are true; those not listed in the set are assumed to be false.

coinductive hypothesis set contains the (partial) answer set. The top-down resolution of query p with program P1 will proceed as follows. :- p :- not q CHS = {} (expand p by Rule P1.a) CHS = {p} (expand q by Rule P1.b)

:- not not p CHS = {p, not q} (simplify not not p  p ) :- p CHS = {p, not q} (coinductive success: p  CHS) :success: answer set is {p, not q} Note that the maintenance of the coinductive hypothesis set (CHS) is critical. If a call is encountered that is already in the CHS, it should not be expanded, it should simply (coinductively) succeed. Note that the query q will produce the other answer set {q, not p} in a symmetrical manner. Note also that the query not q will also produce the answer set {p, not q} as shown below. Thus, answers to negated queries can also be computed, if we apply the coinductive hypothesis rule to negated goals also, i.e., a call to not p succeeds, if an ancestor call to not p is present: :- not q :- not not p :- p :- not q :3.2 OLON Rules CHS = {} (expand q by Rule P1.b) CHS = {not q} (not not p  p) CHS = {p, not q} (expand p by Rule P1.a) CHS = {p, not q} (coinductive success for not q) success: answer set is {p, not q}

That is, if we encounter the goal g (resp. not g) during execution and not g  CHS (resp. g  CHS), then the computation fails and backtracking ensues. As another example, consider the program containing rule P4.1 (which has p in its head), but not rule P4.2, and the query :p. When execution starts, p will be added to the CHS and then expanded by rule P4.1; if the call to q fails, then the goal p also fails. Alternatively, if q succeeds due to other rules in the program, then upon arriving at the call not p, failure will ensue, since not p is inconsistent with the current CHS (which equals {p, q} prior to the call not p). Thus, OLON rules do not pose any problems in top-down execution based on coinduction, however, given an OLON rule with p as its head, if p can be inferred by other means (i.e., through ordinary rules) then the query p should succeed. Likewise, if q succeeds by other means and p does not, then we should report a failure (rather, report the absence of an answer set; note that given our conventions, CHS = {} denotes no answer set). We discuss how top-down execution of OLON rules is handled in Section 3.4. 3.3 Coinductive Success Under ASP

Our goal-directed procedure based on coinduction must also work with OLON rules. OLON rules are problematic because their influence on answer sets is indirect. Under ASP, rules of the form p :- q1 , q2 , ..., qk , not p. hold only for those (stable) models in which p succeeds through other rules in the program or at least one of the qi 's is false. Note that a headless rule of the form: :- q1 , q2 , ..., qn . is another manifestation of an OLON rule, as it is equivalent to the rule: p :- q1 , q2 , ..., qn , not p. where p is a literal that does not occur anywhere else in the program, in the sense that the stable models for the two rules are identical. Without loss of generality, consider the simpler rule: p :- q, not p. For an interpretation to be a (stable) model for this rule, either p must succeed through other rules in the program or q must be false. Two interesting cases arise: (i). p is true through other rules in the program. (ii) q is true through other rules in the program. For case (i), if p is true through other means in the program, then according to the Gelfond-Lifschitz method, it is in the answer set, and the OLON rule is taken out of consideration due to the occurrence of not p in its body. For case (ii), if q is true through other means and the rule is still in consideration due to p not being true through other rules in the program, then there are no answer sets, as q is both true and false. Thus, the answer set of the program P4 below is: {p, not q}. p :- q, not p. p. . . . Rule P4.1 . . . Rule P4.2

While our technique's use of co-SLD resolution has been outlined above, it requires some additional modification to be faithful to the Gelfond-Lifschitz method. Using normal coinductive success, our method will compute the gfp of the residual program after the GL transform, while the GL method computes the lfp. Consider Program P6 below: p :- q. q :- p. . . . Rule P6.1 . . . Rule P6.2

while there is no answer set for program P5 below: p :- q, not p. q. . . . Rule P5.1 . . . Rule P5.2

Our method based on coinduction will succeed for queries :p and :- q producing the answer set {p, q} while under the GL method, the answer set for this program is {not p, not q}. Our top-down method based on coinduction really computes the gfp of the original program. The GL method computes a fixed point of the original program (via the GL transformation and then computation of the lfp of the residual program) that is in between the gfp and the lfp of the original program. In the GL method, direct cyclical reasoning is not allowed, however, cyclical reasoning that goes through at least one negated literal is allowed. Thus, under the GL method, the answer set of program P6 does not contain a single positive literal, while there are two answer sets for the program P1 given earlier, each with exactly one positive literal, even though both programs P1 and P6 have only cyclical rules. Our top-down method can be modified so that it produces answer sets consistent with the GL method: a coinductive recursive call can succeed only if it is in the scope of at least one negation. In other words, the path from a successful coinductive call to its ancestor call must include a call to not. This restriction disallows inferring p from rules such as p :- p. With this operational restriction in place, the CHS will never contain a positive literal that is in the gfp of the residual program obtained after the GLT, but not in its lfp. To show this, let us assume that, for some ASP program, a call to p will always encounter at least one recursive call to p with no intervening negation. In such a case, p will never be part of any answer set:
 Under our goal-directed method, any call to p will fail when a

recursive call is encountered with no intervening negation.
 Under the GL method, p will never be in the lfp of the residual.

Given an OLON rule with p as its head and the query p, execution based on co-SLD resolution will fail, if we require that the coinductive hypothesis set (CHS) remains consistent at all times.

Even if a rule for p is present in the residual and all other dependencies are satisfied, the rule will still depend on the recursive call to p.

3.4

NMR Consistency Check

To summarize, the workings of our goal-directed strategy are as follows: given a goal G, perform co-SLD resolution while restricting coinductive success as outlined in Section 3.3. The CHS serves as the potential answer set. A successful answer will be computed only through ordinary rules, as all OLON rules will lead to failure due to the fact that not h will be encountered with proposition h present in the CHS while expanding with an OLON rule whose head is h. Once success is achieved, the answer set is the CHS. As discussed later, this answer set may be partial. The answer set produced by the process above is only a potential answer set. Once a candidate answer set has been generated by coSLD resolution as outlined above, the set has to be checked to see that it will not be rejected by an OLON rule. Suppose there are n OLON rules in the program of the form: qi :- Bi . where 1  i  n and each Bi is a conjunction of goals. Each Bi must contain a direct or indirect call to the respective qi which is in the scope of odd number of negations in order for qi :- Bi . to qualify as an OLON rule. If a candidate answer set contains qj (1  j  n), then each OLON rule whose head matches qj must be taken out of consideration (this is because Bj leads to not(qj ) which will be false for this candidate answer set). For all the other OLON rules whose head proposition qk (1  j  n) is not in the candidate answer set, their bodies must evaluate to false w.r.t. the candidate answer set, i.e., for each such rule, Bk must evaluate to false w.r.t. the candidate answer set. The above restrictions can be restated as follows: a candidate answer set must satisfy the formula qi  not Bi (1  i  n) for each OLON rule qi :- Bi . (1  i  n) in order to be reported as the final answer set. Thus, for each OLON rule, the check chk qi :- qi . chk qi :- not Bi . is constructed by our method. Furthermore, not Bi will be expanded to produce a chk qi clause for each literal in Bi . For example, if Bi represented the conjunction of literals s, not r, t in the above example, the check created would be: chk qi :- qi . chk qi :- not s. chk qi :- r. chk qi :- not t. A candidate answer set must satisfy each of these checks in order to be reported as a solution. This is enforced by rolling the checks into a single call, termed nmr chk: nmr chk :- chk q1 , chk q2 , ...chk qn . Now each query Q is transformed to Q, nmr chk. before it is posed to our goal-directed system. One can think of Q as the generator of candidate answer sets and nmr chk as the filter. If nmr chk fails, then backtracking will take place and Q will produce another candidate answer set, and so on. Backtracking can also take place within Q itself when a call to p (resp. not p) is encountered and not p (resp. p) is present in the CHS. Note that the CHS must be a part of the execution state, and be restored upon backtracking.

That is, given a proposition H's definition (Bi 's are conjunction of literals): H :- B1 . H :- B2 . ... H :- Bn . we add the dual rule not H :- not B1 , not B2 , ..., not Bn . If a proposition q appears in the body of a rule but not in any of the rule heads, then the fact not q. is added. Note that adding the dual rules is not necessary; it only makes the exposition of our goal-directed method easier to present and understand. 4.2 Goal-directed Method for Computing Answer Sets

Given a propositional query :- Q and a propositional answer set program P, the goal-directed procedure works as described below. Note that the execution state is a pair (G, S), where G is the current goal list, and S the current CHS. 1. Identify the set of ordinary rules and OLON rules in the program. 2. Assert a chk qi rule for every OLON rule with qi as its head and build the nmr check as described in Section 3.4. 3. For each ordinary rule and chk qi rule, construct its dual version. 4. Append the nmr check to the query. 5. Set the initial execution state to: (:- G1 , ..., Gn , {}). 6. Non-deterministically reduce the execution state using the following rules: (a) Call Expansion: (:- G1 , .., Gi , .., Gn , S)  (:- G1 , .., B1 , .., Bm , .., Gn , S  {Gi }) where Gi matches the rule Gi :- B1 , ..., Bm . in P, Gi  / S and not Gi  / S. (b) Coinductive Success: (:- G1 , .., Gi-1 , Gi , Gi+1 , .., Gn , S)  (:- G1 , .., Gi-1 , Gi+1 , .., Gn , S) if Gi  S and either: i. Gi is not a recursive call or ii. Gi is a recursive call in the scope of a non-zero number of intervening negations. (c) Inductive Success: (:- G1 , .., Gi-1 , Gi , Gi+1 , .., Gn , S)  (:- G1 , .., Gi-1 , Gi+1 , .., Gn , S  {Gi }) if Gi matches a fact. (d) Coinductive Failure: (:- G1 , .., Gi , .., Gn , S)  (fail, S) if either: i. not Gi  S or ii. Gi  S and Gi is a recursive call without any intervening negations. (e) Inductive Failure: (:- G1 , .., Gi , .., Gn , S)  (fail, S) if Gi has no matching rule in P. (f) Print Answer: (:- true, S)  success: S is the answer set where `:- true'  empty goal list

4.

Goal-directed Execution of Answer Set Programs

We next describe our general goal-directed procedure for computing answer sets. 4.1 Dual Rules

For simplicity, we add one more step to the process. Similarly to Alferes et al [1], for each rule in the program, we introduce its dual.

Note that when all the goals in the query are exhausted, execution of nmr chk begins. Upon failure, backtracking ensues, the state is restored and another rule tried. Note that negated calls are expanded using dual rules as in [1], so it is not necessary to check whether the number of intervening negations between a recursive call and its ancestor is even or odd. (See the call expansion rule above). A detailed example of goal-directed execution can be found in Appendix B. Next we discuss a few important issues: Identifying OLON and Ordinary Rules Given a propositional answer set program P, OLON rules and ordinary rules can be identified by constructing and traversing the call graph. The complexity of this traversal is O(|P|  n), where n is the number of propositional symbols occurring in the head of clauses in P and |P| is a measure of the program size. Note also that during the execution of a query Q, we need not make a distinction between ordinary and OLON rules; knowledge of OLON rules is needed only for creating the nmr chk. Partial Answer Set Our top-down procedure might not generate the entire answer set. It may generate only the part of the answer set that is needed to evaluate the query. Consider program P7: p q r s ::::not not not not q. p. s. r.

The relevance property is desirable because it would ensure that a partial answer set computed using only relevant rules for each literal could be extended into a complete answer set. However, stable model semantics do not satisfy the definition as given. This is because OLON rules can alter the meaning of a program and the truth values of individual literals without occurring in the set of relevant rules [6, 23]. For instance, an irrelevant rule of the form p :- not p. when added to an answer set program P, where P has one or more stable models and p does not occur in P, results in a program that has no stable models. Approaches such as [23] have addressed the lack of a relevance property by modifying stable model semantics to restore relevance. However, our implementation can be viewed as restoring relevance by expanding the definition of relevant rules to include all OLON rules in a program. Because the NMR check processes every OLON rule, it has the effect of making the truth value of every literal in a program dependent on such rules. That is, nmr rel rul(P, L) = rel rul(P, L)  O, O = { R: R is an OLON rule in P } (2)

Using nmr rel rul(P,L) in place of rel rul(P,L), a modified version of equation 1 above holds for our semantics: SEM (P )(L) = SEM (nmr rel rule(P, L))(L) (3)

As a result, any partial model returned by our semantics is guaranteed to be a subset of one or more complete models. 5.2 Soundness

Under goal-directed execution, the query :- q. for program P7 will produce only {q, not p} as the answer since the rules defining r and s are completely independent of rules for p and q. One could argue that this is an advantage of a goal-directed execution strategy rather than a disadvantage, as only the relevant part of the program will be explored. In contrast, if the query is :- q, s, then the right answer {q, not p, s, not r} will be produced by the goal-directed execution method. Thus, the part of the answer set that gets computed depends on the query. Correct maintenance of the CHS throughout the execution is important as it ensures that only consistent and correct answer sets are produced.

Theorem 1. For the non-empty set X returned by successful topdown execution of some program P, the set of positive literals in X will be an answer set of R, the set of rules of P used during topdown execution. Proof. Let us assume that top-down execution of a program P has succeeded for some query Q consisting of a set of literals in P, returning a non-empty set of literals X. We can observe that R  nmr rel rul(P, L): for each positive literal in Q, one
LQ

5.

Soundness and Correctness of the Goal-directed Method

We will now show the correctness of our goal-directed execution method by showing it to be sound and complete with respect to the GL method. First, we will examine the modified relevance property which holds for our method. 5.1 Relevance

As we stated in the introduction, one of the primary problems with developing a goal-directed ASP implementation is the lack of a relevance property in stable model semantics. Dix introduces relevance by stating that, "given any semantics SEM and a program P, it is perfectly reasonable that the truth-value of a literal L, with respect to SEM(P), only depends on the subprogram formed from the relevant rules of P with respect to L" [6]. He formalizes this using the dependency-graph of P, first establishing that
 "dependencies of (X ) := {A : X depends on A}, and  rel rul(P,X) is the set of relevant rules of P with respect to X,

i.e. the set of rules that contain an A  dependencies of (X ) in their heads" [6] and noting that the dependencies and relevant rules of 촞 are the same as those of X [6]. He then defines relevance as, for all literals L: SEM (P )(L) = SEM (rel rul(P, L))(L) (1)

rule with the literal in its head will need to succeed, for each negative literal in Q all rules with the the positive form of the literal in their head will need to fail, and the resulting set must satisfy the NMR check. We will show that X is a valid answer set of R using the GL method. First, because X may contain negative literals and the residual program produced by the GL method is a positive one, let us remove any rules in R containing the positive version of such literals as a goal, and then remove the negated literals from X to obtain X'. Because our algorithm allows negative literals to succeed if and only if all rules for the positive form fail or no such rules exist, only rules which failed during execution will be removed by this step. Next, let us apply the GL transformation using X' as the candidate answer set to obtain the residual program R'. This will remove rules containing the negation of any literal in X' and remove any negated goals from the remaining rules. We know that X' will be an answer set of R if and only if X' = LFP(R'). Now let us examine the properties of R'. As positive literals, we know that each literal in X' must occur as the head of a rule in R which succeeded during execution. Because such rules would have failed if the negation of any goal was present in the CHS, we know that such rules would not have been eliminated from the residual program by the GL transformation, and are thus still present in R' save for the removal of any negated goals. Because any rules containing the negation of a literal in X had to fail during execution, at least one goal in each of these rules must have failed, resulting in the negation of the goal being added to the CHS. Furthermore, because the NMR check applies the negation of each

OLON rule, again the negation of some goal in each such rule must have been added to the CHS. Thus any rule which failed during execution and yet was included in R will have been removed from R'. Finally, because our algorithm allows coinductive success to occur only in the scope of at least one negation, the removal of negated goals from the residual program will ensure that R' contains no loops. Because the remaining rules in R' must have succeeded during execution, their goals must have been added to the CHS, and therefore those goals consisting of positive literals form X'. Thus R' is a positive program with no loops, and each literal in X' must appear as the head of some rule in R' which is either a fact or whose goals consist only of other elements in X'. Therefore the least fixed point of R' must be equal to X', and X' must be an answer set of R. Theorem 2. Our top-down execution algorithm is sound with respect to the GL method. That is, for the non-empty set of literals X returned by successful execution of some program P, the set of positive literals in X is a subset of one or more answer sets of P. Proof. As shown above, the positive literals in the set returned by successful execution of P will be an answer set of R  nmr rel rul(P, L). Because R will always contain all OLON
LQ

Table 1. N-Queens Problem; Times in Seconds Problem Galliwasp clasp cmodels smodels queens-12 0.033 0.019 0.055 0.112 queens-13 0.034 0.022 0.071 0.132 queens-14 0.076 0.029 0.098 0.362 queens-15 0.071 0.034 0.119 0.592 queens-16 0.293 0.043 0.138 1.356 queens-17 0.198 0.049 0.176 4.293 queens-18 1.239 0.059 0.224 8.653 queens-19 0.148 0.070 0.272 3.288 queens-20 6.744 0.084 0.316 47.782 queens-21 0.420 0.104 0.398 95.710 queens-22 69.224 0.112 0.472 N/A queens-23 1.282 0.132 0.582 N/A queens-24 19.916 0.152 0.602 N/A

rules in P, no unused rules in P are capable of affecting the truth values of the literals in X. Thus the modified definition of relevance holds for all literals in X under our semantics and the partial answer set returned by our algorithm is guaranteed to be extensible to a complete one. Thus our algorithm for top-down execution is sound with respect to the GL method. 5.3 Completeness

Table 2. MxN-Pigeons Problem (No Solution for M>N) Problem Galliwasp clasp cmodels smodels pigeon-10x10 0.020 0.009 0.020 0.025 pigeon-20x20 0.050 0.048 0.163 0.517 pigeon-30x30 0.132 0.178 0.691 4.985 pigeon-8x7 0.123 0.072 0.089 0.535 pigeon-9x8 0.888 0.528 0.569 4.713 pigeon-10x9 8.339 4.590 2.417 46.208 pigeon-11x10 90.082 40.182 102.694 N/A

Theorem 3. Our top-down execution algorithm is complete with respect to the GL method. That is, for a program P, any answer set valid under the GL method will succeed if used as a query for top-down execution. In addition, the set returned by successful execution will contain no additional positive literals. Proof. Let X be a valid answer set of P obtained via the GL method. Then there exists a resultant program P' obtained by removing those rules in P containing the negation of any literal in X and removing any additional negated literals from the goals of the remaining rules. Furthermore, because X is a valid answer set of P, X = LFP(P'). This tells us that for every literal L  X there is a rule in P' with L as its head, which is either a fact or whose goals consist only of other literals in X. Let us assume that X is posed as a query for top-down execution of P. As we know that each L  X has a rule in P' with L as its head and whose positive goals are other literals in X, we know that such a rule also exists in P, with the possible addition of negated literals as goals. However, we know that these negated literals must succeed, that is, all rules with the positive form of such literals in their heads must fail, either by calling the negation of some literal in the answer set or by calling their heads recursively without an intervening negation. Were this not the case, these rules would remain in P', their heads would be included in LFP(P') and X would not be a valid answer set of P. Therefore, a combination of rules may be found such that each literal in X appears as the head of at least one rule which will succeed under top-down execution, and whose positive goals are all other literals in X. Furthermore, because each literal in the query must be satisfied and added to the CHS, and any rule with a goal whose negation is present in the CHS will fail, such a combination of rules will eventually be executed by our algorithm. Because such rules would also be present in P', we know that they cannot add additional positive literals to the CHS, as these would be part of LFP(P'), again rendering X invalid.

Table 3. MxN-Coloring problem (No Solution for M=3) Problem Galliwasp clasp cmodels smodels mapclr-4x20 0.018 0.006 0.011 0.013 mapclr-4x25 0.021 0.007 0.014 0.016 mapclr-4x29 0.023 0.008 0.016 0.018 mapclr-4x30 0.026 0.008 0.016 0.019 mapclr-3x20 0.022 0.005 0.009 0.008 mapclr-3x25 0.065 0.006 0.011 0.010 mapclr-3x29 0.394 0.006 0.012 0.011 mapclr-3x30 0.342 0.007 0.012 0.011 This leaves the NMR check, which ensures the set returned by our algorithm satisfies all OLON rules in P. However, we know this is the case, as the subset of positive literals in the CHS is equal to X. Because X is a valid answer set of P, there cannot be any rule in P which renders X invalid, and thus the NMR check must be satisfiable by a set of literals containing X. We also know that the NMR check will not add additional positive literals to the CHS, as any rules able to succeed would be present in P' and thus present in LFP(P'). Therefore any valid answer set X of a program P must succeed if posed as a query for top-down execution of P. Thus our top-down algorithm is complete with respect to the GL method.

6.

Performance Results

The goal-directed method described in this paper has been implemented in our system Galliwasp. In addition to the goal-directed method presented here, Galliwasp incorporates various other techniques to improve performance, including incremental enforcement of the NMR check [19]. Tables 1, 2 and 3 give performance results for some example programs. For the purpose of comparison, results for clasp, cmodels and smodels are also given. The Galliwasp system consists of two programs, a compiler and an interpreter. The times given are for our interpreter using

a compiled program and for the other solvers reading a program grounded by lparse. Neither compilation nor grounding times are factored into the results. A timeout of 600 seconds was enforced, with the instances which timed out listed as N/A in the tables. As these results demonstrate, our goal-directed method is practical and can be efficiently implemented. While additional performance increases are possible, the Galliwasp interpreter is already significantly faster than smodels in almost every case and comparable to clasp and cmodels in most cases.

implementation in a top-down fashion [1, 23, 24]. However, their approach is to modify stable model semantics so that the property of relevance is restored [23]. For this modified semantics, goaldirected procedures have been designed [24]. In contrast, our goal is to stay faithful to stable model semantics and answer set programming.

8.

Conclusions

7.

Discussion and Related Work

There are many advantages of top-down goal-directed execution of answer set programs, the main one being that it paves the way to answer set programming with predicates. The first step is to extend our method to datalog answer set programs, i.e., programs that allow only constants and variables as arguments in the predicates they contain [20]. Another advantage of goal-directed execution is that answer set programming can be made to work more naturally with other extensions that have been developed within logic programming, such as constraint programming, abduction, parallelism, probabilistic reasoning, etc. This leads to more sophisticated applications. Timed planning, i.e., planning in the presence of real-time constraints, is one such example [2]. With respect to related work, a top-down, goal-directed execution strategy for ASP has been the aim of many researchers in the past. Descriptions of some of these efforts can be found in [1, 4, 5, 8, 9, 15, 23, 24, 26]. The strategy presented in this paper is based on one presented by several of this paper's authors in previous work [14, 21]. However, the strategy presented in those works was limited to call-consistent or order-consistent programs. While the possibility of expansion to arbitrary ASP programs was mentioned, it was not expanded upon, and the proofs of soundness and completeness covered only the restricted cases [21]. A query-driven procedure for computing answer sets via an abductive proof procedure has been explored [7, 16]: a consistency check via integrity constraints is done before a negated literal is added to the answer set. However, "this procedure is not always sound with respect to the above abductive semantics of NAF" [16]. Alferes et al [1] have worked in a similar direction, though this is done in the context of abduction and again goal-directedness of ASP is not the main focus. Gebser and Schaub have developed a tableau based method which can be regarded as a step in this direction, however, the motivation for their work is completely different [9]. Bonatti, Pontelli and Tran [5] have proposed credulous resolution, an extension of earlier work of Bonatti [4], that extends SLD resolution for ASP. However, they place restrictions on the type of programs allowed and the type of queries allowed. Their method can be regarded as allowing coinductive success to be inferred only for negated goals. Thus, given query :- p and program P1, the execution will look as follows: p  not q  not not p  not q  success. Compared to our method, their method performs extra work. For example, if rule P1.1 is changed to p :big goal, not q, then big goal will be executed twice. The main problem in their method is that since it does not take coinduction for positive goals into account, knowing when to succeed inductively and when to succeed coinductively is undecidable. For this reason, their method works correctly only for a limited class of answer set programs (for example, answers to negated queries such ?-not p cannot be computed in a top-down manner). In contrast, our goal-directed method works correctly for all types of answer set programs and all types of queries. Pereira's group has done significant work on defining semantics for normal logic programs and implementing them, including

The main contribution of our paper is to present a practical, topdown method for goal-directed execution of Answer Set programs along with proofs of soundness and completeness. Our method stays faithful to ASP, and works for arbitrary answer set programs as well as arbitrary queries. Other methods in the literature either change the semantics, or work for only restricted programs or queries. Our method achieves this by relying on the coinductive logic programming paradigm. Details of our method were presented, along with proofs of soundness and correctness, and some preliminary performance results. A goal-directed procedure has many advantages, the main one being that execution of answer set programs does not have to be restricted to only finitely groundable ones. Our work thus paves the way for developing execution procedures for ASP over predicates. A goal-directed strategy permits an easier integration with other extensions of logic programming, which in turn makes it possible to develop more interesting applications of ASP and non-monotonic reasoning. Our current work is focused on refining our implementation to improve efficiency and add support for features such as constraints and predicates.

9.

Acknowledgments

Thanks to Michael Gelfond, Vladimir Lifschitz, Enrico Pontelli and Feliks Klu zniak for discussions and feedback.

References
[1] J. J. Alferes, L. M. Pereira, and T. Swift. Abduction in Well-Founded Semantics and Generalized Stable Models via Tabled Dual Programs. Theory and Practice of Logic Programming, 4:383428, July 2004. [2] A. Bansal. Towards Next Generation Logic Programming Systems. PhD thesis, University of Texas at Dallas, 2007. [3] C. Baral. Knowledge Representation, Reasoning and Declarative Problem Solving. Cambridge University Press, 2003. [4] P. Bonatti. Resolution for Skeptical Stable Model Semantics. Journal of Automated Reasoning, 27:391421, November 2001. [5] P. A. Bonatti, E. Pontelli, and T. C. Son. Credulous Resolution for Answer Set Programming. In Proceedings of the 23rd national conference on Artificial Intelligence - Volume 1, AAAI'08, pages 418 423. AAAI Press, 2008. [6] J. Dix. A Classification Theory of Semantics of Normal Logic Programs: II. Weak Properties. Fundamenta Informaticae, 22:257288, 1995. [7] K. Eshghi and R. A. Kowalski. Abduction Compared with Negation by Failure. In ICLP, ICLP'89, pages 234254, 1989. [8] J. Fern andez and J. Lobo. A Proof Procedure for Stable Theories. In CS-TR-3034, Computer Science Technical Report Series. University of Maryland, 1993. [9] M. Gebser and T. Schaub. Tableau Calculi for Answer Set Programming. In Proceedings of the 22nd international conference on Logic Programming, ICLP'06, pages 1125. Springer-Verlag, 2006. [10] M. Gebser, B. Kaufmann, A. Neumann, and T. Schaub. Clasp: A Conflict-Driven Answer Set Solver. In Proceedings of the 9th international conference on Logic Programming and Nonmonotonic Reasoning, LPNMR'07, pages 260265. Springer-Verlag, 2007. [11] M. Gelfond. Representing Knowledge in A-Prolog. In Computational Logic: Logic Programming and Beyond, Essays in Honour of Robert A. Kowalski, Part II, pages 413451. Springer-Verlag, 2002.

[12] M. Gelfond and V. Lifschitz. The Stable Model Semantics for Logic Programming. In Proceedings of the Fifth international conference on Logic Programming, pages 10701080. MIT Press, 1988. [13] E. Giunchiglia, Y. Lierler, and M. Maratea. SAT-Based Answer Set Programming. In Proceedings of the 19th national conference on Artifical Intelligence, AAAI'04, pages 6166. AAAI Press, 2004. [14] G. Gupta, A. Bansal, R. Min, L. Simon, and A. Mallya. Coinductive Logic Programming and Its Applications. In Proceedings of the 23rd international conference on Logic Programming, ICLP'07, pages 27 44. Springer-Verlag, 2007. [15] A. Kakas and F. Toni. Computing Argumentation in Logic Programming. Journal of Logic and Computation, 9(4):515562, 1999. [16] A. C. Kakas, R. A. Kowalski, and F. Toni. Abductive Logic Programming. Journal of Logic and Computation, 2(6):719770, 1992. [17] N. Leone, G. Pfeifer, and W. Faber. DLV. http://www.dbai. tuwien.ac.at/proj/dlv. [18] J. Lloyd. Foundations of Logic Programming. Symbolic Computation: Artificial Intelligence. Springer-Verlag, 1987. [19] K. Marple and G. Gupta. Galliwasp: A Goal-Directed Answer Set Solver. In Proceedings of the 22nd international symposium on Logicbased Program Synthesis and Transformation, LOPSTR '12, pages 8599. Katholieke Universiteit Leuven, 2012. [20] R. Min. Predicate Answer Set Programming with Coinduction. PhD thesis, University of Texas at Dallas, 2010. [21] R. Min, A. Bansal, and G. Gupta. Towards Predicate Answer Set Programming via Coinductive Logic Programming. In AIAI, pages 499508. Springer, 2009. [22] I. Niemel a and P. Simons. Smodels - An Implementation of the Stable Model and Well-Founded Semantics for Normal Logic Programs. In Logic Programming And Nonmonotonic Reasoning, volume 1265 of Lecture Notes in Computer Science, pages 420429. Springer-Verlag, 1997. [23] L. Pereira and A. Pinto. Revised Stable Models - A Semantics for Logic Programs. In Progress in Artificial Intelligence, volume 3808 of Lecture Notes in Computer Science, pages 2942. Springer-Verlag, 2005. [24] L. Pereira and A. Pinto. Layered Models Top-Down Querying of Normal Logic Programs. In Practical Aspects of Declarative Languages, volume 5418 of Lecture Notes in Computer Science, pages 254268. Springer-Verlag, 2009. [25] K. Sagonas, T. Swift, and D. Warren. XSB as an Efficient Deductive Database Engine. ACM SIGMOD Record, 23(2):442453, 1994. [26] Y. Shen, J. You, and L. Yuan. Enhancing Global SLS-Resolution with Loop Cutting and Tabling Mechanisms. Theoretical Computer Science, 328(3):271287, 2004. [27] L. Simon. Extending Logic Programming with Coinduction. PhD thesis, University of Texas at Dallas, 2006.

In the program R2, the meaning of the query ?- stream(X) is semantically null under standard logic programming. In the coLP paradigm the declarative semantics of the predicate stream/1 above is given in terms of infinitary Herbrand (or co-Herbrand) universe, infinitary Herbrand (or co-Herbrand) base [18], and maximal models (computed using greatest fixed-points) [27]. The operational semantics under coinduction is as follows [27]: a pred) succeeds if it unifies with one of its ancestor calls. icate call p(t Thus, every time a call is made, it has to be remembered. This set of ancestor calls constitutes the coinductive hypothesis set (CHS). Under co-LP, infinite rational answers can be computed, and infinite rational terms are allowed as arguments of predicates. Infinite terms are represented as solutions to unification equations and the occurs check is omitted during the unification process: for example, X = [1 | X] represents the binding of X to an infinite list of 1's. Thus, in co-SLD resolution, given a single clause p([ 1 | X ]) :- p(X). The query ?- p(A) will succeed in two resolution steps with the answer A = [1 | A], which is a finite representation of the infinite answer A = [1, 1, 1, ....]. Under coinductive interpretation of R2, the query ?- stream(X) produces all infinite sized streams as answers, e.g., X = [1 | X], X = [1, 2 | X ], etc. Thus, the semantics of R2 is not null, but proofs may be of infinite length. If we take a coinductive interpretation of program R1, then we get all finite and infinite streams as answers to the query ?- stream(X).

B.

Detailed Execution Example

We now present a larger, more complex example of execution using our goal-directed method. Consider program A1: p q r q ::::not not not not q. r. p. p. ... ... ... ... Rule A1.1 Rule A1.2 Rule A1.3 Rule A1.4

Rules A1.1, A1.2 and A1.3 are OLON rules, as calls to propositions p, q, and r in the heads of these rules lead to recursive calls to p, q and r respectively that are in the scope of odd numbers of negations. A1.1 is also an ordinary rule, since in conjunction with rule A1.4, a call to p resolved via rule A1.1 will lead to a call to p in rule A1.4 that is in the scope of an even number of negations. Thus, the nmr check rule can be defined as: nmr check :- not chk p, not chk q, not chk r. chk p :- not p, not q. chk q :- not q, not r. chk r :- not r, not p. The duals of the above rules are as follows: not not not not not not p :- q. ... q :- r, p. ... r :- p. ... chk p :- p; q. . . . chk q :- q; r. . . . chk r :- r; p. . . . Rule A1.6 Rule A1.7 Rule A1.8 Rule A1.9 Rule A1.10 Rule A1.11

A.

Co-SLD Resolution

As mentioned in the introduction, our goal-directed method relies on coinductive logic programming (co-LP) [14]. Co-SLD resolution, the operational semantics of coinduction, is briefly described below. The semantics is limited to regular proofs, i.e., those cases where the infinite behavior is obtained by infinite repetition of a finite number of finite behaviors. Consider the logic programming definition of a stream (list) of numbers as in program R1 below: stream([]). stream([H|T]) :- number(H), stream(T). Under SLD resolution, the query ?- stream(X) will systematically produce all finite streams one by one starting from the [] stream. Suppose now we remove the base case and obtain the program R2: stream([H|T]) :- number(H), stream(T).

Negated calls are resolved using these dual rules. Now the query q will be extended to q, nmr chk and executed as follows: ::::chk. CHS = {}; Rule A1.2 CHS = {q}; Rule A1.8 nmr chk. chk. CHS = {q, not r}; Rule A1.1 nmr chk. CHS = {q, not r} fail: backtrack to step 1 :- q, nmr chk. CHS = {}; Rule A1.4 :- not p, nmr chk. CHS = {q, not p}; Rule A1.6 q, nmr not r, p, nmr not q,

:- q, nmr chk. ::-

:-

:-

::-

:-

CHS = {q, not p} coinductive success nmr chk. CHS = {q, not p} execution of q finished not chk p, not chk q, not chk r. CHS = {q, not p} nmr chk rule (p ; q), not chk q, not chk r. CHS = {q, not p} not p is in CHS q, not chk q, not chk r. CHS = {q, not p} coinductive success for q not chk q, not chk r. CHS = {q, not p}; Rule A1.10 (q ; r), not chk r. CHS = {q, not p} coinductive success for q not chk r. CHS = {q, not p}; Rule A1.11 CHS = {q, not p, r}; Rule A1.3 CHS = {q, not p, r} coinductive success for not p success. answer set is {q, not p, r}

:- r ; p. :- not p ; p. :.

University of Texas at Dallas

Coinductive Logic Programming and its Applications
Gopal Gupta Luke Simon, Ajay Bansal, Ajay Mallya, Richard Min.
Applied Logic, Programming-Languages and Systems (ALPS) Lab The University of Texas at Dallas, Richardson, Texas, USA
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 1

University of Texas at Dallas

Circular Phenomena in Comp. Sci.
 Circularity has dogged Mathematics and Computer Science ever since Set Theory was first developed:
 The well known Russell's Paradox:
 R = { x | x is a set that does not contain itself} Is R contained in R? Yes and No

 All these paradoxes involve self-reference through some type of negation  Russell put the blame squarely on circularity and sought to ban it from scientific discourse:

 Liar Paradox: I am a liar  Hypergame paradox (Zwicker & Smullyan)

``Whatever involves all of the collection must not be one of the collection" -- Russell 1908
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 2

University of Texas at Dallas

Circularity in Computer Science
 Following Russell's lead, Tarski proposed to ban selfreferential sentences in a language  Rather, have a hierarchy of languages  All this changed with Kripke's paper in 1975 who showed that circular phenomenon are far more common and circularity can't simply be banned.  Circularity has been banned from automated theorem proving and logic programming through the occurs check rule:  What if we allowed such unification to proceed (as LP systems always did for efficiency reasons)?
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 3

An unbound variable cannot be unified with a term containing that variable

University of Texas at Dallas

Circularity in Computer Science
 If occurs check is removed, we'll generate circular (infinite) structures:  Such structures, of course, arise in computing (circular linked lists), but banned in logic/LP.  Subsequent LP systems did allow for such circular structures (rational terms), but they only exist as data-structures, there is no proof theory to go along with it.
 One can hold the data-structure in memory within an LP execution, but one can't reason about it.  X = [1,2,3 | X]

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 4

University of Texas at Dallas

Circularity in Everyday Life
 Circularity arises in every day life
 Most natural phenomenon are cyclical
 Cyclical movement of the earth, moon, etc.  Our digestive system works in cycles

 Social interactions are cyclical:
 Conversation = (1st speaker, (2nd Speaker, Conversation)  Shared conventions are cyclical concepts

 Numerous other examples can be found elsewhere (Barwise & Moss 1996)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 5

University of Texas at Dallas

Circularity in Computer Science
 Circular phenomenon are quite common in Computer Science:
       Circular linked lists Graphs (with cycles) Controllers (run forever) Bisimilarity Interactive systems Automata over infinite strings/Kripke structures Perpetual processes

 Logic/LP not equipped to model circularity
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 6

University of Texas at Dallas

Coinduction
 Circular structures are infinite structures
X = [1, 2 | X] is logically speaking X = [1, 2, 1, 2, ....]

 Proofs about their properties are infinite-sized  Coinduction is the technique for proving these properties
 first proposed by Peter Aczel in the 80s

 Systematic presentation of coinduction & its application to computing, math. and set theory: "Vicious Circles" by Moss and Barwise (1996)  Our focus: inclusion of coinductive reasoning techniques into LP and theorem proving
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 7

University of Texas at Dallas

Induction vs Coinduction
 Induction is a mathematical technique for finitely reasoning about an infinite (countable) no. of things.  Examples of inductive structures:
 Naturals: 0, 1, 2, ...  Lists: [ ], [X], [X, X], [X, X, X], ...

 3 components of an inductive definition:
(1) Initiality, (2) iteration, (3) minimality  for example, the set of lists is specified as follows: [ ]  an empty list is a list (initiality initiality) initiality [H | T] is a list if T is a list and H is an element (iteration iteration) iteration nothing else is a list (minimality minimality) minimality
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 8

University of Texas at Dallas

Induction vs Coinduction

 

Coinduction is a mathematical technique for (finitely) reasoning about infinite things.





(1) iteration, (2) maximality  for example, for a list: [ H | T ] is a list if T is a list and H is an element (iteration iteration). iteration Maximal set that satisfies the specification of a list.  This coinductive interpretation specifies all infinite sized lists
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 9

2 components of a coinductive definition:

Mathematical dual of induction If all things were finite, then coinduction would not be needed. Perpetual programs, automata over infinite strings

University of Texas at Dallas

Example: Natural Numbers
  (S) = { 0 }  { succ(x) | x  S }  N = 
 where  is least fixed-point.

 aka "inductive definition"
 Let N be the smallest set such that
 0N  x  N implies x + 1  N

 Induction corresponds to Least Fix Point (LFP) interpretation.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 10

University of Texas at Dallas

Example: Natural Numbers and Infinity
  (S) = { 0 }  { succ(x) | x  S }   unambiguously defines another set  N' =  = N  {  }

 Coinduction corresponds to Greatest Fixed Point (GFP) interpretation.

  = succ( succ( succ( ... ) ) ) = succ(  ) =  + 1  where  is a greatest fixed-point

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 11

University of Texas at Dallas

Mathematical Foundations
 Duality provides a source of new mathematical tools that reflect the sophistication of tried and true techniques. Definition
Least fixed point Greatest fixed point

Proof
Induction Coinduction

Mapping
Recursion Corecursion

 Co-recursion: recursive def'n without a base case
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 12

University of Texas at Dallas

Applications of Coinduction
       model checking bisimilarity proofs lazy evaluation in FP reasoning with infinite structures perpetual processes cyclic structures operational semantics of "coinductive logic programming"  Type inference systems for lazy functional languages
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 13

University of Texas at Dallas

Inductive Logic Programming
 Logic Programming
 is actually inductive logic programming.  has inductive definition.  useful for writing programs for reasoning about finite things: - data structures - properties

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 14

University of Texas at Dallas

Infinite Objects and Properties
 Traditional logic programming is unable to reason about infinite objects and/or properties.  (The glass is only half-full)  Example: perpetual binary streams
 traditional logic programming cannot handle bit(0). bit(1). bitstream( [ H | T ] ) :- bit( H ), bitstream( T ). |?- X = [ 0, 1, 1, 0 | X ], bitstream( X ).

 Goal: Combine traditional LP with coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 15

University of Texas at Dallas

Overview of Coinductive LP
 Coinductive Logic Program is
a definite program with maximal co-Herbrand model declarative semantics.

 Declarative Semantics: across the board dual of traditional LP:
    greatest fixed-points terms: co-Herbrand universe Uco(P) atoms: co-Herbrand base Bco(P) program semantics: maximal co-Herbrand model Mco(P).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 16

University of Texas at Dallas

Coinductive LP: An Example
 Let P1 be the following coinductive program. from(x) = x cons from(x+1) :- coinductive from/2. from( N, [ N | T ] ) :- from( s(N), T ). |?- from( 0, X ).  co-Herbrand Universe: Uco(P1) = N    L where N=[0, s(0), s(s(0)), ... ], ={ s(s(s( . . . ) ) ) }, and L is the the set of all finite and infinite lists of elements in N,  and L.  co-Herbrand Model: Mco(P1)={ from(t, [t, s(t), s(s(t)), ... ]) | t  Uco(P1) }  from(0, [0, s(0), s(s(0)), ... ])  Mco(P1) implies the query holds  Without "coinductive" declaration of from, Mco(P1')= This corresponds to traditional semantics of LP with infinite trees.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 17

University of Texas at Dallas

Operational Semantics: co-SLD
 nondeterministic state transition system  states are pairs of
 a finite list of syntactic atoms [resolvent] (as in Prolog)  a set of syntactic term equations of the form x = f(x) or x = t
 For a program p :- p. => the query |?- p. will succeed.  p( [ 1 | T ] ) :- p( T ). => |?- p(X) to succeed with X= [ 1 | X ].

 transition rules
 definite clause rule  "coinductive hypothesis rule"
 if a coinductive goal Q is called, and Q unifies with a call made earlier (e.g., P :- Q) then Q succeeds.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 18

University of Texas at Dallas

Correctness
 Theorem (soundness). If atom A has a successful co-SLD derivation in program P, then E(A) is true in program P, where E is the resulting variable bindings for the derivation.  Theorem (completeness). If A  Mco(P) has a rational proof, then A has a successful coSLD derivation in program P.
 Completeness only for rational/regular proofs
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 19

University of Texas at Dallas

Implementation
 Search strategy: hypothesis-first, leftmost, depth-first  Meta-Interpreter implementation.
query(Goal) :- solve([],Goal). solve(Hypothesis, (Goal1,Goal2)) :solve( Hypothesis, Goal1), solve(Hypothesis,Goal2). solve( _ , Atom) :- builtin(Atom), Atom. solve(Hypothesis,Atom):- member(Atom, Hypothesis). solve(Hypothesis,Atom):- notbuiltin(Atom), clause(Atom,Atoms), solve([Atom|Hypothesis],Atoms).

 A more efficient implem. atop YAP also available
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 20

University of Texas at Dallas

Example: Number Stream
:- coinductive stream/1. stream( [ H | T ] ) :- num( H ), stream( T ). num( 0 ). num( s( N ) ) :- num( N ). |?- stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ).
1. 2. 3.

Answers: T = [ 0, s(0), s(s(0)), s(s(0)) | T ] T = [ 0, s(0), s(s(0)), s(0), s(s(0)) | T ] T = [ 0, s(0), s(s(0)) | T ] . . . T = [ 0, s(0), s(s(0)) | X ] (where X is any rational list of numbers.)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 21

MEMO: stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( s ( 0 ) ) | T ] )

University of Texas at Dallas

Example: Append
:- coinductive append/3. append( [ ], X, X ). append( [ H | T ], Y, [ H | Z ] ) :- append( T, Y, Z ).
|?- Y = [ 4, 5, 6 | Y ], append( [ 1, 2, 3 ], Y, Z). Answer: Z = [ 1, 2, 3 | Y ], Y=[ 4, 5, 6 | Y] |?- X = [ 1, 2, 3 | X ], Y = [ 3, 4 | Y ], append( X, Y, Z). Answer: Z = [ 1, 2, 3 | Z ]. |?- Z = [ 1, 2 | Z ], append( X, Y, Z ). Answer: X = [ ], Y = [ 1, 2 | Z ] ; X = [1, 2 | X], Y = _ X = [ 1 ], Y = [ 2 | Z ] ; X = [ 1, 2 ], Y = Z; .... ad infinitum
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 22

University of Texas at Dallas

Example: Comember
member(H, [ H | T ]). member(H, [ X | T ]) :- member(H, T). ?- L = [1,2 | L], member(3, L) succeeds. Instead: :- coinductive comember/2. %drop/3 is inductive comember(X, L) :- drop(X, L, R), comember(X, R). drop(H, [ H | T ], T). drop(H, [ X | T ], T1) :- drop(H, T, T1). ?- X=[ 1, 2, 3 | X ], comember(2,X). Answer: yes. ?- X=[ 1, 2, 3, 1, 2, 3], comember(2, X). Answer: no. ?- X=[1, 2, 3 | X], comember(Y, X). Answer: Y = 1; Y = 2; Y = 3; ?- X = [1,2 | X], comember(3, X). Answer: no

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 23

University of Texas at Dallas

Example: Sieve of Eratosthenes
 Lazy evaluation can be elegantly incorporated in LP :- coinductive sieve/2, filter/3, comember/2. primes(X) :- generate_infinite_list(I),sieve(I,L),comember(X,L). sieve([H|T],[H,R]) :- filter(H,T,F),sieve(F,R). filter(H,[ ],[ ]). filter(H,[K | T],[K | T1]):- R is K mod H, R>0,filter(H,T,T1). filter(H,[K | T],T1) :- 0 is K mod H, filter(H,T,T1). :-coinductive int/2 int(X,[X | Y]) :- X1 is X+1, int(X1,Y). generate_infinite_list(I) :- int(2,I).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 24

University of Texas at Dallas

Co-Logic Programming
 combines both halves of logic programming:  syntactically identical to traditional logic programming, except predicates are labeled:  and stratification restriction enforced where:
 Inductive, or  coinductive  traditional logic programming  coinductive logic programming

 Implementation on top of YAP available.

 inductive and coinductive predicates cannot be mutually recursive. e.g., p :- q. q :- p. Program rejected, if p coinductive & q inductive
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 25

University of Texas at Dallas

Application: Model Checking
 automated verification of hardware and software systems  -automata
 accept infinite strings  accepting state must be traversed infinitely often

 requires computation of lfp and gfp  co-logic programming provides an elegant framework for model checking  traditional LP works for safety property (that is based on lfp) in an elegant manner, but not for liveness .

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 26

University of Texas at Dallas

Verification of Properties

 Types of properties: safety and liveness  Search for counter-example

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 27

University of Texas at Dallas

Safety versus Liveness
 Safety
 "nothing bad will happen"  naturally described inductively  straightforward encoding in traditional LP

 liveness
    "something good will eventually happen" dual of safety naturally described coinductively straightforward encoding in coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 28

University of Texas at Dallas

Finite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). automata([ ], St) :- final(St). trans(s0, a, s1). trans(s3, d, s0). trans(s1, b, s2). trans(s2, 3, s0). trans(s2, c, s3). final(s2).

?- automata(X,s0). X=[ a, b]; X=[ a, b, e, a, b]; X=[ a, b, e, a, b, e, a, b];

...... ...... ......

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 29

University of Texas at Dallas

Infinite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). trans(s0,a,s1). trans(s3,d,s0). trans(s1,b,s2). trans(s2,3,s0). trans(s2,c,s3). final(s2).

?- automata(X,s0). X=[ a, b, c, d | X ]; X=[ a, b, e | X ];

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 30

University of Texas at Dallas

Verifying Liveness Properties
 Verifying safety properties in LP is relatively easy: safety modeled by reachability  Accomplished via tabled logic programming  Verifying liveness is much harder: a counterexample to liveness is an infinite trace  Verifying liveness is transformed into a safety check via use of negations in model checking and tabled LP
 Considerable overhead incurred

 Co-LP solves the problem more elegantly:
 Infinite traces that serve as counter-examples are easily produced as answers
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 31

University of Texas at Dallas

Verifying Liveness Properties
 Consider Safety:
 Question: Is an unsafe state, Su, reachable (safe)?  If answer is yes, the path to Su is the counter-ex.

 Consider Liveness, then dually
 Question: Is a state, D, that should be dead, live?  If answer is yes, the infinite path containing D is the counter example
 Co-LP will produce this infinite path as the answer

 Checking for liveness is just as easy as checking for safety
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 32

University of Texas at Dallas

Counter

sm1(N,[sm1|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. s0(N,[s0|T]) :- N1 is N+1 mod 4, s1(N1,T), N1>=0. s1(N,[s1|T]) :- N1 is N+1 mod 4, s2(N1,T), N1>=0. s2(N,[s2|T]) :- N1 is N+1 mod 4, s3(N1,T), N1>=0. s3(N,[s3|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. ?- sm1(-1,X),comember(sm1,X). No. (because sm1 does not occur in X infinitely often).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 33

University of Texas at Dallas

Nested Finite and Infinite Automata
:- coinductive state/2. state(s0, [s0,s1 | T]):- enter, work, state(s1,T). state(s1, [s1 | T]):- exit, state(s2,T). state(s2, [s2 | T]):- repeat, state(s0,T). state(s0, [s0 | T]):- error, state(s3,T). state(s3, [s3 | T]):- repeat, state(s0,T). work. enter. repeat. exit. error. work :- work. |?- state(s0,X), absent(s2,X). X=[ s0, s3 | X ]
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 34

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

Timed Automata  -automata w/ time constrained transitions & stopwatches  straightforward encoding into CLP(R) + Co-LP

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 35

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
:- use_module(library(clpr)). :- coinductive driver/9.

train(X, up, X, T1,T2,T2). % up=idle train(s0,approach,s1,T1,T2,T3) :- {T3=T1}. train(s1,in,s2,T1,T2,T3):-{T1-T2>2,T3=T2} train(s2,out,s3,T1,T2,T3). train(s3,exit,s0,T1,T2,T3):-{T3=T2,T1-T2<5}. train(X,lower,X,T1,T2,T2). train(X,down,X,T1,T2,T2). train(X,raise,X,T1,T2,T2).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 36

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
contr(s0,approach,s1,T1,T2,T1).

contr(s1,lower,s2,T1,T2,T3):- {T3=T2, T1-T2=1}.

contr(s2,exit,s3,T1,T2,T1). contr(s3,raise,s0,T1,T2,T2):-{T1-T2<1}. contr(X,in,X,T1,T2,T2). contr(X,up,X,T1,T2,T2). contr(X,out,X,T1,T2,T2). contr(X,down,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 37

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

gate(s0,lower,s1,T1,T2,T3):- {T3=T1}. gate(s1,down,s2,T1,T2,T3):- {T3=T2,T1-T2<1}. gate(s2,raise,s3,T1,T2,T3):- {T3=T1}.
gate(s3,up,s0,T1,T2,T3):- {T3=T2,T1-T2>1,T1-T2<2 }.

gate(X,approach,X,T1,T2,T2). gate(X,in,X,T1,T2,T2). gate(X,out,X,T1,T2,T2). gate(X,exit,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 38

University of Texas at Dallas

Verification of Real-Time Systems
:- coinductive driver/9. driver(S0,S1,S2, T,T0,T1,T2, [ X | Rest ], [ (X,T) | R ]) :train(S0,X,S00,T,T0,T00), contr(S1,X,S10,T,T1,T10), gate(S2,X,S20,T,T2,T20), {TA > T}, driver(S00,S10,S20,TA,T00,T10,T20,Rest,R). |?- driver(s0,s0,s0,T,Ta,Tb,Tc,X,R). R=[(approach,A), (lower,B), (down,C), (in,D), (out,E), (exit,F), (raise,G), (up,H) | R ], X=[approach, lower, down, in, out, exit, raise, up | X] ; R=[(approach,A),(lower,B),(down,C),(in,D),(out,E),(exit,F),(raise,G), (approach,H),(up,I)|R], X=[approach,lower,down,in,out,exit,raise,approach,up | X] ;
% where A, B, C, ... H, I are the corresponding wall clock time of events generated.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 39

University of Texas at Dallas

Goal-directed execution of ASP
      Answer set programming (ASP) is a popular formalism for non monotonic reasoning Applications in real-world reasoning, planning, etc. Semantics given via lfp of a residual program obtained after "Gelfond-Lifschitz" transform Popular implementations: Smodels, DLV, etc.
1. No goal-directed execution strategy available 2. ASP limited to only finitely groundable programs

Co-logic programming solves both these problems. Also provides a goal-directed method to check if a proposition is true in some model of a prop. formula
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 40

University of Texas at Dallas

   

Most of the time, given a theory, we are interested in knowing if a particular goal is true or not. Top down goal-directed execution provides operational semantics (important for usability) Execution more efficient. Why check the consistency of the whole knowledgebase?
 Tabled LP vs bottom up Deductive Deductive Databases

Why Goal-directed ASP?

 

Most practical examples anyway add a constraint to force the answer set to contain a certain goal. Answer sets of non-finitely groundable programs computable & Constraints incorporated in Prolog style.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 41

 Inconsistency in some unrelated part will scuttle the whole system  E.g. Zebra puzzle: :- not satisfied.

University of Texas at Dallas

Negation in Co-LP
 Given a clause such as ?- p. fails coinductively when not p is encountered  To incorporate negation in coinductive reasoning, need a negative coinductive hypothesis rule:
p :- q, not p.

  Answer set programming makes the "glass completely full" by taking into account failing computations: 

 In the process of establishing not(p), if not(p) is seen again in the resolvent, then not(p) succeeds Also, not not p reduces to p.

 p :- q, not p. is consistent if p = false and q = false However, this takes away monotonicity: q can be constrainted to false, causing q to be withdrawn, if it was established earlier.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 42

University of Texas at Dallas

ASP
 Consider the following program, A:
p :- not q. t. r :- t, s. q :- not p. s. A has 2 answer sets: {p, r, t, s} & {q, r, t, s}.

 Now suppose we add the following rule to A:  Gelfond-Lifschitz Method:
h :- p, not h. (falsify p) Only one answer set remains: {q, r, t, s}

 Given an answer set S, for each p  S, delete all rules whose body contains "not p";  delete all goals of the form "not q" in remaining rules  Compute the least fix point, L, of the residual program  If S = L, then S is an answer set

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 43

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, A':
p :- not q. q :- not p, r. t. s.

 Separate into constraint and non-constraint rules: only 1 constraint rule in this case.  Execute the query under co-LP, candidate answer sets will be generated.  Keep the ones not rejected by the constraints.  Suppose the query is ?- q. Execution: q not p, r not not q, r q, r r t, s s success. Ans = {q, r, t, s}  Next, we need to check that constraint rules will not reject the generated answer set.
 (it doesn't in this case)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 44

r :- t, s. h :- p, not h.

University of Texas at Dallas

Goal-directed ASP
 In general, for the constraint rules of p as head, p1:- B1. p2:- B2. ... pn :- Bn., generate rule(s) of the form: chk_p1 :- not(p1), B1. chk_p2 :- not(p2), B2. ... chk_pn :- not(p), Bn.  Generate: nmr_chk :- not(chk_p1), ... , not(chk_pn).  For each pred. definition, generate its negative version: not_p :- not(B1), not(B2), ... , not(Bn).  If you want to ask query Q, then ask ?- Q, nmr_chk.  Execution keeps track of atoms in the answer set (PCHS) and atoms not in the answer set (NCHS).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 45

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, P1:
(i) p ::- not q. (ii) q:(iii) r :q:- not r. :- not p. P1 has 1 answer set: {q, r}. (iv) q ::- not p.

 Separate into: 3 constraint rules (i, ii, iii) 2 non-constraint rules (i, iv).

Suppose the query is ?- r. Expand as in co-LP: r not p not not q q( not r fail, backtrack) not p success. Ans={r, q} which satisfies the constraint rules of nmr_chk.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 46

p :- not(q). q :- not(r). r :- not(p). q :- not(p). chk_p :- not(p), not(q). chk_q :- not(q), not(r). chk_r :- not(r), not(p). nmr_chk :- not(chk_p), not(chk_q), not(chk_r). not_p :- q. not_q :- r, p. not_r :- p.

University of Texas at Dallas

Next Generation of LP System
 Lot of research in LP resulting in advances:
 CLP, Tabled LP, Parallelism, Andorra, ASP, now co-LP

 However, no "one stop shop" system  Dream: build this "one stop shop" system
ASP Or-Parallelism Tabled LP Next Generation Prolog System CLP Andorra

Rule selection

Goal selection
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 47

University of Texas at Dallas

Related Publications
1. L. Simon, A. Mallya, A. Bansal, and G. Gupta. Coinductive logic programming. In ICLP'06 . 2. L. Simon, A. Bansal, A. Mallya, and G. Gupta. CoLogic programming: Extending logic programming with coinduction. In ICALP'07. 3. ICLP'07 Proceedings (this tutorial) 4. A. Bansal, R. Min, G. Gupta. Goal-directed Execution of ASP. Internal Report, UT Dallas 5. R. Min, A. Bansal, G. Gupta. Goal-directed Execution of ASP with General Predicates. Forthcoming. 6. A. Bansal, R. Min, G. Gupta. Resolution Theorem Proving with Coinduction. Internal Report, UT Dallas
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 48

University of Texas at Dallas

Conclusion
 Circularity is a common concept in everyday life and computer science:  Logic/LP is unable to cope with circularity  Solution: introduce coinduction in Logic/LP
 dual of traditional logic programming  operational semantics for coinduction  combining both halves of logic programming

 applications to verification, non monotonic reasoning, negation in LP, web services, theorem proving, propositional satisfiability.  Acknowledgemt.: V. Santos Costa, R. Rocha, F. Silva
(for help with implementation of co-LP)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 49

University of Texas at Dallas

Coinductive Logic Programming and its Applications
Gopal Gupta Luke Simon, Ajay Bansal, Ajay Mallya, Richard Min.
Applied Logic, Programming-Languages and Systems (ALPS) Lab The University of Texas at Dallas, Richardson, Texas, USA
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 1

University of Texas at Dallas

Circular Phenomena in Comp. Sci.
 Circularity has dogged Mathematics and Computer Science ever since Set Theory was first developed:
 The well known Russell's Paradox:
 R = { x | x is a set that does not contain itself} Is R contained in R? Yes and No

 All these paradoxes involve self-reference through some type of negation  Russell put the blame squarely on circularity and sought to ban it from scientific discourse:

 Liar Paradox: I am a liar  Hypergame paradox (Zwicker & Smullyan)

``Whatever involves all of the collection must not be one of the collection" -- Russell 1908
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 2

University of Texas at Dallas

Circularity in Computer Science
 Following Russell's lead, Tarski proposed to ban selfreferential sentences in a language  Rather, have a hierarchy of languages  All this changed with Kripke's paper in 1975 who showed that circular phenomenon are far more common and circularity can't simply be banned.  Circularity has been banned from automated theorem proving and logic programming through the occurs check rule:  What if we allowed such unification to proceed (as LP systems always did for efficiency reasons)?
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 3

An unbound variable cannot be unified with a term containing that variable

University of Texas at Dallas

Circularity in Computer Science
 If occurs check is removed, we'll generate circular (infinite) structures:  Such structures, of course, arise in computing (circular linked lists), but banned in logic/LP.  Subsequent LP systems did allow for such circular structures (rational terms), but they only exist as data-structures, there is no proof theory to go along with it.
 One can hold the data-structure in memory within an LP execution, but one can't reason about it.  X = [1,2,3 | X]

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 4

University of Texas at Dallas

Circularity in Everyday Life
 Circularity arises in every day life
 Most natural phenomenon are cyclical
 Cyclical movement of the earth, moon, etc.  Our digestive system works in cycles

 Social interactions are cyclical:
 Conversation = (1st speaker, (2nd Speaker, Conversation)  Shared conventions are cyclical concepts

 Numerous other examples can be found elsewhere (Barwise & Moss 1996)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 5

University of Texas at Dallas

Circularity in Computer Science
 Circular phenomenon are quite common in Computer Science:
       Circular linked lists Graphs (with cycles) Controllers (run forever) Bisimilarity Interactive systems Automata over infinite strings/Kripke structures Perpetual processes

 Logic/LP not equipped to model circularity
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 6

University of Texas at Dallas

Coinduction
 Circular structures are infinite structures
X = [1, 2 | X] is logically speaking X = [1, 2, 1, 2, ....]

 Proofs about their properties are infinite-sized  Coinduction is the technique for proving these properties
 first proposed by Peter Aczel in the 80s

 Systematic presentation of coinduction & its application to computing, math. and set theory: "Vicious Circles" by Moss and Barwise (1996)  Our focus: inclusion of coinductive reasoning techniques into LP and theorem proving
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 7

University of Texas at Dallas

Induction vs Coinduction
 Induction is a mathematical technique for finitely reasoning about an infinite (countable) no. of things.  Examples of inductive structures:
 Naturals: 0, 1, 2, ...  Lists: [ ], [X], [X, X], [X, X, X], ...

 3 components of an inductive definition:
(1) Initiality, (2) iteration, (3) minimality  for example, the set of lists is specified as follows: [ ]  an empty list is a list (initiality initiality) initiality [H | T] is a list if T is a list and H is an element (iteration iteration) iteration nothing else is a list (minimality minimality) minimality
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 8

University of Texas at Dallas

Induction vs Coinduction

 

Coinduction is a mathematical technique for (finitely) reasoning about infinite things.





(1) iteration, (2) maximality  for example, for a list: [ H | T ] is a list if T is a list and H is an element (iteration iteration). iteration Maximal set that satisfies the specification of a list.  This coinductive interpretation specifies all infinite sized lists
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 9

2 components of a coinductive definition:

Mathematical dual of induction If all things were finite, then coinduction would not be needed. Perpetual programs, automata over infinite strings

University of Texas at Dallas

Example: Natural Numbers
  (S) = { 0 }  { succ(x) | x  S }  N = 
 where  is least fixed-point.

 aka "inductive definition"
 Let N be the smallest set such that
 0N  x  N implies x + 1  N

 Induction corresponds to Least Fix Point (LFP) interpretation.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 10

University of Texas at Dallas

Example: Natural Numbers and Infinity
  (S) = { 0 }  { succ(x) | x  S }   unambiguously defines another set  N' =  = N  {  }

 Coinduction corresponds to Greatest Fixed Point (GFP) interpretation.

  = succ( succ( succ( ... ) ) ) = succ(  ) =  + 1  where  is a greatest fixed-point

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 11

University of Texas at Dallas

Mathematical Foundations
 Duality provides a source of new mathematical tools that reflect the sophistication of tried and true techniques. Definition
Least fixed point Greatest fixed point

Proof
Induction Coinduction

Mapping
Recursion Corecursion

 Co-recursion: recursive def'n without a base case
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 12

University of Texas at Dallas

Applications of Coinduction
       model checking bisimilarity proofs lazy evaluation in FP reasoning with infinite structures perpetual processes cyclic structures operational semantics of "coinductive logic programming"  Type inference systems for lazy functional languages
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 13

University of Texas at Dallas

Inductive Logic Programming
 Logic Programming
 is actually inductive logic programming.  has inductive definition.  useful for writing programs for reasoning about finite things: - data structures - properties

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 14

University of Texas at Dallas

Infinite Objects and Properties
 Traditional logic programming is unable to reason about infinite objects and/or properties.  (The glass is only half-full)  Example: perpetual binary streams
 traditional logic programming cannot handle bit(0). bit(1). bitstream( [ H | T ] ) :- bit( H ), bitstream( T ). |?- X = [ 0, 1, 1, 0 | X ], bitstream( X ).

 Goal: Combine traditional LP with coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 15

University of Texas at Dallas

Overview of Coinductive LP
 Coinductive Logic Program is
a definite program with maximal co-Herbrand model declarative semantics.

 Declarative Semantics: across the board dual of traditional LP:
    greatest fixed-points terms: co-Herbrand universe Uco(P) atoms: co-Herbrand base Bco(P) program semantics: maximal co-Herbrand model Mco(P).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 16

University of Texas at Dallas

Coinductive LP: An Example
 Let P1 be the following coinductive program. from(x) = x cons from(x+1) :- coinductive from/2. from( N, [ N | T ] ) :- from( s(N), T ). |?- from( 0, X ).  co-Herbrand Universe: Uco(P1) = N    L where N=[0, s(0), s(s(0)), ... ], ={ s(s(s( . . . ) ) ) }, and L is the the set of all finite and infinite lists of elements in N,  and L.  co-Herbrand Model: Mco(P1)={ from(t, [t, s(t), s(s(t)), ... ]) | t  Uco(P1) }  from(0, [0, s(0), s(s(0)), ... ])  Mco(P1) implies the query holds  Without "coinductive" declaration of from, Mco(P1')= This corresponds to traditional semantics of LP with infinite trees.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 17

University of Texas at Dallas

Operational Semantics: co-SLD
 nondeterministic state transition system  states are pairs of
 a finite list of syntactic atoms [resolvent] (as in Prolog)  a set of syntactic term equations of the form x = f(x) or x = t
 For a program p :- p. => the query |?- p. will succeed.  p( [ 1 | T ] ) :- p( T ). => |?- p(X) to succeed with X= [ 1 | X ].

 transition rules
 definite clause rule  "coinductive hypothesis rule"
 if a coinductive goal Q is called, and Q unifies with a call made earlier (e.g., P :- Q) then Q succeeds.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 18

University of Texas at Dallas

Correctness
 Theorem (soundness). If atom A has a successful co-SLD derivation in program P, then E(A) is true in program P, where E is the resulting variable bindings for the derivation.  Theorem (completeness). If A  Mco(P) has a rational proof, then A has a successful coSLD derivation in program P.
 Completeness only for rational/regular proofs
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 19

University of Texas at Dallas

Implementation
 Search strategy: hypothesis-first, leftmost, depth-first  Meta-Interpreter implementation.
query(Goal) :- solve([],Goal). solve(Hypothesis, (Goal1,Goal2)) :solve( Hypothesis, Goal1), solve(Hypothesis,Goal2). solve( _ , Atom) :- builtin(Atom), Atom. solve(Hypothesis,Atom):- member(Atom, Hypothesis). solve(Hypothesis,Atom):- notbuiltin(Atom), clause(Atom,Atoms), solve([Atom|Hypothesis],Atoms).

 A more efficient implem. atop YAP also available
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 20

University of Texas at Dallas

Example: Number Stream
:- coinductive stream/1. stream( [ H | T ] ) :- num( H ), stream( T ). num( 0 ). num( s( N ) ) :- num( N ). |?- stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ).
1. 2. 3.

Answers: T = [ 0, s(0), s(s(0)), s(s(0)) | T ] T = [ 0, s(0), s(s(0)), s(0), s(s(0)) | T ] T = [ 0, s(0), s(s(0)) | T ] . . . T = [ 0, s(0), s(s(0)) | X ] (where X is any rational list of numbers.)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 21

MEMO: stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( s ( 0 ) ) | T ] )

University of Texas at Dallas

Example: Append
:- coinductive append/3. append( [ ], X, X ). append( [ H | T ], Y, [ H | Z ] ) :- append( T, Y, Z ).
|?- Y = [ 4, 5, 6 | Y ], append( [ 1, 2, 3 ], Y, Z). Answer: Z = [ 1, 2, 3 | Y ], Y=[ 4, 5, 6 | Y] |?- X = [ 1, 2, 3 | X ], Y = [ 3, 4 | Y ], append( X, Y, Z). Answer: Z = [ 1, 2, 3 | Z ]. |?- Z = [ 1, 2 | Z ], append( X, Y, Z ). Answer: X = [ ], Y = [ 1, 2 | Z ] ; X = [1, 2 | X], Y = _ X = [ 1 ], Y = [ 2 | Z ] ; X = [ 1, 2 ], Y = Z; .... ad infinitum
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 22

University of Texas at Dallas

Example: Comember
member(H, [ H | T ]). member(H, [ X | T ]) :- member(H, T). ?- L = [1,2 | L], member(3, L) succeeds. Instead: :- coinductive comember/2. %drop/3 is inductive comember(X, L) :- drop(X, L, R), comember(X, R). drop(H, [ H | T ], T). drop(H, [ X | T ], T1) :- drop(H, T, T1). ?- X=[ 1, 2, 3 | X ], comember(2,X). Answer: yes. ?- X=[ 1, 2, 3, 1, 2, 3], comember(2, X). Answer: no. ?- X=[1, 2, 3 | X], comember(Y, X). Answer: Y = 1; Y = 2; Y = 3; ?- X = [1,2 | X], comember(3, X). Answer: no

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 23

University of Texas at Dallas

Example: Sieve of Eratosthenes
 Lazy evaluation can be elegantly incorporated in LP :- coinductive sieve/2, filter/3, comember/2. primes(X) :- generate_infinite_list(I),sieve(I,L),comember(X,L). sieve([H|T],[H,R]) :- filter(H,T,F),sieve(F,R). filter(H,[ ],[ ]). filter(H,[K | T],[K | T1]):- R is K mod H, R>0,filter(H,T,T1). filter(H,[K | T],T1) :- 0 is K mod H, filter(H,T,T1). :-coinductive int/2 int(X,[X | Y]) :- X1 is X+1, int(X1,Y). generate_infinite_list(I) :- int(2,I).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 24

University of Texas at Dallas

Co-Logic Programming
 combines both halves of logic programming:  syntactically identical to traditional logic programming, except predicates are labeled:  and stratification restriction enforced where:
 Inductive, or  coinductive  traditional logic programming  coinductive logic programming

 Implementation on top of YAP available.

 inductive and coinductive predicates cannot be mutually recursive. e.g., p :- q. q :- p. Program rejected, if p coinductive & q inductive
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 25

University of Texas at Dallas

Application: Model Checking
 automated verification of hardware and software systems  -automata
 accept infinite strings  accepting state must be traversed infinitely often

 requires computation of lfp and gfp  co-logic programming provides an elegant framework for model checking  traditional LP works for safety property (that is based on lfp) in an elegant manner, but not for liveness .

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 26

University of Texas at Dallas

Verification of Properties

 Types of properties: safety and liveness  Search for counter-example

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 27

University of Texas at Dallas

Safety versus Liveness
 Safety
 "nothing bad will happen"  naturally described inductively  straightforward encoding in traditional LP

 liveness
    "something good will eventually happen" dual of safety naturally described coinductively straightforward encoding in coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 28

University of Texas at Dallas

Finite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). automata([ ], St) :- final(St). trans(s0, a, s1). trans(s3, d, s0). trans(s1, b, s2). trans(s2, 3, s0). trans(s2, c, s3). final(s2).

?- automata(X,s0). X=[ a, b]; X=[ a, b, e, a, b]; X=[ a, b, e, a, b, e, a, b];

...... ...... ......

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 29

University of Texas at Dallas

Infinite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). trans(s0,a,s1). trans(s3,d,s0). trans(s1,b,s2). trans(s2,3,s0). trans(s2,c,s3). final(s2).

?- automata(X,s0). X=[ a, b, c, d | X ]; X=[ a, b, e | X ];

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 30

University of Texas at Dallas

Verifying Liveness Properties
 Verifying safety properties in LP is relatively easy: safety modeled by reachability  Accomplished via tabled logic programming  Verifying liveness is much harder: a counterexample to liveness is an infinite trace  Verifying liveness is transformed into a safety check via use of negations in model checking and tabled LP
 Considerable overhead incurred

 Co-LP solves the problem more elegantly:
 Infinite traces that serve as counter-examples are easily produced as answers
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 31

University of Texas at Dallas

Verifying Liveness Properties
 Consider Safety:
 Question: Is an unsafe state, Su, reachable (safe)?  If answer is yes, the path to Su is the counter-ex.

 Consider Liveness, then dually
 Question: Is a state, D, that should be dead, live?  If answer is yes, the infinite path containing D is the counter example
 Co-LP will produce this infinite path as the answer

 Checking for liveness is just as easy as checking for safety
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 32

University of Texas at Dallas

Counter

sm1(N,[sm1|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. s0(N,[s0|T]) :- N1 is N+1 mod 4, s1(N1,T), N1>=0. s1(N,[s1|T]) :- N1 is N+1 mod 4, s2(N1,T), N1>=0. s2(N,[s2|T]) :- N1 is N+1 mod 4, s3(N1,T), N1>=0. s3(N,[s3|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. ?- sm1(-1,X),comember(sm1,X). No. (because sm1 does not occur in X infinitely often).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 33

University of Texas at Dallas

Nested Finite and Infinite Automata
:- coinductive state/2. state(s0, [s0,s1 | T]):- enter, work, state(s1,T). state(s1, [s1 | T]):- exit, state(s2,T). state(s2, [s2 | T]):- repeat, state(s0,T). state(s0, [s0 | T]):- error, state(s3,T). state(s3, [s3 | T]):- repeat, state(s0,T). work. enter. repeat. exit. error. work :- work. |?- state(s0,X), absent(s2,X). X=[ s0, s3 | X ]
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 34

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

Timed Automata  -automata w/ time constrained transitions & stopwatches  straightforward encoding into CLP(R) + Co-LP

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 35

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
:- use_module(library(clpr)). :- coinductive driver/9.

train(X, up, X, T1,T2,T2). % up=idle train(s0,approach,s1,T1,T2,T3) :- {T3=T1}. train(s1,in,s2,T1,T2,T3):-{T1-T2>2,T3=T2} train(s2,out,s3,T1,T2,T3). train(s3,exit,s0,T1,T2,T3):-{T3=T2,T1-T2<5}. train(X,lower,X,T1,T2,T2). train(X,down,X,T1,T2,T2). train(X,raise,X,T1,T2,T2).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 36

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
contr(s0,approach,s1,T1,T2,T1).

contr(s1,lower,s2,T1,T2,T3):- {T3=T2, T1-T2=1}.

contr(s2,exit,s3,T1,T2,T1). contr(s3,raise,s0,T1,T2,T2):-{T1-T2<1}. contr(X,in,X,T1,T2,T2). contr(X,up,X,T1,T2,T2). contr(X,out,X,T1,T2,T2). contr(X,down,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 37

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

gate(s0,lower,s1,T1,T2,T3):- {T3=T1}. gate(s1,down,s2,T1,T2,T3):- {T3=T2,T1-T2<1}. gate(s2,raise,s3,T1,T2,T3):- {T3=T1}.
gate(s3,up,s0,T1,T2,T3):- {T3=T2,T1-T2>1,T1-T2<2 }.

gate(X,approach,X,T1,T2,T2). gate(X,in,X,T1,T2,T2). gate(X,out,X,T1,T2,T2). gate(X,exit,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 38

University of Texas at Dallas

Verification of Real-Time Systems
:- coinductive driver/9. driver(S0,S1,S2, T,T0,T1,T2, [ X | Rest ], [ (X,T) | R ]) :train(S0,X,S00,T,T0,T00), contr(S1,X,S10,T,T1,T10), gate(S2,X,S20,T,T2,T20), {TA > T}, driver(S00,S10,S20,TA,T00,T10,T20,Rest,R). |?- driver(s0,s0,s0,T,Ta,Tb,Tc,X,R). R=[(approach,A), (lower,B), (down,C), (in,D), (out,E), (exit,F), (raise,G), (up,H) | R ], X=[approach, lower, down, in, out, exit, raise, up | X] ; R=[(approach,A),(lower,B),(down,C),(in,D),(out,E),(exit,F),(raise,G), (approach,H),(up,I)|R], X=[approach,lower,down,in,out,exit,raise,approach,up | X] ;
% where A, B, C, ... H, I are the corresponding wall clock time of events generated.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 39

University of Texas at Dallas

Goal-directed execution of ASP
      Answer set programming (ASP) is a popular formalism for non monotonic reasoning Applications in real-world reasoning, planning, etc. Semantics given via lfp of a residual program obtained after "Gelfond-Lifschitz" transform Popular implementations: Smodels, DLV, etc.
1. No goal-directed execution strategy available 2. ASP limited to only finitely groundable programs

Co-logic programming solves both these problems. Also provides a goal-directed method to check if a proposition is true in some model of a prop. formula
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 40

University of Texas at Dallas

   

Most of the time, given a theory, we are interested in knowing if a particular goal is true or not. Top down goal-directed execution provides operational semantics (important for usability) Execution more efficient. Why check the consistency of the whole knowledgebase?
 Tabled LP vs bottom up Deductive Deductive Databases

Why Goal-directed ASP?

 

Most practical examples anyway add a constraint to force the answer set to contain a certain goal. Answer sets of non-finitely groundable programs computable & Constraints incorporated in Prolog style.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 41

 Inconsistency in some unrelated part will scuttle the whole system  E.g. Zebra puzzle: :- not satisfied.

University of Texas at Dallas

Negation in Co-LP
 Given a clause such as ?- p. fails coinductively when not p is encountered  To incorporate negation in coinductive reasoning, need a negative coinductive hypothesis rule:
p :- q, not p.

  Answer set programming makes the "glass completely full" by taking into account failing computations: 

 In the process of establishing not(p), if not(p) is seen again in the resolvent, then not(p) succeeds Also, not not p reduces to p.

 p :- q, not p. is consistent if p = false and q = false However, this takes away monotonicity: q can be constrainted to false, causing q to be withdrawn, if it was established earlier.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 42

University of Texas at Dallas

ASP
 Consider the following program, A:
p :- not q. t. r :- t, s. q :- not p. s. A has 2 answer sets: {p, r, t, s} & {q, r, t, s}.

 Now suppose we add the following rule to A:  Gelfond-Lifschitz Method:
h :- p, not h. (falsify p) Only one answer set remains: {q, r, t, s}

 Given an answer set S, for each p  S, delete all rules whose body contains "not p";  delete all goals of the form "not q" in remaining rules  Compute the least fix point, L, of the residual program  If S = L, then S is an answer set

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 43

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, A':
p :- not q. q :- not p, r. t. s.

 Separate into constraint and non-constraint rules: only 1 constraint rule in this case.  Execute the query under co-LP, candidate answer sets will be generated.  Keep the ones not rejected by the constraints.  Suppose the query is ?- q. Execution: q not p, r not not q, r q, r r t, s s success. Ans = {q, r, t, s}  Next, we need to check that constraint rules will not reject the generated answer set.
 (it doesn't in this case)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 44

r :- t, s. h :- p, not h.

University of Texas at Dallas

Goal-directed ASP
 In general, for the constraint rules of p as head, p1:- B1. p2:- B2. ... pn :- Bn., generate rule(s) of the form: chk_p1 :- not(p1), B1. chk_p2 :- not(p2), B2. ... chk_pn :- not(p), Bn.  Generate: nmr_chk :- not(chk_p1), ... , not(chk_pn).  For each pred. definition, generate its negative version: not_p :- not(B1), not(B2), ... , not(Bn).  If you want to ask query Q, then ask ?- Q, nmr_chk.  Execution keeps track of atoms in the answer set (PCHS) and atoms not in the answer set (NCHS).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 45

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, P1:
(i) p ::- not q. (ii) q:(iii) r :q:- not r. :- not p. P1 has 1 answer set: {q, r}. (iv) q ::- not p.

 Separate into: 3 constraint rules (i, ii, iii) 2 non-constraint rules (i, iv).

Suppose the query is ?- r. Expand as in co-LP: r not p not not q q( not r fail, backtrack) not p success. Ans={r, q} which satisfies the constraint rules of nmr_chk.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 46

p :- not(q). q :- not(r). r :- not(p). q :- not(p). chk_p :- not(p), not(q). chk_q :- not(q), not(r). chk_r :- not(r), not(p). nmr_chk :- not(chk_p), not(chk_q), not(chk_r). not_p :- q. not_q :- r, p. not_r :- p.

University of Texas at Dallas

Next Generation of LP System
 Lot of research in LP resulting in advances:
 CLP, Tabled LP, Parallelism, Andorra, ASP, now co-LP

 However, no "one stop shop" system  Dream: build this "one stop shop" system
ASP Or-Parallelism Tabled LP Next Generation Prolog System CLP Andorra

Rule selection

Goal selection
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 47

University of Texas at Dallas

Related Publications
1. L. Simon, A. Mallya, A. Bansal, and G. Gupta. Coinductive logic programming. In ICLP'06 . 2. L. Simon, A. Bansal, A. Mallya, and G. Gupta. CoLogic programming: Extending logic programming with coinduction. In ICALP'07. 3. ICLP'07 Proceedings (this tutorial) 4. A. Bansal, R. Min, G. Gupta. Goal-directed Execution of ASP. Internal Report, UT Dallas 5. R. Min, A. Bansal, G. Gupta. Goal-directed Execution of ASP with General Predicates. Forthcoming. 6. A. Bansal, R. Min, G. Gupta. Resolution Theorem Proving with Coinduction. Internal Report, UT Dallas
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 48

University of Texas at Dallas

Conclusion
 Circularity is a common concept in everyday life and computer science:  Logic/LP is unable to cope with circularity  Solution: introduce coinduction in Logic/LP
 dual of traditional logic programming  operational semantics for coinduction  combining both halves of logic programming

 applications to verification, non monotonic reasoning, negation in LP, web services, theorem proving, propositional satisfiability.  Acknowledgemt.: V. Santos Costa, R. Rocha, F. Silva
(for help with implementation of co-LP)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 49

Reputation-based Web service selection for Composition
Srividya K Bansal Department of Engineering Arizona State University Mesa, Arizona 85212, USA Email: srividya.bansal@asu.edu Ajay Bansal Department of Computer Science Georgetown University Washington, DC 20057, USA Email: bansal@cs.georgetown.edu

Abstract--The success and acceptance of Web service composition depends on computing solutions comprised of trustworthy services. In this paper, we extend our Web service Composition framework to include selection and ranking of services based on their reputation score. With the increasing popularity of Web-based Social Networks like Linkedin, Facebook, and Twitter, there is great potential in determining the reputation score of a particular service provider using Social Network Analysis. We present a technique to calculate a reputation score per service using centrality measure of Social Networks. We use this score to produce composition solutions that consist of services provided by trust-worthy and reputed providers. Keywords-service composition; reputation; social networks;

rest of the network. So our rationale is that these central figures who play a fundamental role in the network are trusted by others in the network who are connected (directly or indirectly) to them. Our work investigates the following research issues: (i) compute the reputation score of composition solutions based on individual scores of service providers obtained using the centrality measure of social networks (ii) set a threshold for reputation that each and every Web service involved in the composition has to satisfy. Failure to meet the threshold will result in filtering out the Web service and it will not be used in any composition solution. II. C ENTRALITY M EASURE IN S OCIAL N ETWORKS : Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds. It involves measuring the formal and informal relationships to understand information/knowldege flow that binds the interacting units that could be a person, group, organization, or any knowledge entity. In order to understand social networks and their participants, the location of an actor in a network is evaluated. The network location is measured in terms of centrality of a node that gives an insight into the various roles and groupings in a network. Centrality gives a rough indication of the social power of a node based on how well they "connect" the network. The graph-theoretic conception of compactness has been extended to the study of Social Networks and simply renamed "graph centrality" [1]. Their measures are all based upon distances between points, and all define graphs as centralized to the degree that their points are all close together. Based on research on communication in Social Networks, the centrality of an entire network should index the tendency of a single point to be more central than all other points in the network. Measures of a graph centrality are based on differences between the centrality of the most central point and that of all others. Thus, they are indexes of the centralization of the network [4]. The three most popular individual centrality measures are Degree Centrality, Betweenness Centrality, and Closeness Centrality. Degree Centrality: The network activity of a node can be measured using the concept of degrees, i.e., the number

I. I NTRODUCTION The next milestone in the evolution of the World Wide Web is making services ubiquitously available. We need infrastructure that applications can use to automatically discover, deploy, compose, and synthesize services. Along with the functional attributes there is a need to consider non-functional attributes (Quality of Service parameters) of Web services in the process of building composite Web services. The current challenge in automatic composition of Web services also includes finding a composite Web service that can be trusted by consumers before using it. In this paper, we present our approach that uses analysis of Social Networks to calculate a reputation score for each service involved in the composition and further prune results based on this score. Web-based Social Networks have become increasingly popular these days. Social Network Analysis is the process of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups, organizations, computers, or any knowledge entity. We propose to measure the reputation of a service by measuring the centrality of a service provider and/or a service provider organization in a well-known Social Network. We adopt our idea of computing a reputation score using centrality measure based on the notion of centrality and prestige being key in the study of social networks [1], [2]. The role of central people (nodes with high centrality) in a network seems to be fundamental as they adopt the innovation and help in transportation and diffusion of information throughout the

ServiceProvider Provider A Provider B Provider C Provider D Provider E Provider F Provider G

Degree 2 3 1 8 3 4 5

ServiceProvider Provider H Provider I Provider J Provider K Provider L Provider M

Degree 3 1 2 4 1 1

n

CD (sk ) =
i=0

a(si , sk )

where a(si , sk ) = 1 iff si and sk are connected 0 otherwise As such it is a straightforward index of the extent to which sk is a focus of activity. CD (sk ) is large if service provider sk is adjacent to, or in direct contact with, a large number of other service providers, and small if sk tends to be cut off from such direct contact. CD (sk ) = 0 for a service provider that is totally isolated from any other point. Our algorithm filters out any services whose provider has a zero degree centrality in a social network, i.e., such services will not be used in building composition solutions. Reputation of the entire composite service is computed as an average of the individual reputation score of the services involved in the composition. We also need to set a reputation threshold and any service with a reputation score that is below this threshold is not used while generating composition solutions. In our initial prototype implementation we set the reputation threshold to zero, i.e., degree centrality of the service provider in the network is zero. A service provider or service provider organization that is not connected to any other nodes in the Social network is not known to anyone else and is an immediate reason to be pruned out from composition solutions as the service cannot be trusted. Composition solutions can be ranked such that solutions with highest reputation score appear on top of the list. IV. C ONCLUSIONS AND F UTURE W ORK In this paper, we presented our approach to compute reputation of services and use this score to select services for composition. A reputation score is computed for every service in the repository based on degree centrality of the service provider in a well-known Web-based Social Network. Our future work includes exploring other measures of centrality such as betweenness centrality and closeness centrality and analyzing the possibility of using a combination of all three measures of centrality to compute reputation of a service and/or provider. R EFERENCES
[1] L. C. Freeman, Centrality in Social Networks Conceptual Clarification, in Social Networks, Vol. 1, No. 3. (1979), pp. 215-239. [2] S. Wasserman, and K. Faust, Social Network Analysis: Methods and Applications, Cambridge: Cambridge Univ. Press, 1994. [3] S. Kona, A. Bansal, M. Blake, and G. Gupta, Generalized Semantics-based Service Composition in Proceedings of IEEE Intl. Conference on Web Services (ICWS), September 2008. [4] H. J. Leavitt, Some effects of communication patterns on group performance in Journal of Abnormal and Social Psychology, pp. 46:38-50, 1951.

Table I D EGREE C ENTRALITY OF N ODES IN F IGURE 1

Figure 1.

A Social Network of Web service Providers

of direct connections a node has. In the example network shown in figure 1 and table I, Provider D has the most direct connections in the network, making it the most active node in the network. In personal Social Networks, the common thought is that "the more connections, the better". Betweenness Centrality: Though Provider D has many direct ties, Provider H has fewer direct connections (close to the average in the network). Yet, in many ways, Provider H has one of the best locations in the network by playing the role of a "broker" between two important components. Closeness Centrality: Provider F and G have fewer connections than Provider D, yet the pattern of their direct and indirect ties allow them to access all the nodes in the network more quickly than anyone else. They have the shortest paths to all other and hence are in an excellent position to have the best visibility into what is happening in the network. Individual network centralities provide insight into the individual's location in the network. The relationship between the centralities of all nodes can reveal much about the overall network structure. III. R EPUTATION - BASED W EB SERVICE SELECTION FOR
COMPOSITION

We extend our previous work on Web service composition [3] (that uses both functional and non-functional parameters to compute composition solutions) by using reputation to filter services. The reputation score of each service in a Web service repository is computed as a measure of the degree centrality (CD ) of the social network to which the service provider belongs. It is calculated as the degree or count of the number of adjacencies for a node, sk :

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Generalized Semantics-based Service Composition
Srividya Kona and Gopal Gupta
Department of Computer Science, The University of Texas at Dallas Richardson, TX 75083

Abstract. Web services and Service-oriented computing is being widely used and accepted. In order to effectively reuse existing services, we need to automatically compose Web services. The automatic composition techniques need to be semantics-based rather than syntax-based, since only semantics can accurately capture the task a service performs. In this paper we present general semanticsbased techniques for automatic service composition for Web services. Our general method takes as input (i) a repository, R, of semantic descriptions of Web services, (ii) a semantic description, Q, of the service that is desired, and produces a composite service consisting of services taken from R that realizes Q. The composite service may combine services linearly as a chain or as a directed acyclic graph, where in the most general case the combination may be conditional. The composite service generated by our automatic composition method can be coded as an OWL-S document. We present details of our initial implementation along with performance results.

1 Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered and consumed. A Web service is an autonomous, platformindependent program accessible over the web that may effect some action or change in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. As automation increases, these services will be accessed directly by applications rather than by humans [7]. In this context, a Web service can be regarded as a "programmatic interface" that makes application to application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order to make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize and compose services automatically. To make services ubiquitously available we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, composition, deployment, and synthesis [1]. Several efforts are underway to build such an infrastructure [1013]. Service Composition involves effectively combining and reusing independently developed component services. A composite service is a collection of services combined together in some way to achieve a desired effect. Traditionally, the task of automatic

service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [20]. Most efforts reported in the literature focus on one or more of these four phases. The first phase involves generating a plan, i.e., all the services and the order in which they are to be composed inorder to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions based on non-functional properties like QoS properties. The last phase involves executing the services as per the plan and in case any of them are not available, an alternate solution has to be used. Note that one must make clear distinction between automatic service composition and manual service composition. Services can be composed manually and the manually specified composition encoded as a program/document in a language such as OWL-S [2] or BPEL4WS [11]. Automatic service composition can be thought of as generating this OWL-S or BPEL4WS description automatically. Thus, languages such as OWL-S and BPEL4WS do not solve the automatic service composition problem--they merely provide notations for specifying service composition. In this paper we present a general approach for automatic service composition. Our composition algorithm performs planning, discovery, and selection automatically, all at once, in one single process. This is in contrast to most methods in the literature where one of the phases (most frequently planning) is performed manually. Additionally, our method generates most general compositions based on (conditional) directed acyclic graphs. In our method, after a solution is obtained, the semantic description of the new composite service is generated using the composition language OWL-S [2]. This description document can be registered in the repository and is thus available for future searches. The composite service can now be discovered as a direct match instead of having to look through the entire repository and build the composition solution again. Our research makes the following novel contributions: (i) We present a generalized composition algorithm based on generating conditional directed acyclic graphs; (ii) we present an efficient and scalable algorithm for solving the composition problem that takes semantics of services into account; our algorithm automatically discovers and selects the individual services involved in composition for a given query, without the need for manual intervention; (iii) we automatically generate OWL-S descriptions of the new composite service obtained; and, (iv) we present a prototype implementation based on constraint logic programming that works efficiently on large repositories. The rest of the paper is organized as follows. In section 2 we present the related work in the area of service composition and discuss their limitations. In section 3, we present the service composition problem, the different kinds of composition with examples, and our technique for automatic composition. In section 4, we discuss automatic generation of OWL-S service descriptions. Section 5 presents our prototype implementation and performance results. The last section presents the conclusions and future work.

2 Related Work
Composition of Web services has been active area of research recently [19, 20]. Most of these approaches present techniques to solve one or more phases listed in section 1.

There are many approaches [1416] that solve the first two phases of composition namely planning and discovery. These are based on capturing the formal semantics of the service using action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available. To the best of our knowledge, most of these approaches that use planning are restricted to sequential compositions, rather than a directed acyclic graph. In this paper we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential but also nonsequential that can be represented in the form of a directed acyclic graph. The authors in [14] present a composition technique by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also relies on a user-defined plan template which is created manually. One of the main objective of our work is to come up with a technique that can automatically produce the composition without the need for any manual intervention. There are industry solutions based on WSDL and BPEL4WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service by composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when an explicit flow is provided. In contrast, our technique automatically determines these complex flows using semantic descriptions of atomic services. A process-level composition solution based on OWL-S is proposed in [16]. In this work the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but instead assume that they already have the list of atomic services. In contrast, we present a technique that automatically find the services that are suitable for composition based on the query requirements for the new composed service. There are solutions such as [18] that solve the selection phase of composition. This work uses pre-defined plans and discovered services provided in a matrix representation. Then the best composition plans are selected and ranked based on QoS parameters like cost, time, and reputation. These criterion are measured using fuzzy numbers. There has been a lot of work on composition languages such as WS-BPEL, WSML, AO4BPEL, etc. which are useful during the execution phase. FuseJ is also once such description language for unifying aspects and components. Though this language was not designed for Web services, the authors present in [17] that it can be used for service composition as well. It uses connectors to interconnect services. There is no centralized process description, but instead is spread across the connectors. With FuseJ, the planning phase has to be performed manually wherein the connectors have to be written. Similarly, OWL-S can also describe a composite service which is actually an abstract

service. Service grounding of OWL-S maps the abstract service to the concrete WSDL specification. These languages are useful after the planning, discovery, and selection is done. The new composite service can be described using one of these languages. In this paper, we present a technique for automatically planning, discovering and selecting services that are suitable for obtaining a composite service, based on the user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use human input on what services would be suitable for composition. We also show different kinds of compositions such as, non-sequential compositions (i.e., composition where there can be more than one service involved at any stage, represented as a directed acyclic graph of services), sequential composition (i.e, a linear chain of services) and composition with if-then-else conditions. We also automatically generate OWL-S descriptions for the new composite service obtained.

3 Web service Composition
Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 1 shows an example composite service made up of five services . In the figure,   and   are the query input parameters and pre-conditions  to respectively. 퓬 and 퓬 are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes and indicates that outputs of constitute (some of) the inputs of .

 









S2 S5 CI',I' S1 S3 S4 CO',O'

Fig. 1. Example of a Composite Service as a Directed Acyclic Graph

As mentioned in section 1, composition can be seen as four phases which include (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution. Most of the related work presents techniques to solve one of these phases or a combination of them. We need a technique that can automatically determine the services involved in composition without the need for any manual intervention or user-defined plans. In this section we present the different kind of compositions and our technique for automatic composition which performs the first three phases - planning, discovery, and selection simultaneously as one phase. Definition (Repository of Services): Repository is a set of Web services. Definition (Service): A service is a 6-tuple of its pre-conditions, inputs, side-effect, ) is the repreaffected object, outputs and post-conditions. = ( sentation of a service where is the list of pre-conditions, is the input list, is the





좋



피 

service's side-effect, is the affected object, is the output list, and is the list of post-conditions.    Definition (Query): The query service is defined as = (    )     where is the pre-conditions, is the input list, is the service affect, is the  affected object,  is the output list, and is the post-conditions. These are all the parameters of the requested service.















좋

 피  

3.1 Sequential Composition A sequential composition is one which has a linear chain of services that form the composite service. When all nodes in the directed acyclic graph of figure 1 have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem. Example 1: Suppose we are looking for a service to make travel arrangements, i.e., flight, hotel, and rental car reservations. The directory of services contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 1 shows the input/output parameters of the user-query and the three services ReserveFlight, ReserveHotel, and ReserveCar. For the sake of simplicity, the query and services have fewer input/output parameters than the real-world services. In this example, service ReserveFlight has to be executed first so that its output ArrivalFlightNum can be used as input by ReserveHotel followed by the service ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Figure 2 shows this example sequential composition as a directed acyclic graph.
Service Query ReserveFlight ReserveHotel ReserveCar Input Parameters PassengerName, OriginAirport,Start Date,DestinationAirport,ReturnDate PassengerName, OriginAirport,Start Date, DestinationAirport,ReturnDate PassengerName, ArrivalFlightNum, StartDate, ReturnDate PassengerName,ArrivalDate ArrivalFlightNum, HotelAddress Output Parameters HotelConfirmationNum, CarConfirmationNum FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress CarConfirmationNum

Table 1. Example Scenario for Sequential Composition

Definition (Sequential Composition): More generally, the Composition problem can be defined as automatically finding a directed acyclic graph =   of services    from repository , given query =(    ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should , , =( , , , , , ) hold on the nodes of the graph:





좋

피   





 앓  않 

  

 1.  , ...,  ,    2. ,   , ...,  The meaning of the is the subsumption (subsumes) relation and is the implication relation. In other words, we are deriving a possible sequence of services where only the provided input parameters are used for the services and at least the required output parameters are provided as an output by the chain of services. The goal is to derive a solution with minimal number of services. Also the post-conditions of a service in the chain should imply the pre-conditions of the next service in the chain.

 防  防  徘 졔  풩     



QueryInputs

ReserveFlight

QueryOutputs ReserveHotel ReserveCar

Fig. 2. Example of Sequential Composition as a Directed Acyclic Graph

3.2 Non-Sequential Composition Let us consider an example where a non-sequential composition can be obtained using the given repository of services. A non-sequential composition can have more than one service involved at any stage of the composition task. Example 2: Suppose we are looking for a service to make international travel arrangements and the directory of services contains ReserveFlight, ReserveHotel, ReserveCar, and ProcessVisa services. In this scenario, we first need to apply for a visa and then make the flight, hotel, and car reservations. Table 2 shows the input/output parameters of the user-query and the four services. In this example, service ProcessVisa has a post-condition VisaApproved. The services ReserveFlight and ReserveHotel have the pre-condition that the visa must be approved before making reservations. ProcessVisa has to be executed first followed by ReserveFlight and ReserveHotel. The post-conditions of ProcessVisa must imply the pre-conditions of ReserveFlight and similarly must imply the pre-conditions of ReserveHotel. The ReserveCar service needs inputs HotelAddress and ArrivalFlightNum. Hence it is executed after both ReserveFlight and ReserveHotel are executed so that their outputs can be used as inputs to ReserveCar. The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Figure 3 shows this non-sequential composition example as a directed acyclic graph. Definition (Non-Sequential Composition): More generally, the Composition problem  of services can be defined as automatically finding a directed acyclic graph =     from repository , given query =(    ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: where has exactly one incoming edge that represents the query inputs 1. and pre-conditions,  ,  .





좋

피 





 앓



 湃    

where has exactly one outgoing edge that represents the query outputs  and post-conditions,  , . 3. where has at least one incoming edge, let , , ...,  be the nodes such that there is a directed edge from each of these nodes to . Then     ,  (  ).   

2.

 앓   密      앓     

 





密 

PreInput Parameters Conditions Query PassengerName, OriginAirport, DestinationAirport, StartDate, ReturnDate Process PassengerName, VisaType, Visa StartDate, ReturnDate Reserve VisaPassengerName, OriginAirport, Flight Approved DestinationAirport, StartDate, ReturnDate Reserve VisaPassengerName, Hotel Approved StartDate, ReturnDate Reserve PassengerName, ArrivalDate, Car ArrivalFlightNum, HotelAddress

Service

Output Parameters

PostConditions

FlightConfirmationNum, HotelConfirmationNum, CarConfirmationNum ConfirmationNum VisaApproved FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress CarConfirmationNum

Table 2. Example Scenario for Non-Sequential Composition

The meaning of is the subsumption (subsumes) relation and is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. Further details on the formal description of the composition problem are in [5].





Condition: VisaApproved QueryInputs ProcessVisa Condition: VisaApproved

ReserveFlight

Parameter: ArrivalFlightNum QueryOutputs ReserveCar Parameter: HotelAddress

ReserveHotel

Fig. 3. Example of Non-Sequential Composition as a Directed Acyclic Graph

3.3 Non-Sequential Conditional Composition Let us now consider an example of a non-sequential composition with if-then-else conditions, i.e., the composition flow varies depending on the result of the post-conditions of a service.

Example 3: This example is similar to Example 2 but with more constraints. Suppose we are looking for a service to make international travel arrangements. We first need to make a tentative flight and hotel reservation and then apply for a visa. If the visa is approved, we can buy the flight ticket and confirm the hotel reservation, else we will have to cancel both the reservations. Also if the visa is approved, we need to make a car reservation. The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table 3 shows the input/output parameters of the user-query and the services.

Service Query

PreConditions

Input Parameters

Output Parameters FlightConfNum,HotelConfNum,CarConfNum FlightConfNum HotelConfNum ConfirmationNum ArrivalFlightNum ConfirmationNum CancelCode CancelCode CarConfirmNum

PostConditions

PasngrName,OriginArprt,Dest-, Arpt,StartDate,ReturnDate Reserve PasngrName,OriginArprt,DestFlight Arpt,StartDate,ReturnDate Reserve PasngrName, StartDate, Hotel ReturnDate Process PasngrName,VisaType, Visa FlightConfNum,HotelConfNum ConfirmFlight VisaApproved FlightConfNum,CreditCardNum ConfirmHotel VisaApproved HotelConfNum,CreditCardNum CancelFlight VisaDenied PasngrName,FlightConfNum CancelHotel VisaDenied PasngrName,HotelConfNum Reserve PasngrName,ArrivalDate Car ArrivalFlightNum

VisaApproved VisaDenied

Table 3. Example Scenario for Non-Sequential Conditional Composition

In this example, service ProcessVisa produces the post-condition VisaApproved VisaDenied. The services ConfirmFlight and ConfirmHotel have the pre-condition VisaApproved. In this case, one cannot determine if the post-conditions of service ProcessVisa implies the pre-conditions of services ConfirmFlight and ConfirmHotel until the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and depending on the outcome of the condition, the corresponding services will be executed. The vertex for service ProcessVisa in the graph is an OR node with the outgoing edges representing the generated conditions and the VisaApproved and outputs. In this case conditions (VisaApproved VisaDenied) (VisaApproved VisaDenied) VisaDenied are generated. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed. Figure 4 shows this conditional composition example as an AND/OR directed acyclic graph. Definition (Non-Sequential Conditional Composition): More generally, the Composition problem can be defined as automatically finding an AND/OR directed acyclic   of services from repository , given query = (    , graph =      ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph is either an OR or AND vertex and represents a service in the composi-













 



tion. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and preconditions of the service. The outgoing edges of an AND node have an arc around them to differentiate from the OR node. The following conditions should hold on the nodes of the graph: 1. where has exactly one incoming edge that represents the query inputs and pre-conditions,  ,  . 2. where has exactly one outgoing edge that represents the query outputs  and post-conditions,  , . 3. where is an AND vertex, let  , , ...,  be the nodes such that  there is a directed edge from to each of these nodes. Then (   ,  (    ). 4. where is an OR vertex, let , , ...,  be the nodes such that  there is a directed edge from to each of these nodes. Then (   ,  (    ).

 앓  앓  앓    앓  

  줆  졔    密                
Condition: VisaApproved ProcessVisa ConfirmFlight





湃  湃 

Parameter: ArrivalFlightNum

ConfirmHotel QueryOutputs

ReserveFlight Query Inputs ReserveHotel

Parameter: ArrivalFlightNum

ReserveCar

Condition: VisaApproved

CancelFlight CancelHotel

Fig. 4. Example of Conditional Composition as an AND/OR Directed Acyclic Graph

The meaning of the is the subsumption (subsumes) relation and is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. When it cannot be determined at compile time whether the post-conditions imply the pre-conditions or not, an OR node is created in the graph. Each outgoing edges represent the possible conditions which will be evaluated at run-time. Depending on the condition that holds, the corresponding services are executed. That is, if a subservice  is composed with subservice  , then the postconditions  of  must imply the preconditions  of  . The following conditions are evaluated at run-time: if (   ) then execute  ; else if (   ) then no-op; else if ( ) then execute  ;







  풩   풩   



 

3.4 Automatic Composition Algorithm In order to produce the composite service which is the graph, as shown in the example figure 1, we filter out services that are not useful for the composition at multiple stages. Figure 5 shows the filtering technique for the particular instance shown in figure 1. The

composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. In figure 5,   are the pre-conditions and the input parameters provided by the query.  and  are the services found after step 1.  is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e.,  =   .  is used to find services at the next stage, i.e., all those services that require a subset of . In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

 





 



I=I CI, I
1

S1 S2 . .

O1

I=IUO
2 1

1

S . .

O
3

2

I=IUO
3 2

2

O S . .
4

3

I=IUO
4 3

3

S . .

O
5

4

O

Fig. 5. Composite Service

4 Automatic Generation of OWL-S descriptions
After we have obtained a composition solution (sequential, non-sequential, or conditional), the next step is to produce a semantic description document for this new composite service. Then this document can be used for execution of the service and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of performing the composition process all over again. We used the existing language OWL-S [2] to describe composite services. OWL-S models services as processes and when used to describe composite services, it maintains the state throughout the process. It provides control constructs such as Sequence, Split-Join, IfThen-Else and many more to describe composite services. These control constructs can be used to describe the kind of composition. OWL-S also provides a property called composedBy using which the services involved in the composition can be specified. Below is the algorithm for generation of the OWL-S document when the composition solution in the form of a graph is provided as the input.
Algorithm: GenerateServiceDescription (Input: G - Solution Graph) 1. Generate generic header constructs 2. Start Composite Service element 3. Start SequenceConstruct 4. If Number(SourceVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each starting/source Vertex V GenerateAtomicService End For

5.

6.

7. 8. 9.

EndSplitJoinConstruct End If If Number(SinkVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each ending/sink Vertex V GenerateAtomicService End For EndSplitJoinConstruct End If For Each remaining vertex V in G If V is AND vertex with one outgoing edge GenerateAtomicService If V is AND vertex with more than one outgoing edge GenerateSplitJoinConstruct If V is OR vertex with one outgoing edge GenerateAtomicService If V is OR vertex with more than one outgoing edge GenerateConditionalConstruct End For End SequenceConstruct End Composite Service element Generate generic footer constructs

A sequential composition can be described using the Sequence construct which indicates that all the services inside this construct have to invoked one after the other in the same order. The non-sequential composition can be described in OWL-S using the Split-Join construct which indicates that all the services inside this construct can be invoked concurrently. The process completes execution only when all the services in this construct have completed their execution. The non-sequential conditional composition can be described in OWL-S using the If-Then-Else construct which specifies the condition and the services that should be executed if the condition holds and also specifies what happens when the condition does not hold. Conditions in OWL-S are described using SWRL. The OWL-S description documents for the example composite services in section 3 are shown in the Appendix. There are other constructs such as looping constructs in OWL-S which can be used to describe composite services with complex looping process flows. We are currently investigating other kinds of compositions with iterations and repeat-until loops and their OWL-S document generation. We are exploring the possibility of unfolding a loop into a linear chain of services that are repeatedly executed. We are also analyzing our choice of the composition language and looking at other possibilities as part of our future work.

5 Implementation
We implemented a prototype composition engine using Prolog with Constraint Logic Programming over finite domain [8], referred to as CLP(FD) hereafter. In our current implementation, we used semantic descriptions of web services written in the language called USDL [4]. The repository of services contains one description document for each service. USDL itself is used to specify the requirements of the service that an application developer is seeking.

Each service description is converted into a tuple: (Pre-Conditions, I, A, O, Post-Conditions). I is the list of inputs and O is the list of outputs. Pre-Conditions are list of conditions on the input parameters and Post-Conditions are the list of conditions on the output parameters. A is the list of side-effects represented as affect-type(affected-object) where the function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. Services are converted to tuples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [6]. In case conditions on a service are not provided, the Pre-Conditions and Post-Conditions in the triple will be null. Similarly if the affect-type is not available, this module assigns a generic affect to the service. The composition engine consists of these modules: (i) Tuple Generator; (ii) Query Reader; (iii) SemanticRelations Generator; (iv) Composition Query Processor; (v) OWLS Description Generator; TupleGenerator converts each service in the repository into the tuple format. The SemanticRelationsGenerator module extracts all the semantic relations and creates a list of Prolog facts. In our current implementation, we use USDL service descriptions which use OWL Wordnet Ontology [3] to specify the semantics. This module is generic enough to be used with other domain-specific ontology as well to obtain semantic relations of concepts. The CompositionQueryProcessor module uses the repository of facts, which contains all the services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs), encodeParam(QueryOutputs, QO), getExtInpList(QueryInputs, InpList), encodeParam(InpList, QI), performForwardTask(QI, QO, LF), performBackwardTask(LF, QO, LR), getMinSolution(LR, QI, QO, A), reverse(A, RevA), confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The output of the query processor is the composition solution which is directed acyclic graph of all the services involved in the composition. Our algorithm selects the optimal solution with least composition length (i.e., the number of stages involved in the composition). If there are any properties with respect to which the solutions can be ranked, then setting up global constraints to get the optimal solution is relatively easy with our constraint based approach. For example, if each service has an associated cost, then the solutions with the minimal cost are returned. The next step is to produce a description of the new composite service solution found. OWL-S DescriptionGenerator automatically generates the OWL-S description of the composite service using constructs depending on the type of composition. We tested our composition algorithm using repositories from WS-Challenge website[9], slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of

services. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema file. We evaluated our approach on different size repositories and tabulated Pre-processing and Query Execution time. We noticed that there was a significant difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we found is that the repository was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 4 shows performance results for the different kind of compositions. The preprocessing time remains the same of all three kinds of composition whereas the query execution time varies. The times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than or equal to the wall clock time. The results are consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which is negligible (just 1 to 3 msecs) even for complex queries with large repositories.
Repository Size (num of services) 2000 2000 2000 2500 2500 2500 3000 3000 3000 Number of I/O param-eters 4-8 16-20 32-36 4-8 16-20 32-36 4-8 16-20 32-36 PreProcessing Time (secs) 36.5 45.8 57.8 47.7 58.7 71.6 56.8 77.1 88.2 QueryExec Time (msecs) Sequential Composition 1 1 2 1 1 2 1 1 3 NonSequential Composition 1 1 2 1 2 2 1 2 3 Conditional Composition 1 2 2 1 2 3 1 3 4

Table 4. Performance of Composition Algorithm

6 Conclusions and Future Work
To make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize and compose services automatically. Our semantics-based approach uses semantic description of Web services to find substitutable and composite services that best match the desired service. Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services, number of services in a composition, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential or non-sequential composition that is possible for a given query and also automatically generates OWL-S description of the composite service. This OWL-S description can be used during the execution phase and subsequent searches for this composite service

will yield a direct match. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. Use of Constraint Logic Programming helped greatly in obtaining an efficient implementation of this system. Our future work includes extending our engine to support an external database to save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size which can easily be the case in future. We are also investigating other kinds of compositions with loops such as repeat-until and iterations and their OWL-S description generation. Analyzing the choice of the composition language and exploring other language possibilities is also part of our future work. Acknowledgments: We are grateful to our co-researcher Ajay Bansal for the helpful discussions and comments.

References
1. S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp. 46-53, March 2001. 2. OWL-S www.daml.org/services/owl-s/1.0/owl-s.html. 3. OWL WordNet: Ontology-based information management system. http://taurus. unine.ch/knowler/wordnet.html. 4. A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal ServiceSemantics Description Language. In ECOWS, pp. 214-225, 2005. 5. S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007. 6. S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics Description Language for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas.edu/~sxk038200/USDL.pdf. 7. A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services: A case study in chemical emergency response. In ICWS, pp.751-758, 2005. 8. K. Marriott and P. Stuckey. Prog. with Constraints: An Introduction. MIT Press, 1998. 9. WS Challenge 2006. http://insel.flp.cs.tu-berlin.de/wsc06. 10. U. Keller, R. Lara, H. Lausen, A. Polleres, and D. Fensel. Automatic Location of Services. In European Semantic Web Conference, May 2005. 11. D. Mandell, S. McIlraith Adapting BPEL4WS for the Semantic Web: The Bottom-Up Approach to Web Service Interoperation. In ISWC, 2003. 12. M. Paolucci, T. Kawamura, T. Payne, and K. Sycara Semantic Matching of Web Service Capabilities. In ISWC, pages 333-347, 2002. 13. S. Grimm, B. Motik, and C. Preist Variance in e-Business Service Discovery. In Semantic Web Services Workshop at ISWC, November 2004. 14. S. McIlraith, T.C. Son Adapting golog for composition of semantic Web services. In KRR, pages 482493, 2002. 15. B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pp.467-477. 16. M. Pistore, P. Roberti, and P. Traverso Process-Level Composition of Executable Web Services In European Semantic Web Conference, pages 62-77, 2005. 17. D. Suvee, B. Fraine, and M. Cibran Evaluating FuseJ as a Web Service Composition Language In European Conference on Web Services, 2005. 18. D. Claro, P. Albers, and J. Hao Selecting Web services for Optimal Compositions In Workshop on Semantic Web Services and Web Service Composition, 2004. 19. J. Rao, X. Su. A Survey of Automated Web Service Composition Methods In Workshop on Semantic Web Services and Web Process Composition(SWSWPC), 2004. 20. J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

Appendix
The sequential composite service shown in Example 1 and Figure 2 is described in OWL-S as follows:
<rdf:RDF ... <process:CompositeProcess rdf:ID="TravelReservation"> ... <process:composedOf><process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ReserveFlight"/> <process:AtomicProcess rdf:about="#ReserveHotel"/> <process:AtomicProcess rdf:about="#ReserveCar"/> </process:components></process:Sequence> </process:composedOf> </process:CompositeProcess></rdf:RDF>

The non-sequential composite service shown in Example 2 and Figure 3 is described in OWL-S as follows:
<rdf:RDF ... <process:CompositeProcess rdf:ID="TravelReservation"> ... <process:composedOf><process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ProcessVisa"/> <process:CompositeProcess rdf:about="#Stage1"/> <process:AtomicProcess rdf:about="#ReserveCar"/> </process:components> </process:Sequence> </process:composedOf> </process:CompositeProcess> <process:CompositeProcess rdf:ID="Stage1"> <process:composedOf><process:Split-Join> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ReserveFlight"/> <process:AtomicProcess rdf:about="#ReserveHotel"/> </process:components></process:Split-Join> </process:composedOf> </process:CompositeProcess></rdf:RDF>

The conditional composite service shown in Example 3 and Figure 4 is described in OWL-S as follows:
<rdf:RDF ... <process:CompositeProcess rdf:ID="TravelReservation"> ... <process:composedOf><process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#Stage1"/> <process:AtomicProcess rdf:about="#ProcessVisa"/> <process:CompositeProcess rdf:about="#IfThenElseStage1"/> </process:components></process:Sequence> </process:composedOf> </process:CompositeProcess>

<process:CompositeProcess rdf:ID="IfThenElseStage1"> ... <process:composedOf> <process:If-Then-Else> <process:ifCondition> <expr:SWRL-Condition rdf:ID="VisaAccepted"> <expr:expressionLanguage rdf:resource="&expr;#SWRL"/> <expr:expressionBody rdf:parseType="Literal"> <swrl:AtomList> <rdf:first> <swrl:IndividualPropertyAtom> <swrlb:equal rdf:resource="#VisaAccepted"/> <swrl:argument1 rdf:resource="#VisaAccepted"/> <swrl:argument2 rdf:resource="&rdf;#true"/> </swrl:IndividualPropertyAtom> </rdf:first> <rdf:rest rdf:resource="&rdf;#nil"/> </swrl:AtomList> </expr:expressionBody> </expr:SWRL-Condition> </process:ifCondition> <process:then> <process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ConfirmFlight"/> <process:CompositeProcess rdf:about="#Stage2"/> </process:components> </process:Sequence> </process:then> <process:else> <process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#CancelFlight"/> <process:AtomicProcess rdf:about="#CancelHotel"/> </process:components></process:Sequence> </process:else> </process:If-Then-Else></process:composedOf> </process:CompositeProcess> <process:CompositeProcess rdf:ID="Stage1"> <process:composedOf><process:Split-Join> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ReserveFlight"/> <process:AtomicProcess rdf:about="#ReserveHotel"/> </process:components></process:Split-Join> </process:composedOf> </process:CompositeProcess> <process:CompositeProcess rdf:ID="Stage2"> <process:composedOf><process:Split-Join> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ConfirmHotel"/> <process:AtomicProcess rdf:about="#ReserveCar"/> </process:components></process:Split-Join> </process:composedOf> </process:CompositeProcess></rdf:RDF>

Towards a General Framework for Web Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake Department of Computer Science, Georgetown University Washington, DC 20057 Abstract
CI',I'

Gopal Gupta Department of Computer Science, The University of Texas at Dallas Richardson, TX 75083

S2 S5 CO',O' S3 S1 S4

Service-oriented computing (SOC) has emerged as the eminent market environment for sharing and reusing service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services. In this paper we present a generalized semantics-based technique for automatic service composition that combines the rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG) where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented.

Figure 1. Example of a Composite Service as a Directed Acyclic Graph

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered, and consumed. A composite service is a collection of services combined together in some way to achieve a desired effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [2]. Most efforts reported in the literature focus on one or more of these four phases. The first phase involves generating a plan, i.e., all the services and the order in which they are to be composed in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions based on non-functional properties like QoS properties. The last phase involves executing the services as per the plan and in case any of them are not available, an alternate solution has to be used. In this paper we formalize the generalized composition problem based on our conditional directed acyclic graph

representation. We present our approach that generates most general compositions based on (conditional) directed acyclic graphs (DAG). In our framework, the DAG representation of the composite service is reified as an OWL-S description. This description document can be registered in a repository and is thus available for future searches. The composite service can now be discovered as a direct match instead of having to look through the entire repository and build the composition solution again.

2. Web service Composition
In this section we formalize the generalized composition problem. In this generalization, we extend our previous notion of composition [1] to handle non-sequential conditional composition (which we believe is the most general case of composition). Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 1 shows an example composite service made up of five services S1 to S5 . In the figure, I and CI are the query input parameters and pre-conditions respectively. O and CO are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and Sj indicates that outputs of Si constitute (some of) the inputs of Sj . Definition (Repository of Services): Repository (R) is a set of Web services.

Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and post-conditions. S = (CI , I , A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions, I is the input list, A is the service's side-effect, AO is the affected object, O is the output list, and CO is the list of post-conditions. The pre- and post-conditions are ground logical predicates. Definition (Query): The query service is defined as Q = (CI , I , A , AO , O , CO ) where CI is the list of preconditions, I is the input list, A is the service affect, AO is the affected object, O is the output list, and CO is the list of post-conditions. These are all the parameters of the requested service. Definition (Generalized Composition): The generalized Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R, given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph either represents a service involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can be determined only after the execution of the service. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: 1. i Si  V where Si has exactly one incoming edge that represents the query inputs and pre-conditions, I i I i , CI i CI i . 2. i Si  V where Si has exactly one outgoing edge that represents the query outputs and post-conditions, O i O i , CO i CO i . 3. i Si  V where Si represents a service and has at least one incoming edge, let Si1 , Si2 , ..., Sim be the nodes such that there is a directed edge from each of these nodes to Si . Then Ii k Oik  I , CI i  (COi1 COi2 ...  COim  CI ). 4. i Si  V where Si represents a condition that is evaluated at run-time and has exactly one incoming edge, let Sj be its immediate predecessor node such that there is a directed edge from Sj to Si . Then the inputs and pre-conditions at node Si are Ii = Oj  I , CI i = COj . The outgoing edges from Si represent the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time. The meaning of the is the subsumption (subsumes) relation and  is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composi-

tion can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. When it cannot be determined at compile time whether the post-conditions imply the pre-conditions or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible conditions which will be evaluated at run-time. Depending on the condition that holds, the corresponding services are executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply the preconditions CI 2 of S2 . The following conditions are evaluated at run-time: if (CO1  CI 2 ) then execute S1 ; else if (CO1   CI 2 ) then no-op; else if (CI 2 ) then execute S1 ;

3. Automatic Generation of Composite Services
In order to produce the composite service which is the graph, we filter out services that are not useful for the composition at multiple stages. The composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e., I2 = O1  I . I2 is used to find services at the next stage, i.e., all those services that require a subset of I2 . In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

4. Conclusions and Future Work
To make Web services more practical we need a general framework for composition of Web services. The generalized approach presented in this paper can handle nonsequential conditional composition that can be used in automatic workflow generation in a number of applications.

References
[1] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007. [2] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Decision Support Systems 53 (2012) 234244

Contents lists available at SciVerse ScienceDirect

Decision Support Systems
journal homepage: www.elsevier.com/locate/dss

Workflow composition of service level agreements for web services
M. Brian Blake a,, David J. Cummings b, Ajay Bansal c, Srividya Kona Bansal c
a b c

Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, United States Department of Computer Science, Stanford University, United States Department of Engineering, Arizona State University, United States

a r t i c l e

i n f o

a b s t r a c t
Service-oriented architecture enables an environment where businesses can expose services for use by their collaborators and their peer organizations. In this dynamic environment, organizations require the use of service level agreements (SLAs) to assure the quality of service (QoS) standards of services provided by their collaborators. In an ad-hoc workflow scenario, a business may need to perform real-time composition of existing services in response to consumer requests. In this work, we suggest that, in parallel to traditional web service composition, the business must also compose the existing SLAs in order to ensure the service levels that must be guaranteed to new consumers. Ultimately, this approach to SLA composition must align with the overarching principles of the provider and the priorities of the consumer. In this paper, we introduce a model and representations of service level agreement attributes appropriate for managing a service provider's expectations when adding new partners. Our evaluations suggest that the SLA composition can efficiently run concurrently with traditional service composition.  2012 Elsevier B.V. All rights reserved.

Article history: Received 19 July 2010 Received in revised form 22 December 2011 Accepted 30 January 2012 Available online 8 February 2012 Keywords: Service level agreements Quality of service Web services Service-oriented computing

1. Introduction A service level agreement, SLA, is a technical contract between two types of businesses, producers and consumers. A SLA captures the agreed-upon terms between organizations with respect to quality of service (QoS) and other related concerns. In simple cases, one consumer forms a SLA with a producer. In more complex cases, a consumer may form a SLA that defines a set of producer businesses. Considering a service-oriented computing environment, capabilities are shared via the implementation of web services exposed by a producer organization. The ultimate goal of service-oriented computing is for consumers to access these shared capabilities on-demand. As such, in cases where businesses have longstanding relationships, such as workflow and supply chain environments, peer companies that share services must be able to assure a level of service to their underlying customers [4,8]. New specifications, such as the Web Service Level Agreement (WSLA) and Web Service Agreement (WS-Agreement) [2] enable SLAs to be associated with an individual web service or even groups of web services. These specifications define an eXtensible Markup Language (XML)based data model that can be used along with the Web Service Description Language (WSDL) documents that traditionally describe the web services. These specifications provide a significant opportunity.
 This paper is a substantial extension of earlier work presented in [7] and [5].  Corresponding author. Tel.: + 1 574 631 1625; fax: + 1 574 631 8007. E-mail addresses: m.brian.blake@nd.edu (M.B. Blake), david.cummings@stanford.edu (D.J. Cummings), srividya.bansal@asu.edu, ajay.bansal@asu.edu (A. Bansal). 0167-9236/$  see front matter  2012 Elsevier B.V. All rights reserved. doi:10.1016/j.dss.2012.01.017

Organizations can specify QoS-related concerns in concert with the functionality concerns already captured in the WSDL files. As a result, when a new organization searches for a pertinent web service, the SLA-enhanced WSDL file can be used to determine the appropriateness of the service to meet the required business need. Furthermore organizations can use the SLA-enhanced WSDL file to negotiate the QoS terms. Although these SLA technologies and specifications present new opportunities for service-oriented business processes, there are a number of significant barriers. When a consumer organization must create a new business capability that requires the workflow composition of multiple web services, then that organization will also need to understand the composite impact of the underlying SLAs. Consequently, in addition to composing web services that are functionally compatible, the organization will need to ensure that the web services are compatible with regard to their service levels. Also, the product of all the SLAs for a composition of web services must be within the required threshold of feasibility as defined by the end users. As the service-oriented computing paradigm increases in popularity, the consumer will have the option of many similar services that may meet a particular requirement. As such, the composition of web services that is most efficient for a particular business purpose will rest on the organization's ability to understand and optimize the corresponding composition of SLAs. To deal with the aforementioned issues, we introduce the phrase, workflow composition of SLAs. Our approach suggests the multidimensional evaluation of existing agreed-upon QoS standards in order to predict the standards possible for the introduction of new agreements. While the notions of multi-dimensional analysis, optimization, dynamic programming are not new [9,10,12,17,35] in this

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

235

work, we identify the specific SLA-based attributes that allows for the introduction of new partners. Furthermore we develop a set of principles and the associated process that utilize the SLA information to estimate service levels. This approach favors services with clean request/response (RPC-type) communication, generally known as WSDL-based web services. Further investigation would be required to assess this approach as it relates to REST-based services [37]. In this work, we investigate several research issues relevant to the integration of web services-based workflow: 1. What SLA measures and principles are appropriate to support QoSbased assessment of existing service level guarantees? 2. Given a group of SLAs and knowledge about current consumer service level needs, can an on-demand request be analyzed against existing SLAs to guarantee a certain service level for a new consumer? The paper proceeds in the following section with a discussion of related work. In Section 3, we discuss how the SLA-based QoS assessment values are derived from higher-level organizational principles. The formal details of the attributes are defined in Section 4, and how the attributes are physically captured in markup languages in shown in Section 5. Finally, in Section 6 we evaluate the performance SLA composition as it runs parallel with traditional service composition routines. 2. Related work There are many related projects that investigate the general use of SLAs for web services [18]. Some projects characterize SLA approaches to specific domains, such as military, database management, or information systems [13,20,26,29]. There is also a large body of work that attempts to automate the management and negotiation of SLAs [11,16,23,27,33]. Other work attempts to use semantics to automate the negotiation of SLAs [15,25]. Our work leverages markup language (i.e. WS-Agreements) for providing SLA measures as in other studies [1,28,30]. All related work describes the importance of composing SLAs. In [28], their emphasis is on compatibility between user requirements and provider constraints. Their approach suggests a promising model-based approach to assuring the compatibility. Our work is closely related to the comprehensive work performed by [9,10,35]. Each of these approaches investigates the QoS-based and constrained composition of web services. Although Canfora et al. [9] has an elegant approach that allows for the insertion and aggregation of any user-defined QoS attribute, our approach identifies the specific SLA measures that support the user-driven assessment of an environment where their SLAs dictate current system state. Table 1 shows a survey of SLA attributes and how they are exploited in related projects specifically in the service-oriented computing domain. Our work can be loosely classified in the body of work that looks to automate the aggregation of QoS attributes [14,21,24,32]. The
Table 1 Survey of research projects that consider SLA attributes for web services. Author names Run time Reputation Uptime (Avail)            Resp time ***      

uniqueness of our approach is that we consider the impacts when new web service workflows must be added as they affect the existing operational SLAs. More specifically, if a composite capability overlaps multiple SLAs, then the characteristics of an early SLA can impact a later SLA in the composition routine. Canfora et al., Cardoso et al., Zeng et al., [9,10,35] concentrate on deriving a specific composition routine as constrained by QoS values. Canfora et al., Zhang et al., and Yu et al. focus on iterative multiattribute utility approaches [12] where to focus is on the overall optimization function and less on the details of each of the attribute. Our work attempts to consider both consumer and producer concerns when assessing the entry of a new workflow. As such, our work contains low-level details for each service level objective such that subsequent optimization approaches use them as a model for optimization that targets each attribute at a low-level. Although [22] has a similar approach where SLAs are aggregated formally, they do not consider consumer and producer services independently as in our work. In summary, we define specific SLA measures and formally integrate measures across multiple SLAs. We also define a principled process for the composition of SLAs. This work extends related work [6] by concentrating on SLA measures in markup language files as opposed to Unified Modeling Language (UML) models. Unlike other work in QoS-based web service composition, we attempt to classify QoS attributes by those associated with the provider and those associated with the consumer. Another variation here is the introduction of several high-level criteria that can be used to characterize organizations. We believe that by aggregating all lower-level attributes into a smaller set of higher-level criteria then organizations can be quantitatively evaluated or scored. Further evaluation in this paper justifies that such on-demand assessment of SLAs performs feasibly in an operational environment where very large numbers of web service workflows exist. 3. Assessing an enterprise based on its SLAs The typical SLA has a large number of measures and criteria. However, in this work, we attempt to choose the measures that are most closely aligned to aggregation of a group of SLAs and ultimately their assessment. In the operational notion of web service composition, a basic web services workflow system must ensure that the input information supplied by the consumer ultimately leads to the required actions and outputs required by that consumer. In parallel, the workflow management system must ensure that the predicates and requisites match (either by syntactical or semantic techniques) in each step of the workflow. It is the operational composition routines that motivate the set of SLA attributes relevant to our work. In order to designate which attributes that are most relevant to our proposed innovation, we developed a set of principles important to managing the quality of an enterprise with many business processes. The three relevant principles are Compliance (Suitability),

Negotiation (rebinding)  

Cost (price)      

Success rate/ reliability      

Problem resolution  

Maintenance  

(Blake et al.) [7] and this paper  (Canfora et al.) [9]  (Yu et al.) [34] Zhang et al.) [36] ** (Zeng et al.) [35] (Cardoso et al.) [10] (Jin et al.)[18] *  (Mohabey et al.) [22]

* [18] list attributes, but do not develop formal approaches for composition. ** Although Canfora et al., Yu et al., and Zhang et al. do not formally define each attribute, their work focuses on an approach that allows any attribute to be aggregated within the composition routine. *** Response time and run time (and in other places Service Rate) have the same or different definitions. This table places those attributes into separate columns to show the different meanings across the survey of related literature.

236

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

Sustainability, and Resiliency. By correlating the QoS attributes to these principles, we have developed a list that is suitable for assessing an organization based on existing SLAs. The three principles (illustrated in Fig. 1 as a Unified Modeling Language class diagram) are defined below in addition to their underlying SLA measures. 3.1. Compliance (suitability) Compliance is the principle that ensures that the consumer receives the requested composite capability at the service level that is required. The functional notion of web service composition (as illustrated in Fig. 1) fits within this principle, since the consumer specifies their required outputs of the composition. Considering SLA terms, the composition process must assure that the aggregate cost, uptime, and run time are compliant with the user requirements. Cost is the sum total price of all services participating in the solution process. Uptime is a guarantee by the service providers that their services will be available a specified percentage of the time per day or month. Finally, run time is the time it takes to complete the process by adding the response times of each service in the composition. 3.2. Sustainability Sustainability is the ability to maintain the underlying services in a timely fashion. Success rate, Negotiation/renegotiation, and problem resolution are related to ensuring the process continues to execute effectively. Success rate is defined by the historical rate at which a provider successfully completes a request. A consumer will require assurance that a particular business is capable of agreeing on contract terms (i.e. negotiation/renegotiation) in a timely manner. Moreover, the result of a negotiation scheme might be the development of new agreement. In addition, the service providers must be capable of resolving highimpact problems (perhaps identified by the consumer) in a timely manner. Success rate, negotiation, and problem resolution times ensure that a consumer can meet the demands of their end users. 3.3. Resiliency Resiliency is the principle of a service to perform at high service levels over an extended period of time. This principle was derived from our work with data-centric government transactional systems. The resiliency principle specializes many of the principles that underly the general notion of mean time between failures (MTBF) [3,31]. Low resiliency is represented by a service that is frequently taken off-line for maintenance. Additional, the frequency of updates may impede the predictability of its operation. Consumers will need adequate notice prior to maintenance downtimes. In addition, resiliency dictates a low frequency of maintenance downtime. Peer consumers may also add comments about a particular provider and thus create a quantified reputation. The reputation rating also influences the resiliency of a service. The model elements of our work were derived from our literature review and from work with collaborating stakeholders acknowledge

in this paper. As such, we assume the validity of this model as the applicability of the work to other domains, in some sense, relies on these underlying elements. The paper proceeds with a formal definition of all attributes relevant to aggregating and assessing a group of SLAs, and subsequently a description of how the information can be stored physically. The final sections discuss the approach and performance for aggregating SLAs of many business processes. 4. Aggregating and assessing service level agreements When assessing a group of web service workflows, the resulting assessment must be a result of the aggregated SLA terms of the underlying services. In some cases, aggregating the SLA terms is as straightforward as adding the measures of each of the dependent services, but in other cases the aggregate measure must be created based on consumer requirements. Since there may be subjectivity with respect to how the aforementioned SLA measures can be aggregated and calculated, we introduce the following formal concepts for composing the SLA measures in the context of web service composition. 4.1. Defining the physical entities for assessing SLAs

Definition 1. Server Composition in a service-oriented architecture involves a consumer collaborating with a producer. The mapping from service to resources (servers and computational units) must consider system boundaries (i.e. which servers are dedicated, which are not), the size of the messages and transactions, and initialization/set up costs. Here, we illustrate an over-simplified correlation of the services to the server. The capabilities of the producers and consumers are hosted on servers. Each server can be characterized by its performance, uptime percentage, throughput, and the pre-notification time, i.e., the server informing its constituents prior to any downtime associated with server enhancements and repairs. A server, v, can be formally defined as a tuple of these SLA measures as shown below:   v  Perf v ; Upv ; Tiv; Tov ; PreNT v 1

where Perfv is the performance of the server measured loosely in computations/second, Upv is the uptime percentage of the server, Tiv is the throughput of input messages measured in bytes/second, Tov is the throughput of output messages measured in bytes/second, and PreNTv is the pre-notification time measured in seconds. Definition 2. Set of servers Let  be the set of servers of all producer and consumers involved in the collaboration.   fv1 ; v2 ; v3 ;...; vn g:

*
<<is a>>

1 1 1

<<has criteria>>

*

*

<<is a>>

<<is a>>

Fig. 1. A taxonomy of SLA measures for web services workflow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

237

where v1 , v2 , v3 , ..., vn are the producer and consumer servers. Each vi  (for all i = 1 to n), is a tuple of SLA measures as described above. Definition 3. Service A service can be defined by some function (in some cases the average) of computations (i.e. processor cycles or relevant measure for the specific domain) it requires based on the nature of the processing domain. This computation must incorporate length of the message it receives when realizing its capabilities. A service, s, can be defined as the following pair: s  Comps ; MLs  2

where Comps is the number of required computations of the service and MLs is the message length measured in bytes. The intention of this formalization is to illuminate conceptually the connection between the services and the servers that host them. Definition 4. Set of services Let  be the set of all services involved in a composition.   fs1 ; s2 ; s3 ;...; sm g where s1 , s2 , s3 , ..., sm are the services involved in the composition. Each si  (for all i = 1 to m), is a pair of the number of required computations and the message length (in bytes) as described above. Definition 5. SLA of a service SLA of a service consists of specific web service-oriented measures, as defined in the lower elements in the taxonomy in Fig. 2. Let Asi be the SLA of a service si. Asi can be represented as a tuple shown below:   Asi  Upsi ; RTimesi ; SRespsi ; Cost si ; PreNT si ; RenegotT si; Repsi; Relsi 3 where Upsi is the uptime of the service specified by the agreement, RTimesi is the allowable run time, SRespsi is the allowable service response time, Costsi is the subscription cost or price of the service, PreNTsi is the maintenance pre-notification time, RenegotTsi is the renegotiation (expiration) time of the agreement, Repsi is the reputation of the service and Relsi is the reliability rating of the service. 4.2. Generating aggregate SLAs across workflows In this context, we define a web service workflow as a preestablished process (as defined by an SLA) between a consumer and provider (consisting of a group of web services). At composition time, this web service workflow is not yet an active process or instance, but the specification of all pre-established partnerships or agreements of organizations to share web services. The SLA for a body of many web service workflows is obtained by composing the set of SLAs of all the services participating in each composition. This process is similar to the traditional QoS-based service composition process although QoS measures must be separated by provider and

consumer concerns. The authors work closely with a federal government organization that leverages data-centric web services to develop the list of attributes that are most closely related to services concerned with sharing data from distributed information stores. Hence, the SLAs here consists of uptime of the composite service, the allowable run time, the service response time, the subscription cost or price, the maintenance pre-notification time, and the renegotiation time of the agreement. Let AWf represent the composite SLA that is calculated by composing the set of all agreements of all relevant services, i.e., the services on the producer servers and any other services on the consumer's server that might impact the overall system-wide assessment. Let APS represent the set of agreements of the services on the producer's servers that are involved in the composition. APS  fAPS1 ; APS2 ; APS3 ;...; APSm g Let ACS represent the set of agreements of the services on the consumer's servers that may impact the composite agreement AWf. ACS  fACS1 ; ACS2 ; ACS3 ;...; ACSn g

4.2.1. Composite service uptime (UpWf) The uptime for the composite SLA must be less than the minimum of:  The uptime Upvi for consumer's server vi  and  The agreed uptimes of the producer's services in APS = {APS1, A ..., A PSm} The relationship can be shown as: UpWf bminUpvi ; mini1tom UpPSi 沙 5
PS2,

4.2.2. Composite run time (RTimeWf) The time required for an individual service to complete its execution can be considered the task time. The repeatability of the task time while the service is in commission translates to the run time. For a web service, the run time is a function of the internet connection, the internal network connection, the hosting hardware, and the software service. Run time is defined by the provider or consumer which is captured within the SLA. The combined run time is illustrated in Fig. 2. The run time for the composite SLA must be less than the minimum of:  The producer server throughput divided by the message lengths of service input and output  The consumer server throughput divided by the message lengths of service input and output Let RTimeC represent the run time of the consumer. Including the sum of run times already guaranteed to others is important to avoid the impact of a voluminous load from external operations. The run time of the consumer server is obtained by dividing the server throughput TC by the message length of the consumer services MLCS. RTimeC  T C =MLCS 6

SLA Task Time (RunTime)

Let RTimeP represent the run time of all the stakeholders, i.e., the producers, p. It is the difference of the run times of the sum, m, of all services on the provider-side and the sum, n, of all client-side services. RTimeP  i1tom RTimePSi -i1ton RTimeCSi The run time for the composite SLA is represented as: 7

Internet

Network

System

Service

Fig. 2. Task time and run time.

RTimeWf bmin RTimeP ; RTimeC 

8

238

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

4.2.3. Composite service cost (CostWf) The cost of using a web service can be aggregated to understand the price of an entire process. The cost of an individual service is the sum of:  The bandwidth used (i.e. the product of the usage frequency and the message length) multiplied by the cost per byte/sec of bandwidth, which is a predefined constant BC. BC could be zero if bandwidth costs are negligible to the service provider.  The computations used multiplied by the cost of computation/sec, which is some predefined constant CC. As above, CC could be zero if computation costs are negligible to the provider.  The sum of the costs of all dependent services. This results in the following equation:     Cost Wf  RTimeWf  MLCS  BC  RTimeWf  C S  CC  i1tom Cost PSi 9

 The reputation for the consumer server vi  The average reputation for all the dependent services This relationship can be illustrated as: RepWf  averageRepvi  i1tom RepPsi =m 12

4.2.7. Composite service reliability (RelWf) Reliability in traditional QoS-based web service composition has been calculated using several approaches in related work [19]. Unlike other approaches, this approach deals with SLA specifications which tend to be the worst case agreement between consumer and provider. As such, we believe closest estimate is to aggregate the values by calculating the minimum of:  The reliability for the consumer server vi  The minimum of reliabilities for all the dependent services This relationship can be illustrated as: RelWf minRelvi ; mini1tom RelPSi 沙 13

4.2.4. Composite service pre-notification time (PreNTWf) When a service must be disabled for maintenance, organizations must inform their collaborators. The minimum time for notification before maintenance begins is calculated as the minimum of:  The time of notification for the consumer server vi  The minimum of all notification times agreed upon for all dependent services This relationship can be illustrated as: PreNT Wf minPreNT vi ; mini1tom PreNT PSi 沙 10

Definition 6. Composite SLA The composite SLA, AWf is a tuple of all the aggregated SLA measures as shown:
  AWf  UpWf ; RTimeWf ; SRespWf ; Cost Wf ; PreNT Wf ; Renegot T Wf ; RepWf ; RelWf

4.2.5. Composite service renegotiation time (RenegotTWf) Negotiation/renegotional time is the guarantee giving by the provider and consumer that defines how quickly a request for new QoS attributes will be acknowledged the other party. The renegotiation date for this agreement must be after the predefined constant waiting time, WT, with respect to each other's agreement to which this server is a party. This notion of renegotiation time is illustrated in Fig. 3. The equation below must hold for each agreement ACSi ACS to which the service provider is a party. This test can be shown as: f or all i  1 to n; Renegot T Wf  maxRenegotT CSi  WT  11

where the aggregated SLA measures UpWf, RTimeWf, SRespWf, CostWf, PreNTWf, RenegTWf , RepWf , RelWf are obtained from the relations shown in Eqs. (5), (8), (9), (10), (11), (12), and (13) respectively. 5. Representing SLA attributes as WS-Agreements (WSAG) In order to store and manage SLAs, the attributes must be represented in a format conducive for distributed data management. XML is a language that allows complex information to be represented with embedded metadata. An XML-based approach to representing SLA information facilitates quick interpretation of data and using translation techniques, such as the eXtensible Stylesheet Language, XSL, allows the comparison and aggregation of the underlying information. WSAG is an XML-based language that is defined with SLA attributes. A brief background is discussed here, but more information can be found at [21]. In a WSAG document, the data are represented hierarchically underneath the notion of an agreement. An agreement can be further specified with name or identifying string. An agreement can also be described by its context. Context information includes the name of the consumer and producer, the timeframe by which the agreement is valid, and other related template information. Each agreement encapsulates a list of terms. Terms describe the information of the services that are included. Of most importance to this work are the guarantee terms. Guarantee terms consist of a service scope which contains the service names of the specific service relevant to the guarantee. The service level objective contains a predicate for the metrics that quantitatively define

Problem resolution is not formally defined because the calculation is a similar formula as renegotiation. As a definition (shown in Fig. 4), problem resolution can be defined as the sum of the time for recognizing the error, the time that it takes for a provider to acknowledge the error, and the actual time for resolving the problem. 4.2.6. Composite service reputation (RepWf) In a service composition scenario, reputation is defined as a numeric score on a relative scale for a web service as captured by consumers and providers in a web service repository. The Reputation for the composite service is calculated as the average of:

SLA Renegotiation Time
SLA Problem Resolution Time

Request

Negotiation (Wait Time)

Error Acknowledge Recognition ment

Resolution Time by Service

Fig. 3. Aspects of renegotiation time.

Fig. 4. Aspects of problem resolution.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

239

the guarantee. The service level objective contains the parameter name, value, and unit of measure. As an example, the SLA attributes of uptime and maintenance are shown in Table 2. Uptime is a common attribute for SLAs. As described earlier, an uptime SLA guarantees the availability of a service by percentage over a designated period of time. The other SLA metric, maintenance notification time, is not as universally used as uptime. The example in Table 2 shows that maintenance notification time is also straightforward with regard to representation in the WS-Agreement notations. Capturing service level agreements in XML-based notations allows the SLA attributes to be represented in a format similar to the WSDL files that represent the operational specifications of the service. WSAG can be transported and negotiated along with WSDL files. This represents a benefit if a service stakeholder wants to evaluate multiple services, side-by-side. Another benefit of capturing SLA attributes in XML-based files is the ability of enhancing attributes with semantics. It is possible that organizations will name attributes with their own specialized naming schemes. Semantics would allow disparate organizations to mediate SLA attributes that are the same but may be named differently. This approach leverages the numerous projects that use semantics to highlight web services for mediation. However, a detriment of capturing these attributes in XML-based notations occurs when organizations have pre-established agreements. It is likely that, in a service-oriented computing environment, coalitions of businesses will form similar to partnerships that occur in traditional businesses. In such cases, the overhead of descriptive tags may be unnecessary. In addition, SLA constraints may become more comprehensive as the partnerships enhance their coordination and negotiation. These files may become too cumbersome for semi-automated manipulation where human inspection may be a required step in overall process. 6. Creating aggregated SLAs on-demand The formalization in previous sections shows the necessary measures for creating SLAs for new business workflows that reflect the effects of other workflows existing at the same organization. The authors collaborated with The MITRE Corporation for the development of a framework for defense information systems organizations shown in
Table 2 Sample SLA attributes shown in WS-Agreement representations. WS-Agreement for uptime b wsag:GuaranteeTerm wsag:Name = "uptimePref" wsag:Obligated = "ACME"> b wsag:ServiceScope > b wsag:ServiceName > AcmeService1b/wsag:ServiceName > b/wsag:ServiceScope > b wsag:ServiceLevelObjective > b wsag:predicate type = "greater"> b wsag:parameter>job:uptimePercentageb/wsag:parameter > b wsag:value>5b/wsag:value > b wsag:unit>time:secondsb/wsag:unit > b/wsag:predicate > b/wsag:ServiceLevelObjective > b/wsag:GuaranteeTerm > WS-Agreement for Maintenance b wsag:GuaranteeTerm wsag:Name="maintenanceNotificationPref"wsag: Obligated="ACME"> b wsag:ServiceScope > b wsag:ServiceName>AcmeService1b/wsag:ServiceName > b/wsag:ServiceScope > b wsag:ServiceLevelObjective > b wsag:predicate type="greater"> b wsag:parameter>job:maintPreNotificationb/wsag:parameter > b wsag:value>7b/wsag:value > b wsag:unit>time:daysb/wsag:unit > b/wsag:predicate > b/wsag:ServiceLevelObjective > b/wsag:GuaranteeTerm >

Fig. 5. These organizations host large numbers of web services that provide battlefield information such as weather information, force location and tracking information, and satellite telemetry. Each of the various defense forces (Army, Navy, Air Force, etc.) provides access their web services via this shared portal. The sources vary in their guaranteed service level objectives. The defense information systems organizations are devising portals that manage SLAs in governance database while recording historical service level information. The approaches devise in this paper are incorporated within the portal logic. As such, when producers provide new services and when clients gain access to existing services, SLAs are verified for validity. In such an ad-hoc environment, it is important to understand if the SLA composition process can be performed efficiently enough to support the real-time insertion of new partners looking to exploit existing services. In this work, we define an integrated process for workflow-based SLA composition when suggesting new SLAs. This process can be decomposed into two steps, SLA composition and evaluation. These two steps are illustrated in Fig. 6. The SLA composition step is similar to traditional QoS-based web service composition approaches. In the evaluation step, user preferences are used to prioritize the list of candidate service chains. Numerous dynamic programming techniques can be used to achieve this step. We present a unique approach where instead of acquiring specific QoS value expectation from the user, instead user's priorities are collected. A quality score is generated based on the user's preferences. Our evaluation shows that, in real-time operations, the composition and evaluation steps are feasible considering a large number of existing business processes. 6.1. Composition: building candidate workflows In order for SLA measures to be useful for both real-time operations and decision support, the web services workflow generation must integrate traditional web service composition with the SLA composition procedures. The SLA composition procedure must be integrated at each step and also applied to the workflow as a whole once the full process is generated. We defined the information provided by the user to be the user.predicate and the desired outcome to be the user.reqresults. The step.predicate is the set of information used to select subsequent services in the composition. At the initiation of a composition routine, the user.predicate is equivalent to the step.predicate. These relationships are illustrated in Fig. 7. We introduce an integrated procedure that combines standard web service composition with SLA composition. Table 3 shows the pseudocode of the integrated process. The composition process has a main integrated process, IntegratedComp(). The StepCompose() process occurs at each step and WorkflowChk() process occurs once the required information and actions are realized with the execution of the sequence of web services. The ComposeSLA() process is the computation mechanisms that implement the SLA aggregation procedures. 6.2. Evaluation: prioritizing SLA compositions In the prior section, candidate service chains are generated that meet the functional and SLA requirements of the user. Nevertheless, at this point, there are still multiple chains that can fulfill a capability. As such, there remains an open requirement to sort the chains based on quality and choose the best chain in the group. In order to prioritize service chains, users are asked to provide a priority, Pr, for each attribute with respect to their environment, where Rr = {Pr1, Pr2, Pr3,..., Prn}. The priorities represent the rank ordered SLA measures from greatest to least importance. The priorities relate to the corresponding set of SLA measures, where SLA = {SLA1, SLA2, SLA3,..., SLAn}. A SLA attribute has the best possible value, B. Our approach will strongly consider chains that perform favorably with respect to the user's preferences. After evaluation, GS and GW represent the quality score for the service and workflow, respectively.

240

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

Governance Governance DB

Certification Module

WS Interface

Web Service DB
WS interface

WS Interface

Historical QoS DB
WS interface

Concerns .... Module WS interface

QoS-Repos/Gov Manager

WS-Repos Manager

QoS Manager

Portal User Interface

Introduce new web service that require new SLA generation in Governance DB

Service Provider Service Consumer

Many different providers/clients on the Portal
Fig. 5. A practical operational scenario.

Require regular access to existing service that affects existing SLAs

As such, the quality score for an individual service can be defined by weighing the user's priority with respect to the ratio of the SLA measure to the best possible measure for that attribute. The calculation is defined as:  n  X SLAn -BSLAn  GS  14 Prn  BSLAn  n0 The corresponding quality score for the service chain is: GW  X
m0

[12] which focuses on decision optimization. However, in the next section, we demonstrate that this approach performs favorably as a real-time assessment during the dynamic composition process. 7. Performance evaluation of the integrated composition process In real-time operations, SLA composition will be required to occur within a reasonable response time. In this work, we evaluate expected response time for the composition and prioritization of SLA measures. Our experiments are performed on a Mobile Intel Pentium 4, 2.4 GHz, 1 GB RAM, running Windows XP and the Java Runtime Environment (JRE) 6 Update 1. An initial experiment was performed that evaluates the response when prioritizing SLA-

GS m



15

This approach relies on user-supplied priority weights and perhaps lacks the precision of standard multiple criteria decision analysis

SLA COMPOSITION

EVALUATION

Assess existing and new workflow chains

2. Prioritize predicted SLAs based on existing services with respect to user preferences
Chain1.effectiveness= WSa WSb + WSc + WSd

Chain 1
WSc

Chain2.effectiveness= WSa WSb + WSc + WSd ChainN.effectiveness= WSa WSb + WSc + WSd

WSa

WSb WSc

WSd

Chain 2
WSa WSb WSc WSd

Chain n
WSa WSb WSc WSd

Fig. 6. Two-step process for integrated SLA workflow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

241

step.predicate user.predicate WS WS

WS WS WS user.ReqResult

Fig. 7. Data flow in a composition routine.

annotated workflows of web services. Based on a set of random WSAG files (with uniformly distributed randomly generated SLA terms and their values), we created software that generates web service objects. In this context, using a uniformly distributed, randomly generated set of attributes for values is appropriate. This experimentation is a proof of concept that the SLA composition can execute fast enough to appropriately be included into the real-time composition scenarios. This is mostly an information management procedure. The authors understand that a non-uniform list of attributes may incur additional overhead. Here, the argument is made that datacentric organizations must keep their list of guarantees (as represented by attributes) uniform, such that their operations are consistent across each of their consumers. It is also not implausible that the provider use such policy to ensure a predictable processing time. Each service objects has a unique identification codes that associates it with the corresponding WSAG files. We duplicated identification codes (i.e. ids) such that increasing numbers of services have the same ids. The process of aggregating ids was done in an attempt to

Table 3 Integrated composition pseudo-code. IntegratedComp: Main integrated composition function StepCompose: Function that occurs at each step WorkflowChk : Process-Level Functional/SLA check composeSLA: Function that aggregates SLA measures PR,RenegotT,Cost : Problem Resolution, Renegotiation, Cost RTime,Up,PreNT: Run time, uptime, maintenance Rel,Rep: Reliability, reputation step.predicate: Message information required to execute service ws WSDL Object with SLA measures user.reqresults: Message information required as output of service service_chain: Candidate workflow of web services IntegratedComp { step.predicate = user.predicate WHILE (step.predicate ! = user.reqresults) { StepCompose() } WorkflowChk() } StepCompose { FOR EACH candidate ws where ws. message step. predicate THEN ADD (ws) to List b ws > FOR EACH candidate ws IN List b ws> IF ((ws.PR b = step.PR) || (ws.RenegotT b = step.RenegotT) || (ws.Cost > step.Cost) || (ws.RTime b = step.RTime) || (ws.Up b = step.Up) || (ws.PreNT b = step.PreNT) || (ws.Rel b = step.Rel) || (ws.Rep b = step.Rep)) THEN REMOVE (ws) from List b ws> ELSE { ADD ws to List b service_chain > ADD ws.outputs to List b step.predicate > } } WorkflowChk { FOR EACH service_chain IF (chain. outputs user.reqresults ) && (ComposeSLA (ws.PR, ws.RenegotT ,ws.Cost, ws.Up, ws.PreNT, ws.RTime, ws.Rep, ws.Rel ) b user.SLA)) THEN ADD (service_chain) to Candidate List }

simulate composition as a prerequisite step to the actual SLA composition. Each web service object was populated with six SLA measures. Although there are more attributes, we decided to experiment with 6 with the expectation that more attributes would scale consistently. Our experimentation searches the repository of web service objects, composes services of the same id, and then prioritizes the resulting chain. This experiment was executed on a repository with varying sizes from 100 to 100,000 services. The workflow size (number of services in a chain) was also varied. The number of services per chain is varied from 10 to 100,000 services within a repository of 1,000,000 services. Fig. 8 shows the results of this first experiment. Response time represents the average time required for the proposed algorithm to correlate the SLA attributes to generate a composite measure for a particular web service composition routine. Considering a reasonable workflow of web services of 100 interconnected services or less, a response time of 1.6 ms per service chain is promising. As a variation of the response time experimentation, we also investigated how the size of the repository affects the performance of our algorithm. Fig. 9 shows the response time of our algorithm as the repository increases. The workflow chain of the composition request is held constant at 10 services but the repository increases from 100 to 100,000. In the results of this experiment, search time is also considered. Given the largest repository of 100,000 services, the SLA composition time (including the processing time for discovering the relevant services (i.e. 10 services) is 30.2 s. Although the results are favorable in a simulated environment, the reader should understand that many other conditions with regard to performance variations and real-time factors on open systems reduce the confidence of these results. In a second experiment, we evaluated the performance by varying the number of SLA attributes that are used for calculation. In this paper, we experiment with up to six attributes, but we expect that other attributes will be required to extend this approach in the future. As such, it is important to understand the overhead associated with adding a new SLA attribute for real-time operations. Although it is understood that the performance increases as the computing hardware is improved, we are generally interested in how the approach scales as the number of attributes increases in a fixed-size repository. Fig. 10 shows the search and calculation time when varying the numbers of attributes (i.e. the number of attributes tested for each). Although this graph has a high concentration of search time (~ 90%), it is clear that the performance degrades favorably (linearly) at less than 12 ms per attribute (i.e. the calculation time increases less than 1 millisecond for the addition of each new attribute). Another variation of this experiment, also shown in Fig. 10, considers the impact of SLAs that process fewer attributes. In some cases, service agreements may only consider a subset of the total possible guarantee conditions. As such, less attributes may be stored per service (i.e. Attributes Stored). The Attributes Stored measure in Fig. 10 shows that there is only a slight advantage for service-oriented providers to be less stringent. Another variation considers increasing performance of SLA composition by limiting the number of attributes that a service composition considers. The major question is "Can runtime performance increase if the business management system only considers a subset of possible SLA attributes?". Experimentation shows that even at the most extreme case, if a service provider only allows services to be constrained by one attribute, the performance is only improved by approximately 30%. Predictably, as shown in

242

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

Performance Evaluation by Number of Services per ID (constant 100000 services with search time subtracted)
1000

SLA Composition Time (Milliseconds)

100

10

1
10 100 1000 10000

0.1

Services per ID
Fig. 8. The performance of the SLA prioritization function as the number of services per workflow increase (service discovery time excluded).

Fig. 10, the improvement in performance decreases as more attributes are considered. 8. Conclusion When provider organizations expose their services for consumption by their peers, it is important for them to understand what they are guaranteeing. Moreover, consumers that receive such commitments must abide by their own commitments such that the providers can meet their guarantees to all consumers. In this work, we introduce a method of assessing an organization based on both pending and existing SLAs. Since the estimation of QoS measures for web services has been investigated in great detail, in this paper, we make a varied contribution. Our work builds on the existing studies by considering the QoS guarantees, as captured in SLAs, such that provider and consumer concerns can be modeled independently. This work also considers that SLA assessment occurs in a separate process than standard QoS estimation at service composition time. Here, we introduce high-level criteria that can be created from the aggregation of a comprehensive list of lower-level QoS-based attributes. This variation to related work facilitates automated cross-enterprise SLA negotiation. In this paper, we define a two-step process for composing SLAs and evaluating their efficiency. Consistent with results in related

work that estimate QoS on fully operational web services, we have found through simulated experimentation that the management of third-party SLA information can be composed efficiently in parallel with the actual operational service composition. In future work, we plan to implement our approach within a real operational setting as opposed to the simulation setting that is represented in this paper. In the real operational setting, data will be produce using a variety of distributions. As such, WS-Agreement files will be appended to WSDL files and evaluated in a network environment for performance and feasibility. In addition, we plan to extend the SLA composition paradigm to protocols that mandate the discovery process within web service registries. In this way, it may be possible for adaptive software (or intelligent agents) to negotiate SLAs, in real time, while they perform on-demand, service discovery. Acknowledgment This work was benefited by the participation of Dr. M. Brian Blake in the Service Level Agreement Technical Exchange Meeting held at The MITRE Corporation on July 2006 in McLean, Virginia. In addition, the service discovery approach/software used in this work was partially funded by the National Science Foundation under award number 0548514.

Performance Evaluation by Repository Size (constant 10 services per workflow chain)
100

SLA Composition Time (Seconds)

10

1
100 1000 10000 100000

0.1

Number of Services
Fig. 9. The performance of the SLA prioritization function as the repository increases (service discovery time included).

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

243

Performance Evaluation by Attributes Stored and Attributes Tested (100,000 services, 10 services per chain)
80

SLA Composition Time (Milliseconds)

70 60 50 40 30 20 10 0 1 2 3 4 5 6

Attributes
Attributes Stored Attributes Tested

Fig. 10. The performance overhead associated with the addition of new attributes.

References
[1] P. Alipio, S. Lima, P. Carvalho, XML service level specification and validation, Proc. of the 10th IEEE Symposium on Computers and Communications (ISCC), June 2005, pp. 975980. [2] A. Andrieux, K. Czajkowski, A. Dan, K. Keahe, H. Ludwig, T. Nakata, J. Pruyne, J. Rofrano, S. Tuecke, M. Xu, "Web Service Agreements Specification (WS-Agreement)," Proposed Recommendation, Open Grid Forum (OGF) Document Number GFD-R-P.107, Mar 2007 OGF Grid Resource Allocation Agreement Protocol Working Group (GRAAP-WG)Accessible at, http://www.gridforum.org/Public_Comment_Docs/Documents/Oct-2005/WSAgreementSpecificationDraft050920.pdf. [3] J.E. Angus, On computing MTBF for a k-out-of-n: G repairable system, IEEE Transactions on Reliability 37 (3) (1988) 312313. [4] M.B. Blake, B2B electronic commerce: where do agents fit in? Proceedings at the AAAI-2002 Workshop on Agent Technologies for B2B E-Commerce/AAAI Press, Edmonton, Alberta, Canada, July 2002. [5] M.B. Blake, Decomposing composition: service-oriented software engineers, IEEE Software 24 (6) (Nov/Dec 2007) 6877. [6] M.B. Blake, A lightweight software design process for web services workflows, Proc. of the 4th IEEE International Conference on Web Services (ICWS), Sept. 2006, pp. 411418. [7] M.B. Blake, D.J. Cummings, Workflow composition of service level agreements, Proc. of the IEEE International Conference on Services Computing (SCC 2007), July 2007, pp. 138145. [8] M.B. Blake, M. Gini, Guest editorial: agent-based approaches to B2B electronic commerce, International Journal of Electronic Commerce 7 (1) (2002) 113114. [9] G. Canfora, G. M Di Penta, R. Esposito, F. Perfetto, M.L. Villani, Service composition (re)binding driven by application-specific QoS, Proc of International Conference on Service-Oriented Computing (ICSOC), 2006, pp. 141152. [10] A.J. Cardoso, "Quality of Service and Semantic Composition of Workflows," Ph.D. Dissertation, University of Georgia, 2002. [11] G. Di Modica, V. Regalbuto, O. Tomarchio, L. Vita, Dynamic re-negotiations of SLA in service composition scenarios, Proc. of the 33rd EUROMICRO Conference on Software Engineeringand Advanced Applications, August 2007, pp. 359366. [12] J. Dyer, Maut -- multiattribute utility theory in multiple criteria decision analysis: state of the art surveys, International Series in Operations Research, Management Science, vol. 78, Springer, 2005, pp. 265292. [13] T. Falkowski, S. Vob, Application service providing as part of intelligent decision support for supply-chain management, Proc. of the 36th Annual Hawaii International Conference on System Sciences (HICSS), vol. 3, 2003, p. 80. [14] M. Gillman, G. Weikum, W. Wonner, Workflow management with service quality guarantees, Proc. of ACM SIGMOD International Conference on Management of Data, 2002, pp. 223239. [15] L. Green, Service level agreements: an ontological approach, Proc. of the 8th ACM International Conference on Electronic Commerce (ICEC), August 2006, pp. 185194, Fredericton, Canada. [16] D. Greenwood, G. Vitaglione, L. Keller, M. Calisti, Service level agreement management with adaptive coordination, Proc. of the International Conference on Networking and Services (ICNS), July 2006, p. 4550, Silicon Valley, USA. [17] C.-W. Hang, M.P. Singh, Trustworthy service selection and composition, ACM Transactions on Autonomous and Adaptive Systems 6 (1) (Feb 2011). [18] L. Jin, V. Machiraju, A. Sahai, Analysis on service level agreement of web services, HP Technical Report, HPL-2002-180, June 2002, accessible at (2008), http://www. hpl.hp.com/techreports/2002/HPL-2002-180.pdf. [19] J. Ko, C.O. Kim, I. Kwon, Quality-of-service oriented web service composition algorithm and planning architecture, Journal of Systems and Software 81 (11) (November 2008) 20792090. [20] H. Ludwig, A. Keller, A. Dan, R.P. King, R. Franck, Web Service Level Agreement (WSLA) Language SpecificationAccessible at, http://www.research.ibm.com/wsla2007.

[21] M. Mecella, M. Scannapieco, A. Virgillito, R. Baldoni, T. Catarci, C. Batini, Managing data quality in cooperative information systems, Lecture Notes in Comptuer Science 2512 (2002) 486502. [22] M. Mohabey, Y. Narahari, S. Mallick, P. Suresh, S.V. Subrahmanya, An intelligent procurement marketplace for web services composition, Proc. of the IEEE/WIC/ACM International Conference on Web Intelligence, Nov. 2007, pp. 551554. [23] N.J. Muller, Managing service level agreements, International Journal of Network Management 9 (3) (May 1999). [24] F. Naurmann, U. Leser, J.C. Freytag, Quality-driven integration of heterogeneous information systems, Proc. of the 25th International Conference on Very Large Databases, September 1999, pp. 447458, Edinburgh, Scotland, UK. [25] N. Oldham, K. Verma, A.P. Sheth, F. Hakimpour, Semantic WS-Agreement partner selection, Proc. of the 15th International World Wide Web Conference (WWW), 2006. [26] F.R. Reiss, T. Kanungo, Satisfying database service level agreements while minimizing cost through storage QoS, Proc. of the IEEE International Conference on Services Computing (SCC), July 2005, pp. 1321. [27] A. Sahai, V. Machiraju, M. Sayal, A. Moorsel, F. Casati, Automated SLA monitoring for web services, Proc. of the IEEE/IFIP International Workshop on Distributed Systems: Operation and Management (DSOM), October 2002, pp. 2841, Montreal, Canada. [28] D. Skene, Lamanna, W. Emmerich, Precise service level agreements, Proc. of the 26th International Conference on Software Engineering (ICSE), 2004, pp. 179188, Edinburgh, UK. [29] I. Sorteberg, O. Kure, The use of service level agreements in tactical military coalition force networks, IEEE Communications Magazine 43 (11) (November 2005) 107114. [30] W. Sun, Y. Xu, F. Liu, The role of XML in service level agreements management, Proc. of the International Conference on Services Systems and Services Management, June 2005, pp. 11181120. [31] K.M. van Hee, L.J. Somers, M. Voorhoeve, A modeling environment for decision support systems, Decision Support Systems 7 (1) (1991) 241251. [32] G. Xiaohui, K. Nahrstedt, A scalable QoS-aware service aggregation model for peer-to-peer computing grids, Proc. of the 11th IEEE International Symposium on Higher Performance Distributed Computing (HPDC), 2002, pp. 7382. [33] J. Yan, R. Kowalczyk, J. Lin, M.B. Chhetri, S.K. Goh, J. Zhang, Autonomous service level agreement negotiation for service composition provision, Future Generation Computer Systems 23 (6) (July 2007) 748759. [34] T. Yu, K.J. Lin, Service selection algorithms for composing complex services with end-to-end QoS constraints. Proc. 3rd International Conference on Service Oriented Computing (ICSOC2005), The Netherlands Amsterdam, 2005. [35] L. Zeng, B. Benatallah, A.H.H. Ngu, M. Dumas, J. Kalagnanam, H. Chang, QoS-aware middleware for web services composition, IEEE Transactions on Software Engineering 30 (5) (May 2004) 311327. [36] Y. Zhang, K.J. Lin, J.Y.J. Hsu, Accountability monitoring and reasoning in service oriented architectures, Service-Oriented Computing and Applications 1 (2007) 3550. [37] M. zur Muehlen, J. Nickerson, K.D. Swenson, Developing web services choreography standards -- the case of REST vs. SOAP, Decision Support Systems 40 (1) (2005).

M. Brian Blake received the BS degree in electrical engineering from the Georgia Institute of Technology, Atlanta and the PhD degree in information technology with a concentration in information and software engineering from George Mason University, Fairfax, Virginia. He is currently Professor of Computer Science and Associate Dean of Engineering at the University of Notre Dame, Indiana. As a professor, he has published more than 120 journal and refereed conference papers in the domains of workflow and agent-based systems, service-oriented computing, distributed data management, and software engineering education. His investigations cover the spectrum of software engineering: design, specification, proof of correctness, implementation/experimentation, performance evaluation, and application.

244

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244 Srividya Kona Bansal received her B. Tech. degree in Computer Science from NIT (previously known as REC), Warangal, India, M.S. in Computer Science from Texas Tech Univ., Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas. She is currently an assistant professor in the College of Technology and Innovation at Arizona State University. She has over 5 years of industry experience. Her research interests include Service-Oriented Computing, Semantic Web, Software Engineering, Engineering Education, and Bioinformatics.

Ajay Bansal received his B. Tech. degree in Computer Science from NIT (previously known as REC), Warangal, India, M.S. in Computer Science from Texas Tech Univ., Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas. He is currently a lecturer in the College of Technology and Innovation at Arizona State University. He has over 3 years of industry experience. His research interests include Programming Languages, Logic Programming, Declarative Programming, Automated Reasoning, Service-Oriented Computing, Semantic Web Services, and Language-based Security.

Swarm and Evolutionary Computation 8 (2013) 3343

Contents lists available at SciVerse ScienceDirect

Swarm and Evolutionary Computation
journal homepage: www.elsevier.com/locate/swevo

Regular Paper

Economic analysis and power management of a stand-alone wind/ photovoltaic hybrid energy system using biogeography based optimization algorithm
Rajesh Kumar a,n, R.A. Gupta a, Ajay Kumar Bansal b
a b

Department of Electrical Engineering, Malaviya National Institute of Technology, J. L. N. Marg, Jaipur 302017, Rajasthan, India Poornima Institute of Engineering & Technology, ISI-02, RIICO Institutional Area, Goner Road, Sitapura, Jaipur 302022, Rajasthan, India

a r t i c l e i n f o
Article history: Received 14 February 2012 Received in revised form 15 August 2012 Accepted 16 August 2012 Available online 8 September 2012 Keywords: Hybrid energy system Wind turbine system Solar photovoltaic energy Renewable energy Remote area power generation Power generation economics

abstract
The stand-alone energy system having a photovoltaic (PV) panels or wind turbines have low reliability and high cost as compared with wind/PV hybrid energy system. In this study, Biogeography Based Optimization (BBO) algorithm is developed for the prediction of the optimal sizing coefficient of wind/ PV hybrid energy system in remote areas. BBO algorithm is used to evaluate optimal component sizing and operational strategy by minimizing the total cost of hybrid energy system, while guaranteeing the availability of energy. A diesel generator is added to ensure uninterrupted power supply due to the intermittent nature of wind and solar resources. Due to the complexity of the hybrid energy system design with nonlinear integral planning, BBO algorithm is used to solve the problem. The developed BBO Algorithm has been applied to design the wind/ PV hybrid energy systems to supply a located in the area of Jaipur, Rajasthan (India). Conventional methods require calculation at every single combination of sizing, operation strategy and the data for each variation of component needs to be entered manually and execute separately. Results show that the hybrid energy systems can deliver energy in a stand-alone installation with an acceptable cost. It is clear from the results that the proposed BBO method has excellent convergence property, require less computational time and can avoid the shortcoming of premature convergence of other optimization techniques to obtain the better solution. & 2012 Elsevier B.V. All rights reserved.

1. Introduction The public attention has remained focused on the renewable technologies as environmentally sustainable and convenient alternatives. Wind and solar power are the two most widely used renewable sources of energy among all renewable sources, since they feature definite merits as compared with the conventional fossil-fuel-fired generation. For instance, wind turbine generators (WTGs) neither generate pollution nor consume depleting fossil fuels. Photovoltaic (PV) systems produce no emissions, are durable, and demand minimal maintenance to operate [1]. Unfortunately, these renewable sources of energy are essentially intermittent and quite variable in their output. In addition, they require high capital costs. Power to off-grid location is usually supplied by a generator using diesel or petrol [2]. These generators are often available at night and only for a certain number of hours as explained by

Corresponding author. Tel.:  91 141 2713372; fax:  91 141 2529029. E-mail addresses: rkumar.ee@gmail.com (R. Kumar), ragmnit@gmail.com (R.A. Gupta), ajaykb007@gmail.com (A.K. Bansal). 2210-6502/$ - see front matter & 2012 Elsevier B.V. All rights reserved. http://dx.doi.org/10.1016/j.swevo.2012.08.002

n

Musseli et al. [3]. During the designing of a hybrid system, it is necessary to select the size of various components with the operation strategy for the long-lasting, reliable and cost-effective system [4]. Many researchers have shown that hybrid energy systems are best suited to diminish dependence on fossil fuel by using available wind speed and solar radiations [5,6]. Hybrid energy system includes photovoltaic (PV) panels and/or wind turbines and batteries, etc. These energy systems are the cost-effective solutions to meet energy requirements of remote areas [7]. Das et al. [8] suggested that Evolutionary Algorithms (EAs), due to their population-based approaches, are able to detect multiple solutions within a population in a single simulation run and have a clear advantage over the classical optimization techniques, which need multiple restarts and multiple runs in the hope that a different solution may be discovered every run, with no guarantee [9]. However, numerous evolutionary optimization techniques have been developed since late 1970s for locating multiple optima (global or local). Due to significant improvement in the capability of computers in recent years [10], evolutionary algorithms (EAs), such as genetic algorithm (GA), evolutionary programming (EP), particle swarm

34

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

optimization (PSO) [11] and differential evolution (DE) are being applied for solving various hybrid energy system optimization problems to overcome some of the drawbacks of conventional techniques [12]. Barley et al. [13] has suggested guidelines regarding main operation strategies, namely frugal discharge, load-following, the state of charge (SOC) set point and the full-power strategy. However, the SOC set point procedure is user-defined, and it is not optimized [14]. Frugal discharge is based on critical load, where if the net load is exceeding the critical load, then it is cost-effective to run the generator set. In load-following strategy, batteries are not charged by the diesel generator. Belfkira et al. [15,16] explained that diesel operating point is set to match the net load. SOC set point strategy is used to charge batteries at the user defined point from the diesel generator. Bernal-Agustin et al. [17] specifies that generator operates at full-power generation with the excess power is used to charge the batteries without dumping power. Otherwise, the generator is set to operate at the maximum point without dumping. In Full power strategy, the diesel generator is operated at full power for a minimum time at a low set point. Seeling-Hochmuth [18] had investigated the application of the genetic algorithm to solve the optimization problem with various constraints. He further suggested an optimization concept combining system sizing and operation control. Koutroulis et al. [19] used Genetic Algorithm(GA) to minimize the total system cost based on the load energy requirements. Daming et al. [20], Gupta et al. [21] and Sopian et al. [22] explained a methodology of finding optimum component sizing and operational strategy using the genetic algorithm. Dufo-Lopez et al. [23] developed a program based on genetic algorithm, called HOGA, for optimizing the configuration of a PV diesel hybrid system with AC loads and the control strategy. Hakimi et al. [24] applied PSO for multi-criterion design of the hybrid power generation system. Bansal et al. [25] use Meta Particle Swarm Optimization algorithm for finding the optimal size of the Wind/ PV energy system. Ashok developed a reliable system operation model based on Hybrid Optimization Model for Electric Renewable (HOMER) [26] found an optimal hybrid system among different renewable-energy combinations while minimizing the total lifecycle cost. Dufo-Lopez et al. [27] later improved HOGA program to include fuel cell and hydrogen in the hybrid system. However, the control strategies in HOGA are same as used in HOMER. It is focused on maximizing the renewable energy components, while trying minimizing the use of the generator to provide for the load demand. Very recently, a new optimization concept, based on biogeography has been proposed by Simon [28]. Biogeography Based Optimization (BBO) is a population-based evolutionary algorithm (EA)[29]. Biogeography is the study of the geographical natural distribution of biological organisms. In the BBO algorithm, each solution of the population is represented by a vector of integers. BBO algorithm adopts the migration operator to share information among solutions [30]. This feature is similar to other biology-based algorithms, such as Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). It makes BBO applicable to the majority of problems, where GA and PSO are applicable [31]. Simon [28] compared BBO with many other Evolutionary Algorithms on a wide set of benchmark functions. The results confirmed the excellent performance of BBO. The Markov analysis also proved that BBO outperforms GA on basic unimodal, multimodal and deceptive benchmark functions when used with low mutation rates. The versatile properties of BBO algorithm encouraged the authors to apply this algorithm to solve the non-convex, complex optimal sizing problem of hybrid energy systems. Hybrid energy system sizing is a nonlinear integral problem, which is a complex problem. The objective of this paper is to explore the application of the BBO algorithm to the hybrid energy system design problem. The combination of components

represents the sequence of the suitability index variables (SIVs), which determine the total cost of the system. After the migration operation in BBO, a SIV in the immigrated island (a bad solution) accepts the sharing information from the emigrated island (a better solution). To keep the new solution feasible, adjust the SIV which has the identical component cost [32]. The BBO algorithm has certain unique features, which overcome several demerits of the conventional methods as mentioned below (1) In BBO and PSO, the solutions survive forever although their characteristics change as the optimization process progresses. However, solutions of evolutionary-based algorithms like GA, DE etc. ``die'' at the end of each generation. Due to the presence of crossover operation in evolutionary based algorithms, many solutions, whose fitness is initially favorable, sometimes lose their quality in later stage of the process. In BBO, there is no crossover like operation as the solution gets fine-tuned gradually as the process goes on through migration operation. Elitism operation has made the algorithm more efficient in this aspect and gives an edge to BBO over other techniques. (2) In PSO, solutions are more likely to clump together in similar groups. While in the case of BBO, solutions do not have the tendency to cluster due to its new mutation operation. (3) BBO involves fewer computational steps per iteration as compared to other algorithms like GA, PSO, DE etc. Due to this, BBO results are faster in convergence. (4) In BBO, poor solutions accept a lot of new features from good ones, which may improve the quality of solutions. This is a unique feature of BBO algorithm compared to other techniques. At the same time, this makes constraint satisfaction to be much easier, compared to other algorithms. In this paper, the BBO optimization algorithm uses the static models of the wind turbine, the PV panel, the battery, the inverter and on the dynamic evaluation of the wind and solar-energy potential. BBO is used to simply solve the size of the hybrid PV/ wind energy system by considering economical and reliability constraints of the system. The new method is suitable to deal with the complex design of hybrid energy system and can avoid the local minimum trap. The developed BBO methodology has been applied to design the stand-alone hybrid wind/PV systems to power supply a varying load located in the area of Jaipur, Rajasthan (India) with geographical coordinates defined as: latitude: 26192 N, longitude: 75182 E and altitude: 431 m above sea level. This paler is organized as follows. In Section 2, the hybrid energy system and its components are explained. Section 3 describes the optimization problem of hybrid system and Section 4 explains the simplified BBO. In Section 5, detail of Case study data is presented and Section 6 shows the comparison of Hybrid Optimization Model for Electric Renewable software (HOMER) [26], Biogeography Based Optimization (BBO) [28], Genetic Algorithm (GA) [22], particle swarm optimization (PSO) [23], comprehensive learning particle swarm optimization (CLPSO) [33] and ensemble of mutation and crossover strategies and parameters in DE (EPSDE) algorithm [34] algorithms. In Section 7, the results of proposed BBO algorithm have been explained and discussed.

2. Hybrid energy systems A hybrid renewable generation system comprises of wind turbine generators (WTGs) of different types, PV panels (PV), storage batteries (SB) with diesel generator are shown in Fig. 1. In the hybrid generation system, they are integrated and complement with each other in order to meet performance targets of the generation systems and access to the most economic power generation.

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

35

Battery Bank Ibat Solar PV Panel Iren-dc Idc bus Battery Charger Wind Turdine Generator Diesel Generator Iren-ac Inverter Idie xs
Fig. 1. Hybrid PV/wind Energy System.

Zbi, where Pd(t) is the power demanded by the load at hour t, Zbi is inverter efficiency, and PDG(t) is the total power produced by the Diesel generator at hour t. For the charging process (DP(t) 4 0) and discharging process (DP(t) o 0) of the battery bank, the state of charge (SOC) can be calculated as:
Iinv

PB t  1  P B t   폛Pt =U bus nZbb nDt
AC Load

5

where Zbb is equal to the round-trip efficiency in the charging process and is equal to the 100% in the discharging process,Ubus is the DC bus voltage, and Dt is the time step which is generally 1 h. The power generated from WTG's and PV's at the time `t' i.e. total renewable power is given by following equation Ptotal t  
Wn X W1

P W t  

Sn X PV  1

PPV t 

6

2.1. Wind turbine generators (WTG) The energy and current output of the WTG for each time instant are computed based on local weather conditions and actual installation height of the turbines. Wind turbines are usually connected in parallel, not in a series. Several wind turbines can be connected in parallel to match the system current requirements. Using the wind speed at a reference height hr from the database, the velocity at a hub height for the location is estimated on an hourly basis are calculated as: vt   vr t :h=hr g 1

where Wn, Sn are the total number of wind turbine generators and photovoltaic panels respectively.

3. Description of the problem For the hybrid energy system design, the objective of optimum design is to minimize Ct(PW, PPV, PB, PDG), subject to the constraint's explained in Eqs. (14)(16). The design parameters that should be derived must include WTG capacity (PW), PV panel capacity (PPV), total battery capacity (PB), and Diesel generator capacity (PDG). minC t P W , PPV , PB , P DG   minC W  C PV  C b  C g  C r  7

where v is the wind speed at the projected height h, vr is the wind speed at reference height hr and g is the power-law exponent ( $ 1/ 7 for open land). The power generated by the wind system at any time `t' can be expressed as: P W t   ZW nZg n0:5nra nC P nAnv2 r 2

where Ct is the total system cost, CW, CPV, Cb, Cg, Cr are the total cost of wind turbine systems, photovoltaic panels, batteries, Diesel Generator and the total cost of considering the power supply reliability respectively. 3.1. The total cost of wind turbines CW 
Wn  X i1

where PW is the wind turbine power output, ZW is efficiency of wind turbine, Zg is efficiency of generator, ra is the density of air, CP is the power coefficient of wind turbine, and A is the wind turbine swept area. 2.2. Photovoltaic generation (PV) The PV sizing variable comprises of size of a PV panel and the number of strings in a PV array. The necessary number of PV panels to be connected in the series is derived by the number of panels needed to match the bus operating voltage. When matching the current requirements of the system, several PV strings which are connected in series, needs to be installed in parallel. The number of parallel PV strings is a design variable that needs optimization. The output of PV panels must include the impact of geographic location, such as solar radiation and temperature, etc. The output power of photovoltaic panels PPV(t) at any time `t' can be calculated as: P PV t   ZPV nN PV P nN PV S nV PV nIPV 3

ai Pi

r 0 1  r 0 m  omP i   repPi  1  r 0 m 1



8

3.2. Total cost of photovoltaic panels C PV   Sn  X r 0 1  r 0 m  omP j   repPj  bj Pj m 1  r 0  1 j1 9

3.3. Total cost of batteries Cb 
Bn  X k1

ck P k

r 0 1  r 0 m  omP k   repP k  1  r 0 m 1



10

where ZPV is conversion efficiency of PV panel, NPVP is the number of PV panels in parallel, NPVS is number of PV panels in series, VPV is the operating Voltage of PV panels, and IPV is operating current of PV panels. 2.3. Storage batteries (SB) The batteries are used to store the excess energy generated by hybrid system and supply energy during the low generation period. The power input to the battery bank is calculated as:

3.4. Total cost of diesel generator Cg   Dn  X r 0 1  r 0 m  om  P dl P l   rep  P   f uel  P  l l l 1  r 0 m 1 l1 11

DP t   Ptotal t  P DG t 事P Load t 

4

where Ptotal(t) is the total power produced by the renewable resources (PV panels and wind turbines) at hour t, PLoad(t)  Pd(t)/

where Wn, Sn, Bn, Dn are the number of wind generators, photovoltaic panels, batteries, diesel generators; ai, bj, ck, dl are the unit cost (Rs/kW); Pi, Pj, Pk, Pl is the power capacity; om(Pi), om(Pj), om(Pk), om(Pl) are the maintenance and operating costs; rep(Pi), rep(Pj), rep(Pk), rep(Pl) are the replacement costs corresponding to ith wind turbine, jth photovoltaic panels, kth battery, lth Diesel Generator; fuel(Pl) is the cost of fuel used in lth

36

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Diesel Generator; m is the life span of the project and r0 is the interest rate. 3.5. The total cost of other parameters The total cost of considering power supply reliability is calculated as: C r  cconEENS 12

respectively 0 r Sn r NPV ,
Pmax Pmax Pmax

0 r W n r NW , 0 r Bn r NBAT ,

16

where NPV , Pmax is maximum capacity of Photovoltaic panel, N W , Pmax is maximum capacity of Wind turbine, and NBAT , Pmax is the maximum capacity of the battery panel. 4. Biogeography-based optimization (BBO) In the science of biogeography, a habitat is an ecological area that is inhabited by particular plant or animal species and geographically isolated from other habitats. Each habitat is classified by Habitat Suitability Index (HSI). Geographical areas, which are well suited as residences for biological species are said to have a high HSI. Features that correlate with HIS include rainfall, diversity of vegetation, diversity of topographic features, land area, temperature, etc. If each of the features is assigned a value, HSI is a function of these values. Each of these features that characterize habitability is known as Suitability Index Variables (SIV). SIVs are the independent variables while HSI are the dependent variables. Habitats with high HSI have the large population and have high emigration rate m, simply by virtue of a large number of species that migrate to other habitats. The immigration rate l is low for those habitats which are already saturated with species. On the other hand, habitats with low HSI have high immigration rate l, low emigration rate m due to sparse population. The value of HIS, for low HSI habitat, may increase with the influx of species from other habitats as suitability of a habitat is the function of its biological diversity. However, if HSI does not increase and remains low, species in that habitat go extinct and this leads to additional immigration. For the sake of simplicity, it is safe to assume a linear relationship between habitats HIS, its immigration and emigration rate. These rates are same for all the habitats and depend upon the number of species in the habitats. Fig. 2 shows the relationships between fitness of habitats (number of species), emigration rate m and immigration rate l. E is the possible maximum value of emigration rate and I is the possible maximum value of immigration rate. S is the number of species in the habitat, which corresponds to fitness. Smax is the maximum number of species the habitat can support. S0 is the equilibrium value. When S  S0, the emigration rate m is equal to the immigration rate l.From Fig. 2, it is clear that island which has outstanding performance like S2 has a high emigration rate and a low immigration rate. On the other hand, island which has poor performance like S1 has a high immigration rate and a low emigration rate. The values of emigration and immigration rates are given as:

where cco is the Compensation Coefficient and EENS is the Expected Energy Not Served. In the calculation of Cr, time series and the Monte Carlo method are used. Due to this, time series is divided into many terms i.e. wind speed, light, load, etc. and then the Monte Carlo method is used to calculate the reliability of the randomly selected sample. Within the run-time T (8760 h), the EENS (kWh/year) is calculated as: EENS 
T X

Pbmin  P bSOC t 事P sup t 沙U t 

13

t1

where U(t) is a step function that is zero when the supply exceeds or equals to the demand and one if there is insufficient power during hour t; PbSOC t  is state of charge (SOC) of storage batteries during hour t; P bmin t  is the minimum permissible storage level of the battery and P sup t   P total t   P DG t 事P d t  is the surplus power during hour t. 3.6. Design constraints Due to the physical or operational limits of the target system, there is a set of constraints that should be satisfied throughout system operations for any feasible solution. 1. For any period t, the total power supplied from the hybrid energy system must supply the total demand Pd with a certain reliability criterion. This relation can be represented as: P W t   P PV t   P B t   PDG t  Z 1픒P d t  P W t   P PV t   P B t   PDG t 事P dump t  r P d t 

14

P bmin r PbSOC r P bmax 0 r P bcap r P bcapmax P bt r P bmax 15

rate

where PW, PPV, PB, PDG, Pdump, Pd are the wind power, solar power, charged/discharged battery power, diesel generator power, dumped power and total load demand respectively; R is the ratio of the maximum permissible unmet power with respect to the total load demand at each time instant. The transmission losses are not considered because the system is considered as the remotely located isolated system and do not have substantial transmission lines. The dump power is the excess power generated by the system which is not utilized for either supply the load or supplied to charge the battery. 2. The state of charge (SOC) of storage batteries P bSOC should not exceed the capacity of storage batteries P bcap and should be larger than the minimum permissible storage level Pbmin . The total storage battery capacity should not exceed the allowed storage capacity Pbcap max . The hourly charge or discharge power Pbt should not exceed the hourly inverter capacity P bmax . These constraints are defined as:

lk 

EK P

17

E=I Immigration  Emigration 

S1
3. The number of wind power generation, batteries and photovoltaic panels is subjected to the following constraints

S0 S2 number of species

Smax

Fig. 2. The model of immigration rate and emigration rate of biology.

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

37

mk  I1픊 =P

18

where K is the number of species of the kth individual, and P is the number of species. Mathematically, the concept of emigration and immigration can be represented by a probabilistic model. Let us consider the probability Ps in which habitat contains exactly S species at t. Ps changes from time t to time t  Dt as: P s t  Dt   Ps t 裟1픩s Dt 픪s Dt   Ps1 t ls1 Dt  P s  1 t ms  1 Dt 19 where ls and ms are the immigration and emigration rates when there are S species in the habitat. This equation holds because in order to have S species at the time t, one of the following conditions must hold (1) There were S species at the time t, and no immigration or emigration occurred between t and t  Dt. (2) There were (S  1) species at the time t, and one species immigrated. (3) There were (S  1) species at the time t, and one species emigrated. If time Dt is small enough so that the probability of more than one immigration or emigration can be ignored, then taking the limit of Eq. (19) as Dt-0 given by following equation 8 S0 쟈ls  ms Ps  ms  1 P s  1 > <  l  m  P  m P  l P 1 r S r Smax 1 쟈 Ps  20 s s1 s1 s s s1 s1 > : 쟈l  m P  l P S  Smax s s1 s1 s s Habitat modification (Migration) algorithm is described as:

population. The mutation rate m is inversely proportional to the solution probability, which is given as:   P m  mmax n 1 21 P max where mmax is a user-defined parameter. BBO also follows a migration and mutation step to reach global minima. The flowchart of BBO algorithm application in a hybrid system design is given in Fig. 3. The basic BBO algorithm is described as:Initialize Parameters P  population size G  Maximum number of generation Keep  Elitism parameter Pmax  Island modification probability Step 1: Initialize P randomly and species count probability of each Habitat. Step2: Evaluate the fitness for each individual in P. Step3: While The termination criterion is not met do. Step4: Save the best habitats in a temporary array. Step5: For each habitat, map the HSI to number of species S, l and m. Step6: Probabilistically choose the immigration island based on the immigration rate m. Step7: Migrate randomly selected SIVs based on the selected island in Step 6. Step8: Mutate the worst half of the population as permutation algorithm. Step9: Evaluate the fitness for each individual in P. Step10: Sort the population from best to worst. Step11: Replace worst with best habitat from temporary array. Step12: Go to step 2 for the next iteration. Step13: end while

5. Case study The developed methodology for BBO Algorithm has been applied to design the stand-alone hybrid wind/PV systems to supply a varying load located in the area of Jaipur in Rajasthan (India) with geographical coordinates defined as: latitude: 26192 N, longitude: 75182 E and altitude: 431 m above sea level. The wind speed, solar irradiance, sunshine duration and ambient temperature recorded for every hour, during the period of 1st January, 2010 to 30th December, 2010. The wind speed was measured at 30 m height. In this application, PV panels, wind turbines, battery, diesel generator and inverter have been used. The cluster of colonies is assumed to be located in a remote area with adequate sunshine, moderate to high wind speeds. The average daily load profile of the study area is shown in Fig. 4. The daily energy consumption of load is 2263 kWh/day with a 261 kW peak demand and day to day variation of 30% is introduced in the load profile. The optimal solution is verified by showing the energy profile during the period from 1st January, 2011 to 7th January, 2011. The monthly solar radiation in Jaipur, Rajasthan is between 4 and 7 kWh/m2/day, with the monthly sunshine duration ranging from 5 h/day to 8 h/day as shown in Table 1. The sunshine hour has been taken for the same duration as for the global solar radiation. These values are essential for sizing of solar energy systems. The monthly solar radiation patterns are shown in Fig. 5. The average wind speed for Jaipur, Rajasthan is between 4 and 11 m/s as shown in Table 2. The average daily wind speeds (m/s) for a year is shown in Fig. 6. The technical, economical data and study assumptions are given in Table 3.

Habitat mutation algorithm is described as:

For each SIV in each solution, it is decided probabilistically, whether or not to immigrate. If immigration is selected for a given solution feature, the emigrating habitat is selected for a given solution probabilistically, using the roulette wheel normalized by m. The mutation operator is probabilistically applied to the habitat, which tends to increase the biological diversity of the

38

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Fig. 3. BBO optimization algorithm for hybrid energy system design.

200 Daily Load (KW) 150 100 50 0 2 4 6 8 10 12 14 Time (Hours) 16 18 20 22 24

Fig. 4. Daily load profile.

6. Comparison of BBO with other algorithms The hybrid energy system optimization is performed on an Intel Core 2 Duo PC with 2.1 (GHz) processor speed, 2 (GB) RAM and Windows 7 operating systems. The experiments are

performed using the MATLAB R2010b program. The BBO results are compared with the HOMER, GA, PSO, CLPSO and EPSDE. For the sake of comparison of performance between the various algorithms, the stopping criterion is set at same. Mutation rate  0.7 and Cross over rate  0.3 are used to get the best results

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

39

for GA. For PSO, C1  1.8, C2  2.2, W starts at 1 and decreases until reaching 0 at the end of the run. In CLPSO, C  1.5 is selected and in EPSDE, CR  0.9, F  0.5 gives the best results. The mating is performed using single point crossover. Fig. 7 depicts convergence of five optimization algorithms for the combination of the hybrid energy system in which PV, WTG, Diesel generator, battery and inverter are present. For comparison of different algorithms, the initial solution points are taken constant. It can be seen that the fitness value decreases rapidly in the first 10 generations. During this stage GA, PSO, CLPSO, EPSDE and BBO concentrate mainly on finding feasible solutions to the problem. Then the value decreases slowly, and they have been converged approximately at around 20 iterations. Consequently, the total system cost, components size has been almost same in BBO, PSO, CLPSO, EPSDE and GA. More details about convergence and optimal solutions are given in Tables 4 and 5. It observed from the Tables 4 and 5 that HOMER, PSO, GA, CLPSO, EPSDE and BBO are able to find the optimum design parameters of the stochastic simulation model. It can be easily seen that BBO algorithm is more rapid and give minimum cost as compared to GA, PSO, CLPSO and EPSDE. Therefore, the proposed BBO-based optimization procedure can comfortably, rapidly approach the optimum state for a large-scale complex simulation of a hybrid energy system. The total cost of the optimized hybrid energy system showed that the system can deliver energy in a stand-alone installation with an acceptable cost. The Homer software with a combination of various components and strategies variables of 38 million would require the calculation time of approximately 15 h to evaluate each combination. The proposed BBO algorithm not only reduces the demerits of HOMER but also uses a certain number of combinations. The proposed BBO has reduced 15 h of calculation time around 0.73 h
Table 1 Monthly global solar radiation (kWh/m2/day) and sunshine hours at Jaipur (Rajasthan). Month Solar radiation (kWh/m2/d) 4.128 4.882 5.717 6.427 6.812 6.830 5.748 5.269 5.756 5.329 4.382 3.866 Sunshine hours (h/day) 5.59 6.14 6.13 6.11 5.87 6.09 6.04 5.52 6.77 5.52 6.23 5.82

on Intel Core 2 Duo PC for complete hybrid energy system optimization.

7. Results and discussions The mathematical modeling is driven by HOMER, the results of HOMER software can be used for comparison and point of reference. The optimization results using HOMER software are shown in Table 6. The biogeography-based optimization algorithm derived the results as shown in Table 7. The total power generated by renewable sources seemed enough, except for its failure to provide the necessary power at peak time, which requires the support of battery and inverter. Optimization calculations obtained by HOMER are slightly different as compared with BBO. However, use of HOMER will be a problem, when the calculation of different types of component needs to be calculated simultaneously. There are three significant disadvantages of HOMER: 1. HOMER requires calculation of every single combination of sizing and operation strategy. 2. The data for each variation of component needs to be entered manually and execute separately. 3. HOMER generally uses diesel generator, so hybrid system cost increases due to increase in fuel intake. As data involved are large and sensitivity analysis needs to be done for selected components, it is unlikely that HOMER can provide fast and reliable solutions for the optimization. After lot of hit and trial, the following parameters are taken for BBO algorithm:

 

No. of habitats or population Generation

: 50, : 50,

Table 2 Monthly wind speed at Jaipur (Rajasthan). Month January February March April May June July August September October November December Wind speed (m/s) 6.312 5.433 4.627 4.993 10.335 10.717 8.413 8.754 9.776 5.850 4.443 5.931

January February March April May June July August September October November December

Average Daily Solar Radiation

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 2 4 6 8 10 12 14 Time (Hours) 16 18 20 22 24
January February March April May June July August September October November December

Fig. 5. Average solar radiation monthly data for 1 year.

40

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Average Daily wind speed

7 6 5 4 3 2 1 0 0 50 100 150 200 Time (Day) 250 300 350

Fig. 6. Average daily wind speed (m/s).

Table 3 Technical data and study assumptions of hybrid energy system. Description PV Capital cost Lifetime Operation and maintenance cost Replacement cost Wind turbine Rated power Capital cost Lifetime Operation and maintenance cost Cut-in speed (m/s) Vci Cut-out speed (m/s) Vco Hub height Replacement cost Diesel generator units Capital cost Rated power of each diesel unit 1 (D1, D2) Minimum allowed power (min load ratio) Operation and maintenance cost Operating hours Replacement cost Batteries Type of batteries Nominal voltage (V) Nominal capacity Nominal energy capacity of each battery Operation and maintenance cost Dispatch/operating strategy Capital cost Replacement cost Converter Capital cost Operation and maintenance cost Lifetime Replacement cost Spinning reserve Minimum renewable fraction Annual interest rate Project life time Data

200,000 Rs/kW 25 years 1000 Rs/kW/year 200,000 Rs/kW Variable (0300 KW) AC 65,000 Rs/kW 25 years 1000 Rs/kw/year 23 25 30 m 65,000 Rs/kW 20,000 Rs/kW Variable (0100 MW) 30% of rated power 1 Rs/h/kW 16,000 h

Tarjan L16P 6V 360 Ah 2.16 kWh 100 Rs/year Multiple diesel load following 10,000 Rs 4000 Rs 50,000 Rs/kW 100 Rs/year/kW 10 years 50,000 Rs/kW 10% 60% 10% 25 years

     

Number of SIVs per habitat Habitat modification probability Island Mutation probability Elitism parameter Maximum emigration Rate E Maximum immigration Rate I

: 20, : : : : : 1, 0.05, 2, 1, 1.

The total cost for the minimization of the hybrid system is achieved by selecting an appropriate system configuration. In Table 7, the optimal sizing results consisting of device numbers and the total cost of the hybrid wind/PV system have been

presented. The comparison of HOMER and BBO results are shown in Table 8. These results verify that the hybrid energy system has a lower total cost as compared to the stand-alone systems. Reliability of the hybrid energy system is much higher than the other systems, and the output is not very much affected by changes in weather conditions. The combination of wind in a hybrid energy system reduces the battery bank and diesel requirements, and PV array has the high capital cost per kW but very less operating and maintenance cost. In order to study the hourly behavior of the power exchange in the hybrid energy system, the simulation results were conducted during 1st to 7th January, 2011, for the optimal configuration obtained from BBO algorithm (Case 1 of the Table 7), are shown in Fig. 8. It shows the power supply from the renewable sources, power demand and input/output power to the battery bank. Diesel Generator is used only when the renewable sources and the batteries are not able to satisfy the load demand. Without much operation reserve, diesel generator can also supply the load demand independently but at much higher cost with Cost of Energy (COE) of Rs. 27.69/kWh. The generator uses 457,100 l of diesel and operating for 8760 h annually, which is almost half of its lifetime operating hours. In all the calculations, the cycle charging process is selected as operation strategy. Two other operation strategies, load following and set point state of charge at 80% can be considered. Since the initial finding lacking input constraints, cycle charging process is enough to guide the selection and distribution of power. Fig. 9 shows the annualized cost of the hybrid energy system (Case 2 of Table 7), which have PV, wind turbine, diesel generator, battery and Inverter connected to the system. Wind turbine contributes 31%, battery costs about 11%, PV system contributed 11%, Diesel generator costs 10%, fuel costs around 30% and inverter at 7% of the total annual cost of Rs. 67,877,356. PV panels and Wind system are assumed to last the lifetime of the project, i.e. for 15 years, while the battery and inverter needs to be replaced after a certain number of hours of operation and discharging respectively. The cost of battery and inverter plays an important part for determining the TNPC and COE. Except for three combinations, the system requires battery and inverter as part of the hybrid system for storing the excess energy generated by the system. As global price of the oil is increasing and the Government of India has indicated that it can no longer provide the oil subsidy. It is important to note that if the true diesel price is used in the calculation, the COE is going up by Rs. 0.1per kWh for 1 Rs/litre increase in diesel price. The sensitivity results for diesel price are shown in Table 9. If diesel price is raised to Rs 50 per litre, the COE is increased by 6%. To check the sensitivity of the results to variations in average wind speed from year to year, the hybrid energy system in Case

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

41

9 8 Generation Cost (Rs/kWh) 7 6 5 4 3 2 1 0

x 108
GA (69932935) PSO (71852619) BBO (67877356) CLPSO (69130095) EPSDE (69240274)

10

20

30

40 50 60 Number of iterations

70

80

90

100

Fig. 7. Convergence characteristics of various optimization algorithms.

Table 4 Optimization results from various optimization algorithms. Algorithm HOMER GA PSO BBO CLPSO EBSDE PV (kW) 55 53 52 52 52 52 WTG 1 1 1 1 1 1 DG (kW) 180 168 165 163 164 164 Battery 800 808 813 813 814 813 Inverter (kW) 100 93 93 93 93 93 Net present costs 89,352,280 71,852,619 69,932,935 67,877,356 69,130,095 69,240,274 Energy produced 1,243,027 1,088,797 1,067,294 1,059,639 1,067,894 1,068,247 COE 14.23 11.45 11.14 10.81 11.01 11.03

Table 5 Comparison of optimization algorithms. Optimization technique HOMER GA PSO BBO CLPSO EBSDE Population size  50 50 50 50 50 Iteration number 21875 100 100 100 100 100 Number of runs 10 10 10 10 10 10 Average time (s) 1648 125 110 82 89 102 Total cost (Rs) 89,352,280 71,852,619 69,932,935 67,877,356 69,130,095 69,240,274 Comments  Pm  0.7, Pc  0.3 C1  1.8, C2  2.2 E  1, I  1 C  1.5 CR  0.9, F  0.5

Table 6 Complete optimization results received through Homer Software. SN 1 2 3 4 5 6 7 8 9 PV (kW) 0 55 350 0 60 600 0 450 0 Wind (units) 1 1 0 2 2 0 0 0 6 Diesel (kW) 160 180 200 220 220 0 250 240 0 Battery (units) 900 800 1300 0 0 4700 0 0 24,000 Inverter (kW) 100 100 200 0 100 250 0 100 300 Net present costs (Rs) 89,295,624 89,352,280 125,044,944 128,571,824 136,609,248 138,103,712 153,526,384 170,852,800 357,250,624 Energy produced (kWh) 1,195,797 1,243,027 1,130,267 1,995,859 2,093,897 1,271,608 888,155 1,588,055 4,612,242 COE (Rs/kWh) 14.21 14.23 19.9 20.46 21.74 21.99 24.44 27.19 56.86

Table 7 Complete optimization results received through proposed BBO algorithm. SN 1 2 3 4 5 6 7 8 9 PV (kW) 0 52 284 0 51 557 0 437 0 Wind (units) 1 1 0 1 1 0 0 0 4 Diesel(kW) 147 163 176 210 201 0 260 245 0 Battery (units) 906 813 1239 0 0 4289 0 0 28,926 Inverter (kW) 92 93 212 0 104 261 0 103 321 Net present costs (Rs) 65,408,516 67,877,356 131,414,657 85,046,660 83,891,664 129,015,885 173,812,087 125,171,224 347,414,450 Energy produced (kWh) 1,135,910 1,059,639 929,470 1,110,834 1,140,773 1,180,840 931,781 1,182,069 3,074,832 COE (Rs/kWh) 10.42 10.81 20.94 13.55 13.37 20.55 27.69 19.94 55.35

42

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Table 8 Comparison of optimization results received through HOMER and proposed BBO algorithm. SN 1 2 3 4 5 6 7 8 9 Components Wind  DG  Battery PV  Wind  DG  Battery PV  DG  Battery Wind  DG PV  Wind  DG PV  Battery DG PV  DG Wind  Battery NPC by HOMER (Rs) 89,295,624 89,352,280 125,044,944 128,571,824 136,609,248 138,103,712 153,526,384 170,852,800 357,250,624 Energy produced (kWh) 1,195,797 1,243,027 1,130,267 1,995,859 2,093,897 1,271,608 888,155 1,588,055 4,612,242 COE (Rs/kWh) 14.21 14.23 19.9 20.46 21.74 21.99 24.44 27.19 56.86 NPC by HOMER (Rs) 65,408,516 67,877,356 131,414,657 85,046,660 83,891,664 129,015,885 173,812,087 125,171,224 347,414,450 Energy produced (kWh) 1,135,910 1,059,639 929,470 1,110,834 1,140,773 1,180,840 931,781 1,182,069 3,074,832 COE Rs/kWh 10.42 10.81 20.94 13.55 13.37 20.55 27.69 19.94 55.35

Table 10 Economic sensitivity to wind speed variations in Case 1 of Table 7. Case Average wind speed (m/s) 6.0 7.2 8.4 Diesel saving (kl) 179 265 336 COE (Rs/kWh)

 17.5% Baseline  17.5%

11.63 10.42 9.58

lower than the present measurement year, COE rose by 11.6%. With the wind speed 17.5% higher, COE dropped by 8.06%.

8. Conclusion
Fig. 8. Power management indicating load power, wind power output, diesel generator output, input/output power of the battery bank of the hybrid System in Case 1 of Table 6.

Fig. 9. Percentage share of components cost in Total net present cost for Hybrid energy system as given in Case 2 of Table 6.

Table 9 Economic sensitivity of diesel cost in Case 1 of Table 7. Diesel price (Rs) 43 46 50 55 Tot NPC (Rs) 65,408,516 67,084,720 69,319,658 72,113,332 COE (Rs/kWh) 10.42 10.69 11.04 11.49

Stand-alone hybrid energy systems are more suitable than stand-alone systems that only have one energy source for the supply of electricity to off-grid applications, especially in remote areas with difficult access. However, the design, control, and optimization of the hybrid energy systems are usually very complex tasks. Power supply reliability under varying weather conditions and the corresponding system cost are the two major concerns in designing PV and/or wind turbine systems. In order to utilize renewable energy resources efficiently and economically, one optimum sizing method is developed in this paper based on a Biogeography Based Optimization (BBO), which has the ability to attain the global optimum with relative computational simplicity compared to the conventional optimization methods. The system configuration, characteristics of the main components, overall sizing, control and power management strategy for the hybrid energy system have been presented. The wind and PV generation systems are the main power generation devices, and the battery acts as a storage device for excess power. The developed methodology is based on the use of long-term data of wind speed, solar irradiance and ambient temperature. The BBO algorithm manages to optimize the size and the operation strategy for a simple daily load. Furthermore, a numerical example (Case study) is used to demonstrate the applicability, power management and usefulness of the proposed method. References
[1] J. Lagorse, D. Paire, A. Miraoui, Sizing optimization of a stand-alone street lighting system powered by a hybrid system using fuel cell, PV and battery, Renewable Energy 34 (3) (2009) 683691. [2] B.S. Borowy, Z.M. Salameh, Methodology for optimally sizing the combination of a Battery Bank and PV array in a wind/PV hybrid system, IEEE Transactions on Energy Conversion 11 (2) (1996) 367375. [3] M. Musseli, G. Notton, A. Louche, Design of hybrid-photovoltaic power generator, with optimization of energy management, Solar Energy 65 (3) (1999) 143157.

1 of Table 7 is run with the wind speeds adjusted upward and downward by 17.5%, which is the inter-annual variability (one standard deviation) found in the historical wind measurements. The results are shown in Table 10. With the wind speed 17.5%

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

43

[4] D.B. Nelson, M.H. Nehrir, C. Wang, Unit sizing and cost analysis of standalone hybrid wind/PV/fuel cell power generation systems, Renewable Energy 31 (2006) 16411656. [5] A.N. Celik, Optimization and techno-economic analysis of autonomous photovoltaic-wind-hybrid energy systems in comparison to single photovoltaic and wind systems, Energy Conservation and Management 43 (18) (2002). [6] H. Yang, L. Lu, W. Zhou, A novel optimization sizing model for hybrid solarwind power generation system, Solar Energy 81 (2007) 7684. [7] Xu Daming, Kang Longyun, Cao Binggang, Stand-alone hybrid wind/PV power system using the NSGA2, Acta energiae solar issinica 27 (6) (2006) 593598. [8] S. Das, S. Maity, Bo-Yang Qu, P.N. Suganthan, Real-parameter evolutionary multimodal optimization  A survey of the state-of-the-art, Swarm and Evolutionary Computation 1 (2) (2011) 7188. [9] E. Mezura-Montes, C.A. Coello, Constraint-handling in nature-inspired numerical optimization: Past, present and future, Swarm and Evolutionary Computation 1 (4) (2011) 173194. [10] G.C. Seeling-Hochmuth, Optimization of hybrid energy systems sizing and operation control, PhD Thesis, University of Kassel, 1998. [11] A.M. El-Zonkoly, Optimal placement of multi-distributed generation units including different load models using particle swarm optimization, Swarm and Evolutionary Computation 1 (1) (2011) 5059. [12] W.D. Kellogg, M.H. Nehrir, Generation unit sizing and cost analysis for standalone wind, photovoltaic, and hybrid Wind/PV systems, IEEE Transactions on Energy Conversion 13 (1) (1998) 7075. [13] C.D. Barley, C.B. Winn, Optimal dispatch strategy in remote hybrid power systems, Solar Energy 58 (4-6) (1996) 165179. [14] R. Chedid, S. Rahman, Unit sizing and control of hybrid wind-solar power systems, IEEE Transactions on Energy Conversion 12 (1) (1997) 7985. [15] R. Belfkira, G. Barakat, C. Nichita, Sizing optimization of a stand-alone hybrid power supply unit: wind/PV system with battery storage, International Review of Electrical Engineering (IREE) 3 (5) (2008). [16] R. Belfkira, O. Hajji, C. Nichita, G. Barakat, Optimal sizing of stand-alone hybrid wind/PV system with battery storage, Proceeding of 2007 European Conference on Power Electronics and Applications, September, 2007, pp.110. [17] J.L. Bernal-Agustin, R. Dufo-Lopez, Efficient design of hybrid renewable energy systems using evolutionary algorithms, Energy Conversion and Management 50 (3) (2009) 479489. [18] G.C. Seeling-Hochmuth, A combined optimization concept for the design and operation strategy of hybrid-PV energy systems, Solar Energy 61 (2) (1997) 7787. [19] E. Koutroulis, D. Kolokotsa, A. Potirakis, K. Kalaitzakis, Methodology for optimal sizing of stand-alone photovoltaic/wind-generator systems using genetic algorithms, Solar Energy 80 (9) (2006) 10721088.

[20] Daming Xu, Longyun Kang, Liuchen Chang, Binggang Cao, Optimal sizing of standalone hybrid wind/PV power systems using genetic algorithms, Proceeding of Canadian Conference on Electrical and Computer Engineering, May 2005, pp.17221725. [21] R.A. Gupta, Rajesh Kumar, Ajay Kumar Bansal, A new methodology for optimizing the size of hybrid PV/wind system using genetic algorithms, Proceeding of National Conference on Recent Advances in Electrical and Electronics (RAEEE-09), Dec 2324, 2009, pp. 133139. [22] Kamaruzzaman Sopian, Azami Zaharim, Yusoff Ali, Zulkifli Mohd Nopiah, Juhari Ab. Razak, Nor Salim Muhammad, Optimal operational strategy for hybrid renewable energy system using genetic algorithms, WSEAS Transactions On Mathematics 7 (4) (2008) 130140. [23] R. Dufo-Lopez, J.L. Bernal-Agustin, Design and control strategies of PV-Diesel systems using genetic algorithms, Solar Energy 79 (2005) 3346. [24] M. Hakimi, M. Moghaddas, Optimal sizing of a stand-alone hybrid power system via particle swarm optimization for Kahnouj area in southeast of Iran, Renewable Energy 34 (7) (2009) 18551862. [25] Ajay Kumar Bansal, R.A. Gupta, Rajesh Kumar, Optimization of hybrid PV/ wind energy system using meta particle swarm optimization (MPSO), Proceeding of India International Conference on Power Electronics, January 2830, 2011, pp. 1-6. [26] HOMER, /http://www.homerenergy.comS. [27] R. Dufo-Lopez, J.L. Bernal-Agustin, Optimization of control strategies for stand-alone renewable energy systems with hydrogen storage, Renewable Energy 32 (2007) 11021126. [28] D. Simon, Biogeography-based optimization, IEEE Transaction of Evolutionary Computing 12 (6) (2008) 702713. [29] A.P. Engelbrecht, Fundamentals of Computational Swarm Intelligence, Wiley, 2005. [30] S. Rajasomashekar, P. Aravindhababu, Biogeography based optimization technique for best compromise solution of economic emission dispatch, Swarm and Evolutionary Computation 15 (June 2012). (Available online. [31] Harish Kumar Urvinder Singh, Tara Singh Kamal, Design of Yagi-Uda antenna using biogeography based optimization, IEEE Transactions on Antennas and Propagation 58 (10) (2010) 33753379. [32] K. Jamuna, K.S. Swarup, Biogeography based optimization for optimal meter placement for security constrained state estimation, Swarm and Evolutionary Computation 1 (2) (2011) 8996. [33] J.J. Liang, A.K. Qin, P.N. Suganthan, S. Baskar, Comprehensive learning particle swarm optimizer for global optimization of multimodal functions, IEEE Transactions on Evolutionary Computing 10 (2006) 281295. [34] R. Mallipeddi, P.N. Suganthan, Q.K. Pan, M.F. Tasgetiren, Differential evolution algorithm with ensemble of parameters and mutation strategies, Applied Soft Computing 11 (2010) 16791696.

Semantics-based Web Service Composition engine
Srividya Kona, Ajay Bansal, Gopal Gupta Department of Computer Science The University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp. 2400 Dallas Parkway Plano, TX 75093

Abstract
Service-oriented computing is gaining wider acceptance. We need an infrastructure that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper we present an approach for automatic service discovery and composition based on semantic description of Web services. The implementation will be used for the WS-Challenge 2007 [1].

position problem. We describe our Service Composition algorithm in section 3. Section 4 presents the design of our software with brief descriptions of the different components of the system followed by conclusions and references.

2. Automated Web service Discovery and Composition
Discovery and Composition are two important tasks related to Web services. In this section we formally describe these tasks. We also develop the requirements of an ideal Discovery/Composition engine.

1. Introduction
In order to make services ubiquitously available, we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis [6]. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. For discovery and composition, one could take the syntactic approach in which the services being sought in response to a query simply have their inputs syntactically match those of the query. Alternatively, one could take the semantic approach in which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the matching process. Several efforts are underway to build an infrastructure for service discovery, composition, etc. These efforts include approaches based on the semantic web (such as USDL [4], OWL-S [7], WSML [8], WSDL-S [9]) as well as those based on XML, such as Web Services Description Language (WSDL [5]). Approaches such as WSDL are purely syntactic in nature, that is, they only address the syntactical aspects of a Web service. In this paper we present our approach for automatic service composition which is an extension of our implementation that we used at WSChallenge 2006 [3]. In section 2 we present the formal definition of the Com-

2.1. The Discovery Problem
Given a repository of Web services, and a query requesting a service (we refer to it as the query service in the rest of the text), automatically finding a service from the repository that matches these requirements is the Web service Discovery problem. Valid solutions to the query satisfy the following conditions: (i) they produce at least the query output parameters and satisfy the query post-conditions; (ii) they use only from the provided input parameters and satisfy the query pre-conditions; (iii) they produce the query side-effects. Some of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre/post conditions, and side-effects requirements.

2.2. The Composition Problem
Given a repository of service descriptions, and a query with the requirements of the requested service, if a matching service is not found, then the composition task can be performed. The composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 1 shows an example composite service made up of five services  to . In the figure,   and   are the query input parameters and pre-conditions respectively. 퓬 and 퓬 are the query





output parameters and post-conditions respectively. Informally, the directed arc between nodes and indicates that outputs of constitute (some of) the inputs of . Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition where the number of services involved in composition is exactly equal to one.









the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem.

S2 S5 CI',I' S1 S3 S4 CO',O'

Figure 1. Example of a Composite Service as a Directed Acyclic Graph Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and =( ) is the reprepost-conditions. sentation of a service where is the pre-conditions, is the input list, is the service's side-effect, is the affected object, is the output list, and is the postconditions. Definition (Repository of Services): Repository is a set of Web services. Definition (Query): The query service is defined as     = (    ) where is the pre  conditions, is the input list, is the service affect,   is the affected object,  is the output list, and is the post-conditions. These are all the parameters of the requested service. Definition (Composition): The Composition problem can be defined as automatically finding a directed acyclic graph =  of services from repository , given query =    (    ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: where has zero incoming edges, 1.  ,  . 2. where has zero outgoing edges,   , . 3. where has at least one incoming edge, let ,  , ...,  be the nodes such that there is a directed edge from each of these nodes to . Then     (  ).  ,    The meaning of the is the subsumption (subsumes) reis the implication relation. Figure 2 explains lation and one instance of the composition problem pictorially. When



좋





피 

Figure 2. Composite Service







2.3

Requirements of an ideal Engine

The features of an ideal Discovery/Composition engine are: Correctness: One of the most important requirement for an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the requirements of the query. Also, the engine should be able to find all services that satisfy the query requirements. Small Query Execution Time: Querying a repository of services for a requested service should take a reasonable amount of (small) time, i.e., a few milliseconds. Here we assume that the repository of services may be pre-processed (indexing, change in format, etc.) and is ready for querying. In case services are not added incrementally, then time for pre-processing a service repository is a one-time effort that takes considerable amount of time, but gets amortized over a large number of queries. Incremental Updates: Adding or updating a service to an existing repository of services should take a small amount of time. A good Discovery/Composition engine should not pre-process the entire repository again, rather incrementally update the pre-processed data (indexes, etc.) of the repository for this new service added. Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition engine should be able to give results based on requirements (minimize, maximize, etc.) over the costs. We can extend this to services having an attribute vector associated with them and the engine should be able to give results based on maximizing or minimizing functions over this attribute vector. These requirements have driven the design of our

 좋    좋

피  





피 







     湃          密      앓     密      





semantics-based Composition engine described in the following sections.

S1 Query described using USDL (S) Infer Sub-queries . . . Sn

Discovery Module (Discovery Engine + Service Directory + Term Generator) S1 Sn

3. A Multi-step Narrowing Solution
We assume that a directory of services has already been compiled, and that this directory includes semantic descriptions for each service. In this section we describe our Service Composition algorithm. Service Composition Algorithm: For service composition, the first step is finding the set of composable services. The correct sequence of execution of these services can be determined by the pre-conditions and post-conditions of the individual services. That is, if a subservice  is composed with subservice  , then the post-conditions of  must imply the pre-conditions of . The goal is to derive a single solution, which is a directed acyclic graph of services that can be composed together to produce the requested service in the query. Figure 4 shows a pictorial representation of our composition engine. In order to produce the composite service which is represented by a graph as shown in figure 1, we filter out services that are not useful for the composition at multiple stages. Figure 3 shows the filtering stages for the particular instance shown in figure 1. The composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. In figure 3,   are the pre-conditions and the input parameters provided by the query.  and  are the services found after step 1.  is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e.,  =   .  is used to find services at the next stage, i.e., all those services that require a subset of . In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

.......................... Composition Engine (implemented using Constraint Logic Programming

Composed Service

Pre-Cond(S) S1 Pre-Cond( S) 1

Post-Cond( S) 1 Pre-Cond( S) 2

S2

................................. S n

Post-Cond( S) n Post-Cond(S)









Figure 4. Composition Engine Pre-Cond, QCO - Post-Cond Output: Result - ListOfServices 1. L NarrowServiceList(QI, QCI); 2. O GetAllOutputParameters(L); 3. CO GetAllPostConditions(L); 4. While Not (O QO) 5. I = QI O; CI QCI CO; 6. L' NarrowServiceList(I, CI); 7. End While; 8. Result RemoveRedundantServices(QO, QCO); 9.Return Result;









4. Implementation
Our composition engine is implemented using Prolog [10] with Constraint Logic Programming over finite domain [11], referred to as CLP(FD) hereafter. In this section we briefly describe our software system and its modules. The details of the implementation along with performance results are shown in [2]. Triple Generator: The triple generator module converts each service description into a triple as follow: (Pre-Conditions, affect-type(affected-object, I, O), PostConditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can be treated as terms in first-order logic. In case conditions on a service are not provided, the Pre-Conditions and Post-Conditions in the triple will be null. Similarly if the affect-type is not available, this module assigns a generic affect to the service. Query Reader: This module reads a query file (in XML format, possibly different from the XML format used for a









I=I CI, I
1

S1 S2 . .

O1

I=IUO
2 1

1

S . .

O
3

2

I=IUO
3 2

2

O S . .
4

3

I=IUO
4 3

3

S . .

O
5

4

O

Figure 3. Composite Service Algorithm: Composition Input: QI - QueryInputs, QO - QueryOutputs, QCI -

service) and converts it into a triple used for querying in our engine. Semantic Relations Generator: We obtain the semantic relations from the provided ontology. This module extracts all the semantic relations and creates a list of Prolog facts. Composition Query Processor: The composition engine is written using Prolog with CLP(FD) library. It uses a repository of facts, which contains list of services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition engine:
composition(sol(Qname,A)) :dQuery(Qname,_,_), minimize(compTask(Qname,A,SeqLen),SeqLen). compTask(Qname, A, SeqLen) :dQuery(Qname,QI,QO), encodeParam(QO,OL), narrowO(OL,SL), fd_set(SL,Sset), fdset_member(S_Index,Sset), getExtInpList(QI,InpList), encodeParam(InpList,IL), list_to_fdset(IL,QIset), serv(S_Index,SI,_), list_to_fdset(SI,SIset), fdset_subtract(SIset,QIset,Iset), comp(QIset,Iset,[S_Index],SA,CompLen), SeqLen #= CompLen + 1, decodeS(SA,A). comp(_, Iset, A, A, 0) :- empty_fdset(Iset),!. comp(QIset, Iset, A, SA, SeqLen) :fdset_to_list(Iset,OL), narrowO(OL,SL), fd_set(SL,Sset), fdset_member(SO_Index,Sset), serv(SO_Index,SI,_), list_to_fdset(SI,SIset), fdset_subtract(SIset,QIset,DIset), comp(QIset,DIset,[SO_Index|A],SA,CompLen), SeqLen #= CompLen + 1.

5. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure to publish services, document services and query repositories for matching services. We presented our approach for Web service composition. Our composition engine can find a graph of atomic services that can be composed to form the desired service as opposed to simple sequential composition in our previous work [3]. Given semantic description of Web services, our solution produces accurate and quick results. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. The use of Constraint Logic Programming (CLP) helped greatly in obtaining an efficient implementation of this system. We used a number of built-in features such as indexing, set operations, and constraints and hence did not have to spend time coding these ourselves. These CLP(FD) built-ins facilitated the fast execution of queries.

References
[1] WS Challenge 2007 http://ws-challenge. org. [2] S. Kona, A. Bansal, G. Gupta, and T. Hite. Efficient Web Service Discovery and Composition using Constraint Logic Programming. In ALPSWS Workshop at FLoC 2006. [3] S. Kona, A. Bansal, G. Gupta, and T. Hite. Web Service Discovery and Composition using USDL. In CEC/EEE, June 2006. [4] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005. [5] Web Services Description Language. http://www. w3.org/TR/wsdl. [6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp. 46-53, March 2001. [7] OWL-S www.daml.org/services/owl-s/1. 0/owl-s.html. [8] WSML: Web Service Modeling Language. www. wsmo.org/wsml/. [9] WSDL-S: Web Service Semantics. http://www. w3.org/Submission/WSDL-S. [10] L. Sterling and S. Shapiro. The Art of Prolog. MIT Press, 1994. [11] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

The query is converted into a Prolog query that looks as follows: composition(queryService, ListOfServices). The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the built-in, higher order predicate "bagof" to return all possible ListOfServices that can be composed to get the requested queryService. Output Generator: After the Composition Query processor finds a matching service, or the graph of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files in any desired XML format. For the WS-Challenge, this module will produce output files in the format provided [1]. For this year's challenge, the software has to receive requests and return results via SOAP. Hence our software will work as a Web service whose interface will accept the discovery/composition query.

Towards Predicate Answer Set Programming via Coinductive Logic Programming
Richard Min, Ajay Bansal, Gopal Gupta
Department of Computer Science The University of Texas at Dallas, Richardson, Texas, U.S.A.

Abstract Answer Set Programming (ASP) is a powerful paradigm based on logic programming for non-monotonic reasoning. Current ASP implementations are restricted to "grounded range-restricted function-free normal programs" and use an evaluation strategy that is "bottom-up" (i.e., not goal-driven). Recent introduction of coinductive Logic Programming (co-LP) has allowed the development of topdown goal evaluation strategies for ASP. In this paper we present this novel goaldirected, top-down approach to executing predicate answer set programs with coLP. Our method eliminates the need for grounding, allows functions, and effectively handles a large class of predicate answer set programs including possibly infinite ones.

1

Introduction

Answer Set Programming (ASP) [1,2] is a powerful and elegant way for incorporating non-monotonic reasoning into logic programming (LP). Many powerful and efficient ASP solvers such as Smodels [3,4], DLV, Cmodels, ASSAT, and NoMoRe have been successfully developed. However, these ASP solvers are restricted to "grounded version of a range-restricted function-free normal programs" since they adopt a "bottom-up" evaluation-strategy with heuristics [2]. Before an answer set program containing predicates can be executed, it must be "grounded"; this is usually achieved with the help of a front-end grounding tool such as Lparse [5] which transform a predicate ASP into a grounded (propositional) ASP. Thus, all of the current ASP solvers and their solution-strategies, in essence, work for only propositional programs. These solution strategies are bottom-up (rather than top-down or goal-directed) and employ intelligent heuristics (enumeration, branch-and-bound or tableau) to reduce the search space. It was widely believed that it is not possible to develop a goal-driven, top-down ASP Solver (i.e., similar to a query driven Prolog engine). However, recent techniques such as Coinductive Logic Programming (Co-LP) [6,7] have shown great promise in developing a top-

500

R. Min, A. Bansal and G. Gupta

down, goal-directed strategy. In this paper, we present a goal-directed or querydriven approach to computing the stable model of an ASP program that is based on co-LP and coinductive SLDNF resolution [8]. We term this ASP Solver coinductive ASP solver (co-ASP Solver). Our method eliminates the need for grounding, allows functions, and effectively handles a large class of (possibly infinite) answer set programs. Note that while the performance of our prototype implementation is not comparable to those of systems such as S-models, our work is a first step towards developing a complete method for computing queries for predicate ASP in a top-down, goal driven manner. The rest of the paper is organized as follows: we first give a brief overview of Answer Set Programming, followed by an overview of coinductive logic programming and co-SLDNF resolution (i.e., SLDNF resolution extended with coinduction). Next we discuss how predicate ASP can be realized using co-SLDNF. Finally, we present some examples and results from our initial implementation.

2

Answer Set Programming (ASP)

Answer Set Programming (ASP) and its stable model semantics [1-4] has been successfully applied to elegantly solving many problems in nonmonotonic reasoning and planning. Answer Set Programming (A-Prolog [1] or AnsProlog [2]) is a declarative logic programming language. Its basic syntax is of the form: L0 :- L1, ... , Lm, not Lm+1, ..., not Ln. (1)

where Li is a literal and n  0 and n  m. This rule states that L0 holds if L1, ... , Lm all hold and none of Lm+1, ..., Ln hold. In the answer set interpretation [2], these rules are interpreted to be specifying a set S of propositions called the answer set. In this interpretation, rule (1) states that Lo must be in the answer set S if L1 through Lm are in S and Lm+1 through Ln are not S. If L0 =  (or null), then the rule-head is null (i.e., false) which forces its body to be false (a constraint rule [3] or a headless-rule). Such a constraint rule is written as follows. :- L1, ... , Lm, not Lm+1, ..., not Ln. (2)

This constraint rule forbids an answer set from simultaneously containing all of the positive literals of the body and not containing any of the negated literals. A constraint can also be expressed in the form: Lo :- not Lo, L1, ... , Lm, not Lm+1, ..., not Ln (3)

A little thought will reveal that (3) can hold only if Lo is false which is only possible if the conjunction L1, ... , Lm, not Lm+1, ..., not Ln is false. Thus, one can observe that (2) and (3) specify the same constraint. The (stable) models of an answer set program are traditionally computed using the Gelfond-Lifschitz method [1,2]; Smodels, NoMoRe, and DLV are some of the

Towards Predicate Answer Set Programming

501

well-known implementations of the Gelfond-Lifschitz method. The main difficulty in the execution of answer set programs is caused by the constraint rules (of the form (2) and (3) above). Such constraint rules force one or more of the literals L1, ... , Lm, to be false or one or more literals "Lm+1, ..., Ln" to be true. Note that "not Lo" may be reached indirectly through other calls when the above rule is invoked in response to the call Lo. Such rules are said to contain an odd-cycle in the predicate dependency graph [9,10]. The predicate dependency graph of an answer set program is a directed graph consisting of the nodes (the predicate symbols) and the signed (positive or negative) edges between nodes, where using clause (1) for illustration, a positive edge is formed from each node corresponding to Li (where 1  i  m) in the body of clause (1) to its head node L0, and a negative edge is formed from each node Lj (where m+1  j  n) in the body of clause (1) to its head node L0. Li depends evenly (oddly, resp.) on Lj if there is a path in the predicate dependency graph from Li to Lj with an even (odd, resp.) number of negative edges. A predicate ASP program is call-consistent if no node depends oddly on itself. The atom dependency graph is very similar to the predicate dependency graph except that it uses the ground instance of the program: its nodes are the ground atoms and its positive and negative edges are defined with the ground instances of the program. A predicate ASP program is order-consistent if the dependency relations of its atom dependency graph is well-founded (that is, finite and acyclic).

3

Coinductive Logic Programming

Coinduction is a powerful technique for reasoning about unfounded sets, unbounded structures, and interactive computations. Coinduction allows one to reason about infinite objects and infinite processes [11,12]. Coinduction has been recently introduced into logic programming (termed coinductive logic programming, or co-LP for brevity) by Simon et al [6] and extended with negation as failure (termed co-SLDNF resolution) by Min and Gupta [8]. Practical applications of co-LP include modeling of and reasoning about infinite processes and objects, model checking and verification [6,7,13], and goal-directed execution of answer set programs [7,13]. Co-LP extends traditional logic programming with the coinductive hypothesis rule (CHR). The coinductive hypothesis rule states that during execution, if the current resolvent R contains a call C' that unifies with an ancestor call C encountered earlier, then the call C' succeeds; the new resolvent is R' where  = mgu(C, C') and R' is obtained by deleting C' from R. Co-LP allows programmers to manipulate rational structures in a decidable manner. Rational structures are: (i) finite structures and (ii) infinite structures consisting of finite number of finite structures interleaved infinite number of times (e.g., a circular list). To achieve this feature of rationality, unification has to be necessarily extended with the "occur-check" removed and bindings such as X = [1 | X]

502

R. Min, A. Bansal and G. Gupta

(which denotes an infinite list of 1's) allowed [7, 14, 15]. SLD resolution extended with the coinductive hypothesis rule is called co-SLD resolution [6,7]. Co-SLDNF resolution, devised by us, extends co-SLD resolution with negation. Essentially, it augments co-SLD with the negative coinductive hypothesis rule, which states that if a negated call not(p) is encountered during resolution, and another call to not(p) has been seen before in the same computation, then not(p) coinductively succeeds. To implement co-SLDNF, the set of positive and negative calls has to be maintained in the positive hypothesis table (denoted +) and negative hypothesis table (denoted -) respectively. Note that nt(A) below denotes coinductive "not" of A. Definition 3.1 Co-SLDNF Resolution: Suppose we are in the state (G, E, +, -). Consider a subgoal A  G: (1) If A occurs in positive context, and A'  + such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with . (2) If A occurs in negative context, and A'  - such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with false. (3) If A occurs in positive context, and A'  - such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with false. (4) If A occurs in negative context, and A'  + such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with . (5) If A occurs in positive context and there is no A'  (+  -) that unifies with A, then the next state is (G', E', {A}  +, -) where G' is obtained by expanding A in G via normal call expansion using a (nondeterministically chosen) clause Ci (where 1 i  n) whose head atom is unifiable with A with E' as the new system of equations obtained. (6) If A occurs in negative context, and there is no A'  (+  -) that unifies with A, then the next state is (G', E', +, {A}  -) where G' is obtained by expanding A in G via normal call expansion using a (nondeterministically chosen) clause Ci (where 1 i  n) whose head atom is unifiable with A and E' is the new system of equations obtained. (7) If A occurs in positive or negative context and there are no matching clauses for A, and there is no A'  (+  -) such that A and A' are unifiable, then the next state is (G', E, +, {A}  -), where G' is obtained by replacing A with false. (8) (a) nt(..., false, ...) reduces to , and (b) nt(A, , B) reduces to nt(A, B) where A and B represent conjunction of subgoals.  Note (i) that the result of expanding a subgoal with a unit clause in step (5) and (6) is an empty clause (), and (ii) that when an initial query goal reduces to an empty (), it denotes a success with the corresponding E as the solution. Definition 3.2 (Co-SLDNF derivation): Co-SLDNF derivation of the goal G of program P is a sequence of co-SLDNF resolution steps (of Definition 3.1) with a selected subgoal A, consisting of (i) a sequence (Gi, Ei, i+, i-) of state (i  0), of (a) a sequence G0, G1, ... of goal, (b) a sequence E0, E1, ... of mgu's, (c) a sequence

Towards Predicate Answer Set Programming

503

0+, 1+, ... of the positive hypothesis table, (d) 0-, 1-, ... of the negative hypothesis table, where (G0, E0, 0+, 0-) = (G, , , ) is the initial state, and (ii) for step (5) or step (6) of Definition 3.1, a sequence C1, C2, ... of variants of program clauses of P where Gi+1 is derived from Gi and Ci+1 using i+1 where Ei+1 = Eii+1 and (i+1+, i+1-) are the resulting positive and negative hypothesis tables. (iii) If a co-SLDNF derivation from G results in an empty clause, that is, the final state of (, Ei, i+, i-) is reached, then the co-SLDNF derivation is successful; a coSLDNF derivation fails if a state is reached in the subgoal-list which is non-empty and no transitions are possible from this state.  Note that due to non-deterministic choice of a clause in steps (5) and (6) of coSLDNF resolution (Definition 3.1) there may be many successful derivations for a goal G. Thus a co-SLDNF resolution step may involve expanding with a program clause with the initial goal G = G0, and the initial state of (G0, E0, 0+, 0-) = (G, , , ), and Ei+1 = Eii+1 (and so on) and may look as follows:
C1,1 C2,2 C3,3

(G0, E0, 0+, 0-)  (G1, E1, 1+, 1-)  (G2, E2, 2+, 2-)  ... The declarative semantics of negation over the rational Herbrand space is based on the work of Fitting [12] (Kripke-Kleene semantics with 3-valued logic), extended by Fages [9] for stable model with completion of program. Their framework based on maintaining a pair of sets (corresponding to a partial interpretation of success set and failure set, resulting in a partial model) provides a good basis for the declarative semantics of co-SLDNF. An interesting property of co-SLDNF is that a program P coincides with its comp(P) under co-SLDNF. The implementation of solving ASP programs in a goal-directed (top-down) fashion (just like Prolog) has been discussed in Gupta et al [7] for propositional answer set programs. Here, we show how it can be extended for predicate answer set programs.

4

Coinductive ASP Solver

Our current work is an extension of our previous work discussed in [7] for grounded (propositional) ASP solver to the predicate case. Our approach possesses the following advantages: First, it works with answer set programs containing first order predicates with no restrictions placed on them. Second, it eliminates the preprocessing requirement of grounding, i.e., it directly executes the predicates in the manner of Prolog. Our method constitutes a top-down/goal-directed/queryoriented paradigm for executing answer set programs, a radically different alternative to current ASP solvers. We term ASP solver realized via co-induction as coinductive ASP Solver (co-ASP Solver). The co-ASP solver's strategy is first to

504

R. Min, A. Bansal and G. Gupta

transform an ASP program into a coinductive ASP (co-ASP) program and use the following solution-strategy: (1) Compute the completion of the program and then execute the query goal using co-SLDNF resolution on the completed program (this may yield a partial model). (2) Avoid loop-positive solution (e.g., p derived coinductively from rules such as { p :- p. }) during co-SLDNF resolution: This is achieved during execution by ensuring that coinductive success is allowed while exercising the coinductive hypothesis rule only if there is at least one intervening call to `not' in between the current call and the matching ancestor call. (3) Perform an integrity check on the partial model generated to account for the constraints: Given an odd-cycle rule of the form { p :- body, not p. }, this integrity check, termed nmr_check is crafted as follows: if p is in the answer set, then this odd-cycle rule is to be discarded. If p is not in the answer set, then body must be false. This can be synthesized as the condition: p  not body must hold true. The integrity check (nmr_chk) synthesizes this condition for all odd-cycle rules, and is appended to the query as a preprocessing step. The solution strategy outlined above has been implemented and preliminary results are reported below. Our current prototype implementation is a first attempt at a top-down predicate ASP solver, and thus is not as efficient as current optimized ASP solvers, SAT solvers, or Constraint Logic Programming in solving practical problems. However, we are confident that further research will result in much greater efficiency; indeed our future research efforts are focused on this aspect. The main contribution of our paper is to demonstrate that top-down execution of predicate ASP is possible with reasonable efficiency. Theorem 4.1 (Soundness of co-ASP Solver for a program which is call-consistent or order-consistent): Let P be a general ASP program which is call-consistent or order-consistent. If a query Q has a successful co-ASP solution, then Q is a subset of an answer set. Theorem 4.2 (Completeness of co-ASP Solver for a program with a stable model): If P is a general ASP program with a stable model M in the rational Herbrand base of P, then a query Q consistent with M has a successful co-ASP solution (i.e., the query Q is present in the answer set corresponding to the stable model). The proofs are straightforward and follow from soundness/completeness results for co-SLDNF [8] (along with Theorem 5.4 in Fages [9] that "an order-consistent logic program has a stable model"). The theorems can also be proved for unrestricted answer set programs, for queries extended with the nmr_check integrity constraint.

Towards Predicate Answer Set Programming

505

5

Preliminary Implementation Results

We next illustrate our top-down system via some example programs and queries. Most of the small ASP examples1 and their queries run very fast, usually under 0.0001 CPU seconds. Our test environment is implemented on top of YAP Prolog2 running under Linux in a shared environment with dual core AMD Opteron Processor 275, with 2GHz with 8GB memory. Our first example is "move-win," a program that computes the winning path in a simple game, tested successfully with various test queries (Fig 5.1). Note that in all cases the nmr_check integrity constraint is hand-produced.
%% A predicate ASP, "move-win" program %% facts: move move(a,b). move(b,a). move(a,c). move(c,d). move(d,e). move(c,f). move(e,f). %% rule: win win(X) :- move(X,Y), not win(Y). %% query: ?- win(a).

win(X) not

move(X,Y)

Fig. 5.1 Predicate-dependency graph of Predicate ASP "move-win".

The "move-win" program consists of two parts: (a) facts of move(x,y), to allow a move from x to y) and (2) a rule { win(X) :- move(X,Y), not win(Y). } to infer X to be a winner if there is a move from X to Y, and Y is not a winner. This is a predicate ASP program which is not call-consistent but order-consistent, and has two answer sets: { win(a), win(c), win(e) } and { win(b), win(c), win(e) }. Existing solvers will operate by first grounding the program using the move predicates. However, our system executes the query without grounding (since the program is order consistent, the nmr_check integrity constraint is null). Thus, in response to the query above, we'll get the answer set { win(a), win(c), win(e) }. The second example is the Schur number problem for NxB (for N numbers with B boxes). The problem is to find a combination of N numbers (consecutive integers from 1 to N) for B boxes (consecutive integers from 1 to B) with one rule and two constraints. The first rule states that a number X should be paired with one and only one box Y. The first constraint states that if a number X is paired with a box B, then double its value, X+X, should not be paired with box B. The second con1

More examples and performance data can be found from our Technical Report, available from: http://www.utdallas.edu/~rkm010300/research/co-ASP.pdf 2 http://www.dcc.fc.up.pt/~vsc/Yap/

506

R. Min, A. Bansal and G. Gupta

straint states that if two numbers, X and Y, are paired with a box B, then their sum, X+Y, should not be paired with the box B.
%% The ASP Schur NxB Program. box(1). box(2). box(3). box(4). box(5). num(1). num(2). num(3). num(4). num(5). num(6). num(7). num(8). num(9). num(10). num(11). num(12). %% rules in(X,B) :- num(X), box(B), not not_in(X,B). not_in(X,B) :- num(X),box(B),box(BB),B  BB,in(X,BB). %% constraint rules :- num(X), box(B), in(X,B), in(X+X,B). :- num(X), num(Y), box(B), in(X,B), in(Y,B), in(X+Y,B).

The ASP program is then transformed to a co-ASP program (with its completed definitions added for execution efficiency); the headless rules are transformed to craft the nmr_check.
%% co-ASP Schur 12x5 Program. %% facts: box(b). num(n). box(1). box(2). box(3). box(4). box(5). num(1). num(2). num(3). num(4). num(5). num(6). num(7). num(8). num(9). num(10). num(11). num(12). %% rules in(X,B) :- num(X), box(B), not not_in(X,B). nt(in(X,B)) :- num(X), box(B), not_in(X,B). not_in(X,B) :- num(X),box(B),box(BB),B\==BB, in(X,BB). nt(not_in(X,B)) :- num(X), box(B), in(X,B). %% constraints nmr_chk :- not nmr_chk1, not nmr_chk2. nmr_chk1 :- num(X),box(B),in(X,B),(Y is X+X),num(Y),in(Y,B). nmr_chk2 :- num(X),num(Y),box(B),in(X,B),in(Y,B), (Z is X+Y), num(Z), in(Z,B). %% query template answer :- in(1,B1), in(2,B2), in(3,B3), in(4,B4), in(5,B5), in(6,B6), in(7,B7), in(8,B8), in(9,B9), in(10,B10), in(11,B11), in(12,B12). %% Sample query: ?- answer, nmr_chk.

First, Schur 12x5 is tested with various queries which include partial solutions of various lengths I (Fig. 5.1; Table 5.1). That is, if I = 12, then the query is a test: all 12 numbers have been placed in the 5 boxes and we are merely checking that the constraints are met. If I = 0, then the co-ASP Solver searches for solutions from scratch (i.e., it will guess the placement of all 12 numbers in the 5 boxes provided subject to constraints). The second case (Fig 5.2; Table 5.2) is the general Schur NxB problems with I=0 where N ranges from 10 to 18 with B=5.

Towards Predicate Answer Set Programming

507

Fig. 5.2 Schur 5x12 (I=Size of the query). Fig. 5.3 Schur BxN (Query size=0).

Table 5.1 Schur 5x12 problem (box=1..5, N=1..12). I=Query size

Schur 5x12 CPU sec.

I=12 0.01

I=11 0.01

I=10 0.19

I=9 0.23

I=8 0.17

I=7 0.44

I=6 0.43

I=5 0.41

I=4 0.43

Table 5.2 Schur BxN problem (B=box, N=number). Query size=0, with a minor tuning.

Schur BxN CPU sec.

5x10 0.13

5x11 0.14

5x12 0.75

5x13 0.80

5x14 0.48

5x15 4.38

5x16 23.17

5x17 24.31

5x18 130

The performance data of the current prototype system is promising but still in need of improvement if we compare it with performance on other existing solvers (even after taking the cost of grounding the program into account). Our main strategy for improving the performance of our current co-ASP solver is to interleave the execution of candidate answer set generation and nmr_check. Given the query ?- goal, nmr_check, the call to goal will act as the generator of candidate answer sets while nmr_check will act as a tester of legitimacy of the answer set. This generation and testing has to be interleaved in the manner of constraint logic programming to reduce the search space. Additional improvements can also be made by improving the representation and look-up of positive and negative hypothesis tables during co-SLDNF (e.g., using a hash table, or a trie data-structure).

6

Conclusion and Future Work

In this paper we presented an execution strategy for answer set programming extended with predicates. Our execution strategy is goal-directed, in that it starts with a query goal G and computes the (partial) answer set containing G in a manner similar to SLD resolution. Our strategy is based on the recent discovery of coinductive logic programming extended with negation as failure. We also presented results from a preliminary implementation of our top-down scheme. Our

508

R. Min, A. Bansal and G. Gupta

future work is directed towards making the implementation more efficient so as to be competitive with the state-of-the-art solvers for ASP. We are also investigating automatic generation of the nmr_check integrity constraint. In many cases, the integrity constraint can be dynamically generated during execution when the negated call nt(p) is reached from a call p through an odd cycle.

References
1. 2. 3. 4. 5. 6. 7. 8. Gelfond M, Lifschitz V (1988). The stable model semantics for logic programming. Proc. of International Logic Programming Conference and Symposium. 1070-1080. Baral C (2003). Knowledge Representation, Reasoning and Declarative Problem Solving. Cambridge University Press. Niemel I, Simons, P (1996). Efficient implementation of the well-founded and stable model semantics. Proc. JICSLP. 289-303. The MIT Press. Simons P, Niemel I, Soininen, T (2002). Extending and implementing the stable model semantics. Artificial Intelligence 138(1-2):181-234. Simons P, Syrjanen, T (2003). SMODELS (version 2.27) and LPARSE (version 1.0.13). http://www.tcs.hut.fi/Software/smodels/ Simon L, Mallya A, Bansal A, Gupta G (2006). Coinductive Logic Programming. ICLP'06. Springer Verlag. Gupta G, Bansal A, Min R et al (2007). Coinductive logic programming and its applications. Proc. ICLP'07. Springer Verlag. Min R, Gupta G (2008). Negation in Coinductive Logic Programming. Technical Report. Department of Computer Science. University of Texas at Dallas. http://www.utdallas.edu/~rkm010300/research/co-SLDNF.pdf Fages F (1994). Consistency of Clark's completion and existence of stable models. Journal of Methods of Logic in Computer Science 1:51-60. Sato, T (1990). Completed logic programs and their consistency. J Logic Prog 9:33-44. Kripke S (1985). Outline of a Theory of Truth. Journal of Philosophy 72:690-716. Fitting, M (1985). A Kripke-Kleene semantics for logic programs. Journal of Logic Programming 2:295-312. Simon L, Bansal A, Mallya A et al (2007). Co-Logic Programming. ICALP'07. Colmerauer A (1978). Prolog and Infinite Trees. In: Clark KL, Tarnlund S-A (eds) Logic Programming. Prenum Press, New York. Maher, MJ (1988). Complete Axiomatizations of the Algebras of Finite, Rational and Infinite Trees. Proc. 3rd Logic in Computer Science Conference. Edinburgh, UK.

9. 10. 11. 12. 13. 14. 15.

USDL: A Service-Semantics Description Language for Automatic Service Discovery and Composition.
Srividya Kona, Ajay Bansal, Luke Simon, Ajay Mallya, and Gopal Gupta University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp 2400 Dallas Parkway Plano, TX 75093

Abstract For web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose, and synthesize services automatically. This automation can take place only if a formal description of the web-services is available. In this paper we present an infrastructure using USDL (Universal Service-Semantics Description Language), a language for formally describing the semantics of web-services. USDL is based on the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding the meaning of services. USDL can be regarded as formal service documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated service composition, and other forms of automated service integration. A theory of service substitution using USDL is presented. The rationale behind the design of USDL along with its formal specification in OWL is presented with examples. We also compare USDL with other approaches like OWL-S, WSDL-S, and WSML and show that USDL is complementary to these approaches.

1

Introduction

A web-service is a program available on a web-site that "effects some action or change" in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. The next milestone in the Web's evolution is making services ubiquitously available. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. In this context, a web-service can be regarded as a "programmatic interface" that makes application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize and compose services automatically needs to be supported in order to make web-services more practical. To make services ubiquitously available we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis. Several efforts are underway to build such an infrastructure. These efforts include approaches based on the semantic web (such as OWL-S [5]) as well as those based on XML, such as Web Services Description Language (WSDL [7]). Approaches such
This is an expanded version of the paper `A Universal Service-Semantics Description Language' that appeared in European Conference On Web Services, 2005 [15] and received its best paper award.


1

as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. In this paper we present an approach that is based on semantics. Our approach can be regarded as providing semantics to WSDL statements. We present the design of a language called Universal Service-Semantics Description Language (USDL) which service developers can use to specify formal semantics of web-services [14, 15]. Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL can be thought of as formal service documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated composition, and other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be determined. The directory can then be searched for the exact service, or two or more services that can be composed to synthesize the required service, etc. To provide formal semantics, a common denominator must be agreed upon that everybody can use as a basis of understanding the meaning of services. This common conceptual ground must also be somewhat coarse-grained so as to be tractable for use by both engineers and computers. That is, semantics of services should not be given in terms of low-level concepts such as Turing machines, first-order logic and their variants, since service description, discovery, and synthesis then become tasks that are practically intractable and theoretically undecidable. Additionally, the semantics should be given at a conceptual level that captures common real world concepts. Furthermore, it is too impractical to expect disparate companies to standardize on application (or domain) specific ontologies to formally define semantics of web-services, and instead a common universal ontology must be agreed upon with additional constructors. Also, application specific ontologies will be an impediment to automatic discovery of services since the application developer will have to be aware of the specific ontology that has been used to describe the semantics of the service in order to frame the query that will search for the service. The danger is that the service may not be defined using the particular domain specific ontology that the application developer uses to frame the query, however, it may be defined using some other domain specific ontology, and so the application developer will be prevented from discovering the service even though it exists. These reasons make an ontology based on OWL WordNet [2, 8] a suitable candidate for a universal ontology of basic concepts upon which arbitrary meets and joins can be added in order to gain tractable flexibility. We describe the meaning of conceptual modeling and how it could be obtained via a common universal ontology based on WordNet in the next section. Section 3, gives a brief overview of how USDL attempts to semantically describe web-services. In section 4, we discuss precisely how a WSDL document can be prescribed meaning in terms of WordNet ontology. Section 5 gives a complete USDL annotation for a Hotel-Reservation service. In section 6 we present the theoretical foundations of service description and substitution in USDL. Automatic discovery of web-services using USDL is discussed in section 7. Composition of web-services using USDL is discussed in section 8. Comparison of USDL with other approaches like OWL-S and WSML is discussed in section 9. Section 10 shows related work. Finally, conclusions and future work are addressed in the last section.

2

2

A Universal Ontology

To describe service semantics, we should agree on a common ground to model our concepts. We can describe what any given web-service does from first principles using approaches based on logic. This is the approach taken by frameworks such as dependent type systems and programming logics prevalent in the field of software verification where a "formal understanding" of the service/software is needed in order to verify it. However, such solutions are both low-level, tedious, and undecidable to be of practical use. Instead, we are interested in modeling higher-level concepts. That is, we are more interested in answering questions such as, what does a service do from the end user's or service integrator's perspective, as opposed to the far more difficult questions, such as, what does the service do from a computational view? We care more about real world concepts such as "customer", "bank account", and "flight itinerary" as opposed to the data structures and algorithms used by a service to model these concepts. The distinction is subtle, but is a distinction of granularity as well as a distinction of scope. In order to allow interoperability and machine-readability of our documents, a common conceptual ground must be agreed upon. The first step towards this common ground are standard languages such as WSDL and OWL. However, these do not go far enough, as for any given type of service there are numerous distinct representations in WSDL and for high-level concepts (e.g., a ternary predicate), there are numerous disparate representations in terms of OWL, representations that are distinct in terms of OWL's formal semantics, yet equal in the actual concepts they model. This is known as the semantic aliasing problem: distinct syntactic representations with distinct formal semantics yet equal conceptual semantics. For the semantics to equate things that are conceptually equal, we need to standardize a sufficiently comprehensive set of basic concepts, i.e., a universal ontology, along with a restricted set of connectives. Industry specific ontologies along with OWL can also be used to formally describe web-services. This is the approach taken by the OWL-S language [5]. The problem with this approach is that it requires standardization and undue foresight. Standardization is a slow, bitter process, and industry specific ontologies would require this process to be iterated for each specific industry. Furthermore, reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing is even more difficult. Undue foresight is required because many useful web services will address innovative applications and industries that don't currently exist. Standardizing an ontology for travel and finances is easy, as these industries are well established, but new innovative services in new upcoming industries also need be ascribed formal meaning. A universal ontology will have no difficulty in describing such new services. We need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. WordNet [8] is a sufficiently comprehensive ontology that meets these criteria. As stated, part of the common ground involves standardized languages such as OWL. For this reason, WordNet cannot be used directly, and instead we make use of an encoding of WordNet as an OWL base ontology [2]. Using an OWL WordNet ontology allows our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which we map web service messages and operations. As long as this mapping is precise and sufficiently expressive, reasoning can be done within the realm of OWL by using automated inference systems (such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes relationships between ontological concepts, especially subsumption (hyponym-hypernym) and equivalence (synonym) relationships.

3

3

USDL: An Overview

As mentioned earlier, USDL can be regarded as a language to formally specify the semantics of web-services. It is perhaps the first attempt to capture the semantics of web-services in a universal, yet decidable manner. It is quite distinct from previous approaches such as WSDL and OWL-S [5]. As mentioned earlier, WSDL only defines syntax of the service; USDL provides the missing semantic component. USDL can be thought of as a formal language for service documentation. Thus, instead of documenting the function of a service as comments in English, one can write USDL statements that describe the function of that service. USDL is quite distinct from OWL-S, which is designed for a similar purpose, and as we shall see the two are in fact complementary. OWL-S primarily describes the states that exist before and after the service and how a service is composed of other smaller sub-services (if any). Description of atomic services is left under-specified in OWLS. They have to be specified using domain specific ontologies; in contrast, atomic services are completely specified in USDL, and USDL relies on a universal ontology (OWL WordNet Ontology) to specify the semantics of atomic services. USDL and OWL-S are complementary in that OWL-S's strength lies in describing the structure of composite services, i.e., how various atomic services are algorithmically combined to produce a new service, while USDL is good for fully describing atomic services. Thus, OWL-S can be used for describing the structure of composite services that combine atomic services described using USDL. USDL describes a service in terms of portType and messages, similar to WSDL. The semantics of a service is given using the OWL WordNet ontology: portType (operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly negated) concepts in the OWL WordNet ontology. The semantics is given in terms of how a service affects the external world. The present design of USDL assumes that each side-effect is one of following four operations: create, update, delete, or find. A generic affects side-effect is used when none of the four apply. An application that wishes to make use of a service automatically should be able to reason with WordNet atoms using the OWL WordNet ontology. We also define the formal semantics of USDL. As stated earlier, the syntactic terms describing portType and messages are mapped to disjunctions of conjunctions of (possibly negated) OWL WordNet ontological terms. A service is then formally defined as a function, labeled by the sideeffect. The main contribution of our work is the design of a universal service-semantics description language (USDL), along with its formal semantics, and a theory of service substitution using it.

4

Design of USDL

The design of USDL rests on two formal languages: Web Services Description Language (WSDL) [7] and Web Ontology Language (OWL) [6]. The Web Services Description Language (WSDL) [7] is used to give a syntactic description of the name and parameters of a service. The description is syntactic in the sense that it describes the formatting of services on a syntactic level of method signatures, but is incapable of describing what concepts are involved in a service and what a service actually does, i.e., the conceptual semantics of the service. Likewise, the Web Ontology Language (OWL) [6], was developed as an extension to the Resource Description Framework (RDF) [3], both standards are designed to allow formal conceptual modeling via logical ontologies, and these languages also allow for the markup of existing web resources with semantic information from the conceptual models. USDL employs WSDL and OWL in order to describe the syntax and semantics 4

of web-services. WSDL is used to describe message formats, types, and method prototypes, while a specialized universal OWL ontology is used to formally describe what these messages and methods mean, on a conceptual level. USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL documents contain two main constructs to which we want to ascribe conceptual meaning: messages and portType. These constructs are aggregates of service components which will be directly ascribed meaning. Messages consist of typed parts and portType consists of operations parameterized on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes, which have properties with values in the OWL WordNet ontology.

4.1

Concept

USDL defines a generic class called Concept which is used to define the semantics of parts of messages.
<owl:Class rdf:ID="Concept"> <rdfs:comment>Generic class of USDL Concept</rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#BasicConcept"/> <owl:Class rdf:about="#QualifiedConcept"/> <owl:Class rdf:about="#InvertedConcept"/> <owl:Class rdf:about="#ConjunctiveConcept"/> <owl:Class rdf:about="#DisjunctiveConcept"/> </owl:unionOf> </owl:Class>

The USDL Concept class denotes the conceptual objects constructed from the OWL WordNet ontology. For most purposes, message parts and other WSDL constructs will be mapped to a subclass of USDL Concept so that useful concepts can be modeled as set theoretic formulas of union, intersection, and negation of basic concepts. These subclasses of Concept are BasicConcept, QualifiedConcept, InvertedConcept, ConjunctiveConcept, and DisjunctiveConcept. 4.1.1 Basic Concept

An BasicConcept is the actual contact point between USDL and WordNet. This class acts as proxy for WordNet lexical entities.
<owl:Class rdf:about="#BasicConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isA"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions require all USDL BasicConcept s to have exactly one defining value for the isA property. An instance of BasicConcept is considered to be equated with a WordNet lexical concept given by the isA property.

5

<owl:ObjectProperty rdf:ID="isA"> <rdfs:domain rdf:resource="#BasicConcept"/> <rdfs:range rdf:resource="&wn;LexicalConcept"/> </owl:ObjectProperty>

4.1.2

Qualified Concept

A QualifiedConcept is a concept classified by another lexical concept.
<owl:Class rdf:about="#QualifiedConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isA"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#ofKind"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions require all USDL QualifiedConcept s to have exactly one defining value for the isA property, and exactly one value for the ofKind property. An instance of QualifiedConcept is considered to be equated with a lexical concept given by the isA property and classified by a lexical concept given by the optional ofKind property.
<owl:ObjectProperty rdf:ID="isA"> <rdfs:domain rdf:resource="#QualifiedConcept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="ofKind"> <rdfs:domain rdf:resource="#QualifiedConcept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.1.3

Inverted Concept

In case of InvertedConcept the corresponding semantics are the complement of USDL concepts.
<owl:Class rdf:about="#InvertedConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1

6

</owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasConcept"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.1.4

Conjunctive and Disjunctive Concept

The ConjunctiveConcept and DisjunctiveConcept respectively denote the intersection and union of USDL Concept s.
<owl:Class rdf:about="#ConjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions on ConjunctiveConcept and DisjunctiveConcept allow for n-ary intersections and unions (where n  2) of USDL concepts. For generality, these concepts are either BasicConcepts, QualifiedConcepts, ConjunctiveConcepts, DisjunctiveConcepts, or InvertedConcepts .

4.2

Affects

The affects property is specialized into four types of actions common to enterprise services: creates, updates, deletes, and finds.
<owl:ObjectProperty rdf:ID="affects"> <rdfs:comment> Generic class of USDL Affects </rdfs:comment> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/>

7

</owl:ObjectProperty> <owl:ObjectProperty rdf:about="#creates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#updates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#deletes"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#finds"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty>

Note that each of these specializations inherits the domain and range of the affects property. Most services can be described using one of these types of effects. For those services that cannot be described in terms of these specializations, the parent affects property can be used instead which is described as an USDL concept.

4.3

Conditions and Constraints

Services may have some external conditions (pre-conditions and post-conditions) specified on the input or output parameters. Condition class is used to describe all such constraints. Conditions are represented as conjunction or disjunction of binary predicates. Predicate is a trait or aspect of the resource being described.
<owl:Class rdf:ID="Condition"> <rdfs:comment> Generic class of USDL Condition </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#AtomicCondition"/> <owl:Class rdf:about="#ConjunctiveCondition"/> <owl:Class rdf:about="#DisjunctiveCondition"/> </owl:unionOf> </owl:Class> <owl:Class rdf:about="#AtomicCondition"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#onPart"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">

8

1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasValue"/> <owl:maxCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:maxCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

A condition has exactly one value for the onPart property and at most one value for the hasValue property, each of which is of type USDL Concept.
<owl:ObjectProperty rdf:ID="onPart"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="hasValue"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.3.1

Conjunctive and Disjunctive Conditions

The ConjunctiveCondition and DisjunctiveCondition respectively denote the conjunction and disjunction of USDL Condition s.
<owl:Class rdf:about="#ConjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

9

<owl:ObjectProperty rdf:ID="hasCondition"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Condition"/> </owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveCondition and DisjunctiveCondition allow for n-ary conjunctions and disjunctions (where n  2) of USDL conditions. In general any n-ary condition can be written as a combination of conjunctions and disjunctions of binary conditions.

4.4

Messages

Services communicate by exchanging messages. As mentioned, messages are simple tuples of actual data, called parts. Take for example, a flight reservation service similar to the SAP ABAP Workbench Interface Repository for flight reservations [4], which makes use of the following message.
<message name="#ReserveFlight_Request"> <part name="#CustomerName" type="xsd:string"> <part name="#FlightNumber" type="xsd:string"> <part name="#DepartureDate" type="xsd:date"> ... </message>

The USDL surrogate for a WSDL message is the Message class, which is a composite entity with zero or more parts. Note that for generality, messages are allowed to contain zero parts.
<owl:Class rdf:ID="Message"> <rdfs:comment> Generic class of USDL Message </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#Input"/> <owl:Class rdf:about="#Output"/> </owl:unionOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasPart"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#Input"> <rdfs:subClassOf rdf:resource="#Message"/> </owl:Class> <owl:Class rdf:about="#Output"> <rdfs:subClassOf rdf:resource="#Message"/> </owl:Class>

Each part of a message is simply a USDL Concept, as defined by the hasPart property. Semantically messages are treated as tuples of concepts.

10

<owl:ObjectProperty rdf:ID="hasPart"> <rdfs:domain rdf:resource="#Message"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

Continuing our example flight reservation service, the ReserveF lightRequest message is given semantics using USDL as follows, where &wn;customer and &wn;name are valid XML references to WordNet lexical concepts.
<Message rdf:about="#ReserveFlight_Request"> <hasPart rdf:resource="#CustomerName"/> <hasPart rdf:resource="#FlightNumber"/> <hasPart rdf:resource="#DepartureDate"/> </Message> <QualifiedConcept rdf:about="#CustomerName"> <isA rdf:resource="#Name"/> <ofKind rdf:resource="#Customer"/> </QualifiedConcept> <BasicConcept rdf:about="#Name"> <isA rdf:resource="&wn;name"/> </BasicConcept> <BasicConcept rdf:about="#Customer"> <isA rdf:resource="&wn;customer"/> </BasicConcept> <!-- Similarly concepts FlightNumber and DepartureDate are defined -->

4.5

PortType

A service consists of portType, which is a collection of procedures or operations that are parametric on messages. Our example flight reservation service might contain a portType definition for a flight reservation service that takes as input an itinerary and outputs a reservation receipt.
<portType rdf:about="#Flight_Reservation"> <hasOperation rdf:resource="#ReserveFlight"> </portType> <operation rdf:about="#ReserveFlight"> <hasInput rdf:resource="#ReserveFlight_Request"/> <hasOutput rdf:resource="#ReserveFlight_Response"/> <creates rdf:resource="#FlightReservation" /> </operation>

The USDL surrogate is defined as the class portType which contains zero or more Operation s as values of the hasOperation property.
<owl:Class rdf:about="#portType"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource=#hasOperation"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality>

11

</owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasOperation"> <rdfs:domain rdf:resource="#portType"/> <rdfs:range rdf:resource="#Operation"/> </owl:ObjectProperty>

As with the case of messages, portTypes are not directly assigned meaning via the OWL WordNet ontology. Instead the individual Operation s are described by their side-effects via an affects property. Note that the parameters of an operation are already given meaning by ascribing meaning to the messages that constitute the parameters.
<owl:Class rdf:about="#Operation"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasInput"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasOutput"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#affects"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

An operation can have one or more values for the affects property, all of which are of type USDL Concept, which is the target of the effect.
<owl:ObjectProperty rdf:ID="hasInput"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Input"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="hasOutput"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Ouput"/> </owl:ObjectProperty>

12

<owl:ObjectProperty rdf:ID="affects"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

5

Semantic Description of a Service

This section shows an example syntactic description of a web-service using WSDL and its corresponding semantic description using USDL. Hotel Reservation Service: The service described here is a simplified hotel-reservation service published in a web-service registry. This service can be treated as atomic: i.e., no interactions between buying and selling agents are required, apart from invocation of the service and receipt of its outputs by the buyer. Given certain inputs and pre-conditions, the service provides certain outputs and has specific effects. This service takes in a HotelChain, StartDate, NumNights, NumPersons, NumRooms, FirstName, and LastName as input parameters. It has a few input pre-conditions that NumNights, NumRooms must be greater than zero and StartDate must be greater than today. This service outputs a Reservation at the end of transaction.

5.1

WSDL definition

The following is WSDL definition of the service. This service provides a single operation called ReserveHotel. The input and output messages are defined below. The conditions on the service cannot be described using WSDL.
<definitions ...> <portType name="ReserveHotel_Service"> <operation name="ReserveHotel"> <input message="ReserveHotel_Request"/> <output message="ReserveHotel_Response"/> </operation> </portType> <message name="ReserveHotel_Request"> <part name="HotelChain" type="xsd:string"/> <part name="StartDate" type="xsd:date"/> <part name="NumNights" type="xsd:integer"/> <part name="NumPersons" type="xsd:integer"/> <part name="NumRooms" type="xsd:integer"/> <part name="FirstName" type="xsd:integer"/> <part name="LastName" type="xsd:integer"/> </message> <message name="ReserveHotel_Response"> <part name="Reservation" type="xsd:string"/> </message> ... </definitions>

13

5.2

USDL annotation
<QualifiedConcept rdf:about="#StartDate"> <isA rdf:resource="#Date"/> <ofKind rdf:resource="#Start"/> </QualifiedConcept> <QualifiedConcept rdf:about="#TodaysDate"> <isA rdf:resource="#Date"/> <ofKind rdf:resource="#Today"/> </QualifiedConcept> <!-- Similarly we can define Qualified Concepts for #NumNights, #NumPersons, #NumRooms, #FirstName and #LastName --> <BasicConcept rdf:about="#Hotel"> <isA rdf:resource="&wn;hotel"/> </BasicConcept> <BasicConcept rdf:about="#Chain"> <isA rdf:resource="&wn;chain"/> </BasicConcept> <BasicConcept rdf:about="#Start"> <isA rdf:resource="&wn;start"/> </BasicConcept> <BasicConcept rdf:about="#Date"> <isA rdf:resource="&wn;date"/> </BasicConcept> <BasicConcept rdf:about="#greaterThan"> <isA rdf:resource="&wn;greater_than"/> </BasicConcept> <BasicConcept rdf:about="#Date"> <isA rdf:resource="&wn;date"/> </BasicConcept> <BasicConcept rdf:about="#Today"> <isA rdf:resource="&wn;today"/> </BasicConcept> <BasicConcept rdf:about="#Reservation"> <isA rdf:resource="&wn;reservation"/> </BasicConcept> <!-- Similarly we can define Basic Concepts for #nights, #rooms, #number, #persons, #name, etc. -->

The following is the complete USDL annotation corresponding to the above mentioned WSDL description. The input pre-condition and the global constraint on the service are also described semantically.
<definitions> <portType rdf:about= "#ReserveHotel_Service"> <hasOperation rdf:resource= "#ReserveHotel"/> </portType> <operation rdf:about="#ReserveHotel"> <hasInput rdf:resource= "#ReserveHotel_Request"/> <hasOutput rdf:resource= "#ReserveHotel_Response"/> <creates rdf:resource= "#HotelReservation"/> </operation> <Message rdf:about= "#ReserveHotel_Request"> <hasPart rdf:resource="#HotelChain"/> <hasPart rdf:resource="#StartDate"/> <hasPart rdf:resource="#NumNights"/> <hasPart rdf:resource="#NumPersons"/> <hasPart rdf:resource="#NumRooms"/> <hasPart rdf:resource="#FirstName"/> <hasPart rdf:resource="#LastName"/> </Message> <Message rdf:about= "#ReserveHotel_Response"> <hasPart rdf:resource= "#HotelReservation"/> </Message> <Condition rdf:about="#greaterThanToday"> <hasConcept rdf:resource="#greaterThan"/> <onPart rdf:resource="#StartDate"/> <hasValue rdf:resource="#TodaysDate"/> </Condition> <!-- Similarly we can define Condition #greaterThanZero on parts #NumRooms and #NumNights -->

</definitions> <QualifiedConcept rdf:about="#HotelChain"> <isA rdf:resource="#Chain"/> A Book-Buying Service example is presented <ofKind rdf:resource="#Hotel"/> in [15]. </QualifiedConcept>

14

6

Theory of Substitution of Services

Next, we will investigate the theoretical aspects of USDL. This involves concepts from set theory. From a systems integration perspective, an engineer is interested in finding (discovering) a service that accomplishes some necessary task. Of course, such a service may not be present in any service directory. In such a case the discovery software should return a set of services that can be used in a context expecting a service that meets that description (of course, this set may be empty). To find services that can be substituted for a given service that is not present in the directory, we need to develop a theory of service substitutability. We develop such a theory in this section. Our theory relates service substitutability to WordNet's semantic relations. In order to develop this theory, we must first formally define constructs such as USDL-described concepts, affects, conditions and services, which we will also call concepts, affects, conditions and services for short. While it is possible to work directly with the XML USDL syntax, doing so is cumbersome and so we will instead opt for set theoretic notation. Definition 1 (Set of WordNet Lexemes) Let  be the set of WordNet lexemes. The following semantic relations exist on elements of . 1. Synonym: A pair of WordNet Lexemes having the same or nearly the same meaning have the synonym relation. Example, `purchase' is a synonym of `buy'. 2. Antonym: A pair of WordNet Lexemes having the opposite meaning have the antonym relation. Example, `start' is an antonym of `end'. 3. Hyponym: A word that is more specific than a given word is called the subordinate or hyponym of the other. Example, `car' is a hyponym of `vehicle'. 4. Hypernym: A word that is more generic than a given word is called the super-ordinate or hypernym of the other. Example, `vehicle' is a hypernym of `car'. 5. Meronym: A word that names a part of a larger whole is a meronym of the whole. Example, `roof' and `door' are meronyms of `house'. 6. Holonym: A word that names the whole of which a given word is a part is a holonym of the part. Example, `house' is a holonym for `roof' and `door'. Definition 2 (Representation of USDL Concepts) 1. A Basic Concept c = x, where x is a WordNet lexeme, defines the values of isA property. Example, customer is a Basic Concept and a WordNet lexeme. 2. A Qualified Concept c = (X, Y ), where X, Y are USDL concepts, defines the values of isA and ofKind properties. Example, concept FlightNumber is a number of kind flight represented as (number, flight ). 3. An Inverted Concept c is represented as 촞 where X is an USDL concept. Example, concept not a customer name can be represented as (name, customer ). 4. Let X, Y be USDL Concepts. (i) Conjunctive Concept c is represented as X  Y . Example, concept EvenRationalNumber is represented as (number, even )  (number, rational ). (ii) Disjunctive Concept c is represented as X Y . Example, concept OrderNumber/AvailabilityMessage is represented as (number, order )  (message, availability ). Definition 3 (Universe of USDL Concepts) 15

Let  be the set of USDL concepts.  can be inductively constructed as follows: 1. x   implies x   2. X, Y   implies (X, Y )   3. X   implies 촞   4. X, Y   implies X  Y   5. X, Y   implies X  Y   Definition 4 (Semantic relations of Basic Concepts) Semantic relations hold between two Basic concepts if their corresponding WordNet lexemes have the same semantic relation in . For example, Basic Concept Purchase is a synonym of Basic Concept Buy. Definition 5 (Synonym and Antonym relation of Qualified Concepts) Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2  . 1. C1 is synonym of C2 if X1 is recursively a synonym of X2 and Y1 is recursively a synonym of Y2 . 2. If X1 = w1 and X2 = w2 where w1 , w2  , then X1 is synonym of X2 if w1 and w2 have the synonym relation in . For example, Qualified Concept (date, begin ) is a synonym of Qualified Concept (date, start ). Similarly we can determine the antonym relation between Qualified Concepts. For example, Qualified Concept (date, begin ) is a antonym of Qualified Concept (date, end ). Definition 6 (Hyponym and Hypernym relation of Qualified Concepts) Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2  . 1. C1 is hypernym of C2 if any one of the following holds: (i) X1 is recursively a hypernym of X2 and Y1 is recursively a hypernym of Y2 . (ii) X1 is recursively a hypernym of X2 and Y1 is recursively a synonym of Y2 . (iii) X1 is recursively a synonym of X2 and Y1 is recursively a hypernym of Y2 . 2. If X1 = w1 and X2 = w2 where w1 , w2  , then X1 is hypernym of X2 if w1 and w2 have the hypernym relation in . For example, Qualified Concept (number, vehicle ) is a hypernym of Qualified Concept (number, car ). Similarly we can determine the hyponym relation of Qualified Concepts. For example, Concept (number, car ) is a hyponym of (number, vehicle ). Definition 7 (Holonym and Meronym relation of Qualified Concepts) Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2  . 1. C1 is meronym of C2 if any one of the following holds: (i) X1 is recursively a meronym of X2 and Y1 is recursively a meronym of Y2 . (ii) X1 is recursively a meronym of X2 and Y1 is recursively a synonym of Y2 . (iii) X1 is recursively a synonym of X2 and Y1 is recursively a meronym of Y2 . 2. If X1 = w1 and X2 = w2 where w1 , w2  , then X1 is meronym of X2 if w1 and w2 have the meronym relation in .

16

For example, Qualified Concept (door, brown ) is a meronym of Qualified Concept (house, brown ). Similarly we can determine the holonym relation of Qualified Concepts. For example, Qualified Concept (house, brown ) is a holonym of Qualified Concept (door, brown ). Definition 8 (Semantic relations between Inverted Concepts) Let C1 and C2 be two Inverted concepts where C1 = 촞1 and C2 = 촞2 . 1. C1 is a synonym of C2 if X1 and X2 are synonyms. 2. C1 is an antonym of C2 if X1 and X2 are antonyms. 3. C1 is a hypernym of C2 if X1 and X2 are hyponyms and vice versa. 4. C1 is a meronym of C2 if X1 and X2 are holonyms and vice versa. For example, Inverted Concept (date, begin ) is a synonym of Inverted Concept (date, start ). The synonym-antonym relation, hyponym-hypernym relation and meronym-holonym relation can be extended to Conjunctive and Disjunctive concepts. Definition 9 (Semantic relations between Conjunctive (resp., Disjunctive) Concepts) Let C1 and C2 be two Conjunctive (resp., Disjunctive) concepts where C1 = X1  Y1 and C2 = X2  Y2 . 1. C1 is a synonym of C2 if X1 is a synonym of X2 and Y1 is a synonym of Y2 OR X1 is a synonym of Y2 and Y1 is a synonym of X2 . 2. C1 is a hypernym of C2 if one of the following holds: (i) X1 is a hypernym of X2 and Y1 is a hypernym/synonym of Y2 (ii) X1 is a hypernym/synonym of X2 and Y1 is a hypernym of Y2 (iii) X1 is a hypernym of Y2 and Y1 is a hypernym/synonym of X2 . (iv) X1 is a hypernym/synonym of Y2 and Y1 is a hypernym of X2 . For example, Conjunctive Concept (vehicle, blue )  (vehicle, automatic ) is a hypernym of (car, blue )  (car, automatic ). Similar to the above defined hypernym relation, we can define the antonym, hyponym, meronym, and holonym relations between Conjunctive (resp., Disjunctive) Concepts. Definition 10 (Substitution of Concepts) 1. Exact Substitution: For any concepts C, C  , if C is a synonym of C , then C is the exact substitutable of C and C can safely be used in a context expecting concept C . Example, concept Purchase is an exact substitutable of concept Buy. 2. Generic Substitution: For any concepts C, C  , if C is a hypernym of C , then C is the generic substitutable of C and C can safely be used in a context expecting concept C or a super-ordinate of C . Example, concept (number, vehicle ) is a generic substitutable of concept (number, car ). 3. Specific Substitution: For any concepts C, C  , if C is a hyponym of C , then C is the specific substitutable of C and C can safely be used in a context expecting concept C or a sub-ordinate of C . Example, concept (number, car ) is a specific substitutable of concept (number, vehicle ). 4. Part Substitution: For any concepts C, C  , if C is a meronym of C , then C is the part substitutable of C and C can safely be used in a context expecting a concept that is a part of C . Example, concept Roof is a part substitutable of concept House.

17

5. Whole Substitution: For any concepts C, C  , if C is a holonym of C , then C is the whole substitutable of C and C can safely be used in a context expecting a concept that is a whole of C . Example, concept House is a whole substitutable of concept Roof. Definition 11 (Representation of Affects) Let  = {(L, E ) | L  (  ), E  } be the set of USDL side-effects, where  = {creates, updates, deletes, f inds}, L is the affect type and E is the affected object. The affect type could be one of the pre-defined affects from  or a generic effect which is described as a concept. Definition 12 (Substitution of Affects) USDL affect is represented as a pair where the first element is the affect type and second element is the affected object. Both affect type and the affected object are described as USDL concepts. Let A1 and A2 be two affects where A1 = (L1 , E1 ) and A2 = (L2 , E2 ). A1 can safely be used in a context expecting affect A2 if all of the following hold: 1. Concept L1 is substitutable for L2 2. Concept E1 is substitutable for E2 . These substitutables can be of kind Exact, Generic, Specific, Part, or Whole which also determines the kind of substitution of the affect A1 in a context expecting A2 . Example, affect (finds, VehicleNumber ) is a generic substitution of affect (lookup, CarNumber ) as concept finds is an exact substitutable of lookup and concept VehicleNumber is a generic substitutable of concept CarNumber. Definition 13 (Representation of Conditions) Let  = {(P, Arg1 , Arg2 ) | P, Arg1 , Arg2  } be the set of USDL conditions. P is the constraint which is either a binary or a unary predicate. Arg1 is the concept on which the predicate acts and Arg2 is the concept which represents a value. Arg2 is an optional parameter. Definition 14 (Substitution of Conditions) USDL condition is represented as a tuple made up of the constraint or predicate and two arguments. The constraint and the arguments are described as USDL concepts. Let C1 and C2 be two conditions where C1 = (P1 , F irstArg1 , SecondArg1 ) and C2 = (P2 , F irstArg2 , SecondArg2 ). C1 can safely be used in a context expecting condition C2 if all of the following hold: 1. Concept P1 is substitutable for P2 2. Concept F irstArg1 is substitutable for F irstArg2 3. Concept SecondArg1 is substitutable for SecondArg2 . These substitutables can be of kind Exact, Generic, Specific, Part, or Whole which also determines the kind of substitution of the condition C1 in a context expecting C2 . Example, condition (greaterThan, NumberOfNights, 0) is an exact substitution of condition (moreThan, NumberOfNights, 0). Definition 15 (Representation of a Web Service)

18

For any set S , let S  = { |  / S }  {(x, y ) | x  S, y  S  } be the set of lists over S . Let  be the set of USDL service descriptions represented in the form of terms. The USDL description of a web service consists of 1. A list of Inputs, I   2. A list of Outputs, O   3. A list of Pre-Conditions, Pre-Condition   4. A list of Post-Conditions, Post-Condition   5. Side-effects, (affect-type, affected-object)   USDL service description can be treated as a term of first-order logic [16]. The side-effect of a service comprises of an affect type and the affected object. The service can be converted into a triple as follows: (Pre-Conditions, affect-type(affected-object, I, O), Post-Conditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters of the service. We represent services as triples so that they can be treated as terms in first-order logic. The first-order logic unification algorithm [16] then can be extended to specialized unifications for exact, generic, specific, part and whole substitutions. This work is in progress [10]. Now that the formal definitions of concept, affects, conditions and service descriptions are given, we would like to extend the theory of substitutability over  so that we can reason about substitutability of services. Definition 16 (Substitution of a Web Service) Let  and  be two services where  is represented as (Pre-Condition, affect-type(affected-object, I, O), Post-Condition) and  is represented as (Pre-Condition , affect-type (affected-object , I , O ), Post-Condition ).  can safely be used in a context expecting service  if all of the following hold: 1. Pre-Condition is substitutable for Pre-Condition 2. If the terms affect-type(affected-object, I, O) and affect-type (affected-object , I , O ) can be unified by applying an extended unification algorithm. The unification mechanism applied is different based on the kind of substitution needed. 3. Post-Condition is substitutable for Post-Condition Definition 17 For any services S1 , S2  , we say S1 WordNet semantic relations. S2 if S1 is a substitutable of S2 based on one of the

Thus far our notions of service substitutability are based on the six WordNet semantic relations discussed earlier. However, one can define the notion of service substitutability independently using the actual semantics (e.g., denotational semantics) of the program that realizes this service. Consider a service S1 with inputs I1 and outputs O1 , and another service S2 with inputs I2 and outputs O2 ; we ignore the side-effects of these services for the moment. The ideal conditions under which service S1 can be substituted for service S2 is the following: I1 I2 and O1 O2 . 19

Essentially, the inputs needed by S1 must be present in the inputs being provided in anticipation of availability of S2 . Likewise, the outputs produced by service S1 should contain the outputs anticipated from service S2 . In such a case, S1 can be directly substituted for S2 . There can be other types of general substitution relation defined. However, for these other types of substitutions, the code of the service being used for substitution may have to be modified or wrappers placed around it. One can, however, develop a more general notion of substitutability based on denotational semantics [17]. Let [[S1 ]] and [[S2 ]] be the semantic denotations of programs that implement services S1 and S2 respectively (note that the side-effects of these services will be captured as the state that becomes an argument in a denotational definition). Note that [[S1 ]] and [[S2 ]] can be regarded as points in a complete partial order [17] that represents the space of all functions. Service S1 can be substituted for S2 if [[S1 ]] and [[S2 ]] lie in the same chain in the complete partial order (i.e., either [[S1 ]] [[S2 ]] or [[S2 ]] [[S1 ]] where is the relation that induces the complete partial order among denotations of the services). Given the definition of substitutability based on denotational semantics, one can prove the soundness and completeness of our notion of substitutability based on the WordNet semantic relations, i.e., S1 S2  [[S1 ]] [[S2 ]] (soundness) [[S1 ]] [[S2 ]]  S1 S2 (completeness) Intuitively, one can see that these relationships hold, since the relation is defined in terms of subsumption of terms describing the service's inputs and outputs and its effect (create, update, delete, find and the generic affects). These soundness and completeness proofs are not included here due to lack of space.

7

Service Discovery

Now that our theory of service substitutability has been developed, it can be used to build tools for automatically discovering services as well as for automatically composing them. It can also be used to build a service search engine that discovers matching services and ranks them. We assume that a directory of services has already been compiled, and that this directory includes a USDL description document for each service. Inclusion of the USDL description, makes service directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory to look for a "matching" service. A discovery engine gets USDL descriptions from a service directory and converts them into terms of logic. The terms corresponding to the USDL query can be compared with the terms from the directory using an extended/special unification algorithm. Depending on the type of match required, the unification mechanism could be different. That is, the matching or unification algorithm used can look for an exact, generic, specific, part or a whole match depending on the desire of the user. Part and Whole substitutions are not useful while looking for matching services, but are very useful while selecting services for service composition. Also using Part or Whole substitutions for discovery may produce undesired side-effects.

20

The discovery engine can also rank the various services discovered. In this scenario, the discovery engine returns a list of substitutable services after applying ranking based on the kind of match obtained. Exact substitutables are assigned the highest rank among the different kind of substitutables. The following is the default ranking order used for the different substitutions. 1. Exact Substitution: The matching service obtained is equivalent to the service in the query. 2. Generic Substitution: The matching service obtained subsumes the service in the query. 3. Specific Substitution: The matching service obtained is subsumed by the service in the query. 4. Whole Substitution: The matching service obtained is a composite service of the service in the query and some other services. 5. Part Substitution: The matching service obtained is a part of a composite service that the query describes. The development of a service discovery engine based on these ideas is in progress. With the USDL descriptions and query language in place, numerous applications become possible ranging from querying a database of services to rapid application development via automated integration tools and even real-time service composition [12]. Take our flight reservation service example. Assume that somebody wants to find a travel reservation service and that they query a USDL database containing general purpose flight reservation services, bus reservation services, etc. One could then form a USDL query consisting of a description of a travel reservation service and the database could respond with a set of travel reservation services whether it be via flight, bus, or some other means of travel. This flexibility of generalization and specialization is gained from semantic information provided by USDL.

8

Service Composition

For service composition, the first step is finding the set of composable services. USDL itself can be used to specify the requirements of the composed service that an application developer is seeking. Using the discovery engine, individual services that make up the composed service can be selected. Part substitution technique can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the individual services. That is, if a subservice S1 is composed with subservice S2 , then the postconditions of S1 must imply the preconditions of S2 . In fact, the WordNet Universal ontology can also be helpful in automatically discovering services that can be composed together to satisfy a service discovery query. To achieve this, the discovery engine looks at the USDL concepts that describe the service in the query. It then searches the WordNet ontology to find out the meronymous components of that concept. The services that exactly match the meronymous components are then discovered using the standard discovery mechanism. Preconditions and postcondition consistency is then used to find the order in which the meronymous components should be stitched together to produce the desired service. A service composition engine of this kind is under development. Such an engine can also aid a systems integrator in rapidly creating composite services, i.e., services consisting of the composition of already existing services. In fact, such an engine can also be extended to automatically generate boilerplate code to manage the composite service, as well as menial inter-service data format conversions needed to glue the meronymous components together.

21

9

Comparison with OWL-S, WSDL-S, and WSML

In this section we present a comparison of USDL with other popular approaches such as OWL-S [5], WSML [1], and WSDL-S [24]. Our goal is to identify the similarities and differences of USDL with these approaches. OWL-S is a service description language which attempts to address the problem of semantic description via a highly detailed service ontology. But OWL-S also allows for complicated combining forms, which seem to defeat the tractability and practicality of OWL-S. The focus in the design of OWL-S is to describe the structure of a service in terms of how it combines other sub-services (if any used). The description of atomic services in OWL-S is left under-specified [9]. OWL-S includes the tags presents to describe the service profile, and the tag describedBy to describe the service model. The profile describes the (possibly conditional) states that exist before and after the service is executed. The service model describes how the service is (algorithmically) constructed from other simpler services. What the service actually accomplishes has to be inferred from these two descriptions in OWL-S. Given that OWL-S uses complicated combining forms, inferring the task that a service actually performs is, in general, undecidable. In contrast, in USDL, what the service actually does is directly described (via the verb affects and its refinements create, update, delete, and find). OWL-S recommends that atomic services be defined using domain specific ontologies. Thus, OWL-S needs users describing the services and users using the services to know, understand and agree on domain specific ontologies in which the services are described. Hence, annotating services with OWL-S is a very time consuming, cumbersome, and invasive process. The complicated nature of OWL-S's combining forms, especially conditions and control constructs, seems to allow for the aforementioned semantic aliasing problem [9]. Other recent approaches such as WSMO, WSML, WSDL-S, etc., suffer from the same limitation [1]. In contrast, USDL uses the universal WordNet ontology to solve this problem. Note that USDL and OWL-S can be used together. A USDL description can be placed under the describedBy tag for atomic processes, while OWL-S can be used to compose atomic USDL services. Thus, USDL along with WordNet can be treated as the universal ontology that can make an OWL-S description complete. USDL documents can be used to describe the semantics of atomic services that OWL-S assumes will be described by domain specific ontologies and pointed to by the OWL-S describedBy tag. In this respect, USDL and OWL-S are complementary: USDL can be treated as an extension to OWL-S which makes OWL-S description easy to write and semantically more complete. OWL-S can also be regarded as the composition language for USDL. If a new service can be built by composing a few already existing services, then this new service can be described in OWL-S using the USDL descriptions of the existing services. Next, this new service can be automatically generated from its OWL-S description. The control constructs like Sequence and If-Then-Else of OWL-S allows us to achieve this. Note once a composite service has been defined using OWL-S that uses atomic services described in USDL, a new USDL description must be written for this composite service (automatic generation of this description is currently being investigated [10]). This USDL description is the formal documentation of the new composite service and will make it automatically searchable once the new service is placed in the directory service. It also allows this composite service to be treated as an atomic service by some other application. For example, the aforementioned ReserveFlight service which creates a flight reservation can be viewed as a composite process of first getting the flight details, then checking the flight availabil-

22

ity and then booking the flight (creating the reservation). If we have these three atomic services namely GetFlightDetails, CheckFlightAvailability and BookFlight then we can create our ReserveFlight service by composing these three services in sequence using the OWL-S Sequence construct. The following is the OWL-S description of the composed ReserveFlight service.
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:process="http://www.daml.org/services/owl-s/1.0/Process.owl#"> <process:CompositeProcess rdf:ID="ReserveFlight"> <process:composedOf> <process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#GetFlightDetails"/> <process:AtomicProcess rdf:about="#CheckFlightAvailability"/> <process:AtomicProcess rdf:about="#BookFlight"/> </process:components> </process:Sequence> </process:composedOf> </process:CompositeProcess> </rdf:RDF>

We can generate this composed ReserveFlight service automatically. The component services can be discovered from existing services using their USDL descriptions. Once we have the component services, the OWL-S description can be used to generate the new composed service.

10

Related Work

Discovery and composition of web services has been active area of research recently [18, 19, 20, 21, 22, 23]. Most of these approaches are based on capturing the formal semantics of the service using an action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. In contrast, we rely more on WordNet (which we use as a universal ontology) and the meronymous relationships of WordNet lexemes to achieve automatic composition. The approaches proposed by others also rely on a domain specific ontology (specified on OWL/DAML), and thus suffer from the problem mentioned earlier, namely, to discover/compose such services the discovery/composition engine has to be aware of the domain specific ontology. Thus, completely general discovery and composition engines cannot be built. Additionally, the domain specific ontology has to be quite extensive in that any relationship that can possibly exist between two terms in the ontology must be included in the ontology. In contrast, in our approach, the complex relationships (USDL concepts) that might be used to describe services or their inputs and outputs are part of USDL descriptions and not the ontology. Note that our approach is quite general, and it will work for domain specific ontologies as well, as long as the synonym, antonym, hyponym, hypernym, meronym, and holonym relations are defined between the various terms of the domain specific ontology. Another related area of research involves message conversation constraints, also known as behavioral signatures [13]. Behavior signature models do not stray far from the explicit description of the lexical form of messages, they expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional 23

implementation constraints, while USDL deals with higher-level real world concepts. However, USDL and behavioral signatures can be regarded as complementary concepts when taken in the context of real world service composition and both technologies are currently being used in the development of a commercial services integration tool [12].

11

Conclusions and Future Work

To reliably catalogue, search and compose services in a semi-automatic to fully-automatic manner we need standards to publish and document services. This requires language standards for specifying not just the syntax, i.e., prototypes of service procedures and messages, but it also necessitates a standard formal, yet high-level means for specifying the semantics of service procedures and messages. We have addressed these issues by defining a universal service-semantics description language, its semantics, and we have proved some useful properties about this language. The current version of USDL incorporates current standards in a way to further aid markup of IT services by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. This approach is more practical and tractable than other approaches because description documents are more easily created by humans and more easily processed by computers. USDL is currently being used to formally describe web-services related to emergency response functions [11]. Our current and future work involves the application of USDL to formally describing commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as to service discovery and rapid application development (RAD) in commercial environments [12]. Current and future work also includes automatically generating USDL description from the code/documentation of a service [12] as well developing tools that will allow automatic generation of new services based on combining USDL descriptions of existing atomic services. The interesting problem that arises then: can USDL description of such automatically generated services be also automatically generated? This problem is also part of our current/future work.

References
[1] A conceptual comparison between WSMO and OWL-S. www.wsmo.org/TR/d4/d4.1/v0.1. [2] Ontology-based information management system, wordnet OWL-Ontology. http://taurus. unine.ch/knowler/wordnet.html. [3] Resource Description Framework. http://www.w3.org/RDF. [4] SAP Interface Repository. http://ifr.sap.com/catalog/query.asp. [5] Semantic markup for web services. www.daml.org/services/owl-s/1.0/owl-s.html. [6] Web Ontology Language Reference. http://www.w3.org/TR/owl-ref. [7] Web Services Description Language. http://www.w3.org/TR/wsdl. [8] WordNet: A Lexical Database for the English Language. www.cogsci.princeton.edu/~wn. [9] S. Balzer, T. Liebig, and M. Wagner. Pitfalls of OWL-S - a practical semantic web use case. In ICSOC, 2004. [10] S. Kona, A. Bansal, G. Gupta, and T. Hite. Automatic Service Discovery and Composition with USDL. Working paper, 2006. 24

[11] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. D. Harris, and J. C. Staves. Towards Intelligent Services: A case study in chemical emergency response. In International Conference on Web Services, pp. 751-758, 2005. [12] T. Hite. Service Composition and Ranking: A strategic overview. Internal Report, Metallect Inc., 2005. [13] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004. [14] L. Simon, A. Bansal, A. Mallya, S. Kona, G. Gupta, and T. Hite. Towards a Universal Service Description Language. In Next Generation Web Services Practices, pp. 175-180, 2005. [15] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal ServiceSemantics Description Language. In European Conference On Web Services, pp. 214-225, 2005. [16] J.W. Lloyd. Foundations of Logic Programming. Springer-Verlag, 1987. [17] D. Schmidt. Denotational Semantics: A Methodology for Language Development. 1986. [18] B. Srivastava, J. Koehler. Web Services Composition - Current Solutions and Open Problems. In ICAPS, 2003. [19] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp. 46-53, Mar. 2001. [20] S. McIlraith, T.C. Son Adapting golog for composition of semantic web services. In KRR, pages 482493, 2002. [21] S. McIlraith, S. Narayanan Simulation, verification and automated composition of web services. In World Wide Web Conference, 2002. [22] G. Picinielli, et al. Web service interfaces for inter-orgranizational business processes - an infrastructure for automated reconciliation. In EDOC, pages 285-292, 2002. [23] B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pages 467477. [24] Web Service Semantics - WSDL-S http://www.w3.org/Submission/WSDL-S.

25

220

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

A Low-Power Integrated x8664 and Graphics Processor for Mobile Computing Devices
Denis Foley, Member, IEEE, Pankaj Bansal, Member, IEEE, Don Cherepacha, Robert Wasmuth, Aswin Gunasekar, Srinivasa Gutta, and Ajay Naini
switching between modes depending on the current allowed power state. Two unique display output streams can be presented over any two of the following: 1) a DisplayPort (DP1.1a) interface; 2) a combination low-voltage differential signaling (LVDS)/DP1.1a port; or 3) an integrated video graphics array (VGA) DAC. A digital frequency synthesizer (DFS) block supplies the CPU core, graphics, multi-media, display, I/O, and NB clocks. The AMD Fusion architecture implements a very efficient form of unified memory architecture (UMA) in which a portion of system memory is reserved as graphics frame buffer memory. The graphics memory controller (GMC) arbitrates between graphics, video, and display memory requests and presents a well-ordered stream of system memory transactions through the NB over dedicated 256-bit-wide read and write busses. These GMC requests bypass all NB coherency mechanisms, allowing for fast direct access to memory and exposing all of the available memory bandwidth (8.53 GB/s). III. TECHNOLOGY Zacate is implemented in a 40 nm bulk CMOS process. Ten metal layers are used, the top two of which are redistribution layer (RDL). The die area is 75 mm . Each Bobcat core is 4.9 mm , and each L2 is 3.1 mm . Each core has approximately 7 nF of on-die capacitance. Excluding PHYs, the graphics, I/O, and multi-media blocks occupy 35 mm . There is approximately 60 nF of on-die capacitance associated with this logic. The die is packaged in a 19 mm 19 mm ball-grid array (BGA) package with 413 0.8 mm balls. The package substrate is a 2-2-2 layup. 10 nF of VDDNB package capacitance and 3.3 F of VDD capacitance are supported. Fig. 2 shows a Zacate die photograph with functional units labeled. IV. BOBCAT CORE The Bobcat core shown in Fig. 3 is a brand-new x86 design making its debut in Zacate. The core features a decoder capable of decoding two complex operations (COPs) per cycle. It supports the AMD64 64-bit ISA. The execution engine supports full out-of-order (OoO) execution, and the load/store engine can execute loads and stores out of order. The design features a high-performance floating-point unit and an advanced branch predictor. Streaming SIMD extensions including SSE1, SSE2, SSE3, SSSE3, SSE4A, and 128-bit mis-aligned data-type extensions are also supported. The design features 32 KB L1 caches and a dedicated 512 KB L2 cache. The design runs at 1.6 GHz, with the L2 running at half that rate. Bobcat supports core power gating.

Abstract--The first AMD FusionTM accelerated processing unit (APU), code-named "Zacate," incorporates a pair of Bobcat x86 processors, a 1 MB L2 cache, an AMD RadeonTM 6310 DirectX11 GPU with 80 stream processors, a media accelerator, an integrated NorthBridge (NB), integrated DisplayPort, LVDS, and VGA display interfaces, a PCIe Gen1 or Gen2 I/O interface, and a single 64-bit memory channel at up to DDR3-1066 on a single die implemented in a 40 nm bulk CMOS process. Index Terms--AMD fusion, APU, Bobcat, integrated graphics, low-power, Zacate.

I. INTRODUCTION HE AMD FusionTM accelerated processing unit (APU) code-named Zacate is implemented in a 40 nm CMOS bulk process. The design features the new synthesizable low-power Bobcat x6464 core and integrated AMD RadeonTM graphics. This paper shares details on the architecture, technology, functional units, justification for the AMD Fusion approach, and details on power savings techniques, power gating, and clocking. Some performance data is shared to provide context for the power profile of the device. This paper is divided into 13 sections including this introduction, technical topics, performance, conclusion, and references.

T

II. ARCHITECTURE The AMD Fusion APU code-named Zacate shown in Fig. 1 incorporates two low-power Bobcat x86 processors, each with a dedicated 512 KB L2 cache, an AMD RadeonTM 6310 DirectX11 graphics processing unit (GPU), and AMD's Universal Video Decoder (UVD) media acceleration engine. Memory access to a single 64-bit DDR3-1066 memory channel is controlled through an integrated NorthBridge (NB). Zacate supports a four-lane PCIe interface to the AMD Fusion Controller Hub (FCH) and a four-lane PCIe interface to an external GPU if desired. The PCIe links are capable of running at either 2.5 Gb/s Gen1 rate or 5 Gb/s Gen2 rate, and are capable of
Manuscript received April 26, 2011; revised June 22, 2011; accepted July 05, 2011. Date of publication October 19, 2011; date of current version December 23, 2011. This paper was approved by Guest Editor Alice Wang. D. Foley is with Advanced Micro Devices (AMD), Inc., Boxborough, MA 01719 USA (e-mail: denis.foley@amd.com). P. Bansal, S. Gutta, and A. Naini are with Advanced Micro Devices (AMD), Hyderabad 500034 A.P., India. D. Cherepacha is with Advanced Micro Devices (AMD), Oakville, Markham, ON, Canada L6H 6T5. R. Wasmuth and A. Gunasekar are with Advanced Micro Devices (AMD), Austin, TX 78735 USA. Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/JSSC.2011.2167776

0018-9200/$26.00  2011 IEEE

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

221

Fig. 1. Zacate block diagram.

Fig. 2. Zacate die shot with labeled functional units.

V. GRAPHICS AND MULTI-MEDIA Zacate contains a small, power-efficient AMD RadeonTM HD 6310 GPU that is sized to take full advantage of the 8.53 GB/s of memory bandwidth provided by the 64-bit channel of DDR31066 memory. The GPU comprises graphics, video, audio, and display capabilities similar to a discrete graphics card. The APU uses a UMA in which the GPU's frame buffer is implemented
Fig. 3. Bobcat low-power core.

by reserving a section of system memory. Typical memory allocation for the frame buffer is 384 MB. Additional GPU memory can be allocated dynamically from system memory.

222

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

TABLE I BLU-RAY PLAYBACK

The DirectX 11-compliant graphics core contains 80 stream processing units, eight texture units, 16 Z/Stencil ROP units, and four color ROP units. The computational units are organized as a 2-SIMD x 8 array of vector processors in which each vector processor consists of five stream processing units. The unified shader architecture allows a flexible set of programs to be executed. At 492 MHz, the core's peak rate of 78.7 GFLOPs provides sufficient compute capability for gaming or compute applications enabled by DirectCompute and OpenCL. Utilizing the AMD's latest video decoder, UVD3, Zacate provides uncompromised HD video playback, including many advanced video quality processing algorithms. UVD3 provides hardware acceleration for decode of H.264, VC-1, MPEG-2, and DivX/Xvid video streams. This facilitates low-power 1080p/1080i video Blu-ray playback. Table I shows measured CPU utilization when playing the Avatar Blu-ray disc (BD) with UVD acceleration enabled and disabled. Without acceleration, the CPU cores are fully utilized and unable to play the video without dropping video frames. With UVD acceleration, the CPU load is reduced to approximately 31%, facilitating a smooth video experience. The UVD also offloads from the CPU the decode of a variety of on-line video content, including Adobe Flash video. Zacate provides two independent display interfaces that can natively support VGA, LVDS, and DisplayPort. HDMI and DVI can be supported with additional system-level components. High-definition audio is supported, including Dolby TrueHD and DTS-HD Master Audio. The GIO block, shown in Fig. 1, performs system connectivity functions to route host and DMA traffic between the CPU cores, system memory, internal devices, and external devices. It contains a root complex supporting two four-lane PCIe Gen1 or Gen2 links. One link serves as the unified media interface (UMI) to the FCH. The second link supports discrete graphics attachments. VI. FUSION BASICS The traditional model of a processor chip (with integrated NB) coupled with an integrated graphics processor has a number of shortfalls. The high-speed PHY coupling the two processors (shown in red in Fig. 4) occupies significant area and consumes power. The power associated with the PHY can exceed 1 W during media playback. Additionally, the link may present a bandwidth bottleneck. When the two dies are integrated, a wide (256 bits in each direction) data path from the graphics memory controller to the NB is added, allowing for full access to system memory from the GMC. This path provides GMC clients with

Fig. 4. AMD fusion advantage.

Fig. 5. Bobcat floorplan.

a low latency path to non-snooped regions of system memory, reducing the minimum read latency by up to 40%. Compared

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

223

Fig. 7. Routing allocation in channel for Sub-chips.

Fig. 6. Sub-chip1 and Sub-chip2 partitioning.

to two-chip solutions, use of the on-die integrated GPU significantly reduces memory latency, improves request ordering, and reduces area and power. VII. IMPLEMENTATION The AMD Fusion APU--including the x86 Bobcat cores--is synthesized from RTL and implemented using standard ASIC-style synthesis auto-place and route (SAPR) flows. The die shown in Fig. 2 has more than 450 million transistors. The Bobcat core is implemented as a single physical entity consisting of 1.1 million instances and uses 35 instances of seven custom memory macros. The GPU core logical RTL is partitioned into multiple physical entities with varying numbers of standard cell instances and memory macros generated using standard memory compilers. The GPU sub-chip uses feed-through repeater bundles to connect the interfaces between different physical entities and has minimal grout space at the boundaries. The Bobcat floorplan shown in Fig. 5 shows the various subblocks in the core as placement regions. L2 sub-array, tag array,

and caches occupy most of the top and bottom of the floorplan, while the bus unit and floating-point unit are placement regions placed using various placement optimization techniques. The typical amoeba placement for various other sub-blocks in the floorplan shows the placement as achieved through careful floorplanning and tool optimization. The SOC was implemented in two sub-chips, and a two-level hierarchical floorplanning and partitioning approach was used. Sub-chip1 consisted of the Bobcat core cluster, NB, and all I/O PHYs. Sub-chip2 consisted of the graphics core and multi-media and I/O control, collectively referred to as the GNB (Fig. 6). Sub-chip1 and Sub-chip2 were further floorplanned to create an overlay channel between the Bobcat cores and DDR PHY through GNB sub-chip. Thus GNB Sub-chip2 could be independently designed and integrated with Sub-chip1 in SOC without causing any routing and integration issues. As shown in Fig. 7, specific metal layers were allocated to Sub-chip1 and Sub-chip2 for their respective signal interactions in horizontal and vertical directions. The CPU and GPU core operate on separate voltage supplies and support dynamic voltage and frequency scaling (DVFS) to optimize power consumption. The design is optimized for power and performance at discrete process and voltage points. The CPU is optimized for power and performance at 1.2 V and 0.8 V. Timing optimization at high voltage secures the performance of the CPU, while timing at low voltage secures the minimum

224

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

Fig. 8. Frequency vs. voltage for a representative GFX (SVT) path.

Fig. 10. Transistor threshold voltage mix.

Fig. 9. SOC statistics.

operating frequency and ensures that the best performance/watt is provided at the low frequency. The GPU is optimized at 1.0 V, 0.9 V, and 0.8 V. Incremental functionality or feature sets can be enabled at progressively higher voltages. The GPU design is closed at multiple points to ensure that the incremental capability is matched across the design elements and is optimized for performance/watt at all operating points. Fig. 8 shows how frequency of a representative GPU path--a mix of device and wire delays using SVT devices--varies with device voltage. Standard cell logic transistors comprise 2/3 of the total 451 million transistors, while memories and macros cover the remaining 1/3. The dual Bobcat core and L2 account for 12 Mb of the 23 Mb on-chip memory. Extensive powers saving techniques were used, including VT swapping across the GPU design and the Bobcat core. The design has transistors fabricated in various threshold voltages and lengths to facilitate performance/leakage trade-offs. At 105 , LVT devices are approximately 4x leakier than SVT devices, which are in turn approximately 3.5x leakier than HVT devices. As shown in Fig. 10, approximately 56% of the logic transistors are HVT, and approximately 42% are SVT. To limit leakage power, less than 2% are LVT devices, and those are used only in critical paths. The VT distribution contains the regular-and different-length variants for the same VT devices to further reduce power leakage without compromising performance/area.

Fig. 11. DFS clock generation.

VIII. CLOCKING The design has 16 functional, 10 scan, and 11 debug mode clocks. A digital frequency synthesizer (DFS) is used to generate nine functional clocks used by the CPUs, NB, and GPU (Fig. 11). The system PLL provides four phase-offset references to the DFS, which combines the phases to generate the required clock frequencies. As shown in Fig. 12, ClkEn_A[3:0] is a 4-bit phase-enable value that is presented to the DFS every VCO period. Each of the bits corresponds to one of the VCO output phases. If a phase is enabled in a particular VCO period (shaded in blue in Fig. 12), that phase is combined with other enabled phases to generate the output clock. By controlling a repeating phase-enable sequence, different clock frequencies can be generated easily. Because the

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

225

Fig. 12. Generating clocks using DFS.

Fig. 13. Clock delivery.

phase-enable sequence can be changed at any time, clock generation is not interrupted on frequency transitions. This clock generation mechanism builds on a similar mechanism used in AMD TurionTM processors, but adds more VCO phases for finer frequency control. The high-speed CPU clock is distributed as a mesh network with six to eight levels of local buffers. The GPU clock (SCLK) is implemented with X-tree and H-tree topologies feeding into the clock tree synthesis (CTS) branches inside a physical tile. All other clocks are implemented as balanced buffer trees from DFS to the physical tile. The NB and GPU clocks are balanced globally with respect to each other within a half-cycle of the fastest clock using programmable delay buffers pre-placed beside the global trees. With such an implementation, Zacate achieves a global skew of 38 ps and 50 ps in CPU and GPU clock distribution, respectively. Fig. 13 shows the two distinctive clock tree structures as deployed for top-level clock distribution in the two sub-chips. A 5% clock jitter target was used for the synthesized clocks.

Fig. 14. Zacate power domains.

IX. DESIGN FOR POWER Zacate's various power domains are shown in Fig. 14. Each Bobcat core and its L2 cache together share a power island (Fig. 15). A single variable VDD rail supplies both core power islands. Core clocks can be varied independently and VDD (CPU voltage) is selected to be the lowest voltage required to support the highest of the selected core clocks. Bobcat supports Core C6 (CC6) power state. If the core is idle, its caches can be flushed, its state saved to memory, and power to the core island is gated off. If both cores are idle and power-gated, the VDD

226

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

Fig. 17. VDDNB global power grid implementation.

Fig. 15. Bobcat power gating.

Fig. 16. VDDNB power gating.

power rail can be further reduced to eliminate leakage in even the power-gating structures. This state is called Package C6 (PC6). In PC6, voltage can be lowered to 0 V, but with frequent entry/exit to PC6, VDD is typically set at 0.45 V to minimize voltage ramp time and energy wasted in charging/discharging motherboard capacitance. GPU and the NB share a variable VDDNB rail as shown in Fig. 16. GPU and NB clocks are varied independently based on activity. VDDNB is selected based on the highest voltage requested. The GPU supports several power islands, allowing for driver-controlled power gating of the UVD video acceleration engine and independent dynamic power gating of the GFX core and the GMC. The display controller supports a static-screen refresh-stutter mode in which the controller requests data periodically from memory; between requests, the memory is kept in self-refresh and the NB enters a low-power state. By stuttering the display requests to memory, the time spent in self-refresh for the memory can be maximized, the time that clocks are running in the NB can be minimized and startup penalties (PLL power on and lock, DLL lock time) can be amortized over a larger memory transfer. Typical stutter efficiency (time spent in self-refresh during static screen rendering) is greater than 94%. APU MM07 power varies by approximately 130 mW for every 10% change in stutter efficiency. The combination of power savings techniques yields an APU with an average MobileMark (MM07) power consumption of less than 1.8 W. X. POWER GATING IMPLEMENTATION On-die power gating is implemented using a leakage-optimized HVT PFET header switch to isolate the VDD (or VDDNB) supplies. The header uses parallel stacked PFET transistors with separate enables for each PFET. This is in contrast to other x86 designs [1], [2] in which VSS isolation is

used. The Bobcat core uses a dense grid implemented in M9 and M10 to satisfy the high current density in the core and to reduce IR drop impact on frequency. The total PFET width for a core and its L2 is about 1.0 m. The corresponding number of power-gating PFETs is approximately 30,000. VDDNB power grid is implemented in M7 and M8 with power-gater PFETs placed on a checkerboard pattern (Fig. 17). Power gating for compiled memories is arranged at the top and bottom of the memory to avoid interference with the memory arrays. Tall memories implement a double row of PFETs at the top and bottom, while shorter memories implement a single row at the top and bottom. This is illustrated in Fig. 18, which is a close-up of a GPU tile showing the checkerboard pattern for logic and the single or double rows of PFETs for compiled memories. Each PFET header, as shown in Fig. 19, is made from two FETS with separate enables. A smaller WAKE FET is used to initiate current flow into the grid on enable. The controls for the WAKE FETs are daisy-chained with a return path to the power controller. Once the WAKE FETs have been enabled, large RUN FETs can be turned on to provide a robust low-onresistance connection to the power grid. The controls for the RUN FETs are similarly daisy-chained; once all the RUN FETs have been enabled, the power to the gated region is completely restored. This WAKE/RUN sequence is critical to avoid in-rush current spikes as the grid is enabled. The WAKE/RUN timing is programmable, allowing for post-silicon tuning of the delays in enabling the grid. The VDDNB grid is designed to drive a current density of 0.5 A/mm and achieve a 2% static IR drop. The total gate width of GPU headers is 1.93 m and resistance is 0.6 m . Fig. 20 shows the simulated voltage drop distribution across PFET headers used in the design. The switches were uniformly distributed in the design through pre-placement, and the voltage drop varies across switches depending on density and logic in the region. Fig. 21 shows four photon-emission captures of the Zacate die. All these captures are on a tester with clocks disabled, so the current flow is purely leakage. Capture duration was 30 seconds. 1.225 V and VDDNB Captures 1, 2, and 3 have VDD 0.875 V; capture 4 has VDD 0.875 V, and VDDNB 0.875 V. In capture 1, all functional units are enabled. In capture 2, CPU0 is power-gated off. In capture 3, CPU1 is gated off. In capture 4, all power-gated areas are off. The effect of power gating is immediately obvious. Capture 4 also highlights areas that are not gated off  the always on (AON) blocks. The small active area adjacent to the gated core in capture 2 and capture 3 is

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

227

Fig. 18. Header distribution in typical GPU tile.

Fig. 20. Voltage across VDDNB PFETs.

Fig. 19. WAKE/RUN power gater control.

interface logic between the core and the NB channel that was not power-gated. XI. BOBCAT FLOP Two flop types were used in the CPU core: a conventional master-slave flop, and a Bobcat flop (BT flop) shown in Fig. 22--a three-stage, pre-charged, asymmetric, MUXD flop. The BT flop is much faster than the master-slave flop, but is also bigger. The BT flop was used only on critical paths, thereby realizing its speed advantages while increasing the core area only a small amount.

Fig. 21. Meridian photon-emission power-gating analysis.

228

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

Fig. 22. Bobcat high-speed flop.

TABLE II IMPACT ON AREA AND FREQUENCY OF BT FLOP

The BT flop operates similarly to a sense amp flop [3]. While clock is low, both NFET stacks of Stage 1 are pre-charged. is The half tri-state of Stage 2 is, therefore, disabled, and determined by the active latch of Stage 3. When clock rises, Stage 1 is evaluated, Stage 2 is enabled, and Stage 3 feedback is disabled. The BT flop is faster than a sense amp flop for two reasons: stack order in Stage 1, and size skewing afforded by asymmetry of the transitions. In Stage 1, cross-coupled NFETs are at the bottom of the stack, and pre-discharged when clock is low. Last-arriving data is at the top of stack. Regarding the second advantage, the BT flop is nominally three gate delays if the left NFET stack of Stage 1 discharges, but only two gate delays if the right stack discharges. This is exploited in the beta ratios of each stage so rising and falling transitions have nearly equal clock-to- delays. A majority of critical timing paths in processor designs are limited by slow setup, clock-to- , or rising/falling edge, but not all of them simultaneously. A few experiments validated the hypothesis that SAPR tools could take advantage of flops designed to be the best in any given performance metric. A flop library development effort focused on using such a flop as the base style and building specialist variants of the same to maximize frequency. A close review of the top critical paths revealed important criteria such as output edge rate, fast rising/ fast falling, clock cycle extension, or clock pin buffering/unbuffering to favor fast clock-to- or setup, or slow buffer to

TABLE III GPU CAPABILITIES

further favor setup. Other criteria included increasing the range of drive strengths and adding combinational functions such as NAND and NOR. The resultant library contained about 100 flop variants to optimize for the various combinations of these features. Performance is summarized in Table II. BT flops have longer hold time and higher dynamic power than master-slave flops, but when their usage was limited to only critical paths, frequency improvement for the CPU core was 7%. XII. PERFORMANCE Bobcat is an AMD64 86 ISA core targeting low-cost/area and high power efficiency while maintaining comparable singlethread performance to mainstream client processors such as the

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

229

TABLE IV ZACATE GPU PERFORMANCE RELATIVE TO RADEON 4270

1.6 GHz AMD Turion Neo X2 L625, derived from the first-generation AMD OpteronTM core [4]. Although many applications are now multi-threaded and can take advantage of higher performance of Zacate's two cores, numerous user-sensitive operations are dependent on the performance of a single thread. Central to Bobcat providing high single-thread performance was the choice of OoO execution with a two-wide decode/retire scheme. The fully OoO design with deep speculation enables energy saving by keeping the pipeline filled while reducing dynamic and static energy spent on stalled cycles as well as significantly increasing IPC relative to in-order designs used in other contemporary small cores. To reduce power, several micro-architectural approaches were used, including minimizing over-provisioning [5], reducing mis-speculation, and minimizing data transfers and data structure accesses. The twowide design, relative to the three-wide design in the comparison machine, minimized over-provisioning in terms of execution units, register file ports, etc., but with some loss of performance, some of which was recovered through other optimizations. To reduce mis-speculation and improve performance, a sophisticated branch prediction structure was developed that utilized 32 k two-way L1 instruction cache with a return stack and indirect dynamic and advanced conditional branch predictors with a high-capacity 512/8 entry (4 k/2 m pages) instruction translation buffer (I-TLB). A physical register file is used both by the integer and floating-point units to reduce data transfers. The floating-point unit (FP) has a two execution stacks capable of two single-precision (SP) single-instruction/multipledata (SIMD) adds and two SP SIMD multiplies per cycle. The load-store unit (LS) is fully OoO with a hazard predictor to minimize mis-speculations, a 32 k eight-way data cache with a 40/8 entry (4 k/2 m) and a 512/64(4 k/2 m) entry Level 2 data translation buffer (L2DTLB), and an eight-stream data pre-fetcher. The instruction and data cache share an ECC-protected 16-way 512 KB per-core private L2 cache, clocked at half the core rate to reduce power. Fig. 23 illustrates single-thread performance comparable ( 0.9x) to the AMD Turion L625 based on SPECint2006 [6]. This is achieved in less than half the process scaled area and dynamic power of the AMD Turion L625 core. SPECint2006 is a benchmark that includes a broad range of compute-in-

Fig. 23. Bobcat performance relative to AMD turion L625.

tensive integer applications that stress different aspects of the processor, from branch prediction to the cache hierarchy. Table III specifies the basic performance metrics for the graphics core. The low-latency, high-bandwidth GMC memory interface increases performance per watt by allowing a higher percentage of the power to be used for computation rather than for moving large amounts of data across high-power chip-to-chip interconnects. For example, the power consumed by the HyperTransportTM link between the CPU and the AMD 880 G chipset was more than 1 W during periods of high GPU memory activity. With the AMD Fusion architecture, most of this interconnect power is saved. The advanced, multi-level memory arbitration units resolve performance issues with previous integrated graphics processors (IGP) by enabling the GPU to achieve memory efficiency similar to a discrete GPU without compromising CPU performance. GPU memory accesses are sent in DRAM efficient streams of reads and writes to avoid memory access penalties while CPU accesses are optimized for low read latency. These improvements yield a 17% increase in Zacate UMA memory efficiency compared to its two-chip predecessor.1 Table IV illustrates the gaming capabilities of Zacate using the industry-standard 3DMark benchmarks. The low-cost,
1Memory efficiency was measured using the Win7 Experience Index (WEI) graphics memory benchmark using system populated with one 64-bit DDR31066 SODIMM.

230

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

TABLE V 3D GAME PERFORMANCE

power-efficient Zacate APU outperforms the mainstream mobile AMD RadeonTM 4270 M GPU in the AMD 880 G chipset. In 3DMark Vantage  Performance, Zacate is 2.26 times faster. Table V shows benchmarking results using several popular games. In addition to providing computational resources for 3D graphics operations, the 80 stream processing units may also be used to speed up many general-purpose floating-point-intensive applications. The OpenCL implementation of the floating-point-intensive Mandelbrot set generator in SiSoftware Sandra 2011 multi-media benchmark [7] illustrates this capability, yielding a 5x improvement in performance of the benchmark running on the GPU versus a multi-threaded, SIMD-vectorized implementation on the CPU cores. This result is consistent with the GPU-to-CPU peak FLOPS ratio of 6 to 1 (78.7 GFLOPs versus 12.8 GFLOPs).

Denis Foley (M'06) received the B.Eng.(Elect.) degree from University College Cork, Ireland, in 1983. He is a Senior Fellow at Advanced Micro Devices, Inc. He has more than 28 years of experience in the computing industry. At Digital Equipment and Compaq, he was a design lead in the Alpha server group. At Hewlett-Packard, he was the implementation lead for a high-end Itanium server. At ATI, he was the chip lead for a game console cost-down before turning his attention to low-power design, first as the system architect for a licensable 3D GPU core, then as the system architect for ATI's Imageon family of application processors for hand-held devices. With AMD's acquisition of ATI, he moved into the SOC architect role for a number of AMD's low-power designs, including the recently announced AMD Fusion Zacate and Ontario APUs.

XIII. CONCLUSION The AMD Fusion Zacate APU brings together x86 processors, NB, I/O, multi-media acceleration, and compelling graphics processing capability on a single die. Using the low-power techniques described in this document, the part provides an excellent balance of performance and long battery life.

Pankaj Bansal (M'11) received the B.Tech. (E.E.) degree from the Indian Institute of Technology, Delhi, India, in 1997. Since then, he has worked on various domains in microarchitecture, logic design, verification, and physical/circuit design/analysis at companies such as Intel, Freescale, Centillium, and Beceem. Prior to joining AMD in 2009, he was SOC lead/manager for 3G baseband ICs at Freescale. Since joining AMD in 2009, he has been responsible for multiple discrete GPU parts. Most recently, he has been leading design of the low-power AMD Fusion APUs from AMD's India Design Center.

REFERENCES
[1] R. Jotwani et al., "An x8664 core implemented in 32 nm SOI CMOS," in 2010 IEEE Int. Solid-State Circuits Conf. (ISSCC) Dig. Tech. Papers, Feb. 711, 2010, pp. 106107. [2] T. Fischer et al., "Design solutions for the bulldozer 32 nm SOI 2-core processor module in an 8-core CPU," in 2011 IEEE Int. Solid-State Circuits Conf. (ISSCC) Dig. Tech. Papers, Feb. 2011, pp. 7879. [3] J. Montanaro et al., "A 160-MHz, 32-b, 0.5-W CMOS RISC microprocessor," IEEE J. Solid-State Circuits, vol. 31, no. 11, pp. 17031714, Nov. 1996. [4] C. N. Keltcher, K. J. McGrath, A. Ahmed, and P. Conway, "The AMD Opteron processor for multiprocessor servers," IEEE Micro, vol. 23, no. 2, pp. 6676, Mar.-Apr. 2003. [5] K. Natarajan, H. Hanson, S. W. Keckler, C. R. Moore, and D. Burger, "Microprocessor pipeline energy analysis," in Proc. 2003 IEEE Int. Symp. Low Power Electronics and Design (ISLPED), Aug. 2527, 2003, pp. 282287, Session 12. [6] SPEC CPU 2006 Benchmark Suite, Standard Performance Evaluation Corp. (SPEC). [Online]. Available: http://www.spec.org/benchmarks. html, Reported results are estimated because measurements were made on pre-production hardware. [7] SiSoftware: Sandra 2011. [Online]. Available: http://www.sisoftware. net/

Don Cherepacha received B.A.Sc. and M.A.Sc. degrees from the University of Toronto, Toronto, ON, Canada, in 1991 and 1994, respectively. He is currently a Fellow at AMD in Markham, Ontario, and has more than 18 years of ASIC architecture, design, and verification experience. He joined LSI Logic in 1993 to work on PC and server chipsets as well as PCI and USB core development. He moved to Cogency Semiconductor, Inc. in 1998, where he led the ASIC architecture and development of that company's first HomePlug powerline networking ICs. He then joined ATI/AMD in 2004 and has worked in the areas of IGP/AMD Fusion architecture and performance for a range of chipsets and APUs.

Robert Wasmuth received the B.S.E.E. degree from the University of Texas at Austin in 1984. He then joined IBM working on logic synthesis, circuit design tools, and performance modeling for the POWER series of microprocessors. He did performance analysis and modeling for various designs, including a network processor, a web-based transaction system, and a VOIP switch at different start-ups in Austin. After joining AMD in 2002, he has been involved in various aspects of x86 design, most recently leading the performance and power modeling team for the Bobcat core and SOC.

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

231

Aswin Gunasekar received the B.S. degree in electrical engineering from the University of Madras, India, in 2001, and the M.S. degree in computer engineering from the University of Texas, Austin, in 2003. He joined AMD in 2004, and has worked in a variety of design roles that include I/Os, SRAMs, and high-performance standard cell libraries. He currently leads design methodology for a low-power family of microprocessors. He has four pending patents on processor circuits.

Ajay Naini received the M.S. degree from Mississippi State University. He is a Senior Director at AMD. He has 25 years of CPU design experience and worked at Motorola, Cyrix, and HaL Computer Systems in various capacities before joining AMD 10 years ago. He was one of the lead engineers on the K8 processor family design at AMD, which delivered the first 64-bit x86 microprocessor and the first dual-core processor. Most recently, he was the Project Director for the AMD Fusion Zacate/Ontario design. He has more than 10 patents and several publications in floating-point and microprocessor design.

Srinivasa Gutta received the B.E. (Electronics and Communication) degree from Osmania University, Hyderabad, India, and the M.S.E.E. from the University of Texas at San Antonio. He has 18 years of experience in SOC design, verification, and architecture. He started his career at CommQuest Technologies (acquired by IBM) designing voice coder DSPs for GSM baseband chips. He then joined Lucent/Agere Systems and spent 13 years working as SOC lead/architect on various communication chips, including the world's first PCI modem, K56Flex modem, and ADSL-Lite modems for client and central office. He also worked as lead engineer on second- and third-generation Sirius satellite radio chipset and mobile application processors in the Agere Mobility division. At AMD, he led the design of the Imageon audio processor and AMD Fusion Zacate/Ontario APU from AMD's India Design Center.

A Universal Service Description Language
Luke Simon, Ajay Mallya, Ajay Bansal, Gopal Gupta Department of Computer Science University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp 2400 Dallas Parkway Plano, TX 75093

Abstract
To fully utilize web-services, users and applications should be able to discover, deploy, compose and synthesize services automatically. This automation can take place only if a formal semantic description of the web-services is available. In this paper we present a markup language called USDL (Universal Service Description Language), for formally describing the semantics of web-services.

1

Introduction

The next milestone in the Web's evolution is making services ubiquitously available. A web service is a program available on a web-site that "effects some action or change" in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. In this context, a web-service can be regarded as a "programmatic interface" that makes application to application communication possible. For web-services to become practical, an infrastructure needs to be supported that allows users to discover, deploy, compose, and synthesize services automatically. Such an infrastructure must be semantics based so that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis. Approaches such as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. Our approach can be regarded as providing semantics to WSDL statements. We present a language called Universal Services Description Language 1

(USDL) which service developers can use to specify formal semantics of web-services. Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL can be thought of as formal program documentation that will allow sophisticated conceptual modeling and searching of available web services, automated composition, and other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be determined. The directory can then be searched to check if this exact service is available, and if not available, then whether it can be synthesized by composing two or more services listed in this (or another) directory. The design of USDL rests on two formal languages: Web Services Description Language (WSDL) [3] and Web Ontology Language (OWL) [2]. The Web Services Description Language (WSDL) [3], is used to give a syntactic description of the name and parameters of a service. The description is syntactic in the sense that it describes the formatting of services on a syntactic level of method signatures, but is incapable of describing what concepts are involved in a service and what a service actually does, i.e. the conceptual semantics of the service. USDL can be regarded as as a merger of WSDL and OWL that yields a language capable of describing the syntax and semantics of web services. WSDL will be used to describe message formats, types, and method prototypes, while a specialized universal OWL ontology will be used to formally describe what these messages and methods mean, on a conceptual level.

Proceedings of the IEEE International Conference on Web Services (ICWS'05)
0-7695-2409-5/05 $20.00 IEEE

2

USDL

3

Applications

As mentioned earlier, USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL documents contain two main constructs to which we want to ascribe conceptual meaning: messages and ports. These constructs are actually aggregates of service components which will actually be directly ascribed meaning. Messages consist of typed parts and ports consist of operations parameterized on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes, which have properties with values in the OWL WordNet ontology. Furthermore, the USDL surrogates for WSDL operations allow for the description of the side-effect of executing the operation, in addition to the semantics of the input and output parameters of the operation, which are also mapped to the WordNet. Using an OWL WordNet ontology allows for our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which we map web service messages and operations. The semantic aliasing problem occurs when the formal semantics distinguishes two identical concepts. Other approaches such as OWL-S suffer from this problem [1, 8]. As long as this mapping into the WordNet ontology is precise and sufficiently expressive, reasoning can be done within the realm of OWL by using an automated inference systems (such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological concepts, especially subsumption (hyponym) and equivalence (synonym) relationships. Finally, USDL restricts its conceptual constructors, in order to address the problems of semantic aliasing and tractability. At present, we presume only four side-effect specific operations (find, create, delete, update) which have an ontology member as their "target"; this set of basic operations may be extended as more experience is gained with USDL. In practice, most services, however, deal with manipulating databases and for such services these four operations are sufficient. As stated, one of the reasons for limiting the side-effect types is to safe-guard against the semantic aliasing problem. This is also one of the main reasons for restricting the combining forms in USDL to conjunction, disjunction, and negated atoms [8].

With a pool of USDL services at one's disposal, rapid application development (RAD) tools could be used to aid a systems integrator with the task of creating composite services, i.e., services consisting of the composition of already existing services. The service designer could use such a RAD tool by describing the desired service via a USDL document, and then the tool would query the pool of services for composable sets of services that can be used to accomplish the task as well as automatically generate boilerplate code for managing the composite service, as well as menial inter-service data format conversions and other glue. Of course these additional RAD steps would require other technologies already being researched and developed [4, 5, 7].

4

Conclusion

The current version of USDL incorporates current standards in a way to further aid markup of IT services by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. Future work involves the application of USDL to formally describe commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as to service discovery and rapid application development (RAD) in commercial environments [6].

References
[1] Semantic markup for web services. http: //www.daml.org/services/owl-s/1.0/ owl-s.html. [2] Web ontology language reference. http://www.w3. org/TR/owl-ref. [3] Web services description language. http://www.w3. org/TR/wsdl. [4] P. A. Bernstein. Applying model management to classical meta data problems. In CIDR, pages 209220, 2003. [5] C. E. Gerede, R. Hull, O. H. Ibarra, and J. Su. Automated composition of e-services:lookaheads. In ICSOC, 2004. [6] T. Hite. Service composition and ranking: A strategic overview. Metallect Inc., 2005. [7] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004. [8] L. Simon, A. Mallya, A. Bansal, G. Gupta, and T. Hite. A universal service description language. Technical Report UTDCS-09-05, University of Texas at Dallas, 2005.

2

Proceedings of the IEEE International Conference on Web Services (ICWS'05)

The author$20.00 has requested 0-7695-2409-5/05 IEEE enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Web Service Discovery and Composition using USDL
Srividya Kona, Ajay Bansal, Gopal Gupta Department of Computer Science University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp. 2400 Dallas Parkway Plano, TX 75093

Abstract
For web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper, we present the design of an automatic service discovery and composition engine using USDL (Universal Service-Semantics Description Language) [1, 2], a language for formally describing the semantics of web-services. The implementation will be used for the WS-Challenge 2006 [3].

1. Introduction
A web-service is a program available on a website that "effects some action or change" in the world (i.e., causes a side-effect). The next milestone in the Web's evolution is making services ubiquitously available. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. To achieve this, we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery and composition. In this paper we present the design of a software system for automatic service discovery and composition. This system uses web service descriptions written in USDL [1, 2]. In section 2 we present a brief overview of USDL. Section 3 shows the design of our software with brief descriptions of the different components of the system followed by conclusions and references.

This is an ongoing project with Metallect Corp. The design and formal specification in OWL was published in European Conference On Web Services, 2005 [1]. The WSDL [4] syntax and USDL semantics of web services can be published in a directory which applications can access to discover services. To provide formal semantics, a common denominator must be agreed upon that everybody can use as a basis of understanding the meaning of services. Additionally, the semantics should be given at a conceptual level that captures common real world concepts. USDL uses an ontology based on OWL WordNet [5] for a universal ontology of basic concepts upon which arbitrary meets and joins can be added in order to gain tractable flexibility. USDL describes a service in terms of portType and messages, similar to WSDL. The semantics of a service is given using the OWL WordNet ontology: portType (operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly negated) concepts in the OWL WordNet ontology. Additional semantics is given in terms of how a service affects the external world. Formal semantic description of any external conditions (pre-conditions or post-conditions) acting on the service can also be provided using USDL.

3. Design
Our discovery and composition engine is written using Prolog [6]. Our software system for the WSChallenge [3] comprises of the following components shown in Figure 1.

3.1. USDL Generator

2. Overview of USDL
USDL is a language that provides formal semantics of web-services thus allowing sophisticated conceptual modeling and searching of available services, automated composition, and automated service integration. This module parses all WSDL descriptions from the given repository and converts them into corresponding USDL descriptions. For the WS-Challenge, this module maps part names of a service to their corresponding type definitions from the XML Schema file, as opposed

3.3. Semantic Relations Generator
For the semantic part of the challenge, we have to match the parmater types. The XML Schema will provide type hierarchy. We will obtain the semantic relations from the XML Schema file provided instead of OWL WordNet ontology. Complex types will be described in the schema file using simple data types, thus providing a type hierarchy. A supertype will subsume its subtype which is nothing but the hyponym and hypernym relation. A hyponym is a word that is more specific than a given word, also called the sub-ordinate. A hypernym is a word that is more generic than a given word, also called the super-ordinate. This module will extract the type definitions from the XML Schema file and create the semantic relations (hypernym, hyponym, etc.) [7] between the different types in a format that the discovery and composition query processors will understand. For example, if the XML Schema has a type p66a9128258 which is a supertype of p56a4809967, then this module will add the prolog fact hyponym(p56a4809967,p66a9128258). to the list of facts. to pointing them to concepts in OWL WordNet ontology. Our approach does not need a separate query language. It parses the query file and converts each query for the desired service also into a USDL description.

3.4. Discovery Query Processor
This module compares the discovery query with all the services in the repository. It uses an extended/special unification algorithm to find a matching term. The unification mechanism is different depending on the type of match (Exact, Specific, Generic, Part or Whole ) required. The processor works as follows: 1. On the input parts of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a type with hypernym relation [7], i.e., a generic substitutable. 2. On the output parts of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a type with hyponym relation [7], i.e., a specific substitutable. The discovery engine is written using prolog (a logic programming language) [6]. It uses a repository of facts, which contains a list of all the services and the semantic relations. The query is converted into a prolog query that looks as follows: discoverServices(queryService, solutionService). The engine will try to find a list of solutionService s that match the queryService. Our code for the engine will have various rules to solve the discoverServices query.

3.2. Triple Generator
The triple generator module converts each service description into a triple as follows: (Pre-Conditions, affect-type(affected-object, I, O), Post-Conditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters of the service. Services are converted to triples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [7]. For the WS-Challenge, conditions on a service will not be provided and hence the Pre-Conditions and PostConditions in the triple will be null. The affect-type will also not be available and so this module will assign a generic affect to all services.

3.5. Composition Query Processor
For service composition, the first step is finding the set of composable services. If a subservice S1 is composed with subservice S2 , then the output parts of S1 must be the input parts of S2 . Thus the processor has to find a set of services such that the outputs of the first service are inputs to the next service and so on. These services are then stitched together to produce the desired service. If the complete semantics of a web service are described using USDL and OWL WordNet ontology, Part substitution technique [7] can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the individual services. For the challenge, we do not have with mappings to OWL WordNet Ontology and description of conditions. So we will be using only Exact, Generic, and Specific substitution [7] techniques to find the list of composable services. Similar to the discovery engine, composition engine is also written using prolog. The query is converted into a prolog query that looks as follows: composeServices(queryService, listOfServices). The engine will try to find a listOfServices that can be composed into the requested queryService. Our code for the engine will have various rules to solve the composeServices query. Our prolog code will be setup in such a way that all possible listofServices that can be used for composition will be returned.

substitutable services that best match the desired service. Our solution will produce very good results when semantic descriptions of web services are provided using USDL. While matching real-world services, our discovery and composition engine will look at OWL WordNet ontology for the meanings. For the challenge, type hierarchy is the only semantics provided and hence we will not be able to use all the semantic relations available in WordNet. We apply some optimization techniques to our system so that it is efficient on the WS-Challenge repositories. We use constraint-logic programming [8] for better pruning of the search space. We have put in efforts for testing the efficiency of our system and identifying the correct optimization strategies.

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005. [2] S. Kona, A. Bansal, G. Gupta, and T. Hite. USDL - Formal Definitions in OWL. Internal Report, University of Texas, Dallas, 2006. Available at http://www.utdallas.edu/~srividya.kona/ USDLFormalDefinitions.pdf. [3] WS Challenge 2006. tu-berlin.de/wsc06. http://insel.flp.cs.

3.6. Output Generator
After the Discovery/Composition Query processor finds a matching service, or the list of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files using the specified XML format for the WS-Challenge[3].

[4] Web Services Description Language. http://www. w3.org/TR/wsdl. [5] Ontology-based information management system, wordnet OWL-Ontology. http://taurus.unine. ch/knowler/wordnet.html. [6] L. Sterling and S. Shapiro. The Art of Prolog. MIT Press, 1994. [7] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics Description Language for Automatic Service Discovery and Composition. Technical Report UTDCS-1806, University of Texas, Dallas, 2006. Submitted to JWSR. Available at http://www.utdallas.edu/ ~srividya.kona/USDL.pdf. [8] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

4. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure to publish services, document services and query repositories for matching services. Our approach uses USDL to formally document the semantics of services and our discovery and composition engines find

A Universal Service-Semantics Description Language
Luke Simon, Ajay Bansal, Ajay Mallya, and Gopal Gupta Department of Computer Science University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp 2400 Dallas Parkway Plano, TX 75093

Abstract
To fully utilize web-services, users and applications should be able to discover, deploy, compose and synthesize services automatically. This automation can take place only if a formal semantic description of the web-services is available. In this paper we present the design of USDL (Universal Service-Semantics Description Language), a language for formally describing the semantics of web-services. USDL is based on the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding the meaning of services. USDL can be regarded as formal program documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated service composition, and other forms of automated service integration. The design of USDL is presented, along with examples, and its formal semantics given. A theory of service composition for USDL is presented and proved sound and complete.

can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis. Several efforts are underway to build this infrastructure. These efforts include approaches based on the semantic web (such as OWL-S [4]) as well as those based on XML, such as Web Services Description Language (WSDL). Approaches such as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. In this paper we present an approach that is based on semantics. Our approach can be regarded as providing semantics to WSDL statements. We present the design of a language called Universal Service-Semantics Description Language (USDL) which service developers can use to specify formal semantics of web-services. Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL can be thought of as formal program documentation that will allow sophisticated conceptual modeling and searching of available web services, automated composition, and other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be determined. The directory can then be searched to check if this exact service is available, and if not available, then whether it can be synthesized by composing two or more services listed in this (or another) directory. To provide formal semantics, a common denominator must be agreed upon that everybody can use as a basis of understanding the meaning of services. This common conceptual ground must also be somewhat coarse-grained so as to be tractable for use by both engineers and computers. That is, semantics of services should not be given in terms of low-level concepts such 1

1

Introduction

The next milestone in the Web's evolution is making services ubiquitously available. A web service is a program available on a web-site that "effects some action or change" in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. In this context, a web-service can be regarded as a "programmatic interface" that makes application to application communication possible. For web-services to become practical, an infrastructure needs to be supported that allows users to discover, deploy, compose, and synthesize services automatically. Such an infrastructure must be semantics based so that applications

as Turing machines, first-order logic and their variants, since service description, discovery, and synthesis then become tasks that are practically intractable and theoretically undecidable. Additionally, the semantics should be given at a conceptual level that captures common real world concepts. Furthermore, it is too impractical to expect disparate companies to standardize on application (or domain) specific ontologies to formally define semantics of web-services, and instead a common universal ontology must be agreed upon with additional constructors. Also, application specific ontologies will be an impediment to automatic discovery of services since the application developer will have to be aware of the specific ontology that has been used to describe the semantics of the service in order to frame the query that will search for the service. The danger is that the service may not be defined using the particular domain specific ontology that the application developer uses to frame the query, however, it may be defined using some other domain specific ontology, and so the application developer will be prevented from discovering the service even though it exists. These reasons make an ontology based on WordNet OWL a suitable candidate for a universal ontology of atomic concepts upon which arbitrary meets and joins can be added in order to gain tractable flexibility. In the next section we describe what is meant by conceptual modeling and how such a thing could be obtained via a common universal ontology based on WordNet. Section 3, gives a brief overview of how USDL attempts to semantically describe web services. In section 4, we discuss precisely how a WSDL document can be prescribed meaning in terms of WordNet ontology, and in addition, a short example WSDL service is annotated, so that a concrete example of instances of the various OWL classes and properties can be seen. Section 5 gives a complete USDL annotation for a BookBuying service. Automatic discovery and composition of web services using USDL is discussed in section 6. In section 7 we explore some of the theoretical aspects of service description in USDL. Comparison of USDL with other approaches like OWL-S and WSML is discussed in section 8. Finally, other related work and conclusion are given in the remaining sections.

2

A Universal Ontology

We can describe what any given computer program does in a logical manner, from first-principles. This is the approach taken by frameworks such as dependent type systems and programming logics prevalent in the field of software verification where a "formal under2

standing" of the software is needed in order to verify it. However, such solutions are both too low-level and too tedious (and not to mention, undecidable) to be of practical use. Instead, we are interested in modeling higher-level concepts. That is, we are more interested in answering questions such as, what does a service do from the point of the end user or integrator of the service, as opposed to the far more difficult questions, such as, what does the program do from a computational view? The distinction is subtle, but is a distinction of granularity as well as a distinction of scope: we care more about real world concepts such as "customer", "bank account", and "flight itinerary" as opposed to the data structures and algorithms used by a program to model these concepts. At some point, a common denominator must be agreed upon in order to allow interoperability and machine-readability of our documents. The first step towards this common ground are standard languages such as WSDL and OWL. However, these do not go far enough, as for any given type of service there are numerous distinct representations in WSDL and for high-level concepts (e.g. a tertiary predicate), there are numerous disparate representations in terms of OWL, representations that are distinct in terms of OWL's formal semantics, yet equal in the actual concepts they model. This is known as the semantic aliasing problem: distinct syntactic representations with distinct formal semantics yet equal conceptual semantics. This problem can be overcome by standardizing on a sufficiently comprehensive set of atomic concepts, i.e. a universal ontology, along with restricted connectives, such that the semantics always equate things that are conceptually equal. Another approach would be to use industry specific ontologies along with OWL (this is the approach taken by the OWL-S language [4]). The problem with this approach is that it requires standardization and undue foresight. Standardization is a slow, bitter process, and industry specific ontologies would require this process to be iterated for each specific industry. Furthermore, reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing is even more difficult. Undue foresight is required because many useful web services will address innovative applications and industries that don't currently exist. Standardizing an ontology for travel and finances is easy, as these industries are well established, but then how will new innovative services in new upcoming industries be ascribed formal meaning? Of course, a universal ontology will have no difficulty in describing such new services. Thus, our common conceptual ground must be

somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. Currently there is only one sufficiently comprehensive ontology that meets these criteria: WordNet [7]. As stated, part of the common ground involves standardized languages such as OWL. For this reason, WordNet cannot be used directly, and instead we make use of an encoding of WordNet as an OWL base ontology [1]. Using an OWL WordNet ontology allows for our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which we map web service messages and operations. As long as this mapping is precise and sufficiently expressive, reasoning can be done within the realm of OWL by using an automated inference systems (such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological concepts, especially subsumption (hyponym) and equivalence (synonym) relationships. Finally, USDL restricts its conceptual constructors, in order to address the problems of semantic aliasing and tractability.

3

USDL: An Overview

Like WSDL, USDL describes a service in terms of ports and messages. The semantics of the service is given using the WordNet OWL ontology. USDL maps ports (operations provided by the service) and messages (operation parameters) to disjunctions of conjunctions of (possibly negated) concepts in the WordNet OWL ontology. The semantics is given in terms of how a service affects the external world. The present design of USDL assumes that each side-effect is one of the following operations: create, update, delete, or find, but also allows for a generic affects side-effect when none of the others apply. An application that wishes to make use of a service automatically should be able to reason with WordNet atoms using the WordNet OWL ontology. USDL is perhaps the first language that attempts to capture the semantics of web-services in a universal, yet decidable manner. It is quite distinct from previous approaches such as WSDL and OWL-S [4]. As mentioned earlier, WSDL only defines syntax of the service; USDL can be thought of as providing the missing semantic component. USDL can be thought of as a formal language for program documentation. Thus instead of documenting the function of a service as comments in English, one writes USDL statements that describe the function of that service. USDL is quite distinct from OWL-S, which is designed for a similar purpose, and as 3

we shall see the two are in fact complimentary. OWL-S primarily describes the states that exists before and after the service and how a service is composed of other smaller sub-services (if any). Description of atomic services is left underspecified in OWL-S. They have to be specified using domain specific ontologies; in contrast atomic services are completely specified in USDL, and USDL relies on a universal ontology (OWL WordNet Ontology) to specify the semantics of atomic services. USDL and OWL-S are complimentary in that OWL-S's strength lies in describing the structure of composite services, i.e., how various atomic services are algorithmically combined to produce a new service, while USDL is good for fully describing atomic services. Thus, OWL-S can be used for describing the structure of composite services that combine atomic services that are described using USDL. In order to develop a theory for composing services, we also define the formal semantics of USDL. The syntactic terms describing ports and messages are mapped to disjunctions and conjunctions of (possibly negated) OWL WordNet ontological terms. These disjunctions and conjunctions are represented by points in the lattice obtained from the WordNet ontology with regards to the OWL subsumption relation. A service is then formally defined as a function, labeled with zero or more side-effects, between points in this lattice. The main contribution of our work is the design of a universal service- semantics description language USDL, along with its formal semantics, and soundness and completeness proofs for a theory of service composition with USDL.

4

Design of USDL

The design of USDL rests on two formal languages: Web Services Description Language (WSDL) [6] and Web Ontology Language (OWL) [5]. The Web Services Description Language (WSDL) [6], is used to give a syntactic description of the name and parameters of a service. The description is syntactic in the sense that it describes the formatting of services on a syntactic level of method signatures, but is incapable of describing what concepts are involved in a service and what a service actually does, i.e. the conceptual semantics of the service. Likewise, the Web Ontology Language (OWL) [5], was developed as an extension to the Resource Description Framework (RDF) [2], both standards are designed to allow formal conceptual modeling via logical ontologies, and these languages also allow for the markup of existing web resources with semantic information from the conceptual models. USDL employs WSDL and OWL in order to describe the syntax

and semantics of web services. WSDL is used to describe message formats, types, and method prototypes, while a specialized universal OWL ontology is used to formally describe what these messages and methods mean, on a conceptual level. As mentioned earlier, USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL documents contain two main constructs to which we want to ascribe conceptual meaning: messages and ports. These constructs are actually aggregates of service components which will actually be directly ascribed meaning. Messages consist of typed parts and ports consist of operations parameterized on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes, which have properties with values in the OWL WordNet ontology.

be modeled as set theoretic formulas of union, intersection, and negation of atomic concepts. These subclasses of Concept are  AtomicConcept  InvertedConcept  ConjunctiveConcept  DisjunctiveConcept 4.1.1 Atomic Concept

AtomicConcept is the actual contact point between USDL and WordNet. This class acts as proxy for WordNet lexical concepts.
<owl:Class rdf:about="#AtomicConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isA"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#ofKind"/> <owl:mincardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:mincardinality> </owl:Restriction> </rdfs:subClassOf> ... </owl:Class>

4.1

Concept

USDL defines a generic class called Concept which is used to define the semantics of parts of messages. Semantically, instances of Concept form a complete lattice, which will be covered in section 7.
<owl:Class rdf:ID="Concept"> <rdfs:comment> Generic class of USDL Concept </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#AtomicConcept"/> <owl:Class rdf:about="#InvertedConcept"/> <owl:Class rdf:about="#ConjunctiveConcept"/> <owl:Class rdf:about="#DisjunctiveConcept"/> </owl:unionOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:mincardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:mincardinality> </owl:Restriction> </rdfs:subClassOf> ... </owl:Class> <owl:ObjectProperty rdf:ID="hasCondition"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Condition"/> </owl:ObjectProperty>

The property cardinality restrictions require all USDL AtomicConcept s to have exactly one defining value for the isA property, and zero or more values for the ofKind property. An instance of AtomicConcept is considered to be equated with the WordNet lexical concept given by the isA property and classified by the lexical concept given by the optional ofKind property.
<owl:ObjectProperty rdf:ID="isA"> <rdfs:domain rdf:resource="#AtomicConcept"/> <rdfs:range rdf:resource="&wn;LexicalConcept"/> ... </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="ofKind"> <rdfs:domain rdf:resource="#AtomicConcept"/> <rdfs:range rdf:resource="&wn;LexicalConcept"/> ... </owl:ObjectProperty>

The USDL Concept class denotes the top element of the lattice of conceptual objects constructed from the OWL WordNet ontology. For most purposes, message parts and other WSDL constructs will be mapped to a subclass of USDL Concept so that useful concepts can 4

4.1.2

Inverted Concept

In the case of InvertedConcept the corresponding semantics are the complement of the WordNet lexical concepts.
<owl:Class rdf:about="#InvertedConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype= "&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasConcept"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

intersections and unions(where n  2) of USDL concepts. For generality, these concepts are either AtomicConcepts, ConjunctiveConcepts, DisjunctiveConcepts, or InvertedConcepts .

4.2

Affects

The affects property is specialized into four types of actions common to enterprise services: creates, updates, deletes, and finds.
<owl:ObjectProperty rdf:ID="affects"> <rdfs:comment> Generic class of USDL Affects </rdfs:comment> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/> ... </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#creates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#updates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#deletes"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#finds"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty>

4.1.3

Conjunctive and Disjunctive Concept

The ConjunctiveConcept and DisjunctiveConcept respectively denote the intersection and union of USDL Concept s.
<owl:Class rdf:about="#ConjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions on ConjunctiveConcept and DisjunctiveConcept allow for n-ary 5

Note that each of these specializations inherits the domain and range of the affects property. Most services can be described as a conjunction of these types of effects. For those services that cannot be described in terms of a combination of these specializations, the parent affects property can be used instead, or the property can be omitted entirely when the meaning of the operation parameter messages are enough for conceptual reasoning. The purpose of limiting the types of services as opposed to allowing the creation of new arbitrary side-effect types, for example via OWL-DL, is to: (i) make USDL more structured and therefore easier to create documents in, (ii) make USDL computationally more tractable for programs that process large volumes of USDL documents, and (iii) help prevent the semantic aliasing problem mentioned in section 2.

4.3

Conditions and Constraints

Services may have some external conditions specified on the input or output parameters. Condition class is used to describe all such constraints. Conditions are represented as conjunction or disjunction of binary predicates. Predicate is a trait or aspect of the resource being described.
<owl:Class rdf:ID="Condition"> <rdfs:comment> Generic class of USDL Condition </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#AtomicCondition"/> <owl:Class rdf:about="#ConjunctiveCondition"/> <owl:Class rdf:about="#DisjunctiveCondition"/> </owl:unionOf> ... </owl:Class> <owl:Class rdf:about="#AtomicCondition"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype= "&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#onPart"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasValue"/> <owl:maxCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:maxCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

<owl:ObjectProperty rdf:ID="onPart"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="hasValue"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.3.1

Conjunctive and Disjunctive Conditions

The ConjunctiveCondition and DisjunctiveCondition respectively denote the conjunction and disjunction of USDL Condition s.
<owl:Class rdf:about="#ConjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasCondition"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Condition"/> </owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveCondition and DisjunctiveCondition allow for nary conjunctions and disjunctions (where n  2) of USDL conditions. In general any n-ary condition can be written as a combination of conjunctions and disjunctions of binary conditions.

4.4
A condition has exactly one value for the onPart property and atmost one value for the hasValue property, each of which is of type USDL Concept. 6

Messages

Services communicate by exchanging messages. As mentioned, messages are simple tuples of actual data,

called parts. Take for example, a flight reservation service similar to the SAP ABAP Workbench Interface Repository for flight reservations [3], which makes use of the following message.
<message name="&flight;ReserveFlight_Request"> <part name="&flight;CustomerName" type="xsd:string"> <part name="&flight;FlightNumber" type="xsd:string"> <part name="&flight;DepartureDate" type="xsd:date"> ... </message>

<AtomicConcept rdf:about="&flight;FlightNumber"> <isA rdf:resource="&wn;number"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;flight"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasPart> ... </Message>

4.5

Ports

The USDL surrogate for a WSDL message is the Message class, which is a composite entity with zero or more parts. Note that for generality, messages are allowed to contain zero parts.
<owl:Class rdf:about="#Message"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasPart"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

Services consist of ports, which are collections of procedures or operations that are parametric on messages. Our example flight reservation service might contain a port definition for a flight reservation service that takes as input an itinerary and outputs a reservation receipt.
<portType name="&flight;Flight_Reservation"> <operation name="&flight;ReserveFlight"> <input message= "&flight;ReserveFlight_Request"/> <output message= "&flight;ReserveFlight_Response"/> </operation> ... </portType>

Each part of a message is simply a USDL Concept, as defined by the hasPart property. Semantically messages are treated as tuples of concepts.
<owl:ObjectProperty rdf:ID="hasPart"> <rdfs:domain rdf:resource="#Message"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

The USDL surrogate is defined as the class Port which contains zero or more Operation s as values of the hasOperation property.
<owl:Class rdf:about="#Port"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource=#hasOperation"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> ... </owl:Class> <owl:ObjectProperty rdf:ID="hasOperation"> <rdfs:domain rdf:resource="#Port"/> <rdfs:range rdf:resource="#Operation"/> </owl:ObjectProperty>

Continuing our example flight reservation service, the Itinerary message is given semantics using USDL as follows, where &wn;customer and &wn;name are valid XML references to WordNet lexical concepts.
<Message rdf:about= "&flight;ReserveFlight_Request"> <hasPart> <AtomicConcept rdf:about="&flight;CustomerName"> <isA rdf:resource="&wn;name"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;customer"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasPart> <hasPart>

As with the case of messages, ports are not directly assigned meaning via the OWL WordNet ontology. Instead the individual Operation s of a port are described by their side-effects via an affects property. Note that the parameters of an operation are already given meaning by ascribing meaning to the messages that constitute the parameters. 7

<owl:Class rdf:about="#Operation"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#affects"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The service has an input pre-condition that a User Identifier for the buyer must exist before invoking the service. It also has a global constraint that a valid credit card number for the buyer must exist.

5.1

WSDL definition

An operation can have multiple or no values for the affects property, all of which are of type USDL Concept, which is the target of the effect.
<owl:ObjectProperty rdf:ID="affects"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

The following is WSDL definition of the service. This service provides a single operation called BookBuying. The input and output messages are defined below. The conditions on the service cannot be described using WSDL.
<definitions> ... <portType name="BookBuying_Service"> <operation name="BookBuying"> <input message="BuyBook_Request"/> <output message="BuyBook_Response"/> </operation> </portType> <message name="BuyBook_Request"> <part name="BookISBN" type="xsd:string"/> <part name="UserIdentifier" type="xsd:string"/> <part name="Password" type="xsd:string"/> </message> <message name="BuyBook_Response"> <part name="OrderNumber/Availability" type="xsd:string"/> </message> ... </definitions>

Finishing our flight reservation service example, we can describe the main side-effect of invoking the reserve operation in USDL as follows.
<Port rdf:about="&flight;Flight_Reservation"> <hasOperation> <Operation rdf:about="&flight;ReserveFlight"> <creates> <AtomicConcept rdf:about="flight_reservation"> <isA rdf:resource="&wn;reservation"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;flight"/> </AtomicConcept> </ofKind> </AtomicConcept> </creates> </Operation> </hasOperation> ... </Port>

5.2

USDL annotation

Again, note that it is not necessary to annotate the operation with regards to its input and output parameters, as these are already annotated by Message surrogates.

The following is the complete USDL annotation corresponding to the above mentioned WSDL description. The input pre-condition and the global constraint on the service are also described semantically.
<definitions> ... <port rdf:about="#BookBuying_Service"> <hasOperation> <operation name="BuyBook"> <creates> <AtomicConcept rdf:about="#BookOrder"> <isA rdf:resource="&wn;order"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;book"/> </AtomicConcept> </ofKind> <hasCondition>

5

Semantic Description of a Service

A simple Book Buying Service: The service described here is a simplified book buying service published in a web service registry. This service can be treated as atomic: i.e., no interactions between buying and selling agents are required, apart from invocation of the service and receipt of its outputs by the buyer. Given certain inputs and preconditions, the service provides certain outputs and has specific effects. 8

<Condition rdf:about="#exists"> <AtomicConcept> <isA rdf:resource="&wn;exists"/> </AtomicConcept> <onPart rdf:about="#CreditCard"> <AtomicConcept> <isA rdf:resource="&wn;card"/> <ofKind> <AtomicConcept> <isA rdf:resource= "&wn;credit"/> </AtomicConcept> </ofKind> </AtomicConcept> </onPart> </Condition> </hasCondition> </AtomicConcept> </creates> </operation> </hasOperation> </port> <portType name="BookBuying_Service"> <operation name="BuyBook"> <input message="BuyBook_Request"/> <output message="BuyBook_Response"/> </operation> </portType> <Message rdf:about="#BuyBook_Request"> <hasPart> <AtomicConcept rdf:about="#BookISBN"> <isA rdf:resource="&wn;identifier"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;book"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasPart> <hasPart> <AtomicConcept rdf:about= "#UserIdentifier"> <isA rdf:resource="&wn;identifier"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;user"/> </AtomicConcept> </ofKind> <hasCondition> <Condition rdf:about="#exists"> <AtomicConcept> <isA rdf:resource="&wn;exists"/> </AtomicConcept> <onPart rdf:resource= "#UserIdentifier"/> </Condition> </hasCondition> </AtomicConcept> </hasPart> <hasPart>

<AtomicConcept> <isA rdf:resource="&wn;Password"/> </AtomicConcept> </hasPart> </Message> <Message rdf:about="#BuyBook_Response"> <hasPart> <DisjunctiveConcept rdf:about="#OrderNumber/Availability"> <hasConcept> <AtomicConcept rdf:about= "#OrderNumber"> <isA rdf:resource="&wn;number"/> <ofKind> <AtomicConcept> <isA rdf:resource="&wn;order"/> </AtomicConcept> </ofKind> </AtomicConcept> </hasConcept> <hasConcept> <InvertedConcept rdf:about= "#NotAvailable"> <AtomicConcept> <isA rdf:resource="&wn;available"/> </AtomicConcept> </InvertedConcept> </hasConcept> </DisjunctiveConcept> </hasPart> </Message> ... </definitions>

6

Service Discovery and Composition

Note that given a directory of services, a USDL description could be included for each service, making the directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. Note that USDL itself could be such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory to look for a "matching" service. For matching we could treat USDL descriptions as well as the USDL query as terms, and perhaps use some kind of extended unification to check for a match. This work is currently in progress [9]. With the USDL descriptions and query language in place, numerous applications become possible ranging from querying a database of services to rapid application development via automated integration tools and even real-time service composition [14]. Take our flight reservation service example. Assume that some9

body wants to find a travel reservation service and that they query a USDL database containing general purpose flight reservation services, bus reservation services, etc. One could then form a USDL query consisting of a description of a travel reservation service and the database could respond with a set of travel reservation services whether it be via flight, bus, or some other means of travel. This flexibility of generalization and specialization is gained from USDL's subtyping relation covered in section 7. Furthermore, with a pool of USDL services at one's disposal, rapid application development (RAD) tools could be used to aid a systems integrator with the task of creating composite services, i.e. services consisting of the composition of already existing services. The service designer could use such a RAD tool by describing the desired service via a USDL document, and then the tool would query the pool of services for composable sets of services that can be used to accomplish the task as well as automatically generate boilerplate code for managing the composite service, as well as menial inter-service data format conversions and other glue. Of course these additional RAD steps would require other technologies already being researched and developed [10, 11, 16, 13, 15]. At present, we presume only four specific operations (find, create, delete, update); this set of basic operations may be extended as more experience is gained with USDL. In practice, most services, however, deal with manipulating databases and for such services these four operations are sufficient. As stated, one of the reasons for limiting the side-effect types is to safeguard against the semantic aliasing problem described in section 2. This is also one of the main reasons for restricting the combing forms in USDL to conjunction, disjunction, and negated atoms. As discussed in the next section, this allows USDL descriptions to be put into a type of disjunctive normal form, from which a sound and complete notion of subtyping is created.

of this section. In order to prove these theorems, we must first formally define constructs such as USDL described objects and services, which we will also call objects and services for short. While it is possible to work directly with the XML USDL syntax, doing so is cumbersome and so we will instead opt for set theoretic notation. Definition 1. Let  be the set of WordNet nouns and  be the OWL subsumption relation on . Definition 2. Let  be the least set of objects such that: 1. x   implies x, 촼   2. x, y   implies x  y , x  y   Hence  is simply the set of objects described by USDL concepts. Definition 3. Let  = {(L, E ) | L  , E  } be the set of USDL side-effects, where  = {creates, updates, deletes, f inds} L is the affect type and E is the effected object. Definition 4. For any set S , let S = { |  / S }  S  {(x, y ) | x  S, y  S  } be the set of lists over S . Let  = {(A, I, O) | A  2 , I   , O   } be the set of USDL service descriptions, where A is the set of side-effects, I is the list of input parameters, and O is the list of output parameters of a particular service. Now that the formal definitions of object and service descriptions are out of the way, we would like to define a subsumption relation  over  so that we can reason about composability of services, but this will, in turn, require a subsumption relation  over . The proof of correctness of  is covered by the principle of safe substitution below. Definition 5. Assuming without loss of generality that all objects are expressed in disjunctive normal form, then let  be the ordering relation defined on objects such that: 1. x  y for x, y   and x  y 2. 촼  촽 if and only if y  x 3.
i wi  j xj if and only if for all wj = k yk there exists some xj = l zl such that for every zl there exists some yk  zl .

7

Theory of Service Description

In this section, we will investigate the theoretical aspects of service description via USDL. This involves concepts from set theory, lattice theory, and type theory. From a systems integration perspective, an engineer is interested in finding a set of composable services that accomplish some necessary task. Therefore service description should allow the engineer to describe a service in USDL and receive in return a set of services that can be used in a context expecting a service that meets that description. We prove that this is possible in the soundness and completeness theorems at the end 10

Definition 6. Let (A, I, O)  (A , I , O ) if and only if (L, E )  A .(L, E )  A .E  and I  I and O  O , where  is the element-wise extension of  to lists of objects.

Note that (,  ) and (,  ) respectively form a complete lattice. Given a description of a service   , we can now define the set of composables C ( ), which in practice corresponds to a query against a database of services , for services that satisfy description  . Notice that the definition is contravariant for inputs and covariant for outputs [17]. This contravariance is also seen in the field of type theory with regards to polymorphic subtyping [17], and will be covered in the proof of the principle of safe substitution below. Definition 7. Let C be the set of composables parametric over services be a function mapping  to a subset of services such that C ( ) = { |    }. In order to be able to prove the soundness and completeness of C , we will first need to prove the following lemma known as the "Principle of Safe Substitution." Lemma 1. For any services ,   , if    then  can safely be used in a context expecting service  . Proof. The proof follows by establishing the principle of safe substitution for objects, which is then used to prove the principle of safe substitution for services. Assume for sake of contradiction that for some objects x and y that x  y such that object x can not be safely used in a context expecting object y . Then under the set-theoretic interpretation of x and y as the sets sx and sy of all objects satisfying respective descriptions, sx contains an element o  / sy . Since o is described by some   x, and so by assumption that x is incompatible with y , it must be true that there is no   y that describes x. However, this contradicts the definition of  that requires that there exists a conjunctive concept   y that describes x. Therefore the principle of safe substitution holds for the lattice (,  ). Now assume that there exists a  such that    , but  can not be used in a context expecting  , where  = (A, I, O) and  = (A , I , O ). Clearly the context expects a service that can accept input of the type described by I , and since  accepts a more general input type, every input of type I is also an input of type I . This explains the contravariance of input in the definition of  . Now, since the expected service will output objects of type O , the context can handle a service that outputs objects of a more specific kind O by the principle of safe substitution for objects. Therefore  can safely be used in place of  . Theorem 1. (Soundness of Composables) For any services  and  , if service  can not be safely used in a context expecting service  then   / C ( ), that is the set of composables for a service does not contain any incompatible services. 11

Proof. Assume the existence of services  and  such that  can not be safely used in a context expecting service  . By the principle of safe substitution it is not true that    , and hence by the principle of extensionality,   / C ( ). Therefore the set of composables only contains correct, i.e. compatible services. Theorem 2. (Completeness of Composables) For any service  , if there exists a service  that can safely be used in a context expecting service  then   C ( ), that is the set of composables contains all services compatible with a given description. Proof. Assume there exists such a service  , then by the principle of safe substitution    . So by the definition of C ( ),   C ( ), and therefore the set of composables is complete for arbitrary   .

8

Comparison with OWL-S and WSML

OWL-S is another service description language [4], which attempts to address the problem of semantic description via a highly detailed service ontology. But OWL-S also allows for complicated combining forms, which seem to defeat the tractability and practicality of OWL-S. The focus in the design of OWL-S is to describe the structure of a service in terms of how it combines other sub-services (if any used). The description of atomic services in OWL-S is left underspecified. OWL-S includes the tags presents to describe the service profile, and the tag describedBy to describe the service model. The profile describes the (possibly conditional) states that exist before and after the service is executed. The service model describes how the service is (algorithmically) constructed from other simpler services. What the service actually accomplishes has to be inferred from these two descriptions in OWL-S. Given that OWL-S uses complicated combining forms, inferring the task that a service actually performs is, in general, undecidable. In contrast, in USDL, what the service actually does is directly described (via the verb affects and its refinements create, update, delete, and find). OWL-S recommends that atomic services be defined using domain specific ontologies. Thus, OWL-S needs users describing the services and users using the services to know, understand and agree on domain specific ontologies in which the services are described. Thus annotating services with OWL-S is a very time consuming, cumbersome, and invasive process. The complicated nature of OWL-S's combining forms, especially conditions and control constructs, seems to allow for

the aforementioned semantic aliasing problem. The other recent approaches like WSMO,WSML etc also suffer from the same limitation. In contrast, USDL uses the universal WordNet ontology to take care of this problem. Note that USDL and OWL-S can be used together. A USDL description can be placed under the describedBy tag for atomic processes, while OWL-S can be used to compose atomic USDL services. Thus, USDL along with WordNet can be treated as the universal ontology that can make an OWL-S description complete. USDL documents can be used to describe the semantics of atomic services that OWL-S assumes will be described by domain specific ontologies and pointed to by the OWL-S `describedBy' tag. In this respect, USDL and OWL-S are complimentary. Thus USDL can be treated as an extension to OWL-S which makes OWL-S description easy to write and more complete. Also, OWL-S can be regarded as the composition language for USDL. If a new service can be build by composing a few already existing services, then this new service can be described in OWL-S using the USDL descriptions of the existing services. Then this new service can be automatically generated from its OWL-S description. The control constructs like Sequence and If-Then-Else of OWL-S allows us to achieve this. Note once a composite service has been defined using OWL-S that uses atomic services described in USDL, a new USDL description must be written for this composite service. This USDL description is the formal documentation of the new composite service and will make it automatically searchable once the new service is placed in the directory service. The USDL description also allows this composite service to be treated as an atomic service by some other application. For example: The aforementioned reserve service which creates a flight reservation can be viewed as a composite process of first getting the flight details, then checking the flight availability and then booking the flight(creating the reservation). If we have these three atomic services namely GetFlightDetails, CheckFlightAvailability and BookFlight then we can create our reserve service by composing these three services in sequence using the OWL-S Sequence construct. The following is the OWL-S description of the composed reserve service.
<rdf:RDF xmlns:rdf="http://www.w3.org/1999 /02/22-rdf-syntax-ns#" xmlns:process="http://www.daml.org/services /owl-s/1.0/Process.owl#"> <process:CompositeProcess rdf:ID="reserve"> <process:composedOf>

<process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="&GetFlightDetails"/> <process:AtomicProcess rdf:about="&CheckFlightAvailability"/> <process:AtomicProcess rdf:about="&BookFlight"/> </process:components> </process:Sequence> </process:composedOf> </process:CompositeProcess> </rdf:RDF>

We can generate this composed reserve service automatically by using the USDL descriptions of the component services for discovering them from the existing services. Once we have the component services, the OWL-S description can be used to generate the new composed service.

9

Other Related Work

Another related area of research involves message conversation constraints, also known as behavioral signatures [12, 13, 15]. Behavior signature models do not stray far from the explicit description of the lexical form of messages, they expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while USDL deals with higher-level real world concepts. However, USDL and behavioral signatures can be regarded as complimentary concepts when taken in the context of real world service composition and both technologies are currently being used in the development of a commercial services integration tool [14].

10

Conclusion

In order to address the mounting complexity of information technology services integration, standards must be used so that services can be published and documented so that they can be reliably cataloged, searched, and composed in a semi-automatic to fullyautomatic manner. This requires language standards for specifying not just the syntax, i.e. prototypes, of service procedures and messages, but it also necessitates a standard formal, yet high-level means for specifying the semantics of service procedures and messages. We have addressed these issues by defining a service semantics description language, its semantics, and we have proved some useful properties about this language. The current version of USDL incorporates 12

current standards in a way to further aid markup of IT services by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. This approach is more practical and tractable than other approaches because description documents are more easily created by humans and more easily processed by computers. USDL is currently being used to formally describe web-services related to emergency response functions [8]. Future work involves the application of USDL to formally describing commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as to service discovery and rapid application development (RAD) in commercial environments [14]. Future work also includes developing tools that will allow automatic generation of new services based on combining USDL descriptions of existing atomic services. The interesting problem to be addressed is: can USDL description of such automatically generated services be also automatically generated?

[14] T. Hite. Service composition and ranking: A strategic overview. Metallect Inc., 2005. [15] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004. [16] S. Melnik, E. Rahm, and P.Bernstein. Rondo: A programming platform for generic model management. In SIGMOD, 2003. [17] B. C. Pierce. Types and Programming Languages. MIT Press, Cambridge, MA, 2002.

References
[1] Ontology-based information management system, wordnet owl-ontology. http://taurus.unine.ch/ knowler/wordnet.html. [2] Resource description framework. http://www.w3. org/RDF. [3] Sap interface repository. http://ifr.sap.com/ catalog/query.asp. [4] Semantic markup for web services. http://www.daml. org/services/owl-s/1.0/owl-s.html. [5] Web ontology language reference. http://www.w3. org/TR/owl-ref. [6] Web services description language. http://www.w3. org/TR/wsdl. [7] Wordnet: a lexical database for the english language. http://www.cogsci.princeton.edu/~wn. [8] A. Bansal, K. Patel, and G. G. et al. Towards intelligent services: A case study in chemical emergency response. In ICWS, 2005. [9] A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. Automatic querying and composite services generation with usdl. Working paper, 2005. [10] P. Bernstein. Generic model management - a database infrastructure for schema manipulation. In CoopIS, 2001. [11] P. A. Bernstein. Applying model management to classical meta data problems. In CIDR, pages 209220, 2003. [12] Z. Dang, O. H. Ibarra, and J. Sun. Composability of infinite-state activity automata. In ISAAC, 2004. [13] C. E. Gerede, R. Hull, O. H. Ibarra, and J. Su. Automated composition of e-services:lookaheads. In ICSOC, 2004.

13

Black Hole Artificial Bee Colony Algorithm
Artificial bee colony (ABC) is an efficient methodology to solve optimization problems. Here, in this article a modified variant of ABC, namely Black Hole ABC algorithm (BHABC) is proposed which is based on the natural space black hole (BH) phenomenon. In BHABC, the implementation of BH gives a high exploration ability while maintaining the original exploitation ability of the ABC algorithm. The suggested algorithm is judged against 12 benchmark test functions and accessed with original ABC and its two modifications, that are Best So Far ABC (BSFABC) and Modified ABC (MABC). The results reveals that BHABC is a competitive variant of ABC.

A Low-Power Integrated x86-64 and Graphics Processor for Mobile Computing Devices
:
80 stream processors, a media accelerator, an integrated NorthBridge (NB), integrated DisplayPort, LVDS, and VGA display interfaces, a PCIe짰혻Gen1 or Gen2 I/O interface, and a single 64-bit memory channel at up to DDR3-1066 on a single die implemented in a 40 nm bulk CMOS process

Artificial intelligence applications in Permanent Magnet Brushless DC motor drives
Permanent Magnet Brushless DC (PMBLDC) machines are more popular due its simple structure and low cost. Improvements in permanent magnetic materials and power electronic devices have resulted in reliable, cost effective PMBLDC drives, for many applications. Advances in artificial intelligent applications like neural network, fuzzy logic, Genetic algorithm etc. have made tremendous impact on electric motor drives. The brushless DC motor is a multivariable and non-linear system. In conventional PMBLDC drives speed and position sensing of brushless DC motors require high degree of accuracy. Unfortunately, traditional methods of control require detailed modelling of all the motor parameters to achieve this. The Intelligent control techniques like, fuzzy logic control/Neural network control etc. uses heuristic input뱋utput relations to deal with vague and complex situations. This paper presents a literature survey on the intelligent control techniques for PMBLDC motor drives. Various AI혻techniques for PMBLDC motor drives are described. Attempt is made to provide a guideline and quick reference for the researchers and practicing engineers those are working in the area of PMBLDC motor drives.
Weaving Functional and Non-Functional Attributes for Dynamic Web Service Composition
Given the numerous, potentially reusable, Web services available on the Internet, search and composition techniques that efficiently discover viable services will be a strong requirement. A major challenge for dynamic Web service composition will be the ability to measure the quality or reliability of services that are delivered. In this paper, we present a solution for dynamic Web service composition that leverages non-functional attributes provided in the form of Service-Level Agreements (SLA's). The objective of our work is to understand the most efficient algorithms for discovering and composing web services into capabilities with predictable quality. As such, we analyze different approaches to composition when web service composition routines must take into account functional and nonfunctional information. We present our algorithm, a prototype implementation, and experimental results obtained from the various approaches to "weaving" attributes (of different dimensions) as a part of the composition process.
Continuing the Web Services Challenge
:
The capabilities of organizations can be openly exposed, easily searched and discovered, and made readily-accessible to humans and particularly to machines, using service-oriented computing approaches. Artificial intelligence and software engineering researchers alike are tantalized by the promise of ubiquitously discovering and incorporating services into their own business processes (i.e. composition and orchestration). With growing acceptance of service-oriented computing, an emerging area of research is the investigation of technologies that will enable the discovery and composition of web services. The Web Services Challenge (WSC) is a forum where academic and industry researchers can share experiences of developing tools that automate the integration of Web services. In the fourth year (i.e. WSC-08) of the Web Services Challenge, software platforms will address several new composition challenges. Requests and results will be transmitted within SOAP messages. In addition, semantics will be represented as ontologies written in OWL, services will be represented in WSDL, and service orchestrations will be represented in WS-BPEL.

An Agent-Based Approach for Composition of Semantic Web Services
:
The paradigm of Service-oriented computing (SOC) introduces emerging concepts for distributed- and e-business processing enabling the sharing and reuse of service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services. Leading industry approaches rely heavily on syntactical approaches for managing service-based business processes. As such, these approaches are limited since the true functionality of ambiguous capabilities (i.e. web service operations) cannot be inferred. We introduce approaches that disambiguate services by interleaving process-based control with semantic annotations. In this paper, we introduce a generalized architecture where intelligent software agents control process-oriented composition that leverages the descriptiveness of semantics. An outcome of this work is the specification of a multiple agent system where a query agent interacts with multiple repository agents to perform business-oriented service composition.

Co-Logic Programming: Extending Logic Programming with Coinduction
In this paper we present the theory and practice of혻co-logic programming혻(co-LP for brevity), a paradigm that combines both inductive and coinductive logic programming. Co-LP is a natural generalization of logic programming and coinductive logic programming, which in turn generalizes other extensions of logic programming, such as infinite trees, lazy predicates, and concurrent communicating predicates. Co-LP has applications to rational trees, verifying infinitary properties, lazy evaluation, concurrent LP, model checking, bisimilarity proofs, etc.

Coinductive Logic Programming
We extend logic programming셲 semantics with the semantic dual of traditional Herbrand semantics by using greatest fixed-points in place of least fixed-points. Executing a logic program then involves using혻coinduction혻to check inclusion in the greatest fixed-point. The resulting혻coinductive logic programming language혻is syntactically identical to, yet semantically subsumes logic programming with rational terms and lazy evaluation. We present a novel formal operational semantics that is based on혻synthesizing a coinductive hypothesis혻for this coinductive logic programming language. We prove that this new operational semantics is equivalent to the declarative semantics. Our operational semantics lends itself to an elegant and efficient goal directed proof search in the presence of rational terms and proofs. We describe a prototype implementation of this operational semantics along with applications of coinductive logic programming.




SOCA (2016) 10:111133 DOI 10.1007/s11761-014-0167-5

ORIGINAL RESEARCH PAPER

Generalized semantic Web service composition
Srividya Bansal  Ajay Bansal  Gopal Gupta  M. Brian Blake

Received: 29 January 2014 / Revised: 31 August 2014 / Accepted: 23 October 2014 / Published online: 8 November 2014  Springer-Verlag London 2014

Abstract With the increasing popularity of Web Services and Service-Oriented Architecture, we need infrastructure to discover and compose Web services. In this paper, we present a generalized semantics-based technique for automatic service composition that combines the rigor of processoriented composition with the descriptiveness of semantics. Our generalized approach presented in this paper introduces the use of a conditional directed acyclic graph where complex interactions, containing control flow, information flow, and pre-/post-conditions are effectively represented. Composition solution obtained is represented semantically as OWL-S documents. Web service composition will gain wider acceptance only when users know that the solutions obtained are comprised of trustworthy services. We present a framework that not only uses functional and non-functional attributes provided by the Web service description document but also filters and ranks solutions based on their trust rating that is computed using Centrality Measure of Social Networks. Our contributions are applied for automatic workflow generation in context of the currently important bioinformatics domain. We evaluate our engine for automatic workflow generation of a phylogenetic inference task. We also evaluate our engine
S. Bansal (B)  A. Bansal Arizona State University, Mesa, AZ, USA e-mail: srividya.bansal@asu.edu A. Bansal e-mail: ajay.bansal@asu.edu G. Gupta The University of Texas at Dallas, Richardson, TX, USA e-mail: gupta@utdallas.edu M. B. Blake University of Miami, Miami, FL, USA e-mail: m.brian.blake@miami.edu

for automated discovery and composition on repositories of different sizes and present the results. Keywords Service composition  Service discovery  Semantic Web  Ontology  Workflow generation

1 Introduction The next milestone in the evolution of the World Wide Web is making services ubiquitously available. As automation increases, Web services will be accessed directly by the applications themselves rather than by humans [1,2]. In this context, a Web service can be regarded as a "programmatic interface" that makes application-to-application communication possible. To make services ubiquitously available, we need infrastructure that applications can use to automatically discover, deploy, compose, and synthesize services. A Web service is an autonomous, platform-independent program accessible over the web that may affect some action or change in the world. Sample of Web services include common plane, hotel, rental car reservation services or device controls like sensors or satellites. A Web service can be regarded as a "programmatic interface" that makes application-to-application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the actions that it initiates. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order to make Web services more practical, we need an infrastructure that allows users to discover, deploy, synthesize, and compose services automatically. To make services ubiquitously available, we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, composition,

123

112

SOCA (2016) 10:111133

deployment, and synthesis [3]. Several efforts are underway to build such an infrastructure [46]. With regard to service composition, a composite service is a collection of services combined together in some way to achieve a desired effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [7]. Most efforts reported in the literature focus on one or more of these four phases. The first phase involves generating a plan, i.e., all the services and the order in which they are to be composed in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions based on non-functional properties like QoS properties. The last phase involves executing the services as per the plan and in case any of them are not available, an alternate solution has to be used. In this paper, we present a general approach for automatic service composition. Our composition algorithm performs planning, discovery, and selection automatically, all at once, in one single process. This is in contrast to most methods in the literature where one of the phases (most frequently planning) is performed manually. Additionally, our method generates most general compositions based on (conditional) directed acyclic graphs (DAG). Note that service discovery is a special case of composition of n services, i.e., when n = 1. Thus, we mainly study the general problem of automatically composing n services to satisfy the demand for a particular service, posed as a query by the user. In our framework, the DAG representation of the composite service is reified as an OWL-S description. This description document can be registered in a repository and is thus available for future searches. The composite service can now be discovered as a direct match instead of having to look through the entire repository and build the composition solution again. We show how service composition can be applied to a Bioinformatics analysis application, for automatic workflow generation in the field of Phylogenetics [8]. One of the current challenges in automatic composition of Web services also includes finding a composite Web service that can be trusted by consumers before using it. Our approach uses analysis of Social Networks to calculate a trust rating for each Web service involved in the composition and further prune results based on this rating. Web-based Social Networks have become increasingly popular these days. Social Network Analysis is the process of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups, organizations, computers, or any knowledge entity. We propose to measure

the trust factor of a service by measuring the centrality of a service provider and/or a service provider organization in a well-known Social Network. The three level indices that can be applied to measure centrality are degree, betweenness, and closeness [9]. We adopt our idea of computing trust using centrality measure based on the notion of centrality and prestige being key in the study of social networks [9,10]. The role of central people (nodes with high centrality) in a network seems to be fundamental as they adopt the innovation and help in transportation and diffusion of information throughout the rest of the network. So our rationale is that these central figures who play a fundamental role in the network are trusted by others in the network who are connected (directly or indirectly) to them. A simple use case scenario Jane is a researcher in the field of Evolutionary Genetics. One evening she is examining the evolution of crab species and needs to build a Phylogenetic tree for various crab species using protein sequence data. In order to complete this task, she will have to go to her lab and access the computer with necessary software and perform multiple computations using various algorithms. She uses the well-known Molecular Evolutionary Genetics Analysis software program (MEGA5) [11] that is an integrated tool for conducting automatic and manual sequence alignment, inferring phylogenetic trees, mining web-based databases, estimating rates of molecular evolution, inferring ancestral sequences, and testing evolutionary hypotheses. Jane has to use this software to first align the sequence data using one of several algorithms (such as Clustal W [12], MUSCLE [13], etc.) provided by MEGA5 for this purpose. Next, she wants to compute and compare the evolutionary distances for sequences from crab species using various algorithms. In order to do this she must first compute relevant models for crab species. The next step is to compute evolutionary distances using Jukes쵣antor model followed by Tamura쵳ei model and compare them. She is also interested in the evolutionary distance computed based on proportion of amino acid differences. Finally, she is interested in building a Phylogenetic tree from the aligned sequence data. She will have to pick one of the algorithms/methods provided by MEGA5 that include Maximum Likelihood, Minimum Evolution, Maximum Parsimony, Neighbor-Joining, etc. Currently, there is no easy way to perform this analysis and she will have to use the software tools manually and go through this step-by-step process and wait while computation for each of the steps is being performed. Jane will have to go through this laborintensive process in order to find an answer to her research question. Imagine a Software-as-a-Service (SaaS) platform [14] available on the cloud and Jane has access to it from any computer or mobile device. With a few simple clicks, Jane provides a query request that includes input parameters to this workflow process and expected final outputs. The software

123

SOCA (2016) 10:111133

113

platform built upon our composition engine produces multiple possible workflows using different combinations of algorithms/methods (available as Web services) for each of the tasks such as sequence alignment, model computation, distance computation, and generation of phylogeny. Jane picks a workflow that is most suited for her research analysis and possibly even edits the workflow by adding a service to compute the diversity in the subpopulation of crabs. She saves off this workflow to her profile for future use. She initiates the workflow execution and on the following day, analyses the output results that were produced and saved off in her account. This software platform is able to save Jane a significant amount of time--not only in performing the computations for analysis, but also with configuring workflows and using interesting workflows already created by her colleagues. This is just one simple example of the potential of Web service discovery and composition in various disciplines. This paper presents the underlying composition engine that is needed in order to build such a software platform. This paper extends our previous work in the area of Web service composition [15] by conducting a case study on automatic workflow generation for Phylogenetic Inference tasks in Bioinformatics using our composition engine and introducing the computation of a trust rating of each Web service, based on Centrality measure in Social Network analysis, and using this trust rating in filtering and ranking services. This work would support the development of a SaaS platform that supports domain-specific workflow generation. Our research makes the following novel contributions: (i) Formalization of the generalized composition problem based on our conditional directed acyclic graph representation; (ii) Computation of trust rating of composition solutions based on individual ratings of service providers obtained using the Centrality measure of Social Networks; (iii) Efficient and scalable algorithm for solving the composition problem that takes semantics of services into account; our algorithm automatically discovers and selects the individual services involved in composition for a given query, without the need for manual intervention; (iv) Automatic generation of OWL-S descriptions of the new composite service obtained; (v) Case study of our generalized composition engine to automatically generate workflows in the field of Bioinformatics for Phylogenetic Inference tasks. The rest of the paper is organized as follows. In Sect. 2, we present the related work in the area of Web service discovery and composition and discuss their limitations. In Sect. 3, we formalize the generalized Web service composition problem. We present our multi-step narrowing technique for automatic Web service composition and automatic generation of OWLS service description in Sect. 4. We present the implementation and experimental results in Sect. 5. Section 6 presents an application of our generalized composition engine to automatically generate workflows for Bioinformatics analy-

sis tasks. The last section presents conclusions and future work.

2 Related work Composition of Web services has been active area of research [7,16,17]. Most of these approaches present techniques to solve one or more phases of composition as listed in Sect. 1. There are many approaches [6,18,19] that solve the first two phases of composition namely planning and discovery. These are based on capturing the formal semantics of the service using action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available. To the best of our knowledge, most of these approaches that use planning are restricted to sequential compositions, rather than a directed acyclic graph. In this paper, we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential but also non-sequential that can be represented in the form of a directed acyclic graph. The authors in [18] present a composition technique by applying logical inferencing on predefined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also relies on a user-defined plan template, which is created manually. One of the main objectives of our work is to come up with a technique that can automatically produce composition without the need for any manual intervention. Boustil et al. [20] present an approach that uses an intermediate ontology built using OWL-DL and SWRL rules to define the affected object and their relationships. Their selection strategy considers relationships between services by looking at object values of affected objects. They use a custom intermediate ontology that is built within their framework using OWL-DL. Our approach focuses on the semantics of the parameters as well as constraints represented as pre- and post-conditions. Also our approach is generic and can be used with any domain ontology to provide semantics. There are industry solutions based on WSDL and BPEL4 WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service by composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when an explicit flow is provided. In contrast, our technique auto-

123

114

SOCA (2016) 10:111133

matically determines these complex flows using semantic descriptions of atomic services. A process-level composition solution based on OWL-S is proposed in [19]. In this work, the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but instead assume that they already have the list of atomic services. In contrast, we present a technique that automatically finds the services that are suitable for composition based on the query requirements for the new composed service. There are solutions such as [21] that solve the selection phase of composition. This work uses pre-defined plans and discovered services provided in a matrix representation. Then, the best composition plans are selected and ranked based on QoS parameters like cost, time, and reputation. These criterions are measured using fuzzy numbers. There has been a lot of work on composition languages such as WS-BPEL, FuseJ, AO4BPEL, etc. which are useful only during the execution phase. FuseJ is a description language for unifying aspects and components [22]. Though this language was not designed for Web services, the authors contend that it can be used for service composition as well. It uses connectors to interconnect services. We believe that there is no centralized process description, but instead information about services is spread across the connectors. With FuseJ, the planning phase has to be performed manually that is the connectors have to be written by the developer. Similarly, OWL-S also describes a composite service but does not automatically find the services involved in the composition. So these languages are only useful for execution which happens after the planning, discovery, and selection of services is done. Service grounding of OWL-S maps that describe abstract services to the concrete WSDL specification helps in executing the service. In contrast, our approach automatically generates the composite service. This new composite service generated can then be described using one of these composition languages. QoS-aware composition has also been active area of research [6,21]. Research on a QoS-aware composition [23 25] consider applying SLA's to workflow compositions or Web service compositions, although they do not perform dynamic composition. They use one of the existing composition languages to create the composite service manually or create a template that is later used to select appropriate services for each stage of composition. After obtaining composition solutions manually or semi-automatically, these approaches present a QoS model and apply the nonfunctional attributes on the potential solutions to confirm that they comply with the pre-defined agreements. Thus, the solutions are pruned based on SLA compliance. Work

on workflow Composition of service-level agreements [26] presents a set of SLA measures and principles that best support QoS-based Composition. A model and representation of SLA attributes were introduced and an approach to compose SLA's associated with a workflow of Web services was presented. The research on creating a QoS-Aware middleware for Web service Composition in [27] is similar to our work as they identify services that can fit into a useful composition based on QoS measures. They use two approaches for selection: one based on local (task-level) selection of services and the second is based on a global allocation of tasks to services. They also use a template for composition; in this case, a state chart that has the generic service tasks defined. Finding a composite service involves finding concrete services that fit into the template. In contrast, we do not use any template but instead find the composition solution automatically. The work presented in [28] combine semantic annotations and SLA's thereby providing better approach to specification of SLA's. Researchers have looked into a fuzzy linguistic preference model to provide preference relations on various QoS dimensions [29]. They use a specific weighting procedure to provide numeric weights to preference relations, and then use a hybrid evolutionary algorithm to find skyline solutions efficiently. Their algorithm is designed on the basis of Pareto-dominance and weighted Tchebycheff distance. In this approach, the authors assume that they have candidate services for composition. Their algorithm helps identify best solution based on their SLA's. Feng's research group proposed an approach to composition that associated QoS attributes to service dependencies and showed their approach could model real-life services and perform effective QoS constraint satisfaction and optimization [30]. The attributes taken into consideration by this study are Response time, Cost, Reliability, Availability, and Reputation. They consider that QoS values of a service may be dependent not only on the service itself but also some other services in the workflow. They propose 3 types of QoS for each attribute namely: default QoS, partially dependent QoS, and totally dependent QoS. Default QoS applies no matter what the preceding service is in a workflow, just like the conventional QoS. Partially dependent QoS applies if and only if some of the inputs of a service are provided by the outputs of another service. Totally dependent QoS applies if and only if all inputs of a service are provided by the outputs of another service. Formal modeling of QoS attributes is provided in OWL-S [27]. Work by Wen et al. [32] presents an approach to obtaining probabilistic top-K dominating services with uncertain QoS. QoS values tend to fluctuate at run-time and hence this approach uses probabilistic characteristics of service instances to identify dominating service abilities for better selection. A detailed survey of approaches for a reliable dynamic Web service composition is presented by Immonen

123

SOCA (2016) 10:111133

115

and Pakkala [33]. They discuss various approaches that use Reliability ontology to manage and achieve reliable composition. They address the lack of formalization to handle reliability of composition, whereas the focus of our approach is the formalization of a generalized composition that uses functional attributes to compose a solution and non-functional attributes help in further filtering and ranking solutions. A number approaches focus on trust and reputation QoS criteria for service selection. Mehdi et al.'s [34] approach assigns trust scores to Web services and only services with highest scores are selected for composition. They use Bayesian networks to learn the structure of composition. The approach presented by Kutler et al. [35] considers social trust in Web service composition. They compute trust based on similarity measures over ratings of users added into a system. They use the correlation between trust and overall similarity measures in online communities. On the contrary, we use the centrality measure in a Web-based Social Network. In this paper, we present a technique for automatically planning, discovering, and selecting services that are suitable for obtaining a composite service based on user-query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about services involved or use human input on what services would be suitable for composition. This work is an extension of our earlier work [15] that introduced a generalized Web service composition engine. In this paper, we use the trust rating of a Web service in addition to the functional and non-functional attributes of a service in filtering and ranking solutions. In this paper, we evaluate our composition the engine using a case study from the bioinformatics domain for Phylogenetic inference tasks to show that this engine can be used for automatic workflow generation. The case study uses example workflows that would be generated as a sequential composition, non-sequential composition as well as non-sequential conditional composition.

tasks and develop the requirements of an ideal discovery/composition engine. 3.1 The discovery problem Given a repository of Web services, and a query requesting a service (hereafter query service), automatically finding a service from the repository that matches these requirements is the Web service discovery problem. Only those services that produce at least the requested output parameters that satisfy the post-conditions and use only from the provided input parameters that satisfy the pre-conditions and produce the same side effects can be valid solutions to the query. Some of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre-/post-conditions, and side effect requirements. This activity is best illustrated using an example: Example (Discovery) A buyer is looking for a service to buy a book and the directory of services contains services S1 and S2 . Table 1 shows the input/output parameters of the query and services S1 and S2 . In this example service S2 satisfies the query; however, S1 does not as it requires BookISBN as an input and it is not provided by the query. Our query requires ConfirmationNumber as the output and S2 produces ConfirmationNumber and TrackingNumber. The extra output produced can be ignored. Also the semantic descriptions of service input/output parameters should be same as the query parameters or satisfy the subsumption relation. The discovery engine should be able to infer that the query parameter BookTitle and input parameter BookName of service S2 are semantically the same concepts. This can be inferred using semantics from the annotation of the service and the ontology (e.g., OWL WordNet ontology) provided. The query also has a pre-condition that the CreditCardNumber is numeric, which should logically imply pre-conditions of the discovered service. Definition (Service) A service is a 6-tuple of its preconditions, inputs, side effect, affected object, outputs and post-conditions. S = (CI, I , A , AO, O , CO) is the representation of a service where CI is the list of pre-conditions, I is the input list,

3 Automated Web service discovery and composition Discovery and composition are two important tasks related to Web services. In this section, we formally describe these
Table 1 Discovery--example Service Query S1 S2 Input parameters BookTitle, CreditCardNumber, AuthorName, CreditCardType BookName, AuthorName, BooklSBN, CreditCardNumber BookName, CreditCardNumber Pre-conditions

Output parameters ConfirmationNumber ConfirmationNumber

Post-conditions

lsNumeric(CreditCard Number)

lsNumeric(CreditCard Number)

ConfirmationNumber, TrackingNumber

123

116

SOCA (2016) 10:111133

A is the service's side effect, AO is the affected object, O is the output list, and CO is the list of post-conditions. The preand post-conditions are ground logical predicates. Definition (Repository of Services) Repository ( R ) is a set of Web services. Definition (Query) The query service is defined as Q = (CI , I , A , AO , O , CO ) where CI is the list of preconditions, I is the input list, A is the service affect, AO is the affected object, O is the output list, and CO is the list of post-conditions. These are all the parameters of the requested service. Definition (Discovery) Given a repository R and a query Q , the discovery problem can be defined as automatically finding a set S of services from R such that S = {s | s = I, A = (CI, I , A , AO, O , CO), s R , CI  CI, I A , AO = AO , CO  CO , O  O }. The meaning of is the subsumption (subsumes) relation and  is the implication relation. For example, say x and y are input and output parameters, respectively, of a service. If a query has (x > 5) as a pre-condition and ( y > -x ) as post-condition, then a service with pre-condition (x > 0) and post-condition ( y > x ) can satisfy the query as (x > 5)  (x > 0) and

( y > x )  ( y > -x ) since (x > 0). Figure 1 shows the substitution rules for the discovery problem. 3.2 The composition problem Given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 2 shows an example composite service made up of five services S1 to S5 . In figure, I and CI are the query input parameters and pre-conditions, respectively. O and CO are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and S j indicates that outputs of Si constitute (some of) the inputs of S j . Example (Sequential Composition) Suppose we are looking for a service to make travel arrangements, i.e., flight, hotel, and rental car reservations. The directory of services contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 2 shows the input/output parameters of the user query and the three services ReserveFlight, ReserveHotel, and ReserveCar. For the sake of simplicity, the query and services have fewer input/output parameters than the realworld services. In this example, service ReserveFlight has to be executed first so that its output ArrivalFlightNum can be used as input by ReserveHotel followed by the service ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Figure 3 shows this example sequential composition as a directed acyclic graph. Definition (Sequential Composition) The sequential Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R , given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges

Fig. 1 Substitutable service

Fig. 2 Composite service represented as a directed acyclic graph

Table 2 Sequential composition example Service Query ReserveFlight ReserveHotel ReserveCar Input parameters PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, ArrivalFlightNum, StartDate, ReturnDate PassengerName, ArrivalDate, ArrivalFlightNum, HotelAddress Pre-conditions Output parameters HotelConfirmationNum, CarConfirmationNum FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress CarConfirmationNum Post-conditions

123

SOCA (2016) 10:111133

117

Fig. 3 Sequential composition example

Fig. 5 Non-sequential composition example

Fig. 4 Sequential composition

of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: i Si V , Si R , Si = (CIi , Ii , Ai , AOi , Oi , COi ) 1. I I 1 , O1 I 2 , . . . , On O 2. CI  CI1 , CO1  CI2 , . . . , COn  CO The meaning of the is the subsumption (subsumes) relation, and  is the implication relation. In other words, we are deriving a possible sequence of services where only the provided input parameters are used for the services and at least the required output parameters are provided as an output by the chain of services. The goal is to derive a solution with minimal number of services. Also, the post-conditions of a service in the chain should imply the pre-conditions of the next service in the chain. Figure 4 depicts an instance of sequential composition. Example (Non-sequential composition) Suppose we are looking for a service to buy a book and the directory of services contains services GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. Table 3 shows the input/output parameters of the query and the four services in the repository. Suppose a single matching service is not
Table 3 Non-sequential composition example Service Query GetlSBN GetAvailability Authorize CreditCard PurchaseBook Input parameters BookTitle, CreditCardNum, AuthorName, CardType BookName, AuthorName BooklSBN CreditCardNum BooklSBN, NumAvailable, AuthCode

found in the repository, a solution is synthesized from among the set of services available in the repository. Figure 5 shows this composite service. The post-conditions of the service GetAvailability should logically imply the pre-conditions of service PurchaseBook. Definition (Non-sequential composition) More generally, the Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R , given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: 1. i Si V where Si has exactly one incoming edge that represents the query inputs and pre-conditions, I i Ii , CI  i CIi . 2. i Si V where Si has exactly one outgoing edge that represents the query outputs and post-conditions, O i Oi , CO  i COi . 3. i Si V where Si has at least one incoming edge, let Si 1 , Si 2 , . . . , Sim be the nodes such that there is a directed edge from each of these nodes to Si . Then, Ii k Oik  I , CIi  (COi 1  COi 2 . . .  COim  CI ). Figure 6 depicts an instance of non sequential composition.

Pre-conditions

Output parameters ConfNumber ConfNumber NumAvailable AuthCode

Post-conditions

NumAvailable > 0 AuthCode > 99  AuthCode < 1000

NumAvailable > 0

ConfNumber

123

118

SOCA (2016) 10:111133

Fig. 6 Non-sequential composition

Example (Non-sequential conditional composition) Nonsequential conditional composition consists of if-then-else conditions, i.e., the composition flow varies depending on the result of the post-conditions of a service. Suppose we are looking for a service to make international travel arrangements. We first need to make a tentative flight and hotel reservation and then apply for a visa. If the visa is approved, we can buy the flight ticket and confirm the hotel reservation, else we will have to cancel both the reservations. Also, if the visa is approved, we need to make a car reservation. The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table 4 shows the input/output parameters of the user query and services. In this example, service ProcessVisa produces the post-condition VisaApproved  VisaDenied. The services ConfirmFlight and ConfirmHotel have the pre-condition VisaApproved. In this case, one cannot determine whether the post-conditions of
Table 4 Non sequential conditional composition example Service Query Pre-conditions Input parameters

service ProcessVisa implies the pre-conditions of services ConfirmFlight and ConfirmHotel until the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and depending on the outcome of the condition, the corresponding services will be executed. The vertex for service ProcessVisa in the graph is followed by a condition node which represents the postcondition of service ProcessVisa. This node has two outgoing edges one representing the case if the condition is satisfied at run-time and other edge for the case where the condition is not satisfied. In other words, these edges represent the generated conditions which in this case are, (VisaApproved  VisaDenied)  VisaApproved and VisaApproved  VisaDenied)  VisaDenied. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed. Figure 7 shows this conditional composition example as a directed acyclic graph.

Definition (Generalized Composition) The generalized Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R , given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph either represents a service involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can be determined only after the execution of the service. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph:

Output parameter FlightConfirmationNum, HotelConfirmationNum, CarConfirmationNum FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress ConfirmationNum

Post-conditions

PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, OriginAirport, StartDate, DestinationAirport, ReturnDate PassengerName, ArrivalFlightNum, StartDate, ReturnDate PassengerName, VisaType, FlightConfirmationNum, HotelConfirmationNum VisaApproved VisaApproved VisaDenied VisaDenied FlightConfirmationNum, CreditCardNum HotelConfirmationNum, CreditCardNum FlightConfirmationNum, PassengerName HotelConfirmationNum, PassengerName PassengerName, ArrivalDate, HotelAddress, ArrivalFlightNum

ReserveFlight ReserveHotel ProcessVisa

VisaApproved V VisaDenied

ConfirmFlight ConfirmHotel CancelFlight Cancel Hotel ReserveCar

FlightConfirmationNum HotelConfirmationNum CancelCode CancelCode CarConfirmationNum

123

SOCA (2016) 10:111133

119

Fig. 7 Non-sequential conditional composition

1. i Si V where Si has exactly one incoming edge that represents the query inputs and pre-conditions, I i Ii , CI  i CIi . 2. i Si V where Si has exactly one outgoing edge that represents the query outputs and post-conditions, O i Oi , CO  i COi . 3. i Si V where Si represents a service and has at least one incoming edge, let Si 1 , Si 2 , ..., Sim be the nodes such that there is a directed edge from each of these nodes to Si . Then, Ii k Oik  I , C Ii  (COi 1  COi 2 . . .  COim  CI ). 4. i Si V where Si represents a condition that is evaluated at run-time and has exactly one incoming edge, let S j be its immediate predecessor node such that there is a directed edge from S j to Si . Then, the inputs and preconditions at node Si are Ii = O j  I ; CIi = CO j . The outgoing edges from Si represent the outputs that are same as the inputs Ii and the post-conditions that are the result of the condition evaluation at run-time.

post-conditions CO1 of S1 must imply the preconditions CI2 of S2 . The following conditions are evaluated at run-time: if (CO1  CI2 ) then execute S1; else if (CO1   CI2 then no-op}; else if (CI2 then execute S1 }; When the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem. Further details and examples are available in our prior work [15]. 3.3 Requirements of an ideal engine The features of an ideal discovery/composition engine are as follows: Correctness One of the most important requirements for an ideal engine is to produce correct results, i.e., the services discovered and composed by it should satisfy all the requirements of the query. Also, the engine should be able to find all services that satisfy the query requirements. Minimal query execution time Querying a repository of services for a requested service should take a reasonable amount of (minimal) time, i.e., a few milliseconds. Here, we assume that the repository of services may be pre-processed (indexing, change in format, etc.) and is ready for querying. In case services are not added incrementally, then time for pre-processing a service repository is a one-time effort that takes considerable amount of time, but gets amortized over a large number of queries. Incremental updates Adding or updating a service to an existing repository of services should take minimal time. An ideal discovery and composition engine should not preprocess the entire repository again; rather incrementally update pre-processed data (indexes, etc.) with data for the new service.

The meaning of the is the subsumption (subsumes) relation and  is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. When it cannot be determined at compile time whether the post-conditions imply the pre-conditions or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible conditions that will be evaluated at run-time. Depending on the condition that holds, the corresponding services are executed. That is, if a subservice S1 is composed with subservice S2 , then the

123

120

SOCA (2016) 10:111133 Table 5 Degree centrality of nodes in Fig. 10 Service provider Provider A Provider B Provider C Provider D Provider E Provider F Provider G Provider H Provider I Provider J Provider K Provider L Provider M Degree 2 3 1 8 3 4 5 3 1 2 4 1

Cost function If there are costs associated with every service in the repository, then an ideal discovery and composition engine should be able to provide results based on requirements (minimize, maximize, etc.) over the costs. We can extend this to services having an associated attribute vector, and the engine should be able to provide results based on maximizing or minimizing functions over the attribute vector. These requirements have driven the design of our semanticsbased discovery and composition engine described in this paper. 3.4 Centrality measure in social networks Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds. It involves measuring the formal and informal relationships to understand information/knowledge flow that binds the interacting units that could be a person, group, organization, or any knowledge entity. Social Network Analysis has an increasing application in social sciences that has been applied to diverse areas such as psychology, health, electronic communications, and business organization. In order to understand social networks and their participants, the location of an actor in a network is evaluated. The network location is measured in terms of centrality of a node that gives an insight into the various roles and groupings in a network. Centrality gives a rough indication of the social power of a node based on how well they "connect" the network. There has been extensive discussion in the Social Network community regarding the meaning of the term centrality when it is applied to Social Networks. One view stems directly from graph theory [9]. The graph-theoretic conception of compactness has been extended to the study of Social Networks and simply renamed "graph centrality". Their measures are all based upon distances between points, and all define graphs as centralized to the degree that their points are all close together. The alternative view emerged from substantive research on communication in Social Networks. From this perspective, the centrality of an entire network should index the tendency of a single point to be more central than all other points in the network. Measures of a graph centrality of this type are based on differences between the centrality of the most central point and that of all others. Thus, they are indexes of the centralization of the network [36]. The three most popular individual centrality measures are Degree, Betweenness, and Closeness Centrality.  Degree centrality The network activity of a node can be measured using the concept of degrees, i.e., the number of direct connections a node has. In the example, network shown in Fig. 10 and Table 5, Provider D has the most direct connections in the network, making it the most

active node in the network. In personal Social Networks, the common thought is that "the more connections, the better".  Betweenness centrality Though Provider D has many direct ties, Provider H has fewer direct connections (close to the average in the network). Yet, in many ways, Provider H has one of the best locations in the network by playing the role of a "broker" between two important components. A node with high betweenness has greater influence over what flows and does not in the network.  Closeness centrality Provider F and G have fewer connections than Provider D, yet the pattern of their direct and indirect ties allow them to access all the nodes in the network more quickly than anyone else. They have the shortest paths to all others, i.e., they are close to everyone else. They are in an excellent position and have the best visibility into what is happening in the network.

Individual network centralities provide insight into the individual's location in the network. The relationship between the centralities of all nodes can reveal much about the overall network structure. 3.5 Trust rating of a service and trust threshold The trust rating of each service in the repository is computed as a measure of the degree centrality (CD) of the social network to which the service provider belongs. It is calculated as the degree or count of the number of adjacencies for a node, sk : C D (s k ) =
n i -0

a (si , sk )

123

SOCA (2016) 10:111133

121

where a (si sk ) = 1 iff si and sk are connected by a line 0 otherwise As such it is a straightforward index of the extent to which sk is a focus of activity [9]. C D (sk ) is large if service provider sk is adjacent to, or in direct contact with, a large number of other service providers, and small if sk tends to be cut off from such direct contact. C D (sk ) = 0 for a service provider that is totally isolated from any other point. Our algorithm filters out any services whose provider has a zero degree centrality in a social network, i.e., such services will not be used in building composition solutions. Trust rating of the entire composite service is computed as an average of the individual trust ratings of the services involved in the composition. We also need to set a Trust Threshold and any service with a Trust rating that is below this threshold is not used while generating composition solutions. In our initial prototype implementation, we set the Trust threshold to zero, i.e., degree centrality of the service provider in the network is zero. A service provider or service provider organization that is not connected to any other nodes in the Social network is not known to anyone else and is an immediate reason to be pruned out from composition solutions, as the service cannot be trusted. Composition solutions can be ranked such that solutions with highest trust rating appear on top of the list. 4 Dynamic Web service composition: methodology In this section, we describe our methodology for automatic Web service composition that produces a general directed acyclic graph. The composition solution produced is for the generalized composition problem presented in Sect. 3. We also present our algorithm for automatic generation of OWLS descriptions for the new composite service produced. 4.1 Algorithms for Web service discovery and composition Our approach is based on a multi-step narrowing of the list of candidate services using various constraints at each step. As mentioned earlier, discovery is a simple case of Composition. When the number of services involved in the composition is exactly equal to one, the problem reduces to a discovery problem. Hence, we use the same engine for both discovery and composition. We assume that a directory of services has already been compiled, and that this directory includes semantic descriptions for each service. In our implementation, we use semantic descriptions written in USDL [37], although the algorithms are general enough that they will work with any semantic annotation language. The repository of services contains one USDL description document for each service. However, we still need a query language to

search this directory, i.e., we need a language to frame the requirements of the service that an application developer is seeking. USDL itself can be used as such as a query language. A USDL description of the desired service can be written (with tool assistance), a query processor can then search the service directory for a "matching" service. For service composition, the first step is finding the set of composable services. USDL itself is used to specify the requirements of the composed service that an application developer is seeking. Using the discovery engine, individual services that make up the composed service can be selected. Part substitution techniques [38] can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the individual services. That is, if a subservice S1 is composed with subservice S2 , then the post-conditions of S1 must imply the pre-conditions of S2 . The goal is to derive a single solution, which is a directed acyclic graph of services that can be composed together to produce the requested service in the query. Figure 8 shows a pictorial representation of our composition engine. 4.2 Multi-step narrowing solution To produce a composite service, as shown in the example Fig. 2, our algorithm filters services that are not useful for the composition at multiple stages. Figure 9 shows the filtering technique for the particular instance graph represented in Fig. 2. The composition routine begins with the query input parameters and finds all those services from the repository that require a subset of the query input parameters. In Fig. 9, CI , I are the pre-conditions and the input parameters provided by the query. S1 and S2 are the services found after step 1. O1 is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e., I2 = O1  I. I2 is used to find services at the next stage, i.e., all those services that require a subset of I2 . To make sure we do not end up in cycles, we get only those services that require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point, we make another pass in the reverse direction to remove redundant services that do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters and working our way backwards. Next, another level of filtering is performed using the trust ratings of services. Table 6 shows the algorithm and we have prototype implementation of this algorithm implemented using Prolog [39] with Constraint Logic Programming over finite domain (CLP(FD)) [40]. The rationale behind the choice of

123

122 Fig. 8 Composition engine--design

SOCA (2016) 10:111133

Fig. 9 Multi-step narrowing solution Table 6 Algorithm for multi-step narrowing Multi-step Narrowing Algorithm Algorithm: Composition (Input QI - QueryInputs, QO - QueryOutputs, QCI - Pre-Cond, QCO - Post-Cond, T - TrustThreshold) (Output: Result - ListOfServices) 1. L  NarrowServiceList(QI, QCI); 2. O  GetAllOutputParameters(L); 3. CO  GetAllPostConditions(L); 4. While Not (O QO) 5. I = QI  O; CI  QCI  CO; 6. L  NarrowServiceList(I, CI); 7. End While; 8. IntResult  RemoveRedundantServices(QO, QCO); 9. Result  RemoveRedundantServices(T, IntResult); 10. Return Result;

CLP(FD), implementation details, and experimental results are available [15]. 4.3 Automatic generation of OWL-S descriptions After obtaining a composition solution (sequential, nonsequential, or conditional), the next step is to produce a semantic description document for this new composite service. This document can be used for execution of the service

and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of performing the composition process all over again. We used the existing language OWL-S [31] to describe composite services. OWL-S models services as processes and when used to describe composite services, it maintains the state throughout the process. It provides control constructs such as Sequence, Split-Join, If-Then-Else and many more to describe composite services. These control constructs can be used to describe

123

SOCA (2016) 10:111133 Table 7 Generation of composite service description Generation of Composite Service Description Algorithm: GenerateCompositeServiceDescription (Input: G - CompositionSolutionGraph) (Output: D - CompositeServiceDescription) 1. Generate generic header constructs 2. Start Composite Service element 3. Start SequenceConstruct 4. If Number(SourceVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each starting/source Vertex V GenerateAtomicService End For EndSplitJoinConstruct End If 5. If Number(SinkVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each ending/sink Vertex V GenerateAtomicService End For EndSplitJoinConstruct End If 6. For Each remaining vertex V in G If V is AND vertex with one outgoing edge GenerateAtomicService If V is AND vertex with > 1 outgoing edge GenerateSplitJoinConstruct If V is OR vertex with one outgoing edge GenerateAtomicService qquad If V is OR vertex with > 1 outgoing edge GenerateConditionalConstruct End For 7. End SequenceConstruct 8. End Composite Service element 9. Generate generic footer constructs

123

rently. The process completes execution only when all the services in this construct have completed their execution. The non-sequential conditional composition can be described in OWL-S using the If-Then-Else construct which specifies the condition and the services that should be executed if the condition holds and also specifies what happens when the condition does not hold. Conditions in OWL-S are described using SWRL. There are other constructs such as looping constructs in OWL-S that can be used to describe composite services with complex looping process flows. We are currently investigating other kinds of compositions with iterations and repeat-until loops and their OWL-S document generation. We are exploring the possibility of unfolding a loop into a linear chain of services that are repeatedly executed. We are also analyzing our choice of the composition language and looking at other possibilities as part of our future work. 5 Implementation and experimental results This section presents implementation details of the composition engine. We also analyze the performance and present experimental results. 5.1 Implementation Our discovery and composition engine is implemented using Prolog [39] with Constraint Logic Programming over finite domain [40], referred to as CLP(FD) hereafter. In our current implementation, we used semantic descriptions written in the language called Universal Semantics-Service Description Language (USDL) [37]. The repository of services contains one USDL description document for each service. USDL itself is used to specify the requirements of the service that an application developer is seeking. USDL is a language that service developers can use to specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real-world concepts. USDL uses WordNet [41] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the "meaning" of input parameters, outputs, and the side effect induced by the service is given by mapping these syntactic terms to concepts in WordNet [38] for details of the representation. Inclusion of USDL descriptions thus makes services directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory for a "matching" service. These algorithms can be used with any other Semantic Web service descrip-

the kind of composition. OWL-S also provides a property called composedBy using which the services involved in the composition can be specified. Table 7 shows the algorithm for generation of the OWL-S document when the composition solution in the form of a graph is provided as the input. A sequential composition can be described using the Sequence construct that indicates that all the services inside this construct have to be invoked one after the other in the same order. The non-sequential composition can be described in OWL-S using the Split-Join construct which indicates that all the services inside this construct can be invoked concur-

123

124

SOCA (2016) 10:111133

A USDL description of the desired service can be written, which is read by the query reader and converted to a triple. This module can be easily extended to read descriptions written in other languages. (iii) Semantic relations generator We obtain the semantic relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms, meronyms, holonyms and many more. USDL descriptions point to OWL WordNet for the meanings of concepts. A theory of service substitution is described in detail in [38] which uses the semantic relations between basic concepts of WordNet to derive the semantic relations between services. This module extracts all the semantic relations and creates a list of Prolog facts. We can also use any other domain-specific ontology to obtain semantic relations of concepts. We are currently looking into making the parser in this module more generic to handle any other ontology written in OWL. (iv) Discovery query processor This module compares the discovery query with all the services in the repository. The processor works as follows: 1. On the output parameters of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a parameter with hyponym relation [38], i.e., a specific substitutable. 2. On the input parameters of a service, the processor first looks for an exact substitutable. If it does not find one, then it looks for a parameter with hypernym relation [38], i.e., a generic substitutable. The discovery engine, written using Prolog with CLP(FD) library, uses a repository of facts, which contains a list of all services, their input and output parameters and semantic relations between parameters. The code snippet of our engine is shown in Table 8. The query is parsed and converted into a Prolog query that looks as follows: discovery(sol(queryService, ListOfSolutionServices). The engine will try to find a list of SolutionServices that match the queryService. (v) Composition engine (ii) Query reader This module reads the query file and passes it on to the Triple Generator. We use USDL itself as the query language. The composition engine is written using Prolog with CLP(FD) library. It uses a repository of facts, which contains all the services, their input and output parameters and the semantic

Fig. 10 A social network of Web service providers

tion language as well. It will involve extending our implementation to work for other description formats, and we are looking into that as part of our future work. The parsing of all the USDL description documents and the universal ontology is written in Java. The parsing is done via SAXReader library of Java and after the parsing, prolog engine is instantiated to run the Composition query processor. The complete discovery and composition engine is implemented as a Web service in Java using Apache Tomcat. The Web service in turn invokes Prolog to do all the processing [4244]. The high-level design of the Discovery and Composition engines is shown in Fig. 10. The software system is made up of the following components: (i) Triple generator The triple generator module converts each service description into a triple. In this case, USDL descriptions are converted to triples like: (Pre-Conditions, affect-type(affected-object, I, O), PostConditions) The function symbol affect-type is the side effect of the service and affected object} is the object that changed due to the side effect. I is the list of inputs, and O is the list of outputs. Pre-Conditions are the conditions on the input parameters, and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [38]. In case conditions on a service are not provided, the Pre-Conditions and PostConditions in the triple will be null. Similarly, if the affecttype is not available, this module assigns a generic affect to the service.

123

SOCA (2016) 10:111133 Table 8 Discovery Algorithm--Code Snippet Discovery Algorithm discovery(sol(Qname,A)) :dQuery(Qname,I,O), encodeParam(O,OL), /* Narrow candidate services(S) using output list(OL) */ narrowO(OL,S), fdset(S,FDs), fdsettolist(FDs,SL), /* Expand InputList(I) using semantic relations */ getExtInpList(I, ExtInpList), encodeParam(ExtInpList,IL), /* Narrow candidate services(SL) using input list (IL) */ narrowI(IL,SL,SA), decodeS(SA,A).

125

step narrowing-based approach to solve these problems and implemented it using constraint logic programming. (i) Correctness Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions. Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always find a correct solution (if one exists) in the minimum possible number of steps. (ii) Pre-processing Our system initially pre-processes the repository and converts all service descriptions into Prolog terms. The semantic relations are also processed and loaded as Prolog terms in memory. Once the pre-processing is done, then discovery or composition queries are run against all these Prolog terms and, hence, we obtain results quickly and efficiently. The built-in indexing scheme and constraints in CLP (FD) facilitate the fast execution of queries. During the pre-processing phase, we use the term representations of services to set up constraints on services and the individual input and output parameters. This further helped us in getting optimal results. (iii) Execution efficiency The use of CLP (FD) helped significantly in rapidly obtaining answers to the discovery and composition queries. We tabulated processing times for different size repositories, and the results are shown in the next section. As one can see, after pre-processing the repository, our system is quite efficient in processing the query. The query execution time is insignificant. (iv) Programming efficiency The use of Constraint Logic Programming helped us in coming up with a simple and elegant code. We used a number of built-in features such as indexing, set operations, and constraints and, hence, did not have to spend time coding these ourselves. This made our approach efficient in terms of programming time as well. Not only the whole system is about 200 lines of code, but we also managed to develop it in less than 2 weeks. (v) Scalability Our system allows for incremental updates on the repository, i.e., once the pre-processing of a repository is done, adding

Table 9 Composition Algorithm--Code Snippet Composition Algorithm composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs), encodeParam(QueryOutputs, QO), getExtInpList(QueryInputs, InpList), encodeParam(InpList, QI), performForwardTask(QI, QO, LF), performBackwardTask(LF, QO, LR), getMinSolution(LR, QI, QO, A), reverse(A, RevA), confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

relations between the parameters. A code snippet of our composition engine is shown in Table 9. The query is converted into a Prolog query that looks as follows: composition(queryService, ListOfServices). The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the built-in, higher order predicate "bagof" to return all possible ListOfServices that can be composed to get the requested queryService. (vi) Output generator After the Composition engine finds a matching service, or the list of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files in any desired XML format. 5.2 Efficiency and scalability issues In this section, we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It is because of these features that we decided on the multi-

123

126

SOCA (2016) 10:111133 Table 10 Sample service interface description Sample WSDL description <message name ="InputName"> <part name = "part0" type = "Name"/> </message > <message name ="OutputAddress"> <part name = "part0" type = "US-Address" /> </message> <portType name = "AdressConverter"> <operation name ="Convert" > <input message ="InputName" /> <output message = "OutputAddress"/> </operation > </ portType >

a new service or updating an existing one will not need reexecution of the entire pre-processing phase. Instead, we can easily update the existing list of CLP (FD) terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will be negligible, perhaps a few milliseconds. With real-world services, it is likely that new services will get added often or updates might be made on existing services. In such a case, avoiding repeated preprocessing of the entire repository will definitely be needed and incremental updates will be of great practical use. The efficiency of the incremental update operation makes our system highly scalable. (vi) Use of external database In case the repository grows extremely large in size, then saving off results from the pre-processing phase into some external database might be useful. This is part of our future work. With extremely large repositories, holding all the results of pre-processing in the main memory may not be feasible. In such a case, we can query a database where all the information is stored. Applying incremental updates to the database is easily possible, thus avoiding recomputation of preprocessed data. (vii) Searching for optimal solution If there are any properties with respect to which the solutions can be ranked, then setting up global constraints to get the optimal solution is relatively easy with the constraint-based approach. For example, if each service has an associated cost, then the discovery and the composition problem can be redefined to find the solutions with the minimal cost. Our system can be easily extended to take these global constraints into account. 5.3 Performance and experimental results To conduct our experiments, we looked at various benchmarks and Web services challenge (WSC) datasets [42,43] best suited our needs and fit well into the overall architecture with minimal changes. They provided semantics through XML schema, provided queries and corresponding solutions. We used repositories from WSC website [42,43], slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The input and output messages of the services may contain multiple parameters. Each parameter is annotated with a semantic concept stored in the attribute type. Table 10 shows a service AddressConverter with one operation named Convert. It can be invoked with an input message (InputName) and produces a response message (OutputAddress). The value of

the attribute message represents a reference to a message element. Each message has a set of part elements as children, which represent the service parameters, annotated with concepts referenced by the type-attribute. The Convert operation in this example requires a parameter of the type Name and returns an instance of US-Address. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema format. Concepts were treated as data types and taxonomies encoded as hierarchies of such data types in XSD schemas. The subsumes relation between two semantic concepts can be compared to the subclass relationship in Object-oriented programming. Table 11 shows a sample XSD schema defining the data types Address and US-Address inheriting from Address. In the context of the WSC, this schema would be interpreted as a taxonomy introducing the concepts Address and US-Address with subsumes(Address, US-Address). We evaluated our approach on different size repositories and tabulated Pre-processing, Query Execution, and Incremental update time. We noticed that there was a significant difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we found is that the repository was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 12 shows performance results of our Composition algorithm on discovery queries, and Table 13 shows the results of our algorithm on composition queries. The times shown in tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than or equal to the wall clock time. The experiments were conducted on different repository sizes as well as varying number of input and output parameters for each service in the repository. The results are plotted in Figs. 11 and 12 where

123

SOCA (2016) 10:111133 Table 11 Sample XSD Schema providing semantics Sample XSD Schema <complexType name =" Address " > <sequence > <element name = "name" type = "string" minOccurs = "0"/> <element name = "street" type = "string" /> <element name = "city" type = "string" /> </ sequence> </complexType> <complexType name = "US - Address"> <complexContent > <extension base = "Address" > <sequence> <element name = "state" type = "US - State"/> <element name = "zip" type = "positiveInteger"/> </sequence> </extension> </complexContent> </complexType> Fig. 11 High-level design on composition engine

127

Table 12 Performance on discovery queries
Repository size Number of I/O parameters 48 1620 3236 48 1620 3236 48 1620 3236 Pre-processing time (ms) Incremental Query execution update (ms) time (ms) 1 1 2 1 1 2 1 1 3 18 23 28 19 23 29 19 26 29

2,000 2,000 2,000 2,500 2,500 2,500 3,000 3,000 3,000

36.5 45.8 57.8 47.7 58.7 71.6 56.8 77.1 88.2

the numbers of I/O parameters were 48, 1620, and 3236, respectively. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the pre-processing time increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which is negligible (just 13 ms) even for complex queries with large repositories. Discussion We evaluated our approach for correctness, efficiency, and scalability of the software. The correctness of the algorithm has been described in Sect. 4. In the experiments conducted, discovery and composition solutions obtained were checked against a pre-defined set of solutions produced for the WSChallenge datasets [43]. This was the first and most important criterion for evaluation of the engine. Our second criterion was query efficiency, i.e., rapidly obtaining answers to discovery and composition queries. The results show that irrespective of the repository size the query execution time was always 12 ms. This was because the repositories were preprocessed, and the queries were run against pre-processed data. Different repository sizes were used along with varying the number of input and output parameters of the Web services. The software also scaled well with varying the size of the repositories. These three criterion are important for the successful adoption of the composition engine to produce a Software-as-a-Service (SaaS) platform for automatic workflow generation in a specific domain as described in the sample scenario in Sect. 1. An important part of scalability is the ability to sustained performance even if pre-processed

Table 13 Performance on composition queries
Repository size Number of I/O parameters 48 1620 3236 48 1620 3236 48 1620 3236 Pre-processing I/O time (ms) 36.1 47.1 60.2 58.4 60.1 102.1 71.2 87.9 129.2 Incremental Query execution update (ms) time (ms) 1 1 1 1 1 1 1 1 1 18 23 30 19 20 34 18 22 32

2,000 2,000 2,000 3,000 3,000 3,000 4,000 4,000 4,000

123

128

SOCA (2016) 10:111133

Fig. 12 Performance on discovery queries

repositories change in size, new services are added, existing services are removed, etc. To evaluate this aspect, we studied the time taken for incremental updates to the repository. The results obtained were all less than one second for different repository sizes and well as varying number of input and output parameters. A network of service providers was introduced synthetically into the experimental datasets. Trust rating of services was computed based on this network. Trust ratings generator helps with filtering services based on the trust threshold and helps in ranking. The performance of the engine on discovery and composition queries still remains the same.

development, etc. In order to perform these tasks, scientists use a number of software tools and programs already available. They use them by putting them together in a particular order (i.e., a workflow) to get their desired results. That is, it involves execution of a sequence of steps using various programs or software tools. These tools use different data formats, and hence translating from one data format to another becomes necessary. 6.2 Automatic workflow generation The software tools and programs created for specific phylogenetic tasks use different data formats for their input and output parameters. The data description language Nexus [46,47] is used as a universal language for representation of these bioinformatics related data. There are translator programs that convert different formats into Nexus and vice versa. For example, one could use the BLAST program to get a sequence set of genes. Once the sequence set is obtained, the sequences can be aligned using the CLUSTAL program. But the output from BLAST cannot be directly fed to CLUSTAL, as their data formats are different. The translator can be used to convert the BLAST format to Nexus and then the Nexus format to CLUSTAL. In order to perform an inferencing task, one has to manually pick all the appropriate programs and corresponding format translators and put them in the correct order to produce a workflow. We show how Web service composition can be directly applied to automate this task of producing a workflow. MyGrid [48] has a wealth of bioinformatics resources and data that provides opportunities for research. It has hundreds of biological Web services and their WSDL descriptions, provided by various research groups around the world. We illustrate our generalized framework for Web service composition by applying it to these services to generate workflows automatically that are practically useful for this field. Example Workflow Generation (Non-sequential Composition) Suppose we are looking for a service that takes a GeneInput and produces its corresponding AccessionNumbers, AGI, and GeneIdentifier as output. The directory of services contains CreateMobyData, MOBYSHoundGetGen-

6 Application to bioinformatics We illustrate the practicality of our general framework for automatically composing services by applying it to phylogenetics, a subfield of bioinformatics, for automatic generation of workflows. In this section, we present a brief description of the field of Phylogenetics [8] followed by an example of a workflow generation problem that can be mapped to a non-sequential conditional composition problem (the most general case of the composition problem) and can be solved using our generalized composition engine. 6.1 Phylogenetics Phylogenetic inferencing involves an attempt to estimate the evolutionary history of a collection of organisms (taxa) or a family of genes [45]. The two major components of this task are the estimation of the evolutionary tree (branching order), then using the estimated trees (phylogenies) as analytical framework for further evolutionary study and finally performing the traditional role of systematics and classification. Using this study, a number of interesting facts can be discovered, for example, who are the closest living relatives of humans, who are whales related to, etc. Different studies can be conducted, for example, studying dynamics of microbial communities, predicting evolution of influenza viruses and other applications such as Drug Discovery, and vaccine

123

SOCA (2016) 10:111133 Table 14 Bioinformatics application--non-sequential composition example Service Query CreateMoby Data MOBYSHoundGet GenBankWbatev erSequence MlPSBlastBetterE13 Extract Accession ExtractBestHit Format (MobyData) = NCBI Pre-conditions Input parameters Genelnput Genelnput MobyData Output parameters AccessionN umbers, AGIj Geneldentity MobyData GeneSequence

129

Post-conditions

Format (MobyData) = NCBI

GeneSequence GeneSequence WUGeneSequence

WUGene Sequence AccessionNumbers AGI, Geneldentity

Fig. 13 Performance on composition queries

Bank WhateverSequence, ExtractAccession, ExtractBestHit, MIPSBlastBetterE13 services. In this scenario, the GeneInput first needs to be converted to NCBI data format and then its corresponding GeneSequence is further passed to ExtractAccession and ExtractBestHit to obtain the AccessionNumbers, AGI, and GeneIdentity respectively. Table 14 shows the input/output parameters of the user query and the services. Figure 13 shows this non-sequential composition example as a directed acyclic graph. In this example:  Service CreateMobyData has a post-condition on its output parameter MobyData that the format is NCBI and service MOBYSHoundGetGenBankWhateverSequence has a pre-condition that its input parameter MobyData has to be in NCBI format for service execution. The postcondition of CreateMobyData must imply pre-condition of MOBYSHoundGetGenBankWhateverSequence service.  Both services ExtractAccession and ExtractBestHit have to be executed to obtain the query outputs.  The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Example Workflow Generation (Non-sequential Conditional Composition) Suppose we are looking for a service that takes

a GeneSequence and produces an EvolutionTree and EvolutionDistance after performing a phylogenetic analysis. Also the service should satisfy the post-condition that the EvolutionTree produced is in the Newick format. This involves producing a sequence set first, followed by aligning the sequence set and then producing the evolution tree and evolution distance. Also any necessary intermediate data format translations have to be performed. Table 15 lists the services in the repository and their corresponding input/output parameters and user query. For the sake of simplicity, the query and services have fewer input/output parameters than the realworld services. In this example, service BLAST has to be executed first so that its output BLASTSequenceSet can be used as input by CLUSTAL after the data format has been translated using BLASTNexus and NexusCLUSTAL. The service BLASTNexus has a post-condition that the format of the output parameter NexusSequenceSet is Nexus which is the pre-condition of the next service NexusCLUSTAL. Similarly the service NexusCLUSTAL has a post-condition that the format of the output parameter ClustalSequenceSet is Clustal that is the pre-condition of the next service CLUSTAL. At every step of composition, the post-conditions of a service should imply the pre-conditions of the following service. The post-condition of the service CLUSTAL is that the output parameter AlignedSequenceSet has either Paup or Phylip format. Depending on which one of these two conditions hold,

123

130 Table 15 Bioinformatics application--non-sequential conditional composition example Service Query Pre-conditions Input parameter Sequence, OrganismType, Word Size, DatabaseName Sequence, Organism Type, DatabaseName Format (Clustal SequenceSet) = Clustal Clustal SequenceSet Outpur parameter EvolutionTree, EvolutionDistance BlastSequenceSet AlignedSequenceSet

SOCA (2016) 10:111133

Post-conditions Format (EvolutionTree) = Newick

BLAST CLUSTAL

Format (AlignedSequence Set) = Paup  Forrmat(AlignedSequence Set) = Phylip Format (NexusSequenceSet) = Nexus Forma(ClustalSequenceSet) = Clustal Format (EvolutionTree) = Newick Format (EvolutionTree) = Newick

BLASTNexus NexusCLUSTAL PAUP PHYLIP MEGA Format (Nexus SequenceSet) = Nexus Format (Aligned SequenceSet) = Paup Format (Aligned SequenceSet) = Phylip Format (Aligned Sequenced) = Paup

BlastSequenceSet NexusSeqtienceSet AlignedSequenceSet, Word Size AlignetBeciuenceSet AlignedSequenceSet

NexusSequenceSet ClustalSequenceSet EvolutionTree EvokiSonTree EvaluationDistance

Fig. 14 Bioinformatics application--non sequential composition as a directed acyclic graph

the next service for composition is chosen. In this case, one cannot determine whether the post-conditions of the service CLUSTAL imply pre-conditions of PAUP or PHYLIP until the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and depending on the outcome of the condition, corresponding services will be executed. The vertex for service CLUSTAL in Fig. 14 has an outgoing edge to a conditional node. The outgoing edge represents the outputs and post-conditions of the service. The conditional node has multiple outgoing edges that represent the generated conditions that are evaluated at run-time. In this case, the following conditions are generated:  (Format(AlignedSequenceSet) = Paup  Format(AlignedSequenceSet) = Phylip)  (Format(AlignedSequenceSet) = Paup)  (Format(AlignedSequenceSet) = Paup  Format(AlignedSequenceSet) = Phylip)  (Format(AlignedSequenceSet) = Phylip)

Depending on the condition that holds, the corresponding services PAUP and MEGA or PHYLIP are executed respectively. The outputs EvolutionTree and EvolutionDistance are produced in both the cases along with the post-condition that the format of the evolution tree is Newick. Figure 14 shows this non-sequential conditional composition example as a conditional directed acyclic graph. 6.3 Implementation In order to apply service composition to obtain the sequence of tasks automatically, these programs have to be made available as Web services and their descriptions should be provided using one of the Web services description languages like WSDL (Web Services Description Language) or USDL (Universal Service-Semantics Description Language). The translator programs also have to be available as Web services. Then, we can write a query specifying the input parameters provided and the output parameters that have to be obtained. The Composition engine then looks up the repository of available services and finds the solution, i.e., a set

123

SOCA (2016) 10:111133 Fig. 15 Bioinformatics application--non sequential conditional composition as a directed acyclic graph

131

Fig. 16 Composition Solution 1 for query in Table 15

of services that can be executed to obtain the requested output parameters. These set of services obtained will need only those input parameters that are provided by the query and their execution produces the output parameters specified by the query. We tested our Composition engine on a repository of services that had descriptions of Web services corresponding to the following programs:

Table 16 Bioinformatics application--workflow query Service Query1 Input parameters Sequence, DatabaseName, OrganismType, SelectionOptions, MaxTargetSequences, ExpectedThreshold, UserTransversion Parsimony, UseThresholdParsimony, WordSize Output parameters NewickEvoIutionTree

1. BLAST: This service compares protein sequences to sequence databases and calculates the statistical significance of matches. It query's a public database of generic information like GenBank and GSDB and produces a molecular sequence. It takes in Sequence, DatabaseName, OrganismType, SelectionOptions, MaxTargetSequences, ExpectedThreshold, WordSize as input parameters and produces BlastSequenceSet as the output. 2. CLUSTAL: This service produces multiple sequence alignment for DNA or proteins. It takes in ClustalSequenceSet as input parameter and produces ClustalAlignedSequenceSet as the output. 3. PHYLIP: This service is used for inferring phylogenies. It analyzes molecular sequences and infers phylogenetic information. It takes in PhylipAlignedSequenceSet, UseThresholdParsimony, UseTransversionParsimony as input parameters and produces NewickEvolutionTree as the output. 4. PAUP: This service is used for inferring phylogentic trees. It analyzes molecular sequences and infers phylogenetic information. It takes in PaupAlignedSequenceSet as input parameter and produces NewickEvolutionTree as the output. 5. BLASTNexus: This service takes input in BLAST format and converts it into Nexus format. It takes in BLAST-

SequenceSet as input and produces NexusSequence Set. 6. NexusCLUSTAL: This service takes input in Nexus format and converts it into CLUSTAL format. It takes in NexusSequenceSet as input parameter and produces CLUSTALSequenceSet as the output. 7. CLUSTALNexus: This service takes input in CLUSTAL format and converts it into Nexus format. It takes in CLUSTALAlignedSequenceSet as input parameter and produces NexusAlignedSequenceSet as the output. 8. NexusPAUP: This service takes input in Nexus format and converts it into PAUP format. It takes in NexusAlignedSequenceSet as input parameter and produces PaupAlignedSequenceSet as the output. 9. NexusPHYLIP: This service takes input in Nexus format and converts it into PHYLIP format. It takes in NexusAlignedSequenceSet as input parameter and produces PhylipAlignedSequenceSet} as the output. 10. MEGA: This service is used for Molecular Evolutionary Genetics Analysis. It takes in MEGAInp as input parameter and produces MEGASequence as the output.

123

132 Fig. 17 Composition Solution 2 for query in Table 15

SOCA (2016) 10:111133

11. KEPLER: This service provides scientific workflows. It takes in KEPLERData as input parameter and produces KEPLERSequence as the output. The composition engine can automatically discover a complex workflow based on the query requirements, from a large repository without having to analyze all the programs manually. Figure 15 shows the non-sequential conditional composition for query specified in Table 16 as a directed cyclic graph. Figures 16 and 17 show the solutions obtained for the query specified in Table 16. A task that had to be performed manually by biologists whenever they had to make phylogenetic inferences can now be done automatically with Web services Composition. The programs have to be made available as Web services and their descriptions provided. Once we have a repository of such services, the composition engine can be used as shown above to automatically generate workflows. 7 Conclusions and future work Due to the growing number of services on the Web, we need automatic and dynamic Web service composition in order to utilize and reuse existing services effectively. It is also important that the composition solutions obtained can be trusted. Our semantics-based approach uses semantic description of Web services to find substitutable and composite services that best match the desired service. Given semantic description of Web services, our engine produces optimal results (based on number of services in the composition). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential, nonsequential or non-sequential conditional composition that is possible for a given query and also automatically generates OWL-S description of the composite service. This OWLS description can be used during the execution phase and subsequent searches for this composite service will yield a direct match. A trust rating is computed for every service in the repository based on the degree centrality of the service provider in a known social network. Currently, we are in the process of testing the trust-based dynamic Web service composition engine in a complete operational setting and running experiments to measure the quality of composition results obtained. We will also explore the other measures of centrality such as betweenness centrality and closeness centrality and analyze the possibility of using a combination of

all three measures of centrality to compute trust rating of a service provider. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. The strengths of this engine is the minimal query execution time that is achieved through pre-processing of repositories and incremental updates to the pre-processed data whenever a service is added, removed, or modified. Use of Constraint Logic Programming helped greatly in obtaining an efficient implementation of this system and made it easy to incorporate non-functional parameters for ranking of results. The limitations of the engine include trust aspect of the Web services involved in a composition solution. A model that provides a trust rating to services or service providers would improve confidence in the generated solutions. Also, when working with domains such as Bioinformatics where software systems involved in a workflow need to be converted into services, generation of semantics of the inputs and outputs of the Web services is a challenge. They have to be manually assigned semantics by a domain expert. Our future work includes investigating other kinds of compositions with loops such as repeat-until and iterations and their OWL-S description generation. Analyzing the choice of the composition language (e.g., BioPerl [49] for phylogenetic workflows) and exploring other language possibilities is also part of our future work. We are also exploring combining technologies of automated service composition and domain-specific languages to develop a framework for problem solving and software engineering.

References
1. Castagna G, Gesbert N, Padovani L (2008) A theory of contracts for web services. ACM SIGPLAN Not 43:261272 2. Bansal A, Patel K, Gupta G, Raghavachari B, Harris ED, Staves JC (2005) Towards intelligent services: a case study in chemical emergency response. In: IEEE International conference on web services (ICWS) 3. McIlraith SA, Son TC, Zeng H (2001) Semantic web services. IEEE Intell Syst 16(2):4653 4. Mandell DJ, McIlraith SA (2003) Adapting BPEL4WS for the semantic web: the bottom-up approach to web service interoperation. In: The semantic web-ISWC. Springer 2003, pp 227241 5. Paolucci M, Kawamura T, Payne TR, Sycara K (2002) Semantic matching of web services capabilities. In: The semantic Web ISWC. Springer 2002, pp 333347 6. Rao J, Dimitrov D, Hofmann P, Sadeh N (2006) A mixed initiative approach to semantic web service discovery and composition: SAP's guided procedures framework. In: International conference on web services. ICWS'06, 2006, pp 401410

123

SOCA (2016) 10:111133 7. Cardoso J, Sheth AP (2006) Semantic web services, processes and applications. Springer, Berlin 8. Edwards AWF, Cavalli-Sforza LL (1964) Reconstruction of evolutionary trees, systematics association publication number 6, No. Phenetic and Phylogenetic Classification, pp 6776 9. Freeman LC (1979) Centrality in social networks conceptual clarification. Social Netw 1(3):215239 10. Wasserman SF (1994) Social network analysis: methods and applications. Cambridge University Press, Cambridge 11. Tamura K, Peterson D, Peterson N, Stecher G, Nei M, Kumar S (2011) MEGA5: molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods. Mol Biol Evol 28(10):27312739 12. Thompson JD, Higgins DG, Gibson TJ (1994) CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice. Nucleic Acids Res 22(22):46734680 13. Edgar RC (2004) MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Res 32(5):1792 1797 14. Badr Y, Caplat G (2010) Software-as-a-service and versionology: towards innovative service differentiation. In: 24th IEEE international conference on advanced information networking and applications (AINA), 2010, pp 237243 15. Kona S, Bansal A, Blake MB, Gupta G (2008) Generalized semantics-based service composition. In: IEEE international conference on web services (ICWS), pp 219227 16. Rao J, Su X (2005) A survey of automated web service composition methods. In: Cardoso J, Sheth A (eds) Semantic web services and web process composition. Springer, Berlin, pp 4354 17. Srivastava B, Koehler J (2003) Web service composition-current solutions and open problems. In: ICAPS 2003 workshop on planning for web services, vol 35, pp 2835 18. McIlraith S, Son TC (2002) Adapting golog for composition of semantic web services. KR 2:482493 19. Pistore M, Roberti P, Traverso P (2005) Process-level composition of executable web services: on-the-fly versus once-for-all composition. In: Gomez-Perez A, Euzenat J (eds) The semantic web: research and applications. Springer, Berlin, pp 6277 20. Boustil A, Maamri R, Sahnoun Z (2013) A semantic selection approach for composite web services using OWL-DL and rules. Serv Oriented Comput Appl 8:118 21. Claro DB, Albers P, Hao JK (2005) Selecting web services for optimal composition. In ICWS international workshop on semantic and dynamic web processes, Orlando-USA 22. Suvee D, De Fraine B, Cibrn MA, Verheecke B, Joncheere N, Vanderperren W (2005) Evaluating FuseJ as a web service composition language. In: Third IEEE European conference on web services (ECOWS) 23. Dong W, Jiao L (2008) QoS-aware Web service composition based on SLA. In: Fourth international conference on natural computation (ICNC) vol 5, pp 247251 24. Yan J, Kowalczyk R, Lin J, Chhetri MB, Goh SK, Zhang J (2007) Autonomous service level agreement negotiation for service composition provision. Future Gener Comput Syst 23(6):748759 25. Wada H, Champrasert P, Suzuki J, Oba K (2008) Multiobjective optimization of SLA-aware service composition. In: IEEE congress on services-Part I, pp 368375 26. Blake MB (2007) Decomposing composition: service-oriented software engineers. IEEE Softw 24(6):6877 27. Zeng L, Benatallah B, Ngu AH, Dumas M, Kalagnanam J, Chang H (2004) QoS-aware middleware for web services composition. IEEE Trans Softw Eng 30(5):311327

133 28. Cardoso J, Sheth A, Miller J, Arnold J, Kochut K (2004) Quality of service for workflows and web service processes. Web Semant 1(3):281308 29. Zhao X, Shen LW, Peng X, Zhao W (2013) Finding preferred skyline solutions for SLA-constrained service composition. In: 2013 IEEE 20th international conference on web services (ICWS), pp 195202 30. Feng Y, Ngan LD, Kanagasabai R (2013) Dynamic service composition with service-dependent QoS attributes. In: 2013 IEEE 20th international conference on web services (ICWS), pp 1017 31. "OWL-S". [Online]. http://www.w3.org/Submission/OWL-S/. (Accessed: 22-Jan-2014) 32. Wen S, Tang C, Li Q, Chiu DK, Liu A, Han X (2014) Probabilistic top-K dominating services composition with uncertain QoS. Serv Oriented Comput Appl 8(1):91103 33. Immonen A, Pakkala D (2014) A survey of methods and approaches for reliable dynamic service compositions. Serv Oriented Comput Appl 8(2):129158 34. Mehdi M, Bouguila N, Bentahar J (2013) A QoS-based trust approach for service selection and composition via Bayesian networks. In: 2013 IEEE 20th international conference on web services (ICWS), pp 211218 35. Kuter U, Golbeck J (2009) Semantic web service composition in social environments. Springer, Berlin 36. Leavitt HJ (1951) Some effects of certain communication patterns on group performance. J Abnorm Soc Psychol 46(1):38 37. Bansal A, Kona S, Simon L, Mallya A, Gupta G, Hite TD (2005) A universal service-semantics description language. In: Third IEEE European conference on web services (ECOWS), pp 214225 38. Kona S, Bansal A, Simon L, Mallya A, Gupta G (2009) USDL: a service-semantics description language for automatic service discovery and composition. Int J Web Serv Res 6(1):2048 39. Sterling L, Shapiro EY, Warren DH (1986) The art of Prolog: advanced programming techniques. MIT Press, Cambridge 40. Marriott K, Stuckey PJ (1998) Programming with constraints: an introduction. MIT Press, Cambridge 41. "RDF/OWL Representation of WordNet". [Online]. http://www. w3.org/TR/wordnet-rdf/. (Accessed: 23-Jan-2014) 42. Blake MB, Cheung W, Jaeger MC, Wombacher A (2006) WSC-06: the web service challenge. In: The 3rd IEEE international conference on E-commerce technology, 2006. The 8th IEEE international conference on enterprise computing, E-commerce, and E-services, pp 6262 43. Blake MB, Cheung WKW, Jaeger MC, Wombacher A (2007) WSC-07: Evolving the web services challenge. In: The 9th IEEE international conference on E-commerce technology and the 4th IEEE international conference on enterprise computing, E-commerce, and E-services, 2007. CEC/EEE 2007, pp 505508 44. Kona S, Bansal A, Gupta G, Hite D (2007) Automatic composition of semantic web services. In: International conference on web services (ICWS), vol 7, pp 150158 45. Felsenstein J (2004) Inferring phylogenies, vol 2. Sinauer Associates, Sunderland 46. Iglesias JR, Gupta G, Pontelli E, Ranjan D, Milligan B (2001) Interoperability between bioinformatics tools: a logic programming approach. In: Practical aspects of declarative languages. Springer, Berlin, pp 153168 47. Maddison DR, Swofford DL, Maddison WP (1997) NEXUS: an extensible file format for systematic information. Syst Biol 46(4):590621 48. "myGrid". [Online]. http://www.mygrid.org.uk/ 49. "BioPerl". [Online]. http://www.bioperl.org/wiki/Main_Page

123

Annotating UDDI Registries to Support the Management of Composite Services
M. Brian Blake, Michael F. Nowlan, Ajay Bansal, and Srividya Kona
Department of Computer Science Georgetown University Washington, DC 202 687-3084

{mb7,mfn3,ab683,sk558}@georgetown.edu ABSTRACT
The future of service-centric environments suggests that organizations will dynamically discover and utilize web services for new business processes particularly those that span multiple organizations. However, as service-oriented architectures mature, it may be impractical for organizations to discover services and orchestrate new business processes on a daily, case-by-case basis. It is more likely that organizations will naturally aggregate themselves into groups of collaborating partners that routinely share services. In such cases, there is a requirement to maintain an organizational memory with regards to the capabilities offered by other enterprises and how they fit within relevant business processes. As a result, registries must maintain information about past business processes (i.e. relevant web services and their performance, availability, and reliability). This paper discusses and evaluates several hybrid approaches for incorporating business process information into standards-based service registries. Moreover, the syntactic and semantic metadata that accompanies these services [9][11] enable the discovery of these capabilities, on-demand. Discovery, in this environment, largely depends on the accessibility and capabilities of the repositories for which these services are stored. Universal Description, Discovery, and Integration (UDDI) is the leading specification for the development of service-based repositories or registries. UDDI registries of the future should facilitate fast search and discovery of relevant web services akin to the performance currently associated with resolving a domain name. Although, performance and federation are two important aspects of UDDI, an additional requirement for UDDI should be effective process-oriented storage and retrieval. Currently, the abilities to browse and discover independent services as characterized by their overarching business name and/or their capability name are important fundamental operations. However, as service-oriented architectures mature, composite services (i.e. capabilities based on the workflow composition of multiple atomic services) will also be important to persist and manage. UDDI currently has limited support for managing business processes [6]. Although not all federated service registries will need business process annotations, we suggest that a subset of registries frequently used by partnering organizations would benefit from maintaining historical process information. Currently, there are numerous languages and protocols that support the specification and execution [3][4][5][9]of composite web services. Unfortunately, techniques for incorporating the underlying process information into UDDI registries are limited. In this work, we address several questions as listed below.    What are the relevant descriptive attributes of composite web services that must be represented in web service registries? What are the relevant use cases for process-oriented service registries? What are the state-of-the-art methods for incorporating process information into registries and the corresponding challenges? What are the most efficient and effective approaches, both qualitatively and quantitatively, for process management of services in UDDI registries?

Categories and Subject Descriptors
D.3.3 [Programming Languages]: Language Contructs and Features  abstract data types, polymorphism, control structures.

General Terms
Performance, Design, Standardization Experimentation, Security, and

Keywords
Keywords are your own designated keywords.

1. INTRODUCTION
Service-oriented computing [10] promotes the development of modular domain-specific capabilities that can be advertised to and shared among collaborating organizations.



Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SAC'09, March 8-12, 2009, Honolulu, Hawaii, U.S.A. Copyright 2009 ACM 978-1-60558-166-8/09/03...$5.00.

This paper proceeds in the next section with a survey of related work and a discussion of the state of the practice. The next section formalizes a composite service by detailing the most relevant descriptive attributes. Next, we describe the requirements of a process-oriented registry. We next introduce

several hybrid approaches for adding process information to web service repositories. Finally, we describe a case study and experimental evaluation of both approaches.

2. RELATED WORK
Universal Description, Discovery and Integration (UDDI) describes the data model associated with a web-accessible registry for the storage and management of web services. Three core hierarchical objects specify the service provider (businessEntity), the web service (businessService), and information about how the service is binded (bindingTemplate). These foundational objects can be extended with the use of technical models or tModels. TModels facilitate the further description of businessEntities, businessServices, and bindingTemplates through classification based on metadata. Each tModel of a businessService represents a certain behavior or classification system that the service must implement. An example would be a web service that takes a state abbreviation as input. This web service would probably choose to reference the tModel that represents the US-State abbreviation classification system. A person looking at the service's bindingTemplate would then be able to see a reference to the USState System and know that a state should be entered with its abbreviation. UDDI also supports other structures called keyedReferences that allow previously mentioned core objects to be associated to tModels. KeyedReferences consist of a tModelKey, keyValue, and keyName. The tModelKey identifies the referenced tModel. The keyValue allows a categorization of the link between the core object, and the tModel and the keyName is a text string readable for humans. In UDDI v3, keyedReferences can be aggregated with keyedReferenceGroups. The strength of the tModels and keyedReferences is that further information about the main UDDI objects (i.e. businessEntities, businessServices, and bindingTemplates) are not populated in the repository. Tmodels merely point to webaccessible documents. This paradigm both reduces maintenance of the registry and promotes overall robustness. However, this paradigm also makes it difficult to associate web services that are stored in the registry, which is a necessary requirement for describing business processes within the registry. The most common research projects tend to address the problem of federating UDDI registries [1] [2][12], although, of most relevance to our work, are the projects that directly address the problem of business process annotations that associate services. Perhaps the leading approach to inserting business processes is the construction of a tModel classification system that mirrors a particular taxonomy of business processes. These tModels can then be used as pointers to the corresponding business process description documents. In industry, several OASIS technical reports [15] [17] describe high-level approaches to integrating tModel classifications with ebXML and BPEL4WS descriptions. Other research projects detail specialized domainspecific methods that leverage the same basic approach [6] [13] These are reasonable approaches, but all business descriptions are defined by external business process documents that are decoupled from the individual services. In fact, since services may be captured at individual locations, replacing services or even discontinuing the offering of a particular business process becomes difficult. In these cases, centralizing some vital part of the process information can be valuable. Luo et al. [8] and Srinivasan et al. [14] use semantic notations embedded in UDDI

tModels to associate services. This approach allows for more rich definitions, but the underlying limitations caused by distribution also apply. Other approaches look at the development of external, integrated software mechanisms that run parallel with the UDDI registry [18] [19]. These approaches tend to depart from the essence of the SOA paradigm as they promote proprietary, less standardized solutions. In this paper, we experiment with a combination of external process documentation and annotations that are embedded directly in the UDDI registry. Prerequisite to any solution, it is important to understand the required aspects of the web services-based business process

3. ANATOMY OF WEB SERVICES-BASED PROCESSES
Web-services based business processes also referred to as web service workflows are similar to traditional processes that are established between human stakeholders. Figure 1 illustrates the metamodel of a web services-based business process using a Unified Modeling Language (UML) class diagram. A difference is, as opposed to human-managed tasks or steps, web services enact the underlying steps. A web services-based business process, BP, contains user data endpoints, DE , defined below.
DataPart UserDataEndpoint -DataPart_ID -DPValue

<< has a>>

-UserDataEndpointID -InputID -OutputID * 1

<< has a>>
*

* 1

BusinessProcess -ProcessID -UserDataEndPointID -TransitionID -ServiceID -Description 1

1

Transition -TransitionID -InputID -OutputID -CFlowType -PreCondID -PostCondID

MessageContainer 1 * -MessageContainerID -DataPartID * QoS

1

*

Service * -ServiceID -InputID -OutputID -QosID -URI_ID

1 * 1 1

-QoSID -SLOType -SLOValue

<< has a>>
URI * -URI_ID -URIValue

Figure 1. Metamodel of a Web Service-Based Business Process. Definition (UserData EndPoint): The user data end point is defined as a pair DE = (ID, OD) where ID represents the input information of the business process provided by the user and OD represents the output information, ultimately generated by the completion of the business process. The business process also has a sequence of tasks (realization of the steps) that are implemented by a set of services,  = {S1, S2, S3, ....Sn}. Each service Si has its own input, ISi, and output, OSi, information; however the set of all input/output information of a service is less relevant than the subset of inputs and outputs that are relevant to the business process. In addition, a service can also be defined with its quality of service information and its URI location. Definition (Service): A service is a tuple of its inputs, outputs, QoS parameters, and its URI location. S = (IS , OS , QS , US) is the representation of a

service where IS is the input list, OS is the output list, QS is the list of quality of service parameters and US represents the URI location. Each step in the business process is defined by a transition, T, that defines the shared information between the output, OT, of the preceding step that connects to the input, IT , of the subsequent step. Definition (Transition): A transition is represented as a tuple of its inputs, outputs, flowtype, pre-conditions, and post-conditions. T = (IT , OT ,FT , CPre , CPost) is the representation of the transition where IT is the input list, OT is the output list, FT is the flow type, CPre represents the pre-conditions of the transition and CPost represent the postconditions of the transition. Ultimately, the business process can be formally defined as follows: Definition (BusinessProcess): A BusinessProcess is defined as a tuple BP = (DE, , ) where DE represents the user data endpoint,  is the set of services involved in the business process, and  is the set of transitions in the workflow .

previous section. Organizations should be able to generally access process information in addition to the service-specific details.

4.1 Potential Use Cases
There are several functions required by a registry that supports composite services as business processes. Figure 2 illustrates the functions of such a repository depicted as a UML use case diagram. The basic registry actors follow the SOA paradigm (i.e. service providers and consumers). Incorporating business process metadata into the registry also supports the interaction of intelligent software components or agents to autonomously maintain the integrity of the information. Service providers should be able to insert one or more services into the registry. Service consumers should be able to either browse or explicitly search the repository based on several attributes such as the name/type of business, service, or process. Although only available using specialized approaches, consumers may also want to search by service/process message names. We focus on three major features of such a repository.  Advertising a set of services aggregated as a process

DE = (ID, OD);  = {S1, S2, ..., Sn} where Si = (ISi , OSi , QSi , USi), for all i = 1 to n;  = {T1, T2, ...., Tn} where Ti = (ITi , OTi ,FTi , CPrei , CPosti), for all i = 1 to n. The following conditions should hold for a valid business process: For all Si  and Ti , the inputs of Si are 1. subsumed by the inputs of Ti , i.e., ISi C ITi For all Si  and Ti , the outputs of Si 2. subsumes the outputs of Ti , i.e., OTi C OSi For all Ti , Ti+1 , the post-conditions of Ti 3. imply the pre-conditions of Ti+1 , i.e., CPosti => CPre i+ 1 For all Ti , Ti+1 , the outputs of Ti along with 4. the user data inputs of the business process subsume the inputs of Ti+1 , i.e., (OTi U ID) C IT i+ 1 The inputs of the business process subsume the 5. inputs of the first transition, T1 (where T1  ), i.e., IT1 C ID The outputs of the business process are subsumed 6. by the outputs of the last transition Tn (where Tn ), i.e., OD C OTn
The cardinality of data endpoints, services, and transitions vary for each step, such that is necessary to develop containers to aggregate the information into sets. The notion of containers is central to business process languages, such as BPEL4WS and BPML [3][4], for aggregating information related to subprocesses.

When partnering organizations decide to share services, there may be a predefined understanding for orchestration. As such, these organizations must insert their relevant services into shared UDDI registries annotated by process-based information (i.e. the underlying control flow and data flow).  Discovering services associated with processes discovering processes associated with services and

Current UDDI registries facilitate browsing of services by business name and by service name. A process-oriented registry will also support browsing by process name or type. Consumers should also be able to find all services associated with a particular process.  Managing process information by software agents Once process information is annotated into a registry, intelligent agents can regularly check the health of the underlying services. Agents can look for indexing configurations that best support the storage of the process information. In addition, agents can record QoS information supplied by service consumers. This QoS information can represent individual services or the process as a whole.

4.2 Alternative Hybrid Approaches
By extending and leveraging the UDDI specification, we have identified several approaches for annotating business processes within service registries. Every service in a UDDI registry has a bindingTemplate structure, which stores references to tModels. TModels are "sources for determining compatibility of Web services and keyed namespace references" Error! Reference source not found.. This means that tModels identify how to interact with a web service by describing the technologies it implements. Although, the UDDI registry usually implements default tModels, such as the State Abbreviation System, it is also possible for an administrator of the registry to create tModels, ondemand. There are two approaches for using tModels to describe web services-based business processes.

4. BUSINESS PROCESS AND SERVICE REGISTRIES
In order for organizations to understand their business processes defined with web services, it is important that their process databases include relevant process information as defined in the

 Annotating business process information directly into the UDDI registry A new tModel is created for every business process that is identified in the registry. In addition, a parent tModel is created that simply classifies any tModel that annotates a business process as such. In this way, when a new process chain is added or identified in the registry, a tModel that points to the parent tModel is created to represent the process. Furthermore, the categoryBag element is used to store references to all the processes of which the service is a part. Figure 3 shows an example tModel. Notice that the keyName of the keyedReference contains the service name and additional control flow information. Also, the keyValue maintains the sequence number of the service in the process. Figure 4 demonstrates the actions taken by the registry to identify and store the existence of a composite web service.  Defining business process information using external markup documents

<keyedReference keyName="uddi:Process-Representing" keyValue="categorization" tModelKey="uddi:uddi.org:categorization:types"/> </categoryBag> </tModel> <businessService> <name>firstService</name> <categoryBag> <keyedReference tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14" keyName="SERV=firstService, TRAN = Sequence" keyValue="1"/> </categoryBag> </businessService> <businessService> <name>secondService</name> <categoryBag> <keyedReference tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14" keyName="SERV = secondService, TRAN = Sequence" keyValue="2"/> </categoryBag> </businessService> <businessService> <name>thirdService</name> <categoryBag> <keyedReference tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14" keyName="SERV=thirdService, TRAN = Sequence" keyValue="3"/> </categoryBag> </businessService>

Figure 5 details the steps for annotating a business process within the registry using the UDDI data structures. Perhaps the leading approach in related work is the use of an external document (e.g. BPEL4WS and ebXML) to store the process information. This method involves simply adding an entry to each of the process documents associated with the relevant services. The document could exist centrally on a main server, or locally with each service provider.
<tModel tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"> <description>This tModel represents the process starting with firstService and ending with thirdService. </description> <categoryBag>
Advertise 1 or More Services

Figure 3. Sample Process-Oriented TModel.

Manage and Update Repository <<includes>>

Service Provider

첿xtends

Repository Agent

Associate Services with Processes

Remove Nonfunctional Services and Record QoS Values

Maintain Indexes in Repository

Query by Service Name or Type Search Repository 쳃nherits Query by Process 쳃nherits <<includes>> Query by Business Name or Type 쳃nherits Filter and Characterize Query <<includes>>

Service Consumer

쳃nherits Query by Message/Part Names Browse Repository

Figure 2. Process-Oriented Service Repository Use Cases (** Shaded areas are not currently well supported in the state-of-the-art)
Annotate( VS): VS GSFIRST GSLAST TM Annotating Function Vector of serviceKeys for services in chain The first service in the vector The last service in the vector The newly created process-tModel TMKEY PM CB S KR The newly created tModel's key The tModel Process-Classification System The CategoryBag of an individual structure An individual service KeyedReference  A pointer to a tModel

A(VS, TM) Function to add a KR to CB of Services MakeTM(GSFIRST , GSLAST ) Function to make a new TM A(VS , TM) KR = TMKEY forAll S in VS S.CB += KR return VS MakeTM(GSFIRST , GSLAST ) TM.CB += PM TM.description = GSFIRST , GSLAST return TM Annotate(VS) TM = MakeTM(GSFIRST , GSLAST ) return A(VS , TM)

 Service Deletions or Changes when Using External Process Documentation External process documents contain one entry for each process. Irrespective of the process document notations, each entry will define the serviceKeys of the services in the particular process. In this approach, these serviceKeys are used to find all services that share a process with the deleted service. The registry then edits the relevant XML documents by removing the entry that corresponds to the broken process.  Query for Processes by Service Name using UDDI Data Structure Method The last action that can be taken on a registry, and perhaps the most desired ability, is the ability to aggregate the information and list of services for every process in which a particular service plays a role. The serviceKey is used to get the service's categoryBag. The categoryBag contains references to all the tModels, that represent all relevant processes. The registry is then queried to find all services which point to any one of the tModels in their categoryBags. The chain of services is then sorted by the tModels and is returned as process.  Query for Processes by Service Name using External Process Documentation The external document method is quicker because it does not require querying the registry. The XML document for a service is retrieved. In this document is a list of entries, each pertaining to one composite web service. Each entry contains the serviceKey for each service in that particular chain. These services can be retrieved with a single call to the registry by compiling a vector of the serviceKeys. The retrieved service information is then sorted by the order of the entries in the initial service's process document and displayed.

Figure 4. Annotating a Business Process of Web Services within UDDI Data Model.
Annotate(VS): Annotating Function MakeEntry(VS ) Function to make a new TM A(VS , E) Function to add entry to XML Doc. VS Vector of serviceKeys for services in chain E An entry containing a list of services S An individual service S.KEY Service key D A service's description element FS A service's XML document file A(VS , E) forAll S in VS S. FS += E return VS MakeEntry(VS) forAll S in VS E += S.KEY return E Annotate(VS) forAll S in VS S.D = FS E = MakeEntry(VS ) return A(VS , E)

Figure 5. Leveraging External Process Documents

5. CASE STUDY: A GENERAL UDDI BUSINESS EXPLORER
Once process information is associated with or incorporated into a UDDI registry, new graphical user interfaces can be created to support enhanced service discovery and manipulation. As a part of this work, we designed and experimented with a new user interface front-end (i.e. UDDI-P) to the jUDDI registry Error! Reference source not found.. A screenshot of the design of the interface is shown in Figure 6. Using this new interface, a user has the capability of browsing known process names as shown on the left side of Figure 6. Once a process is identified, the consumer can further decide to analyze the process to see if it meets the organization need. The interface can display parameters such as estimated delivery date and price range based on service level objectives associated with the process stored in the registry. By embedding process information into the registry, classification of processes can occur to the point where they themselves function as individual services. On the right side of the interface, the user has the ability to browse each process meeting the search criteria by price and delivery. This sort of interface would enable a separate interface that allows the services to be executed.

4.3 Alternative Hybrid Approaches
Both of the hybrid approaches also have other functions relevant to a business-oriented registry.  Service Deletions or Changes when using UDDI Structure Method As the registry is updated when a process is identified, it will also change when a service is removed or replaced. These actions cause the process workflow chain to be incomplete. In this case, all the services in the chain are affected and they must modify their process annotation method (either XML document or CategoryBag) by removing the process that is broken. When a service is deleted, all the tModels that it points that represent composite web services (excluding simple classifications represented in the bindingTemplate) must be deleted. Any other services in the registry that reference these tModels must remove the reference from their categoryBags. In the future, registries may be able to search for replacements services as opposed to deleting the process.

UDDI-P Directions:

Service Request

Enter the necessary data on the left and click Submit. Offers matching your criteria will be displayed in the window on the right.

An alternative approach is incorporating specific business process information directly into the registry. We experimented to characterize the performance of these approaches on our prototype repository. The performance is illustrated in Table 1 and Figure 7 based on the use cases illustrated in Figure 2. In general, the difference in performance between adding individual services, adding a chain, and annotating services with business process annotations were only negligibly different between the two approaches. However, deleting a service was approximately three times faster when external business process documents were used. Another variation in performance is associated with retrieving all service IDs associated with a particular process (or aggregating the services). The aggregation time increased linearly with the increase in size of the UDDI registry embedded process information. The main reason for disparity between the approaches is due to the fact that the latter approach requires a significant query within the repository. This approach must retrieve the categoryBag for all services in the repository and search those structures for a particular keyedReference. The external business process document does not require this step since the process information for a service is stored in one location: the XML-based document. The retrieval of this document is much faster when compared to the query that must take place within the registry. Although the performance for an external file is more favorable, having external BPEL4WS files causes the duplication of process information (i.e. the same process entry could appear in multiple files that may be attached to the underlying services). Depending on the management of the BPEL4WS files, centralizing the process within the repository may be more advantageous. In such cases, embedding processes with the registry represents an effective solution.

Specifications
* = required

Results
(Updated when Submit is pressed)

* Application
Name:

[Process]

Viewing:
1 of 5

Sort By:
Price

* Delivery Date
Date:

Business Chain
dd yyyy

[Month]

FindBookByISBN SubmitPaymentInfo ShipProduct

Consultation
Need Expert Help? No, thanks.

Price Range
From: $0 To:

$100

Ready by: mm/dd/yyyy Price: $50.00

Reset

Submit Prev.

Choose
* Purchase on next page

Next

Figure 6. UDDI-P: A Prototype User Interface for Process-Based Service Management.

6. EXPERIMENTATION & EVALUATION
In previous sections, two distinct hybrid approaches for aggregating UDDI services into business processes were introduced. The leading approaches in published works suggest using external mark-up documents to describe business processes.

Table 1. Performance of External Document and Annotated UDDI Repository (milliseconds) Collect Service IDs by Process (Aggregate) (ms)
1500 1500 1500 1500 2600 4000 5700 7450

Repo Size
Ext. Process Doc 150 330 600 850 150 330 600 850

Add Serv. (ms)
1573 1573 1573 1573 1573 1573 1573 1573

Add Composite (ms)
2500 2700 2600 2600 2800 3000 3000 3000

Annotate (Note) (ms)
1500 1700 1600 1600 2100 2100 2100 2100

Del. Serv. (ms)
1900 1900 1900 1900 7790 7790 7790 7790

Internal UDDI Data Structure

9000 8000 7000 6000 5000 4000 3000 2000 1000 0

Add Service Add Composite Note Aggregate Delete Service

Repo=150

Repo=330

Repo=600

Repo=850

Repo=150

Repo=330

Repo=600

External XML Document

Annotated UDDI

Figure 7. Comparison of Storing Process Information in External Document versus Annotating the UDDI Repository.

7. CONCLUSIONS
An innovation in this paper is the formalized model for web services-based business process and the relevant use cases for using this information. In addition, we introduce the design of a new interface for business-based UDDI interactions. Our experimentation evaluates the two leading approaches for capturing process information in UDDI registries. Overall performance information does not suggest a quantitative advantage for embedding process information directly into the repository. However, qualitatively, maintenance is less extensive since process information is centralized in a potentially federated registry. As future work, we plan to continue developing a process-oriented UDDI explorer and experiment on new approaches for interface design.

[4] BPML (2008): http://www.ebpml.org/bpml.htm (currently moved to OMG) [5] BPMN (2008): http://www.bpmn.org/ [6] Dogac, A., Tambag, Y., Pembecioglu, P, Pektas, S., Laleci, G., Gokhan, K., Toprak, S., and Kabak, Y. "An ebXML infrastructure implementation through UDDI registries and RosettaNet PIPs" Proceedings of the 2002 ACM SIGMOD Conference (SIGMOD 2002), Madison, Wisconsin, June 2002 [7] jUDDI (2008): http://ws.apache.org/juddi/ [8] Luo, J., Montrose, B., Kim, A., Khashnobish, A., Kang, M. "Adding OWL-S Support to. the Existing UDDI Infrastructure" Proceedings of the 4th International Conference on Web Services (ICWS2006), Chicago, Ill, November 2006. [9] OWL-S(2008): http://www.daml.org/services/owl-s/ [10] Papazoglou, M. "Service-oriented computing: Concepts, characteristics and directions. In Proc. of WISE `03 [11] RDF (2008): http://www.w3.org/RDF/ [12] Sivashanmugam, K., Verma, K., and Sheth, A. Discovery of Web Services in a Federated Registry Environment, Proceedings of 4th IEEE International Conference on Web Services (ICWS), pp. 270-278, 2004. [13] Spies, M., Schoning, H., and Swenson, K. "Publishing Interoperable Services and Processes in UDDI" The 11th Enterprise Computing Conference (EDOC 2007), Annapolis, MD, October 2007 [14] Srinivasan, N., Paolucci, M. and Sycara, K. "Adding OWL-S to UDDI, implementation and throughput," Proceedings of First International Workshop on Semantic Web Services and Web Process Composition (SWSWPC 2004), San Diego, California, USA, 2004 [15] UDDI as the registry for ebXML Components, OASIS Technical Note, February 2004, Accessed (2008): http://www.oasis-open.org/committees/uddispec/doc/tn/uddi-spec-tc-tn-uddi-ebxml-20040219.htm [16] Universal Description, Discovery, and Integration (UDDI) (2008): http://www.uddi.org/pubs/uddi_v3.htm [17] Using BPEL4WS in UDDI Registry, OASIS Technical Note, July 2005 Accessed (2008): http://www.oasis-

8. ACKNOWLEDGMENTS
We acknowledge fruitful conversations with Brian Schott and Robert Graybill of the University of Southern California, ISI-East and Suzy Tichenor of the Council of Competitiveness. This material is based on research sponsored by DARPA under agreement number FA8750-06-1-0240. This U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.

9. REFERENCES
[1] Al-Masri, E. and Mahmoud, Q.H., "Crawling Multiple UDDI Business Registries", Proceedings of the 16th International Conference on the World Wide Web, Banff, Alberta, Canada, 2007 [2] Blake, M.B., Sliva, A.L., zur Muehlen, M., and Nickerson, J. "Binding Now or Binding Later: The Performance of UDDI Registries", IEEE Hawaii International Conference of System Sciences (HICSS-2007), Track on Technology and Strategies for Realizing Service-oriented Architectures with Web services, January 2007 [3] WS-BPEL(2008 ): http://www.ibm.com/developerworks/library/specification/w s-bpel/

Repo=850

open.org/committees/uddi-spec/doc/tn/uddi-spec-tc-tn-bpel20040725.htm [18] Zhang, L-J., Zhou, Q., and Chao, T., "A Dynamic Services Discovery Framework for Traversing Web Services Representation Chain", In Proceedings of the International Conference on Web Services (ICWS 2004), 2004

[19] Zhang, M., Cheng, Z., Zhao, Y. Huang, J.Z. Yinsheng, L., Zang, B. "ADDI: an agent-based extension to UDDI for supply chain management" Proceedings of the Ninth Int Conference on CSCW in Design, Shanghai, China, May 2005

Automatic Composition of Semantic Web Services
Srividya Kona, Ajay Bansal, Gopal Gupta Department of Computer Science The University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp. 2400 Dallas Parkway Plano, TX 75093

Abstract
Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. For this automation to be effective, formal semantic descriptions of Web services should be available. In this paper we formally define the Web service discovery and composition problem and present an approach for automatic service discovery and composition based on semantic description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are its scalability, i.e., its ability to handle very large service repositories, and its extremely efficient processing times for discovery and composition queries. We evaluate our engine for automated discovery and composition on repositories of different sizes and present the results.

1

Introduction

A Web service is a program accessible over the web that may effect some action or change in the world (i.e., causes a side-effect). Examples of such side-effects include a webbase being updated because of a plane reservation made over the Internet, a device being controlled, etc. An important future milestone in the Web's evolution is making services ubiquitously available. As automation increases, these Web services will be accessed directly by the applications rather than by humans [8]. In this context, a Web service can be regarded as a "programmatic interface" that makes application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize and compose services automatically is needed in order to make Web services more practical. To make services ubiquitously available we need a

semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis [3]. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. For discovery, composition, etc., one could take the syntactic approach in which the services being sought in response to a query simply have their inputs syntactically match those of the query, or, alternatively, one could take the semantic approach in which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the matching process. Several efforts are underway to build an infrastructure [17, 23, 15] for service discovery, composition, etc. These efforts include approaches based on the semantic web (such as USDL [1], OWL-S [4], WSML [5], WSDL-S [6]) as well as those based on XML, such as Web Services Description Language (WSDL [7]). Approaches such as WSDL are purely syntactic in nature, that is, they only address the syntactical aspects of a Web service [14]. Given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be automatically determined. This task is called discovery. If the service is not found, the directory can be searched for two or more services that can be composed to synthesize the required service. This task is called composition. In this paper we present an approach for automatic discovery and composition of Web services using their semantic descriptions. Our research makes the following novel contributions: (i) We formally define the discovery and composition problems; to the best of our knowledge, the formal description of the generalized composition problem has been given for the first time; (ii) We present efficient and scalable algorithms for solving the discovery and composition problem that take semantics of services into account; our algorithm automatically selects the individual services involved in composition for a given query, without the need for manual intervention;

and, (iii) we present a prototype implementation based on constraint logic programming that works efficiently on large repositories. The rest of the paper is organized as follows. Section 2 describes the two major Web services tasks, namely, discovery and composition with their formal definitions. In section 3 and 4, we present our multi-step narrowing solution and implementation for automatic service discovery and composition. Finally we present our performance results, related work and conclusions.

Q

CI',I'

CI,I

CO,O S

CO',O'

where CI' ==> CI, I' I,

CO ==> CO', O O'

Figure 1. Substitutable Service Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and   ) is the reprepost-conditions.  = (   sentation of a service where  is the pre-conditions,  is the input list, is the service's side-effect,  is the affected object,  is the output list, and  is the postconditions. Definition (Repository of Services): Repository is a set of Web services. Definition (Query): The query service is defined as  = (      퓬 퓬 퓬 ) where   is the preconditions,   is the input list,  is the service affect, 퓬 is the affected object, 퓬 is the output list, and 퓬 is the post-conditions. These are all the parameters of the requested service. Definition (Discovery): Given a repository  and a query , the Discovery problem can be defined as automatically finding a set  of services from  such that  =        풩, s  ,     ,    ,  ,  퓬 ,   퓬 ,   퓬 . The meaning of  is the subsumption (subsumes) relation and  is the implication relation. For example, say  and  are input and output parameters respectively of a service. If a query has 단  as a pre-condition and 닫  昞 as postcondition, then a service with pre-condition 단 섧 and post-condition 닫 昞 can satisfy the query as 단 도 단 섧 and 닫 昞  닫  昞 since 단 섧. In other words, the discovery problem involves finding suitable services from the repository that match the query requirements. Valid solutions have to produce at least those output parameters specified in the query, satisfy the query pre and post-conditions, use at most those input parameters that are provided by the query, and produce the same side-effect as the query requirement. Figure 1 explains the discovery problem pictorially.

2

Automated Web service Discovery and Composition

Discovery and Composition are two important tasks related to Web services. In this section we formally describe these tasks. We also develop the requirements of an ideal Discovery/Composition engine.

2.1

The Discovery Problem

Given a repository of Web services, and a query requesting a service (we refer to it as the query service in the rest of the text), automatically finding a service from the repository that matches the query requirements is the Web service Discovery problem. Valid solutions to the query satisfy the following conditions: (i) they produce at least the query output parameters and satisfy the query post-conditions; (ii) they use only from the provided input parameters and satisfy the query pre-conditions; (iii) they produce the query side-effects. Some of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre/post conditions, and side-effects requirements. Example 1: Say we are looking for a service to buy a book and the directory of services contains services 降 and 鱇 . The table 1 shows the input/output parameters of the query and services 降 and 鱇. In this example service 鱇 satisfies the query, but 降 does not as it requires BookISBN as an input but that is not provided by the query. Our query requires ConfirmationNumber as the output and 鱇 produces ConfirmationNumber and TrackingNumber. The extra output produced can be ignored. Also the semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. The discovery engine should be able to infer that the query parameter BookTitle and input parameter BookName of service 鱇 are semantically the same concepts. This can be inferred using semantics from the ontology provided. The query also has a pre-condition that the CreditCardNumber is numeric which should logically imply the pre-condition of the discovered service.

2.2

The Composition Problem

Given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 2 shows an example composite service made up of

Service Query

降 鱇

Input Parameters BookTitle,CreditCardNumber, AuthorName,CreditCardType BookName,AuthorName BookISBN,CreditCardNumber BookName, CreditCardNumber

Pre-conditions IsNumeric(Credit CardNumber)

Output Parameters Post-Cond ConfirmationNumber ConfirmationNumber

IsNumeric(Credit CardNumber)

ConfirmationNumber, TrackingNumber

Table 1. Example Scenario for Discovery problem graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph:    where  has zero incoming edges, 1. 좁   , 좁   . 2.    where  has zero outgoing edges, 퓬   , 퓬   .    where  has at least one incoming edge, 3. let  ,   , ...,   be the nodes such that there is a directed edge from each of these nodes to  . Then    ,   (        ).   The meaning of the  is the subsumption (subsumes) relation and  is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. Figure 4 explains one instance of the composition problem pictorially. When the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem (i.e., the graph is a linear chain of services).

S2 S5 CI',I' S1 S3 S4 CO',O'

Figure 2. Example of a Composite Service as a Directed Acyclic Graph

 

GetAvailability BookISBN BookName, AuthorName GetISBN NumAvailable ConfNumber PurchaseBook

BookISBN



AuthCode CreditCardNum AuthorizeCreditCard

Figure 3. Example of a Composite Service five services 降 to  . In the figure,   and   are the query input parameters and pre-conditions respectively. 퓬 and   are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes  and  indicates that outputs of  constitute (some of) the inputs of  . Example 2: Suppose we are looking for a service to buy a book and the directory of services contains services GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. The table 2 shows the input/output parameters of the query and these services. Suppose the repository does not find a single service that matches these criteria, then it synthesizes a composite service from among the set of services available in the repository. Figure 3 shows this composite service. The postconditions of the service GetAvailability should logically imply the pre-conditions of service PurchaseBook. Definition (Composition): The Composition problem can be defined as automatically finding a directed acyclic graph = 늴  of services from repository , given query  = (      퓬 퓬 퓬 ), where  is the set of vertices and is the set of edges of the graph. Each vertex in the

2.3

Requirements of an ideal Engine

Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition where the number of services involved in composition is exactly equal to one. The features of an ideal Discovery/Composition engine are: Correctness: One of the most important requirements for an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the

Service Query

Input Parameters

Pre-Conditions

BookTitle,CreditCardNum, AuthorName,CardType GetISBN BookName,AuthorName GetAvailability BookISBN AuthorizeCreditCard CreditCardNum PurchaseBook BookISBN, NumAvailable, AuthCode NumAvailable


Output Params ConfNumber

Post-Conditions

BookISBN NumAvailable NumAvailable  AuthCode AuthCode AuthCode 슨세 ConfNumber

Table 2. Example Scenario for Composition problem semantics-based Discovery and Composition engine described in the following sections.

2.4

Semantic Description of Web Services

Figure 4. Composite Service requirements of the query. Also, the engine should be able to find all services that satisfy the query requirements. Small Query Execution Time: Querying a repository of services for a requested service should take a reasonable amount of (small) time, i.e., a few milliseconds. Here we assume that the repository of services may be pre-processed (indexing, change in format, etc.) and is ready for querying. In case services are not added incrementally, then time for pre-processing a service repository is a one-time effort that takes considerable amount of time, but gets amortized over a large number of queries. Incremental Updates: Adding or updating a service to an existing repository of services should take a small amount of time. A good Discovery/Composition engine should not pre-process the entire repository again, rather incrementally update the pre-processed data (indexes, etc.) of the repository for this new service added. Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition engine should be able to give results based on requirements (minimize, maximize, etc.) over the costs. We can extend this to services having an attribute vector associated with them and the engine should be able to give results based on maximizing or minimizing functions over this attribute vector. These requirements have driven the design of our

A Web service is a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface that is described in a machineprocessible format so that other systems can interact with the Web service through its interface using messages. The automation of Web service tasks (discovery, composition, etc.) can take place effectively only if formal semantic descriptions of Web services are available. Currently, there are a number of approaches for describing the semantics of Web services such as OWL-S [4], WSML [5], WSDL-S [6], and USDL [1].

3

A Multi-step Narrowing Solution

With the formal definition of the Discovery and Composition problem, presented in the previous section, one can see that there can be many approaches to solving the problem. Our approach is based on a multi-step narrowing of the list of candidate services using various constraints at each step. In this section we discuss our Composition algorithm in detail. As mentioned earlier, discovery is a simple case of Composition. When the number of services involved in the composition is exactly equal to one, the problem reduces to a discovery problem. Hence we use the same engine for both discovery and composition. We assume that a directory of services has already been compiled, and that this directory includes semantic descriptions for each service.

3.1

The Service Composition Algorithm

For service composition, the first step is finding the set of composable services. Using the discovery engine, individual services that make up the composed service can be selected. Part substitution techniques [2] can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the

individual services. That is, if a subservice 降 is composed with subservice 鱇 , then the post-conditions of 降 must imply the pre-conditions of 鱇. The goal is to derive a single solution, which is a directed acyclic graph of services that can be composed together to produce the requested service in the query. Figure 6 shows a pictorial representation of our composition engine. In order to produce the composite service which is the graph, as shown in the example figure 2, we filter out services that are not useful for the composition at multiple stages. Figure 5 shows the filtering technique for the particular instance shown in figure 2. The composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. In figure 5,   are the preconditions and the input parameters provided by the query. 降 and 鱇 are the services found after step 1. 퓰 is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e., 종 = 퓰  . 종 is used to find services at the next stage, i.e., all those services that require a subset of 종. In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

S1 Query described using USDL (S) Infer Sub-queries . . . Sn

Discovery Module (Discovery Engine + Service Directory + Term Generator) S1 Sn

.......................... Composition Engine (implemented using Constraint Logic Programming

Composed Service

Pre-Cond(S) S1 Pre-Cond( S) 1

Post-Cond( S) 1 Pre-Cond( S) 2

S2

................................. S n

Post-Cond( S) n Post-Cond(S)

Figure 6. Composition Engine

4

Implementation

I=I CI, I
1

S1 S2 . .

O1

I=IUO
2 1

1

S . .

O
3

2

I=IUO
3 2

2

O S . .
4

3

I=IUO
4 3

3

S . .

O
5

4

O

Figure 5. Composite Service Algorithm: Composition Input: QI - QueryInputs, QO - QueryOutputs, QCI - PreCond, QCO - Post-Cond Output: Result - ListOfServices 1. L NarrowServiceList(QI, QCI); 2. O GetAllOutputParameters(L); 3. CO GetAllPostConditions(L); 4. While Not (O  QO) 5. I = QI O; CI QCI CO; 6. L' NarrowServiceList(I, CI); 7. End While; 8. Result RemoveRedundantServices(QO, QCO); 9.Return Result;

Our discovery and composition engine is implemented using Prolog [11] with Constraint Logic Programming over finite domain [10], referred to as CLP(FD) hereafter. In our current implementation, we used semantic descriptions written in the language called USDL [1]. The repository of services contains one USDL description document for each service. USDL itself is used to specify the requirements of the service that an application developer is seeking. USDL is a language that service developers can use to specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. USDL uses WordNet [9] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the "meaning" of input parameters, outputs, and the side-effect induced by the service is given by mapping these syntactic terms to concepts in WordNet (see [1] for details of the representation). Inclusion of USDL descriptions, thus makes services directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory for a "matching" service. Due to lack of space, we do not go into the details of the language in this paper. They are available in our previous work [2]. These algorithms can be used with any other Semantic Web service description language as well. It will involve extending our implementation to work for other description formats, and we are looking into that as part of our future work. The software system is made up of the following components. Triple Generator: The triple generator module converts

each service description into a triple. In this case, USDL descriptions are converted to triples like: (Pre-Conditions, affect-type(affected-object, I, O), PostConditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [2]. In case conditions on a service are not provided, the Pre-Conditions and Post-Conditions in the triple will be null. Similarly if the affect-type is not available, this module assigns a generic affect to the service. Query Reader: This module reads the query file and passes it on to the Triple Generator. We use USDL itself as the query language. A USDL description of the desired service can be written, which is read by the query reader and converted to a triple. This module can be easily extended to read descriptions written in other languages. Semantic Relations Generator: We obtain the semantic relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms, meronyms, holonyms and many more. USDL descriptions point to OWL WordNet for the meanings of concepts. A theory of service substitution is described in detail in [2] which uses the semantic relations between basic concepts of WordNet, to derive the semantic relations between services. This module extracts all the semantic relations and creates a list of Prolog facts. We can also use any other domainspecific ontology to obtain semantic relations of concepts. We are currently looking into making the parser in this module more generic to handle any other ontology written in OWL. The query is parsed and converted into a Prolog query that looks as follows: discovery(sol(queryService, ListOfSolutionServices). The engine will try to find a list of SolutionServices that match the queryService. Composition Engine: The composition engine is written using Prolog with CLP(FD) library. It uses a repository of facts, which contains all the services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs), encodeParam(QueryOutputs, QO), getExtInpList(QueryInputs, InpList), encodeParam(InpList, QI), performForwardTask(QI, QO, LF),

performBackwardTask(LF, QO, LR), getMinSolution(LR, QI, QO, A), reverse(A, RevA), confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The query is converted into a Prolog query that looks as follows: composition(queryService, ListOfServices). The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the built-in, higher order predicate 'bagof' to return all possible ListOfServices that can be composed to get the requested queryService. Output Generator: After the Composition engine finds a matching service, or the list of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files in any desired XML format.

5

Efficiency and Scalability Issues

In this section we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It is because of these features, we decided on the multi-step narrowing based approach to solve these problems and implemented it using constraint logic programming. Correctness: Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions. Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always find a correct solution (if one exists) in the minimum possible steps. The formal proof of correctness and minimality is beyond the scope of this paper. Pre-processing: Our system initially pre-processes the repository and converts all service descriptions into Prolog terms. The semantic relations are also processed and loaded as Prolog terms in memory. Once the pre-processing is done, then discovery or composition queries are run against all these Prolog terms and hence we obtain results quickly and efficiently. The built-in indexing scheme and constraints in CLP(FD) facilitate the fast execution of queries. During the pre-processing phase, we use the term representations of services to set up constraints on services and the individual input and output parameters. This further helped us in getting optimal results. Execution Efficiency: The use of CLP(FD) helped significantly in rapidly obtaining answers to the discovery and composition queries. We tabulated processing times for different size repositories and the results are shown in Section 6. As one can see, after pre-processing the repository, our system is quite efficient in processing the query. The query execution time is insignificant.

Programming Efficiency: The use of Constraint Logic Programming helped us in coming up with a simple and elegant code. We used a number of built-in features such as indexing, set operations, and constraints and hence did not have to spend time coding these ourselves. This made our approach efficient in terms of programming time as well. Not only the whole system is about 200 lines of code, but we also managed to develop it in less than 2 weeks. Scalability: Our system allows for incremental updates on the repository, i.e., once the pre-processing of a repository is done, adding a new service or updating an existing one will not need re-execution of the entire pre-processing phase. Instead we can easily update the existing list of CLP(FD) terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will be negligible, perhaps a few milliseconds. With real-world services, it is likely that new services will get added often or updates might be made on existing services. In such a case, avoiding repeated pre-processing of the entire repository will definitely be needed and incremental update will be of great practical use. The efficiency of the incremental update operation makes our system highly scalable. Use of external Database: In case the repository grows extremely large in size, then saving off results from the preprocessing phase into some external database might be useful. This is part of our future work. With extremely large repositories, holding all the results of pre-processing in the main memory may not be feasible. In such a case we can query a database where all the information is stored. Applying incremental updates to the database is easily possible thus avoiding recomputation of pre-processed data . Searching for Optimal Solution: If there are any properties with respect to which the solutions can be ranked, then setting up global constraints to get the optimal solution is relatively easy with the constraint based approach. For example, if each service has an associated cost, then the discovery and the composition problem can be redefined to find the solutions with the minimal cost. Our system can be easily extended to take these global constraints into account.

Repository Size (num of services) 2000 2000 2000 2500 2500 2500 3000 3000 3000

Number of I/O parameters 4-8 16-20 32-36 4-8 16-20 32-36 4-8 16-20 32-36

PreProcessing Time (secs) 36.5 45.8 57.8 47.7 58.7 71.6 56.8 77.1 88.2

Query Exec Time (msecs) 1 1 2 1 1 2 1 1 3

Incremental update (msecs) 18 23 28 19 23 29 19 26 29

Table 3. Performance on Discovery Queries

found is that the repository was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 3 shows performance results of our Composition algorithm on discovery queries and table 4 shows results of our algorithm on composition queries. The times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than or equal to the wall clock time. The results are plotted in figure 8. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which is negligible (just 1 to 3 msecs) even for complex queries with large repositories.

6

Performance

We used repositories from WS-Challenge website[13], slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema file. We evaluated our approach on different size repositories and tabulated Preprocessing and Query Execution time. We noticed that there was a significant difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we

Figure 7. Performance on Discovery Queries

Figure 8. Queries Repository Size (num of services) 2000 2000 2000 3000 3000 3000 4000 4000 4000 Table 4. Queries

Performance

on

Composition

Number of I/O parameters 4-8 16-20 32-36 4-8 16-20 32-36 4-8 16-20 32-36

PreProcessing Time (secs) 36.1 47.1 60.2 58.4 60.1 102.1 71.2 87.9 129.2

Query Exec Time (msecs) 1 1 1 1 1 1 1 1 1 on

Incremental update (msecs) 18 23 30 19 20 34 18 22 32

Performance

Composition

7

Related Work

Composition of Web services has been active area of research recently [14, 15, 23, 18]. Most of these approaches are based on capturing the formal semantics of the service using an action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available [17]. To the best of our knowledge, most of these approaches that use planning are restricted to sequential compositions (i.e.,

a linear chain of services), rather than a directed acyclic graph. Our approach automatically selects atomic services from a repository and produces the composition flow in the form of a directed acyclic graph. The authors in [19, 20] present a composition technique by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also relies on a user-defined plan template which is created manually. There are industry solutions based on WSDL and BPEL4WS where the composition of the flow is obtained manually. A comparison of the approaches based on AI planning techniques and approach based on BPEL4WS is presented in [17]. This work shows that in both these approaches, the flow of the composition is determined manually. They do not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when an explicit flow is provided. In contrast, we have shown a technique to automatically determine these complex flows using semantic descriptions of atomic services. A process-level composition solution based on OWL-S is proposed in [21]. In this work the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but instead assume that they already have the list of atomic services. In contrast, we automatically find the services that are suitable for composition based on the query requirements for the new composed service. In [22], a semi-automatic composition technique is presented in which atomic services are selected for each stage of composition. This selection process involves decision making by a human controller at each stage, i.e., the selection process requires some manual intervention. Another related area of research involves message conversation constraints, also known as behavioral signatures [16]. Behavior signature models do not stray far from the explicit description of the lexical form of messages, they expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while our approach deals with higher-level real world concepts. However, both these approaches can be regarded as complementary concepts when taken in the context of real world service composition, and both technologies are currently being used in the development of a commercial services integration tool [24]. Our most important, novel contribution in this paper is

our technique for automatically selecting the services that are suitable for obtaining a composite service, based on the user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use human input on what services would be suitable for composition. Our technique also handles non-sequential compositions (i.e., composition where there can be more than one service involved at any stage, represented as a directed acyclic graph of services) rather than sequential composition (i.e, a linear chain of services) which is the case with most of the existing approaches.

8

Conclusions and Future Work

To catalogue, search and compose Web services in a semi-automatic to fully-automatic manner we need infrastructure to publish Web services, document them, and query them for matching services. Our semantics-based approach uses semantic description of Web services (example USDL descriptions). Our composition engines find substitutable and composite services that best match the desired service. Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services, number of services in a composition, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential or non-sequential composition that is possible for a given query. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. Use of Constraint Logic Programming helped greatly in obtaining an efficient implementation of this system. Our future work includes extending our engine to work with other web services description languages like OWLS, WSML, WSDL-S, etc. This should be possible as long as semantic relations between concepts are provided. It will involve extending the TripleGenerator, QueryReader, and SemanticRelationsGenerator modules. We would also like to extend our engine to support an external database to save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size which can easily be the case in future. Future work also includes developing an industrial-strength system based on the research reported in this paper, in conjunction with a system that allows (semi-) automatic generation of USDL descriptions from code and documentation of a service [24].

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005. [2] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics Description Lan-

guage for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas. edu/~sxk038200/USDL.pdf. [3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems, pp. 46-53, Mar. '01. [4] OWL-S www.daml.org/services/owl-s/1. 0/owl-s.html. [5] WSML: Web Service Modeling Language. www. wsmo.org/wsml/. [6] WSDL-S: Web Service Semantics. http://www. w3.org/Submission/WSDL-S. [7] WSDL: Web Services Description Language. http: //www.w3.org/TR/wsdl. [8] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services. In ICWS, pp 751-758, '05. [9] OWL WordNet http://taurus.unine.ch/ knowler/wordnet.html. [10] K. Marriott, P. Stuckey. Prog. with Constraints: An Introduction. MIT Press, '98. [11] L. Sterling, S. Shapiro. The Art of Prolog. MIT Press. [12] OWL: Web Ontology Language Reference. http: //www.w3.org/TR/owl-ref. [13] WS Challenge 2006. http://insel.flp.cs. tu-berlin.de/wsc06. [14] U. Keller, R. Lara, H. Lausen, A. Polleres, D. Fensel. Automatic Location of Services. In ESWC, '05. [15] S. Grimm, B. Motik, and C. Preist Variance in eBusiness Service Discovery. In Workshop at ISWC, '04. [16] R. Hull and J. Su. Tools for design of composite Web services. In SIGMOD, '04. [17] B. Srivastava, J. Koehler. Web Services Composition - Current Solutions and Open Problems. In ICAPS, '03. [18] B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pp 467-477. [19] S. McIlraith, T.C. Son Adapting golog for composition of Web services. In KRR, pp 482-493, '02. [20] S. McIlraith, S. Narayanan Simulation, verification and automated composition of services. In WWW, '02. [21] M. Pistore, P. Roberti, P. Traverso Process-Level Composition of Executable Services In ESWC, pp 6277, '05. [22] E. Sirin, J. Hendler, and B. Parsia Semi-automatic Composition of Web Services using Semantic Descriptions In Workshop at ICEIS, '02 . [23] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara Semantic Matching of Web Service Capabilities. In ISWC, pp 333-347, '02. [24] Metallect IQ Server. http://metallect.com/ downloads/Metallect_Product_Brief_ IQServer.pdf

International Journal of Computer Mathematics Vol. 89, No. 16, November 2012, 21232142

Mathematical analysis and simulation of multiple keys and S-Boxes in a multinode network for secure transmission
Ajay Kakkara *, M.L. Singhb and P.K. Bansalc
a Department of Electronics and Communication Engineering, Thapar University, Patiala, Punjab, India; b Department of Electronics Technology, Guru Nanak Dev University, Amritsar, Punjab, India; c M.I.M.I.T.,

Malout, India
(Received 15 September 2010; revised version received 1 June 2011; second revision received 2 September 2011; third revision received 1 June 2012; fourth revision received 11 June 2012; accepted 13 June 2012) The requirement of data security is an important parameter for all organizations for their survival in the world. Cryptography is the best method to avoid unauthorized access to data. It involves an encryption algorithm and the keys that are being used by the users. Multiple keys provide a more secure cryptographic model with a minimum number of overheads. There are various factors that affect the security pattern such as the number of keys and their length, encryption algorithm, latency, key shifting time, and users. In this paper, a new approach is proposed for generating keys from the available data. The analysis of various times, such as encryption, decryption, key setup, processing, and key shifting times, has been done. The model takes minimum time to replace the faulty keys with the fresh keys. In this paper, we consider all the above-mentioned factors and suggest an optimized way of using them. Keywords: encryption; S-Boxes; keys; key shifting time; hacking 2010 AMS Subject Classification: 94A60

1.

Introduction

Security attacks against networks are increasing significantly with time using latest software. Over the years, a number of techniques and approaches have been developed to ensure data confidentiality, integrity, and availability. The techniques used for data security include multiple passwords, cryptography, and biometrics. Cryptography is a technique used to avoid unauthorized access to data. It includes an encryption algorithm and keys. The basic problem that is concerned with keys is that their strength gets degraded with time. Using powerful software packages, it is quite easy to determine the keys. It has been observed that a single key does not provide the required secure model [4,26]. The key length and number of keys and their mutual arrangement provide better security; for the above, it is mandatory to use them in an optimized manner to avoid overheads. If we use 128 key lengths to encrypt 8-bit data, then it will consume more power and also more processing time. Therefore, the best method is to use multiple keys of short length to encrypt
*Corresponding author. Email: kakkar_ajay29@rediffmail.com  Current address: Department of Electronics and Communication Engineering, Thapar University, Patiala, Punjab, India.

ISSN 0020-7160 print/ISSN 1029-0265 online  2012 Taylor & Francis http://dx.doi.org/10.1080/00207160.2012.704022 http://www.tandfonline.com

2124

A. Kakkar et al.

the data [12,25]. The main limitations in this scheme are that the short keys are more sensitive and get attacked by the hacker easily. To tackle the problem, first we determine the failure rate of the keys and calculate the time for which their security level remains in the higher level [23]. Another problem with keys is that how these are transported from the sender to the receiver. Therefore, this can be avoided by generating the keys from the available data with the help of an encryption algorithm. Using the information regarding timing and power consumption by a device during the execution of a cryptographic algorithm, cryptanalysts can break the model [15]. Therefore, the main purpose of a secure encryption algorithm is to protect the interests of parties communicating in the presence of adversaries [14]. The modelling of the behaviour of cyber attackers is difficult and determining the appropriate level of attack is very important from the security point of view. We are aware of the fact that in a multinode network (MN), security decreases with an increase in the number of nodes [13]. It is clear that the flaws in the key designing account for 30 45% of security problems, and architectural risk analysis plays an important role in any secure program [7,14,19,24]. In view of this, multiple keys are used to provide resistance against the virtual and real attacks made by hackers. The following section describes the work of various researchers in the area of data security in MNs. Data Encryption Standard is one of the most widely accepted, publicly available cryptographic systems. It was developed by IBM in the 1970s but was later adopted by the US government as a National Standard. In 1990, International Data Encryption Algorithm (IDEA) was originally developed as the Proposed Encrypted Standard, and in 1992, it was renamed as IDEA. It is a block cipher that uses 64-bit data blocks and a 128-bit key. Aiello and Venkatesan [1] described that nselected plaintexts in an MN can be distinguished by the hacker with the numbers from a random function. This means that it is possible to hack the model with a determined probability. Banerjee et al. [3] gave an overview of signalling enhancement and recovery techniques used in an MN. Such techniques are useful to determine the security of a model. Eschenauer and Gligor [11] proposed a random key establishment technique for wireless sensor networks. Lee and Griffith [17] presented a hierarchical approach to resolve multiple failures in an MN in which various security levels have been proposed for different types of attacks, and a recovery mechanism can be selected on the basis of these security levels. Chan et al. [8] extended the technique of nrandom key establishment that enables two neighbouring nodes to establish a secure communication only when they share n common keys (where n  2). Du et al. [10] developed two similar random key pre-distribution techniques that use the multi-space key pool to improve network resilience and memory usage efficiency [18]. Hundessa and DomingoPascual [16] presented a protection mechanism packed with multiple key(s) to handle multiple link/node failures. Furthermore, Backes and Pfitzmann [2] presented the relating symbolic and cryptographic secrecy technique for an MN. Bertino et al. [4] discussed an efficient time-bound hierarchical key management scheme for secure broadcasting. There are numerous cryptographic algorithms for data encryption and authentication techniques for an MN. Using encryption, an efficient generic solution for an MN was proposed by Naor et al. [22]. Naor's model was not compatible with multiple keys having different failure rates. Hundessa and Domingo-Pascual [16] provided data-gathering strategies over all the possible network routes. Blake and Kolesnikov [5,6] did not provide any practical ways to achieve secure re-routing schemes.

2.

Motivation from the literature survey

From the literature survey, the following observations have been drawn:  A single key with a fixed length cannot be used to provide secure communication in an MN. By knowing the data and key length, the hacker is able to generate side-channel and middle-line attacks.

International Journal of Computer Mathematics

2125

 If the key length is short (11024 bits), it is very easy for the hacker to get the hold of the key by using various permutations and combinations. On the other hand, if large key lengths are used, it results in complexity, which increases the probability of error.  A single key with a variable length provides little bit more secure communication than a single key having a fixed length. The technique is preferred only for short data streams in an MN having less number of nodes.  Multiple keys having different failure rates can be achieved by varying the key length. They are always preferred for encrypting the data in an MN having a large number of nodes. Multiple keys have different failure rates: (i) if the length of the keys is of different order, (ii) if different polynomials are used for the encryption, and (iii) if the size of the data block varies.  In case of node failure, the algorithm immediately generates new keys for the corresponding node. It has been found that for an efficient and reliable model, keys should be generated from the available data. Key recovery mechanisms should be available in the model in order to take care of the failure situation. There is a need to minimize the key shifting time () from the first key to the second key in the case of a multiple key encryption-based system. Keeping in mind the importance of multiple keys for secure data transmission, this work incorporated the use of multiple keys. Multiple keys were generated from the available data to reduce the overheads such as the need of sending additional bits along with the data. Eight to 16 S-Boxes were used to perform random round functions for the generation of multiple keys. There is a design and development procedure of an optimized encryption algorithm that is based on an efficient key management scheme in order to provide secure data transmission in an MN. For better security, multiple keys having a minimum key shifting time were used. The failure rate of multiple keys was evaluated and analysed mathematically to make the model secure. The analysis shows that the failure rate plays a vital role in reducing the time available to the hackers for the various attempts made to destroy the model. In the encryption process, a slight increase in the processing time was observed at various nodes, but it is acceptable because it is very small in comparison with that consumed by the existing encryption algorithms. The objective of this work is to develop an optimized efficient key management technique(s) in order to     generate random key(s) from the data by the algorithm, determine the failure rate of multiple key(s) used by various S-Boxes, reduce the time available for the hacker for making attempts to destroy the model, and minimize the key shifting time () from the first key to the second key and so on.

3.

Proposed work

Modern cryptography involves the use of keys for data signing, encoding, and decoding. Some keys are distributed privately between the parties, while others allow the parties to use public keys that can be broadcast openly [8,9]. The level of protection is varied for every situation and also dependent upon the work and technique used; some encryption techniques provide a virtually unbreakable barrier to information theft; others just require a determined attacker with moderate resources to be broken. One way of comparing the techniques on this level is to estimate how much CPU time would be required on a machine of a given processing speed to iterate through all the possible keys to the encoded data, based upon the permutation [27]. System-wide security is always required to make sure that the data are safe; data are safe for some time while using

2126

A. Kakkar et al.

Figure 1.

Conversion of alphabets into their equivalent codes.

Figure 2. Various stations in a model with buffers.

security techniques, but overall system is not safe; using the information about timing, power consumption, and radiation of a device when it executes a cryptographic algorithm, cryptanalysts have been able to break the system. Keeping in mind the importance of multiple keys for secure data transmission, this work incorporated the use of multiple keys. Multiple keys were generated from the available data to reduce the overheads such as the need of sending additional bits along with the data. The data were encoded using Figure 1 in which all the alphabets and the number having their weight were further converted into binary (0/1), for example, 25 = 010101, 55 = 101101, and so on. All the binary data were encoded with the help of a key and further passed through S-Boxes (having a different key for each round) as shown in Figure 2. During the transmission of data if any station fails, due to the attacks made by the hacker or by other means (atmospheric conditions), then it makes the overall model weak. To maintain the security over a model, all the parameters such as nodes, key generation mechanism, and latency need continuous attention; they must be upgraded with time. Each station is required to be packed up by the recovery mechanism. It is important to calculate the latency time concerned with a particular station:
N

D(S , K ) =
i =1

d (Si ) + d (ki ),

International Journal of Computer Mathematics

2127

Figure 3.

Different stations in a model.

where D(S ) and D(K ) are the delay caused by various stations due to the logical effort in the encryption process and delay caused by the various keys to get generated from the data, respectively. In case of failure of a station, let us take a case in which station S2 is hacked by the hacker, then the data of the same station are moved to the neighbouring station S3 ; in such a case, the buffers g and h available at S3 also result in a delay (Figure 3). The total delay is calculated as
N

D(S , K ) =
i =1

d (Si ) + d (ki ) + d (gi ) + d (hi ).

User A wishes to transmit the data in an MN having nodes (S1 , S2 , . . . , S8 ). Multiple paths are provided to send the data over the path P1 , whereas encrypted packets are transmitted by A. Similarly, one can select the other paths P2 and P3 by considering the delay and reliability. For a secure system, all the intermediate nodes should be under the control of master node A. Failure notification of a node is immediately forwarded to the nearest node, preferably the neighbouring node, in order to reduce the latency and congestion. Fast reroute methods are employed in such situations, which deal with the change of path and are known as dynamic routing for a network. Dynamic re-routing is effective when a model has less number of nodes. Whenever the security level of a node falls below a certain level, then master node A has the power to immediately change the path and re-encrypt the data using another key. In the above exercise, we also make sure that there is no faulty node in the new path and determine the failure rates of S-Boxes: Path P1 : A - S1  (K1 ) - S3  (K1 , K1 ) - S4 - S5 - S7 (K1 , K1 , K1 ) - S8 - B. In the above equation, if S3 is the weak station, then either change the key for the encryption Path P1 : A - S1  (K2 ) - S3  (K2 , K2 ) - S4 - S5 - S7 (K2 , K2 , K2 ) - S8 - B or change the path (i) Path P1 : A  S2  (K1 )  S3  (K1 , K1 )  S4  S5  S7 (K1 , K1 , K1 )  S8  B, (ii) Path P2 : A  S2  (K2 )  S3  (K2 , K2 )  S4  S5  S7 (K2 , K2 , K2 )  S8  B.

2128

A. Kakkar et al.

For a highly secure system, transmission is done from A  S4 . Note: All the stations have the power to change the key and encrypt the data with it only iff A permits: If X1 = inputs, S1 = weak stations, S1 = strong stations. Now, we assume that the S-Boxes are under threat due to a high failure rate of keys. The probability of recovering the data and the latency time and making the system reliable by the help of a shifting key is determined in the following section. 3.1 Case 1: when two stations are under attacks made by the hacker (failures of two S-Boxes in a given model)

In this case, it is required to change the key for particular stations, but this will be not treated as a reliable method, so it will be preferred to change the path; for this, it is required to determine the input data as those concerning weak stations: X1 - -  S1 , X2 - -  S2 & X1,2 - -  S1 & S2 . We know that S1 = X1  X1,2 , On the other hand, S1  S2 = (X1  X2 )  X1,2 . If xi = Pr (Xi ), xi,j = Pr (Xi,j ). Complement Pr (XiC ) = 1 - Pr (Xi ),
N Key Key Key

S2 = X2  X1,2 .

(1)

(2)

(3) (4)

P(Xi ) = 1.
i =1

Complement
N N

Pr (XiC )
i =1

=
i =1

1 - Pr (Xi ) = N - 1.

(5)

Similarly, we can determine the probabilities of S-Boxes as si = Pr (Si ), Using Equation (2), we can write s1 = x1 + x1,2 - x1  x1,2 , s2 = x2 + x1,2 - x2  x1,2 , s1,2 = x1  x2 + x1,2 - x1  x2  x1,2 . (7) si,j = Pr (Si,j ). (6)

International Journal of Computer Mathematics

2129

Rearrange the equation in order to get the probability of x1 , x2 , and x1,2 : s1 - x1,2 , 1 - x1,2 s2 - x1,2 x2 = , 1 - x1,2 s1,2 - x1  x2 x1,2 = . 1 - x1 - x2 - x1,2 x1 =

(8)

Similarly, for four weak stations in a model, it is required to determine the single failure Si , double failures Si,i , triple failures Si,j,k , and quadrate failures Si,j,k ,m . For strong stations Ri , the probabilities are determined using complements (Equation (3)). Pr (Ri ) = Pr (XiC ) or ri = Pr (Ri ) = 1 - xi . In terms of strong stations, Pr (S1 ) = 1 - x1 = r1  r1,2  r1,3  r1,4  r1,2,3  r1,3,4  r1,2,4  r1,2,3,4 , Pr (S2 ) = 1 - x2 = r2  r1,2  r2,3  r2,4  r1,2,3  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S3 ) = 1 - x3 = r3  r1,3  r2,3  r3,4  r1,2,3  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S4 ) = 1 - x4 = r4  r1,4  r2,4  r3,4  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 . Also, Pr (S1  S2 ) = Pr (S1  S2 ) = 1 - s1 - s2 + s1,2 = r1  r2  r1,2  r1,3  r2,3  r1,4  r2,4  r1,2,3  r1,3,4  r1,2,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S1  S3 ) = Pr (S1  S3 ) = 1 - s1 - s3 + s1,3 = r1  r3  r1,2  r1,3  r2,3  r1,4  r3,4  r1,2,3  r1,3,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S1  S4 ) = Pr (S1  S4 ) = 1 - s1 - s4 + s1,4 r1  r4  r1,2  r1,4  r1,3  r2,4  r3,4  r2,4  r1,2,3  r1,2,4  r1,3,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S2  S3 ) = Pr (S2  S3 ) = 1 - s2 - s3 + s2,3 = r2  r3  r1,2  r1,3  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S2  S4 ) = Pr (S2  S4 ) = 1 - s2 - s4 + s2,4 = r2  r4  r1,2  r1,4  r2,3  r2,4  r2,4  r3,4  r1,2,3  r1,2,4  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S3  S4 ) = Pr (S3  S4 ) = 1 - s3 - s4 + s3,4 = r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S1  S2  S3 ) = Pr (S1  S2  S3 ) = 1 - s1 - s2 - s3 + s1,2 + s1,3 + s2,3 - s1,2,3 = r1  r2  r3  r1,2  r1,3  r2,3  r1,4  r2,4  r3,4  r1,2,3  r1,3,4  r1,2,4  r2,3,4  r1,2,3,4 , Pr (S1  S2  S4 ) = Pr (S1  S2  S4 ) = 1 - s1 - s2 - s4 + s1,2 + s1,4 + s2,4 - s1,2,4 = r1  r2  r4  r1,2  r1,4  r1,3  r2,3  r2,4  r1,4  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , (9)

2130

A. Kakkar et al.

Pr (S1  S3  S4 ) = Pr (S1  S3  S4 ) = 1 - s1 - s3 - s4 + s1,3 + s1,4 + s2,4 - s1,2,4 = r1  r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S2  S3  S4 ) = Pr (S2  S3  S4 ) = 1 - s2 - s3 - s4 + s1,2 + s2,3 + s2,4 - s1,2,4 = r2  r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 , Pr (S1  S2  S3  S4 ) = Pr (S1  S2  S3  S4 ) = 1 - s1 - s2 - s3 - s4 + s1,2 + s1,3 + s1,4 + s2,3 + s2,4 + s3,4 - s1,2,3 - s1,2,4 - s1,3,4 - s2,3,4 + s1,2,3,4 = r1  r2  r3  r4  r1,2  r1,3  r1,4  r2,3  r2,4  r3,4  r1,2,3  r1,2,4  r1,3,4  r2,3,4  r1,2,3,4 . Now, determine the values of xi , xi,j , xi,j,k , xi,j,k ,l : x1 = 1 - x2 = 1 - x3 = 1 - x4 = 1 - P r (S 1  S 2  S 3  S 4 ) , Pr (S2  S3  S4 ) P r (S 1  S 2  S 3  S 4 ) , Pr (S1  S3  S4 ) P r (S 1  S 2  S 3  S 4 ) , Pr (S 1  S 2  S 4 ) P r (S 1  S 2  S 3  S 4 ) , Pr (S1  S2  S3 ) Pr (S1  S3  S4 )  Pr (S2  S3  S4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 3  S 4 ) P r (S 1  S 2  S 4 )  P r (S 2  S 3  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 2  S 4 ) Pr (S1  S2  S3 )  Pr (S2  S3  S4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 2  S 3 ) P r (S 1  S 2  S 4 )  P r (S 1  S 3  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 1  S 4 ) P r (S 1  S 2  S 3 )  P r (S 1  S 3  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 1  S 4 ) P r ( S 1  S 2  S 3 )  P r ( S1  S 2  S 4 ) , Pr (S 1  S 2  S 3  S 4 )  P r (S 1  S 2 ) P r ( S 1  S 4 )  P r ( S 2  S 4 )  P r ( S 3  S4 )  P r ( S1  S 2  S 3  S 4 ) , Pr (S4 )  (Pr (S1  S2  S4 ))(Pr (S1  S3  S4 ))(Pr (S2  S3  S4 )) Pr (S1  S3 )  Pr (S2  S3 )  Pr (S3  S4 )  Pr (S1  S2  S3  S4 ) , Pr (S3 )  (Pr (S1  S2  S3 ))(Pr (S1  S3  S4 ))(Pr (S2  S3  S4 ))

x1,2 = 1 - x1,3 = 1 - x1,4 = 1 - x2,3 = 1 - x2,4 = 1 - x3,4 = 1 -

x1,2,3 = 1 - x1,2,4 = 1 -

International Journal of Computer Mathematics Table 1. Round functions with key size in encryption process. Key size and data length after encryption 36 4.5 2.25 1.125 0.5625 1296 162 81 40.5 20.25 46656 5832 2916 1458 729

2131

Round functions 8 16 32 64

x1,3,4 = 1 - x2,3,4 = 1 - x1,2,3,4 = 1 - 

Pr (S1  S2 )  Pr (S2  S3 )  Pr (S2  S4 )  Pr (S1  S2  S3  S4 ) , Pr (S2 )  (Pr (S1  S2  S3 ))(Pr (S1  S2  S4 ))(Pr (S1  S3  S4 )) Pr (S1  S2 )  Pr (S1  S3 )  Pr (S1  S4 )  Pr (S1  S2  S3  S4 ) , Pr (S1 )  (Pr (S1  S2  S3 ))(Pr (S1  S2  S4 ))(Pr (S1  S3  S4 )) Pr (S1 )  Pr (S2 )  Pr (S3 )  Pr (S4 )  Pr (S1  S2  S3 ) (Pr (S1  S2  S3  S4 ))  (Pr (S1  S2 ))  (Pr (S1  S3 ))

Pr (S1  S2  S4 )  Pr (S1  S3  S4 )  Pr (S2  S3  S4 ) (Pr (S1  S4 ))  (Pr (S2  S3 ))  (Pr (S2  S4 ))  (Pr (S3  S4 ))

Using the above expressions, all the values of xi , xi,j , xi,j,k , xi,j,k ,l can be evaluated. The cryptographic key pair needs maintenance; in order to keep up with the increasing processing power available for breaking the keys, the keys need to be replaced periodically [5,20]. This will also limit the possibility of damage in a situation where somebody has managed to steal a copy of the secret key. Buffers g and h (Figure 1) were provided to each station to store the incoming data and the key used to re-encrypt the data. The probability of an optimized linear expression was calculated by |Pl - 0.5|  2NS  |Po - 0.5|NS . (10)

The size of the key and the data length after the encryption process provide the flexibility to choose the desired round functions as shown in Table 1.

4.

Latency and energy consumption

The latency and energy consumption for encryption and decryption processes were evaluated in order to get the information about the time which will be available for the hacker during the channel; if this time is observed to be large, then one has to make more number of attempts to break the model; there is the possibility that during this exercise, the hacker would fail to break it but cause harm to it [20]. Using the experimental results, the smallest value of latency was calculated (Table 2). Moreover, the efficiency for the 8-bit processors (Motorola 6811) and for AMD 4600 was calculated (Figure 4). 4.1 Generation of a symmetric key

If C is the arrangement of characters in a set and the nth alphabet (A) in a set is A = {A0 , A1 , A2 , . . . , An-1 }, then the function is given by F = {f (A0 ), f (A1 ), . . . , f (An-1 )}. One-to-one mapping was done for each character of A w.r.t. C .

2132 Table 2.

A. Kakkar et al. Latency in the encryption and decryption process. Operation Encryption Key size (bits) 8 8 16 16 Round functions 8 16 8 16 Latency (s) 22.32 34.23 15.55 21.25 Energy consumption (J) 24.54 32.55 23.43 44.45

Decryption Key size (bits) Round functions 8 8 8 16 16 8 16 16

24.18 36.67 19.10 22.10

23.50 27.57 19.09 42.10

Figure 4. Various factors for different processors.

Encryption: Ek (M ) = f (m0 )f (m1 )    f (mn-1 ). For substitution cipher, N= log2 S  d. D (11)

Using Vernam cipher Ci = (mi + ki ) mod 2, Equation (11) becomes Ek (M ) = (m1 + k1 )  (m2 + k2 ). The  operation with the same key Ek (M ) = Mi results in a symmetric key [ ki  ki = 0]. For an asymmetric key, different operations are required: (i) X-OR and use of S-Boxes and (ii) generation of keys from the data. 4.2 Generation of keys from the data Let us assume a key set K = (kn ) having a security parameter n (Figure 5). If xi is equal to the input data, then yi is equal to the input data, 1  i  N , B is the Boolean function, and An is the adversary A.

International Journal of Computer Mathematics

2133

Figure 5.

Key generation process.

Then, Pr [B(yn ) = 1 : k  Kn , {yi  Ai (xi )}1iN ] < For any positive polynomial P and k , Pi = Pr [B(yn ) = 1 : {yi  Ai (xi )}1iN ] is the conditional probability of success of A for a fixed i, and we know that Pr k Kn 1 . > pi 2p(k ) Pr [k ]  Pk
k  Kn

1 . p(k )

 Pr [B(yn ) = 1 : k  Kn , {yi  Ai (xi )}1iN ] = =
Pr <1/2(p(k ))

Pr [k ]  Pk +
Pr >1/2(p(k ))

Pr [k ]  Pk <
Pr <1/2(p(k ))

Pr [k ] 

1 2P(k )

+
Pr >1/2(p(k ))

Pr [k ]  1, Kn 1  pi 2P(k ) Kn 1  pi 2P(k )   1 + Pr 2P(k ) 1 + Pr 2P(k ) i i In 1 > Pi 2P(k ) In 1 > Pi 2P(k ) < < 1 1 + , 2P(k ) 2P(k ) 1 . P(k )

Pr Pr

k k

For M and R positive polynomials and k  , 1 > Pr [B(yn ) = 1 : k  Kn , {yi  Ai (xi )}1iN ], M (k )R(k ) Pr [k ]  Pk 
k  KN Pi >1/M (k )

Pr [k ]  Pk , Pr [i]  Pi .

Pr [i]  Pi 
i  IN Pi >1/R(k )

From the above calculations, we can obtain the asymmetrical keys used for the S-Boxes.

5.

Cryptographic algorithm used for the generation of keys from the data

The proposed algorithm uses the available data to generate keys and also avoids the need of transmitting additional bits along with the cipher text. It improves the bandwidth and performance of the model, which enhances the data rate. The key generation mechanism was used to know both the parties (sender and receiver), so that the correct combination of keys can be used for retrieving the data. The key generation process also depends upon the input data stream, and Table 3 was used to generate the keys. It also represents the various conditions for the operation

2134

A. Kakkar et al.

Table 3. Algorithm to generate the keys from the available data with S-Boxes. Operation performed by S-Boxes A=B A-B AB  +B A  -B A  B A  A+B  A-B  AB  B A AB  AB

Data type Alphabets

Conditions A>B A=B A<B A>B A=B A<B A>B A=B A<B A>B A=B A<B

Key length (KL) An 8-bit KL is used if the input data stream is  16 bits; otherwise a 16-bit KL is used An 8-bit KL is used if the input data stream is  8 bits; otherwise a 16-bit KL is used An 8-bit KL is used if the input data stream is  16 bits; otherwise a 16-bit KL is used An 8-bit KL is used if the input data stream is  32 bits; otherwise a 16-bit KL is used

Round functions (RFs) 8 RFs are used if the input data stream is  16 bits; otherwise 16 RFs are used 8 RFs are used if the input data stream is  8 bits; otherwise 16 RFs are used 8 RFs are used if the input data stream is  16 bits; otherwise 16 RFs are used 8 RFs are used if the input data stream is  32 bits; otherwise 16 RFs are used

Number

Alphanumeric

Hybrid

to be performed by the S-Boxes in order to design an MN having single or multiple keys of a fixed/variable length. If the key size is small and has a fixed length, then it is discarded due to its poor security response. There is a need to determine the failure rate of all the keys in an MN for secure data transmission. The algorithm checks the input data stream, and further key generation process was used to design the multiple keys using S-Boxes. The input data streams are broadly classified into four categories, namely (i) alphabets, (ii) numeric, (iii) alphanumeric, and (iv) hybrid (includes the combination of alphabets, numbers, and special characters). The input data streams were processed using Figure 1 into numeric values and then further converted into binary strings. This binary data string was used for the random key generation process specified by Table 3. Once the key was generated, the encryption was carried out using the fixed and variable length keys. The variable length key was preferred due to less overheads, and it also provided a more secure model. The padding overheads were very less in this case. The failure rate of a key was checked using mathematical tools; for weak nodes, the re-encryption was done using the second key, which undergoes a procedure the same as that undergone by the first key. The second key is required if there is a node failure or the input data sequence is very large. The key strength is very high in the case of hybrid data sequences because more combinations are available for the generation of keys.

6. Analysis and simulation results for the failure rate of single and multiple keys having variable lengths for each node Multiple keys were generated from the available data sequences using Table 3. Initially, data were placed in a pool and divided into nearly two sections. Both the sections were compared, and further based upon the conditions, proper operations were performed. For example, if A > B and the data are in the terms of alphabets only, then the A + B operation can be performed in the initial phase and is named as the first round function. The output of this operation is used by the second round operation, which requires another operation for its working. These operations are randomly selected and they provide different outputs even if the pool has the same data for multiple keys. This procedure continues for 8 and 16 iterations depending upon the required security level for a given MN. For A > B and data stream greater than 16 bits, a 16-bit key can be used for the encryption of data. The first key was generated using the following method: assume

International Journal of Computer Mathematics

2135

that A = 1000001011010001 and B = 0010100000101110 are the two data streams available in the pool, then the first round function uses the first operation, which is given as follows: A = 1000001011010001, B = 0010100000101110, C = 1010101011111111. C is the output of the first round function, which is further used by the second round function, which is based upon the left shifting of the stream by 1 bit and is given as C = 1010101011111111, D = 0101010111111110. The output of the second round function is further used by the next S-Box and the process continues for 8 or 16 iterations. Similarly, the second key was generated by keeping an eye on the length and type of input data. 6.1 Case 1: when the input data are in the form of Alphabets

When the data are in terms of only alphabets, then the following steps are used for the generation of keys: (a) (b) (c) (d) (e) Converting the data (text) into a number using Figure 1. Checking the conditions based upon the data evaluated using Table 3. Performing the specified operation mentioned in Table 3. Converting a number into a binary format. Using S-Boxes in order to perform round functions for the generation of keys. Checking the next input if it is still in terms of alphabets and then following the same procedure; otherwise, switching to case 2, 3, or 4 as required. Generation of symmetrical and unsymmetrical keys

6.2

This section involves the analysis of symmetrical and unsymmetrical key generation mechanisms from the data streams. In the proposed algorithm, unsymmetrical keys were preferred because it eliminates the key transportation problem. For a small MN, the use of symmetrical keys is also required; that is why the symmetrical keys were been also considered. The nth alphabet A in a set is given as A = {A0 , A1 , A2 , . . . An-1 }. Then, the function for the same can be expressed as F = {f (A0 ), f (A1 ), . . . , f (An-1 )}. One-to-one encryption was done for each character of A w.r.t. the key. As a result, the encryption for the messages was achieved, and it is expressed as Ek (M ) = f (m0 )f (m1 )    f (mn-1 ), where m is the total number of messages. For a symmetrical key, the valid condition is f (1) = f (0). The second key function is given as f (x ) = 0 in [0,1]; then f (1) = f (0) leads to an asymmetrical key.

2136

A. Kakkar et al.

Proof f (1) - f (0) = f (x ), 1-0 which is not equal to 0: f (1) = f (0).

6.3 (a) (b) (c) (d) 6.4

Case 2: when the input data are in the form of Number Converting a number into a binary format. Checking the conditions based upon the data evaluated using Table 3. Performing the specified operation mentioned in Table 3. Using S-Boxes in order to perform round functions for the generation of keys. Case 3: when the input data are in the form of Alphanumeric

It has been observed that all the alphabets have their own frequency, that is, the occurrence of a particular alphabet in the given information is not the same for a given message. If all the common alphabets are processed in one step, then it will reduce the overheads of the system. For a given message M in a set of Y0 , Y1 , . . . , Yn-1 , the probability is defined as
n -1

P(Yi ) = 1.
i =0

The conditional probability of message X in a given message Y is PY (X ), which can also be written as P(X /Y ). The joint probability messages X and Y are given as P(X , Y ) = PY (X )P(Y ). The entropy is calculated as HY (X ) =
X ,Y

P(X , Y ) log2

1 , PY (X )

HY (X ) = -
X ,Y

P(X , Y ) log2 PY (X )

or HY (X ) =
Y

P(Y )
X

PY (X ) log2 PX (Y ) log2
Y

1 , PY (X ) 1 . PX (Y )

HX (Y ) =
X

P(X )

The entropy of the key is expressed as
n -1

Pi =
i =0

M-

1 n

2

.

The probability of the occurrence of an event always lies in the interval 0  Pi  1. For n  , the chance of getting the exact alphabet reduces to Pi  = 0; it means that an increase in the bits

International Journal of Computer Mathematics

2137

of the given information in a processing unit always increases the security of the model. It would be preferable to increase the number of bits and the round functions at the transmitter in order to provide an equal number of bits for A and B. If a system has nearly equal length sequences for the data and key, then the padding time is reduced, which results in fast processing. For a system having 26 alphabets and 09 numeric digits, the encryption of data takes place with the help of multiple keys designed by the S-Boxes. The analysis for the probability of finding the correct message when the input is in terms of alphanumeric is given as
35

Pi =
i =0 35

M-

1 36

2

,
35

Pi =
i =0

(M )2 - 2/36
i =0

(M )2 + 36

1 36

2

.

For identical messages M , the second data are the same as the first data, that is,
35

35 2 i=0 (M )

= 1.

 Pi =
i =0

(M )2 -

2 1 + , 36 36

35

Pi =
i =0

(M )2 - 0.084.

The above example is applicable if a model contains less number of nodes (230, the results were verified on TMS 320 ADP6713). For larger stages, the probability of obtaining the correct message PM is given as PM = S (k ) log2 d ! = , D D

where S (k ) is the number of stages, D is the total data handled by the model, and d is the data of an individual node. For D   and large data for a stage d  n, the equation changes to PM = log2 n! , D

where n is the number of nodes used in the network. Using the above expression, one can obtain the desired message that would provide the information about the number of bits used for the encryption process. The probability of obtaining the correct message also depends upon the number of nodes used in the model. It is very clear that if D increases rapidly, then the decryption process takes more time to decrypt the cipher text. The processing time for each node depends upon the number of keys used for the encryption process, which clearly indicates that the probability of obtaining the correct message is indirectly related to the keys. If the number of nodes is increased, then it suggests that the number of keys in a network also increases; therefore, more time would be required to get the correct message. The following steps are used for the encryption of data: (a) (b) (c) (d) (e) Converting the data (text and number) into a numeric format using Table 1. Checking the conditions based upon the data evaluated using Table 3. Performing the specified operation mentioned in Table 3. Converting a number into a binary format. Using S-Boxes in order to perform round functions for the generation of keys.

2138

A. Kakkar et al. Table 4. Coding for the special character used in the hybrid technique. ! 121 ( 211 " 321 . 411 @ 122 ) 212 ; 322 / 412 # 123 { 213 ` 323 ` 413 $ 124 } 214 ' 324  414 % 125 [ 215 < 325
-

415

^ 126 ] 216 > 326 + 416

& 127 : 217 ? 327
-

417

* 128 " 218 , 328 = 418

Figure 6.

Evaluation of the failure rate of S-Boxes with two keys.

6.5

Case 4: when the input data are in the form of a hybrid

For the hybrid structure, the alphanumeric numbers were processed using case 3, and the special symbols were processed using Table 4. The key strength is very high in the case of hybrid structure. It offers more resistance to the hacker, and as a result, the model remains secure for more time. To protect the data from intruders, powerful encryption algorithms with multiple keys were used. After the encryption process, it is desirable to transmit the cipher text over the channel. The secure model was examined on the basis of its design, mode of transmission of data, and number of nodes. With an increase in the number of nodes, key length, number of keys, and data length, the model consumes more power and takes more time to generate keys from the available data. A new approach in which keys are generated and processed in the cryptographic model with the help of S-Boxes in order to reduce the processing time has been proposed. MATLAB 7.3 was used to determine the failure rate of various keys in an MN (Figure 6). The model is designed in such a way that it comprises multiple keys and S-Boxes and enables the higher classes to retrieve the encrypted data related to the lower classes. The lower classes do not have the power to access the data concerned with the higher classes. A key management scheme was used to provide such kind of facility to the higher classes. Once a key is exchanged,

International Journal of Computer Mathematics

2139

Figure 7.

Determination of the failure rate of S-Boxes with three keys.

the bit string of the key becomes known to the receiver. In such cases, it is highly desirable to reencrypt the same data with the replaced key. This has been adopted only in cases where the failure rate of the previous key exceeds a predefined value. The behaviour of the keys is unpredictable in a real-time environment; there is always a difference between the characteristics of ideal keys and those of the real keys. In order to achieve a secure model, one should determine the extent to which several security patterns are robust to the known categories of attacks. Various classes were created to represent the number of attacks in a given interval of time. Figure 7 shows that if multiple keys are used, a number of attacks (varies from 200 to 9000) are not able to break the system even after 100 min. A total of five S-Boxes were used to design the key using round functions. The S-Boxes result in variable key lengths used for the encryption of data in five stages. This work is focused on the determination of the failure rate of both types of keys (single and multiple) for each node in a specified interval (90 min for the first key and 55 min for the second key). The first key was used to encrypt the data, and it provided only 20 min for the hacker to make the attacks. After 20 min, the first key was replaced by the second key, which remained active for 50 min. This combination can handle 1250 attacks in 100 min without collapsing. Similarly, one can calculate the failure rate of the other S-Boxes. The following observations have been drawn from Figures 6 and 7: (a) The strength of the keys decreases with time; therefore, the keys are used to encrypt the data in a short interval. For the same parameters, if the number of keys is increased, a better secure model is achieved. (b) S-Boxes are used to design the keys using round functions. The encryption of data with multiple keys always provides a better security level than that of data with a single key. (c) Variable key lengths make the hacking process tougher and cause congestion and complexity.

2140

A. Kakkar et al.

Figure 8.

Single key having a variable length designed by eight S-Boxes.

Figure 9.

Multiple keys having variable failure rates.

International Journal of Computer Mathematics

2141

The key shifting time was reduced by starting the generation procedure of the second key whenever the failure rate of the first key increased from 34%. This work includes the response of the nodes having encrypted data with multiple keys. Multiple keys were used for the encryption of data having the key shifting time (0.01 ns) and a better response was achieved and this is shown in Figure 8. Eight S-Boxes were used to design a single key of a variable length for the encryption of 8, 16, 32, 64, 128, 256, 512, and 1024 data sequences. Figure 8 shows that the response of multiple nodes having encrypted data with a single key (8-bit key length) is not acceptable from a security point of view. The failure rate of the key is fixed due to its length and slightly varies in accordance with the data streams. For higher data streams such as 1024 bits, the security level of the node is much poorer than the security level achieved when an 8-bit key is used to encrypt the 8-bit data. For the same parameters, if keys having a low failure rate are used, then the response of the model for a particular node (a4 and b5 in Figure 9) falls below the danger level. Whenever the failure rate of the second key is more than that of the first key, the system reliability tends to decrease. Recovery mechanisms are required to get a smooth response; conversely, if the first key fails, then it does not affect the model much because the second key is used to encrypt the data in that situation.

7.

Conclusion and future work

Secure and timely transmission of data is always an important aspect for an organization. An efficient encryption algorithm should consist of two factors: (i) fast response and (ii) reduced complexity. Key selection techniques and analysis for security provision of an MN were used in this study. The failure rate of multiple keys was calculated by considering the multiple failures in the model, and it has been analytically shown in the paper. The security also increases if the key size is increased and the key shifting time ( ) is reduced; the above combination may be adopted for secure transmission. This work can be extended if more number of S-Boxes (64 and 128) are used for the same task, and the key length would be reduced with nominal processing time. References
[1] W. Aiello and R. Venkatesan, Foiling birthday attacks in length-doubling transformations, in Advances in Cryptology  EUROCRYPT '96, U. Maurer, ed., Lecture Notes in Computer Science Vol. 1070, Springer-Verlag, Berlin, 1996, pp. 307320. [2] M. Backes and B. Pfitzmann, Relating symbolic and cryptographic secrecy, IEEE Trans. Dependable Secure Comput. 2(2) (2005), pp. 109123. [3] A. Banerjee, L. Drake, L. Lang, B. Turner, D. Awduche, L. Berger, K. Kompella, and Y. Rekhter, Generalized multiprotocol label switching: An overview of signaling enhancements and recovery techniques, IEEE Commun. Mag. 39(7) (2001), pp. 144151. [4] E. Bertino, N. Shang, and S.S. Wagstaff Jr., An efficient time-bound hierarchical key management scheme for secure broadcasting, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 6570. [5] I.F. Blake and V. Kolesnikov, Strong conditional oblivious transfer and computing on intervals, Proceedings of Advances in Cryptology  ASIACRYPT 2004, 10th International Conference on the Theory and Application of Cryptology and Information Security, Jeju Island, Korea, December 59, 2004, Lecture Notes in Computer Science Vol. 3329, Springer-Verlag, Berlin, Heidelberg, Germany, 2004, pp. 515529. [6] I.F. Blake and V. Kolesnikov, Conditional encrypted mapping and comparing encrypted numbers, Proceedings of the 10th International conference on Financial Cryptography and Data Security, Springer-Verlag, Berlin, Heidelberg, 2006, pp. 206220. [7] A. Bobbio and K.S. Trivedi, Computing cumulative measures of stiff Markov chains using aggregation, IEEE Trans. Comput. 39(10) (1990), pp. 12911297. [8] H. Chan, A. Perrig, and D. Song, Random key predistribution schemes for sensor networks, Proceedings of IEEE Symposium on Security and Privacy (S & P '03), IEEE Computer Society, Washington, DC, USA, 2003, pp. 197213. [9] G. Ciardo, R. Marmorstein and R. Siminiceanu, Saturation unbound, Proceedings of the 9th international conference on Tools and algorithms for the construction and analysis of systems, Springer-Verlag, Berlin, Heidelberg, 2003, pp. 379393.

2142

A. Kakkar et al.

[10] W. Du, J. Deng, Y. S. Han, P. K. Varshney, J. Katz and A. Khalili, A pairwise key predistribution scheme for wireless sensor networks, Proceedings of the 10th ACM conference on Computer and communications security (CCS'03), ACM, New York, NY, USA, 2003, pp. 4251. [11] L. Eschenauer, V. D. Gligor, A key-management scheme for distributed sensor networks, Proceedings of the 9th ACM conference on Computer and communications security (CCS'02), ACM, New York, NY, USA, 2002, pp. 4147. [12] M. Fischlin, A Cost-Effective Pay-Per-Multiplication Comparison Method for Millionaires, Proceedings of the 2001 Conference on Topics in Cryptology: The Cryptographer's Track at RSA, Springer-Verlag, London, UK, 2001, pp. 457472. [13] Z. Fu, H. Luo, P. Zerfos, S. Lu, and L. Zhang, The impact of Multihop wireless channel on TCP performance, IEEE Trans. Mobile Comput. 4(2) (2005), pp. 209221. [14] S.T. Halkidis, N. Tsantalis, A. Chatzigeorgiou, and G. Stephanides, Architectural risk analysis of software systems based on security patterns, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 129142. [15] X. He, M. Zhang, and Q.K. Yang, SPEK: A storage performance evaluation kernel module for block-level storage systems under faulty conditions, IEEE Trans. Dependable Secure Comput. 2(2) (2005), pp. 138149. [16] L. Hundessa, Optimal and guaranteed alternative LSP for multiple failures, Proceedings of 13th International Conference on Computer Communications and Networks, IEEE Conference, Illinois, Chicago, 2004, pp. 5964. [17] S.K. Lee, C. Kim, and D. Griffith, Hierarchical Restoration Scheme for Multiple Failures in GMPLS Networks, Proceedings of International Conference on Parallel Processing Workshops (ICPPW'02), IEEE Computer Society, Vancouver, BC, Canada, 2002, pp. 177182. [18] D. Liu and P. Ning, Establishing pairwise keys in distributed sensor networks, Proceedings of the 10th ACM conference on Computer and communications security (CCS'03), ACM, New York, NY, USA, 2003, pp. 5261. [19] V.B. Livshits and M.S. Lam, Finding security vulnerabilities in java applications with static analysis, Proceedings of the 14th conference on USENIX Security Symposium (SSYM'05), USENIX Association, Berkeley, CA, USA, 2005, pp. 1936. [20] B.B. Madan, K. Goseva-Popstojanova, K. Vaidyanathan, and K.S. Trivedi, A method for modeling and quantifying the security attributes of intrusion tolerant systems, Perform. Eval. 56(1) (2004), pp. 167186. [21] J. Muppala, M. Malhotra, and K. Trivedi, Stiffness-tolerant methods for transient analysis of stiff Markov chains, Microelectronics Reliab. 34(11) (1994), pp. 18251841. [22] M. Naor, B. Pinkas, and R. Sumner, Privacy preserving auctions and mechanism design, EC'99, ACM Press, New York, 1999, pp. 129139. [23] P. Paillier, Public-key cryptosystems based on composite degree residuosity classes, Proceedings of the 17th International conference on Theory and application of cryptographic techniques, Springer-Verlag, Berlin, Heidelberg, 1999, pp. 223238. [24] P. Papadimitratos and Z.J. Haas, Secure message transmission in mobile ad hoc networks, Ad Hoc Networks 1(1) (2003), pp. 193209. [25] J.T. Park, J.W. Nah, and W.H. Lee, Dynamic path management with resilience constraints under multiple link failures in MPLS/GMPLS networks, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 143154. [26] A. Reibman and K.S. Trivedi, Numerical transient analysis of Markov models, Comput. Oper. Res. 15(1) (1988), pp. 1936. [27] J. Ren and L. Harn, Generalized ring signatures, IEEE Trans. Dependable Secure Comput. 5(3) (2008), pp. 153164.

Copyright of International Journal of Computer Mathematics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.

Goal-Directed Execution of Answer Set Programs
Kyle Marple
University of Texas at Dallas 800 W. Campbell Road Richardson, TX, USA kbm072000@utdallas.edu

Ajay Bansal
Arizona State University 7231 E. Sonoran Arroyo Mall Mesa, Arizona, USA Ajay.Bansal@asu.edu

Richard Min
University of Texas at Dallas 800 W. Campbell Road Richardson, TX, USA min75243@hotmail.com

Gopal Gupta
University of Texas at Dallas 800 W. Campbell Road Richardson, TX, USA gupta@utdallas.edu

Abstract
Answer Set Programming (ASP) represents an elegant way of introducing non-monotonic reasoning into logic programming. ASP has gained popularity due to its applications to planning, default reasoning and other areas of AI. However, none of the approaches and current implementations for ASP are goal-directed. In this paper we present a technique based on coinduction that can be employed to design SLD resolution-style, goal-directed methods for executing answer set programs. We also discuss advantages and applications of such goal-directed execution of answer set programs, and report results from our implementation. Categories and Subject Descriptors D.1.6 [Programming Techniques]: Logic Programming General Terms Algorithms Keywords Answer set programming, goal-directed execution, coinduction

1.

Introduction

Answer Set Programming (ASP) is an elegant way of developing non-monotonic reasoning applications. ASP has gained wide acceptance, and considerable research has been done in developing the paradigm as well as its implementations and applications. ASP has been applied to important areas such as planning, scheduling, default reasoning, reasoning about actions [3], etc. Numerous implementations of ASP have been developed, ranging from DLV
c ACM 2012. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Proceedings of the 14th symposium on Principles and Practice of Declarative Programming (PPDP '12), pages 35-44, http: //dx.doi.org/10.1145/2370776.2370782.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PPDP'12, September 1921, 2012, Leuven, Belgium. Copyright c 2012 ACM 978-1-4503-1522-7/12/09. . . $10.00

([17]) and smodels [22] to SAT-based solvers such as cmodels [13] and the conflict-driven solver clasp [10]. However, these implementations compute the whole answer set: i.e., they are not goaldirected in the fashion of Prolog. Given an answer set program and a query goal Q, a goal-directed execution will systematically enumerate--via SLD style call expansions and backtracking--all answer sets that contain the propositions/predicates in Q. Other efforts have been made to realize goal-directed implementations (e.g., [5]), however, these approaches can handle only a limited class of programs and/or queries. In this paper we describe a goal-directed execution method that works for any answer set program as well as for any query. The method relies on coinductive logic programming (co-LP) [14]. CoLP can be regarded as providing an operational semantics, termed co-SLD resolution, for computing greatest fixed points (gfp) of logic programs. Co-SLD resolution systematically computes elements of the gfp of a program via backtracking [14, 27]. Additionally, calls are allowed to coinductively succeed if they unify with one of their ancestor calls [27]. To permit this, each call is stored in the coinductive hypothesis set (CHS) as the call is made. A more detailed introduction to co-LP and co-SLD resolution can be found in Appendix A. A goal-directed method for executing answer set programs is analogous to top-down, SLD style resolution for Prolog, while current popular methods for ASP are analogous to bottom-up methods that have been used for evaluating Prolog (and Datalog) programs [25]. A goal-directed execution method for answering queries for an answer set program has several advantages. The main advantage is that it paves the way to lifting the restriction to finitely groundable programs, and allows realization of ASP with full first-order predicates [20]. In the rest of the paper we develop a goal-directed strategy for executing answer set programs, and prove that it is sound and complete with respect to the method of Gelfond and Lifschitz. We restrict ourselves to only propositional (grounded) answer set programs in this paper; work is in progress to extend our goaldirected method to predicate answer set programs [20, 21]. Note that the design of a top-down goal-directed execution strategy for answer set programs has been regarded as quite a challenging problem [3]. As pointed out in [6], the difficulty in designing a goal-directed method for ASP comes about due to the absence of a relevance property in stable model semantics, on which answer set

programming is based [6, 23, 24]. We will introduce a modified relevance property that holds for our goal-directed method and guarantees that partial answer sets computed by our method can be extended to complete answer sets.

(p  not q  not not p). Such rules are termed ordinary rules. Rule P1.b is also an ordinary rule, since if used for expanding the call to q, it will lead to a recursive call to q through two negations. For simplicity of presentation, all non-cyclical rules will also be classified as ordinary rules. 2. Cyclical rules which when used to expand a call to subgoal G lead to a recursive call to G that is in the scope of an odd number of negations. Such recursive calls are known as odd loops over negation (OLONs). For example, given the program P2 below: p :- q, not p, r. . . . Rule P2.a a call to p using Rule P2.a will eventually lead to a call to not p. Under ordinary logic programming execution, this will lead to non-termination. Under ASP, however, the program consisting of Rule P2.a has {not p, not q, not r} as its answer set. For brevity, we refer to rules containing OLONs as OLON rules. Note that a rule can be both an ordinary rule and an OLON rule, since given a subgoal G, its expansion can lead to a recursive call to G through both even and odd numbers of negations along different expansion paths. For example in program P3 below, Rule P3.a is both an ordinary rule and an OLON rule. p :- q, not r. r :- not p. q :- t, not p. . . . Rule P3.a . . . Rule P3.b . . . Rule P3.c

2.

Answer Set Programming (ASP)

Answer Set Programming (ASP) [12] (A-Prolog [11] or AnsProlog [3]) is a declarative logic programming paradigm which encapsulates non-monotonic or common sense reasoning. The rules in an ASP program are of the form: p :- q1 , ..., qm , not r1 , ..., not rn . where m  0 and n  0. Each of p and qi (i  m) is a literal, each not rj (j  n) is a naf-literal (not is a logical connective called negation as failure (naf) or default negation). The semantics of an Answer Set program P is given via the Gelfond-Lifschitz method [3] in terms of the answer sets of the program ground(P), obtained by grounding the variables in the program P. Gelfond-Lifschitz Transform (GLT) Given a grounded Answer Set program P and a candidate answer set A, a residual program R is obtained by applying the following transformation rules: for all literals L  A, 1. Delete all rules in P which have not L in their body. 2. Delete all the remaining naf-literals (of the form not M) from the bodies of the remaining rules. The least fixed-point (say, F) of the residual program R is next computed. If F = A, then A is a stable model or an answer set of P. ASP can also have rules of the form: :- q1 , ..., qm , not r1 , ..., not rn . p :- q1 , ..., qm , not r1 , ..., not rn , not p. These rules capture the non-monotonic aspect of Answer Set Programming. Consider an example rule: p :- q, not p. Following the Gelfond-Lifschitz method (GL method) outlined above, this rule restricts q (and p) to not be in the answer set (unless p happens to be in the answer set via other rules, in which case due to presence of not p this rule will be removed while generating the residual program). Note that even though an answer set program can have other rules to establish that q is in the answer set, adding the rule above forces q to not be in the answer set unless p succeeds through another rule, thus making ASP non-monotonic.

Our top-down method requires that we properly identify and handle both ordinary and OLON rules. We will look at each type of rule in turn, followed by the steps taken to ensure that our method remains faithful to the GL method. 3.1 Ordinary Rules

Ordinary rules such as rules P1.a and P1.b in program P1 above exemplify the cyclical reasoning in ASP. The rules in the example force p and q to be mutually exclusive, i.e., either p is true or q is true, but not both. One can argue the reasoning presented in such rules is cyclical: If p is in the answer set, then q cannot be in the answer set, and if q is not in the answer set, then p must be in the answer set. Given a goal, G, and an answer set program comprised of only ordinary rules, G can be executed in a top-down manner using coinduction, through the following steps:
 Record each call in the CHS. The recorded calls constitute the

3.

Goal-directed ASP Issues

Any normal logic program can also be viewed as an answer set program. However, ASP adds complexity to a normal logic program in two ways. In addition to the standard Prolog rules, it allows: 1. Cyclical rules which when used to expand a call to a subgoal G lead to a recursive call to G through an even (but non-zero) number of negations. For example, given the program P1 below: p :- not q. q :- not p. . . . Rule P1.a . . . Rule P1.b

coinductive hypothesis set, which is the potential answer set.
 If at the time of the call, the call is already found in the CHS, it

succeeds coinductively and finishes.
 If the current call is not in the CHS, then expand it in the style

of ordinary SLD resolution (recording the call in the CHS prior to expansion).
 Simplify not not p to p, whenever possible, where p is a

proposition occurring in the program.
 If success is achieved with no goals left to expand, then the

Ordinary logic programming execution for the query ?- p. (or ?- q.) will lead to non-termination. However, ASP will produce two answer sets: {p, not q} and {q, not p} . Expanding the call p using Rule P1.a in the style of SLD resolution will lead to a recursive call to p that is in scope of two negations
Note that we will list all literals that are true in a given answer set. Conventionally, an answer set is specified by listing only the positive literals that are true; those not listed in the set are assumed to be false.

coinductive hypothesis set contains the (partial) answer set. The top-down resolution of query p with program P1 will proceed as follows. :- p :- not q CHS = {} (expand p by Rule P1.a) CHS = {p} (expand q by Rule P1.b)

:- not not p CHS = {p, not q} (simplify not not p  p ) :- p CHS = {p, not q} (coinductive success: p  CHS) :success: answer set is {p, not q} Note that the maintenance of the coinductive hypothesis set (CHS) is critical. If a call is encountered that is already in the CHS, it should not be expanded, it should simply (coinductively) succeed. Note that the query q will produce the other answer set {q, not p} in a symmetrical manner. Note also that the query not q will also produce the answer set {p, not q} as shown below. Thus, answers to negated queries can also be computed, if we apply the coinductive hypothesis rule to negated goals also, i.e., a call to not p succeeds, if an ancestor call to not p is present: :- not q :- not not p :- p :- not q :3.2 OLON Rules CHS = {} (expand q by Rule P1.b) CHS = {not q} (not not p  p) CHS = {p, not q} (expand p by Rule P1.a) CHS = {p, not q} (coinductive success for not q) success: answer set is {p, not q}

That is, if we encounter the goal g (resp. not g) during execution and not g  CHS (resp. g  CHS), then the computation fails and backtracking ensues. As another example, consider the program containing rule P4.1 (which has p in its head), but not rule P4.2, and the query :p. When execution starts, p will be added to the CHS and then expanded by rule P4.1; if the call to q fails, then the goal p also fails. Alternatively, if q succeeds due to other rules in the program, then upon arriving at the call not p, failure will ensue, since not p is inconsistent with the current CHS (which equals {p, q} prior to the call not p). Thus, OLON rules do not pose any problems in top-down execution based on coinduction, however, given an OLON rule with p as its head, if p can be inferred by other means (i.e., through ordinary rules) then the query p should succeed. Likewise, if q succeeds by other means and p does not, then we should report a failure (rather, report the absence of an answer set; note that given our conventions, CHS = {} denotes no answer set). We discuss how top-down execution of OLON rules is handled in Section 3.4. 3.3 Coinductive Success Under ASP

Our goal-directed procedure based on coinduction must also work with OLON rules. OLON rules are problematic because their influence on answer sets is indirect. Under ASP, rules of the form p :- q1 , q2 , ..., qk , not p. hold only for those (stable) models in which p succeeds through other rules in the program or at least one of the qi 's is false. Note that a headless rule of the form: :- q1 , q2 , ..., qn . is another manifestation of an OLON rule, as it is equivalent to the rule: p :- q1 , q2 , ..., qn , not p. where p is a literal that does not occur anywhere else in the program, in the sense that the stable models for the two rules are identical. Without loss of generality, consider the simpler rule: p :- q, not p. For an interpretation to be a (stable) model for this rule, either p must succeed through other rules in the program or q must be false. Two interesting cases arise: (i). p is true through other rules in the program. (ii) q is true through other rules in the program. For case (i), if p is true through other means in the program, then according to the Gelfond-Lifschitz method, it is in the answer set, and the OLON rule is taken out of consideration due to the occurrence of not p in its body. For case (ii), if q is true through other means and the rule is still in consideration due to p not being true through other rules in the program, then there are no answer sets, as q is both true and false. Thus, the answer set of the program P4 below is: {p, not q}. p :- q, not p. p. . . . Rule P4.1 . . . Rule P4.2

While our technique's use of co-SLD resolution has been outlined above, it requires some additional modification to be faithful to the Gelfond-Lifschitz method. Using normal coinductive success, our method will compute the gfp of the residual program after the GL transform, while the GL method computes the lfp. Consider Program P6 below: p :- q. q :- p. . . . Rule P6.1 . . . Rule P6.2

while there is no answer set for program P5 below: p :- q, not p. q. . . . Rule P5.1 . . . Rule P5.2

Our method based on coinduction will succeed for queries :p and :- q producing the answer set {p, q} while under the GL method, the answer set for this program is {not p, not q}. Our top-down method based on coinduction really computes the gfp of the original program. The GL method computes a fixed point of the original program (via the GL transformation and then computation of the lfp of the residual program) that is in between the gfp and the lfp of the original program. In the GL method, direct cyclical reasoning is not allowed, however, cyclical reasoning that goes through at least one negated literal is allowed. Thus, under the GL method, the answer set of program P6 does not contain a single positive literal, while there are two answer sets for the program P1 given earlier, each with exactly one positive literal, even though both programs P1 and P6 have only cyclical rules. Our top-down method can be modified so that it produces answer sets consistent with the GL method: a coinductive recursive call can succeed only if it is in the scope of at least one negation. In other words, the path from a successful coinductive call to its ancestor call must include a call to not. This restriction disallows inferring p from rules such as p :- p. With this operational restriction in place, the CHS will never contain a positive literal that is in the gfp of the residual program obtained after the GLT, but not in its lfp. To show this, let us assume that, for some ASP program, a call to p will always encounter at least one recursive call to p with no intervening negation. In such a case, p will never be part of any answer set:
 Under our goal-directed method, any call to p will fail when a

recursive call is encountered with no intervening negation.
 Under the GL method, p will never be in the lfp of the residual.

Given an OLON rule with p as its head and the query p, execution based on co-SLD resolution will fail, if we require that the coinductive hypothesis set (CHS) remains consistent at all times.

Even if a rule for p is present in the residual and all other dependencies are satisfied, the rule will still depend on the recursive call to p.

3.4

NMR Consistency Check

To summarize, the workings of our goal-directed strategy are as follows: given a goal G, perform co-SLD resolution while restricting coinductive success as outlined in Section 3.3. The CHS serves as the potential answer set. A successful answer will be computed only through ordinary rules, as all OLON rules will lead to failure due to the fact that not h will be encountered with proposition h present in the CHS while expanding with an OLON rule whose head is h. Once success is achieved, the answer set is the CHS. As discussed later, this answer set may be partial. The answer set produced by the process above is only a potential answer set. Once a candidate answer set has been generated by coSLD resolution as outlined above, the set has to be checked to see that it will not be rejected by an OLON rule. Suppose there are n OLON rules in the program of the form: qi :- Bi . where 1  i  n and each Bi is a conjunction of goals. Each Bi must contain a direct or indirect call to the respective qi which is in the scope of odd number of negations in order for qi :- Bi . to qualify as an OLON rule. If a candidate answer set contains qj (1  j  n), then each OLON rule whose head matches qj must be taken out of consideration (this is because Bj leads to not(qj ) which will be false for this candidate answer set). For all the other OLON rules whose head proposition qk (1  j  n) is not in the candidate answer set, their bodies must evaluate to false w.r.t. the candidate answer set, i.e., for each such rule, Bk must evaluate to false w.r.t. the candidate answer set. The above restrictions can be restated as follows: a candidate answer set must satisfy the formula qi  not Bi (1  i  n) for each OLON rule qi :- Bi . (1  i  n) in order to be reported as the final answer set. Thus, for each OLON rule, the check chk qi :- qi . chk qi :- not Bi . is constructed by our method. Furthermore, not Bi will be expanded to produce a chk qi clause for each literal in Bi . For example, if Bi represented the conjunction of literals s, not r, t in the above example, the check created would be: chk qi :- qi . chk qi :- not s. chk qi :- r. chk qi :- not t. A candidate answer set must satisfy each of these checks in order to be reported as a solution. This is enforced by rolling the checks into a single call, termed nmr chk: nmr chk :- chk q1 , chk q2 , ...chk qn . Now each query Q is transformed to Q, nmr chk. before it is posed to our goal-directed system. One can think of Q as the generator of candidate answer sets and nmr chk as the filter. If nmr chk fails, then backtracking will take place and Q will produce another candidate answer set, and so on. Backtracking can also take place within Q itself when a call to p (resp. not p) is encountered and not p (resp. p) is present in the CHS. Note that the CHS must be a part of the execution state, and be restored upon backtracking.

That is, given a proposition H's definition (Bi 's are conjunction of literals): H :- B1 . H :- B2 . ... H :- Bn . we add the dual rule not H :- not B1 , not B2 , ..., not Bn . If a proposition q appears in the body of a rule but not in any of the rule heads, then the fact not q. is added. Note that adding the dual rules is not necessary; it only makes the exposition of our goal-directed method easier to present and understand. 4.2 Goal-directed Method for Computing Answer Sets

Given a propositional query :- Q and a propositional answer set program P, the goal-directed procedure works as described below. Note that the execution state is a pair (G, S), where G is the current goal list, and S the current CHS. 1. Identify the set of ordinary rules and OLON rules in the program. 2. Assert a chk qi rule for every OLON rule with qi as its head and build the nmr check as described in Section 3.4. 3. For each ordinary rule and chk qi rule, construct its dual version. 4. Append the nmr check to the query. 5. Set the initial execution state to: (:- G1 , ..., Gn , {}). 6. Non-deterministically reduce the execution state using the following rules: (a) Call Expansion: (:- G1 , .., Gi , .., Gn , S)  (:- G1 , .., B1 , .., Bm , .., Gn , S  {Gi }) where Gi matches the rule Gi :- B1 , ..., Bm . in P, Gi  / S and not Gi  / S. (b) Coinductive Success: (:- G1 , .., Gi-1 , Gi , Gi+1 , .., Gn , S)  (:- G1 , .., Gi-1 , Gi+1 , .., Gn , S) if Gi  S and either: i. Gi is not a recursive call or ii. Gi is a recursive call in the scope of a non-zero number of intervening negations. (c) Inductive Success: (:- G1 , .., Gi-1 , Gi , Gi+1 , .., Gn , S)  (:- G1 , .., Gi-1 , Gi+1 , .., Gn , S  {Gi }) if Gi matches a fact. (d) Coinductive Failure: (:- G1 , .., Gi , .., Gn , S)  (fail, S) if either: i. not Gi  S or ii. Gi  S and Gi is a recursive call without any intervening negations. (e) Inductive Failure: (:- G1 , .., Gi , .., Gn , S)  (fail, S) if Gi has no matching rule in P. (f) Print Answer: (:- true, S)  success: S is the answer set where `:- true'  empty goal list

4.

Goal-directed Execution of Answer Set Programs

We next describe our general goal-directed procedure for computing answer sets. 4.1 Dual Rules

For simplicity, we add one more step to the process. Similarly to Alferes et al [1], for each rule in the program, we introduce its dual.

Note that when all the goals in the query are exhausted, execution of nmr chk begins. Upon failure, backtracking ensues, the state is restored and another rule tried. Note that negated calls are expanded using dual rules as in [1], so it is not necessary to check whether the number of intervening negations between a recursive call and its ancestor is even or odd. (See the call expansion rule above). A detailed example of goal-directed execution can be found in Appendix B. Next we discuss a few important issues: Identifying OLON and Ordinary Rules Given a propositional answer set program P, OLON rules and ordinary rules can be identified by constructing and traversing the call graph. The complexity of this traversal is O(|P|  n), where n is the number of propositional symbols occurring in the head of clauses in P and |P| is a measure of the program size. Note also that during the execution of a query Q, we need not make a distinction between ordinary and OLON rules; knowledge of OLON rules is needed only for creating the nmr chk. Partial Answer Set Our top-down procedure might not generate the entire answer set. It may generate only the part of the answer set that is needed to evaluate the query. Consider program P7: p q r s ::::not not not not q. p. s. r.

The relevance property is desirable because it would ensure that a partial answer set computed using only relevant rules for each literal could be extended into a complete answer set. However, stable model semantics do not satisfy the definition as given. This is because OLON rules can alter the meaning of a program and the truth values of individual literals without occurring in the set of relevant rules [6, 23]. For instance, an irrelevant rule of the form p :- not p. when added to an answer set program P, where P has one or more stable models and p does not occur in P, results in a program that has no stable models. Approaches such as [23] have addressed the lack of a relevance property by modifying stable model semantics to restore relevance. However, our implementation can be viewed as restoring relevance by expanding the definition of relevant rules to include all OLON rules in a program. Because the NMR check processes every OLON rule, it has the effect of making the truth value of every literal in a program dependent on such rules. That is, nmr rel rul(P, L) = rel rul(P, L)  O, O = { R: R is an OLON rule in P } (2)

Using nmr rel rul(P,L) in place of rel rul(P,L), a modified version of equation 1 above holds for our semantics: SEM (P )(L) = SEM (nmr rel rule(P, L))(L) (3)

As a result, any partial model returned by our semantics is guaranteed to be a subset of one or more complete models. 5.2 Soundness

Under goal-directed execution, the query :- q. for program P7 will produce only {q, not p} as the answer since the rules defining r and s are completely independent of rules for p and q. One could argue that this is an advantage of a goal-directed execution strategy rather than a disadvantage, as only the relevant part of the program will be explored. In contrast, if the query is :- q, s, then the right answer {q, not p, s, not r} will be produced by the goal-directed execution method. Thus, the part of the answer set that gets computed depends on the query. Correct maintenance of the CHS throughout the execution is important as it ensures that only consistent and correct answer sets are produced.

Theorem 1. For the non-empty set X returned by successful topdown execution of some program P, the set of positive literals in X will be an answer set of R, the set of rules of P used during topdown execution. Proof. Let us assume that top-down execution of a program P has succeeded for some query Q consisting of a set of literals in P, returning a non-empty set of literals X. We can observe that R  nmr rel rul(P, L): for each positive literal in Q, one
LQ

5.

Soundness and Correctness of the Goal-directed Method

We will now show the correctness of our goal-directed execution method by showing it to be sound and complete with respect to the GL method. First, we will examine the modified relevance property which holds for our method. 5.1 Relevance

As we stated in the introduction, one of the primary problems with developing a goal-directed ASP implementation is the lack of a relevance property in stable model semantics. Dix introduces relevance by stating that, "given any semantics SEM and a program P, it is perfectly reasonable that the truth-value of a literal L, with respect to SEM(P), only depends on the subprogram formed from the relevant rules of P with respect to L" [6]. He formalizes this using the dependency-graph of P, first establishing that
 "dependencies of (X ) := {A : X depends on A}, and  rel rul(P,X) is the set of relevant rules of P with respect to X,

i.e. the set of rules that contain an A  dependencies of (X ) in their heads" [6] and noting that the dependencies and relevant rules of 촞 are the same as those of X [6]. He then defines relevance as, for all literals L: SEM (P )(L) = SEM (rel rul(P, L))(L) (1)

rule with the literal in its head will need to succeed, for each negative literal in Q all rules with the the positive form of the literal in their head will need to fail, and the resulting set must satisfy the NMR check. We will show that X is a valid answer set of R using the GL method. First, because X may contain negative literals and the residual program produced by the GL method is a positive one, let us remove any rules in R containing the positive version of such literals as a goal, and then remove the negated literals from X to obtain X'. Because our algorithm allows negative literals to succeed if and only if all rules for the positive form fail or no such rules exist, only rules which failed during execution will be removed by this step. Next, let us apply the GL transformation using X' as the candidate answer set to obtain the residual program R'. This will remove rules containing the negation of any literal in X' and remove any negated goals from the remaining rules. We know that X' will be an answer set of R if and only if X' = LFP(R'). Now let us examine the properties of R'. As positive literals, we know that each literal in X' must occur as the head of a rule in R which succeeded during execution. Because such rules would have failed if the negation of any goal was present in the CHS, we know that such rules would not have been eliminated from the residual program by the GL transformation, and are thus still present in R' save for the removal of any negated goals. Because any rules containing the negation of a literal in X had to fail during execution, at least one goal in each of these rules must have failed, resulting in the negation of the goal being added to the CHS. Furthermore, because the NMR check applies the negation of each

OLON rule, again the negation of some goal in each such rule must have been added to the CHS. Thus any rule which failed during execution and yet was included in R will have been removed from R'. Finally, because our algorithm allows coinductive success to occur only in the scope of at least one negation, the removal of negated goals from the residual program will ensure that R' contains no loops. Because the remaining rules in R' must have succeeded during execution, their goals must have been added to the CHS, and therefore those goals consisting of positive literals form X'. Thus R' is a positive program with no loops, and each literal in X' must appear as the head of some rule in R' which is either a fact or whose goals consist only of other elements in X'. Therefore the least fixed point of R' must be equal to X', and X' must be an answer set of R. Theorem 2. Our top-down execution algorithm is sound with respect to the GL method. That is, for the non-empty set of literals X returned by successful execution of some program P, the set of positive literals in X is a subset of one or more answer sets of P. Proof. As shown above, the positive literals in the set returned by successful execution of P will be an answer set of R  nmr rel rul(P, L). Because R will always contain all OLON
LQ

Table 1. N-Queens Problem; Times in Seconds Problem Galliwasp clasp cmodels smodels queens-12 0.033 0.019 0.055 0.112 queens-13 0.034 0.022 0.071 0.132 queens-14 0.076 0.029 0.098 0.362 queens-15 0.071 0.034 0.119 0.592 queens-16 0.293 0.043 0.138 1.356 queens-17 0.198 0.049 0.176 4.293 queens-18 1.239 0.059 0.224 8.653 queens-19 0.148 0.070 0.272 3.288 queens-20 6.744 0.084 0.316 47.782 queens-21 0.420 0.104 0.398 95.710 queens-22 69.224 0.112 0.472 N/A queens-23 1.282 0.132 0.582 N/A queens-24 19.916 0.152 0.602 N/A

rules in P, no unused rules in P are capable of affecting the truth values of the literals in X. Thus the modified definition of relevance holds for all literals in X under our semantics and the partial answer set returned by our algorithm is guaranteed to be extensible to a complete one. Thus our algorithm for top-down execution is sound with respect to the GL method. 5.3 Completeness

Table 2. MxN-Pigeons Problem (No Solution for M>N) Problem Galliwasp clasp cmodels smodels pigeon-10x10 0.020 0.009 0.020 0.025 pigeon-20x20 0.050 0.048 0.163 0.517 pigeon-30x30 0.132 0.178 0.691 4.985 pigeon-8x7 0.123 0.072 0.089 0.535 pigeon-9x8 0.888 0.528 0.569 4.713 pigeon-10x9 8.339 4.590 2.417 46.208 pigeon-11x10 90.082 40.182 102.694 N/A

Theorem 3. Our top-down execution algorithm is complete with respect to the GL method. That is, for a program P, any answer set valid under the GL method will succeed if used as a query for top-down execution. In addition, the set returned by successful execution will contain no additional positive literals. Proof. Let X be a valid answer set of P obtained via the GL method. Then there exists a resultant program P' obtained by removing those rules in P containing the negation of any literal in X and removing any additional negated literals from the goals of the remaining rules. Furthermore, because X is a valid answer set of P, X = LFP(P'). This tells us that for every literal L  X there is a rule in P' with L as its head, which is either a fact or whose goals consist only of other literals in X. Let us assume that X is posed as a query for top-down execution of P. As we know that each L  X has a rule in P' with L as its head and whose positive goals are other literals in X, we know that such a rule also exists in P, with the possible addition of negated literals as goals. However, we know that these negated literals must succeed, that is, all rules with the positive form of such literals in their heads must fail, either by calling the negation of some literal in the answer set or by calling their heads recursively without an intervening negation. Were this not the case, these rules would remain in P', their heads would be included in LFP(P') and X would not be a valid answer set of P. Therefore, a combination of rules may be found such that each literal in X appears as the head of at least one rule which will succeed under top-down execution, and whose positive goals are all other literals in X. Furthermore, because each literal in the query must be satisfied and added to the CHS, and any rule with a goal whose negation is present in the CHS will fail, such a combination of rules will eventually be executed by our algorithm. Because such rules would also be present in P', we know that they cannot add additional positive literals to the CHS, as these would be part of LFP(P'), again rendering X invalid.

Table 3. MxN-Coloring problem (No Solution for M=3) Problem Galliwasp clasp cmodels smodels mapclr-4x20 0.018 0.006 0.011 0.013 mapclr-4x25 0.021 0.007 0.014 0.016 mapclr-4x29 0.023 0.008 0.016 0.018 mapclr-4x30 0.026 0.008 0.016 0.019 mapclr-3x20 0.022 0.005 0.009 0.008 mapclr-3x25 0.065 0.006 0.011 0.010 mapclr-3x29 0.394 0.006 0.012 0.011 mapclr-3x30 0.342 0.007 0.012 0.011 This leaves the NMR check, which ensures the set returned by our algorithm satisfies all OLON rules in P. However, we know this is the case, as the subset of positive literals in the CHS is equal to X. Because X is a valid answer set of P, there cannot be any rule in P which renders X invalid, and thus the NMR check must be satisfiable by a set of literals containing X. We also know that the NMR check will not add additional positive literals to the CHS, as any rules able to succeed would be present in P' and thus present in LFP(P'). Therefore any valid answer set X of a program P must succeed if posed as a query for top-down execution of P. Thus our top-down algorithm is complete with respect to the GL method.

6.

Performance Results

The goal-directed method described in this paper has been implemented in our system Galliwasp. In addition to the goal-directed method presented here, Galliwasp incorporates various other techniques to improve performance, including incremental enforcement of the NMR check [19]. Tables 1, 2 and 3 give performance results for some example programs. For the purpose of comparison, results for clasp, cmodels and smodels are also given. The Galliwasp system consists of two programs, a compiler and an interpreter. The times given are for our interpreter using

a compiled program and for the other solvers reading a program grounded by lparse. Neither compilation nor grounding times are factored into the results. A timeout of 600 seconds was enforced, with the instances which timed out listed as N/A in the tables. As these results demonstrate, our goal-directed method is practical and can be efficiently implemented. While additional performance increases are possible, the Galliwasp interpreter is already significantly faster than smodels in almost every case and comparable to clasp and cmodels in most cases.

implementation in a top-down fashion [1, 23, 24]. However, their approach is to modify stable model semantics so that the property of relevance is restored [23]. For this modified semantics, goaldirected procedures have been designed [24]. In contrast, our goal is to stay faithful to stable model semantics and answer set programming.

8.

Conclusions

7.

Discussion and Related Work

There are many advantages of top-down goal-directed execution of answer set programs, the main one being that it paves the way to answer set programming with predicates. The first step is to extend our method to datalog answer set programs, i.e., programs that allow only constants and variables as arguments in the predicates they contain [20]. Another advantage of goal-directed execution is that answer set programming can be made to work more naturally with other extensions that have been developed within logic programming, such as constraint programming, abduction, parallelism, probabilistic reasoning, etc. This leads to more sophisticated applications. Timed planning, i.e., planning in the presence of real-time constraints, is one such example [2]. With respect to related work, a top-down, goal-directed execution strategy for ASP has been the aim of many researchers in the past. Descriptions of some of these efforts can be found in [1, 4, 5, 8, 9, 15, 23, 24, 26]. The strategy presented in this paper is based on one presented by several of this paper's authors in previous work [14, 21]. However, the strategy presented in those works was limited to call-consistent or order-consistent programs. While the possibility of expansion to arbitrary ASP programs was mentioned, it was not expanded upon, and the proofs of soundness and completeness covered only the restricted cases [21]. A query-driven procedure for computing answer sets via an abductive proof procedure has been explored [7, 16]: a consistency check via integrity constraints is done before a negated literal is added to the answer set. However, "this procedure is not always sound with respect to the above abductive semantics of NAF" [16]. Alferes et al [1] have worked in a similar direction, though this is done in the context of abduction and again goal-directedness of ASP is not the main focus. Gebser and Schaub have developed a tableau based method which can be regarded as a step in this direction, however, the motivation for their work is completely different [9]. Bonatti, Pontelli and Tran [5] have proposed credulous resolution, an extension of earlier work of Bonatti [4], that extends SLD resolution for ASP. However, they place restrictions on the type of programs allowed and the type of queries allowed. Their method can be regarded as allowing coinductive success to be inferred only for negated goals. Thus, given query :- p and program P1, the execution will look as follows: p  not q  not not p  not q  success. Compared to our method, their method performs extra work. For example, if rule P1.1 is changed to p :big goal, not q, then big goal will be executed twice. The main problem in their method is that since it does not take coinduction for positive goals into account, knowing when to succeed inductively and when to succeed coinductively is undecidable. For this reason, their method works correctly only for a limited class of answer set programs (for example, answers to negated queries such ?-not p cannot be computed in a top-down manner). In contrast, our goal-directed method works correctly for all types of answer set programs and all types of queries. Pereira's group has done significant work on defining semantics for normal logic programs and implementing them, including

The main contribution of our paper is to present a practical, topdown method for goal-directed execution of Answer Set programs along with proofs of soundness and completeness. Our method stays faithful to ASP, and works for arbitrary answer set programs as well as arbitrary queries. Other methods in the literature either change the semantics, or work for only restricted programs or queries. Our method achieves this by relying on the coinductive logic programming paradigm. Details of our method were presented, along with proofs of soundness and correctness, and some preliminary performance results. A goal-directed procedure has many advantages, the main one being that execution of answer set programs does not have to be restricted to only finitely groundable ones. Our work thus paves the way for developing execution procedures for ASP over predicates. A goal-directed strategy permits an easier integration with other extensions of logic programming, which in turn makes it possible to develop more interesting applications of ASP and non-monotonic reasoning. Our current work is focused on refining our implementation to improve efficiency and add support for features such as constraints and predicates.

9.

Acknowledgments

Thanks to Michael Gelfond, Vladimir Lifschitz, Enrico Pontelli and Feliks Klu zniak for discussions and feedback.

References
[1] J. J. Alferes, L. M. Pereira, and T. Swift. Abduction in Well-Founded Semantics and Generalized Stable Models via Tabled Dual Programs. Theory and Practice of Logic Programming, 4:383428, July 2004. [2] A. Bansal. Towards Next Generation Logic Programming Systems. PhD thesis, University of Texas at Dallas, 2007. [3] C. Baral. Knowledge Representation, Reasoning and Declarative Problem Solving. Cambridge University Press, 2003. [4] P. Bonatti. Resolution for Skeptical Stable Model Semantics. Journal of Automated Reasoning, 27:391421, November 2001. [5] P. A. Bonatti, E. Pontelli, and T. C. Son. Credulous Resolution for Answer Set Programming. In Proceedings of the 23rd national conference on Artificial Intelligence - Volume 1, AAAI'08, pages 418 423. AAAI Press, 2008. [6] J. Dix. A Classification Theory of Semantics of Normal Logic Programs: II. Weak Properties. Fundamenta Informaticae, 22:257288, 1995. [7] K. Eshghi and R. A. Kowalski. Abduction Compared with Negation by Failure. In ICLP, ICLP'89, pages 234254, 1989. [8] J. Fern andez and J. Lobo. A Proof Procedure for Stable Theories. In CS-TR-3034, Computer Science Technical Report Series. University of Maryland, 1993. [9] M. Gebser and T. Schaub. Tableau Calculi for Answer Set Programming. In Proceedings of the 22nd international conference on Logic Programming, ICLP'06, pages 1125. Springer-Verlag, 2006. [10] M. Gebser, B. Kaufmann, A. Neumann, and T. Schaub. Clasp: A Conflict-Driven Answer Set Solver. In Proceedings of the 9th international conference on Logic Programming and Nonmonotonic Reasoning, LPNMR'07, pages 260265. Springer-Verlag, 2007. [11] M. Gelfond. Representing Knowledge in A-Prolog. In Computational Logic: Logic Programming and Beyond, Essays in Honour of Robert A. Kowalski, Part II, pages 413451. Springer-Verlag, 2002.

[12] M. Gelfond and V. Lifschitz. The Stable Model Semantics for Logic Programming. In Proceedings of the Fifth international conference on Logic Programming, pages 10701080. MIT Press, 1988. [13] E. Giunchiglia, Y. Lierler, and M. Maratea. SAT-Based Answer Set Programming. In Proceedings of the 19th national conference on Artifical Intelligence, AAAI'04, pages 6166. AAAI Press, 2004. [14] G. Gupta, A. Bansal, R. Min, L. Simon, and A. Mallya. Coinductive Logic Programming and Its Applications. In Proceedings of the 23rd international conference on Logic Programming, ICLP'07, pages 27 44. Springer-Verlag, 2007. [15] A. Kakas and F. Toni. Computing Argumentation in Logic Programming. Journal of Logic and Computation, 9(4):515562, 1999. [16] A. C. Kakas, R. A. Kowalski, and F. Toni. Abductive Logic Programming. Journal of Logic and Computation, 2(6):719770, 1992. [17] N. Leone, G. Pfeifer, and W. Faber. DLV. http://www.dbai. tuwien.ac.at/proj/dlv. [18] J. Lloyd. Foundations of Logic Programming. Symbolic Computation: Artificial Intelligence. Springer-Verlag, 1987. [19] K. Marple and G. Gupta. Galliwasp: A Goal-Directed Answer Set Solver. In Proceedings of the 22nd international symposium on Logicbased Program Synthesis and Transformation, LOPSTR '12, pages 8599. Katholieke Universiteit Leuven, 2012. [20] R. Min. Predicate Answer Set Programming with Coinduction. PhD thesis, University of Texas at Dallas, 2010. [21] R. Min, A. Bansal, and G. Gupta. Towards Predicate Answer Set Programming via Coinductive Logic Programming. In AIAI, pages 499508. Springer, 2009. [22] I. Niemel a and P. Simons. Smodels - An Implementation of the Stable Model and Well-Founded Semantics for Normal Logic Programs. In Logic Programming And Nonmonotonic Reasoning, volume 1265 of Lecture Notes in Computer Science, pages 420429. Springer-Verlag, 1997. [23] L. Pereira and A. Pinto. Revised Stable Models - A Semantics for Logic Programs. In Progress in Artificial Intelligence, volume 3808 of Lecture Notes in Computer Science, pages 2942. Springer-Verlag, 2005. [24] L. Pereira and A. Pinto. Layered Models Top-Down Querying of Normal Logic Programs. In Practical Aspects of Declarative Languages, volume 5418 of Lecture Notes in Computer Science, pages 254268. Springer-Verlag, 2009. [25] K. Sagonas, T. Swift, and D. Warren. XSB as an Efficient Deductive Database Engine. ACM SIGMOD Record, 23(2):442453, 1994. [26] Y. Shen, J. You, and L. Yuan. Enhancing Global SLS-Resolution with Loop Cutting and Tabling Mechanisms. Theoretical Computer Science, 328(3):271287, 2004. [27] L. Simon. Extending Logic Programming with Coinduction. PhD thesis, University of Texas at Dallas, 2006.

In the program R2, the meaning of the query ?- stream(X) is semantically null under standard logic programming. In the coLP paradigm the declarative semantics of the predicate stream/1 above is given in terms of infinitary Herbrand (or co-Herbrand) universe, infinitary Herbrand (or co-Herbrand) base [18], and maximal models (computed using greatest fixed-points) [27]. The operational semantics under coinduction is as follows [27]: a pred) succeeds if it unifies with one of its ancestor calls. icate call p(t Thus, every time a call is made, it has to be remembered. This set of ancestor calls constitutes the coinductive hypothesis set (CHS). Under co-LP, infinite rational answers can be computed, and infinite rational terms are allowed as arguments of predicates. Infinite terms are represented as solutions to unification equations and the occurs check is omitted during the unification process: for example, X = [1 | X] represents the binding of X to an infinite list of 1's. Thus, in co-SLD resolution, given a single clause p([ 1 | X ]) :- p(X). The query ?- p(A) will succeed in two resolution steps with the answer A = [1 | A], which is a finite representation of the infinite answer A = [1, 1, 1, ....]. Under coinductive interpretation of R2, the query ?- stream(X) produces all infinite sized streams as answers, e.g., X = [1 | X], X = [1, 2 | X ], etc. Thus, the semantics of R2 is not null, but proofs may be of infinite length. If we take a coinductive interpretation of program R1, then we get all finite and infinite streams as answers to the query ?- stream(X).

B.

Detailed Execution Example

We now present a larger, more complex example of execution using our goal-directed method. Consider program A1: p q r q ::::not not not not q. r. p. p. ... ... ... ... Rule A1.1 Rule A1.2 Rule A1.3 Rule A1.4

Rules A1.1, A1.2 and A1.3 are OLON rules, as calls to propositions p, q, and r in the heads of these rules lead to recursive calls to p, q and r respectively that are in the scope of odd numbers of negations. A1.1 is also an ordinary rule, since in conjunction with rule A1.4, a call to p resolved via rule A1.1 will lead to a call to p in rule A1.4 that is in the scope of an even number of negations. Thus, the nmr check rule can be defined as: nmr check :- not chk p, not chk q, not chk r. chk p :- not p, not q. chk q :- not q, not r. chk r :- not r, not p. The duals of the above rules are as follows: not not not not not not p :- q. ... q :- r, p. ... r :- p. ... chk p :- p; q. . . . chk q :- q; r. . . . chk r :- r; p. . . . Rule A1.6 Rule A1.7 Rule A1.8 Rule A1.9 Rule A1.10 Rule A1.11

A.

Co-SLD Resolution

As mentioned in the introduction, our goal-directed method relies on coinductive logic programming (co-LP) [14]. Co-SLD resolution, the operational semantics of coinduction, is briefly described below. The semantics is limited to regular proofs, i.e., those cases where the infinite behavior is obtained by infinite repetition of a finite number of finite behaviors. Consider the logic programming definition of a stream (list) of numbers as in program R1 below: stream([]). stream([H|T]) :- number(H), stream(T). Under SLD resolution, the query ?- stream(X) will systematically produce all finite streams one by one starting from the [] stream. Suppose now we remove the base case and obtain the program R2: stream([H|T]) :- number(H), stream(T).

Negated calls are resolved using these dual rules. Now the query q will be extended to q, nmr chk and executed as follows: ::::chk. CHS = {}; Rule A1.2 CHS = {q}; Rule A1.8 nmr chk. chk. CHS = {q, not r}; Rule A1.1 nmr chk. CHS = {q, not r} fail: backtrack to step 1 :- q, nmr chk. CHS = {}; Rule A1.4 :- not p, nmr chk. CHS = {q, not p}; Rule A1.6 q, nmr not r, p, nmr not q,

:- q, nmr chk. ::-

:-

:-

::-

:-

CHS = {q, not p} coinductive success nmr chk. CHS = {q, not p} execution of q finished not chk p, not chk q, not chk r. CHS = {q, not p} nmr chk rule (p ; q), not chk q, not chk r. CHS = {q, not p} not p is in CHS q, not chk q, not chk r. CHS = {q, not p} coinductive success for q not chk q, not chk r. CHS = {q, not p}; Rule A1.10 (q ; r), not chk r. CHS = {q, not p} coinductive success for q not chk r. CHS = {q, not p}; Rule A1.11 CHS = {q, not p, r}; Rule A1.3 CHS = {q, not p, r} coinductive success for not p success. answer set is {q, not p, r}

:- r ; p. :- not p ; p. :.

University of Texas at Dallas

Coinductive Logic Programming and its Applications
Gopal Gupta Luke Simon, Ajay Bansal, Ajay Mallya, Richard Min.
Applied Logic, Programming-Languages and Systems (ALPS) Lab The University of Texas at Dallas, Richardson, Texas, USA
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 1

University of Texas at Dallas

Circular Phenomena in Comp. Sci.
 Circularity has dogged Mathematics and Computer Science ever since Set Theory was first developed:
 The well known Russell's Paradox:
 R = { x | x is a set that does not contain itself} Is R contained in R? Yes and No

 All these paradoxes involve self-reference through some type of negation  Russell put the blame squarely on circularity and sought to ban it from scientific discourse:

 Liar Paradox: I am a liar  Hypergame paradox (Zwicker & Smullyan)

``Whatever involves all of the collection must not be one of the collection" -- Russell 1908
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 2

University of Texas at Dallas

Circularity in Computer Science
 Following Russell's lead, Tarski proposed to ban selfreferential sentences in a language  Rather, have a hierarchy of languages  All this changed with Kripke's paper in 1975 who showed that circular phenomenon are far more common and circularity can't simply be banned.  Circularity has been banned from automated theorem proving and logic programming through the occurs check rule:  What if we allowed such unification to proceed (as LP systems always did for efficiency reasons)?
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 3

An unbound variable cannot be unified with a term containing that variable

University of Texas at Dallas

Circularity in Computer Science
 If occurs check is removed, we'll generate circular (infinite) structures:  Such structures, of course, arise in computing (circular linked lists), but banned in logic/LP.  Subsequent LP systems did allow for such circular structures (rational terms), but they only exist as data-structures, there is no proof theory to go along with it.
 One can hold the data-structure in memory within an LP execution, but one can't reason about it.  X = [1,2,3 | X]

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 4

University of Texas at Dallas

Circularity in Everyday Life
 Circularity arises in every day life
 Most natural phenomenon are cyclical
 Cyclical movement of the earth, moon, etc.  Our digestive system works in cycles

 Social interactions are cyclical:
 Conversation = (1st speaker, (2nd Speaker, Conversation)  Shared conventions are cyclical concepts

 Numerous other examples can be found elsewhere (Barwise & Moss 1996)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 5

University of Texas at Dallas

Circularity in Computer Science
 Circular phenomenon are quite common in Computer Science:
       Circular linked lists Graphs (with cycles) Controllers (run forever) Bisimilarity Interactive systems Automata over infinite strings/Kripke structures Perpetual processes

 Logic/LP not equipped to model circularity
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 6

University of Texas at Dallas

Coinduction
 Circular structures are infinite structures
X = [1, 2 | X] is logically speaking X = [1, 2, 1, 2, ....]

 Proofs about their properties are infinite-sized  Coinduction is the technique for proving these properties
 first proposed by Peter Aczel in the 80s

 Systematic presentation of coinduction & its application to computing, math. and set theory: "Vicious Circles" by Moss and Barwise (1996)  Our focus: inclusion of coinductive reasoning techniques into LP and theorem proving
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 7

University of Texas at Dallas

Induction vs Coinduction
 Induction is a mathematical technique for finitely reasoning about an infinite (countable) no. of things.  Examples of inductive structures:
 Naturals: 0, 1, 2, ...  Lists: [ ], [X], [X, X], [X, X, X], ...

 3 components of an inductive definition:
(1) Initiality, (2) iteration, (3) minimality  for example, the set of lists is specified as follows: [ ]  an empty list is a list (initiality initiality) initiality [H | T] is a list if T is a list and H is an element (iteration iteration) iteration nothing else is a list (minimality minimality) minimality
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 8

University of Texas at Dallas

Induction vs Coinduction

 

Coinduction is a mathematical technique for (finitely) reasoning about infinite things.





(1) iteration, (2) maximality  for example, for a list: [ H | T ] is a list if T is a list and H is an element (iteration iteration). iteration Maximal set that satisfies the specification of a list.  This coinductive interpretation specifies all infinite sized lists
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 9

2 components of a coinductive definition:

Mathematical dual of induction If all things were finite, then coinduction would not be needed. Perpetual programs, automata over infinite strings

University of Texas at Dallas

Example: Natural Numbers
  (S) = { 0 }  { succ(x) | x  S }  N = 
 where  is least fixed-point.

 aka "inductive definition"
 Let N be the smallest set such that
 0N  x  N implies x + 1  N

 Induction corresponds to Least Fix Point (LFP) interpretation.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 10

University of Texas at Dallas

Example: Natural Numbers and Infinity
  (S) = { 0 }  { succ(x) | x  S }   unambiguously defines another set  N' =  = N  {  }

 Coinduction corresponds to Greatest Fixed Point (GFP) interpretation.

  = succ( succ( succ( ... ) ) ) = succ(  ) =  + 1  where  is a greatest fixed-point

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 11

University of Texas at Dallas

Mathematical Foundations
 Duality provides a source of new mathematical tools that reflect the sophistication of tried and true techniques. Definition
Least fixed point Greatest fixed point

Proof
Induction Coinduction

Mapping
Recursion Corecursion

 Co-recursion: recursive def'n without a base case
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 12

University of Texas at Dallas

Applications of Coinduction
       model checking bisimilarity proofs lazy evaluation in FP reasoning with infinite structures perpetual processes cyclic structures operational semantics of "coinductive logic programming"  Type inference systems for lazy functional languages
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 13

University of Texas at Dallas

Inductive Logic Programming
 Logic Programming
 is actually inductive logic programming.  has inductive definition.  useful for writing programs for reasoning about finite things: - data structures - properties

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 14

University of Texas at Dallas

Infinite Objects and Properties
 Traditional logic programming is unable to reason about infinite objects and/or properties.  (The glass is only half-full)  Example: perpetual binary streams
 traditional logic programming cannot handle bit(0). bit(1). bitstream( [ H | T ] ) :- bit( H ), bitstream( T ). |?- X = [ 0, 1, 1, 0 | X ], bitstream( X ).

 Goal: Combine traditional LP with coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 15

University of Texas at Dallas

Overview of Coinductive LP
 Coinductive Logic Program is
a definite program with maximal co-Herbrand model declarative semantics.

 Declarative Semantics: across the board dual of traditional LP:
    greatest fixed-points terms: co-Herbrand universe Uco(P) atoms: co-Herbrand base Bco(P) program semantics: maximal co-Herbrand model Mco(P).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 16

University of Texas at Dallas

Coinductive LP: An Example
 Let P1 be the following coinductive program. from(x) = x cons from(x+1) :- coinductive from/2. from( N, [ N | T ] ) :- from( s(N), T ). |?- from( 0, X ).  co-Herbrand Universe: Uco(P1) = N    L where N=[0, s(0), s(s(0)), ... ], ={ s(s(s( . . . ) ) ) }, and L is the the set of all finite and infinite lists of elements in N,  and L.  co-Herbrand Model: Mco(P1)={ from(t, [t, s(t), s(s(t)), ... ]) | t  Uco(P1) }  from(0, [0, s(0), s(s(0)), ... ])  Mco(P1) implies the query holds  Without "coinductive" declaration of from, Mco(P1')= This corresponds to traditional semantics of LP with infinite trees.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 17

University of Texas at Dallas

Operational Semantics: co-SLD
 nondeterministic state transition system  states are pairs of
 a finite list of syntactic atoms [resolvent] (as in Prolog)  a set of syntactic term equations of the form x = f(x) or x = t
 For a program p :- p. => the query |?- p. will succeed.  p( [ 1 | T ] ) :- p( T ). => |?- p(X) to succeed with X= [ 1 | X ].

 transition rules
 definite clause rule  "coinductive hypothesis rule"
 if a coinductive goal Q is called, and Q unifies with a call made earlier (e.g., P :- Q) then Q succeeds.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 18

University of Texas at Dallas

Correctness
 Theorem (soundness). If atom A has a successful co-SLD derivation in program P, then E(A) is true in program P, where E is the resulting variable bindings for the derivation.  Theorem (completeness). If A  Mco(P) has a rational proof, then A has a successful coSLD derivation in program P.
 Completeness only for rational/regular proofs
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 19

University of Texas at Dallas

Implementation
 Search strategy: hypothesis-first, leftmost, depth-first  Meta-Interpreter implementation.
query(Goal) :- solve([],Goal). solve(Hypothesis, (Goal1,Goal2)) :solve( Hypothesis, Goal1), solve(Hypothesis,Goal2). solve( _ , Atom) :- builtin(Atom), Atom. solve(Hypothesis,Atom):- member(Atom, Hypothesis). solve(Hypothesis,Atom):- notbuiltin(Atom), clause(Atom,Atoms), solve([Atom|Hypothesis],Atoms).

 A more efficient implem. atop YAP also available
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 20

University of Texas at Dallas

Example: Number Stream
:- coinductive stream/1. stream( [ H | T ] ) :- num( H ), stream( T ). num( 0 ). num( s( N ) ) :- num( N ). |?- stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ).
1. 2. 3.

Answers: T = [ 0, s(0), s(s(0)), s(s(0)) | T ] T = [ 0, s(0), s(s(0)), s(0), s(s(0)) | T ] T = [ 0, s(0), s(s(0)) | T ] . . . T = [ 0, s(0), s(s(0)) | X ] (where X is any rational list of numbers.)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 21

MEMO: stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( s ( 0 ) ) | T ] )

University of Texas at Dallas

Example: Append
:- coinductive append/3. append( [ ], X, X ). append( [ H | T ], Y, [ H | Z ] ) :- append( T, Y, Z ).
|?- Y = [ 4, 5, 6 | Y ], append( [ 1, 2, 3 ], Y, Z). Answer: Z = [ 1, 2, 3 | Y ], Y=[ 4, 5, 6 | Y] |?- X = [ 1, 2, 3 | X ], Y = [ 3, 4 | Y ], append( X, Y, Z). Answer: Z = [ 1, 2, 3 | Z ]. |?- Z = [ 1, 2 | Z ], append( X, Y, Z ). Answer: X = [ ], Y = [ 1, 2 | Z ] ; X = [1, 2 | X], Y = _ X = [ 1 ], Y = [ 2 | Z ] ; X = [ 1, 2 ], Y = Z; .... ad infinitum
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 22

University of Texas at Dallas

Example: Comember
member(H, [ H | T ]). member(H, [ X | T ]) :- member(H, T). ?- L = [1,2 | L], member(3, L) succeeds. Instead: :- coinductive comember/2. %drop/3 is inductive comember(X, L) :- drop(X, L, R), comember(X, R). drop(H, [ H | T ], T). drop(H, [ X | T ], T1) :- drop(H, T, T1). ?- X=[ 1, 2, 3 | X ], comember(2,X). Answer: yes. ?- X=[ 1, 2, 3, 1, 2, 3], comember(2, X). Answer: no. ?- X=[1, 2, 3 | X], comember(Y, X). Answer: Y = 1; Y = 2; Y = 3; ?- X = [1,2 | X], comember(3, X). Answer: no

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 23

University of Texas at Dallas

Example: Sieve of Eratosthenes
 Lazy evaluation can be elegantly incorporated in LP :- coinductive sieve/2, filter/3, comember/2. primes(X) :- generate_infinite_list(I),sieve(I,L),comember(X,L). sieve([H|T],[H,R]) :- filter(H,T,F),sieve(F,R). filter(H,[ ],[ ]). filter(H,[K | T],[K | T1]):- R is K mod H, R>0,filter(H,T,T1). filter(H,[K | T],T1) :- 0 is K mod H, filter(H,T,T1). :-coinductive int/2 int(X,[X | Y]) :- X1 is X+1, int(X1,Y). generate_infinite_list(I) :- int(2,I).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 24

University of Texas at Dallas

Co-Logic Programming
 combines both halves of logic programming:  syntactically identical to traditional logic programming, except predicates are labeled:  and stratification restriction enforced where:
 Inductive, or  coinductive  traditional logic programming  coinductive logic programming

 Implementation on top of YAP available.

 inductive and coinductive predicates cannot be mutually recursive. e.g., p :- q. q :- p. Program rejected, if p coinductive & q inductive
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 25

University of Texas at Dallas

Application: Model Checking
 automated verification of hardware and software systems  -automata
 accept infinite strings  accepting state must be traversed infinitely often

 requires computation of lfp and gfp  co-logic programming provides an elegant framework for model checking  traditional LP works for safety property (that is based on lfp) in an elegant manner, but not for liveness .

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 26

University of Texas at Dallas

Verification of Properties

 Types of properties: safety and liveness  Search for counter-example

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 27

University of Texas at Dallas

Safety versus Liveness
 Safety
 "nothing bad will happen"  naturally described inductively  straightforward encoding in traditional LP

 liveness
    "something good will eventually happen" dual of safety naturally described coinductively straightforward encoding in coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 28

University of Texas at Dallas

Finite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). automata([ ], St) :- final(St). trans(s0, a, s1). trans(s3, d, s0). trans(s1, b, s2). trans(s2, 3, s0). trans(s2, c, s3). final(s2).

?- automata(X,s0). X=[ a, b]; X=[ a, b, e, a, b]; X=[ a, b, e, a, b, e, a, b];

...... ...... ......

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 29

University of Texas at Dallas

Infinite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). trans(s0,a,s1). trans(s3,d,s0). trans(s1,b,s2). trans(s2,3,s0). trans(s2,c,s3). final(s2).

?- automata(X,s0). X=[ a, b, c, d | X ]; X=[ a, b, e | X ];

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 30

University of Texas at Dallas

Verifying Liveness Properties
 Verifying safety properties in LP is relatively easy: safety modeled by reachability  Accomplished via tabled logic programming  Verifying liveness is much harder: a counterexample to liveness is an infinite trace  Verifying liveness is transformed into a safety check via use of negations in model checking and tabled LP
 Considerable overhead incurred

 Co-LP solves the problem more elegantly:
 Infinite traces that serve as counter-examples are easily produced as answers
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 31

University of Texas at Dallas

Verifying Liveness Properties
 Consider Safety:
 Question: Is an unsafe state, Su, reachable (safe)?  If answer is yes, the path to Su is the counter-ex.

 Consider Liveness, then dually
 Question: Is a state, D, that should be dead, live?  If answer is yes, the infinite path containing D is the counter example
 Co-LP will produce this infinite path as the answer

 Checking for liveness is just as easy as checking for safety
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 32

University of Texas at Dallas

Counter

sm1(N,[sm1|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. s0(N,[s0|T]) :- N1 is N+1 mod 4, s1(N1,T), N1>=0. s1(N,[s1|T]) :- N1 is N+1 mod 4, s2(N1,T), N1>=0. s2(N,[s2|T]) :- N1 is N+1 mod 4, s3(N1,T), N1>=0. s3(N,[s3|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. ?- sm1(-1,X),comember(sm1,X). No. (because sm1 does not occur in X infinitely often).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 33

University of Texas at Dallas

Nested Finite and Infinite Automata
:- coinductive state/2. state(s0, [s0,s1 | T]):- enter, work, state(s1,T). state(s1, [s1 | T]):- exit, state(s2,T). state(s2, [s2 | T]):- repeat, state(s0,T). state(s0, [s0 | T]):- error, state(s3,T). state(s3, [s3 | T]):- repeat, state(s0,T). work. enter. repeat. exit. error. work :- work. |?- state(s0,X), absent(s2,X). X=[ s0, s3 | X ]
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 34

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

Timed Automata  -automata w/ time constrained transitions & stopwatches  straightforward encoding into CLP(R) + Co-LP

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 35

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
:- use_module(library(clpr)). :- coinductive driver/9.

train(X, up, X, T1,T2,T2). % up=idle train(s0,approach,s1,T1,T2,T3) :- {T3=T1}. train(s1,in,s2,T1,T2,T3):-{T1-T2>2,T3=T2} train(s2,out,s3,T1,T2,T3). train(s3,exit,s0,T1,T2,T3):-{T3=T2,T1-T2<5}. train(X,lower,X,T1,T2,T2). train(X,down,X,T1,T2,T2). train(X,raise,X,T1,T2,T2).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 36

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
contr(s0,approach,s1,T1,T2,T1).

contr(s1,lower,s2,T1,T2,T3):- {T3=T2, T1-T2=1}.

contr(s2,exit,s3,T1,T2,T1). contr(s3,raise,s0,T1,T2,T2):-{T1-T2<1}. contr(X,in,X,T1,T2,T2). contr(X,up,X,T1,T2,T2). contr(X,out,X,T1,T2,T2). contr(X,down,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 37

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

gate(s0,lower,s1,T1,T2,T3):- {T3=T1}. gate(s1,down,s2,T1,T2,T3):- {T3=T2,T1-T2<1}. gate(s2,raise,s3,T1,T2,T3):- {T3=T1}.
gate(s3,up,s0,T1,T2,T3):- {T3=T2,T1-T2>1,T1-T2<2 }.

gate(X,approach,X,T1,T2,T2). gate(X,in,X,T1,T2,T2). gate(X,out,X,T1,T2,T2). gate(X,exit,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 38

University of Texas at Dallas

Verification of Real-Time Systems
:- coinductive driver/9. driver(S0,S1,S2, T,T0,T1,T2, [ X | Rest ], [ (X,T) | R ]) :train(S0,X,S00,T,T0,T00), contr(S1,X,S10,T,T1,T10), gate(S2,X,S20,T,T2,T20), {TA > T}, driver(S00,S10,S20,TA,T00,T10,T20,Rest,R). |?- driver(s0,s0,s0,T,Ta,Tb,Tc,X,R). R=[(approach,A), (lower,B), (down,C), (in,D), (out,E), (exit,F), (raise,G), (up,H) | R ], X=[approach, lower, down, in, out, exit, raise, up | X] ; R=[(approach,A),(lower,B),(down,C),(in,D),(out,E),(exit,F),(raise,G), (approach,H),(up,I)|R], X=[approach,lower,down,in,out,exit,raise,approach,up | X] ;
% where A, B, C, ... H, I are the corresponding wall clock time of events generated.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 39

University of Texas at Dallas

Goal-directed execution of ASP
      Answer set programming (ASP) is a popular formalism for non monotonic reasoning Applications in real-world reasoning, planning, etc. Semantics given via lfp of a residual program obtained after "Gelfond-Lifschitz" transform Popular implementations: Smodels, DLV, etc.
1. No goal-directed execution strategy available 2. ASP limited to only finitely groundable programs

Co-logic programming solves both these problems. Also provides a goal-directed method to check if a proposition is true in some model of a prop. formula
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 40

University of Texas at Dallas

   

Most of the time, given a theory, we are interested in knowing if a particular goal is true or not. Top down goal-directed execution provides operational semantics (important for usability) Execution more efficient. Why check the consistency of the whole knowledgebase?
 Tabled LP vs bottom up Deductive Deductive Databases

Why Goal-directed ASP?

 

Most practical examples anyway add a constraint to force the answer set to contain a certain goal. Answer sets of non-finitely groundable programs computable & Constraints incorporated in Prolog style.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 41

 Inconsistency in some unrelated part will scuttle the whole system  E.g. Zebra puzzle: :- not satisfied.

University of Texas at Dallas

Negation in Co-LP
 Given a clause such as ?- p. fails coinductively when not p is encountered  To incorporate negation in coinductive reasoning, need a negative coinductive hypothesis rule:
p :- q, not p.

  Answer set programming makes the "glass completely full" by taking into account failing computations: 

 In the process of establishing not(p), if not(p) is seen again in the resolvent, then not(p) succeeds Also, not not p reduces to p.

 p :- q, not p. is consistent if p = false and q = false However, this takes away monotonicity: q can be constrainted to false, causing q to be withdrawn, if it was established earlier.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 42

University of Texas at Dallas

ASP
 Consider the following program, A:
p :- not q. t. r :- t, s. q :- not p. s. A has 2 answer sets: {p, r, t, s} & {q, r, t, s}.

 Now suppose we add the following rule to A:  Gelfond-Lifschitz Method:
h :- p, not h. (falsify p) Only one answer set remains: {q, r, t, s}

 Given an answer set S, for each p  S, delete all rules whose body contains "not p";  delete all goals of the form "not q" in remaining rules  Compute the least fix point, L, of the residual program  If S = L, then S is an answer set

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 43

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, A':
p :- not q. q :- not p, r. t. s.

 Separate into constraint and non-constraint rules: only 1 constraint rule in this case.  Execute the query under co-LP, candidate answer sets will be generated.  Keep the ones not rejected by the constraints.  Suppose the query is ?- q. Execution: q not p, r not not q, r q, r r t, s s success. Ans = {q, r, t, s}  Next, we need to check that constraint rules will not reject the generated answer set.
 (it doesn't in this case)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 44

r :- t, s. h :- p, not h.

University of Texas at Dallas

Goal-directed ASP
 In general, for the constraint rules of p as head, p1:- B1. p2:- B2. ... pn :- Bn., generate rule(s) of the form: chk_p1 :- not(p1), B1. chk_p2 :- not(p2), B2. ... chk_pn :- not(p), Bn.  Generate: nmr_chk :- not(chk_p1), ... , not(chk_pn).  For each pred. definition, generate its negative version: not_p :- not(B1), not(B2), ... , not(Bn).  If you want to ask query Q, then ask ?- Q, nmr_chk.  Execution keeps track of atoms in the answer set (PCHS) and atoms not in the answer set (NCHS).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 45

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, P1:
(i) p ::- not q. (ii) q:(iii) r :q:- not r. :- not p. P1 has 1 answer set: {q, r}. (iv) q ::- not p.

 Separate into: 3 constraint rules (i, ii, iii) 2 non-constraint rules (i, iv).

Suppose the query is ?- r. Expand as in co-LP: r not p not not q q( not r fail, backtrack) not p success. Ans={r, q} which satisfies the constraint rules of nmr_chk.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 46

p :- not(q). q :- not(r). r :- not(p). q :- not(p). chk_p :- not(p), not(q). chk_q :- not(q), not(r). chk_r :- not(r), not(p). nmr_chk :- not(chk_p), not(chk_q), not(chk_r). not_p :- q. not_q :- r, p. not_r :- p.

University of Texas at Dallas

Next Generation of LP System
 Lot of research in LP resulting in advances:
 CLP, Tabled LP, Parallelism, Andorra, ASP, now co-LP

 However, no "one stop shop" system  Dream: build this "one stop shop" system
ASP Or-Parallelism Tabled LP Next Generation Prolog System CLP Andorra

Rule selection

Goal selection
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 47

University of Texas at Dallas

Related Publications
1. L. Simon, A. Mallya, A. Bansal, and G. Gupta. Coinductive logic programming. In ICLP'06 . 2. L. Simon, A. Bansal, A. Mallya, and G. Gupta. CoLogic programming: Extending logic programming with coinduction. In ICALP'07. 3. ICLP'07 Proceedings (this tutorial) 4. A. Bansal, R. Min, G. Gupta. Goal-directed Execution of ASP. Internal Report, UT Dallas 5. R. Min, A. Bansal, G. Gupta. Goal-directed Execution of ASP with General Predicates. Forthcoming. 6. A. Bansal, R. Min, G. Gupta. Resolution Theorem Proving with Coinduction. Internal Report, UT Dallas
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 48

University of Texas at Dallas

Conclusion
 Circularity is a common concept in everyday life and computer science:  Logic/LP is unable to cope with circularity  Solution: introduce coinduction in Logic/LP
 dual of traditional logic programming  operational semantics for coinduction  combining both halves of logic programming

 applications to verification, non monotonic reasoning, negation in LP, web services, theorem proving, propositional satisfiability.  Acknowledgemt.: V. Santos Costa, R. Rocha, F. Silva
(for help with implementation of co-LP)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 49

University of Texas at Dallas

Coinductive Logic Programming and its Applications
Gopal Gupta Luke Simon, Ajay Bansal, Ajay Mallya, Richard Min.
Applied Logic, Programming-Languages and Systems (ALPS) Lab The University of Texas at Dallas, Richardson, Texas, USA
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 1

University of Texas at Dallas

Circular Phenomena in Comp. Sci.
 Circularity has dogged Mathematics and Computer Science ever since Set Theory was first developed:
 The well known Russell's Paradox:
 R = { x | x is a set that does not contain itself} Is R contained in R? Yes and No

 All these paradoxes involve self-reference through some type of negation  Russell put the blame squarely on circularity and sought to ban it from scientific discourse:

 Liar Paradox: I am a liar  Hypergame paradox (Zwicker & Smullyan)

``Whatever involves all of the collection must not be one of the collection" -- Russell 1908
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 2

University of Texas at Dallas

Circularity in Computer Science
 Following Russell's lead, Tarski proposed to ban selfreferential sentences in a language  Rather, have a hierarchy of languages  All this changed with Kripke's paper in 1975 who showed that circular phenomenon are far more common and circularity can't simply be banned.  Circularity has been banned from automated theorem proving and logic programming through the occurs check rule:  What if we allowed such unification to proceed (as LP systems always did for efficiency reasons)?
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 3

An unbound variable cannot be unified with a term containing that variable

University of Texas at Dallas

Circularity in Computer Science
 If occurs check is removed, we'll generate circular (infinite) structures:  Such structures, of course, arise in computing (circular linked lists), but banned in logic/LP.  Subsequent LP systems did allow for such circular structures (rational terms), but they only exist as data-structures, there is no proof theory to go along with it.
 One can hold the data-structure in memory within an LP execution, but one can't reason about it.  X = [1,2,3 | X]

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 4

University of Texas at Dallas

Circularity in Everyday Life
 Circularity arises in every day life
 Most natural phenomenon are cyclical
 Cyclical movement of the earth, moon, etc.  Our digestive system works in cycles

 Social interactions are cyclical:
 Conversation = (1st speaker, (2nd Speaker, Conversation)  Shared conventions are cyclical concepts

 Numerous other examples can be found elsewhere (Barwise & Moss 1996)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 5

University of Texas at Dallas

Circularity in Computer Science
 Circular phenomenon are quite common in Computer Science:
       Circular linked lists Graphs (with cycles) Controllers (run forever) Bisimilarity Interactive systems Automata over infinite strings/Kripke structures Perpetual processes

 Logic/LP not equipped to model circularity
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 6

University of Texas at Dallas

Coinduction
 Circular structures are infinite structures
X = [1, 2 | X] is logically speaking X = [1, 2, 1, 2, ....]

 Proofs about their properties are infinite-sized  Coinduction is the technique for proving these properties
 first proposed by Peter Aczel in the 80s

 Systematic presentation of coinduction & its application to computing, math. and set theory: "Vicious Circles" by Moss and Barwise (1996)  Our focus: inclusion of coinductive reasoning techniques into LP and theorem proving
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 7

University of Texas at Dallas

Induction vs Coinduction
 Induction is a mathematical technique for finitely reasoning about an infinite (countable) no. of things.  Examples of inductive structures:
 Naturals: 0, 1, 2, ...  Lists: [ ], [X], [X, X], [X, X, X], ...

 3 components of an inductive definition:
(1) Initiality, (2) iteration, (3) minimality  for example, the set of lists is specified as follows: [ ]  an empty list is a list (initiality initiality) initiality [H | T] is a list if T is a list and H is an element (iteration iteration) iteration nothing else is a list (minimality minimality) minimality
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 8

University of Texas at Dallas

Induction vs Coinduction

 

Coinduction is a mathematical technique for (finitely) reasoning about infinite things.





(1) iteration, (2) maximality  for example, for a list: [ H | T ] is a list if T is a list and H is an element (iteration iteration). iteration Maximal set that satisfies the specification of a list.  This coinductive interpretation specifies all infinite sized lists
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 9

2 components of a coinductive definition:

Mathematical dual of induction If all things were finite, then coinduction would not be needed. Perpetual programs, automata over infinite strings

University of Texas at Dallas

Example: Natural Numbers
  (S) = { 0 }  { succ(x) | x  S }  N = 
 where  is least fixed-point.

 aka "inductive definition"
 Let N be the smallest set such that
 0N  x  N implies x + 1  N

 Induction corresponds to Least Fix Point (LFP) interpretation.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 10

University of Texas at Dallas

Example: Natural Numbers and Infinity
  (S) = { 0 }  { succ(x) | x  S }   unambiguously defines another set  N' =  = N  {  }

 Coinduction corresponds to Greatest Fixed Point (GFP) interpretation.

  = succ( succ( succ( ... ) ) ) = succ(  ) =  + 1  where  is a greatest fixed-point

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 11

University of Texas at Dallas

Mathematical Foundations
 Duality provides a source of new mathematical tools that reflect the sophistication of tried and true techniques. Definition
Least fixed point Greatest fixed point

Proof
Induction Coinduction

Mapping
Recursion Corecursion

 Co-recursion: recursive def'n without a base case
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 12

University of Texas at Dallas

Applications of Coinduction
       model checking bisimilarity proofs lazy evaluation in FP reasoning with infinite structures perpetual processes cyclic structures operational semantics of "coinductive logic programming"  Type inference systems for lazy functional languages
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 13

University of Texas at Dallas

Inductive Logic Programming
 Logic Programming
 is actually inductive logic programming.  has inductive definition.  useful for writing programs for reasoning about finite things: - data structures - properties

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 14

University of Texas at Dallas

Infinite Objects and Properties
 Traditional logic programming is unable to reason about infinite objects and/or properties.  (The glass is only half-full)  Example: perpetual binary streams
 traditional logic programming cannot handle bit(0). bit(1). bitstream( [ H | T ] ) :- bit( H ), bitstream( T ). |?- X = [ 0, 1, 1, 0 | X ], bitstream( X ).

 Goal: Combine traditional LP with coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 15

University of Texas at Dallas

Overview of Coinductive LP
 Coinductive Logic Program is
a definite program with maximal co-Herbrand model declarative semantics.

 Declarative Semantics: across the board dual of traditional LP:
    greatest fixed-points terms: co-Herbrand universe Uco(P) atoms: co-Herbrand base Bco(P) program semantics: maximal co-Herbrand model Mco(P).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 16

University of Texas at Dallas

Coinductive LP: An Example
 Let P1 be the following coinductive program. from(x) = x cons from(x+1) :- coinductive from/2. from( N, [ N | T ] ) :- from( s(N), T ). |?- from( 0, X ).  co-Herbrand Universe: Uco(P1) = N    L where N=[0, s(0), s(s(0)), ... ], ={ s(s(s( . . . ) ) ) }, and L is the the set of all finite and infinite lists of elements in N,  and L.  co-Herbrand Model: Mco(P1)={ from(t, [t, s(t), s(s(t)), ... ]) | t  Uco(P1) }  from(0, [0, s(0), s(s(0)), ... ])  Mco(P1) implies the query holds  Without "coinductive" declaration of from, Mco(P1')= This corresponds to traditional semantics of LP with infinite trees.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 17

University of Texas at Dallas

Operational Semantics: co-SLD
 nondeterministic state transition system  states are pairs of
 a finite list of syntactic atoms [resolvent] (as in Prolog)  a set of syntactic term equations of the form x = f(x) or x = t
 For a program p :- p. => the query |?- p. will succeed.  p( [ 1 | T ] ) :- p( T ). => |?- p(X) to succeed with X= [ 1 | X ].

 transition rules
 definite clause rule  "coinductive hypothesis rule"
 if a coinductive goal Q is called, and Q unifies with a call made earlier (e.g., P :- Q) then Q succeeds.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 18

University of Texas at Dallas

Correctness
 Theorem (soundness). If atom A has a successful co-SLD derivation in program P, then E(A) is true in program P, where E is the resulting variable bindings for the derivation.  Theorem (completeness). If A  Mco(P) has a rational proof, then A has a successful coSLD derivation in program P.
 Completeness only for rational/regular proofs
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 19

University of Texas at Dallas

Implementation
 Search strategy: hypothesis-first, leftmost, depth-first  Meta-Interpreter implementation.
query(Goal) :- solve([],Goal). solve(Hypothesis, (Goal1,Goal2)) :solve( Hypothesis, Goal1), solve(Hypothesis,Goal2). solve( _ , Atom) :- builtin(Atom), Atom. solve(Hypothesis,Atom):- member(Atom, Hypothesis). solve(Hypothesis,Atom):- notbuiltin(Atom), clause(Atom,Atoms), solve([Atom|Hypothesis],Atoms).

 A more efficient implem. atop YAP also available
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 20

University of Texas at Dallas

Example: Number Stream
:- coinductive stream/1. stream( [ H | T ] ) :- num( H ), stream( T ). num( 0 ). num( s( N ) ) :- num( N ). |?- stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ).
1. 2. 3.

Answers: T = [ 0, s(0), s(s(0)), s(s(0)) | T ] T = [ 0, s(0), s(s(0)), s(0), s(s(0)) | T ] T = [ 0, s(0), s(s(0)) | T ] . . . T = [ 0, s(0), s(s(0)) | X ] (where X is any rational list of numbers.)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 21

MEMO: stream( [ 0, s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( 0 ), s( s ( 0 ) ) | T ] ) MEMO: stream( [ s( s ( 0 ) ) | T ] )

University of Texas at Dallas

Example: Append
:- coinductive append/3. append( [ ], X, X ). append( [ H | T ], Y, [ H | Z ] ) :- append( T, Y, Z ).
|?- Y = [ 4, 5, 6 | Y ], append( [ 1, 2, 3 ], Y, Z). Answer: Z = [ 1, 2, 3 | Y ], Y=[ 4, 5, 6 | Y] |?- X = [ 1, 2, 3 | X ], Y = [ 3, 4 | Y ], append( X, Y, Z). Answer: Z = [ 1, 2, 3 | Z ]. |?- Z = [ 1, 2 | Z ], append( X, Y, Z ). Answer: X = [ ], Y = [ 1, 2 | Z ] ; X = [1, 2 | X], Y = _ X = [ 1 ], Y = [ 2 | Z ] ; X = [ 1, 2 ], Y = Z; .... ad infinitum
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 22

University of Texas at Dallas

Example: Comember
member(H, [ H | T ]). member(H, [ X | T ]) :- member(H, T). ?- L = [1,2 | L], member(3, L) succeeds. Instead: :- coinductive comember/2. %drop/3 is inductive comember(X, L) :- drop(X, L, R), comember(X, R). drop(H, [ H | T ], T). drop(H, [ X | T ], T1) :- drop(H, T, T1). ?- X=[ 1, 2, 3 | X ], comember(2,X). Answer: yes. ?- X=[ 1, 2, 3, 1, 2, 3], comember(2, X). Answer: no. ?- X=[1, 2, 3 | X], comember(Y, X). Answer: Y = 1; Y = 2; Y = 3; ?- X = [1,2 | X], comember(3, X). Answer: no

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 23

University of Texas at Dallas

Example: Sieve of Eratosthenes
 Lazy evaluation can be elegantly incorporated in LP :- coinductive sieve/2, filter/3, comember/2. primes(X) :- generate_infinite_list(I),sieve(I,L),comember(X,L). sieve([H|T],[H,R]) :- filter(H,T,F),sieve(F,R). filter(H,[ ],[ ]). filter(H,[K | T],[K | T1]):- R is K mod H, R>0,filter(H,T,T1). filter(H,[K | T],T1) :- 0 is K mod H, filter(H,T,T1). :-coinductive int/2 int(X,[X | Y]) :- X1 is X+1, int(X1,Y). generate_infinite_list(I) :- int(2,I).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 24

University of Texas at Dallas

Co-Logic Programming
 combines both halves of logic programming:  syntactically identical to traditional logic programming, except predicates are labeled:  and stratification restriction enforced where:
 Inductive, or  coinductive  traditional logic programming  coinductive logic programming

 Implementation on top of YAP available.

 inductive and coinductive predicates cannot be mutually recursive. e.g., p :- q. q :- p. Program rejected, if p coinductive & q inductive
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 25

University of Texas at Dallas

Application: Model Checking
 automated verification of hardware and software systems  -automata
 accept infinite strings  accepting state must be traversed infinitely often

 requires computation of lfp and gfp  co-logic programming provides an elegant framework for model checking  traditional LP works for safety property (that is based on lfp) in an elegant manner, but not for liveness .

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 26

University of Texas at Dallas

Verification of Properties

 Types of properties: safety and liveness  Search for counter-example

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 27

University of Texas at Dallas

Safety versus Liveness
 Safety
 "nothing bad will happen"  naturally described inductively  straightforward encoding in traditional LP

 liveness
    "something good will eventually happen" dual of safety naturally described coinductively straightforward encoding in coinductive LP
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 28

University of Texas at Dallas

Finite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). automata([ ], St) :- final(St). trans(s0, a, s1). trans(s3, d, s0). trans(s1, b, s2). trans(s2, 3, s0). trans(s2, c, s3). final(s2).

?- automata(X,s0). X=[ a, b]; X=[ a, b, e, a, b]; X=[ a, b, e, a, b, e, a, b];

...... ...... ......

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 29

University of Texas at Dallas

Infinite Automata
automata([X|T], St):- trans(St, X, NewSt), automata(T, NewSt). trans(s0,a,s1). trans(s3,d,s0). trans(s1,b,s2). trans(s2,3,s0). trans(s2,c,s3). final(s2).

?- automata(X,s0). X=[ a, b, c, d | X ]; X=[ a, b, e | X ];

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 30

University of Texas at Dallas

Verifying Liveness Properties
 Verifying safety properties in LP is relatively easy: safety modeled by reachability  Accomplished via tabled logic programming  Verifying liveness is much harder: a counterexample to liveness is an infinite trace  Verifying liveness is transformed into a safety check via use of negations in model checking and tabled LP
 Considerable overhead incurred

 Co-LP solves the problem more elegantly:
 Infinite traces that serve as counter-examples are easily produced as answers
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 31

University of Texas at Dallas

Verifying Liveness Properties
 Consider Safety:
 Question: Is an unsafe state, Su, reachable (safe)?  If answer is yes, the path to Su is the counter-ex.

 Consider Liveness, then dually
 Question: Is a state, D, that should be dead, live?  If answer is yes, the infinite path containing D is the counter example
 Co-LP will produce this infinite path as the answer

 Checking for liveness is just as easy as checking for safety
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 32

University of Texas at Dallas

Counter

sm1(N,[sm1|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. s0(N,[s0|T]) :- N1 is N+1 mod 4, s1(N1,T), N1>=0. s1(N,[s1|T]) :- N1 is N+1 mod 4, s2(N1,T), N1>=0. s2(N,[s2|T]) :- N1 is N+1 mod 4, s3(N1,T), N1>=0. s3(N,[s3|T]) :- N1 is N+1 mod 4, s0(N1,T), N1>=0. ?- sm1(-1,X),comember(sm1,X). No. (because sm1 does not occur in X infinitely often).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 33

University of Texas at Dallas

Nested Finite and Infinite Automata
:- coinductive state/2. state(s0, [s0,s1 | T]):- enter, work, state(s1,T). state(s1, [s1 | T]):- exit, state(s2,T). state(s2, [s2 | T]):- repeat, state(s0,T). state(s0, [s0 | T]):- error, state(s3,T). state(s3, [s3 | T]):- repeat, state(s0,T). work. enter. repeat. exit. error. work :- work. |?- state(s0,X), absent(s2,X). X=[ s0, s3 | X ]
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 34

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

Timed Automata  -automata w/ time constrained transitions & stopwatches  straightforward encoding into CLP(R) + Co-LP

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 35

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
:- use_module(library(clpr)). :- coinductive driver/9.

train(X, up, X, T1,T2,T2). % up=idle train(s0,approach,s1,T1,T2,T3) :- {T3=T1}. train(s1,in,s2,T1,T2,T3):-{T1-T2>2,T3=T2} train(s2,out,s3,T1,T2,T3). train(s3,exit,s0,T1,T2,T3):-{T3=T2,T1-T2<5}. train(X,lower,X,T1,T2,T2). train(X,down,X,T1,T2,T2). train(X,raise,X,T1,T2,T2).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 36

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"
contr(s0,approach,s1,T1,T2,T1).

contr(s1,lower,s2,T1,T2,T3):- {T3=T2, T1-T2=1}.

contr(s2,exit,s3,T1,T2,T1). contr(s3,raise,s0,T1,T2,T2):-{T1-T2<1}. contr(X,in,X,T1,T2,T2). contr(X,up,X,T1,T2,T2). contr(X,out,X,T1,T2,T2). contr(X,down,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 37

University of Texas at Dallas

Verification of Real-Time Systems

"Train, Controller, Gate"

gate(s0,lower,s1,T1,T2,T3):- {T3=T1}. gate(s1,down,s2,T1,T2,T3):- {T3=T2,T1-T2<1}. gate(s2,raise,s3,T1,T2,T3):- {T3=T1}.
gate(s3,up,s0,T1,T2,T3):- {T3=T2,T1-T2>1,T1-T2<2 }.

gate(X,approach,X,T1,T2,T2). gate(X,in,X,T1,T2,T2). gate(X,out,X,T1,T2,T2). gate(X,exit,X,T1,T2,T2).

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 38

University of Texas at Dallas

Verification of Real-Time Systems
:- coinductive driver/9. driver(S0,S1,S2, T,T0,T1,T2, [ X | Rest ], [ (X,T) | R ]) :train(S0,X,S00,T,T0,T00), contr(S1,X,S10,T,T1,T10), gate(S2,X,S20,T,T2,T20), {TA > T}, driver(S00,S10,S20,TA,T00,T10,T20,Rest,R). |?- driver(s0,s0,s0,T,Ta,Tb,Tc,X,R). R=[(approach,A), (lower,B), (down,C), (in,D), (out,E), (exit,F), (raise,G), (up,H) | R ], X=[approach, lower, down, in, out, exit, raise, up | X] ; R=[(approach,A),(lower,B),(down,C),(in,D),(out,E),(exit,F),(raise,G), (approach,H),(up,I)|R], X=[approach,lower,down,in,out,exit,raise,approach,up | X] ;
% where A, B, C, ... H, I are the corresponding wall clock time of events generated.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 39

University of Texas at Dallas

Goal-directed execution of ASP
      Answer set programming (ASP) is a popular formalism for non monotonic reasoning Applications in real-world reasoning, planning, etc. Semantics given via lfp of a residual program obtained after "Gelfond-Lifschitz" transform Popular implementations: Smodels, DLV, etc.
1. No goal-directed execution strategy available 2. ASP limited to only finitely groundable programs

Co-logic programming solves both these problems. Also provides a goal-directed method to check if a proposition is true in some model of a prop. formula
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 40

University of Texas at Dallas

   

Most of the time, given a theory, we are interested in knowing if a particular goal is true or not. Top down goal-directed execution provides operational semantics (important for usability) Execution more efficient. Why check the consistency of the whole knowledgebase?
 Tabled LP vs bottom up Deductive Deductive Databases

Why Goal-directed ASP?

 

Most practical examples anyway add a constraint to force the answer set to contain a certain goal. Answer sets of non-finitely groundable programs computable & Constraints incorporated in Prolog style.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 41

 Inconsistency in some unrelated part will scuttle the whole system  E.g. Zebra puzzle: :- not satisfied.

University of Texas at Dallas

Negation in Co-LP
 Given a clause such as ?- p. fails coinductively when not p is encountered  To incorporate negation in coinductive reasoning, need a negative coinductive hypothesis rule:
p :- q, not p.

  Answer set programming makes the "glass completely full" by taking into account failing computations: 

 In the process of establishing not(p), if not(p) is seen again in the resolvent, then not(p) succeeds Also, not not p reduces to p.

 p :- q, not p. is consistent if p = false and q = false However, this takes away monotonicity: q can be constrainted to false, causing q to be withdrawn, if it was established earlier.

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 42

University of Texas at Dallas

ASP
 Consider the following program, A:
p :- not q. t. r :- t, s. q :- not p. s. A has 2 answer sets: {p, r, t, s} & {q, r, t, s}.

 Now suppose we add the following rule to A:  Gelfond-Lifschitz Method:
h :- p, not h. (falsify p) Only one answer set remains: {q, r, t, s}

 Given an answer set S, for each p  S, delete all rules whose body contains "not p";  delete all goals of the form "not q" in remaining rules  Compute the least fix point, L, of the residual program  If S = L, then S is an answer set

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 43

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, A':
p :- not q. q :- not p, r. t. s.

 Separate into constraint and non-constraint rules: only 1 constraint rule in this case.  Execute the query under co-LP, candidate answer sets will be generated.  Keep the ones not rejected by the constraints.  Suppose the query is ?- q. Execution: q not p, r not not q, r q, r r t, s s success. Ans = {q, r, t, s}  Next, we need to check that constraint rules will not reject the generated answer set.
 (it doesn't in this case)
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 44

r :- t, s. h :- p, not h.

University of Texas at Dallas

Goal-directed ASP
 In general, for the constraint rules of p as head, p1:- B1. p2:- B2. ... pn :- Bn., generate rule(s) of the form: chk_p1 :- not(p1), B1. chk_p2 :- not(p2), B2. ... chk_pn :- not(p), Bn.  Generate: nmr_chk :- not(chk_p1), ... , not(chk_pn).  For each pred. definition, generate its negative version: not_p :- not(B1), not(B2), ... , not(Bn).  If you want to ask query Q, then ask ?- Q, nmr_chk.  Execution keeps track of atoms in the answer set (PCHS) and atoms not in the answer set (NCHS).
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 45

University of Texas at Dallas

Goal-directed ASP
 Consider the following program, P1:
(i) p ::- not q. (ii) q:(iii) r :q:- not r. :- not p. P1 has 1 answer set: {q, r}. (iv) q ::- not p.

 Separate into: 3 constraint rules (i, ii, iii) 2 non-constraint rules (i, iv).

Suppose the query is ?- r. Expand as in co-LP: r not p not not q q( not r fail, backtrack) not p success. Ans={r, q} which satisfies the constraint rules of nmr_chk.
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 46

p :- not(q). q :- not(r). r :- not(p). q :- not(p). chk_p :- not(p), not(q). chk_q :- not(q), not(r). chk_r :- not(r), not(p). nmr_chk :- not(chk_p), not(chk_q), not(chk_r). not_p :- q. not_q :- r, p. not_r :- p.

University of Texas at Dallas

Next Generation of LP System
 Lot of research in LP resulting in advances:
 CLP, Tabled LP, Parallelism, Andorra, ASP, now co-LP

 However, no "one stop shop" system  Dream: build this "one stop shop" system
ASP Or-Parallelism Tabled LP Next Generation Prolog System CLP Andorra

Rule selection

Goal selection
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 47

University of Texas at Dallas

Related Publications
1. L. Simon, A. Mallya, A. Bansal, and G. Gupta. Coinductive logic programming. In ICLP'06 . 2. L. Simon, A. Bansal, A. Mallya, and G. Gupta. CoLogic programming: Extending logic programming with coinduction. In ICALP'07. 3. ICLP'07 Proceedings (this tutorial) 4. A. Bansal, R. Min, G. Gupta. Goal-directed Execution of ASP. Internal Report, UT Dallas 5. R. Min, A. Bansal, G. Gupta. Goal-directed Execution of ASP with General Predicates. Forthcoming. 6. A. Bansal, R. Min, G. Gupta. Resolution Theorem Proving with Coinduction. Internal Report, UT Dallas
Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 48

University of Texas at Dallas

Conclusion
 Circularity is a common concept in everyday life and computer science:  Logic/LP is unable to cope with circularity  Solution: introduce coinduction in Logic/LP
 dual of traditional logic programming  operational semantics for coinduction  combining both halves of logic programming

 applications to verification, non monotonic reasoning, negation in LP, web services, theorem proving, propositional satisfiability.  Acknowledgemt.: V. Santos Costa, R. Rocha, F. Silva
(for help with implementation of co-LP)

Applied Logic, Programming-Languages and Systems (ALPS) Lab @ UTD Slide- 49

Reputation-based Web service selection for Composition
Srividya K Bansal Department of Engineering Arizona State University Mesa, Arizona 85212, USA Email: srividya.bansal@asu.edu Ajay Bansal Department of Computer Science Georgetown University Washington, DC 20057, USA Email: bansal@cs.georgetown.edu

Abstract--The success and acceptance of Web service composition depends on computing solutions comprised of trustworthy services. In this paper, we extend our Web service Composition framework to include selection and ranking of services based on their reputation score. With the increasing popularity of Web-based Social Networks like Linkedin, Facebook, and Twitter, there is great potential in determining the reputation score of a particular service provider using Social Network Analysis. We present a technique to calculate a reputation score per service using centrality measure of Social Networks. We use this score to produce composition solutions that consist of services provided by trust-worthy and reputed providers. Keywords-service composition; reputation; social networks;

rest of the network. So our rationale is that these central figures who play a fundamental role in the network are trusted by others in the network who are connected (directly or indirectly) to them. Our work investigates the following research issues: (i) compute the reputation score of composition solutions based on individual scores of service providers obtained using the centrality measure of social networks (ii) set a threshold for reputation that each and every Web service involved in the composition has to satisfy. Failure to meet the threshold will result in filtering out the Web service and it will not be used in any composition solution. II. C ENTRALITY M EASURE IN S OCIAL N ETWORKS : Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds. It involves measuring the formal and informal relationships to understand information/knowldege flow that binds the interacting units that could be a person, group, organization, or any knowledge entity. In order to understand social networks and their participants, the location of an actor in a network is evaluated. The network location is measured in terms of centrality of a node that gives an insight into the various roles and groupings in a network. Centrality gives a rough indication of the social power of a node based on how well they "connect" the network. The graph-theoretic conception of compactness has been extended to the study of Social Networks and simply renamed "graph centrality" [1]. Their measures are all based upon distances between points, and all define graphs as centralized to the degree that their points are all close together. Based on research on communication in Social Networks, the centrality of an entire network should index the tendency of a single point to be more central than all other points in the network. Measures of a graph centrality are based on differences between the centrality of the most central point and that of all others. Thus, they are indexes of the centralization of the network [4]. The three most popular individual centrality measures are Degree Centrality, Betweenness Centrality, and Closeness Centrality. Degree Centrality: The network activity of a node can be measured using the concept of degrees, i.e., the number

I. I NTRODUCTION The next milestone in the evolution of the World Wide Web is making services ubiquitously available. We need infrastructure that applications can use to automatically discover, deploy, compose, and synthesize services. Along with the functional attributes there is a need to consider non-functional attributes (Quality of Service parameters) of Web services in the process of building composite Web services. The current challenge in automatic composition of Web services also includes finding a composite Web service that can be trusted by consumers before using it. In this paper, we present our approach that uses analysis of Social Networks to calculate a reputation score for each service involved in the composition and further prune results based on this score. Web-based Social Networks have become increasingly popular these days. Social Network Analysis is the process of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups, organizations, computers, or any knowledge entity. We propose to measure the reputation of a service by measuring the centrality of a service provider and/or a service provider organization in a well-known Social Network. We adopt our idea of computing a reputation score using centrality measure based on the notion of centrality and prestige being key in the study of social networks [1], [2]. The role of central people (nodes with high centrality) in a network seems to be fundamental as they adopt the innovation and help in transportation and diffusion of information throughout the

ServiceProvider Provider A Provider B Provider C Provider D Provider E Provider F Provider G

Degree 2 3 1 8 3 4 5

ServiceProvider Provider H Provider I Provider J Provider K Provider L Provider M

Degree 3 1 2 4 1 1

n

CD (sk ) =
i=0

a(si , sk )

where a(si , sk ) = 1 iff si and sk are connected 0 otherwise As such it is a straightforward index of the extent to which sk is a focus of activity. CD (sk ) is large if service provider sk is adjacent to, or in direct contact with, a large number of other service providers, and small if sk tends to be cut off from such direct contact. CD (sk ) = 0 for a service provider that is totally isolated from any other point. Our algorithm filters out any services whose provider has a zero degree centrality in a social network, i.e., such services will not be used in building composition solutions. Reputation of the entire composite service is computed as an average of the individual reputation score of the services involved in the composition. We also need to set a reputation threshold and any service with a reputation score that is below this threshold is not used while generating composition solutions. In our initial prototype implementation we set the reputation threshold to zero, i.e., degree centrality of the service provider in the network is zero. A service provider or service provider organization that is not connected to any other nodes in the Social network is not known to anyone else and is an immediate reason to be pruned out from composition solutions as the service cannot be trusted. Composition solutions can be ranked such that solutions with highest reputation score appear on top of the list. IV. C ONCLUSIONS AND F UTURE W ORK In this paper, we presented our approach to compute reputation of services and use this score to select services for composition. A reputation score is computed for every service in the repository based on degree centrality of the service provider in a well-known Web-based Social Network. Our future work includes exploring other measures of centrality such as betweenness centrality and closeness centrality and analyzing the possibility of using a combination of all three measures of centrality to compute reputation of a service and/or provider. R EFERENCES
[1] L. C. Freeman, Centrality in Social Networks Conceptual Clarification, in Social Networks, Vol. 1, No. 3. (1979), pp. 215-239. [2] S. Wasserman, and K. Faust, Social Network Analysis: Methods and Applications, Cambridge: Cambridge Univ. Press, 1994. [3] S. Kona, A. Bansal, M. Blake, and G. Gupta, Generalized Semantics-based Service Composition in Proceedings of IEEE Intl. Conference on Web Services (ICWS), September 2008. [4] H. J. Leavitt, Some effects of communication patterns on group performance in Journal of Abnormal and Social Psychology, pp. 46:38-50, 1951.

Table I D EGREE C ENTRALITY OF N ODES IN F IGURE 1

Figure 1.

A Social Network of Web service Providers

of direct connections a node has. In the example network shown in figure 1 and table I, Provider D has the most direct connections in the network, making it the most active node in the network. In personal Social Networks, the common thought is that "the more connections, the better". Betweenness Centrality: Though Provider D has many direct ties, Provider H has fewer direct connections (close to the average in the network). Yet, in many ways, Provider H has one of the best locations in the network by playing the role of a "broker" between two important components. Closeness Centrality: Provider F and G have fewer connections than Provider D, yet the pattern of their direct and indirect ties allow them to access all the nodes in the network more quickly than anyone else. They have the shortest paths to all other and hence are in an excellent position to have the best visibility into what is happening in the network. Individual network centralities provide insight into the individual's location in the network. The relationship between the centralities of all nodes can reveal much about the overall network structure. III. R EPUTATION - BASED W EB SERVICE SELECTION FOR
COMPOSITION

We extend our previous work on Web service composition [3] (that uses both functional and non-functional parameters to compute composition solutions) by using reputation to filter services. The reputation score of each service in a Web service repository is computed as a measure of the degree centrality (CD ) of the social network to which the service provider belongs. It is calculated as the degree or count of the number of adjacencies for a node, sk :

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Generalized Semantics-based Service Composition
Srividya Kona and Gopal Gupta
Department of Computer Science, The University of Texas at Dallas Richardson, TX 75083

Abstract. Web services and Service-oriented computing is being widely used and accepted. In order to effectively reuse existing services, we need to automatically compose Web services. The automatic composition techniques need to be semantics-based rather than syntax-based, since only semantics can accurately capture the task a service performs. In this paper we present general semanticsbased techniques for automatic service composition for Web services. Our general method takes as input (i) a repository, R, of semantic descriptions of Web services, (ii) a semantic description, Q, of the service that is desired, and produces a composite service consisting of services taken from R that realizes Q. The composite service may combine services linearly as a chain or as a directed acyclic graph, where in the most general case the combination may be conditional. The composite service generated by our automatic composition method can be coded as an OWL-S document. We present details of our initial implementation along with performance results.

1 Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered and consumed. A Web service is an autonomous, platformindependent program accessible over the web that may effect some action or change in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. As automation increases, these services will be accessed directly by applications rather than by humans [7]. In this context, a Web service can be regarded as a "programmatic interface" that makes application to application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order to make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize and compose services automatically. To make services ubiquitously available we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, composition, deployment, and synthesis [1]. Several efforts are underway to build such an infrastructure [1013]. Service Composition involves effectively combining and reusing independently developed component services. A composite service is a collection of services combined together in some way to achieve a desired effect. Traditionally, the task of automatic

service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [20]. Most efforts reported in the literature focus on one or more of these four phases. The first phase involves generating a plan, i.e., all the services and the order in which they are to be composed inorder to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions based on non-functional properties like QoS properties. The last phase involves executing the services as per the plan and in case any of them are not available, an alternate solution has to be used. Note that one must make clear distinction between automatic service composition and manual service composition. Services can be composed manually and the manually specified composition encoded as a program/document in a language such as OWL-S [2] or BPEL4WS [11]. Automatic service composition can be thought of as generating this OWL-S or BPEL4WS description automatically. Thus, languages such as OWL-S and BPEL4WS do not solve the automatic service composition problem--they merely provide notations for specifying service composition. In this paper we present a general approach for automatic service composition. Our composition algorithm performs planning, discovery, and selection automatically, all at once, in one single process. This is in contrast to most methods in the literature where one of the phases (most frequently planning) is performed manually. Additionally, our method generates most general compositions based on (conditional) directed acyclic graphs. In our method, after a solution is obtained, the semantic description of the new composite service is generated using the composition language OWL-S [2]. This description document can be registered in the repository and is thus available for future searches. The composite service can now be discovered as a direct match instead of having to look through the entire repository and build the composition solution again. Our research makes the following novel contributions: (i) We present a generalized composition algorithm based on generating conditional directed acyclic graphs; (ii) we present an efficient and scalable algorithm for solving the composition problem that takes semantics of services into account; our algorithm automatically discovers and selects the individual services involved in composition for a given query, without the need for manual intervention; (iii) we automatically generate OWL-S descriptions of the new composite service obtained; and, (iv) we present a prototype implementation based on constraint logic programming that works efficiently on large repositories. The rest of the paper is organized as follows. In section 2 we present the related work in the area of service composition and discuss their limitations. In section 3, we present the service composition problem, the different kinds of composition with examples, and our technique for automatic composition. In section 4, we discuss automatic generation of OWL-S service descriptions. Section 5 presents our prototype implementation and performance results. The last section presents the conclusions and future work.

2 Related Work
Composition of Web services has been active area of research recently [19, 20]. Most of these approaches present techniques to solve one or more phases listed in section 1.

There are many approaches [1416] that solve the first two phases of composition namely planning and discovery. These are based on capturing the formal semantics of the service using action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available. To the best of our knowledge, most of these approaches that use planning are restricted to sequential compositions, rather than a directed acyclic graph. In this paper we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential but also nonsequential that can be represented in the form of a directed acyclic graph. The authors in [14] present a composition technique by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also relies on a user-defined plan template which is created manually. One of the main objective of our work is to come up with a technique that can automatically produce the composition without the need for any manual intervention. There are industry solutions based on WSDL and BPEL4WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service by composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process. They select appropriate services using a planner when an explicit flow is provided. In contrast, our technique automatically determines these complex flows using semantic descriptions of atomic services. A process-level composition solution based on OWL-S is proposed in [16]. In this work the authors assume that they already have the appropriate individual services involved in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but instead assume that they already have the list of atomic services. In contrast, we present a technique that automatically find the services that are suitable for composition based on the query requirements for the new composed service. There are solutions such as [18] that solve the selection phase of composition. This work uses pre-defined plans and discovered services provided in a matrix representation. Then the best composition plans are selected and ranked based on QoS parameters like cost, time, and reputation. These criterion are measured using fuzzy numbers. There has been a lot of work on composition languages such as WS-BPEL, WSML, AO4BPEL, etc. which are useful during the execution phase. FuseJ is also once such description language for unifying aspects and components. Though this language was not designed for Web services, the authors present in [17] that it can be used for service composition as well. It uses connectors to interconnect services. There is no centralized process description, but instead is spread across the connectors. With FuseJ, the planning phase has to be performed manually wherein the connectors have to be written. Similarly, OWL-S can also describe a composite service which is actually an abstract

service. Service grounding of OWL-S maps the abstract service to the concrete WSDL specification. These languages are useful after the planning, discovery, and selection is done. The new composite service can be described using one of these languages. In this paper, we present a technique for automatically planning, discovering and selecting services that are suitable for obtaining a composite service, based on the user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use human input on what services would be suitable for composition. We also show different kinds of compositions such as, non-sequential compositions (i.e., composition where there can be more than one service involved at any stage, represented as a directed acyclic graph of services), sequential composition (i.e, a linear chain of services) and composition with if-then-else conditions. We also automatically generate OWL-S descriptions for the new composite service obtained.

3 Web service Composition
Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 1 shows an example composite service made up of five services . In the figure,   and   are the query input parameters and pre-conditions  to respectively. 퓬 and 퓬 are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes and indicates that outputs of constitute (some of) the inputs of .

 









S2 S5 CI',I' S1 S3 S4 CO',O'

Fig. 1. Example of a Composite Service as a Directed Acyclic Graph

As mentioned in section 1, composition can be seen as four phases which include (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution. Most of the related work presents techniques to solve one of these phases or a combination of them. We need a technique that can automatically determine the services involved in composition without the need for any manual intervention or user-defined plans. In this section we present the different kind of compositions and our technique for automatic composition which performs the first three phases - planning, discovery, and selection simultaneously as one phase. Definition (Repository of Services): Repository is a set of Web services. Definition (Service): A service is a 6-tuple of its pre-conditions, inputs, side-effect, ) is the repreaffected object, outputs and post-conditions. = ( sentation of a service where is the list of pre-conditions, is the input list, is the





좋



피 

service's side-effect, is the affected object, is the output list, and is the list of post-conditions.    Definition (Query): The query service is defined as = (    )     where is the pre-conditions, is the input list, is the service affect, is the  affected object,  is the output list, and is the post-conditions. These are all the parameters of the requested service.















좋

 피  

3.1 Sequential Composition A sequential composition is one which has a linear chain of services that form the composite service. When all nodes in the directed acyclic graph of figure 1 have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem. Example 1: Suppose we are looking for a service to make travel arrangements, i.e., flight, hotel, and rental car reservations. The directory of services contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 1 shows the input/output parameters of the user-query and the three services ReserveFlight, ReserveHotel, and ReserveCar. For the sake of simplicity, the query and services have fewer input/output parameters than the real-world services. In this example, service ReserveFlight has to be executed first so that its output ArrivalFlightNum can be used as input by ReserveHotel followed by the service ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Figure 2 shows this example sequential composition as a directed acyclic graph.
Service Query ReserveFlight ReserveHotel ReserveCar Input Parameters PassengerName, OriginAirport,Start Date,DestinationAirport,ReturnDate PassengerName, OriginAirport,Start Date, DestinationAirport,ReturnDate PassengerName, ArrivalFlightNum, StartDate, ReturnDate PassengerName,ArrivalDate ArrivalFlightNum, HotelAddress Output Parameters HotelConfirmationNum, CarConfirmationNum FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress CarConfirmationNum

Table 1. Example Scenario for Sequential Composition

Definition (Sequential Composition): More generally, the Composition problem can be defined as automatically finding a directed acyclic graph =   of services    from repository , given query =(    ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should , , =( , , , , , ) hold on the nodes of the graph:





좋

피   





 앓  않 

  

 1.  , ...,  ,    2. ,   , ...,  The meaning of the is the subsumption (subsumes) relation and is the implication relation. In other words, we are deriving a possible sequence of services where only the provided input parameters are used for the services and at least the required output parameters are provided as an output by the chain of services. The goal is to derive a solution with minimal number of services. Also the post-conditions of a service in the chain should imply the pre-conditions of the next service in the chain.

 防  防  徘 졔  풩     



QueryInputs

ReserveFlight

QueryOutputs ReserveHotel ReserveCar

Fig. 2. Example of Sequential Composition as a Directed Acyclic Graph

3.2 Non-Sequential Composition Let us consider an example where a non-sequential composition can be obtained using the given repository of services. A non-sequential composition can have more than one service involved at any stage of the composition task. Example 2: Suppose we are looking for a service to make international travel arrangements and the directory of services contains ReserveFlight, ReserveHotel, ReserveCar, and ProcessVisa services. In this scenario, we first need to apply for a visa and then make the flight, hotel, and car reservations. Table 2 shows the input/output parameters of the user-query and the four services. In this example, service ProcessVisa has a post-condition VisaApproved. The services ReserveFlight and ReserveHotel have the pre-condition that the visa must be approved before making reservations. ProcessVisa has to be executed first followed by ReserveFlight and ReserveHotel. The post-conditions of ProcessVisa must imply the pre-conditions of ReserveFlight and similarly must imply the pre-conditions of ReserveHotel. The ReserveCar service needs inputs HotelAddress and ArrivalFlightNum. Hence it is executed after both ReserveFlight and ReserveHotel are executed so that their outputs can be used as inputs to ReserveCar. The semantic descriptions of the service input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred using semantics from the ontology provided. Figure 3 shows this non-sequential composition example as a directed acyclic graph. Definition (Non-Sequential Composition): More generally, the Composition problem  of services can be defined as automatically finding a directed acyclic graph =     from repository , given query =(    ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: where has exactly one incoming edge that represents the query inputs 1. and pre-conditions,  ,  .





좋

피 





 앓



 湃    

where has exactly one outgoing edge that represents the query outputs  and post-conditions,  , . 3. where has at least one incoming edge, let , , ...,  be the nodes such that there is a directed edge from each of these nodes to . Then     ,  (  ).   

2.

 앓   密      앓     

 





密 

PreInput Parameters Conditions Query PassengerName, OriginAirport, DestinationAirport, StartDate, ReturnDate Process PassengerName, VisaType, Visa StartDate, ReturnDate Reserve VisaPassengerName, OriginAirport, Flight Approved DestinationAirport, StartDate, ReturnDate Reserve VisaPassengerName, Hotel Approved StartDate, ReturnDate Reserve PassengerName, ArrivalDate, Car ArrivalFlightNum, HotelAddress

Service

Output Parameters

PostConditions

FlightConfirmationNum, HotelConfirmationNum, CarConfirmationNum ConfirmationNum VisaApproved FlightConfirmationNum, ArrivalFlightNum HotelConfirmationNum, HotelAddress CarConfirmationNum

Table 2. Example Scenario for Non-Sequential Composition

The meaning of is the subsumption (subsumes) relation and is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. Further details on the formal description of the composition problem are in [5].





Condition: VisaApproved QueryInputs ProcessVisa Condition: VisaApproved

ReserveFlight

Parameter: ArrivalFlightNum QueryOutputs ReserveCar Parameter: HotelAddress

ReserveHotel

Fig. 3. Example of Non-Sequential Composition as a Directed Acyclic Graph

3.3 Non-Sequential Conditional Composition Let us now consider an example of a non-sequential composition with if-then-else conditions, i.e., the composition flow varies depending on the result of the post-conditions of a service.

Example 3: This example is similar to Example 2 but with more constraints. Suppose we are looking for a service to make international travel arrangements. We first need to make a tentative flight and hotel reservation and then apply for a visa. If the visa is approved, we can buy the flight ticket and confirm the hotel reservation, else we will have to cancel both the reservations. Also if the visa is approved, we need to make a car reservation. The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table 3 shows the input/output parameters of the user-query and the services.

Service Query

PreConditions

Input Parameters

Output Parameters FlightConfNum,HotelConfNum,CarConfNum FlightConfNum HotelConfNum ConfirmationNum ArrivalFlightNum ConfirmationNum CancelCode CancelCode CarConfirmNum

PostConditions

PasngrName,OriginArprt,Dest-, Arpt,StartDate,ReturnDate Reserve PasngrName,OriginArprt,DestFlight Arpt,StartDate,ReturnDate Reserve PasngrName, StartDate, Hotel ReturnDate Process PasngrName,VisaType, Visa FlightConfNum,HotelConfNum ConfirmFlight VisaApproved FlightConfNum,CreditCardNum ConfirmHotel VisaApproved HotelConfNum,CreditCardNum CancelFlight VisaDenied PasngrName,FlightConfNum CancelHotel VisaDenied PasngrName,HotelConfNum Reserve PasngrName,ArrivalDate Car ArrivalFlightNum

VisaApproved VisaDenied

Table 3. Example Scenario for Non-Sequential Conditional Composition

In this example, service ProcessVisa produces the post-condition VisaApproved VisaDenied. The services ConfirmFlight and ConfirmHotel have the pre-condition VisaApproved. In this case, one cannot determine if the post-conditions of service ProcessVisa implies the pre-conditions of services ConfirmFlight and ConfirmHotel until the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and depending on the outcome of the condition, the corresponding services will be executed. The vertex for service ProcessVisa in the graph is an OR node with the outgoing edges representing the generated conditions and the VisaApproved and outputs. In this case conditions (VisaApproved VisaDenied) (VisaApproved VisaDenied) VisaDenied are generated. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed. Figure 4 shows this conditional composition example as an AND/OR directed acyclic graph. Definition (Non-Sequential Conditional Composition): More generally, the Composition problem can be defined as automatically finding an AND/OR directed acyclic   of services from repository , given query = (    , graph =      ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph is either an OR or AND vertex and represents a service in the composi-













 



tion. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and preconditions of the service. The outgoing edges of an AND node have an arc around them to differentiate from the OR node. The following conditions should hold on the nodes of the graph: 1. where has exactly one incoming edge that represents the query inputs and pre-conditions,  ,  . 2. where has exactly one outgoing edge that represents the query outputs  and post-conditions,  , . 3. where is an AND vertex, let  , , ...,  be the nodes such that  there is a directed edge from to each of these nodes. Then (   ,  (    ). 4. where is an OR vertex, let , , ...,  be the nodes such that  there is a directed edge from to each of these nodes. Then (   ,  (    ).

 앓  앓  앓    앓  

  줆  졔    密                
Condition: VisaApproved ProcessVisa ConfirmFlight





湃  湃 

Parameter: ArrivalFlightNum

ConfirmHotel QueryOutputs

ReserveFlight Query Inputs ReserveHotel

Parameter: ArrivalFlightNum

ReserveCar

Condition: VisaApproved

CancelFlight CancelHotel

Fig. 4. Example of Conditional Composition as an AND/OR Directed Acyclic Graph

The meaning of the is the subsumption (subsumes) relation and is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. When it cannot be determined at compile time whether the post-conditions imply the pre-conditions or not, an OR node is created in the graph. Each outgoing edges represent the possible conditions which will be evaluated at run-time. Depending on the condition that holds, the corresponding services are executed. That is, if a subservice  is composed with subservice  , then the postconditions  of  must imply the preconditions  of  . The following conditions are evaluated at run-time: if (   ) then execute  ; else if (   ) then no-op; else if ( ) then execute  ;







  풩   풩   



 

3.4 Automatic Composition Algorithm In order to produce the composite service which is the graph, as shown in the example figure 1, we filter out services that are not useful for the composition at multiple stages. Figure 5 shows the filtering technique for the particular instance shown in figure 1. The

composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. In figure 5,   are the pre-conditions and the input parameters provided by the query.  and  are the services found after step 1.  is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e.,  =   .  is used to find services at the next stage, i.e., all those services that require a subset of . In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

 





 



I=I CI, I
1

S1 S2 . .

O1

I=IUO
2 1

1

S . .

O
3

2

I=IUO
3 2

2

O S . .
4

3

I=IUO
4 3

3

S . .

O
5

4

O

Fig. 5. Composite Service

4 Automatic Generation of OWL-S descriptions
After we have obtained a composition solution (sequential, non-sequential, or conditional), the next step is to produce a semantic description document for this new composite service. Then this document can be used for execution of the service and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of performing the composition process all over again. We used the existing language OWL-S [2] to describe composite services. OWL-S models services as processes and when used to describe composite services, it maintains the state throughout the process. It provides control constructs such as Sequence, Split-Join, IfThen-Else and many more to describe composite services. These control constructs can be used to describe the kind of composition. OWL-S also provides a property called composedBy using which the services involved in the composition can be specified. Below is the algorithm for generation of the OWL-S document when the composition solution in the form of a graph is provided as the input.
Algorithm: GenerateServiceDescription (Input: G - Solution Graph) 1. Generate generic header constructs 2. Start Composite Service element 3. Start SequenceConstruct 4. If Number(SourceVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each starting/source Vertex V GenerateAtomicService End For

5.

6.

7. 8. 9.

EndSplitJoinConstruct End If If Number(SinkVertices) = 1 GenerateAtomicService Else StartSplitJoinConstruct For Each ending/sink Vertex V GenerateAtomicService End For EndSplitJoinConstruct End If For Each remaining vertex V in G If V is AND vertex with one outgoing edge GenerateAtomicService If V is AND vertex with more than one outgoing edge GenerateSplitJoinConstruct If V is OR vertex with one outgoing edge GenerateAtomicService If V is OR vertex with more than one outgoing edge GenerateConditionalConstruct End For End SequenceConstruct End Composite Service element Generate generic footer constructs

A sequential composition can be described using the Sequence construct which indicates that all the services inside this construct have to invoked one after the other in the same order. The non-sequential composition can be described in OWL-S using the Split-Join construct which indicates that all the services inside this construct can be invoked concurrently. The process completes execution only when all the services in this construct have completed their execution. The non-sequential conditional composition can be described in OWL-S using the If-Then-Else construct which specifies the condition and the services that should be executed if the condition holds and also specifies what happens when the condition does not hold. Conditions in OWL-S are described using SWRL. The OWL-S description documents for the example composite services in section 3 are shown in the Appendix. There are other constructs such as looping constructs in OWL-S which can be used to describe composite services with complex looping process flows. We are currently investigating other kinds of compositions with iterations and repeat-until loops and their OWL-S document generation. We are exploring the possibility of unfolding a loop into a linear chain of services that are repeatedly executed. We are also analyzing our choice of the composition language and looking at other possibilities as part of our future work.

5 Implementation
We implemented a prototype composition engine using Prolog with Constraint Logic Programming over finite domain [8], referred to as CLP(FD) hereafter. In our current implementation, we used semantic descriptions of web services written in the language called USDL [4]. The repository of services contains one description document for each service. USDL itself is used to specify the requirements of the service that an application developer is seeking.

Each service description is converted into a tuple: (Pre-Conditions, I, A, O, Post-Conditions). I is the list of inputs and O is the list of outputs. Pre-Conditions are list of conditions on the input parameters and Post-Conditions are the list of conditions on the output parameters. A is the list of side-effects represented as affect-type(affected-object) where the function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. Services are converted to tuples so that they can be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic, specific, part and whole substitutions [6]. In case conditions on a service are not provided, the Pre-Conditions and Post-Conditions in the triple will be null. Similarly if the affect-type is not available, this module assigns a generic affect to the service. The composition engine consists of these modules: (i) Tuple Generator; (ii) Query Reader; (iii) SemanticRelations Generator; (iv) Composition Query Processor; (v) OWLS Description Generator; TupleGenerator converts each service in the repository into the tuple format. The SemanticRelationsGenerator module extracts all the semantic relations and creates a list of Prolog facts. In our current implementation, we use USDL service descriptions which use OWL Wordnet Ontology [3] to specify the semantics. This module is generic enough to be used with other domain-specific ontology as well to obtain semantic relations of concepts. The CompositionQueryProcessor module uses the repository of facts, which contains all the services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs), encodeParam(QueryOutputs, QO), getExtInpList(QueryInputs, InpList), encodeParam(InpList, QI), performForwardTask(QI, QO, LF), performBackwardTask(LF, QO, LR), getMinSolution(LR, QI, QO, A), reverse(A, RevA), confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The output of the query processor is the composition solution which is directed acyclic graph of all the services involved in the composition. Our algorithm selects the optimal solution with least composition length (i.e., the number of stages involved in the composition). If there are any properties with respect to which the solutions can be ranked, then setting up global constraints to get the optimal solution is relatively easy with our constraint based approach. For example, if each service has an associated cost, then the solutions with the minimal cost are returned. The next step is to produce a description of the new composite service solution found. OWL-S DescriptionGenerator automatically generates the OWL-S description of the composite service using constructs depending on the type of composition. We tested our composition algorithm using repositories from WS-Challenge website[9], slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of

services. The queries and solutions are provided in an XML format. The semantic relations between various parameters are provided in an XML Schema file. We evaluated our approach on different size repositories and tabulated Pre-processing and Query Execution time. We noticed that there was a significant difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we found is that the repository was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 4 shows performance results for the different kind of compositions. The preprocessing time remains the same of all three kinds of composition whereas the query execution time varies. The times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the query should be less than or equal to the wall clock time. The results are consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which is negligible (just 1 to 3 msecs) even for complex queries with large repositories.
Repository Size (num of services) 2000 2000 2000 2500 2500 2500 3000 3000 3000 Number of I/O param-eters 4-8 16-20 32-36 4-8 16-20 32-36 4-8 16-20 32-36 PreProcessing Time (secs) 36.5 45.8 57.8 47.7 58.7 71.6 56.8 77.1 88.2 QueryExec Time (msecs) Sequential Composition 1 1 2 1 1 2 1 1 3 NonSequential Composition 1 1 2 1 2 2 1 2 3 Conditional Composition 1 2 2 1 2 3 1 3 4

Table 4. Performance of Composition Algorithm

6 Conclusions and Future Work
To make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize and compose services automatically. Our semantics-based approach uses semantic description of Web services to find substitutable and composite services that best match the desired service. Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services, number of services in a composition, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential or non-sequential composition that is possible for a given query and also automatically generates OWL-S description of the composite service. This OWL-S description can be used during the execution phase and subsequent searches for this composite service

will yield a direct match. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. Use of Constraint Logic Programming helped greatly in obtaining an efficient implementation of this system. Our future work includes extending our engine to support an external database to save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size which can easily be the case in future. We are also investigating other kinds of compositions with loops such as repeat-until and iterations and their OWL-S description generation. Analyzing the choice of the composition language and exploring other language possibilities is also part of our future work. Acknowledgments: We are grateful to our co-researcher Ajay Bansal for the helpful discussions and comments.

References
1. S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp. 46-53, March 2001. 2. OWL-S www.daml.org/services/owl-s/1.0/owl-s.html. 3. OWL WordNet: Ontology-based information management system. http://taurus. unine.ch/knowler/wordnet.html. 4. A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal ServiceSemantics Description Language. In ECOWS, pp. 214-225, 2005. 5. S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007. 6. S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and T. Hite. USDL: A Service-Semantics Description Language for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas.edu/~sxk038200/USDL.pdf. 7. A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services: A case study in chemical emergency response. In ICWS, pp.751-758, 2005. 8. K. Marriott and P. Stuckey. Prog. with Constraints: An Introduction. MIT Press, 1998. 9. WS Challenge 2006. http://insel.flp.cs.tu-berlin.de/wsc06. 10. U. Keller, R. Lara, H. Lausen, A. Polleres, and D. Fensel. Automatic Location of Services. In European Semantic Web Conference, May 2005. 11. D. Mandell, S. McIlraith Adapting BPEL4WS for the Semantic Web: The Bottom-Up Approach to Web Service Interoperation. In ISWC, 2003. 12. M. Paolucci, T. Kawamura, T. Payne, and K. Sycara Semantic Matching of Web Service Capabilities. In ISWC, pages 333-347, 2002. 13. S. Grimm, B. Motik, and C. Preist Variance in e-Business Service Discovery. In Semantic Web Services Workshop at ISWC, November 2004. 14. S. McIlraith, T.C. Son Adapting golog for composition of semantic Web services. In KRR, pages 482493, 2002. 15. B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pp.467-477. 16. M. Pistore, P. Roberti, and P. Traverso Process-Level Composition of Executable Web Services In European Semantic Web Conference, pages 62-77, 2005. 17. D. Suvee, B. Fraine, and M. Cibran Evaluating FuseJ as a Web Service Composition Language In European Conference on Web Services, 2005. 18. D. Claro, P. Albers, and J. Hao Selecting Web services for Optimal Compositions In Workshop on Semantic Web Services and Web Service Composition, 2004. 19. J. Rao, X. Su. A Survey of Automated Web Service Composition Methods In Workshop on Semantic Web Services and Web Process Composition(SWSWPC), 2004. 20. J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

Appendix
The sequential composite service shown in Example 1 and Figure 2 is described in OWL-S as follows:
<rdf:RDF ... <process:CompositeProcess rdf:ID="TravelReservation"> ... <process:composedOf><process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ReserveFlight"/> <process:AtomicProcess rdf:about="#ReserveHotel"/> <process:AtomicProcess rdf:about="#ReserveCar"/> </process:components></process:Sequence> </process:composedOf> </process:CompositeProcess></rdf:RDF>

The non-sequential composite service shown in Example 2 and Figure 3 is described in OWL-S as follows:
<rdf:RDF ... <process:CompositeProcess rdf:ID="TravelReservation"> ... <process:composedOf><process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ProcessVisa"/> <process:CompositeProcess rdf:about="#Stage1"/> <process:AtomicProcess rdf:about="#ReserveCar"/> </process:components> </process:Sequence> </process:composedOf> </process:CompositeProcess> <process:CompositeProcess rdf:ID="Stage1"> <process:composedOf><process:Split-Join> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ReserveFlight"/> <process:AtomicProcess rdf:about="#ReserveHotel"/> </process:components></process:Split-Join> </process:composedOf> </process:CompositeProcess></rdf:RDF>

The conditional composite service shown in Example 3 and Figure 4 is described in OWL-S as follows:
<rdf:RDF ... <process:CompositeProcess rdf:ID="TravelReservation"> ... <process:composedOf><process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#Stage1"/> <process:AtomicProcess rdf:about="#ProcessVisa"/> <process:CompositeProcess rdf:about="#IfThenElseStage1"/> </process:components></process:Sequence> </process:composedOf> </process:CompositeProcess>

<process:CompositeProcess rdf:ID="IfThenElseStage1"> ... <process:composedOf> <process:If-Then-Else> <process:ifCondition> <expr:SWRL-Condition rdf:ID="VisaAccepted"> <expr:expressionLanguage rdf:resource="&expr;#SWRL"/> <expr:expressionBody rdf:parseType="Literal"> <swrl:AtomList> <rdf:first> <swrl:IndividualPropertyAtom> <swrlb:equal rdf:resource="#VisaAccepted"/> <swrl:argument1 rdf:resource="#VisaAccepted"/> <swrl:argument2 rdf:resource="&rdf;#true"/> </swrl:IndividualPropertyAtom> </rdf:first> <rdf:rest rdf:resource="&rdf;#nil"/> </swrl:AtomList> </expr:expressionBody> </expr:SWRL-Condition> </process:ifCondition> <process:then> <process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ConfirmFlight"/> <process:CompositeProcess rdf:about="#Stage2"/> </process:components> </process:Sequence> </process:then> <process:else> <process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#CancelFlight"/> <process:AtomicProcess rdf:about="#CancelHotel"/> </process:components></process:Sequence> </process:else> </process:If-Then-Else></process:composedOf> </process:CompositeProcess> <process:CompositeProcess rdf:ID="Stage1"> <process:composedOf><process:Split-Join> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ReserveFlight"/> <process:AtomicProcess rdf:about="#ReserveHotel"/> </process:components></process:Split-Join> </process:composedOf> </process:CompositeProcess> <process:CompositeProcess rdf:ID="Stage2"> <process:composedOf><process:Split-Join> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#ConfirmHotel"/> <process:AtomicProcess rdf:about="#ReserveCar"/> </process:components></process:Split-Join> </process:composedOf> </process:CompositeProcess></rdf:RDF>

Towards a General Framework for Web Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake Department of Computer Science, Georgetown University Washington, DC 20057 Abstract
CI',I'

Gopal Gupta Department of Computer Science, The University of Texas at Dallas Richardson, TX 75083

S2 S5 CO',O' S3 S1 S4

Service-oriented computing (SOC) has emerged as the eminent market environment for sharing and reusing service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services. In this paper we present a generalized semantics-based technique for automatic service composition that combines the rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG) where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented.

Figure 1. Example of a Composite Service as a Directed Acyclic Graph

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered, and consumed. A composite service is a collection of services combined together in some way to achieve a desired effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [2]. Most efforts reported in the literature focus on one or more of these four phases. The first phase involves generating a plan, i.e., all the services and the order in which they are to be composed in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions based on non-functional properties like QoS properties. The last phase involves executing the services as per the plan and in case any of them are not available, an alternate solution has to be used. In this paper we formalize the generalized composition problem based on our conditional directed acyclic graph

representation. We present our approach that generates most general compositions based on (conditional) directed acyclic graphs (DAG). In our framework, the DAG representation of the composite service is reified as an OWL-S description. This description document can be registered in a repository and is thus available for future searches. The composite service can now be discovered as a direct match instead of having to look through the entire repository and build the composition solution again.

2. Web service Composition
In this section we formalize the generalized composition problem. In this generalization, we extend our previous notion of composition [1] to handle non-sequential conditional composition (which we believe is the most general case of composition). Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 1 shows an example composite service made up of five services S1 to S5 . In the figure, I and CI are the query input parameters and pre-conditions respectively. O and CO are the query output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and Sj indicates that outputs of Si constitute (some of) the inputs of Sj . Definition (Repository of Services): Repository (R) is a set of Web services.

Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and post-conditions. S = (CI , I , A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions, I is the input list, A is the service's side-effect, AO is the affected object, O is the output list, and CO is the list of post-conditions. The pre- and post-conditions are ground logical predicates. Definition (Query): The query service is defined as Q = (CI , I , A , AO , O , CO ) where CI is the list of preconditions, I is the input list, A is the service affect, AO is the affected object, O is the output list, and CO is the list of post-conditions. These are all the parameters of the requested service. Definition (Generalized Composition): The generalized Composition problem can be defined as automatically finding a directed acyclic graph G = (V , E ) of services from repository R, given query Q = (CI , I , A , AO , O , CO ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph either represents a service involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can be determined only after the execution of the service. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: 1. i Si  V where Si has exactly one incoming edge that represents the query inputs and pre-conditions, I i I i , CI i CI i . 2. i Si  V where Si has exactly one outgoing edge that represents the query outputs and post-conditions, O i O i , CO i CO i . 3. i Si  V where Si represents a service and has at least one incoming edge, let Si1 , Si2 , ..., Sim be the nodes such that there is a directed edge from each of these nodes to Si . Then Ii k Oik  I , CI i  (COi1 COi2 ...  COim  CI ). 4. i Si  V where Si represents a condition that is evaluated at run-time and has exactly one incoming edge, let Sj be its immediate predecessor node such that there is a directed edge from Sj to Si . Then the inputs and pre-conditions at node Si are Ii = Oj  I , CI i = COj . The outgoing edges from Si represent the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time. The meaning of the is the subsumption (subsumes) relation and  is the implication relation. In other words, a service at any stage in the composition can potentially have as its inputs all the outputs from its predecessors as well as the query inputs. The services in the first stage of composi-

tion can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should contain all the outputs that the query requires to be produced. Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next stage. When it cannot be determined at compile time whether the post-conditions imply the pre-conditions or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible conditions which will be evaluated at run-time. Depending on the condition that holds, the corresponding services are executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply the preconditions CI 2 of S2 . The following conditions are evaluated at run-time: if (CO1  CI 2 ) then execute S1 ; else if (CO1   CI 2 ) then no-op; else if (CI 2 ) then execute S1 ;

3. Automatic Generation of Composite Services
In order to produce the composite service which is the graph, we filter out services that are not useful for the composition at multiple stages. The composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e., I2 = O1  I . I2 is used to find services at the next stage, i.e., all those services that require a subset of I2 . In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

4. Conclusions and Future Work
To make Web services more practical we need a general framework for composition of Web services. The generalized approach presented in this paper can handle nonsequential conditional composition that can be used in automatic workflow generation in a number of applications.

References
[1] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007. [2] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Decision Support Systems 53 (2012) 234244

Contents lists available at SciVerse ScienceDirect

Decision Support Systems
journal homepage: www.elsevier.com/locate/dss

Workflow composition of service level agreements for web services
M. Brian Blake a,, David J. Cummings b, Ajay Bansal c, Srividya Kona Bansal c
a b c

Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, United States Department of Computer Science, Stanford University, United States Department of Engineering, Arizona State University, United States

a r t i c l e

i n f o

a b s t r a c t
Service-oriented architecture enables an environment where businesses can expose services for use by their collaborators and their peer organizations. In this dynamic environment, organizations require the use of service level agreements (SLAs) to assure the quality of service (QoS) standards of services provided by their collaborators. In an ad-hoc workflow scenario, a business may need to perform real-time composition of existing services in response to consumer requests. In this work, we suggest that, in parallel to traditional web service composition, the business must also compose the existing SLAs in order to ensure the service levels that must be guaranteed to new consumers. Ultimately, this approach to SLA composition must align with the overarching principles of the provider and the priorities of the consumer. In this paper, we introduce a model and representations of service level agreement attributes appropriate for managing a service provider's expectations when adding new partners. Our evaluations suggest that the SLA composition can efficiently run concurrently with traditional service composition.  2012 Elsevier B.V. All rights reserved.

Article history: Received 19 July 2010 Received in revised form 22 December 2011 Accepted 30 January 2012 Available online 8 February 2012 Keywords: Service level agreements Quality of service Web services Service-oriented computing

1. Introduction A service level agreement, SLA, is a technical contract between two types of businesses, producers and consumers. A SLA captures the agreed-upon terms between organizations with respect to quality of service (QoS) and other related concerns. In simple cases, one consumer forms a SLA with a producer. In more complex cases, a consumer may form a SLA that defines a set of producer businesses. Considering a service-oriented computing environment, capabilities are shared via the implementation of web services exposed by a producer organization. The ultimate goal of service-oriented computing is for consumers to access these shared capabilities on-demand. As such, in cases where businesses have longstanding relationships, such as workflow and supply chain environments, peer companies that share services must be able to assure a level of service to their underlying customers [4,8]. New specifications, such as the Web Service Level Agreement (WSLA) and Web Service Agreement (WS-Agreement) [2] enable SLAs to be associated with an individual web service or even groups of web services. These specifications define an eXtensible Markup Language (XML)based data model that can be used along with the Web Service Description Language (WSDL) documents that traditionally describe the web services. These specifications provide a significant opportunity.
 This paper is a substantial extension of earlier work presented in [7] and [5].  Corresponding author. Tel.: + 1 574 631 1625; fax: + 1 574 631 8007. E-mail addresses: m.brian.blake@nd.edu (M.B. Blake), david.cummings@stanford.edu (D.J. Cummings), srividya.bansal@asu.edu, ajay.bansal@asu.edu (A. Bansal). 0167-9236/$  see front matter  2012 Elsevier B.V. All rights reserved. doi:10.1016/j.dss.2012.01.017

Organizations can specify QoS-related concerns in concert with the functionality concerns already captured in the WSDL files. As a result, when a new organization searches for a pertinent web service, the SLA-enhanced WSDL file can be used to determine the appropriateness of the service to meet the required business need. Furthermore organizations can use the SLA-enhanced WSDL file to negotiate the QoS terms. Although these SLA technologies and specifications present new opportunities for service-oriented business processes, there are a number of significant barriers. When a consumer organization must create a new business capability that requires the workflow composition of multiple web services, then that organization will also need to understand the composite impact of the underlying SLAs. Consequently, in addition to composing web services that are functionally compatible, the organization will need to ensure that the web services are compatible with regard to their service levels. Also, the product of all the SLAs for a composition of web services must be within the required threshold of feasibility as defined by the end users. As the service-oriented computing paradigm increases in popularity, the consumer will have the option of many similar services that may meet a particular requirement. As such, the composition of web services that is most efficient for a particular business purpose will rest on the organization's ability to understand and optimize the corresponding composition of SLAs. To deal with the aforementioned issues, we introduce the phrase, workflow composition of SLAs. Our approach suggests the multidimensional evaluation of existing agreed-upon QoS standards in order to predict the standards possible for the introduction of new agreements. While the notions of multi-dimensional analysis, optimization, dynamic programming are not new [9,10,12,17,35] in this

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

235

work, we identify the specific SLA-based attributes that allows for the introduction of new partners. Furthermore we develop a set of principles and the associated process that utilize the SLA information to estimate service levels. This approach favors services with clean request/response (RPC-type) communication, generally known as WSDL-based web services. Further investigation would be required to assess this approach as it relates to REST-based services [37]. In this work, we investigate several research issues relevant to the integration of web services-based workflow: 1. What SLA measures and principles are appropriate to support QoSbased assessment of existing service level guarantees? 2. Given a group of SLAs and knowledge about current consumer service level needs, can an on-demand request be analyzed against existing SLAs to guarantee a certain service level for a new consumer? The paper proceeds in the following section with a discussion of related work. In Section 3, we discuss how the SLA-based QoS assessment values are derived from higher-level organizational principles. The formal details of the attributes are defined in Section 4, and how the attributes are physically captured in markup languages in shown in Section 5. Finally, in Section 6 we evaluate the performance SLA composition as it runs parallel with traditional service composition routines. 2. Related work There are many related projects that investigate the general use of SLAs for web services [18]. Some projects characterize SLA approaches to specific domains, such as military, database management, or information systems [13,20,26,29]. There is also a large body of work that attempts to automate the management and negotiation of SLAs [11,16,23,27,33]. Other work attempts to use semantics to automate the negotiation of SLAs [15,25]. Our work leverages markup language (i.e. WS-Agreements) for providing SLA measures as in other studies [1,28,30]. All related work describes the importance of composing SLAs. In [28], their emphasis is on compatibility between user requirements and provider constraints. Their approach suggests a promising model-based approach to assuring the compatibility. Our work is closely related to the comprehensive work performed by [9,10,35]. Each of these approaches investigates the QoS-based and constrained composition of web services. Although Canfora et al. [9] has an elegant approach that allows for the insertion and aggregation of any user-defined QoS attribute, our approach identifies the specific SLA measures that support the user-driven assessment of an environment where their SLAs dictate current system state. Table 1 shows a survey of SLA attributes and how they are exploited in related projects specifically in the service-oriented computing domain. Our work can be loosely classified in the body of work that looks to automate the aggregation of QoS attributes [14,21,24,32]. The
Table 1 Survey of research projects that consider SLA attributes for web services. Author names Run time Reputation Uptime (Avail)            Resp time ***      

uniqueness of our approach is that we consider the impacts when new web service workflows must be added as they affect the existing operational SLAs. More specifically, if a composite capability overlaps multiple SLAs, then the characteristics of an early SLA can impact a later SLA in the composition routine. Canfora et al., Cardoso et al., Zeng et al., [9,10,35] concentrate on deriving a specific composition routine as constrained by QoS values. Canfora et al., Zhang et al., and Yu et al. focus on iterative multiattribute utility approaches [12] where to focus is on the overall optimization function and less on the details of each of the attribute. Our work attempts to consider both consumer and producer concerns when assessing the entry of a new workflow. As such, our work contains low-level details for each service level objective such that subsequent optimization approaches use them as a model for optimization that targets each attribute at a low-level. Although [22] has a similar approach where SLAs are aggregated formally, they do not consider consumer and producer services independently as in our work. In summary, we define specific SLA measures and formally integrate measures across multiple SLAs. We also define a principled process for the composition of SLAs. This work extends related work [6] by concentrating on SLA measures in markup language files as opposed to Unified Modeling Language (UML) models. Unlike other work in QoS-based web service composition, we attempt to classify QoS attributes by those associated with the provider and those associated with the consumer. Another variation here is the introduction of several high-level criteria that can be used to characterize organizations. We believe that by aggregating all lower-level attributes into a smaller set of higher-level criteria then organizations can be quantitatively evaluated or scored. Further evaluation in this paper justifies that such on-demand assessment of SLAs performs feasibly in an operational environment where very large numbers of web service workflows exist. 3. Assessing an enterprise based on its SLAs The typical SLA has a large number of measures and criteria. However, in this work, we attempt to choose the measures that are most closely aligned to aggregation of a group of SLAs and ultimately their assessment. In the operational notion of web service composition, a basic web services workflow system must ensure that the input information supplied by the consumer ultimately leads to the required actions and outputs required by that consumer. In parallel, the workflow management system must ensure that the predicates and requisites match (either by syntactical or semantic techniques) in each step of the workflow. It is the operational composition routines that motivate the set of SLA attributes relevant to our work. In order to designate which attributes that are most relevant to our proposed innovation, we developed a set of principles important to managing the quality of an enterprise with many business processes. The three relevant principles are Compliance (Suitability),

Negotiation (rebinding)  

Cost (price)      

Success rate/ reliability      

Problem resolution  

Maintenance  

(Blake et al.) [7] and this paper  (Canfora et al.) [9]  (Yu et al.) [34] Zhang et al.) [36] ** (Zeng et al.) [35] (Cardoso et al.) [10] (Jin et al.)[18] *  (Mohabey et al.) [22]

* [18] list attributes, but do not develop formal approaches for composition. ** Although Canfora et al., Yu et al., and Zhang et al. do not formally define each attribute, their work focuses on an approach that allows any attribute to be aggregated within the composition routine. *** Response time and run time (and in other places Service Rate) have the same or different definitions. This table places those attributes into separate columns to show the different meanings across the survey of related literature.

236

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

Sustainability, and Resiliency. By correlating the QoS attributes to these principles, we have developed a list that is suitable for assessing an organization based on existing SLAs. The three principles (illustrated in Fig. 1 as a Unified Modeling Language class diagram) are defined below in addition to their underlying SLA measures. 3.1. Compliance (suitability) Compliance is the principle that ensures that the consumer receives the requested composite capability at the service level that is required. The functional notion of web service composition (as illustrated in Fig. 1) fits within this principle, since the consumer specifies their required outputs of the composition. Considering SLA terms, the composition process must assure that the aggregate cost, uptime, and run time are compliant with the user requirements. Cost is the sum total price of all services participating in the solution process. Uptime is a guarantee by the service providers that their services will be available a specified percentage of the time per day or month. Finally, run time is the time it takes to complete the process by adding the response times of each service in the composition. 3.2. Sustainability Sustainability is the ability to maintain the underlying services in a timely fashion. Success rate, Negotiation/renegotiation, and problem resolution are related to ensuring the process continues to execute effectively. Success rate is defined by the historical rate at which a provider successfully completes a request. A consumer will require assurance that a particular business is capable of agreeing on contract terms (i.e. negotiation/renegotiation) in a timely manner. Moreover, the result of a negotiation scheme might be the development of new agreement. In addition, the service providers must be capable of resolving highimpact problems (perhaps identified by the consumer) in a timely manner. Success rate, negotiation, and problem resolution times ensure that a consumer can meet the demands of their end users. 3.3. Resiliency Resiliency is the principle of a service to perform at high service levels over an extended period of time. This principle was derived from our work with data-centric government transactional systems. The resiliency principle specializes many of the principles that underly the general notion of mean time between failures (MTBF) [3,31]. Low resiliency is represented by a service that is frequently taken off-line for maintenance. Additional, the frequency of updates may impede the predictability of its operation. Consumers will need adequate notice prior to maintenance downtimes. In addition, resiliency dictates a low frequency of maintenance downtime. Peer consumers may also add comments about a particular provider and thus create a quantified reputation. The reputation rating also influences the resiliency of a service. The model elements of our work were derived from our literature review and from work with collaborating stakeholders acknowledge

in this paper. As such, we assume the validity of this model as the applicability of the work to other domains, in some sense, relies on these underlying elements. The paper proceeds with a formal definition of all attributes relevant to aggregating and assessing a group of SLAs, and subsequently a description of how the information can be stored physically. The final sections discuss the approach and performance for aggregating SLAs of many business processes. 4. Aggregating and assessing service level agreements When assessing a group of web service workflows, the resulting assessment must be a result of the aggregated SLA terms of the underlying services. In some cases, aggregating the SLA terms is as straightforward as adding the measures of each of the dependent services, but in other cases the aggregate measure must be created based on consumer requirements. Since there may be subjectivity with respect to how the aforementioned SLA measures can be aggregated and calculated, we introduce the following formal concepts for composing the SLA measures in the context of web service composition. 4.1. Defining the physical entities for assessing SLAs

Definition 1. Server Composition in a service-oriented architecture involves a consumer collaborating with a producer. The mapping from service to resources (servers and computational units) must consider system boundaries (i.e. which servers are dedicated, which are not), the size of the messages and transactions, and initialization/set up costs. Here, we illustrate an over-simplified correlation of the services to the server. The capabilities of the producers and consumers are hosted on servers. Each server can be characterized by its performance, uptime percentage, throughput, and the pre-notification time, i.e., the server informing its constituents prior to any downtime associated with server enhancements and repairs. A server, v, can be formally defined as a tuple of these SLA measures as shown below:   v  Perf v ; Upv ; Tiv; Tov ; PreNT v 1

where Perfv is the performance of the server measured loosely in computations/second, Upv is the uptime percentage of the server, Tiv is the throughput of input messages measured in bytes/second, Tov is the throughput of output messages measured in bytes/second, and PreNTv is the pre-notification time measured in seconds. Definition 2. Set of servers Let  be the set of servers of all producer and consumers involved in the collaboration.   fv1 ; v2 ; v3 ;...; vn g:

*
<<is a>>

1 1 1

<<has criteria>>

*

*

<<is a>>

<<is a>>

Fig. 1. A taxonomy of SLA measures for web services workflow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

237

where v1 , v2 , v3 , ..., vn are the producer and consumer servers. Each vi  (for all i = 1 to n), is a tuple of SLA measures as described above. Definition 3. Service A service can be defined by some function (in some cases the average) of computations (i.e. processor cycles or relevant measure for the specific domain) it requires based on the nature of the processing domain. This computation must incorporate length of the message it receives when realizing its capabilities. A service, s, can be defined as the following pair: s  Comps ; MLs  2

where Comps is the number of required computations of the service and MLs is the message length measured in bytes. The intention of this formalization is to illuminate conceptually the connection between the services and the servers that host them. Definition 4. Set of services Let  be the set of all services involved in a composition.   fs1 ; s2 ; s3 ;...; sm g where s1 , s2 , s3 , ..., sm are the services involved in the composition. Each si  (for all i = 1 to m), is a pair of the number of required computations and the message length (in bytes) as described above. Definition 5. SLA of a service SLA of a service consists of specific web service-oriented measures, as defined in the lower elements in the taxonomy in Fig. 2. Let Asi be the SLA of a service si. Asi can be represented as a tuple shown below:   Asi  Upsi ; RTimesi ; SRespsi ; Cost si ; PreNT si ; RenegotT si; Repsi; Relsi 3 where Upsi is the uptime of the service specified by the agreement, RTimesi is the allowable run time, SRespsi is the allowable service response time, Costsi is the subscription cost or price of the service, PreNTsi is the maintenance pre-notification time, RenegotTsi is the renegotiation (expiration) time of the agreement, Repsi is the reputation of the service and Relsi is the reliability rating of the service. 4.2. Generating aggregate SLAs across workflows In this context, we define a web service workflow as a preestablished process (as defined by an SLA) between a consumer and provider (consisting of a group of web services). At composition time, this web service workflow is not yet an active process or instance, but the specification of all pre-established partnerships or agreements of organizations to share web services. The SLA for a body of many web service workflows is obtained by composing the set of SLAs of all the services participating in each composition. This process is similar to the traditional QoS-based service composition process although QoS measures must be separated by provider and

consumer concerns. The authors work closely with a federal government organization that leverages data-centric web services to develop the list of attributes that are most closely related to services concerned with sharing data from distributed information stores. Hence, the SLAs here consists of uptime of the composite service, the allowable run time, the service response time, the subscription cost or price, the maintenance pre-notification time, and the renegotiation time of the agreement. Let AWf represent the composite SLA that is calculated by composing the set of all agreements of all relevant services, i.e., the services on the producer servers and any other services on the consumer's server that might impact the overall system-wide assessment. Let APS represent the set of agreements of the services on the producer's servers that are involved in the composition. APS  fAPS1 ; APS2 ; APS3 ;...; APSm g Let ACS represent the set of agreements of the services on the consumer's servers that may impact the composite agreement AWf. ACS  fACS1 ; ACS2 ; ACS3 ;...; ACSn g

4.2.1. Composite service uptime (UpWf) The uptime for the composite SLA must be less than the minimum of:  The uptime Upvi for consumer's server vi  and  The agreed uptimes of the producer's services in APS = {APS1, A ..., A PSm} The relationship can be shown as: UpWf bminUpvi ; mini1tom UpPSi 沙 5
PS2,

4.2.2. Composite run time (RTimeWf) The time required for an individual service to complete its execution can be considered the task time. The repeatability of the task time while the service is in commission translates to the run time. For a web service, the run time is a function of the internet connection, the internal network connection, the hosting hardware, and the software service. Run time is defined by the provider or consumer which is captured within the SLA. The combined run time is illustrated in Fig. 2. The run time for the composite SLA must be less than the minimum of:  The producer server throughput divided by the message lengths of service input and output  The consumer server throughput divided by the message lengths of service input and output Let RTimeC represent the run time of the consumer. Including the sum of run times already guaranteed to others is important to avoid the impact of a voluminous load from external operations. The run time of the consumer server is obtained by dividing the server throughput TC by the message length of the consumer services MLCS. RTimeC  T C =MLCS 6

SLA Task Time (RunTime)

Let RTimeP represent the run time of all the stakeholders, i.e., the producers, p. It is the difference of the run times of the sum, m, of all services on the provider-side and the sum, n, of all client-side services. RTimeP  i1tom RTimePSi -i1ton RTimeCSi The run time for the composite SLA is represented as: 7

Internet

Network

System

Service

Fig. 2. Task time and run time.

RTimeWf bmin RTimeP ; RTimeC 

8

238

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

4.2.3. Composite service cost (CostWf) The cost of using a web service can be aggregated to understand the price of an entire process. The cost of an individual service is the sum of:  The bandwidth used (i.e. the product of the usage frequency and the message length) multiplied by the cost per byte/sec of bandwidth, which is a predefined constant BC. BC could be zero if bandwidth costs are negligible to the service provider.  The computations used multiplied by the cost of computation/sec, which is some predefined constant CC. As above, CC could be zero if computation costs are negligible to the provider.  The sum of the costs of all dependent services. This results in the following equation:     Cost Wf  RTimeWf  MLCS  BC  RTimeWf  C S  CC  i1tom Cost PSi 9

 The reputation for the consumer server vi  The average reputation for all the dependent services This relationship can be illustrated as: RepWf  averageRepvi  i1tom RepPsi =m 12

4.2.7. Composite service reliability (RelWf) Reliability in traditional QoS-based web service composition has been calculated using several approaches in related work [19]. Unlike other approaches, this approach deals with SLA specifications which tend to be the worst case agreement between consumer and provider. As such, we believe closest estimate is to aggregate the values by calculating the minimum of:  The reliability for the consumer server vi  The minimum of reliabilities for all the dependent services This relationship can be illustrated as: RelWf minRelvi ; mini1tom RelPSi 沙 13

4.2.4. Composite service pre-notification time (PreNTWf) When a service must be disabled for maintenance, organizations must inform their collaborators. The minimum time for notification before maintenance begins is calculated as the minimum of:  The time of notification for the consumer server vi  The minimum of all notification times agreed upon for all dependent services This relationship can be illustrated as: PreNT Wf minPreNT vi ; mini1tom PreNT PSi 沙 10

Definition 6. Composite SLA The composite SLA, AWf is a tuple of all the aggregated SLA measures as shown:
  AWf  UpWf ; RTimeWf ; SRespWf ; Cost Wf ; PreNT Wf ; Renegot T Wf ; RepWf ; RelWf

4.2.5. Composite service renegotiation time (RenegotTWf) Negotiation/renegotional time is the guarantee giving by the provider and consumer that defines how quickly a request for new QoS attributes will be acknowledged the other party. The renegotiation date for this agreement must be after the predefined constant waiting time, WT, with respect to each other's agreement to which this server is a party. This notion of renegotiation time is illustrated in Fig. 3. The equation below must hold for each agreement ACSi ACS to which the service provider is a party. This test can be shown as: f or all i  1 to n; Renegot T Wf  maxRenegotT CSi  WT  11

where the aggregated SLA measures UpWf, RTimeWf, SRespWf, CostWf, PreNTWf, RenegTWf , RepWf , RelWf are obtained from the relations shown in Eqs. (5), (8), (9), (10), (11), (12), and (13) respectively. 5. Representing SLA attributes as WS-Agreements (WSAG) In order to store and manage SLAs, the attributes must be represented in a format conducive for distributed data management. XML is a language that allows complex information to be represented with embedded metadata. An XML-based approach to representing SLA information facilitates quick interpretation of data and using translation techniques, such as the eXtensible Stylesheet Language, XSL, allows the comparison and aggregation of the underlying information. WSAG is an XML-based language that is defined with SLA attributes. A brief background is discussed here, but more information can be found at [21]. In a WSAG document, the data are represented hierarchically underneath the notion of an agreement. An agreement can be further specified with name or identifying string. An agreement can also be described by its context. Context information includes the name of the consumer and producer, the timeframe by which the agreement is valid, and other related template information. Each agreement encapsulates a list of terms. Terms describe the information of the services that are included. Of most importance to this work are the guarantee terms. Guarantee terms consist of a service scope which contains the service names of the specific service relevant to the guarantee. The service level objective contains a predicate for the metrics that quantitatively define

Problem resolution is not formally defined because the calculation is a similar formula as renegotiation. As a definition (shown in Fig. 4), problem resolution can be defined as the sum of the time for recognizing the error, the time that it takes for a provider to acknowledge the error, and the actual time for resolving the problem. 4.2.6. Composite service reputation (RepWf) In a service composition scenario, reputation is defined as a numeric score on a relative scale for a web service as captured by consumers and providers in a web service repository. The Reputation for the composite service is calculated as the average of:

SLA Renegotiation Time
SLA Problem Resolution Time

Request

Negotiation (Wait Time)

Error Acknowledge Recognition ment

Resolution Time by Service

Fig. 3. Aspects of renegotiation time.

Fig. 4. Aspects of problem resolution.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

239

the guarantee. The service level objective contains the parameter name, value, and unit of measure. As an example, the SLA attributes of uptime and maintenance are shown in Table 2. Uptime is a common attribute for SLAs. As described earlier, an uptime SLA guarantees the availability of a service by percentage over a designated period of time. The other SLA metric, maintenance notification time, is not as universally used as uptime. The example in Table 2 shows that maintenance notification time is also straightforward with regard to representation in the WS-Agreement notations. Capturing service level agreements in XML-based notations allows the SLA attributes to be represented in a format similar to the WSDL files that represent the operational specifications of the service. WSAG can be transported and negotiated along with WSDL files. This represents a benefit if a service stakeholder wants to evaluate multiple services, side-by-side. Another benefit of capturing SLA attributes in XML-based files is the ability of enhancing attributes with semantics. It is possible that organizations will name attributes with their own specialized naming schemes. Semantics would allow disparate organizations to mediate SLA attributes that are the same but may be named differently. This approach leverages the numerous projects that use semantics to highlight web services for mediation. However, a detriment of capturing these attributes in XML-based notations occurs when organizations have pre-established agreements. It is likely that, in a service-oriented computing environment, coalitions of businesses will form similar to partnerships that occur in traditional businesses. In such cases, the overhead of descriptive tags may be unnecessary. In addition, SLA constraints may become more comprehensive as the partnerships enhance their coordination and negotiation. These files may become too cumbersome for semi-automated manipulation where human inspection may be a required step in overall process. 6. Creating aggregated SLAs on-demand The formalization in previous sections shows the necessary measures for creating SLAs for new business workflows that reflect the effects of other workflows existing at the same organization. The authors collaborated with The MITRE Corporation for the development of a framework for defense information systems organizations shown in
Table 2 Sample SLA attributes shown in WS-Agreement representations. WS-Agreement for uptime b wsag:GuaranteeTerm wsag:Name = "uptimePref" wsag:Obligated = "ACME"> b wsag:ServiceScope > b wsag:ServiceName > AcmeService1b/wsag:ServiceName > b/wsag:ServiceScope > b wsag:ServiceLevelObjective > b wsag:predicate type = "greater"> b wsag:parameter>job:uptimePercentageb/wsag:parameter > b wsag:value>5b/wsag:value > b wsag:unit>time:secondsb/wsag:unit > b/wsag:predicate > b/wsag:ServiceLevelObjective > b/wsag:GuaranteeTerm > WS-Agreement for Maintenance b wsag:GuaranteeTerm wsag:Name="maintenanceNotificationPref"wsag: Obligated="ACME"> b wsag:ServiceScope > b wsag:ServiceName>AcmeService1b/wsag:ServiceName > b/wsag:ServiceScope > b wsag:ServiceLevelObjective > b wsag:predicate type="greater"> b wsag:parameter>job:maintPreNotificationb/wsag:parameter > b wsag:value>7b/wsag:value > b wsag:unit>time:daysb/wsag:unit > b/wsag:predicate > b/wsag:ServiceLevelObjective > b/wsag:GuaranteeTerm >

Fig. 5. These organizations host large numbers of web services that provide battlefield information such as weather information, force location and tracking information, and satellite telemetry. Each of the various defense forces (Army, Navy, Air Force, etc.) provides access their web services via this shared portal. The sources vary in their guaranteed service level objectives. The defense information systems organizations are devising portals that manage SLAs in governance database while recording historical service level information. The approaches devise in this paper are incorporated within the portal logic. As such, when producers provide new services and when clients gain access to existing services, SLAs are verified for validity. In such an ad-hoc environment, it is important to understand if the SLA composition process can be performed efficiently enough to support the real-time insertion of new partners looking to exploit existing services. In this work, we define an integrated process for workflow-based SLA composition when suggesting new SLAs. This process can be decomposed into two steps, SLA composition and evaluation. These two steps are illustrated in Fig. 6. The SLA composition step is similar to traditional QoS-based web service composition approaches. In the evaluation step, user preferences are used to prioritize the list of candidate service chains. Numerous dynamic programming techniques can be used to achieve this step. We present a unique approach where instead of acquiring specific QoS value expectation from the user, instead user's priorities are collected. A quality score is generated based on the user's preferences. Our evaluation shows that, in real-time operations, the composition and evaluation steps are feasible considering a large number of existing business processes. 6.1. Composition: building candidate workflows In order for SLA measures to be useful for both real-time operations and decision support, the web services workflow generation must integrate traditional web service composition with the SLA composition procedures. The SLA composition procedure must be integrated at each step and also applied to the workflow as a whole once the full process is generated. We defined the information provided by the user to be the user.predicate and the desired outcome to be the user.reqresults. The step.predicate is the set of information used to select subsequent services in the composition. At the initiation of a composition routine, the user.predicate is equivalent to the step.predicate. These relationships are illustrated in Fig. 7. We introduce an integrated procedure that combines standard web service composition with SLA composition. Table 3 shows the pseudocode of the integrated process. The composition process has a main integrated process, IntegratedComp(). The StepCompose() process occurs at each step and WorkflowChk() process occurs once the required information and actions are realized with the execution of the sequence of web services. The ComposeSLA() process is the computation mechanisms that implement the SLA aggregation procedures. 6.2. Evaluation: prioritizing SLA compositions In the prior section, candidate service chains are generated that meet the functional and SLA requirements of the user. Nevertheless, at this point, there are still multiple chains that can fulfill a capability. As such, there remains an open requirement to sort the chains based on quality and choose the best chain in the group. In order to prioritize service chains, users are asked to provide a priority, Pr, for each attribute with respect to their environment, where Rr = {Pr1, Pr2, Pr3,..., Prn}. The priorities represent the rank ordered SLA measures from greatest to least importance. The priorities relate to the corresponding set of SLA measures, where SLA = {SLA1, SLA2, SLA3,..., SLAn}. A SLA attribute has the best possible value, B. Our approach will strongly consider chains that perform favorably with respect to the user's preferences. After evaluation, GS and GW represent the quality score for the service and workflow, respectively.

240

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

Governance Governance DB

Certification Module

WS Interface

Web Service DB
WS interface

WS Interface

Historical QoS DB
WS interface

Concerns .... Module WS interface

QoS-Repos/Gov Manager

WS-Repos Manager

QoS Manager

Portal User Interface

Introduce new web service that require new SLA generation in Governance DB

Service Provider Service Consumer

Many different providers/clients on the Portal
Fig. 5. A practical operational scenario.

Require regular access to existing service that affects existing SLAs

As such, the quality score for an individual service can be defined by weighing the user's priority with respect to the ratio of the SLA measure to the best possible measure for that attribute. The calculation is defined as:  n  X SLAn -BSLAn  GS  14 Prn  BSLAn  n0 The corresponding quality score for the service chain is: GW  X
m0

[12] which focuses on decision optimization. However, in the next section, we demonstrate that this approach performs favorably as a real-time assessment during the dynamic composition process. 7. Performance evaluation of the integrated composition process In real-time operations, SLA composition will be required to occur within a reasonable response time. In this work, we evaluate expected response time for the composition and prioritization of SLA measures. Our experiments are performed on a Mobile Intel Pentium 4, 2.4 GHz, 1 GB RAM, running Windows XP and the Java Runtime Environment (JRE) 6 Update 1. An initial experiment was performed that evaluates the response when prioritizing SLA-

GS m



15

This approach relies on user-supplied priority weights and perhaps lacks the precision of standard multiple criteria decision analysis

SLA COMPOSITION

EVALUATION

Assess existing and new workflow chains

2. Prioritize predicted SLAs based on existing services with respect to user preferences
Chain1.effectiveness= WSa WSb + WSc + WSd

Chain 1
WSc

Chain2.effectiveness= WSa WSb + WSc + WSd ChainN.effectiveness= WSa WSb + WSc + WSd

WSa

WSb WSc

WSd

Chain 2
WSa WSb WSc WSd

Chain n
WSa WSb WSc WSd

Fig. 6. Two-step process for integrated SLA workflow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

241

step.predicate user.predicate WS WS

WS WS WS user.ReqResult

Fig. 7. Data flow in a composition routine.

annotated workflows of web services. Based on a set of random WSAG files (with uniformly distributed randomly generated SLA terms and their values), we created software that generates web service objects. In this context, using a uniformly distributed, randomly generated set of attributes for values is appropriate. This experimentation is a proof of concept that the SLA composition can execute fast enough to appropriately be included into the real-time composition scenarios. This is mostly an information management procedure. The authors understand that a non-uniform list of attributes may incur additional overhead. Here, the argument is made that datacentric organizations must keep their list of guarantees (as represented by attributes) uniform, such that their operations are consistent across each of their consumers. It is also not implausible that the provider use such policy to ensure a predictable processing time. Each service objects has a unique identification codes that associates it with the corresponding WSAG files. We duplicated identification codes (i.e. ids) such that increasing numbers of services have the same ids. The process of aggregating ids was done in an attempt to

Table 3 Integrated composition pseudo-code. IntegratedComp: Main integrated composition function StepCompose: Function that occurs at each step WorkflowChk : Process-Level Functional/SLA check composeSLA: Function that aggregates SLA measures PR,RenegotT,Cost : Problem Resolution, Renegotiation, Cost RTime,Up,PreNT: Run time, uptime, maintenance Rel,Rep: Reliability, reputation step.predicate: Message information required to execute service ws WSDL Object with SLA measures user.reqresults: Message information required as output of service service_chain: Candidate workflow of web services IntegratedComp { step.predicate = user.predicate WHILE (step.predicate ! = user.reqresults) { StepCompose() } WorkflowChk() } StepCompose { FOR EACH candidate ws where ws. message step. predicate THEN ADD (ws) to List b ws > FOR EACH candidate ws IN List b ws> IF ((ws.PR b = step.PR) || (ws.RenegotT b = step.RenegotT) || (ws.Cost > step.Cost) || (ws.RTime b = step.RTime) || (ws.Up b = step.Up) || (ws.PreNT b = step.PreNT) || (ws.Rel b = step.Rel) || (ws.Rep b = step.Rep)) THEN REMOVE (ws) from List b ws> ELSE { ADD ws to List b service_chain > ADD ws.outputs to List b step.predicate > } } WorkflowChk { FOR EACH service_chain IF (chain. outputs user.reqresults ) && (ComposeSLA (ws.PR, ws.RenegotT ,ws.Cost, ws.Up, ws.PreNT, ws.RTime, ws.Rep, ws.Rel ) b user.SLA)) THEN ADD (service_chain) to Candidate List }

simulate composition as a prerequisite step to the actual SLA composition. Each web service object was populated with six SLA measures. Although there are more attributes, we decided to experiment with 6 with the expectation that more attributes would scale consistently. Our experimentation searches the repository of web service objects, composes services of the same id, and then prioritizes the resulting chain. This experiment was executed on a repository with varying sizes from 100 to 100,000 services. The workflow size (number of services in a chain) was also varied. The number of services per chain is varied from 10 to 100,000 services within a repository of 1,000,000 services. Fig. 8 shows the results of this first experiment. Response time represents the average time required for the proposed algorithm to correlate the SLA attributes to generate a composite measure for a particular web service composition routine. Considering a reasonable workflow of web services of 100 interconnected services or less, a response time of 1.6 ms per service chain is promising. As a variation of the response time experimentation, we also investigated how the size of the repository affects the performance of our algorithm. Fig. 9 shows the response time of our algorithm as the repository increases. The workflow chain of the composition request is held constant at 10 services but the repository increases from 100 to 100,000. In the results of this experiment, search time is also considered. Given the largest repository of 100,000 services, the SLA composition time (including the processing time for discovering the relevant services (i.e. 10 services) is 30.2 s. Although the results are favorable in a simulated environment, the reader should understand that many other conditions with regard to performance variations and real-time factors on open systems reduce the confidence of these results. In a second experiment, we evaluated the performance by varying the number of SLA attributes that are used for calculation. In this paper, we experiment with up to six attributes, but we expect that other attributes will be required to extend this approach in the future. As such, it is important to understand the overhead associated with adding a new SLA attribute for real-time operations. Although it is understood that the performance increases as the computing hardware is improved, we are generally interested in how the approach scales as the number of attributes increases in a fixed-size repository. Fig. 10 shows the search and calculation time when varying the numbers of attributes (i.e. the number of attributes tested for each). Although this graph has a high concentration of search time (~ 90%), it is clear that the performance degrades favorably (linearly) at less than 12 ms per attribute (i.e. the calculation time increases less than 1 millisecond for the addition of each new attribute). Another variation of this experiment, also shown in Fig. 10, considers the impact of SLAs that process fewer attributes. In some cases, service agreements may only consider a subset of the total possible guarantee conditions. As such, less attributes may be stored per service (i.e. Attributes Stored). The Attributes Stored measure in Fig. 10 shows that there is only a slight advantage for service-oriented providers to be less stringent. Another variation considers increasing performance of SLA composition by limiting the number of attributes that a service composition considers. The major question is "Can runtime performance increase if the business management system only considers a subset of possible SLA attributes?". Experimentation shows that even at the most extreme case, if a service provider only allows services to be constrained by one attribute, the performance is only improved by approximately 30%. Predictably, as shown in

242

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

Performance Evaluation by Number of Services per ID (constant 100000 services with search time subtracted)
1000

SLA Composition Time (Milliseconds)

100

10

1
10 100 1000 10000

0.1

Services per ID
Fig. 8. The performance of the SLA prioritization function as the number of services per workflow increase (service discovery time excluded).

Fig. 10, the improvement in performance decreases as more attributes are considered. 8. Conclusion When provider organizations expose their services for consumption by their peers, it is important for them to understand what they are guaranteeing. Moreover, consumers that receive such commitments must abide by their own commitments such that the providers can meet their guarantees to all consumers. In this work, we introduce a method of assessing an organization based on both pending and existing SLAs. Since the estimation of QoS measures for web services has been investigated in great detail, in this paper, we make a varied contribution. Our work builds on the existing studies by considering the QoS guarantees, as captured in SLAs, such that provider and consumer concerns can be modeled independently. This work also considers that SLA assessment occurs in a separate process than standard QoS estimation at service composition time. Here, we introduce high-level criteria that can be created from the aggregation of a comprehensive list of lower-level QoS-based attributes. This variation to related work facilitates automated cross-enterprise SLA negotiation. In this paper, we define a two-step process for composing SLAs and evaluating their efficiency. Consistent with results in related

work that estimate QoS on fully operational web services, we have found through simulated experimentation that the management of third-party SLA information can be composed efficiently in parallel with the actual operational service composition. In future work, we plan to implement our approach within a real operational setting as opposed to the simulation setting that is represented in this paper. In the real operational setting, data will be produce using a variety of distributions. As such, WS-Agreement files will be appended to WSDL files and evaluated in a network environment for performance and feasibility. In addition, we plan to extend the SLA composition paradigm to protocols that mandate the discovery process within web service registries. In this way, it may be possible for adaptive software (or intelligent agents) to negotiate SLAs, in real time, while they perform on-demand, service discovery. Acknowledgment This work was benefited by the participation of Dr. M. Brian Blake in the Service Level Agreement Technical Exchange Meeting held at The MITRE Corporation on July 2006 in McLean, Virginia. In addition, the service discovery approach/software used in this work was partially funded by the National Science Foundation under award number 0548514.

Performance Evaluation by Repository Size (constant 10 services per workflow chain)
100

SLA Composition Time (Seconds)

10

1
100 1000 10000 100000

0.1

Number of Services
Fig. 9. The performance of the SLA prioritization function as the repository increases (service discovery time included).

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244

243

Performance Evaluation by Attributes Stored and Attributes Tested (100,000 services, 10 services per chain)
80

SLA Composition Time (Milliseconds)

70 60 50 40 30 20 10 0 1 2 3 4 5 6

Attributes
Attributes Stored Attributes Tested

Fig. 10. The performance overhead associated with the addition of new attributes.

References
[1] P. Alipio, S. Lima, P. Carvalho, XML service level specification and validation, Proc. of the 10th IEEE Symposium on Computers and Communications (ISCC), June 2005, pp. 975980. [2] A. Andrieux, K. Czajkowski, A. Dan, K. Keahe, H. Ludwig, T. Nakata, J. Pruyne, J. Rofrano, S. Tuecke, M. Xu, "Web Service Agreements Specification (WS-Agreement)," Proposed Recommendation, Open Grid Forum (OGF) Document Number GFD-R-P.107, Mar 2007 OGF Grid Resource Allocation Agreement Protocol Working Group (GRAAP-WG)Accessible at, http://www.gridforum.org/Public_Comment_Docs/Documents/Oct-2005/WSAgreementSpecificationDraft050920.pdf. [3] J.E. Angus, On computing MTBF for a k-out-of-n: G repairable system, IEEE Transactions on Reliability 37 (3) (1988) 312313. [4] M.B. Blake, B2B electronic commerce: where do agents fit in? Proceedings at the AAAI-2002 Workshop on Agent Technologies for B2B E-Commerce/AAAI Press, Edmonton, Alberta, Canada, July 2002. [5] M.B. Blake, Decomposing composition: service-oriented software engineers, IEEE Software 24 (6) (Nov/Dec 2007) 6877. [6] M.B. Blake, A lightweight software design process for web services workflows, Proc. of the 4th IEEE International Conference on Web Services (ICWS), Sept. 2006, pp. 411418. [7] M.B. Blake, D.J. Cummings, Workflow composition of service level agreements, Proc. of the IEEE International Conference on Services Computing (SCC 2007), July 2007, pp. 138145. [8] M.B. Blake, M. Gini, Guest editorial: agent-based approaches to B2B electronic commerce, International Journal of Electronic Commerce 7 (1) (2002) 113114. [9] G. Canfora, G. M Di Penta, R. Esposito, F. Perfetto, M.L. Villani, Service composition (re)binding driven by application-specific QoS, Proc of International Conference on Service-Oriented Computing (ICSOC), 2006, pp. 141152. [10] A.J. Cardoso, "Quality of Service and Semantic Composition of Workflows," Ph.D. Dissertation, University of Georgia, 2002. [11] G. Di Modica, V. Regalbuto, O. Tomarchio, L. Vita, Dynamic re-negotiations of SLA in service composition scenarios, Proc. of the 33rd EUROMICRO Conference on Software Engineeringand Advanced Applications, August 2007, pp. 359366. [12] J. Dyer, Maut -- multiattribute utility theory in multiple criteria decision analysis: state of the art surveys, International Series in Operations Research, Management Science, vol. 78, Springer, 2005, pp. 265292. [13] T. Falkowski, S. Vob, Application service providing as part of intelligent decision support for supply-chain management, Proc. of the 36th Annual Hawaii International Conference on System Sciences (HICSS), vol. 3, 2003, p. 80. [14] M. Gillman, G. Weikum, W. Wonner, Workflow management with service quality guarantees, Proc. of ACM SIGMOD International Conference on Management of Data, 2002, pp. 223239. [15] L. Green, Service level agreements: an ontological approach, Proc. of the 8th ACM International Conference on Electronic Commerce (ICEC), August 2006, pp. 185194, Fredericton, Canada. [16] D. Greenwood, G. Vitaglione, L. Keller, M. Calisti, Service level agreement management with adaptive coordination, Proc. of the International Conference on Networking and Services (ICNS), July 2006, p. 4550, Silicon Valley, USA. [17] C.-W. Hang, M.P. Singh, Trustworthy service selection and composition, ACM Transactions on Autonomous and Adaptive Systems 6 (1) (Feb 2011). [18] L. Jin, V. Machiraju, A. Sahai, Analysis on service level agreement of web services, HP Technical Report, HPL-2002-180, June 2002, accessible at (2008), http://www. hpl.hp.com/techreports/2002/HPL-2002-180.pdf. [19] J. Ko, C.O. Kim, I. Kwon, Quality-of-service oriented web service composition algorithm and planning architecture, Journal of Systems and Software 81 (11) (November 2008) 20792090. [20] H. Ludwig, A. Keller, A. Dan, R.P. King, R. Franck, Web Service Level Agreement (WSLA) Language SpecificationAccessible at, http://www.research.ibm.com/wsla2007.

[21] M. Mecella, M. Scannapieco, A. Virgillito, R. Baldoni, T. Catarci, C. Batini, Managing data quality in cooperative information systems, Lecture Notes in Comptuer Science 2512 (2002) 486502. [22] M. Mohabey, Y. Narahari, S. Mallick, P. Suresh, S.V. Subrahmanya, An intelligent procurement marketplace for web services composition, Proc. of the IEEE/WIC/ACM International Conference on Web Intelligence, Nov. 2007, pp. 551554. [23] N.J. Muller, Managing service level agreements, International Journal of Network Management 9 (3) (May 1999). [24] F. Naurmann, U. Leser, J.C. Freytag, Quality-driven integration of heterogeneous information systems, Proc. of the 25th International Conference on Very Large Databases, September 1999, pp. 447458, Edinburgh, Scotland, UK. [25] N. Oldham, K. Verma, A.P. Sheth, F. Hakimpour, Semantic WS-Agreement partner selection, Proc. of the 15th International World Wide Web Conference (WWW), 2006. [26] F.R. Reiss, T. Kanungo, Satisfying database service level agreements while minimizing cost through storage QoS, Proc. of the IEEE International Conference on Services Computing (SCC), July 2005, pp. 1321. [27] A. Sahai, V. Machiraju, M. Sayal, A. Moorsel, F. Casati, Automated SLA monitoring for web services, Proc. of the IEEE/IFIP International Workshop on Distributed Systems: Operation and Management (DSOM), October 2002, pp. 2841, Montreal, Canada. [28] D. Skene, Lamanna, W. Emmerich, Precise service level agreements, Proc. of the 26th International Conference on Software Engineering (ICSE), 2004, pp. 179188, Edinburgh, UK. [29] I. Sorteberg, O. Kure, The use of service level agreements in tactical military coalition force networks, IEEE Communications Magazine 43 (11) (November 2005) 107114. [30] W. Sun, Y. Xu, F. Liu, The role of XML in service level agreements management, Proc. of the International Conference on Services Systems and Services Management, June 2005, pp. 11181120. [31] K.M. van Hee, L.J. Somers, M. Voorhoeve, A modeling environment for decision support systems, Decision Support Systems 7 (1) (1991) 241251. [32] G. Xiaohui, K. Nahrstedt, A scalable QoS-aware service aggregation model for peer-to-peer computing grids, Proc. of the 11th IEEE International Symposium on Higher Performance Distributed Computing (HPDC), 2002, pp. 7382. [33] J. Yan, R. Kowalczyk, J. Lin, M.B. Chhetri, S.K. Goh, J. Zhang, Autonomous service level agreement negotiation for service composition provision, Future Generation Computer Systems 23 (6) (July 2007) 748759. [34] T. Yu, K.J. Lin, Service selection algorithms for composing complex services with end-to-end QoS constraints. Proc. 3rd International Conference on Service Oriented Computing (ICSOC2005), The Netherlands Amsterdam, 2005. [35] L. Zeng, B. Benatallah, A.H.H. Ngu, M. Dumas, J. Kalagnanam, H. Chang, QoS-aware middleware for web services composition, IEEE Transactions on Software Engineering 30 (5) (May 2004) 311327. [36] Y. Zhang, K.J. Lin, J.Y.J. Hsu, Accountability monitoring and reasoning in service oriented architectures, Service-Oriented Computing and Applications 1 (2007) 3550. [37] M. zur Muehlen, J. Nickerson, K.D. Swenson, Developing web services choreography standards -- the case of REST vs. SOAP, Decision Support Systems 40 (1) (2005).

M. Brian Blake received the BS degree in electrical engineering from the Georgia Institute of Technology, Atlanta and the PhD degree in information technology with a concentration in information and software engineering from George Mason University, Fairfax, Virginia. He is currently Professor of Computer Science and Associate Dean of Engineering at the University of Notre Dame, Indiana. As a professor, he has published more than 120 journal and refereed conference papers in the domains of workflow and agent-based systems, service-oriented computing, distributed data management, and software engineering education. His investigations cover the spectrum of software engineering: design, specification, proof of correctness, implementation/experimentation, performance evaluation, and application.

244

M.B. Blake et al. / Decision Support Systems 53 (2012) 234244 Srividya Kona Bansal received her B. Tech. degree in Computer Science from NIT (previously known as REC), Warangal, India, M.S. in Computer Science from Texas Tech Univ., Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas. She is currently an assistant professor in the College of Technology and Innovation at Arizona State University. She has over 5 years of industry experience. Her research interests include Service-Oriented Computing, Semantic Web, Software Engineering, Engineering Education, and Bioinformatics.

Ajay Bansal received his B. Tech. degree in Computer Science from NIT (previously known as REC), Warangal, India, M.S. in Computer Science from Texas Tech Univ., Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas. He is currently a lecturer in the College of Technology and Innovation at Arizona State University. He has over 3 years of industry experience. His research interests include Programming Languages, Logic Programming, Declarative Programming, Automated Reasoning, Service-Oriented Computing, Semantic Web Services, and Language-based Security.

Swarm and Evolutionary Computation 8 (2013) 3343

Contents lists available at SciVerse ScienceDirect

Swarm and Evolutionary Computation
journal homepage: www.elsevier.com/locate/swevo

Regular Paper

Economic analysis and power management of a stand-alone wind/ photovoltaic hybrid energy system using biogeography based optimization algorithm
Rajesh Kumar a,n, R.A. Gupta a, Ajay Kumar Bansal b
a b

Department of Electrical Engineering, Malaviya National Institute of Technology, J. L. N. Marg, Jaipur 302017, Rajasthan, India Poornima Institute of Engineering & Technology, ISI-02, RIICO Institutional Area, Goner Road, Sitapura, Jaipur 302022, Rajasthan, India

a r t i c l e i n f o
Article history: Received 14 February 2012 Received in revised form 15 August 2012 Accepted 16 August 2012 Available online 8 September 2012 Keywords: Hybrid energy system Wind turbine system Solar photovoltaic energy Renewable energy Remote area power generation Power generation economics

abstract
The stand-alone energy system having a photovoltaic (PV) panels or wind turbines have low reliability and high cost as compared with wind/PV hybrid energy system. In this study, Biogeography Based Optimization (BBO) algorithm is developed for the prediction of the optimal sizing coefficient of wind/ PV hybrid energy system in remote areas. BBO algorithm is used to evaluate optimal component sizing and operational strategy by minimizing the total cost of hybrid energy system, while guaranteeing the availability of energy. A diesel generator is added to ensure uninterrupted power supply due to the intermittent nature of wind and solar resources. Due to the complexity of the hybrid energy system design with nonlinear integral planning, BBO algorithm is used to solve the problem. The developed BBO Algorithm has been applied to design the wind/ PV hybrid energy systems to supply a located in the area of Jaipur, Rajasthan (India). Conventional methods require calculation at every single combination of sizing, operation strategy and the data for each variation of component needs to be entered manually and execute separately. Results show that the hybrid energy systems can deliver energy in a stand-alone installation with an acceptable cost. It is clear from the results that the proposed BBO method has excellent convergence property, require less computational time and can avoid the shortcoming of premature convergence of other optimization techniques to obtain the better solution. & 2012 Elsevier B.V. All rights reserved.

1. Introduction The public attention has remained focused on the renewable technologies as environmentally sustainable and convenient alternatives. Wind and solar power are the two most widely used renewable sources of energy among all renewable sources, since they feature definite merits as compared with the conventional fossil-fuel-fired generation. For instance, wind turbine generators (WTGs) neither generate pollution nor consume depleting fossil fuels. Photovoltaic (PV) systems produce no emissions, are durable, and demand minimal maintenance to operate [1]. Unfortunately, these renewable sources of energy are essentially intermittent and quite variable in their output. In addition, they require high capital costs. Power to off-grid location is usually supplied by a generator using diesel or petrol [2]. These generators are often available at night and only for a certain number of hours as explained by

Corresponding author. Tel.:  91 141 2713372; fax:  91 141 2529029. E-mail addresses: rkumar.ee@gmail.com (R. Kumar), ragmnit@gmail.com (R.A. Gupta), ajaykb007@gmail.com (A.K. Bansal). 2210-6502/$ - see front matter & 2012 Elsevier B.V. All rights reserved. http://dx.doi.org/10.1016/j.swevo.2012.08.002

n

Musseli et al. [3]. During the designing of a hybrid system, it is necessary to select the size of various components with the operation strategy for the long-lasting, reliable and cost-effective system [4]. Many researchers have shown that hybrid energy systems are best suited to diminish dependence on fossil fuel by using available wind speed and solar radiations [5,6]. Hybrid energy system includes photovoltaic (PV) panels and/or wind turbines and batteries, etc. These energy systems are the cost-effective solutions to meet energy requirements of remote areas [7]. Das et al. [8] suggested that Evolutionary Algorithms (EAs), due to their population-based approaches, are able to detect multiple solutions within a population in a single simulation run and have a clear advantage over the classical optimization techniques, which need multiple restarts and multiple runs in the hope that a different solution may be discovered every run, with no guarantee [9]. However, numerous evolutionary optimization techniques have been developed since late 1970s for locating multiple optima (global or local). Due to significant improvement in the capability of computers in recent years [10], evolutionary algorithms (EAs), such as genetic algorithm (GA), evolutionary programming (EP), particle swarm

34

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

optimization (PSO) [11] and differential evolution (DE) are being applied for solving various hybrid energy system optimization problems to overcome some of the drawbacks of conventional techniques [12]. Barley et al. [13] has suggested guidelines regarding main operation strategies, namely frugal discharge, load-following, the state of charge (SOC) set point and the full-power strategy. However, the SOC set point procedure is user-defined, and it is not optimized [14]. Frugal discharge is based on critical load, where if the net load is exceeding the critical load, then it is cost-effective to run the generator set. In load-following strategy, batteries are not charged by the diesel generator. Belfkira et al. [15,16] explained that diesel operating point is set to match the net load. SOC set point strategy is used to charge batteries at the user defined point from the diesel generator. Bernal-Agustin et al. [17] specifies that generator operates at full-power generation with the excess power is used to charge the batteries without dumping power. Otherwise, the generator is set to operate at the maximum point without dumping. In Full power strategy, the diesel generator is operated at full power for a minimum time at a low set point. Seeling-Hochmuth [18] had investigated the application of the genetic algorithm to solve the optimization problem with various constraints. He further suggested an optimization concept combining system sizing and operation control. Koutroulis et al. [19] used Genetic Algorithm(GA) to minimize the total system cost based on the load energy requirements. Daming et al. [20], Gupta et al. [21] and Sopian et al. [22] explained a methodology of finding optimum component sizing and operational strategy using the genetic algorithm. Dufo-Lopez et al. [23] developed a program based on genetic algorithm, called HOGA, for optimizing the configuration of a PV diesel hybrid system with AC loads and the control strategy. Hakimi et al. [24] applied PSO for multi-criterion design of the hybrid power generation system. Bansal et al. [25] use Meta Particle Swarm Optimization algorithm for finding the optimal size of the Wind/ PV energy system. Ashok developed a reliable system operation model based on Hybrid Optimization Model for Electric Renewable (HOMER) [26] found an optimal hybrid system among different renewable-energy combinations while minimizing the total lifecycle cost. Dufo-Lopez et al. [27] later improved HOGA program to include fuel cell and hydrogen in the hybrid system. However, the control strategies in HOGA are same as used in HOMER. It is focused on maximizing the renewable energy components, while trying minimizing the use of the generator to provide for the load demand. Very recently, a new optimization concept, based on biogeography has been proposed by Simon [28]. Biogeography Based Optimization (BBO) is a population-based evolutionary algorithm (EA)[29]. Biogeography is the study of the geographical natural distribution of biological organisms. In the BBO algorithm, each solution of the population is represented by a vector of integers. BBO algorithm adopts the migration operator to share information among solutions [30]. This feature is similar to other biology-based algorithms, such as Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). It makes BBO applicable to the majority of problems, where GA and PSO are applicable [31]. Simon [28] compared BBO with many other Evolutionary Algorithms on a wide set of benchmark functions. The results confirmed the excellent performance of BBO. The Markov analysis also proved that BBO outperforms GA on basic unimodal, multimodal and deceptive benchmark functions when used with low mutation rates. The versatile properties of BBO algorithm encouraged the authors to apply this algorithm to solve the non-convex, complex optimal sizing problem of hybrid energy systems. Hybrid energy system sizing is a nonlinear integral problem, which is a complex problem. The objective of this paper is to explore the application of the BBO algorithm to the hybrid energy system design problem. The combination of components

represents the sequence of the suitability index variables (SIVs), which determine the total cost of the system. After the migration operation in BBO, a SIV in the immigrated island (a bad solution) accepts the sharing information from the emigrated island (a better solution). To keep the new solution feasible, adjust the SIV which has the identical component cost [32]. The BBO algorithm has certain unique features, which overcome several demerits of the conventional methods as mentioned below (1) In BBO and PSO, the solutions survive forever although their characteristics change as the optimization process progresses. However, solutions of evolutionary-based algorithms like GA, DE etc. ``die'' at the end of each generation. Due to the presence of crossover operation in evolutionary based algorithms, many solutions, whose fitness is initially favorable, sometimes lose their quality in later stage of the process. In BBO, there is no crossover like operation as the solution gets fine-tuned gradually as the process goes on through migration operation. Elitism operation has made the algorithm more efficient in this aspect and gives an edge to BBO over other techniques. (2) In PSO, solutions are more likely to clump together in similar groups. While in the case of BBO, solutions do not have the tendency to cluster due to its new mutation operation. (3) BBO involves fewer computational steps per iteration as compared to other algorithms like GA, PSO, DE etc. Due to this, BBO results are faster in convergence. (4) In BBO, poor solutions accept a lot of new features from good ones, which may improve the quality of solutions. This is a unique feature of BBO algorithm compared to other techniques. At the same time, this makes constraint satisfaction to be much easier, compared to other algorithms. In this paper, the BBO optimization algorithm uses the static models of the wind turbine, the PV panel, the battery, the inverter and on the dynamic evaluation of the wind and solar-energy potential. BBO is used to simply solve the size of the hybrid PV/ wind energy system by considering economical and reliability constraints of the system. The new method is suitable to deal with the complex design of hybrid energy system and can avoid the local minimum trap. The developed BBO methodology has been applied to design the stand-alone hybrid wind/PV systems to power supply a varying load located in the area of Jaipur, Rajasthan (India) with geographical coordinates defined as: latitude: 26192 N, longitude: 75182 E and altitude: 431 m above sea level. This paler is organized as follows. In Section 2, the hybrid energy system and its components are explained. Section 3 describes the optimization problem of hybrid system and Section 4 explains the simplified BBO. In Section 5, detail of Case study data is presented and Section 6 shows the comparison of Hybrid Optimization Model for Electric Renewable software (HOMER) [26], Biogeography Based Optimization (BBO) [28], Genetic Algorithm (GA) [22], particle swarm optimization (PSO) [23], comprehensive learning particle swarm optimization (CLPSO) [33] and ensemble of mutation and crossover strategies and parameters in DE (EPSDE) algorithm [34] algorithms. In Section 7, the results of proposed BBO algorithm have been explained and discussed.

2. Hybrid energy systems A hybrid renewable generation system comprises of wind turbine generators (WTGs) of different types, PV panels (PV), storage batteries (SB) with diesel generator are shown in Fig. 1. In the hybrid generation system, they are integrated and complement with each other in order to meet performance targets of the generation systems and access to the most economic power generation.

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

35

Battery Bank Ibat Solar PV Panel Iren-dc Idc bus Battery Charger Wind Turdine Generator Diesel Generator Iren-ac Inverter Idie xs
Fig. 1. Hybrid PV/wind Energy System.

Zbi, where Pd(t) is the power demanded by the load at hour t, Zbi is inverter efficiency, and PDG(t) is the total power produced by the Diesel generator at hour t. For the charging process (DP(t) 4 0) and discharging process (DP(t) o 0) of the battery bank, the state of charge (SOC) can be calculated as:
Iinv

PB t  1  P B t   폛Pt =U bus nZbb nDt
AC Load

5

where Zbb is equal to the round-trip efficiency in the charging process and is equal to the 100% in the discharging process,Ubus is the DC bus voltage, and Dt is the time step which is generally 1 h. The power generated from WTG's and PV's at the time `t' i.e. total renewable power is given by following equation Ptotal t  
Wn X W1

P W t  

Sn X PV  1

PPV t 

6

2.1. Wind turbine generators (WTG) The energy and current output of the WTG for each time instant are computed based on local weather conditions and actual installation height of the turbines. Wind turbines are usually connected in parallel, not in a series. Several wind turbines can be connected in parallel to match the system current requirements. Using the wind speed at a reference height hr from the database, the velocity at a hub height for the location is estimated on an hourly basis are calculated as: vt   vr t :h=hr g 1

where Wn, Sn are the total number of wind turbine generators and photovoltaic panels respectively.

3. Description of the problem For the hybrid energy system design, the objective of optimum design is to minimize Ct(PW, PPV, PB, PDG), subject to the constraint's explained in Eqs. (14)(16). The design parameters that should be derived must include WTG capacity (PW), PV panel capacity (PPV), total battery capacity (PB), and Diesel generator capacity (PDG). minC t P W , PPV , PB , P DG   minC W  C PV  C b  C g  C r  7

where v is the wind speed at the projected height h, vr is the wind speed at reference height hr and g is the power-law exponent ( $ 1/ 7 for open land). The power generated by the wind system at any time `t' can be expressed as: P W t   ZW nZg n0:5nra nC P nAnv2 r 2

where Ct is the total system cost, CW, CPV, Cb, Cg, Cr are the total cost of wind turbine systems, photovoltaic panels, batteries, Diesel Generator and the total cost of considering the power supply reliability respectively. 3.1. The total cost of wind turbines CW 
Wn  X i1

where PW is the wind turbine power output, ZW is efficiency of wind turbine, Zg is efficiency of generator, ra is the density of air, CP is the power coefficient of wind turbine, and A is the wind turbine swept area. 2.2. Photovoltaic generation (PV) The PV sizing variable comprises of size of a PV panel and the number of strings in a PV array. The necessary number of PV panels to be connected in the series is derived by the number of panels needed to match the bus operating voltage. When matching the current requirements of the system, several PV strings which are connected in series, needs to be installed in parallel. The number of parallel PV strings is a design variable that needs optimization. The output of PV panels must include the impact of geographic location, such as solar radiation and temperature, etc. The output power of photovoltaic panels PPV(t) at any time `t' can be calculated as: P PV t   ZPV nN PV P nN PV S nV PV nIPV 3

ai Pi

r 0 1  r 0 m  omP i   repPi  1  r 0 m 1



8

3.2. Total cost of photovoltaic panels C PV   Sn  X r 0 1  r 0 m  omP j   repPj  bj Pj m 1  r 0  1 j1 9

3.3. Total cost of batteries Cb 
Bn  X k1

ck P k

r 0 1  r 0 m  omP k   repP k  1  r 0 m 1



10

where ZPV is conversion efficiency of PV panel, NPVP is the number of PV panels in parallel, NPVS is number of PV panels in series, VPV is the operating Voltage of PV panels, and IPV is operating current of PV panels. 2.3. Storage batteries (SB) The batteries are used to store the excess energy generated by hybrid system and supply energy during the low generation period. The power input to the battery bank is calculated as:

3.4. Total cost of diesel generator Cg   Dn  X r 0 1  r 0 m  om  P dl P l   rep  P   f uel  P  l l l 1  r 0 m 1 l1 11

DP t   Ptotal t  P DG t 事P Load t 

4

where Ptotal(t) is the total power produced by the renewable resources (PV panels and wind turbines) at hour t, PLoad(t)  Pd(t)/

where Wn, Sn, Bn, Dn are the number of wind generators, photovoltaic panels, batteries, diesel generators; ai, bj, ck, dl are the unit cost (Rs/kW); Pi, Pj, Pk, Pl is the power capacity; om(Pi), om(Pj), om(Pk), om(Pl) are the maintenance and operating costs; rep(Pi), rep(Pj), rep(Pk), rep(Pl) are the replacement costs corresponding to ith wind turbine, jth photovoltaic panels, kth battery, lth Diesel Generator; fuel(Pl) is the cost of fuel used in lth

36

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Diesel Generator; m is the life span of the project and r0 is the interest rate. 3.5. The total cost of other parameters The total cost of considering power supply reliability is calculated as: C r  cconEENS 12

respectively 0 r Sn r NPV ,
Pmax Pmax Pmax

0 r W n r NW , 0 r Bn r NBAT ,

16

where NPV , Pmax is maximum capacity of Photovoltaic panel, N W , Pmax is maximum capacity of Wind turbine, and NBAT , Pmax is the maximum capacity of the battery panel. 4. Biogeography-based optimization (BBO) In the science of biogeography, a habitat is an ecological area that is inhabited by particular plant or animal species and geographically isolated from other habitats. Each habitat is classified by Habitat Suitability Index (HSI). Geographical areas, which are well suited as residences for biological species are said to have a high HSI. Features that correlate with HIS include rainfall, diversity of vegetation, diversity of topographic features, land area, temperature, etc. If each of the features is assigned a value, HSI is a function of these values. Each of these features that characterize habitability is known as Suitability Index Variables (SIV). SIVs are the independent variables while HSI are the dependent variables. Habitats with high HSI have the large population and have high emigration rate m, simply by virtue of a large number of species that migrate to other habitats. The immigration rate l is low for those habitats which are already saturated with species. On the other hand, habitats with low HSI have high immigration rate l, low emigration rate m due to sparse population. The value of HIS, for low HSI habitat, may increase with the influx of species from other habitats as suitability of a habitat is the function of its biological diversity. However, if HSI does not increase and remains low, species in that habitat go extinct and this leads to additional immigration. For the sake of simplicity, it is safe to assume a linear relationship between habitats HIS, its immigration and emigration rate. These rates are same for all the habitats and depend upon the number of species in the habitats. Fig. 2 shows the relationships between fitness of habitats (number of species), emigration rate m and immigration rate l. E is the possible maximum value of emigration rate and I is the possible maximum value of immigration rate. S is the number of species in the habitat, which corresponds to fitness. Smax is the maximum number of species the habitat can support. S0 is the equilibrium value. When S  S0, the emigration rate m is equal to the immigration rate l.From Fig. 2, it is clear that island which has outstanding performance like S2 has a high emigration rate and a low immigration rate. On the other hand, island which has poor performance like S1 has a high immigration rate and a low emigration rate. The values of emigration and immigration rates are given as:

where cco is the Compensation Coefficient and EENS is the Expected Energy Not Served. In the calculation of Cr, time series and the Monte Carlo method are used. Due to this, time series is divided into many terms i.e. wind speed, light, load, etc. and then the Monte Carlo method is used to calculate the reliability of the randomly selected sample. Within the run-time T (8760 h), the EENS (kWh/year) is calculated as: EENS 
T X

Pbmin  P bSOC t 事P sup t 沙U t 

13

t1

where U(t) is a step function that is zero when the supply exceeds or equals to the demand and one if there is insufficient power during hour t; PbSOC t  is state of charge (SOC) of storage batteries during hour t; P bmin t  is the minimum permissible storage level of the battery and P sup t   P total t   P DG t 事P d t  is the surplus power during hour t. 3.6. Design constraints Due to the physical or operational limits of the target system, there is a set of constraints that should be satisfied throughout system operations for any feasible solution. 1. For any period t, the total power supplied from the hybrid energy system must supply the total demand Pd with a certain reliability criterion. This relation can be represented as: P W t   P PV t   P B t   PDG t  Z 1픒P d t  P W t   P PV t   P B t   PDG t 事P dump t  r P d t 

14

P bmin r PbSOC r P bmax 0 r P bcap r P bcapmax P bt r P bmax 15

rate

where PW, PPV, PB, PDG, Pdump, Pd are the wind power, solar power, charged/discharged battery power, diesel generator power, dumped power and total load demand respectively; R is the ratio of the maximum permissible unmet power with respect to the total load demand at each time instant. The transmission losses are not considered because the system is considered as the remotely located isolated system and do not have substantial transmission lines. The dump power is the excess power generated by the system which is not utilized for either supply the load or supplied to charge the battery. 2. The state of charge (SOC) of storage batteries P bSOC should not exceed the capacity of storage batteries P bcap and should be larger than the minimum permissible storage level Pbmin . The total storage battery capacity should not exceed the allowed storage capacity Pbcap max . The hourly charge or discharge power Pbt should not exceed the hourly inverter capacity P bmax . These constraints are defined as:

lk 

EK P

17

E=I Immigration  Emigration 

S1
3. The number of wind power generation, batteries and photovoltaic panels is subjected to the following constraints

S0 S2 number of species

Smax

Fig. 2. The model of immigration rate and emigration rate of biology.

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

37

mk  I1픊 =P

18

where K is the number of species of the kth individual, and P is the number of species. Mathematically, the concept of emigration and immigration can be represented by a probabilistic model. Let us consider the probability Ps in which habitat contains exactly S species at t. Ps changes from time t to time t  Dt as: P s t  Dt   Ps t 裟1픩s Dt 픪s Dt   Ps1 t ls1 Dt  P s  1 t ms  1 Dt 19 where ls and ms are the immigration and emigration rates when there are S species in the habitat. This equation holds because in order to have S species at the time t, one of the following conditions must hold (1) There were S species at the time t, and no immigration or emigration occurred between t and t  Dt. (2) There were (S  1) species at the time t, and one species immigrated. (3) There were (S  1) species at the time t, and one species emigrated. If time Dt is small enough so that the probability of more than one immigration or emigration can be ignored, then taking the limit of Eq. (19) as Dt-0 given by following equation 8 S0 쟈ls  ms Ps  ms  1 P s  1 > <  l  m  P  m P  l P 1 r S r Smax 1 쟈 Ps  20 s s1 s1 s s s1 s1 > : 쟈l  m P  l P S  Smax s s1 s1 s s Habitat modification (Migration) algorithm is described as:

population. The mutation rate m is inversely proportional to the solution probability, which is given as:   P m  mmax n 1 21 P max where mmax is a user-defined parameter. BBO also follows a migration and mutation step to reach global minima. The flowchart of BBO algorithm application in a hybrid system design is given in Fig. 3. The basic BBO algorithm is described as:Initialize Parameters P  population size G  Maximum number of generation Keep  Elitism parameter Pmax  Island modification probability Step 1: Initialize P randomly and species count probability of each Habitat. Step2: Evaluate the fitness for each individual in P. Step3: While The termination criterion is not met do. Step4: Save the best habitats in a temporary array. Step5: For each habitat, map the HSI to number of species S, l and m. Step6: Probabilistically choose the immigration island based on the immigration rate m. Step7: Migrate randomly selected SIVs based on the selected island in Step 6. Step8: Mutate the worst half of the population as permutation algorithm. Step9: Evaluate the fitness for each individual in P. Step10: Sort the population from best to worst. Step11: Replace worst with best habitat from temporary array. Step12: Go to step 2 for the next iteration. Step13: end while

5. Case study The developed methodology for BBO Algorithm has been applied to design the stand-alone hybrid wind/PV systems to supply a varying load located in the area of Jaipur in Rajasthan (India) with geographical coordinates defined as: latitude: 26192 N, longitude: 75182 E and altitude: 431 m above sea level. The wind speed, solar irradiance, sunshine duration and ambient temperature recorded for every hour, during the period of 1st January, 2010 to 30th December, 2010. The wind speed was measured at 30 m height. In this application, PV panels, wind turbines, battery, diesel generator and inverter have been used. The cluster of colonies is assumed to be located in a remote area with adequate sunshine, moderate to high wind speeds. The average daily load profile of the study area is shown in Fig. 4. The daily energy consumption of load is 2263 kWh/day with a 261 kW peak demand and day to day variation of 30% is introduced in the load profile. The optimal solution is verified by showing the energy profile during the period from 1st January, 2011 to 7th January, 2011. The monthly solar radiation in Jaipur, Rajasthan is between 4 and 7 kWh/m2/day, with the monthly sunshine duration ranging from 5 h/day to 8 h/day as shown in Table 1. The sunshine hour has been taken for the same duration as for the global solar radiation. These values are essential for sizing of solar energy systems. The monthly solar radiation patterns are shown in Fig. 5. The average wind speed for Jaipur, Rajasthan is between 4 and 11 m/s as shown in Table 2. The average daily wind speeds (m/s) for a year is shown in Fig. 6. The technical, economical data and study assumptions are given in Table 3.

Habitat mutation algorithm is described as:

For each SIV in each solution, it is decided probabilistically, whether or not to immigrate. If immigration is selected for a given solution feature, the emigrating habitat is selected for a given solution probabilistically, using the roulette wheel normalized by m. The mutation operator is probabilistically applied to the habitat, which tends to increase the biological diversity of the

38

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Fig. 3. BBO optimization algorithm for hybrid energy system design.

200 Daily Load (KW) 150 100 50 0 2 4 6 8 10 12 14 Time (Hours) 16 18 20 22 24

Fig. 4. Daily load profile.

6. Comparison of BBO with other algorithms The hybrid energy system optimization is performed on an Intel Core 2 Duo PC with 2.1 (GHz) processor speed, 2 (GB) RAM and Windows 7 operating systems. The experiments are

performed using the MATLAB R2010b program. The BBO results are compared with the HOMER, GA, PSO, CLPSO and EPSDE. For the sake of comparison of performance between the various algorithms, the stopping criterion is set at same. Mutation rate  0.7 and Cross over rate  0.3 are used to get the best results

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

39

for GA. For PSO, C1  1.8, C2  2.2, W starts at 1 and decreases until reaching 0 at the end of the run. In CLPSO, C  1.5 is selected and in EPSDE, CR  0.9, F  0.5 gives the best results. The mating is performed using single point crossover. Fig. 7 depicts convergence of five optimization algorithms for the combination of the hybrid energy system in which PV, WTG, Diesel generator, battery and inverter are present. For comparison of different algorithms, the initial solution points are taken constant. It can be seen that the fitness value decreases rapidly in the first 10 generations. During this stage GA, PSO, CLPSO, EPSDE and BBO concentrate mainly on finding feasible solutions to the problem. Then the value decreases slowly, and they have been converged approximately at around 20 iterations. Consequently, the total system cost, components size has been almost same in BBO, PSO, CLPSO, EPSDE and GA. More details about convergence and optimal solutions are given in Tables 4 and 5. It observed from the Tables 4 and 5 that HOMER, PSO, GA, CLPSO, EPSDE and BBO are able to find the optimum design parameters of the stochastic simulation model. It can be easily seen that BBO algorithm is more rapid and give minimum cost as compared to GA, PSO, CLPSO and EPSDE. Therefore, the proposed BBO-based optimization procedure can comfortably, rapidly approach the optimum state for a large-scale complex simulation of a hybrid energy system. The total cost of the optimized hybrid energy system showed that the system can deliver energy in a stand-alone installation with an acceptable cost. The Homer software with a combination of various components and strategies variables of 38 million would require the calculation time of approximately 15 h to evaluate each combination. The proposed BBO algorithm not only reduces the demerits of HOMER but also uses a certain number of combinations. The proposed BBO has reduced 15 h of calculation time around 0.73 h
Table 1 Monthly global solar radiation (kWh/m2/day) and sunshine hours at Jaipur (Rajasthan). Month Solar radiation (kWh/m2/d) 4.128 4.882 5.717 6.427 6.812 6.830 5.748 5.269 5.756 5.329 4.382 3.866 Sunshine hours (h/day) 5.59 6.14 6.13 6.11 5.87 6.09 6.04 5.52 6.77 5.52 6.23 5.82

on Intel Core 2 Duo PC for complete hybrid energy system optimization.

7. Results and discussions The mathematical modeling is driven by HOMER, the results of HOMER software can be used for comparison and point of reference. The optimization results using HOMER software are shown in Table 6. The biogeography-based optimization algorithm derived the results as shown in Table 7. The total power generated by renewable sources seemed enough, except for its failure to provide the necessary power at peak time, which requires the support of battery and inverter. Optimization calculations obtained by HOMER are slightly different as compared with BBO. However, use of HOMER will be a problem, when the calculation of different types of component needs to be calculated simultaneously. There are three significant disadvantages of HOMER: 1. HOMER requires calculation of every single combination of sizing and operation strategy. 2. The data for each variation of component needs to be entered manually and execute separately. 3. HOMER generally uses diesel generator, so hybrid system cost increases due to increase in fuel intake. As data involved are large and sensitivity analysis needs to be done for selected components, it is unlikely that HOMER can provide fast and reliable solutions for the optimization. After lot of hit and trial, the following parameters are taken for BBO algorithm:

 

No. of habitats or population Generation

: 50, : 50,

Table 2 Monthly wind speed at Jaipur (Rajasthan). Month January February March April May June July August September October November December Wind speed (m/s) 6.312 5.433 4.627 4.993 10.335 10.717 8.413 8.754 9.776 5.850 4.443 5.931

January February March April May June July August September October November December

Average Daily Solar Radiation

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 2 4 6 8 10 12 14 Time (Hours) 16 18 20 22 24
January February March April May June July August September October November December

Fig. 5. Average solar radiation monthly data for 1 year.

40

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Average Daily wind speed

7 6 5 4 3 2 1 0 0 50 100 150 200 Time (Day) 250 300 350

Fig. 6. Average daily wind speed (m/s).

Table 3 Technical data and study assumptions of hybrid energy system. Description PV Capital cost Lifetime Operation and maintenance cost Replacement cost Wind turbine Rated power Capital cost Lifetime Operation and maintenance cost Cut-in speed (m/s) Vci Cut-out speed (m/s) Vco Hub height Replacement cost Diesel generator units Capital cost Rated power of each diesel unit 1 (D1, D2) Minimum allowed power (min load ratio) Operation and maintenance cost Operating hours Replacement cost Batteries Type of batteries Nominal voltage (V) Nominal capacity Nominal energy capacity of each battery Operation and maintenance cost Dispatch/operating strategy Capital cost Replacement cost Converter Capital cost Operation and maintenance cost Lifetime Replacement cost Spinning reserve Minimum renewable fraction Annual interest rate Project life time Data

200,000 Rs/kW 25 years 1000 Rs/kW/year 200,000 Rs/kW Variable (0300 KW) AC 65,000 Rs/kW 25 years 1000 Rs/kw/year 23 25 30 m 65,000 Rs/kW 20,000 Rs/kW Variable (0100 MW) 30% of rated power 1 Rs/h/kW 16,000 h

Tarjan L16P 6V 360 Ah 2.16 kWh 100 Rs/year Multiple diesel load following 10,000 Rs 4000 Rs 50,000 Rs/kW 100 Rs/year/kW 10 years 50,000 Rs/kW 10% 60% 10% 25 years

     

Number of SIVs per habitat Habitat modification probability Island Mutation probability Elitism parameter Maximum emigration Rate E Maximum immigration Rate I

: 20, : : : : : 1, 0.05, 2, 1, 1.

The total cost for the minimization of the hybrid system is achieved by selecting an appropriate system configuration. In Table 7, the optimal sizing results consisting of device numbers and the total cost of the hybrid wind/PV system have been

presented. The comparison of HOMER and BBO results are shown in Table 8. These results verify that the hybrid energy system has a lower total cost as compared to the stand-alone systems. Reliability of the hybrid energy system is much higher than the other systems, and the output is not very much affected by changes in weather conditions. The combination of wind in a hybrid energy system reduces the battery bank and diesel requirements, and PV array has the high capital cost per kW but very less operating and maintenance cost. In order to study the hourly behavior of the power exchange in the hybrid energy system, the simulation results were conducted during 1st to 7th January, 2011, for the optimal configuration obtained from BBO algorithm (Case 1 of the Table 7), are shown in Fig. 8. It shows the power supply from the renewable sources, power demand and input/output power to the battery bank. Diesel Generator is used only when the renewable sources and the batteries are not able to satisfy the load demand. Without much operation reserve, diesel generator can also supply the load demand independently but at much higher cost with Cost of Energy (COE) of Rs. 27.69/kWh. The generator uses 457,100 l of diesel and operating for 8760 h annually, which is almost half of its lifetime operating hours. In all the calculations, the cycle charging process is selected as operation strategy. Two other operation strategies, load following and set point state of charge at 80% can be considered. Since the initial finding lacking input constraints, cycle charging process is enough to guide the selection and distribution of power. Fig. 9 shows the annualized cost of the hybrid energy system (Case 2 of Table 7), which have PV, wind turbine, diesel generator, battery and Inverter connected to the system. Wind turbine contributes 31%, battery costs about 11%, PV system contributed 11%, Diesel generator costs 10%, fuel costs around 30% and inverter at 7% of the total annual cost of Rs. 67,877,356. PV panels and Wind system are assumed to last the lifetime of the project, i.e. for 15 years, while the battery and inverter needs to be replaced after a certain number of hours of operation and discharging respectively. The cost of battery and inverter plays an important part for determining the TNPC and COE. Except for three combinations, the system requires battery and inverter as part of the hybrid system for storing the excess energy generated by the system. As global price of the oil is increasing and the Government of India has indicated that it can no longer provide the oil subsidy. It is important to note that if the true diesel price is used in the calculation, the COE is going up by Rs. 0.1per kWh for 1 Rs/litre increase in diesel price. The sensitivity results for diesel price are shown in Table 9. If diesel price is raised to Rs 50 per litre, the COE is increased by 6%. To check the sensitivity of the results to variations in average wind speed from year to year, the hybrid energy system in Case

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

41

9 8 Generation Cost (Rs/kWh) 7 6 5 4 3 2 1 0

x 108
GA (69932935) PSO (71852619) BBO (67877356) CLPSO (69130095) EPSDE (69240274)

10

20

30

40 50 60 Number of iterations

70

80

90

100

Fig. 7. Convergence characteristics of various optimization algorithms.

Table 4 Optimization results from various optimization algorithms. Algorithm HOMER GA PSO BBO CLPSO EBSDE PV (kW) 55 53 52 52 52 52 WTG 1 1 1 1 1 1 DG (kW) 180 168 165 163 164 164 Battery 800 808 813 813 814 813 Inverter (kW) 100 93 93 93 93 93 Net present costs 89,352,280 71,852,619 69,932,935 67,877,356 69,130,095 69,240,274 Energy produced 1,243,027 1,088,797 1,067,294 1,059,639 1,067,894 1,068,247 COE 14.23 11.45 11.14 10.81 11.01 11.03

Table 5 Comparison of optimization algorithms. Optimization technique HOMER GA PSO BBO CLPSO EBSDE Population size  50 50 50 50 50 Iteration number 21875 100 100 100 100 100 Number of runs 10 10 10 10 10 10 Average time (s) 1648 125 110 82 89 102 Total cost (Rs) 89,352,280 71,852,619 69,932,935 67,877,356 69,130,095 69,240,274 Comments  Pm  0.7, Pc  0.3 C1  1.8, C2  2.2 E  1, I  1 C  1.5 CR  0.9, F  0.5

Table 6 Complete optimization results received through Homer Software. SN 1 2 3 4 5 6 7 8 9 PV (kW) 0 55 350 0 60 600 0 450 0 Wind (units) 1 1 0 2 2 0 0 0 6 Diesel (kW) 160 180 200 220 220 0 250 240 0 Battery (units) 900 800 1300 0 0 4700 0 0 24,000 Inverter (kW) 100 100 200 0 100 250 0 100 300 Net present costs (Rs) 89,295,624 89,352,280 125,044,944 128,571,824 136,609,248 138,103,712 153,526,384 170,852,800 357,250,624 Energy produced (kWh) 1,195,797 1,243,027 1,130,267 1,995,859 2,093,897 1,271,608 888,155 1,588,055 4,612,242 COE (Rs/kWh) 14.21 14.23 19.9 20.46 21.74 21.99 24.44 27.19 56.86

Table 7 Complete optimization results received through proposed BBO algorithm. SN 1 2 3 4 5 6 7 8 9 PV (kW) 0 52 284 0 51 557 0 437 0 Wind (units) 1 1 0 1 1 0 0 0 4 Diesel(kW) 147 163 176 210 201 0 260 245 0 Battery (units) 906 813 1239 0 0 4289 0 0 28,926 Inverter (kW) 92 93 212 0 104 261 0 103 321 Net present costs (Rs) 65,408,516 67,877,356 131,414,657 85,046,660 83,891,664 129,015,885 173,812,087 125,171,224 347,414,450 Energy produced (kWh) 1,135,910 1,059,639 929,470 1,110,834 1,140,773 1,180,840 931,781 1,182,069 3,074,832 COE (Rs/kWh) 10.42 10.81 20.94 13.55 13.37 20.55 27.69 19.94 55.35

42

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

Table 8 Comparison of optimization results received through HOMER and proposed BBO algorithm. SN 1 2 3 4 5 6 7 8 9 Components Wind  DG  Battery PV  Wind  DG  Battery PV  DG  Battery Wind  DG PV  Wind  DG PV  Battery DG PV  DG Wind  Battery NPC by HOMER (Rs) 89,295,624 89,352,280 125,044,944 128,571,824 136,609,248 138,103,712 153,526,384 170,852,800 357,250,624 Energy produced (kWh) 1,195,797 1,243,027 1,130,267 1,995,859 2,093,897 1,271,608 888,155 1,588,055 4,612,242 COE (Rs/kWh) 14.21 14.23 19.9 20.46 21.74 21.99 24.44 27.19 56.86 NPC by HOMER (Rs) 65,408,516 67,877,356 131,414,657 85,046,660 83,891,664 129,015,885 173,812,087 125,171,224 347,414,450 Energy produced (kWh) 1,135,910 1,059,639 929,470 1,110,834 1,140,773 1,180,840 931,781 1,182,069 3,074,832 COE Rs/kWh 10.42 10.81 20.94 13.55 13.37 20.55 27.69 19.94 55.35

Table 10 Economic sensitivity to wind speed variations in Case 1 of Table 7. Case Average wind speed (m/s) 6.0 7.2 8.4 Diesel saving (kl) 179 265 336 COE (Rs/kWh)

 17.5% Baseline  17.5%

11.63 10.42 9.58

lower than the present measurement year, COE rose by 11.6%. With the wind speed 17.5% higher, COE dropped by 8.06%.

8. Conclusion
Fig. 8. Power management indicating load power, wind power output, diesel generator output, input/output power of the battery bank of the hybrid System in Case 1 of Table 6.

Fig. 9. Percentage share of components cost in Total net present cost for Hybrid energy system as given in Case 2 of Table 6.

Table 9 Economic sensitivity of diesel cost in Case 1 of Table 7. Diesel price (Rs) 43 46 50 55 Tot NPC (Rs) 65,408,516 67,084,720 69,319,658 72,113,332 COE (Rs/kWh) 10.42 10.69 11.04 11.49

Stand-alone hybrid energy systems are more suitable than stand-alone systems that only have one energy source for the supply of electricity to off-grid applications, especially in remote areas with difficult access. However, the design, control, and optimization of the hybrid energy systems are usually very complex tasks. Power supply reliability under varying weather conditions and the corresponding system cost are the two major concerns in designing PV and/or wind turbine systems. In order to utilize renewable energy resources efficiently and economically, one optimum sizing method is developed in this paper based on a Biogeography Based Optimization (BBO), which has the ability to attain the global optimum with relative computational simplicity compared to the conventional optimization methods. The system configuration, characteristics of the main components, overall sizing, control and power management strategy for the hybrid energy system have been presented. The wind and PV generation systems are the main power generation devices, and the battery acts as a storage device for excess power. The developed methodology is based on the use of long-term data of wind speed, solar irradiance and ambient temperature. The BBO algorithm manages to optimize the size and the operation strategy for a simple daily load. Furthermore, a numerical example (Case study) is used to demonstrate the applicability, power management and usefulness of the proposed method. References
[1] J. Lagorse, D. Paire, A. Miraoui, Sizing optimization of a stand-alone street lighting system powered by a hybrid system using fuel cell, PV and battery, Renewable Energy 34 (3) (2009) 683691. [2] B.S. Borowy, Z.M. Salameh, Methodology for optimally sizing the combination of a Battery Bank and PV array in a wind/PV hybrid system, IEEE Transactions on Energy Conversion 11 (2) (1996) 367375. [3] M. Musseli, G. Notton, A. Louche, Design of hybrid-photovoltaic power generator, with optimization of energy management, Solar Energy 65 (3) (1999) 143157.

1 of Table 7 is run with the wind speeds adjusted upward and downward by 17.5%, which is the inter-annual variability (one standard deviation) found in the historical wind measurements. The results are shown in Table 10. With the wind speed 17.5%

R. Kumar et al. / Swarm and Evolutionary Computation 8 (2013) 3343

43

[4] D.B. Nelson, M.H. Nehrir, C. Wang, Unit sizing and cost analysis of standalone hybrid wind/PV/fuel cell power generation systems, Renewable Energy 31 (2006) 16411656. [5] A.N. Celik, Optimization and techno-economic analysis of autonomous photovoltaic-wind-hybrid energy systems in comparison to single photovoltaic and wind systems, Energy Conservation and Management 43 (18) (2002). [6] H. Yang, L. Lu, W. Zhou, A novel optimization sizing model for hybrid solarwind power generation system, Solar Energy 81 (2007) 7684. [7] Xu Daming, Kang Longyun, Cao Binggang, Stand-alone hybrid wind/PV power system using the NSGA2, Acta energiae solar issinica 27 (6) (2006) 593598. [8] S. Das, S. Maity, Bo-Yang Qu, P.N. Suganthan, Real-parameter evolutionary multimodal optimization  A survey of the state-of-the-art, Swarm and Evolutionary Computation 1 (2) (2011) 7188. [9] E. Mezura-Montes, C.A. Coello, Constraint-handling in nature-inspired numerical optimization: Past, present and future, Swarm and Evolutionary Computation 1 (4) (2011) 173194. [10] G.C. Seeling-Hochmuth, Optimization of hybrid energy systems sizing and operation control, PhD Thesis, University of Kassel, 1998. [11] A.M. El-Zonkoly, Optimal placement of multi-distributed generation units including different load models using particle swarm optimization, Swarm and Evolutionary Computation 1 (1) (2011) 5059. [12] W.D. Kellogg, M.H. Nehrir, Generation unit sizing and cost analysis for standalone wind, photovoltaic, and hybrid Wind/PV systems, IEEE Transactions on Energy Conversion 13 (1) (1998) 7075. [13] C.D. Barley, C.B. Winn, Optimal dispatch strategy in remote hybrid power systems, Solar Energy 58 (4-6) (1996) 165179. [14] R. Chedid, S. Rahman, Unit sizing and control of hybrid wind-solar power systems, IEEE Transactions on Energy Conversion 12 (1) (1997) 7985. [15] R. Belfkira, G. Barakat, C. Nichita, Sizing optimization of a stand-alone hybrid power supply unit: wind/PV system with battery storage, International Review of Electrical Engineering (IREE) 3 (5) (2008). [16] R. Belfkira, O. Hajji, C. Nichita, G. Barakat, Optimal sizing of stand-alone hybrid wind/PV system with battery storage, Proceeding of 2007 European Conference on Power Electronics and Applications, September, 2007, pp.110. [17] J.L. Bernal-Agustin, R. Dufo-Lopez, Efficient design of hybrid renewable energy systems using evolutionary algorithms, Energy Conversion and Management 50 (3) (2009) 479489. [18] G.C. Seeling-Hochmuth, A combined optimization concept for the design and operation strategy of hybrid-PV energy systems, Solar Energy 61 (2) (1997) 7787. [19] E. Koutroulis, D. Kolokotsa, A. Potirakis, K. Kalaitzakis, Methodology for optimal sizing of stand-alone photovoltaic/wind-generator systems using genetic algorithms, Solar Energy 80 (9) (2006) 10721088.

[20] Daming Xu, Longyun Kang, Liuchen Chang, Binggang Cao, Optimal sizing of standalone hybrid wind/PV power systems using genetic algorithms, Proceeding of Canadian Conference on Electrical and Computer Engineering, May 2005, pp.17221725. [21] R.A. Gupta, Rajesh Kumar, Ajay Kumar Bansal, A new methodology for optimizing the size of hybrid PV/wind system using genetic algorithms, Proceeding of National Conference on Recent Advances in Electrical and Electronics (RAEEE-09), Dec 2324, 2009, pp. 133139. [22] Kamaruzzaman Sopian, Azami Zaharim, Yusoff Ali, Zulkifli Mohd Nopiah, Juhari Ab. Razak, Nor Salim Muhammad, Optimal operational strategy for hybrid renewable energy system using genetic algorithms, WSEAS Transactions On Mathematics 7 (4) (2008) 130140. [23] R. Dufo-Lopez, J.L. Bernal-Agustin, Design and control strategies of PV-Diesel systems using genetic algorithms, Solar Energy 79 (2005) 3346. [24] M. Hakimi, M. Moghaddas, Optimal sizing of a stand-alone hybrid power system via particle swarm optimization for Kahnouj area in southeast of Iran, Renewable Energy 34 (7) (2009) 18551862. [25] Ajay Kumar Bansal, R.A. Gupta, Rajesh Kumar, Optimization of hybrid PV/ wind energy system using meta particle swarm optimization (MPSO), Proceeding of India International Conference on Power Electronics, January 2830, 2011, pp. 1-6. [26] HOMER, /http://www.homerenergy.comS. [27] R. Dufo-Lopez, J.L. Bernal-Agustin, Optimization of control strategies for stand-alone renewable energy systems with hydrogen storage, Renewable Energy 32 (2007) 11021126. [28] D. Simon, Biogeography-based optimization, IEEE Transaction of Evolutionary Computing 12 (6) (2008) 702713. [29] A.P. Engelbrecht, Fundamentals of Computational Swarm Intelligence, Wiley, 2005. [30] S. Rajasomashekar, P. Aravindhababu, Biogeography based optimization technique for best compromise solution of economic emission dispatch, Swarm and Evolutionary Computation 15 (June 2012). (Available online. [31] Harish Kumar Urvinder Singh, Tara Singh Kamal, Design of Yagi-Uda antenna using biogeography based optimization, IEEE Transactions on Antennas and Propagation 58 (10) (2010) 33753379. [32] K. Jamuna, K.S. Swarup, Biogeography based optimization for optimal meter placement for security constrained state estimation, Swarm and Evolutionary Computation 1 (2) (2011) 8996. [33] J.J. Liang, A.K. Qin, P.N. Suganthan, S. Baskar, Comprehensive learning particle swarm optimizer for global optimization of multimodal functions, IEEE Transactions on Evolutionary Computing 10 (2006) 281295. [34] R. Mallipeddi, P.N. Suganthan, Q.K. Pan, M.F. Tasgetiren, Differential evolution algorithm with ensemble of parameters and mutation strategies, Applied Soft Computing 11 (2010) 16791696.

Semantics-based Web Service Composition engine
Srividya Kona, Ajay Bansal, Gopal Gupta Department of Computer Science The University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp. 2400 Dallas Parkway Plano, TX 75093

Abstract
Service-oriented computing is gaining wider acceptance. We need an infrastructure that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper we present an approach for automatic service discovery and composition based on semantic description of Web services. The implementation will be used for the WS-Challenge 2007 [1].

position problem. We describe our Service Composition algorithm in section 3. Section 4 presents the design of our software with brief descriptions of the different components of the system followed by conclusions and references.

2. Automated Web service Discovery and Composition
Discovery and Composition are two important tasks related to Web services. In this section we formally describe these tasks. We also develop the requirements of an ideal Discovery/Composition engine.

1. Introduction
In order to make services ubiquitously available, we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis [6]. Informally, a service is characterized by its input parameters, the outputs it produces, and the side-effect(s) it may cause. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. For discovery and composition, one could take the syntactic approach in which the services being sought in response to a query simply have their inputs syntactically match those of the query. Alternatively, one could take the semantic approach in which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the matching process. Several efforts are underway to build an infrastructure for service discovery, composition, etc. These efforts include approaches based on the semantic web (such as USDL [4], OWL-S [7], WSML [8], WSDL-S [9]) as well as those based on XML, such as Web Services Description Language (WSDL [5]). Approaches such as WSDL are purely syntactic in nature, that is, they only address the syntactical aspects of a Web service. In this paper we present our approach for automatic service composition which is an extension of our implementation that we used at WSChallenge 2006 [3]. In section 2 we present the formal definition of the Com-

2.1. The Discovery Problem
Given a repository of Web services, and a query requesting a service (we refer to it as the query service in the rest of the text), automatically finding a service from the repository that matches these requirements is the Web service Discovery problem. Valid solutions to the query satisfy the following conditions: (i) they produce at least the query output parameters and satisfy the query post-conditions; (ii) they use only from the provided input parameters and satisfy the query pre-conditions; (iii) they produce the query side-effects. Some of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre/post conditions, and side-effects requirements.

2.2. The Composition Problem
Given a repository of service descriptions, and a query with the requirements of the requested service, if a matching service is not found, then the composition task can be performed. The composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service. Figure 1 shows an example composite service made up of five services  to . In the figure,   and   are the query input parameters and pre-conditions respectively. 퓬 and 퓬 are the query





output parameters and post-conditions respectively. Informally, the directed arc between nodes and indicates that outputs of constitute (some of) the inputs of . Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition where the number of services involved in composition is exactly equal to one.









the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition problem.

S2 S5 CI',I' S1 S3 S4 CO',O'

Figure 1. Example of a Composite Service as a Directed Acyclic Graph Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and =( ) is the reprepost-conditions. sentation of a service where is the pre-conditions, is the input list, is the service's side-effect, is the affected object, is the output list, and is the postconditions. Definition (Repository of Services): Repository is a set of Web services. Definition (Query): The query service is defined as     = (    ) where is the pre  conditions, is the input list, is the service affect,   is the affected object,  is the output list, and is the post-conditions. These are all the parameters of the requested service. Definition (Composition): The Composition problem can be defined as automatically finding a directed acyclic graph =  of services from repository , given query =    (    ), where is the set of vertices and is the set of edges of the graph. Each vertex in the graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of the graph: where has zero incoming edges, 1.  ,  . 2. where has zero outgoing edges,   , . 3. where has at least one incoming edge, let ,  , ...,  be the nodes such that there is a directed edge from each of these nodes to . Then     (  ).  ,    The meaning of the is the subsumption (subsumes) reis the implication relation. Figure 2 explains lation and one instance of the composition problem pictorially. When



좋





피 

Figure 2. Composite Service







2.3

Requirements of an ideal Engine

The features of an ideal Discovery/Composition engine are: Correctness: One of the most important requirement for an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the requirements of the query. Also, the engine should be able to find all services that satisfy the query requirements. Small Query Execution Time: Querying a repository of services for a requested service should take a reasonable amount of (small) time, i.e., a few milliseconds. Here we assume that the repository of services may be pre-processed (indexing, change in format, etc.) and is ready for querying. In case services are not added incrementally, then time for pre-processing a service repository is a one-time effort that takes considerable amount of time, but gets amortized over a large number of queries. Incremental Updates: Adding or updating a service to an existing repository of services should take a small amount of time. A good Discovery/Composition engine should not pre-process the entire repository again, rather incrementally update the pre-processed data (indexes, etc.) of the repository for this new service added. Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition engine should be able to give results based on requirements (minimize, maximize, etc.) over the costs. We can extend this to services having an attribute vector associated with them and the engine should be able to give results based on maximizing or minimizing functions over this attribute vector. These requirements have driven the design of our

 좋    좋

피  





피 







     湃          密      앓     密      





semantics-based Composition engine described in the following sections.

S1 Query described using USDL (S) Infer Sub-queries . . . Sn

Discovery Module (Discovery Engine + Service Directory + Term Generator) S1 Sn

3. A Multi-step Narrowing Solution
We assume that a directory of services has already been compiled, and that this directory includes semantic descriptions for each service. In this section we describe our Service Composition algorithm. Service Composition Algorithm: For service composition, the first step is finding the set of composable services. The correct sequence of execution of these services can be determined by the pre-conditions and post-conditions of the individual services. That is, if a subservice  is composed with subservice  , then the post-conditions of  must imply the pre-conditions of . The goal is to derive a single solution, which is a directed acyclic graph of services that can be composed together to produce the requested service in the query. Figure 4 shows a pictorial representation of our composition engine. In order to produce the composite service which is represented by a graph as shown in figure 1, we filter out services that are not useful for the composition at multiple stages. Figure 3 shows the filtering stages for the particular instance shown in figure 1. The composition routine starts with the query input parameters. It finds all those services from the repository which require a subset of the query input parameters. In figure 3,   are the pre-conditions and the input parameters provided by the query.  and  are the services found after step 1.  is the union of all outputs produced by the services at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs produced by the previous stage, i.e.,  =   .  is used to find services at the next stage, i.e., all those services that require a subset of . In order to make sure we do not end up in cycles, we get only those services which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all the query output parameters are produced. At this point we make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute to the query output parameters. This is done starting with the output parameters working our way backwards.

.......................... Composition Engine (implemented using Constraint Logic Programming

Composed Service

Pre-Cond(S) S1 Pre-Cond( S) 1

Post-Cond( S) 1 Pre-Cond( S) 2

S2

................................. S n

Post-Cond( S) n Post-Cond(S)









Figure 4. Composition Engine Pre-Cond, QCO - Post-Cond Output: Result - ListOfServices 1. L NarrowServiceList(QI, QCI); 2. O GetAllOutputParameters(L); 3. CO GetAllPostConditions(L); 4. While Not (O QO) 5. I = QI O; CI QCI CO; 6. L' NarrowServiceList(I, CI); 7. End While; 8. Result RemoveRedundantServices(QO, QCO); 9.Return Result;









4. Implementation
Our composition engine is implemented using Prolog [10] with Constraint Logic Programming over finite domain [11], referred to as CLP(FD) hereafter. In this section we briefly describe our software system and its modules. The details of the implementation along with performance results are shown in [2]. Triple Generator: The triple generator module converts each service description into a triple as follow: (Pre-Conditions, affect-type(affected-object, I, O), PostConditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can be treated as terms in first-order logic. In case conditions on a service are not provided, the Pre-Conditions and Post-Conditions in the triple will be null. Similarly if the affect-type is not available, this module assigns a generic affect to the service. Query Reader: This module reads a query file (in XML format, possibly different from the XML format used for a









I=I CI, I
1

S1 S2 . .

O1

I=IUO
2 1

1

S . .

O
3

2

I=IUO
3 2

2

O S . .
4

3

I=IUO
4 3

3

S . .

O
5

4

O

Figure 3. Composite Service Algorithm: Composition Input: QI - QueryInputs, QO - QueryOutputs, QCI -

service) and converts it into a triple used for querying in our engine. Semantic Relations Generator: We obtain the semantic relations from the provided ontology. This module extracts all the semantic relations and creates a list of Prolog facts. Composition Query Processor: The composition engine is written using Prolog with CLP(FD) library. It uses a repository of facts, which contains list of services, their input and output parameters and the semantic relations between the parameters. The following is the code snippet of our composition engine:
composition(sol(Qname,A)) :dQuery(Qname,_,_), minimize(compTask(Qname,A,SeqLen),SeqLen). compTask(Qname, A, SeqLen) :dQuery(Qname,QI,QO), encodeParam(QO,OL), narrowO(OL,SL), fd_set(SL,Sset), fdset_member(S_Index,Sset), getExtInpList(QI,InpList), encodeParam(InpList,IL), list_to_fdset(IL,QIset), serv(S_Index,SI,_), list_to_fdset(SI,SIset), fdset_subtract(SIset,QIset,Iset), comp(QIset,Iset,[S_Index],SA,CompLen), SeqLen #= CompLen + 1, decodeS(SA,A). comp(_, Iset, A, A, 0) :- empty_fdset(Iset),!. comp(QIset, Iset, A, SA, SeqLen) :fdset_to_list(Iset,OL), narrowO(OL,SL), fd_set(SL,Sset), fdset_member(SO_Index,Sset), serv(SO_Index,SI,_), list_to_fdset(SI,SIset), fdset_subtract(SIset,QIset,DIset), comp(QIset,DIset,[SO_Index|A],SA,CompLen), SeqLen #= CompLen + 1.

5. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure to publish services, document services and query repositories for matching services. We presented our approach for Web service composition. Our composition engine can find a graph of atomic services that can be composed to form the desired service as opposed to simple sequential composition in our previous work [3]. Given semantic description of Web services, our solution produces accurate and quick results. We are able to apply many optimization techniques to our system so that it works efficiently even on large repositories. The use of Constraint Logic Programming (CLP) helped greatly in obtaining an efficient implementation of this system. We used a number of built-in features such as indexing, set operations, and constraints and hence did not have to spend time coding these ourselves. These CLP(FD) built-ins facilitated the fast execution of queries.

References
[1] WS Challenge 2007 http://ws-challenge. org. [2] S. Kona, A. Bansal, G. Gupta, and T. Hite. Efficient Web Service Discovery and Composition using Constraint Logic Programming. In ALPSWS Workshop at FLoC 2006. [3] S. Kona, A. Bansal, G. Gupta, and T. Hite. Web Service Discovery and Composition using USDL. In CEC/EEE, June 2006. [4] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005. [5] Web Services Description Language. http://www. w3.org/TR/wsdl. [6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp. 46-53, March 2001. [7] OWL-S www.daml.org/services/owl-s/1. 0/owl-s.html. [8] WSML: Web Service Modeling Language. www. wsmo.org/wsml/. [9] WSDL-S: Web Service Semantics. http://www. w3.org/Submission/WSDL-S. [10] L. Sterling and S. Shapiro. The Art of Prolog. MIT Press, 1994. [11] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

The query is converted into a Prolog query that looks as follows: composition(queryService, ListOfServices). The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the built-in, higher order predicate "bagof" to return all possible ListOfServices that can be composed to get the requested queryService. Output Generator: After the Composition Query processor finds a matching service, or the graph of atomic services for a composed service, the results are sent to the output generator in the form of triples. This module generates the output files in any desired XML format. For the WS-Challenge, this module will produce output files in the format provided [1]. For this year's challenge, the software has to receive requests and return results via SOAP. Hence our software will work as a Web service whose interface will accept the discovery/composition query.

Towards Predicate Answer Set Programming via Coinductive Logic Programming
Richard Min, Ajay Bansal, Gopal Gupta
Department of Computer Science The University of Texas at Dallas, Richardson, Texas, U.S.A.

Abstract Answer Set Programming (ASP) is a powerful paradigm based on logic programming for non-monotonic reasoning. Current ASP implementations are restricted to "grounded range-restricted function-free normal programs" and use an evaluation strategy that is "bottom-up" (i.e., not goal-driven). Recent introduction of coinductive Logic Programming (co-LP) has allowed the development of topdown goal evaluation strategies for ASP. In this paper we present this novel goaldirected, top-down approach to executing predicate answer set programs with coLP. Our method eliminates the need for grounding, allows functions, and effectively handles a large class of predicate answer set programs including possibly infinite ones.

1

Introduction

Answer Set Programming (ASP) [1,2] is a powerful and elegant way for incorporating non-monotonic reasoning into logic programming (LP). Many powerful and efficient ASP solvers such as Smodels [3,4], DLV, Cmodels, ASSAT, and NoMoRe have been successfully developed. However, these ASP solvers are restricted to "grounded version of a range-restricted function-free normal programs" since they adopt a "bottom-up" evaluation-strategy with heuristics [2]. Before an answer set program containing predicates can be executed, it must be "grounded"; this is usually achieved with the help of a front-end grounding tool such as Lparse [5] which transform a predicate ASP into a grounded (propositional) ASP. Thus, all of the current ASP solvers and their solution-strategies, in essence, work for only propositional programs. These solution strategies are bottom-up (rather than top-down or goal-directed) and employ intelligent heuristics (enumeration, branch-and-bound or tableau) to reduce the search space. It was widely believed that it is not possible to develop a goal-driven, top-down ASP Solver (i.e., similar to a query driven Prolog engine). However, recent techniques such as Coinductive Logic Programming (Co-LP) [6,7] have shown great promise in developing a top-

500

R. Min, A. Bansal and G. Gupta

down, goal-directed strategy. In this paper, we present a goal-directed or querydriven approach to computing the stable model of an ASP program that is based on co-LP and coinductive SLDNF resolution [8]. We term this ASP Solver coinductive ASP solver (co-ASP Solver). Our method eliminates the need for grounding, allows functions, and effectively handles a large class of (possibly infinite) answer set programs. Note that while the performance of our prototype implementation is not comparable to those of systems such as S-models, our work is a first step towards developing a complete method for computing queries for predicate ASP in a top-down, goal driven manner. The rest of the paper is organized as follows: we first give a brief overview of Answer Set Programming, followed by an overview of coinductive logic programming and co-SLDNF resolution (i.e., SLDNF resolution extended with coinduction). Next we discuss how predicate ASP can be realized using co-SLDNF. Finally, we present some examples and results from our initial implementation.

2

Answer Set Programming (ASP)

Answer Set Programming (ASP) and its stable model semantics [1-4] has been successfully applied to elegantly solving many problems in nonmonotonic reasoning and planning. Answer Set Programming (A-Prolog [1] or AnsProlog [2]) is a declarative logic programming language. Its basic syntax is of the form: L0 :- L1, ... , Lm, not Lm+1, ..., not Ln. (1)

where Li is a literal and n  0 and n  m. This rule states that L0 holds if L1, ... , Lm all hold and none of Lm+1, ..., Ln hold. In the answer set interpretation [2], these rules are interpreted to be specifying a set S of propositions called the answer set. In this interpretation, rule (1) states that Lo must be in the answer set S if L1 through Lm are in S and Lm+1 through Ln are not S. If L0 =  (or null), then the rule-head is null (i.e., false) which forces its body to be false (a constraint rule [3] or a headless-rule). Such a constraint rule is written as follows. :- L1, ... , Lm, not Lm+1, ..., not Ln. (2)

This constraint rule forbids an answer set from simultaneously containing all of the positive literals of the body and not containing any of the negated literals. A constraint can also be expressed in the form: Lo :- not Lo, L1, ... , Lm, not Lm+1, ..., not Ln (3)

A little thought will reveal that (3) can hold only if Lo is false which is only possible if the conjunction L1, ... , Lm, not Lm+1, ..., not Ln is false. Thus, one can observe that (2) and (3) specify the same constraint. The (stable) models of an answer set program are traditionally computed using the Gelfond-Lifschitz method [1,2]; Smodels, NoMoRe, and DLV are some of the

Towards Predicate Answer Set Programming

501

well-known implementations of the Gelfond-Lifschitz method. The main difficulty in the execution of answer set programs is caused by the constraint rules (of the form (2) and (3) above). Such constraint rules force one or more of the literals L1, ... , Lm, to be false or one or more literals "Lm+1, ..., Ln" to be true. Note that "not Lo" may be reached indirectly through other calls when the above rule is invoked in response to the call Lo. Such rules are said to contain an odd-cycle in the predicate dependency graph [9,10]. The predicate dependency graph of an answer set program is a directed graph consisting of the nodes (the predicate symbols) and the signed (positive or negative) edges between nodes, where using clause (1) for illustration, a positive edge is formed from each node corresponding to Li (where 1  i  m) in the body of clause (1) to its head node L0, and a negative edge is formed from each node Lj (where m+1  j  n) in the body of clause (1) to its head node L0. Li depends evenly (oddly, resp.) on Lj if there is a path in the predicate dependency graph from Li to Lj with an even (odd, resp.) number of negative edges. A predicate ASP program is call-consistent if no node depends oddly on itself. The atom dependency graph is very similar to the predicate dependency graph except that it uses the ground instance of the program: its nodes are the ground atoms and its positive and negative edges are defined with the ground instances of the program. A predicate ASP program is order-consistent if the dependency relations of its atom dependency graph is well-founded (that is, finite and acyclic).

3

Coinductive Logic Programming

Coinduction is a powerful technique for reasoning about unfounded sets, unbounded structures, and interactive computations. Coinduction allows one to reason about infinite objects and infinite processes [11,12]. Coinduction has been recently introduced into logic programming (termed coinductive logic programming, or co-LP for brevity) by Simon et al [6] and extended with negation as failure (termed co-SLDNF resolution) by Min and Gupta [8]. Practical applications of co-LP include modeling of and reasoning about infinite processes and objects, model checking and verification [6,7,13], and goal-directed execution of answer set programs [7,13]. Co-LP extends traditional logic programming with the coinductive hypothesis rule (CHR). The coinductive hypothesis rule states that during execution, if the current resolvent R contains a call C' that unifies with an ancestor call C encountered earlier, then the call C' succeeds; the new resolvent is R' where  = mgu(C, C') and R' is obtained by deleting C' from R. Co-LP allows programmers to manipulate rational structures in a decidable manner. Rational structures are: (i) finite structures and (ii) infinite structures consisting of finite number of finite structures interleaved infinite number of times (e.g., a circular list). To achieve this feature of rationality, unification has to be necessarily extended with the "occur-check" removed and bindings such as X = [1 | X]

502

R. Min, A. Bansal and G. Gupta

(which denotes an infinite list of 1's) allowed [7, 14, 15]. SLD resolution extended with the coinductive hypothesis rule is called co-SLD resolution [6,7]. Co-SLDNF resolution, devised by us, extends co-SLD resolution with negation. Essentially, it augments co-SLD with the negative coinductive hypothesis rule, which states that if a negated call not(p) is encountered during resolution, and another call to not(p) has been seen before in the same computation, then not(p) coinductively succeeds. To implement co-SLDNF, the set of positive and negative calls has to be maintained in the positive hypothesis table (denoted +) and negative hypothesis table (denoted -) respectively. Note that nt(A) below denotes coinductive "not" of A. Definition 3.1 Co-SLDNF Resolution: Suppose we are in the state (G, E, +, -). Consider a subgoal A  G: (1) If A occurs in positive context, and A'  + such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with . (2) If A occurs in negative context, and A'  - such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with false. (3) If A occurs in positive context, and A'  - such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with false. (4) If A occurs in negative context, and A'  + such that  = mgu(A,A'), then the next state is (G', E, +, -), where G' is obtained by replacing A with . (5) If A occurs in positive context and there is no A'  (+  -) that unifies with A, then the next state is (G', E', {A}  +, -) where G' is obtained by expanding A in G via normal call expansion using a (nondeterministically chosen) clause Ci (where 1 i  n) whose head atom is unifiable with A with E' as the new system of equations obtained. (6) If A occurs in negative context, and there is no A'  (+  -) that unifies with A, then the next state is (G', E', +, {A}  -) where G' is obtained by expanding A in G via normal call expansion using a (nondeterministically chosen) clause Ci (where 1 i  n) whose head atom is unifiable with A and E' is the new system of equations obtained. (7) If A occurs in positive or negative context and there are no matching clauses for A, and there is no A'  (+  -) such that A and A' are unifiable, then the next state is (G', E, +, {A}  -), where G' is obtained by replacing A with false. (8) (a) nt(..., false, ...) reduces to , and (b) nt(A, , B) reduces to nt(A, B) where A and B represent conjunction of subgoals.  Note (i) that the result of expanding a subgoal with a unit clause in step (5) and (6) is an empty clause (), and (ii) that when an initial query goal reduces to an empty (), it denotes a success with the corresponding E as the solution. Definition 3.2 (Co-SLDNF derivation): Co-SLDNF derivation of the goal G of program P is a sequence of co-SLDNF resolution steps (of Definition 3.1) with a selected subgoal A, consisting of (i) a sequence (Gi, Ei, i+, i-) of state (i  0), of (a) a sequence G0, G1, ... of goal, (b) a sequence E0, E1, ... of mgu's, (c) a sequence

Towards Predicate Answer Set Programming

503

0+, 1+, ... of the positive hypothesis table, (d) 0-, 1-, ... of the negative hypothesis table, where (G0, E0, 0+, 0-) = (G, , , ) is the initial state, and (ii) for step (5) or step (6) of Definition 3.1, a sequence C1, C2, ... of variants of program clauses of P where Gi+1 is derived from Gi and Ci+1 using i+1 where Ei+1 = Eii+1 and (i+1+, i+1-) are the resulting positive and negative hypothesis tables. (iii) If a co-SLDNF derivation from G results in an empty clause, that is, the final state of (, Ei, i+, i-) is reached, then the co-SLDNF derivation is successful; a coSLDNF derivation fails if a state is reached in the subgoal-list which is non-empty and no transitions are possible from this state.  Note that due to non-deterministic choice of a clause in steps (5) and (6) of coSLDNF resolution (Definition 3.1) there may be many successful derivations for a goal G. Thus a co-SLDNF resolution step may involve expanding with a program clause with the initial goal G = G0, and the initial state of (G0, E0, 0+, 0-) = (G, , , ), and Ei+1 = Eii+1 (and so on) and may look as follows:
C1,1 C2,2 C3,3

(G0, E0, 0+, 0-)  (G1, E1, 1+, 1-)  (G2, E2, 2+, 2-)  ... The declarative semantics of negation over the rational Herbrand space is based on the work of Fitting [12] (Kripke-Kleene semantics with 3-valued logic), extended by Fages [9] for stable model with completion of program. Their framework based on maintaining a pair of sets (corresponding to a partial interpretation of success set and failure set, resulting in a partial model) provides a good basis for the declarative semantics of co-SLDNF. An interesting property of co-SLDNF is that a program P coincides with its comp(P) under co-SLDNF. The implementation of solving ASP programs in a goal-directed (top-down) fashion (just like Prolog) has been discussed in Gupta et al [7] for propositional answer set programs. Here, we show how it can be extended for predicate answer set programs.

4

Coinductive ASP Solver

Our current work is an extension of our previous work discussed in [7] for grounded (propositional) ASP solver to the predicate case. Our approach possesses the following advantages: First, it works with answer set programs containing first order predicates with no restrictions placed on them. Second, it eliminates the preprocessing requirement of grounding, i.e., it directly executes the predicates in the manner of Prolog. Our method constitutes a top-down/goal-directed/queryoriented paradigm for executing answer set programs, a radically different alternative to current ASP solvers. We term ASP solver realized via co-induction as coinductive ASP Solver (co-ASP Solver). The co-ASP solver's strategy is first to

504

R. Min, A. Bansal and G. Gupta

transform an ASP program into a coinductive ASP (co-ASP) program and use the following solution-strategy: (1) Compute the completion of the program and then execute the query goal using co-SLDNF resolution on the completed program (this may yield a partial model). (2) Avoid loop-positive solution (e.g., p derived coinductively from rules such as { p :- p. }) during co-SLDNF resolution: This is achieved during execution by ensuring that coinductive success is allowed while exercising the coinductive hypothesis rule only if there is at least one intervening call to `not' in between the current call and the matching ancestor call. (3) Perform an integrity check on the partial model generated to account for the constraints: Given an odd-cycle rule of the form { p :- body, not p. }, this integrity check, termed nmr_check is crafted as follows: if p is in the answer set, then this odd-cycle rule is to be discarded. If p is not in the answer set, then body must be false. This can be synthesized as the condition: p  not body must hold true. The integrity check (nmr_chk) synthesizes this condition for all odd-cycle rules, and is appended to the query as a preprocessing step. The solution strategy outlined above has been implemented and preliminary results are reported below. Our current prototype implementation is a first attempt at a top-down predicate ASP solver, and thus is not as efficient as current optimized ASP solvers, SAT solvers, or Constraint Logic Programming in solving practical problems. However, we are confident that further research will result in much greater efficiency; indeed our future research efforts are focused on this aspect. The main contribution of our paper is to demonstrate that top-down execution of predicate ASP is possible with reasonable efficiency. Theorem 4.1 (Soundness of co-ASP Solver for a program which is call-consistent or order-consistent): Let P be a general ASP program which is call-consistent or order-consistent. If a query Q has a successful co-ASP solution, then Q is a subset of an answer set. Theorem 4.2 (Completeness of co-ASP Solver for a program with a stable model): If P is a general ASP program with a stable model M in the rational Herbrand base of P, then a query Q consistent with M has a successful co-ASP solution (i.e., the query Q is present in the answer set corresponding to the stable model). The proofs are straightforward and follow from soundness/completeness results for co-SLDNF [8] (along with Theorem 5.4 in Fages [9] that "an order-consistent logic program has a stable model"). The theorems can also be proved for unrestricted answer set programs, for queries extended with the nmr_check integrity constraint.

Towards Predicate Answer Set Programming

505

5

Preliminary Implementation Results

We next illustrate our top-down system via some example programs and queries. Most of the small ASP examples1 and their queries run very fast, usually under 0.0001 CPU seconds. Our test environment is implemented on top of YAP Prolog2 running under Linux in a shared environment with dual core AMD Opteron Processor 275, with 2GHz with 8GB memory. Our first example is "move-win," a program that computes the winning path in a simple game, tested successfully with various test queries (Fig 5.1). Note that in all cases the nmr_check integrity constraint is hand-produced.
%% A predicate ASP, "move-win" program %% facts: move move(a,b). move(b,a). move(a,c). move(c,d). move(d,e). move(c,f). move(e,f). %% rule: win win(X) :- move(X,Y), not win(Y). %% query: ?- win(a).

win(X) not

move(X,Y)

Fig. 5.1 Predicate-dependency graph of Predicate ASP "move-win".

The "move-win" program consists of two parts: (a) facts of move(x,y), to allow a move from x to y) and (2) a rule { win(X) :- move(X,Y), not win(Y). } to infer X to be a winner if there is a move from X to Y, and Y is not a winner. This is a predicate ASP program which is not call-consistent but order-consistent, and has two answer sets: { win(a), win(c), win(e) } and { win(b), win(c), win(e) }. Existing solvers will operate by first grounding the program using the move predicates. However, our system executes the query without grounding (since the program is order consistent, the nmr_check integrity constraint is null). Thus, in response to the query above, we'll get the answer set { win(a), win(c), win(e) }. The second example is the Schur number problem for NxB (for N numbers with B boxes). The problem is to find a combination of N numbers (consecutive integers from 1 to N) for B boxes (consecutive integers from 1 to B) with one rule and two constraints. The first rule states that a number X should be paired with one and only one box Y. The first constraint states that if a number X is paired with a box B, then double its value, X+X, should not be paired with box B. The second con1

More examples and performance data can be found from our Technical Report, available from: http://www.utdallas.edu/~rkm010300/research/co-ASP.pdf 2 http://www.dcc.fc.up.pt/~vsc/Yap/

506

R. Min, A. Bansal and G. Gupta

straint states that if two numbers, X and Y, are paired with a box B, then their sum, X+Y, should not be paired with the box B.
%% The ASP Schur NxB Program. box(1). box(2). box(3). box(4). box(5). num(1). num(2). num(3). num(4). num(5). num(6). num(7). num(8). num(9). num(10). num(11). num(12). %% rules in(X,B) :- num(X), box(B), not not_in(X,B). not_in(X,B) :- num(X),box(B),box(BB),B  BB,in(X,BB). %% constraint rules :- num(X), box(B), in(X,B), in(X+X,B). :- num(X), num(Y), box(B), in(X,B), in(Y,B), in(X+Y,B).

The ASP program is then transformed to a co-ASP program (with its completed definitions added for execution efficiency); the headless rules are transformed to craft the nmr_check.
%% co-ASP Schur 12x5 Program. %% facts: box(b). num(n). box(1). box(2). box(3). box(4). box(5). num(1). num(2). num(3). num(4). num(5). num(6). num(7). num(8). num(9). num(10). num(11). num(12). %% rules in(X,B) :- num(X), box(B), not not_in(X,B). nt(in(X,B)) :- num(X), box(B), not_in(X,B). not_in(X,B) :- num(X),box(B),box(BB),B\==BB, in(X,BB). nt(not_in(X,B)) :- num(X), box(B), in(X,B). %% constraints nmr_chk :- not nmr_chk1, not nmr_chk2. nmr_chk1 :- num(X),box(B),in(X,B),(Y is X+X),num(Y),in(Y,B). nmr_chk2 :- num(X),num(Y),box(B),in(X,B),in(Y,B), (Z is X+Y), num(Z), in(Z,B). %% query template answer :- in(1,B1), in(2,B2), in(3,B3), in(4,B4), in(5,B5), in(6,B6), in(7,B7), in(8,B8), in(9,B9), in(10,B10), in(11,B11), in(12,B12). %% Sample query: ?- answer, nmr_chk.

First, Schur 12x5 is tested with various queries which include partial solutions of various lengths I (Fig. 5.1; Table 5.1). That is, if I = 12, then the query is a test: all 12 numbers have been placed in the 5 boxes and we are merely checking that the constraints are met. If I = 0, then the co-ASP Solver searches for solutions from scratch (i.e., it will guess the placement of all 12 numbers in the 5 boxes provided subject to constraints). The second case (Fig 5.2; Table 5.2) is the general Schur NxB problems with I=0 where N ranges from 10 to 18 with B=5.

Towards Predicate Answer Set Programming

507

Fig. 5.2 Schur 5x12 (I=Size of the query). Fig. 5.3 Schur BxN (Query size=0).

Table 5.1 Schur 5x12 problem (box=1..5, N=1..12). I=Query size

Schur 5x12 CPU sec.

I=12 0.01

I=11 0.01

I=10 0.19

I=9 0.23

I=8 0.17

I=7 0.44

I=6 0.43

I=5 0.41

I=4 0.43

Table 5.2 Schur BxN problem (B=box, N=number). Query size=0, with a minor tuning.

Schur BxN CPU sec.

5x10 0.13

5x11 0.14

5x12 0.75

5x13 0.80

5x14 0.48

5x15 4.38

5x16 23.17

5x17 24.31

5x18 130

The performance data of the current prototype system is promising but still in need of improvement if we compare it with performance on other existing solvers (even after taking the cost of grounding the program into account). Our main strategy for improving the performance of our current co-ASP solver is to interleave the execution of candidate answer set generation and nmr_check. Given the query ?- goal, nmr_check, the call to goal will act as the generator of candidate answer sets while nmr_check will act as a tester of legitimacy of the answer set. This generation and testing has to be interleaved in the manner of constraint logic programming to reduce the search space. Additional improvements can also be made by improving the representation and look-up of positive and negative hypothesis tables during co-SLDNF (e.g., using a hash table, or a trie data-structure).

6

Conclusion and Future Work

In this paper we presented an execution strategy for answer set programming extended with predicates. Our execution strategy is goal-directed, in that it starts with a query goal G and computes the (partial) answer set containing G in a manner similar to SLD resolution. Our strategy is based on the recent discovery of coinductive logic programming extended with negation as failure. We also presented results from a preliminary implementation of our top-down scheme. Our

508

R. Min, A. Bansal and G. Gupta

future work is directed towards making the implementation more efficient so as to be competitive with the state-of-the-art solvers for ASP. We are also investigating automatic generation of the nmr_check integrity constraint. In many cases, the integrity constraint can be dynamically generated during execution when the negated call nt(p) is reached from a call p through an odd cycle.

References
1. 2. 3. 4. 5. 6. 7. 8. Gelfond M, Lifschitz V (1988). The stable model semantics for logic programming. Proc. of International Logic Programming Conference and Symposium. 1070-1080. Baral C (2003). Knowledge Representation, Reasoning and Declarative Problem Solving. Cambridge University Press. Niemel I, Simons, P (1996). Efficient implementation of the well-founded and stable model semantics. Proc. JICSLP. 289-303. The MIT Press. Simons P, Niemel I, Soininen, T (2002). Extending and implementing the stable model semantics. Artificial Intelligence 138(1-2):181-234. Simons P, Syrjanen, T (2003). SMODELS (version 2.27) and LPARSE (version 1.0.13). http://www.tcs.hut.fi/Software/smodels/ Simon L, Mallya A, Bansal A, Gupta G (2006). Coinductive Logic Programming. ICLP'06. Springer Verlag. Gupta G, Bansal A, Min R et al (2007). Coinductive logic programming and its applications. Proc. ICLP'07. Springer Verlag. Min R, Gupta G (2008). Negation in Coinductive Logic Programming. Technical Report. Department of Computer Science. University of Texas at Dallas. http://www.utdallas.edu/~rkm010300/research/co-SLDNF.pdf Fages F (1994). Consistency of Clark's completion and existence of stable models. Journal of Methods of Logic in Computer Science 1:51-60. Sato, T (1990). Completed logic programs and their consistency. J Logic Prog 9:33-44. Kripke S (1985). Outline of a Theory of Truth. Journal of Philosophy 72:690-716. Fitting, M (1985). A Kripke-Kleene semantics for logic programs. Journal of Logic Programming 2:295-312. Simon L, Bansal A, Mallya A et al (2007). Co-Logic Programming. ICALP'07. Colmerauer A (1978). Prolog and Infinite Trees. In: Clark KL, Tarnlund S-A (eds) Logic Programming. Prenum Press, New York. Maher, MJ (1988). Complete Axiomatizations of the Algebras of Finite, Rational and Infinite Trees. Proc. 3rd Logic in Computer Science Conference. Edinburgh, UK.

9. 10. 11. 12. 13. 14. 15.

USDL: A Service-Semantics Description Language for Automatic Service Discovery and Composition.
Srividya Kona, Ajay Bansal, Luke Simon, Ajay Mallya, and Gopal Gupta University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp 2400 Dallas Parkway Plano, TX 75093

Abstract For web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose, and synthesize services automatically. This automation can take place only if a formal description of the web-services is available. In this paper we present an infrastructure using USDL (Universal Service-Semantics Description Language), a language for formally describing the semantics of web-services. USDL is based on the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding the meaning of services. USDL can be regarded as formal service documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated service composition, and other forms of automated service integration. A theory of service substitution using USDL is presented. The rationale behind the design of USDL along with its formal specification in OWL is presented with examples. We also compare USDL with other approaches like OWL-S, WSDL-S, and WSML and show that USDL is complementary to these approaches.

1

Introduction

A web-service is a program available on a web-site that "effects some action or change" in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. The next milestone in the Web's evolution is making services ubiquitously available. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. In this context, a web-service can be regarded as a "programmatic interface" that makes application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize and compose services automatically needs to be supported in order to make web-services more practical. To make services ubiquitously available we need a semantics-based approach such that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis. Several efforts are underway to build such an infrastructure. These efforts include approaches based on the semantic web (such as OWL-S [5]) as well as those based on XML, such as Web Services Description Language (WSDL [7]). Approaches such
This is an expanded version of the paper `A Universal Service-Semantics Description Language' that appeared in European Conference On Web Services, 2005 [15] and received its best paper award.


1

as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. In this paper we present an approach that is based on semantics. Our approach can be regarded as providing semantics to WSDL statements. We present the design of a language called Universal Service-Semantics Description Language (USDL) which service developers can use to specify formal semantics of web-services [14, 15]. Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL can be thought of as formal service documentation that will allow sophisticated conceptual modeling and searching of available web-services, automated composition, and other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be determined. The directory can then be searched for the exact service, or two or more services that can be composed to synthesize the required service, etc. To provide formal semantics, a common denominator must be agreed upon that everybody can use as a basis of understanding the meaning of services. This common conceptual ground must also be somewhat coarse-grained so as to be tractable for use by both engineers and computers. That is, semantics of services should not be given in terms of low-level concepts such as Turing machines, first-order logic and their variants, since service description, discovery, and synthesis then become tasks that are practically intractable and theoretically undecidable. Additionally, the semantics should be given at a conceptual level that captures common real world concepts. Furthermore, it is too impractical to expect disparate companies to standardize on application (or domain) specific ontologies to formally define semantics of web-services, and instead a common universal ontology must be agreed upon with additional constructors. Also, application specific ontologies will be an impediment to automatic discovery of services since the application developer will have to be aware of the specific ontology that has been used to describe the semantics of the service in order to frame the query that will search for the service. The danger is that the service may not be defined using the particular domain specific ontology that the application developer uses to frame the query, however, it may be defined using some other domain specific ontology, and so the application developer will be prevented from discovering the service even though it exists. These reasons make an ontology based on OWL WordNet [2, 8] a suitable candidate for a universal ontology of basic concepts upon which arbitrary meets and joins can be added in order to gain tractable flexibility. We describe the meaning of conceptual modeling and how it could be obtained via a common universal ontology based on WordNet in the next section. Section 3, gives a brief overview of how USDL attempts to semantically describe web-services. In section 4, we discuss precisely how a WSDL document can be prescribed meaning in terms of WordNet ontology. Section 5 gives a complete USDL annotation for a Hotel-Reservation service. In section 6 we present the theoretical foundations of service description and substitution in USDL. Automatic discovery of web-services using USDL is discussed in section 7. Composition of web-services using USDL is discussed in section 8. Comparison of USDL with other approaches like OWL-S and WSML is discussed in section 9. Section 10 shows related work. Finally, conclusions and future work are addressed in the last section.

2

2

A Universal Ontology

To describe service semantics, we should agree on a common ground to model our concepts. We can describe what any given web-service does from first principles using approaches based on logic. This is the approach taken by frameworks such as dependent type systems and programming logics prevalent in the field of software verification where a "formal understanding" of the service/software is needed in order to verify it. However, such solutions are both low-level, tedious, and undecidable to be of practical use. Instead, we are interested in modeling higher-level concepts. That is, we are more interested in answering questions such as, what does a service do from the end user's or service integrator's perspective, as opposed to the far more difficult questions, such as, what does the service do from a computational view? We care more about real world concepts such as "customer", "bank account", and "flight itinerary" as opposed to the data structures and algorithms used by a service to model these concepts. The distinction is subtle, but is a distinction of granularity as well as a distinction of scope. In order to allow interoperability and machine-readability of our documents, a common conceptual ground must be agreed upon. The first step towards this common ground are standard languages such as WSDL and OWL. However, these do not go far enough, as for any given type of service there are numerous distinct representations in WSDL and for high-level concepts (e.g., a ternary predicate), there are numerous disparate representations in terms of OWL, representations that are distinct in terms of OWL's formal semantics, yet equal in the actual concepts they model. This is known as the semantic aliasing problem: distinct syntactic representations with distinct formal semantics yet equal conceptual semantics. For the semantics to equate things that are conceptually equal, we need to standardize a sufficiently comprehensive set of basic concepts, i.e., a universal ontology, along with a restricted set of connectives. Industry specific ontologies along with OWL can also be used to formally describe web-services. This is the approach taken by the OWL-S language [5]. The problem with this approach is that it requires standardization and undue foresight. Standardization is a slow, bitter process, and industry specific ontologies would require this process to be iterated for each specific industry. Furthermore, reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing is even more difficult. Undue foresight is required because many useful web services will address innovative applications and industries that don't currently exist. Standardizing an ontology for travel and finances is easy, as these industries are well established, but new innovative services in new upcoming industries also need be ascribed formal meaning. A universal ontology will have no difficulty in describing such new services. We need an ontology that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. WordNet [8] is a sufficiently comprehensive ontology that meets these criteria. As stated, part of the common ground involves standardized languages such as OWL. For this reason, WordNet cannot be used directly, and instead we make use of an encoding of WordNet as an OWL base ontology [2]. Using an OWL WordNet ontology allows our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which we map web service messages and operations. As long as this mapping is precise and sufficiently expressive, reasoning can be done within the realm of OWL by using automated inference systems (such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes relationships between ontological concepts, especially subsumption (hyponym-hypernym) and equivalence (synonym) relationships.

3

3

USDL: An Overview

As mentioned earlier, USDL can be regarded as a language to formally specify the semantics of web-services. It is perhaps the first attempt to capture the semantics of web-services in a universal, yet decidable manner. It is quite distinct from previous approaches such as WSDL and OWL-S [5]. As mentioned earlier, WSDL only defines syntax of the service; USDL provides the missing semantic component. USDL can be thought of as a formal language for service documentation. Thus, instead of documenting the function of a service as comments in English, one can write USDL statements that describe the function of that service. USDL is quite distinct from OWL-S, which is designed for a similar purpose, and as we shall see the two are in fact complementary. OWL-S primarily describes the states that exist before and after the service and how a service is composed of other smaller sub-services (if any). Description of atomic services is left under-specified in OWLS. They have to be specified using domain specific ontologies; in contrast, atomic services are completely specified in USDL, and USDL relies on a universal ontology (OWL WordNet Ontology) to specify the semantics of atomic services. USDL and OWL-S are complementary in that OWL-S's strength lies in describing the structure of composite services, i.e., how various atomic services are algorithmically combined to produce a new service, while USDL is good for fully describing atomic services. Thus, OWL-S can be used for describing the structure of composite services that combine atomic services described using USDL. USDL describes a service in terms of portType and messages, similar to WSDL. The semantics of a service is given using the OWL WordNet ontology: portType (operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly negated) concepts in the OWL WordNet ontology. The semantics is given in terms of how a service affects the external world. The present design of USDL assumes that each side-effect is one of following four operations: create, update, delete, or find. A generic affects side-effect is used when none of the four apply. An application that wishes to make use of a service automatically should be able to reason with WordNet atoms using the OWL WordNet ontology. We also define the formal semantics of USDL. As stated earlier, the syntactic terms describing portType and messages are mapped to disjunctions of conjunctions of (possibly negated) OWL WordNet ontological terms. A service is then formally defined as a function, labeled by the sideeffect. The main contribution of our work is the design of a universal service-semantics description language (USDL), along with its formal semantics, and a theory of service substitution using it.

4

Design of USDL

The design of USDL rests on two formal languages: Web Services Description Language (WSDL) [7] and Web Ontology Language (OWL) [6]. The Web Services Description Language (WSDL) [7] is used to give a syntactic description of the name and parameters of a service. The description is syntactic in the sense that it describes the formatting of services on a syntactic level of method signatures, but is incapable of describing what concepts are involved in a service and what a service actually does, i.e., the conceptual semantics of the service. Likewise, the Web Ontology Language (OWL) [6], was developed as an extension to the Resource Description Framework (RDF) [3], both standards are designed to allow formal conceptual modeling via logical ontologies, and these languages also allow for the markup of existing web resources with semantic information from the conceptual models. USDL employs WSDL and OWL in order to describe the syntax and semantics 4

of web-services. WSDL is used to describe message formats, types, and method prototypes, while a specialized universal OWL ontology is used to formally describe what these messages and methods mean, on a conceptual level. USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL documents contain two main constructs to which we want to ascribe conceptual meaning: messages and portType. These constructs are aggregates of service components which will be directly ascribed meaning. Messages consist of typed parts and portType consists of operations parameterized on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes, which have properties with values in the OWL WordNet ontology.

4.1

Concept

USDL defines a generic class called Concept which is used to define the semantics of parts of messages.
<owl:Class rdf:ID="Concept"> <rdfs:comment>Generic class of USDL Concept</rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#BasicConcept"/> <owl:Class rdf:about="#QualifiedConcept"/> <owl:Class rdf:about="#InvertedConcept"/> <owl:Class rdf:about="#ConjunctiveConcept"/> <owl:Class rdf:about="#DisjunctiveConcept"/> </owl:unionOf> </owl:Class>

The USDL Concept class denotes the conceptual objects constructed from the OWL WordNet ontology. For most purposes, message parts and other WSDL constructs will be mapped to a subclass of USDL Concept so that useful concepts can be modeled as set theoretic formulas of union, intersection, and negation of basic concepts. These subclasses of Concept are BasicConcept, QualifiedConcept, InvertedConcept, ConjunctiveConcept, and DisjunctiveConcept. 4.1.1 Basic Concept

An BasicConcept is the actual contact point between USDL and WordNet. This class acts as proxy for WordNet lexical entities.
<owl:Class rdf:about="#BasicConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isA"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions require all USDL BasicConcept s to have exactly one defining value for the isA property. An instance of BasicConcept is considered to be equated with a WordNet lexical concept given by the isA property.

5

<owl:ObjectProperty rdf:ID="isA"> <rdfs:domain rdf:resource="#BasicConcept"/> <rdfs:range rdf:resource="&wn;LexicalConcept"/> </owl:ObjectProperty>

4.1.2

Qualified Concept

A QualifiedConcept is a concept classified by another lexical concept.
<owl:Class rdf:about="#QualifiedConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isA"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#ofKind"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions require all USDL QualifiedConcept s to have exactly one defining value for the isA property, and exactly one value for the ofKind property. An instance of QualifiedConcept is considered to be equated with a lexical concept given by the isA property and classified by a lexical concept given by the optional ofKind property.
<owl:ObjectProperty rdf:ID="isA"> <rdfs:domain rdf:resource="#QualifiedConcept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="ofKind"> <rdfs:domain rdf:resource="#QualifiedConcept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.1.3

Inverted Concept

In case of InvertedConcept the corresponding semantics are the complement of USDL concepts.
<owl:Class rdf:about="#InvertedConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1

6

</owl:cardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasConcept"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.1.4

Conjunctive and Disjunctive Concept

The ConjunctiveConcept and DisjunctiveConcept respectively denote the intersection and union of USDL Concept s.
<owl:Class rdf:about="#ConjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveConcept"> <rdfs:subClassOf rdf:resource="#Concept"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

The property cardinality restrictions on ConjunctiveConcept and DisjunctiveConcept allow for n-ary intersections and unions (where n  2) of USDL concepts. For generality, these concepts are either BasicConcepts, QualifiedConcepts, ConjunctiveConcepts, DisjunctiveConcepts, or InvertedConcepts .

4.2

Affects

The affects property is specialized into four types of actions common to enterprise services: creates, updates, deletes, and finds.
<owl:ObjectProperty rdf:ID="affects"> <rdfs:comment> Generic class of USDL Affects </rdfs:comment> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/>

7

</owl:ObjectProperty> <owl:ObjectProperty rdf:about="#creates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#updates"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#deletes"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:about="#finds"> <rdfs:subPropertyOf rdf:resource="#affects"/> </owl:ObjectProperty>

Note that each of these specializations inherits the domain and range of the affects property. Most services can be described using one of these types of effects. For those services that cannot be described in terms of these specializations, the parent affects property can be used instead which is described as an USDL concept.

4.3

Conditions and Constraints

Services may have some external conditions (pre-conditions and post-conditions) specified on the input or output parameters. Condition class is used to describe all such constraints. Conditions are represented as conjunction or disjunction of binary predicates. Predicate is a trait or aspect of the resource being described.
<owl:Class rdf:ID="Condition"> <rdfs:comment> Generic class of USDL Condition </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#AtomicCondition"/> <owl:Class rdf:about="#ConjunctiveCondition"/> <owl:Class rdf:about="#DisjunctiveCondition"/> </owl:unionOf> </owl:Class> <owl:Class rdf:about="#AtomicCondition"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasConcept"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#onPart"/> <owl:cardinality rdf:datatype="&xsd;nonNegativeInteger">

8

1 </owl:cardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasValue"/> <owl:maxCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:maxCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

A condition has exactly one value for the onPart property and at most one value for the hasValue property, each of which is of type USDL Concept.
<owl:ObjectProperty rdf:ID="onPart"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="hasValue"> <rdfs:domain rdf:resource="#AtomicCondition"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

4.3.1

Conjunctive and Disjunctive Conditions

The ConjunctiveCondition and DisjunctiveCondition respectively denote the conjunction and disjunction of USDL Condition s.
<owl:Class rdf:about="#ConjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#DisjunctiveCondition"> <rdfs:subClassOf rdf:resource="#Condition"/> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasCondition"/> <owl:minCardinality rdf:datatype= "&xsd;nonNegativeInteger"> 2 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

9

<owl:ObjectProperty rdf:ID="hasCondition"> <rdfs:domain rdf:resource="#Concept"/> <rdfs:range rdf:resource="#Condition"/> </owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveCondition and DisjunctiveCondition allow for n-ary conjunctions and disjunctions (where n  2) of USDL conditions. In general any n-ary condition can be written as a combination of conjunctions and disjunctions of binary conditions.

4.4

Messages

Services communicate by exchanging messages. As mentioned, messages are simple tuples of actual data, called parts. Take for example, a flight reservation service similar to the SAP ABAP Workbench Interface Repository for flight reservations [4], which makes use of the following message.
<message name="#ReserveFlight_Request"> <part name="#CustomerName" type="xsd:string"> <part name="#FlightNumber" type="xsd:string"> <part name="#DepartureDate" type="xsd:date"> ... </message>

The USDL surrogate for a WSDL message is the Message class, which is a composite entity with zero or more parts. Note that for generality, messages are allowed to contain zero parts.
<owl:Class rdf:ID="Message"> <rdfs:comment> Generic class of USDL Message </rdfs:comment> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#Input"/> <owl:Class rdf:about="#Output"/> </owl:unionOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasPart"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:Class rdf:about="#Input"> <rdfs:subClassOf rdf:resource="#Message"/> </owl:Class> <owl:Class rdf:about="#Output"> <rdfs:subClassOf rdf:resource="#Message"/> </owl:Class>

Each part of a message is simply a USDL Concept, as defined by the hasPart property. Semantically messages are treated as tuples of concepts.

10

<owl:ObjectProperty rdf:ID="hasPart"> <rdfs:domain rdf:resource="#Message"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

Continuing our example flight reservation service, the ReserveF lightRequest message is given semantics using USDL as follows, where &wn;customer and &wn;name are valid XML references to WordNet lexical concepts.
<Message rdf:about="#ReserveFlight_Request"> <hasPart rdf:resource="#CustomerName"/> <hasPart rdf:resource="#FlightNumber"/> <hasPart rdf:resource="#DepartureDate"/> </Message> <QualifiedConcept rdf:about="#CustomerName"> <isA rdf:resource="#Name"/> <ofKind rdf:resource="#Customer"/> </QualifiedConcept> <BasicConcept rdf:about="#Name"> <isA rdf:resource="&wn;name"/> </BasicConcept> <BasicConcept rdf:about="#Customer"> <isA rdf:resource="&wn;customer"/> </BasicConcept> <!-- Similarly concepts FlightNumber and DepartureDate are defined -->

4.5

PortType

A service consists of portType, which is a collection of procedures or operations that are parametric on messages. Our example flight reservation service might contain a portType definition for a flight reservation service that takes as input an itinerary and outputs a reservation receipt.
<portType rdf:about="#Flight_Reservation"> <hasOperation rdf:resource="#ReserveFlight"> </portType> <operation rdf:about="#ReserveFlight"> <hasInput rdf:resource="#ReserveFlight_Request"/> <hasOutput rdf:resource="#ReserveFlight_Response"/> <creates rdf:resource="#FlightReservation" /> </operation>

The USDL surrogate is defined as the class portType which contains zero or more Operation s as values of the hasOperation property.
<owl:Class rdf:about="#portType"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource=#hasOperation"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality>

11

</owl:Restriction> </rdfs:subClassOf> </owl:Class> <owl:ObjectProperty rdf:ID="hasOperation"> <rdfs:domain rdf:resource="#portType"/> <rdfs:range rdf:resource="#Operation"/> </owl:ObjectProperty>

As with the case of messages, portTypes are not directly assigned meaning via the OWL WordNet ontology. Instead the individual Operation s are described by their side-effects via an affects property. Note that the parameters of an operation are already given meaning by ascribing meaning to the messages that constitute the parameters.
<owl:Class rdf:about="#Operation"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasInput"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#hasOutput"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 0 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#affects"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class>

An operation can have one or more values for the affects property, all of which are of type USDL Concept, which is the target of the effect.
<owl:ObjectProperty rdf:ID="hasInput"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Input"/> </owl:ObjectProperty> <owl:ObjectProperty rdf:ID="hasOutput"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Ouput"/> </owl:ObjectProperty>

12

<owl:ObjectProperty rdf:ID="affects"> <rdfs:domain rdf:resource="#Operation"/> <rdfs:range rdf:resource="#Concept"/> </owl:ObjectProperty>

5

Semantic Description of a Service

This section shows an example syntactic description of a web-service using WSDL and its corresponding semantic description using USDL. Hotel Reservation Service: The service described here is a simplified hotel-reservation service published in a web-service registry. This service can be treated as atomic: i.e., no interactions between buying and selling agents are required, apart from invocation of the service and receipt of its outputs by the buyer. Given certain inputs and pre-conditions, the service provides certain outputs and has specific effects. This service takes in a HotelChain, StartDate, NumNights, NumPersons, NumRooms, FirstName, and LastName as input parameters. It has a few input pre-conditions that NumNights, NumRooms must be greater than zero and StartDate must be greater than today. This service outputs a Reservation at the end of transaction.

5.1

WSDL definition

The following is WSDL definition of the service. This service provides a single operation called ReserveHotel. The input and output messages are defined below. The conditions on the service cannot be described using WSDL.
<definitions ...> <portType name="ReserveHotel_Service"> <operation name="ReserveHotel"> <input message="ReserveHotel_Request"/> <output message="ReserveHotel_Response"/> </operation> </portType> <message name="ReserveHotel_Request"> <part name="HotelChain" type="xsd:string"/> <part name="StartDate" type="xsd:date"/> <part name="NumNights" type="xsd:integer"/> <part name="NumPersons" type="xsd:integer"/> <part name="NumRooms" type="xsd:integer"/> <part name="FirstName" type="xsd:integer"/> <part name="LastName" type="xsd:integer"/> </message> <message name="ReserveHotel_Response"> <part name="Reservation" type="xsd:string"/> </message> ... </definitions>

13

5.2

USDL annotation
<QualifiedConcept rdf:about="#StartDate"> <isA rdf:resource="#Date"/> <ofKind rdf:resource="#Start"/> </QualifiedConcept> <QualifiedConcept rdf:about="#TodaysDate"> <isA rdf:resource="#Date"/> <ofKind rdf:resource="#Today"/> </QualifiedConcept> <!-- Similarly we can define Qualified Concepts for #NumNights, #NumPersons, #NumRooms, #FirstName and #LastName --> <BasicConcept rdf:about="#Hotel"> <isA rdf:resource="&wn;hotel"/> </BasicConcept> <BasicConcept rdf:about="#Chain"> <isA rdf:resource="&wn;chain"/> </BasicConcept> <BasicConcept rdf:about="#Start"> <isA rdf:resource="&wn;start"/> </BasicConcept> <BasicConcept rdf:about="#Date"> <isA rdf:resource="&wn;date"/> </BasicConcept> <BasicConcept rdf:about="#greaterThan"> <isA rdf:resource="&wn;greater_than"/> </BasicConcept> <BasicConcept rdf:about="#Date"> <isA rdf:resource="&wn;date"/> </BasicConcept> <BasicConcept rdf:about="#Today"> <isA rdf:resource="&wn;today"/> </BasicConcept> <BasicConcept rdf:about="#Reservation"> <isA rdf:resource="&wn;reservation"/> </BasicConcept> <!-- Similarly we can define Basic Concepts for #nights, #rooms, #number, #persons, #name, etc. -->

The following is the complete USDL annotation corresponding to the above mentioned WSDL description. The input pre-condition and the global constraint on the service are also described semantically.
<definitions> <portType rdf:about= "#ReserveHotel_Service"> <hasOperation rdf:resource= "#ReserveHotel"/> </portType> <operation rdf:about="#ReserveHotel"> <hasInput rdf:resource= "#ReserveHotel_Request"/> <hasOutput rdf:resource= "#ReserveHotel_Response"/> <creates rdf:resource= "#HotelReservation"/> </operation> <Message rdf:about= "#ReserveHotel_Request"> <hasPart rdf:resource="#HotelChain"/> <hasPart rdf:resource="#StartDate"/> <hasPart rdf:resource="#NumNights"/> <hasPart rdf:resource="#NumPersons"/> <hasPart rdf:resource="#NumRooms"/> <hasPart rdf:resource="#FirstName"/> <hasPart rdf:resource="#LastName"/> </Message> <Message rdf:about= "#ReserveHotel_Response"> <hasPart rdf:resource= "#HotelReservation"/> </Message> <Condition rdf:about="#greaterThanToday"> <hasConcept rdf:resource="#greaterThan"/> <onPart rdf:resource="#StartDate"/> <hasValue rdf:resource="#TodaysDate"/> </Condition> <!-- Similarly we can define Condition #greaterThanZero on parts #NumRooms and #NumNights -->

</definitions> <QualifiedConcept rdf:about="#HotelChain"> <isA rdf:resource="#Chain"/> A Book-Buying Service example is presented <ofKind rdf:resource="#Hotel"/> in [15]. </QualifiedConcept>

14

6

Theory of Substitution of Services

Next, we will investigate the theoretical aspects of USDL. This involves concepts from set theory. From a systems integration perspective, an engineer is interested in finding (discovering) a service that accomplishes some necessary task. Of course, such a service may not be present in any service directory. In such a case the discovery software should return a set of services that can be used in a context expecting a service that meets that description (of course, this set may be empty). To find services that can be substituted for a given service that is not present in the directory, we need to develop a theory of service substitutability. We develop such a theory in this section. Our theory relates service substitutability to WordNet's semantic relations. In order to develop this theory, we must first formally define constructs such as USDL-described concepts, affects, conditions and services, which we will also call concepts, affects, conditions and services for short. While it is possible to work directly with the XML USDL syntax, doing so is cumbersome and so we will instead opt for set theoretic notation. Definition 1 (Set of WordNet Lexemes) Let  be the set of WordNet lexemes. The following semantic relations exist on elements of . 1. Synonym: A pair of WordNet Lexemes having the same or nearly the same meaning have the synonym relation. Example, `purchase' is a synonym of `buy'. 2. Antonym: A pair of WordNet Lexemes having the opposite meaning have the antonym relation. Example, `start' is an antonym of `end'. 3. Hyponym: A word that is more specific than a given word is called the subordinate or hyponym of the other. Example, `car' is a hyponym of `vehicle'. 4. Hypernym: A word that is more generic than a given word is called the super-ordinate or hypernym of the other. Example, `vehicle' is a hypernym of `car'. 5. Meronym: A word that names a part of a larger whole is a meronym of the whole. Example, `roof' and `door' are meronyms of `house'. 6. Holonym: A word that names the whole of which a given word is a part is a holonym of the part. Example, `house' is a holonym for `roof' and `door'. Definition 2 (Representation of USDL Concepts) 1. A Basic Concept c = x, where x is a WordNet lexeme, defines the values of isA property. Example, customer is a Basic Concept and a WordNet lexeme. 2. A Qualified Concept c = (X, Y ), where X, Y are USDL concepts, defines the values of isA and ofKind properties. Example, concept FlightNumber is a number of kind flight represented as (number, flight ). 3. An Inverted Concept c is represented as 촞 where X is an USDL concept. Example, concept not a customer name can be represented as (name, customer ). 4. Let X, Y be USDL Concepts. (i) Conjunctive Concept c is represented as X  Y . Example, concept EvenRationalNumber is represented as (number, even )  (number, rational ). (ii) Disjunctive Concept c is represented as X Y . Example, concept OrderNumber/AvailabilityMessage is represented as (number, order )  (message, availability ). Definition 3 (Universe of USDL Concepts) 15

Let  be the set of USDL concepts.  can be inductively constructed as follows: 1. x   implies x   2. X, Y   implies (X, Y )   3. X   implies 촞   4. X, Y   implies X  Y   5. X, Y   implies X  Y   Definition 4 (Semantic relations of Basic Concepts) Semantic relations hold between two Basic concepts if their corresponding WordNet lexemes have the same semantic relation in . For example, Basic Concept Purchase is a synonym of Basic Concept Buy. Definition 5 (Synonym and Antonym relation of Qualified Concepts) Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2  . 1. C1 is synonym of C2 if X1 is recursively a synonym of X2 and Y1 is recursively a synonym of Y2 . 2. If X1 = w1 and X2 = w2 where w1 , w2  , then X1 is synonym of X2 if w1 and w2 have the synonym relation in . For example, Qualified Concept (date, begin ) is a synonym of Qualified Concept (date, start ). Similarly we can determine the antonym relation between Qualified Concepts. For example, Qualified Concept (date, begin ) is a antonym of Qualified Concept (date, end ). Definition 6 (Hyponym and Hypernym relation of Qualified Concepts) Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2  . 1. C1 is hypernym of C2 if any one of the following holds: (i) X1 is recursively a hypernym of X2 and Y1 is recursively a hypernym of Y2 . (ii) X1 is recursively a hypernym of X2 and Y1 is recursively a synonym of Y2 . (iii) X1 is recursively a synonym of X2 and Y1 is recursively a hypernym of Y2 . 2. If X1 = w1 and X2 = w2 where w1 , w2  , then X1 is hypernym of X2 if w1 and w2 have the hypernym relation in . For example, Qualified Concept (number, vehicle ) is a hypernym of Qualified Concept (number, car ). Similarly we can determine the hyponym relation of Qualified Concepts. For example, Concept (number, car ) is a hyponym of (number, vehicle ). Definition 7 (Holonym and Meronym relation of Qualified Concepts) Let C1 and C2 are Qualified Concepts where C1 =(X1 , Y1 ), C2 =(X2 , Y2 ) and X1 , X2 , Y1 , Y2  . 1. C1 is meronym of C2 if any one of the following holds: (i) X1 is recursively a meronym of X2 and Y1 is recursively a meronym of Y2 . (ii) X1 is recursively a meronym of X2 and Y1 is recursively a synonym of Y2 . (iii) X1 is recursively a synonym of X2 and Y1 is recursively a meronym of Y2 . 2. If X1 = w1 and X2 = w2 where w1 , w2  , then X1 is meronym of X2 if w1 and w2 have the meronym relation in .

16

For example, Qualified Concept (door, brown ) is a meronym of Qualified Concept (house, brown ). Similarly we can determine the holonym relation of Qualified Concepts. For example, Qualified Concept (house, brown ) is a holonym of Qualified Concept (door, brown ). Definition 8 (Semantic relations between Inverted Concepts) Let C1 and C2 be two Inverted concepts where C1 = 촞1 and C2 = 촞2 . 1. C1 is a synonym of C2 if X1 and X2 are synonyms. 2. C1 is an antonym of C2 if X1 and X2 are antonyms. 3. C1 is a hypernym of C2 if X1 and X2 are hyponyms and vice versa. 4. C1 is a meronym of C2 if X1 and X2 are holonyms and vice versa. For example, Inverted Concept (date, begin ) is a synonym of Inverted Concept (date, start ). The synonym-antonym relation, hyponym-hypernym relation and meronym-holonym relation can be extended to Conjunctive and Disjunctive concepts. Definition 9 (Semantic relations between Conjunctive (resp., Disjunctive) Concepts) Let C1 and C2 be two Conjunctive (resp., Disjunctive) concepts where C1 = X1  Y1 and C2 = X2  Y2 . 1. C1 is a synonym of C2 if X1 is a synonym of X2 and Y1 is a synonym of Y2 OR X1 is a synonym of Y2 and Y1 is a synonym of X2 . 2. C1 is a hypernym of C2 if one of the following holds: (i) X1 is a hypernym of X2 and Y1 is a hypernym/synonym of Y2 (ii) X1 is a hypernym/synonym of X2 and Y1 is a hypernym of Y2 (iii) X1 is a hypernym of Y2 and Y1 is a hypernym/synonym of X2 . (iv) X1 is a hypernym/synonym of Y2 and Y1 is a hypernym of X2 . For example, Conjunctive Concept (vehicle, blue )  (vehicle, automatic ) is a hypernym of (car, blue )  (car, automatic ). Similar to the above defined hypernym relation, we can define the antonym, hyponym, meronym, and holonym relations between Conjunctive (resp., Disjunctive) Concepts. Definition 10 (Substitution of Concepts) 1. Exact Substitution: For any concepts C, C  , if C is a synonym of C , then C is the exact substitutable of C and C can safely be used in a context expecting concept C . Example, concept Purchase is an exact substitutable of concept Buy. 2. Generic Substitution: For any concepts C, C  , if C is a hypernym of C , then C is the generic substitutable of C and C can safely be used in a context expecting concept C or a super-ordinate of C . Example, concept (number, vehicle ) is a generic substitutable of concept (number, car ). 3. Specific Substitution: For any concepts C, C  , if C is a hyponym of C , then C is the specific substitutable of C and C can safely be used in a context expecting concept C or a sub-ordinate of C . Example, concept (number, car ) is a specific substitutable of concept (number, vehicle ). 4. Part Substitution: For any concepts C, C  , if C is a meronym of C , then C is the part substitutable of C and C can safely be used in a context expecting a concept that is a part of C . Example, concept Roof is a part substitutable of concept House.

17

5. Whole Substitution: For any concepts C, C  , if C is a holonym of C , then C is the whole substitutable of C and C can safely be used in a context expecting a concept that is a whole of C . Example, concept House is a whole substitutable of concept Roof. Definition 11 (Representation of Affects) Let  = {(L, E ) | L  (  ), E  } be the set of USDL side-effects, where  = {creates, updates, deletes, f inds}, L is the affect type and E is the affected object. The affect type could be one of the pre-defined affects from  or a generic effect which is described as a concept. Definition 12 (Substitution of Affects) USDL affect is represented as a pair where the first element is the affect type and second element is the affected object. Both affect type and the affected object are described as USDL concepts. Let A1 and A2 be two affects where A1 = (L1 , E1 ) and A2 = (L2 , E2 ). A1 can safely be used in a context expecting affect A2 if all of the following hold: 1. Concept L1 is substitutable for L2 2. Concept E1 is substitutable for E2 . These substitutables can be of kind Exact, Generic, Specific, Part, or Whole which also determines the kind of substitution of the affect A1 in a context expecting A2 . Example, affect (finds, VehicleNumber ) is a generic substitution of affect (lookup, CarNumber ) as concept finds is an exact substitutable of lookup and concept VehicleNumber is a generic substitutable of concept CarNumber. Definition 13 (Representation of Conditions) Let  = {(P, Arg1 , Arg2 ) | P, Arg1 , Arg2  } be the set of USDL conditions. P is the constraint which is either a binary or a unary predicate. Arg1 is the concept on which the predicate acts and Arg2 is the concept which represents a value. Arg2 is an optional parameter. Definition 14 (Substitution of Conditions) USDL condition is represented as a tuple made up of the constraint or predicate and two arguments. The constraint and the arguments are described as USDL concepts. Let C1 and C2 be two conditions where C1 = (P1 , F irstArg1 , SecondArg1 ) and C2 = (P2 , F irstArg2 , SecondArg2 ). C1 can safely be used in a context expecting condition C2 if all of the following hold: 1. Concept P1 is substitutable for P2 2. Concept F irstArg1 is substitutable for F irstArg2 3. Concept SecondArg1 is substitutable for SecondArg2 . These substitutables can be of kind Exact, Generic, Specific, Part, or Whole which also determines the kind of substitution of the condition C1 in a context expecting C2 . Example, condition (greaterThan, NumberOfNights, 0) is an exact substitution of condition (moreThan, NumberOfNights, 0). Definition 15 (Representation of a Web Service)

18

For any set S , let S  = { |  / S }  {(x, y ) | x  S, y  S  } be the set of lists over S . Let  be the set of USDL service descriptions represented in the form of terms. The USDL description of a web service consists of 1. A list of Inputs, I   2. A list of Outputs, O   3. A list of Pre-Conditions, Pre-Condition   4. A list of Post-Conditions, Post-Condition   5. Side-effects, (affect-type, affected-object)   USDL service description can be treated as a term of first-order logic [16]. The side-effect of a service comprises of an affect type and the affected object. The service can be converted into a triple as follows: (Pre-Conditions, affect-type(affected-object, I, O), Post-Conditions). The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output parameters of the service. We represent services as triples so that they can be treated as terms in first-order logic. The first-order logic unification algorithm [16] then can be extended to specialized unifications for exact, generic, specific, part and whole substitutions. This work is in progress [10]. Now that the formal definitions of concept, affects, conditions and service descriptions are given, we would like to extend the theory of substitutability over  so that we can reason about substitutability of services. Definition 16 (Substitution of a Web Service) Let  and  be two services where  is represented as (Pre-Condition, affect-type(affected-object, I, O), Post-Condition) and  is represented as (Pre-Condition , affect-type (affected-object , I , O ), Post-Condition ).  can safely be used in a context expecting service  if all of the following hold: 1. Pre-Condition is substitutable for Pre-Condition 2. If the terms affect-type(affected-object, I, O) and affect-type (affected-object , I , O ) can be unified by applying an extended unification algorithm. The unification mechanism applied is different based on the kind of substitution needed. 3. Post-Condition is substitutable for Post-Condition Definition 17 For any services S1 , S2  , we say S1 WordNet semantic relations. S2 if S1 is a substitutable of S2 based on one of the

Thus far our notions of service substitutability are based on the six WordNet semantic relations discussed earlier. However, one can define the notion of service substitutability independently using the actual semantics (e.g., denotational semantics) of the program that realizes this service. Consider a service S1 with inputs I1 and outputs O1 , and another service S2 with inputs I2 and outputs O2 ; we ignore the side-effects of these services for the moment. The ideal conditions under which service S1 can be substituted for service S2 is the following: I1 I2 and O1 O2 . 19

Essentially, the inputs needed by S1 must be present in the inputs being provided in anticipation of availability of S2 . Likewise, the outputs produced by service S1 should contain the outputs anticipated from service S2 . In such a case, S1 can be directly substituted for S2 . There can be other types of general substitution relation defined. However, for these other types of substitutions, the code of the service being used for substitution may have to be modified or wrappers placed around it. One can, however, develop a more general notion of substitutability based on denotational semantics [17]. Let [[S1 ]] and [[S2 ]] be the semantic denotations of programs that implement services S1 and S2 respectively (note that the side-effects of these services will be captured as the state that becomes an argument in a denotational definition). Note that [[S1 ]] and [[S2 ]] can be regarded as points in a complete partial order [17] that represents the space of all functions. Service S1 can be substituted for S2 if [[S1 ]] and [[S2 ]] lie in the same chain in the complete partial order (i.e., either [[S1 ]] [[S2 ]] or [[S2 ]] [[S1 ]] where is the relation that induces the complete partial order among denotations of the services). Given the definition of substitutability based on denotational semantics, one can prove the soundness and completeness of our notion of substitutability based on the WordNet semantic relations, i.e., S1 S2  [[S1 ]] [[S2 ]] (soundness) [[S1 ]] [[S2 ]]  S1 S2 (completeness) Intuitively, one can see that these relationships hold, since the relation is defined in terms of subsumption of terms describing the service's inputs and outputs and its effect (create, update, delete, find and the generic affects). These soundness and completeness proofs are not included here due to lack of space.

7

Service Discovery

Now that our theory of service substitutability has been developed, it can be used to build tools for automatically discovering services as well as for automatically composing them. It can also be used to build a service search engine that discovers matching services and ranks them. We assume that a directory of services has already been compiled, and that this directory includes a USDL description document for each service. Inclusion of the USDL description, makes service directly "semantically" searchable. However, we still need a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as such a query language. A USDL description of the desired service can be written, a query processor can then search the service directory to look for a "matching" service. A discovery engine gets USDL descriptions from a service directory and converts them into terms of logic. The terms corresponding to the USDL query can be compared with the terms from the directory using an extended/special unification algorithm. Depending on the type of match required, the unification mechanism could be different. That is, the matching or unification algorithm used can look for an exact, generic, specific, part or a whole match depending on the desire of the user. Part and Whole substitutions are not useful while looking for matching services, but are very useful while selecting services for service composition. Also using Part or Whole substitutions for discovery may produce undesired side-effects.

20

The discovery engine can also rank the various services discovered. In this scenario, the discovery engine returns a list of substitutable services after applying ranking based on the kind of match obtained. Exact substitutables are assigned the highest rank among the different kind of substitutables. The following is the default ranking order used for the different substitutions. 1. Exact Substitution: The matching service obtained is equivalent to the service in the query. 2. Generic Substitution: The matching service obtained subsumes the service in the query. 3. Specific Substitution: The matching service obtained is subsumed by the service in the query. 4. Whole Substitution: The matching service obtained is a composite service of the service in the query and some other services. 5. Part Substitution: The matching service obtained is a part of a composite service that the query describes. The development of a service discovery engine based on these ideas is in progress. With the USDL descriptions and query language in place, numerous applications become possible ranging from querying a database of services to rapid application development via automated integration tools and even real-time service composition [12]. Take our flight reservation service example. Assume that somebody wants to find a travel reservation service and that they query a USDL database containing general purpose flight reservation services, bus reservation services, etc. One could then form a USDL query consisting of a description of a travel reservation service and the database could respond with a set of travel reservation services whether it be via flight, bus, or some other means of travel. This flexibility of generalization and specialization is gained from semantic information provided by USDL.

8

Service Composition

For service composition, the first step is finding the set of composable services. USDL itself can be used to specify the requirements of the composed service that an application developer is seeking. Using the discovery engine, individual services that make up the composed service can be selected. Part substitution technique can be used to find the different parts of a whole task and the selected services can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and post-conditions of the individual services. That is, if a subservice S1 is composed with subservice S2 , then the postconditions of S1 must imply the preconditions of S2 . In fact, the WordNet Universal ontology can also be helpful in automatically discovering services that can be composed together to satisfy a service discovery query. To achieve this, the discovery engine looks at the USDL concepts that describe the service in the query. It then searches the WordNet ontology to find out the meronymous components of that concept. The services that exactly match the meronymous components are then discovered using the standard discovery mechanism. Preconditions and postcondition consistency is then used to find the order in which the meronymous components should be stitched together to produce the desired service. A service composition engine of this kind is under development. Such an engine can also aid a systems integrator in rapidly creating composite services, i.e., services consisting of the composition of already existing services. In fact, such an engine can also be extended to automatically generate boilerplate code to manage the composite service, as well as menial inter-service data format conversions needed to glue the meronymous components together.

21

9

Comparison with OWL-S, WSDL-S, and WSML

In this section we present a comparison of USDL with other popular approaches such as OWL-S [5], WSML [1], and WSDL-S [24]. Our goal is to identify the similarities and differences of USDL with these approaches. OWL-S is a service description language which attempts to address the problem of semantic description via a highly detailed service ontology. But OWL-S also allows for complicated combining forms, which seem to defeat the tractability and practicality of OWL-S. The focus in the design of OWL-S is to describe the structure of a service in terms of how it combines other sub-services (if any used). The description of atomic services in OWL-S is left under-specified [9]. OWL-S includes the tags presents to describe the service profile, and the tag describedBy to describe the service model. The profile describes the (possibly conditional) states that exist before and after the service is executed. The service model describes how the service is (algorithmically) constructed from other simpler services. What the service actually accomplishes has to be inferred from these two descriptions in OWL-S. Given that OWL-S uses complicated combining forms, inferring the task that a service actually performs is, in general, undecidable. In contrast, in USDL, what the service actually does is directly described (via the verb affects and its refinements create, update, delete, and find). OWL-S recommends that atomic services be defined using domain specific ontologies. Thus, OWL-S needs users describing the services and users using the services to know, understand and agree on domain specific ontologies in which the services are described. Hence, annotating services with OWL-S is a very time consuming, cumbersome, and invasive process. The complicated nature of OWL-S's combining forms, especially conditions and control constructs, seems to allow for the aforementioned semantic aliasing problem [9]. Other recent approaches such as WSMO, WSML, WSDL-S, etc., suffer from the same limitation [1]. In contrast, USDL uses the universal WordNet ontology to solve this problem. Note that USDL and OWL-S can be used together. A USDL description can be placed under the describedBy tag for atomic processes, while OWL-S can be used to compose atomic USDL services. Thus, USDL along with WordNet can be treated as the universal ontology that can make an OWL-S description complete. USDL documents can be used to describe the semantics of atomic services that OWL-S assumes will be described by domain specific ontologies and pointed to by the OWL-S describedBy tag. In this respect, USDL and OWL-S are complementary: USDL can be treated as an extension to OWL-S which makes OWL-S description easy to write and semantically more complete. OWL-S can also be regarded as the composition language for USDL. If a new service can be built by composing a few already existing services, then this new service can be described in OWL-S using the USDL descriptions of the existing services. Next, this new service can be automatically generated from its OWL-S description. The control constructs like Sequence and If-Then-Else of OWL-S allows us to achieve this. Note once a composite service has been defined using OWL-S that uses atomic services described in USDL, a new USDL description must be written for this composite service (automatic generation of this description is currently being investigated [10]). This USDL description is the formal documentation of the new composite service and will make it automatically searchable once the new service is placed in the directory service. It also allows this composite service to be treated as an atomic service by some other application. For example, the aforementioned ReserveFlight service which creates a flight reservation can be viewed as a composite process of first getting the flight details, then checking the flight availabil-

22

ity and then booking the flight (creating the reservation). If we have these three atomic services namely GetFlightDetails, CheckFlightAvailability and BookFlight then we can create our ReserveFlight service by composing these three services in sequence using the OWL-S Sequence construct. The following is the OWL-S description of the composed ReserveFlight service.
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:process="http://www.daml.org/services/owl-s/1.0/Process.owl#"> <process:CompositeProcess rdf:ID="ReserveFlight"> <process:composedOf> <process:Sequence> <process:components rdf:parseType="Collection"> <process:AtomicProcess rdf:about="#GetFlightDetails"/> <process:AtomicProcess rdf:about="#CheckFlightAvailability"/> <process:AtomicProcess rdf:about="#BookFlight"/> </process:components> </process:Sequence> </process:composedOf> </process:CompositeProcess> </rdf:RDF>

We can generate this composed ReserveFlight service automatically. The component services can be discovered from existing services using their USDL descriptions. Once we have the component services, the OWL-S description can be used to generate the new composed service.

10

Related Work

Discovery and composition of web services has been active area of research recently [18, 19, 20, 21, 22, 23]. Most of these approaches are based on capturing the formal semantics of the service using an action description languages or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal. In contrast, we rely more on WordNet (which we use as a universal ontology) and the meronymous relationships of WordNet lexemes to achieve automatic composition. The approaches proposed by others also rely on a domain specific ontology (specified on OWL/DAML), and thus suffer from the problem mentioned earlier, namely, to discover/compose such services the discovery/composition engine has to be aware of the domain specific ontology. Thus, completely general discovery and composition engines cannot be built. Additionally, the domain specific ontology has to be quite extensive in that any relationship that can possibly exist between two terms in the ontology must be included in the ontology. In contrast, in our approach, the complex relationships (USDL concepts) that might be used to describe services or their inputs and outputs are part of USDL descriptions and not the ontology. Note that our approach is quite general, and it will work for domain specific ontologies as well, as long as the synonym, antonym, hyponym, hypernym, meronym, and holonym relations are defined between the various terms of the domain specific ontology. Another related area of research involves message conversation constraints, also known as behavioral signatures [13]. Behavior signature models do not stray far from the explicit description of the lexical form of messages, they expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional 23

implementation constraints, while USDL deals with higher-level real world concepts. However, USDL and behavioral signatures can be regarded as complementary concepts when taken in the context of real world service composition and both technologies are currently being used in the development of a commercial services integration tool [12].

11

Conclusions and Future Work

To reliably catalogue, search and compose services in a semi-automatic to fully-automatic manner we need standards to publish and document services. This requires language standards for specifying not just the syntax, i.e., prototypes of service procedures and messages, but it also necessitates a standard formal, yet high-level means for specifying the semantics of service procedures and messages. We have addressed these issues by defining a universal service-semantics description language, its semantics, and we have proved some useful properties about this language. The current version of USDL incorporates current standards in a way to further aid markup of IT services by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. This approach is more practical and tractable than other approaches because description documents are more easily created by humans and more easily processed by computers. USDL is currently being used to formally describe web-services related to emergency response functions [11]. Our current and future work involves the application of USDL to formally describing commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as to service discovery and rapid application development (RAD) in commercial environments [12]. Current and future work also includes automatically generating USDL description from the code/documentation of a service [12] as well developing tools that will allow automatic generation of new services based on combining USDL descriptions of existing atomic services. The interesting problem that arises then: can USDL description of such automatically generated services be also automatically generated? This problem is also part of our current/future work.

References
[1] A conceptual comparison between WSMO and OWL-S. www.wsmo.org/TR/d4/d4.1/v0.1. [2] Ontology-based information management system, wordnet OWL-Ontology. http://taurus. unine.ch/knowler/wordnet.html. [3] Resource Description Framework. http://www.w3.org/RDF. [4] SAP Interface Repository. http://ifr.sap.com/catalog/query.asp. [5] Semantic markup for web services. www.daml.org/services/owl-s/1.0/owl-s.html. [6] Web Ontology Language Reference. http://www.w3.org/TR/owl-ref. [7] Web Services Description Language. http://www.w3.org/TR/wsdl. [8] WordNet: A Lexical Database for the English Language. www.cogsci.princeton.edu/~wn. [9] S. Balzer, T. Liebig, and M. Wagner. Pitfalls of OWL-S - a practical semantic web use case. In ICSOC, 2004. [10] S. Kona, A. Bansal, G. Gupta, and T. Hite. Automatic Service Discovery and Composition with USDL. Working paper, 2006. 24

[11] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. D. Harris, and J. C. Staves. Towards Intelligent Services: A case study in chemical emergency response. In International Conference on Web Services, pp. 751-758, 2005. [12] T. Hite. Service Composition and Ranking: A strategic overview. Internal Report, Metallect Inc., 2005. [13] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004. [14] L. Simon, A. Bansal, A. Mallya, S. Kona, G. Gupta, and T. Hite. Towards a Universal Service Description Language. In Next Generation Web Services Practices, pp. 175-180, 2005. [15] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta, and T. Hite. A Universal ServiceSemantics Description Language. In European Conference On Web Services, pp. 214-225, 2005. [16] J.W. Lloyd. Foundations of Logic Programming. Springer-Verlag, 1987. [17] D. Schmidt. Denotational Semantics: A Methodology for Language Development. 1986. [18] B. Srivastava, J. Koehler. Web Services Composition - Current Solutions and Open Problems. In ICAPS, 2003. [19] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp. 46-53, Mar. 2001. [20] S. McIlraith, T.C. Son Adapting golog for composition of semantic web services. In KRR, pages 482493, 2002. [21] S. McIlraith, S. Narayanan Simulation, verification and automated composition of web services. In World Wide Web Conference, 2002. [22] G. Picinielli, et al. Web service interfaces for inter-orgranizational business processes - an infrastructure for automated reconciliation. In EDOC, pages 285-292, 2002. [23] B. Srivastava. Automatic Web Services Composition using planning. In KBCS, pages 467477. [24] Web Service Semantics - WSDL-S http://www.w3.org/Submission/WSDL-S.

25

220

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

A Low-Power Integrated x8664 and Graphics Processor for Mobile Computing Devices
Denis Foley, Member, IEEE, Pankaj Bansal, Member, IEEE, Don Cherepacha, Robert Wasmuth, Aswin Gunasekar, Srinivasa Gutta, and Ajay Naini
switching between modes depending on the current allowed power state. Two unique display output streams can be presented over any two of the following: 1) a DisplayPort (DP1.1a) interface; 2) a combination low-voltage differential signaling (LVDS)/DP1.1a port; or 3) an integrated video graphics array (VGA) DAC. A digital frequency synthesizer (DFS) block supplies the CPU core, graphics, multi-media, display, I/O, and NB clocks. The AMD Fusion architecture implements a very efficient form of unified memory architecture (UMA) in which a portion of system memory is reserved as graphics frame buffer memory. The graphics memory controller (GMC) arbitrates between graphics, video, and display memory requests and presents a well-ordered stream of system memory transactions through the NB over dedicated 256-bit-wide read and write busses. These GMC requests bypass all NB coherency mechanisms, allowing for fast direct access to memory and exposing all of the available memory bandwidth (8.53 GB/s). III. TECHNOLOGY Zacate is implemented in a 40 nm bulk CMOS process. Ten metal layers are used, the top two of which are redistribution layer (RDL). The die area is 75 mm . Each Bobcat core is 4.9 mm , and each L2 is 3.1 mm . Each core has approximately 7 nF of on-die capacitance. Excluding PHYs, the graphics, I/O, and multi-media blocks occupy 35 mm . There is approximately 60 nF of on-die capacitance associated with this logic. The die is packaged in a 19 mm 19 mm ball-grid array (BGA) package with 413 0.8 mm balls. The package substrate is a 2-2-2 layup. 10 nF of VDDNB package capacitance and 3.3 F of VDD capacitance are supported. Fig. 2 shows a Zacate die photograph with functional units labeled. IV. BOBCAT CORE The Bobcat core shown in Fig. 3 is a brand-new x86 design making its debut in Zacate. The core features a decoder capable of decoding two complex operations (COPs) per cycle. It supports the AMD64 64-bit ISA. The execution engine supports full out-of-order (OoO) execution, and the load/store engine can execute loads and stores out of order. The design features a high-performance floating-point unit and an advanced branch predictor. Streaming SIMD extensions including SSE1, SSE2, SSE3, SSSE3, SSE4A, and 128-bit mis-aligned data-type extensions are also supported. The design features 32 KB L1 caches and a dedicated 512 KB L2 cache. The design runs at 1.6 GHz, with the L2 running at half that rate. Bobcat supports core power gating.

Abstract--The first AMD FusionTM accelerated processing unit (APU), code-named "Zacate," incorporates a pair of Bobcat x86 processors, a 1 MB L2 cache, an AMD RadeonTM 6310 DirectX11 GPU with 80 stream processors, a media accelerator, an integrated NorthBridge (NB), integrated DisplayPort, LVDS, and VGA display interfaces, a PCIe Gen1 or Gen2 I/O interface, and a single 64-bit memory channel at up to DDR3-1066 on a single die implemented in a 40 nm bulk CMOS process. Index Terms--AMD fusion, APU, Bobcat, integrated graphics, low-power, Zacate.

I. INTRODUCTION HE AMD FusionTM accelerated processing unit (APU) code-named Zacate is implemented in a 40 nm CMOS bulk process. The design features the new synthesizable low-power Bobcat x6464 core and integrated AMD RadeonTM graphics. This paper shares details on the architecture, technology, functional units, justification for the AMD Fusion approach, and details on power savings techniques, power gating, and clocking. Some performance data is shared to provide context for the power profile of the device. This paper is divided into 13 sections including this introduction, technical topics, performance, conclusion, and references.

T

II. ARCHITECTURE The AMD Fusion APU code-named Zacate shown in Fig. 1 incorporates two low-power Bobcat x86 processors, each with a dedicated 512 KB L2 cache, an AMD RadeonTM 6310 DirectX11 graphics processing unit (GPU), and AMD's Universal Video Decoder (UVD) media acceleration engine. Memory access to a single 64-bit DDR3-1066 memory channel is controlled through an integrated NorthBridge (NB). Zacate supports a four-lane PCIe interface to the AMD Fusion Controller Hub (FCH) and a four-lane PCIe interface to an external GPU if desired. The PCIe links are capable of running at either 2.5 Gb/s Gen1 rate or 5 Gb/s Gen2 rate, and are capable of
Manuscript received April 26, 2011; revised June 22, 2011; accepted July 05, 2011. Date of publication October 19, 2011; date of current version December 23, 2011. This paper was approved by Guest Editor Alice Wang. D. Foley is with Advanced Micro Devices (AMD), Inc., Boxborough, MA 01719 USA (e-mail: denis.foley@amd.com). P. Bansal, S. Gutta, and A. Naini are with Advanced Micro Devices (AMD), Hyderabad 500034 A.P., India. D. Cherepacha is with Advanced Micro Devices (AMD), Oakville, Markham, ON, Canada L6H 6T5. R. Wasmuth and A. Gunasekar are with Advanced Micro Devices (AMD), Austin, TX 78735 USA. Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/JSSC.2011.2167776

0018-9200/$26.00  2011 IEEE

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

221

Fig. 1. Zacate block diagram.

Fig. 2. Zacate die shot with labeled functional units.

V. GRAPHICS AND MULTI-MEDIA Zacate contains a small, power-efficient AMD RadeonTM HD 6310 GPU that is sized to take full advantage of the 8.53 GB/s of memory bandwidth provided by the 64-bit channel of DDR31066 memory. The GPU comprises graphics, video, audio, and display capabilities similar to a discrete graphics card. The APU uses a UMA in which the GPU's frame buffer is implemented
Fig. 3. Bobcat low-power core.

by reserving a section of system memory. Typical memory allocation for the frame buffer is 384 MB. Additional GPU memory can be allocated dynamically from system memory.

222

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

TABLE I BLU-RAY PLAYBACK

The DirectX 11-compliant graphics core contains 80 stream processing units, eight texture units, 16 Z/Stencil ROP units, and four color ROP units. The computational units are organized as a 2-SIMD x 8 array of vector processors in which each vector processor consists of five stream processing units. The unified shader architecture allows a flexible set of programs to be executed. At 492 MHz, the core's peak rate of 78.7 GFLOPs provides sufficient compute capability for gaming or compute applications enabled by DirectCompute and OpenCL. Utilizing the AMD's latest video decoder, UVD3, Zacate provides uncompromised HD video playback, including many advanced video quality processing algorithms. UVD3 provides hardware acceleration for decode of H.264, VC-1, MPEG-2, and DivX/Xvid video streams. This facilitates low-power 1080p/1080i video Blu-ray playback. Table I shows measured CPU utilization when playing the Avatar Blu-ray disc (BD) with UVD acceleration enabled and disabled. Without acceleration, the CPU cores are fully utilized and unable to play the video without dropping video frames. With UVD acceleration, the CPU load is reduced to approximately 31%, facilitating a smooth video experience. The UVD also offloads from the CPU the decode of a variety of on-line video content, including Adobe Flash video. Zacate provides two independent display interfaces that can natively support VGA, LVDS, and DisplayPort. HDMI and DVI can be supported with additional system-level components. High-definition audio is supported, including Dolby TrueHD and DTS-HD Master Audio. The GIO block, shown in Fig. 1, performs system connectivity functions to route host and DMA traffic between the CPU cores, system memory, internal devices, and external devices. It contains a root complex supporting two four-lane PCIe Gen1 or Gen2 links. One link serves as the unified media interface (UMI) to the FCH. The second link supports discrete graphics attachments. VI. FUSION BASICS The traditional model of a processor chip (with integrated NB) coupled with an integrated graphics processor has a number of shortfalls. The high-speed PHY coupling the two processors (shown in red in Fig. 4) occupies significant area and consumes power. The power associated with the PHY can exceed 1 W during media playback. Additionally, the link may present a bandwidth bottleneck. When the two dies are integrated, a wide (256 bits in each direction) data path from the graphics memory controller to the NB is added, allowing for full access to system memory from the GMC. This path provides GMC clients with

Fig. 4. AMD fusion advantage.

Fig. 5. Bobcat floorplan.

a low latency path to non-snooped regions of system memory, reducing the minimum read latency by up to 40%. Compared

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

223

Fig. 7. Routing allocation in channel for Sub-chips.

Fig. 6. Sub-chip1 and Sub-chip2 partitioning.

to two-chip solutions, use of the on-die integrated GPU significantly reduces memory latency, improves request ordering, and reduces area and power. VII. IMPLEMENTATION The AMD Fusion APU--including the x86 Bobcat cores--is synthesized from RTL and implemented using standard ASIC-style synthesis auto-place and route (SAPR) flows. The die shown in Fig. 2 has more than 450 million transistors. The Bobcat core is implemented as a single physical entity consisting of 1.1 million instances and uses 35 instances of seven custom memory macros. The GPU core logical RTL is partitioned into multiple physical entities with varying numbers of standard cell instances and memory macros generated using standard memory compilers. The GPU sub-chip uses feed-through repeater bundles to connect the interfaces between different physical entities and has minimal grout space at the boundaries. The Bobcat floorplan shown in Fig. 5 shows the various subblocks in the core as placement regions. L2 sub-array, tag array,

and caches occupy most of the top and bottom of the floorplan, while the bus unit and floating-point unit are placement regions placed using various placement optimization techniques. The typical amoeba placement for various other sub-blocks in the floorplan shows the placement as achieved through careful floorplanning and tool optimization. The SOC was implemented in two sub-chips, and a two-level hierarchical floorplanning and partitioning approach was used. Sub-chip1 consisted of the Bobcat core cluster, NB, and all I/O PHYs. Sub-chip2 consisted of the graphics core and multi-media and I/O control, collectively referred to as the GNB (Fig. 6). Sub-chip1 and Sub-chip2 were further floorplanned to create an overlay channel between the Bobcat cores and DDR PHY through GNB sub-chip. Thus GNB Sub-chip2 could be independently designed and integrated with Sub-chip1 in SOC without causing any routing and integration issues. As shown in Fig. 7, specific metal layers were allocated to Sub-chip1 and Sub-chip2 for their respective signal interactions in horizontal and vertical directions. The CPU and GPU core operate on separate voltage supplies and support dynamic voltage and frequency scaling (DVFS) to optimize power consumption. The design is optimized for power and performance at discrete process and voltage points. The CPU is optimized for power and performance at 1.2 V and 0.8 V. Timing optimization at high voltage secures the performance of the CPU, while timing at low voltage secures the minimum

224

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

Fig. 8. Frequency vs. voltage for a representative GFX (SVT) path.

Fig. 10. Transistor threshold voltage mix.

Fig. 9. SOC statistics.

operating frequency and ensures that the best performance/watt is provided at the low frequency. The GPU is optimized at 1.0 V, 0.9 V, and 0.8 V. Incremental functionality or feature sets can be enabled at progressively higher voltages. The GPU design is closed at multiple points to ensure that the incremental capability is matched across the design elements and is optimized for performance/watt at all operating points. Fig. 8 shows how frequency of a representative GPU path--a mix of device and wire delays using SVT devices--varies with device voltage. Standard cell logic transistors comprise 2/3 of the total 451 million transistors, while memories and macros cover the remaining 1/3. The dual Bobcat core and L2 account for 12 Mb of the 23 Mb on-chip memory. Extensive powers saving techniques were used, including VT swapping across the GPU design and the Bobcat core. The design has transistors fabricated in various threshold voltages and lengths to facilitate performance/leakage trade-offs. At 105 , LVT devices are approximately 4x leakier than SVT devices, which are in turn approximately 3.5x leakier than HVT devices. As shown in Fig. 10, approximately 56% of the logic transistors are HVT, and approximately 42% are SVT. To limit leakage power, less than 2% are LVT devices, and those are used only in critical paths. The VT distribution contains the regular-and different-length variants for the same VT devices to further reduce power leakage without compromising performance/area.

Fig. 11. DFS clock generation.

VIII. CLOCKING The design has 16 functional, 10 scan, and 11 debug mode clocks. A digital frequency synthesizer (DFS) is used to generate nine functional clocks used by the CPUs, NB, and GPU (Fig. 11). The system PLL provides four phase-offset references to the DFS, which combines the phases to generate the required clock frequencies. As shown in Fig. 12, ClkEn_A[3:0] is a 4-bit phase-enable value that is presented to the DFS every VCO period. Each of the bits corresponds to one of the VCO output phases. If a phase is enabled in a particular VCO period (shaded in blue in Fig. 12), that phase is combined with other enabled phases to generate the output clock. By controlling a repeating phase-enable sequence, different clock frequencies can be generated easily. Because the

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

225

Fig. 12. Generating clocks using DFS.

Fig. 13. Clock delivery.

phase-enable sequence can be changed at any time, clock generation is not interrupted on frequency transitions. This clock generation mechanism builds on a similar mechanism used in AMD TurionTM processors, but adds more VCO phases for finer frequency control. The high-speed CPU clock is distributed as a mesh network with six to eight levels of local buffers. The GPU clock (SCLK) is implemented with X-tree and H-tree topologies feeding into the clock tree synthesis (CTS) branches inside a physical tile. All other clocks are implemented as balanced buffer trees from DFS to the physical tile. The NB and GPU clocks are balanced globally with respect to each other within a half-cycle of the fastest clock using programmable delay buffers pre-placed beside the global trees. With such an implementation, Zacate achieves a global skew of 38 ps and 50 ps in CPU and GPU clock distribution, respectively. Fig. 13 shows the two distinctive clock tree structures as deployed for top-level clock distribution in the two sub-chips. A 5% clock jitter target was used for the synthesized clocks.

Fig. 14. Zacate power domains.

IX. DESIGN FOR POWER Zacate's various power domains are shown in Fig. 14. Each Bobcat core and its L2 cache together share a power island (Fig. 15). A single variable VDD rail supplies both core power islands. Core clocks can be varied independently and VDD (CPU voltage) is selected to be the lowest voltage required to support the highest of the selected core clocks. Bobcat supports Core C6 (CC6) power state. If the core is idle, its caches can be flushed, its state saved to memory, and power to the core island is gated off. If both cores are idle and power-gated, the VDD

226

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

Fig. 17. VDDNB global power grid implementation.

Fig. 15. Bobcat power gating.

Fig. 16. VDDNB power gating.

power rail can be further reduced to eliminate leakage in even the power-gating structures. This state is called Package C6 (PC6). In PC6, voltage can be lowered to 0 V, but with frequent entry/exit to PC6, VDD is typically set at 0.45 V to minimize voltage ramp time and energy wasted in charging/discharging motherboard capacitance. GPU and the NB share a variable VDDNB rail as shown in Fig. 16. GPU and NB clocks are varied independently based on activity. VDDNB is selected based on the highest voltage requested. The GPU supports several power islands, allowing for driver-controlled power gating of the UVD video acceleration engine and independent dynamic power gating of the GFX core and the GMC. The display controller supports a static-screen refresh-stutter mode in which the controller requests data periodically from memory; between requests, the memory is kept in self-refresh and the NB enters a low-power state. By stuttering the display requests to memory, the time spent in self-refresh for the memory can be maximized, the time that clocks are running in the NB can be minimized and startup penalties (PLL power on and lock, DLL lock time) can be amortized over a larger memory transfer. Typical stutter efficiency (time spent in self-refresh during static screen rendering) is greater than 94%. APU MM07 power varies by approximately 130 mW for every 10% change in stutter efficiency. The combination of power savings techniques yields an APU with an average MobileMark (MM07) power consumption of less than 1.8 W. X. POWER GATING IMPLEMENTATION On-die power gating is implemented using a leakage-optimized HVT PFET header switch to isolate the VDD (or VDDNB) supplies. The header uses parallel stacked PFET transistors with separate enables for each PFET. This is in contrast to other x86 designs [1], [2] in which VSS isolation is

used. The Bobcat core uses a dense grid implemented in M9 and M10 to satisfy the high current density in the core and to reduce IR drop impact on frequency. The total PFET width for a core and its L2 is about 1.0 m. The corresponding number of power-gating PFETs is approximately 30,000. VDDNB power grid is implemented in M7 and M8 with power-gater PFETs placed on a checkerboard pattern (Fig. 17). Power gating for compiled memories is arranged at the top and bottom of the memory to avoid interference with the memory arrays. Tall memories implement a double row of PFETs at the top and bottom, while shorter memories implement a single row at the top and bottom. This is illustrated in Fig. 18, which is a close-up of a GPU tile showing the checkerboard pattern for logic and the single or double rows of PFETs for compiled memories. Each PFET header, as shown in Fig. 19, is made from two FETS with separate enables. A smaller WAKE FET is used to initiate current flow into the grid on enable. The controls for the WAKE FETs are daisy-chained with a return path to the power controller. Once the WAKE FETs have been enabled, large RUN FETs can be turned on to provide a robust low-onresistance connection to the power grid. The controls for the RUN FETs are similarly daisy-chained; once all the RUN FETs have been enabled, the power to the gated region is completely restored. This WAKE/RUN sequence is critical to avoid in-rush current spikes as the grid is enabled. The WAKE/RUN timing is programmable, allowing for post-silicon tuning of the delays in enabling the grid. The VDDNB grid is designed to drive a current density of 0.5 A/mm and achieve a 2% static IR drop. The total gate width of GPU headers is 1.93 m and resistance is 0.6 m . Fig. 20 shows the simulated voltage drop distribution across PFET headers used in the design. The switches were uniformly distributed in the design through pre-placement, and the voltage drop varies across switches depending on density and logic in the region. Fig. 21 shows four photon-emission captures of the Zacate die. All these captures are on a tester with clocks disabled, so the current flow is purely leakage. Capture duration was 30 seconds. 1.225 V and VDDNB Captures 1, 2, and 3 have VDD 0.875 V; capture 4 has VDD 0.875 V, and VDDNB 0.875 V. In capture 1, all functional units are enabled. In capture 2, CPU0 is power-gated off. In capture 3, CPU1 is gated off. In capture 4, all power-gated areas are off. The effect of power gating is immediately obvious. Capture 4 also highlights areas that are not gated off  the always on (AON) blocks. The small active area adjacent to the gated core in capture 2 and capture 3 is

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

227

Fig. 18. Header distribution in typical GPU tile.

Fig. 20. Voltage across VDDNB PFETs.

Fig. 19. WAKE/RUN power gater control.

interface logic between the core and the NB channel that was not power-gated. XI. BOBCAT FLOP Two flop types were used in the CPU core: a conventional master-slave flop, and a Bobcat flop (BT flop) shown in Fig. 22--a three-stage, pre-charged, asymmetric, MUXD flop. The BT flop is much faster than the master-slave flop, but is also bigger. The BT flop was used only on critical paths, thereby realizing its speed advantages while increasing the core area only a small amount.

Fig. 21. Meridian photon-emission power-gating analysis.

228

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

Fig. 22. Bobcat high-speed flop.

TABLE II IMPACT ON AREA AND FREQUENCY OF BT FLOP

The BT flop operates similarly to a sense amp flop [3]. While clock is low, both NFET stacks of Stage 1 are pre-charged. is The half tri-state of Stage 2 is, therefore, disabled, and determined by the active latch of Stage 3. When clock rises, Stage 1 is evaluated, Stage 2 is enabled, and Stage 3 feedback is disabled. The BT flop is faster than a sense amp flop for two reasons: stack order in Stage 1, and size skewing afforded by asymmetry of the transitions. In Stage 1, cross-coupled NFETs are at the bottom of the stack, and pre-discharged when clock is low. Last-arriving data is at the top of stack. Regarding the second advantage, the BT flop is nominally three gate delays if the left NFET stack of Stage 1 discharges, but only two gate delays if the right stack discharges. This is exploited in the beta ratios of each stage so rising and falling transitions have nearly equal clock-to- delays. A majority of critical timing paths in processor designs are limited by slow setup, clock-to- , or rising/falling edge, but not all of them simultaneously. A few experiments validated the hypothesis that SAPR tools could take advantage of flops designed to be the best in any given performance metric. A flop library development effort focused on using such a flop as the base style and building specialist variants of the same to maximize frequency. A close review of the top critical paths revealed important criteria such as output edge rate, fast rising/ fast falling, clock cycle extension, or clock pin buffering/unbuffering to favor fast clock-to- or setup, or slow buffer to

TABLE III GPU CAPABILITIES

further favor setup. Other criteria included increasing the range of drive strengths and adding combinational functions such as NAND and NOR. The resultant library contained about 100 flop variants to optimize for the various combinations of these features. Performance is summarized in Table II. BT flops have longer hold time and higher dynamic power than master-slave flops, but when their usage was limited to only critical paths, frequency improvement for the CPU core was 7%. XII. PERFORMANCE Bobcat is an AMD64 86 ISA core targeting low-cost/area and high power efficiency while maintaining comparable singlethread performance to mainstream client processors such as the

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

229

TABLE IV ZACATE GPU PERFORMANCE RELATIVE TO RADEON 4270

1.6 GHz AMD Turion Neo X2 L625, derived from the first-generation AMD OpteronTM core [4]. Although many applications are now multi-threaded and can take advantage of higher performance of Zacate's two cores, numerous user-sensitive operations are dependent on the performance of a single thread. Central to Bobcat providing high single-thread performance was the choice of OoO execution with a two-wide decode/retire scheme. The fully OoO design with deep speculation enables energy saving by keeping the pipeline filled while reducing dynamic and static energy spent on stalled cycles as well as significantly increasing IPC relative to in-order designs used in other contemporary small cores. To reduce power, several micro-architectural approaches were used, including minimizing over-provisioning [5], reducing mis-speculation, and minimizing data transfers and data structure accesses. The twowide design, relative to the three-wide design in the comparison machine, minimized over-provisioning in terms of execution units, register file ports, etc., but with some loss of performance, some of which was recovered through other optimizations. To reduce mis-speculation and improve performance, a sophisticated branch prediction structure was developed that utilized 32 k two-way L1 instruction cache with a return stack and indirect dynamic and advanced conditional branch predictors with a high-capacity 512/8 entry (4 k/2 m pages) instruction translation buffer (I-TLB). A physical register file is used both by the integer and floating-point units to reduce data transfers. The floating-point unit (FP) has a two execution stacks capable of two single-precision (SP) single-instruction/multipledata (SIMD) adds and two SP SIMD multiplies per cycle. The load-store unit (LS) is fully OoO with a hazard predictor to minimize mis-speculations, a 32 k eight-way data cache with a 40/8 entry (4 k/2 m) and a 512/64(4 k/2 m) entry Level 2 data translation buffer (L2DTLB), and an eight-stream data pre-fetcher. The instruction and data cache share an ECC-protected 16-way 512 KB per-core private L2 cache, clocked at half the core rate to reduce power. Fig. 23 illustrates single-thread performance comparable ( 0.9x) to the AMD Turion L625 based on SPECint2006 [6]. This is achieved in less than half the process scaled area and dynamic power of the AMD Turion L625 core. SPECint2006 is a benchmark that includes a broad range of compute-in-

Fig. 23. Bobcat performance relative to AMD turion L625.

tensive integer applications that stress different aspects of the processor, from branch prediction to the cache hierarchy. Table III specifies the basic performance metrics for the graphics core. The low-latency, high-bandwidth GMC memory interface increases performance per watt by allowing a higher percentage of the power to be used for computation rather than for moving large amounts of data across high-power chip-to-chip interconnects. For example, the power consumed by the HyperTransportTM link between the CPU and the AMD 880 G chipset was more than 1 W during periods of high GPU memory activity. With the AMD Fusion architecture, most of this interconnect power is saved. The advanced, multi-level memory arbitration units resolve performance issues with previous integrated graphics processors (IGP) by enabling the GPU to achieve memory efficiency similar to a discrete GPU without compromising CPU performance. GPU memory accesses are sent in DRAM efficient streams of reads and writes to avoid memory access penalties while CPU accesses are optimized for low read latency. These improvements yield a 17% increase in Zacate UMA memory efficiency compared to its two-chip predecessor.1 Table IV illustrates the gaming capabilities of Zacate using the industry-standard 3DMark benchmarks. The low-cost,
1Memory efficiency was measured using the Win7 Experience Index (WEI) graphics memory benchmark using system populated with one 64-bit DDR31066 SODIMM.

230

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 47, NO. 1, JANUARY 2012

TABLE V 3D GAME PERFORMANCE

power-efficient Zacate APU outperforms the mainstream mobile AMD RadeonTM 4270 M GPU in the AMD 880 G chipset. In 3DMark Vantage  Performance, Zacate is 2.26 times faster. Table V shows benchmarking results using several popular games. In addition to providing computational resources for 3D graphics operations, the 80 stream processing units may also be used to speed up many general-purpose floating-point-intensive applications. The OpenCL implementation of the floating-point-intensive Mandelbrot set generator in SiSoftware Sandra 2011 multi-media benchmark [7] illustrates this capability, yielding a 5x improvement in performance of the benchmark running on the GPU versus a multi-threaded, SIMD-vectorized implementation on the CPU cores. This result is consistent with the GPU-to-CPU peak FLOPS ratio of 6 to 1 (78.7 GFLOPs versus 12.8 GFLOPs).

Denis Foley (M'06) received the B.Eng.(Elect.) degree from University College Cork, Ireland, in 1983. He is a Senior Fellow at Advanced Micro Devices, Inc. He has more than 28 years of experience in the computing industry. At Digital Equipment and Compaq, he was a design lead in the Alpha server group. At Hewlett-Packard, he was the implementation lead for a high-end Itanium server. At ATI, he was the chip lead for a game console cost-down before turning his attention to low-power design, first as the system architect for a licensable 3D GPU core, then as the system architect for ATI's Imageon family of application processors for hand-held devices. With AMD's acquisition of ATI, he moved into the SOC architect role for a number of AMD's low-power designs, including the recently announced AMD Fusion Zacate and Ontario APUs.

XIII. CONCLUSION The AMD Fusion Zacate APU brings together x86 processors, NB, I/O, multi-media acceleration, and compelling graphics processing capability on a single die. Using the low-power techniques described in this document, the part provides an excellent balance of performance and long battery life.

Pankaj Bansal (M'11) received the B.Tech. (E.E.) degree from the Indian Institute of Technology, Delhi, India, in 1997. Since then, he has worked on various domains in microarchitecture, logic design, verification, and physical/circuit design/analysis at companies such as Intel, Freescale, Centillium, and Beceem. Prior to joining AMD in 2009, he was SOC lead/manager for 3G baseband ICs at Freescale. Since joining AMD in 2009, he has been responsible for multiple discrete GPU parts. Most recently, he has been leading design of the low-power AMD Fusion APUs from AMD's India Design Center.

REFERENCES
[1] R. Jotwani et al., "An x8664 core implemented in 32 nm SOI CMOS," in 2010 IEEE Int. Solid-State Circuits Conf. (ISSCC) Dig. Tech. Papers, Feb. 711, 2010, pp. 106107. [2] T. Fischer et al., "Design solutions for the bulldozer 32 nm SOI 2-core processor module in an 8-core CPU," in 2011 IEEE Int. Solid-State Circuits Conf. (ISSCC) Dig. Tech. Papers, Feb. 2011, pp. 7879. [3] J. Montanaro et al., "A 160-MHz, 32-b, 0.5-W CMOS RISC microprocessor," IEEE J. Solid-State Circuits, vol. 31, no. 11, pp. 17031714, Nov. 1996. [4] C. N. Keltcher, K. J. McGrath, A. Ahmed, and P. Conway, "The AMD Opteron processor for multiprocessor servers," IEEE Micro, vol. 23, no. 2, pp. 6676, Mar.-Apr. 2003. [5] K. Natarajan, H. Hanson, S. W. Keckler, C. R. Moore, and D. Burger, "Microprocessor pipeline energy analysis," in Proc. 2003 IEEE Int. Symp. Low Power Electronics and Design (ISLPED), Aug. 2527, 2003, pp. 282287, Session 12. [6] SPEC CPU 2006 Benchmark Suite, Standard Performance Evaluation Corp. (SPEC). [Online]. Available: http://www.spec.org/benchmarks. html, Reported results are estimated because measurements were made on pre-production hardware. [7] SiSoftware: Sandra 2011. [Online]. Available: http://www.sisoftware. net/

Don Cherepacha received B.A.Sc. and M.A.Sc. degrees from the University of Toronto, Toronto, ON, Canada, in 1991 and 1994, respectively. He is currently a Fellow at AMD in Markham, Ontario, and has more than 18 years of ASIC architecture, design, and verification experience. He joined LSI Logic in 1993 to work on PC and server chipsets as well as PCI and USB core development. He moved to Cogency Semiconductor, Inc. in 1998, where he led the ASIC architecture and development of that company's first HomePlug powerline networking ICs. He then joined ATI/AMD in 2004 and has worked in the areas of IGP/AMD Fusion architecture and performance for a range of chipsets and APUs.

Robert Wasmuth received the B.S.E.E. degree from the University of Texas at Austin in 1984. He then joined IBM working on logic synthesis, circuit design tools, and performance modeling for the POWER series of microprocessors. He did performance analysis and modeling for various designs, including a network processor, a web-based transaction system, and a VOIP switch at different start-ups in Austin. After joining AMD in 2002, he has been involved in various aspects of x86 design, most recently leading the performance and power modeling team for the Bobcat core and SOC.

FOLEY et al.: A LOW-POWER INTEGRATED x8664 AND GRAPHICS PROCESSOR FOR MOBILE COMPUTING DEVICES

231

Aswin Gunasekar received the B.S. degree in electrical engineering from the University of Madras, India, in 2001, and the M.S. degree in computer engineering from the University of Texas, Austin, in 2003. He joined AMD in 2004, and has worked in a variety of design roles that include I/Os, SRAMs, and high-performance standard cell libraries. He currently leads design methodology for a low-power family of microprocessors. He has four pending patents on processor circuits.

Ajay Naini received the M.S. degree from Mississippi State University. He is a Senior Director at AMD. He has 25 years of CPU design experience and worked at Motorola, Cyrix, and HaL Computer Systems in various capacities before joining AMD 10 years ago. He was one of the lead engineers on the K8 processor family design at AMD, which delivered the first 64-bit x86 microprocessor and the first dual-core processor. Most recently, he was the Project Director for the AMD Fusion Zacate/Ontario design. He has more than 10 patents and several publications in floating-point and microprocessor design.

Srinivasa Gutta received the B.E. (Electronics and Communication) degree from Osmania University, Hyderabad, India, and the M.S.E.E. from the University of Texas at San Antonio. He has 18 years of experience in SOC design, verification, and architecture. He started his career at CommQuest Technologies (acquired by IBM) designing voice coder DSPs for GSM baseband chips. He then joined Lucent/Agere Systems and spent 13 years working as SOC lead/architect on various communication chips, including the world's first PCI modem, K56Flex modem, and ADSL-Lite modems for client and central office. He also worked as lead engineer on second- and third-generation Sirius satellite radio chipset and mobile application processors in the Agere Mobility division. At AMD, he led the design of the Imageon audio processor and AMD Fusion Zacate/Ontario APU from AMD's India Design Center.

A Universal Service Description Language
Luke Simon, Ajay Mallya, Ajay Bansal, Gopal Gupta Department of Computer Science University of Texas at Dallas Richardson, TX 75083 Thomas D. Hite Metallect Corp 2400 Dallas Parkway Plano, TX 75093

Abstract
To fully utilize web-services, users and applications should be able to discover, deploy, compose and synthesize services automatically. This automation can take place only if a formal semantic description of the web-services is available. In this paper we present a markup language called USDL (Universal Service Description Language), for formally describing the semantics of web-services.

1

Introduction

The next milestone in the Web's evolution is making services ubiquitously available. A web service is a program available on a web-site that "effects some action or change" in the world (i.e., causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane reservation made over the Internet, a device being controlled, etc. As automation increases, these web-services will be accessed directly by the applications themselves rather than by humans. In this context, a web-service can be regarded as a "programmatic interface" that makes application to application communication possible. For web-services to become practical, an infrastructure needs to be supported that allows users to discover, deploy, compose, and synthesize services automatically. Such an infrastructure must be semantics based so that applications can reason about a service's capability to a level of detail that permits their discovery, deployment, composition and synthesis. Approaches such as WSDL are purely syntactic in nature, that is, they merely specify the format of the service. Our approach can be regarded as providing semantics to WSDL statements. We present a language called Universal Services Description Language 1

(USDL) which service developers can use to specify formal semantics of web-services. Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL can be thought of as formal program documentation that will allow sophisticated conceptual modeling and searching of available web services, automated composition, and other forms of automated service integration. For example, the WSDL syntax and USDL semantics of web services can be published in a directory which applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that need can be determined. The directory can then be searched to check if this exact service is available, and if not available, then whether it can be synthesized by composing two or more services listed in this (or another) directory. The design of USDL rests on two formal languages: Web Services Description Language (WSDL) [3] and Web Ontology Language (OWL) [2]. The Web Services Description Language (WSDL) [3], is used to give a syntactic description of the name and parameters of a service. The description is syntactic in the sense that it describes the formatting of services on a syntactic level of method signatures, but is incapable of describing what concepts are involved in a service and what a service actually does, i.e. the conceptual semantics of the service. USDL can be regarded as as a merger of WSDL and OWL that yields a language capable of describing the syntax and semantics of web services. WSDL will be used to describe message formats, types, and method prototypes, while a specialized universal OWL ontology will be used to formally describe what these messages and methods mean, on a conceptual level.

Proceedings of the IEEE International Conference on Web Services (ICWS'05)
0-7695-2409-5/05 $20.00 IEEE

2

USDL

3

Applications

As mentioned earlier, USDL can be regarded as the semantic counterpart to the syntactic WSDL description. WSDL documents contain two main constructs to which we want to ascribe conceptual meaning: messages and ports. These constructs are actually aggregates of service components which will actually be directly ascribed meaning. Messages consist of typed parts and ports consist of operations parameterized on messages. USDL defines OWL surrogates or proxies of these constructs in the form of classes, which have properties with values in the OWL WordNet ontology. Furthermore, the USDL surrogates for WSDL operations allow for the description of the side-effect of executing the operation, in addition to the semantics of the input and output parameters of the operation, which are also mapped to the WordNet. Using an OWL WordNet ontology allows for our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to which we map web service messages and operations. The semantic aliasing problem occurs when the formal semantics distinguishes two identical concepts. Other approaches such as OWL-S suffer from this problem [1, 8]. As long as this mapping into the WordNet ontology is precise and sufficiently expressive, reasoning can be done within the realm of OWL by using an automated inference systems (such as, one based on description logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological concepts, especially subsumption (hyponym) and equivalence (synonym) relationships. Finally, USDL restricts its conceptual constructors, in order to address the problems of semantic aliasing and tractability. At present, we presume only four side-effect specific operations (find, create, delete, update) which have an ontology member as their "target"; this set of basic operations may be extended as more experience is gained with USDL. In practice, most services, however, deal with manipulating databases and for such services these four operations are sufficient. As stated, one of the reasons for limiting the side-effect types is to safe-guard against the semantic aliasing problem. This is also one of the main reasons for restricting the combining forms in USDL to conjunction, disjunction, and negated atoms [8].

With a pool of USDL services at one's disposal, rapid application development (RAD) tools could be used to aid a systems integrator with the task of creating composite services, i.e., services consisting of the composition of already existing services. The service designer could use such a RAD tool by describing the desired service via a USDL document, and then the tool would query the pool of services for composable sets of services that can be used to accomplish the task as well as automatically generate boilerplate code for managing the composite service, as well as menial inter-service data format conversions and other glue. Of course these additional RAD steps would require other technologies already being researched and developed [4, 5, 7].

4

Conclusion

The current version of USDL incorporates current standards in a way to further aid markup of IT services by allowing constructs to be given meaning in terms of an OWL based WordNet ontology. Future work involves the application of USDL to formally describe commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as to service discovery and rapid application development (RAD) in commercial environments [6].

References
[1] Semantic markup for web services. http: //www.daml.org/services/owl-s/1.0/ owl-s.html. [2] Web ontology language reference. http://www.w3. org/TR/owl-ref. [3] Web services description language. http://www.w3. org/TR/wsdl. [4] P. A. Bernstein. Applying model management to classical meta data problems. In CIDR, pages 209220, 2003. [5] C. E. Gerede, R. Hull, O. H. Ibarra, and J. Su. Automated composition of e-services:lookaheads. In ICSOC, 2004. [6] T. Hite. Service composition and ranking: A strategic overview. Metallect Inc., 2005. [7] R. Hull and J. Su. Tools for design of composite web services. In SIGMOD, 2004. [8] L. Simon, A. Mallya, A. Bansal, G. Gupta, and T. Hite. A universal service description language. Technical Report UTDCS-09-05, University of Texas at Dallas, 2005.

2

Proceedings of the IEEE International Conference on Web Services (ICWS'05)

The author$20.00 has requested 0-7695-2409-5/05 IEEE enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

