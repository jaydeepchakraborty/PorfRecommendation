1

Optimal estimation with missing observations
via balanced time-symmetric stochastic models

arXiv:1503.06014v2 [math.OC] 18 Aug 2015

Tryphon T. Georgiou, Fellow, IEEE and Anders Lindquist, Life Fellow, IEEE

Abstractâ€”We consider data fusion for the purpose of smoothing and interpolation based on observation records with missing
data. Stochastic processes are generated by linear stochastic
models. The paper begins by drawing a connection between
time reversal in stochastic systems and all-pass extensions. A
particular normalization (choice of basis) between the two timedirections allows the two to share the same orthonormalized
state process and simplifies the mathematics of data fusion.
In this framework we derive symmetric and balanced MayneFraser-like formulas that apply simultaneously to smoothing and
interpolation.

I. I NTRODUCTION
Data fusion is the process of integrating different data sets,
or statistics, into a more accurate representation for a quantity
of interest. A case in point in the context of systems and
control is provided by the Mayne-Fraser two-filter formula
[1], [2] in which the estimates generated by two different
filters are merged into a combined more reliable estimate
in fixed-interval smoothing. The purpose of this paper is to
develop such a two-filter formula that is universally applicable
to smoothing and interpolation based on general records with
missing observations.
In [3], [4] the Mayne-Fraser formula was analyzed in the
context of stochastic realization theory and was shown that
it can be formulated in terms a forward and a backward
Kalman filter. In a subsequent series of papers, Pavon [5],
[6] addressed in a similar manner the hitherto challenging
problem of interpolation [7], [8], [9], [10]. This latter problem consists of reconstructing missing values of a stochastic
process over a given interval. In departure from the earlier
statistical literature, [5], [6] considered a stationary process
with rational spectral density and, therefore, reliazable as the
output of a linear stochastic system. Interpolation was then
cast as seeking an estimate of the state process based on an
incomplete observation record. A basic tool in these works
is the concept of time-reversal in stochastic systems which
has been central in stochastic realization theory (see, e.g.,
[11], [12], [13], [14], [5], [6], [15], [16], [17]). For a recent
Research supported by grants from AFOSR, NSF, VR, and the SSF.
T.T. Georgiou is with the Department of Electrical & Computer
Engineering, University of Minnesota, Minneapolis, Minnesota; email:
tryphon@umn.edu and A. Lindquist is with the Department of Automation
and the Department of Mathematics, Shanghai Jiao Tong University, Shanghai,
China, and the Center for Industrial and Applied Mathematics and ACCESS
Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden;
email: alq@kth.se

overview of smoothing and interpolation theory in the context
of stochastic realization theory see [18, Chapter 15].
In the present paper we are taking this program several
steps further. Given intermittent observations of the output
of a linear stochastic system over a finite interval, we want
to determine the linear least-squares estimate of the state
of the system in an arbitrary point in the interior of the
interval, which may either be in a subinterval of missing
data or in one where observations are available. Hence, this
combines smoothing and interpolation over general patterns of
available observations. Our main interest is in continuous-time
(possibly time-varying) systems. However, the absence of data
over subintervals, depending on the information pattern, may
necessitate a hybrid approach involving discrete-time filtering
steps.
In studying the statistics of a process over an interval, it is
natural to decompose the interface between past and future in
a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward
or backward in time. This point was fundamental in early work
in stochastic realization; see [18] and references therein. In a
different context [19] a certain duality between the two timedirections in modeling a stochastic process was introduced in
order to characterize solutions to moment problems. In this
new setting the noise-process was general (not necessarily
white), and the correspondence between the driving inputs to
the two time-opposite models was shown to be captured by
suitable dual all-pass dynamics.
Here, we begin by combining these two sets of ideas
to develop a general framework where two time-opposite
stochastic systems model a given stochastic process. We study
the relationship between these systems and the corresponding
processes. In particular, we recover as a special case certain
results of stochastic realization theory [11], [5], [6], [4] from
the 1970â€™s using a novel procedure. This theory provides a
normalized and balanced version of the forward-backward
duality which is essential for our new formulation of the
two-filter Mayne-Fraser-like formula uniformly applicable to
intervals with or without observations.
The paper is structured as follows. In Section II we
explain how a lifting of state-dynamics into an all-pass system
allows direct correspondence between sample-paths of driving

2

generating processes, in opposite time-directions, via causal
and anti-causal mappings, respectively. This is most easily
understood and explained in discrete-time and hence we begin
with that. In Section III we utilize this mechanism in the
context of general output processes and, similarly, introduce a
pair of time-opposite models. These two introductory sections,
II and III, deal with stationary models for simplicity and
are largely based on [20]. The corresponding generalizations
to time-varying systems are given in Section IV and in the
appendix, in continuous and discrete-time, respectively. In
Section V we explain Kalman filtering for problems with
missing information in the continuous-time setting. In this,
we first consider the case where increments of the output
process across intervals of no information are unavailable as
a simplified preliminary, after which we focus on the central
problem where the output process is the object of observation.
Section VI deals with the geometry of information fusion.
In Section VII we present a generalized balanced two-filter
formula that applies uniformly over intervals where data is or
is not available. We summarize the computational steps of this
approach in Section VIII. Finally, we highlight the use of the
two-filter formula with a numerical example given in Section
IX and provide concluding remarks in Section X.

this is identical to the one for discrete-time given above (as
is well known). In continuous time, stability of the system
of equations is equivalent to A having only eigenvalues with
negative real part.
In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system
is all-pass. This is done next.

A. All-pass extension in discrete-time
Consider the discrete-time Lyapunov equation
P = AP A0 + BB 0 .

Since A has all eigenvalues inside the unit disc of the complex
plane and (3) holds, (6) has as solution a matrix P which is
positive definite. The state transformation
1

Î¾ = P âˆ’ 2 x,

x(t + 1) = Ax(t) + Bw(t)
nÃ—n

(1)

nÃ—p

where t âˆˆ Z, A âˆˆ R
,B âˆˆ R
, A has all eigenvalues
in the open unit disc D = {z | |z| < 1}, and w(t), x(t) are
(centered) stationary vector-valued stochastic processes with
w(t) normalized white noise; i.e.,
E{w(t)w(s)0 } = Ip Î´ts ,

(2)

where E denotes mathematical expectation. The system of
equations is assumed to be reachable, i.e.,


rank B, AB, . . . Anâˆ’1 B = n.
(3)
In continuous-time, state-dynamics take the form of a
system of stochastic differential equations
dx(t) = Ax(t)dt + Bdw(t)

(4)

where, here, x(t) is a stationary continuous-time vector-valued
stochastic process and w(t) is a vector-valued process with
orthogonal increments with the property
E{dwdw0 } = Ip dt,

(5)

where Ip is the p Ã— p identity matrix. Reachability of the
pair (A, B) is also assumed throughout and the condition for

(7)

and
1

1

1

F = P âˆ’ 2 AP 2 , G = P âˆ’ 2 B,

(8)

Î¾(t + 1) = F Î¾(t) + Gw(t).

(9)

brings (1) into

II. S TATE DYNAMICS AND ALL - PASS EXTENSION
In this paper we consider discrete-time as well as
continuous-time stochastic linear state-dynamics. We begin by
explaining basic ideas in a stationary setting. In discrete-time
systems take the form of a set of difference equations

(6)

For this new system, the corresponding Lyapunov equation
X = F XF 0 + GG0 has In as solution, where In denotes the
(n Ã— n) identity matrix. This fact, namely, that
In = F F 0 + GG0

(10)

implies that this [F, G] can be embedded as part of an
orthogonal matrix


F G
U=
,
(11)
H J
i.e., a matrix such that U U 0 = U 0 U = In+p .
Define the transfer function
U(z) := H(zIn âˆ’ F )âˆ’1 G + J

(12)

corresponding to
Î¾(t + 1) = F Î¾(t) + Gw(t)

(13a)

wÌ„(t) = HÎ¾(t) + Jw(t).

(13b)

This is also the transfer function of
x(t + 1) = Ax(t) + Bw(t)

(14a)

0

(14b)

wÌ„(t) = BÌ„ x(t) + Jw(t),
âˆ’ 21

where BÌ„ := P H 0 , since the two systems are related by a
similarity transformation. Hence,
U(z) = BÌ„ 0 (zIn âˆ’ A)âˆ’1 B + J.

(15)

3

Now, using the identity
We claim that U(z) is a stable all-pass transfer function (with
respect to the unit disc), i.e., that U(z) is a transfer function
of a stable system and that
U(z)U(z âˆ’1 )0 = U(z âˆ’1 )0 U(z) = Ip .

(16)

The latter claim is immediate after we observe that, since
U U = In+p ,

 

Î¾(t)
Î¾(t + 1)
,
=
U0
w(t)
wÌ„(t)
0

and hence,

In âˆ’ F F 0 = (zIn âˆ’ F )(z âˆ’1 In âˆ’ F 0 )
+ (zIn âˆ’ F )F 0 + F (z âˆ’1 In âˆ’ F 0 ),
(10) and GJ 0 = âˆ’F H 0 , obtained from U U 0 = In+p , this
yields
U(z)U(z âˆ’1 )0 = HH 0 + JJ 0 = In+p ,
as claimed.

B. All-pass extension in continuous-time
Consider the continuous-time Lyapunov equation

Î¾(t) = F 0 Î¾(t + 1) + H 0 wÌ„(t)
0

0

w(t) = G Î¾(t + 1) + J wÌ„(t)

(17b)

or, equivalently,
1

x(t) = P A0 P âˆ’1 x(t + 1) + P 2 H 0 wÌ„(t)
0

w(t) = B P

âˆ’1

0

x(t + 1) + J wÌ„(t).

(18a)

(19)

dÎ¾(t) = F Î¾(t)dt + Gdw(t).

dwÌ„(t) = HÎ¾(t)dt + Jdw(t)
(20a)

w(t) = B 0 xÌ„(t) + J 0 wÌ„(t)

(20b)

U(z)âˆ— = B 0 (z âˆ’1 In âˆ’ A0 )âˆ’1 BÌ„ + J 0 .

(21)

Either of the above systems inverts the dynamical relation
w â†’ wÌ„ (in (14) or (13)).

-

(23b)

so that the transfer function
U(s) := H(sIn âˆ’ F )âˆ’1 G + J

(24)

is all-pass (with respect to the imaginary axis), i.e.,

with transfer function

w(t)

(23a)

We now seek a completion by adding an output equation

(18) can be written
xÌ„(t âˆ’ 1) = A0 xÌ„(t) + BÌ„ wÌ„(t)

(22)

Since A has all its eigenvalues in the left half of the complex
plane and since (3) holds, (22) has as solution a positive
definite matrix P . Once again, applying (7-8), the system in
(4) becomes

(18b)

Setting
xÌ„(t) := P âˆ’1 x(t + 1),

AP + P A0 + BB 0 = 0.

(17a)

U

U(s)U(âˆ’s)0 = U(âˆ’s)0 U(s) = Ip .

For this new system, the corresponding Lyapunov equation
has as solution the identity matrix and hence,
F + F 0 + GG0 = 0.

wÌ„(t)
-

Fig. 1: Realization (14) in the forward time-direction.

(25)

(26)

Utilizing this relationship we note that
(sIn âˆ’ F )âˆ’1 GG0 (âˆ’sIn âˆ’ F 0 )âˆ’1
= (sIn âˆ’ F )âˆ’1 (sIn âˆ’ F âˆ’ sIn âˆ’ F 0 )(âˆ’sIn âˆ’ F 0 )âˆ’1

w(t)


Uâˆ—

wÌ„(t)


Fig. 2: Realization (20) in the backward time-direction.
An algebraic proof of (16) is also quite immediate. In fact,
U(z)U(z âˆ’1 )0


0
= H(zIn âˆ’ F )âˆ’1 G + J H(z âˆ’1 In âˆ’ F )âˆ’1 G + J
=H(zIn âˆ’ F )âˆ’1 GG0 (z âˆ’1 In âˆ’ F 0 )âˆ’1 H 0 + JJ 0
+ H(zIn âˆ’ F )âˆ’1 GJ 0 + JG0 (z âˆ’1 In âˆ’ F 0 )âˆ’1 H

= (sIn âˆ’ F )âˆ’1 + (âˆ’sIn âˆ’ F 0 )âˆ’1 ,
and we calculate that
U(s)U(âˆ’s)0
= (H(sIn âˆ’ F )âˆ’1 G + J)(G0 (âˆ’sIn âˆ’ F 0 )âˆ’1 H 0 + J 0 )
= JJ 0 + H(sIn âˆ’ F )âˆ’1 (GJ 0 + H 0 )
(JG0 + H)(âˆ’sIn âˆ’ F 0 )âˆ’1 H 0 .
For the product to equal the identity,
JJ 0 = Ip
H = âˆ’JG0 .

4

Thus, we may take

A. Time-reversal of discrete-time stochastic systems
Consider a stochastic linear system

J = Ip
H = âˆ’G0 ,

x(t + 1) = Ax(t) + Bw(t)

(35a)

y(t) = Cx(t) + Dw(t)

(35b)

and the forward dynamics
dÎ¾(t) = F Î¾(t)dt + Gdw(t)

(27a)

dwÌ„(t) = âˆ’G0 Î¾(t)dt + dw(t).

(27b)

Substituting F = âˆ’F 0 âˆ’ GG0 from (26) into (27a) we obtain
the reverse-time dynamics
dÎ¾(t) = âˆ’F 0 Î¾(t)dt + GdwÌ„(t)
0

dw(t) = G Î¾(t)dt + dwÌ„(t).

(28a)
(28b)

Now defining
xÌ„(t) := P âˆ’1 x(t)

(29)

and using (7) and (8), (28) becomes
dxÌ„(t) = âˆ’A0 xÌ„(t)dt + BÌ„dwÌ„(t)
0

dw(t) = B xÌ„(t)dt + dwÌ„(t),

(30a)
(30b)

with transfer function

with an m-dimensional output process y, and x, u, A, B are
defined as in Section II-A. All processes are stationary and
the system can be thought as evolving forward in time from
the remote past (t = âˆ’âˆž).
To formalize this, we introduce some notation. Let H be
the Hilbert space spanned by {wk (t); t âˆˆ Z, k = 1, 2, . . . , n},
endowed with the inner product hÎ», Âµi = E{Î»Âµ}, and let
+
Hâˆ’
t (w) and Ht (w) be the (closed) subspaces spanned by
{wk (s); s â‰¤ t âˆ’ 1, k = 1, . . . , m} and {wk (s); s â‰¥ t, k =
+
1, . . . , m}, respectively. Define Hâˆ’
t (y) and Ht (y) accordingly in terms of the output process process y. Then the
stochastic system (35) evolves forward in time in the sense
that
âˆ’
+
Hâˆ’
(36)
t (z) âŠ‚ Ht (w) âŠ¥ Ht (w),
where A âŠ¥ B means that elements of the subspaces A and
B are mutually orthogonal, and where Hâˆ’
t (z) is formed as
above in terms of


x(t + 1)
z(t) =
;
y(t)

U(s)âˆ— = Ip + B 0 (sIn + A0 )âˆ’1 BÌ„,

(31)

BÌ„ := P âˆ’1 B.

(32)

xÌ„(t âˆ’ 1) = A0 xÌ„(t) + BÌ„ wÌ„(t)

(37a)

Furthermore, the forward dynamics (27) can be expressed in
the form

y(t) = CÌ„ xÌ„(t) + DÌ„wÌ„(t),

(37b)

see [18, Chapter 6] for more details.

where

Next we construct a stochastic system

dx(t) = Ax(t)dt + Bdw(t)
0

dwÌ„(t) = BÌ„ x(t)dt + dw(t)

(33a)
(33b)

with transfer function

which evolves backward in time from the remote future
(t = âˆž) in the sense that the processes xÌ„, x, wÌ„, w relate as in
the previous section. More specifically, as shown in Section
II-A, Hâˆ’ (wÌ„) âŠ‚ Hâˆ’ (w) and H+ (w) âŠ‚ H+ (wÌ„) for all t, as
examplified in Figures 1 and 2.
In fact, the all-pass extension (14) of (35a) yields

U(s) = Ip âˆ’ BÌ„ 0 (sIn âˆ’ A)âˆ’1 B.

(34)

wÌ„(t) = BÌ„ 0 x(t) + Jw(t)

(38)

It follows from (20b) that (38) can be inverted to yield
III. T IME - REVERSAL OF STATIONARY LINEAR
STOCHASTIC SYSTEMS

The development so far allows us to draw a connection
between two linear stochastic systems having the same output
and driven by a pair of arbitrary, but dual, stationary processes
w(t) and wÌ„(t), one evolving forward in time and one evolving
backward in time. When one of these two processes is white
noise (or, orthogonal increment process, in continuous-time),
then so is the other. For this special case we recover results
of [11] and [5], [6] in stochastic realization theory.

w(t) = B 0 xÌ„(t) + J 0 wÌ„(t),

(39)

where xÌ„(t) = P âˆ’1 x(t + 1), and that we have the reverse-time
recursion
xÌ„(t âˆ’ 1) = A0 xÌ„(t) + BÌ„ wÌ„(t).
(40a)
Then inserting (39) and
x(t) = P xÌ„(t âˆ’ 1) = P A0 xÌ„(t) + P BÌ„ wÌ„(t)
into (35b), we obtain
y(t) = CÌ„ xÌ„(t) + DÌ„wÌ„(t),

(40b)

5

where DÌ„ := CP BÌ„ + DJ 0 and
CÌ„ := CP A0 + DB 0 .

(41)

Then, (40) is precisely what we wanted to establish.
The white noise w is normalized in the sense of (2). Since
U, given by (15), is all-pass, wÌ„ is also a normalized white
noise process, i.e.,
E{wÌ„(t)wÌ„(s)0 } = Ip Î´tâˆ’s .
From the reverse-time recursion (37a)
xÌ„(t) =

âˆž
X

+
same inner product as above, and let Hâˆ’
t (du) and Ht (du)
be the (closed) subspaces spanned by the increments of the
components of U on (âˆ’âˆž, t] and [t, âˆž), respectively. Define
+
Hâˆ’
t (dy) and Ht (dy) accordingly in terms of the output
process y. All processes have stationary increments and the
stochastic system (45) evolves forward in time in the sense
that
âˆ’
+
Hâˆ’
(46)
t (dz) âŠ‚ Ht (dw) âŠ¥ Ht (dw),

where Hâˆ’
t (dz) is formed in terms of


x(t)
.
z(t) =
y(t)

(47)

The all-pass extension of Section II-B yields

(A0 )kâˆ’(t+1) BÌ„ wÌ„(k).

dwÌ„ = dw âˆ’ BÌ„ 0 xdt

k=t+1
0

Since, wÌ„ is a white noise process, E{xÌ„(t)wÌ„(s) } = 0 for all
s â‰¤ t. Consequently, (37) is a backward stochastic realization
in the sense defined above.

as well as the reverse-time relation
dxÌ„ = âˆ’A0 xÌ„dt + BÌ„dwÌ„
0

dw = B xÌ„dt + dwÌ„,

Moreover, the transfer functions
W(z) = C(zIn âˆ’ A)âˆ’1 B + D

(42)

(48)

(49a)
(49b)

where xÌ„(t) = P âˆ’1 x(t). Inserting (49b) into
dy = CP xÌ„dt + Ddw

of (35) and
WÌ„(z) = CÌ„(z âˆ’1 In âˆ’ A0 )âˆ’1 BÌ„ + DÌ„

(43)

of (37) satisfy

yields
dy = CÌ„ xÌ„dt + DdwÌ„,
where

W(z) = WÌ„(z)U(z).

In the context of stochastic realization theory, U(z) is called
structural function ([13], [14]).

w(t)
-

W

-

y(t)

WÌ„

(50)

Thus, the reverse-time system is

y(t)

dxÌ„ = âˆ’A0 xÌ„dt + BÌ„dwÌ„

(51a)

dy = CÌ„ xÌ„dt + DdwÌ„.

(51b)

From this, we deduce that the system (45) has the backward
property
+
âˆ’
H+
(52)
t (dzÌ„) âŠ‚ Ht (dwÌ„) âŠ¥ Ht (dwÌ„),

Fig. 3: The forward stochastic system (35).



CÌ„ = CP + DB 0 .

(44)

where H+
t (dzÌ„) is formed as above in terms of


xÌ„(t)
zÌ„(t) =
.
y(t)

wÌ„(t)


We also note that the transfer function
Fig. 4: The backward stochastic system (37)

W(s) = C(sIn âˆ’ A)âˆ’1 B + D
of (45) and the transfer function

B. Time-reversal of continuous-time stochastic systems

WÌ„(s) = CÌ„(sIn + A0 )âˆ’1 BÌ„ + D

We now turn to the continuous-time case. Let

of (51) also satisfy

dx = Axdt + Bdw

(45a)

dy = Cxdt + Ddw

(45b)

be a stochastic system with x, w, A, B as in Section II-B,
evolving forward in time from the remote past (t = âˆ’âˆž).
Now let H be the Hilbert space spanned by the increments
of the components of w on the real line R, endowed with the

W(s) = WÌ„(s)U(s)
as in discrete-time.
Note that the orthogonal-increment process w is normalized in the sense of (5). Since U(s) is all-pass,
dwÌ„ = du âˆ’ BÌ„ 0 xdt

(53)

6

also defines a stationary orthogonal-increment process wÌ„ such
that
{dwÌ„(t)dwÌ„(t)0 } = Ip dt.
It remains to show that (51) is a backward stochastic realization, that is, at each time t the past increments of wÌ„ are
orthogonal to xÌ„(t). But this follows from the fact that
Z âˆž
0
eâˆ’A (tâˆ’s) BÌ„dwÌ„(s)
xÌ„(t) =

1

1

1

P (t)âˆ’ 2 PÌ‡ P (t)âˆ’ 2 = âˆ’R(t) âˆ’ R(t)0 ,
and hence the (55) yields
F (t) + F (t)0 + G(t)G(t)0 = 0.

(61)

Using (61) to eliminate F in (57), we obtain

t

and wÌ„ has orthogonal increments.

1

Differentiating P (t)âˆ’ 2 P (t)P (t)âˆ’ 2 = In , we obtain

dÎ¾ = âˆ’F (t)0 Î¾(t)dt + G(t)dwÌ„,

(62)

dwÌ„ = dw âˆ’ G(t)0 Î¾(t)dt,

(63)

where

IV. T IME REVERSAL OF NON - STATIONARY STOCHASTIC

which can also be written

SYSTEMS

dwÌ„ = dw âˆ’ BÌ„(t)0 x(t)dt,

In a similar manner non-stationary stochastic systems
admit unitary extensions which in turn allows us to construct
dual time-reversed stochastic models that share the same state
process. The case of discrete-time dynamics is documented
in the appendix, whereas the continuous-time counterpart is
explained next as prelude to smoothing and interpolation that
will follow.

A. Unitary extension
The covariance matrix function P (t) := E{x(t)x(t)0 } of
the time-varying state representation
dx = A(t)x(t)dt + B(t)dw,

x(0) = x0

(54)

with x0 a zero-mean stochastic vector with covariance matrix
P0 = E{x0 x00 }, satisfies the matrix-valued differential equation
PÌ‡ (t) = A(t)P (t) + P (t)A(t)0 + B(t)B(t)0
(55)
with P (0) = P0 . Throughout we assume total reachability [18,
Section 15.2], and therefore P (t) > 0 for all t > 0.
A unitary extension of (54) is somewhat more complicated
than in the discrete time case. In fact, differentiating
1

Î¾(t) = P (t)âˆ’ 2 x(t)

(64)

where BÌ„(t) := P (t)âˆ’1 B(t).
Proposition 1: A process wÌ„ satisfying (63) has orthogonal
increments with the normalized property (5). Moreover,
E{[wÌ„(t) âˆ’ wÌ„(s)]Î¾(t)0 } = 0

(65)

for all s â‰¤ t.
Proof: As is well-known, the solution of (57) can be
written in the form
Z t
Î¾(t) = Î¦(t, s)Î¾(s) +
Î¦(t, Ï„ )G(Ï„ )dw,
(66)
s

where Î¦(t, s) is the transition matrix with the property
âˆ‚Î¦
(t, s) = F (t)Î¦(t, s), Î¦(s, s) = In
(67a)
âˆ‚t
âˆ‚Î¦
(t, s) = âˆ’Î¦(t, s)F (s), Î¦(t, t) = In
(67b)
âˆ‚s
Let s â‰¤ t. Then, in view of (63), a straight-forward calculation
yields
wÌ„(t) âˆ’ wÌ„(s) = w(t) âˆ’ w(s)
Z
âˆ’ M (t, s)Î¾(s) âˆ’

t

M (t, Ï„ )G(Ï„ )dw,

(68)

s

where
Z

(56)

M (t, s) =

t

G(Ï„ )0 Î¦(Ï„, s)dÏ„.

(69)

s

we obtain

Therefore,
dÎ¾ = F (t)Î¾(t)dt + G(t)dw,

(57)
E{[wÌ„(t) âˆ’ wÌ„(s)][wÌ„(t) âˆ’ wÌ„(s))0 } = Ip (t âˆ’ s) + âˆ†(t, s),

where
1

1

F (t) = P (t)âˆ’ 2 A(t)P (t) 2 + R(t),
G(t) = P (t)
with

âˆ’ 12

where
(58a)

B(t)

(58b)


1
d
âˆ’ 12
R(t) =
P (t)
P (t) 2 .
dt

(59)



1

Z

t

âˆ†(t, s) = M (t, s)M (t, s) +
M (t, Ï„ )G(Ï„ )G(Ï„ )0 M (t, Ï„ )0 dÏ„
s
Z t
âˆ’
[M (t, Ï„ )G(Ï„ ) + G(Ï„ )0 M (t, Ï„ )0 ] dÏ„.
s

However, âˆ†(t, s) is identically zero. To see this, first note that

In fact,
dÎ¾ = P (t)âˆ’ 2 dx + R(t)Î¾(t)dt.

0

(60)

âˆ‚M
(t, s) = âˆ’M (t, s)F (s) âˆ’ G(s)0 .
âˆ‚s

(70)

7

Then, in view of (61), a simple calculation shows that
âˆ‚âˆ†
(t, s) â‰¡ 0.
âˆ‚s
Since âˆ†(t, t) = 0, the assertion follows. Hence the incremental
covariance is normalized.

B. Time reversal in continuous-time systems

dx = A(t)x(t)dt + B(t)dw,

x(0) = x0

(75a)

Next, we show that wÌ„(t) has orthogonal increments. To
this end, choose arbitrary times s â‰¤ t â‰¤ a â‰¤ b on the interval
[0, T ], where we choose a and b fixed, and show that

dy = C(t)x(t)dt + D(t)dw,

y(0) = 0

(75b)

Q(t, s) := E{[wÌ„(b) âˆ’ wÌ„(a)][wÌ„(t) âˆ’ wÌ„(s))0 }

Next we derive the backward stochastic system corresponding to the non-stationary forward stochastic system

defined on the finite interval [0, T ], where x0 (with covariance
P0 ) and the normalized Wiener process w are uncorrelated.
To this end, apply the transformation
xÌ„(t) = P (t)âˆ’1 x(t)

is identically zero for all s â‰¤ t. Using (68) and
wÌ„(b) âˆ’ wÌ„(a) = w(b) âˆ’ w(s) âˆ’ M (b, a)Î¦(a, s)Î¾(s)
Z b
Z b
âˆ’ M (b, a)
Î¦(a, Ï„ )G(Ï„ )dw âˆ’
M (b, Ï„ )dw
s

a

computed analogously, we obtain
"

Z

(77)

b

Î¦(a, Ï„ )G(Ï„ )dÏ„
#

b
0

Î¦(a, Ï„ )G(Ï„ )G(Ï„ ) M (t, Ï„ )dÏ„ .
s

Then, again using (61), we see that
âˆ‚M
(t, s) â‰¡ 0,
âˆ‚s
so, since Q(t, t) = 0, we see that Q(t, s) is identically zero,
establishing that wÌ„(t) has orthogonal increments.
Finally, we use the same trick to show (65). In fact, for
s â‰¤ t, (66) and (68) yield
E{[wÌ„(t) âˆ’ wÌ„(s))Î¾(t)0 } = âˆ’M (t, s)Î¦(t, s)0
Z t
Z t
+
G(Ï„ )0 Î¦(t, Ï„ )0 dÏ„ âˆ’
M (t, Ï„ )G(Ï„ )G(Ï„ )0 )Î¦(t, Ï„ )0 dÏ„,
s

dy = CÌ„(t)xÌ„(t) + D(t)dwÌ„,
where

s

+

together with (74b) to (75b) to obtain

CÌ„(t) = C(t)P (t) + D(t)B(t).

Q(t, s) = M (b, a) Î¦(a, s)M (t, s)0 âˆ’
Z

(76)

This together with (74a) yields the the backward system
corresponding to (75), namely
dxÌ„ = âˆ’A(t)0 xÌ„(t)dt + BÌ„(t)dwÌ„

(78a)

dy = CÌ„(t)xÌ„(t)dt + D(t)dwÌ„.

(78b)

with end-point condition xÌ„(T ) = P (T )
the Wiener process wÌ„.

âˆ’1

x(T ) uncorelated to

The backward realization (78) was derived in [3], but in
cumbersome way, requiring the proof that wÌ„(t) is a normalized
process with orthogonal increments to be suppressed. What
is new here is imposing the unitary map between w and wÌ„,
making the analysis much simpler and more natural.
V. K ALMAN FILTERING WITH MISSING OBSERVATIONS

s

the partial derivative of which with respect to s is identical
zero; this is seen by again using (61). Therefore, since (65) is
zero for s = t, it is identical zero for all s â‰¤ t, as claimed.
This concludes the proof of Proposition 1.
Consequently, (57) and (64) form a forward unitary system
dx = A(t)x(t)dt + B(t)dw

(71a)

dwÌ„ = dw âˆ’ BÌ„(t)0 x(t)dt,

(71b)

The corresponding backward unitary system is obtained
through the transformation
1

xÌ„(t) = P (t) 2 Î¾(t),

(72)

which yields
1

dxÌ„ = P (t)âˆ’ 2 dÎ¾ + R(t)Î¾(t)dt.

(73)

This together with (62) and (63) yields
dxÌ„ = âˆ’A(t)0 xÌ„(t)dt + BÌ„(t)dwÌ„
0

dw = B(t) xÌ„(t)dt + dwÌ„,

(74a)
(74b)

We consider the linear stochastic system (75) which does
not have a purely deterministic component that enables exact
estimation of components of x from y, an assumption that we
retain in the rest of the paper. In the engineering literature is
often the case that the stochastic system (75) represented as
xÌ‡(t) = A(t)x(t) + B(t)wÌ‡(t),
yÌ‡(t) = C(t)x(t) + D(t)wÌ‡(t)

x(0) = x0

(79a)
(79b)

where the formal â€œderivativeâ€ wÌ‡ is white noise, i.e.,
E{wÌ‡(t)wÌ‡(s)0 } = IÎ´(t âˆ’ s) with Î´(t âˆ’ s) being the Dirac
â€œfunctionâ€. Of course xÌ‡, yÌ‡ and wÌ‡ are to be interpreted
as generalized stochastic processes. From a mathematically
rigorous point of view, observing yÌ‡ makes little sense since,
for any fixed t, yÌ‡(t) has infinite variance and contains no
information about the state process x. However, observations
of yÌ‡ could be interpreted as observations of the increments dy
of y in a precise meaning to be defined next. On the other
hand, one can think of (75) as a system of type


x(t)
dz = M (t)z(t)dt + N (t)dw(t), where z(t) =
,
y(t)

8

and one would like to determine the optimal linear leastsquares estimate of x(t) given past observed values of y.

with R(t) = D(t)D(t)0 and initial conditions xâˆ’ (0) = 0 and
Q(0) = P0 . Here Qâˆ’ (t) is the error covariance

Generally this distinction between observing y or dy is
not important. However, when there is loss of information
over an interval (t1 , t2 ), there are two different information
patterns depending on whether dy or y is observed. The
difference consists in whether âˆ†y := y(t2 ) âˆ’ y(t1 ) is part
of the observation record or not. These two cases will be
dealt with separately in subsections below. In fact, the former,
which is common in engineering applications, is provided as
a simplified preliminary, whereas our main interest is in the
latter. To this end, we first introduce some notation.

Qâˆ’ (t) := E{[x(t) âˆ’ xâˆ’ (t)](x(t) âˆ’ xâˆ’ (t)]0 },

Consider the stochastic system (75) on a finite interval
[0, T ]. As before, let H be the Hilbert space spanned by
{wk (t) âˆ’ wk (s); s, t âˆˆ [0, T ], k = 1, 2, . . . , m}, endowed
with the inner product hÎ», Âµi = E{Î»Âµ}. For any Î» âˆˆ H
and any subspace A, let EA denote the orthogonal projection
of Î» onto A. We denote by H[t1 ,t2 ] (dy) the (closed) subspace generated by the components of the increments of the
observation process y over the window [t1 , t2 ]. In particular,
we shall also use the notations Hâˆ’
t (dy) := H[0,t] (dy) and
H+
t (dy) := H[t,T ] (dy).
Suppose that the output process or its increments are
available for observation only on some subintervals of [0, T ],
â—¦
namely Ik , k = 1, 2, . . . , Î½. Next we want to define H as the
proper subspace of H[0,T ] (dy) spanned by the observed data.
In the case that only the increments dy or, equivalently, the
â€œderivativeâ€ yÌ‡ is observed, we simply define
â—¦

H := HI1 (dy) âˆ¨ HI2 (dy) âˆ¨ Â· Â· Â· âˆ¨ HIÎ½ (dy),
In the case that the process y is observed, we need to expand
â—¦
H by adding the subspaces spanned by the increments âˆ†y over
the complementary intervals without observation. In either
case, we define
â—¦

â—¦

âˆ’
Hâˆ’
t := H âˆ© Ht (dy) and

â—¦

â—¦

+
H+
t := H âˆ© Ht (dy).

(80)

Then Kalman filtering with missing observations amounts to
determining a recursion for xâˆ’ where
â—¦

âˆ’

a0 xâˆ’ (t) = EHt a0 x(t),

for all a âˆˆ Rn .

(81)

A. Observing dy only

dxâˆ’ = A(t)xâˆ’ (t)dt + Kâˆ’ (t)(dy(t) âˆ’ C(t)xâˆ’ (t)dt)
(82a)
0
QÌ‡âˆ’ (t) = AQâˆ’ + Qâˆ’ A0 âˆ’ Kâˆ’ RKâˆ’
+ BB 0

which, by the nondeterministic assumption, is positive definite
for all t.
Next suppose the observation process becomes unavailable
over the interval [t1 , t2 ) âŠ‚ [0, T ]. Then the Kalman filter needs
to be modified accordingly. In fact, for any t âˆˆ [t1 , t2 ), (81)
â—¦

âˆ’
holds with the space of observations Hâˆ’
t := Ht1 (dy), and
consequently
âˆ’

a0 xâˆ’ (t) = EHt1 (dy) a0 x(t) = a0 Î¦(t, t1 )xâˆ’ (t1 ).
This corresponds to setting Kâˆ’ (t) = 0 in (82) on the interval
[t1 , t) so that
dxâˆ’ = A(t)xâˆ’ (t)dt
(84a)
with initial condition xâˆ’ (t1 ) given by (82a). The error covariance Qâˆ’ is then given by the Lyapunov equation
QÌ‡âˆ’ (t) = AQâˆ’ + Qâˆ’ A0 + BB 0

(84b)

with initial the condition Qâˆ’ (t1 ) given by the value produced
in the previous interval.
Then suppose observations of dy become available again
on the interval [t2 , t3 ). Then, for any t âˆˆ [t2 , t3 ), we have
â—¦

H+
t = H[0,t1 ] âˆ¨ H[t2 ,t] ,
so the Kalman estimate is generated by (82) but now with
initial conditions xâˆ’ (t2 ) and Qâˆ’ (t2 ) being those computed
in the previous step without observation. In the case there are
more intervals, one proceeds similarly by alternating between
filters (82) and (84) depending on whether increments dy are
available or not.
In an identical manner, a cascade of backward Kalman
filters generates a process xÌ„+ (t) based on the backward
stochastic realization (78) and the observation windows [t, T ].
Assuming that there are observations in a final interval ending
at t = T , on that interval the Kalman estimate
â—¦

+

a0 xÌ„+ (t) = EHt a0 xÌ„(t),

(85)

â—¦

with initial observation space H+
t := H[t,T ] , is generated by
the backward Kalman filter
dxÌ„+ = âˆ’A(t)0 xÌ„+ (t)dt

When observations are available on the interval [0, t1 ], the
Kalman filter on that interval is given by

Kâˆ’ = (Qâˆ’ C 0 + BD0 )Râˆ’1

(83)

+ KÌ„+ (t)(dy(t) âˆ’ CÌ„(t)xÌ„+ (t)dt)
0

0

âˆ’1

KÌ„+ = âˆ’(QÌ„+ CÌ„ âˆ’ BÌ„D )R
QÌ„Ë™ + = âˆ’A0 QÌ„+ âˆ’ QÌ„+ A + KÌ„+ R(t)KÌ„+ (t)0 âˆ’ BÌ„ BÌ„ 0

(86a)
(86b)
(86c)

(82b)

and initial conditions xÌ„+ (T ) = 0 and QÌ„+ (T ) = PÌ„ (T ) for xÌ„+
and the error covariance

(82c)

QÌ„+ (t) := E{[xÌ„(t) âˆ’ xÌ„+ (t)][xÌ„(t) âˆ’ xÌ„+ (t)]0 },

(87)

9

which like Qâˆ’ (t) is positive definite for all t. During periods
of no observations of dy, we then set the gain KÌ„+ = 0. This
update is obtained from the backward time stochastic model
(74) in an identical manner to that of (84).
Consequently, both the underlying process as well as the
filter can run in either time-direction. This duality becomes
essential in subsequent sections where we will be concerned
with smoothing and interpolation.
B. Observing y
Now consider the case that y, and note merely dy, is
available for observation on all intervals Ik , k = 1, 2, . . . , Î½.
Under this scenario and with a continuous-time process the
dynamics of Kalman filtering become hybrid, requiring both
continuous-time filtering when data is available as well as a
discrete-time update across intervals where measurements are
not available.
Then on the first interval [0, t1 ] the Kalman estimate (82)
will still be valid. However, when t reaches the endpoint t2
of the interval of no information and an observation of y is
obtained again, the subspace of observed data becomes
â—¦

where âˆ†y := y(t2 ) âˆ’ y(t1 ). Computing x(t2 ) across the window (t1 , t2 ] as a function of x(t1 ) and the noise components
we have that
Z t2
x(t2 ) = Î¦(t2 , t1 ) x(t1 ) +
Î¦(t2 , s)Bdw(s)
| {z }
t1
|
{z
}
Ad
u1 (t1 )

while
t2

y(t2 ) = y(t1 ) +

Z

t2

C(t)x(t)dt +
t1

D(t)dw(t).
t1

Therefore,
Z

t2

âˆ†y =
t1

|

C(t)Î¦(t, t1 )dt) x(t1 ) + u2 (t1 )
{z
}
Cd

where
Z

t2

u2 (t1 ) =

Z

t1

C(t)

Î¦(t, s)B(s)dw(s)dt
Z t2
+
D(s)dw(s)
t1

Z t2 Z t2
=
C(t)Î¦(t, s)dtB(s) + D(s) dw(s).
t1
| t
{z
}
t1


u(t1 ) =

  
Bd
u1 (t1 )
=
v(t1 )
u2 (t1 )
Dd

and Bd and Dd are chosen so that


Bd
Dd




Bd0 , Dd0 =

Z

t2

t1

Î¦(t2 , s)BB 0 Î¦(t, s) Î¦(t2 , s)BM (s)0
M (s)B 0 Î¦(t2 , s)0
M (s)M (s)0


ds

while E{v(t1 )v(t1 )0 } = I.
Hence, across the window of missing data the Kalman state
estimate xâˆ’ is now generated by a discrete-time Kalman-filter
step
xâˆ’ (t2 ) = Ad xâˆ’ (t1 ) + Kd (âˆ†y âˆ’ Cd xâˆ’ (t1 ))
Kd =

(Ad Q(t1 )Cd0

+

(89a)

Bd Dd0 )

Ã— (Cd Q(t1 )Cd0 + Dd Dd0 )âˆ’1

(89b)

with initial conditions xâˆ’ (t1 ) and Q(t1 ) given by (82) and
the error covariance at t2 by
Q(t2 ) = Ad Q(t1 )A0d âˆ’ Kd (Cd Q(t1 )Cd0
+ Dd Dd0 )Kd0 + Bd Bd0 .

(89c)

In the next interval [t2 , t3 ], where observations of y are
available, the new Kalman estimate (81) with

Htâˆ’2 = Hâˆ’
t1 âˆ¨ H(âˆ†y),

Z

where

t

M (s)

Thus, we obtain the discrete-time update
x(t2 ) = Ad x(t1 ) + Bd v(t1 )

(88a)

âˆ†y = Cd x(t1 ) + Dd v(t1 )

(88b)

â—¦

H+
t = H[0,t1 ] âˆ¨ H(âˆ†y) âˆ¨ H[t2 ,t]
is again generated by the continuous-time Kalman filter (82)
starting from xâˆ’ (t2 ) and Q(t2 ) given by (89).
Again given an observation pattern, where intermittently
y becomes unavailable for observation, the Kalman estimate
(81) can be generated in precisely this manner by a cascade
of continuous and discrete-time Kalman filters.
Remark 2: The observation pattern of a continuous-time
stochastic model, where y becomes unavailable over particular
time-windows, is closely related to hybrid stochastic models
where continuous-time diffusion is punctuated by discrete-time
transitions. Indeed, unless interpolation of the statistics within
windows of unavailable data is the goal, the end points of such
intervals can be identified and the same hybrid model utilized
to capture the dynamics.
Remark 3: A common engineering scenario is the case
where the signal is lost while the observation noise is still
present. This amounts to having C â‰¡ 0 over the corresponding
window, and the Kalman estimates are obtained by merely
running the filters (82) and (86) in the two time directions with
the modified condition on C. This situation does not cover
the information patterns discussed above since, whenever
BD0 6= 0, the Kalman gains do not vanish and information
about the state process is available even when C is zero.

10

C. Smoothing
Given these intermittent forward and backward Kalman
estimates, we shall derive a formula for the smoothing estimate
â—¦

a0 xÌ‚(t) := EH a0 x(t),

a âˆˆ Rn ,

EX+ (t) a0 xâˆ’ (t) = E{a0 xâˆ’ (t)xÌ„+ (t)}PÌ„+ (t)âˆ’1 xÌ„+ (t)
(90)

valid for both the cases discussed above, where
â—¦

H :=

â—¦

âˆ’
Ht

âˆ¨

â—¦

+
Ht

âŠ‚ H[0,T ] (dy)

(91)

is the complete subspace of observations. This is discussed
next.

Consider the system (75), and let X(t) be the (finitedimensional) subspace in H spanned by the components of the
stochastic state vector x(t). Then it can be shown [18, Chapter
7] that H[0,t] (dy) âŠ¥ H[t,T ] (dy) | Xt , where A âŠ¥ B | X
denotes the conditional orthogonality
X

hÎ± âˆ’ E Î±, Î² âˆ’ E Î²i = 0 for all Î± âˆˆ A, Î² âˆˆ B.

(92)

Next, let Xâˆ’ (t) and X+ (t) be the subspaces spanned by the
components of the (intermittent) Kalman estimates xâˆ’ (t) and
â—¦
xÌ„+ (t), respectively. Then since Xâˆ’ (t) âŠ‚ Hâˆ’
t âŠ‚ H[0,t] (dy)
and X+ (t) âŠ‚

â—¦

+
Ht

âŠ‚ H[t,T ] (dy), we have
Xâˆ’ (t) âŠ¥ X+ (t) | X(t),

which is equivalent to
EX+ (t) a0 xâˆ’ (t) = EX+ (t) EX(t) a0 xâˆ’ (t),

a âˆˆ Rn

(93a)

EX+ |Xâˆ’

âˆ’â†’

EX |Xâˆ’&

where x+ (t) := PÌ„+ (t)âˆ’1 xÌ„+ (t) is the dual basis in X+ (t) such
that E{x+ (t)xÌ„+ (t)0 } = I. Moreover,
EX(t) a0 xâˆ’ (t) = E{a0 xâˆ’ (t)x(t)0 }P (t)âˆ’1 x(t)
= a0 E{xâˆ’ (t)x(t)0 }xÌ„(t) = a0 Pâˆ’ (t)xÌ„(t),

= b0 E{xÌ„(t)xÌ„+ (t)}x+ (t)
= b0 PÌ„+ (t)x+ (t),
by condition (ii), and consequently
EX+ (t) E X(t) a0 xâˆ’ (t) = a0 Pâˆ’ (t)PÌ„+ (t)x+ (t).

Remark 5: The proof of condition (iii) in Lemma 4 could
be simplified if xÌ„+ were a regular backward Kalman estimate
without intermittent loss of information. In this case, x+ =
PÌ„+âˆ’1 xÌ„+ would be generated by a forward stochastic realization
belonging to the same class as (75) and E{xÌ„+ (t)xâˆ’ (t)0 } =
PÌ„+ (t) E{x+ (t)xâˆ’ (t)} = PÌ„+ (t) E{xâˆ’ (t)xâˆ’ (t)}.



a âˆˆ Rn ,

(96)

(93b)

X

where H
t is the subspace
H
t = Xâˆ’ (t) âˆ¨ X+ (t).

commutes, where the argument t has been suppressed.
Lemma 4: Let x(t), xÌ„(t), xâˆ’ (t) and xÌ„+ (t) be defined as
above. Then, for each t âˆˆ [0, T ],
(i) E{x(t)xâˆ’ (t)0 } = Pâˆ’ (t)
(ii) E{xÌ„(t)xÌ„+ (t)0 } = PÌ„+ (t)
(iii) E{xÌ„+ (t)xâˆ’ (t)0 } = PÌ„+ (t)Pâˆ’ (t),
where Pâˆ’ (t) := E{xâˆ’ (t)xâˆ’ (t)0 } is the state covariance of the
Kalman estimate xâˆ’ (t) and P+ (t) := E{xÌ„+ (t)xÌ„+ (t)0 } is the
covariance of the backward Kalman estimate xÌ„+ (t).
Proof: By the definition of the Kalman filter, (81) holds,
and consequently the components of the estimation error
x(t) âˆ’ xâˆ’ (t) are orthogonal to Hâˆ’
t and hence to the components of xâˆ’ (t). Therefore,
E{x(t)xâˆ’ (t)0 } = E{xâˆ’ (t)xâˆ’ (t)0 } = Pâˆ’ (t),

(95)

Then condition (iii) follows from (93a), (94) and (95).

a0 xÌ‚(t) = E Ht a0 x(t),

X+
%EX+ |X

EX+ (t) b0 xÌ„(t) = E{b0 xÌ„(t)xÌ„+ (t)}PÌ„+ (t)âˆ’1 xÌ„+ (t)

Lemma 6: For each t âˆˆ [0, T ], the smoothing estimate
xÌ‚(t), defined by (90), is given by

[18, Proposition 2.4.2]. Therefore the diagram
Xâˆ’

(94)

= a0 E{xâˆ’ (t)xÌ„+ (t)0 }x+ (t),

where we have used condition (i) and (76). Next, set b := Pâˆ’ a
and form

VI. G EOMETRY OF FUSION

X

proving condition (i). Condition (ii) follows from a symmetric
argument. To prove (iii) we use condition (93). To this end,
first note that, by the usual projection formula,

(97)
â—¦

Proof: Following [14], [3], [18], define Nâˆ’ (t) := Hâˆ’
t 	
â—¦
+
+
Xâˆ’ (t) and N (t) := Ht 	 X+ (t). Then
â—¦

âˆ’

+
H = N (t) âŠ• Ht âŠ• N (t).
â—¦

Now, a0 (x(t) âˆ’ xâˆ’ (t)) is orthogonal to Hâˆ’
t and hence to
Nâˆ’ (t). Also a0 xâˆ’ (t) âŠ¥ Nâˆ’ (t). Hence a0 x(t) âŠ¥ Nâˆ’ (t) as
well. In the same way we see that a0 x(t) âŠ¥ N+ (t). Therefore
(96) follows.
Consequently, the information from the two Kalman filters
can be fused into the smoothing estimate
xÌ‚(t) = Lâˆ’ (t)xâˆ’ (t) + LÌ„+ (t)xÌ„+ (t)
for some matrix functions Lâˆ’ and LÌ„+ .

(98)

11

VII. U NIVERSAL TWO - FILTER FORMULA
To obtain a robust and particularly simple smoothing formula that works also with an intermittent observation pattern,
we assume that the stochastic system (75) has already been
transformed via (58) so that, for all t âˆˆ [0, T ],
x(t) = xÌ„(t)

(99)

Then (102) follows from (98) and (107). To prove (104)
eliminate LÌ„+ in (106) to obtain
Lâˆ’ (I âˆ’ Pâˆ’ PÌ„+ ) = QÌ„+ ,
which together with (107) yields
âˆ’1
Qâˆ’1 = Qâˆ’1
âˆ’ (I âˆ’ Pâˆ’ PÌ„+ )QÌ„+ .

However,

and therefore

I âˆ’ Pâˆ’ PÌ„+ = QÌ„+ + Qâˆ’ âˆ’ Qâˆ’ QÌ„+ ,
0

P (t) = E{x(t)x(t) } = I = PÌ„ (t).

(100)

Then the error covariances in the filtering formulas of Section V are
Qâˆ’ = I âˆ’ Pâˆ’

and QÌ„+ = I âˆ’ PÌ„+ .

(101)

Consequently, x(t), xÌ„(t), Pâˆ’ (t) and PÌ„+ (t) are all bounded in
norm by one for all t âˆˆ [0, T ].
Theorem 7: Suppose that (99) holds. For every t âˆˆ [0, T ],
we have the formula

xÌ‚(t) = Q(t) Qâˆ’ (t)âˆ’1 xâˆ’ (t) + QÌ„+ (t)âˆ’1 xÌ„+ (t)
(102)
for the smoothing estimate (90), where the estimation error

0	
Q(t) := E (x(t) âˆ’ xÌ‚(t)) (x(t) âˆ’ xÌ‚(t))
(103)
is given by
Q(t)âˆ’1 = Qâˆ’ (t)âˆ’1 + QÌ„+ (t)âˆ’1 âˆ’ I,

and hence (104) follows.
In the special case with no loss of observation this is a
normalized version of the Mayne-Frazer two-filter formula [1],
[2], which however in [1], [2] was formulated in terms of
xâˆ’ and x+ rather than xÌ„+ , where x+ is the state process of
the forward stochastic system of the backward Kalman filter.
(For the corresponding formula in terms of xâˆ’ and xÌ„+ , see
[3], [18]; also cf. [21], where an independent derivation was
given.) With a single interval of loss of observation the formula
(102) reduces to a version of the interpolation formulas in
[6]. The remarkable fact, discovered here, is that the same
formula (102) holds for any intermittent observations structure
and by a cascade of continuous and discrete-time forward and
backward Kalman filters, as needed depending on the assumed
information pattern.

(104)

and where xâˆ’ , xÌ„+ , Qâˆ’ and Q+ are given by (82) and (86)
with boundary conditions xâˆ’ (0) = xÌ„+ (T ) = 0 and Qâˆ’ (0) =
Q+ (T ) = I.
Proof: Clearly the matrix functions Lâˆ’ and LÌ„+ in (98)
can be determined from the orthogonality relations
E{[x(t) âˆ’ xÌ‚(t)]xâˆ’ (t)0 } = 0

(105a)

E{[x(t) âˆ’ xÌ‚(t)]xÌ„+ (t)0 } = 0.

(105b)

and
By Lemma 4, (105) yields
Pâˆ’ âˆ’ Lâˆ’ Pâˆ’ âˆ’ LÌ„+ PÌ„+ Pâˆ’ = 0
PÌ„+ âˆ’ Lâˆ’ Pâˆ’ PÌ„+ âˆ’ LÌ„+ PÌ„+ = 0,

VIII. R ECAP OF COMPUTATIONAL STEPS
Given a system (75) with state covariance (55), make the
normalizing substitution
1

1

A(t) â† P (t)âˆ’ 2 A(t)P (t) 2 + R(t)
1

B(t) â† P (t)âˆ’ 2 B(t)

C(t) â† C(t)P (t)
h
i
1
1
with R(t) = ddt P (t)âˆ’ 2 P (t) 2 . Next, we compute the
intermittent forward and backward Kalman filter estimates xâˆ’
and xÌ„+ , respectively, along the lines of Section V, where,
due to the normalization, Qâˆ’ (0) = QÌ„+ (T ) = In . Then the
smoothing estimate is given by

xÌ‚(t) = Q(t) Qâˆ’ (t)âˆ’1 xâˆ’ (t) + QÌ„+ (t)âˆ’1 xÌ„+ (t) ,
where

which, in view of the fact that Pâˆ’ and PÌ„+ are positive definite,
yields
Lâˆ’ + LÌ„+ PÌ„+ = I
(106a)
Lâˆ’ Pâˆ’ + LÌ„+ = I

(106b)

Again by orthogonality and Lemma 4,
Q = E {(x âˆ’ xÌ‚) x0 } = I âˆ’ Lâˆ’ Pâˆ’ âˆ’ LÌ„+ PÌ„+ ,
which, in view of (106) and the relations (101), yields
Lâˆ’ =

QQâˆ’1
âˆ’

and

LÌ„+ =

QQÌ„âˆ’1
+ .

(107)

(108)

1
2

Q(t) = Qâˆ’ (t)âˆ’1 + QÌ„+ (t)âˆ’1 âˆ’ I

âˆ’1

.

IX. A N EXAMPLE
We now illustrate the results of the paper on a specific
numerical example. We consider the continuous-time diffusion
process
dx1 (t)

= x2 (t)dt

dx2 (t)

= âˆ’0.3x1 (t)dt âˆ’ 0.7x2 (t)dt + dw(t)

dy(t)

= x1 (t)dt + dv(t)

12

where w and v are thought to be independent standard Wiener
processes. Here, x1 is thought of as position and x2 as velocity
of a particle that is steered by stochastic excitation in dw, in the
presence of a restoring force 0.3x1 and frictional force 0.7x2 .
Then dy/dt represents measurement of the position and dv/dt
represents measurement noise (white).

1

y

0
-50

0
0

5

10

15

20

25

30

35

40

45

dy

0.5

-3
0

5

10

15

20

25

30

35

40

0

5

10

15

20

25

30

35

40

45

x1
x2

10

15

20

25

30

35

40

45

x 2,est
x2

0
-2

0
0

5

10

15

20

25

30

35

40

45

Fig. 5: Sample paths of output process, increment, and state
processes

-4

0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 7: Kalman estimates in the backward time direction
2

KF forward estimate x - missing data in blue intervals
1

2

1

1

0

0

-1

-1

-2

x est,1
x1

-2
0

5

10

15

20

25

30

35

40

-3

0

5

10

15

20

25

30

35

40

45

Smoothed state estimates
4
x 2,smooth
x2

2

x est,2
x2

2

x 1,smooth
x1

45

KF forward estimate x 2 - missing data in blue intervals

4

0

0

-2

-2
-4

5

KF backward estimate x 2 - missing data in blue intervals

2

5

-3

0

4

0

-5

x 1,est
x1

45

5

-5

-1
-2

0
-0.5

KF backward estimate x 1 - missing data in blue intervals

2

Output process and states

50

of intervals, data are not made available for state estimation;
these intervals where data are not to be used are marked by
a thick blue baseline in the figures. In Figure 5 we display
sample paths of the output process y, increments dy, and stateprocesses x1 and x2 .

-4
0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 6: Kalman estimates in the forward time direction
Numerical simulation over [0, T ] with T = 45 (units of
time) produces a time-function y(t) which is sampled with
integer multiples of âˆ†t = 0.01 (units). The interval [0, T ] is
partitioned into
[0, T ] = âˆª9i=1 [tiâˆ’1 , ti ]
where t0 = 0 and ti âˆ’ tiâˆ’1 = i (units). Measurements of y
are made available for purposes of state estimation over the
intervals [tiâˆ’1 , ti ] for i = 1, 3, 5, 9. Over the complement set

0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 8: Interpolation/smoothed estimates by fusion of Kalman
forward and backward estimates
The process increments dy over [tiâˆ’1 , ti ] for i = 1, 3, 5, 9
as well as the increments âˆ†y across the [tiâˆ’1 , ti ] for i =
2, 4, 6, 8 are used in the two-filter formula for the purpose
of smoothing. The Kalman estimates for the states in the
forward and backwards in time directions, xâˆ’ (t) and xÌ„+ (t)
are shown in Figures 6 and 7, respectively. The fusion of the
two using (102) is shown in Figure 8. It is worth observing the
nature and fidelity of the estimates. In the forward direction,
across intervals where data is not available, xâˆ’ becomes

13

increasing more unreliable whereas the opposite is true for
xÌ„+ , as expected. The smoothing estimate is generally an
improvement to those of the two Kalman filters as seen in
Figure 8. In particular, it is worth noting x2 (in subplot 2),
where, over windows of available observations, estimates have
considerably less variance in the middle of the interval where
the weights (Q(t)Qâˆ’ (t)âˆ’1 and Q(t)QÌ„+ (t)âˆ’1 ) in (102) are
equalized, whereas sample paths become increasing rugged
at the two ends where one of the two Kalman estimates has
significantly higher variance, and the corresponding mixing
coefficient becomes relatively smaller.
X. C ONCLUDING REMARKS
Historically the problem of interpolation has been considered from the beginning of the study of stochastic processes
[22], [23]. Early accounts and treatments were cumbersome
and non-explicit as the problem was considered difficult [7],
[8], [9], [10]. In a manner that echoes the development of
Kalman filtering, the problem became transparent and computable for ouput processes of linear stochastic systems [5],
[6], [18].
This paper builds on developments in stochastic realization
theory [11], [24] and presents a unified and generalized twofilter formula for smoothing and interpolation in continuous
time for the case of intermittent availability of data over
an operating window. The analysis considers two alternative
information patterns where increments of the output process or
the output process itself is recorded when information becomes
available. The second information pattern appears most natural
to us in this continuous-time setting, and this is our main
problem. Nevertheless, in either case, two Kalman filters run in
opposite time-directions, designed on the basis of a forward
and a backward model for the process, respectively. Fusion
of the respective estimates is effected via linear mixing in
a manner similar to the Mayne-Fraser formula and applies to
both smoothing and interpolation intermixed. In earlier works,
smoothing and interpolation have been considered separate
problems [18, Chapter 15]. The balancing normalization also
simplifies the mixing formula and makes it completely time
symmetric.
The theory relies on time-reversal of stochastic models.
We provide a new derivation of such a reversal which has the
convenient property of being balanced. It is based on lossless
imbedding of linear systems and effects the time reversal
through a unitary transformation. Interestingly, time symmetry
in statistical and physical laws have occupied some of the most
prominent minds in science and mathematics. In particular,
closer to our immediate interests, dual time-reversed models
have been employed to model, in different time-directions,
Brownian or SchroÌˆdinger bridges [25], [26], a subject which is
related to reciprocal processes [27], [28], [29], [30]. A natural

extension of the present work in fact is in the direction of
general reciprocal dynamics [28], [29] and the question of
whether similar two-filter formula are possible.
A PPENDIX : T IME REVERSAL OF NON - STATIONARY
DISCRETE - TIME SYSTEMS
Next, instead of (1), consider the non-stationary state
dynamics
x(t + 1) = A(t)x(t) + B(t)w(t),

x(0) = x0 ,

(109)

on a finite time-window [0, T ], where, for simplicity we
now assume that the covariance matrix P0 := P (0) of
the zero-mean stochastic vector x0 is positive definite, i.e.,
P0 = E{x0 x00 } > 0. Then the state covariance matrix
P (t) := E{x(t)x(t)0 } will satisfy the Lyapunov difference
equation
P (t + 1) = A(t)P (t)A(t)0 + B(t)B(t)0 .

(110)

The state transformation
1

Î¾(t) = P (t)âˆ’ 2 x(t)

(111)

brings the system (109) into the form
Î¾(t + 1) = F (t)Î¾(t) + G(t)w(t),

(112)

where now E{Î¾(t)Î¾(t)0 } = In for all t and
1

1

F (t) = P (t + 1)âˆ’ 2 A(t)P (t) 2 ,
G(t) = P (t + 1)

âˆ’ 12

B.

(113a)
(113b)

The Lyapunov difference equation then reduces to
In = F (t)F (t)0 + G(t)G(t)0

(114)

allowing us to embed [F, G] as part of a time-varying orthogonal matrix


F (t) G(t)
U (t) =
.
(115)
H(t) J(t)
This amounts to extending (112) to
Î¾(t + 1) = F (t)Î¾(t) + G(t)w(t)

(116a)

wÌ„(t) = H(t)Î¾(t) + J(t)w(t),

(116b)

or, in the equivalent form




Î¾(t + 1)
Î¾(t)
= U (t)
.
wÌ„(t)
w(t)

(117)

Hence, since E{Î¾(t)Î¾(t)0 } = In and E{w(t)w(t)0 } = Ip , and
assuming that E {Î¾(t)w(t)0 } = 0,
(

0 )
Î¾(t + 1) Î¾(t + 1)
E
= U (t)U (t)0 = In+p , (118)
wÌ„(t)
wÌ„(t)
which yields
E{Î¾(t + 1)wÌ„(t)0 } = 0,
0

E{wÌ„(t)uÌ„(t) } = Ip .

(119a)
(119b)

14

where x0 and the normalized white-noise process w are
uncorrelated and E{x0 x00 } = P0 . In fact, inserting the transformations (122) and (123a) into (124b) yields

Moreover, from (116) we have
uÌ„(t + k) = H(t + k)Î¦(t + k, t)Î¾(t)
+

t+kâˆ’1
X

H(t + k)Î¦(t + k, j + 1)G(j)w(j) + J(t)w(t)

y(t) = CÌ„ xÌ„(t) + DÌ„wÌ„(t),

j=t

for k > 0, where
(
F (s âˆ’ 1)F (s âˆ’ 2) Â· Â· Â· F (t) for s > t
Î¦(s, t) =
In for s = t.

where

Therefore, since F (t)H(t)0 + G(t)J(t)0 = 0 by the unitarity
of U (t),

From that we have the backward system

CÌ„ = C(t)P (t)A(t)0 + D(t)B(t)0
DÌ„ = C(t)P (t)BÌ„(t) + D(t)J(t)

0

E{uÌ„(t + k)uÌ„(t) }
= H(t + k)Î¦(t + k, t + 1)[F (t)H(t)0 + G(t)J(t)0 ] = 0.
Consequently, uÌ„ is a white noise process. Finally, premultiplying (117) by U (t)0 , we then obtain
Î¾(t) = F (t)0 Î¾(t + 1) + H(t)0 wÌ„(t)
0

0

w(t) = G(t) Î¾(t + 1) + J(t) wÌ„(t),

(120a)

0

(125)
(126)

xÌ„(t âˆ’ 1) = A(t)0 xÌ„(t) + BÌ„(t)wÌ„(t)

(127a)

y(t) = CÌ„(t)xÌ„(t) + DÌ„(t)wÌ„(t)

(127b)

with the boundary condition xÌ„(T âˆ’ 1) = P (T )âˆ’1 x(T ) being
uncorrelated to the white-noise process wÌ„.

R EFERENCES

(120b)

which, in view of (119), is a backward stochastic system.

[1] D. Q. Mayne, â€œA solution of the smoothing problem for linear dynamic
systems,â€ Automatica, vol. 4, pp. 73â€“92, 1966.

Using the transformation (111), (116) yields the forward
representation

[2] D. Fraser and J. Potter, â€œThe optimum linear smoother as a combination
of two optimum linear filters,â€ Automatic Control, IEEE Transactions
on, vol. 14, no. 4, pp. 387â€“390, 1969.

x(t + 1) = A(t)x(t) + B(t)w(t)

[3] F. A. Badawi, A. Lindquist, and M. Pavon, â€œA stochastic realization
approach to the smoothing problem,â€ IEEE Trans. Automat. Control,
vol. 24, no. 6, pp. 878â€“888, 1979.

wÌ„(t) = BÌ„(t)0 x(t) + J(t)w(t),

(121a)
(121b)

1

where BÌ„(t) := P (t)âˆ’ 2 H(t)0 . Likewise (120) and
xÌ„(t) = P (t + 1)âˆ’1 x(t + 1),

(122)

yields the backward representation
xÌ„(t âˆ’ 1) = A(t)0 xÌ„(t) + BÌ„(t)wÌ„(t)
w(t) = B(t)0 xÌ„(t) + J(t)0 wÌ„(t).

(123a)
(123b)

Remark 8: When considered on the doubly infinite time
axis, equation (117) defines an isometry. Indeed, assuming that
the input is squarely summable, the fact that U (t) is unitary
for all t directly implies that
N
X

2

2

kwÌ„k + kÎ¾(t + 1)k =

âˆ’âˆž

N
X

Then, Î¾(t) â†’ 0 as t â†’ âˆž, provided Î¦(t, s) â†’ 0 as s â†’ âˆ’âˆž.
It follows that

t=âˆ’âˆž

kwÌ„(t)k2 =

âˆž
X

kw(t)k2 .

t=âˆ’âˆž

We are now in a position to derive a backward version of
a non-stationary stochastic system
x(t + 1) = A(t)x(t) + B(t)w(t),
y(t) = C(t)x(t) + D(t)w(t)

x(0) = x0

[5] M. Pavon, â€œNew results on the interpolation problem for continuoustime stationary increments processes,â€ SIAM journal on Control and
Optimization, vol. 22, no. 1, pp. 133â€“142, 1984.
[6] â€”â€”, â€œOptimal interpolation for linear stochastic systems,â€ SIAM journal on Control and Optimization, vol. 22, no. 4, pp. 618â€“629, 1984.
[7] K. Karhunen, Zur Interpolation von stationaÌˆren zufaÌˆlligen Funktionen.
Suomalainen tiedeakatemia, 1952.
[8] Y. Rozanov, Stationary random processes. Holden-Day, San Francisco,
1967.
[9] P. Masani, â€œReview: Yu.A. Rozanov, stationary random processes,â€ The
Annals of Mathematical Statistics, vol. 42, no. 4, pp. 1463â€“1467, 1971.

kw(t)k2 .

âˆ’âˆž

âˆž
X

[4] F. Badawi, A. Lindquist, and M. Pavon, â€œOn the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear
stochastic systems,â€ in Decision and Control including the Symposium
on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE,
1979, pp. 505â€“510.

(124a)
(124b)

[10] H. Dym and H. P. McKean, Gaussian processes, function theory, and
the inverse spectral problem. Courier Dover Publications, 2008.
[11] A. Lindquist and G. Picci, â€œOn the stochastic realization problem,â€ SIAM
J. Control Optim., vol. 17, no. 3, pp. 365â€“389, 1979.
[12] â€”â€”, â€œForward and backward semimartingale models for Gaussian
processes with stationary increments,â€ Stochastics, vol. 15, no. 1, pp.
1â€“50, 1985.
[13] â€”â€”, â€œRealization theory for multivariate stationary Gaussian processes,â€ SIAM J. Control Optim., vol. 23, no. 6, pp. 809â€“857, 1985.
[14] â€”â€”, â€œA geometric approach to modelling and estimation of linear
stochastic systems,â€ J. Math. Systems Estim. Control, vol. 1, no. 3, pp.
241â€“333, 1991.

15

[15] A. Lindquist and M. Pavon, â€œOn the structure of state-space models
for discrete-time stochastic vector processes,â€ IEEE Trans. Automat.
Control, vol. 29, no. 5, pp. 418â€“432, 1984.
[16] G. Michaletzky, J. Bokor, and P. VaÌrlaki, Representability of stochastic
systems. Budapest: AkadeÌmiai KiadoÌ, 1998.
[17] G. Michaletzky and A. Ferrante, â€œSplitting subspaces and acausal
spectral factors,â€ J. Math. Systems Estim. Control, vol. 5, no. 3, pp.
1â€“26, 1995.
[18] A. Lindquist and G. Picci, Linear Stochastic Systems: A Geometric
Approach to Modeling, Estimation and Identification. Springer-Verlag,
Berlin Heidelberg, 2015.
[19] T. T. Georgiou, â€œThe CaratheÌodoryâ€“FejeÌrâ€“Pisarenko decomposition and
its multivariable counterpart,â€ Automatic Control, IEEE Transactions on,
vol. 52, no. 2, pp. 212â€“228, 2007.

vol. 4, no. 4, pp. 173â€“178, 1949.
[24] M. Pavon, â€œStochastic realization and invariant directions of the matrix
Riccati equation,â€ SIAM Journal on Control and Optimization, vol. 18,
no. 2, pp. 155â€“180, 1980.
[25] M. Pavon and A. Wakolbinger, â€œOn free energy, stochastic control, and
SchroÌˆdinger processes,â€ in Modeling, Estimation and Control of Systems
with Uncertainty. Springer, 1991, pp. 334â€“348.
[26] P. Dai Pra and M. Pavon, â€œOn the Markov processes of SchroÌˆdinger,
the Feynman-Kac formula and stochastic control,â€ in Realization and
Modelling in System Theory. Springer, 1990, pp. 497â€“504.
[27] B. Jamison, â€œReciprocal processes,â€ Probability Theory and Related
Fields, vol. 30, no. 1, pp. 65â€“86, 1974.

[20] T. Georgiou and A. Lindquist, â€œOn time-reversibility of linear stochastic
models,â€ arXiv preprint arXiv:1309.0165, 2013.

[28] A. Krener, â€œReciprocal processes and the stochastic realization problem
for acausal systems,â€ in Modelling, Identification and Robust Control,
C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986,
pp. 197â€“211.

[21] J. E. Wall Jr, A. S. Willsky, and N. R. Sandell Jr, â€œOn the fixedinterval smoothing problem,â€ Stochastics: An International Journal of
Probability and Stochastic Processes, vol. 5, no. 1-2, pp. 1â€“41, 1981.

[29] B. C. Levy, R. Frezza, and A. J. Krener, â€œModeling and estimation of
discrete-time Gaussian reciprocal processes,â€ Automatic Control, IEEE
Transactions on, vol. 35, no. 9, pp. 1013â€“1023, 1990.

[22] A. N. Kolmogorov, Stationary sequences in Hilbert space. John Crerar
Library National Translations Center, 1978.

[30] P. Dai Pra, â€œA stochastic control approach to reciprocal diffusion
processes,â€ Applied mathematics and Optimization, vol. 23, no. 1, pp.
313â€“329, 1991.

[23] A. M. Yaglom, â€œOn problems about the linear interpolation of stationary
random sequences and processes,â€ Uspekhi Matematicheskikh Nauk,

