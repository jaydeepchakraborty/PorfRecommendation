A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION
OF TIME SERIES*
JORGE MARIâ€ , ANDERS DAHLEÌNâ€ , AND ANDERS LINDQUISTâ€ 

Abstract. In this paper we consider a three-step procedure for identiï¬cation of
time series, based on covariance extension and model reduction, and we present a
complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy
model is determined, which is ï¬nally approximated by a lower-order model by
stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all
three steps has been lacking. Supposing the data is generated from a true ï¬nitedimensional system which is minimum phase, it is shown that the transfer function
of the estimated system tends in Hâˆ to the true transfer function as the data length
tends to inï¬nity, if the covariance extension and the model reduction is done properly. The proposed identiï¬cation procedure, and some variations of it, are evaluated
by simulations.

1. Introduction
In recent years there has been quite some interest in a certain type of procedures
for identiï¬cation of time series known as subspace methods [1, 42, 41, 28, 29]. These
identiï¬cation procedures are based on geometric projection methods, and they could
be understood in the context of splitting geometry and partial stochastic realization
theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9],
these procedures are algebraically equivalent to minimal factorization of a Hankel
matrix of covariance estimates, and they make no distinction between stochastic and
deterministic partial realizations. Therefore they may fail because of loss of positive
realness in the spectral estimation phase.
In an attempt to overcome these problems we analyze an alternative approach
to time series identiï¬cation proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the
maximum-entropy method, leading to a high order autoregressive (AR) process, and
ï¬nally stochastically balanced truncation. This method shares certain features with
stochastic subspace identiï¬cation methods, the most obvious one being that it is
based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
âˆ— This research was supported by a grant from the Swedish Research Council for Engineering
Sciences (TFR).
â€  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
1

2

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for
maximum-likelihood (ML) methods, are needed.
The idea of approximating an autoregressive moving-average (ARMA) process by
an AR process is by no means new. Its origins can be traced back to the Wold
decomposition [55] where L2 -convergence of high-order AR models to general analytic
models is shown. Pioneers in the use of this concept for systems identiï¬cation are
Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations
were studied by Berk [2] and later reï¬ned in [36, 34, 33, 7]. The interesting paper [7]
contains nice proofs of some of the convergence results needed in this paper, but, for
the sake of completeness and insight, we provide new proofs based on some properties
of fast ï¬ltering algorithms [5] and simple methods of complex analysis and SzegoÌ‹
polynomials. The power of the theory of SzegoÌ‹ polynomials and Toeplitz matrices in
analyzing stochastic processes is reported in [24], but, except for elementary theory,
it has not been much used in systems identiï¬cation [39]. This is even more true for
the newer results [16, 40, 37, 27] on orthogonal polynomials.
The idea of using model reduction for systems identiï¬cation appears in the thesis
by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency
weighted reduction. Instead, we use stochastically balanced truncation, for which we
develop a simple computational procedure, exploiting the special structure of the AR
model. We also show the advantage of this reduction procedure by theoretical analysis
and simulations. In fact, a comprehensive study comprising all the steps mentioned
above together with a qualitative and quantitative analysis of the entire identiï¬cation
strategy has been lacking, and that is what we oï¬€er in this paper.
The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identiï¬cation
procedure contributes to the estimation error. In Section 3 we show that the transfer
function of the maximum-entropy ï¬lter, constructed from true covariances, tends to
that of the true ï¬lter in Hâˆ norm at a geometric rate determined by the largest
modulus of the zeros of the true ï¬lter as the order of the maximum-entropy ï¬lter
becomes large. However the order of the approximation is too high, and therefore
model reduction is performed. This is studied in Section 4. A stochastic balancing
procedure, based only on linear-algebra operations so that no Riccati equations need
to be solved, is provided together with the analysis of the model-reduction error.
Both deterministically and stochastically balanced truncation lead to good results.
However, when the covariances are estimated from statistical data, stochastic model
reduction is found to be superior. In particular, variances are considerably closer to
the CrameÌr-Rao bounds. In Section 5 we state our statistical convergence theorems,
proving that the total error tends to zero as the length of the data string tends to
inï¬nity, provided the degree of the AR model tends to inï¬nity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using
stochastic subspace identiï¬cation [43] is included. For clarity of exposition, all the
proofs have been deferred to two appendices, Appendix A dealing with the asymptotic
properties of the maximum-entropy ï¬lter, and Appendix B devoted to the statistical
error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identiï¬cation
Time series identiï¬cation in the form studied here amounts to estimating the matrices
(A, B, C, D) in some n-dimensional linear stochastic system

x(t + 1) = Ax(t) + Bw(t)
(2.1)
y(t)
= Cx(t) + Dw(t)
driven by normalized white noise {w(t)}, from a data string of observations
{y0 , y1 , y2 , . . . , yN }

(2.2)

of the output process {y(t)}, which here will be taken to be scalar.
The basic idea behind our approach is very simple: given estimates of a partial
sequence
c 0 , c 1 , c 2 , . . . , cÎ½

(2.3)

of the covariances ck = E{y(t+k)y(t)}, which satisï¬es the condition that the Toeplitz
matrix
ï£®
ï£¹
c2 Â· Â· Â· cÎ½
c0 c1
ï£¯ c1 c0
c1 Â· Â· Â· cÎ½âˆ’1 ï£º
ï£¯
ï£º
ï£¯
c
c
c0 Â· Â· Â· cÎ½âˆ’2 ï£º
1
TÎ½+1 := ï£¯ 2
(2.4)
..
.. ï£º
..
...
ï£° ...
ï£»
.
.
.
cÎ½ cÎ½âˆ’1 cÎ½âˆ’2 Â· Â· Â· c0
is positive deï¬nite, ï¬rst construct a high-order model continuing (2.3) by covariance
extension. This model has all the required positivity properties, but the order is too
high. Then reduce the order by means of a positivity-preserving model reduction
procedure to be speciï¬ed below. That this simple recipe will in fact provide a good
identiï¬cation method is by no means a trivial matter but is based on some rather
deep results, which will be presented here.
More speciï¬cally, the approach consists of three steps, for which there are several
possible variants that will be discussed below. The rigorous mathematical analysis,
however, will be carried out for the following procedure, for which we shall give
theoretical bounds.
(i) Estimate a partial covariance sequence
cÌ‚0 , cÌ‚1 , cÌ‚2 , . . . , cÌ‚Î½

(2.5)

from the time-series data (2.2) via the ergodic estimate
N âˆ’k
1 	
yt+k yt
cÌ‚k =
N + 1 t=0

k = 0, 1, . . . , Î½.

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer
function
zÎ½
,
(2.7)
WÌ‚Î½ (z) =
Ï†Ì‚Î½ (z)
where Ï†Ì‚Î½ (z) is the normalized SzegoÌˆ polynomial of degree Î½, to be introduced
in Section 3, computed from the estimated covariance data (2.5).

4

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

(iii) Determine a reduced-degree approximation WÌ‚ (z) of WÌ‚Î½ (z) via a stochastic
model reduction procedure [11] to be described in more detail in Section 4.
In this procedure, the idea is that Î½ >> n, the order of the system to be identiï¬ed,
and ideally nÌ‚ := deg WÌ‚ equals the degree n of the true system (2.1). However, the
method will produce a valid model even if this is not the case or even if there is
no â€œtrueâ€ underlying model. This is in contrast to stochastic subspace identiï¬cation
models, which may fail to produce any model at all [9].
There are possibilities for variations of the procedure described above. In Step (i)
we could use alternative covariance estimates or Burgâ€™s estimation of Schur parameters
[3], the only requirements being that the estimated Toeplitz matrix TÌ‚Î½+1 of (2.5) is
positive deï¬nite and that cÌ‚k â†’ ck a.s. as N â†’ âˆ. In Step (ii) we could instead use
approximate covariance extension or covariance extension with prescribed zeros, for
which there is now a complete parameterization [5] and an algorithm [4]. (In the latter
case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction
methods could be used. For example, an important model reduction paradigm is the
one based on optimal Hankel norm approximation [21].
Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable
transfer function
W (z) = C(zI âˆ’ A)âˆ’1 B + D,

(2.8)

of McMillan degree n. We also assume that W (z) is minimum-phase so that both
zeros and poles are located in the open unit disc. Then, we need to be able to measure
how the estimated model, with transfer function WÌ‚ (z), converges to the true one as
N â†’ âˆ. In this paper we have chosen to use distance between W (z) and WÌ‚ (z) in
âˆ
norm as a measure of proximity between the true and estimated model. From an
engineering point of view this could be called worst case identiï¬cation. The modern
literature in robust control makes extensive use of the worst case philosophy; see for
example [20, 52]. There are also other reasons for using the âˆ , as discussed in [35].
Returning, then, to the identiï¬cation approach outlined above, the estimation error
can be decomposed into three parts, one corresponding to each of the steps (i), (ii)
and (iii). Hence we have the error bound

L

L

W âˆ’ WÌ‚ âˆ â‰¤ W âˆ’ WÎ½ âˆ + WÎ½ âˆ’ WÌ‚Î½ âˆ + WÌ‚Î½ âˆ’ WÌ‚ âˆ ,

(2.9)

where WÎ½ is the AR model corresponding to the true covariances (2.3) and WÌ‚Î½ is
the one determined from the estimated covariances (2.6). To prove convergence to
zero of the estimation error (2.9), we shall need to assume that W is minimum-phase,
and hence WÌ‚ should have the same property, which moreover is desirable in many
applications. Our procedure insures this.
Estimating the ï¬rst term in (2.9) is a problem in stochastic partial realization
theory and function theory and will be dealt with in the next section. The third term
concerns model reduction which will be studied, in the particular setting required
here, in Sections 4 and 5. In Section 5, ï¬nally, we consider the second term together
with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence
Step (ii) in the identiï¬cation procedure outlined in Section 2 is based on rational
covariance extension. To understand this, let us consider the covariance extension
problem from a more general point of view. Given a partial covariance sequence
c 0 , c 1 , c 2 , . . . , cÎ½ ,

(3.1)

covariance extension amounts to ï¬nding an inï¬nite extension cÎ½+1 , cÎ½+2 , cÎ½+3 , . . . of
this sequence such that the function
V (z) := 12 c0 + c1 z âˆ’1 + c2 z âˆ’2 + . . .

is strictly positive real, i.e., it is an analytic function in the complement Dc of the
open unit disc D, which maps Dc to the open right complex half-plane. Then
Î¦(z) := V (z) + V (z âˆ’1 )
is a spectral density for a process having c0 , c1 , . . . , cÎ½ as its ï¬rst Î½ covariances and
which is coercive in the sense that
Î¦(eiÎ¸ ) > 0 for all Î¸.
Spectral factorization is then to ï¬nd a stable transfer function W (z) such that
|W (eiÎ¸ )|2 = Î¦(eiÎ¸ ).
In particular, we are interested in ï¬nding covariance extensions for which V (z), and
hence W (z), have at most degree Î½.
For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is
classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one
correspondence between inï¬nite covariance sequences
c0 , c1 , c2 , c3 , . . .

(3.2)

and a sequence of Schur parameters, or reï¬‚ection coeï¬ƒcients,
Î³0 , Î³1 , Î³2 , Î³3 , . . . ,

(3.3)

with the property |Î³t | < 1 for all t. In fact, ï¬xing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and Î³0 , Î³1 , . . . , Î³mâˆ’1 for
each m. The Schur parameters can be determined from the covariances via the SzegoÌˆ
polynomials
Ï•t (z) = z t + Ï•t1 z tâˆ’1 + Â· Â· Â· + Ï•tt t = 0, 1, 2 . . . ,
computed by means of the SzegoÌˆ-Levinson recursion
 






 
 


z
âˆ’Î³t Ï•t (z)
Ï•0 (z)
1
Ï•t+1 (z)
=
;
=
,
(3.4)
âˆ’zÎ³t 1
1
Ï•âˆ—t+1 (z)
Ï•âˆ—t (z)
Ï•âˆ—0 (z)
where

Ï•âˆ—t (z) := z t Ï•t (z âˆ’1 )
is the reciprocal polynomial of Ï•t (z), and the Schur parameters are computed via

= r1t tj=0 Ï•t,tâˆ’j cj+1
Î³t
(3.5)
rt+1 = rt (1 âˆ’ |Î³t |2 ), r0 = c0 .

6

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

Hence Î³t = âˆ’Ï•t+1 (0), a fact that we shall use below.
In the problem to ï¬nd a covariance extension for (3.1), therefore, Î³0 , Î³1 , . . . , Î³Î½âˆ’1
are ï¬xed and the inï¬nite continuation Î³Î½ , Î³Î½+1 , . . . can be chosen freely. In particular,
if we take Î³t = 0 for t = Î½, Î½ + 1, Î½ + 2, . . . . We obtain the maximum entropy solution
WÎ½ (z) =

zÎ½
,
Ï†Î½ (z)

(3.6)

where Ï†Î½ (z) is the normalized SzegoÌˆ polynomial
1
Ï†Î½ (z) := âˆš Ï•Î½ (z).
rÎ½

(3.7)

Thus, in this particular case, the solution to the covariance extension problem turns
out to be rational of degree at most Î½ as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations
are needed. In fact, it has recently been shown [5] that there is exactly one such
solution for each choice of zeros of WÎ½ (z), thus proving a long-standing conjecture
by Georgiou [18], who had established existence. Nevertheless, as we shall see next,
rationality implies that the Schur parameters tend geometrically to zero, provided
W (z) has no zeros on the unit circle.
In this section we shall demonstrate that the rational transfer function (2.8) can be
approximated arbitrarily closely in Lâˆ by the transfer function WÎ½ (z) of a maximum
entropy ï¬lter for suï¬ƒciently large Î½ and that this Î½ depends on the maximum modulus
of the zeros of W (z). We shall ï¬rst present a heuristic argument in support of this
conclusion.
To this end, let (3.2) be the inï¬nite covariance sequence of the output process y in
(2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via
the SzegoÌˆ-Levinson algorithm presented above. Then we have the following special
case of Corollary 2.1 in [5].
Lemma 3.1. Let the spectral density
Î¦(eiÎ¸ ) = |W (eiÎ¸ )|2

(3.8)

be coercive in the sense that it is positive for all Î¸ and let (3.3) be the corresponding
inï¬nite sequence of Schur parameters. Moreover, let Î³ âˆˆ (0, 1) be greater than the
maximum of the moduli of the zeros of W (z). Then
|Î³t | = O(Î³ t ),

(3.9)

i.e., |Î³t | â‰¤ M Î³ t for some M âˆˆ R and for suï¬ƒciently large t.
Remark 3.2. Since (3.9) holds for all Î³ greater than the the maximum of the moduli
of the zeros of W (z), we have in fact that |Î³t | = o(Î³ t ), i.e., limtâ†’âˆ |Î³t |Î³ âˆ’t = 0.
For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis
of certain fast algorithms for Kalman ï¬ltering [6].

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the
spectral density
z(z âˆ’ 1)2
Î¦(z) = âˆ’ 2
(z + z + 2)(2z 2 + z + 1)
is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters
are seen to be âˆ’1/2, âˆ’2/3, âˆ’2/5, âˆ’2/7, âˆ’2/9, âˆ’2/11, . . . , which tend to zero but not
geometrically. On the other hand, there are coercive, analytic but nonrational models
which also exhibit geometric convergence rate. A classical example [23] is obtained
2
when ck = Î¸k for some Î¸ âˆˆ (âˆ’1, 1). The Schur parameters in this case form an exact
geometric sequence, Î³k = (âˆ’Î¸)k+1 , k â‰¥ 0.
Lemma 3.1 implies that, for a suï¬ƒciently large Î½ which depends on Î³, the Schur
parameters Î³t are close to zero for t = Î½, Î½ + 1, Î½ + 2, . . . . But, the Schur parameters
of WÎ½ are exactly zero for t = Î½, Î½ + 1, Î½ + 2, . . . , and hence geometric convergence
would insure that WÎ½ is a good approximation of W (z) for suï¬ƒciently large Î½. We
shall prove that this is indeed the case.
Theorem 3.4. Suppose W (z) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let Î³ âˆˆ (0, 1) be greater than the maximum of the moduli of the
zeros of W (z). Then
lim WÎ½ âˆ’ W âˆ = 0,

Î½â†’âˆ

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that
WÎ½ âˆ’ W âˆ â‰¤ M Î³ Î½ .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to ï¬rst showing
that
lim WÎ½âˆ’1 âˆ’ W âˆ’1 âˆ = 0.

Î½â†’âˆ

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we
give an alternative proof of this fact based on SzegoÌˆ theory, and also show that the
convergence is geometric. In fact, we can choose Î³ arbitrarily close to the maximum
modulus of the zeros of W .
However, as we shall see next, we can actually prove more. To this end, let us ï¬rst
observe that, since WÎ½âˆ’1 and W âˆ’1 have their poles in the open unit disc D and thus
are bounded and analytic in the complement Dc of D, they belong to the Hardy space
âˆ
of functions which are analytic and bounded in {z âˆˆ C | |z| > 1}. Hence the
Hâˆ’
âˆ
, and
convergence (3.12) is in Hâˆ’
z âˆ’Î½ Ï†Î½ (z) â†’ W âˆ’1 (z)

(3.13)

uniformly in each compact subset of Dc . Now, W âˆ’1 is analytic in {z âˆˆ C | |z| â‰¥ Î³},
a region that is strictly larger than Dc . This in itself of course does not insure that
the convergence (3.13) extends to this larger region. In fact, even if z âˆ’Î½ Ï†Î½ (z) did
converge in {z âˆˆ C | Î³ â‰¤ |z| â‰¤ 1}, it could fail to converge to W âˆ’1 (z) there. The fact
that it really does converge uniformly to this limit is another consequence of Lemma
3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and
hence an estimate of the convergence rate Î³, is given in [35].
Theorem 3.5. Suppose W (z) is a minimum-phase rational function having all its
poles in the open unit disc D and all its zeros in

DÏ := {z âˆˆ C | |z| â‰¤ Ï} âŠ‚ D

where 0 < Ï < 1,

and let {Ï†Î½ (z)}âˆ
0 be the normalized SzegoÌˆ polynomial (3.7) determined from the covariances in the spectral density
|W (e )| = c0 + 2
iÎ¸

2

âˆ
	

ck cos kÎ¸.

k=1

Then, as Î½ â†’ âˆ, z âˆ’Î½ Ï†Î½ (z) â†’ W âˆ’1 (z) uniformly in every compact subset of
{z âˆˆ C | |z| > Ï}, the complement of DÏ .

DcÏ :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of Ï†Î½ (z) and hence of the poles of the high-order AR
model with transfer function WÎ½ (z). It is known that, if the Toeplitz matrix TÎ½+1
is positive deï¬nite, all roots of Ï†Î½ (z) are located in the open unit disc D, but little
has been reported in the literature on their behavior as Î½ â†’ âˆ. This behavior is
illustrated in Figure 3.1.
Original system

Original system

1

1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

âˆ’0.5

0

0.5

1

âˆ’1
âˆ’1

Original and AR(24)
1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’0.5

0

0.5

0

0.5

1

Original and AR(24)

1

âˆ’1
âˆ’1

âˆ’0.5

1

âˆ’1
âˆ’1

âˆ’0.5

0

0.5

1

Figure 3.1: Distribution of zeros of Ï†Î½ (z).

The top two diagrams show the zero-pole positions, within the boundaries of the
unit circle, of two minimum phase spectral factors W , both of degree ï¬ve. Also
indicated is a circle of radius equal to the maximum modulus of the zeros of these
spectral factors. The little circles â€œâ—¦â€ represent zeros and the â€œ+â€ sign represent
poles. The lower two ï¬gures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the
exact covariance sequence. The poles of the latter models are indicated with â€œÃ—â€.
The left part of Figure 3.1 illustrates what may happen if all the poles of W (z) are
located in {z âˆˆ C | |z| < Ï}, where Ï is chosen to be the maximum of the moduli of
the zeros of W (z). The roots of Ï†Î½ (z) tend to cluster inside a circle of radius Ï as
Î½ â†’ âˆ. This phenomenon is in a sense predictable, since the constant term of the
SzegoÌˆ polynomials is Ï•n+1 (0) = âˆ’Î³n , which equals the product of the roots and, by
Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to Ï.
This does not preclude that other types of crowns may occur, because subsequences
of {Î³n } could decay faster than the overall rate Î³, as follows from [5]. Very general
statements about the distribution of zeros of orthogonal polynomials, derived with
the help of potential-theoretic methods, can be found in [37, 27].
To the right in Figure 3.1 we see what happens in the case that W has poles with
moduli larger than Ï. Then, for Î½ suï¬ƒciently large, the normalized SzegoÌˆ polynomial
Ï†Î½ (z) has roots in {z âˆˆ C | Ï â‰¤ |z| < 1}, but exactly as many as the poles of W in
this region and approximately at the same place as these. This is of course due to
the uniform convergence of z âˆ’Î½ Ï†Î½ (z) to W âˆ’1 (z) in every compact subset of DcÏ . The
other roots of Ï†Î½ (z) behave exactly as in the previous case and tend to accumulate in
a crown inside and very close to the circle {z âˆˆ C | |z| = Ï}.
âˆ
approximation WÎ½ of W which can be made
We have thus constructed an Hâˆ’
arbitrarily good by choosing Î½ suï¬ƒciently large. However, WÎ½ will have much larger
degree and, except for the poles outside the circle {z âˆˆ C | |z| = Ï}, a completely
diï¬€erent zero-pole pattern. We shall rectify this situation by model reduction. In
fact, for the moment considering the perfect modeling problem to identify the rational
transfer function (2.8) given an exact partial covariance sequence (3.1), the last step
in our procedure consists in approximating WÎ½ by a rational function Wred of smaller
degree, ideally of the same degree as W .
The simplest model reduction procedure is deterministically balanced truncation
(DBT), ï¬rst introduced by Moore [38]. Though easy to implement, it may fail to
yield a minimum-phase approximation, a requirement which is important in certain
contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced
truncation (SBT), ï¬rst introduced by Desai and Pal [10], which is based on a diï¬€erent
balancing strategy to be explained in detail in Section 4.
Original system

Reduction by DBT

Reduction by SBT

1

1

1

0.5

0.5

0.5

0

0

0

âˆ’0.5

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

0

1

âˆ’1
âˆ’1

0

1

âˆ’1
âˆ’1

0

1

Figure 3.2: Zero-pole pattern of W (z) and Wred (z) for diï¬€erent model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This ï¬fth-order

10

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

model has ï¬rst been approximated by WÎ½ of degree Î½ = 24, producing the pole-zero
pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens
when the model is reduced back to order ï¬ve by either deterministically balanced
truncation or stochastically balanced truncation. The zeros are denoted by â€œâ—¦â€ and
the poles by â€œ+â€. Both reduction procedures give good approximations when applied
to exact covariance data. However, as we shall see in Section 5, the advantages of SBT
becomes apparent when applied to statistical data. Also, as explained in Remark 4.5,
there are theoretical reasons to prefer stochastic model reduction.
4. Model reduction
In the present setting, model reduction amounts to replacing a stochastic system
(2.1) of dimension Î½ by one of some dimension r < Î½ in such a way that most of
its statistical features are retained. In particular, we want to remove the part of the
system which corresponds to the weakest correlation between past and future. This
idea can be formalized in the following way.
Basic concepts. In the Hilbert space generated by the random variables {y(t) |
âˆ’âˆ < t < âˆ} in the inner product u, v = E{uv}, let H âˆ’ be the subspace generated
by the past, i.e., {y(t) | t < 0}, and H + that generated by the future {y(t) | t â‰¥ 0}.
Consider the Hankel operator H : H + â†’ H âˆ’ and its adjoint Hâˆ— : H âˆ’ â†’ H + deï¬ned
as

H = EH

âˆ’

|H +

and

Hâˆ— = E H

+

|H âˆ’ ,

(4.1)

âˆ’

where E H denotes orthogonal projection onto the past space H âˆ’ . More precisely,
H sends Î¾ âˆˆ H + to E H âˆ’ Î¾ âˆˆ H âˆ’ and Hâˆ— sends Î· âˆˆ H âˆ’ to E H + Î· âˆˆ H +. Since the
process y is the output of a minimal stochastic system of dimension Î½, rank H = Î½ by
Kroneckerâ€™s Theorem [56], and hence H has exactly Î½ singular values, Ïƒ1 , Ïƒ2 , . . . , ÏƒÎ½ ,
which are positive, as usually listed so that Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ ÏƒÎ½ . These singular
values are the canonical correlation coeï¬ƒcients and hence the cosines of the angles
between the principal directions of the past space H âˆ’ and the future space H + . They
are therefore less than one, and the part of the stochastic system corresponding to
singular values which are close to zero have a weak coupling between past and future,
i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic
model reduction is to truncate the system so that this part is removed.
To each singular value Ïƒk there is an associated Schmidt pair (Î¾k , Î·k ) with Î¾k âˆˆ H +
and Î·k âˆˆ H âˆ’ such that

HÎ¾k = Ïƒk Î·k ,

Hâˆ—Î·k = Ïƒk Î¾k ,

and such that the sequences Î¾1 , Î¾2 , Î¾3 , . . . and Î·1 , Î·2 , Î·3 , . . . of singular vectors are
orthonormal. The singular vectors corresponding to nonzero singular values span the
predictor spaces
Xâˆ’ := span{Î·1 , Î·2 , . . . , Î·Î½ },

X+ := span{Î¾1 , Î¾2 , . . . , Î¾Î½ }.

Clearly, Xâˆ’ âŠ‚ H âˆ’ and X+ âŠ‚ H + .
The process y has one representation (2.1) for each minimal spectral factor W ,
having W as its transfer function. Such representations are called minimal stochastic
realizations and the corresponding subspaces X := {a x(0) | a âˆˆ RÎ½ } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, Xâˆ’ is the splitting subspace of the stochastic
realization

xâˆ’ (t + 1) = Axâˆ’ (t) + Bâˆ’ wâˆ’ (t)
(4.2)
y(t)
= Cxâˆ’ (t) + Dâˆ’ wâˆ’ (t)
with the transfer function Wâˆ’ (z), the minimum-phase spectral factor; and X+ is the
splitting subspace of

x+ (t + 1) = Ax+ (t) + B+ w+ (t)
(4.3)
y(t)
= Cx+ (t) + D+ w+ (t)
with transfer function W+ (z), the maximum-phase spectral factor, having all its zeros
in Dc . Note that A and C are the same in both realizations (uniform choice of bases).
Each realization has a counterpart which evolves backwards in time and has the
same splitting subspace. For example, the backward realization of X+ ,

xÌ„+ (t âˆ’ 1) = A xÌ„+ (t) + BÌ„+ wÌ„+ (t)
,
(4.4)
y(t)
= CÌ„ xÌ„+ (t) + DÌ„+ wÌ„+ (t)
has transfer function WÌ„+ (z), the coanalytic minimum-phase spectral factor, having all
its poles and zeros in Dc . In the present case with scalar y, we have WÌ„+ (z) = Wâˆ’ (z âˆ’1 ).
Now, in order to identify the part of the system which has the weakest coupling
between past and future, and hence will be removed in the model reduction, we need
to balance the system in the sense of Desai and Pal, as we shall explain next. To this
end, we make a coordinate transformation
(A, C, CÌ„) â†’ (SAS âˆ’1 , CS âˆ’1 , CÌ„S  ),

(4.5)

in the minimal realization of
1
(4.6)
V (z) = C(zI âˆ’ A)âˆ’1 CÌ„  + c0 ,
2
the strictly positive real part of the spectral density of y, so that the state covariances
Pâˆ’ := E{xâˆ’ (t)xâˆ’ (t) } and PÌ„+ = E{xÌ„+ (t)xÌ„+ (t) } coincide with the diagonal Î½ Ã— Î½
matrix Î£ of nonzero canonical correlation coeï¬ƒcients, i.e.,
Pâˆ’ = PÌ„+ = Î£ := diag(Ïƒ1 , Ïƒ2 , . . . , ÏƒÎ½ ).

(4.7)

1

This is done by choosing S so that Sxâˆ’ (0) = Î£ 2 Î·, where Î· = (Î·1 , Î·2 , . . . , Î·Î½ ) , and
1
(S  )âˆ’1 xÌ„+ (0) = Î£ 2 Î¾, where Î¾ := (Î¾1 , Î¾2 , . . . , Î¾Î½ ) .
To compute the canonical correlation coeï¬ƒcients, we ï¬rst observe that the eigenvalues of the product Pâˆ’ PÌ„+ are precisely the squares of the canonical correlation
coeï¬ƒcients, i.e.,
Î»(Pâˆ’ PÌ„+ ) = Î»(Pâˆ’ P+âˆ’1 ) = {Ïƒ12 , Ïƒ22 , . . . , ÏƒÎ½2 },

(4.8)

where we have used the fact that the state covariance of (4.3) is P+ = PÌ„+âˆ’1 . Therefore
the canonical correlation coeï¬ƒcients can then be determined via (4.8) by solving the
Lyapunov equations

Pâˆ’ = APâˆ’ A + Bâˆ’ Bâˆ’


and P+ = AP+ A + B+ B+
.

(4.9)

12

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

The point is now to identify the canonical correlation coeï¬ƒcients Ïƒ1 , Ïƒ2 , . . . , Ïƒr corresponding to the part of the system one wants to keep. The part corresponding to
Ïƒr+1 , Ïƒr+2 , . . . , ÏƒÎ½ will be disposed of. This amounts to partitioning Î£ as



Î£1
,
(4.10)
Î£=
Î£2
where Î£1 is r Ã— r.
In order to reduce model (2.1) we make the coordinate transformation (A, B, C) â†’
(SAS âˆ’1 , SB, CS âˆ’1 ), with the same balancing transformation S. Then, partition the
new triplet (A, B, C) conformally with (4.10) as


 




B1
A11 A12
(4.11)
, B=
, C = C1 C2 ,
A=
A21 A22
B2
and perform a principal subsystem truncation to obtain the transfer function of a
reduced-order system
Wred (z) = C1 (zI âˆ’ A11 )âˆ’1 B1 + D

(4.12)

of degree r. If Î£2 is close to zero, while Î£1 is not, the rank of H is close to r, and the
discarded part of the system gives a negligible contribution to y.
Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy ï¬lter
âˆš Î½
rÎ½ z
(4.13)
Wâˆ’ (z) := WÎ½ (z) =
Ï•Î½ (z)
of order Î½, which, for the moment we denote Wâˆ’ (z) to emphasize its character as the
minimum-phase spectral factor of the spectral density
rÎ½
.
Ï•Î½ (z)Ï•Î½ (z âˆ’1 )
Remark 4.1. Without loss of generality we assume that Ï•Î½ (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller Î½ for which this condition holds.
In fact, Ï•Î½ (0) = Î³Î½âˆ’1 , and if Î³Î½âˆ’p = Î³Î½âˆ’p+1 = Â· Â· Â· = Î³Î½âˆ’1 = 0 and Î³Î½âˆ’pâˆ’1 = 0 for some
p = 1, 2, . . . , Î½, then Ï•Î½ (z) = z Î½âˆ’p Ï•Î½âˆ’p (z) by (3.4), and hence (3.6) can be replaced
by WÎ½ (z) = WÎ½âˆ’p (z), and for WÎ½âˆ’p (z) the required condition holds.
The maximum-phase spectral factor W+ (z) has all its zeros at inï¬nity, and hence
âˆš
rÎ½

âˆ’1
W+ (z) = h (zI âˆ’ F ) b =
,
(4.14)
Ï•Î½ (z)
where (F, b, g) is the (observable) canonical form
ï£¹
ï£®
ï£¹
ï£®
0
0
1
Â·Â·Â·
0
..
.. ï£º
. ï£º
...
ï£¯ ...
.
. ï£º, b = ï£¯
ï£¯ .. ï£º ,
F =ï£¯
ï£° 0 ï£»
ï£° 0
0
Â·Â·Â·
1 ï£»
âˆš
âˆ’Ï•Î½Î½ âˆ’Ï•Î½,Î½âˆ’1 Â· Â· Â· âˆ’Ï•Î½1
rÎ½

ï£® ï£¹
1
ï£¯0ï£º
ï£º
h=ï£¯
ï£° ... ï£» ,
0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

Ï•Î½1 , Ï•Î½2 , . . . , Ï•Î½Î½ being the coeï¬ƒcients of the SzegoÌˆ polynomial Ï•Î½ (z). In this basis,
it follows from (4.9) that

  Ï€

1
iÎ¸
âˆ’1  âˆ’iÎ¸
 âˆ’1
(e I âˆ’ A) bb (e I âˆ’ A ) dÎ¸
[P+ ]jk =
2Ï€ âˆ’Ï€
jk
 Ï€
1
r
Î½
=
eâˆ’(jâˆ’k)iÎ¸
dÎ¸ = cjâˆ’k ,
2Ï€ âˆ’Ï€
Ï•Î½ (eiÎ¸ )Ï•Î½ (eâˆ’iÎ¸ )
and hence P+ = TÎ½ . It is well-known and easy to prove that Î¦Î½ TÎ½ Î¦Î½ = RÎ½ , where

Î¦Î½+1

ï£®
Ï•Î½Î½
ï£¯ ..
ï£¯ .
ï£¯
= ï£¯Ï•Î½2
ï£¯
ï£°Ï•Î½1
1

Ï•Î½âˆ’1,Î½âˆ’1
..
.

Ï•Î½âˆ’2,Î½âˆ’2
..
.

Ï•Î½âˆ’1,1
1

1

Â·Â·Â·

ï£¹
1
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£®
rÎ½âˆ’1
ï£¯
ï£¯
and RÎ½ = ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º , (4.16)
ï£»

rÎ½âˆ’2
..

.
r0

and consequently
PÌ„+ = TÎ½âˆ’1 = Î¦Î½ RÎ½âˆ’1 Î¦Î½ .
It remains to determine Pâˆ’ . From (4.13) is easy to see that
âˆš
Wâˆ’ (z) = âˆ’Ï•Î½ (zI âˆ’ F )âˆ’1 b + rÎ½ ,
where



Ï•Î½ := Ï•Î½Î½ Ï•Î½,Î½âˆ’1 Â· Â· Â· Ï•Î½1 ,

(4.17)

(4.18)

(4.19)

but, in order to determine Pâˆ’ , this realization needs to be transformed so that the A
and C matrices are the same as in (4.14) (uniform choice of bases). More precisely,
we need to perform a transformation
(F, b, âˆ’Ï•Î½ ) â†’ (QF Qâˆ’1 , Qb, âˆ’Ï•Î½ Qâˆ’1 ) =: (F, Qb, h ).
Then Pâˆ’ is the solution of the Lyapunov equation Pâˆ’ = F Pâˆ’ F  + Qbb Q , and therefore, since TÎ½ = F TÎ½ F  + bb and QF = F Q and consequently
QTÎ½ Q = F QTÎ½ Q F  + Qbb Q ,
we have
Pâˆ’ = QTÎ½ Q .
To determine Q, notice that âˆ’Ï•Î½ = h Q and QF
ï£¹ ï£®
ï£®
h
âˆ’Ï•Î½

ï£¯ âˆ’Ï•Î½ F ï£º ï£¯ h F
ï£º=ï£¯ .
ï£¯
..
ï£» ï£° ..
ï£°
.
âˆ’Ï•Î½ F Î½âˆ’1

(4.20)
= F Q to form
ï£¹
ï£º
ï£º Q = Q.
ï£»

(4.21)

h F Î½âˆ’1

Next, deï¬ne the symmetric matrix
M := RÎ½âˆ’1/2 Î¦Î½ QTÎ½ Q Î¦Î½ RÎ½âˆ’1/2 .

(4.22)

14

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

In view of (4.20) and (4.17), det(zI âˆ’ M ) = det(zI âˆ’ Pâˆ’ PÌ„+ ), and hence, by (4.8), M
has the eigenvalues Ïƒ12 , Ïƒ22 , . . . , ÏƒÎ½2 , and the singular-value decomposition
M = U Î£2 U  ,

(4.23)

where U  U = U U  = I. It is then well-known and simple to check that
S := Î£âˆ’1/2 U  RÎ½âˆ’1/2 Î¦Î½

(4.24)

is the required balancing transformation (4.5) such that SPâˆ’ S  = (S  )âˆ’1 PÌ„+ S âˆ’1 = Î£.
Proposition 4.2. Given the partial covariance sequence
ck = E{y(t + k)y(t)},

k = 0, 1, . . . , Î½,

let Ï•1 (z), Ï•2 (z), . . . , Ï•Î½ (z) and r0 , r1 , . . . , rÎ½ be the corresponding SzegoÌˆ polynomials
and error variances. Supposing that Î³Î½âˆ’1 = âˆ’Ï•Î½ (0) = 0, let (F, b, h) be given by
(4.15), RÎ½ and Î¦Î½ by (4.16) and Q by (4.21). Moreover, let U and Î£ be deï¬ned by
the singular value decomposition (4.23) of (4.22). Then, the canonical correlation
coeï¬ƒcients Ïƒ1 , Ïƒ2 , . . . , ÏƒÎ½ are the diagonal elements of Î£, as described in (4.7), and
the stochastically balanced realization of WÎ½ is given by
âˆš
(4.25)
(A, B, C, D) = (SF S âˆ’1 , SQb, h S âˆ’1 , rÎ½ ),
where S is deï¬ned by (4.24).
Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to
yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase
property. In fact, we have the following result, the proof of which is given in Section A.
Theorem 4.3. Let Wred be the SBT approximation of degree r of WÎ½ , and set

Î½
Î½âˆ’1
	
âˆš  1 + |Î³k |
Ïƒk
9 := 2
and Îº := c0
,
(4.26)
1
âˆ’
Ïƒ
1
âˆ’
|Î³
k
k|
k=r+1
k=0
where Î³0 , Î³1 , . . . , Î³Î½âˆ’1 are the Schur parameters of c0 , c1 , c2 , . . . , cÎ½ . Then
c0 (1 âˆ’ 9)Îºâˆ’1 â‰¤ |Wred (eiÎ¸ )| â‰¤ (1 + 9)Îº for all Î¸,

(4.27)

and, if 9 < 1, Wred is minimum phase. Finally, the approximation error has the bound
WÎ½ âˆ’ Wred âˆ â‰¤ 9Îº.

(4.28)

A properly executed SBT procedure should imply that the canonical correlation
coeï¬ƒcients Ïƒr+1 , . . . , ÏƒÎ½ , and hence 9, are close to zero, insuring the minimum-phase
condition.
Remark 4.4. Stochastic model reduction can also be carried out by instead performing principal subsystem truncation on (A, C, CÌ„) in VÎ½ (z) = C(zI âˆ’ A)âˆ’1 CÌ„ + 12 c0 ,
where A and C are given by (4.25) and CÌ„  = S(c1 , c2 , . . . , cn ). It was shown in
[32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally,
the spectral density Î¦red (z) := Vred (z) + Vred (z âˆ’1 ) is factorized to yield a minimumphase spectral factor WÌƒ . This is in a sense a more natural procedure, but we do
not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small Î£2 it yields almost the same result. In fact, it is shown in [53], that
|WÌƒ (eiÎ¸ )|2 = |Wred (eiÎ¸ )|2 + H(eiÎ¸ )Î£2 H(eâˆ’iÎ¸ ), where H(z) = C1 (zI âˆ’ A11 )âˆ’1 A12 .
Remark 4.5. There are good reasons to prefer stochastic over deterministic model
reduction, as seen from the following heuristics. In fact, it can be seen that
VÎ½ (z) =

c0 ÏˆÎ½ (z)
,
2 Ï•Î½ (z)

(4.29)

where ÏˆÎ½ (z) is the SzegoÌˆ polynomial of the second kind (obtained by exchanging âˆ’Î³t
for Î³t in the recursion (3.4)). Now, the matrix representation of the Hankel operator
H in the innovation bases of the past and the future, provided by wâˆ’ and wÌ„+ respecis the inï¬nite Hankel matrix of the sequence
tively, is given by Lâˆ’1 (Lâˆ’1 ) , where
c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix Tâˆ ;
see, e.g., [32, p. 714]. It is easy to see that ÏˆÎ½ (z) has the same asymptotic behavior as
Ï•Î½ (z), i.e., the roots tend to cluster uniformly inside the circle z = Ï as Î½ â†’ âˆ, and
hence these roots are close to canceling in (4.29). Consequently, the corresponding
Hankel matrix is close to having low rank. This massive â€œalmost cancellationâ€ does
not occur in WÎ½ (z), and hence the corresponding inï¬nite Hankel matrix, constructed
from the Laurent coeï¬ƒcients of WÎ½ (z), may have a less distinct separation between
Î£1 and Î£2 . On the other hand, since the Schur parameters tend geometrically to
zero, the lower part of L tends to the identity, and hence the asymptotic behavior of
the canonical correlation coeï¬ƒcients is very much like that of the singular values of
. Therefore we may expect SBT to have better statistical behavior than DBT. In
Section 6 we shall see that this is the case.

H

H

H

H

5. Identiï¬cation from statistical data
We now return to our original problem of time series identiï¬cation: Given a data
string (2.2) of observations of the output process y of some n-dimensional linear
stochastic system (2.1) with minimum-phase transfer function W (z), given by (2.8),
ï¬nd an estimate (AÌ‚, BÌ‚, CÌ‚, DÌ‚) of the matrices (A, B, C, D).
The identiï¬cation method proceeds as follows. Given the covariance estimates (2.5),
we compute the corresponding maximum entropy ï¬lter (2.7), a balanced realization
(4.25), and the canonical correlation coeï¬ƒcients
ÏƒÌ‚1 , ÏƒÌ‚2 , ÏƒÌ‚3 , . . . , ÏƒÌ‚Î½ ,

(5.1)

determined as in Proposition 4.2 from the covariance estimates cÌ‚0 , cÌ‚1 , . . . , cÌ‚Î½ .
Based on (5.1), choose an integer nÌ‚ such that ÏƒÌ‚nÌ‚+1 , ÏƒÌ‚nÌ‚+2 , . . . , ÏƒÌ‚Î½ are close to zero or
at least distinctively smaller than ÏƒÌ‚1 , ÏƒÌ‚2 , . . . , ÏƒÌ‚nÌ‚ . Then, the balanced realization (4.25)
is truncated accordingly as in (4.11) to yield a nÌ‚-dimensional triplet (A11 , B1 , C1 ) and
a transfer function
WÌ‚ (z) = C1 (zI âˆ’ A11 )âˆ’1 B1 + D.

(5.2)

Then, (A11 , B1 , C1 , D) is the required estimate (AÌ‚, BÌ‚, CÌ‚, DÌ‚).
As pointed out in Section 2, we have a bound
W âˆ’ WÌ‚ âˆ â‰¤ W âˆ’ WÎ½ âˆ + WÎ½ âˆ’ WÌ‚Î½ âˆ + WÌ‚Î½ âˆ’ WÌ‚ âˆ ,

(5.3)

16

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the ï¬rst term W âˆ’ WÎ½ âˆ , which
does not depend on the statistical data (2.2) but only on the underlying system (2.1),
tends to zero geometrically with a rate Î³ âˆˆ (0, 1) as Î½ â†’ âˆ. The other two terms
depend on the data (2.2), and here N must grow at a faster rate than Î½. In fact, we
shall assume that
Î½ = Î½(N ) = O(log N ),

(5.4)

which in particular requires that limN â†’âˆ NÎ½ = 0. We also need to assume that the
white noise process in (2.1) satisï¬es a mild technical condition, namely
E{w(t)4 } < âˆ.

(5.5)

This condition is, of course, satisï¬ed if w is Gaussian.
Next, we present our main convergence theorem.
Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then,
to each length N of the data string (2.2), there is a Î½(N ), tending to inï¬nity with N
at the rate (5.4), such that any sequence of estimated transfer functions WÌ‚ of ï¬xed
degree nÌ‚ â‰¥ n, determined, for each N and corresponding Î½ = Î½(N ), by the procedure
described above, satisï¬es
W âˆ’ WÌ‚ âˆ â†’ 0
almost surely as Î½(N ) â†’ âˆ. For suï¬ƒciently large Î½(N ), the transfer function WÌ‚ has
minimum phase.
We have already proven that the ï¬rst term in (5.3) tends to zero, so Theorem 5.1
follows from the next two theorems, each corresponding to one of the remaining terms
in (5.3). As for the second term, we have the following result, the proof of which is
deferred to Appendix B.
Theorem 5.2. Suppose the system (2.1) satisï¬es the conditions of Theorem 5.1. Let
WÎ½ be the maximum-entropy ï¬lter (3.6) determined from the partial covariance sequence (3.1) of y and let WÌ‚Î½ be the corresponding function determined from the ergodic
estimates (2.5). Then, if Î½(N ) is deï¬ned as in Theorem 5.1,
WÎ½(N ) âˆ’ WÌ‚Î½(N ) âˆ â†’ 0
almost surely as Î½(N ) â†’ âˆ.
There are several results of this type in the literature [2, 36, 7, 33]. In particular,
3
Berk [2] proved that, provided Î½N â†’ 0 as N â†’ âˆ and Î¦ is coercive (i.e. positive
 iÎ¸ ) â†’ Î¦(eiÎ¸ ) in probability.
on the unit circle), the estimated AR spectral density Î¦(e
Under the same hypotheses, Caines and Baykal-GuÌˆrsoy [7] showed that if N â‰¥ Î½ 5+Î·
for some Î· > 0, then WÌ‚Î½âˆ’1 âˆ’ W âˆ’1 âˆ â†’ 0 almost surely as Î½ â†’ âˆ. However, in both
cases, ergodic estimates are used which are not quite the same as (2.5).
Finally, we consider the last term in (5.3). The proof of the following theorem is
given in Appendix B.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function Î½(N ) are deï¬ned as in
Theorem 5.1. Moreover, for each N , let WÌ‚Î½(N ) be deï¬ned as in Theorem 5.2 and WÌ‚
as in Theorem 5.1. Then, for suï¬ƒciently large Î½(N ), WÌ‚ has minimum phase, and
WÌ‚Î½(N ) âˆ’ WÌ‚ âˆ â†’ 0
almost surely as Î½(N ) â†’ âˆ.
6. Simulations
Performing model reduction on WÌ‚Î½ , rather than on the maximum-entropy ï¬lter of
exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier
and more accurate order determination, as the heuristics of Remark 4.5 suggest.
There are also alternative order determination statistical tests based on the canonical
correlation coeï¬ƒcients [17, 26, 46]. But, even more importantly, there is less bias,
and the error variances are closer to the CrameÌr-Rao bound.
Since we are approximating rational models with AR models the method will be
biased for ï¬nite amount of data, unless the model generating the data really is an
AR model. The consistency result given in Theorem 5.1 implies that the method is
asymptotically unbiased and therefore we consider the CrameÌr-Rao bound for unbiased methods; see [44, pp. 137â€“138]. The CrameÌr-Rao bound for biased estimation
requires knowledge about the bias as a function of the parameter to be estimated.
As already mentioned, the method will be unbiased and even statistically eï¬ƒcient for
Gaussian AR processes if the model reduction step is omitted. Despite the fact that
an algorithm based on covariance estimates (2.6) is not asymptotically eï¬ƒcient for
general ARMA models [44, p. 144], our method can be used to provide a starting
guess for other algorithms, for example the maximum likelihood method.
6
SBT dashed line, DBT dotted line.
5

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identiï¬cation estimates.

To illustrate our procedure, let us consider data generated by passing white noise
through a â€œtrue systemâ€ with transfer function
W (z) =

z 5 âˆ’ 0.0550z 4 âˆ’ 0.1497z 3 âˆ’ 0.2159z 2 + 0.1717z âˆ’ 0.0495
.
z 5 âˆ’ 0.7031z 4 + 0.3029z 3 + 0.1103z 2 âˆ’ 0.1461z + 0.2845

18

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy
model WÌ‚Î½ of degree Î½ = 24 determined from estimated covariances. Based on 100
test runs, the empirical means and standard deviations are determined. Figure 6.1
illustrates the statistical bias as a function of the length N of the data string when
using stochastic (dashed curve) and deterministic (dotted curve) model reduction
respectively.
For the same test runs, Figure 6.2 illustrates the corresponding standard deviations
together with the CrameÌr-Rao bound (solid curve). More precisely, the ï¬gures depict
the sums of the moduli of the biases and standard deviations respectively for the
coeï¬ƒcients of the numerator and denominator polynomials of WÌ‚ (z).
5
CRB solid line, SBT dashed line, DBT dotted line.
4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the CrameÌr-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z)
with poles and zeros closer to the unit circle is considered. The poles and zeros of
WÌ‚ (z) are determined for 100 runs and a data length N = 500. As before, Î½ = 24.
Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with
the poles and zeros of W (z), which are denoted by â€œâ—¦â€.

1

1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

âˆ’0.5

0
zeros

0.5

1

âˆ’1
âˆ’1

âˆ’0.5

0
poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and Î½ = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT.
To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in
Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

âˆ’0.5

0
zeros

0.5

âˆ’1
âˆ’1

1

âˆ’0.5

0
poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and Î½ = 24.

1

1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

âˆ’0.5

0
zeros

0.5

1

âˆ’1
âˆ’1

âˆ’0.5

0
poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identiï¬cation.

Figure 6.5 describes the result obtained when applying stochastic subspace identiï¬cation to the same data. More precisely, Algorithm # 2 in [43] is used. In order
to make the experiments comparable, we have chosen a Hankel matrix of dimension
13 Ã— 13, which corresponds to Î½ = 25 in our procedure.
Note that the estimates are much less focused, and many zeros tend to cluster on the
unit circle, implying that coercivity becomes critical. This is related to the positivity
issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the
subspace identiï¬cation method performs worse than our SBT identiï¬cation method,
yielding larger biases and standard deviations, but performs better than when DBT
is used.

20

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as
illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will
disappear as Î½ and N are increased. In Figure 6.6 we show the same experiment for
Î½ = 64 and N = 2000.

1

1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

âˆ’0.5

0
zeros

0.5

âˆ’1
âˆ’1

1

âˆ’0.5

0
poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and Î½ = 64.

1

1

0.5

0.5

0

0

âˆ’0.5

âˆ’0.5

âˆ’1
âˆ’1

âˆ’0.5

0
zeros

0.5

âˆ’1
âˆ’1

1

âˆ’0.5

0
poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and Î½ = 40 using Burgâ€™s method.

In practice, there is a trade-oï¬€ between the quality of the ergodic estimates, which
roughly speaking depend on |Î»max (A)|, the âˆ -error tolerance, which is a function of
|Î»max (A âˆ’ BDâˆ’1 C)|, and the numerical accuracy of the computations. For example,
if the zeros of W (z) are far from the unit circle and Î½ is chosen very large, the error
may increase.
In the present example, it turns out that using Burgâ€™s method [3] in lieu of the
ergodic estimate (2.6) yields better estimates for smaller Î½ and N , as illustrated in
Figure 6.7 which shows the case N = 500 and Î½ = 40.
A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There
we give the empirical bias and standard deviation for the coeï¬ƒcients of the numerator
and the denominator, respectively, of the estimated transfer functions together with
the CrameÌr-Rao bound. It is the authors experience that Burgâ€™s method gives at
least as good results as when using the ergodic covariance estimate (2.6), unless the
intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

Parameter
True value
Bias:
CE:
Burg:
Std.dev.:
CE:
Burg:
CRB:

21

Ïƒw2
b1
b2
b3
b4
b5
1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491
0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895
0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734
0.2332 0.1314 0.0508 0.0611 0.0722 0.0802
0.0712 0.0411 0.0381 0.0339 0.0339 0.0356
0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and Î½ = 40 using covariance estimation (CE) or Burg estimation and , in
both cases, followed by SBT.

Parameter
a1
a2
a3
a4
a5
True value
-0.6281 0.3597 0.2634 -0.5322 0.7900
Bias:
CE:
0.0087 -0.0044 -0.0003 0.0066 -0.0152
Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125
Std.dev.:
CE:
0.0274 0.0304 0.0371 0.0305 0.0304
Burg: 0.0336 0.0307 0.0358 0.0324 0.0306
0.0293 0.0321 0.0342 0.0322 0.0290
CRB:
Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and Î½ = 40 using covariance estimation (CE) or Burg estimation and,
in both cases, followed by SBT.

7. Conclusions
We have presented a three-step procedure for identiï¬cation of time series, which is easy
to understand and implement. Just like for subspace identiï¬cation methods, robust
linear-algebra algorithms can be used and no nonconvex optimization computations
are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identiï¬cation, as our extensive simulations indicate.
In particular, its good performance has been conï¬rmed by Monte Carlo simulations.
The paper only covers the scalar case, but the multivariate case is presently being
worked out.
The three steps, covariance estimation, covariance extension and model reduction
have each been studied separately before. This is an advantage which should make the
method easy to grasp. However, a comprehensive study of the entire identiï¬cation
strategy, giving appropriate bounds, has been missing and this is what we oï¬€ered
here.
The observation that the Schur parameters converge geometrically simpliï¬es our
application of SzegoÌˆ theory and allows us to give a complete account of the asymptotic
behavior of maximum entropy models of growing order. This analysis provides us with
a clear indication as to when the identiï¬cation strategy is good and when it might face
diï¬ƒculties, based purely on the closeness of the maximum modulus zero to the unit
circle. The parsimony permeating other system identiï¬cation methods should not be
a reason for refraining from high-order modeling as an intermediate step. In fact,
such a strategy might be desirable, since we have shown that the poles of the â€œtrueâ€
system which lie outside a circle in the complex plane containing all of its zeros are

22

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the
perimeter of this circle, providing a justiï¬cation for choosing stochastically balanced
model reduction, rather than deterministically balanced truncation, in the last step.
With this reduction procedure, we have conï¬rmed better statistical properties with
variances closer to the CrameÌr Rao bound. The procedure could also be modiï¬ed by
exchanging exact covariance extension for approximate one, as outlined in [35].
Even though, in general, stochastic balancing would require the solution of a pair
of Riccati equations, this is not the case for the particular maximum entropy models
used here. In fact, the balancing procedure only requires linear algebra, and hence
an intelligent use of the Levinson algorithm may substantially reduce the number of
arithmetic operations.
Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N , Î½ and nÌ‚, we gave worst-case guaranteed
bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive
stochastic system, but the method returns a valid model also for generic data. In fact,
in contrast to many stochastic subspace identiï¬cation [9], all steps of the procedure
preserve the positive real property.
Appendix A. Asymptotic behavior of the maximum entropy ï¬lter
Theorem 3.4 is actually a modiï¬cation to the rational setting of a theorem due to
SzegoÌˆ [47], and the proof is modeled after [19], which in turn includes aspects already
present in the work of Schur [45]. See also [48], [49] and [16] for more facts on
orthogonal polynomials. However, rationality and coercivity allows us to present a
simpliï¬ed and self-contained proof of a version of SzegoÌˆâ€™s classical theorem, to which
we also are able to add geometric convergence. The derivation of Caines and BaykalGuÌˆrsoy [7] is shorter, but we feel that our approach is more systematic and gives
additional insight into the mechanism of identiï¬cation.
To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas.
âˆ’Î½
Ï†Î½ (z)|
Lemma A.1. Let {Ï†Î½ (z)}âˆ
0 be the normalized SzegoÌˆ polynomials (3.7). Then |z
c
is uniformly bounded from above and away from zero in the complement D of the open
unit disc, i.e., there are positive numbers Î±, Î² âˆˆ R such that

Î± â‰¤ |z âˆ’Î½ Ï†Î½ (z)| â‰¤ Î²
for all Î½ and all z âˆˆ Dc .
Proof. In view of the SzegoÌˆ-Levinson recursion (3.4),



Ï•âˆ—t (z)
Ï•t+1 (z) = Ï•t (z) z âˆ’ Î³Ì„t
,
Ï•t (z)
and hence
z

âˆ’Î½

Ï•Î½ (z) =

Î½âˆ’1


k=0


Ï•âˆ—k (z)
1 âˆ’ z Î³Ì„k
.
Ï•k (z)
âˆ’1

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , zÎ½ are the roots of Ï•Î½ (z), it is immediately seen that
Ï•âˆ—Î½ (z)  1 âˆ’ z zÌ„k
,
=
Ï•Î½ (z) k=1 z âˆ’ zk
Î½

which is a Blaschke product, analytic in Dc and having modulus one on the unit circle,
and thus modulus less than or equal to one in Dc . Hence, since |z âˆ’1 | â‰¤ 1 in Dc ,
Î½âˆ’1


(1 âˆ’ |Î³k |) â‰¤ |z

âˆ’Î½

Ï•Î½ (z)| â‰¤

k=0

Î½âˆ’1


(1 + |Î³k |)

(A.1)

k=0

for all z âˆˆ Dc . But, these products converge to positivenumbers as Î½ â†’ âˆ. This
follows from the absolute convergence of the inï¬nite sum âˆ
k=0 |Î³k |, a fact that, in the
present context, stems from Lemma 3.1. From (3.5) we also have 0 < râˆ â‰¤ rÎ½ â‰¤ r0 ,
and consequently the lemma follows.
Remark A.2. An equivalent statement of this lemma is that the maximum entropy
solution WÎ½ (z), deï¬ned by (3.6), is uniformly bounded from above and away from
zero for all Î½ and z âˆˆ Dc .
Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of
functions
fÎ½ (z) := z âˆ’Î½ Ï†Î½ (z)
converges uniformly to an analytic function fâˆ in
statement of Theorem 3.5.

DcÏ,

where

DcÏ

is deï¬ned in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the
purely algebraic relation
Î½
	

Ï†k (z)Ï†k (w) =

k=0

Ï†âˆ—Î½ (z)Ï†âˆ—Î½ (w) âˆ’ z wÌ„Ï†Î½ (z)Ï†Î½ (w)
,
1 âˆ’ z wÌ„

(A.2)

which is called the Christoï¬€el-Darboux-SzegoÌˆ formula. In particular, setting w = 0
and exchanging z for z âˆ’1 in (A.2), (3.5) and (3.7) yield
fÎ½ (z)
1 	
Î³kâˆ’1
âˆ’
Ï†k (z âˆ’1 ) âˆš .
âˆš =
rÎ½
c0 k=1
rk
Î½

(A.3)

Observe that Ï†k (z âˆ’1 ) is analytic and bounded in DcÏ , and hence in Dc , and therefore it
âˆ
. Moreover, by the maximum modulus principle, it attains its maximum
belongs to âˆ’
c
value in D on the unit circle where, by Lemma A.1, it is bounded by Î². Hence

H

|Ï†k (z âˆ’1 )| â‰¤ Î²

for z âˆˆ Dc and for all k.

Therefore, in view of (A.3) and the fact that rk â‰¥ râˆ , we have


Î½
	
 fÎ½ (z) fÂµ (z) 
 âˆš âˆ’ âˆš  â‰¤ âˆšÎ²
|Î³kâˆ’1 |,
 rÎ½
rÂµ 
râˆ k=Âµ+1

(A.4)

(A.5)

24

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for suï¬ƒciently large Î½ and Âµ.
âˆ
. The same holds for fÎ½ (z). In
This establishes (A.3) as a Cauchy sequence in âˆ’
c
fact, since rÎ½ â‰¤ c0 , for all z âˆˆ D


âˆš  fÎ½ (z) fÂµ (z) 
|fÎ½ (z) âˆ’ fÂµ (z)| â‰¤ c0  âˆš âˆ’ âˆš 
rÎ½
rÎ½




 1
âˆš  fÎ½ (z) fÂµ (z) 
1 

â‰¤ c0  âˆš âˆ’ âˆš  + |fÂµ (z)|  âˆš âˆ’ âˆš  .
(A.6)
rÎ½
rÂµ
rÎ½
rÂµ

H

But, by Lemma A.1, |fÂµ (z)| â‰¤ Î² for all Î½ and z âˆˆ Dc , and therefore, in view of (A.5),
we obtain



Î½
 1

c0 	
1
|Î³kâˆ’1 | + Î²  âˆš âˆ’ âˆš  for all z âˆˆ Dc . (A.7)
|fÎ½ (z) âˆ’ fÂµ (z)| â‰¤ Î²
râˆ k=Âµ+1
rÎ½
rÂµ
Since rÎ½ â†’ râˆ as Î½ â†’ âˆ, we see that, for each 9 > 0, |fÎ½ (z) âˆ’ fÂµ (z)| < 9 for
suï¬ƒciently large Î½ and Âµ. Consequently, fÎ½ tends uniformly in Dc to a function
âˆ
.
fâˆ âˆˆ Hâˆ’
The uniform convergence and the analyticity can be extended to any compact
subset of DcÏ . To see this, ï¬rst note that z âˆˆ D if and only if z âˆ’1 âˆˆ Dc . Therefore, by
Lemma A.1,
|Ï†k (z âˆ’1 )| â‰¤ Î²|z|âˆ’k for z âˆˆ D,
and consequently, since rÎ½ â‰¤ rk , (A.3) yields
Î½âˆ’1
	
1
|fÎ½ (z)| â‰¤ âˆš + Î²|z|âˆ’1
|Î³k ||z|âˆ’k .
c0
k=0

Similarly, instead of (A.5) we have


Î½âˆ’1
	
 fÎ½ (z) fÂµ (z) 
 âˆš âˆ’ âˆš  â‰¤ âˆšÎ² |z|âˆ’1
|Î³k ||z|âˆ’k .
 rÎ½
rÂµ 
râˆ
k=Âµ

(A.8)

(A.9)

Now, for any compact subset K âˆˆ DcÏ , there is a Î³ âˆˆ (Ï, 1) and an 9 > 0 such
that |z| > Î³ + 9 for all z âˆˆ K. Hence, by Lemma 3.1, |Î³k ||z|âˆ’k â‰¤ M Î³Ì‚ k where
Î³Ì‚ := Î³(Î³ + 9)âˆ’1 < 1. Consequently, by (A.8), fÎ½ (z) is uniformly bounded in K, and
(A.9) can be made arbitrarily small for suï¬ƒciently large Î½ and Âµ. Therefore, by (A.6),
fÎ½ tends uniformly in K to the analytic function fâˆ .
Lemma A.4. Let Î³ be a real number such that Ï < Î³ < 1. Then fÎ½ âˆ’fâˆ âˆ = O(Î³ Î½ ).
Proof. It follows from (A.7) that


 
âˆ

âˆš 	
Î²
râˆ 
c0
|Î³kâˆ’1 | + 1 âˆ’
|fÎ½ (z) âˆ’ fâˆ (z)| â‰¤ âˆš
râˆ
rÎ½ 
k=Î½+1

for all z âˆˆ Dc .
(A.10)

By Lemma 3.1, the ï¬rst term is O(Î³ Î½ ). It remains to show that the same holds for
the second term. To this end, ï¬rst note that, by (3.5),

âˆ 

râˆ
=1âˆ’
1 âˆ’ Î³k2 .
1âˆ’
rÎ½
k=Î½

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

But, by Lemma 3.1, |Î³k | â‰¤ M Î³ k for some M . Therefore, since
each x âˆˆ [0, 1],

âˆ

râˆ
â‰¤ 1 âˆ’ (1 âˆ’ M Î³ k ) = O(Î³ t )
1âˆ’
rt
k=t

âˆš

25

1 âˆ’ x2 â‰¥ 1 âˆ’ x for

for t large enough. This concludes the proof.
Recalling the deï¬nition (3.6) of WÎ½ , we note that Lemma A.4 may be written
WÎ½âˆ’1 âˆ’ fâˆ âˆ = O(Î³ Î½ ).
âˆ’1
in the same manner.
As it turns out, by coercivity, this implies that WÎ½ â†’ fâˆ

Lemma A.5. Let WÎ½ be the transfer function (3.6) of the maximum entropy ï¬lter.
Then
âˆ’1
âˆ = O(Î³ Î½ ),
WÎ½ âˆ’ fâˆ
where fâˆ is the limit function of Lemma A.3.
Proof. Note that the limit function fâˆ has the same uniform bounds as fÎ½ in Lemma
A.1. In particular, |fâˆ (z)| â‰¥ Î±, |fâˆ (z)|âˆ’1 â‰¤ Î±âˆ’1 , and |WÎ½ (z)| â‰¤ Î±âˆ’1 for all z âˆˆ Dc .
Consequently,
âˆ’1
âˆ’1
WÎ½ âˆ’ fâˆ
âˆ â‰¤ WÎ½ âˆ fâˆ
âˆ WÎ½âˆ’1 âˆ’ fâˆ âˆ â‰¤ Î±âˆ’2 WÎ½âˆ’1 âˆ’ fâˆ âˆ ,

so the required result follows from Lemma A.4.
Lemma A.6. Let W be the rational minimum-phase function deï¬ned above, and let
fâˆ be the limit function in Lemma A.3. Then W (z) = fâˆ (z)âˆ’1 for all z âˆˆ DcÏ .
Proof. Let Î¦Î½ (eiÎ¸ ) := |WÎ½ (eiÎ¸ )|2 be the spectral density of the maximum entropy
process. Then, in view of the interpolation condition,
 Ï€
 Ï€
1
1
ikÎ¸
iÎ¸
e Î¦(e )dÎ¸ = ck =
eikÎ¸ Î¦Î½ (eiÎ¸ )dÎ¸ for k = 0, 1, . . . , Î½, (A.11)
2Ï€ âˆ’Ï€
2Ï€ âˆ’Ï€
from which we have pointwise convergence of the Fourier coeï¬ƒcients of Î¦Î½ (eiÎ¸ ) to
those of Î¦(eiÎ¸ ) as Î½ â†’ âˆ, and hence Î¦Î½ (eiÎ¸ ) â†’ Î¦(eiÎ¸ ) in the 2 sense. However, by
Lemma A.5, Î¦Î½ (eiÎ¸ ) â†’ |fâˆ (eiÎ¸ )|âˆ’2 in âˆ norm, and hence a fortiori in 2 norm, as
Î½ â†’ âˆ. Since, in addition, not only Î¦(eiÎ¸ ) but also fâˆ is analytic in a neighborhood
of the unit circle (Lemma A.3), we have

L

L

L

Î¦(eiÎ¸ ) = |fâˆ (eiÎ¸ )|âˆ’2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer.
In particular, WÎ½ is an outer spectral factor of Î¦Î½ (eiÎ¸ ) satisfying

  Ï€ it

e +z
1
it 2
log |WÎ½ (e )| dt .
WÎ½ (z) = exp
4Ï€ âˆ’Ï€ eit âˆ’ z
But Lemma A.5, Equation (A.12) and the fact that Î¦(eiÎ¸ ) = |W (eiÎ¸ )|2 ,

  Ï€ it

1
e +z
it 2
log |W (e )| dt = W (z),
WÎ½ (z) â†’ exp
4Ï€ âˆ’Ï€ eit âˆ’ z
the outer spectral factor of Î¦. But, by Lemma A.3, WÎ½ (z) â†’ fâˆ (z)âˆ’1 in
therefore fâˆ (z) = W âˆ’1 (z) as claimed.

DcÏ, and

26

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6.
Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6.
Proof of Theorem 4.3. Following [53] we see that
WÎ½âˆ’1 (WÎ½ âˆ’ Wred )âˆ â‰¤ 9,

(A.13)

and consequently
|WÎ½ (eiÎ¸ ) âˆ’ Wred (eiÎ¸ )| â‰¤ 9|WÎ½ (eiÎ¸ )|
holds for all Î¸, from which we have
(1 âˆ’ 9)|WÎ½ (eiÎ¸ )| â‰¤ |Wred (eiÎ¸ )| â‰¤ (1 + 9)|WÎ½ (eiÎ¸ )|.
However, in view of (3.6) and (3.7), it follows from (A.1) that
âˆš
âˆš
rÎ½
rÎ½
iÎ¸
â‰¤ |WÎ½ (e )| â‰¤ Î½âˆ’1
,
Î½âˆ’1
k=0 (1 + |Î³k |)
k=0 (1 âˆ’ |Î³k |)
which together with (3.5) yields
c0
â‰¤ |WÎ½ (eiÎ¸ )| â‰¤ Îº
Îº

(A.14)

for all Î¸. This establishes (4.27). To see that Wred is minimum phase if 9 < 1, note
that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by RoucheÌâ€™s
Theorem, Wred has the same number of zeros in Dc (including âˆ) as WÎ½ . Hence,
since WÎ½ is minimum phase, so is Wred .
To establish the bound (4.28) note that
WÎ½ âˆ’ Wred âˆ â‰¤ WÎ½ âˆ WÎ½âˆ’1 (WÎ½ âˆ’ Wred )âˆ .
From (A.14) we have WÎ½ âˆ â‰¤ Îº, and hence (4.28) follows from (A.13).
Appendix B. Statistical convergence proofs
Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corresponding SzegoÌˆ polynomial Ï•Ì‚Î½ (z) and predictor error variance rÌ‚Î½ from (3.4) and (3.5),
and form the maximum-entropy ï¬lter
âˆš Î½
rÌ‚Î½ z
.
WÌ‚Î½ (z) =
Ï•Ì‚Î½ (z)
To determine WÎ½ âˆ’ WÌ‚Î½ âˆ let z âˆˆ Dc and form
âˆš Î½
âˆš Î½
rÎ½ z
rÌ‚Î½ z
WÎ½ (z) âˆ’ WÌ‚Î½ (z) =
âˆ’
Ï•Î½ (z)
Ï•Ì‚Î½ (z)
âˆš
âˆš
âˆš
( rÎ½ âˆ’ rÌ‚Î½ )z âˆ’Î½ Ï•Î½ (z) âˆ’ rÎ½ z âˆ’Î½ (Ï•Î½ (z) âˆ’ Ï•Ì‚Î½ (z))
.
=
z âˆ’Î½ Ï•Î½ (z)z âˆ’Î½ Ï•Ì‚Î½ (z)
Since râˆ > 0, by (3.7) and Lemma A.1,
âˆš
âˆš
0 < Âµ := râˆ Î± â‰¤ |z âˆ’Î½ Ï•Î½ (z)| â‰¤ c0 Î² =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1),
|z

âˆ’Î½

Ï•Ì‚Î½ (z)| â‰¥ ÂµÌ‚Î½ :=

Î½âˆ’1


(1 âˆ’ |Î³Ì‚k |),

k=0

where Î³Ì‚0 , Î³Ì‚1 , . . . , Î³Ì‚Î½âˆ’1 are the Schur parameters corresponding to the estimated covariances (2.6). Therefore, by the maximum-modulus principle,

âˆš
âˆš
1
{M | rÎ½ âˆ’ rÌ‚Î½ | + c0 |Ï•Î½ (z) âˆ’ Ï•Ì‚Î½ (z)|},
|WÎ½ (z) âˆ’ WÌ‚Î½ (z)| â‰¤ max
|z|=1 ÂµÂµÌ‚Î½
where we have also used the fact that rÎ½ â‰¤ c0 . But, for |z| = 1,
|Ï•Î½ (z) âˆ’ Ï•Ì‚Î½ (z)| â‰¤ Ï•Î½ âˆ’ Ï•Ì‚Î½ 1 ,
where Ï•Î½ and Ï•Ì‚Î½ are the Î½-vectors formed as in (4.19) and Â·1 is the B1 norm. Recall
that Ï•Î½ is the unique solution of the normal equations


TÎ½ Ï•Î½ = âˆ’cÎ½ where cÎ½ := cÎ½ cÎ½âˆ’1 . . . c1 ,
(B.1)
where TÎ½ is the Toeplitz matrix deï¬ned by (2.4), and that
rÎ½ = c0 + cÎ½ Ï•Î½ .

(B.2)

Also, the analogous relations hold for Ï•Ì‚Î½ and rÌ‚Î½ . Then,
rÎ½ âˆ’ rÌ‚Î½ = (c0 âˆ’ cÌ‚0 ) + (cÎ½ âˆ’ cÌ‚Î½ ) Ï•Î½ + cÌ‚Î½ (Ï•Î½ âˆ’ Ï•Ì‚Î½ )
and hence
|rÎ½ âˆ’ rÌ‚Î½ | â‰¤ |c0 âˆ’ cÌ‚0 | + cÎ½ âˆ’ cÌ‚Î½ 1 Ï•Î½ âˆ + cÌ‚Î½ âˆ Ï•Î½ âˆ’ Ï•Ì‚Î½ 1 .
Finally,


âˆš
|rÎ½ âˆ’ rÌ‚Î½ |
|r âˆ’ rÌ‚ |
âˆš â‰¤ Î½âˆš Î½ ,
| rÎ½ âˆ’ rÌ‚Î½ | â‰¤ âˆš
râˆ
rÎ½ + rÌ‚Î½

and consequently, since x1 â‰¤ Î½xâˆ for any x âˆˆ RÎ½ ,

M
âˆš {|c0 âˆ’ cÌ‚0 | + Ï•Î½ âˆ Î½cÎ½ âˆ’ cÌ‚Î½ âˆ }
ÂµÂµÌ‚Î½ râˆ



M cÌ‚Î½ âˆ
1 âˆš
c0 + âˆš
+
Î½Ï•Î½ âˆ’ Ï•Ì‚Î½ âˆ .
ÂµÂµÌ‚Î½
râˆ

WÎ½ âˆ’ WÌ‚Î½ âˆ â‰¤

(B.3)

Recall now that Ï•Î½ and Ï•Ì‚Î½ are each solutions of a normal equation (B.1). More
precisely, TÎ½ Ï•Î½ = âˆ’cÎ½ and TÌ‚Î½ Ï•Ì‚Î½ = âˆ’cÌ‚Î½ . Since ck = CAkâˆ’1 CÌ„  for k > 0, where all
eigenvalues of A are less than one in modulus, ck â†’ 0 exponentially, we have
cÎ½ âˆ â‰¤ K1

and TÎ½ âˆ â‰¤ c0 + 2

Î½âˆ’1
	

|ck | â‰¤ K2

k=1

for some constants K1 and K2 . Moreover, from [8] we have
TÎ½âˆ’1 âˆ â‰¤

Î½âˆ’1
1  1 + |Î³k |
â‰¤ K3
c0 k=0 1 âˆ’ |Î³k |

28

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence
Ï•Î½ âˆ â‰¤ TÎ½âˆ’1 âˆ cÎ½ âˆ â‰¤ K1 K3
and the condition number
Îº(TÎ½ ) := TÎ½ âˆ TÎ½âˆ’1 âˆ â‰¤ K := K2 K3
is bounded for all Î½.
Now, it is known [25] that for each data length N in (2.2), there is a Î½(N ) of order
O(log N ) such that


log log N
,
(B.4)
max |ck âˆ’ cÌ‚k | = O
0â‰¤kâ‰¤Î½(N )
N
and therefore, for any a âˆˆ R,
Î½ a |c0 âˆ’ cÌ‚0 | â†’ 0 and Î½ a cÎ½ âˆ’ cÌ‚Î½ âˆ â†’ 0 as Î½ = Î½(N ) â†’ âˆ.

(B.5)

Consequently the ï¬rst term in the bound (B.3) tends to zero as N â†’ âˆ and
Î½(N ) â†’ âˆ provided it is done at the speciï¬ed relative rates and provided ÂµÌ‚Î½ is
bounded away from zero. However, the estimate (2.6) has the property that the
corresponding Toeplitz matrix TÌ‚Î½ is positive deï¬nite for each ï¬nite Î½, and this in turn
is equivalent to |Î³Ì‚k | < 1 for k = 0, 1, . . . , Î½ âˆ’ 1 so that ÂµÌ‚Î½ > 0. Since, in addition
ÂµÌ‚Î½ â†’ Âµ > 0 as Î½(N ) â†’ âˆ by (B.4) and continuity, the second requirement is also
fulï¬lled. To simplify notations, we have suppressed the index N in the quantities
marked with a hat, which of course depend on the data (2.2) and hence also on N .
Next we show that also the second term in (B.3) tends to zero. Since cÌ‚Î½ âˆ â‰¤
cÎ½ âˆ + cÎ½ âˆ’ cÌ‚Î½ âˆ is bounded, it thus remains to demonstrate that
Î½Ï•Î½(N ) âˆ’ Ï•Ì‚Î½(N ) âˆ â†’ 0 as Î½(N ) â†’ âˆ.
This follows from the more general fact, needed for the proof of Corollary B.1, that
Î½ a Ï•Î½(N ) âˆ’ Ï•Ì‚Î½(N ) âˆ â†’ 0 as Î½(N ) â†’ âˆ

(B.6)

for any a âˆˆ R. To prove this, ï¬rst note that
TÎ½ âˆ’ TÌ‚Î½ âˆ â‰¤ |c0 âˆ’ cÌ‚0 | + 2Î½cÎ½ âˆ’ cÌ‚Î½ âˆ ,
and hence TÎ½ âˆ’ TÌ‚Î½ âˆ â†’ 0. Therefore ÏÎ½ := TÎ½ âˆ’ TÌ‚Î½ âˆ TÎ½âˆ’1 âˆ < 1 for Î½ := Î½(N )
suï¬ƒciently large, and, provided cÎ½ = 0, the standard perturbation estimate [22] yields


1
TÎ½ âˆ’ TÌ‚Î½ âˆ cÎ½ âˆ’ cÌ‚Î½ âˆ
Ï•Î½ âˆ’ Ï•Ì‚Î½ âˆ
â‰¤
Îº(TÎ½ )
+
,
(B.7)
Ï•Î½ âˆ
1 âˆ’ ÏÎ½
TÎ½ âˆ
cÎ½ âˆ
and consequently, since TÎ½ âˆ â‰¥ c0 > 0, it follows from (B.5) that (B.6) tends to
zero in the required manner.
If cÎ½ = 0, Ï•Î½ = 0, and hence
Ï•Î½ âˆ’ Ï•Ì‚Î½ âˆ = Ï•Ì‚Î½ âˆ â‰¤ TÌ‚Î½âˆ’1 âˆ cÌ‚Î½ âˆ = TÌ‚Î½âˆ’1 âˆ cÎ½ âˆ’ cÌ‚Î½ âˆ ,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case cÎ½ = 0. In fact, since ÂµÌ‚ is
bounded away from zero, by continuity, for each 9 > 0, there is a N0 such that
TÌ‚Î½âˆ’1 âˆ

Î½âˆ’1
Î½âˆ’1
1  1 + |Î³k |
1  1 + |Î³Ë†k |
â‰¤
+ 9 â‰¤ K3 + 9
â‰¤
cÌ‚0 k=0 1 âˆ’ |Î³Ë†k |
c0 k=0 1 âˆ’ |Î³k |

for Î½ â‰¥ N0 .

Corollary B.1. If Î½(N ) is deï¬ned as in Theorem 5.1, then, for any a âˆˆ R,
Î½ a WÎ½ âˆ’ WÌ‚Î½ âˆ â†’ 0

almost surely as Î½ := Î½(N ) â†’ âˆ.

To prove Theorem 5.3, we ï¬rst note that the Hankel operator H, deï¬ned by (4.1),
has a nice representation in the space 2 of square-integrable functions. In fact, let
2
2
of functions with vanishing negative Fourier coeï¬ƒcients,
+ be the subspace in
hence being analytic in the unit disc D. In this setting, H has the representation
2
2
â†’ 2 +
given by
HÎ˜ : +

L

L

H

H

L H

HÎ˜ f = P âŠ¥ Î˜f,

(B.8)

where P âŠ¥ is the orthogonal projection onto the orthogonal complement
2
2
, and where Î˜ is the âˆ -function
+ in

H

L

L

Î˜(z) = Wâˆ’ (z)WÌ„+ (z)âˆ’1 .

L H
2

2
+

of

(B.9)

Here Wâˆ’ (z) and WÌ„+ (z) are the analytic and coanalytic minimum-phase spectral factors deï¬ned in Section 4. (See, e.g., [30, 31].) In the present scalar case, WÌ„+ (z) =
Wâˆ’ (z âˆ’1 ). In fact, the phase function Î˜ is the transfer function of an all-pass ï¬lter
transforming the white noise wâˆ’ in (4.2) to the white noise w+ in (4.3) [30, p. 834].
Ë† + be the stochastic measures such that
Let dwÌ‚âˆ’ and dwÌ„
 Ï€
 Ï€
iÎ¸t
Ë†+
e dwÌ‚âˆ’ and wÌ„+ (t) =
eiÎ¸t dwÌ„
wâˆ’ (t) =
âˆ’Ï€

Then

âˆ’Ï€


H

+

Ï€

=

Hâˆ’ =
and consequently
H := E

f â†” f dwÌ‚âˆ’ .

Hâˆ’

âˆ’Ï€
Ï€

H

L H
2

âˆ’Ï€



2 Ë†
+ dwÌ„ +

Ï€

=
âˆ’Ï€

H

2
iÎ¸t
+ Î˜(e )dwÌ‚âˆ’

2
+ dwÌ‚âˆ’

|H + corresponds to HÎ˜ under the isomorphism deï¬ned by





Proof of Theorem 5.3. It follows from Theorem 5.2 that |WÌ‚Î½ (eiÎ¸ )| âˆ’ |WÎ½ (eiÎ¸ )| â†’ 0
uniformly in Î¸ as Î½ â†’ âˆ, and hence, by Lemma A.1, there are positive real numbers
Âµ1 and Âµ2 such that
Âµ1 â‰¤ |WÌ‚Î½ (eiÎ¸ )| â‰¤ Âµ2
for all Î¸ and suï¬ƒciently large Î½. Therefore, since
WÌ‚Î½ âˆ’ WÌ‚ âˆ â‰¤ WÌ‚Î½ âˆ WÌ‚Î½âˆ’1 (WÌ‚Î½ âˆ’ WÌ‚ )âˆ ,

(B.10)

30

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

(A.13) and (4.26) imply that
WÌ‚Î½ âˆ’ WÌ‚ âˆ â‰¤ 2Âµ2

Î½
	

ÏƒÌ‚k
,
1 âˆ’ ÏƒÌ‚k
k=nÌ‚+1

(B.11)

for suï¬ƒciently large Î½, where ÏƒÌ‚1 , ÏƒÌ‚2 , . . . , ÏƒÌ‚Î½ are the singular values (5.1) determined
from the covariance estimates (2.6).
It is well-known (see, e.g., [56, p. 204]) that the singular value Ïƒk of the Hankel
operator HÎ˜ , deï¬ned by (B.8) equals the inï¬mum of HÎ˜ âˆ’ K over all operators
2
2
â†’ 2 +
of ï¬nite rank at most k. Recall that Î˜(z) = WÎ½ (z)/WÎ½ (z âˆ’1 ).
K : +
The singular value ÏƒÌ‚k of HÎ˜Ì‚ , where Î˜Ì‚(z) = WÌ‚Î½ (z)/WÌ‚Î½ (z âˆ’1 ), is described analogously.
Therefore, since

H

L H

HÎ˜Ì‚ âˆ’ K â‰¤ HÎ˜Ì‚ âˆ’ HÎ˜  + HÎ˜ âˆ’ K â‰¤ Î˜Ì‚ âˆ’ Î˜âˆ + HÎ˜ âˆ’ K,
we have ÏƒÌ‚k â‰¤ Î˜Ì‚ âˆ’ Î˜âˆ + Ïƒk . But, for k > n, Ïƒk = 0, and hence ÏƒÌ‚k â‰¤ Î˜Ì‚ âˆ’ Î˜âˆ .
Consequently, (B.11) yields
WÌ‚Î½ âˆ’ WÌ‚ âˆ â‰¤ M1 Î½Î˜Ì‚ âˆ’ Î˜âˆ ,

(B.12)

where M1 := 2Âµ2 (1 âˆ’ ÏƒÌ‚nÌ‚+1 )âˆ’1 . However,


âˆ’1 âˆ’1
âˆ’1
âˆ’1
WÌ‚Î½ (z) âˆ’ W (z) âˆ’ Î˜(z)[WÌ‚Î½ (z ) âˆ’ W (z )] ,
Î˜Ì‚(z) âˆ’ Î˜(z) = WÌ‚Î½ (z )
so, since WÌ‚Î½ (z âˆ’1 )âˆ is uniformly bounded by (B.10), and Î˜âˆ is constant,
Î˜Ì‚ âˆ’ Î˜âˆ â‰¤ M2 W âˆ’ WÌ‚Î½ âˆ ,
which together with (B.12) yields
WÌ‚Î½ âˆ’ WÌ‚ âˆ â‰¤ M1 M2 Î½W âˆ’ WÎ½ âˆ + M1 M2 Î½WÎ½ âˆ’ WÌ‚Î½ âˆ
for suï¬ƒciently large Î½. Consequently the theorem follows from Theorem 3.4 and
Corollary B.1.
Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A.
Gombani, W. B. Gragg, G. Picci and T. SoÌˆderstroÌˆm for stimulating discussions and
for providing us with appropriate references. We are also indebted to the anonymous
referees for several useful suggestions.
References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987.
2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489â€“502, 1974.
3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975.
4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational
covariance extension problem. SIAM Journal on Control and Optimization, 37:211â€“229, 1999.
5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of
all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841â€“1857, 1995.
6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast ï¬ltering algorithms.
SIAM Journal on Control and Optimization, 32:744â€“789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-GuÌˆrsoy. On the Lâˆ consistency of L2 estimators. Systems & Control
Letters, 12:71â€“76, 1989.
8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978.
9. A. DahleÌn, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace
identiï¬cation methods may fail. Systems and Control Letters, 34:303â€“312, 1998.
10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced
stochatic realizations. In Proc. 21st IEEE CDC, pages 1105â€“1112, 1983.
11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans.
Automatic Control, AC-29:1097â€“1100, 1984.
12. J. Durbin. Eï¬ƒcient estimation of parameters in moving average models. Biometrika, 46:306â€“316,
1959.
13. J. Durbin. The ï¬tting of time-series models. Rev. Inst. Int. Stat., pages 223â€“243, 1960.
14. P. Duren. Theory of Hp spaces. Academic Press, 1970.
15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear.
16. G. Freud. Orthogonale Polynome. BirkhaÌˆuser Verlag, 1969.
17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic
Control, AC-32:358â€“361, 1987.
18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac.,
Speech and Signal Processing, ASSP-35:438â€“449, 1987.
19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961.
20. M. Gevers. Towards a joint design of identiï¬cation and control. In J. Willems and H. Trentelman,
editors, Essays on Control: Perspectives in the Theory and its Applications, 1993.
21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their
lâˆ -error bounds. Int. J. Contr., 39:1115â€“1193, 1984.
22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989.
23. W. B. Gragg. Positive deï¬nite Toeplitz matrices, the Arnoldi process for isometric operators,
and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in
Linear Algebra, pages 16â€“32. Moscow U. P., 1982.
24. U. Grenander and G. SzegoÌˆ. Toeplitz forms and their applications. Univ. California Press, 1958.
25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons,
1988.
26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples.
Biometrika, pages 297â€“307, 1989.
27. W. Jones and E. Saï¬€. SzegoÌˆ polynomials and frequency analysis. In Approximation Theory,
pages 341â€“352. Dekker Inc., 1992.
28. S. Y. Kung. A new identiï¬cation method and model reduction algorithm via singular value
decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705â€“714, 1978.
29. W. E. Larimore. System identiï¬cation, reduced ordered ï¬ltering and modeling via canonical
variate analysis. In Proc. of the American Control Conference, 1983.
30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes.
SIAM J. Control and Optimization, 23:809â€“857, 1985.
31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic
systems. J. of Math. Systems, Estimation and Control, 1:241â€“333, 1991.
32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension,
and identiï¬cation of stationary time series. Automatica, 32(5):709â€“733, 1996.
33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating
transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412â€“440, 1992.
34. L. Ljung and Z. Yuan. Asymptotic properties of black box identiï¬cation of transfer functions.
IEEE Trans. Automatic Control, AC-26:514â€“530, 1985.
35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis,
Royal Instiute of Technology, 1998.
36. D. Q. Mayne and F. Firoozan. Linear identiï¬cation of ARMA processes. Automatica, 18:461â€“466,
1982.

32

J. MARI, A. DAHLEÌN, AND A. LINDQUIST

37. H. Mhaskar and E. Saï¬€. The distribution of zeros of asymptotically extremal polynomials. J.
Approx. Theory, 3:279â€“300, 1991.
38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66â€“73, 1978.
39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward,
editors, Approximation Theory VI, 1989.
40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991.
41. P. Van Overschee. Subspace Identiï¬cation, Theory - Implementation - Application. PhD thesis,
Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De
Moor.
42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identiï¬cation problem.
In Proc. 30th Conference on Decision and Control, Brighton, 1991.
43. P. Van Overschee and B. De Moor. Subspace Identiï¬cation for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996.
44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994.
45. I. Schur. UÌˆber Potenzreihen, die im Innern des Einheitskreises beschraÌˆnkt sind. J. fuÌˆr die Reine
und Angewandte Mathematik, 147:205â€“232, 1917.
46. J. Sorelius, T. SoÌˆderstroÌˆm, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identiï¬cation. In Proc. SYSID â€™97, 1997.
47. G. SzegoÌˆ. BeitraÌˆge zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift,
6:167â€“202, 1920.
48. G. SzegoÌˆ. UÌˆber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232â€“244, 1921.
49. G. SzegoÌˆ. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications,
1939 (4th edition 1975).
50. B. Wahlberg. On the Identiï¬cation and Approximation of Linear Systems. PhD thesis, LinkoÌˆping
University, 1987. LinkoÌˆping Studies in Science and technology. Dissertations No. 163.
51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive
approximations. Journal of Time Series Analysis, 10:283â€“299, 1989.
52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like
identiï¬cation techniques. IEEE Trans. Automatic Control, 37:900â€“912, 1992.
53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation.
Systems and Control Letters, 14:307â€“317, 1990.
54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423â€“
434, 1953.
55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938.
56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

