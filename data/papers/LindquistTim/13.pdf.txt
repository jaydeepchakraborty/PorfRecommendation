A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION OF TIME SERIES*
´ , AND ANDERS LINDQUIST JORGE MARI, ANDERS DAHLEN

Abstract. In this paper we consider a three-step procedure for identification of time series, based on covariance extension and model reduction, and we present a complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy model is determined, which is finally approximated by a lower-order model by stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all three steps has been lacking. Supposing the data is generated from a true finitedimensional system which is minimum phase, it is shown that the transfer function of the estimated system tends in H to the true transfer function as the data length tends to infinity, if the covariance extension and the model reduction is done properly. The proposed identification procedure, and some variations of it, are evaluated by simulations.

1. Introduction In recent years there has been quite some interest in a certain type of procedures for identification of time series known as subspace methods [1, 42, 41, 28, 29]. These identification procedures are based on geometric projection methods, and they could be understood in the context of splitting geometry and partial stochastic realization theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9], these procedures are algebraically equivalent to minimal factorization of a Hankel matrix of covariance estimates, and they make no distinction between stochastic and deterministic partial realizations. Therefore they may fail because of loss of positive realness in the spectral estimation phase. In an attempt to overcome these problems we analyze an alternative approach to time series identification proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the maximum-entropy method, leading to a high order autoregressive (AR) process, and finally stochastically balanced truncation. This method shares certain features with stochastic subspace identification methods, the most obvious one being that it is based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
 This research was supported by a grant from the Swedish Research Council for Engineering Sciences (TFR).  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden 1

2

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for maximum-likelihood (ML) methods, are needed. The idea of approximating an autoregressive moving-average (ARMA) process by an AR process is by no means new. Its origins can be traced back to the Wold decomposition [55] where L2 -convergence of high-order AR models to general analytic models is shown. Pioneers in the use of this concept for systems identification are Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations were studied by Berk [2] and later refined in [36, 34, 33, 7]. The interesting paper [7] contains nice proofs of some of the convergence results needed in this paper, but, for the sake of completeness and insight, we provide new proofs based on some properties of fast filtering algorithms [5] and simple methods of complex analysis and Szeg o polynomials. The power of the theory of Szeg o polynomials and Toeplitz matrices in analyzing stochastic processes is reported in [24], but, except for elementary theory, it has not been much used in systems identification [39]. This is even more true for the newer results [16, 40, 37, 27] on orthogonal polynomials. The idea of using model reduction for systems identification appears in the thesis by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency weighted reduction. Instead, we use stochastically balanced truncation, for which we develop a simple computational procedure, exploiting the special structure of the AR model. We also show the advantage of this reduction procedure by theoretical analysis and simulations. In fact, a comprehensive study comprising all the steps mentioned above together with a qualitative and quantitative analysis of the entire identification strategy has been lacking, and that is what we offer in this paper. The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identification procedure contributes to the estimation error. In Section 3 we show that the transfer function of the maximum-entropy filter, constructed from true covariances, tends to that of the true filter in H norm at a geometric rate determined by the largest modulus of the zeros of the true filter as the order of the maximum-entropy filter becomes large. However the order of the approximation is too high, and therefore model reduction is performed. This is studied in Section 4. A stochastic balancing procedure, based only on linear-algebra operations so that no Riccati equations need to be solved, is provided together with the analysis of the model-reduction error. Both deterministically and stochastically balanced truncation lead to good results. However, when the covariances are estimated from statistical data, stochastic model reduction is found to be superior. In particular, variances are considerably closer to the Cram´ er-Rao bounds. In Section 5 we state our statistical convergence theorems, proving that the total error tends to zero as the length of the data string tends to infinity, provided the degree of the AR model tends to infinity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using stochastic subspace identification [43] is included. For clarity of exposition, all the proofs have been deferred to two appendices, Appendix A dealing with the asymptotic properties of the maximum-entropy filter, and Appendix B devoted to the statistical error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identification Time series identification in the form studied here amounts to estimating the matrices (A, B, C, D) in some n-dimensional linear stochastic system x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) driven by normalized white noise {w(t)}, from a data string of observations {y0 , y1 , y2 , . . . , yN } (2.2) of the output process {y (t)}, which here will be taken to be scalar. The basic idea behind our approach is very simple: given estimates of a partial sequence c 0 , c 1 , c 2 , . . . , c (2.3) of the covariances ck = E{y (t + k )y (t)}, which satisfies the condition that the Toeplitz matrix   c2 · · · c c0 c1  c1 c0 c 1 · · · c  -1     c c c 0 · · · c  -2  1 T +1 :=  2 (2.4)  . . . ... .  . . . . . . . . c  c  -1 c  -2 · · · c0 is positive definite, first construct a high-order model continuing (2.3) by covariance extension. This model has all the required positivity properties, but the order is too high. Then reduce the order by means of a positivity-preserving model reduction procedure to be specified below. That this simple recipe will in fact provide a good identification method is by no means a trivial matter but is based on some rather deep results, which will be presented here. More specifically, the approach consists of three steps, for which there are several possible variants that will be discussed below. The rigorous mathematical analysis, however, will be carried out for the following procedure, for which we shall give theoretical bounds. (i) Estimate a partial covariance sequence ^1 , c ^2 , . . . , c ^ c ^0 , c from the time-series data (2.2) via the ergodic estimate 1 c ^k = N +1
N -k

(2.1)

(2.5)

yt+k yt
t=0

k = 0, 1, . . . , .

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer function  ^  (z ) = z , (2.7) W ^ (z )  ^ (z ) is the normalized Szeg¨ where  o polynomial of degree  , to be introduced in Section 3, computed from the estimated covariance data (2.5).

4

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

^ (z ) of W ^  (z ) via a stochastic (iii) Determine a reduced-degree approximation W model reduction procedure [11] to be described in more detail in Section 4. In this procedure, the idea is that  >> n, the order of the system to be identified, ^ equals the degree n of the true system (2.1). However, the and ideally n ^ := deg W method will produce a valid model even if this is not the case or even if there is no "true" underlying model. This is in contrast to stochastic subspace identification models, which may fail to produce any model at all [9]. There are possibilities for variations of the procedure described above. In Step (i) we could use alternative covariance estimates or Burg's estimation of Schur parameters ^ +1 of (2.5) is [3], the only requirements being that the estimated Toeplitz matrix T positive definite and that c ^k  ck a.s. as N  . In Step (ii) we could instead use approximate covariance extension or covariance extension with prescribed zeros, for which there is now a complete parameterization [5] and an algorithm [4]. (In the latter case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction methods could be used. For example, an important model reduction paradigm is the one based on optimal Hankel norm approximation [21]. Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable transfer function W (z ) = C (zI - A)-1 B + D, (2.8)

L

of McMillan degree n. We also assume that W (z ) is minimum-phase so that both zeros and poles are located in the open unit disc. Then, we need to be able to measure ^ (z ), converges to the true one as how the estimated model, with transfer function W ^ (z ) in N  . In this paper we have chosen to use distance between W (z ) and W  norm as a measure of proximity between the true and estimated model. From an engineering point of view this could be called worst case identification. The modern literature in robust control makes extensive use of the worst case philosophy; see for example [20, 52]. There are also other reasons for using the  , as discussed in [35]. Returning, then, to the identification approach outlined above, the estimation error can be decomposed into three parts, one corresponding to each of the steps (i), (ii) and (iii). Hence we have the error bound

L

^ W -W



 W - W



^ + W - W



^ - W ^ + W

,

(2.9)

^  is where W is the AR model corresponding to the true covariances (2.3) and W the one determined from the estimated covariances (2.6). To prove convergence to zero of the estimation error (2.9), we shall need to assume that W is minimum-phase, ^ should have the same property, which moreover is desirable in many and hence W applications. Our procedure insures this. Estimating the first term in (2.9) is a problem in stochastic partial realization theory and function theory and will be dealt with in the next section. The third term concerns model reduction which will be studied, in the particular setting required here, in Sections 4 and 5. In Section 5, finally, we consider the second term together with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence Step (ii) in the identification procedure outlined in Section 2 is based on rational covariance extension. To understand this, let us consider the covariance extension problem from a more general point of view. Given a partial covariance sequence c 0 , c 1 , c 2 , . . . , c , (3.1)

covariance extension amounts to finding an infinite extension c +1 , c +2 , c +3 , . . . of this sequence such that the function is strictly positive real, i.e., it is an analytic function in the complement Dc of the open unit disc D, which maps Dc to the open right complex half-plane. Then (z ) := V (z ) + V (z -1 ) is a spectral density for a process having c0 , c1 , . . . , c as its first  covariances and which is coercive in the sense that (ei ) > 0 for all . Spectral factorization is then to find a stable transfer function W (z ) such that |W (ei )|2 = (ei ). In particular, we are interested in finding covariance extensions for which V (z ), and hence W (z ), have at most degree  . For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one correspondence between infinite covariance sequences c0 , c1 , c2 , c3 , . . . and a sequence of Schur parameters, or reflection coefficients, 0 , 1 , 2 , 3 , . . . , (3.3) with the property |t | < 1 for all t. In fact, fixing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and 0 , 1 , . . . , m-1 for each m. The Schur parameters can be determined from the covariances via the Szeg¨ o polynomials t (z ) = z t + t1 z t-1 + · · · + tt t = 0, 1, 2 . . . , computed by means of the Szeg¨ o-Levinson recursion z -t t+1 (z ) = ( z ) - z 1  t t+1 where t (z ) ;  t (z ) 0 (z ) 1 = , ( z ) 1  0 (3.4) (3.2) c + c 1 z -1 + c 2 z -2 + . . . V (z ) := 1 2 0

t -1  t (z ) := z t (z ) is the reciprocal polynomial of t (z ), and the Schur parameters are computed via

= r1t t t j =0 t,t-j cj +1 rt+1 = rt (1 - |t |2 ), r0 = c0 .

(3.5)

6

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Hence t = -t+1 (0), a fact that we shall use below. In the problem to find a covariance extension for (3.1), therefore, 0 , 1 , . . . ,  -1 are fixed and the infinite continuation  ,  +1 , . . . can be chosen freely. In particular, if we take t = 0 for t = ,  + 1,  + 2, . . . . We obtain the maximum entropy solution W (z ) = z ,  (z ) (3.6)

where  (z ) is the normalized Szeg¨ o polynomial 1  (z ) :=   (z ). r (3.7)

Thus, in this particular case, the solution to the covariance extension problem turns out to be rational of degree at most  as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations are needed. In fact, it has recently been shown [5] that there is exactly one such solution for each choice of zeros of W (z ), thus proving a long-standing conjecture by Georgiou [18], who had established existence. Nevertheless, as we shall see next, rationality implies that the Schur parameters tend geometrically to zero, provided W (z ) has no zeros on the unit circle. In this section we shall demonstrate that the rational transfer function (2.8) can be approximated arbitrarily closely in L by the transfer function W (z ) of a maximum entropy filter for sufficiently large  and that this  depends on the maximum modulus of the zeros of W (z ). We shall first present a heuristic argument in support of this conclusion. To this end, let (3.2) be the infinite covariance sequence of the output process y in (2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via the Szeg¨ o-Levinson algorithm presented above. Then we have the following special case of Corollary 2.1 in [5]. Lemma 3.1. Let the spectral density (ei ) = |W (ei )|2 (3.8)

be coercive in the sense that it is positive for all  and let (3.3) be the corresponding infinite sequence of Schur parameters. Moreover, let   (0, 1) be greater than the maximum of the moduli of the zeros of W (z ). Then |t | = O( t ), i.e., |t |  M  t for some M  R and for sufficiently large t. Remark 3.2. Since (3.9) holds for all  greater than the the maximum of the moduli of the zeros of W (z ), we have in fact that |t | = o( t ), i.e., limt |t | -t = 0. For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis of certain fast algorithms for Kalman filtering [6]. (3.9)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the spectral density z (z - 1)2 (z ) = - 2 (z + z + 2)(2z 2 + z + 1) is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters are seen to be -1/2, -2/3, -2/5, -2/7, -2/9, -2/11, . . . , which tend to zero but not geometrically. On the other hand, there are coercive, analytic but nonrational models which also exhibit geometric convergence rate. A classical example [23] is obtained 2 when ck = k for some   (-1, 1). The Schur parameters in this case form an exact geometric sequence, k = (-)k+1 , k  0. Lemma 3.1 implies that, for a sufficiently large  which depends on  , the Schur parameters t are close to zero for t = ,  + 1,  + 2, . . . . But, the Schur parameters of W are exactly zero for t = ,  + 1,  + 2, . . . , and hence geometric convergence would insure that W is a good approximation of W (z ) for sufficiently large  . We shall prove that this is indeed the case. Theorem 3.4. Suppose W (z ) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let   (0, 1) be greater than the maximum of the moduli of the zeros of W (z ). Then
 

lim W - W



= 0,

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that W - W


 M  .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to first showing that
  -1 - W -1 lim W 

= 0.

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we give an alternative proof of this fact based on Szeg¨ o theory, and also show that the convergence is geometric. In fact, we can choose  arbitrarily close to the maximum modulus of the zeros of W . However, as we shall see next, we can actually prove more. To this end, let us first -1 and W -1 have their poles in the open unit disc D and thus observe that, since W are bounded and analytic in the complement Dc of D, they belong to the Hardy space  of functions which are analytic and bounded in {z  C | |z | > 1}. Hence the H-  , and convergence (3.12) is in H- z -  (z )  W -1 (z ) (3.13) uniformly in each compact subset of Dc . Now, W -1 is analytic in {z  C | |z |   }, a region that is strictly larger than Dc . This in itself of course does not insure that the convergence (3.13) extends to this larger region. In fact, even if z -  (z ) did converge in {z  C |   |z |  1}, it could fail to converge to W -1 (z ) there. The fact that it really does converge uniformly to this limit is another consequence of Lemma 3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and hence an estimate of the convergence rate  , is given in [35]. Theorem 3.5. Suppose W (z ) is a minimum-phase rational function having all its poles in the open unit disc D and all its zeros in

D := {z  C | |z |  }  D

where 0 <  < 1,

and let { (z )} o polynomial (3.7) determined from the co0 be the normalized Szeg¨ variances in the spectral density


|W (e )| = c0 + 2
i 2 k=1

ck cos k.

Then, as   , z -  (z )  W -1 (z ) uniformly in every compact subset of {z  C | |z | > }, the complement of D .

Dc  :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of  (z ) and hence of the poles of the high-order AR model with transfer function W (z ). It is known that, if the Toeplitz matrix T +1 is positive definite, all roots of  (z ) are located in the open unit disc D, but little has been reported in the literature on their behavior as   . This behavior is illustrated in Figure 3.1.
Original system 1 1 Original system

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0

0.5

1

-1 -1

-0.5

0

0.5

1

Original and AR(24) 1 1

Original and AR(24)

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0

0.5

1

-1 -1

-0.5

0

0.5

1

Figure 3.1: Distribution of zeros of  (z ).

The top two diagrams show the zero-pole positions, within the boundaries of the unit circle, of two minimum phase spectral factors W , both of degree five. Also indicated is a circle of radius equal to the maximum modulus of the zeros of these spectral factors. The little circles "" represent zeros and the "+" sign represent poles. The lower two figures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the exact covariance sequence. The poles of the latter models are indicated with "×". The left part of Figure 3.1 illustrates what may happen if all the poles of W (z ) are located in {z  C | |z | < }, where  is chosen to be the maximum of the moduli of the zeros of W (z ). The roots of  (z ) tend to cluster inside a circle of radius  as   . This phenomenon is in a sense predictable, since the constant term of the Szeg¨ o polynomials is n+1 (0) = -n , which equals the product of the roots and, by Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to . This does not preclude that other types of crowns may occur, because subsequences of {n } could decay faster than the overall rate  , as follows from [5]. Very general statements about the distribution of zeros of orthogonal polynomials, derived with the help of potential-theoretic methods, can be found in [37, 27]. To the right in Figure 3.1 we see what happens in the case that W has poles with moduli larger than . Then, for  sufficiently large, the normalized Szeg¨ o polynomial  (z ) has roots in {z  C |   |z | < 1}, but exactly as many as the poles of W in this region and approximately at the same place as these. This is of course due to the uniform convergence of z -  (z ) to W -1 (z ) in every compact subset of Dc  . The other roots of  (z ) behave exactly as in the previous case and tend to accumulate in a crown inside and very close to the circle {z  C | |z | = }.  approximation W of W which can be made We have thus constructed an H- arbitrarily good by choosing  sufficiently large. However, W will have much larger degree and, except for the poles outside the circle {z  C | |z | = }, a completely different zero-pole pattern. We shall rectify this situation by model reduction. In fact, for the moment considering the perfect modeling problem to identify the rational transfer function (2.8) given an exact partial covariance sequence (3.1), the last step in our procedure consists in approximating W by a rational function Wred of smaller degree, ideally of the same degree as W . The simplest model reduction procedure is deterministically balanced truncation (DBT), first introduced by Moore [38]. Though easy to implement, it may fail to yield a minimum-phase approximation, a requirement which is important in certain contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced truncation (SBT), first introduced by Desai and Pal [10], which is based on a different balancing strategy to be explained in detail in Section 4.
Original system 1 0.5 0 -0.5 -1 -1 1 0.5 0 -0.5 -1 -1 Reduction by DBT 1 0.5 0 -0.5 -1 -1 Reduction by SBT

0

1

0

1

0

1

Figure 3.2: Zero-pole pattern of W (z ) and Wred (z ) for different model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This fifth-order

10

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

model has first been approximated by W of degree  = 24, producing the pole-zero pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens when the model is reduced back to order five by either deterministically balanced truncation or stochastically balanced truncation. The zeros are denoted by "" and the poles by "+". Both reduction procedures give good approximations when applied to exact covariance data. However, as we shall see in Section 5, the advantages of SBT becomes apparent when applied to statistical data. Also, as explained in Remark 4.5, there are theoretical reasons to prefer stochastic model reduction. 4. Model reduction In the present setting, model reduction amounts to replacing a stochastic system (2.1) of dimension  by one of some dimension r <  in such a way that most of its statistical features are retained. In particular, we want to remove the part of the system which corresponds to the weakest correlation between past and future. This idea can be formalized in the following way. Basic concepts. In the Hilbert space generated by the random variables {y (t) | - < t < } in the inner product u, v = E{uv }, let H - be the subspace generated by the past, i.e., {y (t) | t < 0}, and H + that generated by the future {y (t) | t  0}. Consider the Hankel operator H : H +  H - and its adjoint H : H -  H + defined as

H = EH
-

-

|H +

and

H = E H

+

|H - ,

(4.1)

where E H denotes orthogonal projection onto the past space H - . More precisely, H sends   H + to E H -   H - and H sends   H - to E H +   H +. Since the process y is the output of a minimal stochastic system of dimension  , rank H =  by Kronecker's Theorem [56], and hence H has exactly  singular values, 1 , 2 , . . . ,  , which are positive, as usually listed so that 1  2  · · ·   . These singular values are the canonical correlation coefficients and hence the cosines of the angles between the principal directions of the past space H - and the future space H + . They are therefore less than one, and the part of the stochastic system corresponding to singular values which are close to zero have a weak coupling between past and future, i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic model reduction is to truncate the system so that this part is removed. To each singular value k there is an associated Schmidt pair (k , k ) with k  H + and k  H - such that

Hk = k k ,

Hk = k k ,

and such that the sequences 1 , 2 , 3 , . . . and 1 , 2 , 3 , . . . of singular vectors are orthonormal. The singular vectors corresponding to nonzero singular values span the predictor spaces X- := span{1 , 2 , . . . ,  }, X+ := span{1 , 2 , . . . ,  }. Clearly, X-  H - and X+  H + . The process y has one representation (2.1) for each minimal spectral factor W , having W as its transfer function. Such representations are called minimal stochastic realizations and the corresponding subspaces X := {a x(0) | a  R } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X- is the splitting subspace of the stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) (4.2)

with the transfer function W- (z ), the minimum-phase spectral factor; and X+ is the splitting subspace of x+ (t + 1) = Ax+ (t) + B+ w+ (t) y (t) = Cx+ (t) + D+ w+ (t) (4.3)

with transfer function W+ (z ), the maximum-phase spectral factor, having all its zeros in Dc . Note that A and C are the same in both realizations (uniform choice of bases). Each realization has a counterpart which evolves backwards in time and has the same splitting subspace. For example, the backward realization of X+ , ¯+ w ¯+ (t) + B ¯+ (t) x ¯+ (t - 1) = A x , ¯ ¯ ¯+ (t) y (t) = Cx ¯+ (t) + D+ w (4.4)

¯ + (z ), the coanalytic minimum-phase spectral factor, having all has transfer function W ¯ + (z ) = W- (z -1 ). its poles and zeros in Dc . In the present case with scalar y , we have W Now, in order to identify the part of the system which has the weakest coupling between past and future, and hence will be removed in the model reduction, we need to balance the system in the sense of Desai and Pal, as we shall explain next. To this end, we make a coordinate transformation ¯ ), ¯ )  (SAS -1 , CS -1 , CS (A, C, C in the minimal realization of ¯ + 1 c0 , (4.6) V (z ) = C (zI - A)-1 C 2 the strictly positive real part of the spectral density of y , so that the state covariances ¯+ = E{x ¯+ (t)¯ x+ (t) } coincide with the diagonal  ×  P- := E{x- (t)x- (t) } and P matrix  of nonzero canonical correlation coefficients, i.e., ¯+ =  := diag(1 , 2 , . . . ,  ). P- = P
1

(4.5)

(4.7)

This is done by choosing S so that Sx- (0) =  2  , where  = (1 , 2 , . . . ,  ) , and 1 (S )-1 x ¯+ (0) =  2  , where  := (1 , 2 , . . . ,  ) . To compute the canonical correlation coefficients, we first observe that the eigen¯+ are precisely the squares of the canonical correlation values of the product P- P coefficients, i.e.,
-1 2 2 2 ¯+ ) = (P- P+ ) = {1 , 2 , . . . ,  }, (P- P

(4.8)

-1 ¯+ where we have used the fact that the state covariance of (4.3) is P+ = P . Therefore the canonical correlation coefficients can then be determined via (4.8) by solving the Lyapunov equations

P- = AP- A + B- B-

and P+ = AP+ A + B+ B+ .

(4.9)

12

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

The point is now to identify the canonical correlation coefficients 1 , 2 , . . . , r corresponding to the part of the system one wants to keep. The part corresponding to r+1 , r+2 , . . . ,  will be disposed of. This amounts to partitioning  as = 1 2 , (4.10)

where 1 is r × r. In order to reduce model (2.1) we make the coordinate transformation (A, B, C )  (SAS -1 , SB, CS -1 ), with the same balancing transformation S . Then, partition the new triplet (A, B, C ) conformally with (4.10) as A= A11 A12 , A21 A22 B= B1 , B2 C = C1 C2 , (4.11)

and perform a principal subsystem truncation to obtain the transfer function of a reduced-order system Wred (z ) = C1 (zI - A11 )-1 B1 + D (4.12) of degree r. If 2 is close to zero, while 1 is not, the rank of H is close to r, and the discarded part of the system gives a negligible contribution to y . Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy filter   r z (4.13) W- (z ) := W (z ) =  (z ) of order  , which, for the moment we denote W- (z ) to emphasize its character as the minimum-phase spectral factor of the spectral density r .  (z ) (z -1 ) Remark 4.1. Without loss of generality we assume that  (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller  for which this condition holds. In fact,  (0) =  -1 , and if  -p =  -p+1 = · · · =  -1 = 0 and  -p-1 = 0 for some p = 1, 2, . . . ,  , then  (z ) = z  -p  -p (z ) by (3.4), and hence (3.6) can be replaced by W (z ) = W -p (z ), and for W -p (z ) the required condition holds. The maximum-phase spectral factor W+ (z ) has all its zeros at infinity, and hence  r -1 W+ (z ) = h (zI - F ) b = , (4.14)  (z ) where (F, b, g ) is the (observable) canonical form     0 0 1 ··· 0 . . ...  .  . . . . . .  .  . , b =  .  F =  0 ,  0 0 ··· 1   - -, -1 · · · - 1 r   1 0 , h= . . . 0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

 1 ,  2 , . . . ,  being the coefficients of the Szeg¨ o polynomial  (z ). In this basis, it follows from (4.9) that [P+ ]jk = 1 2
 - 

(ei I - A)-1 bb (e-i I - A )-1 d
jk

1 = 2
   .  .  . =   2    1 1

r e-(j -k)i d = cj -k ,  (ei ) (e-i ) -
···  1        r -1   and R =    r -2 .. . r0    , (4.16) 

and hence P+ = T . It is well-known and easy to prove that  T  = R , where
 -1, -1 . . .  -1,1 1  -2, -2 . . . 1

 +1

and consequently
-1 -1 ¯+ = T P =  R  .

(4.17)

It remains to determine P- . From (4.13) is easy to see that  W- (z ) = - (zI - F )-1 b + r , where  :=  , -1 · · ·  1 ,

(4.18)

(4.19)

but, in order to determine P- , this realization needs to be transformed so that the A and C matrices are the same as in (4.14) (uniform choice of bases). More precisely, we need to perform a transformation (F, b, - )  (QF Q-1 , Qb, - Q-1 ) =: (F, Qb, h ). Then P- is the solution of the Lyapunov equation P- = F P- F + Qbb Q , and therefore, since T = F T F + bb and QF = F Q and consequently QT Q = F QT Q F + Qbb Q , we have P- = QT Q . To determine Q, notice that - = h Q and QF    h -  - F   h F = .  .   .  . . . - F  -1 Next, define the symmetric matrix
- 1/2 - 1/ 2  QT Q  R . M := R

(4.20) = F Q to form    Q = Q.  (4.21)

h F  -1

(4.22)

14

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

¯+ ), and hence, by (4.8), M In view of (4.20) and (4.17), det(zI - M ) = det(zI - P- P 2 2 2 has the eigenvalues 1 , 2 , . . . ,  , and the singular-value decomposition M = U 2 U , where U U = U U = I . It is then well-known and simple to check that
- 1/2  S := -1/2 U R

(4.23) (4.24)

¯+ S -1 = . is the required balancing transformation (4.5) such that SP- S = (S )-1 P Proposition 4.2. Given the partial covariance sequence ck = E{y (t + k )y (t)}, k = 0, 1, . . . , , o polynomials let 1 (z ), 2 (z ), . . . ,  (z ) and r0 , r1 , . . . , r be the corresponding Szeg¨ and error variances. Supposing that  -1 = - (0) = 0, let (F, b, h) be given by (4.15), R and  by (4.16) and Q by (4.21). Moreover, let U and  be defined by the singular value decomposition (4.23) of (4.22). Then, the canonical correlation coefficients 1 , 2 , . . . ,  are the diagonal elements of , as described in (4.7), and the stochastically balanced realization of W is given by  (4.25) (A, B, C, D) = (SF S -1 , SQb, h S -1 , r ), where S is defined by (4.24). Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase property. In fact, we have the following result, the proof of which is given in Section A. Theorem 4.3. Let Wred be the SBT approximation of degree r of W , and set k := 2 1 - k k=r+1
  -1

and

 :=



c0
k=0

1 + |k | , 1 - |k |

(4.26)

where 0 , 1 , . . . ,  -1 are the Schur parameters of c0 , c1 , c2 , . . . , c . Then c0 (1 - )-1  |Wred (ei )|  (1 + ) for all , W - Wred  . (4.27) (4.28) and, if < 1, Wred is minimum phase. Finally, the approximation error has the bound


A properly executed SBT procedure should imply that the canonical correlation coefficients r+1 , . . . ,  , and hence , are close to zero, insuring the minimum-phase condition. Remark 4.4. Stochastic model reduction can also be carried out by instead per¯ + 1 c0 , ¯ ) in V (z ) = C (zI - A)-1 C forming principal subsystem truncation on (A, C, C 2 ¯ = S (c1 , c2 , . . . , cn ). It was shown in where A and C are given by (4.25) and C [32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally, the spectral density red (z ) := Vred (z ) + Vred (z -1 ) is factorized to yield a minimum~ . This is in a sense a more natural procedure, but we do phase spectral factor W not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small 2 it yields almost the same result. In fact, it is shown in [53], that ~ (ei )|2 = |Wred (ei )|2 + H (ei )2 H (e-i ), where H (z ) = C1 (zI - A11 )-1 A12 . |W Remark 4.5. There are good reasons to prefer stochastic over deterministic model reduction, as seen from the following heuristics. In fact, it can be seen that V (z ) = c0  (z ) , 2  (z ) (4.29)

where  (z ) is the Szeg¨ o polynomial of the second kind (obtained by exchanging -t for t in the recursion (3.4)). Now, the matrix representation of the Hankel operator ¯+ respecH in the innovation bases of the past and the future, provided by w- and w is the infinite Hankel matrix of the sequence tively, is given by L-1 (L-1 ) , where c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T ; see, e.g., [32, p. 714]. It is easy to see that  (z ) has the same asymptotic behavior as  (z ), i.e., the roots tend to cluster uniformly inside the circle z =  as   , and hence these roots are close to canceling in (4.29). Consequently, the corresponding Hankel matrix is close to having low rank. This massive "almost cancellation" does not occur in W (z ), and hence the corresponding infinite Hankel matrix, constructed from the Laurent coefficients of W (z ), may have a less distinct separation between 1 and 2 . On the other hand, since the Schur parameters tend geometrically to zero, the lower part of L tends to the identity, and hence the asymptotic behavior of the canonical correlation coefficients is very much like that of the singular values of . Therefore we may expect SBT to have better statistical behavior than DBT. In Section 6 we shall see that this is the case.

H

H

H

H

5. Identification from statistical data We now return to our original problem of time series identification: Given a data string (2.2) of observations of the output process y of some n-dimensional linear stochastic system (2.1) with minimum-phase transfer function W (z ), given by (2.8), ^ B, ^ C, ^ D ^ ) of the matrices (A, B, C, D). find an estimate (A, The identification method proceeds as follows. Given the covariance estimates (2.5), we compute the corresponding maximum entropy filter (2.7), a balanced realization (4.25), and the canonical correlation coefficients ^2 ,  ^3 , . . . ,  ^ ,  ^1 ,  (5.1)

^1 , . . . , c ^ . determined as in Proposition 4.2 from the covariance estimates c ^0 , c Based on (5.1), choose an integer n ^ such that  ^n ^n ^ are close to zero or ^ +1 ,  ^ +2 , . . . ,  ^2 , . . . ,  ^n . Then, the balanced realization (4.25) at least distinctively smaller than  ^1 ,  ^ is truncated accordingly as in (4.11) to yield a n ^ -dimensional triplet (A11 , B1 , C1 ) and a transfer function ^ (z ) = C1 (zI - A11 )-1 B1 + D. W ^ B, ^ C, ^ D ^ ). Then, (A11 , B1 , C1 , D) is the required estimate (A, As pointed out in Section 2, we have a bound ^ W -W


(5.2)

 W - W



^ + W - W



^ - W ^ + W

,

(5.3)

16

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the first term W - W  , which does not depend on the statistical data (2.2) but only on the underlying system (2.1), tends to zero geometrically with a rate   (0, 1) as   . The other two terms depend on the data (2.2), and here N must grow at a faster rate than  . In fact, we shall assume that  =  (N ) = O(log N ), (5.4)

 = 0. We also need to assume that the which in particular requires that limN  N white noise process in (2.1) satisfies a mild technical condition, namely

E{w(t)4 } < . This condition is, of course, satisfied if w is Gaussian. Next, we present our main convergence theorem.

(5.5)

Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then, to each length N of the data string (2.2), there is a  (N ), tending to infinity with N ^ of fixed at the rate (5.4), such that any sequence of estimated transfer functions W degree n ^  n, determined, for each N and corresponding  =  (N ), by the procedure described above, satisfies ^ 0 W -W ^ has almost surely as  (N )  . For sufficiently large  (N ), the transfer function W minimum phase. We have already proven that the first term in (5.3) tends to zero, so Theorem 5.1 follows from the next two theorems, each corresponding to one of the remaining terms in (5.3). As for the second term, we have the following result, the proof of which is deferred to Appendix B. Theorem 5.2. Suppose the system (2.1) satisfies the conditions of Theorem 5.1. Let W be the maximum-entropy filter (3.6) determined from the partial covariance se^  be the corresponding function determined from the ergodic quence (3.1) of y and let W estimates (2.5). Then, if  (N ) is defined as in Theorem 5.1, ^  (N ) W  (N ) - W almost surely as  (N )  . There are several results of this type in the literature [2, 36, 7, 33]. In particular, 3  0 as N   and  is coercive (i.e. positive Berk [2] proved that, provided  N on the unit circle), the estimated AR spectral density (ei )  (ei ) in probability. Under the same hypotheses, Caines and Baykal-G¨ ursoy [7] showed that if N   5+ -1 ^ for some  > 0, then W - W -1   0 almost surely as   . However, in both cases, ergodic estimates are used which are not quite the same as (2.5). Finally, we consider the last term in (5.3). The proof of the following theorem is given in Appendix B.


0

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function  (N ) are defined as in ^ ^  (N ) be defined as in Theorem 5.2 and W Theorem 5.1. Moreover, for each N , let W ^ has minimum phase, and as in Theorem 5.1. Then, for sufficiently large  (N ), W ^ ^  (N ) - W W almost surely as  (N )  . 6. Simulations ^  , rather than on the maximum-entropy filter of Performing model reduction on W exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier and more accurate order determination, as the heuristics of Remark 4.5 suggest. There are also alternative order determination statistical tests based on the canonical correlation coefficients [17, 26, 46]. But, even more importantly, there is less bias, and the error variances are closer to the Cram´ er-Rao bound. Since we are approximating rational models with AR models the method will be biased for finite amount of data, unless the model generating the data really is an AR model. The consistency result given in Theorem 5.1 implies that the method is asymptotically unbiased and therefore we consider the Cram´ er-Rao bound for unbiased methods; see [44, pp. 137­138]. The Cram´ er-Rao bound for biased estimation requires knowledge about the bias as a function of the parameter to be estimated. As already mentioned, the method will be unbiased and even statistically efficient for Gaussian AR processes if the model reduction step is omitted. Despite the fact that an algorithm based on covariance estimates (2.6) is not asymptotically efficient for general ARMA models [44, p. 144], our method can be used to provide a starting guess for other algorithms, for example the maximum likelihood method.
6 SBT dashed line, DBT dotted line. 5



0

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identification estimates.

To illustrate our procedure, let us consider data generated by passing white noise through a "true system" with transfer function W (z ) = z 5 - 0.0550z 4 - 0.1497z 3 - 0.2159z 2 + 0.1717z - 0.0495 . z 5 - 0.7031z 4 + 0.3029z 3 + 0.1103z 2 - 0.1461z + 0.2845

18

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy ^  of degree  = 24 determined from estimated covariances. Based on 100 model W test runs, the empirical means and standard deviations are determined. Figure 6.1 illustrates the statistical bias as a function of the length N of the data string when using stochastic (dashed curve) and deterministic (dotted curve) model reduction respectively. For the same test runs, Figure 6.2 illustrates the corresponding standard deviations together with the Cram´ er-Rao bound (solid curve). More precisely, the figures depict the sums of the moduli of the biases and standard deviations respectively for the ^ (z ). coefficients of the numerator and denominator polynomials of W
5 CRB solid line, SBT dashed line, DBT dotted line. 4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the Cram´ er-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z ) with poles and zeros closer to the unit circle is considered. The poles and zeros of ^ (z ) are determined for 100 runs and a data length N = 500. As before,  = 24. W Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with the poles and zeros of W (z ), which are denoted by "".

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and  = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT. To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and  = 24.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identification.

Figure 6.5 describes the result obtained when applying stochastic subspace identification to the same data. More precisely, Algorithm # 2 in [43] is used. In order to make the experiments comparable, we have chosen a Hankel matrix of dimension 13 × 13, which corresponds to  = 25 in our procedure. Note that the estimates are much less focused, and many zeros tend to cluster on the unit circle, implying that coercivity becomes critical. This is related to the positivity issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the subspace identification method performs worse than our SBT identification method, yielding larger biases and standard deviations, but performs better than when DBT is used.

20

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will disappear as  and N are increased. In Figure 6.6 we show the same experiment for  = 64 and N = 2000.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and  = 64.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and  = 40 using Burg's method.

In practice, there is a trade-off between the quality of the ergodic estimates, which roughly speaking depend on |max (A)|, the  -error tolerance, which is a function of |max (A - BD-1 C )|, and the numerical accuracy of the computations. For example, if the zeros of W (z ) are far from the unit circle and  is chosen very large, the error may increase. In the present example, it turns out that using Burg's method [3] in lieu of the ergodic estimate (2.6) yields better estimates for smaller  and N , as illustrated in Figure 6.7 which shows the case N = 500 and  = 40. A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There we give the empirical bias and standard deviation for the coefficients of the numerator and the denominator, respectively, of the estimated transfer functions together with the Cram´ er-Rao bound. It is the authors experience that Burg's method gives at least as good results as when using the ergodic covariance estimate (2.6), unless the intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

21

Parameter True value Bias: CE: Burg: Std.dev.: CE: Burg: CRB:

2 w b1 b2 b3 b4 b5 1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491 0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895 0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734 0.2332 0.1314 0.0508 0.0611 0.0722 0.0802 0.0712 0.0411 0.0381 0.0339 0.0339 0.0356 0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and  = 40 using covariance estimation (CE) or Burg estimation and , in both cases, followed by SBT.

Parameter a1 a2 a3 a4 a5 True value -0.6281 0.3597 0.2634 -0.5322 0.7900 Bias: CE: 0.0087 -0.0044 -0.0003 0.0066 -0.0152 Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125 Std.dev.: CE: 0.0274 0.0304 0.0371 0.0305 0.0304 Burg: 0.0336 0.0307 0.0358 0.0324 0.0306 0.0293 0.0321 0.0342 0.0322 0.0290 CRB: Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and  = 40 using covariance estimation (CE) or Burg estimation and, in both cases, followed by SBT.

7. Conclusions We have presented a three-step procedure for identification of time series, which is easy to understand and implement. Just like for subspace identification methods, robust linear-algebra algorithms can be used and no nonconvex optimization computations are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identification, as our extensive simulations indicate. In particular, its good performance has been confirmed by Monte Carlo simulations. The paper only covers the scalar case, but the multivariate case is presently being worked out. The three steps, covariance estimation, covariance extension and model reduction have each been studied separately before. This is an advantage which should make the method easy to grasp. However, a comprehensive study of the entire identification strategy, giving appropriate bounds, has been missing and this is what we offered here. The observation that the Schur parameters converge geometrically simplifies our application of Szeg¨ o theory and allows us to give a complete account of the asymptotic behavior of maximum entropy models of growing order. This analysis provides us with a clear indication as to when the identification strategy is good and when it might face difficulties, based purely on the closeness of the maximum modulus zero to the unit circle. The parsimony permeating other system identification methods should not be a reason for refraining from high-order modeling as an intermediate step. In fact, such a strategy might be desirable, since we have shown that the poles of the "true" system which lie outside a circle in the complex plane containing all of its zeros are

22

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the perimeter of this circle, providing a justification for choosing stochastically balanced model reduction, rather than deterministically balanced truncation, in the last step. With this reduction procedure, we have confirmed better statistical properties with variances closer to the Cram´ er Rao bound. The procedure could also be modified by exchanging exact covariance extension for approximate one, as outlined in [35]. Even though, in general, stochastic balancing would require the solution of a pair of Riccati equations, this is not the case for the particular maximum entropy models used here. In fact, the balancing procedure only requires linear algebra, and hence an intelligent use of the Levinson algorithm may substantially reduce the number of arithmetic operations. Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N ,  and n ^ , we gave worst-case guaranteed bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive stochastic system, but the method returns a valid model also for generic data. In fact, in contrast to many stochastic subspace identification [9], all steps of the procedure preserve the positive real property. Appendix A. Asymptotic behavior of the maximum entropy filter Theorem 3.4 is actually a modification to the rational setting of a theorem due to Szeg¨ o [47], and the proof is modeled after [19], which in turn includes aspects already present in the work of Schur [45]. See also [48], [49] and [16] for more facts on orthogonal polynomials. However, rationality and coercivity allows us to present a simplified and self-contained proof of a version of Szeg¨ o's classical theorem, to which we also are able to add geometric convergence. The derivation of Caines and BaykalG¨ ursoy [7] is shorter, but we feel that our approach is more systematic and gives additional insight into the mechanism of identification. To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas. o polynomials (3.7). Then |z -  (z )| Lemma A.1. Let { (z )} 0 be the normalized Szeg¨ is uniformly bounded from above and away from zero in the complement Dc of the open unit disc, i.e., there are positive numbers ,   R such that   |z -  (z )|   for all  and all z  Dc . Proof. In view of the Szeg¨ o-Levinson recursion (3.4), t+1 (z ) = t (z ) z -  ¯t and hence z -  (z ) =
 -1

 t (z ) , t (z )  k (z ) . k (z )

¯k 1 - z -1 

k=0

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , z are the roots of  (z ), it is immediately seen that 1 - zz ¯k   (z ) , =  (z ) k=1 z - zk which is a Blaschke product, analytic in Dc and having modulus one on the unit circle, and thus modulus less than or equal to one in Dc . Hence, since |z -1 |  1 in Dc ,
 -1 

(1 - |k |)  |z
k=0

-

 -1

 (z )| 
k=0

(1 + |k |)

(A.1)

for all z  Dc . But, these products converge to positive numbers as   . This follows from the absolute convergence of the infinite sum  k=0 |k |, a fact that, in the present context, stems from Lemma 3.1. From (3.5) we also have 0 < r  r  r0 , and consequently the lemma follows. Remark A.2. An equivalent statement of this lemma is that the maximum entropy solution W (z ), defined by (3.6), is uniformly bounded from above and away from zero for all  and z  Dc . Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of functions f (z ) := z -  (z ) converges uniformly to an analytic function f in statement of Theorem 3.5.

Dc ,

where

Dc 

is defined in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the purely algebraic relation


k (z )k (w) =
k=0

  ¯  (z ) (w)  (z ) (w ) - z w , 1 - zw ¯

(A.2)

which is called the Christoffel-Darboux-Szeg¨ o formula. In particular, setting w = 0 -1 and exchanging z for z in (A.2), (3.5) and (3.7) yield f (z ) 1 k-1 - k (z -1 )  .  = r c0 k=1 rk


(A.3)

c Observe that k (z -1 ) is analytic and bounded in Dc  , and hence in D , and therefore it  belongs to - . Moreover, by the maximum modulus principle, it attains its maximum value in Dc on the unit circle where, by Lemma A.1, it is bounded by  . Hence

H

|k (z -1 )|  

for z  Dc and for all k.

(A.4)

Therefore, in view of (A.3) and the fact that rk  r , we have f (z ) fµ (z )    -  r rµ r


|k-1 |,
k=µ+1

(A.5)

24

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for sufficiently large  and µ.  . The same holds for f (z ). In This establishes (A.3) as a Cauchy sequence in - c fact, since r  c0 , for all z  D

H

|f (z ) - fµ (z )| 

f (z ) fµ (z ) c0  -  r r  f (z ) fµ (z ) 1 1  c0  -  + |fµ (z )|  -  . r rµ r rµ



(A.6)

But, by Lemma A.1, |fµ (z )|   for all  and z  Dc , and therefore, in view of (A.5), we obtain |f (z ) - fµ (z )|   c0 r 1 1 |k-1 | +   -  r rµ k=µ+1


for all z  Dc . (A.7)

Since r  r as   , we see that, for each > 0, |f (z ) - fµ (z )| < for sufficiently large  and µ. Consequently, f tends uniformly in Dc to a function  . f  H- The uniform convergence and the analyticity can be extended to any compact -1  Dc . Therefore, by subset of Dc  . To see this, first note that z  D if and only if z Lemma A.1, |k (z -1 )|   |z |-k for z  D, and consequently, since r  rk , (A.3) yields 1 |f (z )|   +  |z |-1 |k ||z |-k . c0 k=0 Similarly, instead of (A.5) we have f (z ) fµ (z )  |k ||z |-k .   |z |-1  -  r rµ r k =µ
 -1  -1

(A.8)

(A.9)

> 0 such Now, for any compact subset K  Dc  , there is a   (, 1) and an -k ^ k where that |z | >  + for all z  K . Hence, by Lemma 3.1, |k ||z |  M   ^ :=  ( + )-1 < 1. Consequently, by (A.8), f (z ) is uniformly bounded in K , and (A.9) can be made arbitrarily small for sufficiently large  and µ. Therefore, by (A.6), f tends uniformly in K to the analytic function f . Lemma A.4. Let  be a real number such that  <  < 1. Then f -f Proof. It follows from (A.7) that   c0 |k-1 | + 1 - |f (z ) - f (z )|   r k= +1
 

= O(  ).

r r

for all z  Dc . (A.10)

By Lemma 3.1, the first term is O(  ). It remains to show that the same holds for the second term. To this end, first note that, by (3.5), 1- r =1- r k =
 2 1 - k .

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

25

But, by Lemma 3.1, |k |  M  k for some M . Therefore, since each x  [0, 1],  r  1 - (1 - M  k ) = O( t ) 1- rt k =t for t large enough. This concludes the proof.



1 - x2  1 - x for

Recalling the definition (3.6) of W , we note that Lemma A.4 may be written
-1 W - f 

= O(  ).

-1 in the same manner. As it turns out, by coercivity, this implies that W  f

Lemma A.5. Let W be the transfer function (3.6) of the maximum entropy filter. Then -1  W - f  = O ( ), where f is the limit function of Lemma A.3. Proof. Note that the limit function f has the same uniform bounds as f in Lemma A.1. In particular, |f (z )|  , |f (z )|-1  -1 , and |W (z )|  -1 for all z  Dc . Consequently,
-1 W - f 

 W



-1 f



-1 W - f



-1   -2 W - f

,

so the required result follows from Lemma A.4. Lemma A.6. Let W be the rational minimum-phase function defined above, and let f be the limit function in Lemma A.3. Then W (z ) = f (z )-1 for all z  Dc . Proof. Let  (ei ) := |W (ei )|2 be the spectral density of the maximum entropy process. Then, in view of the interpolation condition, 1 2


e
-

ik

1 (e )d = ck = 2
i

 -

eik  (ei )d

for k = 0, 1, . . . , , (A.11)

from which we have pointwise convergence of the Fourier coefficients of  (ei ) to those of (ei ) as   , and hence  (ei )  (ei ) in the 2 sense. However, by Lemma A.5,  (ei )  |f (ei )|-2 in  norm, and hence a fortiori in 2 norm, as   . Since, in addition, not only (ei ) but also f is analytic in a neighborhood of the unit circle (Lemma A.3), we have

L

L

L

(ei ) = |f (ei )|-2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer. In particular, W is an outer spectral factor of  (ei ) satisfying W (z ) = exp 1 4
 -  -

eit + z log |W (eit )|2 dt . eit - z

But Lemma A.5, Equation (A.12) and the fact that (ei ) = |W (ei )|2 , W (z )  exp 1 4 eit + z log |W (eit )|2 dt = W (z ), it e -z

the outer spectral factor of . But, by Lemma A.3, W (z )  f (z )-1 in therefore f (z ) = W -1 (z ) as claimed.

Dc  , and

26

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6. Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6. Proof of Theorem 4.3. Following [53] we see that
-1 (W - Wred ) W 

 ,

(A.13)

and consequently |W (ei ) - Wred (ei )|  |W (ei )| holds for all , from which we have (1 - )|W (ei )|  |Wred (ei )|  (1 + )|W (ei )|. However, in view of (3.6) and (3.7), it follows from (A.1) that   r r i  |W (e )|   -1 ,  -1 k=0 (1 + |k |) k=0 (1 - |k |) which together with (3.5) yields c0  |W (ei )|    (A.14)

for all . This establishes (4.27). To see that Wred is minimum phase if < 1, note e's that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by Rouch´ c Theorem, Wred has the same number of zeros in D (including ) as W . Hence, since W is minimum phase, so is Wred . To establish the bound (4.28) note that W - Wred From (A.14) we have W
 

 W



-1 W (W - Wred )

.

 , and hence (4.28) follows from (A.13).

Appendix B. Statistical convergence proofs Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corre^ from (3.4) and (3.5), sponding Szeg¨ o polynomial  ^ (z ) and predictor error variance r and form the maximum-entropy filter   r ^ z ^ . W (z ) =  ^ (z ) ^ To determine W - W let z  Dc and form     r z r ^ z ^ W (z ) - W (z ) = -  (z )  ^ (z )    ^ )z -  (z ) - r z - ( (z ) -  ^ (z )) ( r - r . = z -  (z )z -  ^ (z )


Since r > 0, by (3.7) and Lemma A.1,   0 < µ := r   |z -  (z )|  c0  =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1), |z
-  -1

 ^ (z )|  µ ^ :=
k=0

(1 - | ^k |),

^1 , . . . ,  ^ -1 are the Schur parameters corresponding to the estimated cowhere  ^0 ,  variances (2.6). Therefore, by the maximum-modulus principle, ^  (z )|  max 1 {M |r - |W (z ) - W |z |=1 µµ ^ r ^ | +  c0 | (z ) -  ^ (z )|},

where we have also used the fact that r  c0 . But, for |z | = 1, ^  1, ^ (z )|   -  | (z ) -  ^  are the  -vectors formed as in (4.19) and · where  and  that  is the unique solution of the normal equations T  = -c where c := c c -1 . . .
1

is the

1

norm. Recall (B.1)

c1 ,

where T is the Toeplitz matrix defined by (2.4), and that r  = c 0 + c   . ^ . Then, Also, the analogous relations hold for  ^ and r ^  )  + c ^ ( -  ^ ) ^ = (c0 - c ^0 ) + (c - c r - r and hence ^ ^ |  |c0 - c ^0 | + c - c |r - r Finally,  | r -
1 1

(B.2)





^ + c



^  1.  - 

^ | ^ | |r - r |r - r  r ^ |     , r r + r ^


and consequently, since x ^ W - W


 x M 

for any x  R ,

^ {|c0 - c ^0 | +    c - c µµ ^  r ^  M c 1  ^  . c0 +  +   -  µµ ^ r



}

(B.3)

^  are each solutions of a normal equation (B.1). More Recall now that  and  ¯ for k > 0, where all ^  ^  = -c ^ . Since ck = CAk-1 C precisely, T  = -c and T eigenvalues of A are less than one in modulus, ck  0 exponentially, we have
 -1

c



 K1

and

T



 c0 + 2
k=1

|ck |  K2

for some constants K1 and K2 . Moreover, from [8] we have
-1 T 



1 c0

 -1

1 + |k |  K3 1 - |  k| k=0

28

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence  and the condition number (T ) := T
 -1 T   -1  T 

c



 K 1 K3

 K := K2 K3

is bounded for all  . Now, it is known [25] that for each data length N in (2.2), there is a  (N ) of order O(log N ) such that ^k | = O max |ck - c log log N N , (B.4)

0 k   (N )

and therefore, for any a  R, ^ ^0 |  0 and  a c - c  a |c0 - c


 0 as  =  (N )  .

(B.5)

Consequently the first term in the bound (B.3) tends to zero as N   and  (N )   provided it is done at the specified relative rates and provided µ ^ is bounded away from zero. However, the estimate (2.6) has the property that the ^ is positive definite for each finite  , and this in turn corresponding Toeplitz matrix T ^ > 0. Since, in addition is equivalent to | ^k | < 1 for k = 0, 1, . . . ,  - 1 so that µ µ ^  µ > 0 as  (N )   by (B.4) and continuity, the second requirement is also fulfilled. To simplify notations, we have suppressed the index N in the quantities marked with a hat, which of course depend on the data (2.2) and hence also on N . ^   Next we show that also the second term in (B.3) tends to zero. Since c ^  is bounded, it thus remains to demonstrate that c   + c - c ^  (N )    (N ) - 


 0 as  (N )  .

This follows from the more general fact, needed for the proof of Corollary B.1, that ^  (N )  a  (N ) - 


 0 as  (N )  

(B.6)

for any a  R. To prove this, first note that ^ T - T


^  |c0 - c ^0 | + 2 c - c

,

-1 ^   0. Therefore  := T - T ^  T and hence T - T  < 1 for  :=  (N ) sufficiently large, and, provided c = 0, the standard perturbation estimate [22] yields

^  -   





1 (T ) 1 - 

^ T - T T 



+

^ c - c c 



,

(B.7)

and consequently, since T   c0 > 0, it follows from (B.5) that (B.6) tends to zero in the required manner. If c = 0,  = 0, and hence ^  - 


^ = 



-1 ^  T



^ c



-1 ^ = T



^ c - c

,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case c = 0. In fact, since µ ^ is bounded away from zero, by continuity, for each > 0, there is a N0 such that ^ -1 T  for   N0 .


1  c ^0

 -1

1 1 + | ^ k|  1 - | ^ c0 k| k=0

 -1

1 + |k | +  K3 + 1 - |k | k=0

Corollary B.1. If  (N ) is defined as in Theorem 5.1, then, for any a  R, ^  a W - W


0

almost surely as  :=  (N )  .

To prove Theorem 5.3, we first note that the Hankel operator H, defined by (4.1), has a nice representation in the space 2 of square-integrable functions. In fact, let 2 2 of functions with vanishing negative Fourier coefficients, + be the subspace in hence being analytic in the unit disc D. In this setting, H has the representation 2 2  2 H : + + given by

H

L

L

H

L H

H f = P  f, where P  is the orthogonal projection onto the orthogonal complement 2 2 , and where  is the  -function + in

H

L

L

L H
2

(B.8)
2 +

of

¯ + (z )-1 . (z ) = W- (z )W

(B.9)

¯ + (z ) are the analytic and coanalytic minimum-phase spectral facHere W- (z ) and W ¯ + (z ) = tors defined in Section 4. (See, e.g., [30, 31].) In the present scalar case, W -1 W- (z ). In fact, the phase function  is the transfer function of an all-pass filter transforming the white noise w- in (4.2) to the white noise w+ in (4.3) [30, p. 834]. ^ ¯ + be the stochastic measures such that Let dw ^- and dw
 

w- (t) = Then

-

eit dw ^-


and w ¯+ (t) =


-

^ eit dw ¯+

H+ = H- = and consequently H := E f  f dw ^- .
H- -  -

H

2 ^ ¯+ + dw

=
-

H

2 it ^- + (e )dw

L H
2

2 ^- + dw

|H + corresponds to H under the isomorphism defined by

^  (ei )| - |W (ei )|  0 Proof of Theorem 5.3. It follows from Theorem 5.2 that |W uniformly in  as   , and hence, by Lemma A.1, there are positive real numbers µ1 and µ2 such that ^  (ei )|  µ2 µ1  |W for all  and sufficiently large  . Therefore, since -1 ^ ^ - W ^  W ^  W ^ ^) W (W - W
,

(B.10)

30

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

(A.13) and (4.26) imply that ^ - W ^ W


 2µ2

 ^k , 1- ^k k=^ n+1



(B.11)

^2 , . . . ,  ^ are the singular values (5.1) determined for sufficiently large  , where  ^1 ,  from the covariance estimates (2.6). It is well-known (see, e.g., [56, p. 204]) that the singular value k of the Hankel operator H , defined by (B.8) equals the infimum of H - K over all operators 2 2 -1  2 K : + + of finite rank at most k . Recall that (z ) = W (z )/W (z ). ^ ^ ^ -1 The singular value  ^k of H ^ , where (z ) = W (z )/W (z ), is described analogously. Therefore, since

H

L H

^ H ^ - K  H ^ - H  + H - K   - 



+ H - K ,
.

^ -   + k . But, for k > n, k = 0, and hence  ^ - we have  ^k   ^k   Consequently, (B.11) yields ^ ^ - W W


^ -  M1  

,

(B.12)

-1 ^n where M1 := 2µ2 (1 -  ^ +1 ) . However,

^  (z -1 ) - W (z -1 )] , ^  (z ) - W (z ) - (z )[W ^ z ) - (z ) = W ^  (z -1 )-1 W ( ^  (z -1 ) so, since W


is uniformly bounded by (B.10), and  ^ - 




is constant,

^  M2 W - W

,

which together with (B.12) yields ^ ^ - W W


 M1 M 2  W - W 



^ + M1 M2  W  - W



for sufficiently large  . Consequently the theorem follows from Theorem 3.4 and Corollary B.1. Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A. Gombani, W. B. Gragg, G. Picci and T. S¨ oderstr¨ om for stimulating discussions and for providing us with appropriate references. We are also indebted to the anonymous referees for several useful suggestions. References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987. 2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489­502, 1974. 3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975. 4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational covariance extension problem. SIAM Journal on Control and Optimization, 37:211­229, 1999. 5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841­1857, 1995. 6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast filtering algorithms. SIAM Journal on Control and Optimization, 32:744­789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-G¨ ursoy. On the L consistency of L2 estimators. Systems & Control Letters, 12:71­76, 1989. 8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978. 9. A. Dahl´ en, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace identification methods may fail. Systems and Control Letters, 34:303­312, 1998. 10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced stochatic realizations. In Proc. 21st IEEE CDC, pages 1105­1112, 1983. 11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29:1097­1100, 1984. 12. J. Durbin. Efficient estimation of parameters in moving average models. Biometrika, 46:306­316, 1959. 13. J. Durbin. The fitting of time-series models. Rev. Inst. Int. Stat., pages 223­243, 1960. 14. P. Duren. Theory of Hp spaces. Academic Press, 1970. 15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear. 16. G. Freud. Orthogonale Polynome. Birkh¨ auser Verlag, 1969. 17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic Control, AC-32:358­361, 1987. 18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac., Speech and Signal Processing, ASSP-35:438­449, 1987. 19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961. 20. M. Gevers. Towards a joint design of identification and control. In J. Willems and H. Trentelman, editors, Essays on Control: Perspectives in the Theory and its Applications, 1993. 21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their l -error bounds. Int. J. Contr., 39:1115­1193, 1984. 22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989. 23. W. B. Gragg. Positive definite Toeplitz matrices, the Arnoldi process for isometric operators, and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in Linear Algebra, pages 16­32. Moscow U. P., 1982. 24. U. Grenander and G. Szeg¨ o. Toeplitz forms and their applications. Univ. California Press, 1958. 25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons, 1988. 26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples. Biometrika, pages 297­307, 1989. 27. W. Jones and E. Saff. Szeg¨ o polynomials and frequency analysis. In Approximation Theory, pages 341­352. Dekker Inc., 1992. 28. S. Y. Kung. A new identification method and model reduction algorithm via singular value decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705­714, 1978. 29. W. E. Larimore. System identification, reduced ordered filtering and modeling via canonical variate analysis. In Proc. of the American Control Conference, 1983. 30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes. SIAM J. Control and Optimization, 23:809­857, 1985. 31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic systems. J. of Math. Systems, Estimation and Control, 1:241­333, 1991. 32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension, and identification of stationary time series. Automatica, 32(5):709­733, 1996. 33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412­440, 1992. 34. L. Ljung and Z. Yuan. Asymptotic properties of black box identification of transfer functions. IEEE Trans. Automatic Control, AC-26:514­530, 1985. 35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis, Royal Instiute of Technology, 1998. 36. D. Q. Mayne and F. Firoozan. Linear identification of ARMA processes. Automatica, 18:461­466, 1982.

32

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

37. H. Mhaskar and E. Saff. The distribution of zeros of asymptotically extremal polynomials. J. Approx. Theory, 3:279­300, 1991. 38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66­73, 1978. 39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward, editors, Approximation Theory VI, 1989. 40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991. 41. P. Van Overschee. Subspace Identification, Theory - Implementation - Application. PhD thesis, Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De Moor. 42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identification problem. In Proc. 30th Conference on Decision and Control, Brighton, 1991. 43. P. Van Overschee and B. De Moor. Subspace Identification for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996. 44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994. ¨ 45. I. Schur. Uber Potenzreihen, die im Innern des Einheitskreises beschr¨ ankt sind. J. f¨ ur die Reine und Angewandte Mathematik, 147:205­232, 1917. 46. J. Sorelius, T. S¨ oderstr¨ om, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identification. In Proc. SYSID '97, 1997. 47. G. Szeg¨ o. Beitr¨ age zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift, 6:167­202, 1920. ¨ 48. G. Szeg¨ o. Uber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232­244, 1921. 49. G. Szeg¨ o. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications, 1939 (4th edition 1975). 50. B. Wahlberg. On the Identification and Approximation of Linear Systems. PhD thesis, Link¨ oping University, 1987. Link¨ oping Studies in Science and technology. Dissertations No. 163. 51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive approximations. Journal of Time Series Analysis, 10:283­299, 1989. 52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like identification techniques. IEEE Trans. Automatic Control, 37:900­912, 1992. 53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation. Systems and Control Letters, 14:307­317, 1990. 54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423­ 434, 1953. 55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938. 56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

