A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION
OF TIME SERIES*
JORGE MARI†, ANDERS DAHLÉN†, AND ANDERS LINDQUIST†

Abstract. In this paper we consider a three-step procedure for identiﬁcation of
time series, based on covariance extension and model reduction, and we present a
complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy
model is determined, which is ﬁnally approximated by a lower-order model by
stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all
three steps has been lacking. Supposing the data is generated from a true ﬁnitedimensional system which is minimum phase, it is shown that the transfer function
of the estimated system tends in H∞ to the true transfer function as the data length
tends to inﬁnity, if the covariance extension and the model reduction is done properly. The proposed identiﬁcation procedure, and some variations of it, are evaluated
by simulations.

1. Introduction
In recent years there has been quite some interest in a certain type of procedures
for identiﬁcation of time series known as subspace methods [1, 42, 41, 28, 29]. These
identiﬁcation procedures are based on geometric projection methods, and they could
be understood in the context of splitting geometry and partial stochastic realization
theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9],
these procedures are algebraically equivalent to minimal factorization of a Hankel
matrix of covariance estimates, and they make no distinction between stochastic and
deterministic partial realizations. Therefore they may fail because of loss of positive
realness in the spectral estimation phase.
In an attempt to overcome these problems we analyze an alternative approach
to time series identiﬁcation proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the
maximum-entropy method, leading to a high order autoregressive (AR) process, and
ﬁnally stochastically balanced truncation. This method shares certain features with
stochastic subspace identiﬁcation methods, the most obvious one being that it is
based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
∗ This research was supported by a grant from the Swedish Research Council for Engineering
Sciences (TFR).
† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
1

2

J. MARI, A. DAHLÉN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for
maximum-likelihood (ML) methods, are needed.
The idea of approximating an autoregressive moving-average (ARMA) process by
an AR process is by no means new. Its origins can be traced back to the Wold
decomposition [55] where L2 -convergence of high-order AR models to general analytic
models is shown. Pioneers in the use of this concept for systems identiﬁcation are
Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations
were studied by Berk [2] and later reﬁned in [36, 34, 33, 7]. The interesting paper [7]
contains nice proofs of some of the convergence results needed in this paper, but, for
the sake of completeness and insight, we provide new proofs based on some properties
of fast ﬁltering algorithms [5] and simple methods of complex analysis and Szegő
polynomials. The power of the theory of Szegő polynomials and Toeplitz matrices in
analyzing stochastic processes is reported in [24], but, except for elementary theory,
it has not been much used in systems identiﬁcation [39]. This is even more true for
the newer results [16, 40, 37, 27] on orthogonal polynomials.
The idea of using model reduction for systems identiﬁcation appears in the thesis
by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency
weighted reduction. Instead, we use stochastically balanced truncation, for which we
develop a simple computational procedure, exploiting the special structure of the AR
model. We also show the advantage of this reduction procedure by theoretical analysis
and simulations. In fact, a comprehensive study comprising all the steps mentioned
above together with a qualitative and quantitative analysis of the entire identiﬁcation
strategy has been lacking, and that is what we oﬀer in this paper.
The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identiﬁcation
procedure contributes to the estimation error. In Section 3 we show that the transfer
function of the maximum-entropy ﬁlter, constructed from true covariances, tends to
that of the true ﬁlter in H∞ norm at a geometric rate determined by the largest
modulus of the zeros of the true ﬁlter as the order of the maximum-entropy ﬁlter
becomes large. However the order of the approximation is too high, and therefore
model reduction is performed. This is studied in Section 4. A stochastic balancing
procedure, based only on linear-algebra operations so that no Riccati equations need
to be solved, is provided together with the analysis of the model-reduction error.
Both deterministically and stochastically balanced truncation lead to good results.
However, when the covariances are estimated from statistical data, stochastic model
reduction is found to be superior. In particular, variances are considerably closer to
the Cramér-Rao bounds. In Section 5 we state our statistical convergence theorems,
proving that the total error tends to zero as the length of the data string tends to
inﬁnity, provided the degree of the AR model tends to inﬁnity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using
stochastic subspace identiﬁcation [43] is included. For clarity of exposition, all the
proofs have been deferred to two appendices, Appendix A dealing with the asymptotic
properties of the maximum-entropy ﬁlter, and Appendix B devoted to the statistical
error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identiﬁcation
Time series identiﬁcation in the form studied here amounts to estimating the matrices
(A, B, C, D) in some n-dimensional linear stochastic system

x(t + 1) = Ax(t) + Bw(t)
(2.1)
y(t)
= Cx(t) + Dw(t)
driven by normalized white noise {w(t)}, from a data string of observations
{y0 , y1 , y2 , . . . , yN }

(2.2)

of the output process {y(t)}, which here will be taken to be scalar.
The basic idea behind our approach is very simple: given estimates of a partial
sequence
c 0 , c 1 , c 2 , . . . , cν

(2.3)

of the covariances ck = E{y(t+k)y(t)}, which satisﬁes the condition that the Toeplitz
matrix


c2 · · · cν
c0 c1
 c1 c0
c1 · · · cν−1 



c
c
c0 · · · cν−2 
1
Tν+1 :=  2
(2.4)
..
.. 
..
...
 ...

.
.
.
cν cν−1 cν−2 · · · c0
is positive deﬁnite, ﬁrst construct a high-order model continuing (2.3) by covariance
extension. This model has all the required positivity properties, but the order is too
high. Then reduce the order by means of a positivity-preserving model reduction
procedure to be speciﬁed below. That this simple recipe will in fact provide a good
identiﬁcation method is by no means a trivial matter but is based on some rather
deep results, which will be presented here.
More speciﬁcally, the approach consists of three steps, for which there are several
possible variants that will be discussed below. The rigorous mathematical analysis,
however, will be carried out for the following procedure, for which we shall give
theoretical bounds.
(i) Estimate a partial covariance sequence
ĉ0 , ĉ1 , ĉ2 , . . . , ĉν

(2.5)

from the time-series data (2.2) via the ergodic estimate
N −k
1 	
yt+k yt
ĉk =
N + 1 t=0

k = 0, 1, . . . , ν.

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer
function
zν
,
(2.7)
Ŵν (z) =
φ̂ν (z)
where φ̂ν (z) is the normalized Szegö polynomial of degree ν, to be introduced
in Section 3, computed from the estimated covariance data (2.5).

4

J. MARI, A. DAHLÉN, AND A. LINDQUIST

(iii) Determine a reduced-degree approximation Ŵ (z) of Ŵν (z) via a stochastic
model reduction procedure [11] to be described in more detail in Section 4.
In this procedure, the idea is that ν >> n, the order of the system to be identiﬁed,
and ideally n̂ := deg Ŵ equals the degree n of the true system (2.1). However, the
method will produce a valid model even if this is not the case or even if there is
no “true” underlying model. This is in contrast to stochastic subspace identiﬁcation
models, which may fail to produce any model at all [9].
There are possibilities for variations of the procedure described above. In Step (i)
we could use alternative covariance estimates or Burg’s estimation of Schur parameters
[3], the only requirements being that the estimated Toeplitz matrix T̂ν+1 of (2.5) is
positive deﬁnite and that ĉk → ck a.s. as N → ∞. In Step (ii) we could instead use
approximate covariance extension or covariance extension with prescribed zeros, for
which there is now a complete parameterization [5] and an algorithm [4]. (In the latter
case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction
methods could be used. For example, an important model reduction paradigm is the
one based on optimal Hankel norm approximation [21].
Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable
transfer function
W (z) = C(zI − A)−1 B + D,

(2.8)

of McMillan degree n. We also assume that W (z) is minimum-phase so that both
zeros and poles are located in the open unit disc. Then, we need to be able to measure
how the estimated model, with transfer function Ŵ (z), converges to the true one as
N → ∞. In this paper we have chosen to use distance between W (z) and Ŵ (z) in
∞
norm as a measure of proximity between the true and estimated model. From an
engineering point of view this could be called worst case identiﬁcation. The modern
literature in robust control makes extensive use of the worst case philosophy; see for
example [20, 52]. There are also other reasons for using the ∞ , as discussed in [35].
Returning, then, to the identiﬁcation approach outlined above, the estimation error
can be decomposed into three parts, one corresponding to each of the steps (i), (ii)
and (iii). Hence we have the error bound

L

L

W − Ŵ ∞ ≤ W − Wν ∞ + Wν − Ŵν ∞ + Ŵν − Ŵ ∞ ,

(2.9)

where Wν is the AR model corresponding to the true covariances (2.3) and Ŵν is
the one determined from the estimated covariances (2.6). To prove convergence to
zero of the estimation error (2.9), we shall need to assume that W is minimum-phase,
and hence Ŵ should have the same property, which moreover is desirable in many
applications. Our procedure insures this.
Estimating the ﬁrst term in (2.9) is a problem in stochastic partial realization
theory and function theory and will be dealt with in the next section. The third term
concerns model reduction which will be studied, in the particular setting required
here, in Sections 4 and 5. In Section 5, ﬁnally, we consider the second term together
with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence
Step (ii) in the identiﬁcation procedure outlined in Section 2 is based on rational
covariance extension. To understand this, let us consider the covariance extension
problem from a more general point of view. Given a partial covariance sequence
c 0 , c 1 , c 2 , . . . , cν ,

(3.1)

covariance extension amounts to ﬁnding an inﬁnite extension cν+1 , cν+2 , cν+3 , . . . of
this sequence such that the function
V (z) := 12 c0 + c1 z −1 + c2 z −2 + . . .

is strictly positive real, i.e., it is an analytic function in the complement Dc of the
open unit disc D, which maps Dc to the open right complex half-plane. Then
Φ(z) := V (z) + V (z −1 )
is a spectral density for a process having c0 , c1 , . . . , cν as its ﬁrst ν covariances and
which is coercive in the sense that
Φ(eiθ ) > 0 for all θ.
Spectral factorization is then to ﬁnd a stable transfer function W (z) such that
|W (eiθ )|2 = Φ(eiθ ).
In particular, we are interested in ﬁnding covariance extensions for which V (z), and
hence W (z), have at most degree ν.
For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is
classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one
correspondence between inﬁnite covariance sequences
c0 , c1 , c2 , c3 , . . .

(3.2)

and a sequence of Schur parameters, or reﬂection coeﬃcients,
γ0 , γ1 , γ2 , γ3 , . . . ,

(3.3)

with the property |γt | < 1 for all t. In fact, ﬁxing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and γ0 , γ1 , . . . , γm−1 for
each m. The Schur parameters can be determined from the covariances via the Szegö
polynomials
ϕt (z) = z t + ϕt1 z t−1 + · · · + ϕtt t = 0, 1, 2 . . . ,
computed by means of the Szegö-Levinson recursion
 






 
 


z
−γt ϕt (z)
ϕ0 (z)
1
ϕt+1 (z)
=
;
=
,
(3.4)
−zγt 1
1
ϕ∗t+1 (z)
ϕ∗t (z)
ϕ∗0 (z)
where

ϕ∗t (z) := z t ϕt (z −1 )
is the reciprocal polynomial of ϕt (z), and the Schur parameters are computed via

= r1t tj=0 ϕt,t−j cj+1
γt
(3.5)
rt+1 = rt (1 − |γt |2 ), r0 = c0 .

6

J. MARI, A. DAHLÉN, AND A. LINDQUIST

Hence γt = −ϕt+1 (0), a fact that we shall use below.
In the problem to ﬁnd a covariance extension for (3.1), therefore, γ0 , γ1 , . . . , γν−1
are ﬁxed and the inﬁnite continuation γν , γν+1 , . . . can be chosen freely. In particular,
if we take γt = 0 for t = ν, ν + 1, ν + 2, . . . . We obtain the maximum entropy solution
Wν (z) =

zν
,
φν (z)

(3.6)

where φν (z) is the normalized Szegö polynomial
1
φν (z) := √ ϕν (z).
rν

(3.7)

Thus, in this particular case, the solution to the covariance extension problem turns
out to be rational of degree at most ν as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations
are needed. In fact, it has recently been shown [5] that there is exactly one such
solution for each choice of zeros of Wν (z), thus proving a long-standing conjecture
by Georgiou [18], who had established existence. Nevertheless, as we shall see next,
rationality implies that the Schur parameters tend geometrically to zero, provided
W (z) has no zeros on the unit circle.
In this section we shall demonstrate that the rational transfer function (2.8) can be
approximated arbitrarily closely in L∞ by the transfer function Wν (z) of a maximum
entropy ﬁlter for suﬃciently large ν and that this ν depends on the maximum modulus
of the zeros of W (z). We shall ﬁrst present a heuristic argument in support of this
conclusion.
To this end, let (3.2) be the inﬁnite covariance sequence of the output process y in
(2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via
the Szegö-Levinson algorithm presented above. Then we have the following special
case of Corollary 2.1 in [5].
Lemma 3.1. Let the spectral density
Φ(eiθ ) = |W (eiθ )|2

(3.8)

be coercive in the sense that it is positive for all θ and let (3.3) be the corresponding
inﬁnite sequence of Schur parameters. Moreover, let γ ∈ (0, 1) be greater than the
maximum of the moduli of the zeros of W (z). Then
|γt | = O(γ t ),

(3.9)

i.e., |γt | ≤ M γ t for some M ∈ R and for suﬃciently large t.
Remark 3.2. Since (3.9) holds for all γ greater than the the maximum of the moduli
of the zeros of W (z), we have in fact that |γt | = o(γ t ), i.e., limt→∞ |γt |γ −t = 0.
For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis
of certain fast algorithms for Kalman ﬁltering [6].

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the
spectral density
z(z − 1)2
Φ(z) = − 2
(z + z + 2)(2z 2 + z + 1)
is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters
are seen to be −1/2, −2/3, −2/5, −2/7, −2/9, −2/11, . . . , which tend to zero but not
geometrically. On the other hand, there are coercive, analytic but nonrational models
which also exhibit geometric convergence rate. A classical example [23] is obtained
2
when ck = θk for some θ ∈ (−1, 1). The Schur parameters in this case form an exact
geometric sequence, γk = (−θ)k+1 , k ≥ 0.
Lemma 3.1 implies that, for a suﬃciently large ν which depends on γ, the Schur
parameters γt are close to zero for t = ν, ν + 1, ν + 2, . . . . But, the Schur parameters
of Wν are exactly zero for t = ν, ν + 1, ν + 2, . . . , and hence geometric convergence
would insure that Wν is a good approximation of W (z) for suﬃciently large ν. We
shall prove that this is indeed the case.
Theorem 3.4. Suppose W (z) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let γ ∈ (0, 1) be greater than the maximum of the moduli of the
zeros of W (z). Then
lim Wν − W ∞ = 0,

ν→∞

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that
Wν − W ∞ ≤ M γ ν .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to ﬁrst showing
that
lim Wν−1 − W −1 ∞ = 0.

ν→∞

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we
give an alternative proof of this fact based on Szegö theory, and also show that the
convergence is geometric. In fact, we can choose γ arbitrarily close to the maximum
modulus of the zeros of W .
However, as we shall see next, we can actually prove more. To this end, let us ﬁrst
observe that, since Wν−1 and W −1 have their poles in the open unit disc D and thus
are bounded and analytic in the complement Dc of D, they belong to the Hardy space
∞
of functions which are analytic and bounded in {z ∈ C | |z| > 1}. Hence the
H−
∞
, and
convergence (3.12) is in H−
z −ν φν (z) → W −1 (z)

(3.13)

uniformly in each compact subset of Dc . Now, W −1 is analytic in {z ∈ C | |z| ≥ γ},
a region that is strictly larger than Dc . This in itself of course does not insure that
the convergence (3.13) extends to this larger region. In fact, even if z −ν φν (z) did
converge in {z ∈ C | γ ≤ |z| ≤ 1}, it could fail to converge to W −1 (z) there. The fact
that it really does converge uniformly to this limit is another consequence of Lemma
3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

J. MARI, A. DAHLÉN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and
hence an estimate of the convergence rate γ, is given in [35].
Theorem 3.5. Suppose W (z) is a minimum-phase rational function having all its
poles in the open unit disc D and all its zeros in

Dρ := {z ∈ C | |z| ≤ ρ} ⊂ D

where 0 < ρ < 1,

and let {φν (z)}∞
0 be the normalized Szegö polynomial (3.7) determined from the covariances in the spectral density
|W (e )| = c0 + 2
iθ

2

∞
	

ck cos kθ.

k=1

Then, as ν → ∞, z −ν φν (z) → W −1 (z) uniformly in every compact subset of
{z ∈ C | |z| > ρ}, the complement of Dρ .

Dcρ :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of φν (z) and hence of the poles of the high-order AR
model with transfer function Wν (z). It is known that, if the Toeplitz matrix Tν+1
is positive deﬁnite, all roots of φν (z) are located in the open unit disc D, but little
has been reported in the literature on their behavior as ν → ∞. This behavior is
illustrated in Figure 3.1.
Original system

Original system

1

1

0.5

0.5

0

0

−0.5

−0.5

−1
−1

−0.5

0

0.5

1

−1
−1

Original and AR(24)
1

0.5

0.5

0

0

−0.5

−0.5

−0.5

0

0.5

0

0.5

1

Original and AR(24)

1

−1
−1

−0.5

1

−1
−1

−0.5

0

0.5

1

Figure 3.1: Distribution of zeros of φν (z).

The top two diagrams show the zero-pole positions, within the boundaries of the
unit circle, of two minimum phase spectral factors W , both of degree ﬁve. Also
indicated is a circle of radius equal to the maximum modulus of the zeros of these
spectral factors. The little circles “◦” represent zeros and the “+” sign represent
poles. The lower two ﬁgures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the
exact covariance sequence. The poles of the latter models are indicated with “×”.
The left part of Figure 3.1 illustrates what may happen if all the poles of W (z) are
located in {z ∈ C | |z| < ρ}, where ρ is chosen to be the maximum of the moduli of
the zeros of W (z). The roots of φν (z) tend to cluster inside a circle of radius ρ as
ν → ∞. This phenomenon is in a sense predictable, since the constant term of the
Szegö polynomials is ϕn+1 (0) = −γn , which equals the product of the roots and, by
Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to ρ.
This does not preclude that other types of crowns may occur, because subsequences
of {γn } could decay faster than the overall rate γ, as follows from [5]. Very general
statements about the distribution of zeros of orthogonal polynomials, derived with
the help of potential-theoretic methods, can be found in [37, 27].
To the right in Figure 3.1 we see what happens in the case that W has poles with
moduli larger than ρ. Then, for ν suﬃciently large, the normalized Szegö polynomial
φν (z) has roots in {z ∈ C | ρ ≤ |z| < 1}, but exactly as many as the poles of W in
this region and approximately at the same place as these. This is of course due to
the uniform convergence of z −ν φν (z) to W −1 (z) in every compact subset of Dcρ . The
other roots of φν (z) behave exactly as in the previous case and tend to accumulate in
a crown inside and very close to the circle {z ∈ C | |z| = ρ}.
∞
approximation Wν of W which can be made
We have thus constructed an H−
arbitrarily good by choosing ν suﬃciently large. However, Wν will have much larger
degree and, except for the poles outside the circle {z ∈ C | |z| = ρ}, a completely
diﬀerent zero-pole pattern. We shall rectify this situation by model reduction. In
fact, for the moment considering the perfect modeling problem to identify the rational
transfer function (2.8) given an exact partial covariance sequence (3.1), the last step
in our procedure consists in approximating Wν by a rational function Wred of smaller
degree, ideally of the same degree as W .
The simplest model reduction procedure is deterministically balanced truncation
(DBT), ﬁrst introduced by Moore [38]. Though easy to implement, it may fail to
yield a minimum-phase approximation, a requirement which is important in certain
contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced
truncation (SBT), ﬁrst introduced by Desai and Pal [10], which is based on a diﬀerent
balancing strategy to be explained in detail in Section 4.
Original system

Reduction by DBT

Reduction by SBT

1

1

1

0.5

0.5

0.5

0

0

0

−0.5

−0.5

−0.5

−1
−1

0

1

−1
−1

0

1

−1
−1

0

1

Figure 3.2: Zero-pole pattern of W (z) and Wred (z) for diﬀerent model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This ﬁfth-order

10

J. MARI, A. DAHLÉN, AND A. LINDQUIST

model has ﬁrst been approximated by Wν of degree ν = 24, producing the pole-zero
pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens
when the model is reduced back to order ﬁve by either deterministically balanced
truncation or stochastically balanced truncation. The zeros are denoted by “◦” and
the poles by “+”. Both reduction procedures give good approximations when applied
to exact covariance data. However, as we shall see in Section 5, the advantages of SBT
becomes apparent when applied to statistical data. Also, as explained in Remark 4.5,
there are theoretical reasons to prefer stochastic model reduction.
4. Model reduction
In the present setting, model reduction amounts to replacing a stochastic system
(2.1) of dimension ν by one of some dimension r < ν in such a way that most of
its statistical features are retained. In particular, we want to remove the part of the
system which corresponds to the weakest correlation between past and future. This
idea can be formalized in the following way.
Basic concepts. In the Hilbert space generated by the random variables {y(t) |
−∞ < t < ∞} in the inner product u, v = E{uv}, let H − be the subspace generated
by the past, i.e., {y(t) | t < 0}, and H + that generated by the future {y(t) | t ≥ 0}.
Consider the Hankel operator H : H + → H − and its adjoint H∗ : H − → H + deﬁned
as

H = EH

−

|H +

and

H∗ = E H

+

|H − ,

(4.1)

−

where E H denotes orthogonal projection onto the past space H − . More precisely,
H sends ξ ∈ H + to E H − ξ ∈ H − and H∗ sends η ∈ H − to E H + η ∈ H +. Since the
process y is the output of a minimal stochastic system of dimension ν, rank H = ν by
Kronecker’s Theorem [56], and hence H has exactly ν singular values, σ1 , σ2 , . . . , σν ,
which are positive, as usually listed so that σ1 ≥ σ2 ≥ · · · ≥ σν . These singular
values are the canonical correlation coeﬃcients and hence the cosines of the angles
between the principal directions of the past space H − and the future space H + . They
are therefore less than one, and the part of the stochastic system corresponding to
singular values which are close to zero have a weak coupling between past and future,
i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic
model reduction is to truncate the system so that this part is removed.
To each singular value σk there is an associated Schmidt pair (ξk , ηk ) with ξk ∈ H +
and ηk ∈ H − such that

Hξk = σk ηk ,

H∗ηk = σk ξk ,

and such that the sequences ξ1 , ξ2 , ξ3 , . . . and η1 , η2 , η3 , . . . of singular vectors are
orthonormal. The singular vectors corresponding to nonzero singular values span the
predictor spaces
X− := span{η1 , η2 , . . . , ην },

X+ := span{ξ1 , ξ2 , . . . , ξν }.

Clearly, X− ⊂ H − and X+ ⊂ H + .
The process y has one representation (2.1) for each minimal spectral factor W ,
having W as its transfer function. Such representations are called minimal stochastic
realizations and the corresponding subspaces X := {a x(0) | a ∈ Rν } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X− is the splitting subspace of the stochastic
realization

x− (t + 1) = Ax− (t) + B− w− (t)
(4.2)
y(t)
= Cx− (t) + D− w− (t)
with the transfer function W− (z), the minimum-phase spectral factor; and X+ is the
splitting subspace of

x+ (t + 1) = Ax+ (t) + B+ w+ (t)
(4.3)
y(t)
= Cx+ (t) + D+ w+ (t)
with transfer function W+ (z), the maximum-phase spectral factor, having all its zeros
in Dc . Note that A and C are the same in both realizations (uniform choice of bases).
Each realization has a counterpart which evolves backwards in time and has the
same splitting subspace. For example, the backward realization of X+ ,

x̄+ (t − 1) = A x̄+ (t) + B̄+ w̄+ (t)
,
(4.4)
y(t)
= C̄ x̄+ (t) + D̄+ w̄+ (t)
has transfer function W̄+ (z), the coanalytic minimum-phase spectral factor, having all
its poles and zeros in Dc . In the present case with scalar y, we have W̄+ (z) = W− (z −1 ).
Now, in order to identify the part of the system which has the weakest coupling
between past and future, and hence will be removed in the model reduction, we need
to balance the system in the sense of Desai and Pal, as we shall explain next. To this
end, we make a coordinate transformation
(A, C, C̄) → (SAS −1 , CS −1 , C̄S  ),

(4.5)

in the minimal realization of
1
(4.6)
V (z) = C(zI − A)−1 C̄  + c0 ,
2
the strictly positive real part of the spectral density of y, so that the state covariances
P− := E{x− (t)x− (t) } and P̄+ = E{x̄+ (t)x̄+ (t) } coincide with the diagonal ν × ν
matrix Σ of nonzero canonical correlation coeﬃcients, i.e.,
P− = P̄+ = Σ := diag(σ1 , σ2 , . . . , σν ).

(4.7)

1

This is done by choosing S so that Sx− (0) = Σ 2 η, where η = (η1 , η2 , . . . , ην ) , and
1
(S  )−1 x̄+ (0) = Σ 2 ξ, where ξ := (ξ1 , ξ2 , . . . , ξν ) .
To compute the canonical correlation coeﬃcients, we ﬁrst observe that the eigenvalues of the product P− P̄+ are precisely the squares of the canonical correlation
coeﬃcients, i.e.,
λ(P− P̄+ ) = λ(P− P+−1 ) = {σ12 , σ22 , . . . , σν2 },

(4.8)

where we have used the fact that the state covariance of (4.3) is P+ = P̄+−1 . Therefore
the canonical correlation coeﬃcients can then be determined via (4.8) by solving the
Lyapunov equations

P− = AP− A + B− B−


and P+ = AP+ A + B+ B+
.

(4.9)

12

J. MARI, A. DAHLÉN, AND A. LINDQUIST

The point is now to identify the canonical correlation coeﬃcients σ1 , σ2 , . . . , σr corresponding to the part of the system one wants to keep. The part corresponding to
σr+1 , σr+2 , . . . , σν will be disposed of. This amounts to partitioning Σ as



Σ1
,
(4.10)
Σ=
Σ2
where Σ1 is r × r.
In order to reduce model (2.1) we make the coordinate transformation (A, B, C) →
(SAS −1 , SB, CS −1 ), with the same balancing transformation S. Then, partition the
new triplet (A, B, C) conformally with (4.10) as


 




B1
A11 A12
(4.11)
, B=
, C = C1 C2 ,
A=
A21 A22
B2
and perform a principal subsystem truncation to obtain the transfer function of a
reduced-order system
Wred (z) = C1 (zI − A11 )−1 B1 + D

(4.12)

of degree r. If Σ2 is close to zero, while Σ1 is not, the rank of H is close to r, and the
discarded part of the system gives a negligible contribution to y.
Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy ﬁlter
√ ν
rν z
(4.13)
W− (z) := Wν (z) =
ϕν (z)
of order ν, which, for the moment we denote W− (z) to emphasize its character as the
minimum-phase spectral factor of the spectral density
rν
.
ϕν (z)ϕν (z −1 )
Remark 4.1. Without loss of generality we assume that ϕν (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller ν for which this condition holds.
In fact, ϕν (0) = γν−1 , and if γν−p = γν−p+1 = · · · = γν−1 = 0 and γν−p−1 = 0 for some
p = 1, 2, . . . , ν, then ϕν (z) = z ν−p ϕν−p (z) by (3.4), and hence (3.6) can be replaced
by Wν (z) = Wν−p (z), and for Wν−p (z) the required condition holds.
The maximum-phase spectral factor W+ (z) has all its zeros at inﬁnity, and hence
√
rν

−1
W+ (z) = h (zI − F ) b =
,
(4.14)
ϕν (z)
where (F, b, g) is the (observable) canonical form




0
0
1
···
0
..
.. 
. 
...
 ...
.
. , b = 
 ..  ,
F =
 0 
 0
0
···
1 
√
−ϕνν −ϕν,ν−1 · · · −ϕν1
rν

 
1
0

h=
 ...  ,
0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

ϕν1 , ϕν2 , . . . , ϕνν being the coeﬃcients of the Szegö polynomial ϕν (z). In this basis,
it follows from (4.9) that

  π

1
iθ
−1  −iθ
 −1
(e I − A) bb (e I − A ) dθ
[P+ ]jk =
2π −π
jk
 π
1
r
ν
=
e−(j−k)iθ
dθ = cj−k ,
2π −π
ϕν (eiθ )ϕν (e−iθ )
and hence P+ = Tν . It is well-known and easy to prove that Φν Tν Φν = Rν , where

Φν+1


ϕνν
 ..
 .

= ϕν2

ϕν1
1

ϕν−1,ν−1
..
.

ϕν−2,ν−2
..
.

ϕν−1,1
1

1

···


1








rν−1


and Rν = 





 , (4.16)


rν−2
..

.
r0

and consequently
P̄+ = Tν−1 = Φν Rν−1 Φν .
It remains to determine P− . From (4.13) is easy to see that
√
W− (z) = −ϕν (zI − F )−1 b + rν ,
where



ϕν := ϕνν ϕν,ν−1 · · · ϕν1 ,

(4.17)

(4.18)

(4.19)

but, in order to determine P− , this realization needs to be transformed so that the A
and C matrices are the same as in (4.14) (uniform choice of bases). More precisely,
we need to perform a transformation
(F, b, −ϕν ) → (QF Q−1 , Qb, −ϕν Q−1 ) =: (F, Qb, h ).
Then P− is the solution of the Lyapunov equation P− = F P− F  + Qbb Q , and therefore, since Tν = F Tν F  + bb and QF = F Q and consequently
QTν Q = F QTν Q F  + Qbb Q ,
we have
P− = QTν Q .
To determine Q, notice that −ϕν = h Q and QF
 

h
−ϕν

 −ϕν F   h F
= .

..
  ..

.
−ϕν F ν−1

(4.20)
= F Q to form


 Q = Q.


(4.21)

h F ν−1

Next, deﬁne the symmetric matrix
M := Rν−1/2 Φν QTν Q Φν Rν−1/2 .

(4.22)

14

J. MARI, A. DAHLÉN, AND A. LINDQUIST

In view of (4.20) and (4.17), det(zI − M ) = det(zI − P− P̄+ ), and hence, by (4.8), M
has the eigenvalues σ12 , σ22 , . . . , σν2 , and the singular-value decomposition
M = U Σ2 U  ,

(4.23)

where U  U = U U  = I. It is then well-known and simple to check that
S := Σ−1/2 U  Rν−1/2 Φν

(4.24)

is the required balancing transformation (4.5) such that SP− S  = (S  )−1 P̄+ S −1 = Σ.
Proposition 4.2. Given the partial covariance sequence
ck = E{y(t + k)y(t)},

k = 0, 1, . . . , ν,

let ϕ1 (z), ϕ2 (z), . . . , ϕν (z) and r0 , r1 , . . . , rν be the corresponding Szegö polynomials
and error variances. Supposing that γν−1 = −ϕν (0) = 0, let (F, b, h) be given by
(4.15), Rν and Φν by (4.16) and Q by (4.21). Moreover, let U and Σ be deﬁned by
the singular value decomposition (4.23) of (4.22). Then, the canonical correlation
coeﬃcients σ1 , σ2 , . . . , σν are the diagonal elements of Σ, as described in (4.7), and
the stochastically balanced realization of Wν is given by
√
(4.25)
(A, B, C, D) = (SF S −1 , SQb, h S −1 , rν ),
where S is deﬁned by (4.24).
Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to
yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase
property. In fact, we have the following result, the proof of which is given in Section A.
Theorem 4.3. Let Wred be the SBT approximation of degree r of Wν , and set

ν
ν−1
	
√  1 + |γk |
σk
9 := 2
and κ := c0
,
(4.26)
1
−
σ
1
−
|γ
k
k|
k=r+1
k=0
where γ0 , γ1 , . . . , γν−1 are the Schur parameters of c0 , c1 , c2 , . . . , cν . Then
c0 (1 − 9)κ−1 ≤ |Wred (eiθ )| ≤ (1 + 9)κ for all θ,

(4.27)

and, if 9 < 1, Wred is minimum phase. Finally, the approximation error has the bound
Wν − Wred ∞ ≤ 9κ.

(4.28)

A properly executed SBT procedure should imply that the canonical correlation
coeﬃcients σr+1 , . . . , σν , and hence 9, are close to zero, insuring the minimum-phase
condition.
Remark 4.4. Stochastic model reduction can also be carried out by instead performing principal subsystem truncation on (A, C, C̄) in Vν (z) = C(zI − A)−1 C̄ + 12 c0 ,
where A and C are given by (4.25) and C̄  = S(c1 , c2 , . . . , cn ). It was shown in
[32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally,
the spectral density Φred (z) := Vred (z) + Vred (z −1 ) is factorized to yield a minimumphase spectral factor W̃ . This is in a sense a more natural procedure, but we do
not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small Σ2 it yields almost the same result. In fact, it is shown in [53], that
|W̃ (eiθ )|2 = |Wred (eiθ )|2 + H(eiθ )Σ2 H(e−iθ ), where H(z) = C1 (zI − A11 )−1 A12 .
Remark 4.5. There are good reasons to prefer stochastic over deterministic model
reduction, as seen from the following heuristics. In fact, it can be seen that
Vν (z) =

c0 ψν (z)
,
2 ϕν (z)

(4.29)

where ψν (z) is the Szegö polynomial of the second kind (obtained by exchanging −γt
for γt in the recursion (3.4)). Now, the matrix representation of the Hankel operator
H in the innovation bases of the past and the future, provided by w− and w̄+ respecis the inﬁnite Hankel matrix of the sequence
tively, is given by L−1 (L−1 ) , where
c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T∞ ;
see, e.g., [32, p. 714]. It is easy to see that ψν (z) has the same asymptotic behavior as
ϕν (z), i.e., the roots tend to cluster uniformly inside the circle z = ρ as ν → ∞, and
hence these roots are close to canceling in (4.29). Consequently, the corresponding
Hankel matrix is close to having low rank. This massive “almost cancellation” does
not occur in Wν (z), and hence the corresponding inﬁnite Hankel matrix, constructed
from the Laurent coeﬃcients of Wν (z), may have a less distinct separation between
Σ1 and Σ2 . On the other hand, since the Schur parameters tend geometrically to
zero, the lower part of L tends to the identity, and hence the asymptotic behavior of
the canonical correlation coeﬃcients is very much like that of the singular values of
. Therefore we may expect SBT to have better statistical behavior than DBT. In
Section 6 we shall see that this is the case.

H

H

H

H

5. Identiﬁcation from statistical data
We now return to our original problem of time series identiﬁcation: Given a data
string (2.2) of observations of the output process y of some n-dimensional linear
stochastic system (2.1) with minimum-phase transfer function W (z), given by (2.8),
ﬁnd an estimate (Â, B̂, Ĉ, D̂) of the matrices (A, B, C, D).
The identiﬁcation method proceeds as follows. Given the covariance estimates (2.5),
we compute the corresponding maximum entropy ﬁlter (2.7), a balanced realization
(4.25), and the canonical correlation coeﬃcients
σ̂1 , σ̂2 , σ̂3 , . . . , σ̂ν ,

(5.1)

determined as in Proposition 4.2 from the covariance estimates ĉ0 , ĉ1 , . . . , ĉν .
Based on (5.1), choose an integer n̂ such that σ̂n̂+1 , σ̂n̂+2 , . . . , σ̂ν are close to zero or
at least distinctively smaller than σ̂1 , σ̂2 , . . . , σ̂n̂ . Then, the balanced realization (4.25)
is truncated accordingly as in (4.11) to yield a n̂-dimensional triplet (A11 , B1 , C1 ) and
a transfer function
Ŵ (z) = C1 (zI − A11 )−1 B1 + D.

(5.2)

Then, (A11 , B1 , C1 , D) is the required estimate (Â, B̂, Ĉ, D̂).
As pointed out in Section 2, we have a bound
W − Ŵ ∞ ≤ W − Wν ∞ + Wν − Ŵν ∞ + Ŵν − Ŵ ∞ ,

(5.3)

16

J. MARI, A. DAHLÉN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the ﬁrst term W − Wν ∞ , which
does not depend on the statistical data (2.2) but only on the underlying system (2.1),
tends to zero geometrically with a rate γ ∈ (0, 1) as ν → ∞. The other two terms
depend on the data (2.2), and here N must grow at a faster rate than ν. In fact, we
shall assume that
ν = ν(N ) = O(log N ),

(5.4)

which in particular requires that limN →∞ Nν = 0. We also need to assume that the
white noise process in (2.1) satisﬁes a mild technical condition, namely
E{w(t)4 } < ∞.

(5.5)

This condition is, of course, satisﬁed if w is Gaussian.
Next, we present our main convergence theorem.
Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then,
to each length N of the data string (2.2), there is a ν(N ), tending to inﬁnity with N
at the rate (5.4), such that any sequence of estimated transfer functions Ŵ of ﬁxed
degree n̂ ≥ n, determined, for each N and corresponding ν = ν(N ), by the procedure
described above, satisﬁes
W − Ŵ ∞ → 0
almost surely as ν(N ) → ∞. For suﬃciently large ν(N ), the transfer function Ŵ has
minimum phase.
We have already proven that the ﬁrst term in (5.3) tends to zero, so Theorem 5.1
follows from the next two theorems, each corresponding to one of the remaining terms
in (5.3). As for the second term, we have the following result, the proof of which is
deferred to Appendix B.
Theorem 5.2. Suppose the system (2.1) satisﬁes the conditions of Theorem 5.1. Let
Wν be the maximum-entropy ﬁlter (3.6) determined from the partial covariance sequence (3.1) of y and let Ŵν be the corresponding function determined from the ergodic
estimates (2.5). Then, if ν(N ) is deﬁned as in Theorem 5.1,
Wν(N ) − Ŵν(N ) ∞ → 0
almost surely as ν(N ) → ∞.
There are several results of this type in the literature [2, 36, 7, 33]. In particular,
3
Berk [2] proved that, provided νN → 0 as N → ∞ and Φ is coercive (i.e. positive
 iθ ) → Φ(eiθ ) in probability.
on the unit circle), the estimated AR spectral density Φ(e
Under the same hypotheses, Caines and Baykal-Gürsoy [7] showed that if N ≥ ν 5+η
for some η > 0, then Ŵν−1 − W −1 ∞ → 0 almost surely as ν → ∞. However, in both
cases, ergodic estimates are used which are not quite the same as (2.5).
Finally, we consider the last term in (5.3). The proof of the following theorem is
given in Appendix B.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function ν(N ) are deﬁned as in
Theorem 5.1. Moreover, for each N , let Ŵν(N ) be deﬁned as in Theorem 5.2 and Ŵ
as in Theorem 5.1. Then, for suﬃciently large ν(N ), Ŵ has minimum phase, and
Ŵν(N ) − Ŵ ∞ → 0
almost surely as ν(N ) → ∞.
6. Simulations
Performing model reduction on Ŵν , rather than on the maximum-entropy ﬁlter of
exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier
and more accurate order determination, as the heuristics of Remark 4.5 suggest.
There are also alternative order determination statistical tests based on the canonical
correlation coeﬃcients [17, 26, 46]. But, even more importantly, there is less bias,
and the error variances are closer to the Cramér-Rao bound.
Since we are approximating rational models with AR models the method will be
biased for ﬁnite amount of data, unless the model generating the data really is an
AR model. The consistency result given in Theorem 5.1 implies that the method is
asymptotically unbiased and therefore we consider the Cramér-Rao bound for unbiased methods; see [44, pp. 137–138]. The Cramér-Rao bound for biased estimation
requires knowledge about the bias as a function of the parameter to be estimated.
As already mentioned, the method will be unbiased and even statistically eﬃcient for
Gaussian AR processes if the model reduction step is omitted. Despite the fact that
an algorithm based on covariance estimates (2.6) is not asymptotically eﬃcient for
general ARMA models [44, p. 144], our method can be used to provide a starting
guess for other algorithms, for example the maximum likelihood method.
6
SBT dashed line, DBT dotted line.
5

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identiﬁcation estimates.

To illustrate our procedure, let us consider data generated by passing white noise
through a “true system” with transfer function
W (z) =

z 5 − 0.0550z 4 − 0.1497z 3 − 0.2159z 2 + 0.1717z − 0.0495
.
z 5 − 0.7031z 4 + 0.3029z 3 + 0.1103z 2 − 0.1461z + 0.2845

18

J. MARI, A. DAHLÉN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy
model Ŵν of degree ν = 24 determined from estimated covariances. Based on 100
test runs, the empirical means and standard deviations are determined. Figure 6.1
illustrates the statistical bias as a function of the length N of the data string when
using stochastic (dashed curve) and deterministic (dotted curve) model reduction
respectively.
For the same test runs, Figure 6.2 illustrates the corresponding standard deviations
together with the Cramér-Rao bound (solid curve). More precisely, the ﬁgures depict
the sums of the moduli of the biases and standard deviations respectively for the
coeﬃcients of the numerator and denominator polynomials of Ŵ (z).
5
CRB solid line, SBT dashed line, DBT dotted line.
4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the Cramér-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z)
with poles and zeros closer to the unit circle is considered. The poles and zeros of
Ŵ (z) are determined for 100 runs and a data length N = 500. As before, ν = 24.
Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with
the poles and zeros of W (z), which are denoted by “◦”.

1

1

0.5

0.5

0

0

−0.5

−0.5

−1
−1

−0.5

0
zeros

0.5

1

−1
−1

−0.5

0
poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and ν = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT.
To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in
Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

−0.5

−0.5

−1
−1

−0.5

0
zeros

0.5

−1
−1

1

−0.5

0
poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and ν = 24.

1

1

0.5

0.5

0

0

−0.5

−0.5

−1
−1

−0.5

0
zeros

0.5

1

−1
−1

−0.5

0
poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identiﬁcation.

Figure 6.5 describes the result obtained when applying stochastic subspace identiﬁcation to the same data. More precisely, Algorithm # 2 in [43] is used. In order
to make the experiments comparable, we have chosen a Hankel matrix of dimension
13 × 13, which corresponds to ν = 25 in our procedure.
Note that the estimates are much less focused, and many zeros tend to cluster on the
unit circle, implying that coercivity becomes critical. This is related to the positivity
issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the
subspace identiﬁcation method performs worse than our SBT identiﬁcation method,
yielding larger biases and standard deviations, but performs better than when DBT
is used.

20

J. MARI, A. DAHLÉN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as
illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will
disappear as ν and N are increased. In Figure 6.6 we show the same experiment for
ν = 64 and N = 2000.

1

1

0.5

0.5

0

0

−0.5

−0.5

−1
−1

−0.5

0
zeros

0.5

−1
−1

1

−0.5

0
poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and ν = 64.

1

1

0.5

0.5

0

0

−0.5

−0.5

−1
−1

−0.5

0
zeros

0.5

−1
−1

1

−0.5

0
poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and ν = 40 using Burg’s method.

In practice, there is a trade-oﬀ between the quality of the ergodic estimates, which
roughly speaking depend on |λmax (A)|, the ∞ -error tolerance, which is a function of
|λmax (A − BD−1 C)|, and the numerical accuracy of the computations. For example,
if the zeros of W (z) are far from the unit circle and ν is chosen very large, the error
may increase.
In the present example, it turns out that using Burg’s method [3] in lieu of the
ergodic estimate (2.6) yields better estimates for smaller ν and N , as illustrated in
Figure 6.7 which shows the case N = 500 and ν = 40.
A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There
we give the empirical bias and standard deviation for the coeﬃcients of the numerator
and the denominator, respectively, of the estimated transfer functions together with
the Cramér-Rao bound. It is the authors experience that Burg’s method gives at
least as good results as when using the ergodic covariance estimate (2.6), unless the
intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

Parameter
True value
Bias:
CE:
Burg:
Std.dev.:
CE:
Burg:
CRB:

21

σw2
b1
b2
b3
b4
b5
1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491
0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895
0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734
0.2332 0.1314 0.0508 0.0611 0.0722 0.0802
0.0712 0.0411 0.0381 0.0339 0.0339 0.0356
0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and ν = 40 using covariance estimation (CE) or Burg estimation and , in
both cases, followed by SBT.

Parameter
a1
a2
a3
a4
a5
True value
-0.6281 0.3597 0.2634 -0.5322 0.7900
Bias:
CE:
0.0087 -0.0044 -0.0003 0.0066 -0.0152
Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125
Std.dev.:
CE:
0.0274 0.0304 0.0371 0.0305 0.0304
Burg: 0.0336 0.0307 0.0358 0.0324 0.0306
0.0293 0.0321 0.0342 0.0322 0.0290
CRB:
Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and ν = 40 using covariance estimation (CE) or Burg estimation and,
in both cases, followed by SBT.

7. Conclusions
We have presented a three-step procedure for identiﬁcation of time series, which is easy
to understand and implement. Just like for subspace identiﬁcation methods, robust
linear-algebra algorithms can be used and no nonconvex optimization computations
are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identiﬁcation, as our extensive simulations indicate.
In particular, its good performance has been conﬁrmed by Monte Carlo simulations.
The paper only covers the scalar case, but the multivariate case is presently being
worked out.
The three steps, covariance estimation, covariance extension and model reduction
have each been studied separately before. This is an advantage which should make the
method easy to grasp. However, a comprehensive study of the entire identiﬁcation
strategy, giving appropriate bounds, has been missing and this is what we oﬀered
here.
The observation that the Schur parameters converge geometrically simpliﬁes our
application of Szegö theory and allows us to give a complete account of the asymptotic
behavior of maximum entropy models of growing order. This analysis provides us with
a clear indication as to when the identiﬁcation strategy is good and when it might face
diﬃculties, based purely on the closeness of the maximum modulus zero to the unit
circle. The parsimony permeating other system identiﬁcation methods should not be
a reason for refraining from high-order modeling as an intermediate step. In fact,
such a strategy might be desirable, since we have shown that the poles of the “true”
system which lie outside a circle in the complex plane containing all of its zeros are

22

J. MARI, A. DAHLÉN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the
perimeter of this circle, providing a justiﬁcation for choosing stochastically balanced
model reduction, rather than deterministically balanced truncation, in the last step.
With this reduction procedure, we have conﬁrmed better statistical properties with
variances closer to the Cramér Rao bound. The procedure could also be modiﬁed by
exchanging exact covariance extension for approximate one, as outlined in [35].
Even though, in general, stochastic balancing would require the solution of a pair
of Riccati equations, this is not the case for the particular maximum entropy models
used here. In fact, the balancing procedure only requires linear algebra, and hence
an intelligent use of the Levinson algorithm may substantially reduce the number of
arithmetic operations.
Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N , ν and n̂, we gave worst-case guaranteed
bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive
stochastic system, but the method returns a valid model also for generic data. In fact,
in contrast to many stochastic subspace identiﬁcation [9], all steps of the procedure
preserve the positive real property.
Appendix A. Asymptotic behavior of the maximum entropy ﬁlter
Theorem 3.4 is actually a modiﬁcation to the rational setting of a theorem due to
Szegö [47], and the proof is modeled after [19], which in turn includes aspects already
present in the work of Schur [45]. See also [48], [49] and [16] for more facts on
orthogonal polynomials. However, rationality and coercivity allows us to present a
simpliﬁed and self-contained proof of a version of Szegö’s classical theorem, to which
we also are able to add geometric convergence. The derivation of Caines and BaykalGürsoy [7] is shorter, but we feel that our approach is more systematic and gives
additional insight into the mechanism of identiﬁcation.
To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas.
−ν
φν (z)|
Lemma A.1. Let {φν (z)}∞
0 be the normalized Szegö polynomials (3.7). Then |z
c
is uniformly bounded from above and away from zero in the complement D of the open
unit disc, i.e., there are positive numbers α, β ∈ R such that

α ≤ |z −ν φν (z)| ≤ β
for all ν and all z ∈ Dc .
Proof. In view of the Szegö-Levinson recursion (3.4),



ϕ∗t (z)
ϕt+1 (z) = ϕt (z) z − γ̄t
,
ϕt (z)
and hence
z

−ν

ϕν (z) =

ν−1


k=0


ϕ∗k (z)
1 − z γ̄k
.
ϕk (z)
−1

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , zν are the roots of ϕν (z), it is immediately seen that
ϕ∗ν (z)  1 − z z̄k
,
=
ϕν (z) k=1 z − zk
ν

which is a Blaschke product, analytic in Dc and having modulus one on the unit circle,
and thus modulus less than or equal to one in Dc . Hence, since |z −1 | ≤ 1 in Dc ,
ν−1


(1 − |γk |) ≤ |z

−ν

ϕν (z)| ≤

k=0

ν−1


(1 + |γk |)

(A.1)

k=0

for all z ∈ Dc . But, these products converge to positivenumbers as ν → ∞. This
follows from the absolute convergence of the inﬁnite sum ∞
k=0 |γk |, a fact that, in the
present context, stems from Lemma 3.1. From (3.5) we also have 0 < r∞ ≤ rν ≤ r0 ,
and consequently the lemma follows.
Remark A.2. An equivalent statement of this lemma is that the maximum entropy
solution Wν (z), deﬁned by (3.6), is uniformly bounded from above and away from
zero for all ν and z ∈ Dc .
Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of
functions
fν (z) := z −ν φν (z)
converges uniformly to an analytic function f∞ in
statement of Theorem 3.5.

Dcρ,

where

Dcρ

is deﬁned in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the
purely algebraic relation
ν
	

φk (z)φk (w) =

k=0

φ∗ν (z)φ∗ν (w) − z w̄φν (z)φν (w)
,
1 − z w̄

(A.2)

which is called the Christoﬀel-Darboux-Szegö formula. In particular, setting w = 0
and exchanging z for z −1 in (A.2), (3.5) and (3.7) yield
fν (z)
1 	
γk−1
−
φk (z −1 ) √ .
√ =
rν
c0 k=1
rk
ν

(A.3)

Observe that φk (z −1 ) is analytic and bounded in Dcρ , and hence in Dc , and therefore it
∞
. Moreover, by the maximum modulus principle, it attains its maximum
belongs to −
c
value in D on the unit circle where, by Lemma A.1, it is bounded by β. Hence

H

|φk (z −1 )| ≤ β

for z ∈ Dc and for all k.

Therefore, in view of (A.3) and the fact that rk ≥ r∞ , we have


ν
	
 fν (z) fµ (z) 
 √ − √  ≤ √β
|γk−1 |,
 rν
rµ 
r∞ k=µ+1

(A.4)

(A.5)

24

J. MARI, A. DAHLÉN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for suﬃciently large ν and µ.
∞
. The same holds for fν (z). In
This establishes (A.3) as a Cauchy sequence in −
c
fact, since rν ≤ c0 , for all z ∈ D


√  fν (z) fµ (z) 
|fν (z) − fµ (z)| ≤ c0  √ − √ 
rν
rν




 1
√  fν (z) fµ (z) 
1 

≤ c0  √ − √  + |fµ (z)|  √ − √  .
(A.6)
rν
rµ
rν
rµ

H

But, by Lemma A.1, |fµ (z)| ≤ β for all ν and z ∈ Dc , and therefore, in view of (A.5),
we obtain



ν
 1

c0 	
1
|γk−1 | + β  √ − √  for all z ∈ Dc . (A.7)
|fν (z) − fµ (z)| ≤ β
r∞ k=µ+1
rν
rµ
Since rν → r∞ as ν → ∞, we see that, for each 9 > 0, |fν (z) − fµ (z)| < 9 for
suﬃciently large ν and µ. Consequently, fν tends uniformly in Dc to a function
∞
.
f∞ ∈ H−
The uniform convergence and the analyticity can be extended to any compact
subset of Dcρ . To see this, ﬁrst note that z ∈ D if and only if z −1 ∈ Dc . Therefore, by
Lemma A.1,
|φk (z −1 )| ≤ β|z|−k for z ∈ D,
and consequently, since rν ≤ rk , (A.3) yields
ν−1
	
1
|fν (z)| ≤ √ + β|z|−1
|γk ||z|−k .
c0
k=0

Similarly, instead of (A.5) we have


ν−1
	
 fν (z) fµ (z) 
 √ − √  ≤ √β |z|−1
|γk ||z|−k .
 rν
rµ 
r∞
k=µ

(A.8)

(A.9)

Now, for any compact subset K ∈ Dcρ , there is a γ ∈ (ρ, 1) and an 9 > 0 such
that |z| > γ + 9 for all z ∈ K. Hence, by Lemma 3.1, |γk ||z|−k ≤ M γ̂ k where
γ̂ := γ(γ + 9)−1 < 1. Consequently, by (A.8), fν (z) is uniformly bounded in K, and
(A.9) can be made arbitrarily small for suﬃciently large ν and µ. Therefore, by (A.6),
fν tends uniformly in K to the analytic function f∞ .
Lemma A.4. Let γ be a real number such that ρ < γ < 1. Then fν −f∞ ∞ = O(γ ν ).
Proof. It follows from (A.7) that


 
∞

√ 	
β
r∞ 
c0
|γk−1 | + 1 −
|fν (z) − f∞ (z)| ≤ √
r∞
rν 
k=ν+1

for all z ∈ Dc .
(A.10)

By Lemma 3.1, the ﬁrst term is O(γ ν ). It remains to show that the same holds for
the second term. To this end, ﬁrst note that, by (3.5),

∞ 

r∞
=1−
1 − γk2 .
1−
rν
k=ν

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

But, by Lemma 3.1, |γk | ≤ M γ k for some M . Therefore, since
each x ∈ [0, 1],

∞

r∞
≤ 1 − (1 − M γ k ) = O(γ t )
1−
rt
k=t

√

25

1 − x2 ≥ 1 − x for

for t large enough. This concludes the proof.
Recalling the deﬁnition (3.6) of Wν , we note that Lemma A.4 may be written
Wν−1 − f∞ ∞ = O(γ ν ).
−1
in the same manner.
As it turns out, by coercivity, this implies that Wν → f∞

Lemma A.5. Let Wν be the transfer function (3.6) of the maximum entropy ﬁlter.
Then
−1
∞ = O(γ ν ),
Wν − f∞
where f∞ is the limit function of Lemma A.3.
Proof. Note that the limit function f∞ has the same uniform bounds as fν in Lemma
A.1. In particular, |f∞ (z)| ≥ α, |f∞ (z)|−1 ≤ α−1 , and |Wν (z)| ≤ α−1 for all z ∈ Dc .
Consequently,
−1
−1
Wν − f∞
∞ ≤ Wν ∞ f∞
∞ Wν−1 − f∞ ∞ ≤ α−2 Wν−1 − f∞ ∞ ,

so the required result follows from Lemma A.4.
Lemma A.6. Let W be the rational minimum-phase function deﬁned above, and let
f∞ be the limit function in Lemma A.3. Then W (z) = f∞ (z)−1 for all z ∈ Dcρ .
Proof. Let Φν (eiθ ) := |Wν (eiθ )|2 be the spectral density of the maximum entropy
process. Then, in view of the interpolation condition,
 π
 π
1
1
ikθ
iθ
e Φ(e )dθ = ck =
eikθ Φν (eiθ )dθ for k = 0, 1, . . . , ν, (A.11)
2π −π
2π −π
from which we have pointwise convergence of the Fourier coeﬃcients of Φν (eiθ ) to
those of Φ(eiθ ) as ν → ∞, and hence Φν (eiθ ) → Φ(eiθ ) in the 2 sense. However, by
Lemma A.5, Φν (eiθ ) → |f∞ (eiθ )|−2 in ∞ norm, and hence a fortiori in 2 norm, as
ν → ∞. Since, in addition, not only Φ(eiθ ) but also f∞ is analytic in a neighborhood
of the unit circle (Lemma A.3), we have

L

L

L

Φ(eiθ ) = |f∞ (eiθ )|−2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer.
In particular, Wν is an outer spectral factor of Φν (eiθ ) satisfying

  π it

e +z
1
it 2
log |Wν (e )| dt .
Wν (z) = exp
4π −π eit − z
But Lemma A.5, Equation (A.12) and the fact that Φ(eiθ ) = |W (eiθ )|2 ,

  π it

1
e +z
it 2
log |W (e )| dt = W (z),
Wν (z) → exp
4π −π eit − z
the outer spectral factor of Φ. But, by Lemma A.3, Wν (z) → f∞ (z)−1 in
therefore f∞ (z) = W −1 (z) as claimed.

Dcρ, and

26

J. MARI, A. DAHLÉN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6.
Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6.
Proof of Theorem 4.3. Following [53] we see that
Wν−1 (Wν − Wred )∞ ≤ 9,

(A.13)

and consequently
|Wν (eiθ ) − Wred (eiθ )| ≤ 9|Wν (eiθ )|
holds for all θ, from which we have
(1 − 9)|Wν (eiθ )| ≤ |Wred (eiθ )| ≤ (1 + 9)|Wν (eiθ )|.
However, in view of (3.6) and (3.7), it follows from (A.1) that
√
√
rν
rν
iθ
≤ |Wν (e )| ≤ ν−1
,
ν−1
k=0 (1 + |γk |)
k=0 (1 − |γk |)
which together with (3.5) yields
c0
≤ |Wν (eiθ )| ≤ κ
κ

(A.14)

for all θ. This establishes (4.27). To see that Wred is minimum phase if 9 < 1, note
that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by Rouché’s
Theorem, Wred has the same number of zeros in Dc (including ∞) as Wν . Hence,
since Wν is minimum phase, so is Wred .
To establish the bound (4.28) note that
Wν − Wred ∞ ≤ Wν ∞ Wν−1 (Wν − Wred )∞ .
From (A.14) we have Wν ∞ ≤ κ, and hence (4.28) follows from (A.13).
Appendix B. Statistical convergence proofs
Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corresponding Szegö polynomial ϕ̂ν (z) and predictor error variance r̂ν from (3.4) and (3.5),
and form the maximum-entropy ﬁlter
√ ν
r̂ν z
.
Ŵν (z) =
ϕ̂ν (z)
To determine Wν − Ŵν ∞ let z ∈ Dc and form
√ ν
√ ν
rν z
r̂ν z
Wν (z) − Ŵν (z) =
−
ϕν (z)
ϕ̂ν (z)
√
√
√
( rν − r̂ν )z −ν ϕν (z) − rν z −ν (ϕν (z) − ϕ̂ν (z))
.
=
z −ν ϕν (z)z −ν ϕ̂ν (z)
Since r∞ > 0, by (3.7) and Lemma A.1,
√
√
0 < µ := r∞ α ≤ |z −ν ϕν (z)| ≤ c0 β =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1),
|z

−ν

ϕ̂ν (z)| ≥ µ̂ν :=

ν−1


(1 − |γ̂k |),

k=0

where γ̂0 , γ̂1 , . . . , γ̂ν−1 are the Schur parameters corresponding to the estimated covariances (2.6). Therefore, by the maximum-modulus principle,

√
√
1
{M | rν − r̂ν | + c0 |ϕν (z) − ϕ̂ν (z)|},
|Wν (z) − Ŵν (z)| ≤ max
|z|=1 µµ̂ν
where we have also used the fact that rν ≤ c0 . But, for |z| = 1,
|ϕν (z) − ϕ̂ν (z)| ≤ ϕν − ϕ̂ν 1 ,
where ϕν and ϕ̂ν are the ν-vectors formed as in (4.19) and ·1 is the B1 norm. Recall
that ϕν is the unique solution of the normal equations


Tν ϕν = −cν where cν := cν cν−1 . . . c1 ,
(B.1)
where Tν is the Toeplitz matrix deﬁned by (2.4), and that
rν = c0 + cν ϕν .

(B.2)

Also, the analogous relations hold for ϕ̂ν and r̂ν . Then,
rν − r̂ν = (c0 − ĉ0 ) + (cν − ĉν ) ϕν + ĉν (ϕν − ϕ̂ν )
and hence
|rν − r̂ν | ≤ |c0 − ĉ0 | + cν − ĉν 1 ϕν ∞ + ĉν ∞ ϕν − ϕ̂ν 1 .
Finally,


√
|rν − r̂ν |
|r − r̂ |
√ ≤ ν√ ν ,
| rν − r̂ν | ≤ √
r∞
rν + r̂ν

and consequently, since x1 ≤ νx∞ for any x ∈ Rν ,

M
√ {|c0 − ĉ0 | + ϕν ∞ νcν − ĉν ∞ }
µµ̂ν r∞



M ĉν ∞
1 √
c0 + √
+
νϕν − ϕ̂ν ∞ .
µµ̂ν
r∞

Wν − Ŵν ∞ ≤

(B.3)

Recall now that ϕν and ϕ̂ν are each solutions of a normal equation (B.1). More
precisely, Tν ϕν = −cν and T̂ν ϕ̂ν = −ĉν . Since ck = CAk−1 C̄  for k > 0, where all
eigenvalues of A are less than one in modulus, ck → 0 exponentially, we have
cν ∞ ≤ K1

and Tν ∞ ≤ c0 + 2

ν−1
	

|ck | ≤ K2

k=1

for some constants K1 and K2 . Moreover, from [8] we have
Tν−1 ∞ ≤

ν−1
1  1 + |γk |
≤ K3
c0 k=0 1 − |γk |

28

J. MARI, A. DAHLÉN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence
ϕν ∞ ≤ Tν−1 ∞ cν ∞ ≤ K1 K3
and the condition number
κ(Tν ) := Tν ∞ Tν−1 ∞ ≤ K := K2 K3
is bounded for all ν.
Now, it is known [25] that for each data length N in (2.2), there is a ν(N ) of order
O(log N ) such that


log log N
,
(B.4)
max |ck − ĉk | = O
0≤k≤ν(N )
N
and therefore, for any a ∈ R,
ν a |c0 − ĉ0 | → 0 and ν a cν − ĉν ∞ → 0 as ν = ν(N ) → ∞.

(B.5)

Consequently the ﬁrst term in the bound (B.3) tends to zero as N → ∞ and
ν(N ) → ∞ provided it is done at the speciﬁed relative rates and provided µ̂ν is
bounded away from zero. However, the estimate (2.6) has the property that the
corresponding Toeplitz matrix T̂ν is positive deﬁnite for each ﬁnite ν, and this in turn
is equivalent to |γ̂k | < 1 for k = 0, 1, . . . , ν − 1 so that µ̂ν > 0. Since, in addition
µ̂ν → µ > 0 as ν(N ) → ∞ by (B.4) and continuity, the second requirement is also
fulﬁlled. To simplify notations, we have suppressed the index N in the quantities
marked with a hat, which of course depend on the data (2.2) and hence also on N .
Next we show that also the second term in (B.3) tends to zero. Since ĉν ∞ ≤
cν ∞ + cν − ĉν ∞ is bounded, it thus remains to demonstrate that
νϕν(N ) − ϕ̂ν(N ) ∞ → 0 as ν(N ) → ∞.
This follows from the more general fact, needed for the proof of Corollary B.1, that
ν a ϕν(N ) − ϕ̂ν(N ) ∞ → 0 as ν(N ) → ∞

(B.6)

for any a ∈ R. To prove this, ﬁrst note that
Tν − T̂ν ∞ ≤ |c0 − ĉ0 | + 2νcν − ĉν ∞ ,
and hence Tν − T̂ν ∞ → 0. Therefore ρν := Tν − T̂ν ∞ Tν−1 ∞ < 1 for ν := ν(N )
suﬃciently large, and, provided cν = 0, the standard perturbation estimate [22] yields


1
Tν − T̂ν ∞ cν − ĉν ∞
ϕν − ϕ̂ν ∞
≤
κ(Tν )
+
,
(B.7)
ϕν ∞
1 − ρν
Tν ∞
cν ∞
and consequently, since Tν ∞ ≥ c0 > 0, it follows from (B.5) that (B.6) tends to
zero in the required manner.
If cν = 0, ϕν = 0, and hence
ϕν − ϕ̂ν ∞ = ϕ̂ν ∞ ≤ T̂ν−1 ∞ ĉν ∞ = T̂ν−1 ∞ cν − ĉν ∞ ,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case cν = 0. In fact, since µ̂ is
bounded away from zero, by continuity, for each 9 > 0, there is a N0 such that
T̂ν−1 ∞

ν−1
ν−1
1  1 + |γk |
1  1 + |γˆk |
≤
+ 9 ≤ K3 + 9
≤
ĉ0 k=0 1 − |γˆk |
c0 k=0 1 − |γk |

for ν ≥ N0 .

Corollary B.1. If ν(N ) is deﬁned as in Theorem 5.1, then, for any a ∈ R,
ν a Wν − Ŵν ∞ → 0

almost surely as ν := ν(N ) → ∞.

To prove Theorem 5.3, we ﬁrst note that the Hankel operator H, deﬁned by (4.1),
has a nice representation in the space 2 of square-integrable functions. In fact, let
2
2
of functions with vanishing negative Fourier coeﬃcients,
+ be the subspace in
hence being analytic in the unit disc D. In this setting, H has the representation
2
2
→ 2 +
given by
HΘ : +

L

L

H

H

L H

HΘ f = P ⊥ Θf,

(B.8)

where P ⊥ is the orthogonal projection onto the orthogonal complement
2
2
, and where Θ is the ∞ -function
+ in

H

L

L

Θ(z) = W− (z)W̄+ (z)−1 .

L H
2

2
+

of

(B.9)

Here W− (z) and W̄+ (z) are the analytic and coanalytic minimum-phase spectral factors deﬁned in Section 4. (See, e.g., [30, 31].) In the present scalar case, W̄+ (z) =
W− (z −1 ). In fact, the phase function Θ is the transfer function of an all-pass ﬁlter
transforming the white noise w− in (4.2) to the white noise w+ in (4.3) [30, p. 834].
ˆ + be the stochastic measures such that
Let dŵ− and dw̄
 π
 π
iθt
ˆ+
e dŵ− and w̄+ (t) =
eiθt dw̄
w− (t) =
−π

Then

−π


H

+

π

=

H− =
and consequently
H := E

f ↔ f dŵ− .

H−

−π
π

H

L H
2

−π



2 ˆ
+ dw̄ +

π

=
−π

H

2
iθt
+ Θ(e )dŵ−

2
+ dŵ−

|H + corresponds to HΘ under the isomorphism deﬁned by





Proof of Theorem 5.3. It follows from Theorem 5.2 that |Ŵν (eiθ )| − |Wν (eiθ )| → 0
uniformly in θ as ν → ∞, and hence, by Lemma A.1, there are positive real numbers
µ1 and µ2 such that
µ1 ≤ |Ŵν (eiθ )| ≤ µ2
for all θ and suﬃciently large ν. Therefore, since
Ŵν − Ŵ ∞ ≤ Ŵν ∞ Ŵν−1 (Ŵν − Ŵ )∞ ,

(B.10)

30

J. MARI, A. DAHLÉN, AND A. LINDQUIST

(A.13) and (4.26) imply that
Ŵν − Ŵ ∞ ≤ 2µ2

ν
	

σ̂k
,
1 − σ̂k
k=n̂+1

(B.11)

for suﬃciently large ν, where σ̂1 , σ̂2 , . . . , σ̂ν are the singular values (5.1) determined
from the covariance estimates (2.6).
It is well-known (see, e.g., [56, p. 204]) that the singular value σk of the Hankel
operator HΘ , deﬁned by (B.8) equals the inﬁmum of HΘ − K over all operators
2
2
→ 2 +
of ﬁnite rank at most k. Recall that Θ(z) = Wν (z)/Wν (z −1 ).
K : +
The singular value σ̂k of HΘ̂ , where Θ̂(z) = Ŵν (z)/Ŵν (z −1 ), is described analogously.
Therefore, since

H

L H

HΘ̂ − K ≤ HΘ̂ − HΘ  + HΘ − K ≤ Θ̂ − Θ∞ + HΘ − K,
we have σ̂k ≤ Θ̂ − Θ∞ + σk . But, for k > n, σk = 0, and hence σ̂k ≤ Θ̂ − Θ∞ .
Consequently, (B.11) yields
Ŵν − Ŵ ∞ ≤ M1 νΘ̂ − Θ∞ ,

(B.12)

where M1 := 2µ2 (1 − σ̂n̂+1 )−1 . However,


−1 −1
−1
−1
Ŵν (z) − W (z) − Θ(z)[Ŵν (z ) − W (z )] ,
Θ̂(z) − Θ(z) = Ŵν (z )
so, since Ŵν (z −1 )∞ is uniformly bounded by (B.10), and Θ∞ is constant,
Θ̂ − Θ∞ ≤ M2 W − Ŵν ∞ ,
which together with (B.12) yields
Ŵν − Ŵ ∞ ≤ M1 M2 νW − Wν ∞ + M1 M2 νWν − Ŵν ∞
for suﬃciently large ν. Consequently the theorem follows from Theorem 3.4 and
Corollary B.1.
Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A.
Gombani, W. B. Gragg, G. Picci and T. Söderström for stimulating discussions and
for providing us with appropriate references. We are also indebted to the anonymous
referees for several useful suggestions.
References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987.
2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489–502, 1974.
3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975.
4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational
covariance extension problem. SIAM Journal on Control and Optimization, 37:211–229, 1999.
5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of
all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841–1857, 1995.
6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast ﬁltering algorithms.
SIAM Journal on Control and Optimization, 32:744–789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-Gürsoy. On the L∞ consistency of L2 estimators. Systems & Control
Letters, 12:71–76, 1989.
8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978.
9. A. Dahlén, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace
identiﬁcation methods may fail. Systems and Control Letters, 34:303–312, 1998.
10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced
stochatic realizations. In Proc. 21st IEEE CDC, pages 1105–1112, 1983.
11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans.
Automatic Control, AC-29:1097–1100, 1984.
12. J. Durbin. Eﬃcient estimation of parameters in moving average models. Biometrika, 46:306–316,
1959.
13. J. Durbin. The ﬁtting of time-series models. Rev. Inst. Int. Stat., pages 223–243, 1960.
14. P. Duren. Theory of Hp spaces. Academic Press, 1970.
15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear.
16. G. Freud. Orthogonale Polynome. Birkhäuser Verlag, 1969.
17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic
Control, AC-32:358–361, 1987.
18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac.,
Speech and Signal Processing, ASSP-35:438–449, 1987.
19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961.
20. M. Gevers. Towards a joint design of identiﬁcation and control. In J. Willems and H. Trentelman,
editors, Essays on Control: Perspectives in the Theory and its Applications, 1993.
21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their
l∞ -error bounds. Int. J. Contr., 39:1115–1193, 1984.
22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989.
23. W. B. Gragg. Positive deﬁnite Toeplitz matrices, the Arnoldi process for isometric operators,
and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in
Linear Algebra, pages 16–32. Moscow U. P., 1982.
24. U. Grenander and G. Szegö. Toeplitz forms and their applications. Univ. California Press, 1958.
25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons,
1988.
26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples.
Biometrika, pages 297–307, 1989.
27. W. Jones and E. Saﬀ. Szegö polynomials and frequency analysis. In Approximation Theory,
pages 341–352. Dekker Inc., 1992.
28. S. Y. Kung. A new identiﬁcation method and model reduction algorithm via singular value
decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705–714, 1978.
29. W. E. Larimore. System identiﬁcation, reduced ordered ﬁltering and modeling via canonical
variate analysis. In Proc. of the American Control Conference, 1983.
30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes.
SIAM J. Control and Optimization, 23:809–857, 1985.
31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic
systems. J. of Math. Systems, Estimation and Control, 1:241–333, 1991.
32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension,
and identiﬁcation of stationary time series. Automatica, 32(5):709–733, 1996.
33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating
transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412–440, 1992.
34. L. Ljung and Z. Yuan. Asymptotic properties of black box identiﬁcation of transfer functions.
IEEE Trans. Automatic Control, AC-26:514–530, 1985.
35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis,
Royal Instiute of Technology, 1998.
36. D. Q. Mayne and F. Firoozan. Linear identiﬁcation of ARMA processes. Automatica, 18:461–466,
1982.

32

J. MARI, A. DAHLÉN, AND A. LINDQUIST

37. H. Mhaskar and E. Saﬀ. The distribution of zeros of asymptotically extremal polynomials. J.
Approx. Theory, 3:279–300, 1991.
38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66–73, 1978.
39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward,
editors, Approximation Theory VI, 1989.
40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991.
41. P. Van Overschee. Subspace Identiﬁcation, Theory - Implementation - Application. PhD thesis,
Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De
Moor.
42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identiﬁcation problem.
In Proc. 30th Conference on Decision and Control, Brighton, 1991.
43. P. Van Overschee and B. De Moor. Subspace Identiﬁcation for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996.
44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994.
45. I. Schur. Über Potenzreihen, die im Innern des Einheitskreises beschränkt sind. J. für die Reine
und Angewandte Mathematik, 147:205–232, 1917.
46. J. Sorelius, T. Söderström, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identiﬁcation. In Proc. SYSID ’97, 1997.
47. G. Szegö. Beiträge zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift,
6:167–202, 1920.
48. G. Szegö. Über die Randwerte analytischer Funktionen. Mat. Annalen, 84:232–244, 1921.
49. G. Szegö. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications,
1939 (4th edition 1975).
50. B. Wahlberg. On the Identiﬁcation and Approximation of Linear Systems. PhD thesis, Linköping
University, 1987. Linköping Studies in Science and technology. Dissertations No. 163.
51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive
approximations. Journal of Time Series Analysis, 10:283–299, 1989.
52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like
identiﬁcation techniques. IEEE Trans. Automatic Control, 37:900–912, 1992.
53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation.
Systems and Control Letters, 14:307–317, 1990.
54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423–
434, 1953.
55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938.
56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

