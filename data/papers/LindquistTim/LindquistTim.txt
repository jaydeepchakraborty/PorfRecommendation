Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

FRAMEWORKS FOR SECURING LIMITED-DEVICE APPLICATIONS
Timothy Lindquist
Aarthi Ramamurthy
Ramon Anguamea
Timothy.Lindquist@asu.edu Aarthi.Ramamurthy@asu.edu Ramon.Anguamea@asu.edu
Division of Computing Studies
Arizona State University
Abstract

In this paper, we compare the features
available for developing secure distributed
applications for limited devices, such as smart
phones. We limit our scope to examine
frameworks for Java. This work is part of a
continuing project which is considering
capabilities and performance for application
development on these platforms. The paper
considers performance as it relates to various
approaches to securing applications.
The paper addresses two separate concerns.
First is protecting access to resources by an
executing application. The facilities for defining,
limiting and controlling applications during
their development, installation and execution are
described. Second, we discuss approaches
available for securing communication among
application components running on servers or
limited devices.

1. Introduction
In this paper we consider limited devices
that are connected to the Internet and other
communication media, for example, handheld
devices such as intelligent cell phones and PDAs
with Internet connections or platforms which
combine these functionalities. These devices
have limited memory, limited processing power,
no hard disk storage, small display screens, and
limited human input capability. We consider
only those having communication facilities
(WiFi or EV-DO)..
The operating environments for these
devices are comprised of three base components:
local operating system, network operating
system and language runtime environment. The
leading operating systems for these devices are
Symbian, Palm and Windows Mobile 6.
Connected limited-device configuration and
mobile
information
device
profile
(CLDC1.1/MIDP2) are the Java frameworks
designed for resource constrained devices, such
as phones and PDA‚Äôs. CLDC1.1/MIDP2 as

realized by the IBM J9 runtime and SUN
Wireless toolkit pre-defined classes, is the
security environment we evaluated for this paper.
Various development environments are available
depending upon platform and language. For
Microsoft Windows Mobile 6 the application
development environment for the .NET
languages, such as C#, is the .NET Framework
together with Visual Studio 2005 with the
Compact Framework 2.0. Several alternatives are
available for configurations utilizing Java, in part
depending on the Java runtime environment
being used. Sun‚Äôs CLDC HotSpot and IBM‚Äôs J9
are two popular Java runtime environment
choices. Add-on packages and various
configurations are available to support different
security approaches, device capabilities and
networking needs.

2. Background
The connectivity of computing devices to
the Internet, has enabled malicious attacks. The
motivation for attacks varies from willful
espionage to experimentation. Equally important
to protection from attack is the ability to prevent
harm from mistakes in coding, configuration or
user operations. Protection and detection are
difficult in handheld devices because of limited
capability.
Trust
is
confidence
in
expected
functionality. When running an application the
user must trust that it produces valid information
and that privacy, integrity, or confidentiality will
not be compromised. There are several security
policies, protocols and mechanisms that are of
particular interest to the limited devices.
Languages such as Java, C#, and other scripting
languages are widely used in distributed
applications and provide varying degrees
security support. Java and C# both permit
examination of compiled intermediate code for
unsafe actions. Both the Java CLDC/MIDP and
Mobile 6 execution environments support an
array of cryptographic functions that can be used

1530-1605/08 $25.00 ¬© 2008 IEEE

1

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

by communication protocols and by the system
elements to aid in access control at the device
and application level.

3. Java CLDC/MIDP
Three different limited device configurations
exist for the Java 2 Micro Edition (J2ME). For
more capable devices such as set-top boxes and
high-end wireless devices the Connected Device
Configuration (CDC) defines an API whose
functionality is close to J2SE, but is reduced as
appropriate for the limited hardware and
applications. At the lowest level of functionality
is the JavaCard API (for Smart-cards/Sim-cards).
JavaCard as can include functionality for
asynchronous security operations, such as
encryption, decryption, digital signature,
verification and others for limited devices whose
computing capacity is unable to perform such
operations without disrupting user-functionality.
The Connected Limited Device Configuration
(CLDC) is defined for PDA and wireless phone
devices. A device such as a PDA or smart-phone
running Java applications would include a virtual
software stack with the following components:
‚Ä¢ Mobile Information Device Profile
(MIDP2) that supports the application
life-time model, persistent storage,
network resources and the user-interface.
‚Ä¢ CLDC1.1 that supports the core Java
language, IO and networking classes,
security features and internationalization
facilities.
‚Ä¢ The selected Java runtime environment
‚Ä¢ The device operating system and related
services

3.1 Application Security Model
The J2SE model for securing the operations in
an executing virtual machine changed
dramatically as Java evolved. Java originally,
used the sandbox model for application security.
Initial versions of Java provided full trust to
classes loaded locally and prohibited all sensitive
operations from any code obtained dynamically.
Java1.2 introduced support for a continuum of
access control. Access to system resources (such
as files, sockets, runtime, properties, security
permissions, serializable, reflection, and window
toolkit) is granted based on domains. A domain
is defined to include:
‚Ä¢ a set of permissions (resources),

‚Ä¢
‚Ä¢
‚Ä¢

a set of operations associated with
each permission,
codebase indicating the code origin,
a digital signature of the code which
allows identification of the signer
and verification that the code has not
been modified.

The codebase indicates the file or URL from
which the code is loaded. If signed, the alias of
the public key can also be used to define a
domain. Each class loaded into a Java virtual
machine has an associated protection domain,
which defines the access it has to resources.
When execution encounters an operation that
requires a system resource, all classes
representing the currently executing methods
(contents of the runtime stack) are checked to
assure all have access to the resource. The Figure
below is taken from the On-line Java Tutorial
and shows how an execution can include a range
of protection domains ranging from no access to
resources to full access.

Figure 1.

Controlling Access to Java Resources

In J2SE (Java2), security domains are defined
by a policy file granting permissions to the
domain. For example, suppose the company
GrowthStocksExpress publishes an applet on
their (hypothetical) web site at the URL:
http://GSE.com/applets
Assuming the applet needs connections to one
or more hosts having a domain address ending
with GSE.com on ports beginning at 2575, a
policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase
"http://GSE.com/applets"
{
permission java.net.SocketPermission

2

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

"*.GSE.com:2575-", "accept, connect, listen, resolve";
};

A policy may consist of one or more grants each
defining different domains. Each domain may
have one or more associated permissions.
CLDC/MIDP2 security. The application
security model for CLDC/MIDP2 draws on the
model for J2SE, in that it includes domains and
signed code. CLDC/MIDP2, however has a
simpler model, in part because of the constraints
imposed by the configuration and profile. The
following CLDC/MIDP2 constraints are most
significant
‚Ä¢ Java Native Interface (JNI). JNI provides
J2SE applications access to native code
running on the platform. CLDC provides
similar capabilities in Kilo Native
Interface (KNI), but prohibits dynamically
loading and calling arbitrary native
functions.
‚Ä¢ No reflection, remote method invocation
or serialization. In J2SE, an RMI server or
client can cause remote code to be
automatically downloaded and executed
to satisfy argument or return (sub)classes.
When a serializable RMI parameter is
provided an argument of an extended
type, the RMI system will attempt to load
(if necessary from an http codebase) the
needed class.
‚Ä¢ No user-defined class loaders. Related to
the constraint above, the developer cannot
define a class loader in CLDC. The classloader in CLDC cannot be extended or
replaced
by
the
developer.
A
CLDC/MIDP2 application can only load
classes from its own (signed) Java
archive. As a result, the developer cannot
extend or modify any classes in the CLDC
configuration, MIDP2 profile, or which
are provided by the runtime environment
vendor.
‚Ä¢ CLDC supports multi-threading, but it
does not provide facilities to build
daemons or thread-groups.
MIDP2 security protects access to
sensitive API‚Äôs by permissions. Protecting
resources includes the concept of a domain,
which is conceptually similar to J2SE. The
full scope of protection includes the following
elements:
‚Ä¢ Protection domains (4) that are
statically defined in a policy file (by

‚Ä¢

‚Ä¢

the vendor) and associated to
resource permissions; for example,
socket, http, https, PushRegistry. The
protection domains are Minimum,
Maximum
(or
Trusted)
and
Untrusted.
Certificate and archive signature.
The jar file containing application
class files and other resources can be
digitally signed.
Level of access ‚Äì either Allowed or
User.

Unlike J2SE, the 4 protection domains are
device-specific and defined by the runtime
vendor. They can be modified only as
provided by the vendor. Each of the four
domains is associated with a set of
permissions together with a level of access.
The 4 protection domains are defined by the
runtime vendor.
‚Ä¢ Minimum. None of the permissions
are allowed.
‚Ä¢ Maximum. All of the permissions are
allowed.
‚Ä¢ Trusted. All of the permissions are
allowed.
‚Ä¢ Untrusted. To be allowed, the user
must provide consent.
The permissions defined by the MIDP2
specification include: http, socket, https, ssl,
datagram, serversocket, datagramreceiver, and
PushRegistry (invoke other applications). These
permissions may be grouped together by the
vendor into meaningful subsets and assigned to
domains based on the subsets; for example,
NetworkAccess.
Within a domain, the level of access may be
different for different permission (sets). The
accesses are:
‚Ä¢ Allowed. The permission (set) is
allowed without involving the device
user.
‚Ä¢ User level. The application‚Äôs access
to the permission(set) depends on
explicit authorization from the
device user.
With user level of access, a dialog box is
presented to the user indicating information
about the permission and asking the user

3

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

whether access should be granted. User level
access can be specified on one of 3 modes:
‚Ä¢ Oneshot. The user must be prompted
for each operation on the protected
resource.
‚Ä¢ Session. When the user grants
access, it applies to all operations on
the resource during a single
execution of the MIDlet.
‚Ä¢ Blanket. When the user grants
access, it applies to all operations on
the resource during any execution of
the MIDlet.
CLDC/MIDP2 provides MIDlet access to the
Record Management System (RMS), which
provides persistent storage for application data
via a record store. MIDP2 provides shared access
to the record store of other MIDlet suites, and
provides that access should be provided as readwrite or read-only.
Low-level security is provided by the J2ME
Java virtual machine. A virtual machine
supporting CLDC must reject invalid class files.
This is accomplished by a two-step process. At
development time, classes are pre-verified by a
tool which adds special attributes to class files to
facilitate runtime class verification on the device.
Much of the verification process can be handled
statically by the pre-verifier. At runtime, the
virtual machine rejects classes that have not been
pre-verified.

3.2 Security and Trust API
MIDP2 provides HTTPS and SSL for secure
communications with other devices. But, runtime
environment
providers
are
increasingly
providing additional options. For example,
IBM‚Äôs J9 version 5.7 provides web service
security package which allows web method calls
using encrypted SOAP envelopes or digitally
signed method calls for authentication and
information integrity.
Security and Trust Services API (SATSA)
provides access to more comprehensive hash
code,
digital
signature/verification,
key/certificate management, as well as
encryption and decryption. SATSA is designed
as 4 optional components. The primary purpose
is to provide access to a SmartCard Java device,
which provides security functionality in an
asynchronous manner that does not disrupt
applications supporting the device user.

SmartCard includes the Java Card Protection
Profile. The protection profile supports both
open and closed cards. Open cards provide the
end-user with the ability to install or activate
new applications on the card. Closed cards have
applications set by the vendor at the time the
card is personalized for the end-user. A good
example of a closed card may be a banking card
that supports personal electronic purchases and
bank account functions. Open cards that allow
new applications to be downloaded and installed
on the card present special security risk that
would exclude open cards that include banking
applications. Nevertheless, applications for open
cards that support other aspects of security may
become increasingly important. An example may
be securely communicating information outside
of direct e-commerce applications. Data integrity
and authentication are becoming increasingly
important
as
electronic
communication
proliferates.
The Java Card Protection Profile defines
four different configurations for a Java Card
based on open and closed cards. The minimum
configuration corresponds to a closed card in
which no applications can be installed on the
card after it‚Äôs been issued to an end-user. The
three
remaining
configurations
provide
additional functionality that‚Äôs available through
the evolution of the Java Card specification, such
as RMI (a limited version), logical channels,
applet deletion, object deletion, external
memory, biometry, and contactless interface.
The Java virtual machine for the device includes
an API (RTE API) that may contain classes for
performing security operations on information
and for certificate and key management.
SATSA runs on the limited-device, not on
the Java Card. SATSA provides an interface to
card security functionality, or when there is no
associated smart card, provides security
operations for the limited-device. SATSA has
four optional packages.
‚Ä¢ SATSA-APDU provides low-level
stream/socket-based protocol for communicating
between the limited-device and the card.
‚Ä¢ SATSA-JCRMI provides an RMI
interface that allows an application running on
the limited-device to call methods running in
applications on the Card. This interface would be
used to instead of SATSA-APDU to avoid the
overhead of programming with a low-level
socket data protocol.
‚Ä¢ SATSA-PKI allows limited device
applications to use the smart card to digitally
sign information or to verify digital signatures.

4

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

PKI also provides for key and certificate
management.
‚Ä¢ SATSA-CRYPTO. When a Java Card is
not available, the CRYPTO API is used to
compute security operations directly on the
limited-device.
Use of APDU, JCRMI, and/or PKI is
accomplished using threading on the limiteddevice. Threading allows security operations to
take place on the card while other applications
continue to run on the device supporting the enduser. In this scenario, SATSA is appropriate for
limited-devices with constrained processing
power. Independent of processing capability,
using a Java Card may be necessary to provide
assurance level that is appropriate to the
application. The open-device nature of cell
phones and PDA‚Äôs make it difficult to certify
trustworthiness of applications on the device.
Instead, we can isolate all high-risk user-specific
information and computations to a certified
secure Java Card.

HTML page, it returns an XML message in
Simple Object Application Protocol (SOAP)
format.
The service description - specified in Web
Services Description Language (WSDL) - this
description defines the web methods (functions)
that a service will accept - the inputs that go into
these methods, and the format of the output that
can be expected in return. This is used in
generating a web service client proxy class for
the limited device.
The web service registry - is a directory of
web services. The directory is optional because a
web service need not be listed in a registry to be
used. The registry provides a catalogue of
available services - similar to Java Naming and
Directory Service (JNDI).
The web service client proxy ‚Äì The proxy
negotiates the communication between a limited
device client and the web service. It marshals
arguments, signs or encrypts as appropriate,
posts the message and interprets the result.

4. Secure Web Services

4.1 Types of Security Services

Web services provide an XML-based
service protocol for communicating among
components of a distributed application. Web
services differ from prior similar technologies,
such as Microsoft DCOM, Object Management
Group CORBA and Java Remote Method
Invocation through reliance on http protocol and
XML.

The following are the security services that
may be required by a distributed limited device
application.
Authentication: Ensures that the sender and
receiver are who they claim to be. Mechanisms
such as username/password, smart cards, and
Public Key Infrastructure (PKI) can be used to
assure authentication.
Authorization or Access Control: Ensures that
an authenticated entity can access only those
services they are allowed to access. Access
control lists are used to implement this.
Confidentiality: This assures that information
in storage and in-transit are accessible only for
reading by authorized parties. Encryption is used
to
assure
message
confidentiality.
Integrity: Ensures that information, either in
storage or in-transit cannot be modified
intentionally unintentionally. Digital signatures
are used to assure message integrity.
Non-repudiation: Requires that neither the
sender nor the receiver of a message be able to
legitimately claim they didn't send/receive the
message.

Figure 2. Web Services Architecture [9]
In Figure 2, the service, is performed by a web
server acting as a container for executing the
service code. This is generally just a web like
page that gets posted similar to the way other
http web requests are done. Instead of returning a

4.2 Transport Level Security
The most popular security scheme for web
services is SSL (Secure Socket Layer), which is
typically used with http, and is supported by

5

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

CLDC/MIDP2.
However using SSL for
securing web services has a number of
limitations. The inadequacy of SSL can be easily
explained by a simple example.
Consider a Web Service that can be provided
indirectly to a user. A user accesses a website
which indirectly invokes another remote web
service. In this case, we have two security
contexts:
1. Between the user and the website
2. Between the user and the web service
The second security context requires the
security of SOAP request/reply message
(between the web site and the web service) to be
assured over more than one client-server
connection. SSL is inadequate to provide this
type of security mainly because of the fact that
while it encrypts the data stream, it does not
support end-to-end confidentiality.
The shortcomings of SSL (https) should be
considered when being used for a distributed
application to reside on a limited-device.
SSL is designed to provide point-to-point
security. Often, Web services require end-to-end
security, where multiple intermediary nodes
could exist between the two endpoints. In a
typical Web services environment XML-based
business documents route through multiple
intermediary nodes.
Https in its current form does not support
non-repudiation well. Non-repudiation is critical
for business Web services and, for that matter,
any business transaction.
Finally, SSL does not provide element-wise
signing and encryption. For example, if there is a
large purchase order XML document, yet only a
single element, say, a credit card element needs
to be encrypted. Signing or encrypting a single
element is difficult with transport level security.

4.3 XML Signature
XML based security schemes, provide unified
and comprehensive security functionalities for
Web Services. The important ones being, XML
Signature, XML Encryption, WS-Security (Web
ServicesSecurity).
The W3C (World Wide Web Consortium)
and the IETF (Internet Engineering Task Force)
jointly coordinated to generate the XML digital
signature technology. The XML digital signature
specification [10] defines XML syntax for
representing digital signatures over any data
type. It also specifies the procedures for
computing and verifying such signatures.

Another important area that XML digital
signature addresses is the canonicalization of
XML documents. Canonicalization enables the
generation of the identical message digest and
thus identical digital signatures for XML
documents that are syntactically equivalent but
different in appearance due to, for example, a
different number of white spaces present in the
documents.
The advantages of using XML digital
signature can be summarized as below.
‚Ä¢
‚Ä¢

‚Ä¢

‚Ä¢
‚Ä¢

It accounts for and takes advantages of two
existing and popular technologies, viz., the
Internet and XML.
XML digital signature provides a flexible
means of signing. For example, individual
item or multiple items of an XML document
can be signed. This becomes extremely
useful in a scenario where each person in a
workflow is responsible ONLY for certain
work.
It supports diverse sets of Internet
transaction models. For instance, the
document signed can be local or even a
remote object, as long as those objects can
be referenced through a URI (Uniform
Resource Identifier). A signature can be
either enveloped or enveloping, which
means the signature can be either embedded
in a document being signed or reside outside
the document.
It provides important security features like
authentication, data integrity (tamperproofing), and non-repudiation.
XML digital signature also allows multiple
signing levels for the same content, thus
allowing flexible signing semantics. For
example, the same content can be
semantically signed, cosigned, witnessed,
and notarized by different people.

Figure 3. WSE ‚Äì Input/Output filters [1]

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

Web Service Deployment Descriptor
(WSDD) and Handlers play the pivotal role in
the implementation of digital signature in Java.
A deployment descriptor specifies aspects such
as handlers and communication protocol. The
Handler is a java class (implementing the Input
and output filters of Figure 3) that provides a
MessageContext through which access is
provided to the input/output stream of XML \In
the Apache AXIS framework, MessageContext
is a structure, that contains: 1) a request message,
2) a response message, and 3) a number of
properties.
All the SOAP message manipulation is done
within the handler class.

6. Conclusions and Issues
The authors have continuing efforts in this
area which include obtaining devices, software
development environments, and simulators /
emulators related to securing limited-devices.
Our approach considers the operating
environment on the device as well as their
applications.
We are in the midst of consolidation of
small hand-held devices to provide integrated
functionality. Common applications including
personal organizers, cell-phones, and multimedia players can effectively be placed on a
single platform. While users who desire more
than one of these functionalities are exploring
integrated solutions, the industry is pushing
separation (partly for financial reasons.)
Consolidated functionality brings a higher
diversity of applications onto limited devices, as
does special purpose applications (for example
autonomous vehicle control). Either way,
security concerns increase.
The use of smart cards in the United States
is just beginning after lagging behind use in
some other regions. The integration of smart
cards (SIM-Cards) on cell phones is an
indication of this trend. Enabling high-risk
applications, such as banking and purchasing,
by leveraging smart cards integrated with other
devices presents an attractive alternative. Of
course the concern for security places new
demands on platforms in which security has not
historically been a high priority.
Performance has been the primary
impediment to the use of more strongly objectoriented languages such as Java for limited
device applications. Securing a distributed

application complicates the issue. Important
considerations include:
‚Ä¢ Underlying
architecture
processor
performance,
ancillary
processing
capability such as SmartCard,
‚Ä¢ Frameworks supporting securing the
application, as well as communications,
‚Ä¢ Use of security mechanisms appropriate
to application needs (authentication,
integrity, confidentiality),
‚Ä¢ Proper use of available frameworks
including proper handling of passwords,
certificates, keys, digital signatures, and
encrypted information.
Frameworks discussed in the paper are an
important enabler to developing more secure
distributed limited-device applications. Further
usage reports and benchmarking for security
mechanisms would better support developers.

References
[1.] Tim Ewald, ‚ÄúProgramming with Web
Services Enhancements 1.0 for
Microsoft.NET‚Äù, Available, see:
http://msdn.microsoft.com/webservices/buil
ding/wse/default.aspx?pull=/library/enus/dnwse/html/progwse.asp
[2.] Sun Microsystems Java Security and
Crypto Implementation,
http://www.cs.wustl.edu/~luther/Classes/Cs
502/WHITE-PAPERS/jcsi.html
[3.] Knudsen, Jonathan; Understanding MIDP
2.0‚Äôs Security Architecture.
http://developers.sun.com/techtopics/mobilit
y/midp/articles/permissions/
[4.] WebSphere Everyplace Micro Environment
v5.7; MIDP Installation guide for J9 Palm
runtime environment. Available online from
IBM.
[5.] Mourad Debbabi, Mohamed Saleh,
Chmseddine Talhi and Sami Zhioua:
‚ÄúSecurity Evaluation of J2ME CLDC
Embedded Java Platform‚Äù, in Journal of
Object Technology, (5,2) Mar-Apr 2006. pp.
125-54.
[6.] Security and Trust Service APIs for Java
Platform Micro Edition Developers Guide.
Available from http://www.java.sun.com/
[7.] Pannu, K.; Lindquist, TE; Whitehouse, RO;
and Li, YH; ‚ÄúJava Performance on Limited
Devices‚Äù; Proc The 2005 International
Conference on Embedded Systems and
Applications, CSREA Press, Las Vegas,
June, 2005.

7

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

[8.] Lindquist, TE, Diarra, M, and Millard, BR;
‚ÄúA Java Cryptography Service Provider
Implementing One-Time Pad‚Äù; Proc. 37th
Annual Hawaii Int'l Conf on Systems
Sciences, ACM, IEEE Computer Society,
January 2004.
[9.] Online Documentation on Web Services,

Available from: http://www.servicearchitecture.com/webservices/articles/web_services_explained.ht
ml
[10.] XML Digital Signature Specification, W3C
Recommendations, Available from:
http://www.w3.org/TR/xmldsig-core

8

ARTICLE IN PRESS

The Journal of Systems and Software xxx (2004) xxx≠xxx www.elsevier.com/locate/jss

Automated support for service-based software development and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a Department of Computer Science and Engineering, Arizona State University≠≠Main, P.O. Box 875406, Tempe, AZ 85287-5406, USA Department of Electronics and Computer Engineering Technology, Arizona State University≠≠East 7001 E, Williams Field Road, Building 50, Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003

Abstract A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. ” 2003 Published by Elsevier Inc.

1. Introduction A service-based development paradigm, or services model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can interact with one another and be providers or consumers of data and behavior. Some of the defining characteristics of service-based technologies include modularity, availability, description, implementation-independence, and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon the definition of the interface needed to access a service (description) while hiding the details of its implementation (implementation-independence). Since the client and service are decoupled, other concerns such as side effects become non-factors (modularity). One of the potential benefits of using a service-based approach for developing software is that at any given time, a wide variety of alternatives may be available that meet the needs of a given client (availability). As a result, any or all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. The technique utilizes an architecture description language to describe services and achieves run-time integration using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and involves the automatic generation of ``glue'' code for both services and applications. The Jini interconnection technology (Edwards, 1999) is used as a broker for facilitating service registration, lookup, and integration at runtime. The remainder of this paper is organized as follows. Section 2 describes background material in the areas of software architecture and the middleware technology we are using to enable dynamic integration (i.e. Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work, and Section 5 draws conclusions and suggests further investigations.

2. Background
Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751. E-mail address: gannod@asu.edu (G.C. Gannod). 0164-1212/$ - see front matter ” 2003 Published by Elsevier Inc. doi:10.1016/j.jss.2003.05.002
*

This section describes background material on software architecture and Jini.

ARTICLE IN PRESS
2 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx

2.1. Software architecture A software architecture describes the overall organization of a software system in terms of its constituent elements, including computational units and their interrelationships (Shaw and Garlan, 1996). In general, an architecture is defined as a configuration of components and connectors. A component is an encapsulation of a computational unit and has an interface (e.g. port) that specifies the capabilities that the component can provide. Connectors encapsulate the ways that components interact. A connector is specified by the type of the connector, the roles defined by the connector type, and the constraints imposed on the roles of the connector. A connector defines a set of roles for the participants of the interaction specified by the connector. Components are connected by attaching their ports to the roles of connectors. Another important concept is an architectural style. An architectural style defines patterns and semantic constraints on a configuration of components and connectors. As such, a style can define a set or family of systems that share common architectural semantics (Medvidovic and Taylor, 1997).

3. Approach This section describes the service-based development approach including the techniques used for defining services, specifying client applications, realizing integration, and generating glue code. 3.1. Example Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on the health of systems in a network. This application utilizes a network sniffer service and a port monitoring service. The network sniffer service gives an administrator information about traffic on the network. The port monitoring service provides information about the open ports on the various machines on a network. Together, these services facilitate determining whether certain kinds of attacks (such as ping storms) are being directed to a machine or machines. The client application supports analysis of several networks, each of which is accessed using the buttons shown on the top portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services that utilize different models of execution (strict call return and data streams). The remainder of this section refers to architectural specifications that were used in the construction of this example. 3.2. Overview The methodology that we have developed follows closely the model suggested by Stal (2002) for web ser-

2.2. Jini The primary enabling feature of the work described in this paper is the existence of Jini (Edwards, 1999) for the delivery and management of services. In a typical Jini network, services are provided by devices that are connected to the network. A Jini technology layer provides distributed system services for activities such as discovery, lookup, remote event management, transaction management, service registration, and service leasing. When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is registered, a proxy (Gamma et al., 1995) is stored by the lookup service. The proxy can later be transported to the clients of the service. Other network members can discover the availability of the service via the lookup service. When a client application finds an appropriate device, the lookup service sets up the connection. In our approach to component integration, we use Jini to provide a standard method for registering and connecting a client to corresponding software components that are acting as services. One of the advantages of using this Jini-based integration technique is that it facilitates construction of applications ``on-the-fly'' whereby components can be used on an as-needed basis. One of the disadvantages is that clients of services must have some prior knowledge about how to use each respective service.

Fig. 1. Running example.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx 3

vices, although the technology that we are using to realize our approach is Jini. The approach itself focuses on two concerns with respect to software reuse. That is, it addresses both for reuse and with reuse concerns. With respect to for reuse, the approach involves the construction of services via the use of adapter and proxy synthesis. Specifically, the methodology involves two steps for creating services as follows: (1) specification of components as services, and (2) generation of services using proxies via the construction of appropriate adapters and glue code. These services are consequently registered and made available on a network. With respect to with reuse concerns, the approach involves the construction of applications using services as follows: (1) specification of a client to make use of services from a repository or network, (2) generation of the client (both manual construction of client application specific code and automated generation of glue code), and (3) execution of the client, including integration of the specified services at runtime. Within our approach, a user (e.g. developer) is responsible for writing the source code for the client application along with the specification of the architecture for a client. Among other things, the client specification contains a description of the basic services that the client application will need in order to be a complete system. All other source code, including code necessary to realize the connections between the client and employed services, is generated based on the specifications describing clients, services, and connectors. 3.3. Service generation In this section we describe some of the issues related to automating the creation of service wrappers. To support these activities, we have developed an automated tool that takes as input a software architecture and produces glue code. A primary source of reusable components that we employ in our approach are legacy command-line applications (Gannod et al., 2000). In order to generate services from legacy components, we take the approach of wrapping the components by utilizing the interface provided by the component. Since command-line applications have a well-defined input and output interface, the interface of the application as a service can be based entirely upon the knowledge of what the application intends to provide. 3.3.1. Specification and synthesis The concept of using an adapter for wrapping legacy software is not a new one (Gamma et al., 1995). As a migration strategy, component wrapping has many benefits in terms of re-engineering including a reduction in the amount of new code that must be created and a reduction in the amount of existing code that must be rewritten.

In regards to wrapping components, our approach uses two steps. First, a specification of the legacy software as an architectural component is created. These specifications provide vital information that is required to define the interface to the legacy software. Second, the appropriate adapter source code is synthesized based on the specification. 3.3.2. Specification requirements To aid in the development of an appropriate scheme for the wrapping activity, we defined the following requirements upon specifications. These requirements are as follows: (S1) a sufficient amount of information should be captured in the interface specification in order to minimize the amount of source code that must be manually constructed, (S2) a specification of the interface of the adapted component should be as loosely coupled as possible from the target implementation language, and (S3) the specification of the adapted component should be usable within a more general architectural context. The requirement S1 addresses the fact that we are interested in gaining a benefit from reusing legacy software. As a consequence, we must avoid modifying the source code of the legacy software. At the same time, we must provide an interface that is sufficient for use by a target application. To provide that interface, a sufficient amount of information is needed in order to automatically construct the adapter. Our selection of command-line applications addresses the modification concern of requirement S1 since source code is not available. As such, we are required to provide an interface that is based solely on the knowledge of how the application is used rather than how it works. Table 1 shows the properties used in the specification of services, clients and connectors. A service component specification consists of two parts: properties and ports. The properties section describes style of the service, while the ports section describes functions provided by the service. In addition, the service specifications indicate style-based information as well as conditions or commands that need to be true or executed, respectively, in order to establish an environment necessary to use the service. Finally, a key in terms of a ``service type'' (e.g. interface property) is used to support a service lookup, which is later utilized during application integration. The requirement S2 (i.e. the decoupling of a specification from a target implementation language) is based on the desire to apply the synthesis approach to a variety of target languages and implementations. In addition, this requirement facilitates enforcement of requirement S1 by ensuring that new source code is not artificially embedded in the specification. While satisfying this requirement is ideal, we found in our strategy that a certain amount of implementation dependence was

ARTICLE IN PRESS
4 Table 1 Properties Group Service properties Service port properties Attribute Component-Type Signature Return Cmd Pre Post Interface Path Port-Type Shared-GUI Part-of-client GUI-CodeFile Component-Type Shared-GUI Port-Type Interface Connector-Type Prop-type Description Architectural style this component adheres to The port's signature The port's return type The command-line program being wrapped Pre-processing command Post-processing command The generic interface implemented by this port Path to the wrapped command-line program The port's type based on the Component-Type Boolean indicating shared (true) or exclusive (false) GUI Identifies inclusion in client application The filename for client's GUI code Architectural style this component adheres to Boolean indicating shared (true) or exclusive (false) GUI The port's type based on the Component-Type The generic interface that this port can bind with Architectural style this connector adheres to The connectors role based on the Connector-Type G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx

Client properties

Client port properties Connector properties Connector role

necessary due to the fact that our implementation would make use of Jini. When a component has been wrapped using our technique, an interface is defined that facilitates the use of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also desirable to be able to use the specification of the adapted component within a more general architectural context. That is, it is advantageous to be able to use the specification as part of the software architecture specification for new systems. In using a content-rich specification, where interfaces are defined explicitly, the added benefit of providing information that can be integrated into an architectural specification of a target application is gained. In order to realize the requirements placed upon desired interface specifications for legacy software wrappers, we used the ACME (Garlan et al., 1997) architecture description language (ADL). Specifically, we used the properties section of the ACME ADL to specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an ADL that has been used for high-level architectural specification and interchange (Garlan et al., 1997). 3.3.3. Synthesis As stated earlier, the class of legacy systems that we are considering are command-line applications (Gannod et al., 2000). Given this constraint, we make the assumption that any client applications utilizing the wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We find this assumption to be reasonable due to the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage profiles. In our approach, the specification that is needed to generate wrappers contains properties associated with the ports as shown in Fig. 2. These properties include Signature, Command, Pre, Post, Path, Interface, and Return. In this case, the specification describes the NetworkSniffing and PortMonitor services, which are services created by wrapping tcpdump, and nmap, respectively. In the synthesis process, ACME specifica-

Fig. 2. ACME services section.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx 5

tions are combined with a standard template that implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we have constructed to automate this process generates the appropriate source code for facilitating interaction between a potential client and the wrapped component. At present, this is an automated tool that generates fully executable code for the wrapped application and does not require the user to modify or write any new code outside of option GUI code. Both the service and client synthesis steps utilize a template-based approach to synthesize code. That is, a standard file has been created that has stubs containing place holders that must be instantiated with either service or client specific parameters. Fig. 3 contains a portion of the ServiceTemplate file which contains all of the application and service independent source code and provides the routines necessary to integrate the legacy code into a Jini network. Specifically, the ServiceTemplate contains functions that implement the discover and join protocol for registering a service with the lookup service. The ServiceTemplate also contains tags that are place-holders for the automatically generated functions. For instance, in Fig. 3 the tag <put-ServerName> is a place-holder for the final name of the adapter component. In addition to the ServiceTemplate, there is also a reusable set of functions that can be utilized in an interface specification and consequently in the generated wrappers. For instance, the getOutputStream( ) routine (shown in Fig. 4) is available as a function for use within the Java code to provide standard stream input support.

The amount of automation that has been achieved through the approach described above is dependent on the degree of graphical user interface (GUI) support that is desired. For a service, the code synthesis step can be fully automated if no GUI support is desired. Otherwise, the amount of manual code construction is limited to GUI support. 3.4. Client generation Once the services are generated and stored in a repository, a client application can be architected. First we need to specify the client application taking into account the architectural style of each of the services. Once a client is specified, it can be verified and generated. In this subsection we look at the requirements for specifying the client and then describe synthesis of the client. 3.4.1. Specification Refer again to Table 1 which, in addition to the properties for service specifications, contains the properties of client application components and connectors. When dealing with integration at the component level, two issues arise (among others) that are of interest. First, the problem of architectural style mismatch (Shaw and Garlan, 1996) occurs when the underlying assumptions made by components conflict. Second, most modern applications provide a graphical user interface (GUI). As a result, integration of off-the-shelf components can leverage these user interfaces in order to take advantage of previously built technology. To cope with these issues we impose two requirements on the specification of client applications as follows: (C1) the specification of the components should capture the notion of architectural style so that the high-level interaction between clients and services can be verified, and (C2) the specification must facilitate the use of shared and exclusive GUI components. The requirement C1 addresses the fact that a component must provide a notion of architectural style. A component's style plays a very important role when it interacts with other components by imposing interaction constraints. Using a basic style attribute (by name) architectural mismatches can be determined by simple keyword matching. Requirement C2 addresses the fact that a service may provide a GUI that allows a user to access and control the service. In this context, there may be GUI components provided by services that are either sharable by other services or exclusive to the service. A sharable GUI component can be used by both the client as well as other integrated services while an exclusive GUI component can only be used by the service that provides the interface.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

ARTICLE IN PRESS
6 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx

3.4.2. Synthesis The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample specification of a client. The information contained within client specifications are used to support the synthesis of client code. This synthesis step utilizes two features; first, the information regarding connectors and attachments, such as those shown in Fig. 5 are used to determine the relationships between client applications and desired services. Second, information regarding GUIs provided by services is used to determine how to realize the GUI in a client application. In our framework, the wrappers for the various services can implement a common interface that allows the client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially used across multiple services and are identified using a name taken from a standard GUI vocabulary (for example ``ResultsWindow''). The name is then used to identify which GUI components can be shared across services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive component is independent and cannot be shared between services. The exclusive GUI components of the wrappers are used as is but may interact with one or more of the shared components. For both shared and exclusive components, the interaction with the client GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers. 3.5. Discussion As stated in Section 1, the service-oriented domain are characterized by modularity, availability, description, implementation-independence, and publication. As a result, services and service-based approaches are more coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a software architecture to specify applications that operate under these characteristics. As such, a software architecture in this context defines components, their interfaces, and the mechanisms by which services (as components) can be joined in order to fulfill needed software behavior. Consequently, services enable the use of a software architecture as an integration vehicle in which the architecture facilitates generation of glue code. It is the very fact that services adhere to the characteristics described above that the integration and code generation become possible at this level. However, the approach does lack in its ability to address needs that are more specific than what individual services provide. To cope with this, we are developing an approach that allows for the creation of federated services, where services are combined to meet some higher-level objective.

4. Related work Recently, the use of web services has gained attention with vendors releasing webservices toolkits that allow for building and using webservices. Webservices and .NET (Meyer, 2001) are based on the SOAP and XML (Seely and Sharkey, 2001) protocols. The Jini approach to service integration goes beyond what the webservices paradigm provides by defining how services can be used within a larger application context and providing support for code transportation. FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server that distributed messages to other tools that were interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this approach is a precursor to service-based development. Urnes and Graham (1999) describe an approach to facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the component space across a network. In our approach, services are potentially developed by different organizations and thus the choice of what to distribute is not

Fig. 5. Portion of ACME client specification.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx 7

available. The component model being addressed by Urnes and Graham, as such, is finer-grained and violates implementation-independence, a tenet of servicebased development. Grundy et al. (2000) discuss issues and experiences in constructing component-based software engineering environments. They created a variety of useful software engineering tools using their tool set (JViews, JComposer, etc.). They use ``plug and play'' and an event-based composition approach to achieve component integration. In this framework, components are more tightly coupled and their granularity is finegrained. In contrast, our approach is based on dynamic integration of coarse-grained services that are loosely coupled. Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and component gluing. This creates a clean separation of customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work provides a potential strategy for dealing with component mismatches, which is currently ignored in our approach.

Acknowledgements G. Gannod is supported in part by NSF CAREER grant CCR-0133956. References
Edwards, W.K., 1999. Core Jini. Prentice-Hall. Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services. Commun. ACM 45 (10), 77≠80. Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison Wesley Longman. Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for legacy software. In: Proc. 7th Working Conf. Reverse Eng., IEEE, pp. 128≠137. Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture description interchange language. In: Proc. CASCON'97, pp. 69≠ 183. Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2). Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to develop a family of applications. IEE Proc. Software Eng. 144 (5≠ 6), 237≠248. Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92≠97. Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration with pluggable composite adapters. Software Archit. Comp. Technol.. Reiss, S.P., 1990. Connecting tools using message passing in field environment. IEEE Software 7 (7), 57≠66. Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services Development Using XML. Prentice-Hall. Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on an Emerging Discipline. Prentice-Hall. Stal, M., 2002. Web services: beyond component-based computing. Commun. ACM 45 (10), 71≠76. Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In Proc. of Design, Specification and Verification of Interactive Systems. Gerald C. Gannod is an Assistant Professor in the Department of Computer Science and Engineering at Arizona State University and is a recipient of a 2002 NSF CAREER Award. He received the M.S. (1994) and Ph.D. (1998) degrees in Computer Science from Michigan State University. His research interests include software product lines, software reverse engineering, formal methods for software development, software architecture, and software for embedded systems. Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras (Chennai), India. His research interests include software engineering, distributed and object-oriented systems, software design, software architecture, service-oriented software engineering, and Wireless Application platforms. Timothy E. Lindquist is Professor and Chair in the Department of Electronics and Computer Engineering Technology at Arizona State University East Campus in Mesa, Arizona. He received the Ph.D. (1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

5. Conclusions The web-based services paradigm has gained attention recently with the development of technologies such as SOAP (Seely and Sharkey, 2001). The benefits of such technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these benefits by providing facilities for on-the-fly integration and component introspection. In this paper, we described an approach for addressing component integration via the use of services in the context of Jini interconnection technology. Specifically, the approach utilizes synthesis to generate code necessary to realize component integration. To facilitate integration, the ACME ADL is used to specify both services and target applications, and is used a medium for performing service compatibility checking. We are currently developing an environment that will assist in the creation of applications within the servicebased paradigm and will support service browsing to facilitate application design. In addition, we are investigating approaches for allowing services to collaborate beyond the scope of a client application in order to create federated groups of services. Furthermore, we are developing technologies similar to the ones described in this paper in order to support service-based application within the .NET and web service frameworks.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION
OF TIME SERIES*
JORGE MARI‚Ä†, ANDERS DAHLEÃÅN‚Ä†, AND ANDERS LINDQUIST‚Ä†

Abstract. In this paper we consider a three-step procedure for identiÔ¨Åcation of
time series, based on covariance extension and model reduction, and we present a
complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy
model is determined, which is Ô¨Ånally approximated by a lower-order model by
stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all
three steps has been lacking. Supposing the data is generated from a true Ô¨Ånitedimensional system which is minimum phase, it is shown that the transfer function
of the estimated system tends in H‚àû to the true transfer function as the data length
tends to inÔ¨Ånity, if the covariance extension and the model reduction is done properly. The proposed identiÔ¨Åcation procedure, and some variations of it, are evaluated
by simulations.

1. Introduction
In recent years there has been quite some interest in a certain type of procedures
for identiÔ¨Åcation of time series known as subspace methods [1, 42, 41, 28, 29]. These
identiÔ¨Åcation procedures are based on geometric projection methods, and they could
be understood in the context of splitting geometry and partial stochastic realization
theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9],
these procedures are algebraically equivalent to minimal factorization of a Hankel
matrix of covariance estimates, and they make no distinction between stochastic and
deterministic partial realizations. Therefore they may fail because of loss of positive
realness in the spectral estimation phase.
In an attempt to overcome these problems we analyze an alternative approach
to time series identiÔ¨Åcation proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the
maximum-entropy method, leading to a high order autoregressive (AR) process, and
Ô¨Ånally stochastically balanced truncation. This method shares certain features with
stochastic subspace identiÔ¨Åcation methods, the most obvious one being that it is
based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
‚àó This research was supported by a grant from the Swedish Research Council for Engineering
Sciences (TFR).
‚Ä† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
1

2

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for
maximum-likelihood (ML) methods, are needed.
The idea of approximating an autoregressive moving-average (ARMA) process by
an AR process is by no means new. Its origins can be traced back to the Wold
decomposition [55] where L2 -convergence of high-order AR models to general analytic
models is shown. Pioneers in the use of this concept for systems identiÔ¨Åcation are
Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations
were studied by Berk [2] and later reÔ¨Åned in [36, 34, 33, 7]. The interesting paper [7]
contains nice proofs of some of the convergence results needed in this paper, but, for
the sake of completeness and insight, we provide new proofs based on some properties
of fast Ô¨Åltering algorithms [5] and simple methods of complex analysis and SzegoÃã
polynomials. The power of the theory of SzegoÃã polynomials and Toeplitz matrices in
analyzing stochastic processes is reported in [24], but, except for elementary theory,
it has not been much used in systems identiÔ¨Åcation [39]. This is even more true for
the newer results [16, 40, 37, 27] on orthogonal polynomials.
The idea of using model reduction for systems identiÔ¨Åcation appears in the thesis
by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency
weighted reduction. Instead, we use stochastically balanced truncation, for which we
develop a simple computational procedure, exploiting the special structure of the AR
model. We also show the advantage of this reduction procedure by theoretical analysis
and simulations. In fact, a comprehensive study comprising all the steps mentioned
above together with a qualitative and quantitative analysis of the entire identiÔ¨Åcation
strategy has been lacking, and that is what we oÔ¨Äer in this paper.
The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identiÔ¨Åcation
procedure contributes to the estimation error. In Section 3 we show that the transfer
function of the maximum-entropy Ô¨Ålter, constructed from true covariances, tends to
that of the true Ô¨Ålter in H‚àû norm at a geometric rate determined by the largest
modulus of the zeros of the true Ô¨Ålter as the order of the maximum-entropy Ô¨Ålter
becomes large. However the order of the approximation is too high, and therefore
model reduction is performed. This is studied in Section 4. A stochastic balancing
procedure, based only on linear-algebra operations so that no Riccati equations need
to be solved, is provided together with the analysis of the model-reduction error.
Both deterministically and stochastically balanced truncation lead to good results.
However, when the covariances are estimated from statistical data, stochastic model
reduction is found to be superior. In particular, variances are considerably closer to
the CrameÃÅr-Rao bounds. In Section 5 we state our statistical convergence theorems,
proving that the total error tends to zero as the length of the data string tends to
inÔ¨Ånity, provided the degree of the AR model tends to inÔ¨Ånity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using
stochastic subspace identiÔ¨Åcation [43] is included. For clarity of exposition, all the
proofs have been deferred to two appendices, Appendix A dealing with the asymptotic
properties of the maximum-entropy Ô¨Ålter, and Appendix B devoted to the statistical
error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identiÔ¨Åcation
Time series identiÔ¨Åcation in the form studied here amounts to estimating the matrices
(A, B, C, D) in some n-dimensional linear stochastic system

x(t + 1) = Ax(t) + Bw(t)
(2.1)
y(t)
= Cx(t) + Dw(t)
driven by normalized white noise {w(t)}, from a data string of observations
{y0 , y1 , y2 , . . . , yN }

(2.2)

of the output process {y(t)}, which here will be taken to be scalar.
The basic idea behind our approach is very simple: given estimates of a partial
sequence
c 0 , c 1 , c 2 , . . . , cŒΩ

(2.3)

of the covariances ck = E{y(t+k)y(t)}, which satisÔ¨Åes the condition that the Toeplitz
matrix
Ô£Æ
Ô£π
c2 ¬∑ ¬∑ ¬∑ cŒΩ
c0 c1
Ô£Ø c1 c0
c1 ¬∑ ¬∑ ¬∑ cŒΩ‚àí1 Ô£∫
Ô£Ø
Ô£∫
Ô£Ø
c
c
c0 ¬∑ ¬∑ ¬∑ cŒΩ‚àí2 Ô£∫
1
TŒΩ+1 := Ô£Ø 2
(2.4)
..
.. Ô£∫
..
...
Ô£∞ ...
Ô£ª
.
.
.
cŒΩ cŒΩ‚àí1 cŒΩ‚àí2 ¬∑ ¬∑ ¬∑ c0
is positive deÔ¨Ånite, Ô¨Årst construct a high-order model continuing (2.3) by covariance
extension. This model has all the required positivity properties, but the order is too
high. Then reduce the order by means of a positivity-preserving model reduction
procedure to be speciÔ¨Åed below. That this simple recipe will in fact provide a good
identiÔ¨Åcation method is by no means a trivial matter but is based on some rather
deep results, which will be presented here.
More speciÔ¨Åcally, the approach consists of three steps, for which there are several
possible variants that will be discussed below. The rigorous mathematical analysis,
however, will be carried out for the following procedure, for which we shall give
theoretical bounds.
(i) Estimate a partial covariance sequence
cÃÇ0 , cÃÇ1 , cÃÇ2 , . . . , cÃÇŒΩ

(2.5)

from the time-series data (2.2) via the ergodic estimate
N ‚àík
1 	
yt+k yt
cÃÇk =
N + 1 t=0

k = 0, 1, . . . , ŒΩ.

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer
function
zŒΩ
,
(2.7)
WÃÇŒΩ (z) =
œÜÃÇŒΩ (z)
where œÜÃÇŒΩ (z) is the normalized SzegoÃà polynomial of degree ŒΩ, to be introduced
in Section 3, computed from the estimated covariance data (2.5).

4

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

(iii) Determine a reduced-degree approximation WÃÇ (z) of WÃÇŒΩ (z) via a stochastic
model reduction procedure [11] to be described in more detail in Section 4.
In this procedure, the idea is that ŒΩ >> n, the order of the system to be identiÔ¨Åed,
and ideally nÃÇ := deg WÃÇ equals the degree n of the true system (2.1). However, the
method will produce a valid model even if this is not the case or even if there is
no ‚Äútrue‚Äù underlying model. This is in contrast to stochastic subspace identiÔ¨Åcation
models, which may fail to produce any model at all [9].
There are possibilities for variations of the procedure described above. In Step (i)
we could use alternative covariance estimates or Burg‚Äôs estimation of Schur parameters
[3], the only requirements being that the estimated Toeplitz matrix TÃÇŒΩ+1 of (2.5) is
positive deÔ¨Ånite and that cÃÇk ‚Üí ck a.s. as N ‚Üí ‚àû. In Step (ii) we could instead use
approximate covariance extension or covariance extension with prescribed zeros, for
which there is now a complete parameterization [5] and an algorithm [4]. (In the latter
case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction
methods could be used. For example, an important model reduction paradigm is the
one based on optimal Hankel norm approximation [21].
Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable
transfer function
W (z) = C(zI ‚àí A)‚àí1 B + D,

(2.8)

of McMillan degree n. We also assume that W (z) is minimum-phase so that both
zeros and poles are located in the open unit disc. Then, we need to be able to measure
how the estimated model, with transfer function WÃÇ (z), converges to the true one as
N ‚Üí ‚àû. In this paper we have chosen to use distance between W (z) and WÃÇ (z) in
‚àû
norm as a measure of proximity between the true and estimated model. From an
engineering point of view this could be called worst case identiÔ¨Åcation. The modern
literature in robust control makes extensive use of the worst case philosophy; see for
example [20, 52]. There are also other reasons for using the ‚àû , as discussed in [35].
Returning, then, to the identiÔ¨Åcation approach outlined above, the estimation error
can be decomposed into three parts, one corresponding to each of the steps (i), (ii)
and (iii). Hence we have the error bound

L

L

W ‚àí WÃÇ ‚àû ‚â§ W ‚àí WŒΩ ‚àû + WŒΩ ‚àí WÃÇŒΩ ‚àû + WÃÇŒΩ ‚àí WÃÇ ‚àû ,

(2.9)

where WŒΩ is the AR model corresponding to the true covariances (2.3) and WÃÇŒΩ is
the one determined from the estimated covariances (2.6). To prove convergence to
zero of the estimation error (2.9), we shall need to assume that W is minimum-phase,
and hence WÃÇ should have the same property, which moreover is desirable in many
applications. Our procedure insures this.
Estimating the Ô¨Årst term in (2.9) is a problem in stochastic partial realization
theory and function theory and will be dealt with in the next section. The third term
concerns model reduction which will be studied, in the particular setting required
here, in Sections 4 and 5. In Section 5, Ô¨Ånally, we consider the second term together
with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence
Step (ii) in the identiÔ¨Åcation procedure outlined in Section 2 is based on rational
covariance extension. To understand this, let us consider the covariance extension
problem from a more general point of view. Given a partial covariance sequence
c 0 , c 1 , c 2 , . . . , cŒΩ ,

(3.1)

covariance extension amounts to Ô¨Ånding an inÔ¨Ånite extension cŒΩ+1 , cŒΩ+2 , cŒΩ+3 , . . . of
this sequence such that the function
V (z) := 12 c0 + c1 z ‚àí1 + c2 z ‚àí2 + . . .

is strictly positive real, i.e., it is an analytic function in the complement Dc of the
open unit disc D, which maps Dc to the open right complex half-plane. Then
Œ¶(z) := V (z) + V (z ‚àí1 )
is a spectral density for a process having c0 , c1 , . . . , cŒΩ as its Ô¨Årst ŒΩ covariances and
which is coercive in the sense that
Œ¶(eiŒ∏ ) > 0 for all Œ∏.
Spectral factorization is then to Ô¨Ånd a stable transfer function W (z) such that
|W (eiŒ∏ )|2 = Œ¶(eiŒ∏ ).
In particular, we are interested in Ô¨Ånding covariance extensions for which V (z), and
hence W (z), have at most degree ŒΩ.
For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is
classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one
correspondence between inÔ¨Ånite covariance sequences
c0 , c1 , c2 , c3 , . . .

(3.2)

and a sequence of Schur parameters, or reÔ¨Çection coeÔ¨Écients,
Œ≥0 , Œ≥1 , Œ≥2 , Œ≥3 , . . . ,

(3.3)

with the property |Œ≥t | < 1 for all t. In fact, Ô¨Åxing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and Œ≥0 , Œ≥1 , . . . , Œ≥m‚àí1 for
each m. The Schur parameters can be determined from the covariances via the SzegoÃà
polynomials
œït (z) = z t + œït1 z t‚àí1 + ¬∑ ¬∑ ¬∑ + œïtt t = 0, 1, 2 . . . ,
computed by means of the SzegoÃà-Levinson recursion
 






 
 


z
‚àíŒ≥t œït (z)
œï0 (z)
1
œït+1 (z)
=
;
=
,
(3.4)
‚àízŒ≥t 1
1
œï‚àót+1 (z)
œï‚àót (z)
œï‚àó0 (z)
where

œï‚àót (z) := z t œït (z ‚àí1 )
is the reciprocal polynomial of œït (z), and the Schur parameters are computed via

= r1t tj=0 œït,t‚àíj cj+1
Œ≥t
(3.5)
rt+1 = rt (1 ‚àí |Œ≥t |2 ), r0 = c0 .

6

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Hence Œ≥t = ‚àíœït+1 (0), a fact that we shall use below.
In the problem to Ô¨Ånd a covariance extension for (3.1), therefore, Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1
are Ô¨Åxed and the inÔ¨Ånite continuation Œ≥ŒΩ , Œ≥ŒΩ+1 , . . . can be chosen freely. In particular,
if we take Œ≥t = 0 for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . . We obtain the maximum entropy solution
WŒΩ (z) =

zŒΩ
,
œÜŒΩ (z)

(3.6)

where œÜŒΩ (z) is the normalized SzegoÃà polynomial
1
œÜŒΩ (z) := ‚àö œïŒΩ (z).
rŒΩ

(3.7)

Thus, in this particular case, the solution to the covariance extension problem turns
out to be rational of degree at most ŒΩ as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations
are needed. In fact, it has recently been shown [5] that there is exactly one such
solution for each choice of zeros of WŒΩ (z), thus proving a long-standing conjecture
by Georgiou [18], who had established existence. Nevertheless, as we shall see next,
rationality implies that the Schur parameters tend geometrically to zero, provided
W (z) has no zeros on the unit circle.
In this section we shall demonstrate that the rational transfer function (2.8) can be
approximated arbitrarily closely in L‚àû by the transfer function WŒΩ (z) of a maximum
entropy Ô¨Ålter for suÔ¨Éciently large ŒΩ and that this ŒΩ depends on the maximum modulus
of the zeros of W (z). We shall Ô¨Årst present a heuristic argument in support of this
conclusion.
To this end, let (3.2) be the inÔ¨Ånite covariance sequence of the output process y in
(2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via
the SzegoÃà-Levinson algorithm presented above. Then we have the following special
case of Corollary 2.1 in [5].
Lemma 3.1. Let the spectral density
Œ¶(eiŒ∏ ) = |W (eiŒ∏ )|2

(3.8)

be coercive in the sense that it is positive for all Œ∏ and let (3.3) be the corresponding
inÔ¨Ånite sequence of Schur parameters. Moreover, let Œ≥ ‚àà (0, 1) be greater than the
maximum of the moduli of the zeros of W (z). Then
|Œ≥t | = O(Œ≥ t ),

(3.9)

i.e., |Œ≥t | ‚â§ M Œ≥ t for some M ‚àà R and for suÔ¨Éciently large t.
Remark 3.2. Since (3.9) holds for all Œ≥ greater than the the maximum of the moduli
of the zeros of W (z), we have in fact that |Œ≥t | = o(Œ≥ t ), i.e., limt‚Üí‚àû |Œ≥t |Œ≥ ‚àít = 0.
For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis
of certain fast algorithms for Kalman Ô¨Åltering [6].

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the
spectral density
z(z ‚àí 1)2
Œ¶(z) = ‚àí 2
(z + z + 2)(2z 2 + z + 1)
is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters
are seen to be ‚àí1/2, ‚àí2/3, ‚àí2/5, ‚àí2/7, ‚àí2/9, ‚àí2/11, . . . , which tend to zero but not
geometrically. On the other hand, there are coercive, analytic but nonrational models
which also exhibit geometric convergence rate. A classical example [23] is obtained
2
when ck = Œ∏k for some Œ∏ ‚àà (‚àí1, 1). The Schur parameters in this case form an exact
geometric sequence, Œ≥k = (‚àíŒ∏)k+1 , k ‚â• 0.
Lemma 3.1 implies that, for a suÔ¨Éciently large ŒΩ which depends on Œ≥, the Schur
parameters Œ≥t are close to zero for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . . But, the Schur parameters
of WŒΩ are exactly zero for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . , and hence geometric convergence
would insure that WŒΩ is a good approximation of W (z) for suÔ¨Éciently large ŒΩ. We
shall prove that this is indeed the case.
Theorem 3.4. Suppose W (z) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let Œ≥ ‚àà (0, 1) be greater than the maximum of the moduli of the
zeros of W (z). Then
lim WŒΩ ‚àí W ‚àû = 0,

ŒΩ‚Üí‚àû

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that
WŒΩ ‚àí W ‚àû ‚â§ M Œ≥ ŒΩ .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to Ô¨Årst showing
that
lim WŒΩ‚àí1 ‚àí W ‚àí1 ‚àû = 0.

ŒΩ‚Üí‚àû

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we
give an alternative proof of this fact based on SzegoÃà theory, and also show that the
convergence is geometric. In fact, we can choose Œ≥ arbitrarily close to the maximum
modulus of the zeros of W .
However, as we shall see next, we can actually prove more. To this end, let us Ô¨Årst
observe that, since WŒΩ‚àí1 and W ‚àí1 have their poles in the open unit disc D and thus
are bounded and analytic in the complement Dc of D, they belong to the Hardy space
‚àû
of functions which are analytic and bounded in {z ‚àà C | |z| > 1}. Hence the
H‚àí
‚àû
, and
convergence (3.12) is in H‚àí
z ‚àíŒΩ œÜŒΩ (z) ‚Üí W ‚àí1 (z)

(3.13)

uniformly in each compact subset of Dc . Now, W ‚àí1 is analytic in {z ‚àà C | |z| ‚â• Œ≥},
a region that is strictly larger than Dc . This in itself of course does not insure that
the convergence (3.13) extends to this larger region. In fact, even if z ‚àíŒΩ œÜŒΩ (z) did
converge in {z ‚àà C | Œ≥ ‚â§ |z| ‚â§ 1}, it could fail to converge to W ‚àí1 (z) there. The fact
that it really does converge uniformly to this limit is another consequence of Lemma
3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and
hence an estimate of the convergence rate Œ≥, is given in [35].
Theorem 3.5. Suppose W (z) is a minimum-phase rational function having all its
poles in the open unit disc D and all its zeros in

DœÅ := {z ‚àà C | |z| ‚â§ œÅ} ‚äÇ D

where 0 < œÅ < 1,

and let {œÜŒΩ (z)}‚àû
0 be the normalized SzegoÃà polynomial (3.7) determined from the covariances in the spectral density
|W (e )| = c0 + 2
iŒ∏

2

‚àû
	

ck cos kŒ∏.

k=1

Then, as ŒΩ ‚Üí ‚àû, z ‚àíŒΩ œÜŒΩ (z) ‚Üí W ‚àí1 (z) uniformly in every compact subset of
{z ‚àà C | |z| > œÅ}, the complement of DœÅ .

DcœÅ :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of œÜŒΩ (z) and hence of the poles of the high-order AR
model with transfer function WŒΩ (z). It is known that, if the Toeplitz matrix TŒΩ+1
is positive deÔ¨Ånite, all roots of œÜŒΩ (z) are located in the open unit disc D, but little
has been reported in the literature on their behavior as ŒΩ ‚Üí ‚àû. This behavior is
illustrated in Figure 3.1.
Original system

Original system

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0

0.5

1

‚àí1
‚àí1

Original and AR(24)
1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí0.5

0

0.5

0

0.5

1

Original and AR(24)

1

‚àí1
‚àí1

‚àí0.5

1

‚àí1
‚àí1

‚àí0.5

0

0.5

1

Figure 3.1: Distribution of zeros of œÜŒΩ (z).

The top two diagrams show the zero-pole positions, within the boundaries of the
unit circle, of two minimum phase spectral factors W , both of degree Ô¨Åve. Also
indicated is a circle of radius equal to the maximum modulus of the zeros of these
spectral factors. The little circles ‚Äú‚ó¶‚Äù represent zeros and the ‚Äú+‚Äù sign represent
poles. The lower two Ô¨Ågures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the
exact covariance sequence. The poles of the latter models are indicated with ‚Äú√ó‚Äù.
The left part of Figure 3.1 illustrates what may happen if all the poles of W (z) are
located in {z ‚àà C | |z| < œÅ}, where œÅ is chosen to be the maximum of the moduli of
the zeros of W (z). The roots of œÜŒΩ (z) tend to cluster inside a circle of radius œÅ as
ŒΩ ‚Üí ‚àû. This phenomenon is in a sense predictable, since the constant term of the
SzegoÃà polynomials is œïn+1 (0) = ‚àíŒ≥n , which equals the product of the roots and, by
Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to œÅ.
This does not preclude that other types of crowns may occur, because subsequences
of {Œ≥n } could decay faster than the overall rate Œ≥, as follows from [5]. Very general
statements about the distribution of zeros of orthogonal polynomials, derived with
the help of potential-theoretic methods, can be found in [37, 27].
To the right in Figure 3.1 we see what happens in the case that W has poles with
moduli larger than œÅ. Then, for ŒΩ suÔ¨Éciently large, the normalized SzegoÃà polynomial
œÜŒΩ (z) has roots in {z ‚àà C | œÅ ‚â§ |z| < 1}, but exactly as many as the poles of W in
this region and approximately at the same place as these. This is of course due to
the uniform convergence of z ‚àíŒΩ œÜŒΩ (z) to W ‚àí1 (z) in every compact subset of DcœÅ . The
other roots of œÜŒΩ (z) behave exactly as in the previous case and tend to accumulate in
a crown inside and very close to the circle {z ‚àà C | |z| = œÅ}.
‚àû
approximation WŒΩ of W which can be made
We have thus constructed an H‚àí
arbitrarily good by choosing ŒΩ suÔ¨Éciently large. However, WŒΩ will have much larger
degree and, except for the poles outside the circle {z ‚àà C | |z| = œÅ}, a completely
diÔ¨Äerent zero-pole pattern. We shall rectify this situation by model reduction. In
fact, for the moment considering the perfect modeling problem to identify the rational
transfer function (2.8) given an exact partial covariance sequence (3.1), the last step
in our procedure consists in approximating WŒΩ by a rational function Wred of smaller
degree, ideally of the same degree as W .
The simplest model reduction procedure is deterministically balanced truncation
(DBT), Ô¨Årst introduced by Moore [38]. Though easy to implement, it may fail to
yield a minimum-phase approximation, a requirement which is important in certain
contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced
truncation (SBT), Ô¨Årst introduced by Desai and Pal [10], which is based on a diÔ¨Äerent
balancing strategy to be explained in detail in Section 4.
Original system

Reduction by DBT

Reduction by SBT

1

1

1

0.5

0.5

0.5

0

0

0

‚àí0.5

‚àí0.5

‚àí0.5

‚àí1
‚àí1

0

1

‚àí1
‚àí1

0

1

‚àí1
‚àí1

0

1

Figure 3.2: Zero-pole pattern of W (z) and Wred (z) for diÔ¨Äerent model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This Ô¨Åfth-order

10

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

model has Ô¨Årst been approximated by WŒΩ of degree ŒΩ = 24, producing the pole-zero
pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens
when the model is reduced back to order Ô¨Åve by either deterministically balanced
truncation or stochastically balanced truncation. The zeros are denoted by ‚Äú‚ó¶‚Äù and
the poles by ‚Äú+‚Äù. Both reduction procedures give good approximations when applied
to exact covariance data. However, as we shall see in Section 5, the advantages of SBT
becomes apparent when applied to statistical data. Also, as explained in Remark 4.5,
there are theoretical reasons to prefer stochastic model reduction.
4. Model reduction
In the present setting, model reduction amounts to replacing a stochastic system
(2.1) of dimension ŒΩ by one of some dimension r < ŒΩ in such a way that most of
its statistical features are retained. In particular, we want to remove the part of the
system which corresponds to the weakest correlation between past and future. This
idea can be formalized in the following way.
Basic concepts. In the Hilbert space generated by the random variables {y(t) |
‚àí‚àû < t < ‚àû} in the inner product u, v = E{uv}, let H ‚àí be the subspace generated
by the past, i.e., {y(t) | t < 0}, and H + that generated by the future {y(t) | t ‚â• 0}.
Consider the Hankel operator H : H + ‚Üí H ‚àí and its adjoint H‚àó : H ‚àí ‚Üí H + deÔ¨Åned
as

H = EH

‚àí

|H +

and

H‚àó = E H

+

|H ‚àí ,

(4.1)

‚àí

where E H denotes orthogonal projection onto the past space H ‚àí . More precisely,
H sends Œæ ‚àà H + to E H ‚àí Œæ ‚àà H ‚àí and H‚àó sends Œ∑ ‚àà H ‚àí to E H + Œ∑ ‚àà H +. Since the
process y is the output of a minimal stochastic system of dimension ŒΩ, rank H = ŒΩ by
Kronecker‚Äôs Theorem [56], and hence H has exactly ŒΩ singular values, œÉ1 , œÉ2 , . . . , œÉŒΩ ,
which are positive, as usually listed so that œÉ1 ‚â• œÉ2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉŒΩ . These singular
values are the canonical correlation coeÔ¨Écients and hence the cosines of the angles
between the principal directions of the past space H ‚àí and the future space H + . They
are therefore less than one, and the part of the stochastic system corresponding to
singular values which are close to zero have a weak coupling between past and future,
i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic
model reduction is to truncate the system so that this part is removed.
To each singular value œÉk there is an associated Schmidt pair (Œæk , Œ∑k ) with Œæk ‚àà H +
and Œ∑k ‚àà H ‚àí such that

HŒæk = œÉk Œ∑k ,

H‚àóŒ∑k = œÉk Œæk ,

and such that the sequences Œæ1 , Œæ2 , Œæ3 , . . . and Œ∑1 , Œ∑2 , Œ∑3 , . . . of singular vectors are
orthonormal. The singular vectors corresponding to nonzero singular values span the
predictor spaces
X‚àí := span{Œ∑1 , Œ∑2 , . . . , Œ∑ŒΩ },

X+ := span{Œæ1 , Œæ2 , . . . , ŒæŒΩ }.

Clearly, X‚àí ‚äÇ H ‚àí and X+ ‚äÇ H + .
The process y has one representation (2.1) for each minimal spectral factor W ,
having W as its transfer function. Such representations are called minimal stochastic
realizations and the corresponding subspaces X := {a x(0) | a ‚àà RŒΩ } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X‚àí is the splitting subspace of the stochastic
realization

x‚àí (t + 1) = Ax‚àí (t) + B‚àí w‚àí (t)
(4.2)
y(t)
= Cx‚àí (t) + D‚àí w‚àí (t)
with the transfer function W‚àí (z), the minimum-phase spectral factor; and X+ is the
splitting subspace of

x+ (t + 1) = Ax+ (t) + B+ w+ (t)
(4.3)
y(t)
= Cx+ (t) + D+ w+ (t)
with transfer function W+ (z), the maximum-phase spectral factor, having all its zeros
in Dc . Note that A and C are the same in both realizations (uniform choice of bases).
Each realization has a counterpart which evolves backwards in time and has the
same splitting subspace. For example, the backward realization of X+ ,

xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ wÃÑ+ (t)
,
(4.4)
y(t)
= CÃÑ xÃÑ+ (t) + DÃÑ+ wÃÑ+ (t)
has transfer function WÃÑ+ (z), the coanalytic minimum-phase spectral factor, having all
its poles and zeros in Dc . In the present case with scalar y, we have WÃÑ+ (z) = W‚àí (z ‚àí1 ).
Now, in order to identify the part of the system which has the weakest coupling
between past and future, and hence will be removed in the model reduction, we need
to balance the system in the sense of Desai and Pal, as we shall explain next. To this
end, we make a coordinate transformation
(A, C, CÃÑ) ‚Üí (SAS ‚àí1 , CS ‚àí1 , CÃÑS  ),

(4.5)

in the minimal realization of
1
(4.6)
V (z) = C(zI ‚àí A)‚àí1 CÃÑ  + c0 ,
2
the strictly positive real part of the spectral density of y, so that the state covariances
P‚àí := E{x‚àí (t)x‚àí (t) } and PÃÑ+ = E{xÃÑ+ (t)xÃÑ+ (t) } coincide with the diagonal ŒΩ √ó ŒΩ
matrix Œ£ of nonzero canonical correlation coeÔ¨Écients, i.e.,
P‚àí = PÃÑ+ = Œ£ := diag(œÉ1 , œÉ2 , . . . , œÉŒΩ ).

(4.7)

1

This is done by choosing S so that Sx‚àí (0) = Œ£ 2 Œ∑, where Œ∑ = (Œ∑1 , Œ∑2 , . . . , Œ∑ŒΩ ) , and
1
(S  )‚àí1 xÃÑ+ (0) = Œ£ 2 Œæ, where Œæ := (Œæ1 , Œæ2 , . . . , ŒæŒΩ ) .
To compute the canonical correlation coeÔ¨Écients, we Ô¨Årst observe that the eigenvalues of the product P‚àí PÃÑ+ are precisely the squares of the canonical correlation
coeÔ¨Écients, i.e.,
Œª(P‚àí PÃÑ+ ) = Œª(P‚àí P+‚àí1 ) = {œÉ12 , œÉ22 , . . . , œÉŒΩ2 },

(4.8)

where we have used the fact that the state covariance of (4.3) is P+ = PÃÑ+‚àí1 . Therefore
the canonical correlation coeÔ¨Écients can then be determined via (4.8) by solving the
Lyapunov equations

P‚àí = AP‚àí A + B‚àí B‚àí


and P+ = AP+ A + B+ B+
.

(4.9)

12

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

The point is now to identify the canonical correlation coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉr corresponding to the part of the system one wants to keep. The part corresponding to
œÉr+1 , œÉr+2 , . . . , œÉŒΩ will be disposed of. This amounts to partitioning Œ£ as



Œ£1
,
(4.10)
Œ£=
Œ£2
where Œ£1 is r √ó r.
In order to reduce model (2.1) we make the coordinate transformation (A, B, C) ‚Üí
(SAS ‚àí1 , SB, CS ‚àí1 ), with the same balancing transformation S. Then, partition the
new triplet (A, B, C) conformally with (4.10) as


 




B1
A11 A12
(4.11)
, B=
, C = C1 C2 ,
A=
A21 A22
B2
and perform a principal subsystem truncation to obtain the transfer function of a
reduced-order system
Wred (z) = C1 (zI ‚àí A11 )‚àí1 B1 + D

(4.12)

of degree r. If Œ£2 is close to zero, while Œ£1 is not, the rank of H is close to r, and the
discarded part of the system gives a negligible contribution to y.
Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy Ô¨Ålter
‚àö ŒΩ
rŒΩ z
(4.13)
W‚àí (z) := WŒΩ (z) =
œïŒΩ (z)
of order ŒΩ, which, for the moment we denote W‚àí (z) to emphasize its character as the
minimum-phase spectral factor of the spectral density
rŒΩ
.
œïŒΩ (z)œïŒΩ (z ‚àí1 )
Remark 4.1. Without loss of generality we assume that œïŒΩ (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller ŒΩ for which this condition holds.
In fact, œïŒΩ (0) = Œ≥ŒΩ‚àí1 , and if Œ≥ŒΩ‚àíp = Œ≥ŒΩ‚àíp+1 = ¬∑ ¬∑ ¬∑ = Œ≥ŒΩ‚àí1 = 0 and Œ≥ŒΩ‚àíp‚àí1 = 0 for some
p = 1, 2, . . . , ŒΩ, then œïŒΩ (z) = z ŒΩ‚àíp œïŒΩ‚àíp (z) by (3.4), and hence (3.6) can be replaced
by WŒΩ (z) = WŒΩ‚àíp (z), and for WŒΩ‚àíp (z) the required condition holds.
The maximum-phase spectral factor W+ (z) has all its zeros at inÔ¨Ånity, and hence
‚àö
rŒΩ

‚àí1
W+ (z) = h (zI ‚àí F ) b =
,
(4.14)
œïŒΩ (z)
where (F, b, g) is the (observable) canonical form
Ô£π
Ô£Æ
Ô£π
Ô£Æ
0
0
1
¬∑¬∑¬∑
0
..
.. Ô£∫
. Ô£∫
...
Ô£Ø ...
.
. Ô£∫, b = Ô£Ø
Ô£Ø .. Ô£∫ ,
F =Ô£Ø
Ô£∞ 0 Ô£ª
Ô£∞ 0
0
¬∑¬∑¬∑
1 Ô£ª
‚àö
‚àíœïŒΩŒΩ ‚àíœïŒΩ,ŒΩ‚àí1 ¬∑ ¬∑ ¬∑ ‚àíœïŒΩ1
rŒΩ

Ô£Æ Ô£π
1
Ô£Ø0Ô£∫
Ô£∫
h=Ô£Ø
Ô£∞ ... Ô£ª ,
0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

œïŒΩ1 , œïŒΩ2 , . . . , œïŒΩŒΩ being the coeÔ¨Écients of the SzegoÃà polynomial œïŒΩ (z). In this basis,
it follows from (4.9) that

  œÄ

1
iŒ∏
‚àí1  ‚àíiŒ∏
 ‚àí1
(e I ‚àí A) bb (e I ‚àí A ) dŒ∏
[P+ ]jk =
2œÄ ‚àíœÄ
jk
 œÄ
1
r
ŒΩ
=
e‚àí(j‚àík)iŒ∏
dŒ∏ = cj‚àík ,
2œÄ ‚àíœÄ
œïŒΩ (eiŒ∏ )œïŒΩ (e‚àíiŒ∏ )
and hence P+ = TŒΩ . It is well-known and easy to prove that Œ¶ŒΩ TŒΩ Œ¶ŒΩ = RŒΩ , where

Œ¶ŒΩ+1

Ô£Æ
œïŒΩŒΩ
Ô£Ø ..
Ô£Ø .
Ô£Ø
= Ô£ØœïŒΩ2
Ô£Ø
Ô£∞œïŒΩ1
1

œïŒΩ‚àí1,ŒΩ‚àí1
..
.

œïŒΩ‚àí2,ŒΩ‚àí2
..
.

œïŒΩ‚àí1,1
1

1

¬∑¬∑¬∑

Ô£π
1
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ
rŒΩ‚àí1
Ô£Ø
Ô£Ø
and RŒΩ = Ô£Ø
Ô£∞

Ô£π
Ô£∫
Ô£∫
Ô£∫ , (4.16)
Ô£ª

rŒΩ‚àí2
..

.
r0

and consequently
PÃÑ+ = TŒΩ‚àí1 = Œ¶ŒΩ RŒΩ‚àí1 Œ¶ŒΩ .
It remains to determine P‚àí . From (4.13) is easy to see that
‚àö
W‚àí (z) = ‚àíœïŒΩ (zI ‚àí F )‚àí1 b + rŒΩ ,
where



œïŒΩ := œïŒΩŒΩ œïŒΩ,ŒΩ‚àí1 ¬∑ ¬∑ ¬∑ œïŒΩ1 ,

(4.17)

(4.18)

(4.19)

but, in order to determine P‚àí , this realization needs to be transformed so that the A
and C matrices are the same as in (4.14) (uniform choice of bases). More precisely,
we need to perform a transformation
(F, b, ‚àíœïŒΩ ) ‚Üí (QF Q‚àí1 , Qb, ‚àíœïŒΩ Q‚àí1 ) =: (F, Qb, h ).
Then P‚àí is the solution of the Lyapunov equation P‚àí = F P‚àí F  + Qbb Q , and therefore, since TŒΩ = F TŒΩ F  + bb and QF = F Q and consequently
QTŒΩ Q = F QTŒΩ Q F  + Qbb Q ,
we have
P‚àí = QTŒΩ Q .
To determine Q, notice that ‚àíœïŒΩ = h Q and QF
Ô£π Ô£Æ
Ô£Æ
h
‚àíœïŒΩ

Ô£Ø ‚àíœïŒΩ F Ô£∫ Ô£Ø h F
Ô£∫=Ô£Ø .
Ô£Ø
..
Ô£ª Ô£∞ ..
Ô£∞
.
‚àíœïŒΩ F ŒΩ‚àí1

(4.20)
= F Q to form
Ô£π
Ô£∫
Ô£∫ Q = Q.
Ô£ª

(4.21)

h F ŒΩ‚àí1

Next, deÔ¨Åne the symmetric matrix
M := RŒΩ‚àí1/2 Œ¶ŒΩ QTŒΩ Q Œ¶ŒΩ RŒΩ‚àí1/2 .

(4.22)

14

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

In view of (4.20) and (4.17), det(zI ‚àí M ) = det(zI ‚àí P‚àí PÃÑ+ ), and hence, by (4.8), M
has the eigenvalues œÉ12 , œÉ22 , . . . , œÉŒΩ2 , and the singular-value decomposition
M = U Œ£2 U  ,

(4.23)

where U  U = U U  = I. It is then well-known and simple to check that
S := Œ£‚àí1/2 U  RŒΩ‚àí1/2 Œ¶ŒΩ

(4.24)

is the required balancing transformation (4.5) such that SP‚àí S  = (S  )‚àí1 PÃÑ+ S ‚àí1 = Œ£.
Proposition 4.2. Given the partial covariance sequence
ck = E{y(t + k)y(t)},

k = 0, 1, . . . , ŒΩ,

let œï1 (z), œï2 (z), . . . , œïŒΩ (z) and r0 , r1 , . . . , rŒΩ be the corresponding SzegoÃà polynomials
and error variances. Supposing that Œ≥ŒΩ‚àí1 = ‚àíœïŒΩ (0) = 0, let (F, b, h) be given by
(4.15), RŒΩ and Œ¶ŒΩ by (4.16) and Q by (4.21). Moreover, let U and Œ£ be deÔ¨Åned by
the singular value decomposition (4.23) of (4.22). Then, the canonical correlation
coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉŒΩ are the diagonal elements of Œ£, as described in (4.7), and
the stochastically balanced realization of WŒΩ is given by
‚àö
(4.25)
(A, B, C, D) = (SF S ‚àí1 , SQb, h S ‚àí1 , rŒΩ ),
where S is deÔ¨Åned by (4.24).
Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to
yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase
property. In fact, we have the following result, the proof of which is given in Section A.
Theorem 4.3. Let Wred be the SBT approximation of degree r of WŒΩ , and set

ŒΩ
ŒΩ‚àí1
	
‚àö  1 + |Œ≥k |
œÉk
9 := 2
and Œ∫ := c0
,
(4.26)
1
‚àí
œÉ
1
‚àí
|Œ≥
k
k|
k=r+1
k=0
where Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1 are the Schur parameters of c0 , c1 , c2 , . . . , cŒΩ . Then
c0 (1 ‚àí 9)Œ∫‚àí1 ‚â§ |Wred (eiŒ∏ )| ‚â§ (1 + 9)Œ∫ for all Œ∏,

(4.27)

and, if 9 < 1, Wred is minimum phase. Finally, the approximation error has the bound
WŒΩ ‚àí Wred ‚àû ‚â§ 9Œ∫.

(4.28)

A properly executed SBT procedure should imply that the canonical correlation
coeÔ¨Écients œÉr+1 , . . . , œÉŒΩ , and hence 9, are close to zero, insuring the minimum-phase
condition.
Remark 4.4. Stochastic model reduction can also be carried out by instead performing principal subsystem truncation on (A, C, CÃÑ) in VŒΩ (z) = C(zI ‚àí A)‚àí1 CÃÑ + 12 c0 ,
where A and C are given by (4.25) and CÃÑ  = S(c1 , c2 , . . . , cn ). It was shown in
[32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally,
the spectral density Œ¶red (z) := Vred (z) + Vred (z ‚àí1 ) is factorized to yield a minimumphase spectral factor WÃÉ . This is in a sense a more natural procedure, but we do
not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small Œ£2 it yields almost the same result. In fact, it is shown in [53], that
|WÃÉ (eiŒ∏ )|2 = |Wred (eiŒ∏ )|2 + H(eiŒ∏ )Œ£2 H(e‚àíiŒ∏ ), where H(z) = C1 (zI ‚àí A11 )‚àí1 A12 .
Remark 4.5. There are good reasons to prefer stochastic over deterministic model
reduction, as seen from the following heuristics. In fact, it can be seen that
VŒΩ (z) =

c0 œàŒΩ (z)
,
2 œïŒΩ (z)

(4.29)

where œàŒΩ (z) is the SzegoÃà polynomial of the second kind (obtained by exchanging ‚àíŒ≥t
for Œ≥t in the recursion (3.4)). Now, the matrix representation of the Hankel operator
H in the innovation bases of the past and the future, provided by w‚àí and wÃÑ+ respecis the inÔ¨Ånite Hankel matrix of the sequence
tively, is given by L‚àí1 (L‚àí1 ) , where
c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T‚àû ;
see, e.g., [32, p. 714]. It is easy to see that œàŒΩ (z) has the same asymptotic behavior as
œïŒΩ (z), i.e., the roots tend to cluster uniformly inside the circle z = œÅ as ŒΩ ‚Üí ‚àû, and
hence these roots are close to canceling in (4.29). Consequently, the corresponding
Hankel matrix is close to having low rank. This massive ‚Äúalmost cancellation‚Äù does
not occur in WŒΩ (z), and hence the corresponding inÔ¨Ånite Hankel matrix, constructed
from the Laurent coeÔ¨Écients of WŒΩ (z), may have a less distinct separation between
Œ£1 and Œ£2 . On the other hand, since the Schur parameters tend geometrically to
zero, the lower part of L tends to the identity, and hence the asymptotic behavior of
the canonical correlation coeÔ¨Écients is very much like that of the singular values of
. Therefore we may expect SBT to have better statistical behavior than DBT. In
Section 6 we shall see that this is the case.

H

H

H

H

5. IdentiÔ¨Åcation from statistical data
We now return to our original problem of time series identiÔ¨Åcation: Given a data
string (2.2) of observations of the output process y of some n-dimensional linear
stochastic system (2.1) with minimum-phase transfer function W (z), given by (2.8),
Ô¨Ånd an estimate (AÃÇ, BÃÇ, CÃÇ, DÃÇ) of the matrices (A, B, C, D).
The identiÔ¨Åcation method proceeds as follows. Given the covariance estimates (2.5),
we compute the corresponding maximum entropy Ô¨Ålter (2.7), a balanced realization
(4.25), and the canonical correlation coeÔ¨Écients
œÉÃÇ1 , œÉÃÇ2 , œÉÃÇ3 , . . . , œÉÃÇŒΩ ,

(5.1)

determined as in Proposition 4.2 from the covariance estimates cÃÇ0 , cÃÇ1 , . . . , cÃÇŒΩ .
Based on (5.1), choose an integer nÃÇ such that œÉÃÇnÃÇ+1 , œÉÃÇnÃÇ+2 , . . . , œÉÃÇŒΩ are close to zero or
at least distinctively smaller than œÉÃÇ1 , œÉÃÇ2 , . . . , œÉÃÇnÃÇ . Then, the balanced realization (4.25)
is truncated accordingly as in (4.11) to yield a nÃÇ-dimensional triplet (A11 , B1 , C1 ) and
a transfer function
WÃÇ (z) = C1 (zI ‚àí A11 )‚àí1 B1 + D.

(5.2)

Then, (A11 , B1 , C1 , D) is the required estimate (AÃÇ, BÃÇ, CÃÇ, DÃÇ).
As pointed out in Section 2, we have a bound
W ‚àí WÃÇ ‚àû ‚â§ W ‚àí WŒΩ ‚àû + WŒΩ ‚àí WÃÇŒΩ ‚àû + WÃÇŒΩ ‚àí WÃÇ ‚àû ,

(5.3)

16

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the Ô¨Årst term W ‚àí WŒΩ ‚àû , which
does not depend on the statistical data (2.2) but only on the underlying system (2.1),
tends to zero geometrically with a rate Œ≥ ‚àà (0, 1) as ŒΩ ‚Üí ‚àû. The other two terms
depend on the data (2.2), and here N must grow at a faster rate than ŒΩ. In fact, we
shall assume that
ŒΩ = ŒΩ(N ) = O(log N ),

(5.4)

which in particular requires that limN ‚Üí‚àû NŒΩ = 0. We also need to assume that the
white noise process in (2.1) satisÔ¨Åes a mild technical condition, namely
E{w(t)4 } < ‚àû.

(5.5)

This condition is, of course, satisÔ¨Åed if w is Gaussian.
Next, we present our main convergence theorem.
Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then,
to each length N of the data string (2.2), there is a ŒΩ(N ), tending to inÔ¨Ånity with N
at the rate (5.4), such that any sequence of estimated transfer functions WÃÇ of Ô¨Åxed
degree nÃÇ ‚â• n, determined, for each N and corresponding ŒΩ = ŒΩ(N ), by the procedure
described above, satisÔ¨Åes
W ‚àí WÃÇ ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû. For suÔ¨Éciently large ŒΩ(N ), the transfer function WÃÇ has
minimum phase.
We have already proven that the Ô¨Årst term in (5.3) tends to zero, so Theorem 5.1
follows from the next two theorems, each corresponding to one of the remaining terms
in (5.3). As for the second term, we have the following result, the proof of which is
deferred to Appendix B.
Theorem 5.2. Suppose the system (2.1) satisÔ¨Åes the conditions of Theorem 5.1. Let
WŒΩ be the maximum-entropy Ô¨Ålter (3.6) determined from the partial covariance sequence (3.1) of y and let WÃÇŒΩ be the corresponding function determined from the ergodic
estimates (2.5). Then, if ŒΩ(N ) is deÔ¨Åned as in Theorem 5.1,
WŒΩ(N ) ‚àí WÃÇŒΩ(N ) ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû.
There are several results of this type in the literature [2, 36, 7, 33]. In particular,
3
Berk [2] proved that, provided ŒΩN ‚Üí 0 as N ‚Üí ‚àû and Œ¶ is coercive (i.e. positive
 iŒ∏ ) ‚Üí Œ¶(eiŒ∏ ) in probability.
on the unit circle), the estimated AR spectral density Œ¶(e
Under the same hypotheses, Caines and Baykal-GuÃàrsoy [7] showed that if N ‚â• ŒΩ 5+Œ∑
for some Œ∑ > 0, then WÃÇŒΩ‚àí1 ‚àí W ‚àí1 ‚àû ‚Üí 0 almost surely as ŒΩ ‚Üí ‚àû. However, in both
cases, ergodic estimates are used which are not quite the same as (2.5).
Finally, we consider the last term in (5.3). The proof of the following theorem is
given in Appendix B.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function ŒΩ(N ) are deÔ¨Åned as in
Theorem 5.1. Moreover, for each N , let WÃÇŒΩ(N ) be deÔ¨Åned as in Theorem 5.2 and WÃÇ
as in Theorem 5.1. Then, for suÔ¨Éciently large ŒΩ(N ), WÃÇ has minimum phase, and
WÃÇŒΩ(N ) ‚àí WÃÇ ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû.
6. Simulations
Performing model reduction on WÃÇŒΩ , rather than on the maximum-entropy Ô¨Ålter of
exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier
and more accurate order determination, as the heuristics of Remark 4.5 suggest.
There are also alternative order determination statistical tests based on the canonical
correlation coeÔ¨Écients [17, 26, 46]. But, even more importantly, there is less bias,
and the error variances are closer to the CrameÃÅr-Rao bound.
Since we are approximating rational models with AR models the method will be
biased for Ô¨Ånite amount of data, unless the model generating the data really is an
AR model. The consistency result given in Theorem 5.1 implies that the method is
asymptotically unbiased and therefore we consider the CrameÃÅr-Rao bound for unbiased methods; see [44, pp. 137‚Äì138]. The CrameÃÅr-Rao bound for biased estimation
requires knowledge about the bias as a function of the parameter to be estimated.
As already mentioned, the method will be unbiased and even statistically eÔ¨Écient for
Gaussian AR processes if the model reduction step is omitted. Despite the fact that
an algorithm based on covariance estimates (2.6) is not asymptotically eÔ¨Écient for
general ARMA models [44, p. 144], our method can be used to provide a starting
guess for other algorithms, for example the maximum likelihood method.
6
SBT dashed line, DBT dotted line.
5

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identiÔ¨Åcation estimates.

To illustrate our procedure, let us consider data generated by passing white noise
through a ‚Äútrue system‚Äù with transfer function
W (z) =

z 5 ‚àí 0.0550z 4 ‚àí 0.1497z 3 ‚àí 0.2159z 2 + 0.1717z ‚àí 0.0495
.
z 5 ‚àí 0.7031z 4 + 0.3029z 3 + 0.1103z 2 ‚àí 0.1461z + 0.2845

18

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy
model WÃÇŒΩ of degree ŒΩ = 24 determined from estimated covariances. Based on 100
test runs, the empirical means and standard deviations are determined. Figure 6.1
illustrates the statistical bias as a function of the length N of the data string when
using stochastic (dashed curve) and deterministic (dotted curve) model reduction
respectively.
For the same test runs, Figure 6.2 illustrates the corresponding standard deviations
together with the CrameÃÅr-Rao bound (solid curve). More precisely, the Ô¨Ågures depict
the sums of the moduli of the biases and standard deviations respectively for the
coeÔ¨Écients of the numerator and denominator polynomials of WÃÇ (z).
5
CRB solid line, SBT dashed line, DBT dotted line.
4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the CrameÃÅr-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z)
with poles and zeros closer to the unit circle is considered. The poles and zeros of
WÃÇ (z) are determined for 100 runs and a data length N = 500. As before, ŒΩ = 24.
Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with
the poles and zeros of W (z), which are denoted by ‚Äú‚ó¶‚Äù.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

1

‚àí1
‚àí1

‚àí0.5

0
poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT.
To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in
Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 24.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

1

‚àí1
‚àí1

‚àí0.5

0
poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identiÔ¨Åcation.

Figure 6.5 describes the result obtained when applying stochastic subspace identiÔ¨Åcation to the same data. More precisely, Algorithm # 2 in [43] is used. In order
to make the experiments comparable, we have chosen a Hankel matrix of dimension
13 √ó 13, which corresponds to ŒΩ = 25 in our procedure.
Note that the estimates are much less focused, and many zeros tend to cluster on the
unit circle, implying that coercivity becomes critical. This is related to the positivity
issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the
subspace identiÔ¨Åcation method performs worse than our SBT identiÔ¨Åcation method,
yielding larger biases and standard deviations, but performs better than when DBT
is used.

20

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as
illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will
disappear as ŒΩ and N are increased. In Figure 6.6 we show the same experiment for
ŒΩ = 64 and N = 2000.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and ŒΩ = 64.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 40 using Burg‚Äôs method.

In practice, there is a trade-oÔ¨Ä between the quality of the ergodic estimates, which
roughly speaking depend on |Œªmax (A)|, the ‚àû -error tolerance, which is a function of
|Œªmax (A ‚àí BD‚àí1 C)|, and the numerical accuracy of the computations. For example,
if the zeros of W (z) are far from the unit circle and ŒΩ is chosen very large, the error
may increase.
In the present example, it turns out that using Burg‚Äôs method [3] in lieu of the
ergodic estimate (2.6) yields better estimates for smaller ŒΩ and N , as illustrated in
Figure 6.7 which shows the case N = 500 and ŒΩ = 40.
A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There
we give the empirical bias and standard deviation for the coeÔ¨Écients of the numerator
and the denominator, respectively, of the estimated transfer functions together with
the CrameÃÅr-Rao bound. It is the authors experience that Burg‚Äôs method gives at
least as good results as when using the ergodic covariance estimate (2.6), unless the
intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

Parameter
True value
Bias:
CE:
Burg:
Std.dev.:
CE:
Burg:
CRB:

21

œÉw2
b1
b2
b3
b4
b5
1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491
0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895
0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734
0.2332 0.1314 0.0508 0.0611 0.0722 0.0802
0.0712 0.0411 0.0381 0.0339 0.0339 0.0356
0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and ŒΩ = 40 using covariance estimation (CE) or Burg estimation and , in
both cases, followed by SBT.

Parameter
a1
a2
a3
a4
a5
True value
-0.6281 0.3597 0.2634 -0.5322 0.7900
Bias:
CE:
0.0087 -0.0044 -0.0003 0.0066 -0.0152
Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125
Std.dev.:
CE:
0.0274 0.0304 0.0371 0.0305 0.0304
Burg: 0.0336 0.0307 0.0358 0.0324 0.0306
0.0293 0.0321 0.0342 0.0322 0.0290
CRB:
Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and ŒΩ = 40 using covariance estimation (CE) or Burg estimation and,
in both cases, followed by SBT.

7. Conclusions
We have presented a three-step procedure for identiÔ¨Åcation of time series, which is easy
to understand and implement. Just like for subspace identiÔ¨Åcation methods, robust
linear-algebra algorithms can be used and no nonconvex optimization computations
are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identiÔ¨Åcation, as our extensive simulations indicate.
In particular, its good performance has been conÔ¨Årmed by Monte Carlo simulations.
The paper only covers the scalar case, but the multivariate case is presently being
worked out.
The three steps, covariance estimation, covariance extension and model reduction
have each been studied separately before. This is an advantage which should make the
method easy to grasp. However, a comprehensive study of the entire identiÔ¨Åcation
strategy, giving appropriate bounds, has been missing and this is what we oÔ¨Äered
here.
The observation that the Schur parameters converge geometrically simpliÔ¨Åes our
application of SzegoÃà theory and allows us to give a complete account of the asymptotic
behavior of maximum entropy models of growing order. This analysis provides us with
a clear indication as to when the identiÔ¨Åcation strategy is good and when it might face
diÔ¨Éculties, based purely on the closeness of the maximum modulus zero to the unit
circle. The parsimony permeating other system identiÔ¨Åcation methods should not be
a reason for refraining from high-order modeling as an intermediate step. In fact,
such a strategy might be desirable, since we have shown that the poles of the ‚Äútrue‚Äù
system which lie outside a circle in the complex plane containing all of its zeros are

22

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the
perimeter of this circle, providing a justiÔ¨Åcation for choosing stochastically balanced
model reduction, rather than deterministically balanced truncation, in the last step.
With this reduction procedure, we have conÔ¨Årmed better statistical properties with
variances closer to the CrameÃÅr Rao bound. The procedure could also be modiÔ¨Åed by
exchanging exact covariance extension for approximate one, as outlined in [35].
Even though, in general, stochastic balancing would require the solution of a pair
of Riccati equations, this is not the case for the particular maximum entropy models
used here. In fact, the balancing procedure only requires linear algebra, and hence
an intelligent use of the Levinson algorithm may substantially reduce the number of
arithmetic operations.
Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N , ŒΩ and nÃÇ, we gave worst-case guaranteed
bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive
stochastic system, but the method returns a valid model also for generic data. In fact,
in contrast to many stochastic subspace identiÔ¨Åcation [9], all steps of the procedure
preserve the positive real property.
Appendix A. Asymptotic behavior of the maximum entropy Ô¨Ålter
Theorem 3.4 is actually a modiÔ¨Åcation to the rational setting of a theorem due to
SzegoÃà [47], and the proof is modeled after [19], which in turn includes aspects already
present in the work of Schur [45]. See also [48], [49] and [16] for more facts on
orthogonal polynomials. However, rationality and coercivity allows us to present a
simpliÔ¨Åed and self-contained proof of a version of SzegoÃà‚Äôs classical theorem, to which
we also are able to add geometric convergence. The derivation of Caines and BaykalGuÃàrsoy [7] is shorter, but we feel that our approach is more systematic and gives
additional insight into the mechanism of identiÔ¨Åcation.
To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas.
‚àíŒΩ
œÜŒΩ (z)|
Lemma A.1. Let {œÜŒΩ (z)}‚àû
0 be the normalized SzegoÃà polynomials (3.7). Then |z
c
is uniformly bounded from above and away from zero in the complement D of the open
unit disc, i.e., there are positive numbers Œ±, Œ≤ ‚àà R such that

Œ± ‚â§ |z ‚àíŒΩ œÜŒΩ (z)| ‚â§ Œ≤
for all ŒΩ and all z ‚àà Dc .
Proof. In view of the SzegoÃà-Levinson recursion (3.4),



œï‚àót (z)
œït+1 (z) = œït (z) z ‚àí Œ≥ÃÑt
,
œït (z)
and hence
z

‚àíŒΩ

œïŒΩ (z) =

ŒΩ‚àí1


k=0


œï‚àók (z)
1 ‚àí z Œ≥ÃÑk
.
œïk (z)
‚àí1

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , zŒΩ are the roots of œïŒΩ (z), it is immediately seen that
œï‚àóŒΩ (z)  1 ‚àí z zÃÑk
,
=
œïŒΩ (z) k=1 z ‚àí zk
ŒΩ

which is a Blaschke product, analytic in Dc and having modulus one on the unit circle,
and thus modulus less than or equal to one in Dc . Hence, since |z ‚àí1 | ‚â§ 1 in Dc ,
ŒΩ‚àí1


(1 ‚àí |Œ≥k |) ‚â§ |z

‚àíŒΩ

œïŒΩ (z)| ‚â§

k=0

ŒΩ‚àí1


(1 + |Œ≥k |)

(A.1)

k=0

for all z ‚àà Dc . But, these products converge to positivenumbers as ŒΩ ‚Üí ‚àû. This
follows from the absolute convergence of the inÔ¨Ånite sum ‚àû
k=0 |Œ≥k |, a fact that, in the
present context, stems from Lemma 3.1. From (3.5) we also have 0 < r‚àû ‚â§ rŒΩ ‚â§ r0 ,
and consequently the lemma follows.
Remark A.2. An equivalent statement of this lemma is that the maximum entropy
solution WŒΩ (z), deÔ¨Åned by (3.6), is uniformly bounded from above and away from
zero for all ŒΩ and z ‚àà Dc .
Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of
functions
fŒΩ (z) := z ‚àíŒΩ œÜŒΩ (z)
converges uniformly to an analytic function f‚àû in
statement of Theorem 3.5.

DcœÅ,

where

DcœÅ

is deÔ¨Åned in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the
purely algebraic relation
ŒΩ
	

œÜk (z)œÜk (w) =

k=0

œÜ‚àóŒΩ (z)œÜ‚àóŒΩ (w) ‚àí z wÃÑœÜŒΩ (z)œÜŒΩ (w)
,
1 ‚àí z wÃÑ

(A.2)

which is called the ChristoÔ¨Äel-Darboux-SzegoÃà formula. In particular, setting w = 0
and exchanging z for z ‚àí1 in (A.2), (3.5) and (3.7) yield
fŒΩ (z)
1 	
Œ≥k‚àí1
‚àí
œÜk (z ‚àí1 ) ‚àö .
‚àö =
rŒΩ
c0 k=1
rk
ŒΩ

(A.3)

Observe that œÜk (z ‚àí1 ) is analytic and bounded in DcœÅ , and hence in Dc , and therefore it
‚àû
. Moreover, by the maximum modulus principle, it attains its maximum
belongs to ‚àí
c
value in D on the unit circle where, by Lemma A.1, it is bounded by Œ≤. Hence

H

|œÜk (z ‚àí1 )| ‚â§ Œ≤

for z ‚àà Dc and for all k.

Therefore, in view of (A.3) and the fact that rk ‚â• r‚àû , we have


ŒΩ
	
 fŒΩ (z) f¬µ (z) 
 ‚àö ‚àí ‚àö  ‚â§ ‚àöŒ≤
|Œ≥k‚àí1 |,
 rŒΩ
r¬µ 
r‚àû k=¬µ+1

(A.4)

(A.5)

24

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for suÔ¨Éciently large ŒΩ and ¬µ.
‚àû
. The same holds for fŒΩ (z). In
This establishes (A.3) as a Cauchy sequence in ‚àí
c
fact, since rŒΩ ‚â§ c0 , for all z ‚àà D


‚àö  fŒΩ (z) f¬µ (z) 
|fŒΩ (z) ‚àí f¬µ (z)| ‚â§ c0  ‚àö ‚àí ‚àö 
rŒΩ
rŒΩ




 1
‚àö  fŒΩ (z) f¬µ (z) 
1 

‚â§ c0  ‚àö ‚àí ‚àö  + |f¬µ (z)|  ‚àö ‚àí ‚àö  .
(A.6)
rŒΩ
r¬µ
rŒΩ
r¬µ

H

But, by Lemma A.1, |f¬µ (z)| ‚â§ Œ≤ for all ŒΩ and z ‚àà Dc , and therefore, in view of (A.5),
we obtain



ŒΩ
 1

c0 	
1
|Œ≥k‚àí1 | + Œ≤  ‚àö ‚àí ‚àö  for all z ‚àà Dc . (A.7)
|fŒΩ (z) ‚àí f¬µ (z)| ‚â§ Œ≤
r‚àû k=¬µ+1
rŒΩ
r¬µ
Since rŒΩ ‚Üí r‚àû as ŒΩ ‚Üí ‚àû, we see that, for each 9 > 0, |fŒΩ (z) ‚àí f¬µ (z)| < 9 for
suÔ¨Éciently large ŒΩ and ¬µ. Consequently, fŒΩ tends uniformly in Dc to a function
‚àû
.
f‚àû ‚àà H‚àí
The uniform convergence and the analyticity can be extended to any compact
subset of DcœÅ . To see this, Ô¨Årst note that z ‚àà D if and only if z ‚àí1 ‚àà Dc . Therefore, by
Lemma A.1,
|œÜk (z ‚àí1 )| ‚â§ Œ≤|z|‚àík for z ‚àà D,
and consequently, since rŒΩ ‚â§ rk , (A.3) yields
ŒΩ‚àí1
	
1
|fŒΩ (z)| ‚â§ ‚àö + Œ≤|z|‚àí1
|Œ≥k ||z|‚àík .
c0
k=0

Similarly, instead of (A.5) we have


ŒΩ‚àí1
	
 fŒΩ (z) f¬µ (z) 
 ‚àö ‚àí ‚àö  ‚â§ ‚àöŒ≤ |z|‚àí1
|Œ≥k ||z|‚àík .
 rŒΩ
r¬µ 
r‚àû
k=¬µ

(A.8)

(A.9)

Now, for any compact subset K ‚àà DcœÅ , there is a Œ≥ ‚àà (œÅ, 1) and an 9 > 0 such
that |z| > Œ≥ + 9 for all z ‚àà K. Hence, by Lemma 3.1, |Œ≥k ||z|‚àík ‚â§ M Œ≥ÃÇ k where
Œ≥ÃÇ := Œ≥(Œ≥ + 9)‚àí1 < 1. Consequently, by (A.8), fŒΩ (z) is uniformly bounded in K, and
(A.9) can be made arbitrarily small for suÔ¨Éciently large ŒΩ and ¬µ. Therefore, by (A.6),
fŒΩ tends uniformly in K to the analytic function f‚àû .
Lemma A.4. Let Œ≥ be a real number such that œÅ < Œ≥ < 1. Then fŒΩ ‚àíf‚àû ‚àû = O(Œ≥ ŒΩ ).
Proof. It follows from (A.7) that


 
‚àû

‚àö 	
Œ≤
r‚àû 
c0
|Œ≥k‚àí1 | + 1 ‚àí
|fŒΩ (z) ‚àí f‚àû (z)| ‚â§ ‚àö
r‚àû
rŒΩ 
k=ŒΩ+1

for all z ‚àà Dc .
(A.10)

By Lemma 3.1, the Ô¨Årst term is O(Œ≥ ŒΩ ). It remains to show that the same holds for
the second term. To this end, Ô¨Årst note that, by (3.5),

‚àû 

r‚àû
=1‚àí
1 ‚àí Œ≥k2 .
1‚àí
rŒΩ
k=ŒΩ

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

But, by Lemma 3.1, |Œ≥k | ‚â§ M Œ≥ k for some M . Therefore, since
each x ‚àà [0, 1],

‚àû

r‚àû
‚â§ 1 ‚àí (1 ‚àí M Œ≥ k ) = O(Œ≥ t )
1‚àí
rt
k=t

‚àö

25

1 ‚àí x2 ‚â• 1 ‚àí x for

for t large enough. This concludes the proof.
Recalling the deÔ¨Ånition (3.6) of WŒΩ , we note that Lemma A.4 may be written
WŒΩ‚àí1 ‚àí f‚àû ‚àû = O(Œ≥ ŒΩ ).
‚àí1
in the same manner.
As it turns out, by coercivity, this implies that WŒΩ ‚Üí f‚àû

Lemma A.5. Let WŒΩ be the transfer function (3.6) of the maximum entropy Ô¨Ålter.
Then
‚àí1
‚àû = O(Œ≥ ŒΩ ),
WŒΩ ‚àí f‚àû
where f‚àû is the limit function of Lemma A.3.
Proof. Note that the limit function f‚àû has the same uniform bounds as fŒΩ in Lemma
A.1. In particular, |f‚àû (z)| ‚â• Œ±, |f‚àû (z)|‚àí1 ‚â§ Œ±‚àí1 , and |WŒΩ (z)| ‚â§ Œ±‚àí1 for all z ‚àà Dc .
Consequently,
‚àí1
‚àí1
WŒΩ ‚àí f‚àû
‚àû ‚â§ WŒΩ ‚àû f‚àû
‚àû WŒΩ‚àí1 ‚àí f‚àû ‚àû ‚â§ Œ±‚àí2 WŒΩ‚àí1 ‚àí f‚àû ‚àû ,

so the required result follows from Lemma A.4.
Lemma A.6. Let W be the rational minimum-phase function deÔ¨Åned above, and let
f‚àû be the limit function in Lemma A.3. Then W (z) = f‚àû (z)‚àí1 for all z ‚àà DcœÅ .
Proof. Let Œ¶ŒΩ (eiŒ∏ ) := |WŒΩ (eiŒ∏ )|2 be the spectral density of the maximum entropy
process. Then, in view of the interpolation condition,
 œÄ
 œÄ
1
1
ikŒ∏
iŒ∏
e Œ¶(e )dŒ∏ = ck =
eikŒ∏ Œ¶ŒΩ (eiŒ∏ )dŒ∏ for k = 0, 1, . . . , ŒΩ, (A.11)
2œÄ ‚àíœÄ
2œÄ ‚àíœÄ
from which we have pointwise convergence of the Fourier coeÔ¨Écients of Œ¶ŒΩ (eiŒ∏ ) to
those of Œ¶(eiŒ∏ ) as ŒΩ ‚Üí ‚àû, and hence Œ¶ŒΩ (eiŒ∏ ) ‚Üí Œ¶(eiŒ∏ ) in the 2 sense. However, by
Lemma A.5, Œ¶ŒΩ (eiŒ∏ ) ‚Üí |f‚àû (eiŒ∏ )|‚àí2 in ‚àû norm, and hence a fortiori in 2 norm, as
ŒΩ ‚Üí ‚àû. Since, in addition, not only Œ¶(eiŒ∏ ) but also f‚àû is analytic in a neighborhood
of the unit circle (Lemma A.3), we have

L

L

L

Œ¶(eiŒ∏ ) = |f‚àû (eiŒ∏ )|‚àí2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer.
In particular, WŒΩ is an outer spectral factor of Œ¶ŒΩ (eiŒ∏ ) satisfying

  œÄ it

e +z
1
it 2
log |WŒΩ (e )| dt .
WŒΩ (z) = exp
4œÄ ‚àíœÄ eit ‚àí z
But Lemma A.5, Equation (A.12) and the fact that Œ¶(eiŒ∏ ) = |W (eiŒ∏ )|2 ,

  œÄ it

1
e +z
it 2
log |W (e )| dt = W (z),
WŒΩ (z) ‚Üí exp
4œÄ ‚àíœÄ eit ‚àí z
the outer spectral factor of Œ¶. But, by Lemma A.3, WŒΩ (z) ‚Üí f‚àû (z)‚àí1 in
therefore f‚àû (z) = W ‚àí1 (z) as claimed.

DcœÅ, and

26

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6.
Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6.
Proof of Theorem 4.3. Following [53] we see that
WŒΩ‚àí1 (WŒΩ ‚àí Wred )‚àû ‚â§ 9,

(A.13)

and consequently
|WŒΩ (eiŒ∏ ) ‚àí Wred (eiŒ∏ )| ‚â§ 9|WŒΩ (eiŒ∏ )|
holds for all Œ∏, from which we have
(1 ‚àí 9)|WŒΩ (eiŒ∏ )| ‚â§ |Wred (eiŒ∏ )| ‚â§ (1 + 9)|WŒΩ (eiŒ∏ )|.
However, in view of (3.6) and (3.7), it follows from (A.1) that
‚àö
‚àö
rŒΩ
rŒΩ
iŒ∏
‚â§ |WŒΩ (e )| ‚â§ ŒΩ‚àí1
,
ŒΩ‚àí1
k=0 (1 + |Œ≥k |)
k=0 (1 ‚àí |Œ≥k |)
which together with (3.5) yields
c0
‚â§ |WŒΩ (eiŒ∏ )| ‚â§ Œ∫
Œ∫

(A.14)

for all Œ∏. This establishes (4.27). To see that Wred is minimum phase if 9 < 1, note
that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by RoucheÃÅ‚Äôs
Theorem, Wred has the same number of zeros in Dc (including ‚àû) as WŒΩ . Hence,
since WŒΩ is minimum phase, so is Wred .
To establish the bound (4.28) note that
WŒΩ ‚àí Wred ‚àû ‚â§ WŒΩ ‚àû WŒΩ‚àí1 (WŒΩ ‚àí Wred )‚àû .
From (A.14) we have WŒΩ ‚àû ‚â§ Œ∫, and hence (4.28) follows from (A.13).
Appendix B. Statistical convergence proofs
Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corresponding SzegoÃà polynomial œïÃÇŒΩ (z) and predictor error variance rÃÇŒΩ from (3.4) and (3.5),
and form the maximum-entropy Ô¨Ålter
‚àö ŒΩ
rÃÇŒΩ z
.
WÃÇŒΩ (z) =
œïÃÇŒΩ (z)
To determine WŒΩ ‚àí WÃÇŒΩ ‚àû let z ‚àà Dc and form
‚àö ŒΩ
‚àö ŒΩ
rŒΩ z
rÃÇŒΩ z
WŒΩ (z) ‚àí WÃÇŒΩ (z) =
‚àí
œïŒΩ (z)
œïÃÇŒΩ (z)
‚àö
‚àö
‚àö
( rŒΩ ‚àí rÃÇŒΩ )z ‚àíŒΩ œïŒΩ (z) ‚àí rŒΩ z ‚àíŒΩ (œïŒΩ (z) ‚àí œïÃÇŒΩ (z))
.
=
z ‚àíŒΩ œïŒΩ (z)z ‚àíŒΩ œïÃÇŒΩ (z)
Since r‚àû > 0, by (3.7) and Lemma A.1,
‚àö
‚àö
0 < ¬µ := r‚àû Œ± ‚â§ |z ‚àíŒΩ œïŒΩ (z)| ‚â§ c0 Œ≤ =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1),
|z

‚àíŒΩ

œïÃÇŒΩ (z)| ‚â• ¬µÃÇŒΩ :=

ŒΩ‚àí1


(1 ‚àí |Œ≥ÃÇk |),

k=0

where Œ≥ÃÇ0 , Œ≥ÃÇ1 , . . . , Œ≥ÃÇŒΩ‚àí1 are the Schur parameters corresponding to the estimated covariances (2.6). Therefore, by the maximum-modulus principle,

‚àö
‚àö
1
{M | rŒΩ ‚àí rÃÇŒΩ | + c0 |œïŒΩ (z) ‚àí œïÃÇŒΩ (z)|},
|WŒΩ (z) ‚àí WÃÇŒΩ (z)| ‚â§ max
|z|=1 ¬µ¬µÃÇŒΩ
where we have also used the fact that rŒΩ ‚â§ c0 . But, for |z| = 1,
|œïŒΩ (z) ‚àí œïÃÇŒΩ (z)| ‚â§ œïŒΩ ‚àí œïÃÇŒΩ 1 ,
where œïŒΩ and œïÃÇŒΩ are the ŒΩ-vectors formed as in (4.19) and ¬∑1 is the B1 norm. Recall
that œïŒΩ is the unique solution of the normal equations


TŒΩ œïŒΩ = ‚àícŒΩ where cŒΩ := cŒΩ cŒΩ‚àí1 . . . c1 ,
(B.1)
where TŒΩ is the Toeplitz matrix deÔ¨Åned by (2.4), and that
rŒΩ = c0 + cŒΩ œïŒΩ .

(B.2)

Also, the analogous relations hold for œïÃÇŒΩ and rÃÇŒΩ . Then,
rŒΩ ‚àí rÃÇŒΩ = (c0 ‚àí cÃÇ0 ) + (cŒΩ ‚àí cÃÇŒΩ ) œïŒΩ + cÃÇŒΩ (œïŒΩ ‚àí œïÃÇŒΩ )
and hence
|rŒΩ ‚àí rÃÇŒΩ | ‚â§ |c0 ‚àí cÃÇ0 | + cŒΩ ‚àí cÃÇŒΩ 1 œïŒΩ ‚àû + cÃÇŒΩ ‚àû œïŒΩ ‚àí œïÃÇŒΩ 1 .
Finally,


‚àö
|rŒΩ ‚àí rÃÇŒΩ |
|r ‚àí rÃÇ |
‚àö ‚â§ ŒΩ‚àö ŒΩ ,
| rŒΩ ‚àí rÃÇŒΩ | ‚â§ ‚àö
r‚àû
rŒΩ + rÃÇŒΩ

and consequently, since x1 ‚â§ ŒΩx‚àû for any x ‚àà RŒΩ ,

M
‚àö {|c0 ‚àí cÃÇ0 | + œïŒΩ ‚àû ŒΩcŒΩ ‚àí cÃÇŒΩ ‚àû }
¬µ¬µÃÇŒΩ r‚àû



M cÃÇŒΩ ‚àû
1 ‚àö
c0 + ‚àö
+
ŒΩœïŒΩ ‚àí œïÃÇŒΩ ‚àû .
¬µ¬µÃÇŒΩ
r‚àû

WŒΩ ‚àí WÃÇŒΩ ‚àû ‚â§

(B.3)

Recall now that œïŒΩ and œïÃÇŒΩ are each solutions of a normal equation (B.1). More
precisely, TŒΩ œïŒΩ = ‚àícŒΩ and TÃÇŒΩ œïÃÇŒΩ = ‚àícÃÇŒΩ . Since ck = CAk‚àí1 CÃÑ  for k > 0, where all
eigenvalues of A are less than one in modulus, ck ‚Üí 0 exponentially, we have
cŒΩ ‚àû ‚â§ K1

and TŒΩ ‚àû ‚â§ c0 + 2

ŒΩ‚àí1
	

|ck | ‚â§ K2

k=1

for some constants K1 and K2 . Moreover, from [8] we have
TŒΩ‚àí1 ‚àû ‚â§

ŒΩ‚àí1
1  1 + |Œ≥k |
‚â§ K3
c0 k=0 1 ‚àí |Œ≥k |

28

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence
œïŒΩ ‚àû ‚â§ TŒΩ‚àí1 ‚àû cŒΩ ‚àû ‚â§ K1 K3
and the condition number
Œ∫(TŒΩ ) := TŒΩ ‚àû TŒΩ‚àí1 ‚àû ‚â§ K := K2 K3
is bounded for all ŒΩ.
Now, it is known [25] that for each data length N in (2.2), there is a ŒΩ(N ) of order
O(log N ) such that


log log N
,
(B.4)
max |ck ‚àí cÃÇk | = O
0‚â§k‚â§ŒΩ(N )
N
and therefore, for any a ‚àà R,
ŒΩ a |c0 ‚àí cÃÇ0 | ‚Üí 0 and ŒΩ a cŒΩ ‚àí cÃÇŒΩ ‚àû ‚Üí 0 as ŒΩ = ŒΩ(N ) ‚Üí ‚àû.

(B.5)

Consequently the Ô¨Årst term in the bound (B.3) tends to zero as N ‚Üí ‚àû and
ŒΩ(N ) ‚Üí ‚àû provided it is done at the speciÔ¨Åed relative rates and provided ¬µÃÇŒΩ is
bounded away from zero. However, the estimate (2.6) has the property that the
corresponding Toeplitz matrix TÃÇŒΩ is positive deÔ¨Ånite for each Ô¨Ånite ŒΩ, and this in turn
is equivalent to |Œ≥ÃÇk | < 1 for k = 0, 1, . . . , ŒΩ ‚àí 1 so that ¬µÃÇŒΩ > 0. Since, in addition
¬µÃÇŒΩ ‚Üí ¬µ > 0 as ŒΩ(N ) ‚Üí ‚àû by (B.4) and continuity, the second requirement is also
fulÔ¨Ålled. To simplify notations, we have suppressed the index N in the quantities
marked with a hat, which of course depend on the data (2.2) and hence also on N .
Next we show that also the second term in (B.3) tends to zero. Since cÃÇŒΩ ‚àû ‚â§
cŒΩ ‚àû + cŒΩ ‚àí cÃÇŒΩ ‚àû is bounded, it thus remains to demonstrate that
ŒΩœïŒΩ(N ) ‚àí œïÃÇŒΩ(N ) ‚àû ‚Üí 0 as ŒΩ(N ) ‚Üí ‚àû.
This follows from the more general fact, needed for the proof of Corollary B.1, that
ŒΩ a œïŒΩ(N ) ‚àí œïÃÇŒΩ(N ) ‚àû ‚Üí 0 as ŒΩ(N ) ‚Üí ‚àû

(B.6)

for any a ‚àà R. To prove this, Ô¨Årst note that
TŒΩ ‚àí TÃÇŒΩ ‚àû ‚â§ |c0 ‚àí cÃÇ0 | + 2ŒΩcŒΩ ‚àí cÃÇŒΩ ‚àû ,
and hence TŒΩ ‚àí TÃÇŒΩ ‚àû ‚Üí 0. Therefore œÅŒΩ := TŒΩ ‚àí TÃÇŒΩ ‚àû TŒΩ‚àí1 ‚àû < 1 for ŒΩ := ŒΩ(N )
suÔ¨Éciently large, and, provided cŒΩ = 0, the standard perturbation estimate [22] yields


1
TŒΩ ‚àí TÃÇŒΩ ‚àû cŒΩ ‚àí cÃÇŒΩ ‚àû
œïŒΩ ‚àí œïÃÇŒΩ ‚àû
‚â§
Œ∫(TŒΩ )
+
,
(B.7)
œïŒΩ ‚àû
1 ‚àí œÅŒΩ
TŒΩ ‚àû
cŒΩ ‚àû
and consequently, since TŒΩ ‚àû ‚â• c0 > 0, it follows from (B.5) that (B.6) tends to
zero in the required manner.
If cŒΩ = 0, œïŒΩ = 0, and hence
œïŒΩ ‚àí œïÃÇŒΩ ‚àû = œïÃÇŒΩ ‚àû ‚â§ TÃÇŒΩ‚àí1 ‚àû cÃÇŒΩ ‚àû = TÃÇŒΩ‚àí1 ‚àû cŒΩ ‚àí cÃÇŒΩ ‚àû ,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case cŒΩ = 0. In fact, since ¬µÃÇ is
bounded away from zero, by continuity, for each 9 > 0, there is a N0 such that
TÃÇŒΩ‚àí1 ‚àû

ŒΩ‚àí1
ŒΩ‚àí1
1  1 + |Œ≥k |
1  1 + |Œ≥ÀÜk |
‚â§
+ 9 ‚â§ K3 + 9
‚â§
cÃÇ0 k=0 1 ‚àí |Œ≥ÀÜk |
c0 k=0 1 ‚àí |Œ≥k |

for ŒΩ ‚â• N0 .

Corollary B.1. If ŒΩ(N ) is deÔ¨Åned as in Theorem 5.1, then, for any a ‚àà R,
ŒΩ a WŒΩ ‚àí WÃÇŒΩ ‚àû ‚Üí 0

almost surely as ŒΩ := ŒΩ(N ) ‚Üí ‚àû.

To prove Theorem 5.3, we Ô¨Årst note that the Hankel operator H, deÔ¨Åned by (4.1),
has a nice representation in the space 2 of square-integrable functions. In fact, let
2
2
of functions with vanishing negative Fourier coeÔ¨Écients,
+ be the subspace in
hence being analytic in the unit disc D. In this setting, H has the representation
2
2
‚Üí 2 +
given by
HŒò : +

L

L

H

H

L H

HŒò f = P ‚ä• Œòf,

(B.8)

where P ‚ä• is the orthogonal projection onto the orthogonal complement
2
2
, and where Œò is the ‚àû -function
+ in

H

L

L

Œò(z) = W‚àí (z)WÃÑ+ (z)‚àí1 .

L H
2

2
+

of

(B.9)

Here W‚àí (z) and WÃÑ+ (z) are the analytic and coanalytic minimum-phase spectral factors deÔ¨Åned in Section 4. (See, e.g., [30, 31].) In the present scalar case, WÃÑ+ (z) =
W‚àí (z ‚àí1 ). In fact, the phase function Œò is the transfer function of an all-pass Ô¨Ålter
transforming the white noise w‚àí in (4.2) to the white noise w+ in (4.3) [30, p. 834].
ÀÜ + be the stochastic measures such that
Let dwÃÇ‚àí and dwÃÑ
 œÄ
 œÄ
iŒ∏t
ÀÜ+
e dwÃÇ‚àí and wÃÑ+ (t) =
eiŒ∏t dwÃÑ
w‚àí (t) =
‚àíœÄ

Then

‚àíœÄ


H

+

œÄ

=

H‚àí =
and consequently
H := E

f ‚Üî f dwÃÇ‚àí .

H‚àí

‚àíœÄ
œÄ

H

L H
2

‚àíœÄ



2 ÀÜ
+ dwÃÑ +

œÄ

=
‚àíœÄ

H

2
iŒ∏t
+ Œò(e )dwÃÇ‚àí

2
+ dwÃÇ‚àí

|H + corresponds to HŒò under the isomorphism deÔ¨Åned by





Proof of Theorem 5.3. It follows from Theorem 5.2 that |WÃÇŒΩ (eiŒ∏ )| ‚àí |WŒΩ (eiŒ∏ )| ‚Üí 0
uniformly in Œ∏ as ŒΩ ‚Üí ‚àû, and hence, by Lemma A.1, there are positive real numbers
¬µ1 and ¬µ2 such that
¬µ1 ‚â§ |WÃÇŒΩ (eiŒ∏ )| ‚â§ ¬µ2
for all Œ∏ and suÔ¨Éciently large ŒΩ. Therefore, since
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ WÃÇŒΩ ‚àû WÃÇŒΩ‚àí1 (WÃÇŒΩ ‚àí WÃÇ )‚àû ,

(B.10)

30

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

(A.13) and (4.26) imply that
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ 2¬µ2

ŒΩ
	

œÉÃÇk
,
1 ‚àí œÉÃÇk
k=nÃÇ+1

(B.11)

for suÔ¨Éciently large ŒΩ, where œÉÃÇ1 , œÉÃÇ2 , . . . , œÉÃÇŒΩ are the singular values (5.1) determined
from the covariance estimates (2.6).
It is well-known (see, e.g., [56, p. 204]) that the singular value œÉk of the Hankel
operator HŒò , deÔ¨Åned by (B.8) equals the inÔ¨Åmum of HŒò ‚àí K over all operators
2
2
‚Üí 2 +
of Ô¨Ånite rank at most k. Recall that Œò(z) = WŒΩ (z)/WŒΩ (z ‚àí1 ).
K : +
The singular value œÉÃÇk of HŒòÃÇ , where ŒòÃÇ(z) = WÃÇŒΩ (z)/WÃÇŒΩ (z ‚àí1 ), is described analogously.
Therefore, since

H

L H

HŒòÃÇ ‚àí K ‚â§ HŒòÃÇ ‚àí HŒò  + HŒò ‚àí K ‚â§ ŒòÃÇ ‚àí Œò‚àû + HŒò ‚àí K,
we have œÉÃÇk ‚â§ ŒòÃÇ ‚àí Œò‚àû + œÉk . But, for k > n, œÉk = 0, and hence œÉÃÇk ‚â§ ŒòÃÇ ‚àí Œò‚àû .
Consequently, (B.11) yields
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ M1 ŒΩŒòÃÇ ‚àí Œò‚àû ,

(B.12)

where M1 := 2¬µ2 (1 ‚àí œÉÃÇnÃÇ+1 )‚àí1 . However,


‚àí1 ‚àí1
‚àí1
‚àí1
WÃÇŒΩ (z) ‚àí W (z) ‚àí Œò(z)[WÃÇŒΩ (z ) ‚àí W (z )] ,
ŒòÃÇ(z) ‚àí Œò(z) = WÃÇŒΩ (z )
so, since WÃÇŒΩ (z ‚àí1 )‚àû is uniformly bounded by (B.10), and Œò‚àû is constant,
ŒòÃÇ ‚àí Œò‚àû ‚â§ M2 W ‚àí WÃÇŒΩ ‚àû ,
which together with (B.12) yields
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ M1 M2 ŒΩW ‚àí WŒΩ ‚àû + M1 M2 ŒΩWŒΩ ‚àí WÃÇŒΩ ‚àû
for suÔ¨Éciently large ŒΩ. Consequently the theorem follows from Theorem 3.4 and
Corollary B.1.
Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A.
Gombani, W. B. Gragg, G. Picci and T. SoÃàderstroÃàm for stimulating discussions and
for providing us with appropriate references. We are also indebted to the anonymous
referees for several useful suggestions.
References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987.
2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489‚Äì502, 1974.
3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975.
4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational
covariance extension problem. SIAM Journal on Control and Optimization, 37:211‚Äì229, 1999.
5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of
all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841‚Äì1857, 1995.
6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast Ô¨Åltering algorithms.
SIAM Journal on Control and Optimization, 32:744‚Äì789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-GuÃàrsoy. On the L‚àû consistency of L2 estimators. Systems & Control
Letters, 12:71‚Äì76, 1989.
8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978.
9. A. DahleÃÅn, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace
identiÔ¨Åcation methods may fail. Systems and Control Letters, 34:303‚Äì312, 1998.
10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced
stochatic realizations. In Proc. 21st IEEE CDC, pages 1105‚Äì1112, 1983.
11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans.
Automatic Control, AC-29:1097‚Äì1100, 1984.
12. J. Durbin. EÔ¨Écient estimation of parameters in moving average models. Biometrika, 46:306‚Äì316,
1959.
13. J. Durbin. The Ô¨Åtting of time-series models. Rev. Inst. Int. Stat., pages 223‚Äì243, 1960.
14. P. Duren. Theory of Hp spaces. Academic Press, 1970.
15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear.
16. G. Freud. Orthogonale Polynome. BirkhaÃàuser Verlag, 1969.
17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic
Control, AC-32:358‚Äì361, 1987.
18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac.,
Speech and Signal Processing, ASSP-35:438‚Äì449, 1987.
19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961.
20. M. Gevers. Towards a joint design of identiÔ¨Åcation and control. In J. Willems and H. Trentelman,
editors, Essays on Control: Perspectives in the Theory and its Applications, 1993.
21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their
l‚àû -error bounds. Int. J. Contr., 39:1115‚Äì1193, 1984.
22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989.
23. W. B. Gragg. Positive deÔ¨Ånite Toeplitz matrices, the Arnoldi process for isometric operators,
and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in
Linear Algebra, pages 16‚Äì32. Moscow U. P., 1982.
24. U. Grenander and G. SzegoÃà. Toeplitz forms and their applications. Univ. California Press, 1958.
25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons,
1988.
26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples.
Biometrika, pages 297‚Äì307, 1989.
27. W. Jones and E. SaÔ¨Ä. SzegoÃà polynomials and frequency analysis. In Approximation Theory,
pages 341‚Äì352. Dekker Inc., 1992.
28. S. Y. Kung. A new identiÔ¨Åcation method and model reduction algorithm via singular value
decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705‚Äì714, 1978.
29. W. E. Larimore. System identiÔ¨Åcation, reduced ordered Ô¨Åltering and modeling via canonical
variate analysis. In Proc. of the American Control Conference, 1983.
30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes.
SIAM J. Control and Optimization, 23:809‚Äì857, 1985.
31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic
systems. J. of Math. Systems, Estimation and Control, 1:241‚Äì333, 1991.
32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension,
and identiÔ¨Åcation of stationary time series. Automatica, 32(5):709‚Äì733, 1996.
33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating
transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412‚Äì440, 1992.
34. L. Ljung and Z. Yuan. Asymptotic properties of black box identiÔ¨Åcation of transfer functions.
IEEE Trans. Automatic Control, AC-26:514‚Äì530, 1985.
35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis,
Royal Instiute of Technology, 1998.
36. D. Q. Mayne and F. Firoozan. Linear identiÔ¨Åcation of ARMA processes. Automatica, 18:461‚Äì466,
1982.

32

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

37. H. Mhaskar and E. SaÔ¨Ä. The distribution of zeros of asymptotically extremal polynomials. J.
Approx. Theory, 3:279‚Äì300, 1991.
38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66‚Äì73, 1978.
39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward,
editors, Approximation Theory VI, 1989.
40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991.
41. P. Van Overschee. Subspace IdentiÔ¨Åcation, Theory - Implementation - Application. PhD thesis,
Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De
Moor.
42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identiÔ¨Åcation problem.
In Proc. 30th Conference on Decision and Control, Brighton, 1991.
43. P. Van Overschee and B. De Moor. Subspace IdentiÔ¨Åcation for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996.
44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994.
45. I. Schur. UÃàber Potenzreihen, die im Innern des Einheitskreises beschraÃànkt sind. J. fuÃàr die Reine
und Angewandte Mathematik, 147:205‚Äì232, 1917.
46. J. Sorelius, T. SoÃàderstroÃàm, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identiÔ¨Åcation. In Proc. SYSID ‚Äô97, 1997.
47. G. SzegoÃà. BeitraÃàge zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift,
6:167‚Äì202, 1920.
48. G. SzegoÃà. UÃàber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232‚Äì244, 1921.
49. G. SzegoÃà. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications,
1939 (4th edition 1975).
50. B. Wahlberg. On the IdentiÔ¨Åcation and Approximation of Linear Systems. PhD thesis, LinkoÃàping
University, 1987. LinkoÃàping Studies in Science and technology. Dissertations No. 163.
51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive
approximations. Journal of Time Series Analysis, 10:283‚Äì299, 1989.
52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like
identiÔ¨Åcation techniques. IEEE Trans. Automatic Control, 37:900‚Äì912, 1992.
53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation.
Systems and Control Letters, 14:307‚Äì317, 1990.
54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423‚Äì
434, 1953.
55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938.
56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

CANONICAL CORRELATION ANALYSIS, APPROXIMATE COVARIANCE EXTENSION, AND IDENTIFICATION OF STATIONARY TIME SERIES*
ANDERS LINDQUIST AND GIORGIO PICCI

Abstract. In this paper we analyze a class of state-space identification algorithms for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called "subspace methods" can be described as covariance estimation followed by stochastic realization. The methods offer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identification into the solution of a Riccati equation but introduce at the same time some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identification procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not explicitly reported. In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We review the basic ideas of stochastic realization theory in the context of identification, discuss the concept of stochastic balancing and of stochastic model reduction by principal subsystem truncation. The model reduction method of Desai and Pal, based on truncated balanced stochastic realizations, is partially justified, showing that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing conditions under which the "subspace identification" methods produce bona fide stochastic systems.

1. Introduction Recently there has been a renewed interest in state-space identification algorithms for time series based on a two steps procedure which in principle can be described as estimation of a rational covariance model from observed data followed by stochastic realization. The method offers the major advantage of converting the nonlinear parameter estimation phase which is necessary in traditional ARMA models identification into a partial realization problem, involving a Hankel matrix of estimated
 This research was supported in part by grants from TFR, the G® oran Gustafsson Foundation, the SCIENCE project "System Identification" and LADSEB-CNR.  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden  Dipartimento di Elettronica e Informatica, Universita' di Padova, 35131 Padova, Italy 1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which efficient numerical solution techniques are available. In this framework we can naturally accommodate multivariate processes and there are indications that the algorithms may work also with data containing purely deterministic components (van Overschee and De Moor, 1993). A drawback, however, to be emphasized in this paper, is that, unlike, say, least-squares identification of ARMA models, these methods do not work for arbitrary data. This type of procedure was apparently first advocated by Faurre (1969); see also Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work, based on canonical correlation analysis (Akaike, 1975) (or some other singular-value decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki (1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor, 1993). Numerical experience shows that the computation time needed to get the final model parameters estimates compares very favorably with traditional iterative prediction error methods for ARMA models. On the other hand there is a price to be paid for this simplification. These methods introduce some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic realization arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. Note that positivity is the natural condition insuring solvability of the Riccati equation required to compute state-space models of the signal from the covariance estimates. Central in the procedures described above is the following classical problem of identification of a covariance sequence. Let {0 , 1 , . . . ,  } (1.1)

be a finite set of sample m ◊ m covariance matrices estimated in some unspecified way from a certain m-dimensional sequence of observations {y0 , y1 , y2 , . . . yT }, Ø = k CAk-1 C and such that the infinite sequence {0 , 1 , 2 , . . . }, (1.4) (1.2)

Ø ) such that and consider the problem of finding a minimal1 triplet of matrices (A, C, C k = 1, 2, . . . ,  (1.3)

Ø for k =  + 1,  + 2, . . . , is a bona fide obtained from (1.1) by setting k := CAk-1 C covariance sequence. In the literature the last condition is generally ignored. The remaining problem of Ø ) satisfying (1.3) is called the minimal partial realfinding a minimal triplet (A, C, C Ø ) is usually computed by minimal factorization ization problem. The triplet (A, C, C
1

Ø ) is minimal if (A, C ) is completely observable and (A, C Ø ) is completely reachable. Here (A, C, C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

3

of a block Hankel matrix corresponding to the data (1.1) as follows:      Ø C 1 2 C 3 ∑ ∑ ∑ j Ø 2 3  4 ∑ ∑ ∑ j +1   CA   CA      , H= = . . . . . .  .      . .. . . . . . . . . . . . Ø (A )j -1 i i+1 i+2 ∑ ∑ ∑ i+j -1 CAi-1 C

(1.5)

where i + j - 1 =  and the Hankel matrix H is chosen as close to square as possible by taking |i - j |  1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j ) such that i + j - 1 =  , and hence the minimal factorization must be made for a choice of (i, j ) in which the Hankel matrix (1.5) has maximal rank. The infinite sequence Ø for k =  +1,  +2, . . . {0 , 1 , 2 , . . . } obtained in this way by setting k := CAk-1 C is called a minimal rational extension of the finite sequence (1.1) and is in general not a covariance sequence. The dimension r of a minimal rational extension is called the (algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to the McMillan degree of the m ◊ m rational matrix Ø + 1 0 , (1.6) Z (z ) = C (zI - A)-1 C 2 and the elements of the infinite sequence (1.4) are the coefficients of the Laurent expansion 1 Z (z ) = 0 + 1 z -1 + 2 z -2 + . . . 2 (1.7)

about z = . The underlying identification problem is however a great deal more complicated than the classical partial realization problem. In fact, the requirement that (1.4) be a bona fide covariance sequence amounts to (1.4) being a positive sequence in the sense that, for every t  Z+ , the block Toeplitz matrices Tt ,   2 ∑ ∑ ∑ t 0 1 1 0 1 ∑ ∑ ∑ t-1  , Tt =  (1.8) . . . . ...  . . . . . .  . . t t-1 t-2 ∑ ∑ ∑ 0 formed from the infinite sequence (1.4), be positive definite or, equivalently, that the matrix function (z ) := Z (z ) + Z (1/z ) be positive semidefinite on the unit circle, i.e. (ei )  0   [0, 2 ). (1.10) (1.9)

This property is equivalent to  being a spectral density matrix. In fact, it will be the spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial covariance sequence unless T > 0, but this is not enough. From the point of view of identification there seem to be two possible routes to Ø ) from the finite covariance sequence (1.1). One that determine a model (A, C, C has been proposed in the literature is do minimal factorization (1.5) of a finite block Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown in this paper, there is no a priori guarantee that this method will yield a positive extension. This fact has nothing to do with sample variability (random fluctuations) of the covariance estimates (1.1), and to emphasize this point we initially assume that all strings of data (1.2) are infinitely long. A theoretically sounder identification method, which will not be considered in this paper, could instead be to do positive extension first and then to use a stochastic model reduction procedure on the triplet Ø ) of the positive extended sequence. (A, C, C The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this difficulty into consideration have been pointed out by the authors of this paper at many scientific meetings in the last ten years. This has had no apparent effect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identification procedure of Aoki (1990) and demonstrate that there is a hidden, and not easily tested, assumption without which the procedure will not be guaranteed to succeed. The punch line is that none of the subspace identification methods under consideration can be expected to always work for generic data but that some not entirely natural conditions on the data are needed. The analysis of the basic theoretical issues behind subspace identification is carried out in the geometric framework of stochastic realization theory; see, e.g., Lindquist and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this theory and adapt them to the problem of identification. To this end, we first discuss an idealized situation in which the time series (1.2) is infinitely long i.e. T = , and the available covariance data are given by the ergodic limit 1 T  T + 1 lim
T

yt+k yt+j = k-j
t=0

(1.11)

for all k and j . Then the sample estimates in the sequence (1.1) are bona fide covariance matrices and the Toeplitz matrix T formed from the data will be positive definite and symmetric. We introduce a Hilbert space of observed (infinite) strings of data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we establish a correspondence which turns operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. Canonical correlations and balanced stochastic realizations are then analyzed in this setting in Section 4, and the basic concepts and principles used in the subspace identification methods, as well as in the model reduction procedures of Desai and Pal, are translated into the more natural context of geometric stochastic realization theory. Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact infinite covariance sequence (1.4). Consequently, the difficult question of positivity is not an issue here. Nor is it the finite sample size per se which is the problem, but the fact that only a finite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is finite. Therefore we separate these issues by discussing stochastic realization theory from finite covariance data in Section 5 and subspace identification in Section 6. In this framework we show that the method of van Overschee and De Moor (1993) is valid under some rather stringent assumptions. We stress that we are only concerned with identification procedures for state space modeling of time series. "Subspace identification" methods for deterministic systems with measurable inputs or for spectral factors do not involve positivity, but stability may still be a problem. However, the algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part, so the problem of positivity arises here too. Another idea behind the subspace identification methods considered in this paper is to disregard modes corresponding to "small" canonical correlation coefficients. This is called balanced truncation and is in fact a stochastic model reduction procedure. In all such procedures there must be a guarantee that the reduced-degree matrix function (1.6) is positive real, and therefore the preservation of positivity in such reductions is a main concern of this paper. Section 7 is devoted to such issues. The model reduction procedure of Desai and Pal (1982) was never theoretically justified in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 . Here we shall demonstrate that this reduction procedure produces a positive real, but not in general balanced, reduced model structure. In fact, the singular values of the truncated system are usually not equal to the r first singular values of the original system. It is an interesting fact that the procedure of Desai and Pal does produce balanced truncations for continuous-time stochastic systems. A partial result in this direction was given by Harshavardana, Jonckheere and Silverman (1984), who showed that the truncated function is positive real and conjectured that it is balanced. We shall demonstrate that it is indeed balanced, a result that is actually already contained in the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time depends on the fact that the spectral factors of the truncated approximate spectrum behave differently than in continuous time. While in continuous time the realizations of the reduced spectral factors are proper subsystems, obtained by partitioning the matrices of the realizations of the factors of , this is not the case in discrete time, contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced truncation procedure is available in discrete time, but the systems matrices are no longer submatrices of those of the original system, and therefore it is not equivalent to the truncation procedure used in subspace identification. Several of the results of this paper have previously been announced in Lindquist and Picci (1994a)3 and in Lindquist and Picci (1994b).

In Desai et al. (1985) a different model reduction procedure, which is not relevant to subspace identification, is considered, namely "deterministic" model reduction of the minimum phase spectral factors. 3 We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some erroneous statements, was accidentally published in place of the paper finally submitted for publication. The correct version can be obtained from the authors.

2

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances The solution to the minimal partial realization problem , i.e., the problem to find Ø ) satisfying (1.1) is in general not unique. This lack of uniquethe triplet (A, C, C ness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1) equals that of {0 , 1 , . . . ,  -1 } so that we can use a Hankel matrix (1.5) based allowing us to define the shifted Hankel matrix  3 4 2  3 4 5  (H ) =  . . .  . . . . . . i+1 i+2 i+3 (2.1)

on this data, i.e., with i + j =  ,  ∑ ∑ ∑ j +1 ∑ ∑ ∑ j +2   . ... . .  ∑ ∑ ∑ 

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) proØ ) which is unique up to a similarity transformation. duces a minimal solution (A, C, C As first pointed out by Zeiger and McEwen (1974), the minimal factorization on which the Ho-Kalman procedure is based may be performed by singular-value decomØ ) uniquely; see also Kung (1978). In fact, the Hankel position, thereby fixing (A, C, C matrix H may be factored as H = U V U U = I = V V, (2.3) where  is the square n ◊ n diagonal matrix of the nonzero singular values taken in Ø := V 1/2 this leads to a factorization decreasing order. Setting  := U 1/2 and  Ø H =  Ø Ø == (2.4)

Ø ) is obtained by solving of the type (1.5). Then a minimal realization (A, C, C Ø =  (H ), A Ø = 1 (H ) and C Ø  = 1 (H ), C

where  (H ) is the shifted Hankel matrix (2.2) and 1 (H ) is the first block row of H . Ø ) must be given by It follows that the triplet (A, C, C A = -1/2 U  (H )V -1/2 , C = 1 (H )V -1/2 , Ø = 1 (H )U -1/2 , C (2.5a) (2.5b) (2.5c)

a form to which we refer as finite-interval balanced, since it is balanced in the sense Ø are both equal to , and that Ø that   and      Ø C C Ø  CA   CA  Ø   .  = (2.6) = . .    . . .  . Ø (A )j -1 CAi-1 C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identification of time series. The problem with such a strategy is that this algorithm is a deterministic realization procedure and hence does not a priori insure that (1.6) is positive real, or even stable for that matter, even if the Toeplitz matrix T is positive definite. In fact, it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space of covariance data (1.1) for which A is not stable, and a fortiori the same holds for positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in Aoki (1990) is based on the following hidden assumption which is not entirely natural. Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H . Therefore, not only must we know that there exists an underlying finite-dimensional system, but we must also have some upper bound for its dimension. A conservative ]. upper bound which will always suffice is [  2 Is this assumption natural? If the covariance data are really generated exactly from a "true" stochastic system and there is a reliable estimate of its order which is no more than half of the length of the covariance sequence, then the assumption will hold. However, and this is an important point of this paper, one cannot expect Assumption 2.1 to hold for an arbitrary covariance sequence (1.1). To clarify this point, let us agree to call {0 , 1 , 2 , . . . } a minimal rational extension of {0 , 1 , . . . ,  } if the rational function (1.7) has minimal degree. By definition this is the algebraic degree of {0 , 1 , . . . ,  }. A rational extension is called positive if, for every µ >  , the block Toeplitz matrices Tµ formed from the corresponding infinite sequence (1.4) are positive definite. An extension with this property is called a positive rational extension. It is well known that the extension {0 , 1 , 2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function Z (z ) is analytic in the closed unit disc and the matrix function (z ) = Z (z ) + Z (1/z ) (2.7)

is nonnegative definite on the unit circle, making  a spectral density matrix. A minimal positive rational extension of the finite sequence (1.1) is one for which the Ø ) in (1.6) is as small as possible. dimension of the triplet (A, C, C Definition 2.2. The positive degree p of the finite covariance sequence {0 , 1 , . . . ,  } is the dimension of any minimal positive extension. A well-known example of a positive extension is the maximum entropy extension (Whittle, 1963) corresponding to the spectral density (z ) := W (z )W (1/z ) , where the spectral factor W (z ) is (modulo a multiplicative constant matrix) the inverse of the Levinson-Szeg® o matrix polynomial of order  corresponding to the finite covariance sequence (1.1). Since the rational function W (z ) generically has the McMillan degree equal to m , it follows from spectral factorization theory (Anderson, 1958) that Z (z ) has also degree m . Consequently, the positive degree p is bounded from below by the algebraic degree r and from above by m . As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use algebraic rather than positive extensions, usually computed by minimal factorization a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1 may also be formulated in the following way. Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree. This assumption prescribes a property of the covariance sequence (1.1) which is not generic. We can illustrate this point by considering the rational extension problem for a finite scalar covariance sequence (1.1). The positive degree p lies between the algebraic degree r and  . Note that neither the case p =  nor the case p <  are "rare events", because there are open sets of covariance sequences (1.1) of both categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each µ  µ   there is an open set of covariance data in R for which p = µ. such that  2 If the upper limit p =  is attained there are infinitely many nonequivalent minimal Ø ) providing a positive extension, one of which is the maximum entropy triplets (A, C, C extension. In fact, it can be shown that these  -dimensional extensions form an Euclidean space (Byrnes and Lindquist, 1989). This shows that the finite data (1.1) never contains enough information to establish a "true" underlying system. A similar statement can be made in the case when p <  . Example 2.3. Consider the case m = 1 and  = 2, i.e., consider a scalar partial covariance sequence {0 , 1 , 2 }. If 1 = 2 = 0, we have r = p = 0. Otherwise, we always have r = 1, whereas the positive degree can be either one or two. In fact, 2 setting 0 := 1 /0 and 1 := (2 1 + 2 )/(1 - 1 ), it can be shown (Georgiou, 1987; also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1 if and only if |0 | |1 | < 1 + |0 | and p = 2 otherwise. In fact, it is not hard to construct examples for which the gap between algebraic and positive rank is arbitrarily large, as the following theorem shows. Theorem 2.4. Let n  Z+ be fixed. Then for an arbitrarily large  there is a stable rational function Z (z ) of degree n, such that the Toeplitz matrix T formed as in ( 1.8) from the coefficients of the Laurent expansion ( 1.7), is positive definite while T +1 is indefinite. Consequently, you cannot test the positivity of a rational extension of (1.1) by checking a finite Toeplitz matrix, however large is its dimension. The proof of Theorem 2.4 is given in Appendix A. Let us now return to the identification procedure of Aoki (1990). In practice the rank of H will always be full, and to compute a partial realization of reasonable dimension the basic idea is to partition  as = 1 0 , 0 2 (2.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in 2 are smaller than those in 1 , perhaps close to zero, and then take 2 = 0 so that H is approximated by H1 = U 1 0 V = U1 1 V1 . 0 0 (2.9)

The matrix H1 is a best approximation (given the rank) of H in (the induced) 2 ≠ norm, but it is in general not Hankel and hence can not be used to determine a reduced order system. Of course, one may instead use Hankel-norm approximation (Adamjan, Arov and Krein, 1971), which produces another best approximation of H in 2 -norm that is Hankel and has the same rank as H1 . However, if 2 is "very small" compared to 1 , then H1 is close to H and hence approximately Hankel. For this reason, Aoki's procedure (Aoki, 1990) is based on the original data H and  (H ). Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I , the Ør ) given by same type of calculation as above yields the reduced triplet (Ar , Cr , C Ar = 1
- 1/ 2

U1  (H )V1 1
- 1/2

- 1/ 2

,

(2.10a) (2.10b) (2.10c)

Cr = 1 (H )V1 1 , 1 /2 Ør = 1 (H )U1 - C . 1

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal subsystem truncation in the sense that, if H is produced by a finite-dimensional system Ø ) having finite-interval balanced form (2.5), we have with (A, C, C Ar = A11 , where A= A11 A12 A21 A22 C = C1 C2 Ø1 C Ø2 . Ø= C C (2.12) Cr = C1 , Ør = C Ø1 , C (2.11)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for  (H ), 1 (H ) and 1 (H ) and inserting in (2.10). However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that Ø1 + 1 0 (2.13) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real, provided of course that Z , defined by (1.6), is positive real. The question of stability was answered in the affirmative in Pernebo and Silverman (1982) and is addressed in Aoki (1990). The crucial question of positivity, however, is not discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be proven for a somewhat modified procedure described below. In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall consider a normalized Hankel matrix
1 -T ^ = L- H + HL- ,

(2.14)

where L- and L+ are lower triangular Cholesky factors of the Toeplitz matrices T- and T+ of (1.1) and the corresponding sequence of transposed covariances respectively; see Section 4 below. This is also the Hankel matrix considered in van Overschee and ^ instead of H , the De Moor (1993). Taking the singular value decomposition of H

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coefficients, i.e., the cosines of the angles between the past and the future of the process y . The systems matrices can be determined in a manner analogous to (2.5), but now
-1 -1 Ø ^ = Ø T-  =  T+

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as ^ =U ^ ^U ^ so that H = above. To see this, consider the singular value decomposition H Ø ^ ^ ^ (L+ U )(L- V ) . Since H =  and this factorization is unique modulo coordinate Ø = L- V ^ ^ 1/2 and  ^ ^ 1/2 . Then transformation in state space, we may take  = L+ U ^ ^ ^ ^ (2.15) follows from U U = I = V V . As we shall see next, (2.15) corresponds to a more natural type of balancing corresponding to a Hankel operator describing the interface between the past and the future of the time series y . 3. Stochastic realization theory in the Hilbert space of a sample function In this section we introduce a mathematical framework which is suitable for the identification problem described above. We define a Hilbert space of observed (infinite) strings of data {yt }. This framework turns out to be isomorphic to that of geometric stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we also establish a correspondence which converts operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. In identification we have access only to a finite string of data {y0 , y1 , y2 , . . . , yT }. (3.1)

Here T may be quite large but, of course, always finite. To begin with, we shall, however, consider the idealized situation that we are given a doubly infinite sequence of m-dimensional data {. . . , y-3 , y-2 , y-1 , y0 , y1 , y2 , y3 . . . } (3.2)

together with a corresponding covariance sequence {k }k0 , each matrix k of the sequence being computed from the data (3.2) by an ergodic limit of the type (1.11). In Section 5 we then modify the theory to handle the situation of finite data (3.1). For each k  Z define the m ◊  matrix y (t) := [yt , yt+1 , yt+2 , . . . ] (3.3)

and consider the sequence y := {y (t)}tZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of all finite linear combinations ak y (tk ); ak  Rm , tk  Z

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product defined by linear extension of the bilinear form 1 a y (k ), b y (j ) := lim T  T + 1
t0 +T

a yt+k yt+j b = a k-j b,
t=t0

(3.4)

which clearly does not depend on t0 . This inner product is nondegenerate if the Toeplitz matrix Tk , constructed from the covariance data {0 , 1 , . . . , k }, is a positive definite symmetric matrix for all k . Here we shall assume that the sequence {Tk }k0 is actually coercive, i.e., Tk > cI for some c > 0 and all k  0. (See Assumption 3.2 below for an alternative characterization.) We also define a shift operator U on the family of semi-infinite matrices (3.3), by setting Ua y (t) = a y (t + 1) t  Z, a  Rm ,

defining a linear map which is isometric with respect to the inner product (3.4) and extendable by linearity to all of Y . In particular the sequence of matrices {y (k )} corresponding to the time series y is propagated in time by the action of the operator U, i.e., yi (t) = Ut yi (0), i = 1, 2, . . . , m, t  Z, (3.5)

where yi denotes the i:th row component of y . Then, closing the vector space Y in the inner product (3.4), we obtain a Hilbert space H (y ) := cl Y . The shift operator U is extended by continuity to all of H (y ) and is a unitary operator there. As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results in the geometric theory of stochastic realization can be carried over to the present framework by merely identifying the time series y with a stationary stochastic process y. In particular, the subspaces H - and H + of H (y ) generated by the elements (3.3) for t < 0 and t  0 respectively can be regarded as the past and future subspaces of the stationary process y. For reasons of uniformity of notation the inner product (3.4) will also be denoted ,  = E { }, (3.6)

as the frameworks are completely equivalent. Here we allow E {∑} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity condition introduced above insures that tZ Ut H - = 0 and tZ Ut H + = 0, i.e., y is a purely nondeterministic sequence. As we have pointed out above, the subspace identification methods of Aoki (1990) and van Overschee and De Moor (1993) are based on the assumption that the available data is generated by an underlying stochastic system of finite dimension. More specifically, using the notations introduced above, we assume that the data are generated by a linear system of the type x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (3.7)

12

ANDERS LINDQUIST AND GIORGIO PICCI

defined for all t  Z, where w is some vector-valued normalized white noise time series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability matrix. Throughout this paper we shall assume (without restriction) that (A, B, C ) B is a minimal triplet and that the matrix has linearly independent columns. D The system is assumed to be in statistical steady state so that the n-dimensional state x and the m-dimensional output y are uniquely defined by (3.7) as linear causal functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and y (s) will depend only on the difference t - s. We shall think of the system (3.7) as a representation of the output time series y . The state and input variables x and w are introduced in order to display the special structure of the dynamic model of y and are by no means unique. Such a representation is called a state-space realization of y . Remark 3.1. Despite the fact that the model (3.7) is defined in terms of sample sequences, all equalities must be understood in the sense of Hilbert space metric, just as in the case of models based on random variables. The number of state variables n is called the dimension of the realization. A realization is minimal if there is no other realization of y of smaller dimension. In this case the covariance matrix of the state vector, P = E {x(t)x(t) } is positive definite. Moreover as the matrix (3.8)

B is taken with linearly independent D columns, the number of (scalar) white noise inputs p is also as small as possible. Clearly, the covariance sequence {0 , 1 , 2 , . . . } of the output {y (t)} of a minimal model (3.7) is a rational sequence of degree n, i.e., represented as Ø = AP C + BD Ø k = 0, 1, 2, . . . where C k = CAk-1 C 0 = CP C + DD . (3.9)

In the following we shall need to assume that the corresponding spectral density (z ) satisfies the following condition. Assumption 3.2. The spectral density  of the output process of the underlying system (3.7) is coercive in the sense that (ei ) > 0 for all   [0, 2 ]. (3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent sequences. Recall that a positive real function Z such that (z ) := Z (z ) + Z (z -1 ) satisfies (3.10) is called strictly positive real. Let H (w) be the Hilbert space generated by w, i.e. the closure of the linear space spanned by the family {wi (t), i = 1 . . . p, t  Z} with respect to the metric induced by the inner product ,  = E {  } where E {∑} is defined by (3.6). Let H + and H - be the subspaces of H (w) generated by the components of future {y (0), y (1), y (2) . . . } and past outputs {y (-1), y (-2), y (-3) . . . }, respectively.
4

This means that E {w(t)w(s) } = Its where ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace X := {a x(0) | a  Rn } (3.11)

is invariant under coordinate changes of the type (A, B, C )  (T AT -1 , T B, CT -1 ) and is a coordinate-free representation of the realization (3.7). Such an object is called a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next define the stationary Hankel operator of y , H : H +  H - as H := E H |H +
- -

(3.12)

where E H  is the orthogonal projection of  onto H - . The splitting subspace property of X is equivalent to the commutativity of the diagram H+ O i.e. to the factorization H = CO ,
+ -

- H - C X

H

(3.13)

where the operators O := E H |X and C := E H |X are the observability respectively constructibility operators relative to the splitting subspace X . It can be shown that the splitting subspace X is minimal if and only if O and C are both injective. (See, e.g., Lindquist and Picci (1991).) The system (3.7) is a forward or causal realization of y in the sense that the subspace + H (w), generated by the future of w, is orthogonal to X and H - , i.e. to the present state and past output. Corresponding to (3.7) there is another realization Øw Ø(t) + B Ø (t - 1) x Ø(t - 1) = A x Øx Øw y (t - 1) = C Ø(t) + D Ø (t - 1) (3.14)

Ø ), generated by which is backward or anticausal in the sense that the subspace H - (w + Ø(0) is a basis in X , i.e. the past of w Ø , is orthogonal to X and H . Like x(0), x X := {a x Ø(0) | a  Rn }. Ø = P -1 P x Ø(0) = P -1 x(0). (3.15)

In fact, x Ø(0) is the dual basis of x(0) in the sense that E {x(0)Ø x(0) } = I . Hence (3.16)

The particular notations used in (3.7) and (3.14) reflect the special meaning of the Ø ). Computing the covariance matrix of the output using the dual parameters (A, C, C Ø ) is precisely a realizations (3.7) and (3.14), it is in fact readily seen that (A, C, C triplet realizing the positive real part (1.6) of the spectral density matrix (z ) of the time series y . There are infinitely many minimal factorizations (3.13), one for each Markovian splitting subspace, but the basis in each state space X can be chosen so Ø ) are the same for each minimal X . This is called a uniform that the triplets (A, C, C choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward predictor spaces X- = E H H +
-

X+ = E H H - ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12) and of its adjoint, respectively. Ø ), the splitting Fixing a uniform choice of bases, and thus the triplets (A, C, C subspace X- has the forward stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) with state covariance P- , and X+ has the backward realization Ø+ w x Ø+ (t - 1) = A x Ø+ (t) + B Ø+ (t - 1) Ø +w Øx Ø+ (t - 1) y (t - 1) = C Ø+ (t) + D (3.19) (3.18)

Ø+ . with state covariance P These two stochastic realizations will play an important role in what follows. In fact, an important interpretation of these realizations is that
-1 [y (t) - Cx- (t)] x- (t + 1) = Ax- (t) + B- D-

is the unique steady-state Kalman filter of any minimal realization (3.7) of y in the fixed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8) Ø+ )-1 , then corresponding to the forward counterpart of (3.19), i.e., P+ = (P P -  P  P+ for the state covariance of any minimal realization (3.7). In the same way
-1 Ø+ Ø+ D Ø+ (t) + B [y (t - 1) - C x Ø+ (t)] x Ø+ (t - 1) = A x

(3.20)

is the backward steady-state Kalman filter of all minimal backward realizations (3.14), and Ø+  P ØP Ø- P Ø- is the backward counfor an arbitrary backward minimal realization (3.14), where P terpart of P- . 4. Canonical correlations and balanced stochastic realization In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a finite-dimensional realization of the type (3.7). Equivalently, we study certain factorizations of the infinite Hankel matrix of the corresponding infinite covariance sequence {0 , 1 , 2 , . . . }. Some portions of this section can be found in an equivalent but somewhat different setting in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and set notations. This will be done in the geometric framework of Section 3, thereby providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall introduce orthonormal bases in H - and H + . To this end it will be useful to represent past and future outputs as infinite vectors in the form,     y (-1) y (0) y (-2) y (1)    y = (4.1) y- =  + y (-3) y (2) . . . . . . Let L- and L+ be the lower triangular Cholesky factors of the infinite block Toeplitz matrices T+ := E {y+ y+ } = L+ L+ T- := E {y- y- } = L- L- and let
1  := L- - y- 1  Ø := L- + y+

(4.2)

be the corresponding orthonormal implies that  1 2 H := E {y+ y- } =  3 . . .

bases in H - and H + respectively. Now, (3.9) 2 3 3 4 4 5 . . . . . .    Ø  C C ... Ø  . . .  CA   CA = 2  Ø 2 , . . . CA  C (A )  . . ... . . . .

(4.3)

and therefore we have the following representation result, which can be found in Desai et al. (1985). Proposition 4.1. Let y be realized by a finite dimensional model of the form (3.7). Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator H is
1 -T -1 Ø -T ^  = L- H + E {y+ y- }L- = L+  L- ,

(4.4)

where

 C  CA   = CA2  . . . 

and

Ø  C Ø  CA  Ø   = C Ø (A )2  . . . .



(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization (4.3) for H , irrespective of which X (i.e. which minimal realization of y ) is chosen. Recall that the adjoint O of the observability operator O is defined as the unique linear operator H +  X such that O,  = , O  for all   X and   H + . Orthogonality implies that E H ,  = ,  = , E X  , and therefore O = E X |H + . In the same way, we see that C  = E X |H - . The finiterank linear operators O O and C  C are defined on X and are the coordinate-free representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O O is full rank and constructible if and only if C  C is full rank. The following representations show that these gramians are related
+

16

ANDERS LINDQUIST AND GIORGIO PICCI

Ø+ , the state covariances of the forward and backward steady-state Kalman to P- and P filters (Picci and Pinzoni, 1994). Proposition 4.2. Let x(0) and x Ø(0) be the conjugate basis vectors in a minimal splitting subspace X as defined above. Then, in a uniform choice of bases, Ø+ x(0) Ø(0) = a P O O a x and C  C a x(0) = a P- x Ø(0), (4.7) Ø+ , respectively, independently i.e., C  C and O O have matrix representations P- and P of X . Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal, E H a x(0) = a x- (0), C  C a x(0) = E X a x- (0) = E X a P- x Ø- (0). But, since the bases x Ø(0) and x Ø- (0) are chosen uniformly, EX a x Ø- (0) = a x Ø(0) a  Rn , and consequently (4.7) follows. The proof of (4.6) is analogous. The factorization (4.4) can also be derived from (3.13) and the following useful matrix representations of the observability and constructibility operators. Proposition 4.3. Let x(0) and x Ø(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
T Ø(0) = a  L- Ø Oax +  1 O b  Ø = b L- + x(0)
-

(4.6)

and therefore

(4.8)

and
T Ø L- C a x(0) = a  -  1Ø x(0), C  b  = b L- - Ø

(4.9)

Ø are given by (4.5). where  and  Proof. Since, in view of (3.7), y+ = x(0) + terms which are orthogonal to X,
1 and  Ø = L- + y+ , we have 1 E { Øx(0) } = L- + P.

(4.10)

Consequently, for any a  Rn , the usual projection formula5 yields O a x(0) = E H a x(0) = a E {x(0)Ø  } Ø and O b  Ø = EX b  Ø = b E { Øx(0) }P -1 x(0), from which (4.8) follows. A symmetric argument yields (4.9).
If   H (w) and the subspace Z  H (w) is spanned by the components of the full-rank random vector z , then E Z  = E {z }(E {zz })-1 z .
5
+

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should note that the matrix representations of O and C  are the transposes of those of O Ø = I . Moreover, it follows and C if and only if x(0) is an orthogonal basis, i.e., P = P from (4.8) that -1 Ø(0) = a  T+ x(0), O Oa x
-1 showing that  T+  is a matrix representation of O O, in harmony with the analysis at the end of Section 2. In the same way, (4.9) yields -1 Ø Ø T- Ø x(0), C  C a x(0) = a  -1 Ø Ø T-  is a matrix representation of C  C . Together with Proposition 4.2 and hence  Ø+ : this yields the following explicit formulas for P- and P -1 Ø+ =P  T+ -1 Ø Ø T-   = P- .

(4.11)

Now, let {1 , 2 , 3 , . . . } be the singular values of the Hankel operator H. Since rank H = n, i = 0 for i > n. The nonzero singular values 1  1  2  3 . . .  n > 0 (4.12)

are the cosines of the angles between the subspaces H- and H+ ; they are known as the canonical correlation coefficients of y (Hotelling, 1936, Anderson, 1958). Obviously 1 < 1 if and and only if H-  H+ = 0. The squares of the canonical correlation coefficients are the eigenvalues of H H, i.e.,
2 i , H H i = i

which, in view of (3.13) may be written
2 (O i ), O OC  C (O i ) = i

and therefore, as was also demonstrated in Picci and Pinzoni (1994),
2 2 2 , 2 , . . . , n }, {O OC  C} = {1

(4.13)

2 2 2 , 2 , . . . , n are the eigenvalues of O OC  C . But, in view of Proposition 4.2, i.e., 1 this is precisely the coordinate-free version of the invariance condition 2 2 2 Ø+ } , 2 , . . . , n } = {P- P {1

(4.14)

of Desai and Pal (1984). This suggests that an appropriate uniform choice of bases would be the one that Ø+ equal and equal to the diagonal matrix of nonzero canonical corremakes P- and P lation coefficients. ^  is the In fact, in view of Proposition 4.1, the infinite normalized Hankel matrix H matrix representation of the operator H in the orthonormal bases (4.2). Therefore ^  has the singular-value decomposition H ^  = U   V = U  V , H   = diag{1 , 2 , 3 , . . . , n }, (4.15) where  is the diagonal n ◊ n matrix consisting of the canonical correlation coefficients (4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and  is the infinite matrix  =  0 . 0 0

Moreover U and V are infinite orthogonal matrices, and U and V are  ◊ n submatrices of U and V with the the property that U U = I = V V. (4.17) We now rotate the the orthonormal bases (4.2) in H + and H - to obtain u := U  Ø and v := V  respectively. Note that E {uv } =  . What makes these orthonormal bases useful is that they are adapted to the orthogonal decompositions6 H -  H + = [H -  (H + ) ]  H  [H +  (H - ) ], (4.18)

where H := X-  X+ is the so-called frame space (Lindquist and Picci (1985, 1991), in the sense that X- = span{v1 , v2 , . . . , vn } X+ = span{u1 , u2 , . . . , un }. This is true since X- is precisely the subspace of random variables in H - having nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H - . Since therefore {vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H -  (H + ) and H +  (H - ) , respectively, these spaces will play no role in what follows. Now define the n-dimensional vectors  1/2   1/2  1 u1 1 v1   1/2 u    1/2 v  2 2   1 1 z Ø =  2 .  = 1/2 U L- (4.19) z =  2 .  = 1/2 V L- - y- + y+ . .  .   .  n vn
1/2

n un

1/ 2

Ø is a basis in X+ , and they From what we have seen before, z is a basis in X- and z have the properties Øz Ø }. E {zz } =  = E {z (4.20)

In fact, we even have more as seen from the following amplification7 of a theorem by Desai and Pal (1984) (Theorem 1). Theorem 4.4. The basis vectors x- (0) = z x Ø+ (0) = z Ø (4.21)

in X- and X+ respectively belong to the same uniform choice of basis, i.e. to the Ø ), and in this uniform choice same choice of triplets (A, C, C Ø+ . P- =  = P
6 7

(4.22)

The symbols  and  denote vector sum and orthogonal vector sum of subspaces. Ø ). A priori there is no reason why choosing bases in X- and X+ would lead to the same (A, C, C This important property is explicitly mentioned in Theorem 4.4.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coefficients {1 , 2 , 3 , . . . , n } are distinct, this is, modulo multiplication with a signature matrix 8 , the only uniform choice of bases for which ( 4.22) holds. Ø ) is know as stochastically balanced, and, in the case of Such a choice of (A, C, C distinct canonical correlation coefficients, it defines a canonical form with respect to state space isomorphism in (1.6) by fixing the sign in, say, the first element in each row of C . Such canonical forms have also been studied by Ober (1991). Proof. It follows from (4.4) and (4.15) that E {z Øz } = 2 . (4.23) Ø ) so that x Ø, and let the bases in the other splitting Now, choose (A, C, C Ø+ (0) = z subspaces be chosen accordingly so that the choice of bases is uniform. We want Ø+ (0) and that to show that x- (0) = z . To this end, first note that x+ (0) = -1 x x- (0) = E X- x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula and the fact that z is a basis in X- , Øz }-1 z, x- (0) = -1 E {z which, in view of (4.23), yields x- (0) = z as claimed. Hence (4.22) follows from (4.20). Ø ) is another uniform choice of bases which Next, suppose that (QAQ-1 , CQ-1 , CQ is also stochastically balanced. Since then x- (0) = Qz and, as is readily seen from Ø+ = Q-T Q-1 , Ø so that P- = QQ and P the backward system (3.14), x Ø+ (0) = Q-T z (4.22) yields QQ =  and Q-T Q-1 = , from which we have Q2 = 2 Q. Since  has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959) that there is a scalar polynomial (z ) such that Q = (2 ). Hence Q is diagonal and commutes with  so that, by QQ = , we have QQ = I. Consequently, since Q is diagonal, it must be a signature matrix. In view of (4.21) and (3.16), the first of relations (4.9) and (4.8) respectively yield -1 -1 Ø T- y- z Ø =  T+ y+ . (4.24) z= Consequently, in view of (4.20), (2.15) holds also for the case of an infinite Hankel matrix. This can of course also be seen from (4.11). Note that the normalization of the block Hankel matrix H is necessary in order for the singular values to become the canonical correlation coefficients, i.e., the singular values of H. In fact, if we were to use the unnormalized matrix representation (4.3) of H instead, as may seem simpler and more natural, the transpose of (4.3) would not be the matrix representation of H in the same bases, a property which is crucial in the singular value decomposition above. This is because (4.3) corresponds to the bases y- in H - and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the finite-dimensional case studied in ^ , defined in Section 2, is Section 2, and therefore the normalized Hankel matrix H preferable to the unnormalized H . Ø in terms of the Hankel matrix H , can Formulas, such as (2.5), expressing A, C, C be easily derived from basic principles. In fact, standard calculations based on the forward model (3.7) and the backward model (3.14) yield A = E {x(1)x(0) }P -1 C = E {y (0)x(0) }P -1 , Ø -1 = E {y (-1)x(0) } Ø = E {y (-1)Ø C x(0) }P for any dual pair of bases x(0) and x Ø(0). Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases (4.19) can be computed by means of the formulas
1 -T - 1/ 2 , A = -1/2 U L- +  (H )L- V  T - 1/2 C = 1 (H )L- , - V - T - 1 / 2 Ø = 1 (H )L+ U  C ,

(4.25a) (4.25b) (4.25c)

(4.26a) (4.26b) (4.26c)

where H is the unnormalized Hankel matrix (4.3),  (H ) is obtained from H by deleting the first block row, and 1 (H ) is the first block row. Proof. First, in (4.25a) and (4.25b), we take x(0) to be x- (0). By the Kalman filter representation a [x+ (1) - x- (1)]  UH -  H - for all a  Rn ,
-1 Ø+ E {x Ø+ (1)x- (0) }. E {x- (1)x- (0) } = E {x+ (1)x- (0) } = P

Ø ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19), But (A, C, C 1 1 Ø+ , x- (0) = 1/2 V L- Ø+ (1) = 1/2 U L- P- =  = P - y- and x +  (y+ ), where  (y+ ) is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),
1 -T - 1/ 2 , A = -1/2 U L- + E { (y+ )y- }L- V 

which is identical to (4.26a). Likewise, from (4.26b),
T - 1/ 2 , C = E {y (0)y- }L- - V

which yields (4.26b). Finally, taking x Ø(0) to be x Ø+ (0) in (4.25c), a symmetric argument yields (4.26c). Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from finite covariance data In this section we modify the realization theory of Section 4 to the case that only a finite segment {y (0), y (1), y (2), . . . , y ( )}, (5.1) of the time series {y (t)} is available. We still define each y (t) as the semi-infinite string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact partial covariance sequence {0 , 1 , 2 . . . ,  }. (5.2)

The corresponding realization problem, which is purely theoretical and is intended to prepare for the more realistic identification situation with finite strings of observed data (Section 6), is therefore the partial stochastic realization problem mentioned in Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the output of some minimal "true" system (3.7) of dimension n and that  is large enough for n to equal the positive degree of the partial sequence (5.2). Now, suppose that  = 2 - 1, and partition the data into two matrices     y (0) y ( )  y (1)   y ( + 1)  - +   , = y = (5.3) y . .      . . . . y ( - 1) y (2 - 1) representing the past and the future respectively, and define the corresponding (finite- + and y respectively as dimensional) subspaces Y- and Y+ spanned by the rows of y explained in Section 3. Since the data size  will be important in the considerations that will follow, we denote the finite block Hankel matrix H of Section 2, relative to the data (5.3), by H , i.e.,
+ - H = E {y (y ) }.

(5.4)

Let 0 be the smallest integer  such that rank H = n. It is well-known that 0 is Ø ), so n is an the maximum of the observability and constructibility indicies of (A, C, C upper bound for 0 . As pointed out in the beginning of Section 2, we need  > 0 to Ø ). be certain that the factorization of H yields a unique (A, C, C Next we shall consider the class of minimal splitting subspaces for Y- and Y+ , i.e., the subspaces X admitting a canonical factorization Y+  O of the finite-interval Hankel operator H := E Y |Y+ .
-  - Y- C X

H

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces, ^  - = E Y- Y+ X ^  + = E Y+ Y- , and X

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred to Appendix D. Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary time series {y (t)}. Then, if  > 0 , X := U X is a minimal splitting subspace for Y- and Y+ , and ^  - = E Y- X , X ^  + = E Y+ X . X (5.7) (5.6)

^  - has a unique representation9 Conversely, any basis x ^( ) in X x ^( ) = E Y x( ),
-

(5.8)

^  + has a unique representation ^ where x( ) is a basis in X , and any basis x Ø( ) in X ^ x Ø( ) = E Y x Ø( ),
+

with x Ø( ) a basis in X . As X varies over the family of all minimal Markovian splitting subspaces, the corresponding x(0) [Ø x(0)] constitute a uniform choice of bases. ^ - The stochastic realizations corresponding to the finite-interval predictor spaces X ^  + are nonstationary. However, taking advantage of the representations (5.8) and X and (5.9), we shall be able to express these realizations in such a way that they can Ø ) corresponding to one uniform be parameterized by the stationary triplet (A, C, C choice of bases, both for the forward and the backward settings. In fact, if the bases ^ x ^( ) and x Ø( ) are chosen so that x( ) and x Ø( ) in representations (5.8) and (5.9) are Ø ) is used for x( )} = I , then the same choice of (A, C, C dual bases in X , i.e., E {x( )Ø ^  + is called coherent. ^  - and X all X  . Such a choice of bases in X The realizations generated by these coherent bases are precisely the (transient) forward and backward Kalman filters. In fact, the vector x ^( ) is the one-step predictor of x( ) based on Y- and, as shown in Appendix C, it evolves in time as the Kalman filter

X

(5.9)

X

x ^(t + 1) = Ax ^(t) + K (t)[y (t) - C x ^(t)]; where the gain K (t) is given by

x ^(0) = 0,

(5.10)

Ø - AP- (t)C )(0 - CP- (t)C )-1 K (t) = (C and the filter estimate covariance ^(t)^ x(t) } P- (t) = E {x is the solution of the matrix Riccati equation

(5.11)

(5.12)

Ø - AP- (t)C )(0 - CP- (t)C )-1 (C Ø - AP- (t)C ) P- (t + 1) = AP- (t)A + (C P- (0)) = 0. (5.13)
With slight misuse of notations, the orthogonal projection operator applied to a vector will denote the vector of the projections of the components.
9

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the components of ^ Ø( ) x Ø( ) = E Y x ^  + and are generated by the backward Kalman filter form a basis in X Ø (t)[y (t - 1) - C Øx ^ ^ ^ x Ø(t - 1) = A x Ø(t) + K Ø(t)]; with Ø+ (t)C Ø )(0 - CP Ø - (t)C Ø )-1 , Ø (t) = (C - A P K where Ø+ (t) = E {x ^ ^ P Ø(t)x Ø(t) } is obtained by solving the matrix Riccati equation Ø+ (t)A + (C - A P Ø+ (t)C Ø+ (t)C Ø )(0 - C ØP Ø+ (t)C Ø )-1 (C - A P Ø) Ø+ (t - 1) = A P P Ø+ (2 - 1) = 0. P (5.18) Now, it is well-known that both ^(t)]  (t) = (0 - CP- (t)C )-1/2 [y (t) - C x and ØP Ø+ (t)C Ø )-1/2 [y (t - 1) - C Øx ^  Ø(t) = (0 - C Ø(t)] (5.20) (5.19) (5.17) (5.16) ^ x Ø(2 - 1) = 0, (5.15)
+

(5.14)

are normalized white noises, called the forward respectively the backward (transient) innovation processes. Consequently, we may write the Kalman filter (5.10) as x ^(t + 1) = Ax ^(t) + B- (t) (t) y (t) = C x ^(t) + D- (t) (t) (5.21)

where D- (t) := (0 - CP- (t)C )1/2 and B- (t) := K (t)D- (t). Likewise, the backward Kalman filter (5.10) may be written Ø+ (t)Ø ^ ^ Ø(t) + B x Ø(t - 1) = A x  (t - 1) Ø Ø ^ y (t - 1) = C x Ø(t) + D+ (t)Ø  (t - 1) (5.22)

Ø + (t) := (0 - C ØP Ø+ (t)C Ø )1/2 and B Ø+ (t) := K Ø (t)D Ø + (t). Comparing with (3.7) where D and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7) and (3.14) are time-varying and describe the output y only on the interval [0, 2 - 1]. In fact, since ^(t)][x(t) - x ^(t)] }  0, P - P- (t) = E {[x(t) - x Ø Ø and, for the same reason, P - P+ (t)  0, we have Ø+ (t)-1 , P- (t)  P  P+ (t) := P (5.23)

^  - and X ^  + are extremal splitting subspaces, so we see that the predictor spaces X just as X- and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the finite-interval counterparts of equations (4.25) are given by A = E {x ^( + 1)^ x( ) }P- ( )-1 C = E {y ( )^ x( ) }P- ( )-1 , Ø+ ( )-1 = E {y ( - 1)^ Ø = E {y ( - 1)x ^ x( ) } C Ø( ) }P (5.24a) (5.24b) (5.24c)

In complete analogy with the stationary framework in Section 4, the canonical correlation coefficients 1  1 ( )  2 ( )  ∑ ∑ ∑  n ( ) > 0 (5.25) between the finite past Y- and the finite future Y+ are now defined as the singular values of the operator H given by (5.5). To determine these we need a matrix representation of H in some orthonormal bases. Using the pair (5.19)≠(5.20) of transient innovation processes for this purpose, we obtain the normalized matrix (2.14), which ^  . Singular value decomposition yields we shall here denote H ^  = U  V , H (5.26) where U U = I = V V , and  is the diagonal matrix of canonical correlation coefficients. As in Section 4 it is seen that
-1 - z ( ) =  V (L-  ) y 1/2 -1 + z Ø( ) =  U (L+  ) y 1/2

(5.27)

^  - and X ^  + respectively and that are bases in X E {z ( )z ( ) } =  = E {z Ø z Ø }. (5.28)
+ Here L-  and L are the finite-interval counterparts of L- and L+ respectively, and they are of course submatrices of these. Note that H , as defined by (5.4), is now given by - ^ H  = L+  H (L ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z ( ) and z Ø( ) are coherent bases, and the Ø corresponding triplet (A, C, C ) is a finite-interval stochastically balanced realization, i.e., Ø+ ( ). P- ( ) =  = P (5.30)

The following finite-interval modification of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the finite block hankel matrix H , and the proof is analogous. Ø ), obProposition 5.2. The finite-interval stochastically balanced triplet (A , C , C ^ tained from (5.24) by choosing the bases x ^( ) = z ( ) and x Ø( ) = z Ø( ), is given by
1 /2 -1 - -T 1/ 2 U (L+ V - , A = -   )  (H )(L )  -T 1/ 2 V - , C = 1 (H )(L-  )  + -T - 1 /2 Ø C = 1 (H )(L ) U  ,

(5.31a) (5.31b) (5.31c)

where the operators  (∑) and 1 (∑) are defined as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

Ø ) actually varies with  , but that, for each  , it Note that the triplet (A , C , C Ø ) of Section 4, i.e., there is a is similar to the stochastically balanced triplet (A, C, C nonsingular matrix Q so that Ø ) = (Q AQ-1 , CQ-1 , CQ Ø ). (A , C , C    (5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the stationary predictor spaces X- and X+ will have the state covariances P- = Q Q
T -1 Ø+ = Q- and P  Q ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state covariances are not diagonal and equal is a manifestation of the fact that the triplet Ø ) is not stochastically balanced in the sense of Section 4. It is well known (A , C , C Ø+ , respectively, as t  , and Ø+ (t) tend monotonically to P- and P that P- (t) and P therefore we have the following ordering Ø+ )-1  (P Ø+ ( ))-1 := -1 . P- ( ) :=   P-  (P


Since the number n of nonzero singular values (5.25) is in general too large too yield a reasonable model, we must consider what happens when some of the smallest singular values are set equal to zero. The truncation procedure employed by van Overschee and De Moor (1993) is equivalent to the principal subsystem truncation presented in Section 2, except that, and this is very important, the singular-value ^  , which is the decomposition is performed on the normalized block Hankel matrix H natural matrix representation of the operator  . It will be shown in Section 7 that such a truncation will preserve positivity in the stationary case (Theorem 7.3). In order to carry this result over to the case of finite  , we need to assume that the spectral density  of the time series {y (t)} is coercive so that Assumption 3.2 is fulfilled, i.e., that the function Z is strictly positive real. The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D, shows that principal subsystem truncation preserves positivity provided  is chosen large enough.

H

Theorem 5.3. Suppose that the spectral density  of the time series {y (t)} is coercive. Then, there is an integer 1 > 0 such that, for   1 , the principal subsystem Ø )1 ) of (A , C , C Ø ) is a minimal realization of a strictly truncation ((A )11 , (C )1 , (C positive real function (2.13). 6. Subspace identification The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have access to an infinite sequence (3.2) of data. In reality we will have a finite string of observed data {y0 , y1 , y2 , . . . , yN }, (6.1)

where, however, N may be quite large. More specifically, we assume that N is sufficiently large that replacing the ergodic limits (1.11) by truncated sums yields good approximations of {0 , 1 , 2 . . . ,  }, (6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course,  << N . This is equivalent to saying that T := N -  is sufficiently large for 1 T +1
T

a yt+k yt+j b
t=0

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we shall use the finite-interval realization theory of Section 5 as if we had a finite time series {y (0), y (1), y (2), . . . , y ( )}, while substituting the semi-infinite string (3.3) of data by y (t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . ,  . (6.5) (6.4)

In particular, in this case the inner product becomes merely that of a finite-dimensional Euclidean space so that the block Hankel matrix H can be written H = where   y  -1 y  . . . yT + -1 y -2 y -1 . . . yT + -2  -  y = . . ..  . . . . . . .  . y0 y1 . . . yT 1 y + (y - ) T +1    y +1 . . . yT + y  y +1 y +2 . . . yT + +1  + . = and y . . ..  . . . . . . .  . y2 -1 y2 . . . yT +2 -1 

Consequently, the identification of a minimal stationary state-space innovation model describing the data (6.1) can be performed in the following steps. - + , y to obtain, from (1) Perform canonical correlation analysis on the data y ^ Ø+ ( ) = z Ø( ) and, from (5.26), the (5.27), the state vectors x ^- ( ) = z ( ) and x corresponding common state covariance matrix  , i.e., the diagonal matrix of the (finite interval) canonical correlation coefficients (5.25). (2) Given the singular value decomposition (5.26), compute via (5.31) a minimal Ø ). This realization will be in finite-interval balanced form, realization (A, C, C i.e., (5.30) will hold instead of (4.22). (3) To obtain a state space model (3.7) for y we need to compute the matrices B Ø 0 ) defines and D. Note that such matrices will exist if and only if (A, C, C, a positive real function (1.6), or, in other words, if and only if there is a symmetric positive definite P = P such that M (P ) := P - AP A Ø - CP A C Ø - AP C C 0 - CP C  0. (6.6)

[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6), B and D can be determined (in a nonunique way) by a full rank factorization of M (P ), i.e., B D B D = M (P ). (6.7)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P- = E {x- (t)x- (t) } has been determined. Obtaining P- amounts to finding the minimal solution of the algebraic Riccati equation Ø - AP C )(0 - CP C )-1 (C Ø - AP C ) P = AP A + (C (6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t   with initial condition P- ( ) =  . (The corresponding dual procedures yield Ø+ .) Again, in both cases, a positive definite P- can be found if and only P Ø 0 ) defines a positive real function (1.6). In fact, in general, if (A, C, C, {P- (t)}t0 may not even converge unless this positivity condition is fulfilled and may in fact exhibit dynamical behavior with several of the characteristics of chaotic dynamics (Byrnes et al., 1991, 1994). Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that, for  fixed but sufficiently large (see Section 2), we will have rank H = n as T  , Ø ) will be uniquely determined from the data and similar to the and the triplet (A, C, C Ø triplet (A, C, C ) of the "true" generating system. Hence, in particular, in the limit as T  , at least in theory positivity will be guaranteed. If n ^ is an upper bound for the order of the "true" system, we may choose  to be any integer larger than n ^. In practice, however, T is finite, and even if we had a true system generating exact data, the spectral estimate T , although converging to the true spectrum  as T   may in principle fail to be positive for any finite T if there are frequencies  for which (ei ) = 0. Positivity for a suitably large T can however be guaranteed if the "true" spectrum is coercive. The following proposition, which also applies to Aoki's method discussed in Section 2, is proved in Appendix D. Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulØ ) defined by filled. Then, there is a T0  Z+ such that, for T  T0 , the triplet (A, C, C (5.31) yields a function (1.6) which is strictly positive real. However, in practice, rank H normally will keep increasing with  , even for very large T , so that one must resort to some kind of truncation of the Hankel singular values. As we have pointed out in Section 5, setting all canonical correlation coefficients r+1 ( ), r+2 ( ), . . . equal to zero for some suitable r, as is done in, for example, van Overschee and De Moor (1993), is equivalent to principal subsystem truncation. An important issue is therefore under what conditions such a procedure will insure positivity. Here we must distinguish between problems generated by the sample fluctuations of the data due to finite sample size T , as considered in Proposition 6.1, and the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an infinite string of data generated by a "true" high-dimensional system, such a truncation procedure may fail if  is smaller than that dimension. Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following result, which justifies this approximation procedure, provided the rather stringent Assumption 2.1 holds and we have coercivity, and provided T and  are sufficiently large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulfilled. Then, there are positive integers T0 and 1 > 0 such that, for T  T0 and   1 , the Ø1 ), obtained from (2.12) by taking H := H in (2.10), is a minimal triplet (A11 , C1 , C realization of a strictly positive real function (2.13). We note that, in van Overschee and De Moor (1993), the large Hankel matrix
+ + + + - - - - ~  = (y ) (E {y (y ) })-1 E {y (y ) }(E {y (y ) })-1 y H

^  . This leads to a procedure which is equivalent to the one is used in place of H described above. Moreover, the computation of a second singular-value decomposition + - in van Overschee and De Moor (1993), based on H +1 := E {y +1 (y +1 ) }, together with a subsequent change of bases, is actually redundant, as can be deduced from the following proposition. In fact, a considerable amount of computation is needed in van Overschee and De Moor (1993) to compensate for the fact that taking z ( + 1), ^ ( +1)- would computed from a second singular-value decomposition, as a basis in X lead to a Kalman filter model with time-varying parameters. ^ Proposition 6.3. To each coherent pair of bases x ^( ) and x Ø( ) in the finite-interval ^ ^ predictor spaces X - and X + , there corresponds a minimal factorization Ø H =   of the block Hankel matrix H . Here
+  x ^( ) = E Y y
- + - Øx ^ Ø( ) = E Y y  .

(6.9)

and

(6.10)

Conversely, given a minimal factorization (6.9), Ø (T - )-1 y - x ^( ) =     and
+ ^ x Ø( ) =  (T+ )-1 y

(6.11)

^ +. ^  - and X is a coherent pair of bases in X ^  - and X ^  + . Then, for ^ Proof. Let x ^( ) and x Ø( ) be a coherent choice of bases in X Ø( )) of dual bases any X as defined in Theorem 5.1, there is a unique pair (x( ), x Ø  be the matrices defined via such that (5.8) and (5.9) hold. Let  and 
+ - Øx E X y =  x( ) and E X y = Ø( ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of X with respect to Y- and Y+ yields + - + - (y ) } = E {E X y (E X y )) }, E {y which, in view of (6.12), is the same as (6.9). Applying E Y and E Y to respectively the first and second equations of (6.12), the splitting property yields (6.10). As for the converse statement, equations (6.11) follow from the construction in the proof of Theorem 5.1, from which it also follows that the resulting bases x ^( ) and Ø ^ x Ø( ) are constructed from the same (A, C, C ) and therefore coherent. Ø ) have been fixed by a particular choice of x( ) As soon as the parameters (A, C, C in the representation (5.8) in Theorem 5.1, we must choose x ^( + 1) as x ^( + 1) = E Y +1 Ux( ) (6.13)
- +

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More specifically Proposition 6.3 Ø  are uniquely determined once x( ) has been selected. Hence implies that  and  Ø ) is uniquely determined by the Ho-Kalman algorithm so that (A, C, C Ø Ø  +1 = C  Ø  A is prescribed, as is
-1 - Ø  (T- x ^( + 1) =  +1 ) y +1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining x ^( + 1) by an extra singular-value decomposition, as in van Overschee and De Moor (1993), is actually redundant. If we actually were to determine x ^( + 1) as described -L Ø Ø above, we would better compute  +1 from  +1 =  H +1 , where the left inverse is very easily obtained from the singular-value decomposition of H . We stress that Assumption 2.1, although quite limiting, is absolutely crucial in insuring that the subspace identification algorithms mentioned above will actually work. Note that for generic data these algorithms may break down for any fixed  . The same is true for all other subspace methods which deal with identification of covariance models (or equivalent) involving stochastic signals. On the other hand, Assumption 2.1 introduces a quite unrealistic condition which, as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure to estimate T0 and 1 in Proposition 6.2, as the proof is based only on continuity arguments. 7. Stochastic model reduction As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step in order to keep the dimension of the model at a reasonable level. To justify any such procedure one must either assume that there is an underlying "true" system of sufficiently low order, i.e., invoke Assumption 2.1, or to perform rational covariance extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995), Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an infinite one. The latter can be done in many ways, one of which is the maximum entropy extension. In either case, the truncation problem is equivalent to approximating a positive real matrix function Ø + 1 0 , Z (z ) = C (zI - A)-1 C 2 (7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of lower degree. In this section we shall investigate how this can be done and also how such an approximation affects the canonical correlation structure. One main question to be addressed is whether the principal subsystem truncation (2.11) preserves positive realness and balancing, and hence the leading canonical correlation coefficients, as originally claimed by Desai and Pal (1982). As it turns out, the answer is affirmative to the first but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identification approximation obtained by setting some canonical correlation coefficients equal to zero. It is instructive to first consider the continuous-time counterpart of this problem since the latter is simpler and exhibits more desirable properties. Also, it has been widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true. It is well-known [see, e.g., Faurre et al. (1979)] that an m ◊ m matrix function Z with minimal realization Ø + 1 R, (7.2) Z (s) = C (sI - A)-1 C 2 is positive real with respect to the right half plane if and only if there is a symmetric matrix P > 0 such that Ø - PC -AP - P A C  0, (7.3) M (P ) := Ø C - CP R where here we assume that R is positive definite and symmetric. In this case there are two solutions of (7.3), P- and P+ , with the property that any other solution of (7.3) satisfies P -  P  P+ . (7.4)

These extremal solutions play the same role as P- and P+ in the discrete-time setting, and rank M (P- ) = m = rank M (P+ ). (7.5)

-1 Ø+ := P+ If the state-space coordinates are chosen so that both P- and P are diagonal and equal, and thus, by (4.14), equal to the diagonal matrix  of canonical correlation Ø ) is stochastically balanced. coefficients, we say that (A, C, C Now, suppose that  is partitioned as in (2.8) with r+1 < r , and consider the corresponding principal subsystem truncation (2.12). Using the stochastic realization framework, Harshavaradana, Jonckheere and Silverman (1984) showed that

Ø1 + 1 R, Z1 (s) = C1 (sI - A11 )-1 C 2

(7.6)

Ø1 ) is a minimal realization of a positive real function and conjectured that (A11 , C1 , C is stochastically balanced. We shall next show that this conjecture is true, as has already been done by Ober (1991) in a framework of canonical forms. First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield   Ø1 - 1 C1 -A11 1 - 1 A11  C   0,     (7.7) Ø  R C1 - C1 1 where blocks which play no role in the analysis are marked by an asterisk. Consequently, M1 (1 ) = Ø1 - 1 C1 -A11 1 - 1 A11 C  0. Ø C1 - C1 1 R (7.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982), Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane, Ø1 ) is a minimal realization. (7.6) is positive real, but it remains to prove that (A11 , C1 , C This was done in Harshavaradhana et al. (1984). It is important to observe here that, contrary to the situation in the discrete-time setting, rank M1 (1 ) = rank M () = m 1 -1 and rank M1 (- 1 ) = rank M ( ) = m, important facts that will be seen to imply that the reduced system is stochastically balanced. Recall that in the continuous-time setting the spectral density (s) = Z (s)+ Z (-s) is coercive if, for some > 0, we have (s)  I for all s on the imaginary axis. This is equivalent to the condition that R > 0 and  has no zeros on the imaginary axis (Faurre et al., 1979, Theorem 4.17). Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with (s) := Ø ) be in stochastically balanced form. Then, Z (s) + Z (-s) coercive, and let (A, C, C Ø1 ) defines a positive real function (7.6) if r+1 < r , the reduced system (A11 , C1 , C for which it is a minimal realization in stochastically balanced form, and 1 (s) := Z1 (s) + Z1 (-s) is coercive. Proof. We have already shown that Z1 is positive real, and we refer the reader to Ø1 ) is a minimal realization Harshavaradhana et al. (1984) for the proof that (A11 , C1 , C Ø1 ) is stochastically of Z1 . It remains to show that 1 is coercive and that (A11 , C1 , C -1 balanced, i.e., that P1- = 1 = P1+ , where P1- and P1+ are solutions to the algebraic Riccati equation Ø - P1 C1 )R-1 (C Ø - P1 C1 ) = 0 A11 P1 + P1 A11 + (C (7.9) such that any other solution P1 of (7.9) satisfies P1-  P1  P1+ . To this end, 1 -1 note that since M1 (1 ) and M1 (- 1 ) have rank m, both 1 and 1 satisfy (7.9). 1 Therefore, as is well-known (Molinari, 1977) and easy to show, Q := - 1 - 1 satisfies 1 Q + Q1 + QC1 R-1 C1 Q = 0, where Ø - 1 C )R-1 C1 . 1 = A11 - (C 1 (7.11) Since  is coercive, -1 -  = P+ - P- > 0 (Faurre et al., 1979, Theorem 4.17) so that 1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to 1 Q-1 + Q-1 1 + C1 R-1 C1 = 0. (7.12) (7.10)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , 1 ). Since, in addition, the Lyapunov equation (7.12) has a positive definite solution Q-1 , 1 must be a stability matrix. Therefore 1 is the minimal (stabilizing) solution P1- of -1 Ø1+ := P1+ = 1 . (7.9). In the same way, using the backward setting, we show that P Ø1 ) is stochastically balanced. Since P1+ - P1- > 0, 1 is Consequently, (A11 , C1 , C coercive. Ø 1 0 ) Let us now return to the discrete-time setting. Let us recall that, if (A, C, C, 2 is a minimal realization of (7.1), the matrix function Z is positive real if and only if the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given the positive real rational function (7.1) with the property that (z ) = Z (z ) + Z (z -1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y , the state covariance P of any minimal stochastic realization (3.7) of y satisfies (6.6) and the matrices B, D in (3.7) satisfy (6.7). Consequently, as pointed out in Section 5, the matrices B and D can be determined via a matrix factorization of M (P ) once P has been determined. Ø ) is in stochastically balanced form, Theorem 4.4 implies that Now, if (A, C, C M ()  0. In view of (4.16) and (2.12), M () may be written   Ø1 - A11 1 C1 - A12 2 C2 1 - A11 1 A11 - A12 2 A12  C ,     Ø C1 - C1 1 A11 - C2 2 A12  0 - C1 1 C1 - C2 2 C2 where, as before, the blocks which do not enter the analysis are marked with an asterisk. Since M ()  0, this implies that M1 (1 ) - where M1 (1 ) = Ø1 - A11 1 C1 1 - A11 1 A11 C Ø C1 - C1 1 A11 0 - C1 1 C1 (7.14) A A12 2 12 C2 C2  0, (7.13)

Ø1 ). Thereis the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , C fore, M (1 )  0, so if we can show that A11 is stable, i.e., has all its eigenvalues strictly inside the unit circle, it follows that Ø1 + 1 0 , (7.15) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real. As we shall see below this is true without the requirement needed in continuous time that r+1 < r . Ø1 ) also to be balanced, 1 would have to be the minimal solution P1- For (A11 , C1 , C of M1 (P1 )  0, which in turn would require that rank M1 (1 ) = rank M () = m. Due to the extra positive semidefinite term in (7.13), however, this will in general not be the case and therefore 1  P1- will correspond to an external realization, as will 1 - 1  P1+ ; see Lindquist and Picci (1991). Ø1 ) is minimal we need to assume that  is coercive, or, To show that (A11 , C1 , C equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979, Theorem A4.4) that this implies that P+ - P- > 0. (7.16)

In fact, if 0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to coercivity. Coercivity also implies that 0 - CP- C > 0. (7.17)

Ø ) in balanced form, P- =  = P Ø+ and, in view of (3.16), Remark 7.2. With (A, C, C -1 -1 P+ =  . Hence (7.16) becomes  > , which obviously holds if and only if 1 < 1, which in turn is equivalent to H -  H + = 0. Consequently, given the full rank condition 0 > 0, coercivity is equivalent to the past and the future spaces of y having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

Ø ) be in stochastically balTheorem 7.3. Let (7.1) be positive real, and let (A, C, C anced form. Then the reduced-degree function (7.15) obtained via principal subsystem decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so Ø1 , 1 0 ) is a minimal realization of Z1 . is Z1 , and (A11 , C1 , C 2 For the proof we need the following lemma, the proof of which is given in Appendix D. Lemma 7.4. Let the matrix function Z be given by (7.1), where 0 > 0, but where Ø A ) are not necessarily observable, and suppose that (6.6) has two (C, A) and (C, positive definite symmetric solutions, P1 and P2 , such that P2 - P1 > 0. Then Z is strictly positive real. Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is stable. To this end, we note that P is the reachability gramian of (3.7). In particular, Ø ) is stochastically balanced, the reachability gramian of the system (3.18) if (A, C, C equals  so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable. By Remark 7.2, coercivity of  implies that -1 -  > 0, from which it follows 1 that - 1 - 1 > 0 and that 0 > 0. Moreover, By construction, M1 (1 )  0 and -1 M1 (1 )  0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is. To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows by symmetry. By regularity condition (7.17), 0 - C1 1 C1  0 - C C > 0, and consequently, since M1 (1 )  0, 1 satisfies the algebraic Riccati inequality Ø1 - A11 P1 C1 )(0 - C1 P1 C1 )-1 (C Ø1 - A11 P1 C1 )  0, (7.19) A11 P1 A11 - P1 + (C but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable. Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density 1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a unique symmetric P1- > 0 which satisfies (7.19) with equality and for which Ø1 - A11 P1- C1 )(0 - C1 P1- C1 )-1 C1 1- := A11 - (C is stable. It is well-known (Faurre et al., 1979) that P1- is the minimal symmetric solution of the linear matrix inequality M1 (P1 )  0, i.e., that any other symmetric 1 -1 solution P1 satisfies P1  P1- . We also know that M1 (- 1 )  0. Next, since 1 - 1 1 > 0, a fortiori it holds that Q := - 1 - P1- > 0. A tedious but straight-forward calculation shows that Q satisfies 1- (Q-1 - C1 R-1 C1 )-1 1- - Q  0, from which it follows that Q-1 - C1 R-1 C1 - 1- Q-1 1-  0. Cf. Faurre et al. (1979), pp. 85 and 95. (7.20) (7.18)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a  and a   C such that [C1 , I - A11 ]a = 0. and therefore, in view of (7.20), (1 - ||2 )a Q-1 a  0.

Cr

But  is an eigenvalue of the stable matrix A11 , implying that || < 1, so we must have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable. A remaining question is whether there is some balanced order-reduction procedure in discrete time which preserves both positivity and balancing. That this is the case in continuous time implies that the answer is affirmative, but the reduced system cannot be a simple principal subsystem truncation. Ø ) be in stochastically Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, C balanced form. Moreover, given a decomposition ( 2.12) such that r+1 < r , let Ar Cr Ør C r0 = = = = A11 - A12 (I + A22 )-1 A21 C1 - C2 (I + A22 )-1 A21 Ø1 - C Ø2 (I + A22 )-1 A12 C Ø2 - C Ø2 (I + A22 )-1 C2 0 - C2 (I + A22 )-1 C

Ør , r0 ) is a minimal realization of a strictly positive real function Then (Ar , Cr , C Ør + 1 r0 . Zr (z ) = Cr (zI - Ar )-1 C 2 (7.21)

Ør , r0 ) is stochastically balanced with canonical correlation coeffiMoreover, (Ar , Cr , C cients 1 , 2 , . . . , r . To understand why this reduced-order system does preserve both positivity and balancing, note that for   I -A12 (I + A22 )-1 0 I 0 T = 0 -1 I 0 -C2 (I + A22 ) we obtain   Ør - Ar 1 Cr 1 - Ar 1 Ar  C ,    T M ()T =  Ør - Cr 1 A  r0 - Cr 1 C C r r

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the reduced-order system, Mr (1 )  0 and rank Mr (1 )  rank M (). Ør , r0 ) is precisely what one obtains To prove Theorem 7.5 we observe that (Ar , Cr , C Ø 0 ) by the appropriate linear fractional transform to the if one transforms (A, C, C, continuous-time setting and then, after reduction, back to discrete time again as suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions The purpose of this paper is to analyze a class of popular subspace identification procedures for state space models in the theoretical framework of rational covariance extension, balanced model reduction, and geometric theory for splitting subspaces. We have pointed out that these methods are based on the hidden Assumption 2.1 which is not entirely natural and which is in general untestable. The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially equivalent to the Ho-Kalman algorithm in which the basic factorization is performed by singular-value decomposition of a block Hankel matrix of finite covariance data, as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in orthonormal coordinates and allows for theoretical verification of the truncation step. A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of the data equals the algebraic degree. To achieve this, one must assume that the data are generated exactly by an underlying system and that the amount of data is sufficient for constructing an accurate partial covariance sequence the length of which is sufficient in relation to the dimension of the underlying system. Hence it is absolutely crucial that a reliable upper bound of the dimension of the "true" underlying system is available. We stress that these stringent assumptions are not satisfied for generic data, as was pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that the positive degree has no generic value. In fact, just for the moment considering the single-output case, for each p such that r  p   there is a nonempty open set of partial covariance sequences having positive degree p in the space of sequences of length  . Secondly, for any r, it is possible to construct examples of long partial covariance sequences having algebraic degree r but having arbitrarily large positive degree (Theorem 2.4). In Section 7 we proved an open question concerning the preservation of positivity in the original (discrete-time) model reduction procedure of Desai and Pal (1984). Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to the principal subsystem truncation used in van Overschee and De Moor (1993), but not to the one in Aoki (1990). We prove that positivity is preserved provided that the original data satisfies Assumption 2.1, justifying setting the smaller canonical correlation coefficients equal to zero. Unlike the situation in continuous time, this truncation does not preserve balancing. The validity of the corresponding procedure of Aoki (1990) has not been settled. The contribution of this paper is to provide theoretical understanding of these identification algorithms and to point out possible pitfalls of such procedures. Hence the primary purpose is not to suggest alternative procedures. Nevertheless, we would like to point out that a two-stage procedure equivalent to covariance extension followed by model reduction would work on any finite string of data, thus elimination the need for Assumptions 2.1. However, we leave open the question of how such a procedure should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension or some other. Acknowledgment. We would like to thank the referees and the associate editor for the careful review of our paper and for many useful suggestions, which have led to considerable improvements of this paper. References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31≠73. 2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner. 3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley. 4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM J. Control, 13, 162≠173. 5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag. 6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems and Control Letters, 2, 2301≠2312. 7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization of modelling filter. Inter. J. of Control, 50, 2301≠2321. 8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted for publication. 9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40. 10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in Kalman filtering. IEEE Trans. Autom. Control, 36, 563≠579. 11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast filtering algorithms. SIAM J. Control and Optimization, 32, 744≠789. 12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105≠1112. 13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29, 1097≠1100. 14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821≠839. 15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer. 16. Faurre, P. (1969). Identification par minimisation d'une representation Markovienne de processus aleatoires. Symposium on Optimization, Nice. 17. Faurre, P. and Chataigner (1971). Identification en temp reel et en temp differee par factorisation de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt. 18. Faurre, P., M. Clerget, and F. Germain (1979). Op¥ erateurs Rationnels Positifs, Dunod. 19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de r¥ ealisation stochastique. C. R. Academie Sciences Paris 268. 20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York. 21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438≠449. 22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and their L error bounds. Intern. J. Control, 39, 1115≠1193. 23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and its Applications, 50, 277≠319. 24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York. 25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling of time series. Econometric Reviews 11, 379≠396. 26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321≠377. 27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744≠746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference, Tel Aviv, Israel. 29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory, McGraw-Hill. 30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta Polytech. Scand., MA31, 9≠39. 31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499≠513. 32. Kung, S. Y. (1978). A new identification and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705≠714. 33. Larimore, W. E. (1990). System identification, reduced-order filtering and modeling via canonical variate analysis. Proc. 29th Conf. Decison and Control, pp. 445≠451. 34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809≠857. 35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241≠333. 36. Lindquist, A. and G. Picci (1994a). On "subspace methods" identification. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds., Akademie Verlag, 315≠320. 37. Lindquist, A. and G. Picci (1994b). On "subspace methods" identification and stochastic model reduction. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 397≠403. 38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE Trans. Automatic Control, 20, 396≠399. 39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica, 13, 347≠357. 40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and Optimization, 29, 1251≠1287. 41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identification problem. Automatica, 29 , 649-660. 42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica, 30, 75≠93. 43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identification algorithms and its interpretation. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 145≠156. 44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382≠387. 45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes. Linear Algebra and its Applications, 205-206, 957-1003. 46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day. 47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II. Journal fur die reine und angewandte Mathematik, 148, 122≠145. 48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401≠ 421. 49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation. IEEE Trans. Automatic Control, AC-16, 621≠634. 50. Whittle, P. (1963). On the fitting of multivariate autoregressions and the approximate canonical factorization of a spectral density matrix. Biometrica, 50, 129≠134. 51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its Applications, Cambridge U.P. 52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via Ho's algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4. We first give a proof for the special case n = 1. Consider a scalar function 1z+b (A.1) 2z +a with a scalar sequence (1.4) such that 0 = 1. Now it is well-known [see, e.g., Schur (1918), Akhiezer (1965)] that T is positive definite if and only if Z (z ) = |t | < 1 t = 0, 1, 2, . . . ,  - 1 (A.2) where {0 , 1 , 2 , . . . } are the so called Schur parameters. There is a bijective relation between partial sequences (1.1) and partial sequences {0 , 1 , . . . ,  -1 } of the same length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the Schur parameters of (A.1) are generated by the nonlinear dynamical system t+1 = t+1 =
t 2 1 - t - t  t 2 1 - t

0 = 1 (a + b) 2 0 = 1 (b - a) 2

(A.3)

and that Tt becomes singular precisely when there is finite escape. It was also shown in Byrnes et al. (1991) that {t } is generated by a linear system 2/ -1 ut+1 = vt+1 1 0 ut , vt (A.4)

where t = vt /ut and  := (a + b)(1 + ab)-1 . If  is greater than one in modulus, the coefficient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant scalar factor, similar to cos  sin  , - sin  cos   where  := arctan 2 - 1. Hence t is the slope of a line through the origin in R2 which rotates counter-clockwise with the constant angle  in each time step. Consequently, arctan t+1 = arctan t + . Moreover, assuming that 0 > 0, the Schur condition t < 1 will fail as soon as t+1 negative or infinite, as can be seen from the first of recursions (A.3). Hence (A.2) holds if and only if  (A.5) arctan  < . 2 Therefore for a small > 0, take a = 1 - and b = 1 + , yielding a stable Z . Then  2 4 - 2 . We may choose so that  = 2- 2 > 1 and  = arctan 2- 2   << , +1  - arctan 0 . Then (A.5) holds so that T > 0, but we also have where  :=  2  arctan  +1 > 2 so that T +1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function (a + b)n-1 (z ) 1 n (z ) + 1 2 Z (z ) = 1 2 n (z ) + 2 (a + b)n-1 (z ) o polynomials of the first and second kind rewhere {t } and {t } are the Szeg® spectively (Akhiezer, 1965). The function Z has the property that its first n Schur parameters, {0 , 1 , . . . , n-1 }, are precisely the data which uniquely determines n , n-1 , n and n-1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated by t 0 = 1 (a + b) t+1 = 1- 2 2 t+1 =
- t  t 2 1- t +n-1
t+n-1

Hence, we have reduced the problem to the case n = 1. If we choose the initial Schur parameters sufficiently small so that n (z ) and n-1 (z ) are approximately z n and z n-1 , n (z ) + 0 n-1 (z ) is stable if we choose a := 1 - 2 and b := 1 + for some small > 0. Then  > 1 and the proof for the case n = 1 carries through with a trivial modification. Appendix B. The Hilbert space of a sample function Let y = {y(t)}t0 be a zero-mean wide-sense-stationary stochastic process defined on a probability space {, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t,  ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit exists, the m ◊ m matrix function k  k obtained from a particular trajectory is then a bona-fide covariance function. [The continuous-time analog of this property was observed already by Wiener (1933)]. If moreover the sample limit is (almost surely) independent of the particular trajectory and hence necessarily coincides with the "ensemble" covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in the book of Hannan (1970). It is obvious from Birkhoff's ergodic theorem that any (zero-mean) strictly stationary ergodic process is also second-order ergodic. In this Appendix we shall show that the properties of the Hilbert space structure associated to a stationary time series y , defined on page 10, are identical to those of the Hilbert space induced by a second-order ergodic process.10 The two frameworks, i.e., the statistical "time-series" structure and the "probabilistic" structure, are in fact isomorphic. To see this, pick a "representative" trajectory of y, i.e. one in the subset of  (of probability one) for which the limit (1.11) exists. Clearly there will be no loss of generality in assuming that the probability space  of y is the "sample space", of all possible trajectories of y, i.e. the set of all semi-infinite sequences  = {0 , 1 , 2 , . . . }, t  Rm . With this choice, A will be the usual  algebra of cylinder subsets of  and the t:th random variable of the process, y(t), is just the canonical projection function y(t,  ) :   t .
For a process of this kind the Hilbert space H (y) is the closure in L2 (, A, P ) of the linear vector space generated by the scalar random variables   yi (t,  ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence of m ◊  matrices y := {y (k )}k0 as in (3.3). For  in the subset of  where the time averages converge, define the map T , T : a y(t)  a y (t) t  0 a  Rm associating the i:th scalar components of each m-dimensional random vector y(t) of the process to the corresponding i:th (infinite) row of the m ◊  matrix y (t) constructed from the corresponding sample path {y(t,  ); t  Z}. By second-order ergodicity, the set of all such    will have probability measure one and the map T will in fact be norm preserving, since by construction we have t-s = E y(t)y(s) = Ey (t)y (s) , where t is the covariance matrix of y. The map T can then be extended by linearity and continuity to a unitary linear operator T : H (y)  H (y ) which commutes with the action of the natural shift operators (both of which we denote U), in these two Hilbert spaces: H (y) -H (y)  T T  H (y ) -H (y ) This isomorphism allows us to employ exactly the same formalism and notations used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991) in the present statistical setup, where we build estimates of the parameters of models describing the data in terms of an observed time series instead of stochastic processes. This provides a remarkable conceptual unity and admits a straightforward derivation in the style of stochastic realization theory of the formulas in the paper van Overschee and De Moor (1993), there obtained with considerable effort through lengthy and formal manipulations. Appendix C. The invariant form of the Kalman filter Given a stationary stochastic system (3.7), the Kalman filter is usually determined via the matrix Riccati equation Q(t + 1) = AQ(t)A - [AQ(t)C + BD ][CQ(t)C + DD ]-1 [AQ(t)C + BD ] + BB (C.1) where Q(0) = P := E{x(0)x(0) }. Here Q(t) = E{[x(t) - x ^(t)][x(t) - x ^(t)] }, and the Kalman gain is given by K (t) = [AQ(t)C + BD ][CQ(t)C + DD ]-1 . (C.3) (C.2)
U U

These equations of course depend on P , B and D, which vary as the splitting subspace Ø ) is invariant if a uniform choice of bases is made. X varies over , whereas (A, C, C Ø ) and hence However, as shall see, the gain K depends only on the triplet (A, C, C one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

Ø ), and hence are invariant over . Clearly, in view of Theorem 5.1, P- (t), on (A, C, C as defined by (5.12), has this property. Moreover, Q(t) = P - P- (t), and, consequently, in view of (3.9), and the Lyapunov equation P = AP A + BB , P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11). A symmetric argument yields the backward equations. It is easy to see that as Q(t)  Q monotonously, P- (t)  P- , and hence P  P- , as should be. Appendix D. Some deferred proofs Proof of Theorem 5.1. Since X is a splitting subspace for the infinite past H - and the - + := U  H - and H := U  H + . But infinite future H + , by stationarity, X splits H - - + + - + Y  H and Y  H , and hence X splits Y and Y also. (See, e.g., Lindquist and Picci (1985, 1991).) Now, using the projection formula in the footnote of page +  Y+ 16, we have for any b y   -1  1 2 . . . 0 1 . . .  2 3 . . .  +1  1 0 . . .  -1  - + -  .  y E Y b y =b  . . . . .. ..  .    . . . . . . . . . . . . . .   +1 ∑ ∑ ∑ 2 -1   -1 ∑ ∑ ∑ 0 - Ø  (T- )-1 y = b   = b   Ø  are appropriate finite-dimensional observability and constructibility where  and  Ø  such matrices (2.6) of full rank. If  > 0 , there is a minimal factorization H =   - -1 - Ø that  :=  (T ) y has n components, and Ø  > 0. Ø  (T- )-1  E { } =  ^  - , dim X ^  -  n = dim X so, since Therefore, since the components of  belong to X ^ ^ X - is minimal, X must also be minimal and X - be spanned by the components of  . Next, from the backward system (3.14) we see that - Øx = Ø( ) + terms ortogonal to X , y and therefore, by the same projection formula,
- Ø (T - )-1 y - = a . E Y a x( ) = a E {x( )Ø x( ) }    - ^  - , establishing the first of identities Consequently, E Y X = {a  | a  Rn } = X (5.7). The second follows from a symmetric argument. The representation formula (5.8) follows from the minimality of X as a splitting subspace for Y+ and Y- , which, in particular, implies that the constructibility operator, -  ^ Ct := E|Y X : X  X -

X

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n, ^k ( ). there is a unique random variable xk ( )  X whose projection onto Y- is x To show that x(0) form a uniform choice of bases as X varies over , first take X to be the stationary backward predictor space X+ and let x+ ( ) be the unique basis - ^( ) = E Y x+ ( ). Now, let X  be arbitrary. Then, since in U X+ such that x -   + X is a splitting subspace for Y and U X+  U H (Lindquist and Picci, 1991, Proposition 2.1(vi)), we have

X

X

x ^( ) = E Y x+ ( ) = E Y E X x+ ( ), and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all X  , which is a well-known characterization of uniform choice of bases; see Section 6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields the corresponding statement for (5.9).

-

-

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a corresponding partial covariance sequence , where  is large enough for the Hankel Ø ) be the triplet determined matrix H , defined by (1.5), to have rank n. Let (A, C, C from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the covariance data by estimates {0T , 1T , . . . , T } ØT ) be the corresponding triplet obtained via (2.5). of type (6.3), and let (AT , CT , C We want to prove that ØT + 1 0T ZT (z ) := CT (zI - AT )-1 C 2 is strictly positive real for a sufficiently large T . Now, if deg ZT = deg Z , replace  by -1 0  0 in (2.5) in the appropriate , U by U 0 , V by V 0 , and -1 by 0 0 0 0 Ø ) and (AT , CT , C ØT ) have the same dimensions. This will calculation so that (A, C, C ØT , 0T ) can be made arbitrarily close not affect Z and ZT . By continuity, (AT , CT , C Ø to (A, C, C, 0 ) in any norm by choosing T sufficiently large. Thus the same holds for max Z (ei ) - ZT (ei )
[0,2 ]

and hence, since (z ) := Z (z ) + Z (z -1 ) satisfies (3.10), so will T (z ) := ZT (z ) + ZT (z -1 ) for sufficiently large T . Moreover, since |(A)| < 1, we have |(AT )| < 1 by continuity for sufficiently large T . Consequently, there is a T0 such that ZT is strictly positive real for T  T0 . Ø) Proof of Theorem 5.3. Let Z , defined by (1.6), be strictly positive real, and let (A, C, C be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , defined by Ø1 ), is also strictly (7.15) in terms of the principal subsystem truncation (A11 , C1 , C positive real. We want to prove that this property is carried over to rational matrix function Ø )1 ) + 1 0 Z 1 (z ) = (C )1 (zI - (A )11 )-1 (C 2 for  sufficiently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let Q be defined by (5.32). Since the canonical correlation coefficients (5.25) tend to the canonical correlation coefficients (4.12) as   ,   . Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P- (t) tends to Q Q as t   if the initial condition is taken to be P- ( ) =  . Consequently, for any > 0, there is a sufficiently large  such that  -  < 2 and  - Q Q < 2 so that  - Q Q < . Hence Q tends to a limit Q with the property  = Q Q . Using the same argument in the backward direction, the T -1 second of relations (5.33) shows that Q also satisfies  = Q-  Q . Consequently, by the same argument as in the proof of Theorem 4.4, Q is a signature matrix, and hence in particular diagonal. Therefore,
1 -1 Ø Ø )1 )  ((Q )11 A(Q )- ((A )11 , (C )1 , (C 11 , C (Q )11 , C (Q )11 ) as   ,

where (Q )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, Z 1  Z1 . Hence, since Z1 is positive real, then so is Z 1 for  sufficiently large. Ø ) is a minimal triplet. Proof of Lemma 7.4. Let us first consider the case when (A, C, C Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P- and P+ respectively, which, in particular, have the property that P-  P1 and P2  P+ . Then, in view of (7.18), P+ - P- > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem A4.4). Next, let us reduce the general case to the case considered above. If (C, A) is not observable, change the coordinates in state space, through a transformation Ø ), so that Ø )  (QAQ-1 , CQ-1 , QC (A, C, C ^ 0 C= C A= ^ 0 A   ^ Ø= C Ø  , C

^ A ^) is observable. Then, if P1 and P2 have the corresponding representations where (C, P1 = ^1  P   P2 = ^2  P ,  

^2 satisfy the reduced version of the linear matrix ^1 and P it is easy to see that P ^) and that, in this new Ø ) for (A, ^ C, ^ C Ø inequality (6.6) obtained by exchanging (A, C, C ^ ^2 - P ^1 > 0. If (C, Ø A ^ ) is not observable, we proceed setting, (7.18) holds, i.e., P -1 -1 ^2 ^1 and P satisfy the by removing these unobservable modes. First note that P ^ ^ ^ C, ^ C Ø ) by (A ^ , C, Ø C ^ ). Then, dual linear matrix inequality obtained by exchanging (A, changing coordinates in state space so that ^ ~ Ø= C Ø  C ~ ^ = A A  0  ^= C ~ 0 , C

^ Ø A ~ ) observable, and defining with (C, ~ -1  P -1 ^1 P = 1   ~ -1  ^ - 1 = P2 P , 2  

44

ANDERS LINDQUIST AND GIORGIO PICCI

~ ~ C, ~ C, Ø 1 0 ) is a minimal realization of Z . Moreover, P ~1 and P ~2 satisfy we see that (A, 2 the corresponding linear matrix inequality (6.6) and have the property (7.18) in this setting. Hence the problem is reduced to the case already studied above. Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans-1 , mapping formed to the continuous-time setting via a bilinear transformation s = z z +1 the unit disc onto the left half plane so that Zc (s) = Zd 1+s 1-s (D.1)

is positive real in the continuous-time sense if and only if Zd is positive real in the discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al. Ød , 1 0 ) and (Ac , Cc , C Øc , 1 R) are realizations of Zd and Zc (1979)] that, if (Ad , Cd , C 2 2 respectively, we have  Ac = (Ad + I )-1 (Ad - I )    C = 2C (A + I )-1 c  d d (D.2) Ø Ød (A + I )-1  Cc = 2C  d   Ø -C Ød (A + I )-1 C R = 0 - Cd (Ad + I )-1 C d d d and inversely  Ad = (I - Ac )-1 (I + Ac )    C = 2C (I - A )-1 d c  c -1 Ø Ø  = 2 C ( I - A C d c  c)   Ø +C Øc (I - Ac )-1 Cc 0 = R + Cc (I - Ac )-1 C c

(D.3)

Under this transformation the observability gramian and the constructibility gramian Ød , 1 0 ) is Ø A )) are preserved so that (Ad , Cd , C (i.e., the observability gramian of (C, 2 Øc , 1 R) is; see, e.g., p. 1119 in Glover a minimal realization if and only if (Ac , Cc , C 2 (1984). Moreover, coercivity is preserved, and the solution sets of the corresponding linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the reachability gramian of a spectral factor and this gramian is also preserved.) Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact, transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting, all the requirements of Theorem 7.1 are satisfied. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order positive real function thus obtained via (D.3) back to discrete time, the desired result is obtained.

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann Computer Science and Engineering Department Arizona State University Tempe, Arizona 85287-5406 Jean-Claude Derniame Laboratoire lorrain de Recherche en Informatique et Applications LORIA : Bd des Aiguillettes BP 239 54 506 Vandoeuvre Cedex Abstract
Only recently has the research community started to consider how to make software process models interoperable and reusable. The task is difficult. Software processes are inherently creative and dynamic, difficult to define and repeat at an enactable level of detail. Additionally, interoperability and reusability have not been considered important issues. Recent interoperability and reusability solutions advocate the development of standard process model representations based on common concepts or generic schemas, which are used as a basis for translating between heterogeneous process representations. In this paper we propose an alternative approach through the development of process-based components. We present the Open Process Components Framework, a componentbased framework for software process modeling. In this approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of process representations, an explicit representation of process state, and an extendable set of class relationships. their benefits, systems based on these formalisms create enactable process models which are not interoperable nor reusable with one another. The prevailing solution is to advocate an intermediary standard process representation and provide translations for interoperability and reuse. We do not believe this approach is scalable and defeats the purpose of using heterogeneous process representations. We advocate an object-oriented, component-based philosophy for providing software process interoperability and reuse. This paper presents Open Process Components, a component-based framework for software process definition and enactment. In this framework, components are well-encapsulated representations of process entities that interact in meaningful ways. The framework is solidly founded on mature concepts in the software process field, and yet is extendable so that process models may be customized in a particular domain. A componentized view of process representations results in easier process definition, modularized process enactment, natural interoperability, and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable. Instances vary according to constantly changing demands of specific projects. Fully elaborating a software process model to an enactable level of granularity is often too tedious, time-consuming, and costly[4]. Motivated by the need for interoperability and reuse, we advocate applying component-based techniques to software process modeling. Constructing software process models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined components.

1.0 Introduction
Since Osterweil's proposal[12] for automating the software process a decade ago, there has been significant debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets [9], rule-based formalisms [1,8,13], process programming languages [15], event-based representations [3,6,10], object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research under Award number N00014-97-1-0872

A component-based approach:

2.1.2 Process Component States
The elements of the meta-model appear in most process models. Each model requires a different enactment service to interpret the representation and execute the process. Regardless of the formalism employed and the interpreter used, all models define actions on the entities within the process domain, which effect the states of those entities. The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes finite state machines as part of its basic abstractions. Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for process modeling is not unique[7,11]. The OPC framework defines a basic set of states and transitions for Process and Activity components. These include states such as executing, suspended , and aborted, with corresponding transitions between states defined by actions such as startProcess, suspendProcess, and abortProcess. Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object's unique behavior when interacting with other components within the framework. The current class definition for state transition graphs include operations to add and remove states and allowable transitions between states, making a component's state and behaviors affecting state explicit and manipulable.

∑ ∑ ∑ ∑

avoids deep integration of semantic models handles the natural complexity of software processes, responds to dynamic software processes, and facilitates reuse, minimizing one-shot process models. Component-based process modeling requires a framework for developing components. The framework must identify process entities, define meaningful interactions between entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides a foundation for developing, integrating, maintaining, and reusing a variety of process representations. The framework defines basic abstractions of the problem space that can be specialized. Yet, the framework must make some commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this meta-model as a basis for translation between process models, we use it as a foundation for identifying elements of the process space for componentization, and for defining meaningful ways in which process components interact.
has_sub

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a layered, three-tier software architecture (Figure 2). The Framework Layer defines classes and interfaces modeling process entities derived from the OPC meta-model. The Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component Layer extends representations to particular domains. It is from this layer that actual Process component objects are instantiated. A process model in the OPC framework is a set of components, realized as objects of Component Layer classes, and a set of relations between those components, created under the constraints of the Framework Layer, implemented using Representation Layer semantics. Figure 2 shows example classes at each layer of abstraction for the meta-model element Process. The Framework Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation Layer is comprised of class definitions for specific process representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets (PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any other representations we wish to encapsulate. The Compo-

role
assigned_to can_perform

activity

has_input has_output

product

consists_of

has_version has_variant
has_sub

agent

process

FIGURE 1. The Open Process Components Framework Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and Agent), and artifacts (Product). The meta-model defines the "rules of engagement" for components. It identifies what component types interact with what other component types under what relationships. These relationships are not static; process components and component relationships are highly dynamic during the course of the component's life cycle.

Framework Layer

Process

Process
Representation Layer ECAProcess OrderedProcess PetriNetProcess RuleProcess

PDLProcess

Component Layer Bug Fix Code Module Integration Test Design

Code Module

Peer Review Stress Test

FIGURE 2. Object-oriented class diagram for Process components

nent Layer contains type definitions for actual process types. The dashed lines between layers in Figure 2 denotes that the Representation and Component Layers in fact can have many levels. This allows for multiple ways in which to extend and specialize the framework. The first step identifies a base set of classes and interfaces at the Framework Layer. The next step is to construct encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are encapsulated behind the interfaces inherited from the Framework Layer. For example, the implementation may come from a COTS process tool. Finally, components defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used since inheritance would tie the component's type to its implementation. Component Layer objects are configurable. Component Layer classes represent generic process models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product, Role, or Agent) fully specified and bound is part of an instantiated process model[5]. Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given representations. Delegating Representation Layer classes provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi's levels of process specialization[5].

software development support. PCIS2 services include Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA. The process support services in PCIS2 are based on the OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification, known as jFlow[11], submitted to the OMG. The jFlow specification is largely an "object-ization" of existing WfMC interfaces[16]. This is not a drawback, but one of the strengths of the OMG's approach to adopting and adapting existing technology. The jFlow specification improves upon the original WAPI specifications by defining appropriate interactions between objects to gain interoperability and maintainability of workflow systems. The PCIS2 specification is object-oriented from the ground up, but has borrowed some of the jFlow concepts in order to maintain compliance with emerging standards. PCIS2 and the jFlow specification differ in three areas. First, PCIS2 supports dynamic processes through ad hoc process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2 incorporates support for the metaprocess, by defining views on its services for controlling, defining, performing, and monitoring processes. jFlow only defines interfaces for performing (enacting) and monitoring workflows. It should also be pointed out that jFlow identifies concepts not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these specifications, they are largely complementary and both provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The authors propose the construction of software process components for producing process artifacts. A "software process component" is essentially a process model fragment written in some Process Modeling Language (PML). Components are dynamically combined to construct complete process models through interface types and their respective "connectors ports". The authors correctly motivate the need to eliminate monolithic process systems and instead provide reuse and integration capabilities for process representations. However, the approach lacks adherence to foundational concepts, such as those used in OPC (see Section 2.1). The three-tier layering of the OPC framework provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite their differences, the Pynode component approach is simi-

3.0 PCIS2 Process Services
The Open Process Components framework is currently used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

lar in philosophy and motivation to the OPC framework, and appears to be at roughly the same level of maturity. Results of these two experiments will be very useful to the software process modeling community. A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14]. The authors view the object space and the process space at different levels. The object space is data-oriented, whereas the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that brings together the object level and the process level together. Shams-Aliee and Warboys[14] also advocate modeling a process as a collection of objects or components. However, we find the distinction between the object level and the process level unnecessary. In particular, we do not agree that the object level is a data-oriented model. In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of these objects as defined by their interfaces. OPC merges objects and processes into components through an explicit representation of process state contained in the component. We propose a full object-oriented framework that includes class definitions, inheritance, and rules for component interaction. This merging of objects and processes into a complete component-based model allows OPC the full potential to achieve interoperability and reuse by being independent of any process modeling formalism.

Software Process (ICSP4). December, 1996. [3.] Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995. Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R. Towards a Reference Framework for Process Concepts. Proc. of EWSPT'92, pp. 3-17, Trondheim, Norway. September 1992. Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. Derniame, J.C. Life Cycle Process Support in PCIS. Proc. of the PCTE `94 Conference. 1994. Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994. Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proc. of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Object Management Group. jFlow Joint Submission. OMG Document Number bom/98-06-07. July 4, 1998. Osterweil, L. Software Processes are Software Too. Proc. of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987. Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings of the First World Conference on Design and Process Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for Design and Process Science, Austin, TX. December 1995. Sutton, S., Heimbigner, D., and Osterweil, L. Language Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium on Software Development Environments, Irvine, CA. December 1990. Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003, January 1995.

[4.]

[5.]

[6.]

[7.] [8.]

[9.]

[10.]

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software processes. This framework identifies common concepts in the research community and defines an object-oriented framework for applying these concepts. This framework is currently employed in the construction of a software architecture for support distributed software development. This approach, together with related efforts in the field of workflow, makes the important contribution that the software process automation field is maturing to the point that efforts such as the one described herein can be attempted. Despite whether the reader agrees with the design of this framework, providing interoperability and reusability will overcome one of the serious hurdles preventing wide scale deployment of software process automation technology.

[11.] [12.]

[13.]

[14.]

6.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proc. of EWSPT `92, Trondheim, Norway. September, 1992. Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

[15.]

[16.]

[2.]

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann
Computer Science and Engineering Department
Arizona State University
Tempe, Arizona 85287-5406
Jean-Claude Derniame
Laboratoire lorrain de Recherche en Informatique et Applications
LORIA : Bd des Aiguillettes
BP 239 54 506 Vandoeuvre Cedex
Abstract
Only recently has the research community started to consider how to make software process models interoperable
and reusable. The task is difficult. Software processes are
inherently creative and dynamic, difficult to define and
repeat at an enactable level of detail. Additionally,
interoperability and reusability have not been considered
important issues. Recent interoperability and reusability
solutions advocate the development of standard process
model representations based on common concepts or
generic schemas, which are used as a basis for translating
between heterogeneous process representations. In this
paper we propose an alternative approach through the
development of process-based components. We present the
Open Process Components Framework, a componentbased framework for software process modeling. In this
approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of
process representations, an explicit representation of process state, and an extendable set of class relationships.

1.0 Introduction
Since Osterweil‚Äôs proposal[12] for automating the software process a decade ago, there has been significant
debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets
[9], rule-based formalisms [1,8,13], process programming
languages [15], event-based representations [3,6,10],
object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research
under Award number N00014-97-1-0872

their benefits, systems based on these formalisms create
enactable process models which are not interoperable nor
reusable with one another. The prevailing solution is to
advocate an intermediary standard process representation
and provide translations for interoperability and reuse. We
do not believe this approach is scalable and defeats the
purpose of using heterogeneous process representations.
We advocate an object-oriented, component-based philosophy for providing software process interoperability and
reuse. This paper presents Open Process Components, a
component-based framework for software process definition and enactment. In this framework, components are
well-encapsulated representations of process entities that
interact in meaningful ways. The framework is solidly
founded on mature concepts in the software process field,
and yet is extendable so that process models may be customized in a particular domain. A componentized view of
process representations results in easier process definition,
modularized process enactment, natural interoperability,
and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable.
Instances vary according to constantly changing demands
of specific projects. Fully elaborating a software process
model to an enactable level of granularity is often too
tedious, time-consuming, and costly[4].
Motivated by the need for interoperability and reuse, we
advocate applying component-based techniques to software process modeling. Constructing software process
models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined
components.

A component-based approach:

2.1.2 Process Component States

‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

The elements of the meta-model appear in most process
models. Each model requires a different enactment service
to interpret the representation and execute the process.
Regardless of the formalism employed and the interpreter
used, all models define actions on the entities within the
process domain, which effect the states of those entities.
The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes
finite state machines as part of its basic abstractions.
Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for
process modeling is not unique[7,11]. The OPC framework
defines a basic set of states and transitions for Process and
Activity components. These include states such as executing, suspended, and aborted, with corresponding transitions between states defined by actions such as
startProcess, suspendProcess, and abortProcess.
Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object‚Äôs unique behavior when
interacting with other components within the framework.
The current class definition for state transition graphs
include operations to add and remove states and allowable
transitions between states, making a component‚Äôs state and
behaviors affecting state explicit and manipulable.

avoids deep integration of semantic models
handles the natural complexity of software processes,
responds to dynamic software processes, and
facilitates reuse, minimizing one-shot process models.
Component-based process modeling requires a framework
for developing components. The framework must identify
process entities, define meaningful interactions between
entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides
a foundation for developing, integrating, maintaining, and
reusing a variety of process representations. The framework defines basic abstractions of the problem space that
can be specialized. Yet, the framework must make some
commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon
three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a
per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this
meta-model as a basis for translation between process
models, we use it as a foundation for identifying elements
of the process space for componentization, and for defining
meaningful ways in which process components interact.
has_sub

role

activity
assigned_to

can_perform

has_input

product

has_output
consists_of

has_version
has_variant

agent

process

has_sub

FIGURE 1. The Open Process Components Framework
Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and
Agent), and artifacts (Product). The meta-model defines
the ‚Äúrules of engagement‚Äù for components. It identifies
what component types interact with what other component
types under what relationships. These relationships are not
static; process components and component relationships
are highly dynamic during the course of the component‚Äôs
life cycle.

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a
layered, three-tier software architecture (Figure 2). The
Framework Layer defines classes and interfaces modeling
process entities derived from the OPC meta-model. The
Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component
Layer extends representations to particular domains. It is
from this layer that actual Process component objects are
instantiated. A process model in the OPC framework is a
set of components, realized as objects of Component Layer
classes, and a set of relations between those components,
created under the constraints of the Framework Layer,
implemented using Representation Layer semantics.
Figure 2 shows example classes at each layer of abstraction
for the meta-model element Process. The Framework
Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation
Layer is comprised of class definitions for specific process
representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets
(PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any
other representations we wish to encapsulate. The Compo-

Process

Framework
Layer

Process
Representation
Layer

PDLProcess

ECAProcess

OrderedProcess
PetriNetProcess

Component
Layer
Bug Fix

RuleProcess

Code Module
Code Module

Integration Test

Design

Peer Review
Stress Test

FIGURE 2. Object-oriented class diagram for Process
components

nent Layer contains type definitions for actual process
types. The dashed lines between layers in Figure 2 denotes
that the Representation and Component Layers in fact can
have many levels. This allows for multiple ways in which
to extend and specialize the framework.
The first step identifies a base set of classes and interfaces
at the Framework Layer. The next step is to construct
encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are
encapsulated behind the interfaces inherited from the
Framework Layer. For example, the implementation may
come from a COTS process tool. Finally, components
defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used
since inheritance would tie the component‚Äôs type to its
implementation. Component Layer objects are configurable. Component Layer classes represent generic process
models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product,
Role, or Agent) fully specified and bound is part of an
instantiated process model[5].
Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given
representations. Delegating Representation Layer classes
provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between
components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi‚Äôs levels of process specialization[5].

3.0 PCIS2 Process Services
The Open Process Components framework is currently
used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

software development support. PCIS2 services include
Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA.
The process support services in PCIS2 are based on the
OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification,
known as jFlow[11], submitted to the OMG. The jFlow
specification is largely an ‚Äúobject-ization‚Äù of existing
WfMC interfaces[16]. This is not a drawback, but one of
the strengths of the OMG‚Äôs approach to adopting and
adapting existing technology. The jFlow specification
improves upon the original WAPI specifications by defining appropriate interactions between objects to gain
interoperability and maintainability of workflow systems.
The PCIS2 specification is object-oriented from the ground
up, but has borrowed some of the jFlow concepts in order
to maintain compliance with emerging standards.
PCIS2 and the jFlow specification differ in three areas.
First, PCIS2 supports dynamic processes through ad hoc
process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared
and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2
incorporates support for the metaprocess, by defining
views on its services for controlling, defining, performing,
and monitoring processes. jFlow only defines interfaces for
performing (enacting) and monitoring workflows.
It should also be pointed out that jFlow identifies concepts
not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these
specifications, they are largely complementary and both
provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The
authors propose the construction of software process components for producing process artifacts. A ‚Äúsoftware process component‚Äù is essentially a process model fragment
written in some Process Modeling Language (PML). Components are dynamically combined to construct complete
process models through interface types and their respective
‚Äúconnectors ports‚Äù. The authors correctly motivate the
need to eliminate monolithic process systems and instead
provide reuse and integration capabilities for process representations. However, the approach lacks adherence to
foundational concepts, such as those used in OPC (see
Section 2.1). The three-tier layering of the OPC framework
provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite
their differences, the Pynode component approach is simi-

lar in philosophy and motivation to the OPC framework,
and appears to be at roughly the same level of maturity.
Results of these two experiments will be very useful to the
software process modeling community.
A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14].
The authors view the object space and the process space at
different levels. The object space is data-oriented, whereas
the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that
brings together the object level and the process level
together. Shams-Aliee and Warboys[14] also advocate
modeling a process as a collection of objects or components. However, we find the distinction between the object
level and the process level unnecessary. In particular, we
do not agree that the object level is a data-oriented model.
In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of
these objects as defined by their interfaces. OPC merges
objects and processes into components through an explicit
representation of process state contained in the component.
We propose a full object-oriented framework that includes
class definitions, inheritance, and rules for component
interaction. This merging of objects and processes into a
complete component-based model allows OPC the full
potential to achieve interoperability and reuse by being
independent of any process modeling formalism.

Software Process (ICSP4). December, 1996.
[3.]

Ben-Shaul, I. and Kaiser, G. An Interoperability Model for
Process-Centered Software Engineering Environments
and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995.

[4.]

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T.,
Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the
NSF Workshop on Workflow and Process Automation in
Information Systems, Athens, GA, May, 1996.

[5.]

Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R.
Towards a Reference Framework for Process Concepts.
Proc. of EWSPT‚Äô92, pp. 3-17, Trondheim, Norway. September 1992.

[6.]

Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling
and Technology, A. Finklestein, J. Kramer, and
B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994.

[7.]

Derniame, J.C. Life Cycle Process Support in PCIS. Proc.
of the PCTE ‚Äò94 Conference. 1994.

[8.]

Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994.

[9.]

Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net
Based Software Process Modeling Language. Proc. of the
6th International Workshop on Software Specification and
Design, Como, Italy. September 1991.

[10.]

Melo, W.L. and Belkhatir, N. TEMPO: A Support for the
Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers,
North Holland. 1994.

[11.]

Object Management Group. jFlow Joint Submission.
OMG Document Number bom/98-06-07. July 4, 1998.

[12.]

Osterweil, L. Software Processes are Software Too. Proc.
of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987.

[13.]

Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the
14th International Conference on Software Engineering,
pp. 262-279. May, 1992.

[14.]

Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings
of the First World Conference on Design and Process
Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for
Design and Process Science, Austin, TX. December 1995.

[15.]

Sutton, S., Heimbigner, D., and Osterweil, L. Language
Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium
on Software Development Environments, Irvine, CA. December 1990.

[16.]

Workflow Management Coalition. The Reference Model.
WfMC Document Number TC00-1003, January 1995.

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software
processes. This framework identifies common concepts in
the research community and defines an object-oriented
framework for applying these concepts. This framework is
currently employed in the construction of a software architecture for support distributed software development.
This approach, together with related efforts in the field of
workflow, makes the important contribution that the software process automation field is maturing to the point that
efforts such as the one described herein can be attempted.
Despite whether the reader agrees with the design of this
framework, providing interoperability and reusability will
overcome one of the serious hurdles preventing wide scale
deployment of software process automation technology.

6.0 References
[1.]

Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G.
PEACE: Describing and Managing Evolving Knowledge
in Software Process. Proc. of EWSPT ‚Äò92, Trondheim,
Norway. September, 1992.

[2.]

Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified
Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

Towards Target-Level Testing and Debugging Tools for Embedded Software
Harry Koehnemann, Arizona State University
Dr. Timothy Lindquist, Arizona State University

Abstract

The current process for testing and debugging
embedded sojware is ine~ective at revealing errors. There
are currently huge costs associated with the validation of
embedded applications. Despite the huge costs, the most
dl~cult errors to reveal and locate are found extremely late
in the testing process, making them even more costly to
repm‚Äùr. This paper first presents a discussion of embedded
testing research andpractice. This discussion raises a need
to improve the existing process and tools for embe&@i
testing as well as enable better processes and tools for the
jWure. To fmilitate
this improvement, architectural and
software capabilities which support testing and &bugging
with minimal intrusion on the executing system must be
developed. Execution visibility and control must come
@om the underlying system, which should ofJer interjbces
to testing and debugging tools in the same numner it offers
them to a compiler. Finally we propose txtenswns to the
underlying system, which consists of adiiitions to both the
architecture and run-time system that will help reulize
target-level tools.
1. Introduction
Software validation involves many activities that
take place throughout
the lifecycle
of soft w are
development.
A substantial
portion
of the validation
process is software testing, which is the development
of
test procedures and the generation and execution of test
eases.
Notice we are not only concerned
with the
generation of a test case, but are also concerned with how
that test is executed. Therefore, a test case is not simply
composed of inputs to a system, but rdso includes any
environmental
factors.
Other research has examined the
issues behind test case selection, but few are addressing the
problems that surround the execution of those test cases.
The goal of this paper is to identify
the problems
associated with test case execution for embedded systems
and to propose solutions for making embedded testing more
effective at revealing errors.
1.1

Testing
and Debugging
Process
Many of the activities, tools, and methods used
during software testing are shared by software debugging.
Software testing is concerned with executing a piece of
software in order to reveal errors, while software debugging
is concerned with locating and correcting the cause of an
error once it has been revealed.
Though
these two
activities are often referenced separately, their ac$ivi$ies are
tightly coupled and share many common features.
Permission to copy without
fee all or part of this material is granted
provided that the copies are not made or distributed
for direct commercial
advantage, the ACM copyright notice and the title of the publication
and its
date appear, and notice is given that copying
is by permission
of the
Association
for Computing
Machinery.
To copy otherwise
or republish,
requires a fee and/or specific permiss~m.

During debugging, a developer must recreate the
exact execution scenario that revealed the fault during
testing.
Not only must the code execute
the same
instruction sequences, but all environmental
variants must
be accounted for during the debugging session. In addition,
the tools assisting
in the debugging
process
must
providing a &veloper
with a certain degree of execution
visibility
and control while not impacting
the execution
behavior of the program.
1.2

Embedded
Systems
The testing and debugging
process is greatly
restricted by embedded systems. Embedded applications are
among the most
complex
software
systems
being
developed today. Such software is often constrained by
‚óè Concaumnt designs
‚óè RcaI-time
constraints
‚óè l%lbedded
target Imvilrmments
‚óè Distributed
hardware ambitectures
‚óè Device control dependencies
Each of these properties of embedded software severely
restrict
execution
visibility
and
control,
which
conseqmdy
restricts the testing and debugging process.
Our current methods and tools for software testing and
debugging require a great deal of computing
resources.
Such resources are not available on the target environment.
Therefore, a large gap exists between the methods and tools
used during evaluation on the host and those used on the
target. Unfortunately,
mauy errors are only revealed during
testing in the target environment.
Because of the above issues, concerns are raised
over the effectiveness of software validation for embedded
systems.
Embedded
applications
are responsible
for
controlling physical devices and their correct execution is
critical in avoiding and/or recovetig
from device failure.
Often these physical devices control life-critical
processes,
making the embedded software a key element of a lifecritical system. Software failure can lead to system failure,
which in turn could lead to loss of life.
In addition, designers are increasing their use of
embedded software to conuol the physical elements of large
systems. This rate of increase is likely to increase as the
cost for embedded controllers becomes cheaper and more
attractive when compared with other mechanical techniques.
Computer
networks
are fast replacing
point-to-point
wiring,
due to the networks
light
weight,
easy
cotilgurability
and expansibility,
and lower
design
complexity.
The advancement
in the complexity
of
problems
addressed
by software
in these types of

01993

ACM

0-89791-621

-2/93/0009--0288

1.50

applications
may soon be limited
satisfy reliability needs and concerns.

by our inability

to

2. Software
Testing
The software testing phase is concerned with
executing a software program in order to reveal errors.
Software testing for embedded systems takes place in four
basic stages:
1) Module Level Testing
2) Integration Testing
3) System Testing
4) Hardware/Software
Integmtion Testing
The first three testing stages are typical of any software
product.
Testing begins with exercising each soft ware
module and concludes when the software is shown to meet
system specifications
by passing some rigorous
set of
system tests. The fourth phase is unique to embedded
systems. The software must not only be correct, but must
also interface properly with the devices it is controlling.
Testing
literature
contains
countless
methodologies,
techniques,
and tools that support the
software
testing process.
They range from software
verification
and program proving
to random test case
selection.
All testing
methods
indirectly
apply
to
embedded systems, as they do all software.
Of particular
interest to this paper are those techniques that address the
problems identified for embedded software - concurrency,
real-time
constraints,
embedded
environment,
etc.
Unfortunately,
there exists little research into the unique
with testing
embedded
software.
problems SSSOCilltd

techniques, must examine a large set of statw and therefore
must constrain itself to small, simple programs.
Research in dynamic testing of concurrent Ada
programs has largely focused on the detection of deadlocks
~emb85],
the saving of event
histories
KeDo85,
Maug85],
and other tec.huiques that passively watch a
program execute then allow the execution sequences to be
replayed after a failure has been detected.
Hanson Eans78] was among the first to discuss
run-time
control of concment
programs.
In order to
regulate
the sequences of events, he assigned each
concurrent event in the test program a unique time value.
He then introduced a test clock that regulated the system
during execution. A given event could only execute if it‚Äôs
time was greater than that of the clock.
Tai ~ai86, Tai91] extended Hanson‚Äôs work to the
Ada programnn ‚Äúng language.
His method takes an Ada
program P and a rendezvous ordering R and produces a new
Ada program P‚Äô such that the intertask communication
in
P‚Äô is always R. A similar approach was used in Koeh89]
to apply these techniques to testing and debugging tools.
This work addressed the facl that in order to,
testa specific
program state, values in a program
may need to be
modified during run-time.
Modification
o~f the program
state is a capability provided by any debugging tool and is
a required property of a tool debugging tasked programs. It
is important to note that both techniques explicitly perform
rendezvous scheduling, removing those decisions from the
run-time system and placing control in the hands of the
tool.

2.1

2.2

Testing
Concurrent
Systems
Concurrency increases the difficulty
of software
testing,
Given a concurrent program and some set of
input, there exists an unmanageably
large set of legal
execution sequences the program will take. Furthermore,
subsequent execution
with the same input may yield
different,
yet correct results due to differences
in the
operating environment.
This is all complicated
by Ada‚Äôs
nondetermins tic select construct. Therefore, when testing
concurrent software, we are not only concerned with a valid
resuk but must also be concerned with how the program
arrived at that result.
Since multiple executions of a concurrent program
may yield different results, it is not enough to ensure that
the system produces the correct output for a given input.
One must also ensure that the system always produces an
acceptable output for each execution sequence that is legal
under the language definition.
Without sufficient control
over program execution,
there is no way of ensuring a
given test is exerasing the code it was intended to test.
Taylor and Osterweil
~ay180]
examined static
analysis of concument programs,
However, this research
considered processes in isolation and does not consider
interprocess cmrummication.
Taylor later extended this
work to Ada and a subset of the Ada rendezvous mechanism
~ay183]. Through this static aualysis technique, one could
determine aIl parallel actions and states that could block a
task from executing.
This method, as with most static

Non-intrusive
testing
Intrusion plays a significant role in the testing and
debugging of embedded software. Any technique used to
raise execution visibility
or provide for program control
must not interfere with the behavior of the teat program.
Embedded applications have strict timing requirements and
any intrusion on a test execution will likely make that test
void.
Intrusion
is typical for host-based
testing, but
becomes a large problem
for target-level
testing and
debugging activities.
The above approaches
address the need for
visibility,
control, and predictability
for testing concurrent
software.
However,
they are all intrusive
and use
instrumentation
(inserting probes into a usem program and
rewriting
certain constructs before submission
to the
compiler) to gather run-time information
and to control
~gram
exmtion.
After the probes are added, the user‚Äôs
object code is linked with the rest of the debugging system
and then executed under test. This additional
code has a
serious impact on the execution behavior of the program.
Instrumentation
is not appropriate
for testing real-time,
embedded applications.
A non-intrusive
debugger for Ada is proposed in
[Gil188]. A separate processor executes the testing system
and communicates with the target processor through some
special purpose hardware.
Lyttle and Ford ~ytt90]
have
also implemented a non-intrusive
embedded debugger for
Ada. Their tool provides monitoring,
breakpoints,
and

289

of hardware and the run-time system is called upon to
bridge the impending gap. No argument is made as to the
rate of increase identified
by the line slopes; nor is an
argument ma& that these increases are even linear.

display facilities
for executing embedded applications.
While these efforts provide an excellent start towards targetlevel tools, they do have severe limitations.
These
implementation
do not deal with high level activities such
as task interactions and are only concerned with items that
can be translated from monitoring
system bus activity. As
discussed later in Chapter 5, techniques dependent on bus
activity will likely fail for future architecture designs. In
addition,
many of the error detwted
in the target
environment are indeed concerned with high-level activities
(process scheduling
and interactions,
fault handling,
interrupt response, etc.).
Other real-time,
embedded
tools have been
proposed for crossdevelopment
environments.
They can
typically
be classified into one of the following
three
categories: 1) ROM monitors, 2) Emulators, and 3) Bus
monitors.
These types of tools will be further discussed
later in this paper.

c
o
m
P
1
e
x

i~
t

Y

constructs

Hardware

Time
Figure 2.1

2.3 Impact

of the Underlying
System
One of the large problems with testing concurrent
systems
is dealing
with
abstraction.
The Ada
programming
language
abstracts concurrent
activities
through task objects ~D83].
Tasks allow a developer to
abstract the concepts of concurrency
and interprocess
cxmmmnication
and discuss them at a high level. The
burden of implementation
is then placed on the compiler,
and typically the run-time system.
While abstraction
is a powerful
design tool, it
leads to significant
prthlems
during the testing phase of
software development.
Implementation
details become
buried in the underlying system. At the development level,
this high degree of abstraction is appropriate.
However,
abstraction complicates the testing process. Not only are
we concerned with implementation
details, but we must
aIso control them to demonstrate that certain properties
about a program will hold for every legal
execution
scenario.
Without
sufficient
control
over program
execution, there is no way of ensuring that a spedc
test is
exerasing the code it was intended to evaluate. In addition,
cmmxt operation
in one environment
(host) does not
necessarily imply comet operation in another (target) due
to implementation
difference in the underlying system.
The underlying
system is composed of two parts,
the features of the hardware architecture and the operations
provided by the run-time system. As language constructs
become more abstract, compilers are required to generate
more code to implement them. There is no longer a trivial
mapping from language construct to machine instruction.
Rather, the compiler must provide an algorithmic
solution
in order to implement these high level constructs. Those
solutions
exist as operations
in the run-time
system.
Rather than generate code for these constructs, the compiler
generates a call to a run-time system operation or servim.
As the constmcts
become
more
abstract,
compilers
develop
an increasing
dependency
on the
underlying
system. This increase in shown in figure 2.1.
As new constructs
are introduced
to programming
languages, their increase in abstraction is greater than that

L=

Language

Embedded
systems raise many problems
for
software testing and debugging.
Such software typically
must deal with concurrency,
real-time
constraints,
au
embedded
target environment,
distributed
hardware
architectures,
and a great deal of hardware-software
interfaces for controlling
externaI devices. These issues
tdone do not provide a complete view of the problems
SyStetUS are
extcotttttered by embedded testing. bbedded
typically developed on custom hardware configurations
m@ng
that each system introduces
it‚Äôs own unique
problems. Tools and techniques that apply to one are not
generally applicable on another, which leads to ad hoc
approaches to integration and system testing of embedded
software. The program is executed for some length of time
and continual y bombarded with inputs in an attempt to
show it adheres to some speeifkation,
3.1

Current
state of embedded
testing
As described
earlier, the testing
process for
embedded systems consists of 4 phases that conclude with
Hardware/Software
(H/S) Integration
During
H/S
integration
testing, device and timing related errors are
reveakd. These errors eneompass problems such as:
‚óè incorrect
handling of interrupts
‚óè distributed
communication problems
‚óè incorrect
ordering of concumen t events
‚óè resource contention
‚óè incorrect
use of device protocols and timing
‚Äú incomect response to failures or transients
These errors are often extremely difficult
problems to fix
and often require significant modifications
to the software
system.
In addition,
software is for~d
to conform
to
custom hardware that may itself have errors. As stated
above, H/S integration
is the last phase of testing for an
embedded system. Since errors are much cheaper to fix the
earlier they are revealed,. why would one wait until the last
phase of product development to reveal the most diftlcult to

290

locate, costly errors to fix? Our goal should be to reveal
these errors as early as possible.
Unfortunately,
target
level testing tools have yet to become a reality.
The target processor of au embedded computer is
typically minimal in function and size. It is only a small
portion of a larger system, whose goals are to minimize
cost and space. Therefore, target hardware of au embedded
systems will not support software development
nor any
development tools. To resolve this problem, the source is
developed on a larger host platform and cross axnpilers and
linkers are used to generate code and download it to the
target processor.
Consequently,
two environments
exist in our
development process, the host environment
and the target
environment,
each having
completely
different
functionality
and interface to a user. Tools that run on the
host provide a high level interface and give users detailed
information
on and control over their program execution.
However, little is provided on the target. Typically,
the
best information
obtainable is a low-level execution trace
provided by an in-circuit emulator.

3.2

Current
Solutions
Approaches to dealing with the above problems
can be divided
into hardware
solution
and software
solutions. The hardware solutions are attempts at gaining
execution visibility
and program control and include the
bus monitors, ROM monitors, and in-circuit emulators. A
bus monitor gains visibility
of an executing program by
observing
data and instructions
transferred
across the
system bus. With a ROM monitor,
debugger code is
placed into ROM on the target board. When a break point
is encountered,
control is transfered to the debug code
which can accept commands from the user to examine and
change the program‚Äôs state. Finally, an in-circuit emulator
connects with a host system across an ethernet connection.
At the other end, a probe replaces the processor on the
target board. The emulator then simulates the behavior of
the processor in (ideally)
real-time,
which allows the
emulator to tell the outaide world what it‚Äôs doing while it‚Äôs
doing it.
The
hardware
solutions
have
mini
m al
effectiveness for software development.
They can only
gather information
based on low-level machine data. The
developer must then create the mapping between low-level
system eventa and the entities defiied in the program. That
-ping
is the implementation
strategy chosen by a given
compilation
system and becomes severely complicated for
abstractions such as tasks Maintaining
an understanding of
the mapping is extremely difficult and cumbersome.
The software solutions cart be viewed as attempts
to reduce the tremendous costs of testing on the target.
Several factors determine how a pitxe of software is tested
1) Level of criticality of software module
Each software module is assigned a different level of
criticality
based on it‚Äôs importance
to the overall
operation of the system.
2) Test platform availability

Typically,
there will exist several test environments
available to test a piece of soft ware, each providing
a
closer approximation
to the actual target environment:
‚óè Host-baaed sours
level debugger
‚óè Host-based
instruction set simulator
‚óè Target emulator
‚óè Integrated
validation faality
3) Test Classification
The tests to be performed
can be categorized
to
determine what they are attempting
to demonstrate.
The goal of a test plays a large role in determining
the
platform on which it will execute. Some examples are
shown below.
‚óè Algorithmic
‚óè Inter-module
‚óè Intra-module
‚óè Performance
‚óè HE
integration
‚óè InttX-cabinet
Each of these factors play a role in assigning program
modules to the various test platforms
based on some
criteria that might contain the following:
‚óè Type of software
‚óè Hardware requirements
‚óè Test chssifkation
‚óè Platform
availability
‚óè Coverage requirements
‚óè Test support software
availability (drivers, stubs)
‚óè Certification
Requirements
‚óè Level of effort required for test
This criteria takes into account the 3 factors discussed
above as well as additional ones.
The software solutions are an attempt to minimize
the time spent testing
in the target
environment.
Validation facilities are expensive to build and time utilimd
for testing is expensive. This is due to the f;act that target
level testing occurs extremely late in the development
lifecycle and only a small window is allocated for HAS
integration
testing.
However,
the target is the only
location that can reveal tin
errors. It is ironic that our
current solutions attempt to reduce the anmunt of target
testing, but will likely lead to extensive modifications
and
thercfom extensive retesting.

4.

Problems with Embedded Irestinq
The solutions proposedaboveare not effective at

revealing errors. Effective implies that a technique reveals
a high percentage of the errors and that it does so in a costef!iaent manner. Instead, what the above tools provide is a
minimal, low-level view of the execution of a program and
those tools become available
at a very late stage in
development.
Below is a list of problems associated with
current approaches to embedded testing
4.1

Expense
of Testing
Process
Target
testing
requires
expensive,
custom
validation facilities.
The expense of these target facilities
is incurred for every project, since little
reuse across
projects is ever realized. The effort required to build these
validation
facilities
means that every test execution is
expensive, making retests extremely costly. Yet, hardware
often arrives late and full of errors, forcing software to be

291

modified
and subsequently retested.
This late arrival of
hardware also impacts the cost of an error, since certain
errom are only revealed during I-IN integrations testing.
Perhaps the largest factor associated with the high
costs of testing will be the questions and concerns that
certification processes are beginning to raise about sofhvare
tools. Typically, development tools have not been required
to meet any validation
criteria and certairdy not the strict
criteria imposed on the development system. This luxury
may soon disappear
as the role tools play in the
development
process comes under tighter scrutiny.
The
huge expense of validation
facilities
will
increase
dmmaticauy.

4.2 Level

of Functionality
on Target
The level of functionality
found on a target
machine is minimal and does not support tools. This lack
of functionality
greatly limits the effectiveness of testing,
since more time and effort is required to locate an error.
While a host system provides a high-level
interface and
discussed software in terms of the high-level language, the
target typically deals in machine instructions and physical
addresses. Translating these low-level entities requires time
and a great deal of tedious, error-prone activities.

4.3

Errora
revealed
late
in
development
lifecycle
Embedded
system designs
often incorporate
custom ASIC parta that are typicaIly not available until
very late in the development
process,
delaying
the
availability
of any target validation facility.
In addition,
errors designed into the ASICa are extremely expensive to
fix, requiring
new masks be created and complete
refabrication.
InsteaA errors in ASICs and other hardware
problems are resolved by modifying
the software.
As
stated before, this greatly delays the time which errors are
revealed, which in turn increasing the cost of software
testing.
4.4

Poor
teat selection
criteria
AIl to often, tool availability
diclates the quality
of a testing process.
Tests cases and scenarios
are
determined by what will work on available platforms and
which test are achedulable rather than being determined by
some theoretical
test cxitcria.
A prime example is the
FAA‚Äôs requirements ~AAS51 that 1) all testing be done in
the target environment
and 2) testing include statement
coverage.
Of course, test coverage is not currently
measured on the target.
Unfortunately,
it is cheaper for a company
to
spend it‚Äôs resources preparing an argument to obtain some
form of ‚Äòwaiver‚Äù than to actually perform a test. In time,
the argument approach will no longer be accepted and the
solutions
for embedded
testing must be in place to
accommodate
this
change.
It will
only take one
implementation
that performs statement coverage on the
target to force every embedded, real-time software developer
to perform statement coverage on the target to meet such a
certification requirement.

4.S Potential
use in advancing
architectures
Perhaps the largest problem
facing embedded
testing is that the current solutions cannot be applied to
future h~dware
architectures.
Future architectures
are
Proposing
‚óè wider addreas Spaces
‚óè higher -Sor
speeds
‚óè huge numbers of pins
‚óè internal
pipes
‚óè multiple
execution units
‚óè large internal
caches
‚óè multi-dip
modules
Such complexities
cast a dark shadow over the hardware
solutions previously discussed. With internal caching and
parallel activity being done on the chip, one will no longer
be able to gain processor state information
from simply
monitoring
the system bus. And as on-chip functions
become more complex, emulator vendors will no longer be
able to see into the chip through the pins making them
obsolete as well.
In [Chi191] an even stronger claim is made that
the debugging capabilities provided by the chip will need to
become more sophisticated. In future architectures, perhaps
the only possibility
to view and control the execution of
hardware is to gain that information
from the hardware
itself.

The previous
sections raised issues about the
effectiveness of our testing process and claimed that tcating
is currently being limited by tool functionality.
l%e goal
of this paper is to identify shortcomings in the embedded
testing proccas and propose a solution to those problems.
The view taken by the authors is that tool support for
embedded systems is lacking. Further, those approaches
currently used for gaining execution visibility
and control
will soon be obsolete for future architectures.
We propose
adding facilities to the underlying system to better support
testing and debugging tools for embedded software.
As stated previously,
the underlying
system is
composed of the hardware architecture and the run-time
system (RTS). Both are composed of data structures and
operations that implement
common system abstractions
such as processes, semaphores,
ports, timers, memory
heaps, and faultdexceptions.
It should be noted that there
is no distinct line between features of hardware and features
of the RTS. In fact, as these features and abstractions
become more standardized,
newer architectures
are
attempting to incorporate
them into their instruction
sets
~Nl%92].
In addition,
the implementation,of
a feature
may span parts of the architecture,
RTS, and compiler
generated code (i.e. fauhdexceptions).
5.1

Model
Debugging
System
Below is an illustration
of a debugging
system
(Figure 5.1). The data path from the debugging/testing
tool represents symbol table information
that allows the
tool to map machine level information
to source level

292

Test/Debug
Compiler
Generated
Code

\

Tool

1

A
e
x
t
e

3

Ada
Compilation

‚Äús-

Figure 5.1
constmcts.
The
ASIS
toolkit
provides
easy
required for this physical connection.
The
implementation
for this facility.
ASIS is a proposed
sw-tions describe ‚Äòtie architecture
additions
standard interface between an Ada library and any tool
interfaces in more detail.
requiring compilation information.
Of more interest is the communication
path
between the target processor and the testing tool. A tool
sits external to the rest of the embedded system, while the
RTS resides internally on the target board. At frost glance,
this conceptual
path seems rather difficult
to realize.
However, the implementation
becomes easier if thought
about as a typical host debugging system. Any debugging
system has a least two processes executing, one running
the test program and one running the &bugga.
These two
p=
Sa common physical machine, which allows
one process to gain information
about the other.
The
debugger procem simple requires data and computation
time, which it shares with the test program.
This same scenario is rcqnired
for embedded
debugging, except that the debugger process is split. Part
of the debugger process runs on the target machine and part
runs on the host. The goal is to minimize the portion that
must be run on the target so that it does not intrude on
execution
of the test program.
To realize this nonintrusive
execution
of the debug software,
the target
1) Execute debug code only at a break poinL
2) Run the debugger as a separate process, or
3) Provide a separate execution unit to execute the
debugger.
The details of these options are explored in depth later in
this paper.
The problem now lies with the interfa=
between
the embedded part of the debugger (intermddebuggcr)
and
the portion that lies on the host (external-debugger).
The
solution requires hardware additions that will be discussed
later in this paper. A high level view is given in figure
5.2. In this figure, the tool makes logical calls to services
provided by the RTS. These calls are actually implemented
by the debugging system through data passed between the
internal and external debuggers.
Hardware additions arc

1

I

next two
and RTS

I

Figure 5.2

-=

The past decade has seen hug{: advances in
microprocessor
designs.
Several of these advancements
were listed previously and include pipelining
and separate
functional
units.
The concept
of partitioning
a
microprocessor in order to perform parallel activities is of
great interest to this work.
It was noted
earlier
that these parallel
amputations
severely restrict current methods for testing
and debugging embedded systems, since on{e must simulate
a great amount of computations.
However,
debugging
tools can also use architectural
parallelism
to their
advantage. If a hardware design is partitioned successfully
to allow certain activities to occur concurrently,
then the
testing and debugging methodologies
might wish to add

293

their own computational
requirements to the list of parallel
activities.
This section will explore additions to hardware
architectures.
No claim is made as to the costs associated
with these features.
They assured y will require space
(transistors) and possibly even add to the execution cycles
required to implement certain instructions.
6.1 Hardware
Partitioning
of Memory
One primary concern for industry is reducing the
huge volume of retests associated with development.
The
current testing process ensures that errors are revealed late,
which forces retesting
large portions
of the system.
Despite correcting
these problems, industry will still be
faced with software that is constantly changing. Software
is deceivingly
easy to change and often the element of a
system assigned to unknown or ‚Äúrisky‚Äù aspects during
design.
Changing software is extremely expensive late in
the development
for critical
systems.
Such systems
typically
have requirement
that an error raised in one
portion of the system won‚Äôt interfere
with the correct
operation
of the rest of the system.
Current software
certification
agencies ~AA85]
have several software
restricdons including
‚óè Any modikation
made to a software module forces the
retesting of all other modules operating on that same
physical device.
. All software on a device must be developed under the
highest level of criticality of any module that will
execute on the same device.
Without
the ability
of hardware to guarantee software
boundaries, such requirements must be enforced. However,
these requirements
add a great deal of costs to software
development.
Consequently,
software is often physically
partitioned
based on critical level, rather than design
factors.
Partitioning
software modules based on critical
levels greatly interferes
with the design process.
One
would rather partition
modules based on factors such as
processor utilization
and inter-module
exmnmnication
requirements.
In fact, load balancing and p17XXsSmigration
are techniques
that would not be usable by embedded
system developers unless all software is developed at the
highest critical level.
The solution
to these issues is hardware
partitioning.
Each process should have it‚Äôs own protected
address space that is not accessible by any other process.
In addition, sets of processes may wish to share memory.
The processor should tdso provide the capability to restrict
access to segments of memory based on some criteria.
6.2 Computational
Facilities
for
Debugger.
The debugging
system is partitioned
into an
internal debugger and au external debugger. The internal
debugger must physically
exist on the target board and
communicate
with the external debugger through some
dedicated medium. The internal debugger will also require
execution from the target without
interfering
with the

operation of the application
program.
There are two
possible scenarios
‚óè The internal
debugger runs as a regular process on the
-Or
The architecture provides separate facilities to execute
the internal debugger code
In either case, control is transfered to the debugger when a
breakpoint is encountered
In the fiit
scemuio, the debugger is executed by
the processor as any other process.
If the debugger
executes as a low-level process, it would not interfere with
the operation of the rest of the system. However, this is
not a feasible approach.
Most intern-sting errors occur
during peak system loads, which would mean that the
debugger could only execute when the probability
of an
error occurring was low. Another approach wouId be to
execute the internal debugger as a periodic process of high
priority and design the entire system to take this process
into account when determining issues such as scheduling.
The second scenario requires the target processor
to provide some form of computational
facilities.
This
extra execution will certainly
require some amount of
utilization
of architecture
resources such as internal
registers and bus accesses.
The simplest
example of
architecture facilities would be a machine that contirtuaU y
dumps some representation of the instruction it is currently
executing.
This would require a dedicated bus to the
external world (proposed later in this section) and that
additional circuitry be attached to the computation units to
gain access to the current instruction.
The problem
with fis
approach
is that the
processor is not aware of what data is required by the tools
at the other end. Therefore, it must dump everything.
At
high processor speeds, the amount of information
being
sent could become overwhelming.
However, the data could
be faltered and then captured so that a tool could parse it
later and recreate an execution history of the program. The
hardware required for filtering
is not trivial and requires
great speed and storage capaaty to maintain pace with the
target processor.
lle next step is to allow software to dictate the
information
sent by the processor. The functional
unit of
the hardware sed.ing messagea could be implemented
as a
state machine, emitting
different
messages based on its
current state. The default state would be all processor
transactions. Basically, in this eontiguration,
the processor
is performing
the filtering
rather than the external
debugger.
This addition
should
not add much in
complexity to the hardware architecture and would greatly
reduee the wmplexity
of the external debugging hardware.
The final step is to take the (now
stateful)
functional
unit and make it programmable.
Instead of a
state machine, it now becomes a complete functional
unit
within the processor itself.
The internal debugger code
would then be loaded into this portion of the prowssor at
boot time and reside there for the entire execution,
transmitting
and receiving
messages to and from the
external debugger.
‚óè

294

6.3 Hardware
Break
Points
Software break points are intrusive and require
instructions be inserted into the code of the test program.
Conditioned
break points present a more significant
problem, since they require a computation every time they
are encountered to determine if the proper conditions are
met to halt execution.
Such breakpoints are unacceptable
for d-time
programs.
To resolve this issue, architectures need to provide
the capability
to set breakpointa in hardware.
A set of
registers would be classified
as BreakPoint
Registers
(BPR), which the processor would check against the
operands for each instruction.
Two types of breakpoints
are required, data and instruction.
Each data BPRs inside
the processor would be compared with the address of every
data operand for each instruction.
Instruction BPRs would
be compared with instruction
addresses or type. When a
match occurs, a breakpoint
fault would be raised and
control trsnafered to the internal debugger
Upon returning
from a break, the processor is
required
to restart execution
precisely
where it had
terminated.
The state of the processor consists of all it‚Äôs
internal registers, including
any pipeline information
and
cache memory. These values must be saved automatically
when a break is encountered.
Another issues is that of conditional breakpoints.
Such breakpoints
require computations
by the processor
that run in the background behind the program under test.
The evaluation of the conditional expression must begin far
enough in advance so that it may complete before the
processor has passed the breakpoint
location.
This
evaluation will require memory accesses, raising additional
problems. The current value of operands in the expressions
must be available to the processor, which might involve
accessing it from memory or cache. Any accesses to
memory must be scheduled in such a manner that they do
not block any resources required by the program under test.
F@dly, the value used must be valid and not in danger of
-g

before the breakpoint.
While the problems raised above seem difficult,
they are not insurmountable.
The extend
debugger must
compile the conditional expression and download the code.
At that point it can determine the scheduktbili~
of this
evaluation by comparing the @e for the conditional to the
other code that will occur in parallel. The user could then
be notified of problems with their additional
breakpoint.
The hardware is responsible for detecting any collisions in
parallel activity
and must not assume the debugger is
always accurate. Any debugger activity intruding
on the
behavior of the test program is important information
and
must be flagged by the processor.
The primary additions required for hardware break
points are additional registers from the architecture and the
logic necessary to compare them with the operands of the
current instruction.
To support conditional
breakpoints,
the processor must provide background
computational
support.
This support could come from a portion of the
processor dedicated to conditional breakpoints, or the code

could be downloaded to the internal debugger, given the
internal debugger support described previously.
6.4 Architectural
Support
for Abstractions
As common programming
paradigms
become
more refined, architectures will begin to inax-porate them
into their instruction
sets. It would be unlikely
that the
only abstractions supported by architectures would remain
simple data types (integer,
real) and their associated
operations (add, subtract, convert). Other abstractions such
as processes,
semaphores,
ports,
timers,
memory
management,
and faults
that are found
in typical
applications
should be supported as well, along with
associated operations on those abstractions.
M&ing
hardware to another level of abstraction
provides huge advantages for testing tools.
As stated
earlier, the architecture must be the basis for emulation
capabilities
and providing
execution visibility.
As the
hardware becomes more aware of programming elements, it
gains the abdity to send more meaningful messages to the
external world. A context switch between processes could
be sent with a single message, rather than the hundreds of
machine instructions it takes to implement the switch.
As the processor becom-es the single point of
visibility,
awareness of the progrdng
environment
becomes important.
A processor with a high-level
understanding
of program ,entities can emit fewer, more
meaningful
- messag-es than a processor
that only
comprehends low-level instructions.
6.5 Dedicated
Bus
Embedded
testing and debugging
require
an
interface that aliows the processor to communicate with the
external world without interfering with the behavior of the
system under test. This physical connection should reside
on the target and interface
extemall y tlhrough
some
detachable
mechanism.
The separation
technique
is
important,
since the external debugging system will be
detached from this connection once the system is placed
into operation.
The execution behavior alf the program
should be independent of whether or not any external tool
is attached.
Assuming an adequate physical connection,
the
next detmminah ‚Äúon is the protocol across it. ‚ÄòIle following
issues must be addressed
1) At what rate will messrwes need to be sent?
‚ÄòProcessor speed raises i~teresting problems, since future
speeds might be too quick for external
processing
techniques.
A solution to this problem was discussed
previously where the processor became aware of highlevel program elements.
The goal is to decrease the
number of messages required relative to the number of
machine cycles. 2) How much data is associated with a message?
If an architecture is required to emit large volumes of data
for messages, there may be instanms where the processor
must be suspended to allow the internal
debugging
hardware to catch up to the current processor state.
Higher level messages may compound
the problem,
since more maningful
messages might require more

295

This paper does not address the question of how
these interfaces should be UtdiZSd. Such SllSWerS should be
given by methodologies
and techniques for detecting and
locating
errors in embedded,
real-time
systems.
As
discussed earlier, the lack of these methods has led to
difficulties
for determining
adequate RTS services for
testing and debugging tools, which has forced a different
approach to determine the required operations.
Since the
RTS is in essence offering au implementation
of high-level
abstractions,
services that provide
visibility
into the
implementation
of RTS abstractions should adequately
fidfdl the needs of most testing and debugging techniques.
A standard currently exists for implementing
these
abstractions
in the MRTSI
[ARTE89]
and CIFO
[ARTE91].
In addition, most of the needs for testing and
debugging can be fulfiiled
by these standards. This is not
surprising, since our solution is based on implementation
visibility,
and the MRTSI and CEO are providing
an
implementation
interface. However, it is important to note
that this approach also indicates that implementations
that
support these staudards should require minimal additions to
and debugging tools as prOpOSed
by
akw SUppCWt testing
this paper. Below is a small discussion surrounding each
of these abstractions
and a list of shortcomings
in the
MRTSI and CIFO for testing and debugging.

information.
There is likely a tradeoff between message
level and data volume.
3) Is the connection bidirectional?
Visibility
concerns dictate that state information
travel
However,
methods
requiring
out of the processor.
control
of the executing
program require that state
information
travel the other direction.
Protoczds must be
in place to handle contention across the bus and those
must be extremely well defined, due to the extreme data
rate that could will be emmuntered across the bus.
4) Who is the active element in sending message9?
Either the processor or the RTS must determine the
information
sent from the processor.
The processor
cannot provide all the state information needed, while the
RTS will likely not be able to maintain adequate speeds
for sending messages.
These questions play a role in determining
the interface
between the internal and external debuggers.
A likely
solution would be a master-slave relation, where either the
internal or external debugger regulated the other. This
scenario does not seem likely, since each has such critical
processing concerns. Therefore, each will likely execute
independently,
while communication
is handled via some
bus and protocol.
There does exist a master-slave relationship
in
respect to the bus, howevex.
During program executiw,
the internal
debugger
must ‚Äòownn the bus, since it‚Äôs
processing concerns are the greatest.
It must meet the
message sending deadlines without altering computations
in other parts of the system.
There are points dting
execution where the extend
debugger must aeiz cmtrol.
If the internal debugger cannot allocate the bus to meet the
demands of the extcmal debuggm, the user must be notifkd
that their requested operation
cannot be accomplished
during a real-time execution.
The final determination
is that of the active
element
within
the processor.
There are two basic
approaches to detemnining control of the internal debugging
activities.
In the fiit
the processor is active and becomes
responsible for sending messages to the extend debugger.
‚Äòfhesecondappmachuse
aaspecialdebugge
rportionofthe
RTS to emit messagea, which is loaded into a dedicated
functional
unit within
the architecture.
Tools require
information
maintained
by both the architecture and the
RTS. Perhaps the solution lies between the two where
both the RTS and architecture have the ability to dump
messages, depending on the cmrcnt mquiremcnts dictated by
the external tool.

7.1

Processes
Concurrency
is a common abstraction
used in
embedded systems. A design can be decomposed without
concern for computational
resources, which can then be
determined
by a scheduler
during
run-time.
A&
irqplements concurren cy through tasks and task types. The
CIFO and MRTSI
provide
extensive
tasking
support
includlng
identifieation,
creation
and activation,
communication
through rendezvous, concurmat access to
shared entities,
and support for scheduling
control.
Elements of interest that are not provided by the CIFO or
MRTSI include
‚óè Task State - A developer
must have the ability to query
and modify the task state for each task in their system.
However, a modification
could leave the RTS in an
inanaistent
state. For example, changing a task‚Äôs
state from ‚Äúdelaying‚Äù to %unning‚Äù without removing
it from the &lay queue would place the RTS into a
state that could not be achieved
through
normal
execution. However, the same modification
ability is
available on typical debugging systems and should be
offered by em beddcddebwrgera as Wd.
‚óè Commm&ation
and Synchronization
- A developer
must have the ability tb view and modify eaeh entry
queue to determine the concurrent state of the system.
Again, modifications
could leave the RTS in an
unobtainable state.
‚óè Scheduling
Control
- In addition
to the extensive
operations provided by the CIFO for concurrency
control, a developer must have awess to the dispatch
port (or ports for muhiprqxssor
systems).

7. Run-Time
Svs tern Additions
The RTS requirements
deseribe an interface
between a tool and the underlying
system.
This is a
logical interface requiring substantial hardware support as
outlined
above.
An obvious goal is to minimize
the
required data and computational requhements of the internal
debugger as well as the required communications
between
the internal and external debuggers.

7.2

296

Interrupt

Management

One of our criticism of the current approach to
embedded testing is that timing errors are revealed late in
Interrupts are very related to
the development
process.
timing issues and their correctness is an important element
in embedded testing. Therefore, support for interrupts is
extremely
important
to target testing and debugging.
Faalities
provided through the CIFO and MRTSI would
allow developers
to bind various
interrupt
handling
routines, enable and disable certain interrupts, mask and
unmask interrupts,
and generate software interrupts
all
controlled dynamically duting program a program test.
7.3

Time
Management
As stated earlier,
important‚Äù to target testing
target tools require sfilaertt
time. Tools must be allowed
(although
such modifications
results) and the delay Iist of
by the RTS.

timing
issues are extremely
and debugging.
Therefore,
control over issues relating to
to view and modify the clock
might produce undefined
waiting processes maintained

7.4 Memory

Management
Dynamic
memory
is not typically
used by
due to diffldtk%
in dcmonstradng
embedded ti@iC4itiOliS
reliability.
However,
future
systems
will
likely
incorporate
algorithms
that requite dynamic storage. In
addition, memory ~agemcnt
for dynamic allocations is
part of a RTS and should therefore be included in RTS
visibility
and control discussions.
A tool will likely
require that ability to demonstrate an application programs
behavior when memory is exhausted.
The MRTSI would need to be extended to provide
operations that mim
a collection
making it smaller to
show execution behavior when memory is exhausted or
larger to demonstrate correct execution should a collection
be expanded by the developer.
Resizing is not cheap and
could require a gnat deal of computation and data transfers,
depending on an implementation.
7.5

Exception/Fault
Handling
Proper handling of exceptional events is evaluated
during hardwaresoftware
integration
testing. Therefore,
tools require a great deal of cattrol over exceptions and
One must be able to raise an
recovery mechanisms.
exception
or fault during program execution and also
modify handler binding during execution.
Another question of interest might be to locate the
handler for a given fault or exception at a given program
location.
Such information
is not easily gained from the
underlying
system.
The compiler
is responsible
for
handling exception propagation [ARTE89], so &k
‚Äú ‚Äú g
the handler from only RTS information
might be an
impossibility
and is at best resolved uniquely for each
compilation
system.

architectural and RTS additions. The architectural additions
will certainly be costly in both time and space, requiring
space (transistors)
on the chip and access to internal
registers and busses that could cause contention and slow
the execution
of other instructions
provided
by the
architecture.
However, the RTS additions
are minimal.
We defined
the needs of testing
as making
the
implementation
details of common system abstractions
visible and then determined the functionality
required to
view and control them.
The ARTEWG‚ÄôS
MRTSI
and
CIFO provided an outstanding basis for this approach.
The RTS additions are admittedly
weak.
Our
initial goal was to have the methodologies
and techniques
used for testing embedded, real-time
systems drive the
operations
required
by the RTS.
Unfort.tmatcl y, such
methods do not yet exist. As stated earlier, testing and
debugging of embedded, real-time software remains a black
art, with ad hoc methods and techniques. While there has
been much research into the concurrency and distribution
issues, none has examined real-time constraints, embedded
environments,
and other issues relating
to embedded
systems Perhaps the MRTSI and CIFO are sufficient
for
implementing
target level testing and debugging
tools.
However, this question cannot fully be resolved until more
formal methods exist.
Our next step is to evaluate the additions
and
determine their feasibility.
Questions relating the cost of
these additions to au architecture and RTS in terms of time
and space must be answered.
Also, a more complete
mapping should exist between the added feattues and the
impact they have on the desired features.
One can then
make a valid comparison between a feature and the costs
associated with it.
‚Äòl‚Äôhe embedded contmllermark~
is currently huge,
but has only begun to require the computational
powers
~SOCiKltti With lUiCrOpKXXWOrS. Embedded i@k.i3tiOttS
have traditional
been event driven rather than computation
dependent. Due to their light weigh~ easy con@mbility
and expansibility,
and lower design complexity,
computers
are quickly being chosen over mechanical techniques for
controlling
devices. As this transition continues, the size
and complexity
of embedded
programs
will
grow.
Controllers will not only have strict timing requirements,
but also have significant
computational
needs as well.
This combination
requires new approaches to our current
testing process for embedded systems and therefore, more
effective tools to aid in testing and debugging embedded
applications.
References

[ARTES9]

Ada Run-time
Environment
Working
Oroup,
‚ÄúA Model
Run-Time
System
Interface for A&m Ada Letters, January,
1989.

[ARTE91]

Ada Run-time
Environment
Working
Group, ‚ÄúCatslogue of Interface Features

s

co nclusions
The goal of this paper is two fold.

The first goal
is to identify defkienaes
in embedded system testing and
raise questions about the future of current tools.
The
second is to propose a solution to these problems through

297

and Options
for the Ada Runtime
Environment,‚Äù
Special Edition of Ada
Letters, Fall 1991 (fI).

lyxn6J

Tai, K.C., ‚Äú&producing
Testing of Ada
Tasking Programs,‚Äù IEEB Transactions
on Software Engineering,
1986.

[CHIL91]

Child,
Jeffrey,
‚Äú32-bit
Emulators
Struggle with Processor Complexities,n
Computer Design, May 1,1991.

~A191]

DD83]

Department
of Defense,
Reference
Manual
for the Ada Programming
Language,
ANSI/MIL-STD1815a,
United States DoD, 1983.

Tai, K.C., Carver, R.H., and Obaid,
E.E., ‚ÄúDebugging
Concurrent
Ada
Programs by Deterministic
Execution,‚Äù
IEEE
Transactions
on
Software
Engineering, January, 1991.

~AYL80]

Taylor,
R.N.
and Osterweil,
L. J.,
‚ÄúAnomaly
Detection
in Concurrent
Software by Static Data Flow Analysis,‚Äù
IEEE
Transactions
on
Software
Engineering, May, 1980.

~AYIJ33]

Taylor,
R.N., ‚ÄúA General
Purpose
Algorithm
for Analyzing
Concurrent
Programs,‚Äù
Communications
of the
ACM, ~y,
1983.

Federal Aviation
Association,
Software
Consideration
in Airlx)me
Systems and
Equipment
Certification,
RTCA/DO178A, 1985.
[GILL88]

Gilles, Jeff aud Ford, Ray, ‚ÄúA Guided
Tour Through
a Window
Oriented
Debugging Environment
for Embedded
Real Time
Ada
Systems,‚Äù
IEEE
Transactions
on Software Engineering,
1988.

&IAm78]

Hansen, B., ~eproduable
Testing of
Monitors,‚Äù
Software-practice
and
Experience, Volume 8,1978.
Hembold,
D. and Luckham,
D.,
‚ÄúDebugging
Ada Tasking
Rograms,‚Äù
IEEE software, March, 1985.

Iw‚ÄôfJ=l

Intel Corporation,
i960
Architecture
programmer‚Äôs
Manual, 1993.

KOEH91]

Koehnemann,
H.E. and LindquisL
T.E.,
‚ÄúRuntime Control of Ada Rendezvous for
Testing and ~U@llg,‚Äù
Procedm - gs of
the 24th Hawaii International Conference
on System Sciences, Volume II, 1991.

Extended
Reference

LeDoux, C, and Parker, D.S., ‚ÄúSaving
Traces for Ada Debugging,‚Äù
Ada in Use
Proceedings
of the Paris Conference,
1985.
Km]

Lyttle, D. and Ford, R., ‚ÄúA Symbolic
Debugger for Red-Time Embedded Ada
Software,‚Äù
Software
- Practice
and
Experience, May 1990.

@fAUG851

Mauger, C. and Pammett K., ‚ÄúAn EventDriven Debugger for Ati‚Äù
Ada in Use:
Proceedings
of the Paris Conference,
1985.

298

INDUSTRY TRENDS

Moving Java into Mobile Phones
George Lawton

Telecom are now selling Java phones. And, said Ben Wang, manager of systems development for Sprint PCS, 80 percent of the new phones the company sells will be Java enabled after the big rollout next month. Nokia alone plans to ship 50 million Java phones this year and 100 million next year. In fact, 15 handset makers either are or soon will be selling 50 models of Java phones.

Advantages

A

s mobile technology matures, handheld-device vendors are looking for ways to make their products more functional, and Java is one approach they are turning to. This is particularly the case with smart cellular phones, which are using Java to help add new capabilities. In smart phones, Java functions as a layer between the operating system and the hardware, or runs parallel to the OS within a separate chip. In the past, the key constraint to running Java on mobile devices has been their processing, memory, and powerconsumption limitations. However, new mobile hardware and software developments are reducing these limitations. Thus, industry observers expect Java use in mobile devices, which is already supported by many vendors, to explode during the coming years. Nick Jones, a fellow at Gartner Inc., a market research firm, said Java will become a de facto standard on midrange and high-end cellular phones. He predicted that at least 80 percent of mobile phones will support Java by 2006, although some may also run on other technologies, such as Microsoft's Pocket PC operating system. According to Jones, mobile-device manufacturers' desire for an aftermarket is driving interest in Java as a mechanism for easily adding software to devices. Java also permits applications to work across platforms. This is important in the mobile-phone market,

which features many platforms. However, questions about Java's performance and a dearth of Java-based applications for cellular phones, particularly in Europe and the US, remain as obstacles to the technology's widespread adoption in mobile devices.

DRIVING JAVA USE IN HANDHELDS
Work on Java-enabled handheld devices began several years ago, but completion of the Java 2 Platform Mobile Edition (J2ME) and support from device vendors and cellularphone-service providers have driven the recent level of interest, explained Eric Chu, Sun Microsystems' group product manager for industry marketing.

Adoption levels
Korea's LG Telecom in became the first service provider to deploy Java in September 2000. Since then, users have deployed between 18 million and 20 million Java-enabled telephones, said Sun spokesperson Marie Domingo. Companies such as Nextel in the US, NTT DoCoMo in Japan, and British

According to Sun's Chu, one of Java's major benefits for cellular phones is support for packet-based networks running TCP/IP. Using TCP/IP makes it easier to write applications that communicate directly with the phone, rather than relying on an intermediate technology such as the wireless application protocol (WAP). Also, Chu said, Java, unlike WAP, supports pictures and colors. In addition, he explained, the Java environment provides good security because it includes a sandbox that limits downloaded code's access to the rest of a host system. Moreover, Java's ability to work with different platforms is important in the fragmented cellular-phone market. This capability lets a Java-enabled phone run applications and services written for other mobile platforms and also lets software vendors save time and money by writing a single, Java-based version of an application to run on multiple platforms. And Java-enabled phones and servers could communicate directly with each other, thereby enhancing interactive applications. Java enables smart-phone users to download applications directly from the Internet. Similarly, Java lets users download Java applets that customize their devices in various ways, such as with special ring tones or improved caller ID. This lets users get new features more easily. In the past, users had to buy new phones, run new applications remotely using WAP, or download programs first downloaded to a PC. Meanwhile, there are many Java developers, which makes it easier for
June 2002

17

I n d u s t r y Tr e n d s

Java 2 Platform Micro Edition (J2ME)

Optional packages Optional packages Java 2 Platform Enterprise Edition (J2EE) Java 2 Platform Standard Edition (J2SE) Personal basis profile Personal profile MIDP CLDC KVM
Source: Sun Microsystems

Foundation profile CDC JVM

Figure 1. Sun's three primary Java platforms are each designed primarily to run on a different type of machine. The Java 2 Platform Enterprise Edition is designed for servers; the Java 2 Platform Standard Edition for workstations, PCs, and laptops; and the Java 2 Platform Micro Edition for PDAs, smart cellular phones, and other smaller systems. J2EE and J2SE use the full Java virtual machine (JVM). J2ME also works with the slimmed-down K virtual machine (KVM), the connected limited device configuration (CLDC), and the mobile information device profile (MIDP).

Other approaches help Java technologies designed for larger computers work on mobile devices. For example, SavaJe developed the SavaJe OS, which supports Java applications in a mobile environment by optimizing J2SE libraries for common mobile CPUs. Mathew Catino, SavaJe's cofounder and vice president of marketing, said Java applications typically spend 80 to 90 percent of their time executing the libraries. Therefore, he explained, optimizing the libraries enables applications to run 10 to 20 times faster. Zeosoft has developed ZeoSphere Developer, which permits the creation of mobile applications that support Enterprise Java Beans, Sun's Java-based software-component architecture. This could simplify the development of complex enterprise applications that communicate and run across servers (via J2EE), PCs (via J2SE), and mobile devices (via J2ME).

Software development tools
Application developers can use existing tools to create Java programs for handheld devices by limiting their code to libraries and APIs supported by J2ME. However, J2ME includes only a limited number of development libraries, noted Jacob Christfort, chief technology officer of Oracle's Mobile Division. Also, said Gartner's Jones, enterprises might shy away from J2ME because of the poor user interface designed for small device screens, the primitive threading model, and minimal native data-handling facilities. In essence, he explained, the design approach that lets J2ME work on small devices sometimes makes it inappropriate for large-scale enterprise uses. To address these concerns, several vendors have released or will soon release development toolkits or toolkit extensions to help developers more easily meet enterprise applications' needs. The new approaches include Sun's Forte for Java Programming Tools, the Oracle 9i Application Server

vendors of Java-enabled mobile devices to find people to write their software.

MAKING JAVA WORK IN HANDHELDS
Sun, which designed and manages development of Java, is in the forefront of making the technology work in handheld devices. However, other vendors have also become active in this area.

Sun Microsystems
Sun and a group of partners created J2ME to make Java work on smaller devices. J2ME includes some core Java instructions and APIs but runs more easily on small devices because it has a smaller footprint than the Java 2 Platform Standard Edition (J2SE) or Enterprise Edition (J2EE), shown in Figure 1, and has only those features relevant for the targeted devices. For example, J2ME's graphics and database-access capabilities are less sophisticated.
18
Computer

J2ME generally incorporates the connected limited device configuration (CLDC), which is implemented on top of operating systems and serves as an interface between the OS and Javabased applications. The CLDC generally uses the K virtual machine (KVM), a slimmed-down, less-functional version of the Java virtual machine (JVM) for small devices. The J2ME mobile information device profile (MIDP) sits on top of the CLDC and provides a set of APIs that define how mobile phones will interface with applications.

Other vendors
Several vendors besides Sun are creating Java-based technologies for handheld devices. Hewlett-Packard makes the MicroChaiVM (http://www.hp. com/products1/embedded/products/dev tools/microchai_vm.html), a cloned JVM that doesn't have Sun's licensing fees and usage restrictions. Several vendors, including Ericsson and HP, plan to use MicroChaiVM-based phones.

Wireless architecture toolkit, and the Sprint PCS Wireless Toolkit. Because of J2ME's shortcomings, Jones said, corporate applications will probably be based on the larger-footprint J2SE as mobile devices get more processing power. Regardless, said John Montgomery, product manager with Microsoft's .NET Development Group, current Java tools are too primitive and difficult to use for most developers.

ETM9 interface Instruction TCM interface Instruction cache Memory management unit ARM9EJ-S core Data TCM interface Data cache Memory management unit Write buffer Control logic and bus interface unit

Server-side handheld Java
Another Java-enabling approach would link handheld devices to Java applications and services on servers. AT&T Wireless, BEA Systems, IBM, Nokia, NTT DoCoMo, Sun, and other companies have created the Java-based Open Mobile Architecture for linking cellular phones and servers. The project would augment J2EE, designed primarily for servers, so that it would support standards that mobile devices can use with Internet-based information. The standards include XHTML (for displaying Web pages on mobile devices), SyncML (for synchronizing data between mobile devices and other machines), WAP 2.0 (to access Internet content and services), and the multimedia messaging service (for handheld messaging).

A R M96EJ- S

Coprocessor interface

AHB interface Instruction Data

Source: ARM Ltd.

Figure 2. ARM Ltd.'s ARM926EJ-S chip includes the company's Jazelle technology in its ARM9EJ-S Java-enabled processor core. In addition, the chip includes separate ETM (embedded trace macrocell), data TCM (tightly coupled memory), and AHB (advanced high-performance bus) interfaces.

IMPLEMENTATION IN HARDWARE AND SOFTWARE
Java technology can be implemented in software or in hardware on either a specialized Java acceleration chip or a core within the main processor. Software implementations tend to run less efficiently because systems must translate each Java instruction into native instructions that the CPU can run. Separate hardware chips are more efficient but represent additional device components and cost. Java cores integrate some of both approaches.

Components Group, said his company has developed techniques for speeding up the software process, which used to bog down when the CPU switched from instructions it could accelerate to instructions it couldn't. In addition, Intel and other software-based Java proponents say the latest mobile processors can run Java fast enough to compete with hardware-based approaches. Analyst Markus Levy with MicroDesign Resources, a semiconductorindustry research firm, disagreed. He said, "People are spending a lot of energy fine-tuning the software-based approaches. For some people that may be good enough, but if you really want the most efficient implementation you need a hardware-based approach."

Software approach
In the software approach, a device's CPU runs the Java code. David Rogers, marketing manager for Intel's PCA

Java hardware
Companies such as ARC Cores, ARM Ltd., Aurora VLSI, Digital Communications Technologies, inSili-

con, and Zucotto Wireless are developing hardware that runs Java, either as Java coprocessing cores for integration into CPUs or as stand-alone Java chips. Both hardware-based approaches promise to increase Java-based application performance and, by running more efficiently, reduce power demands on battery-dependent cellular phones. Different companies' chips execute different subsets of the Java instructions. For example, ARM's Jazelle chip, shown in Figure 2, executes about 68.2 percent of all possible Java instructions, while Aurora's DeCaf runs about 95 percent. Running a bigger set of Java instructions provides more functionality but makes a chip cost more and consume more power. Joan Pendleton, Aurora's cofounder and chief architect, said there are two classes of acceleration. The first, used by most vendors, translates Java byteJune 2002

19

I n d u s t r y Tr e n d s

code into native processor instructions. The second directly executes Java bytecode, which offers better performance but requires a larger footprint because of the additional circuitry necessary to run the software in hardware. Levy predicted that Java cores will be more popular than stand-alone Java processors. This approach's primary constraint is that developers must use a system-on-chip approach to create their products. Putting multiple functions on a chip is more expensive to develop, but the elimination of additional chips reduces device costs. Standalone Java chips are less expensive to design but lead to higher device costs.

performance across platforms. Levy has thus launched a Java-processor group within the Embedded Microprocessor Benchmark Consortium (http://www.eembc.org/). The group expects to release its first benchmark by next month.

"We are still in a phase of market confusion and have not yet gotten to a state of market consolidation," Jones explained.

Not enough applications
There are currently some mobileJava applications, including games and weather and traffic maps. However, Jones said, there are not enough desirable mobile-Java applications yet. The reason is not the technology, he said, but instead the lack of an effective business model and a commercial infrastructure that would enable developers to profit from their work.

A

CONCERNS AND CHALLENGES
Mobile Java is still a relatively new technology. Many industry watchers say the technology has kinks that still need to be worked out. For example, Gartner's Jones expressed concern about vendors' differing Java implementations. He said some developers are complaining about having to manually optimize their Java games for different cellular phones. And although there are many Java developers, there are fewer who have experience working with J2ME and writing code for small, resource-constrained devices. Overall, said Microsoft's Montgomery, "J2ME is an interesting set of engineering compromises, but I would argue exactly the wrong set of compromises. It is too big for the smallest devices but too small to have the features you want on the smartest devices."

Industry observers say mobile Java still has kinks that must be worked out.
The growth of publishing intermediaries that would certify and sell mobile-Java software may eliminate this problem.

HANDHELDS AND THE FUTURE OF JAVA
Jones said Java is doing well on back-end servers because Java-based applications can easily be redeployed as companies buy new servers. However, he noted, client-side Java use has faded considerably because many enterprise-application developers turned to Visual Basic to work within the corporate environment, which is typically Microsoft-based. Thus, the battle for the mobile platform is important to Sun. However, Sun's Java initiatives for cellular phones are facing stiff competition from various sources, including Microsoft's wireless efforts, the Symbian operating system, Linux, and Qualcomm's binary runtime environment for wireless (http://www.qualcomm.com/ brew/).

ccording to Jones, J2ME will attract more application developers as it becomes a richer and less constrained environment. A survey by Evans Data, a market research firm, found that wireless developers who have used Java expect to use the technology a bit more in 2003 than they will this year. Java will also become even more attractive as smart phones get more processing power and vendors design better APIs for color screens, higher quality sound, intellectual-property protection, and user-location capabilities, he added. However, he cautioned, these extra features would give vendors more opportunity to create their own Java implementations, which could fragment the application-development environment. Sprint PCS's Wang said the initial focus of mobile Java will be on games, multimedia, and ring tones. Over time, Levy added, Java will become a de facto standard built into smart phones. SavaJe's Catino predicted that Microsoft and Java-based technologies are likely to coexist in phones during the coming years. Third-party vendors could help this process by developing software-integration techniques that would combine the two environments in devices. I

Performance
Jones said that mobile Java can be somewhat slow because the KVM is not particularly fast. However, he added, the KVM should become faster in the future, particularly as phones with more memory can run just-in-time compiler technology, which enhances performance. "In five years," he said, "[performance] will be a nonissue." Another problem, said Levy, is a lack of standards to objectively measure
20
Computer

George Lawton is a freelance technology writer based in Brisbane, California. Contact him at glawton@ glawton.com.

Editor: Lee Garber, Computer, 10662 Los Vaqueros Circle, PO Box 3014, Los Alamitos, CA 90720-1314; l.garber@computer.org

ARTICLE IN PRESS

The Journal of Systems and Software xxx (2004) xxx‚Äìxxx
www.elsevier.com/locate/jss

Automated support for service-based software development
and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a
Department of Computer Science and Engineering, Arizona State University‚Äì‚ÄìMain, P.O. Box 875406, Tempe, AZ 85287-5406, USA
Department of Electronics and Computer Engineering Technology, Arizona State University‚Äì‚ÄìEast 7001 E, Williams Field Road, Building 50,
Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003

Abstract
A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can
be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on
available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications.
√ì 2003 Published by Elsevier Inc.

1. Introduction
A service-based development paradigm, or services
model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can
interact with one another and be providers or consumers
of data and behavior. Some of the deÔ¨Åning characteristics of service-based technologies include modularity,
availability, description, implementation-independence,
and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon
the deÔ¨Ånition of the interface needed to access a service
(description) while hiding the details of its implementation (implementation-independence). Since the client
and service are decoupled, other concerns such as side
eÔ¨Äects become non-factors (modularity). One of the
potential beneÔ¨Åts of using a service-based approach for
developing software is that at any given time, a wide
variety of alternatives may be available that meet the
needs of a given client (availability). As a result, any or
all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach
for the creation of services and their subsequent integration with service-requesting client applications. The
technique utilizes an architecture description language
to describe services and achieves run-time integration
using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and
involves the automatic generation of ‚Äò‚Äòglue‚Äô‚Äô code for
both services and applications. The Jini interconnection
technology (Edwards, 1999) is used as a broker for
facilitating service registration, lookup, and integration
at runtime.
The remainder of this paper is organized as follows.
Section 2 describes background material in the areas
of software architecture and the middleware technology we are using to enable dynamic integration (i.e.
Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work,
and Section 5 draws conclusions and suggests further
investigations.

2. Background
*

Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751.
E-mail address: gannod@asu.edu (G.C. Gannod).
0164-1212/$ - see front matter √ì 2003 Published by Elsevier Inc.
doi:10.1016/j.jss.2003.05.002

This section describes background material on software architecture and Jini.

ARTICLE IN PRESS
2

G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

2.1. Software architecture

3. Approach

A software architecture describes the overall organization of a software system in terms of its constituent
elements, including computational units and their
interrelationships (Shaw and Garlan, 1996). In general,
an architecture is deÔ¨Åned as a conÔ¨Åguration of components and connectors. A component is an encapsulation
of a computational unit and has an interface (e.g. port)
that speciÔ¨Åes the capabilities that the component can
provide.
Connectors encapsulate the ways that components
interact. A connector is speciÔ¨Åed by the type of the
connector, the roles deÔ¨Åned by the connector type, and
the constraints imposed on the roles of the connector. A
connector deÔ¨Ånes a set of roles for the participants of the
interaction speciÔ¨Åed by the connector. Components are
connected by attaching their ports to the roles of connectors.
Another important concept is an architectural style.
An architectural style deÔ¨Ånes patterns and semantic
constraints on a conÔ¨Åguration of components and connectors. As such, a style can deÔ¨Åne a set or family of
systems that share common architectural semantics
(Medvidovic and Taylor, 1997).

This section describes the service-based development
approach including the techniques used for deÔ¨Åning
services, specifying client applications, realizing integration, and generating glue code.

2.2. Jini
The primary enabling feature of the work described
in this paper is the existence of Jini (Edwards, 1999) for
the delivery and management of services. In a typical
Jini network, services are provided by devices that are
connected to the network. A Jini technology layer provides distributed system services for activities such as
discovery, lookup, remote event management, transaction
management, service registration, and service leasing.
When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is
registered, a proxy (Gamma et al., 1995) is stored by the
lookup service. The proxy can later be transported to
the clients of the service. Other network members can
discover the availability of the service via the lookup
service. When a client application Ô¨Ånds an appropriate
device, the lookup service sets up the connection. In our
approach to component integration, we use Jini to
provide a standard method for registering and connecting a client to corresponding software components
that are acting as services.
One of the advantages of using this Jini-based integration technique is that it facilitates construction of
applications ‚Äò‚Äòon-the-Ô¨Çy‚Äô‚Äô whereby components can be
used on an as-needed basis. One of the disadvantages is
that clients of services must have some prior knowledge
about how to use each respective service.

3.1. Example
Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on
the health of systems in a network. This application
utilizes a network sniÔ¨Äer service and a port monitoring
service. The network sniÔ¨Äer service gives an administrator information about traÔ¨Éc on the network. The
port monitoring service provides information about the
open ports on the various machines on a network. Together, these services facilitate determining whether
certain kinds of attacks (such as ping storms) are being
directed to a machine or machines. The client application supports analysis of several networks, each of
which is accessed using the buttons shown on the top
portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services
that utilize diÔ¨Äerent models of execution (strict call return and data streams). The remainder of this section
refers to architectural speciÔ¨Åcations that were used in the
construction of this example.
3.2. Overview
The methodology that we have developed follows
closely the model suggested by Stal (2002) for web ser-

Fig. 1. Running example.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

vices, although the technology that we are using to
realize our approach is Jini. The approach itself focuses
on two concerns with respect to software reuse. That is,
it addresses both for reuse and with reuse concerns. With
respect to for reuse, the approach involves the construction of services via the use of adapter and proxy
synthesis. SpeciÔ¨Åcally, the methodology involves two
steps for creating services as follows: (1) speciÔ¨Åcation of
components as services, and (2) generation of services
using proxies via the construction of appropriate
adapters and glue code. These services are consequently
registered and made available on a network.
With respect to with reuse concerns, the approach
involves the construction of applications using services
as follows: (1) speciÔ¨Åcation of a client to make use of
services from a repository or network, (2) generation of
the client (both manual construction of client application speciÔ¨Åc code and automated generation of glue
code), and (3) execution of the client, including integration of the speciÔ¨Åed services at runtime.
Within our approach, a user (e.g. developer) is
responsible for writing the source code for the client
application along with the speciÔ¨Åcation of the architecture for a client. Among other things, the client speciÔ¨Åcation contains a description of the basic services that
the client application will need in order to be a complete
system. All other source code, including code necessary
to realize the connections between the client and employed services, is generated based on the speciÔ¨Åcations
describing clients, services, and connectors.
3.3. Service generation
In this section we describe some of the issues related
to automating the creation of service wrappers. To
support these activities, we have developed an automated tool that takes as input a software architecture
and produces glue code. A primary source of reusable
components that we employ in our approach are legacy
command-line applications (Gannod et al., 2000). In
order to generate services from legacy components, we
take the approach of wrapping the components by utilizing the interface provided by the component. Since
command-line applications have a well-deÔ¨Åned input
and output interface, the interface of the application as a
service can be based entirely upon the knowledge of
what the application intends to provide.
3.3.1. SpeciÔ¨Åcation and synthesis
The concept of using an adapter for wrapping legacy
software is not a new one (Gamma et al., 1995). As a
migration strategy, component wrapping has many
beneÔ¨Åts in terms of re-engineering including a reduction
in the amount of new code that must be created and a
reduction in the amount of existing code that must be
rewritten.

3

In regards to wrapping components, our approach
uses two steps. First, a speciÔ¨Åcation of the legacy software as an architectural component is created. These
speciÔ¨Åcations provide vital information that is required
to deÔ¨Åne the interface to the legacy software. Second,
the appropriate adapter source code is synthesized based
on the speciÔ¨Åcation.
3.3.2. SpeciÔ¨Åcation requirements
To aid in the development of an appropriate scheme
for the wrapping activity, we deÔ¨Åned the following
requirements upon speciÔ¨Åcations. These requirements
are as follows: (S1) a suÔ¨Écient amount of information
should be captured in the interface speciÔ¨Åcation in order
to minimize the amount of source code that must be
manually constructed, (S2) a speciÔ¨Åcation of the interface of the adapted component should be as loosely
coupled as possible from the target implementation
language, and (S3) the speciÔ¨Åcation of the adapted
component should be usable within a more general
architectural context.
The requirement S1 addresses the fact that we are
interested in gaining a beneÔ¨Åt from reusing legacy software. As a consequence, we must avoid modifying the
source code of the legacy software. At the same time, we
must provide an interface that is suÔ¨Écient for use by a
target application. To provide that interface, a suÔ¨Écient
amount of information is needed in order to automatically construct the adapter.
Our selection of command-line applications addresses
the modiÔ¨Åcation concern of requirement S1 since source
code is not available. As such, we are required to provide an interface that is based solely on the knowledge of
how the application is used rather than how it works.
Table 1 shows the properties used in the speciÔ¨Åcation
of services, clients and connectors. A service component speciÔ¨Åcation consists of two parts: properties and
ports. The properties section describes style of the service, while the ports section describes functions provided
by the service. In addition, the service speciÔ¨Åcations
indicate style-based information as well as conditions
or commands that need to be true or executed, respectively, in order to establish an environment necessary to
use the service. Finally, a key in terms of a ‚Äò‚Äòservice
type‚Äô‚Äô (e.g. interface property) is used to support a service lookup, which is later utilized during application
integration.
The requirement S2 (i.e. the decoupling of a speciÔ¨Åcation from a target implementation language) is based
on the desire to apply the synthesis approach to a variety
of target languages and implementations. In addition,
this requirement facilitates enforcement of requirement
S1 by ensuring that new source code is not artiÔ¨Åcially
embedded in the speciÔ¨Åcation. While satisfying this
requirement is ideal, we found in our strategy that a
certain amount of implementation dependence was

ARTICLE IN PRESS
4

G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

Table 1
Properties
Group

Attribute

Description

Service properties

Component-Type

Architectural style this component adheres to

Service port properties

Signature
Return
Cmd
Pre
Post
Interface
Path
Port-Type
Shared-GUI

The port‚Äôs signature
The port‚Äôs return type
The command-line program being wrapped
Pre-processing command
Post-processing command
The generic interface implemented by this port
Path to the wrapped command-line program
The port‚Äôs type based on the Component-Type
Boolean indicating shared (true) or exclusive (false) GUI

Client properties

Part-of-client
GUI-CodeFile
Component-Type
Shared-GUI

IdentiÔ¨Åes inclusion in client application
The Ô¨Ålename for client‚Äôs GUI code
Architectural style this component adheres to
Boolean indicating shared (true) or exclusive (false) GUI

Client port properties

Port-Type
Interface

The port‚Äôs type based on the Component-Type
The generic interface that this port can bind with

Connector properties
Connector role

Connector-Type
Prop-type

Architectural style this connector adheres to
The connectors role based on the Connector-Type

necessary due to the fact that our implementation would
make use of Jini.
When a component has been wrapped using our
technique, an interface is deÔ¨Åned that facilitates the use
of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also
desirable to be able to use the speciÔ¨Åcation of the
adapted component within a more general architectural
context. That is, it is advantageous to be able to use the
speciÔ¨Åcation as part of the software architecture speciÔ¨Åcation for new systems. In using a content-rich speciÔ¨Åcation, where interfaces are deÔ¨Åned explicitly, the
added beneÔ¨Åt of providing information that can be
integrated into an architectural speciÔ¨Åcation of a target
application is gained.
In order to realize the requirements placed upon desired interface speciÔ¨Åcations for legacy software wrappers, we used the ACME (Garlan et al., 1997)
architecture description language (ADL). SpeciÔ¨Åcally,
we used the properties section of the ACME ADL to
specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an
ADL that has been used for high-level architectural
speciÔ¨Åcation and interchange (Garlan et al., 1997).
3.3.3. Synthesis
As stated earlier, the class of legacy systems that we
are considering are command-line applications (Gannod
et al., 2000). Given this constraint, we make the
assumption that any client applications utilizing the
wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We Ô¨Ånd this assumption to be reasonable due to
the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage proÔ¨Åles.
In our approach, the speciÔ¨Åcation that is needed to
generate wrappers contains properties associated with
the ports as shown in Fig. 2. These properties include
Signature, Command, Pre, Post, Path, Interface, and
Return. In this case, the speciÔ¨Åcation describes the
NetworkSniÔ¨Éng and PortMonitor services, which are
services created by wrapping tcpdump, and nmap,
respectively. In the synthesis process, ACME speciÔ¨Åca-

Fig. 2. ACME services section.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

tions are combined with a standard template that
implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we
have constructed to automate this process generates the
appropriate source code for facilitating interaction between a potential client and the wrapped component. At
present, this is an automated tool that generates fully
executable code for the wrapped application and does
not require the user to modify or write any new code
outside of option GUI code.
Both the service and client synthesis steps utilize a
template-based approach to synthesize code. That is, a
standard Ô¨Åle has been created that has stubs containing
place holders that must be instantiated with either service or client speciÔ¨Åc parameters. Fig. 3 contains a
portion of the ServiceTemplate Ô¨Åle which contains all of
the application and service independent source code and
provides the routines necessary to integrate the legacy
code into a Jini network. SpeciÔ¨Åcally, the ServiceTemplate contains functions that implement the discover and
join protocol for registering a service with the lookup
service. The ServiceTemplate also contains tags that are
place-holders for the automatically generated functions.
For instance, in Fig. 3 the tag <put-ServerName> is
a place-holder for the Ô¨Ånal name of the adapter component.
In addition to the ServiceTemplate, there is also a
reusable set of functions that can be utilized in an
interface speciÔ¨Åcation and consequently in the generated
wrappers. For instance, the getOutputStream( )
routine (shown in Fig. 4) is available as a function for
use within the Java code to provide standard stream
input support.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

5

The amount of automation that has been achieved
through the approach described above is dependent on
the degree of graphical user interface (GUI) support
that is desired. For a service, the code synthesis step can
be fully automated if no GUI support is desired.
Otherwise, the amount of manual code construction is
limited to GUI support.
3.4. Client generation
Once the services are generated and stored in a
repository, a client application can be architected. First
we need to specify the client application taking into
account the architectural style of each of the services.
Once a client is speciÔ¨Åed, it can be veriÔ¨Åed and generated. In this subsection we look at the requirements for
specifying the client and then describe synthesis of the
client.
3.4.1. SpeciÔ¨Åcation
Refer again to Table 1 which, in addition to the
properties for service speciÔ¨Åcations, contains the properties of client application components and connectors.
When dealing with integration at the component level,
two issues arise (among others) that are of interest. First,
the problem of architectural style mismatch (Shaw and
Garlan, 1996) occurs when the underlying assumptions
made by components conÔ¨Çict. Second, most modern
applications provide a graphical user interface (GUI). As
a result, integration of oÔ¨Ä-the-shelf components can
leverage these user interfaces in order to take advantage
of previously built technology. To cope with these issues
we impose two requirements on the speciÔ¨Åcation of client
applications as follows: (C1) the speciÔ¨Åcation of the
components should capture the notion of architectural
style so that the high-level interaction between clients
and services can be veriÔ¨Åed, and (C2) the speciÔ¨Åcation
must facilitate the use of shared and exclusive GUI
components.
The requirement C1 addresses the fact that a component must provide a notion of architectural style. A
component‚Äôs style plays a very important role when it
interacts with other components by imposing interaction
constraints. Using a basic style attribute (by name)
architectural mismatches can be determined by simple
keyword matching.
Requirement C2 addresses the fact that a service may
provide a GUI that allows a user to access and control
the service. In this context, there may be GUI components provided by services that are either sharable by
other services or exclusive to the service. A sharable
GUI component can be used by both the client as well as
other integrated services while an exclusive GUI component can only be used by the service that provides the
interface.

ARTICLE IN PRESS
6

G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

3.4.2. Synthesis
The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample speciÔ¨Åcation of a client. The information contained within
client speciÔ¨Åcations are used to support the synthesis of
client code. This synthesis step utilizes two features; Ô¨Årst,
the information regarding connectors and attachments,
such as those shown in Fig. 5 are used to determine the
relationships between client applications and desired
services. Second, information regarding GUIs provided
by services is used to determine how to realize the GUI
in a client application.
In our framework, the wrappers for the various services can implement a common interface that allows the
client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially
used across multiple services and are identiÔ¨Åed using a
name taken from a standard GUI vocabulary (for
example ‚Äò‚ÄòResultsWindow‚Äô‚Äô). The name is then used to
identify which GUI components can be shared across
services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive
component is independent and cannot be shared between services. The exclusive GUI components of the
wrappers are used as is but may interact with one or
more of the shared components. For both shared and
exclusive components, the interaction with the client
GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers.
3.5. Discussion
As stated in Section 1, the service-oriented domain
are characterized by modularity, availability, description, implementation-independence, and publication. As
a result, services and service-based approaches are more
coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a
software architecture to specify applications that operate under these characteristics. As such, a software
architecture in this context deÔ¨Ånes components, their
interfaces, and the mechanisms by which services (as
components) can be joined in order to fulÔ¨Åll needed
software behavior. Consequently, services enable the use
of a software architecture as an integration vehicle in
which the architecture facilitates generation of glue
code. It is the very fact that services adhere to the
characteristics described above that the integration and
code generation become possible at this level. However,
the approach does lack in its ability to address needs
that are more speciÔ¨Åc than what individual services
provide. To cope with this, we are developing an approach that allows for the creation of federated services,
where services are combined to meet some higher-level
objective.

4. Related work

Fig. 5. Portion of ACME client speciÔ¨Åcation.

Recently, the use of web services has gained attention
with vendors releasing webservices toolkits that allow
for building and using webservices. Webservices and
.NET (Meyer, 2001) are based on the SOAP and XML
(Seely and Sharkey, 2001) protocols. The Jini approach
to service integration goes beyond what the webservices
paradigm provides by deÔ¨Åning how services can be used
within a larger application context and providing support for code transportation.
FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server
that distributed messages to other tools that were
interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this
approach is a precursor to service-based development.
Urnes and Graham (1999) describe an approach to
facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the
component space across a network. In our approach,
services are potentially developed by diÔ¨Äerent organizations and thus the choice of what to distribute is not

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

available. The component model being addressed by
Urnes and Graham, as such, is Ô¨Åner-grained and violates implementation-independence, a tenet of servicebased development.
Grundy et al. (2000) discuss issues and experiences in
constructing component-based software engineering
environments. They created a variety of useful software engineering tools using their tool set (JViews,
JComposer, etc.). They use ‚Äò‚Äòplug and play‚Äô‚Äô and an
event-based composition approach to achieve component integration. In this framework, components are
more tightly coupled and their granularity is Ô¨Ånegrained. In contrast, our approach is based on dynamic
integration of coarse-grained services that are loosely
coupled.
Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and
component gluing. This creates a clean separation of
customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work
provides a potential strategy for dealing with component mismatches, which is currently ignored in our
approach.

5. Conclusions
The web-based services paradigm has gained attention
recently with the development of technologies such as
SOAP (Seely and Sharkey, 2001). The beneÔ¨Åts of such
technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these beneÔ¨Åts by providing
facilities for on-the-Ô¨Çy integration and component introspection. In this paper, we described an approach for
addressing component integration via the use of services
in the context of Jini interconnection technology. SpeciÔ¨Åcally, the approach utilizes synthesis to generate code
necessary to realize component integration. To facilitate
integration, the ACME ADL is used to specify both
services and target applications, and is used a medium
for performing service compatibility checking.
We are currently developing an environment that will
assist in the creation of applications within the servicebased paradigm and will support service browsing to
facilitate application design. In addition, we are investigating approaches for allowing services to collaborate
beyond the scope of a client application in order to
create federated groups of services. Furthermore, we are
developing technologies similar to the ones described in
this paper in order to support service-based application
within the .NET and web service frameworks.

7

Acknowledgements
G. Gannod is supported in part by NSF CAREER
grant CCR-0133956.
References
Edwards, W.K., 1999. Core Jini. Prentice-Hall.
Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services.
Commun. ACM 45 (10), 77‚Äì80.
Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design
Patterns: Elements of Reusable Object-Oriented Software. Addison Wesley Longman.
Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for
legacy software. In: Proc. 7th Working Conf. Reverse Eng., IEEE,
pp. 128‚Äì137.
Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture
description interchange language. In: Proc. CASCON‚Äô97, pp. 69‚Äì
183.
Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2).
Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to
develop a family of applications. IEE Proc. Software Eng. 144 (5‚Äì
6), 237‚Äì248.
Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92‚Äì97.
Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration
with pluggable composite adapters. Software Archit. Comp.
Technol..
Reiss, S.P., 1990. Connecting tools using message passing in Ô¨Åeld
environment. IEEE Software 7 (7), 57‚Äì66.
Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services
Development Using XML. Prentice-Hall.
Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on
an Emerging Discipline. Prentice-Hall.
Stal, M., 2002. Web services: beyond component-based computing.
Commun. ACM 45 (10), 71‚Äì76.
Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In Proc. of
Design, SpeciÔ¨Åcation and VeriÔ¨Åcation of Interactive Systems.
Gerald C. Gannod is an Assistant Professor in the Department of
Computer Science and Engineering at Arizona State University and is
a recipient of a 2002 NSF CAREER Award. He received the M.S.
(1994) and Ph.D. (1998) degrees in Computer Science from Michigan
State University. His research interests include software product lines,
software reverse engineering, formal methods for software development, software architecture, and software for embedded systems.
Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona
State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras
(Chennai), India. His research interests include software engineering,
distributed and object-oriented systems, software design, software
architecture, service-oriented software engineering, and Wireless
Application platforms.
Timothy E. Lindquist is Professor and Chair in the Department of
Electronics and Computer Engineering Technology at Arizona State
University East Campus in Mesa, Arizona. He received the Ph.D.
(1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

1

On time-reversibility of linear stochastic models
Tryphon T. Georgiou and Anders Lindquist

Abstract Reversal of the time direction in stochastic systems driven by white noise has been central throughout the development of stochastic realization theory, filtering and smoothing. Similar ideas were developed in connection with certain problems in the theory of moments, where a duality induced by time reversal was introduced to parametrize solutions. In this latter work it was shown that stochastic systems driven by arbitrary second-order stationary processes can be similarly time-reversed. By combining these two sets of ideas we present herein a generalization of time-reversal in stochastic realization theory.

arXiv:1309.0165v1 [cs.SY] 31 Aug 2013

I. I NTRODUCTION Time reversal of stochastic systems is central in stochastic realization theory (see, e.g., [1], [2], [3], [4], [5], [6], [7], [8]), filtering (see [9]), smoothing (see [10], [11], [12]) and system identification. The principal construction is to model a stochastic process as the output of a linear system driven by a noise process which is assumed to be white in discrete time, and orthogonal-increment in continuous time. In studying the dependence between past and future of the process, it is natural to decompose the interface between past and future in a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward or backward in time. In a different context (see [13]) a certain duality between the two time-directions in modeling a stochastic process was introduced in order to characterize solutions to moment problems. In this new setting the noise-process was general (not necessarily white), and the correspondence between the driving inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics. In the present note, we combine these two sets of ideas to develop a general framework where two time-opposite stochastic systems model a given stochastic process. We study the relationship between these systems and the corresponding processes. In particular, we recover as a special case certain results of stochastic realization theory ([1], [5], [10]) from the 1970's using a novel procedure. In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving generating processes, in opposite time-directions, via causal and anti-causal mappings, respectively. In Section III we utilize this mechanism in the context of general output processes and, similarly, introduce a pair of time-opposite models. Finally, in Section IV, we draw connection to literature on time reversibility and related issues in physics, and we indicate directions for future research. II. S TATE
DYNAMICS AND ALL - PASS EXTENSION

In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. As usual, in discrete-time these take the form of a set of difference equations x(t + 1) = Ax(t) + Bu(t) (1)

This research was supported by grants from AFOSR, NSF, VR, and the SSF. Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota, tryphon@umn.edu Department of Automation, Shanghai Jiao Tong University, Shanghai, China, and Center for Industrial and Applied Mathematics and ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden, alq@kth.se

2

where t  Z, A  Rn◊n , B  Rn◊p , n, p  N, A has all eigenvalues in the open unit disc D = {z | |z | < 1}, and u(t), x(t) are stationary vector-valued stochastic processes. The system of equations is assumed to be reachable, i.e., rank B, AB, . . . An-1 B = n, and non-trivial in the sense that B is full rank. In continuous-time, state-dynamics take the form of a system of stochastic differential equations dx(t) = Ax(t)dt + Bdu(t) (3) (2)

where, here, u(t), x(t) are stationary continuous-time vector-valued stochastic processes. Reachability (which in this case, is equivalent to controllability) of the pair (A, B ) is also assumed throughout and the condition for this is identical to the one for discrete-time given above (as is well known). In continuous time, stability of the system of equations is equivalent to A having only eigenvalues with negative real part, and will be assumed throughout along with the condition that B has full rank. In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system is all-pass. This is done next. The assumptions of stationarity and constant parameter matrices is made for simplicity of notation and brevity and can be easily removed. A. All-pass extension in discrete-time Consider the discrete-time Lyapunov equation P = AP A + BB  . (4)

Since A has all eigenvalues inside the unit disc of the complex plane and (2) holds, (4) has as solution a matrix P which is positive definite. The state transformation  = P - 2 x, and F = P - 2 AP 2 , G = P - 2 B, brings (1) into  (t + 1) = F  (t) + Gu(t). (7)
1 1 1 1

(5)

(6)

For this new system, the corresponding Lyapunov equation X = F XF  + GG has In as solution, where In denotes the (n ◊ n) identity matrix. This fact, namely, that In = F F  + GG implies that this [F, G] can be embedded as part of an orthogonal matrix U= i.e., such that UU  = U  U = In+p . Define the transfer function U(z ) := H (zIn - F )-1 G + J (10) F G H J , (9) (8)

3

corresponding to  (t + 1) = F  (t) + Gu(t) u Ø(t) = H (t) + Ju(t). This is also the transfer function of x(t + 1) = Ax(t) + Bu(t) Ø  x(t) + Ju(t), u Ø(t) = B
1 Ø := P - 2 where B H , since the two systems are related by a similarity transformation. Hence,

(11a) (11b)

(12a) (12b)

Ø  (zIn - A)-1 B + J. U(z ) = B

(13)

We claim that U(z ) is an all-pass transfer function (with respect to the unit disc), i.e., that U(z ) is a transfer function of a stable system (obvious) and that U(z )U(z -1 ) = U(z -1 ) U(z ) = Ip . The latter claim is immediate after we observe that, since U  U = In+p , U and hence,  (t) = F   (t + 1) + H  u Ø(t)   u(t) = G  (t + 1) + J u Ø(t) or, equivalently, x(t) = P AP -1 x(t + 1) + P 2 H  u(t) u(t) = B  P -1 x(t + 1) + J  u Ø(t). Setting x Ø(t) := P -1x(t + 1), (16) can be written Øu x Ø(t - 1) = A x Ø(t) + B Ø(t)   u(t) = B x Ø(t) + J u Ø(t) with transfer function Ø + J . U(z ) = B  (z -1 In - A )-1 B Either of the above systems inverts the dynamical relation u  u Ø (in (12) or (11)). (19) (18a) (18b) (17)
1

(14)

 (t + 1) u Ø(t)

=

 (t) u(t)

,

(15a) (15b)

(16a) (16b)

u(t )

U

u Ø(t) 

Fig. 1.

Realization (12) in the forward time-direction.

4



u(t)

U



u Ø(t)

Fig. 2.

Realization (18) in the backward time-direction.

B. All-pass extension in continuous-time Consider the continuous-time Lyapunov equation AP + P A + BB  = 0. (20)

Since A has all its eigenvalues in the left half of the complex plane and since (2) holds, (20) has as solution a positive definite matrix P . Once again, applying (5-6), the system in (3) becomes d (t) = F  (t)dt + Gdu(t). We now seek a completion by adding an output equation du Ø(t) = H (t)dt + Jdu(t) so that the transfer function U(s) := H (sIn - F )-1 G + J is all-pass (with respect to the imaginary axis), i.e., U(s)U(-s) = U(-s) U(s) = Ip . (23) (22) (21b) (21a)

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and hence, F + F  + GG = 0. Utilizing this relationship we note that (sIn - F )-1 GG (-sIn - F  )-1 = (sIn - F )-1 (sIn - F - sIn - F  )(-sIn - F  )-1 = (sIn - F )-1 + (-sIn - F  )-1 , and we calculate that U(s)U(-s) = (H (sIn - F )-1 G + J )(G (-sIn - F  )-1 H  + J  ) = JJ  + H (sIn - F )-1 (GJ  + H  ) (JG + H )(-sIn - F  )-1 H  . For the product to equal the identity, JJ  = Ip H = -JG . Thus, we may take J = Ip H = -G , (24)

5

and the forward dynamics d (t) = F  (t)dt + Gdu(t) du Ø(t) = -G  (t)dt + du(t). Substituting F = -F  - GG from (24) into (25a) we obtain the reverse-time dynamics d (t) = -F   (t)dt + Gdu Ø(t)  du(t) = G  (t)dt + du Ø(t). Now defining x Ø(t) := P -1x(t) and using (5) and (6), (26) becomes Ø u dx Ø(t) = -A x Ø(t)dt + Bd Ø(t)  du(t) = B x Ø(t)dt + du Ø(t), with transfer function Ø + Ip , U(s) = B  (sIn + A )-1 B where Ø := P -1 B. B (29) (30) (28a) (28b) (27) (26a) (26b) (25a) (25b)

Furthermore, the forward dynamics (25) can be expressed in the form dx(t) = Ax(t)dt + Bdu(t) Ø  x(t)dt + du(t) du Ø(t) = B with transfer function Ø  (sIn - A )-1 B + Ip . U(s) = B III. T IME - REVERSAL
OF LINEAR STOCHASTIC SYSTEMS

(31a) (31b)

(32)

The development so far allows us to draw a connection between two linear stochastic systems having the same output and driven by a pair of arbitrary, but dual, stationary processes u(t) and u Ø(t), one evolving forward in time and one evolving backward in time. When one of these two processes is white noise (or, orthogonal increment process, in continuous-time), then so is the other. For this special case we recover results of [1] and [5] in stochastic realization theory. A. Time-reversal of discrete-time stochastic systems Consider a stochastic linear system x(t + 1) = Ax(t) + Bu(t) y (t) = Cx(t) + Du(t) (33a) (33b)

with an m-dimensional output process y , and x, u, A, B are defined as in Section II-A. All processes are stationary and the system can be thought as evolving forward in time from the remote past (t = -). In particular, x(t + 1) is Ftu -measurable y (t)

6

for all t  Z, where Ftu is the  -algebra generated by {u(s) | s  t}. Next we construct a stochastic system Øu x Ø(t - 1) = A x Ø(t) + B Ø(t) Øx Øu y (t) = C Ø(t) + D Ø(t), which evolves backward in time from the remote future (t = ), and for which x Ø(t - 1) y (t)
Ø Øu is F t -measurable

(34a) (34b)

Ø Øu for all t  Z, where F Ø(s) | s  t}. The processes x Ø, x, u Ø, u relate as in t is the  -algebra generated by {u the previous section. More specifically, as shown in Section II-A,

u Ø(t) is Ftu -measurable while
Ø Øu u(t) is F t -measurable

for all t, as examplified in Figures 1 and 2. In fact, the all-pass extension (12) of (33a) yields Ø  x(t) + Ju(t) u Ø(t) = B It follows from (18b) that (35) can be inverted to yield u(t) = B  x Ø(t) + J  u Ø(t), where x Ø(t) = P -1 x(t + 1), and that we have the reverse-time recursion Øu x Ø(t - 1) = A x Ø(t) + B Ø(t). Then inserting (36) and into (33b), we obtain Ø := DJ  and where D Øu x(t) = P x Ø(t - 1) = P A x Ø(t) + P B Ø(t) Øx Øu y (t) = C Ø(t) + D Ø(t), Ø := CP A + DB  . C (37b) (38) (37a) (36) (35)

Then, (37) is precisely what we wanted to establish. Moreover, the transfer functions W(z ) = C (zIn - A)-1 B + D of (33) and Ø (z ) = C Ø (z -1 In - A )-1 B Ø +D Ø W of (34) satisfy Ø (z )U(z ). W (z ) = W (41) (40) (39)

In the context of stochastic realization theory, discussed next, U(z ) is called structural function ([3], [4]).

7

u(t )

W

y (t) 

Fig. 3.

The forward stochastic system (33).



y (t)

Ø W



u Ø(t)

Fig. 4.

The backward stochastic system (34)

1) Time-reversal of stochastic realizations.: Given an m-dimensional stationary process y , consider a minimal stochastic realization (33), evolving forward in time, where now u is a normalized white noise process, i.e., E{u(t)u(s)} = Ip t-s . Since U, given by (13), is all-pass, u Ø is also a normalized white noise process, i.e., E{u Ø(t)Ø u(s) } = Ip t-s . From the reverse-time recursion (34a)


x Ø(t) =

Øu (A )k-(t+1) B Ø (k ).
k =t+1

Since, u Ø is a white noise process, E{x Ø(t)Ø u(s) } = 0 for all s  t. Consequently, (34) is a backward stochastic realization in the sense of stochastic realization theory. B. Time-reversal of continuous-time stochastic systems We now turn to the continuous-time case. Let dx = Axdt + Bdu dy = Cxdt + Ddu (42a) (42b)

be a stochastic system with x, u, A, B as in Section II-B, evolving forward in time from the remote past (t = -). All processes have stationary increments and x(t) y (t) is Ftu -measurable

for all t  R, where Ftu is the  -algebra generated by {u(s) | s  t}. The all-pass extension of Section II-B yields Ø  xdt du Ø = du - B as well as the reverse-time relation Ø u dx Ø = -A x Ødt + Bd Ø  du = B x Ødt + du Ø, (44a) (44b) (43)

8

where x Ø(t) = P -1 x(t). Inserting (44b) into dy = CP x Ødt + Ddu yields Øx dy = C Ødt + Ddu Ø, where Ø = CP + DB  . C Thus, the reverse-time system is Ø u dx Ø = -A x Ødt + Bd Ø Øx dy = C Ødt + Ddu Ø. From this, we deduce that x Ø(t) y (t)
Ø Øu is F t -measurable

(45)

(46a) (46b)

for all t  R. We also note that the transfer function W(s) = C (sIn - A)-1 B + D of (42) and the transfer function Ø ( s) = C Ø (sIn + A )-1 B Ø +D W of (46) also satisfy Ø (s)U(s) W ( s) = W as in discrete-time. 1) Time-reversal of stochastic realizations.: In continuous-time stochastic realization theory, (42) is a forward minimal stochastic realization of an m-dimensional process y with stationary increments provided u is a normalized orthogonal-increment process satisfying E{du(t)du(t)} = Ip dt. Since U(s) is all-pass, Ø  xdt du Ø = du - B also defines a stationary orthogonal-increment process u Ø such that E{du Ø(t)du Ø(t) } = Ip dt. It remains to show that (46) is a backward stochastic realization, that is, at each time t the past increments of u Ø are orthogonal to x Ø(t). But this follows from the fact that


(47)

x Ø(t) =
t

Ø u e-A (t-s) Bd Ø ( s)


and u Ø has orthogonal increments.

9

IV. C ONCLUDING

REMARKS

The direction of time in physical laws and the fact that physical laws are symmetric with respect to time have occupied some of the most prominent minds in science and mathematics ([14], [15], [16]). These early consideration were motivated by no less an issue than that of the very nature of the quantum. Indeed, Erwin Schr® odinger's aim appears to have been to draw a classical analog to his famous equation. A large body of work followed. In particular, closer to our immediate interests, dual time-reversed models have been employed to model, in different time-directions, Brownian or Schr® odinger bridges (see [17], [18]), a subject which is related to reciprocal processes ([19], [20], [21], [22]). The topic of time reversibility has also been central to thermodynamics, and in recent years studies have sought to elucidate its relation to systems theory (see [23], [24]). Possible connections between this body of work and our present paper will be the subject of future work. The thesis of the present work is that under mild assumptions on a stochastic process, any model that consists of a linear stable dynamical system driven by an appropriate input process can be reversed in time. In fact, a reverse-time dual system along with the corresponding input process can be obtained via an all-pass extension of the state equation. The correspondence between the two input processes can be expressed in terms of each other by a causal and an anti-causal map, respectively. The formalism of our paper can easily be extended to a non-stationary setting at a price of increased notational, but not conceptual, complexity. Informally, and in order to underscore the point, if u(t) is a non-stationary process and the linear system is time-varying, under suitable conditions, a reverse-time system and a process u Ø(t) can be similarly constructed via a time-varying orthogonal transformation. R EFERENCES
[1] A. Lindquist and G. Picci, "On the stochastic realization problem," SIAM J. Control Optim., vol. 17, no. 3, pp. 365≠389, 1979. [2] ----, "Forward and backward semimartingale models for Gaussian processes with stationary increments," Stochastics, vol. 15, no. 1, pp. 1≠50, 1985. [3] ----, "Realization theory for multivariate stationary Gaussian processes," SIAM J. Control Optim., vol. 23, no. 6, pp. 809≠857, 1985. [4] ----, "A geometric approach to modelling and estimation of linear stochastic systems," J. Math. Systems Estim. Control, vol. 1, no. 3, pp. 241≠333, 1991. [5] M. Pavon, "Stochastic realization and invariant directions of the matrix Riccati equation," SIAM Journal on Control and Optimization, vol. 18, no. 2, pp. 155≠180, 1980. [6] A. Lindquist and M. Pavon, "On the structure of state-space models for discrete-time stochastic vector processes," IEEE Trans. Automat. Control, vol. 29, no. 5, pp. 418≠432, 1984. [7] G. Michaletzky, J. Bokor, and P. V¥ arlaki, Representability of stochastic systems. Budapest: Akad¥ emiai Kiad¥ o, 1998. [8] G. Michaletzky and A. Ferrante, "Splitting subspaces and acausal spectral factors," J. Math. Systems Estim. Control, vol. 5, no. 3, pp. 1≠26, 1995. [9] A. Lindquist, "A new algorithm for optimal filtering of discrete-time stationary processes," SIAM J. Control, vol. 12, pp. 736≠746, 1974. [10] F. Badawi, A. Lindquist, and M. Pavon, "On the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear stochastic systems," in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE, 1979, pp. 505≠510. [11] F. A. Badawi, A. Lindquist, and M. Pavon, "A stochastic realization approach to the smoothing problem," IEEE Trans. Automat. Control, vol. 24, no. 6, pp. 878≠888, 1979. [12] A. Ferrante and G. Picci, "Minimal realization and dynamic properties of optimal smoothers," Automatic Control, IEEE Transactions on, vol. 45, no. 11, pp. 2028≠2046, 2000. [13] T. T. Georgiou, "The Carath¥ eodory≠Fej¥ er≠Pisarenko decomposition and its multivariable counterpart," Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 212≠228, 2007. ® [14] E. Schr® odinger, Uber die Umkehrung der Naturgesetze. Akad. d. Wissenschaften, 1931. [15] A. Kolmogorov, Selected Works of AN Kolmogorov: Probability theory and mathematical statistics. Springer, 1992, vol. 26. [16] A. Shiryayev, "On the reversibility of the statistical laws of nature," in Selected Works of AN Kolmogorov. Springer, 1992, pp. 209≠215. [17] M. Pavon and A. Wakolbinger, "On free energy, stochastic control, and Schr® odinger processes," in Modeling, Estimation and Control of Systems with Uncertainty. Springer, 1991, pp. 334≠348. [18] P. Dai Pra and M. Pavon, "On the Markov processes of Schr® odinger, the Feynman-Kac formula and stochastic control," in Realization and Modelling in System Theory. Springer, 1990, pp. 497≠504.

10

[19] B. Jamison, "Reciprocal processes," Probability Theory and Related Fields, vol. 30, no. 1, pp. 65≠86, 1974. [20] A. Krener, "Reciprocal processes and the stochastic realization problem for acausal systems," in Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197≠211. [21] B. C. Levy, R. Frezza, and A. J. Krener, "Modeling and estimation of discrete-time gaussian reciprocal processes," Automatic Control, IEEE Transactions on, vol. 35, no. 9, pp. 1013≠1023, 1990. [22] P. Dai Pra, "A stochastic control approach to reciprocal diffusion processes," Applied mathematics and Optimization, vol. 23, no. 1, pp. 313≠329, 1991. [23] W. M. Haddad, V. Chellaboina, and S. G. Nersesov, "Time-reversal symmetry, poincar¥ e recurrence, irreversibility, and the entropic arrow of time: From mechanics to system thermodynamics," Nonlinear Analysis: Real World Applications, vol. 9, no. 2, pp. 250≠271, 2008. [24] ----, Thermodynamics: A dynamical systems approach. Princeton University Press, 2009.

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

A Java Cryptography Service Provider Implementing One-Time Pad
Timothy E. Lindquist, Mohamed Diarra, and Bruce R. Millard
Electronics and Computer Engineering Technology
Arizona State University East
http://www.east.asu.edu/ctas/ecet
mailto:Tim@asu.edu

Abstract
Security is a challenging aspect of communications
today that touches many areas including memory space,
processing speed, code development and maintenance
issues. When it comes to dealing with lightweight computing devices, each of these problems is amplified. In
an attempt to address some of these problems, SUN‚Äôs
Java 2 Standard Edition version 1.4 includes the Java
Cryptography Architecture (JCA). The JCA provides a
single encryption API for application developers within
a framework where multiple service providers may
implement different algorithms. To the extent possible
application developers have available multiple encryption technologies through a framework of common
classes, interfaces and methods.
The One Time Pad encryption method is a simple and
reliable cryptographic algorithm whose characteristics
make it attractive for communication with limited computing devices. The major difficulty of the One-Time pad
is key distribution.In this paper, we present an implementation of One-Time Pad as a JCA service provider,
and demonstrate its usefulness on Palm devices.

1. Problem
Dependence on the communications infrastructure
continues to grow as the size of computing devices
decreases. The growing dependence on Internet accessibility to services that do not reside in a local machine
brings with it the need for secure communications. The
target of this work are relatively small devices and their
related systems, such as Windows CE, PalmTE, Handspring and cell phones used to access Internet services.
While several large computer service organizations have
spent millions of dollars recovering from cyber attacks,
the potential economic impact of insecure e-commerce
communications on limited devices is huge[1], [3].

Java continues to enjoy dominance in server-side
technologies, however, a small but growing number of
limited device applications are developed in Java. Nevertheless, Sun Microsystems Inc., added Java Cryptography Extension (JCE) and JCA (to the Java T M 2
Development Kit Standard Edition v1.4 (J2SDK), and
has created a substantial market for applications running
on J2ME (Java 2 Micro Edition). Other vendors are
offering Java runtimes for limited devices. These versions bring Java to client application developers [9],
[11], and raise the issue of appropriate Java-based security mechanisms.
J2ME does not include JCE and JCA, however The
Legion Of The Bouncy Castle has developed a lightweight Cryptography API and a Provider for JCE and
JCA [14]. Neither provider offers implementation of the
One-Time Pad cryptography service [14].
The simplicity of the One-Time Pad method and the
fact that it does not require high processor speed, make
it ideal for lightweight computing devices.

1.1

Context

This paper focuses on integrating the JCA cryptography service provider, starting by defining the engine
classes and then implementing the One-Time Pad
method. We include simple evaluation programs to test
the provider. The problem of pad distribution is one of
the tasks taken-on in order to have successful deployment.
Implementations of the one-time pad encryption0.9.4 are readily available. For example, one product is
available for Windows command line launching. The
source code written in ANSI-C and DOS executable are
available for download at http://www.vidwest.com/otp/
[1].
The Security documentation provided with J2SDK
includes detailed information on the implementation of

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

1

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

the Provider for the JCA [12]. The documentation for ‚Äúa
Provider for JCE and JCA‚Äù of The Legion Of The
Bouncy Castle is also available [14].
The possibility of using the One-Time pad for data
encryption and decryption for security purposes on
lightweight computing devices was covered at the 35th
Hawaii International Conference on System Science
2002 [2].

Sender

Message

2. One Time Pad
The one-time pad algorithm is among the simplest in
the world of cryptography and is considered by some to
be unbreakable. It is nothing more than an exclusive OR
between the message (to be encrypted) and the pad (a
random key - sequence of bits). The principles that govern the encryption technique are not that simple to
apply. First, the key must be random, which by itself is a
big challenge. Second, parts of the key that have already
been used to do encryption must not be available for
other encryption. The key (Pad) must be a sequence of
random bits as long as the message to be encrypted. The
sender exclusive-OR's the message with the pad and
sends the result through a communication channel. The
one time requirement that makes it unbreakable and difficult at the same time is that after use, the sender must
get rid of part of the pad, and not use it again. At the
other end of the communication, the receiver must have
an identical copy of the pad. The receiver decrypts the
cipher text to obtain the original message by doing an
exclusive-OR of the incoming cipher with its copy of
the pad [1], [3], [4]. The receiver should also destroy the
pad after use. See Figure 1.

2.1

Advantages of the One Time Pad

If the pad is actually random and has been distributed
securely to the receiver, then no third party can decrypt
the message. Even guessing part of the key will not
allow a third party to determine the remainder. This is
why some people claim that one-time pad is unbreakable. While there are a number of very good pseudo-random number generators, so far any attempt to generate a
truly random key with computers appears to generate
the same sequence after a certain point. Several
approaches avoid this problem by personalizing the key.
Another advantage of this technique resides in the simplicity of its algorithm. It does not involve complex
operations that challenge the computational speed of
some relatively small processors.

XOR

Encrypted
message
transmitted
normally

OTP
Receiver

Encrypted
message
transmitted
normally

XOR

Original
Message

OTP

Figure 1. One-time pad (OTP) cryptography.

2.2

Disadvantages of the One Time Pad

The key must be as large as the message being
encrypted; this fact is sometimes inconvenient especially in the case of large messages. The principle of the
one-time pad is to have a unique key for each communication, which makes the generation and management of
keys problematic as the number of recipients and frequency of use escalates. The last challenging aspect of
the One Time Pad is the Key distribution. In fact the key
should remain undisclosed (secret) to any other party
besides the communicating parties. Extensions to the
One-Time Pad provider discussed in this paper center on
portable memory devices [15][16] that are frequently
synchronized with more capable machine (laptops or
desktops) for key exchange. However, our implementation is aimed at handheld devices in general. General
purpose (non-proprietary) portable memory interfaces
for handheld devices don‚Äôt exist yet, so another
approach is necessary. Some possibilities are discussed
in the Key Management section below. Each application
must deal with this issue in its own effective way(s).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

2

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3. Java Cryptography Architecture
The Java security model has been evolving to adjust
to new security issues. The JCA is a framework providing cryptography functionality development capabilities
for a Java platform. It was introduced early in Java‚Äôs
evolution as an add-on package. The first release of the
Security API was an extension of JCA including API‚Äôs
for encryption, key exchange, and coding message
authentication. Prior to J2SDK 1.4, JCE was optional, in
part due to export restrictions. The ‚ÄúJava Secure Socket
Extension‚Äù (JSSE) and ‚ÄúJava Authentication and Authorization Service‚Äù (JAAS) security features have also
been integrated into the J2SDK, version 1.4. Two new
security features have been introduced: ‚ÄúJava GSS-API‚Äù
(Java Generic Security Services Application Program
Interface) that can be used for securely exchanging messages between communicating applications using the
Kerberos V5 mechanism and ‚ÄúJava Certification Path
API‚Äù that includes classes and methods in the java.security.cert package. These classes allow the developer to
build and validate certificate chains.
The java cryptography architecture includes a provider architecture [2], [5], [6], [7], [10], [12]. The notion
of Cryptography Service Provider (CSP), or just provider, has been introduced in JCA. The provider archit e ct u r e a l l o w s f o r m u l t i p l e an d i n t er o p er ab l e
cryptography implementations. An application developer can create or specify his/her own cryptography service provider. The service provider interface (SPI)
presents a single interface for implementors. Classes,
methods and properties are accessible to applications
through the JCA application program interface (API).
The SPI allows a cryptography service provider to plugin implementations for java applications. A provider can
be used to implement any security service. Several providers can be available and they may or may not provide
similar cryptography services and algorithms. Figure 2
depicts the layers of Java Cryptography Architecture,
and is taken from Sun Java documentation [6].
A given installation of J2SDK may have several
cryptography service providers installed, which may
provide implementations of different algorithms and/or
may provide multiple implementations of a single cryptography algorithm. Each provider has a name that is
used by application programmers to specify the desired
provider. It is also possible to specify the order of preference of providers. The default provider that comes
with the J2SDK is the Sun provider, which includes a

wide variety of cryptographic algorithms and tools [2],
[5], [6], [7], [10], [12].

Figure 2. Java cryptography architecture

3.1

Java Cryptographic Service Providers

The Java Cryptography Architecture, which
includes the provider(s), has two main design principles,
First, is independence from implementation and interoperability: This derives from using the services without
knowing their implementation details [12]. Second, is
algorithm independence and extensibility; meaning that
new service providers and/or algorithms can be added
without effecting existing providers. Together these provide a modular architecture that allows for encryption to
be done by an implementation of a specific algorithm
and subsequent decryption to be done by another implementation.

3.2

Provider Implementing One Time Pad

The primary question when building such a provider
is: does the nature of the one-time pad allow it to be
implemented in a pluggable architecture? Figure 1
shows the interoperability between Sender and Receiver
as independent systems in communication. The element
they are required to have in common is the key. The
implementation of the algorithm does not matter. As far
as extensibility is concerned, it is up to the provider programmer to remain independent of other cryptographic
services.
A Cryptographic Service provider is a package or set
of packages providing concrete implementations of a
subset of the cryptography portion of the Java security
SPI. The Java Security Guide in J2SDK, v1.4 documentation lists a series of nine steps to follow for implementing a Provider. This paper follows those steps as
guidelines for its development.
Two aspects in the structures of cryptographic service were needed to write the implementation code: the
Engine Class and the Service Provider Interface (SPI).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

3

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3.3

Engine Class

An Engine Class is an abstraction of a cryptographic
service. It defines the service without a concrete implementation of the particular associated algorithms. Applications access instances of the engine class through the
API, to carry out available operations. Every engine
class has a corresponding service provider interface,
which provides abstract classes accessing the engine
class features. The service provider interface indicates
all the methods that the actual cryptographic service
provider should implement for a particular cryptographic algorithm or type. A service provider interface
is named with its engine class name followed by ‚ÄúSpi‚Äù
[12].
For each service that a provider implements, we must
define the engine class, and then write its service provider interface. For the One Time Pad technique, the
service provider interface‚Äôs abstract class is called OneTimePadSpi. The engine class for OneTimePadSpi in
compliance to the nomenclature of JCA is called OneTimePad. The engine class is a concrete subclass of the
service provider interface, implementing all the abstract
methods.
The provider class is a final subclass of java.security.provider. Our provider is named ASUEcetProvider.
The provider name is used by applications to access our
one-time pad service [12].

3.4

Provider‚Äôs Information

The provider class provides access to various properties of the service, including the version, and other information about the service(s) it provides such as
algorithm, type, and techniques [2], [12]. The value provided for this argument in this project is: ‚ÄúASUEcetProvider v1.0, implementing One Time Pad (OTP)
cryptographic technique, Arizona State University East,
Electronic and Computer Engineering Technology. May
2003‚Äù

3.5

Install and Configure the Provider

The provider needs to be correctly installed and configured for the application program to utilize its cryptographic service(s). There are two different ways to
install a provider. The first method consists of creating a
JAR file (Java Archive File) or ZIP file containing all
the class files belonging to the Cryptography Service
Provider. The JAR file is added to the CLASSPATH
environment variable. The exact steps of doing this last
action, depends upon the local operating system [2],
[12]. The second approach deploys the provider‚Äôs JAR
file of classes as an extension (optional package) to

Java. The file can be bundled with a particular application (with a manifest indicating relative URLs), or it can
be installed in the Java Runtime Environment to be
shared by all running applications.

3.6

Registering the Service

Configuring the service provider enables client
access to the service(s) by registering the provider and
defining default preferences where more than one provider is registered for the same service algorithm.
Static Registration consists of editing the java.security file (located in ‚Äúlib\security‚Äù subdirectory of the
Java Runtime Environment) to add the provider name to
the list of approved providers. For each available provider for a given algorithm, there is a corresponding line
in the java.security file with the form:
security.provider.<n>=<providerClassName>
Where ‚Äún‚Äù is the preference number for the provider.
For example the line:
security.provider.2=asue.provider.ASUEcetProvider
registers our provider with an order of preference 2.
Dynamic Registration can be done by a client application upon requesting service(s) from a provider. The
client application calls a class method, such as:
Security.addProvider (Provider providerName).

3.7

Test Programs and Documentation

Several test programs were written to exercise three
aspects of the service provider. For client applications to
be able to request service(s) provided by a specific provider, the provider should be successfully registered
with the security API. A simple test program can verify
registration by creating an instance of the provider and
accessing its name, version, and info (getName (), getVersion (), and getInfo () methods.
After making sure that the provider is accessible
from the security API, we need to retrieve the provided
service(s) by calling its ‚ÄúgetAlgorithm ()‚Äù method.
Finally, we check the functionality provided by the
service by writing sender and receiver applications that
use the service.
In addition to providing sample service test programs, our implementation provides documentation.
Our documentation is generated by the Javadoc tool
from the source code and it targets application programmers.

4. Key Management
The generation and distribution of the random keys
for this method is of primary concern. Since the size of
the key must match the size of the message being

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

4

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

encrypted, we limit our application to transmission of
relatively small messages. The key can be any random
array of bits, the key type used in JCA has not been
used.
The OneTimePad class has been designed, so that
when a new random key is generated, it is stored in an
external file that could be later sent to the receivers.
There are many approaches to providing random keys
for the One-Time Pad. The primary problem with using
the One-Time Pad is the amount of random bits needed.
Every message sent needs a key of the same length. If
every message is sent encrypted, the required key space
becomes large very quickly. The alternative of only
encrypting sensitive data suffers from forcing the application to make these choices, and only delays the issue
of when and how to replenish the key once all the bits
have been used on earlier messages.
Handheld devices, the target for this work, generally
only send small messages requiring encryption. This
allows for many messages using a small key of say 1
MB. A nightly synchronization using a recharging cradle can be used to also replenish the One-Time Pad key.
For devices that don‚Äôt have a connection to a host while
charging, another approach is portable memory devices
[15][16]. With a 2 GB pad, the replenishment cycle
would be much longer. Generally long enough to add a
few jpeg or gif pictures a day. The primary problem
with portable memory and handheld devices is that of
interfacing requirements. Currently, no general-purpose
interfaces (e.g., USB) are available for handhelds. Portable memory devices, to be useful, must come with general purpose interfaces, such as USB. An alternative to
the cradle and portable memory approach is to update or
replenish the pad on-line. In this approach, a One-Time
Pad is used until almost exhausted. Then the remaining
pad is used to exchange a block-cipher key for a more
computationally complex cryptography algorithm, such
as RC4 or AES. The One-Time Pad can then be generated by the server and sent to the handheld using the
complex cipher. The encryption process would then
return to the One-Time Pad methodology. This
approach is expensive on both network and CPU utilization. It may only be acceptable on larger handhelds such
as Windows CE (Pocket PC) machines. A final alternative would be to use PRNG (pseudo random number
generator) that is cryptographically appropriate. An
example PRNG is ISAAC [17]. ISAAC is a relatively
fast random number generator with a very long cycle
(i.e., guaranteed 240 with an average 28295). Generating
random pads then requires knowing the seed. Using the
prior approach of a trailing seed from the last pad as a
seed for the new pad would permit, possibly, near realtime generation of One-Time Pads.

5. Running on Limited Devices
The fact that JCA and JCE are not part of J2ME, limits applicability to our intended application space. One
approach is to configure limited client applications by
embedding the provider directly in the deployed J2ME
application.
Another approach is to use the lightweight cryptography API defined by The Legion Of The Bouncy Castle to
develop a provider based on their design principle of A
provider for the JCE and JCA [14]. This solution results
with a provider not fitting exactly the Java Cryptography Architecture, but which is usable on J2ME devices,
such as PalmOS [14].

6. Conclusions and Enhancements
As long as electronic communication continues to
expand, security will remain an issue. Cryptography is
one of the most effective tools available to address these
issues. One-time pad is among the most powerful existing cryptographic techniques, providing it is used within
the constraints of its applicability. Primarily the constraints are key management, compute limited devices
and encryption tasks that do not require encrypting large
files. The Java Cryptography Architecture offers the
potential of a single interface for applications that
allows plug-in of any number of participating service
providers. The approach allows evolution of security
approaches with the promise of minimal impact on
applications.
Security remains an open field on every computing
platform. As platforms continue to evolve to smaller and
better connected devices, they will meet the information
needs of a broader range of consumers. Its importance to
provide frameworks for developing secure distributed
and web-based applications on such mobile devices.

7. References
[1.]

d@vidwest.net (2000, November 17). ‚ÄúOne Time Pad
Encryption v0.9.4‚Äù Retrieved January 25, 2002 from
the World Wide Web: http://www.vidwest.com/otp/

[2.]

Gong, L. (1998). ‚ÄúJavaTM 2 Platform Security Architecture Version 1.1‚Äù. Sun Microsystems, Inc. Retrieved
February 20, 2003 from the World Wide Web: http://java.sun.com/j2se/1.4/docs/guide/security/spec/securityspec.doc.html

[3.]

Jenkin M. & Dymond P. (2002) ‚ÄúSecure communication between lightweight computing devices over the
Internet‚Äù. HICSS 35 January 2002.

[4.]

Kahn D. (1967) The Codebreakers, New York, NY,
MacMillan.

[5.]

Knudsen J. (1998) Java Cryptography, Sebastopol, CA,

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

5

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

O‚ÄôReilly.
[6.]

[7.]

[8.]

[9.]

Lindquist T. (2003, January 14). ‚ÄúCet427/598 Distributed Object Systems‚Äù. Retrieved February 02, 2003
from the World Wide Web:
http://pooh.east.asu.edu/Cet427/ClassNotes/Security/
cnSecurity.html
McGraw, G. and Felten, E. (1996) Java Security: Hostile Applets, Holes, and Antidotes. New York, NY.
John Wiley & Sons.
McGraw, G. (1998) ‚ÄúTesting for security during development: why we should scrap penetrate and patch‚Äù.
IEEE Aerospace and Electronic Systems, 13(4):13-15,
April 1998.
Muchow J. (2001) Core J2METM Technology and
MIDP, Upper Saddle River, NJ, Prentice Hall.

[10.]

Oaks, S. (1998) Java Security, Sebastopol, CA, O'Reilly & Associates.

[11.]

Rubin, A, Geer, D. and Ranum, M. (1997) The Web Security Sourcebook. New York, NY, John Wiley &
Sons.

[12.]

Sun Microsystems, Inc (2002). ‚ÄúJava 2 Platform, Standard Edition, v 1.4.0 API Specification‚Äù. Retried January 12, 2002 from World Wide Web: http://
java.sun.com/docs/

[13.]

Sundsted T. (2001). ‚ÄúJava, J2ME, and Cryptography‚Äù
Retrieved March 20, 2003 from the World Wide Web:
http://www.itworld.com/nl/java_sec/10262001/

[14.]

The Legion Of The Bouncy Castle (2000). ‚ÄúThe Bouncy Castle Crypto APIs‚Äù Retrieved March 20, 2003 from
the World Wide Web: http://www.bouncycastle.org/index.html

[15.]

Zbar, Jeff, ‚ÄúPortable Memory Gets Small,‚Äù retrieved
Sept. 2003; http://www.beststuff.com/article.php3?story_id=4395.

[16.]

Lexar Media, ‚ÄúSamsung Sampling 2Gb NAND Flash
Memory Devices to Lexar Media,‚Äù retrieved Sept.
2003; http://www.digitalfilm.com/newsroom/press/
press_02_25_02a.html.

[17.]

Jenkins, Bob, ‚ÄúISAAC: a fast cryptographic random
number generator,‚Äù retrieved Sept. 2003; http://
www.burtleburtle.net/bob/rand/isaacafa.html.

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

FRAMEWORKS FOR SECURING LIMITED-DEVICE APPLICATIONS
Timothy Lindquist
Aarthi Ramamurthy
Ramon Anguamea
Timothy.Lindquist@asu.edu Aarthi.Ramamurthy@asu.edu Ramon.Anguamea@asu.edu
Division of Computing Studies
Arizona State University
Abstract

In this paper, we compare the features
available for developing secure distributed
applications for limited devices, such as smart
phones. We limit our scope to examine
frameworks for Java. This work is part of a
continuing project which is considering
capabilities and performance for application
development on these platforms. The paper
considers performance as it relates to various
approaches to securing applications.
The paper addresses two separate concerns.
First is protecting access to resources by an
executing application. The facilities for defining,
limiting and controlling applications during
their development, installation and execution are
described. Second, we discuss approaches
available for securing communication among
application components running on servers or
limited devices.

1. Introduction
In this paper we consider limited devices
that are connected to the Internet and other
communication media, for example, handheld
devices such as intelligent cell phones and PDAs
with Internet connections or platforms which
combine these functionalities. These devices
have limited memory, limited processing power,
no hard disk storage, small display screens, and
limited human input capability. We consider
only those having communication facilities
(WiFi or EV-DO)..
The operating environments for these
devices are comprised of three base components:
local operating system, network operating
system and language runtime environment. The
leading operating systems for these devices are
Symbian, Palm and Windows Mobile 6.
Connected limited-device configuration and
mobile
information
device
profile
(CLDC1.1/MIDP2) are the Java frameworks
designed for resource constrained devices, such
as phones and PDA‚Äôs. CLDC1.1/MIDP2 as

realized by the IBM J9 runtime and SUN
Wireless toolkit pre-defined classes, is the
security environment we evaluated for this paper.
Various development environments are available
depending upon platform and language. For
Microsoft Windows Mobile 6 the application
development environment for the .NET
languages, such as C#, is the .NET Framework
together with Visual Studio 2005 with the
Compact Framework 2.0. Several alternatives are
available for configurations utilizing Java, in part
depending on the Java runtime environment
being used. Sun‚Äôs CLDC HotSpot and IBM‚Äôs J9
are two popular Java runtime environment
choices. Add-on packages and various
configurations are available to support different
security approaches, device capabilities and
networking needs.

2. Background
The connectivity of computing devices to
the Internet, has enabled malicious attacks. The
motivation for attacks varies from willful
espionage to experimentation. Equally important
to protection from attack is the ability to prevent
harm from mistakes in coding, configuration or
user operations. Protection and detection are
difficult in handheld devices because of limited
capability.
Trust
is
confidence
in
expected
functionality. When running an application the
user must trust that it produces valid information
and that privacy, integrity, or confidentiality will
not be compromised. There are several security
policies, protocols and mechanisms that are of
particular interest to the limited devices.
Languages such as Java, C#, and other scripting
languages are widely used in distributed
applications and provide varying degrees
security support. Java and C# both permit
examination of compiled intermediate code for
unsafe actions. Both the Java CLDC/MIDP and
Mobile 6 execution environments support an
array of cryptographic functions that can be used

1530-1605/08 $25.00 ¬© 2008 IEEE

1

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

by communication protocols and by the system
elements to aid in access control at the device
and application level.

3. Java CLDC/MIDP
Three different limited device configurations
exist for the Java 2 Micro Edition (J2ME). For
more capable devices such as set-top boxes and
high-end wireless devices the Connected Device
Configuration (CDC) defines an API whose
functionality is close to J2SE, but is reduced as
appropriate for the limited hardware and
applications. At the lowest level of functionality
is the JavaCard API (for Smart-cards/Sim-cards).
JavaCard as can include functionality for
asynchronous security operations, such as
encryption, decryption, digital signature,
verification and others for limited devices whose
computing capacity is unable to perform such
operations without disrupting user-functionality.
The Connected Limited Device Configuration
(CLDC) is defined for PDA and wireless phone
devices. A device such as a PDA or smart-phone
running Java applications would include a virtual
software stack with the following components:
‚Ä¢ Mobile Information Device Profile
(MIDP2) that supports the application
life-time model, persistent storage,
network resources and the user-interface.
‚Ä¢ CLDC1.1 that supports the core Java
language, IO and networking classes,
security features and internationalization
facilities.
‚Ä¢ The selected Java runtime environment
‚Ä¢ The device operating system and related
services

3.1 Application Security Model
The J2SE model for securing the operations in
an executing virtual machine changed
dramatically as Java evolved. Java originally,
used the sandbox model for application security.
Initial versions of Java provided full trust to
classes loaded locally and prohibited all sensitive
operations from any code obtained dynamically.
Java1.2 introduced support for a continuum of
access control. Access to system resources (such
as files, sockets, runtime, properties, security
permissions, serializable, reflection, and window
toolkit) is granted based on domains. A domain
is defined to include:
‚Ä¢ a set of permissions (resources),

‚Ä¢
‚Ä¢
‚Ä¢

a set of operations associated with
each permission,
codebase indicating the code origin,
a digital signature of the code which
allows identification of the signer
and verification that the code has not
been modified.

The codebase indicates the file or URL from
which the code is loaded. If signed, the alias of
the public key can also be used to define a
domain. Each class loaded into a Java virtual
machine has an associated protection domain,
which defines the access it has to resources.
When execution encounters an operation that
requires a system resource, all classes
representing the currently executing methods
(contents of the runtime stack) are checked to
assure all have access to the resource. The Figure
below is taken from the On-line Java Tutorial
and shows how an execution can include a range
of protection domains ranging from no access to
resources to full access.

Figure 1.

Controlling Access to Java Resources

In J2SE (Java2), security domains are defined
by a policy file granting permissions to the
domain. For example, suppose the company
GrowthStocksExpress publishes an applet on
their (hypothetical) web site at the URL:
http://GSE.com/applets
Assuming the applet needs connections to one
or more hosts having a domain address ending
with GSE.com on ports beginning at 2575, a
policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase
"http://GSE.com/applets"
{
permission java.net.SocketPermission

2

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

"*.GSE.com:2575-", "accept, connect, listen, resolve";
};

A policy may consist of one or more grants each
defining different domains. Each domain may
have one or more associated permissions.
CLDC/MIDP2 security. The application
security model for CLDC/MIDP2 draws on the
model for J2SE, in that it includes domains and
signed code. CLDC/MIDP2, however has a
simpler model, in part because of the constraints
imposed by the configuration and profile. The
following CLDC/MIDP2 constraints are most
significant
‚Ä¢ Java Native Interface (JNI). JNI provides
J2SE applications access to native code
running on the platform. CLDC provides
similar capabilities in Kilo Native
Interface (KNI), but prohibits dynamically
loading and calling arbitrary native
functions.
‚Ä¢ No reflection, remote method invocation
or serialization. In J2SE, an RMI server or
client can cause remote code to be
automatically downloaded and executed
to satisfy argument or return (sub)classes.
When a serializable RMI parameter is
provided an argument of an extended
type, the RMI system will attempt to load
(if necessary from an http codebase) the
needed class.
‚Ä¢ No user-defined class loaders. Related to
the constraint above, the developer cannot
define a class loader in CLDC. The classloader in CLDC cannot be extended or
replaced
by
the
developer.
A
CLDC/MIDP2 application can only load
classes from its own (signed) Java
archive. As a result, the developer cannot
extend or modify any classes in the CLDC
configuration, MIDP2 profile, or which
are provided by the runtime environment
vendor.
‚Ä¢ CLDC supports multi-threading, but it
does not provide facilities to build
daemons or thread-groups.
MIDP2 security protects access to
sensitive API‚Äôs by permissions. Protecting
resources includes the concept of a domain,
which is conceptually similar to J2SE. The
full scope of protection includes the following
elements:
‚Ä¢ Protection domains (4) that are
statically defined in a policy file (by

‚Ä¢

‚Ä¢

the vendor) and associated to
resource permissions; for example,
socket, http, https, PushRegistry. The
protection domains are Minimum,
Maximum
(or
Trusted)
and
Untrusted.
Certificate and archive signature.
The jar file containing application
class files and other resources can be
digitally signed.
Level of access ‚Äì either Allowed or
User.

Unlike J2SE, the 4 protection domains are
device-specific and defined by the runtime
vendor. They can be modified only as
provided by the vendor. Each of the four
domains is associated with a set of
permissions together with a level of access.
The 4 protection domains are defined by the
runtime vendor.
‚Ä¢ Minimum. None of the permissions
are allowed.
‚Ä¢ Maximum. All of the permissions are
allowed.
‚Ä¢ Trusted. All of the permissions are
allowed.
‚Ä¢ Untrusted. To be allowed, the user
must provide consent.
The permissions defined by the MIDP2
specification include: http, socket, https, ssl,
datagram, serversocket, datagramreceiver, and
PushRegistry (invoke other applications). These
permissions may be grouped together by the
vendor into meaningful subsets and assigned to
domains based on the subsets; for example,
NetworkAccess.
Within a domain, the level of access may be
different for different permission (sets). The
accesses are:
‚Ä¢ Allowed. The permission (set) is
allowed without involving the device
user.
‚Ä¢ User level. The application‚Äôs access
to the permission(set) depends on
explicit authorization from the
device user.
With user level of access, a dialog box is
presented to the user indicating information
about the permission and asking the user

3

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

whether access should be granted. User level
access can be specified on one of 3 modes:
‚Ä¢ Oneshot. The user must be prompted
for each operation on the protected
resource.
‚Ä¢ Session. When the user grants
access, it applies to all operations on
the resource during a single
execution of the MIDlet.
‚Ä¢ Blanket. When the user grants
access, it applies to all operations on
the resource during any execution of
the MIDlet.
CLDC/MIDP2 provides MIDlet access to the
Record Management System (RMS), which
provides persistent storage for application data
via a record store. MIDP2 provides shared access
to the record store of other MIDlet suites, and
provides that access should be provided as readwrite or read-only.
Low-level security is provided by the J2ME
Java virtual machine. A virtual machine
supporting CLDC must reject invalid class files.
This is accomplished by a two-step process. At
development time, classes are pre-verified by a
tool which adds special attributes to class files to
facilitate runtime class verification on the device.
Much of the verification process can be handled
statically by the pre-verifier. At runtime, the
virtual machine rejects classes that have not been
pre-verified.

3.2 Security and Trust API
MIDP2 provides HTTPS and SSL for secure
communications with other devices. But, runtime
environment
providers
are
increasingly
providing additional options. For example,
IBM‚Äôs J9 version 5.7 provides web service
security package which allows web method calls
using encrypted SOAP envelopes or digitally
signed method calls for authentication and
information integrity.
Security and Trust Services API (SATSA)
provides access to more comprehensive hash
code,
digital
signature/verification,
key/certificate management, as well as
encryption and decryption. SATSA is designed
as 4 optional components. The primary purpose
is to provide access to a SmartCard Java device,
which provides security functionality in an
asynchronous manner that does not disrupt
applications supporting the device user.

SmartCard includes the Java Card Protection
Profile. The protection profile supports both
open and closed cards. Open cards provide the
end-user with the ability to install or activate
new applications on the card. Closed cards have
applications set by the vendor at the time the
card is personalized for the end-user. A good
example of a closed card may be a banking card
that supports personal electronic purchases and
bank account functions. Open cards that allow
new applications to be downloaded and installed
on the card present special security risk that
would exclude open cards that include banking
applications. Nevertheless, applications for open
cards that support other aspects of security may
become increasingly important. An example may
be securely communicating information outside
of direct e-commerce applications. Data integrity
and authentication are becoming increasingly
important
as
electronic
communication
proliferates.
The Java Card Protection Profile defines
four different configurations for a Java Card
based on open and closed cards. The minimum
configuration corresponds to a closed card in
which no applications can be installed on the
card after it‚Äôs been issued to an end-user. The
three
remaining
configurations
provide
additional functionality that‚Äôs available through
the evolution of the Java Card specification, such
as RMI (a limited version), logical channels,
applet deletion, object deletion, external
memory, biometry, and contactless interface.
The Java virtual machine for the device includes
an API (RTE API) that may contain classes for
performing security operations on information
and for certificate and key management.
SATSA runs on the limited-device, not on
the Java Card. SATSA provides an interface to
card security functionality, or when there is no
associated smart card, provides security
operations for the limited-device. SATSA has
four optional packages.
‚Ä¢ SATSA-APDU provides low-level
stream/socket-based protocol for communicating
between the limited-device and the card.
‚Ä¢ SATSA-JCRMI provides an RMI
interface that allows an application running on
the limited-device to call methods running in
applications on the Card. This interface would be
used to instead of SATSA-APDU to avoid the
overhead of programming with a low-level
socket data protocol.
‚Ä¢ SATSA-PKI allows limited device
applications to use the smart card to digitally
sign information or to verify digital signatures.

4

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

PKI also provides for key and certificate
management.
‚Ä¢ SATSA-CRYPTO. When a Java Card is
not available, the CRYPTO API is used to
compute security operations directly on the
limited-device.
Use of APDU, JCRMI, and/or PKI is
accomplished using threading on the limiteddevice. Threading allows security operations to
take place on the card while other applications
continue to run on the device supporting the enduser. In this scenario, SATSA is appropriate for
limited-devices with constrained processing
power. Independent of processing capability,
using a Java Card may be necessary to provide
assurance level that is appropriate to the
application. The open-device nature of cell
phones and PDA‚Äôs make it difficult to certify
trustworthiness of applications on the device.
Instead, we can isolate all high-risk user-specific
information and computations to a certified
secure Java Card.

HTML page, it returns an XML message in
Simple Object Application Protocol (SOAP)
format.
The service description - specified in Web
Services Description Language (WSDL) - this
description defines the web methods (functions)
that a service will accept - the inputs that go into
these methods, and the format of the output that
can be expected in return. This is used in
generating a web service client proxy class for
the limited device.
The web service registry - is a directory of
web services. The directory is optional because a
web service need not be listed in a registry to be
used. The registry provides a catalogue of
available services - similar to Java Naming and
Directory Service (JNDI).
The web service client proxy ‚Äì The proxy
negotiates the communication between a limited
device client and the web service. It marshals
arguments, signs or encrypts as appropriate,
posts the message and interprets the result.

4. Secure Web Services

4.1 Types of Security Services

Web services provide an XML-based
service protocol for communicating among
components of a distributed application. Web
services differ from prior similar technologies,
such as Microsoft DCOM, Object Management
Group CORBA and Java Remote Method
Invocation through reliance on http protocol and
XML.

The following are the security services that
may be required by a distributed limited device
application.
Authentication: Ensures that the sender and
receiver are who they claim to be. Mechanisms
such as username/password, smart cards, and
Public Key Infrastructure (PKI) can be used to
assure authentication.
Authorization or Access Control: Ensures that
an authenticated entity can access only those
services they are allowed to access. Access
control lists are used to implement this.
Confidentiality: This assures that information
in storage and in-transit are accessible only for
reading by authorized parties. Encryption is used
to
assure
message
confidentiality.
Integrity: Ensures that information, either in
storage or in-transit cannot be modified
intentionally unintentionally. Digital signatures
are used to assure message integrity.
Non-repudiation: Requires that neither the
sender nor the receiver of a message be able to
legitimately claim they didn't send/receive the
message.

Figure 2. Web Services Architecture [9]
In Figure 2, the service, is performed by a web
server acting as a container for executing the
service code. This is generally just a web like
page that gets posted similar to the way other
http web requests are done. Instead of returning a

4.2 Transport Level Security
The most popular security scheme for web
services is SSL (Secure Socket Layer), which is
typically used with http, and is supported by

5

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

CLDC/MIDP2.
However using SSL for
securing web services has a number of
limitations. The inadequacy of SSL can be easily
explained by a simple example.
Consider a Web Service that can be provided
indirectly to a user. A user accesses a website
which indirectly invokes another remote web
service. In this case, we have two security
contexts:
1. Between the user and the website
2. Between the user and the web service
The second security context requires the
security of SOAP request/reply message
(between the web site and the web service) to be
assured over more than one client-server
connection. SSL is inadequate to provide this
type of security mainly because of the fact that
while it encrypts the data stream, it does not
support end-to-end confidentiality.
The shortcomings of SSL (https) should be
considered when being used for a distributed
application to reside on a limited-device.
SSL is designed to provide point-to-point
security. Often, Web services require end-to-end
security, where multiple intermediary nodes
could exist between the two endpoints. In a
typical Web services environment XML-based
business documents route through multiple
intermediary nodes.
Https in its current form does not support
non-repudiation well. Non-repudiation is critical
for business Web services and, for that matter,
any business transaction.
Finally, SSL does not provide element-wise
signing and encryption. For example, if there is a
large purchase order XML document, yet only a
single element, say, a credit card element needs
to be encrypted. Signing or encrypting a single
element is difficult with transport level security.

4.3 XML Signature
XML based security schemes, provide unified
and comprehensive security functionalities for
Web Services. The important ones being, XML
Signature, XML Encryption, WS-Security (Web
ServicesSecurity).
The W3C (World Wide Web Consortium)
and the IETF (Internet Engineering Task Force)
jointly coordinated to generate the XML digital
signature technology. The XML digital signature
specification [10] defines XML syntax for
representing digital signatures over any data
type. It also specifies the procedures for
computing and verifying such signatures.

Another important area that XML digital
signature addresses is the canonicalization of
XML documents. Canonicalization enables the
generation of the identical message digest and
thus identical digital signatures for XML
documents that are syntactically equivalent but
different in appearance due to, for example, a
different number of white spaces present in the
documents.
The advantages of using XML digital
signature can be summarized as below.
‚Ä¢
‚Ä¢

‚Ä¢

‚Ä¢
‚Ä¢

It accounts for and takes advantages of two
existing and popular technologies, viz., the
Internet and XML.
XML digital signature provides a flexible
means of signing. For example, individual
item or multiple items of an XML document
can be signed. This becomes extremely
useful in a scenario where each person in a
workflow is responsible ONLY for certain
work.
It supports diverse sets of Internet
transaction models. For instance, the
document signed can be local or even a
remote object, as long as those objects can
be referenced through a URI (Uniform
Resource Identifier). A signature can be
either enveloped or enveloping, which
means the signature can be either embedded
in a document being signed or reside outside
the document.
It provides important security features like
authentication, data integrity (tamperproofing), and non-repudiation.
XML digital signature also allows multiple
signing levels for the same content, thus
allowing flexible signing semantics. For
example, the same content can be
semantically signed, cosigned, witnessed,
and notarized by different people.

Figure 3. WSE ‚Äì Input/Output filters [1]

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

Web Service Deployment Descriptor
(WSDD) and Handlers play the pivotal role in
the implementation of digital signature in Java.
A deployment descriptor specifies aspects such
as handlers and communication protocol. The
Handler is a java class (implementing the Input
and output filters of Figure 3) that provides a
MessageContext through which access is
provided to the input/output stream of XML \In
the Apache AXIS framework, MessageContext
is a structure, that contains: 1) a request message,
2) a response message, and 3) a number of
properties.
All the SOAP message manipulation is done
within the handler class.

6. Conclusions and Issues
The authors have continuing efforts in this
area which include obtaining devices, software
development environments, and simulators /
emulators related to securing limited-devices.
Our approach considers the operating
environment on the device as well as their
applications.
We are in the midst of consolidation of
small hand-held devices to provide integrated
functionality. Common applications including
personal organizers, cell-phones, and multimedia players can effectively be placed on a
single platform. While users who desire more
than one of these functionalities are exploring
integrated solutions, the industry is pushing
separation (partly for financial reasons.)
Consolidated functionality brings a higher
diversity of applications onto limited devices, as
does special purpose applications (for example
autonomous vehicle control). Either way,
security concerns increase.
The use of smart cards in the United States
is just beginning after lagging behind use in
some other regions. The integration of smart
cards (SIM-Cards) on cell phones is an
indication of this trend. Enabling high-risk
applications, such as banking and purchasing,
by leveraging smart cards integrated with other
devices presents an attractive alternative. Of
course the concern for security places new
demands on platforms in which security has not
historically been a high priority.
Performance has been the primary
impediment to the use of more strongly objectoriented languages such as Java for limited
device applications. Securing a distributed

application complicates the issue. Important
considerations include:
‚Ä¢ Underlying
architecture
processor
performance,
ancillary
processing
capability such as SmartCard,
‚Ä¢ Frameworks supporting securing the
application, as well as communications,
‚Ä¢ Use of security mechanisms appropriate
to application needs (authentication,
integrity, confidentiality),
‚Ä¢ Proper use of available frameworks
including proper handling of passwords,
certificates, keys, digital signatures, and
encrypted information.
Frameworks discussed in the paper are an
important enabler to developing more secure
distributed limited-device applications. Further
usage reports and benchmarking for security
mechanisms would better support developers.

References
[1.] Tim Ewald, ‚ÄúProgramming with Web
Services Enhancements 1.0 for
Microsoft.NET‚Äù, Available, see:
http://msdn.microsoft.com/webservices/buil
ding/wse/default.aspx?pull=/library/enus/dnwse/html/progwse.asp
[2.] Sun Microsystems Java Security and
Crypto Implementation,
http://www.cs.wustl.edu/~luther/Classes/Cs
502/WHITE-PAPERS/jcsi.html
[3.] Knudsen, Jonathan; Understanding MIDP
2.0‚Äôs Security Architecture.
http://developers.sun.com/techtopics/mobilit
y/midp/articles/permissions/
[4.] WebSphere Everyplace Micro Environment
v5.7; MIDP Installation guide for J9 Palm
runtime environment. Available online from
IBM.
[5.] Mourad Debbabi, Mohamed Saleh,
Chmseddine Talhi and Sami Zhioua:
‚ÄúSecurity Evaluation of J2ME CLDC
Embedded Java Platform‚Äù, in Journal of
Object Technology, (5,2) Mar-Apr 2006. pp.
125-54.
[6.] Security and Trust Service APIs for Java
Platform Micro Edition Developers Guide.
Available from http://www.java.sun.com/
[7.] Pannu, K.; Lindquist, TE; Whitehouse, RO;
and Li, YH; ‚ÄúJava Performance on Limited
Devices‚Äù; Proc The 2005 International
Conference on Embedded Systems and
Applications, CSREA Press, Las Vegas,
June, 2005.

7

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

[8.] Lindquist, TE, Diarra, M, and Millard, BR;
‚ÄúA Java Cryptography Service Provider
Implementing One-Time Pad‚Äù; Proc. 37th
Annual Hawaii Int'l Conf on Systems
Sciences, ACM, IEEE Computer Society,
January 2004.
[9.] Online Documentation on Web Services,

Available from: http://www.servicearchitecture.com/webservices/articles/web_services_explained.ht
ml
[10.] XML Digital Signature Specification, W3C
Recommendations, Available from:
http://www.w3.org/TR/xmldsig-core

8

ARTICLE IN PRESS

The Journal of Systems and Software xxx (2004) xxx≠xxx www.elsevier.com/locate/jss

Automated support for service-based software development and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a Department of Computer Science and Engineering, Arizona State University≠≠Main, P.O. Box 875406, Tempe, AZ 85287-5406, USA Department of Electronics and Computer Engineering Technology, Arizona State University≠≠East 7001 E, Williams Field Road, Building 50, Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003

Abstract A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. ” 2003 Published by Elsevier Inc.

1. Introduction A service-based development paradigm, or services model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can interact with one another and be providers or consumers of data and behavior. Some of the defining characteristics of service-based technologies include modularity, availability, description, implementation-independence, and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon the definition of the interface needed to access a service (description) while hiding the details of its implementation (implementation-independence). Since the client and service are decoupled, other concerns such as side effects become non-factors (modularity). One of the potential benefits of using a service-based approach for developing software is that at any given time, a wide variety of alternatives may be available that meet the needs of a given client (availability). As a result, any or all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. The technique utilizes an architecture description language to describe services and achieves run-time integration using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and involves the automatic generation of ``glue'' code for both services and applications. The Jini interconnection technology (Edwards, 1999) is used as a broker for facilitating service registration, lookup, and integration at runtime. The remainder of this paper is organized as follows. Section 2 describes background material in the areas of software architecture and the middleware technology we are using to enable dynamic integration (i.e. Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work, and Section 5 draws conclusions and suggests further investigations.

2. Background
Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751. E-mail address: gannod@asu.edu (G.C. Gannod). 0164-1212/$ - see front matter ” 2003 Published by Elsevier Inc. doi:10.1016/j.jss.2003.05.002
*

This section describes background material on software architecture and Jini.

ARTICLE IN PRESS
2 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx

2.1. Software architecture A software architecture describes the overall organization of a software system in terms of its constituent elements, including computational units and their interrelationships (Shaw and Garlan, 1996). In general, an architecture is defined as a configuration of components and connectors. A component is an encapsulation of a computational unit and has an interface (e.g. port) that specifies the capabilities that the component can provide. Connectors encapsulate the ways that components interact. A connector is specified by the type of the connector, the roles defined by the connector type, and the constraints imposed on the roles of the connector. A connector defines a set of roles for the participants of the interaction specified by the connector. Components are connected by attaching their ports to the roles of connectors. Another important concept is an architectural style. An architectural style defines patterns and semantic constraints on a configuration of components and connectors. As such, a style can define a set or family of systems that share common architectural semantics (Medvidovic and Taylor, 1997).

3. Approach This section describes the service-based development approach including the techniques used for defining services, specifying client applications, realizing integration, and generating glue code. 3.1. Example Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on the health of systems in a network. This application utilizes a network sniffer service and a port monitoring service. The network sniffer service gives an administrator information about traffic on the network. The port monitoring service provides information about the open ports on the various machines on a network. Together, these services facilitate determining whether certain kinds of attacks (such as ping storms) are being directed to a machine or machines. The client application supports analysis of several networks, each of which is accessed using the buttons shown on the top portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services that utilize different models of execution (strict call return and data streams). The remainder of this section refers to architectural specifications that were used in the construction of this example. 3.2. Overview The methodology that we have developed follows closely the model suggested by Stal (2002) for web ser-

2.2. Jini The primary enabling feature of the work described in this paper is the existence of Jini (Edwards, 1999) for the delivery and management of services. In a typical Jini network, services are provided by devices that are connected to the network. A Jini technology layer provides distributed system services for activities such as discovery, lookup, remote event management, transaction management, service registration, and service leasing. When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is registered, a proxy (Gamma et al., 1995) is stored by the lookup service. The proxy can later be transported to the clients of the service. Other network members can discover the availability of the service via the lookup service. When a client application finds an appropriate device, the lookup service sets up the connection. In our approach to component integration, we use Jini to provide a standard method for registering and connecting a client to corresponding software components that are acting as services. One of the advantages of using this Jini-based integration technique is that it facilitates construction of applications ``on-the-fly'' whereby components can be used on an as-needed basis. One of the disadvantages is that clients of services must have some prior knowledge about how to use each respective service.

Fig. 1. Running example.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx 3

vices, although the technology that we are using to realize our approach is Jini. The approach itself focuses on two concerns with respect to software reuse. That is, it addresses both for reuse and with reuse concerns. With respect to for reuse, the approach involves the construction of services via the use of adapter and proxy synthesis. Specifically, the methodology involves two steps for creating services as follows: (1) specification of components as services, and (2) generation of services using proxies via the construction of appropriate adapters and glue code. These services are consequently registered and made available on a network. With respect to with reuse concerns, the approach involves the construction of applications using services as follows: (1) specification of a client to make use of services from a repository or network, (2) generation of the client (both manual construction of client application specific code and automated generation of glue code), and (3) execution of the client, including integration of the specified services at runtime. Within our approach, a user (e.g. developer) is responsible for writing the source code for the client application along with the specification of the architecture for a client. Among other things, the client specification contains a description of the basic services that the client application will need in order to be a complete system. All other source code, including code necessary to realize the connections between the client and employed services, is generated based on the specifications describing clients, services, and connectors. 3.3. Service generation In this section we describe some of the issues related to automating the creation of service wrappers. To support these activities, we have developed an automated tool that takes as input a software architecture and produces glue code. A primary source of reusable components that we employ in our approach are legacy command-line applications (Gannod et al., 2000). In order to generate services from legacy components, we take the approach of wrapping the components by utilizing the interface provided by the component. Since command-line applications have a well-defined input and output interface, the interface of the application as a service can be based entirely upon the knowledge of what the application intends to provide. 3.3.1. Specification and synthesis The concept of using an adapter for wrapping legacy software is not a new one (Gamma et al., 1995). As a migration strategy, component wrapping has many benefits in terms of re-engineering including a reduction in the amount of new code that must be created and a reduction in the amount of existing code that must be rewritten.

In regards to wrapping components, our approach uses two steps. First, a specification of the legacy software as an architectural component is created. These specifications provide vital information that is required to define the interface to the legacy software. Second, the appropriate adapter source code is synthesized based on the specification. 3.3.2. Specification requirements To aid in the development of an appropriate scheme for the wrapping activity, we defined the following requirements upon specifications. These requirements are as follows: (S1) a sufficient amount of information should be captured in the interface specification in order to minimize the amount of source code that must be manually constructed, (S2) a specification of the interface of the adapted component should be as loosely coupled as possible from the target implementation language, and (S3) the specification of the adapted component should be usable within a more general architectural context. The requirement S1 addresses the fact that we are interested in gaining a benefit from reusing legacy software. As a consequence, we must avoid modifying the source code of the legacy software. At the same time, we must provide an interface that is sufficient for use by a target application. To provide that interface, a sufficient amount of information is needed in order to automatically construct the adapter. Our selection of command-line applications addresses the modification concern of requirement S1 since source code is not available. As such, we are required to provide an interface that is based solely on the knowledge of how the application is used rather than how it works. Table 1 shows the properties used in the specification of services, clients and connectors. A service component specification consists of two parts: properties and ports. The properties section describes style of the service, while the ports section describes functions provided by the service. In addition, the service specifications indicate style-based information as well as conditions or commands that need to be true or executed, respectively, in order to establish an environment necessary to use the service. Finally, a key in terms of a ``service type'' (e.g. interface property) is used to support a service lookup, which is later utilized during application integration. The requirement S2 (i.e. the decoupling of a specification from a target implementation language) is based on the desire to apply the synthesis approach to a variety of target languages and implementations. In addition, this requirement facilitates enforcement of requirement S1 by ensuring that new source code is not artificially embedded in the specification. While satisfying this requirement is ideal, we found in our strategy that a certain amount of implementation dependence was

ARTICLE IN PRESS
4 Table 1 Properties Group Service properties Service port properties Attribute Component-Type Signature Return Cmd Pre Post Interface Path Port-Type Shared-GUI Part-of-client GUI-CodeFile Component-Type Shared-GUI Port-Type Interface Connector-Type Prop-type Description Architectural style this component adheres to The port's signature The port's return type The command-line program being wrapped Pre-processing command Post-processing command The generic interface implemented by this port Path to the wrapped command-line program The port's type based on the Component-Type Boolean indicating shared (true) or exclusive (false) GUI Identifies inclusion in client application The filename for client's GUI code Architectural style this component adheres to Boolean indicating shared (true) or exclusive (false) GUI The port's type based on the Component-Type The generic interface that this port can bind with Architectural style this connector adheres to The connectors role based on the Connector-Type G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx

Client properties

Client port properties Connector properties Connector role

necessary due to the fact that our implementation would make use of Jini. When a component has been wrapped using our technique, an interface is defined that facilitates the use of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also desirable to be able to use the specification of the adapted component within a more general architectural context. That is, it is advantageous to be able to use the specification as part of the software architecture specification for new systems. In using a content-rich specification, where interfaces are defined explicitly, the added benefit of providing information that can be integrated into an architectural specification of a target application is gained. In order to realize the requirements placed upon desired interface specifications for legacy software wrappers, we used the ACME (Garlan et al., 1997) architecture description language (ADL). Specifically, we used the properties section of the ACME ADL to specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an ADL that has been used for high-level architectural specification and interchange (Garlan et al., 1997). 3.3.3. Synthesis As stated earlier, the class of legacy systems that we are considering are command-line applications (Gannod et al., 2000). Given this constraint, we make the assumption that any client applications utilizing the wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We find this assumption to be reasonable due to the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage profiles. In our approach, the specification that is needed to generate wrappers contains properties associated with the ports as shown in Fig. 2. These properties include Signature, Command, Pre, Post, Path, Interface, and Return. In this case, the specification describes the NetworkSniffing and PortMonitor services, which are services created by wrapping tcpdump, and nmap, respectively. In the synthesis process, ACME specifica-

Fig. 2. ACME services section.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx 5

tions are combined with a standard template that implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we have constructed to automate this process generates the appropriate source code for facilitating interaction between a potential client and the wrapped component. At present, this is an automated tool that generates fully executable code for the wrapped application and does not require the user to modify or write any new code outside of option GUI code. Both the service and client synthesis steps utilize a template-based approach to synthesize code. That is, a standard file has been created that has stubs containing place holders that must be instantiated with either service or client specific parameters. Fig. 3 contains a portion of the ServiceTemplate file which contains all of the application and service independent source code and provides the routines necessary to integrate the legacy code into a Jini network. Specifically, the ServiceTemplate contains functions that implement the discover and join protocol for registering a service with the lookup service. The ServiceTemplate also contains tags that are place-holders for the automatically generated functions. For instance, in Fig. 3 the tag <put-ServerName> is a place-holder for the final name of the adapter component. In addition to the ServiceTemplate, there is also a reusable set of functions that can be utilized in an interface specification and consequently in the generated wrappers. For instance, the getOutputStream( ) routine (shown in Fig. 4) is available as a function for use within the Java code to provide standard stream input support.

The amount of automation that has been achieved through the approach described above is dependent on the degree of graphical user interface (GUI) support that is desired. For a service, the code synthesis step can be fully automated if no GUI support is desired. Otherwise, the amount of manual code construction is limited to GUI support. 3.4. Client generation Once the services are generated and stored in a repository, a client application can be architected. First we need to specify the client application taking into account the architectural style of each of the services. Once a client is specified, it can be verified and generated. In this subsection we look at the requirements for specifying the client and then describe synthesis of the client. 3.4.1. Specification Refer again to Table 1 which, in addition to the properties for service specifications, contains the properties of client application components and connectors. When dealing with integration at the component level, two issues arise (among others) that are of interest. First, the problem of architectural style mismatch (Shaw and Garlan, 1996) occurs when the underlying assumptions made by components conflict. Second, most modern applications provide a graphical user interface (GUI). As a result, integration of off-the-shelf components can leverage these user interfaces in order to take advantage of previously built technology. To cope with these issues we impose two requirements on the specification of client applications as follows: (C1) the specification of the components should capture the notion of architectural style so that the high-level interaction between clients and services can be verified, and (C2) the specification must facilitate the use of shared and exclusive GUI components. The requirement C1 addresses the fact that a component must provide a notion of architectural style. A component's style plays a very important role when it interacts with other components by imposing interaction constraints. Using a basic style attribute (by name) architectural mismatches can be determined by simple keyword matching. Requirement C2 addresses the fact that a service may provide a GUI that allows a user to access and control the service. In this context, there may be GUI components provided by services that are either sharable by other services or exclusive to the service. A sharable GUI component can be used by both the client as well as other integrated services while an exclusive GUI component can only be used by the service that provides the interface.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

ARTICLE IN PRESS
6 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx

3.4.2. Synthesis The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample specification of a client. The information contained within client specifications are used to support the synthesis of client code. This synthesis step utilizes two features; first, the information regarding connectors and attachments, such as those shown in Fig. 5 are used to determine the relationships between client applications and desired services. Second, information regarding GUIs provided by services is used to determine how to realize the GUI in a client application. In our framework, the wrappers for the various services can implement a common interface that allows the client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially used across multiple services and are identified using a name taken from a standard GUI vocabulary (for example ``ResultsWindow''). The name is then used to identify which GUI components can be shared across services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive component is independent and cannot be shared between services. The exclusive GUI components of the wrappers are used as is but may interact with one or more of the shared components. For both shared and exclusive components, the interaction with the client GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers. 3.5. Discussion As stated in Section 1, the service-oriented domain are characterized by modularity, availability, description, implementation-independence, and publication. As a result, services and service-based approaches are more coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a software architecture to specify applications that operate under these characteristics. As such, a software architecture in this context defines components, their interfaces, and the mechanisms by which services (as components) can be joined in order to fulfill needed software behavior. Consequently, services enable the use of a software architecture as an integration vehicle in which the architecture facilitates generation of glue code. It is the very fact that services adhere to the characteristics described above that the integration and code generation become possible at this level. However, the approach does lack in its ability to address needs that are more specific than what individual services provide. To cope with this, we are developing an approach that allows for the creation of federated services, where services are combined to meet some higher-level objective.

4. Related work Recently, the use of web services has gained attention with vendors releasing webservices toolkits that allow for building and using webservices. Webservices and .NET (Meyer, 2001) are based on the SOAP and XML (Seely and Sharkey, 2001) protocols. The Jini approach to service integration goes beyond what the webservices paradigm provides by defining how services can be used within a larger application context and providing support for code transportation. FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server that distributed messages to other tools that were interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this approach is a precursor to service-based development. Urnes and Graham (1999) describe an approach to facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the component space across a network. In our approach, services are potentially developed by different organizations and thus the choice of what to distribute is not

Fig. 5. Portion of ACME client specification.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx≠xxx 7

available. The component model being addressed by Urnes and Graham, as such, is finer-grained and violates implementation-independence, a tenet of servicebased development. Grundy et al. (2000) discuss issues and experiences in constructing component-based software engineering environments. They created a variety of useful software engineering tools using their tool set (JViews, JComposer, etc.). They use ``plug and play'' and an event-based composition approach to achieve component integration. In this framework, components are more tightly coupled and their granularity is finegrained. In contrast, our approach is based on dynamic integration of coarse-grained services that are loosely coupled. Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and component gluing. This creates a clean separation of customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work provides a potential strategy for dealing with component mismatches, which is currently ignored in our approach.

Acknowledgements G. Gannod is supported in part by NSF CAREER grant CCR-0133956. References
Edwards, W.K., 1999. Core Jini. Prentice-Hall. Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services. Commun. ACM 45 (10), 77≠80. Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison Wesley Longman. Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for legacy software. In: Proc. 7th Working Conf. Reverse Eng., IEEE, pp. 128≠137. Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture description interchange language. In: Proc. CASCON'97, pp. 69≠ 183. Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2). Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to develop a family of applications. IEE Proc. Software Eng. 144 (5≠ 6), 237≠248. Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92≠97. Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration with pluggable composite adapters. Software Archit. Comp. Technol.. Reiss, S.P., 1990. Connecting tools using message passing in field environment. IEEE Software 7 (7), 57≠66. Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services Development Using XML. Prentice-Hall. Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on an Emerging Discipline. Prentice-Hall. Stal, M., 2002. Web services: beyond component-based computing. Commun. ACM 45 (10), 71≠76. Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In Proc. of Design, Specification and Verification of Interactive Systems. Gerald C. Gannod is an Assistant Professor in the Department of Computer Science and Engineering at Arizona State University and is a recipient of a 2002 NSF CAREER Award. He received the M.S. (1994) and Ph.D. (1998) degrees in Computer Science from Michigan State University. His research interests include software product lines, software reverse engineering, formal methods for software development, software architecture, and software for embedded systems. Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras (Chennai), India. His research interests include software engineering, distributed and object-oriented systems, software design, software architecture, service-oriented software engineering, and Wireless Application platforms. Timothy E. Lindquist is Professor and Chair in the Department of Electronics and Computer Engineering Technology at Arizona State University East Campus in Mesa, Arizona. He received the Ph.D. (1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

5. Conclusions The web-based services paradigm has gained attention recently with the development of technologies such as SOAP (Seely and Sharkey, 2001). The benefits of such technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these benefits by providing facilities for on-the-fly integration and component introspection. In this paper, we described an approach for addressing component integration via the use of services in the context of Jini interconnection technology. Specifically, the approach utilizes synthesis to generate code necessary to realize component integration. To facilitate integration, the ACME ADL is used to specify both services and target applications, and is used a medium for performing service compatibility checking. We are currently developing an environment that will assist in the creation of applications within the servicebased paradigm and will support service browsing to facilitate application design. In addition, we are investigating approaches for allowing services to collaborate beyond the scope of a client application in order to create federated groups of services. Furthermore, we are developing technologies similar to the ones described in this paper in order to support service-based application within the .NET and web service frameworks.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION
OF TIME SERIES*
JORGE MARI‚Ä†, ANDERS DAHLEÃÅN‚Ä†, AND ANDERS LINDQUIST‚Ä†

Abstract. In this paper we consider a three-step procedure for identiÔ¨Åcation of
time series, based on covariance extension and model reduction, and we present a
complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy
model is determined, which is Ô¨Ånally approximated by a lower-order model by
stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all
three steps has been lacking. Supposing the data is generated from a true Ô¨Ånitedimensional system which is minimum phase, it is shown that the transfer function
of the estimated system tends in H‚àû to the true transfer function as the data length
tends to inÔ¨Ånity, if the covariance extension and the model reduction is done properly. The proposed identiÔ¨Åcation procedure, and some variations of it, are evaluated
by simulations.

1. Introduction
In recent years there has been quite some interest in a certain type of procedures
for identiÔ¨Åcation of time series known as subspace methods [1, 42, 41, 28, 29]. These
identiÔ¨Åcation procedures are based on geometric projection methods, and they could
be understood in the context of splitting geometry and partial stochastic realization
theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9],
these procedures are algebraically equivalent to minimal factorization of a Hankel
matrix of covariance estimates, and they make no distinction between stochastic and
deterministic partial realizations. Therefore they may fail because of loss of positive
realness in the spectral estimation phase.
In an attempt to overcome these problems we analyze an alternative approach
to time series identiÔ¨Åcation proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the
maximum-entropy method, leading to a high order autoregressive (AR) process, and
Ô¨Ånally stochastically balanced truncation. This method shares certain features with
stochastic subspace identiÔ¨Åcation methods, the most obvious one being that it is
based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
‚àó This research was supported by a grant from the Swedish Research Council for Engineering
Sciences (TFR).
‚Ä† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
1

2

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for
maximum-likelihood (ML) methods, are needed.
The idea of approximating an autoregressive moving-average (ARMA) process by
an AR process is by no means new. Its origins can be traced back to the Wold
decomposition [55] where L2 -convergence of high-order AR models to general analytic
models is shown. Pioneers in the use of this concept for systems identiÔ¨Åcation are
Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations
were studied by Berk [2] and later reÔ¨Åned in [36, 34, 33, 7]. The interesting paper [7]
contains nice proofs of some of the convergence results needed in this paper, but, for
the sake of completeness and insight, we provide new proofs based on some properties
of fast Ô¨Åltering algorithms [5] and simple methods of complex analysis and SzegoÃã
polynomials. The power of the theory of SzegoÃã polynomials and Toeplitz matrices in
analyzing stochastic processes is reported in [24], but, except for elementary theory,
it has not been much used in systems identiÔ¨Åcation [39]. This is even more true for
the newer results [16, 40, 37, 27] on orthogonal polynomials.
The idea of using model reduction for systems identiÔ¨Åcation appears in the thesis
by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency
weighted reduction. Instead, we use stochastically balanced truncation, for which we
develop a simple computational procedure, exploiting the special structure of the AR
model. We also show the advantage of this reduction procedure by theoretical analysis
and simulations. In fact, a comprehensive study comprising all the steps mentioned
above together with a qualitative and quantitative analysis of the entire identiÔ¨Åcation
strategy has been lacking, and that is what we oÔ¨Äer in this paper.
The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identiÔ¨Åcation
procedure contributes to the estimation error. In Section 3 we show that the transfer
function of the maximum-entropy Ô¨Ålter, constructed from true covariances, tends to
that of the true Ô¨Ålter in H‚àû norm at a geometric rate determined by the largest
modulus of the zeros of the true Ô¨Ålter as the order of the maximum-entropy Ô¨Ålter
becomes large. However the order of the approximation is too high, and therefore
model reduction is performed. This is studied in Section 4. A stochastic balancing
procedure, based only on linear-algebra operations so that no Riccati equations need
to be solved, is provided together with the analysis of the model-reduction error.
Both deterministically and stochastically balanced truncation lead to good results.
However, when the covariances are estimated from statistical data, stochastic model
reduction is found to be superior. In particular, variances are considerably closer to
the CrameÃÅr-Rao bounds. In Section 5 we state our statistical convergence theorems,
proving that the total error tends to zero as the length of the data string tends to
inÔ¨Ånity, provided the degree of the AR model tends to inÔ¨Ånity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using
stochastic subspace identiÔ¨Åcation [43] is included. For clarity of exposition, all the
proofs have been deferred to two appendices, Appendix A dealing with the asymptotic
properties of the maximum-entropy Ô¨Ålter, and Appendix B devoted to the statistical
error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identiÔ¨Åcation
Time series identiÔ¨Åcation in the form studied here amounts to estimating the matrices
(A, B, C, D) in some n-dimensional linear stochastic system

x(t + 1) = Ax(t) + Bw(t)
(2.1)
y(t)
= Cx(t) + Dw(t)
driven by normalized white noise {w(t)}, from a data string of observations
{y0 , y1 , y2 , . . . , yN }

(2.2)

of the output process {y(t)}, which here will be taken to be scalar.
The basic idea behind our approach is very simple: given estimates of a partial
sequence
c 0 , c 1 , c 2 , . . . , cŒΩ

(2.3)

of the covariances ck = E{y(t+k)y(t)}, which satisÔ¨Åes the condition that the Toeplitz
matrix
Ô£Æ
Ô£π
c2 ¬∑ ¬∑ ¬∑ cŒΩ
c0 c1
Ô£Ø c1 c0
c1 ¬∑ ¬∑ ¬∑ cŒΩ‚àí1 Ô£∫
Ô£Ø
Ô£∫
Ô£Ø
c
c
c0 ¬∑ ¬∑ ¬∑ cŒΩ‚àí2 Ô£∫
1
TŒΩ+1 := Ô£Ø 2
(2.4)
..
.. Ô£∫
..
...
Ô£∞ ...
Ô£ª
.
.
.
cŒΩ cŒΩ‚àí1 cŒΩ‚àí2 ¬∑ ¬∑ ¬∑ c0
is positive deÔ¨Ånite, Ô¨Årst construct a high-order model continuing (2.3) by covariance
extension. This model has all the required positivity properties, but the order is too
high. Then reduce the order by means of a positivity-preserving model reduction
procedure to be speciÔ¨Åed below. That this simple recipe will in fact provide a good
identiÔ¨Åcation method is by no means a trivial matter but is based on some rather
deep results, which will be presented here.
More speciÔ¨Åcally, the approach consists of three steps, for which there are several
possible variants that will be discussed below. The rigorous mathematical analysis,
however, will be carried out for the following procedure, for which we shall give
theoretical bounds.
(i) Estimate a partial covariance sequence
cÃÇ0 , cÃÇ1 , cÃÇ2 , . . . , cÃÇŒΩ

(2.5)

from the time-series data (2.2) via the ergodic estimate
N ‚àík
1 	
yt+k yt
cÃÇk =
N + 1 t=0

k = 0, 1, . . . , ŒΩ.

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer
function
zŒΩ
,
(2.7)
WÃÇŒΩ (z) =
œÜÃÇŒΩ (z)
where œÜÃÇŒΩ (z) is the normalized SzegoÃà polynomial of degree ŒΩ, to be introduced
in Section 3, computed from the estimated covariance data (2.5).

4

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

(iii) Determine a reduced-degree approximation WÃÇ (z) of WÃÇŒΩ (z) via a stochastic
model reduction procedure [11] to be described in more detail in Section 4.
In this procedure, the idea is that ŒΩ >> n, the order of the system to be identiÔ¨Åed,
and ideally nÃÇ := deg WÃÇ equals the degree n of the true system (2.1). However, the
method will produce a valid model even if this is not the case or even if there is
no ‚Äútrue‚Äù underlying model. This is in contrast to stochastic subspace identiÔ¨Åcation
models, which may fail to produce any model at all [9].
There are possibilities for variations of the procedure described above. In Step (i)
we could use alternative covariance estimates or Burg‚Äôs estimation of Schur parameters
[3], the only requirements being that the estimated Toeplitz matrix TÃÇŒΩ+1 of (2.5) is
positive deÔ¨Ånite and that cÃÇk ‚Üí ck a.s. as N ‚Üí ‚àû. In Step (ii) we could instead use
approximate covariance extension or covariance extension with prescribed zeros, for
which there is now a complete parameterization [5] and an algorithm [4]. (In the latter
case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction
methods could be used. For example, an important model reduction paradigm is the
one based on optimal Hankel norm approximation [21].
Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable
transfer function
W (z) = C(zI ‚àí A)‚àí1 B + D,

(2.8)

of McMillan degree n. We also assume that W (z) is minimum-phase so that both
zeros and poles are located in the open unit disc. Then, we need to be able to measure
how the estimated model, with transfer function WÃÇ (z), converges to the true one as
N ‚Üí ‚àû. In this paper we have chosen to use distance between W (z) and WÃÇ (z) in
‚àû
norm as a measure of proximity between the true and estimated model. From an
engineering point of view this could be called worst case identiÔ¨Åcation. The modern
literature in robust control makes extensive use of the worst case philosophy; see for
example [20, 52]. There are also other reasons for using the ‚àû , as discussed in [35].
Returning, then, to the identiÔ¨Åcation approach outlined above, the estimation error
can be decomposed into three parts, one corresponding to each of the steps (i), (ii)
and (iii). Hence we have the error bound

L

L

W ‚àí WÃÇ ‚àû ‚â§ W ‚àí WŒΩ ‚àû + WŒΩ ‚àí WÃÇŒΩ ‚àû + WÃÇŒΩ ‚àí WÃÇ ‚àû ,

(2.9)

where WŒΩ is the AR model corresponding to the true covariances (2.3) and WÃÇŒΩ is
the one determined from the estimated covariances (2.6). To prove convergence to
zero of the estimation error (2.9), we shall need to assume that W is minimum-phase,
and hence WÃÇ should have the same property, which moreover is desirable in many
applications. Our procedure insures this.
Estimating the Ô¨Årst term in (2.9) is a problem in stochastic partial realization
theory and function theory and will be dealt with in the next section. The third term
concerns model reduction which will be studied, in the particular setting required
here, in Sections 4 and 5. In Section 5, Ô¨Ånally, we consider the second term together
with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence
Step (ii) in the identiÔ¨Åcation procedure outlined in Section 2 is based on rational
covariance extension. To understand this, let us consider the covariance extension
problem from a more general point of view. Given a partial covariance sequence
c 0 , c 1 , c 2 , . . . , cŒΩ ,

(3.1)

covariance extension amounts to Ô¨Ånding an inÔ¨Ånite extension cŒΩ+1 , cŒΩ+2 , cŒΩ+3 , . . . of
this sequence such that the function
V (z) := 12 c0 + c1 z ‚àí1 + c2 z ‚àí2 + . . .

is strictly positive real, i.e., it is an analytic function in the complement Dc of the
open unit disc D, which maps Dc to the open right complex half-plane. Then
Œ¶(z) := V (z) + V (z ‚àí1 )
is a spectral density for a process having c0 , c1 , . . . , cŒΩ as its Ô¨Årst ŒΩ covariances and
which is coercive in the sense that
Œ¶(eiŒ∏ ) > 0 for all Œ∏.
Spectral factorization is then to Ô¨Ånd a stable transfer function W (z) such that
|W (eiŒ∏ )|2 = Œ¶(eiŒ∏ ).
In particular, we are interested in Ô¨Ånding covariance extensions for which V (z), and
hence W (z), have at most degree ŒΩ.
For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is
classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one
correspondence between inÔ¨Ånite covariance sequences
c0 , c1 , c2 , c3 , . . .

(3.2)

and a sequence of Schur parameters, or reÔ¨Çection coeÔ¨Écients,
Œ≥0 , Œ≥1 , Œ≥2 , Œ≥3 , . . . ,

(3.3)

with the property |Œ≥t | < 1 for all t. In fact, Ô¨Åxing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and Œ≥0 , Œ≥1 , . . . , Œ≥m‚àí1 for
each m. The Schur parameters can be determined from the covariances via the SzegoÃà
polynomials
œït (z) = z t + œït1 z t‚àí1 + ¬∑ ¬∑ ¬∑ + œïtt t = 0, 1, 2 . . . ,
computed by means of the SzegoÃà-Levinson recursion
 






 
 


z
‚àíŒ≥t œït (z)
œï0 (z)
1
œït+1 (z)
=
;
=
,
(3.4)
‚àízŒ≥t 1
1
œï‚àót+1 (z)
œï‚àót (z)
œï‚àó0 (z)
where

œï‚àót (z) := z t œït (z ‚àí1 )
is the reciprocal polynomial of œït (z), and the Schur parameters are computed via

= r1t tj=0 œït,t‚àíj cj+1
Œ≥t
(3.5)
rt+1 = rt (1 ‚àí |Œ≥t |2 ), r0 = c0 .

6

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Hence Œ≥t = ‚àíœït+1 (0), a fact that we shall use below.
In the problem to Ô¨Ånd a covariance extension for (3.1), therefore, Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1
are Ô¨Åxed and the inÔ¨Ånite continuation Œ≥ŒΩ , Œ≥ŒΩ+1 , . . . can be chosen freely. In particular,
if we take Œ≥t = 0 for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . . We obtain the maximum entropy solution
WŒΩ (z) =

zŒΩ
,
œÜŒΩ (z)

(3.6)

where œÜŒΩ (z) is the normalized SzegoÃà polynomial
1
œÜŒΩ (z) := ‚àö œïŒΩ (z).
rŒΩ

(3.7)

Thus, in this particular case, the solution to the covariance extension problem turns
out to be rational of degree at most ŒΩ as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations
are needed. In fact, it has recently been shown [5] that there is exactly one such
solution for each choice of zeros of WŒΩ (z), thus proving a long-standing conjecture
by Georgiou [18], who had established existence. Nevertheless, as we shall see next,
rationality implies that the Schur parameters tend geometrically to zero, provided
W (z) has no zeros on the unit circle.
In this section we shall demonstrate that the rational transfer function (2.8) can be
approximated arbitrarily closely in L‚àû by the transfer function WŒΩ (z) of a maximum
entropy Ô¨Ålter for suÔ¨Éciently large ŒΩ and that this ŒΩ depends on the maximum modulus
of the zeros of W (z). We shall Ô¨Årst present a heuristic argument in support of this
conclusion.
To this end, let (3.2) be the inÔ¨Ånite covariance sequence of the output process y in
(2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via
the SzegoÃà-Levinson algorithm presented above. Then we have the following special
case of Corollary 2.1 in [5].
Lemma 3.1. Let the spectral density
Œ¶(eiŒ∏ ) = |W (eiŒ∏ )|2

(3.8)

be coercive in the sense that it is positive for all Œ∏ and let (3.3) be the corresponding
inÔ¨Ånite sequence of Schur parameters. Moreover, let Œ≥ ‚àà (0, 1) be greater than the
maximum of the moduli of the zeros of W (z). Then
|Œ≥t | = O(Œ≥ t ),

(3.9)

i.e., |Œ≥t | ‚â§ M Œ≥ t for some M ‚àà R and for suÔ¨Éciently large t.
Remark 3.2. Since (3.9) holds for all Œ≥ greater than the the maximum of the moduli
of the zeros of W (z), we have in fact that |Œ≥t | = o(Œ≥ t ), i.e., limt‚Üí‚àû |Œ≥t |Œ≥ ‚àít = 0.
For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis
of certain fast algorithms for Kalman Ô¨Åltering [6].

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the
spectral density
z(z ‚àí 1)2
Œ¶(z) = ‚àí 2
(z + z + 2)(2z 2 + z + 1)
is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters
are seen to be ‚àí1/2, ‚àí2/3, ‚àí2/5, ‚àí2/7, ‚àí2/9, ‚àí2/11, . . . , which tend to zero but not
geometrically. On the other hand, there are coercive, analytic but nonrational models
which also exhibit geometric convergence rate. A classical example [23] is obtained
2
when ck = Œ∏k for some Œ∏ ‚àà (‚àí1, 1). The Schur parameters in this case form an exact
geometric sequence, Œ≥k = (‚àíŒ∏)k+1 , k ‚â• 0.
Lemma 3.1 implies that, for a suÔ¨Éciently large ŒΩ which depends on Œ≥, the Schur
parameters Œ≥t are close to zero for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . . But, the Schur parameters
of WŒΩ are exactly zero for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . , and hence geometric convergence
would insure that WŒΩ is a good approximation of W (z) for suÔ¨Éciently large ŒΩ. We
shall prove that this is indeed the case.
Theorem 3.4. Suppose W (z) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let Œ≥ ‚àà (0, 1) be greater than the maximum of the moduli of the
zeros of W (z). Then
lim WŒΩ ‚àí W ‚àû = 0,

ŒΩ‚Üí‚àû

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that
WŒΩ ‚àí W ‚àû ‚â§ M Œ≥ ŒΩ .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to Ô¨Årst showing
that
lim WŒΩ‚àí1 ‚àí W ‚àí1 ‚àû = 0.

ŒΩ‚Üí‚àû

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we
give an alternative proof of this fact based on SzegoÃà theory, and also show that the
convergence is geometric. In fact, we can choose Œ≥ arbitrarily close to the maximum
modulus of the zeros of W .
However, as we shall see next, we can actually prove more. To this end, let us Ô¨Årst
observe that, since WŒΩ‚àí1 and W ‚àí1 have their poles in the open unit disc D and thus
are bounded and analytic in the complement Dc of D, they belong to the Hardy space
‚àû
of functions which are analytic and bounded in {z ‚àà C | |z| > 1}. Hence the
H‚àí
‚àû
, and
convergence (3.12) is in H‚àí
z ‚àíŒΩ œÜŒΩ (z) ‚Üí W ‚àí1 (z)

(3.13)

uniformly in each compact subset of Dc . Now, W ‚àí1 is analytic in {z ‚àà C | |z| ‚â• Œ≥},
a region that is strictly larger than Dc . This in itself of course does not insure that
the convergence (3.13) extends to this larger region. In fact, even if z ‚àíŒΩ œÜŒΩ (z) did
converge in {z ‚àà C | Œ≥ ‚â§ |z| ‚â§ 1}, it could fail to converge to W ‚àí1 (z) there. The fact
that it really does converge uniformly to this limit is another consequence of Lemma
3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and
hence an estimate of the convergence rate Œ≥, is given in [35].
Theorem 3.5. Suppose W (z) is a minimum-phase rational function having all its
poles in the open unit disc D and all its zeros in

DœÅ := {z ‚àà C | |z| ‚â§ œÅ} ‚äÇ D

where 0 < œÅ < 1,

and let {œÜŒΩ (z)}‚àû
0 be the normalized SzegoÃà polynomial (3.7) determined from the covariances in the spectral density
|W (e )| = c0 + 2
iŒ∏

2

‚àû
	

ck cos kŒ∏.

k=1

Then, as ŒΩ ‚Üí ‚àû, z ‚àíŒΩ œÜŒΩ (z) ‚Üí W ‚àí1 (z) uniformly in every compact subset of
{z ‚àà C | |z| > œÅ}, the complement of DœÅ .

DcœÅ :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of œÜŒΩ (z) and hence of the poles of the high-order AR
model with transfer function WŒΩ (z). It is known that, if the Toeplitz matrix TŒΩ+1
is positive deÔ¨Ånite, all roots of œÜŒΩ (z) are located in the open unit disc D, but little
has been reported in the literature on their behavior as ŒΩ ‚Üí ‚àû. This behavior is
illustrated in Figure 3.1.
Original system

Original system

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0

0.5

1

‚àí1
‚àí1

Original and AR(24)
1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí0.5

0

0.5

0

0.5

1

Original and AR(24)

1

‚àí1
‚àí1

‚àí0.5

1

‚àí1
‚àí1

‚àí0.5

0

0.5

1

Figure 3.1: Distribution of zeros of œÜŒΩ (z).

The top two diagrams show the zero-pole positions, within the boundaries of the
unit circle, of two minimum phase spectral factors W , both of degree Ô¨Åve. Also
indicated is a circle of radius equal to the maximum modulus of the zeros of these
spectral factors. The little circles ‚Äú‚ó¶‚Äù represent zeros and the ‚Äú+‚Äù sign represent
poles. The lower two Ô¨Ågures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the
exact covariance sequence. The poles of the latter models are indicated with ‚Äú√ó‚Äù.
The left part of Figure 3.1 illustrates what may happen if all the poles of W (z) are
located in {z ‚àà C | |z| < œÅ}, where œÅ is chosen to be the maximum of the moduli of
the zeros of W (z). The roots of œÜŒΩ (z) tend to cluster inside a circle of radius œÅ as
ŒΩ ‚Üí ‚àû. This phenomenon is in a sense predictable, since the constant term of the
SzegoÃà polynomials is œïn+1 (0) = ‚àíŒ≥n , which equals the product of the roots and, by
Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to œÅ.
This does not preclude that other types of crowns may occur, because subsequences
of {Œ≥n } could decay faster than the overall rate Œ≥, as follows from [5]. Very general
statements about the distribution of zeros of orthogonal polynomials, derived with
the help of potential-theoretic methods, can be found in [37, 27].
To the right in Figure 3.1 we see what happens in the case that W has poles with
moduli larger than œÅ. Then, for ŒΩ suÔ¨Éciently large, the normalized SzegoÃà polynomial
œÜŒΩ (z) has roots in {z ‚àà C | œÅ ‚â§ |z| < 1}, but exactly as many as the poles of W in
this region and approximately at the same place as these. This is of course due to
the uniform convergence of z ‚àíŒΩ œÜŒΩ (z) to W ‚àí1 (z) in every compact subset of DcœÅ . The
other roots of œÜŒΩ (z) behave exactly as in the previous case and tend to accumulate in
a crown inside and very close to the circle {z ‚àà C | |z| = œÅ}.
‚àû
approximation WŒΩ of W which can be made
We have thus constructed an H‚àí
arbitrarily good by choosing ŒΩ suÔ¨Éciently large. However, WŒΩ will have much larger
degree and, except for the poles outside the circle {z ‚àà C | |z| = œÅ}, a completely
diÔ¨Äerent zero-pole pattern. We shall rectify this situation by model reduction. In
fact, for the moment considering the perfect modeling problem to identify the rational
transfer function (2.8) given an exact partial covariance sequence (3.1), the last step
in our procedure consists in approximating WŒΩ by a rational function Wred of smaller
degree, ideally of the same degree as W .
The simplest model reduction procedure is deterministically balanced truncation
(DBT), Ô¨Årst introduced by Moore [38]. Though easy to implement, it may fail to
yield a minimum-phase approximation, a requirement which is important in certain
contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced
truncation (SBT), Ô¨Årst introduced by Desai and Pal [10], which is based on a diÔ¨Äerent
balancing strategy to be explained in detail in Section 4.
Original system

Reduction by DBT

Reduction by SBT

1

1

1

0.5

0.5

0.5

0

0

0

‚àí0.5

‚àí0.5

‚àí0.5

‚àí1
‚àí1

0

1

‚àí1
‚àí1

0

1

‚àí1
‚àí1

0

1

Figure 3.2: Zero-pole pattern of W (z) and Wred (z) for diÔ¨Äerent model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This Ô¨Åfth-order

10

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

model has Ô¨Årst been approximated by WŒΩ of degree ŒΩ = 24, producing the pole-zero
pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens
when the model is reduced back to order Ô¨Åve by either deterministically balanced
truncation or stochastically balanced truncation. The zeros are denoted by ‚Äú‚ó¶‚Äù and
the poles by ‚Äú+‚Äù. Both reduction procedures give good approximations when applied
to exact covariance data. However, as we shall see in Section 5, the advantages of SBT
becomes apparent when applied to statistical data. Also, as explained in Remark 4.5,
there are theoretical reasons to prefer stochastic model reduction.
4. Model reduction
In the present setting, model reduction amounts to replacing a stochastic system
(2.1) of dimension ŒΩ by one of some dimension r < ŒΩ in such a way that most of
its statistical features are retained. In particular, we want to remove the part of the
system which corresponds to the weakest correlation between past and future. This
idea can be formalized in the following way.
Basic concepts. In the Hilbert space generated by the random variables {y(t) |
‚àí‚àû < t < ‚àû} in the inner product u, v = E{uv}, let H ‚àí be the subspace generated
by the past, i.e., {y(t) | t < 0}, and H + that generated by the future {y(t) | t ‚â• 0}.
Consider the Hankel operator H : H + ‚Üí H ‚àí and its adjoint H‚àó : H ‚àí ‚Üí H + deÔ¨Åned
as

H = EH

‚àí

|H +

and

H‚àó = E H

+

|H ‚àí ,

(4.1)

‚àí

where E H denotes orthogonal projection onto the past space H ‚àí . More precisely,
H sends Œæ ‚àà H + to E H ‚àí Œæ ‚àà H ‚àí and H‚àó sends Œ∑ ‚àà H ‚àí to E H + Œ∑ ‚àà H +. Since the
process y is the output of a minimal stochastic system of dimension ŒΩ, rank H = ŒΩ by
Kronecker‚Äôs Theorem [56], and hence H has exactly ŒΩ singular values, œÉ1 , œÉ2 , . . . , œÉŒΩ ,
which are positive, as usually listed so that œÉ1 ‚â• œÉ2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉŒΩ . These singular
values are the canonical correlation coeÔ¨Écients and hence the cosines of the angles
between the principal directions of the past space H ‚àí and the future space H + . They
are therefore less than one, and the part of the stochastic system corresponding to
singular values which are close to zero have a weak coupling between past and future,
i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic
model reduction is to truncate the system so that this part is removed.
To each singular value œÉk there is an associated Schmidt pair (Œæk , Œ∑k ) with Œæk ‚àà H +
and Œ∑k ‚àà H ‚àí such that

HŒæk = œÉk Œ∑k ,

H‚àóŒ∑k = œÉk Œæk ,

and such that the sequences Œæ1 , Œæ2 , Œæ3 , . . . and Œ∑1 , Œ∑2 , Œ∑3 , . . . of singular vectors are
orthonormal. The singular vectors corresponding to nonzero singular values span the
predictor spaces
X‚àí := span{Œ∑1 , Œ∑2 , . . . , Œ∑ŒΩ },

X+ := span{Œæ1 , Œæ2 , . . . , ŒæŒΩ }.

Clearly, X‚àí ‚äÇ H ‚àí and X+ ‚äÇ H + .
The process y has one representation (2.1) for each minimal spectral factor W ,
having W as its transfer function. Such representations are called minimal stochastic
realizations and the corresponding subspaces X := {a x(0) | a ‚àà RŒΩ } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X‚àí is the splitting subspace of the stochastic
realization

x‚àí (t + 1) = Ax‚àí (t) + B‚àí w‚àí (t)
(4.2)
y(t)
= Cx‚àí (t) + D‚àí w‚àí (t)
with the transfer function W‚àí (z), the minimum-phase spectral factor; and X+ is the
splitting subspace of

x+ (t + 1) = Ax+ (t) + B+ w+ (t)
(4.3)
y(t)
= Cx+ (t) + D+ w+ (t)
with transfer function W+ (z), the maximum-phase spectral factor, having all its zeros
in Dc . Note that A and C are the same in both realizations (uniform choice of bases).
Each realization has a counterpart which evolves backwards in time and has the
same splitting subspace. For example, the backward realization of X+ ,

xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ wÃÑ+ (t)
,
(4.4)
y(t)
= CÃÑ xÃÑ+ (t) + DÃÑ+ wÃÑ+ (t)
has transfer function WÃÑ+ (z), the coanalytic minimum-phase spectral factor, having all
its poles and zeros in Dc . In the present case with scalar y, we have WÃÑ+ (z) = W‚àí (z ‚àí1 ).
Now, in order to identify the part of the system which has the weakest coupling
between past and future, and hence will be removed in the model reduction, we need
to balance the system in the sense of Desai and Pal, as we shall explain next. To this
end, we make a coordinate transformation
(A, C, CÃÑ) ‚Üí (SAS ‚àí1 , CS ‚àí1 , CÃÑS  ),

(4.5)

in the minimal realization of
1
(4.6)
V (z) = C(zI ‚àí A)‚àí1 CÃÑ  + c0 ,
2
the strictly positive real part of the spectral density of y, so that the state covariances
P‚àí := E{x‚àí (t)x‚àí (t) } and PÃÑ+ = E{xÃÑ+ (t)xÃÑ+ (t) } coincide with the diagonal ŒΩ √ó ŒΩ
matrix Œ£ of nonzero canonical correlation coeÔ¨Écients, i.e.,
P‚àí = PÃÑ+ = Œ£ := diag(œÉ1 , œÉ2 , . . . , œÉŒΩ ).

(4.7)

1

This is done by choosing S so that Sx‚àí (0) = Œ£ 2 Œ∑, where Œ∑ = (Œ∑1 , Œ∑2 , . . . , Œ∑ŒΩ ) , and
1
(S  )‚àí1 xÃÑ+ (0) = Œ£ 2 Œæ, where Œæ := (Œæ1 , Œæ2 , . . . , ŒæŒΩ ) .
To compute the canonical correlation coeÔ¨Écients, we Ô¨Årst observe that the eigenvalues of the product P‚àí PÃÑ+ are precisely the squares of the canonical correlation
coeÔ¨Écients, i.e.,
Œª(P‚àí PÃÑ+ ) = Œª(P‚àí P+‚àí1 ) = {œÉ12 , œÉ22 , . . . , œÉŒΩ2 },

(4.8)

where we have used the fact that the state covariance of (4.3) is P+ = PÃÑ+‚àí1 . Therefore
the canonical correlation coeÔ¨Écients can then be determined via (4.8) by solving the
Lyapunov equations

P‚àí = AP‚àí A + B‚àí B‚àí


and P+ = AP+ A + B+ B+
.

(4.9)

12

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

The point is now to identify the canonical correlation coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉr corresponding to the part of the system one wants to keep. The part corresponding to
œÉr+1 , œÉr+2 , . . . , œÉŒΩ will be disposed of. This amounts to partitioning Œ£ as



Œ£1
,
(4.10)
Œ£=
Œ£2
where Œ£1 is r √ó r.
In order to reduce model (2.1) we make the coordinate transformation (A, B, C) ‚Üí
(SAS ‚àí1 , SB, CS ‚àí1 ), with the same balancing transformation S. Then, partition the
new triplet (A, B, C) conformally with (4.10) as


 




B1
A11 A12
(4.11)
, B=
, C = C1 C2 ,
A=
A21 A22
B2
and perform a principal subsystem truncation to obtain the transfer function of a
reduced-order system
Wred (z) = C1 (zI ‚àí A11 )‚àí1 B1 + D

(4.12)

of degree r. If Œ£2 is close to zero, while Œ£1 is not, the rank of H is close to r, and the
discarded part of the system gives a negligible contribution to y.
Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy Ô¨Ålter
‚àö ŒΩ
rŒΩ z
(4.13)
W‚àí (z) := WŒΩ (z) =
œïŒΩ (z)
of order ŒΩ, which, for the moment we denote W‚àí (z) to emphasize its character as the
minimum-phase spectral factor of the spectral density
rŒΩ
.
œïŒΩ (z)œïŒΩ (z ‚àí1 )
Remark 4.1. Without loss of generality we assume that œïŒΩ (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller ŒΩ for which this condition holds.
In fact, œïŒΩ (0) = Œ≥ŒΩ‚àí1 , and if Œ≥ŒΩ‚àíp = Œ≥ŒΩ‚àíp+1 = ¬∑ ¬∑ ¬∑ = Œ≥ŒΩ‚àí1 = 0 and Œ≥ŒΩ‚àíp‚àí1 = 0 for some
p = 1, 2, . . . , ŒΩ, then œïŒΩ (z) = z ŒΩ‚àíp œïŒΩ‚àíp (z) by (3.4), and hence (3.6) can be replaced
by WŒΩ (z) = WŒΩ‚àíp (z), and for WŒΩ‚àíp (z) the required condition holds.
The maximum-phase spectral factor W+ (z) has all its zeros at inÔ¨Ånity, and hence
‚àö
rŒΩ

‚àí1
W+ (z) = h (zI ‚àí F ) b =
,
(4.14)
œïŒΩ (z)
where (F, b, g) is the (observable) canonical form
Ô£π
Ô£Æ
Ô£π
Ô£Æ
0
0
1
¬∑¬∑¬∑
0
..
.. Ô£∫
. Ô£∫
...
Ô£Ø ...
.
. Ô£∫, b = Ô£Ø
Ô£Ø .. Ô£∫ ,
F =Ô£Ø
Ô£∞ 0 Ô£ª
Ô£∞ 0
0
¬∑¬∑¬∑
1 Ô£ª
‚àö
‚àíœïŒΩŒΩ ‚àíœïŒΩ,ŒΩ‚àí1 ¬∑ ¬∑ ¬∑ ‚àíœïŒΩ1
rŒΩ

Ô£Æ Ô£π
1
Ô£Ø0Ô£∫
Ô£∫
h=Ô£Ø
Ô£∞ ... Ô£ª ,
0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

œïŒΩ1 , œïŒΩ2 , . . . , œïŒΩŒΩ being the coeÔ¨Écients of the SzegoÃà polynomial œïŒΩ (z). In this basis,
it follows from (4.9) that

  œÄ

1
iŒ∏
‚àí1  ‚àíiŒ∏
 ‚àí1
(e I ‚àí A) bb (e I ‚àí A ) dŒ∏
[P+ ]jk =
2œÄ ‚àíœÄ
jk
 œÄ
1
r
ŒΩ
=
e‚àí(j‚àík)iŒ∏
dŒ∏ = cj‚àík ,
2œÄ ‚àíœÄ
œïŒΩ (eiŒ∏ )œïŒΩ (e‚àíiŒ∏ )
and hence P+ = TŒΩ . It is well-known and easy to prove that Œ¶ŒΩ TŒΩ Œ¶ŒΩ = RŒΩ , where

Œ¶ŒΩ+1

Ô£Æ
œïŒΩŒΩ
Ô£Ø ..
Ô£Ø .
Ô£Ø
= Ô£ØœïŒΩ2
Ô£Ø
Ô£∞œïŒΩ1
1

œïŒΩ‚àí1,ŒΩ‚àí1
..
.

œïŒΩ‚àí2,ŒΩ‚àí2
..
.

œïŒΩ‚àí1,1
1

1

¬∑¬∑¬∑

Ô£π
1
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ
rŒΩ‚àí1
Ô£Ø
Ô£Ø
and RŒΩ = Ô£Ø
Ô£∞

Ô£π
Ô£∫
Ô£∫
Ô£∫ , (4.16)
Ô£ª

rŒΩ‚àí2
..

.
r0

and consequently
PÃÑ+ = TŒΩ‚àí1 = Œ¶ŒΩ RŒΩ‚àí1 Œ¶ŒΩ .
It remains to determine P‚àí . From (4.13) is easy to see that
‚àö
W‚àí (z) = ‚àíœïŒΩ (zI ‚àí F )‚àí1 b + rŒΩ ,
where



œïŒΩ := œïŒΩŒΩ œïŒΩ,ŒΩ‚àí1 ¬∑ ¬∑ ¬∑ œïŒΩ1 ,

(4.17)

(4.18)

(4.19)

but, in order to determine P‚àí , this realization needs to be transformed so that the A
and C matrices are the same as in (4.14) (uniform choice of bases). More precisely,
we need to perform a transformation
(F, b, ‚àíœïŒΩ ) ‚Üí (QF Q‚àí1 , Qb, ‚àíœïŒΩ Q‚àí1 ) =: (F, Qb, h ).
Then P‚àí is the solution of the Lyapunov equation P‚àí = F P‚àí F  + Qbb Q , and therefore, since TŒΩ = F TŒΩ F  + bb and QF = F Q and consequently
QTŒΩ Q = F QTŒΩ Q F  + Qbb Q ,
we have
P‚àí = QTŒΩ Q .
To determine Q, notice that ‚àíœïŒΩ = h Q and QF
Ô£π Ô£Æ
Ô£Æ
h
‚àíœïŒΩ

Ô£Ø ‚àíœïŒΩ F Ô£∫ Ô£Ø h F
Ô£∫=Ô£Ø .
Ô£Ø
..
Ô£ª Ô£∞ ..
Ô£∞
.
‚àíœïŒΩ F ŒΩ‚àí1

(4.20)
= F Q to form
Ô£π
Ô£∫
Ô£∫ Q = Q.
Ô£ª

(4.21)

h F ŒΩ‚àí1

Next, deÔ¨Åne the symmetric matrix
M := RŒΩ‚àí1/2 Œ¶ŒΩ QTŒΩ Q Œ¶ŒΩ RŒΩ‚àí1/2 .

(4.22)

14

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

In view of (4.20) and (4.17), det(zI ‚àí M ) = det(zI ‚àí P‚àí PÃÑ+ ), and hence, by (4.8), M
has the eigenvalues œÉ12 , œÉ22 , . . . , œÉŒΩ2 , and the singular-value decomposition
M = U Œ£2 U  ,

(4.23)

where U  U = U U  = I. It is then well-known and simple to check that
S := Œ£‚àí1/2 U  RŒΩ‚àí1/2 Œ¶ŒΩ

(4.24)

is the required balancing transformation (4.5) such that SP‚àí S  = (S  )‚àí1 PÃÑ+ S ‚àí1 = Œ£.
Proposition 4.2. Given the partial covariance sequence
ck = E{y(t + k)y(t)},

k = 0, 1, . . . , ŒΩ,

let œï1 (z), œï2 (z), . . . , œïŒΩ (z) and r0 , r1 , . . . , rŒΩ be the corresponding SzegoÃà polynomials
and error variances. Supposing that Œ≥ŒΩ‚àí1 = ‚àíœïŒΩ (0) = 0, let (F, b, h) be given by
(4.15), RŒΩ and Œ¶ŒΩ by (4.16) and Q by (4.21). Moreover, let U and Œ£ be deÔ¨Åned by
the singular value decomposition (4.23) of (4.22). Then, the canonical correlation
coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉŒΩ are the diagonal elements of Œ£, as described in (4.7), and
the stochastically balanced realization of WŒΩ is given by
‚àö
(4.25)
(A, B, C, D) = (SF S ‚àí1 , SQb, h S ‚àí1 , rŒΩ ),
where S is deÔ¨Åned by (4.24).
Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to
yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase
property. In fact, we have the following result, the proof of which is given in Section A.
Theorem 4.3. Let Wred be the SBT approximation of degree r of WŒΩ , and set

ŒΩ
ŒΩ‚àí1
	
‚àö  1 + |Œ≥k |
œÉk
9 := 2
and Œ∫ := c0
,
(4.26)
1
‚àí
œÉ
1
‚àí
|Œ≥
k
k|
k=r+1
k=0
where Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1 are the Schur parameters of c0 , c1 , c2 , . . . , cŒΩ . Then
c0 (1 ‚àí 9)Œ∫‚àí1 ‚â§ |Wred (eiŒ∏ )| ‚â§ (1 + 9)Œ∫ for all Œ∏,

(4.27)

and, if 9 < 1, Wred is minimum phase. Finally, the approximation error has the bound
WŒΩ ‚àí Wred ‚àû ‚â§ 9Œ∫.

(4.28)

A properly executed SBT procedure should imply that the canonical correlation
coeÔ¨Écients œÉr+1 , . . . , œÉŒΩ , and hence 9, are close to zero, insuring the minimum-phase
condition.
Remark 4.4. Stochastic model reduction can also be carried out by instead performing principal subsystem truncation on (A, C, CÃÑ) in VŒΩ (z) = C(zI ‚àí A)‚àí1 CÃÑ + 12 c0 ,
where A and C are given by (4.25) and CÃÑ  = S(c1 , c2 , . . . , cn ). It was shown in
[32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally,
the spectral density Œ¶red (z) := Vred (z) + Vred (z ‚àí1 ) is factorized to yield a minimumphase spectral factor WÃÉ . This is in a sense a more natural procedure, but we do
not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small Œ£2 it yields almost the same result. In fact, it is shown in [53], that
|WÃÉ (eiŒ∏ )|2 = |Wred (eiŒ∏ )|2 + H(eiŒ∏ )Œ£2 H(e‚àíiŒ∏ ), where H(z) = C1 (zI ‚àí A11 )‚àí1 A12 .
Remark 4.5. There are good reasons to prefer stochastic over deterministic model
reduction, as seen from the following heuristics. In fact, it can be seen that
VŒΩ (z) =

c0 œàŒΩ (z)
,
2 œïŒΩ (z)

(4.29)

where œàŒΩ (z) is the SzegoÃà polynomial of the second kind (obtained by exchanging ‚àíŒ≥t
for Œ≥t in the recursion (3.4)). Now, the matrix representation of the Hankel operator
H in the innovation bases of the past and the future, provided by w‚àí and wÃÑ+ respecis the inÔ¨Ånite Hankel matrix of the sequence
tively, is given by L‚àí1 (L‚àí1 ) , where
c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T‚àû ;
see, e.g., [32, p. 714]. It is easy to see that œàŒΩ (z) has the same asymptotic behavior as
œïŒΩ (z), i.e., the roots tend to cluster uniformly inside the circle z = œÅ as ŒΩ ‚Üí ‚àû, and
hence these roots are close to canceling in (4.29). Consequently, the corresponding
Hankel matrix is close to having low rank. This massive ‚Äúalmost cancellation‚Äù does
not occur in WŒΩ (z), and hence the corresponding inÔ¨Ånite Hankel matrix, constructed
from the Laurent coeÔ¨Écients of WŒΩ (z), may have a less distinct separation between
Œ£1 and Œ£2 . On the other hand, since the Schur parameters tend geometrically to
zero, the lower part of L tends to the identity, and hence the asymptotic behavior of
the canonical correlation coeÔ¨Écients is very much like that of the singular values of
. Therefore we may expect SBT to have better statistical behavior than DBT. In
Section 6 we shall see that this is the case.

H

H

H

H

5. IdentiÔ¨Åcation from statistical data
We now return to our original problem of time series identiÔ¨Åcation: Given a data
string (2.2) of observations of the output process y of some n-dimensional linear
stochastic system (2.1) with minimum-phase transfer function W (z), given by (2.8),
Ô¨Ånd an estimate (AÃÇ, BÃÇ, CÃÇ, DÃÇ) of the matrices (A, B, C, D).
The identiÔ¨Åcation method proceeds as follows. Given the covariance estimates (2.5),
we compute the corresponding maximum entropy Ô¨Ålter (2.7), a balanced realization
(4.25), and the canonical correlation coeÔ¨Écients
œÉÃÇ1 , œÉÃÇ2 , œÉÃÇ3 , . . . , œÉÃÇŒΩ ,

(5.1)

determined as in Proposition 4.2 from the covariance estimates cÃÇ0 , cÃÇ1 , . . . , cÃÇŒΩ .
Based on (5.1), choose an integer nÃÇ such that œÉÃÇnÃÇ+1 , œÉÃÇnÃÇ+2 , . . . , œÉÃÇŒΩ are close to zero or
at least distinctively smaller than œÉÃÇ1 , œÉÃÇ2 , . . . , œÉÃÇnÃÇ . Then, the balanced realization (4.25)
is truncated accordingly as in (4.11) to yield a nÃÇ-dimensional triplet (A11 , B1 , C1 ) and
a transfer function
WÃÇ (z) = C1 (zI ‚àí A11 )‚àí1 B1 + D.

(5.2)

Then, (A11 , B1 , C1 , D) is the required estimate (AÃÇ, BÃÇ, CÃÇ, DÃÇ).
As pointed out in Section 2, we have a bound
W ‚àí WÃÇ ‚àû ‚â§ W ‚àí WŒΩ ‚àû + WŒΩ ‚àí WÃÇŒΩ ‚àû + WÃÇŒΩ ‚àí WÃÇ ‚àû ,

(5.3)

16

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the Ô¨Årst term W ‚àí WŒΩ ‚àû , which
does not depend on the statistical data (2.2) but only on the underlying system (2.1),
tends to zero geometrically with a rate Œ≥ ‚àà (0, 1) as ŒΩ ‚Üí ‚àû. The other two terms
depend on the data (2.2), and here N must grow at a faster rate than ŒΩ. In fact, we
shall assume that
ŒΩ = ŒΩ(N ) = O(log N ),

(5.4)

which in particular requires that limN ‚Üí‚àû NŒΩ = 0. We also need to assume that the
white noise process in (2.1) satisÔ¨Åes a mild technical condition, namely
E{w(t)4 } < ‚àû.

(5.5)

This condition is, of course, satisÔ¨Åed if w is Gaussian.
Next, we present our main convergence theorem.
Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then,
to each length N of the data string (2.2), there is a ŒΩ(N ), tending to inÔ¨Ånity with N
at the rate (5.4), such that any sequence of estimated transfer functions WÃÇ of Ô¨Åxed
degree nÃÇ ‚â• n, determined, for each N and corresponding ŒΩ = ŒΩ(N ), by the procedure
described above, satisÔ¨Åes
W ‚àí WÃÇ ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû. For suÔ¨Éciently large ŒΩ(N ), the transfer function WÃÇ has
minimum phase.
We have already proven that the Ô¨Årst term in (5.3) tends to zero, so Theorem 5.1
follows from the next two theorems, each corresponding to one of the remaining terms
in (5.3). As for the second term, we have the following result, the proof of which is
deferred to Appendix B.
Theorem 5.2. Suppose the system (2.1) satisÔ¨Åes the conditions of Theorem 5.1. Let
WŒΩ be the maximum-entropy Ô¨Ålter (3.6) determined from the partial covariance sequence (3.1) of y and let WÃÇŒΩ be the corresponding function determined from the ergodic
estimates (2.5). Then, if ŒΩ(N ) is deÔ¨Åned as in Theorem 5.1,
WŒΩ(N ) ‚àí WÃÇŒΩ(N ) ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû.
There are several results of this type in the literature [2, 36, 7, 33]. In particular,
3
Berk [2] proved that, provided ŒΩN ‚Üí 0 as N ‚Üí ‚àû and Œ¶ is coercive (i.e. positive
 iŒ∏ ) ‚Üí Œ¶(eiŒ∏ ) in probability.
on the unit circle), the estimated AR spectral density Œ¶(e
Under the same hypotheses, Caines and Baykal-GuÃàrsoy [7] showed that if N ‚â• ŒΩ 5+Œ∑
for some Œ∑ > 0, then WÃÇŒΩ‚àí1 ‚àí W ‚àí1 ‚àû ‚Üí 0 almost surely as ŒΩ ‚Üí ‚àû. However, in both
cases, ergodic estimates are used which are not quite the same as (2.5).
Finally, we consider the last term in (5.3). The proof of the following theorem is
given in Appendix B.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function ŒΩ(N ) are deÔ¨Åned as in
Theorem 5.1. Moreover, for each N , let WÃÇŒΩ(N ) be deÔ¨Åned as in Theorem 5.2 and WÃÇ
as in Theorem 5.1. Then, for suÔ¨Éciently large ŒΩ(N ), WÃÇ has minimum phase, and
WÃÇŒΩ(N ) ‚àí WÃÇ ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû.
6. Simulations
Performing model reduction on WÃÇŒΩ , rather than on the maximum-entropy Ô¨Ålter of
exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier
and more accurate order determination, as the heuristics of Remark 4.5 suggest.
There are also alternative order determination statistical tests based on the canonical
correlation coeÔ¨Écients [17, 26, 46]. But, even more importantly, there is less bias,
and the error variances are closer to the CrameÃÅr-Rao bound.
Since we are approximating rational models with AR models the method will be
biased for Ô¨Ånite amount of data, unless the model generating the data really is an
AR model. The consistency result given in Theorem 5.1 implies that the method is
asymptotically unbiased and therefore we consider the CrameÃÅr-Rao bound for unbiased methods; see [44, pp. 137‚Äì138]. The CrameÃÅr-Rao bound for biased estimation
requires knowledge about the bias as a function of the parameter to be estimated.
As already mentioned, the method will be unbiased and even statistically eÔ¨Écient for
Gaussian AR processes if the model reduction step is omitted. Despite the fact that
an algorithm based on covariance estimates (2.6) is not asymptotically eÔ¨Écient for
general ARMA models [44, p. 144], our method can be used to provide a starting
guess for other algorithms, for example the maximum likelihood method.
6
SBT dashed line, DBT dotted line.
5

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identiÔ¨Åcation estimates.

To illustrate our procedure, let us consider data generated by passing white noise
through a ‚Äútrue system‚Äù with transfer function
W (z) =

z 5 ‚àí 0.0550z 4 ‚àí 0.1497z 3 ‚àí 0.2159z 2 + 0.1717z ‚àí 0.0495
.
z 5 ‚àí 0.7031z 4 + 0.3029z 3 + 0.1103z 2 ‚àí 0.1461z + 0.2845

18

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy
model WÃÇŒΩ of degree ŒΩ = 24 determined from estimated covariances. Based on 100
test runs, the empirical means and standard deviations are determined. Figure 6.1
illustrates the statistical bias as a function of the length N of the data string when
using stochastic (dashed curve) and deterministic (dotted curve) model reduction
respectively.
For the same test runs, Figure 6.2 illustrates the corresponding standard deviations
together with the CrameÃÅr-Rao bound (solid curve). More precisely, the Ô¨Ågures depict
the sums of the moduli of the biases and standard deviations respectively for the
coeÔ¨Écients of the numerator and denominator polynomials of WÃÇ (z).
5
CRB solid line, SBT dashed line, DBT dotted line.
4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the CrameÃÅr-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z)
with poles and zeros closer to the unit circle is considered. The poles and zeros of
WÃÇ (z) are determined for 100 runs and a data length N = 500. As before, ŒΩ = 24.
Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with
the poles and zeros of W (z), which are denoted by ‚Äú‚ó¶‚Äù.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

1

‚àí1
‚àí1

‚àí0.5

0
poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT.
To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in
Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 24.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

1

‚àí1
‚àí1

‚àí0.5

0
poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identiÔ¨Åcation.

Figure 6.5 describes the result obtained when applying stochastic subspace identiÔ¨Åcation to the same data. More precisely, Algorithm # 2 in [43] is used. In order
to make the experiments comparable, we have chosen a Hankel matrix of dimension
13 √ó 13, which corresponds to ŒΩ = 25 in our procedure.
Note that the estimates are much less focused, and many zeros tend to cluster on the
unit circle, implying that coercivity becomes critical. This is related to the positivity
issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the
subspace identiÔ¨Åcation method performs worse than our SBT identiÔ¨Åcation method,
yielding larger biases and standard deviations, but performs better than when DBT
is used.

20

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as
illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will
disappear as ŒΩ and N are increased. In Figure 6.6 we show the same experiment for
ŒΩ = 64 and N = 2000.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and ŒΩ = 64.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 40 using Burg‚Äôs method.

In practice, there is a trade-oÔ¨Ä between the quality of the ergodic estimates, which
roughly speaking depend on |Œªmax (A)|, the ‚àû -error tolerance, which is a function of
|Œªmax (A ‚àí BD‚àí1 C)|, and the numerical accuracy of the computations. For example,
if the zeros of W (z) are far from the unit circle and ŒΩ is chosen very large, the error
may increase.
In the present example, it turns out that using Burg‚Äôs method [3] in lieu of the
ergodic estimate (2.6) yields better estimates for smaller ŒΩ and N , as illustrated in
Figure 6.7 which shows the case N = 500 and ŒΩ = 40.
A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There
we give the empirical bias and standard deviation for the coeÔ¨Écients of the numerator
and the denominator, respectively, of the estimated transfer functions together with
the CrameÃÅr-Rao bound. It is the authors experience that Burg‚Äôs method gives at
least as good results as when using the ergodic covariance estimate (2.6), unless the
intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

Parameter
True value
Bias:
CE:
Burg:
Std.dev.:
CE:
Burg:
CRB:

21

œÉw2
b1
b2
b3
b4
b5
1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491
0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895
0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734
0.2332 0.1314 0.0508 0.0611 0.0722 0.0802
0.0712 0.0411 0.0381 0.0339 0.0339 0.0356
0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and ŒΩ = 40 using covariance estimation (CE) or Burg estimation and , in
both cases, followed by SBT.

Parameter
a1
a2
a3
a4
a5
True value
-0.6281 0.3597 0.2634 -0.5322 0.7900
Bias:
CE:
0.0087 -0.0044 -0.0003 0.0066 -0.0152
Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125
Std.dev.:
CE:
0.0274 0.0304 0.0371 0.0305 0.0304
Burg: 0.0336 0.0307 0.0358 0.0324 0.0306
0.0293 0.0321 0.0342 0.0322 0.0290
CRB:
Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and ŒΩ = 40 using covariance estimation (CE) or Burg estimation and,
in both cases, followed by SBT.

7. Conclusions
We have presented a three-step procedure for identiÔ¨Åcation of time series, which is easy
to understand and implement. Just like for subspace identiÔ¨Åcation methods, robust
linear-algebra algorithms can be used and no nonconvex optimization computations
are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identiÔ¨Åcation, as our extensive simulations indicate.
In particular, its good performance has been conÔ¨Årmed by Monte Carlo simulations.
The paper only covers the scalar case, but the multivariate case is presently being
worked out.
The three steps, covariance estimation, covariance extension and model reduction
have each been studied separately before. This is an advantage which should make the
method easy to grasp. However, a comprehensive study of the entire identiÔ¨Åcation
strategy, giving appropriate bounds, has been missing and this is what we oÔ¨Äered
here.
The observation that the Schur parameters converge geometrically simpliÔ¨Åes our
application of SzegoÃà theory and allows us to give a complete account of the asymptotic
behavior of maximum entropy models of growing order. This analysis provides us with
a clear indication as to when the identiÔ¨Åcation strategy is good and when it might face
diÔ¨Éculties, based purely on the closeness of the maximum modulus zero to the unit
circle. The parsimony permeating other system identiÔ¨Åcation methods should not be
a reason for refraining from high-order modeling as an intermediate step. In fact,
such a strategy might be desirable, since we have shown that the poles of the ‚Äútrue‚Äù
system which lie outside a circle in the complex plane containing all of its zeros are

22

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the
perimeter of this circle, providing a justiÔ¨Åcation for choosing stochastically balanced
model reduction, rather than deterministically balanced truncation, in the last step.
With this reduction procedure, we have conÔ¨Årmed better statistical properties with
variances closer to the CrameÃÅr Rao bound. The procedure could also be modiÔ¨Åed by
exchanging exact covariance extension for approximate one, as outlined in [35].
Even though, in general, stochastic balancing would require the solution of a pair
of Riccati equations, this is not the case for the particular maximum entropy models
used here. In fact, the balancing procedure only requires linear algebra, and hence
an intelligent use of the Levinson algorithm may substantially reduce the number of
arithmetic operations.
Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N , ŒΩ and nÃÇ, we gave worst-case guaranteed
bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive
stochastic system, but the method returns a valid model also for generic data. In fact,
in contrast to many stochastic subspace identiÔ¨Åcation [9], all steps of the procedure
preserve the positive real property.
Appendix A. Asymptotic behavior of the maximum entropy Ô¨Ålter
Theorem 3.4 is actually a modiÔ¨Åcation to the rational setting of a theorem due to
SzegoÃà [47], and the proof is modeled after [19], which in turn includes aspects already
present in the work of Schur [45]. See also [48], [49] and [16] for more facts on
orthogonal polynomials. However, rationality and coercivity allows us to present a
simpliÔ¨Åed and self-contained proof of a version of SzegoÃà‚Äôs classical theorem, to which
we also are able to add geometric convergence. The derivation of Caines and BaykalGuÃàrsoy [7] is shorter, but we feel that our approach is more systematic and gives
additional insight into the mechanism of identiÔ¨Åcation.
To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas.
‚àíŒΩ
œÜŒΩ (z)|
Lemma A.1. Let {œÜŒΩ (z)}‚àû
0 be the normalized SzegoÃà polynomials (3.7). Then |z
c
is uniformly bounded from above and away from zero in the complement D of the open
unit disc, i.e., there are positive numbers Œ±, Œ≤ ‚àà R such that

Œ± ‚â§ |z ‚àíŒΩ œÜŒΩ (z)| ‚â§ Œ≤
for all ŒΩ and all z ‚àà Dc .
Proof. In view of the SzegoÃà-Levinson recursion (3.4),



œï‚àót (z)
œït+1 (z) = œït (z) z ‚àí Œ≥ÃÑt
,
œït (z)
and hence
z

‚àíŒΩ

œïŒΩ (z) =

ŒΩ‚àí1


k=0


œï‚àók (z)
1 ‚àí z Œ≥ÃÑk
.
œïk (z)
‚àí1

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , zŒΩ are the roots of œïŒΩ (z), it is immediately seen that
œï‚àóŒΩ (z)  1 ‚àí z zÃÑk
,
=
œïŒΩ (z) k=1 z ‚àí zk
ŒΩ

which is a Blaschke product, analytic in Dc and having modulus one on the unit circle,
and thus modulus less than or equal to one in Dc . Hence, since |z ‚àí1 | ‚â§ 1 in Dc ,
ŒΩ‚àí1


(1 ‚àí |Œ≥k |) ‚â§ |z

‚àíŒΩ

œïŒΩ (z)| ‚â§

k=0

ŒΩ‚àí1


(1 + |Œ≥k |)

(A.1)

k=0

for all z ‚àà Dc . But, these products converge to positivenumbers as ŒΩ ‚Üí ‚àû. This
follows from the absolute convergence of the inÔ¨Ånite sum ‚àû
k=0 |Œ≥k |, a fact that, in the
present context, stems from Lemma 3.1. From (3.5) we also have 0 < r‚àû ‚â§ rŒΩ ‚â§ r0 ,
and consequently the lemma follows.
Remark A.2. An equivalent statement of this lemma is that the maximum entropy
solution WŒΩ (z), deÔ¨Åned by (3.6), is uniformly bounded from above and away from
zero for all ŒΩ and z ‚àà Dc .
Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of
functions
fŒΩ (z) := z ‚àíŒΩ œÜŒΩ (z)
converges uniformly to an analytic function f‚àû in
statement of Theorem 3.5.

DcœÅ,

where

DcœÅ

is deÔ¨Åned in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the
purely algebraic relation
ŒΩ
	

œÜk (z)œÜk (w) =

k=0

œÜ‚àóŒΩ (z)œÜ‚àóŒΩ (w) ‚àí z wÃÑœÜŒΩ (z)œÜŒΩ (w)
,
1 ‚àí z wÃÑ

(A.2)

which is called the ChristoÔ¨Äel-Darboux-SzegoÃà formula. In particular, setting w = 0
and exchanging z for z ‚àí1 in (A.2), (3.5) and (3.7) yield
fŒΩ (z)
1 	
Œ≥k‚àí1
‚àí
œÜk (z ‚àí1 ) ‚àö .
‚àö =
rŒΩ
c0 k=1
rk
ŒΩ

(A.3)

Observe that œÜk (z ‚àí1 ) is analytic and bounded in DcœÅ , and hence in Dc , and therefore it
‚àû
. Moreover, by the maximum modulus principle, it attains its maximum
belongs to ‚àí
c
value in D on the unit circle where, by Lemma A.1, it is bounded by Œ≤. Hence

H

|œÜk (z ‚àí1 )| ‚â§ Œ≤

for z ‚àà Dc and for all k.

Therefore, in view of (A.3) and the fact that rk ‚â• r‚àû , we have


ŒΩ
	
 fŒΩ (z) f¬µ (z) 
 ‚àö ‚àí ‚àö  ‚â§ ‚àöŒ≤
|Œ≥k‚àí1 |,
 rŒΩ
r¬µ 
r‚àû k=¬µ+1

(A.4)

(A.5)

24

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for suÔ¨Éciently large ŒΩ and ¬µ.
‚àû
. The same holds for fŒΩ (z). In
This establishes (A.3) as a Cauchy sequence in ‚àí
c
fact, since rŒΩ ‚â§ c0 , for all z ‚àà D


‚àö  fŒΩ (z) f¬µ (z) 
|fŒΩ (z) ‚àí f¬µ (z)| ‚â§ c0  ‚àö ‚àí ‚àö 
rŒΩ
rŒΩ




 1
‚àö  fŒΩ (z) f¬µ (z) 
1 

‚â§ c0  ‚àö ‚àí ‚àö  + |f¬µ (z)|  ‚àö ‚àí ‚àö  .
(A.6)
rŒΩ
r¬µ
rŒΩ
r¬µ

H

But, by Lemma A.1, |f¬µ (z)| ‚â§ Œ≤ for all ŒΩ and z ‚àà Dc , and therefore, in view of (A.5),
we obtain



ŒΩ
 1

c0 	
1
|Œ≥k‚àí1 | + Œ≤  ‚àö ‚àí ‚àö  for all z ‚àà Dc . (A.7)
|fŒΩ (z) ‚àí f¬µ (z)| ‚â§ Œ≤
r‚àû k=¬µ+1
rŒΩ
r¬µ
Since rŒΩ ‚Üí r‚àû as ŒΩ ‚Üí ‚àû, we see that, for each 9 > 0, |fŒΩ (z) ‚àí f¬µ (z)| < 9 for
suÔ¨Éciently large ŒΩ and ¬µ. Consequently, fŒΩ tends uniformly in Dc to a function
‚àû
.
f‚àû ‚àà H‚àí
The uniform convergence and the analyticity can be extended to any compact
subset of DcœÅ . To see this, Ô¨Årst note that z ‚àà D if and only if z ‚àí1 ‚àà Dc . Therefore, by
Lemma A.1,
|œÜk (z ‚àí1 )| ‚â§ Œ≤|z|‚àík for z ‚àà D,
and consequently, since rŒΩ ‚â§ rk , (A.3) yields
ŒΩ‚àí1
	
1
|fŒΩ (z)| ‚â§ ‚àö + Œ≤|z|‚àí1
|Œ≥k ||z|‚àík .
c0
k=0

Similarly, instead of (A.5) we have


ŒΩ‚àí1
	
 fŒΩ (z) f¬µ (z) 
 ‚àö ‚àí ‚àö  ‚â§ ‚àöŒ≤ |z|‚àí1
|Œ≥k ||z|‚àík .
 rŒΩ
r¬µ 
r‚àû
k=¬µ

(A.8)

(A.9)

Now, for any compact subset K ‚àà DcœÅ , there is a Œ≥ ‚àà (œÅ, 1) and an 9 > 0 such
that |z| > Œ≥ + 9 for all z ‚àà K. Hence, by Lemma 3.1, |Œ≥k ||z|‚àík ‚â§ M Œ≥ÃÇ k where
Œ≥ÃÇ := Œ≥(Œ≥ + 9)‚àí1 < 1. Consequently, by (A.8), fŒΩ (z) is uniformly bounded in K, and
(A.9) can be made arbitrarily small for suÔ¨Éciently large ŒΩ and ¬µ. Therefore, by (A.6),
fŒΩ tends uniformly in K to the analytic function f‚àû .
Lemma A.4. Let Œ≥ be a real number such that œÅ < Œ≥ < 1. Then fŒΩ ‚àíf‚àû ‚àû = O(Œ≥ ŒΩ ).
Proof. It follows from (A.7) that


 
‚àû

‚àö 	
Œ≤
r‚àû 
c0
|Œ≥k‚àí1 | + 1 ‚àí
|fŒΩ (z) ‚àí f‚àû (z)| ‚â§ ‚àö
r‚àû
rŒΩ 
k=ŒΩ+1

for all z ‚àà Dc .
(A.10)

By Lemma 3.1, the Ô¨Årst term is O(Œ≥ ŒΩ ). It remains to show that the same holds for
the second term. To this end, Ô¨Årst note that, by (3.5),

‚àû 

r‚àû
=1‚àí
1 ‚àí Œ≥k2 .
1‚àí
rŒΩ
k=ŒΩ

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

But, by Lemma 3.1, |Œ≥k | ‚â§ M Œ≥ k for some M . Therefore, since
each x ‚àà [0, 1],

‚àû

r‚àû
‚â§ 1 ‚àí (1 ‚àí M Œ≥ k ) = O(Œ≥ t )
1‚àí
rt
k=t

‚àö

25

1 ‚àí x2 ‚â• 1 ‚àí x for

for t large enough. This concludes the proof.
Recalling the deÔ¨Ånition (3.6) of WŒΩ , we note that Lemma A.4 may be written
WŒΩ‚àí1 ‚àí f‚àû ‚àû = O(Œ≥ ŒΩ ).
‚àí1
in the same manner.
As it turns out, by coercivity, this implies that WŒΩ ‚Üí f‚àû

Lemma A.5. Let WŒΩ be the transfer function (3.6) of the maximum entropy Ô¨Ålter.
Then
‚àí1
‚àû = O(Œ≥ ŒΩ ),
WŒΩ ‚àí f‚àû
where f‚àû is the limit function of Lemma A.3.
Proof. Note that the limit function f‚àû has the same uniform bounds as fŒΩ in Lemma
A.1. In particular, |f‚àû (z)| ‚â• Œ±, |f‚àû (z)|‚àí1 ‚â§ Œ±‚àí1 , and |WŒΩ (z)| ‚â§ Œ±‚àí1 for all z ‚àà Dc .
Consequently,
‚àí1
‚àí1
WŒΩ ‚àí f‚àû
‚àû ‚â§ WŒΩ ‚àû f‚àû
‚àû WŒΩ‚àí1 ‚àí f‚àû ‚àû ‚â§ Œ±‚àí2 WŒΩ‚àí1 ‚àí f‚àû ‚àû ,

so the required result follows from Lemma A.4.
Lemma A.6. Let W be the rational minimum-phase function deÔ¨Åned above, and let
f‚àû be the limit function in Lemma A.3. Then W (z) = f‚àû (z)‚àí1 for all z ‚àà DcœÅ .
Proof. Let Œ¶ŒΩ (eiŒ∏ ) := |WŒΩ (eiŒ∏ )|2 be the spectral density of the maximum entropy
process. Then, in view of the interpolation condition,
 œÄ
 œÄ
1
1
ikŒ∏
iŒ∏
e Œ¶(e )dŒ∏ = ck =
eikŒ∏ Œ¶ŒΩ (eiŒ∏ )dŒ∏ for k = 0, 1, . . . , ŒΩ, (A.11)
2œÄ ‚àíœÄ
2œÄ ‚àíœÄ
from which we have pointwise convergence of the Fourier coeÔ¨Écients of Œ¶ŒΩ (eiŒ∏ ) to
those of Œ¶(eiŒ∏ ) as ŒΩ ‚Üí ‚àû, and hence Œ¶ŒΩ (eiŒ∏ ) ‚Üí Œ¶(eiŒ∏ ) in the 2 sense. However, by
Lemma A.5, Œ¶ŒΩ (eiŒ∏ ) ‚Üí |f‚àû (eiŒ∏ )|‚àí2 in ‚àû norm, and hence a fortiori in 2 norm, as
ŒΩ ‚Üí ‚àû. Since, in addition, not only Œ¶(eiŒ∏ ) but also f‚àû is analytic in a neighborhood
of the unit circle (Lemma A.3), we have

L

L

L

Œ¶(eiŒ∏ ) = |f‚àû (eiŒ∏ )|‚àí2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer.
In particular, WŒΩ is an outer spectral factor of Œ¶ŒΩ (eiŒ∏ ) satisfying

  œÄ it

e +z
1
it 2
log |WŒΩ (e )| dt .
WŒΩ (z) = exp
4œÄ ‚àíœÄ eit ‚àí z
But Lemma A.5, Equation (A.12) and the fact that Œ¶(eiŒ∏ ) = |W (eiŒ∏ )|2 ,

  œÄ it

1
e +z
it 2
log |W (e )| dt = W (z),
WŒΩ (z) ‚Üí exp
4œÄ ‚àíœÄ eit ‚àí z
the outer spectral factor of Œ¶. But, by Lemma A.3, WŒΩ (z) ‚Üí f‚àû (z)‚àí1 in
therefore f‚àû (z) = W ‚àí1 (z) as claimed.

DcœÅ, and

26

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6.
Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6.
Proof of Theorem 4.3. Following [53] we see that
WŒΩ‚àí1 (WŒΩ ‚àí Wred )‚àû ‚â§ 9,

(A.13)

and consequently
|WŒΩ (eiŒ∏ ) ‚àí Wred (eiŒ∏ )| ‚â§ 9|WŒΩ (eiŒ∏ )|
holds for all Œ∏, from which we have
(1 ‚àí 9)|WŒΩ (eiŒ∏ )| ‚â§ |Wred (eiŒ∏ )| ‚â§ (1 + 9)|WŒΩ (eiŒ∏ )|.
However, in view of (3.6) and (3.7), it follows from (A.1) that
‚àö
‚àö
rŒΩ
rŒΩ
iŒ∏
‚â§ |WŒΩ (e )| ‚â§ ŒΩ‚àí1
,
ŒΩ‚àí1
k=0 (1 + |Œ≥k |)
k=0 (1 ‚àí |Œ≥k |)
which together with (3.5) yields
c0
‚â§ |WŒΩ (eiŒ∏ )| ‚â§ Œ∫
Œ∫

(A.14)

for all Œ∏. This establishes (4.27). To see that Wred is minimum phase if 9 < 1, note
that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by RoucheÃÅ‚Äôs
Theorem, Wred has the same number of zeros in Dc (including ‚àû) as WŒΩ . Hence,
since WŒΩ is minimum phase, so is Wred .
To establish the bound (4.28) note that
WŒΩ ‚àí Wred ‚àû ‚â§ WŒΩ ‚àû WŒΩ‚àí1 (WŒΩ ‚àí Wred )‚àû .
From (A.14) we have WŒΩ ‚àû ‚â§ Œ∫, and hence (4.28) follows from (A.13).
Appendix B. Statistical convergence proofs
Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corresponding SzegoÃà polynomial œïÃÇŒΩ (z) and predictor error variance rÃÇŒΩ from (3.4) and (3.5),
and form the maximum-entropy Ô¨Ålter
‚àö ŒΩ
rÃÇŒΩ z
.
WÃÇŒΩ (z) =
œïÃÇŒΩ (z)
To determine WŒΩ ‚àí WÃÇŒΩ ‚àû let z ‚àà Dc and form
‚àö ŒΩ
‚àö ŒΩ
rŒΩ z
rÃÇŒΩ z
WŒΩ (z) ‚àí WÃÇŒΩ (z) =
‚àí
œïŒΩ (z)
œïÃÇŒΩ (z)
‚àö
‚àö
‚àö
( rŒΩ ‚àí rÃÇŒΩ )z ‚àíŒΩ œïŒΩ (z) ‚àí rŒΩ z ‚àíŒΩ (œïŒΩ (z) ‚àí œïÃÇŒΩ (z))
.
=
z ‚àíŒΩ œïŒΩ (z)z ‚àíŒΩ œïÃÇŒΩ (z)
Since r‚àû > 0, by (3.7) and Lemma A.1,
‚àö
‚àö
0 < ¬µ := r‚àû Œ± ‚â§ |z ‚àíŒΩ œïŒΩ (z)| ‚â§ c0 Œ≤ =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1),
|z

‚àíŒΩ

œïÃÇŒΩ (z)| ‚â• ¬µÃÇŒΩ :=

ŒΩ‚àí1


(1 ‚àí |Œ≥ÃÇk |),

k=0

where Œ≥ÃÇ0 , Œ≥ÃÇ1 , . . . , Œ≥ÃÇŒΩ‚àí1 are the Schur parameters corresponding to the estimated covariances (2.6). Therefore, by the maximum-modulus principle,

‚àö
‚àö
1
{M | rŒΩ ‚àí rÃÇŒΩ | + c0 |œïŒΩ (z) ‚àí œïÃÇŒΩ (z)|},
|WŒΩ (z) ‚àí WÃÇŒΩ (z)| ‚â§ max
|z|=1 ¬µ¬µÃÇŒΩ
where we have also used the fact that rŒΩ ‚â§ c0 . But, for |z| = 1,
|œïŒΩ (z) ‚àí œïÃÇŒΩ (z)| ‚â§ œïŒΩ ‚àí œïÃÇŒΩ 1 ,
where œïŒΩ and œïÃÇŒΩ are the ŒΩ-vectors formed as in (4.19) and ¬∑1 is the B1 norm. Recall
that œïŒΩ is the unique solution of the normal equations


TŒΩ œïŒΩ = ‚àícŒΩ where cŒΩ := cŒΩ cŒΩ‚àí1 . . . c1 ,
(B.1)
where TŒΩ is the Toeplitz matrix deÔ¨Åned by (2.4), and that
rŒΩ = c0 + cŒΩ œïŒΩ .

(B.2)

Also, the analogous relations hold for œïÃÇŒΩ and rÃÇŒΩ . Then,
rŒΩ ‚àí rÃÇŒΩ = (c0 ‚àí cÃÇ0 ) + (cŒΩ ‚àí cÃÇŒΩ ) œïŒΩ + cÃÇŒΩ (œïŒΩ ‚àí œïÃÇŒΩ )
and hence
|rŒΩ ‚àí rÃÇŒΩ | ‚â§ |c0 ‚àí cÃÇ0 | + cŒΩ ‚àí cÃÇŒΩ 1 œïŒΩ ‚àû + cÃÇŒΩ ‚àû œïŒΩ ‚àí œïÃÇŒΩ 1 .
Finally,


‚àö
|rŒΩ ‚àí rÃÇŒΩ |
|r ‚àí rÃÇ |
‚àö ‚â§ ŒΩ‚àö ŒΩ ,
| rŒΩ ‚àí rÃÇŒΩ | ‚â§ ‚àö
r‚àû
rŒΩ + rÃÇŒΩ

and consequently, since x1 ‚â§ ŒΩx‚àû for any x ‚àà RŒΩ ,

M
‚àö {|c0 ‚àí cÃÇ0 | + œïŒΩ ‚àû ŒΩcŒΩ ‚àí cÃÇŒΩ ‚àû }
¬µ¬µÃÇŒΩ r‚àû



M cÃÇŒΩ ‚àû
1 ‚àö
c0 + ‚àö
+
ŒΩœïŒΩ ‚àí œïÃÇŒΩ ‚àû .
¬µ¬µÃÇŒΩ
r‚àû

WŒΩ ‚àí WÃÇŒΩ ‚àû ‚â§

(B.3)

Recall now that œïŒΩ and œïÃÇŒΩ are each solutions of a normal equation (B.1). More
precisely, TŒΩ œïŒΩ = ‚àícŒΩ and TÃÇŒΩ œïÃÇŒΩ = ‚àícÃÇŒΩ . Since ck = CAk‚àí1 CÃÑ  for k > 0, where all
eigenvalues of A are less than one in modulus, ck ‚Üí 0 exponentially, we have
cŒΩ ‚àû ‚â§ K1

and TŒΩ ‚àû ‚â§ c0 + 2

ŒΩ‚àí1
	

|ck | ‚â§ K2

k=1

for some constants K1 and K2 . Moreover, from [8] we have
TŒΩ‚àí1 ‚àû ‚â§

ŒΩ‚àí1
1  1 + |Œ≥k |
‚â§ K3
c0 k=0 1 ‚àí |Œ≥k |

28

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence
œïŒΩ ‚àû ‚â§ TŒΩ‚àí1 ‚àû cŒΩ ‚àû ‚â§ K1 K3
and the condition number
Œ∫(TŒΩ ) := TŒΩ ‚àû TŒΩ‚àí1 ‚àû ‚â§ K := K2 K3
is bounded for all ŒΩ.
Now, it is known [25] that for each data length N in (2.2), there is a ŒΩ(N ) of order
O(log N ) such that


log log N
,
(B.4)
max |ck ‚àí cÃÇk | = O
0‚â§k‚â§ŒΩ(N )
N
and therefore, for any a ‚àà R,
ŒΩ a |c0 ‚àí cÃÇ0 | ‚Üí 0 and ŒΩ a cŒΩ ‚àí cÃÇŒΩ ‚àû ‚Üí 0 as ŒΩ = ŒΩ(N ) ‚Üí ‚àû.

(B.5)

Consequently the Ô¨Årst term in the bound (B.3) tends to zero as N ‚Üí ‚àû and
ŒΩ(N ) ‚Üí ‚àû provided it is done at the speciÔ¨Åed relative rates and provided ¬µÃÇŒΩ is
bounded away from zero. However, the estimate (2.6) has the property that the
corresponding Toeplitz matrix TÃÇŒΩ is positive deÔ¨Ånite for each Ô¨Ånite ŒΩ, and this in turn
is equivalent to |Œ≥ÃÇk | < 1 for k = 0, 1, . . . , ŒΩ ‚àí 1 so that ¬µÃÇŒΩ > 0. Since, in addition
¬µÃÇŒΩ ‚Üí ¬µ > 0 as ŒΩ(N ) ‚Üí ‚àû by (B.4) and continuity, the second requirement is also
fulÔ¨Ålled. To simplify notations, we have suppressed the index N in the quantities
marked with a hat, which of course depend on the data (2.2) and hence also on N .
Next we show that also the second term in (B.3) tends to zero. Since cÃÇŒΩ ‚àû ‚â§
cŒΩ ‚àû + cŒΩ ‚àí cÃÇŒΩ ‚àû is bounded, it thus remains to demonstrate that
ŒΩœïŒΩ(N ) ‚àí œïÃÇŒΩ(N ) ‚àû ‚Üí 0 as ŒΩ(N ) ‚Üí ‚àû.
This follows from the more general fact, needed for the proof of Corollary B.1, that
ŒΩ a œïŒΩ(N ) ‚àí œïÃÇŒΩ(N ) ‚àû ‚Üí 0 as ŒΩ(N ) ‚Üí ‚àû

(B.6)

for any a ‚àà R. To prove this, Ô¨Årst note that
TŒΩ ‚àí TÃÇŒΩ ‚àû ‚â§ |c0 ‚àí cÃÇ0 | + 2ŒΩcŒΩ ‚àí cÃÇŒΩ ‚àû ,
and hence TŒΩ ‚àí TÃÇŒΩ ‚àû ‚Üí 0. Therefore œÅŒΩ := TŒΩ ‚àí TÃÇŒΩ ‚àû TŒΩ‚àí1 ‚àû < 1 for ŒΩ := ŒΩ(N )
suÔ¨Éciently large, and, provided cŒΩ = 0, the standard perturbation estimate [22] yields


1
TŒΩ ‚àí TÃÇŒΩ ‚àû cŒΩ ‚àí cÃÇŒΩ ‚àû
œïŒΩ ‚àí œïÃÇŒΩ ‚àû
‚â§
Œ∫(TŒΩ )
+
,
(B.7)
œïŒΩ ‚àû
1 ‚àí œÅŒΩ
TŒΩ ‚àû
cŒΩ ‚àû
and consequently, since TŒΩ ‚àû ‚â• c0 > 0, it follows from (B.5) that (B.6) tends to
zero in the required manner.
If cŒΩ = 0, œïŒΩ = 0, and hence
œïŒΩ ‚àí œïÃÇŒΩ ‚àû = œïÃÇŒΩ ‚àû ‚â§ TÃÇŒΩ‚àí1 ‚àû cÃÇŒΩ ‚àû = TÃÇŒΩ‚àí1 ‚àû cŒΩ ‚àí cÃÇŒΩ ‚àû ,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case cŒΩ = 0. In fact, since ¬µÃÇ is
bounded away from zero, by continuity, for each 9 > 0, there is a N0 such that
TÃÇŒΩ‚àí1 ‚àû

ŒΩ‚àí1
ŒΩ‚àí1
1  1 + |Œ≥k |
1  1 + |Œ≥ÀÜk |
‚â§
+ 9 ‚â§ K3 + 9
‚â§
cÃÇ0 k=0 1 ‚àí |Œ≥ÀÜk |
c0 k=0 1 ‚àí |Œ≥k |

for ŒΩ ‚â• N0 .

Corollary B.1. If ŒΩ(N ) is deÔ¨Åned as in Theorem 5.1, then, for any a ‚àà R,
ŒΩ a WŒΩ ‚àí WÃÇŒΩ ‚àû ‚Üí 0

almost surely as ŒΩ := ŒΩ(N ) ‚Üí ‚àû.

To prove Theorem 5.3, we Ô¨Årst note that the Hankel operator H, deÔ¨Åned by (4.1),
has a nice representation in the space 2 of square-integrable functions. In fact, let
2
2
of functions with vanishing negative Fourier coeÔ¨Écients,
+ be the subspace in
hence being analytic in the unit disc D. In this setting, H has the representation
2
2
‚Üí 2 +
given by
HŒò : +

L

L

H

H

L H

HŒò f = P ‚ä• Œòf,

(B.8)

where P ‚ä• is the orthogonal projection onto the orthogonal complement
2
2
, and where Œò is the ‚àû -function
+ in

H

L

L

Œò(z) = W‚àí (z)WÃÑ+ (z)‚àí1 .

L H
2

2
+

of

(B.9)

Here W‚àí (z) and WÃÑ+ (z) are the analytic and coanalytic minimum-phase spectral factors deÔ¨Åned in Section 4. (See, e.g., [30, 31].) In the present scalar case, WÃÑ+ (z) =
W‚àí (z ‚àí1 ). In fact, the phase function Œò is the transfer function of an all-pass Ô¨Ålter
transforming the white noise w‚àí in (4.2) to the white noise w+ in (4.3) [30, p. 834].
ÀÜ + be the stochastic measures such that
Let dwÃÇ‚àí and dwÃÑ
 œÄ
 œÄ
iŒ∏t
ÀÜ+
e dwÃÇ‚àí and wÃÑ+ (t) =
eiŒ∏t dwÃÑ
w‚àí (t) =
‚àíœÄ

Then

‚àíœÄ


H

+

œÄ

=

H‚àí =
and consequently
H := E

f ‚Üî f dwÃÇ‚àí .

H‚àí

‚àíœÄ
œÄ

H

L H
2

‚àíœÄ



2 ÀÜ
+ dwÃÑ +

œÄ

=
‚àíœÄ

H

2
iŒ∏t
+ Œò(e )dwÃÇ‚àí

2
+ dwÃÇ‚àí

|H + corresponds to HŒò under the isomorphism deÔ¨Åned by





Proof of Theorem 5.3. It follows from Theorem 5.2 that |WÃÇŒΩ (eiŒ∏ )| ‚àí |WŒΩ (eiŒ∏ )| ‚Üí 0
uniformly in Œ∏ as ŒΩ ‚Üí ‚àû, and hence, by Lemma A.1, there are positive real numbers
¬µ1 and ¬µ2 such that
¬µ1 ‚â§ |WÃÇŒΩ (eiŒ∏ )| ‚â§ ¬µ2
for all Œ∏ and suÔ¨Éciently large ŒΩ. Therefore, since
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ WÃÇŒΩ ‚àû WÃÇŒΩ‚àí1 (WÃÇŒΩ ‚àí WÃÇ )‚àû ,

(B.10)

30

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

(A.13) and (4.26) imply that
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ 2¬µ2

ŒΩ
	

œÉÃÇk
,
1 ‚àí œÉÃÇk
k=nÃÇ+1

(B.11)

for suÔ¨Éciently large ŒΩ, where œÉÃÇ1 , œÉÃÇ2 , . . . , œÉÃÇŒΩ are the singular values (5.1) determined
from the covariance estimates (2.6).
It is well-known (see, e.g., [56, p. 204]) that the singular value œÉk of the Hankel
operator HŒò , deÔ¨Åned by (B.8) equals the inÔ¨Åmum of HŒò ‚àí K over all operators
2
2
‚Üí 2 +
of Ô¨Ånite rank at most k. Recall that Œò(z) = WŒΩ (z)/WŒΩ (z ‚àí1 ).
K : +
The singular value œÉÃÇk of HŒòÃÇ , where ŒòÃÇ(z) = WÃÇŒΩ (z)/WÃÇŒΩ (z ‚àí1 ), is described analogously.
Therefore, since

H

L H

HŒòÃÇ ‚àí K ‚â§ HŒòÃÇ ‚àí HŒò  + HŒò ‚àí K ‚â§ ŒòÃÇ ‚àí Œò‚àû + HŒò ‚àí K,
we have œÉÃÇk ‚â§ ŒòÃÇ ‚àí Œò‚àû + œÉk . But, for k > n, œÉk = 0, and hence œÉÃÇk ‚â§ ŒòÃÇ ‚àí Œò‚àû .
Consequently, (B.11) yields
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ M1 ŒΩŒòÃÇ ‚àí Œò‚àû ,

(B.12)

where M1 := 2¬µ2 (1 ‚àí œÉÃÇnÃÇ+1 )‚àí1 . However,


‚àí1 ‚àí1
‚àí1
‚àí1
WÃÇŒΩ (z) ‚àí W (z) ‚àí Œò(z)[WÃÇŒΩ (z ) ‚àí W (z )] ,
ŒòÃÇ(z) ‚àí Œò(z) = WÃÇŒΩ (z )
so, since WÃÇŒΩ (z ‚àí1 )‚àû is uniformly bounded by (B.10), and Œò‚àû is constant,
ŒòÃÇ ‚àí Œò‚àû ‚â§ M2 W ‚àí WÃÇŒΩ ‚àû ,
which together with (B.12) yields
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ M1 M2 ŒΩW ‚àí WŒΩ ‚àû + M1 M2 ŒΩWŒΩ ‚àí WÃÇŒΩ ‚àû
for suÔ¨Éciently large ŒΩ. Consequently the theorem follows from Theorem 3.4 and
Corollary B.1.
Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A.
Gombani, W. B. Gragg, G. Picci and T. SoÃàderstroÃàm for stimulating discussions and
for providing us with appropriate references. We are also indebted to the anonymous
referees for several useful suggestions.
References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987.
2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489‚Äì502, 1974.
3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975.
4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational
covariance extension problem. SIAM Journal on Control and Optimization, 37:211‚Äì229, 1999.
5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of
all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841‚Äì1857, 1995.
6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast Ô¨Åltering algorithms.
SIAM Journal on Control and Optimization, 32:744‚Äì789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-GuÃàrsoy. On the L‚àû consistency of L2 estimators. Systems & Control
Letters, 12:71‚Äì76, 1989.
8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978.
9. A. DahleÃÅn, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace
identiÔ¨Åcation methods may fail. Systems and Control Letters, 34:303‚Äì312, 1998.
10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced
stochatic realizations. In Proc. 21st IEEE CDC, pages 1105‚Äì1112, 1983.
11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans.
Automatic Control, AC-29:1097‚Äì1100, 1984.
12. J. Durbin. EÔ¨Écient estimation of parameters in moving average models. Biometrika, 46:306‚Äì316,
1959.
13. J. Durbin. The Ô¨Åtting of time-series models. Rev. Inst. Int. Stat., pages 223‚Äì243, 1960.
14. P. Duren. Theory of Hp spaces. Academic Press, 1970.
15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear.
16. G. Freud. Orthogonale Polynome. BirkhaÃàuser Verlag, 1969.
17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic
Control, AC-32:358‚Äì361, 1987.
18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac.,
Speech and Signal Processing, ASSP-35:438‚Äì449, 1987.
19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961.
20. M. Gevers. Towards a joint design of identiÔ¨Åcation and control. In J. Willems and H. Trentelman,
editors, Essays on Control: Perspectives in the Theory and its Applications, 1993.
21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their
l‚àû -error bounds. Int. J. Contr., 39:1115‚Äì1193, 1984.
22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989.
23. W. B. Gragg. Positive deÔ¨Ånite Toeplitz matrices, the Arnoldi process for isometric operators,
and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in
Linear Algebra, pages 16‚Äì32. Moscow U. P., 1982.
24. U. Grenander and G. SzegoÃà. Toeplitz forms and their applications. Univ. California Press, 1958.
25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons,
1988.
26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples.
Biometrika, pages 297‚Äì307, 1989.
27. W. Jones and E. SaÔ¨Ä. SzegoÃà polynomials and frequency analysis. In Approximation Theory,
pages 341‚Äì352. Dekker Inc., 1992.
28. S. Y. Kung. A new identiÔ¨Åcation method and model reduction algorithm via singular value
decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705‚Äì714, 1978.
29. W. E. Larimore. System identiÔ¨Åcation, reduced ordered Ô¨Åltering and modeling via canonical
variate analysis. In Proc. of the American Control Conference, 1983.
30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes.
SIAM J. Control and Optimization, 23:809‚Äì857, 1985.
31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic
systems. J. of Math. Systems, Estimation and Control, 1:241‚Äì333, 1991.
32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension,
and identiÔ¨Åcation of stationary time series. Automatica, 32(5):709‚Äì733, 1996.
33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating
transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412‚Äì440, 1992.
34. L. Ljung and Z. Yuan. Asymptotic properties of black box identiÔ¨Åcation of transfer functions.
IEEE Trans. Automatic Control, AC-26:514‚Äì530, 1985.
35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis,
Royal Instiute of Technology, 1998.
36. D. Q. Mayne and F. Firoozan. Linear identiÔ¨Åcation of ARMA processes. Automatica, 18:461‚Äì466,
1982.

32

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

37. H. Mhaskar and E. SaÔ¨Ä. The distribution of zeros of asymptotically extremal polynomials. J.
Approx. Theory, 3:279‚Äì300, 1991.
38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66‚Äì73, 1978.
39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward,
editors, Approximation Theory VI, 1989.
40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991.
41. P. Van Overschee. Subspace IdentiÔ¨Åcation, Theory - Implementation - Application. PhD thesis,
Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De
Moor.
42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identiÔ¨Åcation problem.
In Proc. 30th Conference on Decision and Control, Brighton, 1991.
43. P. Van Overschee and B. De Moor. Subspace IdentiÔ¨Åcation for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996.
44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994.
45. I. Schur. UÃàber Potenzreihen, die im Innern des Einheitskreises beschraÃànkt sind. J. fuÃàr die Reine
und Angewandte Mathematik, 147:205‚Äì232, 1917.
46. J. Sorelius, T. SoÃàderstroÃàm, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identiÔ¨Åcation. In Proc. SYSID ‚Äô97, 1997.
47. G. SzegoÃà. BeitraÃàge zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift,
6:167‚Äì202, 1920.
48. G. SzegoÃà. UÃàber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232‚Äì244, 1921.
49. G. SzegoÃà. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications,
1939 (4th edition 1975).
50. B. Wahlberg. On the IdentiÔ¨Åcation and Approximation of Linear Systems. PhD thesis, LinkoÃàping
University, 1987. LinkoÃàping Studies in Science and technology. Dissertations No. 163.
51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive
approximations. Journal of Time Series Analysis, 10:283‚Äì299, 1989.
52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like
identiÔ¨Åcation techniques. IEEE Trans. Automatic Control, 37:900‚Äì912, 1992.
53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation.
Systems and Control Letters, 14:307‚Äì317, 1990.
54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423‚Äì
434, 1953.
55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938.
56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

CANONICAL CORRELATION ANALYSIS, APPROXIMATE COVARIANCE EXTENSION, AND IDENTIFICATION OF STATIONARY TIME SERIES*
ANDERS LINDQUIST AND GIORGIO PICCI

Abstract. In this paper we analyze a class of state-space identification algorithms for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called "subspace methods" can be described as covariance estimation followed by stochastic realization. The methods offer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identification into the solution of a Riccati equation but introduce at the same time some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identification procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not explicitly reported. In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We review the basic ideas of stochastic realization theory in the context of identification, discuss the concept of stochastic balancing and of stochastic model reduction by principal subsystem truncation. The model reduction method of Desai and Pal, based on truncated balanced stochastic realizations, is partially justified, showing that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing conditions under which the "subspace identification" methods produce bona fide stochastic systems.

1. Introduction Recently there has been a renewed interest in state-space identification algorithms for time series based on a two steps procedure which in principle can be described as estimation of a rational covariance model from observed data followed by stochastic realization. The method offers the major advantage of converting the nonlinear parameter estimation phase which is necessary in traditional ARMA models identification into a partial realization problem, involving a Hankel matrix of estimated
 This research was supported in part by grants from TFR, the G® oran Gustafsson Foundation, the SCIENCE project "System Identification" and LADSEB-CNR.  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden  Dipartimento di Elettronica e Informatica, Universita' di Padova, 35131 Padova, Italy 1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which efficient numerical solution techniques are available. In this framework we can naturally accommodate multivariate processes and there are indications that the algorithms may work also with data containing purely deterministic components (van Overschee and De Moor, 1993). A drawback, however, to be emphasized in this paper, is that, unlike, say, least-squares identification of ARMA models, these methods do not work for arbitrary data. This type of procedure was apparently first advocated by Faurre (1969); see also Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work, based on canonical correlation analysis (Akaike, 1975) (or some other singular-value decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki (1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor, 1993). Numerical experience shows that the computation time needed to get the final model parameters estimates compares very favorably with traditional iterative prediction error methods for ARMA models. On the other hand there is a price to be paid for this simplification. These methods introduce some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic realization arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. Note that positivity is the natural condition insuring solvability of the Riccati equation required to compute state-space models of the signal from the covariance estimates. Central in the procedures described above is the following classical problem of identification of a covariance sequence. Let {0 , 1 , . . . ,  } (1.1)

be a finite set of sample m ◊ m covariance matrices estimated in some unspecified way from a certain m-dimensional sequence of observations {y0 , y1 , y2 , . . . yT }, Ø = k CAk-1 C and such that the infinite sequence {0 , 1 , 2 , . . . }, (1.4) (1.2)

Ø ) such that and consider the problem of finding a minimal1 triplet of matrices (A, C, C k = 1, 2, . . . ,  (1.3)

Ø for k =  + 1,  + 2, . . . , is a bona fide obtained from (1.1) by setting k := CAk-1 C covariance sequence. In the literature the last condition is generally ignored. The remaining problem of Ø ) satisfying (1.3) is called the minimal partial realfinding a minimal triplet (A, C, C Ø ) is usually computed by minimal factorization ization problem. The triplet (A, C, C
1

Ø ) is minimal if (A, C ) is completely observable and (A, C Ø ) is completely reachable. Here (A, C, C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

3

of a block Hankel matrix corresponding to the data (1.1) as follows:      Ø C 1 2 C 3 ∑ ∑ ∑ j Ø 2 3  4 ∑ ∑ ∑ j +1   CA   CA      , H= = . . . . . .  .      . .. . . . . . . . . . . . Ø (A )j -1 i i+1 i+2 ∑ ∑ ∑ i+j -1 CAi-1 C

(1.5)

where i + j - 1 =  and the Hankel matrix H is chosen as close to square as possible by taking |i - j |  1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j ) such that i + j - 1 =  , and hence the minimal factorization must be made for a choice of (i, j ) in which the Hankel matrix (1.5) has maximal rank. The infinite sequence Ø for k =  +1,  +2, . . . {0 , 1 , 2 , . . . } obtained in this way by setting k := CAk-1 C is called a minimal rational extension of the finite sequence (1.1) and is in general not a covariance sequence. The dimension r of a minimal rational extension is called the (algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to the McMillan degree of the m ◊ m rational matrix Ø + 1 0 , (1.6) Z (z ) = C (zI - A)-1 C 2 and the elements of the infinite sequence (1.4) are the coefficients of the Laurent expansion 1 Z (z ) = 0 + 1 z -1 + 2 z -2 + . . . 2 (1.7)

about z = . The underlying identification problem is however a great deal more complicated than the classical partial realization problem. In fact, the requirement that (1.4) be a bona fide covariance sequence amounts to (1.4) being a positive sequence in the sense that, for every t  Z+ , the block Toeplitz matrices Tt ,   2 ∑ ∑ ∑ t 0 1 1 0 1 ∑ ∑ ∑ t-1  , Tt =  (1.8) . . . . ...  . . . . . .  . . t t-1 t-2 ∑ ∑ ∑ 0 formed from the infinite sequence (1.4), be positive definite or, equivalently, that the matrix function (z ) := Z (z ) + Z (1/z ) be positive semidefinite on the unit circle, i.e. (ei )  0   [0, 2 ). (1.10) (1.9)

This property is equivalent to  being a spectral density matrix. In fact, it will be the spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial covariance sequence unless T > 0, but this is not enough. From the point of view of identification there seem to be two possible routes to Ø ) from the finite covariance sequence (1.1). One that determine a model (A, C, C has been proposed in the literature is do minimal factorization (1.5) of a finite block Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown in this paper, there is no a priori guarantee that this method will yield a positive extension. This fact has nothing to do with sample variability (random fluctuations) of the covariance estimates (1.1), and to emphasize this point we initially assume that all strings of data (1.2) are infinitely long. A theoretically sounder identification method, which will not be considered in this paper, could instead be to do positive extension first and then to use a stochastic model reduction procedure on the triplet Ø ) of the positive extended sequence. (A, C, C The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this difficulty into consideration have been pointed out by the authors of this paper at many scientific meetings in the last ten years. This has had no apparent effect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identification procedure of Aoki (1990) and demonstrate that there is a hidden, and not easily tested, assumption without which the procedure will not be guaranteed to succeed. The punch line is that none of the subspace identification methods under consideration can be expected to always work for generic data but that some not entirely natural conditions on the data are needed. The analysis of the basic theoretical issues behind subspace identification is carried out in the geometric framework of stochastic realization theory; see, e.g., Lindquist and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this theory and adapt them to the problem of identification. To this end, we first discuss an idealized situation in which the time series (1.2) is infinitely long i.e. T = , and the available covariance data are given by the ergodic limit 1 T  T + 1 lim
T

yt+k yt+j = k-j
t=0

(1.11)

for all k and j . Then the sample estimates in the sequence (1.1) are bona fide covariance matrices and the Toeplitz matrix T formed from the data will be positive definite and symmetric. We introduce a Hilbert space of observed (infinite) strings of data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we establish a correspondence which turns operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. Canonical correlations and balanced stochastic realizations are then analyzed in this setting in Section 4, and the basic concepts and principles used in the subspace identification methods, as well as in the model reduction procedures of Desai and Pal, are translated into the more natural context of geometric stochastic realization theory. Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact infinite covariance sequence (1.4). Consequently, the difficult question of positivity is not an issue here. Nor is it the finite sample size per se which is the problem, but the fact that only a finite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is finite. Therefore we separate these issues by discussing stochastic realization theory from finite covariance data in Section 5 and subspace identification in Section 6. In this framework we show that the method of van Overschee and De Moor (1993) is valid under some rather stringent assumptions. We stress that we are only concerned with identification procedures for state space modeling of time series. "Subspace identification" methods for deterministic systems with measurable inputs or for spectral factors do not involve positivity, but stability may still be a problem. However, the algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part, so the problem of positivity arises here too. Another idea behind the subspace identification methods considered in this paper is to disregard modes corresponding to "small" canonical correlation coefficients. This is called balanced truncation and is in fact a stochastic model reduction procedure. In all such procedures there must be a guarantee that the reduced-degree matrix function (1.6) is positive real, and therefore the preservation of positivity in such reductions is a main concern of this paper. Section 7 is devoted to such issues. The model reduction procedure of Desai and Pal (1982) was never theoretically justified in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 . Here we shall demonstrate that this reduction procedure produces a positive real, but not in general balanced, reduced model structure. In fact, the singular values of the truncated system are usually not equal to the r first singular values of the original system. It is an interesting fact that the procedure of Desai and Pal does produce balanced truncations for continuous-time stochastic systems. A partial result in this direction was given by Harshavardana, Jonckheere and Silverman (1984), who showed that the truncated function is positive real and conjectured that it is balanced. We shall demonstrate that it is indeed balanced, a result that is actually already contained in the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time depends on the fact that the spectral factors of the truncated approximate spectrum behave differently than in continuous time. While in continuous time the realizations of the reduced spectral factors are proper subsystems, obtained by partitioning the matrices of the realizations of the factors of , this is not the case in discrete time, contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced truncation procedure is available in discrete time, but the systems matrices are no longer submatrices of those of the original system, and therefore it is not equivalent to the truncation procedure used in subspace identification. Several of the results of this paper have previously been announced in Lindquist and Picci (1994a)3 and in Lindquist and Picci (1994b).

In Desai et al. (1985) a different model reduction procedure, which is not relevant to subspace identification, is considered, namely "deterministic" model reduction of the minimum phase spectral factors. 3 We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some erroneous statements, was accidentally published in place of the paper finally submitted for publication. The correct version can be obtained from the authors.

2

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances The solution to the minimal partial realization problem , i.e., the problem to find Ø ) satisfying (1.1) is in general not unique. This lack of uniquethe triplet (A, C, C ness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1) equals that of {0 , 1 , . . . ,  -1 } so that we can use a Hankel matrix (1.5) based allowing us to define the shifted Hankel matrix  3 4 2  3 4 5  (H ) =  . . .  . . . . . . i+1 i+2 i+3 (2.1)

on this data, i.e., with i + j =  ,  ∑ ∑ ∑ j +1 ∑ ∑ ∑ j +2   . ... . .  ∑ ∑ ∑ 

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) proØ ) which is unique up to a similarity transformation. duces a minimal solution (A, C, C As first pointed out by Zeiger and McEwen (1974), the minimal factorization on which the Ho-Kalman procedure is based may be performed by singular-value decomØ ) uniquely; see also Kung (1978). In fact, the Hankel position, thereby fixing (A, C, C matrix H may be factored as H = U V U U = I = V V, (2.3) where  is the square n ◊ n diagonal matrix of the nonzero singular values taken in Ø := V 1/2 this leads to a factorization decreasing order. Setting  := U 1/2 and  Ø H =  Ø Ø == (2.4)

Ø ) is obtained by solving of the type (1.5). Then a minimal realization (A, C, C Ø =  (H ), A Ø = 1 (H ) and C Ø  = 1 (H ), C

where  (H ) is the shifted Hankel matrix (2.2) and 1 (H ) is the first block row of H . Ø ) must be given by It follows that the triplet (A, C, C A = -1/2 U  (H )V -1/2 , C = 1 (H )V -1/2 , Ø = 1 (H )U -1/2 , C (2.5a) (2.5b) (2.5c)

a form to which we refer as finite-interval balanced, since it is balanced in the sense Ø are both equal to , and that Ø that   and      Ø C C Ø  CA   CA  Ø   .  = (2.6) = . .    . . .  . Ø (A )j -1 CAi-1 C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identification of time series. The problem with such a strategy is that this algorithm is a deterministic realization procedure and hence does not a priori insure that (1.6) is positive real, or even stable for that matter, even if the Toeplitz matrix T is positive definite. In fact, it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space of covariance data (1.1) for which A is not stable, and a fortiori the same holds for positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in Aoki (1990) is based on the following hidden assumption which is not entirely natural. Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H . Therefore, not only must we know that there exists an underlying finite-dimensional system, but we must also have some upper bound for its dimension. A conservative ]. upper bound which will always suffice is [  2 Is this assumption natural? If the covariance data are really generated exactly from a "true" stochastic system and there is a reliable estimate of its order which is no more than half of the length of the covariance sequence, then the assumption will hold. However, and this is an important point of this paper, one cannot expect Assumption 2.1 to hold for an arbitrary covariance sequence (1.1). To clarify this point, let us agree to call {0 , 1 , 2 , . . . } a minimal rational extension of {0 , 1 , . . . ,  } if the rational function (1.7) has minimal degree. By definition this is the algebraic degree of {0 , 1 , . . . ,  }. A rational extension is called positive if, for every µ >  , the block Toeplitz matrices Tµ formed from the corresponding infinite sequence (1.4) are positive definite. An extension with this property is called a positive rational extension. It is well known that the extension {0 , 1 , 2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function Z (z ) is analytic in the closed unit disc and the matrix function (z ) = Z (z ) + Z (1/z ) (2.7)

is nonnegative definite on the unit circle, making  a spectral density matrix. A minimal positive rational extension of the finite sequence (1.1) is one for which the Ø ) in (1.6) is as small as possible. dimension of the triplet (A, C, C Definition 2.2. The positive degree p of the finite covariance sequence {0 , 1 , . . . ,  } is the dimension of any minimal positive extension. A well-known example of a positive extension is the maximum entropy extension (Whittle, 1963) corresponding to the spectral density (z ) := W (z )W (1/z ) , where the spectral factor W (z ) is (modulo a multiplicative constant matrix) the inverse of the Levinson-Szeg® o matrix polynomial of order  corresponding to the finite covariance sequence (1.1). Since the rational function W (z ) generically has the McMillan degree equal to m , it follows from spectral factorization theory (Anderson, 1958) that Z (z ) has also degree m . Consequently, the positive degree p is bounded from below by the algebraic degree r and from above by m . As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use algebraic rather than positive extensions, usually computed by minimal factorization a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1 may also be formulated in the following way. Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree. This assumption prescribes a property of the covariance sequence (1.1) which is not generic. We can illustrate this point by considering the rational extension problem for a finite scalar covariance sequence (1.1). The positive degree p lies between the algebraic degree r and  . Note that neither the case p =  nor the case p <  are "rare events", because there are open sets of covariance sequences (1.1) of both categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each µ  µ   there is an open set of covariance data in R for which p = µ. such that  2 If the upper limit p =  is attained there are infinitely many nonequivalent minimal Ø ) providing a positive extension, one of which is the maximum entropy triplets (A, C, C extension. In fact, it can be shown that these  -dimensional extensions form an Euclidean space (Byrnes and Lindquist, 1989). This shows that the finite data (1.1) never contains enough information to establish a "true" underlying system. A similar statement can be made in the case when p <  . Example 2.3. Consider the case m = 1 and  = 2, i.e., consider a scalar partial covariance sequence {0 , 1 , 2 }. If 1 = 2 = 0, we have r = p = 0. Otherwise, we always have r = 1, whereas the positive degree can be either one or two. In fact, 2 setting 0 := 1 /0 and 1 := (2 1 + 2 )/(1 - 1 ), it can be shown (Georgiou, 1987; also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1 if and only if |0 | |1 | < 1 + |0 | and p = 2 otherwise. In fact, it is not hard to construct examples for which the gap between algebraic and positive rank is arbitrarily large, as the following theorem shows. Theorem 2.4. Let n  Z+ be fixed. Then for an arbitrarily large  there is a stable rational function Z (z ) of degree n, such that the Toeplitz matrix T formed as in ( 1.8) from the coefficients of the Laurent expansion ( 1.7), is positive definite while T +1 is indefinite. Consequently, you cannot test the positivity of a rational extension of (1.1) by checking a finite Toeplitz matrix, however large is its dimension. The proof of Theorem 2.4 is given in Appendix A. Let us now return to the identification procedure of Aoki (1990). In practice the rank of H will always be full, and to compute a partial realization of reasonable dimension the basic idea is to partition  as = 1 0 , 0 2 (2.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in 2 are smaller than those in 1 , perhaps close to zero, and then take 2 = 0 so that H is approximated by H1 = U 1 0 V = U1 1 V1 . 0 0 (2.9)

The matrix H1 is a best approximation (given the rank) of H in (the induced) 2 ≠ norm, but it is in general not Hankel and hence can not be used to determine a reduced order system. Of course, one may instead use Hankel-norm approximation (Adamjan, Arov and Krein, 1971), which produces another best approximation of H in 2 -norm that is Hankel and has the same rank as H1 . However, if 2 is "very small" compared to 1 , then H1 is close to H and hence approximately Hankel. For this reason, Aoki's procedure (Aoki, 1990) is based on the original data H and  (H ). Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I , the Ør ) given by same type of calculation as above yields the reduced triplet (Ar , Cr , C Ar = 1
- 1/ 2

U1  (H )V1 1
- 1/2

- 1/ 2

,

(2.10a) (2.10b) (2.10c)

Cr = 1 (H )V1 1 , 1 /2 Ør = 1 (H )U1 - C . 1

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal subsystem truncation in the sense that, if H is produced by a finite-dimensional system Ø ) having finite-interval balanced form (2.5), we have with (A, C, C Ar = A11 , where A= A11 A12 A21 A22 C = C1 C2 Ø1 C Ø2 . Ø= C C (2.12) Cr = C1 , Ør = C Ø1 , C (2.11)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for  (H ), 1 (H ) and 1 (H ) and inserting in (2.10). However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that Ø1 + 1 0 (2.13) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real, provided of course that Z , defined by (1.6), is positive real. The question of stability was answered in the affirmative in Pernebo and Silverman (1982) and is addressed in Aoki (1990). The crucial question of positivity, however, is not discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be proven for a somewhat modified procedure described below. In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall consider a normalized Hankel matrix
1 -T ^ = L- H + HL- ,

(2.14)

where L- and L+ are lower triangular Cholesky factors of the Toeplitz matrices T- and T+ of (1.1) and the corresponding sequence of transposed covariances respectively; see Section 4 below. This is also the Hankel matrix considered in van Overschee and ^ instead of H , the De Moor (1993). Taking the singular value decomposition of H

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coefficients, i.e., the cosines of the angles between the past and the future of the process y . The systems matrices can be determined in a manner analogous to (2.5), but now
-1 -1 Ø ^ = Ø T-  =  T+

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as ^ =U ^ ^U ^ so that H = above. To see this, consider the singular value decomposition H Ø ^ ^ ^ (L+ U )(L- V ) . Since H =  and this factorization is unique modulo coordinate Ø = L- V ^ ^ 1/2 and  ^ ^ 1/2 . Then transformation in state space, we may take  = L+ U ^ ^ ^ ^ (2.15) follows from U U = I = V V . As we shall see next, (2.15) corresponds to a more natural type of balancing corresponding to a Hankel operator describing the interface between the past and the future of the time series y . 3. Stochastic realization theory in the Hilbert space of a sample function In this section we introduce a mathematical framework which is suitable for the identification problem described above. We define a Hilbert space of observed (infinite) strings of data {yt }. This framework turns out to be isomorphic to that of geometric stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we also establish a correspondence which converts operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. In identification we have access only to a finite string of data {y0 , y1 , y2 , . . . , yT }. (3.1)

Here T may be quite large but, of course, always finite. To begin with, we shall, however, consider the idealized situation that we are given a doubly infinite sequence of m-dimensional data {. . . , y-3 , y-2 , y-1 , y0 , y1 , y2 , y3 . . . } (3.2)

together with a corresponding covariance sequence {k }k0 , each matrix k of the sequence being computed from the data (3.2) by an ergodic limit of the type (1.11). In Section 5 we then modify the theory to handle the situation of finite data (3.1). For each k  Z define the m ◊  matrix y (t) := [yt , yt+1 , yt+2 , . . . ] (3.3)

and consider the sequence y := {y (t)}tZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of all finite linear combinations ak y (tk ); ak  Rm , tk  Z

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product defined by linear extension of the bilinear form 1 a y (k ), b y (j ) := lim T  T + 1
t0 +T

a yt+k yt+j b = a k-j b,
t=t0

(3.4)

which clearly does not depend on t0 . This inner product is nondegenerate if the Toeplitz matrix Tk , constructed from the covariance data {0 , 1 , . . . , k }, is a positive definite symmetric matrix for all k . Here we shall assume that the sequence {Tk }k0 is actually coercive, i.e., Tk > cI for some c > 0 and all k  0. (See Assumption 3.2 below for an alternative characterization.) We also define a shift operator U on the family of semi-infinite matrices (3.3), by setting Ua y (t) = a y (t + 1) t  Z, a  Rm ,

defining a linear map which is isometric with respect to the inner product (3.4) and extendable by linearity to all of Y . In particular the sequence of matrices {y (k )} corresponding to the time series y is propagated in time by the action of the operator U, i.e., yi (t) = Ut yi (0), i = 1, 2, . . . , m, t  Z, (3.5)

where yi denotes the i:th row component of y . Then, closing the vector space Y in the inner product (3.4), we obtain a Hilbert space H (y ) := cl Y . The shift operator U is extended by continuity to all of H (y ) and is a unitary operator there. As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results in the geometric theory of stochastic realization can be carried over to the present framework by merely identifying the time series y with a stationary stochastic process y. In particular, the subspaces H - and H + of H (y ) generated by the elements (3.3) for t < 0 and t  0 respectively can be regarded as the past and future subspaces of the stationary process y. For reasons of uniformity of notation the inner product (3.4) will also be denoted ,  = E { }, (3.6)

as the frameworks are completely equivalent. Here we allow E {∑} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity condition introduced above insures that tZ Ut H - = 0 and tZ Ut H + = 0, i.e., y is a purely nondeterministic sequence. As we have pointed out above, the subspace identification methods of Aoki (1990) and van Overschee and De Moor (1993) are based on the assumption that the available data is generated by an underlying stochastic system of finite dimension. More specifically, using the notations introduced above, we assume that the data are generated by a linear system of the type x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (3.7)

12

ANDERS LINDQUIST AND GIORGIO PICCI

defined for all t  Z, where w is some vector-valued normalized white noise time series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability matrix. Throughout this paper we shall assume (without restriction) that (A, B, C ) B is a minimal triplet and that the matrix has linearly independent columns. D The system is assumed to be in statistical steady state so that the n-dimensional state x and the m-dimensional output y are uniquely defined by (3.7) as linear causal functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and y (s) will depend only on the difference t - s. We shall think of the system (3.7) as a representation of the output time series y . The state and input variables x and w are introduced in order to display the special structure of the dynamic model of y and are by no means unique. Such a representation is called a state-space realization of y . Remark 3.1. Despite the fact that the model (3.7) is defined in terms of sample sequences, all equalities must be understood in the sense of Hilbert space metric, just as in the case of models based on random variables. The number of state variables n is called the dimension of the realization. A realization is minimal if there is no other realization of y of smaller dimension. In this case the covariance matrix of the state vector, P = E {x(t)x(t) } is positive definite. Moreover as the matrix (3.8)

B is taken with linearly independent D columns, the number of (scalar) white noise inputs p is also as small as possible. Clearly, the covariance sequence {0 , 1 , 2 , . . . } of the output {y (t)} of a minimal model (3.7) is a rational sequence of degree n, i.e., represented as Ø = AP C + BD Ø k = 0, 1, 2, . . . where C k = CAk-1 C 0 = CP C + DD . (3.9)

In the following we shall need to assume that the corresponding spectral density (z ) satisfies the following condition. Assumption 3.2. The spectral density  of the output process of the underlying system (3.7) is coercive in the sense that (ei ) > 0 for all   [0, 2 ]. (3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent sequences. Recall that a positive real function Z such that (z ) := Z (z ) + Z (z -1 ) satisfies (3.10) is called strictly positive real. Let H (w) be the Hilbert space generated by w, i.e. the closure of the linear space spanned by the family {wi (t), i = 1 . . . p, t  Z} with respect to the metric induced by the inner product ,  = E {  } where E {∑} is defined by (3.6). Let H + and H - be the subspaces of H (w) generated by the components of future {y (0), y (1), y (2) . . . } and past outputs {y (-1), y (-2), y (-3) . . . }, respectively.
4

This means that E {w(t)w(s) } = Its where ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace X := {a x(0) | a  Rn } (3.11)

is invariant under coordinate changes of the type (A, B, C )  (T AT -1 , T B, CT -1 ) and is a coordinate-free representation of the realization (3.7). Such an object is called a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next define the stationary Hankel operator of y , H : H +  H - as H := E H |H +
- -

(3.12)

where E H  is the orthogonal projection of  onto H - . The splitting subspace property of X is equivalent to the commutativity of the diagram H+ O i.e. to the factorization H = CO ,
+ -

- H - C X

H

(3.13)

where the operators O := E H |X and C := E H |X are the observability respectively constructibility operators relative to the splitting subspace X . It can be shown that the splitting subspace X is minimal if and only if O and C are both injective. (See, e.g., Lindquist and Picci (1991).) The system (3.7) is a forward or causal realization of y in the sense that the subspace + H (w), generated by the future of w, is orthogonal to X and H - , i.e. to the present state and past output. Corresponding to (3.7) there is another realization Øw Ø(t) + B Ø (t - 1) x Ø(t - 1) = A x Øx Øw y (t - 1) = C Ø(t) + D Ø (t - 1) (3.14)

Ø ), generated by which is backward or anticausal in the sense that the subspace H - (w + Ø(0) is a basis in X , i.e. the past of w Ø , is orthogonal to X and H . Like x(0), x X := {a x Ø(0) | a  Rn }. Ø = P -1 P x Ø(0) = P -1 x(0). (3.15)

In fact, x Ø(0) is the dual basis of x(0) in the sense that E {x(0)Ø x(0) } = I . Hence (3.16)

The particular notations used in (3.7) and (3.14) reflect the special meaning of the Ø ). Computing the covariance matrix of the output using the dual parameters (A, C, C Ø ) is precisely a realizations (3.7) and (3.14), it is in fact readily seen that (A, C, C triplet realizing the positive real part (1.6) of the spectral density matrix (z ) of the time series y . There are infinitely many minimal factorizations (3.13), one for each Markovian splitting subspace, but the basis in each state space X can be chosen so Ø ) are the same for each minimal X . This is called a uniform that the triplets (A, C, C choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward predictor spaces X- = E H H +
-

X+ = E H H - ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12) and of its adjoint, respectively. Ø ), the splitting Fixing a uniform choice of bases, and thus the triplets (A, C, C subspace X- has the forward stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) with state covariance P- , and X+ has the backward realization Ø+ w x Ø+ (t - 1) = A x Ø+ (t) + B Ø+ (t - 1) Ø +w Øx Ø+ (t - 1) y (t - 1) = C Ø+ (t) + D (3.19) (3.18)

Ø+ . with state covariance P These two stochastic realizations will play an important role in what follows. In fact, an important interpretation of these realizations is that
-1 [y (t) - Cx- (t)] x- (t + 1) = Ax- (t) + B- D-

is the unique steady-state Kalman filter of any minimal realization (3.7) of y in the fixed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8) Ø+ )-1 , then corresponding to the forward counterpart of (3.19), i.e., P+ = (P P -  P  P+ for the state covariance of any minimal realization (3.7). In the same way
-1 Ø+ Ø+ D Ø+ (t) + B [y (t - 1) - C x Ø+ (t)] x Ø+ (t - 1) = A x

(3.20)

is the backward steady-state Kalman filter of all minimal backward realizations (3.14), and Ø+  P ØP Ø- P Ø- is the backward counfor an arbitrary backward minimal realization (3.14), where P terpart of P- . 4. Canonical correlations and balanced stochastic realization In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a finite-dimensional realization of the type (3.7). Equivalently, we study certain factorizations of the infinite Hankel matrix of the corresponding infinite covariance sequence {0 , 1 , 2 , . . . }. Some portions of this section can be found in an equivalent but somewhat different setting in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and set notations. This will be done in the geometric framework of Section 3, thereby providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall introduce orthonormal bases in H - and H + . To this end it will be useful to represent past and future outputs as infinite vectors in the form,     y (-1) y (0) y (-2) y (1)    y = (4.1) y- =  + y (-3) y (2) . . . . . . Let L- and L+ be the lower triangular Cholesky factors of the infinite block Toeplitz matrices T+ := E {y+ y+ } = L+ L+ T- := E {y- y- } = L- L- and let
1  := L- - y- 1  Ø := L- + y+

(4.2)

be the corresponding orthonormal implies that  1 2 H := E {y+ y- } =  3 . . .

bases in H - and H + respectively. Now, (3.9) 2 3 3 4 4 5 . . . . . .    Ø  C C ... Ø  . . .  CA   CA = 2  Ø 2 , . . . CA  C (A )  . . ... . . . .

(4.3)

and therefore we have the following representation result, which can be found in Desai et al. (1985). Proposition 4.1. Let y be realized by a finite dimensional model of the form (3.7). Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator H is
1 -T -1 Ø -T ^  = L- H + E {y+ y- }L- = L+  L- ,

(4.4)

where

 C  CA   = CA2  . . . 

and

Ø  C Ø  CA  Ø   = C Ø (A )2  . . . .



(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization (4.3) for H , irrespective of which X (i.e. which minimal realization of y ) is chosen. Recall that the adjoint O of the observability operator O is defined as the unique linear operator H +  X such that O,  = , O  for all   X and   H + . Orthogonality implies that E H ,  = ,  = , E X  , and therefore O = E X |H + . In the same way, we see that C  = E X |H - . The finiterank linear operators O O and C  C are defined on X and are the coordinate-free representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O O is full rank and constructible if and only if C  C is full rank. The following representations show that these gramians are related
+

16

ANDERS LINDQUIST AND GIORGIO PICCI

Ø+ , the state covariances of the forward and backward steady-state Kalman to P- and P filters (Picci and Pinzoni, 1994). Proposition 4.2. Let x(0) and x Ø(0) be the conjugate basis vectors in a minimal splitting subspace X as defined above. Then, in a uniform choice of bases, Ø+ x(0) Ø(0) = a P O O a x and C  C a x(0) = a P- x Ø(0), (4.7) Ø+ , respectively, independently i.e., C  C and O O have matrix representations P- and P of X . Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal, E H a x(0) = a x- (0), C  C a x(0) = E X a x- (0) = E X a P- x Ø- (0). But, since the bases x Ø(0) and x Ø- (0) are chosen uniformly, EX a x Ø- (0) = a x Ø(0) a  Rn , and consequently (4.7) follows. The proof of (4.6) is analogous. The factorization (4.4) can also be derived from (3.13) and the following useful matrix representations of the observability and constructibility operators. Proposition 4.3. Let x(0) and x Ø(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
T Ø(0) = a  L- Ø Oax +  1 O b  Ø = b L- + x(0)
-

(4.6)

and therefore

(4.8)

and
T Ø L- C a x(0) = a  -  1Ø x(0), C  b  = b L- - Ø

(4.9)

Ø are given by (4.5). where  and  Proof. Since, in view of (3.7), y+ = x(0) + terms which are orthogonal to X,
1 and  Ø = L- + y+ , we have 1 E { Øx(0) } = L- + P.

(4.10)

Consequently, for any a  Rn , the usual projection formula5 yields O a x(0) = E H a x(0) = a E {x(0)Ø  } Ø and O b  Ø = EX b  Ø = b E { Øx(0) }P -1 x(0), from which (4.8) follows. A symmetric argument yields (4.9).
If   H (w) and the subspace Z  H (w) is spanned by the components of the full-rank random vector z , then E Z  = E {z }(E {zz })-1 z .
5
+

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should note that the matrix representations of O and C  are the transposes of those of O Ø = I . Moreover, it follows and C if and only if x(0) is an orthogonal basis, i.e., P = P from (4.8) that -1 Ø(0) = a  T+ x(0), O Oa x
-1 showing that  T+  is a matrix representation of O O, in harmony with the analysis at the end of Section 2. In the same way, (4.9) yields -1 Ø Ø T- Ø x(0), C  C a x(0) = a  -1 Ø Ø T-  is a matrix representation of C  C . Together with Proposition 4.2 and hence  Ø+ : this yields the following explicit formulas for P- and P -1 Ø+ =P  T+ -1 Ø Ø T-   = P- .

(4.11)

Now, let {1 , 2 , 3 , . . . } be the singular values of the Hankel operator H. Since rank H = n, i = 0 for i > n. The nonzero singular values 1  1  2  3 . . .  n > 0 (4.12)

are the cosines of the angles between the subspaces H- and H+ ; they are known as the canonical correlation coefficients of y (Hotelling, 1936, Anderson, 1958). Obviously 1 < 1 if and and only if H-  H+ = 0. The squares of the canonical correlation coefficients are the eigenvalues of H H, i.e.,
2 i , H H i = i

which, in view of (3.13) may be written
2 (O i ), O OC  C (O i ) = i

and therefore, as was also demonstrated in Picci and Pinzoni (1994),
2 2 2 , 2 , . . . , n }, {O OC  C} = {1

(4.13)

2 2 2 , 2 , . . . , n are the eigenvalues of O OC  C . But, in view of Proposition 4.2, i.e., 1 this is precisely the coordinate-free version of the invariance condition 2 2 2 Ø+ } , 2 , . . . , n } = {P- P {1

(4.14)

of Desai and Pal (1984). This suggests that an appropriate uniform choice of bases would be the one that Ø+ equal and equal to the diagonal matrix of nonzero canonical corremakes P- and P lation coefficients. ^  is the In fact, in view of Proposition 4.1, the infinite normalized Hankel matrix H matrix representation of the operator H in the orthonormal bases (4.2). Therefore ^  has the singular-value decomposition H ^  = U   V = U  V , H   = diag{1 , 2 , 3 , . . . , n }, (4.15) where  is the diagonal n ◊ n matrix consisting of the canonical correlation coefficients (4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and  is the infinite matrix  =  0 . 0 0

Moreover U and V are infinite orthogonal matrices, and U and V are  ◊ n submatrices of U and V with the the property that U U = I = V V. (4.17) We now rotate the the orthonormal bases (4.2) in H + and H - to obtain u := U  Ø and v := V  respectively. Note that E {uv } =  . What makes these orthonormal bases useful is that they are adapted to the orthogonal decompositions6 H -  H + = [H -  (H + ) ]  H  [H +  (H - ) ], (4.18)

where H := X-  X+ is the so-called frame space (Lindquist and Picci (1985, 1991), in the sense that X- = span{v1 , v2 , . . . , vn } X+ = span{u1 , u2 , . . . , un }. This is true since X- is precisely the subspace of random variables in H - having nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H - . Since therefore {vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H -  (H + ) and H +  (H - ) , respectively, these spaces will play no role in what follows. Now define the n-dimensional vectors  1/2   1/2  1 u1 1 v1   1/2 u    1/2 v  2 2   1 1 z Ø =  2 .  = 1/2 U L- (4.19) z =  2 .  = 1/2 V L- - y- + y+ . .  .   .  n vn
1/2

n un

1/ 2

Ø is a basis in X+ , and they From what we have seen before, z is a basis in X- and z have the properties Øz Ø }. E {zz } =  = E {z (4.20)

In fact, we even have more as seen from the following amplification7 of a theorem by Desai and Pal (1984) (Theorem 1). Theorem 4.4. The basis vectors x- (0) = z x Ø+ (0) = z Ø (4.21)

in X- and X+ respectively belong to the same uniform choice of basis, i.e. to the Ø ), and in this uniform choice same choice of triplets (A, C, C Ø+ . P- =  = P
6 7

(4.22)

The symbols  and  denote vector sum and orthogonal vector sum of subspaces. Ø ). A priori there is no reason why choosing bases in X- and X+ would lead to the same (A, C, C This important property is explicitly mentioned in Theorem 4.4.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coefficients {1 , 2 , 3 , . . . , n } are distinct, this is, modulo multiplication with a signature matrix 8 , the only uniform choice of bases for which ( 4.22) holds. Ø ) is know as stochastically balanced, and, in the case of Such a choice of (A, C, C distinct canonical correlation coefficients, it defines a canonical form with respect to state space isomorphism in (1.6) by fixing the sign in, say, the first element in each row of C . Such canonical forms have also been studied by Ober (1991). Proof. It follows from (4.4) and (4.15) that E {z Øz } = 2 . (4.23) Ø ) so that x Ø, and let the bases in the other splitting Now, choose (A, C, C Ø+ (0) = z subspaces be chosen accordingly so that the choice of bases is uniform. We want Ø+ (0) and that to show that x- (0) = z . To this end, first note that x+ (0) = -1 x x- (0) = E X- x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula and the fact that z is a basis in X- , Øz }-1 z, x- (0) = -1 E {z which, in view of (4.23), yields x- (0) = z as claimed. Hence (4.22) follows from (4.20). Ø ) is another uniform choice of bases which Next, suppose that (QAQ-1 , CQ-1 , CQ is also stochastically balanced. Since then x- (0) = Qz and, as is readily seen from Ø+ = Q-T Q-1 , Ø so that P- = QQ and P the backward system (3.14), x Ø+ (0) = Q-T z (4.22) yields QQ =  and Q-T Q-1 = , from which we have Q2 = 2 Q. Since  has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959) that there is a scalar polynomial (z ) such that Q = (2 ). Hence Q is diagonal and commutes with  so that, by QQ = , we have QQ = I. Consequently, since Q is diagonal, it must be a signature matrix. In view of (4.21) and (3.16), the first of relations (4.9) and (4.8) respectively yield -1 -1 Ø T- y- z Ø =  T+ y+ . (4.24) z= Consequently, in view of (4.20), (2.15) holds also for the case of an infinite Hankel matrix. This can of course also be seen from (4.11). Note that the normalization of the block Hankel matrix H is necessary in order for the singular values to become the canonical correlation coefficients, i.e., the singular values of H. In fact, if we were to use the unnormalized matrix representation (4.3) of H instead, as may seem simpler and more natural, the transpose of (4.3) would not be the matrix representation of H in the same bases, a property which is crucial in the singular value decomposition above. This is because (4.3) corresponds to the bases y- in H - and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the finite-dimensional case studied in ^ , defined in Section 2, is Section 2, and therefore the normalized Hankel matrix H preferable to the unnormalized H . Ø in terms of the Hankel matrix H , can Formulas, such as (2.5), expressing A, C, C be easily derived from basic principles. In fact, standard calculations based on the forward model (3.7) and the backward model (3.14) yield A = E {x(1)x(0) }P -1 C = E {y (0)x(0) }P -1 , Ø -1 = E {y (-1)x(0) } Ø = E {y (-1)Ø C x(0) }P for any dual pair of bases x(0) and x Ø(0). Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases (4.19) can be computed by means of the formulas
1 -T - 1/ 2 , A = -1/2 U L- +  (H )L- V  T - 1/2 C = 1 (H )L- , - V - T - 1 / 2 Ø = 1 (H )L+ U  C ,

(4.25a) (4.25b) (4.25c)

(4.26a) (4.26b) (4.26c)

where H is the unnormalized Hankel matrix (4.3),  (H ) is obtained from H by deleting the first block row, and 1 (H ) is the first block row. Proof. First, in (4.25a) and (4.25b), we take x(0) to be x- (0). By the Kalman filter representation a [x+ (1) - x- (1)]  UH -  H - for all a  Rn ,
-1 Ø+ E {x Ø+ (1)x- (0) }. E {x- (1)x- (0) } = E {x+ (1)x- (0) } = P

Ø ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19), But (A, C, C 1 1 Ø+ , x- (0) = 1/2 V L- Ø+ (1) = 1/2 U L- P- =  = P - y- and x +  (y+ ), where  (y+ ) is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),
1 -T - 1/ 2 , A = -1/2 U L- + E { (y+ )y- }L- V 

which is identical to (4.26a). Likewise, from (4.26b),
T - 1/ 2 , C = E {y (0)y- }L- - V

which yields (4.26b). Finally, taking x Ø(0) to be x Ø+ (0) in (4.25c), a symmetric argument yields (4.26c). Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from finite covariance data In this section we modify the realization theory of Section 4 to the case that only a finite segment {y (0), y (1), y (2), . . . , y ( )}, (5.1) of the time series {y (t)} is available. We still define each y (t) as the semi-infinite string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact partial covariance sequence {0 , 1 , 2 . . . ,  }. (5.2)

The corresponding realization problem, which is purely theoretical and is intended to prepare for the more realistic identification situation with finite strings of observed data (Section 6), is therefore the partial stochastic realization problem mentioned in Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the output of some minimal "true" system (3.7) of dimension n and that  is large enough for n to equal the positive degree of the partial sequence (5.2). Now, suppose that  = 2 - 1, and partition the data into two matrices     y (0) y ( )  y (1)   y ( + 1)  - +   , = y = (5.3) y . .      . . . . y ( - 1) y (2 - 1) representing the past and the future respectively, and define the corresponding (finite- + and y respectively as dimensional) subspaces Y- and Y+ spanned by the rows of y explained in Section 3. Since the data size  will be important in the considerations that will follow, we denote the finite block Hankel matrix H of Section 2, relative to the data (5.3), by H , i.e.,
+ - H = E {y (y ) }.

(5.4)

Let 0 be the smallest integer  such that rank H = n. It is well-known that 0 is Ø ), so n is an the maximum of the observability and constructibility indicies of (A, C, C upper bound for 0 . As pointed out in the beginning of Section 2, we need  > 0 to Ø ). be certain that the factorization of H yields a unique (A, C, C Next we shall consider the class of minimal splitting subspaces for Y- and Y+ , i.e., the subspaces X admitting a canonical factorization Y+  O of the finite-interval Hankel operator H := E Y |Y+ .
-  - Y- C X

H

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces, ^  - = E Y- Y+ X ^  + = E Y+ Y- , and X

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred to Appendix D. Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary time series {y (t)}. Then, if  > 0 , X := U X is a minimal splitting subspace for Y- and Y+ , and ^  - = E Y- X , X ^  + = E Y+ X . X (5.7) (5.6)

^  - has a unique representation9 Conversely, any basis x ^( ) in X x ^( ) = E Y x( ),
-

(5.8)

^  + has a unique representation ^ where x( ) is a basis in X , and any basis x Ø( ) in X ^ x Ø( ) = E Y x Ø( ),
+

with x Ø( ) a basis in X . As X varies over the family of all minimal Markovian splitting subspaces, the corresponding x(0) [Ø x(0)] constitute a uniform choice of bases. ^ - The stochastic realizations corresponding to the finite-interval predictor spaces X ^  + are nonstationary. However, taking advantage of the representations (5.8) and X and (5.9), we shall be able to express these realizations in such a way that they can Ø ) corresponding to one uniform be parameterized by the stationary triplet (A, C, C choice of bases, both for the forward and the backward settings. In fact, if the bases ^ x ^( ) and x Ø( ) are chosen so that x( ) and x Ø( ) in representations (5.8) and (5.9) are Ø ) is used for x( )} = I , then the same choice of (A, C, C dual bases in X , i.e., E {x( )Ø ^  + is called coherent. ^  - and X all X  . Such a choice of bases in X The realizations generated by these coherent bases are precisely the (transient) forward and backward Kalman filters. In fact, the vector x ^( ) is the one-step predictor of x( ) based on Y- and, as shown in Appendix C, it evolves in time as the Kalman filter

X

(5.9)

X

x ^(t + 1) = Ax ^(t) + K (t)[y (t) - C x ^(t)]; where the gain K (t) is given by

x ^(0) = 0,

(5.10)

Ø - AP- (t)C )(0 - CP- (t)C )-1 K (t) = (C and the filter estimate covariance ^(t)^ x(t) } P- (t) = E {x is the solution of the matrix Riccati equation

(5.11)

(5.12)

Ø - AP- (t)C )(0 - CP- (t)C )-1 (C Ø - AP- (t)C ) P- (t + 1) = AP- (t)A + (C P- (0)) = 0. (5.13)
With slight misuse of notations, the orthogonal projection operator applied to a vector will denote the vector of the projections of the components.
9

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the components of ^ Ø( ) x Ø( ) = E Y x ^  + and are generated by the backward Kalman filter form a basis in X Ø (t)[y (t - 1) - C Øx ^ ^ ^ x Ø(t - 1) = A x Ø(t) + K Ø(t)]; with Ø+ (t)C Ø )(0 - CP Ø - (t)C Ø )-1 , Ø (t) = (C - A P K where Ø+ (t) = E {x ^ ^ P Ø(t)x Ø(t) } is obtained by solving the matrix Riccati equation Ø+ (t)A + (C - A P Ø+ (t)C Ø+ (t)C Ø )(0 - C ØP Ø+ (t)C Ø )-1 (C - A P Ø) Ø+ (t - 1) = A P P Ø+ (2 - 1) = 0. P (5.18) Now, it is well-known that both ^(t)]  (t) = (0 - CP- (t)C )-1/2 [y (t) - C x and ØP Ø+ (t)C Ø )-1/2 [y (t - 1) - C Øx ^  Ø(t) = (0 - C Ø(t)] (5.20) (5.19) (5.17) (5.16) ^ x Ø(2 - 1) = 0, (5.15)
+

(5.14)

are normalized white noises, called the forward respectively the backward (transient) innovation processes. Consequently, we may write the Kalman filter (5.10) as x ^(t + 1) = Ax ^(t) + B- (t) (t) y (t) = C x ^(t) + D- (t) (t) (5.21)

where D- (t) := (0 - CP- (t)C )1/2 and B- (t) := K (t)D- (t). Likewise, the backward Kalman filter (5.10) may be written Ø+ (t)Ø ^ ^ Ø(t) + B x Ø(t - 1) = A x  (t - 1) Ø Ø ^ y (t - 1) = C x Ø(t) + D+ (t)Ø  (t - 1) (5.22)

Ø + (t) := (0 - C ØP Ø+ (t)C Ø )1/2 and B Ø+ (t) := K Ø (t)D Ø + (t). Comparing with (3.7) where D and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7) and (3.14) are time-varying and describe the output y only on the interval [0, 2 - 1]. In fact, since ^(t)][x(t) - x ^(t)] }  0, P - P- (t) = E {[x(t) - x Ø Ø and, for the same reason, P - P+ (t)  0, we have Ø+ (t)-1 , P- (t)  P  P+ (t) := P (5.23)

^  - and X ^  + are extremal splitting subspaces, so we see that the predictor spaces X just as X- and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the finite-interval counterparts of equations (4.25) are given by A = E {x ^( + 1)^ x( ) }P- ( )-1 C = E {y ( )^ x( ) }P- ( )-1 , Ø+ ( )-1 = E {y ( - 1)^ Ø = E {y ( - 1)x ^ x( ) } C Ø( ) }P (5.24a) (5.24b) (5.24c)

In complete analogy with the stationary framework in Section 4, the canonical correlation coefficients 1  1 ( )  2 ( )  ∑ ∑ ∑  n ( ) > 0 (5.25) between the finite past Y- and the finite future Y+ are now defined as the singular values of the operator H given by (5.5). To determine these we need a matrix representation of H in some orthonormal bases. Using the pair (5.19)≠(5.20) of transient innovation processes for this purpose, we obtain the normalized matrix (2.14), which ^  . Singular value decomposition yields we shall here denote H ^  = U  V , H (5.26) where U U = I = V V , and  is the diagonal matrix of canonical correlation coefficients. As in Section 4 it is seen that
-1 - z ( ) =  V (L-  ) y 1/2 -1 + z Ø( ) =  U (L+  ) y 1/2

(5.27)

^  - and X ^  + respectively and that are bases in X E {z ( )z ( ) } =  = E {z Ø z Ø }. (5.28)
+ Here L-  and L are the finite-interval counterparts of L- and L+ respectively, and they are of course submatrices of these. Note that H , as defined by (5.4), is now given by - ^ H  = L+  H (L ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z ( ) and z Ø( ) are coherent bases, and the Ø corresponding triplet (A, C, C ) is a finite-interval stochastically balanced realization, i.e., Ø+ ( ). P- ( ) =  = P (5.30)

The following finite-interval modification of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the finite block hankel matrix H , and the proof is analogous. Ø ), obProposition 5.2. The finite-interval stochastically balanced triplet (A , C , C ^ tained from (5.24) by choosing the bases x ^( ) = z ( ) and x Ø( ) = z Ø( ), is given by
1 /2 -1 - -T 1/ 2 U (L+ V - , A = -   )  (H )(L )  -T 1/ 2 V - , C = 1 (H )(L-  )  + -T - 1 /2 Ø C = 1 (H )(L ) U  ,

(5.31a) (5.31b) (5.31c)

where the operators  (∑) and 1 (∑) are defined as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

Ø ) actually varies with  , but that, for each  , it Note that the triplet (A , C , C Ø ) of Section 4, i.e., there is a is similar to the stochastically balanced triplet (A, C, C nonsingular matrix Q so that Ø ) = (Q AQ-1 , CQ-1 , CQ Ø ). (A , C , C    (5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the stationary predictor spaces X- and X+ will have the state covariances P- = Q Q
T -1 Ø+ = Q- and P  Q ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state covariances are not diagonal and equal is a manifestation of the fact that the triplet Ø ) is not stochastically balanced in the sense of Section 4. It is well known (A , C , C Ø+ , respectively, as t  , and Ø+ (t) tend monotonically to P- and P that P- (t) and P therefore we have the following ordering Ø+ )-1  (P Ø+ ( ))-1 := -1 . P- ( ) :=   P-  (P


Since the number n of nonzero singular values (5.25) is in general too large too yield a reasonable model, we must consider what happens when some of the smallest singular values are set equal to zero. The truncation procedure employed by van Overschee and De Moor (1993) is equivalent to the principal subsystem truncation presented in Section 2, except that, and this is very important, the singular-value ^  , which is the decomposition is performed on the normalized block Hankel matrix H natural matrix representation of the operator  . It will be shown in Section 7 that such a truncation will preserve positivity in the stationary case (Theorem 7.3). In order to carry this result over to the case of finite  , we need to assume that the spectral density  of the time series {y (t)} is coercive so that Assumption 3.2 is fulfilled, i.e., that the function Z is strictly positive real. The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D, shows that principal subsystem truncation preserves positivity provided  is chosen large enough.

H

Theorem 5.3. Suppose that the spectral density  of the time series {y (t)} is coercive. Then, there is an integer 1 > 0 such that, for   1 , the principal subsystem Ø )1 ) of (A , C , C Ø ) is a minimal realization of a strictly truncation ((A )11 , (C )1 , (C positive real function (2.13). 6. Subspace identification The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have access to an infinite sequence (3.2) of data. In reality we will have a finite string of observed data {y0 , y1 , y2 , . . . , yN }, (6.1)

where, however, N may be quite large. More specifically, we assume that N is sufficiently large that replacing the ergodic limits (1.11) by truncated sums yields good approximations of {0 , 1 , 2 . . . ,  }, (6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course,  << N . This is equivalent to saying that T := N -  is sufficiently large for 1 T +1
T

a yt+k yt+j b
t=0

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we shall use the finite-interval realization theory of Section 5 as if we had a finite time series {y (0), y (1), y (2), . . . , y ( )}, while substituting the semi-infinite string (3.3) of data by y (t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . ,  . (6.5) (6.4)

In particular, in this case the inner product becomes merely that of a finite-dimensional Euclidean space so that the block Hankel matrix H can be written H = where   y  -1 y  . . . yT + -1 y -2 y -1 . . . yT + -2  -  y = . . ..  . . . . . . .  . y0 y1 . . . yT 1 y + (y - ) T +1    y +1 . . . yT + y  y +1 y +2 . . . yT + +1  + . = and y . . ..  . . . . . . .  . y2 -1 y2 . . . yT +2 -1 

Consequently, the identification of a minimal stationary state-space innovation model describing the data (6.1) can be performed in the following steps. - + , y to obtain, from (1) Perform canonical correlation analysis on the data y ^ Ø+ ( ) = z Ø( ) and, from (5.26), the (5.27), the state vectors x ^- ( ) = z ( ) and x corresponding common state covariance matrix  , i.e., the diagonal matrix of the (finite interval) canonical correlation coefficients (5.25). (2) Given the singular value decomposition (5.26), compute via (5.31) a minimal Ø ). This realization will be in finite-interval balanced form, realization (A, C, C i.e., (5.30) will hold instead of (4.22). (3) To obtain a state space model (3.7) for y we need to compute the matrices B Ø 0 ) defines and D. Note that such matrices will exist if and only if (A, C, C, a positive real function (1.6), or, in other words, if and only if there is a symmetric positive definite P = P such that M (P ) := P - AP A Ø - CP A C Ø - AP C C 0 - CP C  0. (6.6)

[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6), B and D can be determined (in a nonunique way) by a full rank factorization of M (P ), i.e., B D B D = M (P ). (6.7)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P- = E {x- (t)x- (t) } has been determined. Obtaining P- amounts to finding the minimal solution of the algebraic Riccati equation Ø - AP C )(0 - CP C )-1 (C Ø - AP C ) P = AP A + (C (6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t   with initial condition P- ( ) =  . (The corresponding dual procedures yield Ø+ .) Again, in both cases, a positive definite P- can be found if and only P Ø 0 ) defines a positive real function (1.6). In fact, in general, if (A, C, C, {P- (t)}t0 may not even converge unless this positivity condition is fulfilled and may in fact exhibit dynamical behavior with several of the characteristics of chaotic dynamics (Byrnes et al., 1991, 1994). Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that, for  fixed but sufficiently large (see Section 2), we will have rank H = n as T  , Ø ) will be uniquely determined from the data and similar to the and the triplet (A, C, C Ø triplet (A, C, C ) of the "true" generating system. Hence, in particular, in the limit as T  , at least in theory positivity will be guaranteed. If n ^ is an upper bound for the order of the "true" system, we may choose  to be any integer larger than n ^. In practice, however, T is finite, and even if we had a true system generating exact data, the spectral estimate T , although converging to the true spectrum  as T   may in principle fail to be positive for any finite T if there are frequencies  for which (ei ) = 0. Positivity for a suitably large T can however be guaranteed if the "true" spectrum is coercive. The following proposition, which also applies to Aoki's method discussed in Section 2, is proved in Appendix D. Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulØ ) defined by filled. Then, there is a T0  Z+ such that, for T  T0 , the triplet (A, C, C (5.31) yields a function (1.6) which is strictly positive real. However, in practice, rank H normally will keep increasing with  , even for very large T , so that one must resort to some kind of truncation of the Hankel singular values. As we have pointed out in Section 5, setting all canonical correlation coefficients r+1 ( ), r+2 ( ), . . . equal to zero for some suitable r, as is done in, for example, van Overschee and De Moor (1993), is equivalent to principal subsystem truncation. An important issue is therefore under what conditions such a procedure will insure positivity. Here we must distinguish between problems generated by the sample fluctuations of the data due to finite sample size T , as considered in Proposition 6.1, and the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an infinite string of data generated by a "true" high-dimensional system, such a truncation procedure may fail if  is smaller than that dimension. Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following result, which justifies this approximation procedure, provided the rather stringent Assumption 2.1 holds and we have coercivity, and provided T and  are sufficiently large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulfilled. Then, there are positive integers T0 and 1 > 0 such that, for T  T0 and   1 , the Ø1 ), obtained from (2.12) by taking H := H in (2.10), is a minimal triplet (A11 , C1 , C realization of a strictly positive real function (2.13). We note that, in van Overschee and De Moor (1993), the large Hankel matrix
+ + + + - - - - ~  = (y ) (E {y (y ) })-1 E {y (y ) }(E {y (y ) })-1 y H

^  . This leads to a procedure which is equivalent to the one is used in place of H described above. Moreover, the computation of a second singular-value decomposition + - in van Overschee and De Moor (1993), based on H +1 := E {y +1 (y +1 ) }, together with a subsequent change of bases, is actually redundant, as can be deduced from the following proposition. In fact, a considerable amount of computation is needed in van Overschee and De Moor (1993) to compensate for the fact that taking z ( + 1), ^ ( +1)- would computed from a second singular-value decomposition, as a basis in X lead to a Kalman filter model with time-varying parameters. ^ Proposition 6.3. To each coherent pair of bases x ^( ) and x Ø( ) in the finite-interval ^ ^ predictor spaces X - and X + , there corresponds a minimal factorization Ø H =   of the block Hankel matrix H . Here
+  x ^( ) = E Y y
- + - Øx ^ Ø( ) = E Y y  .

(6.9)

and

(6.10)

Conversely, given a minimal factorization (6.9), Ø (T - )-1 y - x ^( ) =     and
+ ^ x Ø( ) =  (T+ )-1 y

(6.11)

^ +. ^  - and X is a coherent pair of bases in X ^  - and X ^  + . Then, for ^ Proof. Let x ^( ) and x Ø( ) be a coherent choice of bases in X Ø( )) of dual bases any X as defined in Theorem 5.1, there is a unique pair (x( ), x Ø  be the matrices defined via such that (5.8) and (5.9) hold. Let  and 
+ - Øx E X y =  x( ) and E X y = Ø( ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of X with respect to Y- and Y+ yields + - + - (y ) } = E {E X y (E X y )) }, E {y which, in view of (6.12), is the same as (6.9). Applying E Y and E Y to respectively the first and second equations of (6.12), the splitting property yields (6.10). As for the converse statement, equations (6.11) follow from the construction in the proof of Theorem 5.1, from which it also follows that the resulting bases x ^( ) and Ø ^ x Ø( ) are constructed from the same (A, C, C ) and therefore coherent. Ø ) have been fixed by a particular choice of x( ) As soon as the parameters (A, C, C in the representation (5.8) in Theorem 5.1, we must choose x ^( + 1) as x ^( + 1) = E Y +1 Ux( ) (6.13)
- +

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More specifically Proposition 6.3 Ø  are uniquely determined once x( ) has been selected. Hence implies that  and  Ø ) is uniquely determined by the Ho-Kalman algorithm so that (A, C, C Ø Ø  +1 = C  Ø  A is prescribed, as is
-1 - Ø  (T- x ^( + 1) =  +1 ) y +1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining x ^( + 1) by an extra singular-value decomposition, as in van Overschee and De Moor (1993), is actually redundant. If we actually were to determine x ^( + 1) as described -L Ø Ø above, we would better compute  +1 from  +1 =  H +1 , where the left inverse is very easily obtained from the singular-value decomposition of H . We stress that Assumption 2.1, although quite limiting, is absolutely crucial in insuring that the subspace identification algorithms mentioned above will actually work. Note that for generic data these algorithms may break down for any fixed  . The same is true for all other subspace methods which deal with identification of covariance models (or equivalent) involving stochastic signals. On the other hand, Assumption 2.1 introduces a quite unrealistic condition which, as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure to estimate T0 and 1 in Proposition 6.2, as the proof is based only on continuity arguments. 7. Stochastic model reduction As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step in order to keep the dimension of the model at a reasonable level. To justify any such procedure one must either assume that there is an underlying "true" system of sufficiently low order, i.e., invoke Assumption 2.1, or to perform rational covariance extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995), Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an infinite one. The latter can be done in many ways, one of which is the maximum entropy extension. In either case, the truncation problem is equivalent to approximating a positive real matrix function Ø + 1 0 , Z (z ) = C (zI - A)-1 C 2 (7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of lower degree. In this section we shall investigate how this can be done and also how such an approximation affects the canonical correlation structure. One main question to be addressed is whether the principal subsystem truncation (2.11) preserves positive realness and balancing, and hence the leading canonical correlation coefficients, as originally claimed by Desai and Pal (1982). As it turns out, the answer is affirmative to the first but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identification approximation obtained by setting some canonical correlation coefficients equal to zero. It is instructive to first consider the continuous-time counterpart of this problem since the latter is simpler and exhibits more desirable properties. Also, it has been widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true. It is well-known [see, e.g., Faurre et al. (1979)] that an m ◊ m matrix function Z with minimal realization Ø + 1 R, (7.2) Z (s) = C (sI - A)-1 C 2 is positive real with respect to the right half plane if and only if there is a symmetric matrix P > 0 such that Ø - PC -AP - P A C  0, (7.3) M (P ) := Ø C - CP R where here we assume that R is positive definite and symmetric. In this case there are two solutions of (7.3), P- and P+ , with the property that any other solution of (7.3) satisfies P -  P  P+ . (7.4)

These extremal solutions play the same role as P- and P+ in the discrete-time setting, and rank M (P- ) = m = rank M (P+ ). (7.5)

-1 Ø+ := P+ If the state-space coordinates are chosen so that both P- and P are diagonal and equal, and thus, by (4.14), equal to the diagonal matrix  of canonical correlation Ø ) is stochastically balanced. coefficients, we say that (A, C, C Now, suppose that  is partitioned as in (2.8) with r+1 < r , and consider the corresponding principal subsystem truncation (2.12). Using the stochastic realization framework, Harshavaradana, Jonckheere and Silverman (1984) showed that

Ø1 + 1 R, Z1 (s) = C1 (sI - A11 )-1 C 2

(7.6)

Ø1 ) is a minimal realization of a positive real function and conjectured that (A11 , C1 , C is stochastically balanced. We shall next show that this conjecture is true, as has already been done by Ober (1991) in a framework of canonical forms. First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield   Ø1 - 1 C1 -A11 1 - 1 A11  C   0,     (7.7) Ø  R C1 - C1 1 where blocks which play no role in the analysis are marked by an asterisk. Consequently, M1 (1 ) = Ø1 - 1 C1 -A11 1 - 1 A11 C  0. Ø C1 - C1 1 R (7.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982), Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane, Ø1 ) is a minimal realization. (7.6) is positive real, but it remains to prove that (A11 , C1 , C This was done in Harshavaradhana et al. (1984). It is important to observe here that, contrary to the situation in the discrete-time setting, rank M1 (1 ) = rank M () = m 1 -1 and rank M1 (- 1 ) = rank M ( ) = m, important facts that will be seen to imply that the reduced system is stochastically balanced. Recall that in the continuous-time setting the spectral density (s) = Z (s)+ Z (-s) is coercive if, for some > 0, we have (s)  I for all s on the imaginary axis. This is equivalent to the condition that R > 0 and  has no zeros on the imaginary axis (Faurre et al., 1979, Theorem 4.17). Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with (s) := Ø ) be in stochastically balanced form. Then, Z (s) + Z (-s) coercive, and let (A, C, C Ø1 ) defines a positive real function (7.6) if r+1 < r , the reduced system (A11 , C1 , C for which it is a minimal realization in stochastically balanced form, and 1 (s) := Z1 (s) + Z1 (-s) is coercive. Proof. We have already shown that Z1 is positive real, and we refer the reader to Ø1 ) is a minimal realization Harshavaradhana et al. (1984) for the proof that (A11 , C1 , C Ø1 ) is stochastically of Z1 . It remains to show that 1 is coercive and that (A11 , C1 , C -1 balanced, i.e., that P1- = 1 = P1+ , where P1- and P1+ are solutions to the algebraic Riccati equation Ø - P1 C1 )R-1 (C Ø - P1 C1 ) = 0 A11 P1 + P1 A11 + (C (7.9) such that any other solution P1 of (7.9) satisfies P1-  P1  P1+ . To this end, 1 -1 note that since M1 (1 ) and M1 (- 1 ) have rank m, both 1 and 1 satisfy (7.9). 1 Therefore, as is well-known (Molinari, 1977) and easy to show, Q := - 1 - 1 satisfies 1 Q + Q1 + QC1 R-1 C1 Q = 0, where Ø - 1 C )R-1 C1 . 1 = A11 - (C 1 (7.11) Since  is coercive, -1 -  = P+ - P- > 0 (Faurre et al., 1979, Theorem 4.17) so that 1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to 1 Q-1 + Q-1 1 + C1 R-1 C1 = 0. (7.12) (7.10)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , 1 ). Since, in addition, the Lyapunov equation (7.12) has a positive definite solution Q-1 , 1 must be a stability matrix. Therefore 1 is the minimal (stabilizing) solution P1- of -1 Ø1+ := P1+ = 1 . (7.9). In the same way, using the backward setting, we show that P Ø1 ) is stochastically balanced. Since P1+ - P1- > 0, 1 is Consequently, (A11 , C1 , C coercive. Ø 1 0 ) Let us now return to the discrete-time setting. Let us recall that, if (A, C, C, 2 is a minimal realization of (7.1), the matrix function Z is positive real if and only if the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given the positive real rational function (7.1) with the property that (z ) = Z (z ) + Z (z -1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y , the state covariance P of any minimal stochastic realization (3.7) of y satisfies (6.6) and the matrices B, D in (3.7) satisfy (6.7). Consequently, as pointed out in Section 5, the matrices B and D can be determined via a matrix factorization of M (P ) once P has been determined. Ø ) is in stochastically balanced form, Theorem 4.4 implies that Now, if (A, C, C M ()  0. In view of (4.16) and (2.12), M () may be written   Ø1 - A11 1 C1 - A12 2 C2 1 - A11 1 A11 - A12 2 A12  C ,     Ø C1 - C1 1 A11 - C2 2 A12  0 - C1 1 C1 - C2 2 C2 where, as before, the blocks which do not enter the analysis are marked with an asterisk. Since M ()  0, this implies that M1 (1 ) - where M1 (1 ) = Ø1 - A11 1 C1 1 - A11 1 A11 C Ø C1 - C1 1 A11 0 - C1 1 C1 (7.14) A A12 2 12 C2 C2  0, (7.13)

Ø1 ). Thereis the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , C fore, M (1 )  0, so if we can show that A11 is stable, i.e., has all its eigenvalues strictly inside the unit circle, it follows that Ø1 + 1 0 , (7.15) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real. As we shall see below this is true without the requirement needed in continuous time that r+1 < r . Ø1 ) also to be balanced, 1 would have to be the minimal solution P1- For (A11 , C1 , C of M1 (P1 )  0, which in turn would require that rank M1 (1 ) = rank M () = m. Due to the extra positive semidefinite term in (7.13), however, this will in general not be the case and therefore 1  P1- will correspond to an external realization, as will 1 - 1  P1+ ; see Lindquist and Picci (1991). Ø1 ) is minimal we need to assume that  is coercive, or, To show that (A11 , C1 , C equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979, Theorem A4.4) that this implies that P+ - P- > 0. (7.16)

In fact, if 0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to coercivity. Coercivity also implies that 0 - CP- C > 0. (7.17)

Ø ) in balanced form, P- =  = P Ø+ and, in view of (3.16), Remark 7.2. With (A, C, C -1 -1 P+ =  . Hence (7.16) becomes  > , which obviously holds if and only if 1 < 1, which in turn is equivalent to H -  H + = 0. Consequently, given the full rank condition 0 > 0, coercivity is equivalent to the past and the future spaces of y having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

Ø ) be in stochastically balTheorem 7.3. Let (7.1) be positive real, and let (A, C, C anced form. Then the reduced-degree function (7.15) obtained via principal subsystem decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so Ø1 , 1 0 ) is a minimal realization of Z1 . is Z1 , and (A11 , C1 , C 2 For the proof we need the following lemma, the proof of which is given in Appendix D. Lemma 7.4. Let the matrix function Z be given by (7.1), where 0 > 0, but where Ø A ) are not necessarily observable, and suppose that (6.6) has two (C, A) and (C, positive definite symmetric solutions, P1 and P2 , such that P2 - P1 > 0. Then Z is strictly positive real. Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is stable. To this end, we note that P is the reachability gramian of (3.7). In particular, Ø ) is stochastically balanced, the reachability gramian of the system (3.18) if (A, C, C equals  so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable. By Remark 7.2, coercivity of  implies that -1 -  > 0, from which it follows 1 that - 1 - 1 > 0 and that 0 > 0. Moreover, By construction, M1 (1 )  0 and -1 M1 (1 )  0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is. To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows by symmetry. By regularity condition (7.17), 0 - C1 1 C1  0 - C C > 0, and consequently, since M1 (1 )  0, 1 satisfies the algebraic Riccati inequality Ø1 - A11 P1 C1 )(0 - C1 P1 C1 )-1 (C Ø1 - A11 P1 C1 )  0, (7.19) A11 P1 A11 - P1 + (C but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable. Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density 1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a unique symmetric P1- > 0 which satisfies (7.19) with equality and for which Ø1 - A11 P1- C1 )(0 - C1 P1- C1 )-1 C1 1- := A11 - (C is stable. It is well-known (Faurre et al., 1979) that P1- is the minimal symmetric solution of the linear matrix inequality M1 (P1 )  0, i.e., that any other symmetric 1 -1 solution P1 satisfies P1  P1- . We also know that M1 (- 1 )  0. Next, since 1 - 1 1 > 0, a fortiori it holds that Q := - 1 - P1- > 0. A tedious but straight-forward calculation shows that Q satisfies 1- (Q-1 - C1 R-1 C1 )-1 1- - Q  0, from which it follows that Q-1 - C1 R-1 C1 - 1- Q-1 1-  0. Cf. Faurre et al. (1979), pp. 85 and 95. (7.20) (7.18)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a  and a   C such that [C1 , I - A11 ]a = 0. and therefore, in view of (7.20), (1 - ||2 )a Q-1 a  0.

Cr

But  is an eigenvalue of the stable matrix A11 , implying that || < 1, so we must have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable. A remaining question is whether there is some balanced order-reduction procedure in discrete time which preserves both positivity and balancing. That this is the case in continuous time implies that the answer is affirmative, but the reduced system cannot be a simple principal subsystem truncation. Ø ) be in stochastically Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, C balanced form. Moreover, given a decomposition ( 2.12) such that r+1 < r , let Ar Cr Ør C r0 = = = = A11 - A12 (I + A22 )-1 A21 C1 - C2 (I + A22 )-1 A21 Ø1 - C Ø2 (I + A22 )-1 A12 C Ø2 - C Ø2 (I + A22 )-1 C2 0 - C2 (I + A22 )-1 C

Ør , r0 ) is a minimal realization of a strictly positive real function Then (Ar , Cr , C Ør + 1 r0 . Zr (z ) = Cr (zI - Ar )-1 C 2 (7.21)

Ør , r0 ) is stochastically balanced with canonical correlation coeffiMoreover, (Ar , Cr , C cients 1 , 2 , . . . , r . To understand why this reduced-order system does preserve both positivity and balancing, note that for   I -A12 (I + A22 )-1 0 I 0 T = 0 -1 I 0 -C2 (I + A22 ) we obtain   Ør - Ar 1 Cr 1 - Ar 1 Ar  C ,    T M ()T =  Ør - Cr 1 A  r0 - Cr 1 C C r r

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the reduced-order system, Mr (1 )  0 and rank Mr (1 )  rank M (). Ør , r0 ) is precisely what one obtains To prove Theorem 7.5 we observe that (Ar , Cr , C Ø 0 ) by the appropriate linear fractional transform to the if one transforms (A, C, C, continuous-time setting and then, after reduction, back to discrete time again as suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions The purpose of this paper is to analyze a class of popular subspace identification procedures for state space models in the theoretical framework of rational covariance extension, balanced model reduction, and geometric theory for splitting subspaces. We have pointed out that these methods are based on the hidden Assumption 2.1 which is not entirely natural and which is in general untestable. The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially equivalent to the Ho-Kalman algorithm in which the basic factorization is performed by singular-value decomposition of a block Hankel matrix of finite covariance data, as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in orthonormal coordinates and allows for theoretical verification of the truncation step. A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of the data equals the algebraic degree. To achieve this, one must assume that the data are generated exactly by an underlying system and that the amount of data is sufficient for constructing an accurate partial covariance sequence the length of which is sufficient in relation to the dimension of the underlying system. Hence it is absolutely crucial that a reliable upper bound of the dimension of the "true" underlying system is available. We stress that these stringent assumptions are not satisfied for generic data, as was pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that the positive degree has no generic value. In fact, just for the moment considering the single-output case, for each p such that r  p   there is a nonempty open set of partial covariance sequences having positive degree p in the space of sequences of length  . Secondly, for any r, it is possible to construct examples of long partial covariance sequences having algebraic degree r but having arbitrarily large positive degree (Theorem 2.4). In Section 7 we proved an open question concerning the preservation of positivity in the original (discrete-time) model reduction procedure of Desai and Pal (1984). Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to the principal subsystem truncation used in van Overschee and De Moor (1993), but not to the one in Aoki (1990). We prove that positivity is preserved provided that the original data satisfies Assumption 2.1, justifying setting the smaller canonical correlation coefficients equal to zero. Unlike the situation in continuous time, this truncation does not preserve balancing. The validity of the corresponding procedure of Aoki (1990) has not been settled. The contribution of this paper is to provide theoretical understanding of these identification algorithms and to point out possible pitfalls of such procedures. Hence the primary purpose is not to suggest alternative procedures. Nevertheless, we would like to point out that a two-stage procedure equivalent to covariance extension followed by model reduction would work on any finite string of data, thus elimination the need for Assumptions 2.1. However, we leave open the question of how such a procedure should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension or some other. Acknowledgment. We would like to thank the referees and the associate editor for the careful review of our paper and for many useful suggestions, which have led to considerable improvements of this paper. References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31≠73. 2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner. 3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley. 4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM J. Control, 13, 162≠173. 5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag. 6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems and Control Letters, 2, 2301≠2312. 7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization of modelling filter. Inter. J. of Control, 50, 2301≠2321. 8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted for publication. 9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40. 10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in Kalman filtering. IEEE Trans. Autom. Control, 36, 563≠579. 11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast filtering algorithms. SIAM J. Control and Optimization, 32, 744≠789. 12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105≠1112. 13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29, 1097≠1100. 14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821≠839. 15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer. 16. Faurre, P. (1969). Identification par minimisation d'une representation Markovienne de processus aleatoires. Symposium on Optimization, Nice. 17. Faurre, P. and Chataigner (1971). Identification en temp reel et en temp differee par factorisation de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt. 18. Faurre, P., M. Clerget, and F. Germain (1979). Op¥ erateurs Rationnels Positifs, Dunod. 19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de r¥ ealisation stochastique. C. R. Academie Sciences Paris 268. 20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York. 21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438≠449. 22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and their L error bounds. Intern. J. Control, 39, 1115≠1193. 23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and its Applications, 50, 277≠319. 24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York. 25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling of time series. Econometric Reviews 11, 379≠396. 26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321≠377. 27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744≠746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference, Tel Aviv, Israel. 29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory, McGraw-Hill. 30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta Polytech. Scand., MA31, 9≠39. 31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499≠513. 32. Kung, S. Y. (1978). A new identification and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705≠714. 33. Larimore, W. E. (1990). System identification, reduced-order filtering and modeling via canonical variate analysis. Proc. 29th Conf. Decison and Control, pp. 445≠451. 34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809≠857. 35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241≠333. 36. Lindquist, A. and G. Picci (1994a). On "subspace methods" identification. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds., Akademie Verlag, 315≠320. 37. Lindquist, A. and G. Picci (1994b). On "subspace methods" identification and stochastic model reduction. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 397≠403. 38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE Trans. Automatic Control, 20, 396≠399. 39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica, 13, 347≠357. 40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and Optimization, 29, 1251≠1287. 41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identification problem. Automatica, 29 , 649-660. 42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica, 30, 75≠93. 43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identification algorithms and its interpretation. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 145≠156. 44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382≠387. 45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes. Linear Algebra and its Applications, 205-206, 957-1003. 46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day. 47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II. Journal fur die reine und angewandte Mathematik, 148, 122≠145. 48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401≠ 421. 49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation. IEEE Trans. Automatic Control, AC-16, 621≠634. 50. Whittle, P. (1963). On the fitting of multivariate autoregressions and the approximate canonical factorization of a spectral density matrix. Biometrica, 50, 129≠134. 51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its Applications, Cambridge U.P. 52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via Ho's algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4. We first give a proof for the special case n = 1. Consider a scalar function 1z+b (A.1) 2z +a with a scalar sequence (1.4) such that 0 = 1. Now it is well-known [see, e.g., Schur (1918), Akhiezer (1965)] that T is positive definite if and only if Z (z ) = |t | < 1 t = 0, 1, 2, . . . ,  - 1 (A.2) where {0 , 1 , 2 , . . . } are the so called Schur parameters. There is a bijective relation between partial sequences (1.1) and partial sequences {0 , 1 , . . . ,  -1 } of the same length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the Schur parameters of (A.1) are generated by the nonlinear dynamical system t+1 = t+1 =
t 2 1 - t - t  t 2 1 - t

0 = 1 (a + b) 2 0 = 1 (b - a) 2

(A.3)

and that Tt becomes singular precisely when there is finite escape. It was also shown in Byrnes et al. (1991) that {t } is generated by a linear system 2/ -1 ut+1 = vt+1 1 0 ut , vt (A.4)

where t = vt /ut and  := (a + b)(1 + ab)-1 . If  is greater than one in modulus, the coefficient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant scalar factor, similar to cos  sin  , - sin  cos   where  := arctan 2 - 1. Hence t is the slope of a line through the origin in R2 which rotates counter-clockwise with the constant angle  in each time step. Consequently, arctan t+1 = arctan t + . Moreover, assuming that 0 > 0, the Schur condition t < 1 will fail as soon as t+1 negative or infinite, as can be seen from the first of recursions (A.3). Hence (A.2) holds if and only if  (A.5) arctan  < . 2 Therefore for a small > 0, take a = 1 - and b = 1 + , yielding a stable Z . Then  2 4 - 2 . We may choose so that  = 2- 2 > 1 and  = arctan 2- 2   << , +1  - arctan 0 . Then (A.5) holds so that T > 0, but we also have where  :=  2  arctan  +1 > 2 so that T +1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function (a + b)n-1 (z ) 1 n (z ) + 1 2 Z (z ) = 1 2 n (z ) + 2 (a + b)n-1 (z ) o polynomials of the first and second kind rewhere {t } and {t } are the Szeg® spectively (Akhiezer, 1965). The function Z has the property that its first n Schur parameters, {0 , 1 , . . . , n-1 }, are precisely the data which uniquely determines n , n-1 , n and n-1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated by t 0 = 1 (a + b) t+1 = 1- 2 2 t+1 =
- t  t 2 1- t +n-1
t+n-1

Hence, we have reduced the problem to the case n = 1. If we choose the initial Schur parameters sufficiently small so that n (z ) and n-1 (z ) are approximately z n and z n-1 , n (z ) + 0 n-1 (z ) is stable if we choose a := 1 - 2 and b := 1 + for some small > 0. Then  > 1 and the proof for the case n = 1 carries through with a trivial modification. Appendix B. The Hilbert space of a sample function Let y = {y(t)}t0 be a zero-mean wide-sense-stationary stochastic process defined on a probability space {, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t,  ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit exists, the m ◊ m matrix function k  k obtained from a particular trajectory is then a bona-fide covariance function. [The continuous-time analog of this property was observed already by Wiener (1933)]. If moreover the sample limit is (almost surely) independent of the particular trajectory and hence necessarily coincides with the "ensemble" covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in the book of Hannan (1970). It is obvious from Birkhoff's ergodic theorem that any (zero-mean) strictly stationary ergodic process is also second-order ergodic. In this Appendix we shall show that the properties of the Hilbert space structure associated to a stationary time series y , defined on page 10, are identical to those of the Hilbert space induced by a second-order ergodic process.10 The two frameworks, i.e., the statistical "time-series" structure and the "probabilistic" structure, are in fact isomorphic. To see this, pick a "representative" trajectory of y, i.e. one in the subset of  (of probability one) for which the limit (1.11) exists. Clearly there will be no loss of generality in assuming that the probability space  of y is the "sample space", of all possible trajectories of y, i.e. the set of all semi-infinite sequences  = {0 , 1 , 2 , . . . }, t  Rm . With this choice, A will be the usual  algebra of cylinder subsets of  and the t:th random variable of the process, y(t), is just the canonical projection function y(t,  ) :   t .
For a process of this kind the Hilbert space H (y) is the closure in L2 (, A, P ) of the linear vector space generated by the scalar random variables   yi (t,  ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence of m ◊  matrices y := {y (k )}k0 as in (3.3). For  in the subset of  where the time averages converge, define the map T , T : a y(t)  a y (t) t  0 a  Rm associating the i:th scalar components of each m-dimensional random vector y(t) of the process to the corresponding i:th (infinite) row of the m ◊  matrix y (t) constructed from the corresponding sample path {y(t,  ); t  Z}. By second-order ergodicity, the set of all such    will have probability measure one and the map T will in fact be norm preserving, since by construction we have t-s = E y(t)y(s) = Ey (t)y (s) , where t is the covariance matrix of y. The map T can then be extended by linearity and continuity to a unitary linear operator T : H (y)  H (y ) which commutes with the action of the natural shift operators (both of which we denote U), in these two Hilbert spaces: H (y) -H (y)  T T  H (y ) -H (y ) This isomorphism allows us to employ exactly the same formalism and notations used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991) in the present statistical setup, where we build estimates of the parameters of models describing the data in terms of an observed time series instead of stochastic processes. This provides a remarkable conceptual unity and admits a straightforward derivation in the style of stochastic realization theory of the formulas in the paper van Overschee and De Moor (1993), there obtained with considerable effort through lengthy and formal manipulations. Appendix C. The invariant form of the Kalman filter Given a stationary stochastic system (3.7), the Kalman filter is usually determined via the matrix Riccati equation Q(t + 1) = AQ(t)A - [AQ(t)C + BD ][CQ(t)C + DD ]-1 [AQ(t)C + BD ] + BB (C.1) where Q(0) = P := E{x(0)x(0) }. Here Q(t) = E{[x(t) - x ^(t)][x(t) - x ^(t)] }, and the Kalman gain is given by K (t) = [AQ(t)C + BD ][CQ(t)C + DD ]-1 . (C.3) (C.2)
U U

These equations of course depend on P , B and D, which vary as the splitting subspace Ø ) is invariant if a uniform choice of bases is made. X varies over , whereas (A, C, C Ø ) and hence However, as shall see, the gain K depends only on the triplet (A, C, C one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

Ø ), and hence are invariant over . Clearly, in view of Theorem 5.1, P- (t), on (A, C, C as defined by (5.12), has this property. Moreover, Q(t) = P - P- (t), and, consequently, in view of (3.9), and the Lyapunov equation P = AP A + BB , P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11). A symmetric argument yields the backward equations. It is easy to see that as Q(t)  Q monotonously, P- (t)  P- , and hence P  P- , as should be. Appendix D. Some deferred proofs Proof of Theorem 5.1. Since X is a splitting subspace for the infinite past H - and the - + := U  H - and H := U  H + . But infinite future H + , by stationarity, X splits H - - + + - + Y  H and Y  H , and hence X splits Y and Y also. (See, e.g., Lindquist and Picci (1985, 1991).) Now, using the projection formula in the footnote of page +  Y+ 16, we have for any b y   -1  1 2 . . . 0 1 . . .  2 3 . . .  +1  1 0 . . .  -1  - + -  .  y E Y b y =b  . . . . .. ..  .    . . . . . . . . . . . . . .   +1 ∑ ∑ ∑ 2 -1   -1 ∑ ∑ ∑ 0 - Ø  (T- )-1 y = b   = b   Ø  are appropriate finite-dimensional observability and constructibility where  and  Ø  such matrices (2.6) of full rank. If  > 0 , there is a minimal factorization H =   - -1 - Ø that  :=  (T ) y has n components, and Ø  > 0. Ø  (T- )-1  E { } =  ^  - , dim X ^  -  n = dim X so, since Therefore, since the components of  belong to X ^ ^ X - is minimal, X must also be minimal and X - be spanned by the components of  . Next, from the backward system (3.14) we see that - Øx = Ø( ) + terms ortogonal to X , y and therefore, by the same projection formula,
- Ø (T - )-1 y - = a . E Y a x( ) = a E {x( )Ø x( ) }    - ^  - , establishing the first of identities Consequently, E Y X = {a  | a  Rn } = X (5.7). The second follows from a symmetric argument. The representation formula (5.8) follows from the minimality of X as a splitting subspace for Y+ and Y- , which, in particular, implies that the constructibility operator, -  ^ Ct := E|Y X : X  X -

X

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n, ^k ( ). there is a unique random variable xk ( )  X whose projection onto Y- is x To show that x(0) form a uniform choice of bases as X varies over , first take X to be the stationary backward predictor space X+ and let x+ ( ) be the unique basis - ^( ) = E Y x+ ( ). Now, let X  be arbitrary. Then, since in U X+ such that x -   + X is a splitting subspace for Y and U X+  U H (Lindquist and Picci, 1991, Proposition 2.1(vi)), we have

X

X

x ^( ) = E Y x+ ( ) = E Y E X x+ ( ), and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all X  , which is a well-known characterization of uniform choice of bases; see Section 6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields the corresponding statement for (5.9).

-

-

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a corresponding partial covariance sequence , where  is large enough for the Hankel Ø ) be the triplet determined matrix H , defined by (1.5), to have rank n. Let (A, C, C from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the covariance data by estimates {0T , 1T , . . . , T } ØT ) be the corresponding triplet obtained via (2.5). of type (6.3), and let (AT , CT , C We want to prove that ØT + 1 0T ZT (z ) := CT (zI - AT )-1 C 2 is strictly positive real for a sufficiently large T . Now, if deg ZT = deg Z , replace  by -1 0  0 in (2.5) in the appropriate , U by U 0 , V by V 0 , and -1 by 0 0 0 0 Ø ) and (AT , CT , C ØT ) have the same dimensions. This will calculation so that (A, C, C ØT , 0T ) can be made arbitrarily close not affect Z and ZT . By continuity, (AT , CT , C Ø to (A, C, C, 0 ) in any norm by choosing T sufficiently large. Thus the same holds for max Z (ei ) - ZT (ei )
[0,2 ]

and hence, since (z ) := Z (z ) + Z (z -1 ) satisfies (3.10), so will T (z ) := ZT (z ) + ZT (z -1 ) for sufficiently large T . Moreover, since |(A)| < 1, we have |(AT )| < 1 by continuity for sufficiently large T . Consequently, there is a T0 such that ZT is strictly positive real for T  T0 . Ø) Proof of Theorem 5.3. Let Z , defined by (1.6), be strictly positive real, and let (A, C, C be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , defined by Ø1 ), is also strictly (7.15) in terms of the principal subsystem truncation (A11 , C1 , C positive real. We want to prove that this property is carried over to rational matrix function Ø )1 ) + 1 0 Z 1 (z ) = (C )1 (zI - (A )11 )-1 (C 2 for  sufficiently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let Q be defined by (5.32). Since the canonical correlation coefficients (5.25) tend to the canonical correlation coefficients (4.12) as   ,   . Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P- (t) tends to Q Q as t   if the initial condition is taken to be P- ( ) =  . Consequently, for any > 0, there is a sufficiently large  such that  -  < 2 and  - Q Q < 2 so that  - Q Q < . Hence Q tends to a limit Q with the property  = Q Q . Using the same argument in the backward direction, the T -1 second of relations (5.33) shows that Q also satisfies  = Q-  Q . Consequently, by the same argument as in the proof of Theorem 4.4, Q is a signature matrix, and hence in particular diagonal. Therefore,
1 -1 Ø Ø )1 )  ((Q )11 A(Q )- ((A )11 , (C )1 , (C 11 , C (Q )11 , C (Q )11 ) as   ,

where (Q )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, Z 1  Z1 . Hence, since Z1 is positive real, then so is Z 1 for  sufficiently large. Ø ) is a minimal triplet. Proof of Lemma 7.4. Let us first consider the case when (A, C, C Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P- and P+ respectively, which, in particular, have the property that P-  P1 and P2  P+ . Then, in view of (7.18), P+ - P- > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem A4.4). Next, let us reduce the general case to the case considered above. If (C, A) is not observable, change the coordinates in state space, through a transformation Ø ), so that Ø )  (QAQ-1 , CQ-1 , QC (A, C, C ^ 0 C= C A= ^ 0 A   ^ Ø= C Ø  , C

^ A ^) is observable. Then, if P1 and P2 have the corresponding representations where (C, P1 = ^1  P   P2 = ^2  P ,  

^2 satisfy the reduced version of the linear matrix ^1 and P it is easy to see that P ^) and that, in this new Ø ) for (A, ^ C, ^ C Ø inequality (6.6) obtained by exchanging (A, C, C ^ ^2 - P ^1 > 0. If (C, Ø A ^ ) is not observable, we proceed setting, (7.18) holds, i.e., P -1 -1 ^2 ^1 and P satisfy the by removing these unobservable modes. First note that P ^ ^ ^ C, ^ C Ø ) by (A ^ , C, Ø C ^ ). Then, dual linear matrix inequality obtained by exchanging (A, changing coordinates in state space so that ^ ~ Ø= C Ø  C ~ ^ = A A  0  ^= C ~ 0 , C

^ Ø A ~ ) observable, and defining with (C, ~ -1  P -1 ^1 P = 1   ~ -1  ^ - 1 = P2 P , 2  

44

ANDERS LINDQUIST AND GIORGIO PICCI

~ ~ C, ~ C, Ø 1 0 ) is a minimal realization of Z . Moreover, P ~1 and P ~2 satisfy we see that (A, 2 the corresponding linear matrix inequality (6.6) and have the property (7.18) in this setting. Hence the problem is reduced to the case already studied above. Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans-1 , mapping formed to the continuous-time setting via a bilinear transformation s = z z +1 the unit disc onto the left half plane so that Zc (s) = Zd 1+s 1-s (D.1)

is positive real in the continuous-time sense if and only if Zd is positive real in the discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al. Ød , 1 0 ) and (Ac , Cc , C Øc , 1 R) are realizations of Zd and Zc (1979)] that, if (Ad , Cd , C 2 2 respectively, we have  Ac = (Ad + I )-1 (Ad - I )    C = 2C (A + I )-1 c  d d (D.2) Ø Ød (A + I )-1  Cc = 2C  d   Ø -C Ød (A + I )-1 C R = 0 - Cd (Ad + I )-1 C d d d and inversely  Ad = (I - Ac )-1 (I + Ac )    C = 2C (I - A )-1 d c  c -1 Ø Ø  = 2 C ( I - A C d c  c)   Ø +C Øc (I - Ac )-1 Cc 0 = R + Cc (I - Ac )-1 C c

(D.3)

Under this transformation the observability gramian and the constructibility gramian Ød , 1 0 ) is Ø A )) are preserved so that (Ad , Cd , C (i.e., the observability gramian of (C, 2 Øc , 1 R) is; see, e.g., p. 1119 in Glover a minimal realization if and only if (Ac , Cc , C 2 (1984). Moreover, coercivity is preserved, and the solution sets of the corresponding linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the reachability gramian of a spectral factor and this gramian is also preserved.) Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact, transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting, all the requirements of Theorem 7.1 are satisfied. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order positive real function thus obtained via (D.3) back to discrete time, the desired result is obtained.

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann Computer Science and Engineering Department Arizona State University Tempe, Arizona 85287-5406 Jean-Claude Derniame Laboratoire lorrain de Recherche en Informatique et Applications LORIA : Bd des Aiguillettes BP 239 54 506 Vandoeuvre Cedex Abstract
Only recently has the research community started to consider how to make software process models interoperable and reusable. The task is difficult. Software processes are inherently creative and dynamic, difficult to define and repeat at an enactable level of detail. Additionally, interoperability and reusability have not been considered important issues. Recent interoperability and reusability solutions advocate the development of standard process model representations based on common concepts or generic schemas, which are used as a basis for translating between heterogeneous process representations. In this paper we propose an alternative approach through the development of process-based components. We present the Open Process Components Framework, a componentbased framework for software process modeling. In this approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of process representations, an explicit representation of process state, and an extendable set of class relationships. their benefits, systems based on these formalisms create enactable process models which are not interoperable nor reusable with one another. The prevailing solution is to advocate an intermediary standard process representation and provide translations for interoperability and reuse. We do not believe this approach is scalable and defeats the purpose of using heterogeneous process representations. We advocate an object-oriented, component-based philosophy for providing software process interoperability and reuse. This paper presents Open Process Components, a component-based framework for software process definition and enactment. In this framework, components are well-encapsulated representations of process entities that interact in meaningful ways. The framework is solidly founded on mature concepts in the software process field, and yet is extendable so that process models may be customized in a particular domain. A componentized view of process representations results in easier process definition, modularized process enactment, natural interoperability, and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable. Instances vary according to constantly changing demands of specific projects. Fully elaborating a software process model to an enactable level of granularity is often too tedious, time-consuming, and costly[4]. Motivated by the need for interoperability and reuse, we advocate applying component-based techniques to software process modeling. Constructing software process models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined components.

1.0 Introduction
Since Osterweil's proposal[12] for automating the software process a decade ago, there has been significant debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets [9], rule-based formalisms [1,8,13], process programming languages [15], event-based representations [3,6,10], object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research under Award number N00014-97-1-0872

A component-based approach:

2.1.2 Process Component States
The elements of the meta-model appear in most process models. Each model requires a different enactment service to interpret the representation and execute the process. Regardless of the formalism employed and the interpreter used, all models define actions on the entities within the process domain, which effect the states of those entities. The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes finite state machines as part of its basic abstractions. Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for process modeling is not unique[7,11]. The OPC framework defines a basic set of states and transitions for Process and Activity components. These include states such as executing, suspended , and aborted, with corresponding transitions between states defined by actions such as startProcess, suspendProcess, and abortProcess. Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object's unique behavior when interacting with other components within the framework. The current class definition for state transition graphs include operations to add and remove states and allowable transitions between states, making a component's state and behaviors affecting state explicit and manipulable.

∑ ∑ ∑ ∑

avoids deep integration of semantic models handles the natural complexity of software processes, responds to dynamic software processes, and facilitates reuse, minimizing one-shot process models. Component-based process modeling requires a framework for developing components. The framework must identify process entities, define meaningful interactions between entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides a foundation for developing, integrating, maintaining, and reusing a variety of process representations. The framework defines basic abstractions of the problem space that can be specialized. Yet, the framework must make some commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this meta-model as a basis for translation between process models, we use it as a foundation for identifying elements of the process space for componentization, and for defining meaningful ways in which process components interact.
has_sub

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a layered, three-tier software architecture (Figure 2). The Framework Layer defines classes and interfaces modeling process entities derived from the OPC meta-model. The Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component Layer extends representations to particular domains. It is from this layer that actual Process component objects are instantiated. A process model in the OPC framework is a set of components, realized as objects of Component Layer classes, and a set of relations between those components, created under the constraints of the Framework Layer, implemented using Representation Layer semantics. Figure 2 shows example classes at each layer of abstraction for the meta-model element Process. The Framework Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation Layer is comprised of class definitions for specific process representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets (PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any other representations we wish to encapsulate. The Compo-

role
assigned_to can_perform

activity

has_input has_output

product

consists_of

has_version has_variant
has_sub

agent

process

FIGURE 1. The Open Process Components Framework Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and Agent), and artifacts (Product). The meta-model defines the "rules of engagement" for components. It identifies what component types interact with what other component types under what relationships. These relationships are not static; process components and component relationships are highly dynamic during the course of the component's life cycle.

Framework Layer

Process

Process
Representation Layer ECAProcess OrderedProcess PetriNetProcess RuleProcess

PDLProcess

Component Layer Bug Fix Code Module Integration Test Design

Code Module

Peer Review Stress Test

FIGURE 2. Object-oriented class diagram for Process components

nent Layer contains type definitions for actual process types. The dashed lines between layers in Figure 2 denotes that the Representation and Component Layers in fact can have many levels. This allows for multiple ways in which to extend and specialize the framework. The first step identifies a base set of classes and interfaces at the Framework Layer. The next step is to construct encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are encapsulated behind the interfaces inherited from the Framework Layer. For example, the implementation may come from a COTS process tool. Finally, components defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used since inheritance would tie the component's type to its implementation. Component Layer objects are configurable. Component Layer classes represent generic process models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product, Role, or Agent) fully specified and bound is part of an instantiated process model[5]. Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given representations. Delegating Representation Layer classes provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi's levels of process specialization[5].

software development support. PCIS2 services include Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA. The process support services in PCIS2 are based on the OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification, known as jFlow[11], submitted to the OMG. The jFlow specification is largely an "object-ization" of existing WfMC interfaces[16]. This is not a drawback, but one of the strengths of the OMG's approach to adopting and adapting existing technology. The jFlow specification improves upon the original WAPI specifications by defining appropriate interactions between objects to gain interoperability and maintainability of workflow systems. The PCIS2 specification is object-oriented from the ground up, but has borrowed some of the jFlow concepts in order to maintain compliance with emerging standards. PCIS2 and the jFlow specification differ in three areas. First, PCIS2 supports dynamic processes through ad hoc process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2 incorporates support for the metaprocess, by defining views on its services for controlling, defining, performing, and monitoring processes. jFlow only defines interfaces for performing (enacting) and monitoring workflows. It should also be pointed out that jFlow identifies concepts not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these specifications, they are largely complementary and both provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The authors propose the construction of software process components for producing process artifacts. A "software process component" is essentially a process model fragment written in some Process Modeling Language (PML). Components are dynamically combined to construct complete process models through interface types and their respective "connectors ports". The authors correctly motivate the need to eliminate monolithic process systems and instead provide reuse and integration capabilities for process representations. However, the approach lacks adherence to foundational concepts, such as those used in OPC (see Section 2.1). The three-tier layering of the OPC framework provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite their differences, the Pynode component approach is simi-

3.0 PCIS2 Process Services
The Open Process Components framework is currently used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

lar in philosophy and motivation to the OPC framework, and appears to be at roughly the same level of maturity. Results of these two experiments will be very useful to the software process modeling community. A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14]. The authors view the object space and the process space at different levels. The object space is data-oriented, whereas the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that brings together the object level and the process level together. Shams-Aliee and Warboys[14] also advocate modeling a process as a collection of objects or components. However, we find the distinction between the object level and the process level unnecessary. In particular, we do not agree that the object level is a data-oriented model. In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of these objects as defined by their interfaces. OPC merges objects and processes into components through an explicit representation of process state contained in the component. We propose a full object-oriented framework that includes class definitions, inheritance, and rules for component interaction. This merging of objects and processes into a complete component-based model allows OPC the full potential to achieve interoperability and reuse by being independent of any process modeling formalism.

Software Process (ICSP4). December, 1996. [3.] Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995. Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R. Towards a Reference Framework for Process Concepts. Proc. of EWSPT'92, pp. 3-17, Trondheim, Norway. September 1992. Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. Derniame, J.C. Life Cycle Process Support in PCIS. Proc. of the PCTE `94 Conference. 1994. Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994. Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proc. of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Object Management Group. jFlow Joint Submission. OMG Document Number bom/98-06-07. July 4, 1998. Osterweil, L. Software Processes are Software Too. Proc. of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987. Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings of the First World Conference on Design and Process Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for Design and Process Science, Austin, TX. December 1995. Sutton, S., Heimbigner, D., and Osterweil, L. Language Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium on Software Development Environments, Irvine, CA. December 1990. Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003, January 1995.

[4.]

[5.]

[6.]

[7.] [8.]

[9.]

[10.]

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software processes. This framework identifies common concepts in the research community and defines an object-oriented framework for applying these concepts. This framework is currently employed in the construction of a software architecture for support distributed software development. This approach, together with related efforts in the field of workflow, makes the important contribution that the software process automation field is maturing to the point that efforts such as the one described herein can be attempted. Despite whether the reader agrees with the design of this framework, providing interoperability and reusability will overcome one of the serious hurdles preventing wide scale deployment of software process automation technology.

[11.] [12.]

[13.]

[14.]

6.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proc. of EWSPT `92, Trondheim, Norway. September, 1992. Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

[15.]

[16.]

[2.]

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann
Computer Science and Engineering Department
Arizona State University
Tempe, Arizona 85287-5406
Jean-Claude Derniame
Laboratoire lorrain de Recherche en Informatique et Applications
LORIA : Bd des Aiguillettes
BP 239 54 506 Vandoeuvre Cedex
Abstract
Only recently has the research community started to consider how to make software process models interoperable
and reusable. The task is difficult. Software processes are
inherently creative and dynamic, difficult to define and
repeat at an enactable level of detail. Additionally,
interoperability and reusability have not been considered
important issues. Recent interoperability and reusability
solutions advocate the development of standard process
model representations based on common concepts or
generic schemas, which are used as a basis for translating
between heterogeneous process representations. In this
paper we propose an alternative approach through the
development of process-based components. We present the
Open Process Components Framework, a componentbased framework for software process modeling. In this
approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of
process representations, an explicit representation of process state, and an extendable set of class relationships.

1.0 Introduction
Since Osterweil‚Äôs proposal[12] for automating the software process a decade ago, there has been significant
debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets
[9], rule-based formalisms [1,8,13], process programming
languages [15], event-based representations [3,6,10],
object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research
under Award number N00014-97-1-0872

their benefits, systems based on these formalisms create
enactable process models which are not interoperable nor
reusable with one another. The prevailing solution is to
advocate an intermediary standard process representation
and provide translations for interoperability and reuse. We
do not believe this approach is scalable and defeats the
purpose of using heterogeneous process representations.
We advocate an object-oriented, component-based philosophy for providing software process interoperability and
reuse. This paper presents Open Process Components, a
component-based framework for software process definition and enactment. In this framework, components are
well-encapsulated representations of process entities that
interact in meaningful ways. The framework is solidly
founded on mature concepts in the software process field,
and yet is extendable so that process models may be customized in a particular domain. A componentized view of
process representations results in easier process definition,
modularized process enactment, natural interoperability,
and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable.
Instances vary according to constantly changing demands
of specific projects. Fully elaborating a software process
model to an enactable level of granularity is often too
tedious, time-consuming, and costly[4].
Motivated by the need for interoperability and reuse, we
advocate applying component-based techniques to software process modeling. Constructing software process
models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined
components.

A component-based approach:

2.1.2 Process Component States

‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

The elements of the meta-model appear in most process
models. Each model requires a different enactment service
to interpret the representation and execute the process.
Regardless of the formalism employed and the interpreter
used, all models define actions on the entities within the
process domain, which effect the states of those entities.
The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes
finite state machines as part of its basic abstractions.
Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for
process modeling is not unique[7,11]. The OPC framework
defines a basic set of states and transitions for Process and
Activity components. These include states such as executing, suspended, and aborted, with corresponding transitions between states defined by actions such as
startProcess, suspendProcess, and abortProcess.
Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object‚Äôs unique behavior when
interacting with other components within the framework.
The current class definition for state transition graphs
include operations to add and remove states and allowable
transitions between states, making a component‚Äôs state and
behaviors affecting state explicit and manipulable.

avoids deep integration of semantic models
handles the natural complexity of software processes,
responds to dynamic software processes, and
facilitates reuse, minimizing one-shot process models.
Component-based process modeling requires a framework
for developing components. The framework must identify
process entities, define meaningful interactions between
entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides
a foundation for developing, integrating, maintaining, and
reusing a variety of process representations. The framework defines basic abstractions of the problem space that
can be specialized. Yet, the framework must make some
commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon
three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a
per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this
meta-model as a basis for translation between process
models, we use it as a foundation for identifying elements
of the process space for componentization, and for defining
meaningful ways in which process components interact.
has_sub

role

activity
assigned_to

can_perform

has_input

product

has_output
consists_of

has_version
has_variant

agent

process

has_sub

FIGURE 1. The Open Process Components Framework
Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and
Agent), and artifacts (Product). The meta-model defines
the ‚Äúrules of engagement‚Äù for components. It identifies
what component types interact with what other component
types under what relationships. These relationships are not
static; process components and component relationships
are highly dynamic during the course of the component‚Äôs
life cycle.

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a
layered, three-tier software architecture (Figure 2). The
Framework Layer defines classes and interfaces modeling
process entities derived from the OPC meta-model. The
Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component
Layer extends representations to particular domains. It is
from this layer that actual Process component objects are
instantiated. A process model in the OPC framework is a
set of components, realized as objects of Component Layer
classes, and a set of relations between those components,
created under the constraints of the Framework Layer,
implemented using Representation Layer semantics.
Figure 2 shows example classes at each layer of abstraction
for the meta-model element Process. The Framework
Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation
Layer is comprised of class definitions for specific process
representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets
(PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any
other representations we wish to encapsulate. The Compo-

Process

Framework
Layer

Process
Representation
Layer

PDLProcess

ECAProcess

OrderedProcess
PetriNetProcess

Component
Layer
Bug Fix

RuleProcess

Code Module
Code Module

Integration Test

Design

Peer Review
Stress Test

FIGURE 2. Object-oriented class diagram for Process
components

nent Layer contains type definitions for actual process
types. The dashed lines between layers in Figure 2 denotes
that the Representation and Component Layers in fact can
have many levels. This allows for multiple ways in which
to extend and specialize the framework.
The first step identifies a base set of classes and interfaces
at the Framework Layer. The next step is to construct
encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are
encapsulated behind the interfaces inherited from the
Framework Layer. For example, the implementation may
come from a COTS process tool. Finally, components
defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used
since inheritance would tie the component‚Äôs type to its
implementation. Component Layer objects are configurable. Component Layer classes represent generic process
models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product,
Role, or Agent) fully specified and bound is part of an
instantiated process model[5].
Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given
representations. Delegating Representation Layer classes
provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between
components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi‚Äôs levels of process specialization[5].

3.0 PCIS2 Process Services
The Open Process Components framework is currently
used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

software development support. PCIS2 services include
Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA.
The process support services in PCIS2 are based on the
OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification,
known as jFlow[11], submitted to the OMG. The jFlow
specification is largely an ‚Äúobject-ization‚Äù of existing
WfMC interfaces[16]. This is not a drawback, but one of
the strengths of the OMG‚Äôs approach to adopting and
adapting existing technology. The jFlow specification
improves upon the original WAPI specifications by defining appropriate interactions between objects to gain
interoperability and maintainability of workflow systems.
The PCIS2 specification is object-oriented from the ground
up, but has borrowed some of the jFlow concepts in order
to maintain compliance with emerging standards.
PCIS2 and the jFlow specification differ in three areas.
First, PCIS2 supports dynamic processes through ad hoc
process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared
and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2
incorporates support for the metaprocess, by defining
views on its services for controlling, defining, performing,
and monitoring processes. jFlow only defines interfaces for
performing (enacting) and monitoring workflows.
It should also be pointed out that jFlow identifies concepts
not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these
specifications, they are largely complementary and both
provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The
authors propose the construction of software process components for producing process artifacts. A ‚Äúsoftware process component‚Äù is essentially a process model fragment
written in some Process Modeling Language (PML). Components are dynamically combined to construct complete
process models through interface types and their respective
‚Äúconnectors ports‚Äù. The authors correctly motivate the
need to eliminate monolithic process systems and instead
provide reuse and integration capabilities for process representations. However, the approach lacks adherence to
foundational concepts, such as those used in OPC (see
Section 2.1). The three-tier layering of the OPC framework
provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite
their differences, the Pynode component approach is simi-

lar in philosophy and motivation to the OPC framework,
and appears to be at roughly the same level of maturity.
Results of these two experiments will be very useful to the
software process modeling community.
A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14].
The authors view the object space and the process space at
different levels. The object space is data-oriented, whereas
the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that
brings together the object level and the process level
together. Shams-Aliee and Warboys[14] also advocate
modeling a process as a collection of objects or components. However, we find the distinction between the object
level and the process level unnecessary. In particular, we
do not agree that the object level is a data-oriented model.
In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of
these objects as defined by their interfaces. OPC merges
objects and processes into components through an explicit
representation of process state contained in the component.
We propose a full object-oriented framework that includes
class definitions, inheritance, and rules for component
interaction. This merging of objects and processes into a
complete component-based model allows OPC the full
potential to achieve interoperability and reuse by being
independent of any process modeling formalism.

Software Process (ICSP4). December, 1996.
[3.]

Ben-Shaul, I. and Kaiser, G. An Interoperability Model for
Process-Centered Software Engineering Environments
and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995.

[4.]

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T.,
Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the
NSF Workshop on Workflow and Process Automation in
Information Systems, Athens, GA, May, 1996.

[5.]

Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R.
Towards a Reference Framework for Process Concepts.
Proc. of EWSPT‚Äô92, pp. 3-17, Trondheim, Norway. September 1992.

[6.]

Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling
and Technology, A. Finklestein, J. Kramer, and
B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994.

[7.]

Derniame, J.C. Life Cycle Process Support in PCIS. Proc.
of the PCTE ‚Äò94 Conference. 1994.

[8.]

Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994.

[9.]

Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net
Based Software Process Modeling Language. Proc. of the
6th International Workshop on Software Specification and
Design, Como, Italy. September 1991.

[10.]

Melo, W.L. and Belkhatir, N. TEMPO: A Support for the
Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers,
North Holland. 1994.

[11.]

Object Management Group. jFlow Joint Submission.
OMG Document Number bom/98-06-07. July 4, 1998.

[12.]

Osterweil, L. Software Processes are Software Too. Proc.
of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987.

[13.]

Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the
14th International Conference on Software Engineering,
pp. 262-279. May, 1992.

[14.]

Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings
of the First World Conference on Design and Process
Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for
Design and Process Science, Austin, TX. December 1995.

[15.]

Sutton, S., Heimbigner, D., and Osterweil, L. Language
Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium
on Software Development Environments, Irvine, CA. December 1990.

[16.]

Workflow Management Coalition. The Reference Model.
WfMC Document Number TC00-1003, January 1995.

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software
processes. This framework identifies common concepts in
the research community and defines an object-oriented
framework for applying these concepts. This framework is
currently employed in the construction of a software architecture for support distributed software development.
This approach, together with related efforts in the field of
workflow, makes the important contribution that the software process automation field is maturing to the point that
efforts such as the one described herein can be attempted.
Despite whether the reader agrees with the design of this
framework, providing interoperability and reusability will
overcome one of the serious hurdles preventing wide scale
deployment of software process automation technology.

6.0 References
[1.]

Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G.
PEACE: Describing and Managing Evolving Knowledge
in Software Process. Proc. of EWSPT ‚Äò92, Trondheim,
Norway. September, 1992.

[2.]

Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified
Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

Towards Target-Level Testing and Debugging Tools for Embedded Software
Harry Koehnemann, Arizona State University
Dr. Timothy Lindquist, Arizona State University

Abstract

The current process for testing and debugging
embedded sojware is ine~ective at revealing errors. There
are currently huge costs associated with the validation of
embedded applications. Despite the huge costs, the most
dl~cult errors to reveal and locate are found extremely late
in the testing process, making them even more costly to
repm‚Äùr. This paper first presents a discussion of embedded
testing research andpractice. This discussion raises a need
to improve the existing process and tools for embe&@i
testing as well as enable better processes and tools for the
jWure. To fmilitate
this improvement, architectural and
software capabilities which support testing and &bugging
with minimal intrusion on the executing system must be
developed. Execution visibility and control must come
@om the underlying system, which should ofJer interjbces
to testing and debugging tools in the same numner it offers
them to a compiler. Finally we propose txtenswns to the
underlying system, which consists of adiiitions to both the
architecture and run-time system that will help reulize
target-level tools.
1. Introduction
Software validation involves many activities that
take place throughout
the lifecycle
of soft w are
development.
A substantial
portion
of the validation
process is software testing, which is the development
of
test procedures and the generation and execution of test
eases.
Notice we are not only concerned
with the
generation of a test case, but are also concerned with how
that test is executed. Therefore, a test case is not simply
composed of inputs to a system, but rdso includes any
environmental
factors.
Other research has examined the
issues behind test case selection, but few are addressing the
problems that surround the execution of those test cases.
The goal of this paper is to identify
the problems
associated with test case execution for embedded systems
and to propose solutions for making embedded testing more
effective at revealing errors.
1.1

Testing
and Debugging
Process
Many of the activities, tools, and methods used
during software testing are shared by software debugging.
Software testing is concerned with executing a piece of
software in order to reveal errors, while software debugging
is concerned with locating and correcting the cause of an
error once it has been revealed.
Though
these two
activities are often referenced separately, their ac$ivi$ies are
tightly coupled and share many common features.
Permission to copy without
fee all or part of this material is granted
provided that the copies are not made or distributed
for direct commercial
advantage, the ACM copyright notice and the title of the publication
and its
date appear, and notice is given that copying
is by permission
of the
Association
for Computing
Machinery.
To copy otherwise
or republish,
requires a fee and/or specific permiss~m.

During debugging, a developer must recreate the
exact execution scenario that revealed the fault during
testing.
Not only must the code execute
the same
instruction sequences, but all environmental
variants must
be accounted for during the debugging session. In addition,
the tools assisting
in the debugging
process
must
providing a &veloper
with a certain degree of execution
visibility
and control while not impacting
the execution
behavior of the program.
1.2

Embedded
Systems
The testing and debugging
process is greatly
restricted by embedded systems. Embedded applications are
among the most
complex
software
systems
being
developed today. Such software is often constrained by
‚óè Concaumnt designs
‚óè RcaI-time
constraints
‚óè l%lbedded
target Imvilrmments
‚óè Distributed
hardware ambitectures
‚óè Device control dependencies
Each of these properties of embedded software severely
restrict
execution
visibility
and
control,
which
conseqmdy
restricts the testing and debugging process.
Our current methods and tools for software testing and
debugging require a great deal of computing
resources.
Such resources are not available on the target environment.
Therefore, a large gap exists between the methods and tools
used during evaluation on the host and those used on the
target. Unfortunately,
mauy errors are only revealed during
testing in the target environment.
Because of the above issues, concerns are raised
over the effectiveness of software validation for embedded
systems.
Embedded
applications
are responsible
for
controlling physical devices and their correct execution is
critical in avoiding and/or recovetig
from device failure.
Often these physical devices control life-critical
processes,
making the embedded software a key element of a lifecritical system. Software failure can lead to system failure,
which in turn could lead to loss of life.
In addition, designers are increasing their use of
embedded software to conuol the physical elements of large
systems. This rate of increase is likely to increase as the
cost for embedded controllers becomes cheaper and more
attractive when compared with other mechanical techniques.
Computer
networks
are fast replacing
point-to-point
wiring,
due to the networks
light
weight,
easy
cotilgurability
and expansibility,
and lower
design
complexity.
The advancement
in the complexity
of
problems
addressed
by software
in these types of

01993

ACM

0-89791-621

-2/93/0009--0288

1.50

applications
may soon be limited
satisfy reliability needs and concerns.

by our inability

to

2. Software
Testing
The software testing phase is concerned with
executing a software program in order to reveal errors.
Software testing for embedded systems takes place in four
basic stages:
1) Module Level Testing
2) Integration Testing
3) System Testing
4) Hardware/Software
Integmtion Testing
The first three testing stages are typical of any software
product.
Testing begins with exercising each soft ware
module and concludes when the software is shown to meet
system specifications
by passing some rigorous
set of
system tests. The fourth phase is unique to embedded
systems. The software must not only be correct, but must
also interface properly with the devices it is controlling.
Testing
literature
contains
countless
methodologies,
techniques,
and tools that support the
software
testing process.
They range from software
verification
and program proving
to random test case
selection.
All testing
methods
indirectly
apply
to
embedded systems, as they do all software.
Of particular
interest to this paper are those techniques that address the
problems identified for embedded software - concurrency,
real-time
constraints,
embedded
environment,
etc.
Unfortunately,
there exists little research into the unique
with testing
embedded
software.
problems SSSOCilltd

techniques, must examine a large set of statw and therefore
must constrain itself to small, simple programs.
Research in dynamic testing of concurrent Ada
programs has largely focused on the detection of deadlocks
~emb85],
the saving of event
histories
KeDo85,
Maug85],
and other tec.huiques that passively watch a
program execute then allow the execution sequences to be
replayed after a failure has been detected.
Hanson Eans78] was among the first to discuss
run-time
control of concment
programs.
In order to
regulate
the sequences of events, he assigned each
concurrent event in the test program a unique time value.
He then introduced a test clock that regulated the system
during execution. A given event could only execute if it‚Äôs
time was greater than that of the clock.
Tai ~ai86, Tai91] extended Hanson‚Äôs work to the
Ada programnn ‚Äúng language.
His method takes an Ada
program P and a rendezvous ordering R and produces a new
Ada program P‚Äô such that the intertask communication
in
P‚Äô is always R. A similar approach was used in Koeh89]
to apply these techniques to testing and debugging tools.
This work addressed the facl that in order to,
testa specific
program state, values in a program
may need to be
modified during run-time.
Modification
o~f the program
state is a capability provided by any debugging tool and is
a required property of a tool debugging tasked programs. It
is important to note that both techniques explicitly perform
rendezvous scheduling, removing those decisions from the
run-time system and placing control in the hands of the
tool.

2.1

2.2

Testing
Concurrent
Systems
Concurrency increases the difficulty
of software
testing,
Given a concurrent program and some set of
input, there exists an unmanageably
large set of legal
execution sequences the program will take. Furthermore,
subsequent execution
with the same input may yield
different,
yet correct results due to differences
in the
operating environment.
This is all complicated
by Ada‚Äôs
nondetermins tic select construct. Therefore, when testing
concurrent software, we are not only concerned with a valid
resuk but must also be concerned with how the program
arrived at that result.
Since multiple executions of a concurrent program
may yield different results, it is not enough to ensure that
the system produces the correct output for a given input.
One must also ensure that the system always produces an
acceptable output for each execution sequence that is legal
under the language definition.
Without sufficient control
over program execution,
there is no way of ensuring a
given test is exerasing the code it was intended to test.
Taylor and Osterweil
~ay180]
examined static
analysis of concument programs,
However, this research
considered processes in isolation and does not consider
interprocess cmrummication.
Taylor later extended this
work to Ada and a subset of the Ada rendezvous mechanism
~ay183]. Through this static aualysis technique, one could
determine aIl parallel actions and states that could block a
task from executing.
This method, as with most static

Non-intrusive
testing
Intrusion plays a significant role in the testing and
debugging of embedded software. Any technique used to
raise execution visibility
or provide for program control
must not interfere with the behavior of the teat program.
Embedded applications have strict timing requirements and
any intrusion on a test execution will likely make that test
void.
Intrusion
is typical for host-based
testing, but
becomes a large problem
for target-level
testing and
debugging activities.
The above approaches
address the need for
visibility,
control, and predictability
for testing concurrent
software.
However,
they are all intrusive
and use
instrumentation
(inserting probes into a usem program and
rewriting
certain constructs before submission
to the
compiler) to gather run-time information
and to control
~gram
exmtion.
After the probes are added, the user‚Äôs
object code is linked with the rest of the debugging system
and then executed under test. This additional
code has a
serious impact on the execution behavior of the program.
Instrumentation
is not appropriate
for testing real-time,
embedded applications.
A non-intrusive
debugger for Ada is proposed in
[Gil188]. A separate processor executes the testing system
and communicates with the target processor through some
special purpose hardware.
Lyttle and Ford ~ytt90]
have
also implemented a non-intrusive
embedded debugger for
Ada. Their tool provides monitoring,
breakpoints,
and

289

of hardware and the run-time system is called upon to
bridge the impending gap. No argument is made as to the
rate of increase identified
by the line slopes; nor is an
argument ma& that these increases are even linear.

display facilities
for executing embedded applications.
While these efforts provide an excellent start towards targetlevel tools, they do have severe limitations.
These
implementation
do not deal with high level activities such
as task interactions and are only concerned with items that
can be translated from monitoring
system bus activity. As
discussed later in Chapter 5, techniques dependent on bus
activity will likely fail for future architecture designs. In
addition,
many of the error detwted
in the target
environment are indeed concerned with high-level activities
(process scheduling
and interactions,
fault handling,
interrupt response, etc.).
Other real-time,
embedded
tools have been
proposed for crossdevelopment
environments.
They can
typically
be classified into one of the following
three
categories: 1) ROM monitors, 2) Emulators, and 3) Bus
monitors.
These types of tools will be further discussed
later in this paper.

c
o
m
P
1
e
x

i~
t

Y

constructs

Hardware

Time
Figure 2.1

2.3 Impact

of the Underlying
System
One of the large problems with testing concurrent
systems
is dealing
with
abstraction.
The Ada
programming
language
abstracts concurrent
activities
through task objects ~D83].
Tasks allow a developer to
abstract the concepts of concurrency
and interprocess
cxmmmnication
and discuss them at a high level. The
burden of implementation
is then placed on the compiler,
and typically the run-time system.
While abstraction
is a powerful
design tool, it
leads to significant
prthlems
during the testing phase of
software development.
Implementation
details become
buried in the underlying system. At the development level,
this high degree of abstraction is appropriate.
However,
abstraction complicates the testing process. Not only are
we concerned with implementation
details, but we must
aIso control them to demonstrate that certain properties
about a program will hold for every legal
execution
scenario.
Without
sufficient
control
over program
execution, there is no way of ensuring that a spedc
test is
exerasing the code it was intended to evaluate. In addition,
cmmxt operation
in one environment
(host) does not
necessarily imply comet operation in another (target) due
to implementation
difference in the underlying system.
The underlying
system is composed of two parts,
the features of the hardware architecture and the operations
provided by the run-time system. As language constructs
become more abstract, compilers are required to generate
more code to implement them. There is no longer a trivial
mapping from language construct to machine instruction.
Rather, the compiler must provide an algorithmic
solution
in order to implement these high level constructs. Those
solutions
exist as operations
in the run-time
system.
Rather than generate code for these constructs, the compiler
generates a call to a run-time system operation or servim.
As the constmcts
become
more
abstract,
compilers
develop
an increasing
dependency
on the
underlying
system. This increase in shown in figure 2.1.
As new constructs
are introduced
to programming
languages, their increase in abstraction is greater than that

L=

Language

Embedded
systems raise many problems
for
software testing and debugging.
Such software typically
must deal with concurrency,
real-time
constraints,
au
embedded
target environment,
distributed
hardware
architectures,
and a great deal of hardware-software
interfaces for controlling
externaI devices. These issues
tdone do not provide a complete view of the problems
SyStetUS are
extcotttttered by embedded testing. bbedded
typically developed on custom hardware configurations
m@ng
that each system introduces
it‚Äôs own unique
problems. Tools and techniques that apply to one are not
generally applicable on another, which leads to ad hoc
approaches to integration and system testing of embedded
software. The program is executed for some length of time
and continual y bombarded with inputs in an attempt to
show it adheres to some speeifkation,
3.1

Current
state of embedded
testing
As described
earlier, the testing
process for
embedded systems consists of 4 phases that conclude with
Hardware/Software
(H/S) Integration
During
H/S
integration
testing, device and timing related errors are
reveakd. These errors eneompass problems such as:
‚óè incorrect
handling of interrupts
‚óè distributed
communication problems
‚óè incorrect
ordering of concumen t events
‚óè resource contention
‚óè incorrect
use of device protocols and timing
‚Äú incomect response to failures or transients
These errors are often extremely difficult
problems to fix
and often require significant modifications
to the software
system.
In addition,
software is for~d
to conform
to
custom hardware that may itself have errors. As stated
above, H/S integration
is the last phase of testing for an
embedded system. Since errors are much cheaper to fix the
earlier they are revealed,. why would one wait until the last
phase of product development to reveal the most diftlcult to

290

locate, costly errors to fix? Our goal should be to reveal
these errors as early as possible.
Unfortunately,
target
level testing tools have yet to become a reality.
The target processor of au embedded computer is
typically minimal in function and size. It is only a small
portion of a larger system, whose goals are to minimize
cost and space. Therefore, target hardware of au embedded
systems will not support software development
nor any
development tools. To resolve this problem, the source is
developed on a larger host platform and cross axnpilers and
linkers are used to generate code and download it to the
target processor.
Consequently,
two environments
exist in our
development process, the host environment
and the target
environment,
each having
completely
different
functionality
and interface to a user. Tools that run on the
host provide a high level interface and give users detailed
information
on and control over their program execution.
However, little is provided on the target. Typically,
the
best information
obtainable is a low-level execution trace
provided by an in-circuit emulator.

3.2

Current
Solutions
Approaches to dealing with the above problems
can be divided
into hardware
solution
and software
solutions. The hardware solutions are attempts at gaining
execution visibility
and program control and include the
bus monitors, ROM monitors, and in-circuit emulators. A
bus monitor gains visibility
of an executing program by
observing
data and instructions
transferred
across the
system bus. With a ROM monitor,
debugger code is
placed into ROM on the target board. When a break point
is encountered,
control is transfered to the debug code
which can accept commands from the user to examine and
change the program‚Äôs state. Finally, an in-circuit emulator
connects with a host system across an ethernet connection.
At the other end, a probe replaces the processor on the
target board. The emulator then simulates the behavior of
the processor in (ideally)
real-time,
which allows the
emulator to tell the outaide world what it‚Äôs doing while it‚Äôs
doing it.
The
hardware
solutions
have
mini
m al
effectiveness for software development.
They can only
gather information
based on low-level machine data. The
developer must then create the mapping between low-level
system eventa and the entities defiied in the program. That
-ping
is the implementation
strategy chosen by a given
compilation
system and becomes severely complicated for
abstractions such as tasks Maintaining
an understanding of
the mapping is extremely difficult and cumbersome.
The software solutions cart be viewed as attempts
to reduce the tremendous costs of testing on the target.
Several factors determine how a pitxe of software is tested
1) Level of criticality of software module
Each software module is assigned a different level of
criticality
based on it‚Äôs importance
to the overall
operation of the system.
2) Test platform availability

Typically,
there will exist several test environments
available to test a piece of soft ware, each providing
a
closer approximation
to the actual target environment:
‚óè Host-baaed sours
level debugger
‚óè Host-based
instruction set simulator
‚óè Target emulator
‚óè Integrated
validation faality
3) Test Classification
The tests to be performed
can be categorized
to
determine what they are attempting
to demonstrate.
The goal of a test plays a large role in determining
the
platform on which it will execute. Some examples are
shown below.
‚óè Algorithmic
‚óè Inter-module
‚óè Intra-module
‚óè Performance
‚óè HE
integration
‚óè InttX-cabinet
Each of these factors play a role in assigning program
modules to the various test platforms
based on some
criteria that might contain the following:
‚óè Type of software
‚óè Hardware requirements
‚óè Test chssifkation
‚óè Platform
availability
‚óè Coverage requirements
‚óè Test support software
availability (drivers, stubs)
‚óè Certification
Requirements
‚óè Level of effort required for test
This criteria takes into account the 3 factors discussed
above as well as additional ones.
The software solutions are an attempt to minimize
the time spent testing
in the target
environment.
Validation facilities are expensive to build and time utilimd
for testing is expensive. This is due to the f;act that target
level testing occurs extremely late in the development
lifecycle and only a small window is allocated for HAS
integration
testing.
However,
the target is the only
location that can reveal tin
errors. It is ironic that our
current solutions attempt to reduce the anmunt of target
testing, but will likely lead to extensive modifications
and
thercfom extensive retesting.

4.

Problems with Embedded Irestinq
The solutions proposedaboveare not effective at

revealing errors. Effective implies that a technique reveals
a high percentage of the errors and that it does so in a costef!iaent manner. Instead, what the above tools provide is a
minimal, low-level view of the execution of a program and
those tools become available
at a very late stage in
development.
Below is a list of problems associated with
current approaches to embedded testing
4.1

Expense
of Testing
Process
Target
testing
requires
expensive,
custom
validation facilities.
The expense of these target facilities
is incurred for every project, since little
reuse across
projects is ever realized. The effort required to build these
validation
facilities
means that every test execution is
expensive, making retests extremely costly. Yet, hardware
often arrives late and full of errors, forcing software to be

291

modified
and subsequently retested.
This late arrival of
hardware also impacts the cost of an error, since certain
errom are only revealed during I-IN integrations testing.
Perhaps the largest factor associated with the high
costs of testing will be the questions and concerns that
certification processes are beginning to raise about sofhvare
tools. Typically, development tools have not been required
to meet any validation
criteria and certairdy not the strict
criteria imposed on the development system. This luxury
may soon disappear
as the role tools play in the
development
process comes under tighter scrutiny.
The
huge expense of validation
facilities
will
increase
dmmaticauy.

4.2 Level

of Functionality
on Target
The level of functionality
found on a target
machine is minimal and does not support tools. This lack
of functionality
greatly limits the effectiveness of testing,
since more time and effort is required to locate an error.
While a host system provides a high-level
interface and
discussed software in terms of the high-level language, the
target typically deals in machine instructions and physical
addresses. Translating these low-level entities requires time
and a great deal of tedious, error-prone activities.

4.3

Errora
revealed
late
in
development
lifecycle
Embedded
system designs
often incorporate
custom ASIC parta that are typicaIly not available until
very late in the development
process,
delaying
the
availability
of any target validation facility.
In addition,
errors designed into the ASICa are extremely expensive to
fix, requiring
new masks be created and complete
refabrication.
InsteaA errors in ASICs and other hardware
problems are resolved by modifying
the software.
As
stated before, this greatly delays the time which errors are
revealed, which in turn increasing the cost of software
testing.
4.4

Poor
teat selection
criteria
AIl to often, tool availability
diclates the quality
of a testing process.
Tests cases and scenarios
are
determined by what will work on available platforms and
which test are achedulable rather than being determined by
some theoretical
test cxitcria.
A prime example is the
FAA‚Äôs requirements ~AAS51 that 1) all testing be done in
the target environment
and 2) testing include statement
coverage.
Of course, test coverage is not currently
measured on the target.
Unfortunately,
it is cheaper for a company
to
spend it‚Äôs resources preparing an argument to obtain some
form of ‚Äòwaiver‚Äù than to actually perform a test. In time,
the argument approach will no longer be accepted and the
solutions
for embedded
testing must be in place to
accommodate
this
change.
It will
only take one
implementation
that performs statement coverage on the
target to force every embedded, real-time software developer
to perform statement coverage on the target to meet such a
certification requirement.

4.S Potential
use in advancing
architectures
Perhaps the largest problem
facing embedded
testing is that the current solutions cannot be applied to
future h~dware
architectures.
Future architectures
are
Proposing
‚óè wider addreas Spaces
‚óè higher -Sor
speeds
‚óè huge numbers of pins
‚óè internal
pipes
‚óè multiple
execution units
‚óè large internal
caches
‚óè multi-dip
modules
Such complexities
cast a dark shadow over the hardware
solutions previously discussed. With internal caching and
parallel activity being done on the chip, one will no longer
be able to gain processor state information
from simply
monitoring
the system bus. And as on-chip functions
become more complex, emulator vendors will no longer be
able to see into the chip through the pins making them
obsolete as well.
In [Chi191] an even stronger claim is made that
the debugging capabilities provided by the chip will need to
become more sophisticated. In future architectures, perhaps
the only possibility
to view and control the execution of
hardware is to gain that information
from the hardware
itself.

The previous
sections raised issues about the
effectiveness of our testing process and claimed that tcating
is currently being limited by tool functionality.
l%e goal
of this paper is to identify shortcomings in the embedded
testing proccas and propose a solution to those problems.
The view taken by the authors is that tool support for
embedded systems is lacking. Further, those approaches
currently used for gaining execution visibility
and control
will soon be obsolete for future architectures.
We propose
adding facilities to the underlying system to better support
testing and debugging tools for embedded software.
As stated previously,
the underlying
system is
composed of the hardware architecture and the run-time
system (RTS). Both are composed of data structures and
operations that implement
common system abstractions
such as processes, semaphores,
ports, timers, memory
heaps, and faultdexceptions.
It should be noted that there
is no distinct line between features of hardware and features
of the RTS. In fact, as these features and abstractions
become more standardized,
newer architectures
are
attempting to incorporate
them into their instruction
sets
~Nl%92].
In addition,
the implementation,of
a feature
may span parts of the architecture,
RTS, and compiler
generated code (i.e. fauhdexceptions).
5.1

Model
Debugging
System
Below is an illustration
of a debugging
system
(Figure 5.1). The data path from the debugging/testing
tool represents symbol table information
that allows the
tool to map machine level information
to source level

292

Test/Debug
Compiler
Generated
Code

\

Tool

1

A
e
x
t
e

3

Ada
Compilation

‚Äús-

Figure 5.1
constmcts.
The
ASIS
toolkit
provides
easy
required for this physical connection.
The
implementation
for this facility.
ASIS is a proposed
sw-tions describe ‚Äòtie architecture
additions
standard interface between an Ada library and any tool
interfaces in more detail.
requiring compilation information.
Of more interest is the communication
path
between the target processor and the testing tool. A tool
sits external to the rest of the embedded system, while the
RTS resides internally on the target board. At frost glance,
this conceptual
path seems rather difficult
to realize.
However, the implementation
becomes easier if thought
about as a typical host debugging system. Any debugging
system has a least two processes executing, one running
the test program and one running the &bugga.
These two
p=
Sa common physical machine, which allows
one process to gain information
about the other.
The
debugger procem simple requires data and computation
time, which it shares with the test program.
This same scenario is rcqnired
for embedded
debugging, except that the debugger process is split. Part
of the debugger process runs on the target machine and part
runs on the host. The goal is to minimize the portion that
must be run on the target so that it does not intrude on
execution
of the test program.
To realize this nonintrusive
execution
of the debug software,
the target
1) Execute debug code only at a break poinL
2) Run the debugger as a separate process, or
3) Provide a separate execution unit to execute the
debugger.
The details of these options are explored in depth later in
this paper.
The problem now lies with the interfa=
between
the embedded part of the debugger (intermddebuggcr)
and
the portion that lies on the host (external-debugger).
The
solution requires hardware additions that will be discussed
later in this paper. A high level view is given in figure
5.2. In this figure, the tool makes logical calls to services
provided by the RTS. These calls are actually implemented
by the debugging system through data passed between the
internal and external debuggers.
Hardware additions arc

1

I

next two
and RTS

I

Figure 5.2

-=

The past decade has seen hug{: advances in
microprocessor
designs.
Several of these advancements
were listed previously and include pipelining
and separate
functional
units.
The concept
of partitioning
a
microprocessor in order to perform parallel activities is of
great interest to this work.
It was noted
earlier
that these parallel
amputations
severely restrict current methods for testing
and debugging embedded systems, since on{e must simulate
a great amount of computations.
However,
debugging
tools can also use architectural
parallelism
to their
advantage. If a hardware design is partitioned successfully
to allow certain activities to occur concurrently,
then the
testing and debugging methodologies
might wish to add

293

their own computational
requirements to the list of parallel
activities.
This section will explore additions to hardware
architectures.
No claim is made as to the costs associated
with these features.
They assured y will require space
(transistors) and possibly even add to the execution cycles
required to implement certain instructions.
6.1 Hardware
Partitioning
of Memory
One primary concern for industry is reducing the
huge volume of retests associated with development.
The
current testing process ensures that errors are revealed late,
which forces retesting
large portions
of the system.
Despite correcting
these problems, industry will still be
faced with software that is constantly changing. Software
is deceivingly
easy to change and often the element of a
system assigned to unknown or ‚Äúrisky‚Äù aspects during
design.
Changing software is extremely expensive late in
the development
for critical
systems.
Such systems
typically
have requirement
that an error raised in one
portion of the system won‚Äôt interfere
with the correct
operation
of the rest of the system.
Current software
certification
agencies ~AA85]
have several software
restricdons including
‚óè Any modikation
made to a software module forces the
retesting of all other modules operating on that same
physical device.
. All software on a device must be developed under the
highest level of criticality of any module that will
execute on the same device.
Without
the ability
of hardware to guarantee software
boundaries, such requirements must be enforced. However,
these requirements
add a great deal of costs to software
development.
Consequently,
software is often physically
partitioned
based on critical level, rather than design
factors.
Partitioning
software modules based on critical
levels greatly interferes
with the design process.
One
would rather partition
modules based on factors such as
processor utilization
and inter-module
exmnmnication
requirements.
In fact, load balancing and p17XXsSmigration
are techniques
that would not be usable by embedded
system developers unless all software is developed at the
highest critical level.
The solution
to these issues is hardware
partitioning.
Each process should have it‚Äôs own protected
address space that is not accessible by any other process.
In addition, sets of processes may wish to share memory.
The processor should tdso provide the capability to restrict
access to segments of memory based on some criteria.
6.2 Computational
Facilities
for
Debugger.
The debugging
system is partitioned
into an
internal debugger and au external debugger. The internal
debugger must physically
exist on the target board and
communicate
with the external debugger through some
dedicated medium. The internal debugger will also require
execution from the target without
interfering
with the

operation of the application
program.
There are two
possible scenarios
‚óè The internal
debugger runs as a regular process on the
-Or
The architecture provides separate facilities to execute
the internal debugger code
In either case, control is transfered to the debugger when a
breakpoint is encountered
In the fiit
scemuio, the debugger is executed by
the processor as any other process.
If the debugger
executes as a low-level process, it would not interfere with
the operation of the rest of the system. However, this is
not a feasible approach.
Most intern-sting errors occur
during peak system loads, which would mean that the
debugger could only execute when the probability
of an
error occurring was low. Another approach wouId be to
execute the internal debugger as a periodic process of high
priority and design the entire system to take this process
into account when determining issues such as scheduling.
The second scenario requires the target processor
to provide some form of computational
facilities.
This
extra execution will certainly
require some amount of
utilization
of architecture
resources such as internal
registers and bus accesses.
The simplest
example of
architecture facilities would be a machine that contirtuaU y
dumps some representation of the instruction it is currently
executing.
This would require a dedicated bus to the
external world (proposed later in this section) and that
additional circuitry be attached to the computation units to
gain access to the current instruction.
The problem
with fis
approach
is that the
processor is not aware of what data is required by the tools
at the other end. Therefore, it must dump everything.
At
high processor speeds, the amount of information
being
sent could become overwhelming.
However, the data could
be faltered and then captured so that a tool could parse it
later and recreate an execution history of the program. The
hardware required for filtering
is not trivial and requires
great speed and storage capaaty to maintain pace with the
target processor.
lle next step is to allow software to dictate the
information
sent by the processor. The functional
unit of
the hardware sed.ing messagea could be implemented
as a
state machine, emitting
different
messages based on its
current state. The default state would be all processor
transactions. Basically, in this eontiguration,
the processor
is performing
the filtering
rather than the external
debugger.
This addition
should
not add much in
complexity to the hardware architecture and would greatly
reduee the wmplexity
of the external debugging hardware.
The final step is to take the (now
stateful)
functional
unit and make it programmable.
Instead of a
state machine, it now becomes a complete functional
unit
within the processor itself.
The internal debugger code
would then be loaded into this portion of the prowssor at
boot time and reside there for the entire execution,
transmitting
and receiving
messages to and from the
external debugger.
‚óè

294

6.3 Hardware
Break
Points
Software break points are intrusive and require
instructions be inserted into the code of the test program.
Conditioned
break points present a more significant
problem, since they require a computation every time they
are encountered to determine if the proper conditions are
met to halt execution.
Such breakpoints are unacceptable
for d-time
programs.
To resolve this issue, architectures need to provide
the capability
to set breakpointa in hardware.
A set of
registers would be classified
as BreakPoint
Registers
(BPR), which the processor would check against the
operands for each instruction.
Two types of breakpoints
are required, data and instruction.
Each data BPRs inside
the processor would be compared with the address of every
data operand for each instruction.
Instruction BPRs would
be compared with instruction
addresses or type. When a
match occurs, a breakpoint
fault would be raised and
control trsnafered to the internal debugger
Upon returning
from a break, the processor is
required
to restart execution
precisely
where it had
terminated.
The state of the processor consists of all it‚Äôs
internal registers, including
any pipeline information
and
cache memory. These values must be saved automatically
when a break is encountered.
Another issues is that of conditional breakpoints.
Such breakpoints
require computations
by the processor
that run in the background behind the program under test.
The evaluation of the conditional expression must begin far
enough in advance so that it may complete before the
processor has passed the breakpoint
location.
This
evaluation will require memory accesses, raising additional
problems. The current value of operands in the expressions
must be available to the processor, which might involve
accessing it from memory or cache. Any accesses to
memory must be scheduled in such a manner that they do
not block any resources required by the program under test.
F@dly, the value used must be valid and not in danger of
-g

before the breakpoint.
While the problems raised above seem difficult,
they are not insurmountable.
The extend
debugger must
compile the conditional expression and download the code.
At that point it can determine the scheduktbili~
of this
evaluation by comparing the @e for the conditional to the
other code that will occur in parallel. The user could then
be notified of problems with their additional
breakpoint.
The hardware is responsible for detecting any collisions in
parallel activity
and must not assume the debugger is
always accurate. Any debugger activity intruding
on the
behavior of the test program is important information
and
must be flagged by the processor.
The primary additions required for hardware break
points are additional registers from the architecture and the
logic necessary to compare them with the operands of the
current instruction.
To support conditional
breakpoints,
the processor must provide background
computational
support.
This support could come from a portion of the
processor dedicated to conditional breakpoints, or the code

could be downloaded to the internal debugger, given the
internal debugger support described previously.
6.4 Architectural
Support
for Abstractions
As common programming
paradigms
become
more refined, architectures will begin to inax-porate them
into their instruction
sets. It would be unlikely
that the
only abstractions supported by architectures would remain
simple data types (integer,
real) and their associated
operations (add, subtract, convert). Other abstractions such
as processes,
semaphores,
ports,
timers,
memory
management,
and faults
that are found
in typical
applications
should be supported as well, along with
associated operations on those abstractions.
M&ing
hardware to another level of abstraction
provides huge advantages for testing tools.
As stated
earlier, the architecture must be the basis for emulation
capabilities
and providing
execution visibility.
As the
hardware becomes more aware of programming elements, it
gains the abdity to send more meaningful messages to the
external world. A context switch between processes could
be sent with a single message, rather than the hundreds of
machine instructions it takes to implement the switch.
As the processor becom-es the single point of
visibility,
awareness of the progrdng
environment
becomes important.
A processor with a high-level
understanding
of program ,entities can emit fewer, more
meaningful
- messag-es than a processor
that only
comprehends low-level instructions.
6.5 Dedicated
Bus
Embedded
testing and debugging
require
an
interface that aliows the processor to communicate with the
external world without interfering with the behavior of the
system under test. This physical connection should reside
on the target and interface
extemall y tlhrough
some
detachable
mechanism.
The separation
technique
is
important,
since the external debugging system will be
detached from this connection once the system is placed
into operation.
The execution behavior alf the program
should be independent of whether or not any external tool
is attached.
Assuming an adequate physical connection,
the
next detmminah ‚Äúon is the protocol across it. ‚ÄòIle following
issues must be addressed
1) At what rate will messrwes need to be sent?
‚ÄòProcessor speed raises i~teresting problems, since future
speeds might be too quick for external
processing
techniques.
A solution to this problem was discussed
previously where the processor became aware of highlevel program elements.
The goal is to decrease the
number of messages required relative to the number of
machine cycles. 2) How much data is associated with a message?
If an architecture is required to emit large volumes of data
for messages, there may be instanms where the processor
must be suspended to allow the internal
debugging
hardware to catch up to the current processor state.
Higher level messages may compound
the problem,
since more maningful
messages might require more

295

This paper does not address the question of how
these interfaces should be UtdiZSd. Such SllSWerS should be
given by methodologies
and techniques for detecting and
locating
errors in embedded,
real-time
systems.
As
discussed earlier, the lack of these methods has led to
difficulties
for determining
adequate RTS services for
testing and debugging tools, which has forced a different
approach to determine the required operations.
Since the
RTS is in essence offering au implementation
of high-level
abstractions,
services that provide
visibility
into the
implementation
of RTS abstractions should adequately
fidfdl the needs of most testing and debugging techniques.
A standard currently exists for implementing
these
abstractions
in the MRTSI
[ARTE89]
and CIFO
[ARTE91].
In addition, most of the needs for testing and
debugging can be fulfiiled
by these standards. This is not
surprising, since our solution is based on implementation
visibility,
and the MRTSI and CEO are providing
an
implementation
interface. However, it is important to note
that this approach also indicates that implementations
that
support these staudards should require minimal additions to
and debugging tools as prOpOSed
by
akw SUppCWt testing
this paper. Below is a small discussion surrounding each
of these abstractions
and a list of shortcomings
in the
MRTSI and CIFO for testing and debugging.

information.
There is likely a tradeoff between message
level and data volume.
3) Is the connection bidirectional?
Visibility
concerns dictate that state information
travel
However,
methods
requiring
out of the processor.
control
of the executing
program require that state
information
travel the other direction.
Protoczds must be
in place to handle contention across the bus and those
must be extremely well defined, due to the extreme data
rate that could will be emmuntered across the bus.
4) Who is the active element in sending message9?
Either the processor or the RTS must determine the
information
sent from the processor.
The processor
cannot provide all the state information needed, while the
RTS will likely not be able to maintain adequate speeds
for sending messages.
These questions play a role in determining
the interface
between the internal and external debuggers.
A likely
solution would be a master-slave relation, where either the
internal or external debugger regulated the other. This
scenario does not seem likely, since each has such critical
processing concerns. Therefore, each will likely execute
independently,
while communication
is handled via some
bus and protocol.
There does exist a master-slave relationship
in
respect to the bus, howevex.
During program executiw,
the internal
debugger
must ‚Äòownn the bus, since it‚Äôs
processing concerns are the greatest.
It must meet the
message sending deadlines without altering computations
in other parts of the system.
There are points dting
execution where the extend
debugger must aeiz cmtrol.
If the internal debugger cannot allocate the bus to meet the
demands of the extcmal debuggm, the user must be notifkd
that their requested operation
cannot be accomplished
during a real-time execution.
The final determination
is that of the active
element
within
the processor.
There are two basic
approaches to detemnining control of the internal debugging
activities.
In the fiit
the processor is active and becomes
responsible for sending messages to the extend debugger.
‚Äòfhesecondappmachuse
aaspecialdebugge
rportionofthe
RTS to emit messagea, which is loaded into a dedicated
functional
unit within
the architecture.
Tools require
information
maintained
by both the architecture and the
RTS. Perhaps the solution lies between the two where
both the RTS and architecture have the ability to dump
messages, depending on the cmrcnt mquiremcnts dictated by
the external tool.

7.1

Processes
Concurrency
is a common abstraction
used in
embedded systems. A design can be decomposed without
concern for computational
resources, which can then be
determined
by a scheduler
during
run-time.
A&
irqplements concurren cy through tasks and task types. The
CIFO and MRTSI
provide
extensive
tasking
support
includlng
identifieation,
creation
and activation,
communication
through rendezvous, concurmat access to
shared entities,
and support for scheduling
control.
Elements of interest that are not provided by the CIFO or
MRTSI include
‚óè Task State - A developer
must have the ability to query
and modify the task state for each task in their system.
However, a modification
could leave the RTS in an
inanaistent
state. For example, changing a task‚Äôs
state from ‚Äúdelaying‚Äù to %unning‚Äù without removing
it from the &lay queue would place the RTS into a
state that could not be achieved
through
normal
execution. However, the same modification
ability is
available on typical debugging systems and should be
offered by em beddcddebwrgera as Wd.
‚óè Commm&ation
and Synchronization
- A developer
must have the ability tb view and modify eaeh entry
queue to determine the concurrent state of the system.
Again, modifications
could leave the RTS in an
unobtainable state.
‚óè Scheduling
Control
- In addition
to the extensive
operations provided by the CIFO for concurrency
control, a developer must have awess to the dispatch
port (or ports for muhiprqxssor
systems).

7. Run-Time
Svs tern Additions
The RTS requirements
deseribe an interface
between a tool and the underlying
system.
This is a
logical interface requiring substantial hardware support as
outlined
above.
An obvious goal is to minimize
the
required data and computational requhements of the internal
debugger as well as the required communications
between
the internal and external debuggers.

7.2

296

Interrupt

Management

One of our criticism of the current approach to
embedded testing is that timing errors are revealed late in
Interrupts are very related to
the development
process.
timing issues and their correctness is an important element
in embedded testing. Therefore, support for interrupts is
extremely
important
to target testing and debugging.
Faalities
provided through the CIFO and MRTSI would
allow developers
to bind various
interrupt
handling
routines, enable and disable certain interrupts, mask and
unmask interrupts,
and generate software interrupts
all
controlled dynamically duting program a program test.
7.3

Time
Management
As stated earlier,
important‚Äù to target testing
target tools require sfilaertt
time. Tools must be allowed
(although
such modifications
results) and the delay Iist of
by the RTS.

timing
issues are extremely
and debugging.
Therefore,
control over issues relating to
to view and modify the clock
might produce undefined
waiting processes maintained

7.4 Memory

Management
Dynamic
memory
is not typically
used by
due to diffldtk%
in dcmonstradng
embedded ti@iC4itiOliS
reliability.
However,
future
systems
will
likely
incorporate
algorithms
that requite dynamic storage. In
addition, memory ~agemcnt
for dynamic allocations is
part of a RTS and should therefore be included in RTS
visibility
and control discussions.
A tool will likely
require that ability to demonstrate an application programs
behavior when memory is exhausted.
The MRTSI would need to be extended to provide
operations that mim
a collection
making it smaller to
show execution behavior when memory is exhausted or
larger to demonstrate correct execution should a collection
be expanded by the developer.
Resizing is not cheap and
could require a gnat deal of computation and data transfers,
depending on an implementation.
7.5

Exception/Fault
Handling
Proper handling of exceptional events is evaluated
during hardwaresoftware
integration
testing. Therefore,
tools require a great deal of cattrol over exceptions and
One must be able to raise an
recovery mechanisms.
exception
or fault during program execution and also
modify handler binding during execution.
Another question of interest might be to locate the
handler for a given fault or exception at a given program
location.
Such information
is not easily gained from the
underlying
system.
The compiler
is responsible
for
handling exception propagation [ARTE89], so &k
‚Äú ‚Äú g
the handler from only RTS information
might be an
impossibility
and is at best resolved uniquely for each
compilation
system.

architectural and RTS additions. The architectural additions
will certainly be costly in both time and space, requiring
space (transistors)
on the chip and access to internal
registers and busses that could cause contention and slow
the execution
of other instructions
provided
by the
architecture.
However, the RTS additions
are minimal.
We defined
the needs of testing
as making
the
implementation
details of common system abstractions
visible and then determined the functionality
required to
view and control them.
The ARTEWG‚ÄôS
MRTSI
and
CIFO provided an outstanding basis for this approach.
The RTS additions are admittedly
weak.
Our
initial goal was to have the methodologies
and techniques
used for testing embedded, real-time
systems drive the
operations
required
by the RTS.
Unfort.tmatcl y, such
methods do not yet exist. As stated earlier, testing and
debugging of embedded, real-time software remains a black
art, with ad hoc methods and techniques. While there has
been much research into the concurrency and distribution
issues, none has examined real-time constraints, embedded
environments,
and other issues relating
to embedded
systems Perhaps the MRTSI and CIFO are sufficient
for
implementing
target level testing and debugging
tools.
However, this question cannot fully be resolved until more
formal methods exist.
Our next step is to evaluate the additions
and
determine their feasibility.
Questions relating the cost of
these additions to au architecture and RTS in terms of time
and space must be answered.
Also, a more complete
mapping should exist between the added feattues and the
impact they have on the desired features.
One can then
make a valid comparison between a feature and the costs
associated with it.
‚Äòl‚Äôhe embedded contmllermark~
is currently huge,
but has only begun to require the computational
powers
~SOCiKltti With lUiCrOpKXXWOrS. Embedded i@k.i3tiOttS
have traditional
been event driven rather than computation
dependent. Due to their light weigh~ easy con@mbility
and expansibility,
and lower design complexity,
computers
are quickly being chosen over mechanical techniques for
controlling
devices. As this transition continues, the size
and complexity
of embedded
programs
will
grow.
Controllers will not only have strict timing requirements,
but also have significant
computational
needs as well.
This combination
requires new approaches to our current
testing process for embedded systems and therefore, more
effective tools to aid in testing and debugging embedded
applications.
References

[ARTES9]

Ada Run-time
Environment
Working
Oroup,
‚ÄúA Model
Run-Time
System
Interface for A&m Ada Letters, January,
1989.

[ARTE91]

Ada Run-time
Environment
Working
Group, ‚ÄúCatslogue of Interface Features

s

co nclusions
The goal of this paper is two fold.

The first goal
is to identify defkienaes
in embedded system testing and
raise questions about the future of current tools.
The
second is to propose a solution to these problems through

297

and Options
for the Ada Runtime
Environment,‚Äù
Special Edition of Ada
Letters, Fall 1991 (fI).

lyxn6J

Tai, K.C., ‚Äú&producing
Testing of Ada
Tasking Programs,‚Äù IEEB Transactions
on Software Engineering,
1986.

[CHIL91]

Child,
Jeffrey,
‚Äú32-bit
Emulators
Struggle with Processor Complexities,n
Computer Design, May 1,1991.

~A191]

DD83]

Department
of Defense,
Reference
Manual
for the Ada Programming
Language,
ANSI/MIL-STD1815a,
United States DoD, 1983.

Tai, K.C., Carver, R.H., and Obaid,
E.E., ‚ÄúDebugging
Concurrent
Ada
Programs by Deterministic
Execution,‚Äù
IEEE
Transactions
on
Software
Engineering, January, 1991.

~AYL80]

Taylor,
R.N.
and Osterweil,
L. J.,
‚ÄúAnomaly
Detection
in Concurrent
Software by Static Data Flow Analysis,‚Äù
IEEE
Transactions
on
Software
Engineering, May, 1980.

~AYIJ33]

Taylor,
R.N., ‚ÄúA General
Purpose
Algorithm
for Analyzing
Concurrent
Programs,‚Äù
Communications
of the
ACM, ~y,
1983.

Federal Aviation
Association,
Software
Consideration
in Airlx)me
Systems and
Equipment
Certification,
RTCA/DO178A, 1985.
[GILL88]

Gilles, Jeff aud Ford, Ray, ‚ÄúA Guided
Tour Through
a Window
Oriented
Debugging Environment
for Embedded
Real Time
Ada
Systems,‚Äù
IEEE
Transactions
on Software Engineering,
1988.

&IAm78]

Hansen, B., ~eproduable
Testing of
Monitors,‚Äù
Software-practice
and
Experience, Volume 8,1978.
Hembold,
D. and Luckham,
D.,
‚ÄúDebugging
Ada Tasking
Rograms,‚Äù
IEEE software, March, 1985.

Iw‚ÄôfJ=l

Intel Corporation,
i960
Architecture
programmer‚Äôs
Manual, 1993.

KOEH91]

Koehnemann,
H.E. and LindquisL
T.E.,
‚ÄúRuntime Control of Ada Rendezvous for
Testing and ~U@llg,‚Äù
Procedm - gs of
the 24th Hawaii International Conference
on System Sciences, Volume II, 1991.

Extended
Reference

LeDoux, C, and Parker, D.S., ‚ÄúSaving
Traces for Ada Debugging,‚Äù
Ada in Use
Proceedings
of the Paris Conference,
1985.
Km]

Lyttle, D. and Ford, R., ‚ÄúA Symbolic
Debugger for Red-Time Embedded Ada
Software,‚Äù
Software
- Practice
and
Experience, May 1990.

@fAUG851

Mauger, C. and Pammett K., ‚ÄúAn EventDriven Debugger for Ati‚Äù
Ada in Use:
Proceedings
of the Paris Conference,
1985.

298

INDUSTRY TRENDS

Moving Java into Mobile Phones
George Lawton

Telecom are now selling Java phones. And, said Ben Wang, manager of systems development for Sprint PCS, 80 percent of the new phones the company sells will be Java enabled after the big rollout next month. Nokia alone plans to ship 50 million Java phones this year and 100 million next year. In fact, 15 handset makers either are or soon will be selling 50 models of Java phones.

Advantages

A

s mobile technology matures, handheld-device vendors are looking for ways to make their products more functional, and Java is one approach they are turning to. This is particularly the case with smart cellular phones, which are using Java to help add new capabilities. In smart phones, Java functions as a layer between the operating system and the hardware, or runs parallel to the OS within a separate chip. In the past, the key constraint to running Java on mobile devices has been their processing, memory, and powerconsumption limitations. However, new mobile hardware and software developments are reducing these limitations. Thus, industry observers expect Java use in mobile devices, which is already supported by many vendors, to explode during the coming years. Nick Jones, a fellow at Gartner Inc., a market research firm, said Java will become a de facto standard on midrange and high-end cellular phones. He predicted that at least 80 percent of mobile phones will support Java by 2006, although some may also run on other technologies, such as Microsoft's Pocket PC operating system. According to Jones, mobile-device manufacturers' desire for an aftermarket is driving interest in Java as a mechanism for easily adding software to devices. Java also permits applications to work across platforms. This is important in the mobile-phone market,

which features many platforms. However, questions about Java's performance and a dearth of Java-based applications for cellular phones, particularly in Europe and the US, remain as obstacles to the technology's widespread adoption in mobile devices.

DRIVING JAVA USE IN HANDHELDS
Work on Java-enabled handheld devices began several years ago, but completion of the Java 2 Platform Mobile Edition (J2ME) and support from device vendors and cellularphone-service providers have driven the recent level of interest, explained Eric Chu, Sun Microsystems' group product manager for industry marketing.

Adoption levels
Korea's LG Telecom in became the first service provider to deploy Java in September 2000. Since then, users have deployed between 18 million and 20 million Java-enabled telephones, said Sun spokesperson Marie Domingo. Companies such as Nextel in the US, NTT DoCoMo in Japan, and British

According to Sun's Chu, one of Java's major benefits for cellular phones is support for packet-based networks running TCP/IP. Using TCP/IP makes it easier to write applications that communicate directly with the phone, rather than relying on an intermediate technology such as the wireless application protocol (WAP). Also, Chu said, Java, unlike WAP, supports pictures and colors. In addition, he explained, the Java environment provides good security because it includes a sandbox that limits downloaded code's access to the rest of a host system. Moreover, Java's ability to work with different platforms is important in the fragmented cellular-phone market. This capability lets a Java-enabled phone run applications and services written for other mobile platforms and also lets software vendors save time and money by writing a single, Java-based version of an application to run on multiple platforms. And Java-enabled phones and servers could communicate directly with each other, thereby enhancing interactive applications. Java enables smart-phone users to download applications directly from the Internet. Similarly, Java lets users download Java applets that customize their devices in various ways, such as with special ring tones or improved caller ID. This lets users get new features more easily. In the past, users had to buy new phones, run new applications remotely using WAP, or download programs first downloaded to a PC. Meanwhile, there are many Java developers, which makes it easier for
June 2002

17

I n d u s t r y Tr e n d s

Java 2 Platform Micro Edition (J2ME)

Optional packages Optional packages Java 2 Platform Enterprise Edition (J2EE) Java 2 Platform Standard Edition (J2SE) Personal basis profile Personal profile MIDP CLDC KVM
Source: Sun Microsystems

Foundation profile CDC JVM

Figure 1. Sun's three primary Java platforms are each designed primarily to run on a different type of machine. The Java 2 Platform Enterprise Edition is designed for servers; the Java 2 Platform Standard Edition for workstations, PCs, and laptops; and the Java 2 Platform Micro Edition for PDAs, smart cellular phones, and other smaller systems. J2EE and J2SE use the full Java virtual machine (JVM). J2ME also works with the slimmed-down K virtual machine (KVM), the connected limited device configuration (CLDC), and the mobile information device profile (MIDP).

Other approaches help Java technologies designed for larger computers work on mobile devices. For example, SavaJe developed the SavaJe OS, which supports Java applications in a mobile environment by optimizing J2SE libraries for common mobile CPUs. Mathew Catino, SavaJe's cofounder and vice president of marketing, said Java applications typically spend 80 to 90 percent of their time executing the libraries. Therefore, he explained, optimizing the libraries enables applications to run 10 to 20 times faster. Zeosoft has developed ZeoSphere Developer, which permits the creation of mobile applications that support Enterprise Java Beans, Sun's Java-based software-component architecture. This could simplify the development of complex enterprise applications that communicate and run across servers (via J2EE), PCs (via J2SE), and mobile devices (via J2ME).

Software development tools
Application developers can use existing tools to create Java programs for handheld devices by limiting their code to libraries and APIs supported by J2ME. However, J2ME includes only a limited number of development libraries, noted Jacob Christfort, chief technology officer of Oracle's Mobile Division. Also, said Gartner's Jones, enterprises might shy away from J2ME because of the poor user interface designed for small device screens, the primitive threading model, and minimal native data-handling facilities. In essence, he explained, the design approach that lets J2ME work on small devices sometimes makes it inappropriate for large-scale enterprise uses. To address these concerns, several vendors have released or will soon release development toolkits or toolkit extensions to help developers more easily meet enterprise applications' needs. The new approaches include Sun's Forte for Java Programming Tools, the Oracle 9i Application Server

vendors of Java-enabled mobile devices to find people to write their software.

MAKING JAVA WORK IN HANDHELDS
Sun, which designed and manages development of Java, is in the forefront of making the technology work in handheld devices. However, other vendors have also become active in this area.

Sun Microsystems
Sun and a group of partners created J2ME to make Java work on smaller devices. J2ME includes some core Java instructions and APIs but runs more easily on small devices because it has a smaller footprint than the Java 2 Platform Standard Edition (J2SE) or Enterprise Edition (J2EE), shown in Figure 1, and has only those features relevant for the targeted devices. For example, J2ME's graphics and database-access capabilities are less sophisticated.
18
Computer

J2ME generally incorporates the connected limited device configuration (CLDC), which is implemented on top of operating systems and serves as an interface between the OS and Javabased applications. The CLDC generally uses the K virtual machine (KVM), a slimmed-down, less-functional version of the Java virtual machine (JVM) for small devices. The J2ME mobile information device profile (MIDP) sits on top of the CLDC and provides a set of APIs that define how mobile phones will interface with applications.

Other vendors
Several vendors besides Sun are creating Java-based technologies for handheld devices. Hewlett-Packard makes the MicroChaiVM (http://www.hp. com/products1/embedded/products/dev tools/microchai_vm.html), a cloned JVM that doesn't have Sun's licensing fees and usage restrictions. Several vendors, including Ericsson and HP, plan to use MicroChaiVM-based phones.

Wireless architecture toolkit, and the Sprint PCS Wireless Toolkit. Because of J2ME's shortcomings, Jones said, corporate applications will probably be based on the larger-footprint J2SE as mobile devices get more processing power. Regardless, said John Montgomery, product manager with Microsoft's .NET Development Group, current Java tools are too primitive and difficult to use for most developers.

ETM9 interface Instruction TCM interface Instruction cache Memory management unit ARM9EJ-S core Data TCM interface Data cache Memory management unit Write buffer Control logic and bus interface unit

Server-side handheld Java
Another Java-enabling approach would link handheld devices to Java applications and services on servers. AT&T Wireless, BEA Systems, IBM, Nokia, NTT DoCoMo, Sun, and other companies have created the Java-based Open Mobile Architecture for linking cellular phones and servers. The project would augment J2EE, designed primarily for servers, so that it would support standards that mobile devices can use with Internet-based information. The standards include XHTML (for displaying Web pages on mobile devices), SyncML (for synchronizing data between mobile devices and other machines), WAP 2.0 (to access Internet content and services), and the multimedia messaging service (for handheld messaging).

A R M96EJ- S

Coprocessor interface

AHB interface Instruction Data

Source: ARM Ltd.

Figure 2. ARM Ltd.'s ARM926EJ-S chip includes the company's Jazelle technology in its ARM9EJ-S Java-enabled processor core. In addition, the chip includes separate ETM (embedded trace macrocell), data TCM (tightly coupled memory), and AHB (advanced high-performance bus) interfaces.

IMPLEMENTATION IN HARDWARE AND SOFTWARE
Java technology can be implemented in software or in hardware on either a specialized Java acceleration chip or a core within the main processor. Software implementations tend to run less efficiently because systems must translate each Java instruction into native instructions that the CPU can run. Separate hardware chips are more efficient but represent additional device components and cost. Java cores integrate some of both approaches.

Components Group, said his company has developed techniques for speeding up the software process, which used to bog down when the CPU switched from instructions it could accelerate to instructions it couldn't. In addition, Intel and other software-based Java proponents say the latest mobile processors can run Java fast enough to compete with hardware-based approaches. Analyst Markus Levy with MicroDesign Resources, a semiconductorindustry research firm, disagreed. He said, "People are spending a lot of energy fine-tuning the software-based approaches. For some people that may be good enough, but if you really want the most efficient implementation you need a hardware-based approach."

Software approach
In the software approach, a device's CPU runs the Java code. David Rogers, marketing manager for Intel's PCA

Java hardware
Companies such as ARC Cores, ARM Ltd., Aurora VLSI, Digital Communications Technologies, inSili-

con, and Zucotto Wireless are developing hardware that runs Java, either as Java coprocessing cores for integration into CPUs or as stand-alone Java chips. Both hardware-based approaches promise to increase Java-based application performance and, by running more efficiently, reduce power demands on battery-dependent cellular phones. Different companies' chips execute different subsets of the Java instructions. For example, ARM's Jazelle chip, shown in Figure 2, executes about 68.2 percent of all possible Java instructions, while Aurora's DeCaf runs about 95 percent. Running a bigger set of Java instructions provides more functionality but makes a chip cost more and consume more power. Joan Pendleton, Aurora's cofounder and chief architect, said there are two classes of acceleration. The first, used by most vendors, translates Java byteJune 2002

19

I n d u s t r y Tr e n d s

code into native processor instructions. The second directly executes Java bytecode, which offers better performance but requires a larger footprint because of the additional circuitry necessary to run the software in hardware. Levy predicted that Java cores will be more popular than stand-alone Java processors. This approach's primary constraint is that developers must use a system-on-chip approach to create their products. Putting multiple functions on a chip is more expensive to develop, but the elimination of additional chips reduces device costs. Standalone Java chips are less expensive to design but lead to higher device costs.

performance across platforms. Levy has thus launched a Java-processor group within the Embedded Microprocessor Benchmark Consortium (http://www.eembc.org/). The group expects to release its first benchmark by next month.

"We are still in a phase of market confusion and have not yet gotten to a state of market consolidation," Jones explained.

Not enough applications
There are currently some mobileJava applications, including games and weather and traffic maps. However, Jones said, there are not enough desirable mobile-Java applications yet. The reason is not the technology, he said, but instead the lack of an effective business model and a commercial infrastructure that would enable developers to profit from their work.

A

CONCERNS AND CHALLENGES
Mobile Java is still a relatively new technology. Many industry watchers say the technology has kinks that still need to be worked out. For example, Gartner's Jones expressed concern about vendors' differing Java implementations. He said some developers are complaining about having to manually optimize their Java games for different cellular phones. And although there are many Java developers, there are fewer who have experience working with J2ME and writing code for small, resource-constrained devices. Overall, said Microsoft's Montgomery, "J2ME is an interesting set of engineering compromises, but I would argue exactly the wrong set of compromises. It is too big for the smallest devices but too small to have the features you want on the smartest devices."

Industry observers say mobile Java still has kinks that must be worked out.
The growth of publishing intermediaries that would certify and sell mobile-Java software may eliminate this problem.

HANDHELDS AND THE FUTURE OF JAVA
Jones said Java is doing well on back-end servers because Java-based applications can easily be redeployed as companies buy new servers. However, he noted, client-side Java use has faded considerably because many enterprise-application developers turned to Visual Basic to work within the corporate environment, which is typically Microsoft-based. Thus, the battle for the mobile platform is important to Sun. However, Sun's Java initiatives for cellular phones are facing stiff competition from various sources, including Microsoft's wireless efforts, the Symbian operating system, Linux, and Qualcomm's binary runtime environment for wireless (http://www.qualcomm.com/ brew/).

ccording to Jones, J2ME will attract more application developers as it becomes a richer and less constrained environment. A survey by Evans Data, a market research firm, found that wireless developers who have used Java expect to use the technology a bit more in 2003 than they will this year. Java will also become even more attractive as smart phones get more processing power and vendors design better APIs for color screens, higher quality sound, intellectual-property protection, and user-location capabilities, he added. However, he cautioned, these extra features would give vendors more opportunity to create their own Java implementations, which could fragment the application-development environment. Sprint PCS's Wang said the initial focus of mobile Java will be on games, multimedia, and ring tones. Over time, Levy added, Java will become a de facto standard built into smart phones. SavaJe's Catino predicted that Microsoft and Java-based technologies are likely to coexist in phones during the coming years. Third-party vendors could help this process by developing software-integration techniques that would combine the two environments in devices. I

Performance
Jones said that mobile Java can be somewhat slow because the KVM is not particularly fast. However, he added, the KVM should become faster in the future, particularly as phones with more memory can run just-in-time compiler technology, which enhances performance. "In five years," he said, "[performance] will be a nonissue." Another problem, said Levy, is a lack of standards to objectively measure
20
Computer

George Lawton is a freelance technology writer based in Brisbane, California. Contact him at glawton@ glawton.com.

Editor: Lee Garber, Computer, 10662 Los Vaqueros Circle, PO Box 3014, Los Alamitos, CA 90720-1314; l.garber@computer.org

ARTICLE IN PRESS

The Journal of Systems and Software xxx (2004) xxx‚Äìxxx
www.elsevier.com/locate/jss

Automated support for service-based software development
and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a
Department of Computer Science and Engineering, Arizona State University‚Äì‚ÄìMain, P.O. Box 875406, Tempe, AZ 85287-5406, USA
Department of Electronics and Computer Engineering Technology, Arizona State University‚Äì‚ÄìEast 7001 E, Williams Field Road, Building 50,
Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003

Abstract
A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can
be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on
available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications.
√ì 2003 Published by Elsevier Inc.

1. Introduction
A service-based development paradigm, or services
model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can
interact with one another and be providers or consumers
of data and behavior. Some of the deÔ¨Åning characteristics of service-based technologies include modularity,
availability, description, implementation-independence,
and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon
the deÔ¨Ånition of the interface needed to access a service
(description) while hiding the details of its implementation (implementation-independence). Since the client
and service are decoupled, other concerns such as side
eÔ¨Äects become non-factors (modularity). One of the
potential beneÔ¨Åts of using a service-based approach for
developing software is that at any given time, a wide
variety of alternatives may be available that meet the
needs of a given client (availability). As a result, any or
all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach
for the creation of services and their subsequent integration with service-requesting client applications. The
technique utilizes an architecture description language
to describe services and achieves run-time integration
using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and
involves the automatic generation of ‚Äò‚Äòglue‚Äô‚Äô code for
both services and applications. The Jini interconnection
technology (Edwards, 1999) is used as a broker for
facilitating service registration, lookup, and integration
at runtime.
The remainder of this paper is organized as follows.
Section 2 describes background material in the areas
of software architecture and the middleware technology we are using to enable dynamic integration (i.e.
Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work,
and Section 5 draws conclusions and suggests further
investigations.

2. Background
*

Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751.
E-mail address: gannod@asu.edu (G.C. Gannod).
0164-1212/$ - see front matter √ì 2003 Published by Elsevier Inc.
doi:10.1016/j.jss.2003.05.002

This section describes background material on software architecture and Jini.

ARTICLE IN PRESS
2

G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

2.1. Software architecture

3. Approach

A software architecture describes the overall organization of a software system in terms of its constituent
elements, including computational units and their
interrelationships (Shaw and Garlan, 1996). In general,
an architecture is deÔ¨Åned as a conÔ¨Åguration of components and connectors. A component is an encapsulation
of a computational unit and has an interface (e.g. port)
that speciÔ¨Åes the capabilities that the component can
provide.
Connectors encapsulate the ways that components
interact. A connector is speciÔ¨Åed by the type of the
connector, the roles deÔ¨Åned by the connector type, and
the constraints imposed on the roles of the connector. A
connector deÔ¨Ånes a set of roles for the participants of the
interaction speciÔ¨Åed by the connector. Components are
connected by attaching their ports to the roles of connectors.
Another important concept is an architectural style.
An architectural style deÔ¨Ånes patterns and semantic
constraints on a conÔ¨Åguration of components and connectors. As such, a style can deÔ¨Åne a set or family of
systems that share common architectural semantics
(Medvidovic and Taylor, 1997).

This section describes the service-based development
approach including the techniques used for deÔ¨Åning
services, specifying client applications, realizing integration, and generating glue code.

2.2. Jini
The primary enabling feature of the work described
in this paper is the existence of Jini (Edwards, 1999) for
the delivery and management of services. In a typical
Jini network, services are provided by devices that are
connected to the network. A Jini technology layer provides distributed system services for activities such as
discovery, lookup, remote event management, transaction
management, service registration, and service leasing.
When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is
registered, a proxy (Gamma et al., 1995) is stored by the
lookup service. The proxy can later be transported to
the clients of the service. Other network members can
discover the availability of the service via the lookup
service. When a client application Ô¨Ånds an appropriate
device, the lookup service sets up the connection. In our
approach to component integration, we use Jini to
provide a standard method for registering and connecting a client to corresponding software components
that are acting as services.
One of the advantages of using this Jini-based integration technique is that it facilitates construction of
applications ‚Äò‚Äòon-the-Ô¨Çy‚Äô‚Äô whereby components can be
used on an as-needed basis. One of the disadvantages is
that clients of services must have some prior knowledge
about how to use each respective service.

3.1. Example
Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on
the health of systems in a network. This application
utilizes a network sniÔ¨Äer service and a port monitoring
service. The network sniÔ¨Äer service gives an administrator information about traÔ¨Éc on the network. The
port monitoring service provides information about the
open ports on the various machines on a network. Together, these services facilitate determining whether
certain kinds of attacks (such as ping storms) are being
directed to a machine or machines. The client application supports analysis of several networks, each of
which is accessed using the buttons shown on the top
portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services
that utilize diÔ¨Äerent models of execution (strict call return and data streams). The remainder of this section
refers to architectural speciÔ¨Åcations that were used in the
construction of this example.
3.2. Overview
The methodology that we have developed follows
closely the model suggested by Stal (2002) for web ser-

Fig. 1. Running example.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

vices, although the technology that we are using to
realize our approach is Jini. The approach itself focuses
on two concerns with respect to software reuse. That is,
it addresses both for reuse and with reuse concerns. With
respect to for reuse, the approach involves the construction of services via the use of adapter and proxy
synthesis. SpeciÔ¨Åcally, the methodology involves two
steps for creating services as follows: (1) speciÔ¨Åcation of
components as services, and (2) generation of services
using proxies via the construction of appropriate
adapters and glue code. These services are consequently
registered and made available on a network.
With respect to with reuse concerns, the approach
involves the construction of applications using services
as follows: (1) speciÔ¨Åcation of a client to make use of
services from a repository or network, (2) generation of
the client (both manual construction of client application speciÔ¨Åc code and automated generation of glue
code), and (3) execution of the client, including integration of the speciÔ¨Åed services at runtime.
Within our approach, a user (e.g. developer) is
responsible for writing the source code for the client
application along with the speciÔ¨Åcation of the architecture for a client. Among other things, the client speciÔ¨Åcation contains a description of the basic services that
the client application will need in order to be a complete
system. All other source code, including code necessary
to realize the connections between the client and employed services, is generated based on the speciÔ¨Åcations
describing clients, services, and connectors.
3.3. Service generation
In this section we describe some of the issues related
to automating the creation of service wrappers. To
support these activities, we have developed an automated tool that takes as input a software architecture
and produces glue code. A primary source of reusable
components that we employ in our approach are legacy
command-line applications (Gannod et al., 2000). In
order to generate services from legacy components, we
take the approach of wrapping the components by utilizing the interface provided by the component. Since
command-line applications have a well-deÔ¨Åned input
and output interface, the interface of the application as a
service can be based entirely upon the knowledge of
what the application intends to provide.
3.3.1. SpeciÔ¨Åcation and synthesis
The concept of using an adapter for wrapping legacy
software is not a new one (Gamma et al., 1995). As a
migration strategy, component wrapping has many
beneÔ¨Åts in terms of re-engineering including a reduction
in the amount of new code that must be created and a
reduction in the amount of existing code that must be
rewritten.

3

In regards to wrapping components, our approach
uses two steps. First, a speciÔ¨Åcation of the legacy software as an architectural component is created. These
speciÔ¨Åcations provide vital information that is required
to deÔ¨Åne the interface to the legacy software. Second,
the appropriate adapter source code is synthesized based
on the speciÔ¨Åcation.
3.3.2. SpeciÔ¨Åcation requirements
To aid in the development of an appropriate scheme
for the wrapping activity, we deÔ¨Åned the following
requirements upon speciÔ¨Åcations. These requirements
are as follows: (S1) a suÔ¨Écient amount of information
should be captured in the interface speciÔ¨Åcation in order
to minimize the amount of source code that must be
manually constructed, (S2) a speciÔ¨Åcation of the interface of the adapted component should be as loosely
coupled as possible from the target implementation
language, and (S3) the speciÔ¨Åcation of the adapted
component should be usable within a more general
architectural context.
The requirement S1 addresses the fact that we are
interested in gaining a beneÔ¨Åt from reusing legacy software. As a consequence, we must avoid modifying the
source code of the legacy software. At the same time, we
must provide an interface that is suÔ¨Écient for use by a
target application. To provide that interface, a suÔ¨Écient
amount of information is needed in order to automatically construct the adapter.
Our selection of command-line applications addresses
the modiÔ¨Åcation concern of requirement S1 since source
code is not available. As such, we are required to provide an interface that is based solely on the knowledge of
how the application is used rather than how it works.
Table 1 shows the properties used in the speciÔ¨Åcation
of services, clients and connectors. A service component speciÔ¨Åcation consists of two parts: properties and
ports. The properties section describes style of the service, while the ports section describes functions provided
by the service. In addition, the service speciÔ¨Åcations
indicate style-based information as well as conditions
or commands that need to be true or executed, respectively, in order to establish an environment necessary to
use the service. Finally, a key in terms of a ‚Äò‚Äòservice
type‚Äô‚Äô (e.g. interface property) is used to support a service lookup, which is later utilized during application
integration.
The requirement S2 (i.e. the decoupling of a speciÔ¨Åcation from a target implementation language) is based
on the desire to apply the synthesis approach to a variety
of target languages and implementations. In addition,
this requirement facilitates enforcement of requirement
S1 by ensuring that new source code is not artiÔ¨Åcially
embedded in the speciÔ¨Åcation. While satisfying this
requirement is ideal, we found in our strategy that a
certain amount of implementation dependence was

ARTICLE IN PRESS
4

G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

Table 1
Properties
Group

Attribute

Description

Service properties

Component-Type

Architectural style this component adheres to

Service port properties

Signature
Return
Cmd
Pre
Post
Interface
Path
Port-Type
Shared-GUI

The port‚Äôs signature
The port‚Äôs return type
The command-line program being wrapped
Pre-processing command
Post-processing command
The generic interface implemented by this port
Path to the wrapped command-line program
The port‚Äôs type based on the Component-Type
Boolean indicating shared (true) or exclusive (false) GUI

Client properties

Part-of-client
GUI-CodeFile
Component-Type
Shared-GUI

IdentiÔ¨Åes inclusion in client application
The Ô¨Ålename for client‚Äôs GUI code
Architectural style this component adheres to
Boolean indicating shared (true) or exclusive (false) GUI

Client port properties

Port-Type
Interface

The port‚Äôs type based on the Component-Type
The generic interface that this port can bind with

Connector properties
Connector role

Connector-Type
Prop-type

Architectural style this connector adheres to
The connectors role based on the Connector-Type

necessary due to the fact that our implementation would
make use of Jini.
When a component has been wrapped using our
technique, an interface is deÔ¨Åned that facilitates the use
of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also
desirable to be able to use the speciÔ¨Åcation of the
adapted component within a more general architectural
context. That is, it is advantageous to be able to use the
speciÔ¨Åcation as part of the software architecture speciÔ¨Åcation for new systems. In using a content-rich speciÔ¨Åcation, where interfaces are deÔ¨Åned explicitly, the
added beneÔ¨Åt of providing information that can be
integrated into an architectural speciÔ¨Åcation of a target
application is gained.
In order to realize the requirements placed upon desired interface speciÔ¨Åcations for legacy software wrappers, we used the ACME (Garlan et al., 1997)
architecture description language (ADL). SpeciÔ¨Åcally,
we used the properties section of the ACME ADL to
specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an
ADL that has been used for high-level architectural
speciÔ¨Åcation and interchange (Garlan et al., 1997).
3.3.3. Synthesis
As stated earlier, the class of legacy systems that we
are considering are command-line applications (Gannod
et al., 2000). Given this constraint, we make the
assumption that any client applications utilizing the
wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We Ô¨Ånd this assumption to be reasonable due to
the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage proÔ¨Åles.
In our approach, the speciÔ¨Åcation that is needed to
generate wrappers contains properties associated with
the ports as shown in Fig. 2. These properties include
Signature, Command, Pre, Post, Path, Interface, and
Return. In this case, the speciÔ¨Åcation describes the
NetworkSniÔ¨Éng and PortMonitor services, which are
services created by wrapping tcpdump, and nmap,
respectively. In the synthesis process, ACME speciÔ¨Åca-

Fig. 2. ACME services section.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

tions are combined with a standard template that
implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we
have constructed to automate this process generates the
appropriate source code for facilitating interaction between a potential client and the wrapped component. At
present, this is an automated tool that generates fully
executable code for the wrapped application and does
not require the user to modify or write any new code
outside of option GUI code.
Both the service and client synthesis steps utilize a
template-based approach to synthesize code. That is, a
standard Ô¨Åle has been created that has stubs containing
place holders that must be instantiated with either service or client speciÔ¨Åc parameters. Fig. 3 contains a
portion of the ServiceTemplate Ô¨Åle which contains all of
the application and service independent source code and
provides the routines necessary to integrate the legacy
code into a Jini network. SpeciÔ¨Åcally, the ServiceTemplate contains functions that implement the discover and
join protocol for registering a service with the lookup
service. The ServiceTemplate also contains tags that are
place-holders for the automatically generated functions.
For instance, in Fig. 3 the tag <put-ServerName> is
a place-holder for the Ô¨Ånal name of the adapter component.
In addition to the ServiceTemplate, there is also a
reusable set of functions that can be utilized in an
interface speciÔ¨Åcation and consequently in the generated
wrappers. For instance, the getOutputStream( )
routine (shown in Fig. 4) is available as a function for
use within the Java code to provide standard stream
input support.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

5

The amount of automation that has been achieved
through the approach described above is dependent on
the degree of graphical user interface (GUI) support
that is desired. For a service, the code synthesis step can
be fully automated if no GUI support is desired.
Otherwise, the amount of manual code construction is
limited to GUI support.
3.4. Client generation
Once the services are generated and stored in a
repository, a client application can be architected. First
we need to specify the client application taking into
account the architectural style of each of the services.
Once a client is speciÔ¨Åed, it can be veriÔ¨Åed and generated. In this subsection we look at the requirements for
specifying the client and then describe synthesis of the
client.
3.4.1. SpeciÔ¨Åcation
Refer again to Table 1 which, in addition to the
properties for service speciÔ¨Åcations, contains the properties of client application components and connectors.
When dealing with integration at the component level,
two issues arise (among others) that are of interest. First,
the problem of architectural style mismatch (Shaw and
Garlan, 1996) occurs when the underlying assumptions
made by components conÔ¨Çict. Second, most modern
applications provide a graphical user interface (GUI). As
a result, integration of oÔ¨Ä-the-shelf components can
leverage these user interfaces in order to take advantage
of previously built technology. To cope with these issues
we impose two requirements on the speciÔ¨Åcation of client
applications as follows: (C1) the speciÔ¨Åcation of the
components should capture the notion of architectural
style so that the high-level interaction between clients
and services can be veriÔ¨Åed, and (C2) the speciÔ¨Åcation
must facilitate the use of shared and exclusive GUI
components.
The requirement C1 addresses the fact that a component must provide a notion of architectural style. A
component‚Äôs style plays a very important role when it
interacts with other components by imposing interaction
constraints. Using a basic style attribute (by name)
architectural mismatches can be determined by simple
keyword matching.
Requirement C2 addresses the fact that a service may
provide a GUI that allows a user to access and control
the service. In this context, there may be GUI components provided by services that are either sharable by
other services or exclusive to the service. A sharable
GUI component can be used by both the client as well as
other integrated services while an exclusive GUI component can only be used by the service that provides the
interface.

ARTICLE IN PRESS
6

G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

3.4.2. Synthesis
The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample speciÔ¨Åcation of a client. The information contained within
client speciÔ¨Åcations are used to support the synthesis of
client code. This synthesis step utilizes two features; Ô¨Årst,
the information regarding connectors and attachments,
such as those shown in Fig. 5 are used to determine the
relationships between client applications and desired
services. Second, information regarding GUIs provided
by services is used to determine how to realize the GUI
in a client application.
In our framework, the wrappers for the various services can implement a common interface that allows the
client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially
used across multiple services and are identiÔ¨Åed using a
name taken from a standard GUI vocabulary (for
example ‚Äò‚ÄòResultsWindow‚Äô‚Äô). The name is then used to
identify which GUI components can be shared across
services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive
component is independent and cannot be shared between services. The exclusive GUI components of the
wrappers are used as is but may interact with one or
more of the shared components. For both shared and
exclusive components, the interaction with the client
GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers.
3.5. Discussion
As stated in Section 1, the service-oriented domain
are characterized by modularity, availability, description, implementation-independence, and publication. As
a result, services and service-based approaches are more
coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a
software architecture to specify applications that operate under these characteristics. As such, a software
architecture in this context deÔ¨Ånes components, their
interfaces, and the mechanisms by which services (as
components) can be joined in order to fulÔ¨Åll needed
software behavior. Consequently, services enable the use
of a software architecture as an integration vehicle in
which the architecture facilitates generation of glue
code. It is the very fact that services adhere to the
characteristics described above that the integration and
code generation become possible at this level. However,
the approach does lack in its ability to address needs
that are more speciÔ¨Åc than what individual services
provide. To cope with this, we are developing an approach that allows for the creation of federated services,
where services are combined to meet some higher-level
objective.

4. Related work

Fig. 5. Portion of ACME client speciÔ¨Åcation.

Recently, the use of web services has gained attention
with vendors releasing webservices toolkits that allow
for building and using webservices. Webservices and
.NET (Meyer, 2001) are based on the SOAP and XML
(Seely and Sharkey, 2001) protocols. The Jini approach
to service integration goes beyond what the webservices
paradigm provides by deÔ¨Åning how services can be used
within a larger application context and providing support for code transportation.
FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server
that distributed messages to other tools that were
interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this
approach is a precursor to service-based development.
Urnes and Graham (1999) describe an approach to
facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the
component space across a network. In our approach,
services are potentially developed by diÔ¨Äerent organizations and thus the choice of what to distribute is not

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx‚Äìxxx

available. The component model being addressed by
Urnes and Graham, as such, is Ô¨Åner-grained and violates implementation-independence, a tenet of servicebased development.
Grundy et al. (2000) discuss issues and experiences in
constructing component-based software engineering
environments. They created a variety of useful software engineering tools using their tool set (JViews,
JComposer, etc.). They use ‚Äò‚Äòplug and play‚Äô‚Äô and an
event-based composition approach to achieve component integration. In this framework, components are
more tightly coupled and their granularity is Ô¨Ånegrained. In contrast, our approach is based on dynamic
integration of coarse-grained services that are loosely
coupled.
Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and
component gluing. This creates a clean separation of
customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work
provides a potential strategy for dealing with component mismatches, which is currently ignored in our
approach.

5. Conclusions
The web-based services paradigm has gained attention
recently with the development of technologies such as
SOAP (Seely and Sharkey, 2001). The beneÔ¨Åts of such
technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these beneÔ¨Åts by providing
facilities for on-the-Ô¨Çy integration and component introspection. In this paper, we described an approach for
addressing component integration via the use of services
in the context of Jini interconnection technology. SpeciÔ¨Åcally, the approach utilizes synthesis to generate code
necessary to realize component integration. To facilitate
integration, the ACME ADL is used to specify both
services and target applications, and is used a medium
for performing service compatibility checking.
We are currently developing an environment that will
assist in the creation of applications within the servicebased paradigm and will support service browsing to
facilitate application design. In addition, we are investigating approaches for allowing services to collaborate
beyond the scope of a client application in order to
create federated groups of services. Furthermore, we are
developing technologies similar to the ones described in
this paper in order to support service-based application
within the .NET and web service frameworks.

7

Acknowledgements
G. Gannod is supported in part by NSF CAREER
grant CCR-0133956.
References
Edwards, W.K., 1999. Core Jini. Prentice-Hall.
Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services.
Commun. ACM 45 (10), 77‚Äì80.
Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design
Patterns: Elements of Reusable Object-Oriented Software. Addison Wesley Longman.
Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for
legacy software. In: Proc. 7th Working Conf. Reverse Eng., IEEE,
pp. 128‚Äì137.
Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture
description interchange language. In: Proc. CASCON‚Äô97, pp. 69‚Äì
183.
Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2).
Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to
develop a family of applications. IEE Proc. Software Eng. 144 (5‚Äì
6), 237‚Äì248.
Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92‚Äì97.
Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration
with pluggable composite adapters. Software Archit. Comp.
Technol..
Reiss, S.P., 1990. Connecting tools using message passing in Ô¨Åeld
environment. IEEE Software 7 (7), 57‚Äì66.
Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services
Development Using XML. Prentice-Hall.
Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on
an Emerging Discipline. Prentice-Hall.
Stal, M., 2002. Web services: beyond component-based computing.
Commun. ACM 45 (10), 71‚Äì76.
Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In Proc. of
Design, SpeciÔ¨Åcation and VeriÔ¨Åcation of Interactive Systems.
Gerald C. Gannod is an Assistant Professor in the Department of
Computer Science and Engineering at Arizona State University and is
a recipient of a 2002 NSF CAREER Award. He received the M.S.
(1994) and Ph.D. (1998) degrees in Computer Science from Michigan
State University. His research interests include software product lines,
software reverse engineering, formal methods for software development, software architecture, and software for embedded systems.
Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona
State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras
(Chennai), India. His research interests include software engineering,
distributed and object-oriented systems, software design, software
architecture, service-oriented software engineering, and Wireless
Application platforms.
Timothy E. Lindquist is Professor and Chair in the Department of
Electronics and Computer Engineering Technology at Arizona State
University East Campus in Mesa, Arizona. He received the Ph.D.
(1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

1

On time-reversibility of linear stochastic models
Tryphon T. Georgiou and Anders Lindquist

Abstract Reversal of the time direction in stochastic systems driven by white noise has been central throughout the development of stochastic realization theory, filtering and smoothing. Similar ideas were developed in connection with certain problems in the theory of moments, where a duality induced by time reversal was introduced to parametrize solutions. In this latter work it was shown that stochastic systems driven by arbitrary second-order stationary processes can be similarly time-reversed. By combining these two sets of ideas we present herein a generalization of time-reversal in stochastic realization theory.

arXiv:1309.0165v1 [cs.SY] 31 Aug 2013

I. I NTRODUCTION Time reversal of stochastic systems is central in stochastic realization theory (see, e.g., [1], [2], [3], [4], [5], [6], [7], [8]), filtering (see [9]), smoothing (see [10], [11], [12]) and system identification. The principal construction is to model a stochastic process as the output of a linear system driven by a noise process which is assumed to be white in discrete time, and orthogonal-increment in continuous time. In studying the dependence between past and future of the process, it is natural to decompose the interface between past and future in a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward or backward in time. In a different context (see [13]) a certain duality between the two time-directions in modeling a stochastic process was introduced in order to characterize solutions to moment problems. In this new setting the noise-process was general (not necessarily white), and the correspondence between the driving inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics. In the present note, we combine these two sets of ideas to develop a general framework where two time-opposite stochastic systems model a given stochastic process. We study the relationship between these systems and the corresponding processes. In particular, we recover as a special case certain results of stochastic realization theory ([1], [5], [10]) from the 1970's using a novel procedure. In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving generating processes, in opposite time-directions, via causal and anti-causal mappings, respectively. In Section III we utilize this mechanism in the context of general output processes and, similarly, introduce a pair of time-opposite models. Finally, in Section IV, we draw connection to literature on time reversibility and related issues in physics, and we indicate directions for future research. II. S TATE
DYNAMICS AND ALL - PASS EXTENSION

In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. As usual, in discrete-time these take the form of a set of difference equations x(t + 1) = Ax(t) + Bu(t) (1)

This research was supported by grants from AFOSR, NSF, VR, and the SSF. Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota, tryphon@umn.edu Department of Automation, Shanghai Jiao Tong University, Shanghai, China, and Center for Industrial and Applied Mathematics and ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden, alq@kth.se

2

where t  Z, A  Rn◊n , B  Rn◊p , n, p  N, A has all eigenvalues in the open unit disc D = {z | |z | < 1}, and u(t), x(t) are stationary vector-valued stochastic processes. The system of equations is assumed to be reachable, i.e., rank B, AB, . . . An-1 B = n, and non-trivial in the sense that B is full rank. In continuous-time, state-dynamics take the form of a system of stochastic differential equations dx(t) = Ax(t)dt + Bdu(t) (3) (2)

where, here, u(t), x(t) are stationary continuous-time vector-valued stochastic processes. Reachability (which in this case, is equivalent to controllability) of the pair (A, B ) is also assumed throughout and the condition for this is identical to the one for discrete-time given above (as is well known). In continuous time, stability of the system of equations is equivalent to A having only eigenvalues with negative real part, and will be assumed throughout along with the condition that B has full rank. In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system is all-pass. This is done next. The assumptions of stationarity and constant parameter matrices is made for simplicity of notation and brevity and can be easily removed. A. All-pass extension in discrete-time Consider the discrete-time Lyapunov equation P = AP A + BB  . (4)

Since A has all eigenvalues inside the unit disc of the complex plane and (2) holds, (4) has as solution a matrix P which is positive definite. The state transformation  = P - 2 x, and F = P - 2 AP 2 , G = P - 2 B, brings (1) into  (t + 1) = F  (t) + Gu(t). (7)
1 1 1 1

(5)

(6)

For this new system, the corresponding Lyapunov equation X = F XF  + GG has In as solution, where In denotes the (n ◊ n) identity matrix. This fact, namely, that In = F F  + GG implies that this [F, G] can be embedded as part of an orthogonal matrix U= i.e., such that UU  = U  U = In+p . Define the transfer function U(z ) := H (zIn - F )-1 G + J (10) F G H J , (9) (8)

3

corresponding to  (t + 1) = F  (t) + Gu(t) u Ø(t) = H (t) + Ju(t). This is also the transfer function of x(t + 1) = Ax(t) + Bu(t) Ø  x(t) + Ju(t), u Ø(t) = B
1 Ø := P - 2 where B H , since the two systems are related by a similarity transformation. Hence,

(11a) (11b)

(12a) (12b)

Ø  (zIn - A)-1 B + J. U(z ) = B

(13)

We claim that U(z ) is an all-pass transfer function (with respect to the unit disc), i.e., that U(z ) is a transfer function of a stable system (obvious) and that U(z )U(z -1 ) = U(z -1 ) U(z ) = Ip . The latter claim is immediate after we observe that, since U  U = In+p , U and hence,  (t) = F   (t + 1) + H  u Ø(t)   u(t) = G  (t + 1) + J u Ø(t) or, equivalently, x(t) = P AP -1 x(t + 1) + P 2 H  u(t) u(t) = B  P -1 x(t + 1) + J  u Ø(t). Setting x Ø(t) := P -1x(t + 1), (16) can be written Øu x Ø(t - 1) = A x Ø(t) + B Ø(t)   u(t) = B x Ø(t) + J u Ø(t) with transfer function Ø + J . U(z ) = B  (z -1 In - A )-1 B Either of the above systems inverts the dynamical relation u  u Ø (in (12) or (11)). (19) (18a) (18b) (17)
1

(14)

 (t + 1) u Ø(t)

=

 (t) u(t)

,

(15a) (15b)

(16a) (16b)

u(t )

U

u Ø(t) 

Fig. 1.

Realization (12) in the forward time-direction.

4



u(t)

U



u Ø(t)

Fig. 2.

Realization (18) in the backward time-direction.

B. All-pass extension in continuous-time Consider the continuous-time Lyapunov equation AP + P A + BB  = 0. (20)

Since A has all its eigenvalues in the left half of the complex plane and since (2) holds, (20) has as solution a positive definite matrix P . Once again, applying (5-6), the system in (3) becomes d (t) = F  (t)dt + Gdu(t). We now seek a completion by adding an output equation du Ø(t) = H (t)dt + Jdu(t) so that the transfer function U(s) := H (sIn - F )-1 G + J is all-pass (with respect to the imaginary axis), i.e., U(s)U(-s) = U(-s) U(s) = Ip . (23) (22) (21b) (21a)

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and hence, F + F  + GG = 0. Utilizing this relationship we note that (sIn - F )-1 GG (-sIn - F  )-1 = (sIn - F )-1 (sIn - F - sIn - F  )(-sIn - F  )-1 = (sIn - F )-1 + (-sIn - F  )-1 , and we calculate that U(s)U(-s) = (H (sIn - F )-1 G + J )(G (-sIn - F  )-1 H  + J  ) = JJ  + H (sIn - F )-1 (GJ  + H  ) (JG + H )(-sIn - F  )-1 H  . For the product to equal the identity, JJ  = Ip H = -JG . Thus, we may take J = Ip H = -G , (24)

5

and the forward dynamics d (t) = F  (t)dt + Gdu(t) du Ø(t) = -G  (t)dt + du(t). Substituting F = -F  - GG from (24) into (25a) we obtain the reverse-time dynamics d (t) = -F   (t)dt + Gdu Ø(t)  du(t) = G  (t)dt + du Ø(t). Now defining x Ø(t) := P -1x(t) and using (5) and (6), (26) becomes Ø u dx Ø(t) = -A x Ø(t)dt + Bd Ø(t)  du(t) = B x Ø(t)dt + du Ø(t), with transfer function Ø + Ip , U(s) = B  (sIn + A )-1 B where Ø := P -1 B. B (29) (30) (28a) (28b) (27) (26a) (26b) (25a) (25b)

Furthermore, the forward dynamics (25) can be expressed in the form dx(t) = Ax(t)dt + Bdu(t) Ø  x(t)dt + du(t) du Ø(t) = B with transfer function Ø  (sIn - A )-1 B + Ip . U(s) = B III. T IME - REVERSAL
OF LINEAR STOCHASTIC SYSTEMS

(31a) (31b)

(32)

The development so far allows us to draw a connection between two linear stochastic systems having the same output and driven by a pair of arbitrary, but dual, stationary processes u(t) and u Ø(t), one evolving forward in time and one evolving backward in time. When one of these two processes is white noise (or, orthogonal increment process, in continuous-time), then so is the other. For this special case we recover results of [1] and [5] in stochastic realization theory. A. Time-reversal of discrete-time stochastic systems Consider a stochastic linear system x(t + 1) = Ax(t) + Bu(t) y (t) = Cx(t) + Du(t) (33a) (33b)

with an m-dimensional output process y , and x, u, A, B are defined as in Section II-A. All processes are stationary and the system can be thought as evolving forward in time from the remote past (t = -). In particular, x(t + 1) is Ftu -measurable y (t)

6

for all t  Z, where Ftu is the  -algebra generated by {u(s) | s  t}. Next we construct a stochastic system Øu x Ø(t - 1) = A x Ø(t) + B Ø(t) Øx Øu y (t) = C Ø(t) + D Ø(t), which evolves backward in time from the remote future (t = ), and for which x Ø(t - 1) y (t)
Ø Øu is F t -measurable

(34a) (34b)

Ø Øu for all t  Z, where F Ø(s) | s  t}. The processes x Ø, x, u Ø, u relate as in t is the  -algebra generated by {u the previous section. More specifically, as shown in Section II-A,

u Ø(t) is Ftu -measurable while
Ø Øu u(t) is F t -measurable

for all t, as examplified in Figures 1 and 2. In fact, the all-pass extension (12) of (33a) yields Ø  x(t) + Ju(t) u Ø(t) = B It follows from (18b) that (35) can be inverted to yield u(t) = B  x Ø(t) + J  u Ø(t), where x Ø(t) = P -1 x(t + 1), and that we have the reverse-time recursion Øu x Ø(t - 1) = A x Ø(t) + B Ø(t). Then inserting (36) and into (33b), we obtain Ø := DJ  and where D Øu x(t) = P x Ø(t - 1) = P A x Ø(t) + P B Ø(t) Øx Øu y (t) = C Ø(t) + D Ø(t), Ø := CP A + DB  . C (37b) (38) (37a) (36) (35)

Then, (37) is precisely what we wanted to establish. Moreover, the transfer functions W(z ) = C (zIn - A)-1 B + D of (33) and Ø (z ) = C Ø (z -1 In - A )-1 B Ø +D Ø W of (34) satisfy Ø (z )U(z ). W (z ) = W (41) (40) (39)

In the context of stochastic realization theory, discussed next, U(z ) is called structural function ([3], [4]).

7

u(t )

W

y (t) 

Fig. 3.

The forward stochastic system (33).



y (t)

Ø W



u Ø(t)

Fig. 4.

The backward stochastic system (34)

1) Time-reversal of stochastic realizations.: Given an m-dimensional stationary process y , consider a minimal stochastic realization (33), evolving forward in time, where now u is a normalized white noise process, i.e., E{u(t)u(s)} = Ip t-s . Since U, given by (13), is all-pass, u Ø is also a normalized white noise process, i.e., E{u Ø(t)Ø u(s) } = Ip t-s . From the reverse-time recursion (34a)


x Ø(t) =

Øu (A )k-(t+1) B Ø (k ).
k =t+1

Since, u Ø is a white noise process, E{x Ø(t)Ø u(s) } = 0 for all s  t. Consequently, (34) is a backward stochastic realization in the sense of stochastic realization theory. B. Time-reversal of continuous-time stochastic systems We now turn to the continuous-time case. Let dx = Axdt + Bdu dy = Cxdt + Ddu (42a) (42b)

be a stochastic system with x, u, A, B as in Section II-B, evolving forward in time from the remote past (t = -). All processes have stationary increments and x(t) y (t) is Ftu -measurable

for all t  R, where Ftu is the  -algebra generated by {u(s) | s  t}. The all-pass extension of Section II-B yields Ø  xdt du Ø = du - B as well as the reverse-time relation Ø u dx Ø = -A x Ødt + Bd Ø  du = B x Ødt + du Ø, (44a) (44b) (43)

8

where x Ø(t) = P -1 x(t). Inserting (44b) into dy = CP x Ødt + Ddu yields Øx dy = C Ødt + Ddu Ø, where Ø = CP + DB  . C Thus, the reverse-time system is Ø u dx Ø = -A x Ødt + Bd Ø Øx dy = C Ødt + Ddu Ø. From this, we deduce that x Ø(t) y (t)
Ø Øu is F t -measurable

(45)

(46a) (46b)

for all t  R. We also note that the transfer function W(s) = C (sIn - A)-1 B + D of (42) and the transfer function Ø ( s) = C Ø (sIn + A )-1 B Ø +D W of (46) also satisfy Ø (s)U(s) W ( s) = W as in discrete-time. 1) Time-reversal of stochastic realizations.: In continuous-time stochastic realization theory, (42) is a forward minimal stochastic realization of an m-dimensional process y with stationary increments provided u is a normalized orthogonal-increment process satisfying E{du(t)du(t)} = Ip dt. Since U(s) is all-pass, Ø  xdt du Ø = du - B also defines a stationary orthogonal-increment process u Ø such that E{du Ø(t)du Ø(t) } = Ip dt. It remains to show that (46) is a backward stochastic realization, that is, at each time t the past increments of u Ø are orthogonal to x Ø(t). But this follows from the fact that


(47)

x Ø(t) =
t

Ø u e-A (t-s) Bd Ø ( s)


and u Ø has orthogonal increments.

9

IV. C ONCLUDING

REMARKS

The direction of time in physical laws and the fact that physical laws are symmetric with respect to time have occupied some of the most prominent minds in science and mathematics ([14], [15], [16]). These early consideration were motivated by no less an issue than that of the very nature of the quantum. Indeed, Erwin Schr® odinger's aim appears to have been to draw a classical analog to his famous equation. A large body of work followed. In particular, closer to our immediate interests, dual time-reversed models have been employed to model, in different time-directions, Brownian or Schr® odinger bridges (see [17], [18]), a subject which is related to reciprocal processes ([19], [20], [21], [22]). The topic of time reversibility has also been central to thermodynamics, and in recent years studies have sought to elucidate its relation to systems theory (see [23], [24]). Possible connections between this body of work and our present paper will be the subject of future work. The thesis of the present work is that under mild assumptions on a stochastic process, any model that consists of a linear stable dynamical system driven by an appropriate input process can be reversed in time. In fact, a reverse-time dual system along with the corresponding input process can be obtained via an all-pass extension of the state equation. The correspondence between the two input processes can be expressed in terms of each other by a causal and an anti-causal map, respectively. The formalism of our paper can easily be extended to a non-stationary setting at a price of increased notational, but not conceptual, complexity. Informally, and in order to underscore the point, if u(t) is a non-stationary process and the linear system is time-varying, under suitable conditions, a reverse-time system and a process u Ø(t) can be similarly constructed via a time-varying orthogonal transformation. R EFERENCES
[1] A. Lindquist and G. Picci, "On the stochastic realization problem," SIAM J. Control Optim., vol. 17, no. 3, pp. 365≠389, 1979. [2] ----, "Forward and backward semimartingale models for Gaussian processes with stationary increments," Stochastics, vol. 15, no. 1, pp. 1≠50, 1985. [3] ----, "Realization theory for multivariate stationary Gaussian processes," SIAM J. Control Optim., vol. 23, no. 6, pp. 809≠857, 1985. [4] ----, "A geometric approach to modelling and estimation of linear stochastic systems," J. Math. Systems Estim. Control, vol. 1, no. 3, pp. 241≠333, 1991. [5] M. Pavon, "Stochastic realization and invariant directions of the matrix Riccati equation," SIAM Journal on Control and Optimization, vol. 18, no. 2, pp. 155≠180, 1980. [6] A. Lindquist and M. Pavon, "On the structure of state-space models for discrete-time stochastic vector processes," IEEE Trans. Automat. Control, vol. 29, no. 5, pp. 418≠432, 1984. [7] G. Michaletzky, J. Bokor, and P. V¥ arlaki, Representability of stochastic systems. Budapest: Akad¥ emiai Kiad¥ o, 1998. [8] G. Michaletzky and A. Ferrante, "Splitting subspaces and acausal spectral factors," J. Math. Systems Estim. Control, vol. 5, no. 3, pp. 1≠26, 1995. [9] A. Lindquist, "A new algorithm for optimal filtering of discrete-time stationary processes," SIAM J. Control, vol. 12, pp. 736≠746, 1974. [10] F. Badawi, A. Lindquist, and M. Pavon, "On the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear stochastic systems," in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE, 1979, pp. 505≠510. [11] F. A. Badawi, A. Lindquist, and M. Pavon, "A stochastic realization approach to the smoothing problem," IEEE Trans. Automat. Control, vol. 24, no. 6, pp. 878≠888, 1979. [12] A. Ferrante and G. Picci, "Minimal realization and dynamic properties of optimal smoothers," Automatic Control, IEEE Transactions on, vol. 45, no. 11, pp. 2028≠2046, 2000. [13] T. T. Georgiou, "The Carath¥ eodory≠Fej¥ er≠Pisarenko decomposition and its multivariable counterpart," Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 212≠228, 2007. ® [14] E. Schr® odinger, Uber die Umkehrung der Naturgesetze. Akad. d. Wissenschaften, 1931. [15] A. Kolmogorov, Selected Works of AN Kolmogorov: Probability theory and mathematical statistics. Springer, 1992, vol. 26. [16] A. Shiryayev, "On the reversibility of the statistical laws of nature," in Selected Works of AN Kolmogorov. Springer, 1992, pp. 209≠215. [17] M. Pavon and A. Wakolbinger, "On free energy, stochastic control, and Schr® odinger processes," in Modeling, Estimation and Control of Systems with Uncertainty. Springer, 1991, pp. 334≠348. [18] P. Dai Pra and M. Pavon, "On the Markov processes of Schr® odinger, the Feynman-Kac formula and stochastic control," in Realization and Modelling in System Theory. Springer, 1990, pp. 497≠504.

10

[19] B. Jamison, "Reciprocal processes," Probability Theory and Related Fields, vol. 30, no. 1, pp. 65≠86, 1974. [20] A. Krener, "Reciprocal processes and the stochastic realization problem for acausal systems," in Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197≠211. [21] B. C. Levy, R. Frezza, and A. J. Krener, "Modeling and estimation of discrete-time gaussian reciprocal processes," Automatic Control, IEEE Transactions on, vol. 35, no. 9, pp. 1013≠1023, 1990. [22] P. Dai Pra, "A stochastic control approach to reciprocal diffusion processes," Applied mathematics and Optimization, vol. 23, no. 1, pp. 313≠329, 1991. [23] W. M. Haddad, V. Chellaboina, and S. G. Nersesov, "Time-reversal symmetry, poincar¥ e recurrence, irreversibility, and the entropic arrow of time: From mechanics to system thermodynamics," Nonlinear Analysis: Real World Applications, vol. 9, no. 2, pp. 250≠271, 2008. [24] ----, Thermodynamics: A dynamical systems approach. Princeton University Press, 2009.

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

A Java Cryptography Service Provider Implementing One-Time Pad
Timothy E. Lindquist, Mohamed Diarra, and Bruce R. Millard
Electronics and Computer Engineering Technology
Arizona State University East
http://www.east.asu.edu/ctas/ecet
mailto:Tim@asu.edu

Abstract
Security is a challenging aspect of communications
today that touches many areas including memory space,
processing speed, code development and maintenance
issues. When it comes to dealing with lightweight computing devices, each of these problems is amplified. In
an attempt to address some of these problems, SUN‚Äôs
Java 2 Standard Edition version 1.4 includes the Java
Cryptography Architecture (JCA). The JCA provides a
single encryption API for application developers within
a framework where multiple service providers may
implement different algorithms. To the extent possible
application developers have available multiple encryption technologies through a framework of common
classes, interfaces and methods.
The One Time Pad encryption method is a simple and
reliable cryptographic algorithm whose characteristics
make it attractive for communication with limited computing devices. The major difficulty of the One-Time pad
is key distribution.In this paper, we present an implementation of One-Time Pad as a JCA service provider,
and demonstrate its usefulness on Palm devices.

1. Problem
Dependence on the communications infrastructure
continues to grow as the size of computing devices
decreases. The growing dependence on Internet accessibility to services that do not reside in a local machine
brings with it the need for secure communications. The
target of this work are relatively small devices and their
related systems, such as Windows CE, PalmTE, Handspring and cell phones used to access Internet services.
While several large computer service organizations have
spent millions of dollars recovering from cyber attacks,
the potential economic impact of insecure e-commerce
communications on limited devices is huge[1], [3].

Java continues to enjoy dominance in server-side
technologies, however, a small but growing number of
limited device applications are developed in Java. Nevertheless, Sun Microsystems Inc., added Java Cryptography Extension (JCE) and JCA (to the Java T M 2
Development Kit Standard Edition v1.4 (J2SDK), and
has created a substantial market for applications running
on J2ME (Java 2 Micro Edition). Other vendors are
offering Java runtimes for limited devices. These versions bring Java to client application developers [9],
[11], and raise the issue of appropriate Java-based security mechanisms.
J2ME does not include JCE and JCA, however The
Legion Of The Bouncy Castle has developed a lightweight Cryptography API and a Provider for JCE and
JCA [14]. Neither provider offers implementation of the
One-Time Pad cryptography service [14].
The simplicity of the One-Time Pad method and the
fact that it does not require high processor speed, make
it ideal for lightweight computing devices.

1.1

Context

This paper focuses on integrating the JCA cryptography service provider, starting by defining the engine
classes and then implementing the One-Time Pad
method. We include simple evaluation programs to test
the provider. The problem of pad distribution is one of
the tasks taken-on in order to have successful deployment.
Implementations of the one-time pad encryption0.9.4 are readily available. For example, one product is
available for Windows command line launching. The
source code written in ANSI-C and DOS executable are
available for download at http://www.vidwest.com/otp/
[1].
The Security documentation provided with J2SDK
includes detailed information on the implementation of

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

1

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

the Provider for the JCA [12]. The documentation for ‚Äúa
Provider for JCE and JCA‚Äù of The Legion Of The
Bouncy Castle is also available [14].
The possibility of using the One-Time pad for data
encryption and decryption for security purposes on
lightweight computing devices was covered at the 35th
Hawaii International Conference on System Science
2002 [2].

Sender

Message

2. One Time Pad
The one-time pad algorithm is among the simplest in
the world of cryptography and is considered by some to
be unbreakable. It is nothing more than an exclusive OR
between the message (to be encrypted) and the pad (a
random key - sequence of bits). The principles that govern the encryption technique are not that simple to
apply. First, the key must be random, which by itself is a
big challenge. Second, parts of the key that have already
been used to do encryption must not be available for
other encryption. The key (Pad) must be a sequence of
random bits as long as the message to be encrypted. The
sender exclusive-OR's the message with the pad and
sends the result through a communication channel. The
one time requirement that makes it unbreakable and difficult at the same time is that after use, the sender must
get rid of part of the pad, and not use it again. At the
other end of the communication, the receiver must have
an identical copy of the pad. The receiver decrypts the
cipher text to obtain the original message by doing an
exclusive-OR of the incoming cipher with its copy of
the pad [1], [3], [4]. The receiver should also destroy the
pad after use. See Figure 1.

2.1

Advantages of the One Time Pad

If the pad is actually random and has been distributed
securely to the receiver, then no third party can decrypt
the message. Even guessing part of the key will not
allow a third party to determine the remainder. This is
why some people claim that one-time pad is unbreakable. While there are a number of very good pseudo-random number generators, so far any attempt to generate a
truly random key with computers appears to generate
the same sequence after a certain point. Several
approaches avoid this problem by personalizing the key.
Another advantage of this technique resides in the simplicity of its algorithm. It does not involve complex
operations that challenge the computational speed of
some relatively small processors.

XOR

Encrypted
message
transmitted
normally

OTP
Receiver

Encrypted
message
transmitted
normally

XOR

Original
Message

OTP

Figure 1. One-time pad (OTP) cryptography.

2.2

Disadvantages of the One Time Pad

The key must be as large as the message being
encrypted; this fact is sometimes inconvenient especially in the case of large messages. The principle of the
one-time pad is to have a unique key for each communication, which makes the generation and management of
keys problematic as the number of recipients and frequency of use escalates. The last challenging aspect of
the One Time Pad is the Key distribution. In fact the key
should remain undisclosed (secret) to any other party
besides the communicating parties. Extensions to the
One-Time Pad provider discussed in this paper center on
portable memory devices [15][16] that are frequently
synchronized with more capable machine (laptops or
desktops) for key exchange. However, our implementation is aimed at handheld devices in general. General
purpose (non-proprietary) portable memory interfaces
for handheld devices don‚Äôt exist yet, so another
approach is necessary. Some possibilities are discussed
in the Key Management section below. Each application
must deal with this issue in its own effective way(s).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

2

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3. Java Cryptography Architecture
The Java security model has been evolving to adjust
to new security issues. The JCA is a framework providing cryptography functionality development capabilities
for a Java platform. It was introduced early in Java‚Äôs
evolution as an add-on package. The first release of the
Security API was an extension of JCA including API‚Äôs
for encryption, key exchange, and coding message
authentication. Prior to J2SDK 1.4, JCE was optional, in
part due to export restrictions. The ‚ÄúJava Secure Socket
Extension‚Äù (JSSE) and ‚ÄúJava Authentication and Authorization Service‚Äù (JAAS) security features have also
been integrated into the J2SDK, version 1.4. Two new
security features have been introduced: ‚ÄúJava GSS-API‚Äù
(Java Generic Security Services Application Program
Interface) that can be used for securely exchanging messages between communicating applications using the
Kerberos V5 mechanism and ‚ÄúJava Certification Path
API‚Äù that includes classes and methods in the java.security.cert package. These classes allow the developer to
build and validate certificate chains.
The java cryptography architecture includes a provider architecture [2], [5], [6], [7], [10], [12]. The notion
of Cryptography Service Provider (CSP), or just provider, has been introduced in JCA. The provider archit e ct u r e a l l o w s f o r m u l t i p l e an d i n t er o p er ab l e
cryptography implementations. An application developer can create or specify his/her own cryptography service provider. The service provider interface (SPI)
presents a single interface for implementors. Classes,
methods and properties are accessible to applications
through the JCA application program interface (API).
The SPI allows a cryptography service provider to plugin implementations for java applications. A provider can
be used to implement any security service. Several providers can be available and they may or may not provide
similar cryptography services and algorithms. Figure 2
depicts the layers of Java Cryptography Architecture,
and is taken from Sun Java documentation [6].
A given installation of J2SDK may have several
cryptography service providers installed, which may
provide implementations of different algorithms and/or
may provide multiple implementations of a single cryptography algorithm. Each provider has a name that is
used by application programmers to specify the desired
provider. It is also possible to specify the order of preference of providers. The default provider that comes
with the J2SDK is the Sun provider, which includes a

wide variety of cryptographic algorithms and tools [2],
[5], [6], [7], [10], [12].

Figure 2. Java cryptography architecture

3.1

Java Cryptographic Service Providers

The Java Cryptography Architecture, which
includes the provider(s), has two main design principles,
First, is independence from implementation and interoperability: This derives from using the services without
knowing their implementation details [12]. Second, is
algorithm independence and extensibility; meaning that
new service providers and/or algorithms can be added
without effecting existing providers. Together these provide a modular architecture that allows for encryption to
be done by an implementation of a specific algorithm
and subsequent decryption to be done by another implementation.

3.2

Provider Implementing One Time Pad

The primary question when building such a provider
is: does the nature of the one-time pad allow it to be
implemented in a pluggable architecture? Figure 1
shows the interoperability between Sender and Receiver
as independent systems in communication. The element
they are required to have in common is the key. The
implementation of the algorithm does not matter. As far
as extensibility is concerned, it is up to the provider programmer to remain independent of other cryptographic
services.
A Cryptographic Service provider is a package or set
of packages providing concrete implementations of a
subset of the cryptography portion of the Java security
SPI. The Java Security Guide in J2SDK, v1.4 documentation lists a series of nine steps to follow for implementing a Provider. This paper follows those steps as
guidelines for its development.
Two aspects in the structures of cryptographic service were needed to write the implementation code: the
Engine Class and the Service Provider Interface (SPI).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

3

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3.3

Engine Class

An Engine Class is an abstraction of a cryptographic
service. It defines the service without a concrete implementation of the particular associated algorithms. Applications access instances of the engine class through the
API, to carry out available operations. Every engine
class has a corresponding service provider interface,
which provides abstract classes accessing the engine
class features. The service provider interface indicates
all the methods that the actual cryptographic service
provider should implement for a particular cryptographic algorithm or type. A service provider interface
is named with its engine class name followed by ‚ÄúSpi‚Äù
[12].
For each service that a provider implements, we must
define the engine class, and then write its service provider interface. For the One Time Pad technique, the
service provider interface‚Äôs abstract class is called OneTimePadSpi. The engine class for OneTimePadSpi in
compliance to the nomenclature of JCA is called OneTimePad. The engine class is a concrete subclass of the
service provider interface, implementing all the abstract
methods.
The provider class is a final subclass of java.security.provider. Our provider is named ASUEcetProvider.
The provider name is used by applications to access our
one-time pad service [12].

3.4

Provider‚Äôs Information

The provider class provides access to various properties of the service, including the version, and other information about the service(s) it provides such as
algorithm, type, and techniques [2], [12]. The value provided for this argument in this project is: ‚ÄúASUEcetProvider v1.0, implementing One Time Pad (OTP)
cryptographic technique, Arizona State University East,
Electronic and Computer Engineering Technology. May
2003‚Äù

3.5

Install and Configure the Provider

The provider needs to be correctly installed and configured for the application program to utilize its cryptographic service(s). There are two different ways to
install a provider. The first method consists of creating a
JAR file (Java Archive File) or ZIP file containing all
the class files belonging to the Cryptography Service
Provider. The JAR file is added to the CLASSPATH
environment variable. The exact steps of doing this last
action, depends upon the local operating system [2],
[12]. The second approach deploys the provider‚Äôs JAR
file of classes as an extension (optional package) to

Java. The file can be bundled with a particular application (with a manifest indicating relative URLs), or it can
be installed in the Java Runtime Environment to be
shared by all running applications.

3.6

Registering the Service

Configuring the service provider enables client
access to the service(s) by registering the provider and
defining default preferences where more than one provider is registered for the same service algorithm.
Static Registration consists of editing the java.security file (located in ‚Äúlib\security‚Äù subdirectory of the
Java Runtime Environment) to add the provider name to
the list of approved providers. For each available provider for a given algorithm, there is a corresponding line
in the java.security file with the form:
security.provider.<n>=<providerClassName>
Where ‚Äún‚Äù is the preference number for the provider.
For example the line:
security.provider.2=asue.provider.ASUEcetProvider
registers our provider with an order of preference 2.
Dynamic Registration can be done by a client application upon requesting service(s) from a provider. The
client application calls a class method, such as:
Security.addProvider (Provider providerName).

3.7

Test Programs and Documentation

Several test programs were written to exercise three
aspects of the service provider. For client applications to
be able to request service(s) provided by a specific provider, the provider should be successfully registered
with the security API. A simple test program can verify
registration by creating an instance of the provider and
accessing its name, version, and info (getName (), getVersion (), and getInfo () methods.
After making sure that the provider is accessible
from the security API, we need to retrieve the provided
service(s) by calling its ‚ÄúgetAlgorithm ()‚Äù method.
Finally, we check the functionality provided by the
service by writing sender and receiver applications that
use the service.
In addition to providing sample service test programs, our implementation provides documentation.
Our documentation is generated by the Javadoc tool
from the source code and it targets application programmers.

4. Key Management
The generation and distribution of the random keys
for this method is of primary concern. Since the size of
the key must match the size of the message being

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

4

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

encrypted, we limit our application to transmission of
relatively small messages. The key can be any random
array of bits, the key type used in JCA has not been
used.
The OneTimePad class has been designed, so that
when a new random key is generated, it is stored in an
external file that could be later sent to the receivers.
There are many approaches to providing random keys
for the One-Time Pad. The primary problem with using
the One-Time Pad is the amount of random bits needed.
Every message sent needs a key of the same length. If
every message is sent encrypted, the required key space
becomes large very quickly. The alternative of only
encrypting sensitive data suffers from forcing the application to make these choices, and only delays the issue
of when and how to replenish the key once all the bits
have been used on earlier messages.
Handheld devices, the target for this work, generally
only send small messages requiring encryption. This
allows for many messages using a small key of say 1
MB. A nightly synchronization using a recharging cradle can be used to also replenish the One-Time Pad key.
For devices that don‚Äôt have a connection to a host while
charging, another approach is portable memory devices
[15][16]. With a 2 GB pad, the replenishment cycle
would be much longer. Generally long enough to add a
few jpeg or gif pictures a day. The primary problem
with portable memory and handheld devices is that of
interfacing requirements. Currently, no general-purpose
interfaces (e.g., USB) are available for handhelds. Portable memory devices, to be useful, must come with general purpose interfaces, such as USB. An alternative to
the cradle and portable memory approach is to update or
replenish the pad on-line. In this approach, a One-Time
Pad is used until almost exhausted. Then the remaining
pad is used to exchange a block-cipher key for a more
computationally complex cryptography algorithm, such
as RC4 or AES. The One-Time Pad can then be generated by the server and sent to the handheld using the
complex cipher. The encryption process would then
return to the One-Time Pad methodology. This
approach is expensive on both network and CPU utilization. It may only be acceptable on larger handhelds such
as Windows CE (Pocket PC) machines. A final alternative would be to use PRNG (pseudo random number
generator) that is cryptographically appropriate. An
example PRNG is ISAAC [17]. ISAAC is a relatively
fast random number generator with a very long cycle
(i.e., guaranteed 240 with an average 28295). Generating
random pads then requires knowing the seed. Using the
prior approach of a trailing seed from the last pad as a
seed for the new pad would permit, possibly, near realtime generation of One-Time Pads.

5. Running on Limited Devices
The fact that JCA and JCE are not part of J2ME, limits applicability to our intended application space. One
approach is to configure limited client applications by
embedding the provider directly in the deployed J2ME
application.
Another approach is to use the lightweight cryptography API defined by The Legion Of The Bouncy Castle to
develop a provider based on their design principle of A
provider for the JCE and JCA [14]. This solution results
with a provider not fitting exactly the Java Cryptography Architecture, but which is usable on J2ME devices,
such as PalmOS [14].

6. Conclusions and Enhancements
As long as electronic communication continues to
expand, security will remain an issue. Cryptography is
one of the most effective tools available to address these
issues. One-time pad is among the most powerful existing cryptographic techniques, providing it is used within
the constraints of its applicability. Primarily the constraints are key management, compute limited devices
and encryption tasks that do not require encrypting large
files. The Java Cryptography Architecture offers the
potential of a single interface for applications that
allows plug-in of any number of participating service
providers. The approach allows evolution of security
approaches with the promise of minimal impact on
applications.
Security remains an open field on every computing
platform. As platforms continue to evolve to smaller and
better connected devices, they will meet the information
needs of a broader range of consumers. Its importance to
provide frameworks for developing secure distributed
and web-based applications on such mobile devices.

7. References
[1.]

d@vidwest.net (2000, November 17). ‚ÄúOne Time Pad
Encryption v0.9.4‚Äù Retrieved January 25, 2002 from
the World Wide Web: http://www.vidwest.com/otp/

[2.]

Gong, L. (1998). ‚ÄúJavaTM 2 Platform Security Architecture Version 1.1‚Äù. Sun Microsystems, Inc. Retrieved
February 20, 2003 from the World Wide Web: http://java.sun.com/j2se/1.4/docs/guide/security/spec/securityspec.doc.html

[3.]

Jenkin M. & Dymond P. (2002) ‚ÄúSecure communication between lightweight computing devices over the
Internet‚Äù. HICSS 35 January 2002.

[4.]

Kahn D. (1967) The Codebreakers, New York, NY,
MacMillan.

[5.]

Knudsen J. (1998) Java Cryptography, Sebastopol, CA,

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

5

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

O‚ÄôReilly.
[6.]

[7.]

[8.]

[9.]

Lindquist T. (2003, January 14). ‚ÄúCet427/598 Distributed Object Systems‚Äù. Retrieved February 02, 2003
from the World Wide Web:
http://pooh.east.asu.edu/Cet427/ClassNotes/Security/
cnSecurity.html
McGraw, G. and Felten, E. (1996) Java Security: Hostile Applets, Holes, and Antidotes. New York, NY.
John Wiley & Sons.
McGraw, G. (1998) ‚ÄúTesting for security during development: why we should scrap penetrate and patch‚Äù.
IEEE Aerospace and Electronic Systems, 13(4):13-15,
April 1998.
Muchow J. (2001) Core J2METM Technology and
MIDP, Upper Saddle River, NJ, Prentice Hall.

[10.]

Oaks, S. (1998) Java Security, Sebastopol, CA, O'Reilly & Associates.

[11.]

Rubin, A, Geer, D. and Ranum, M. (1997) The Web Security Sourcebook. New York, NY, John Wiley &
Sons.

[12.]

Sun Microsystems, Inc (2002). ‚ÄúJava 2 Platform, Standard Edition, v 1.4.0 API Specification‚Äù. Retried January 12, 2002 from World Wide Web: http://
java.sun.com/docs/

[13.]

Sundsted T. (2001). ‚ÄúJava, J2ME, and Cryptography‚Äù
Retrieved March 20, 2003 from the World Wide Web:
http://www.itworld.com/nl/java_sec/10262001/

[14.]

The Legion Of The Bouncy Castle (2000). ‚ÄúThe Bouncy Castle Crypto APIs‚Äù Retrieved March 20, 2003 from
the World Wide Web: http://www.bouncycastle.org/index.html

[15.]

Zbar, Jeff, ‚ÄúPortable Memory Gets Small,‚Äù retrieved
Sept. 2003; http://www.beststuff.com/article.php3?story_id=4395.

[16.]

Lexar Media, ‚ÄúSamsung Sampling 2Gb NAND Flash
Memory Devices to Lexar Media,‚Äù retrieved Sept.
2003; http://www.digitalfilm.com/newsroom/press/
press_02_25_02a.html.

[17.]

Jenkins, Bob, ‚ÄúISAAC: a fast cryptographic random
number generator,‚Äù retrieved Sept. 2003; http://
www.burtleburtle.net/bob/rand/isaacafa.html.

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

6

Tracking Personal Processes in Group Projects
Ly Danielle Sauer, Timothy E. Lindquist, Jeremy Cairney Computer Science and Engineering Arizona State University Tempe, Arizona 85287-5406 {sauer, lindquist, cairney}@asu.edu February 8, 1999 Abstract
Software engineering continues to develop methods for process improvement and quality. The Personal Software Process is one way to introduce software engineers to aspects of process tracking, assessment and improvement. In this paper, we describe the software tools that we've constructed to support the planning and postmortem of software activities. We describe an approach that allows the personal software process to be used in group projects, while still allowing the individual engineer to employ personal process quality and improvement techniques on their own activities. The tools supporting planning and postmortem are used in the context of a workflow system developed at ASU, called Open Process Components, whose aim is to componentize software services and provide interoperability among various approaches. These tools and approaches explore software development in the increasingly distributed environment in which the software engineer is responsible for their own assessment and improvement.

1.0 Introduction
Measuring, guiding and refining an organization's software process improves effectiveness of development resources and provides a level of control on software quality. The development of the SEI Capability Maturity Model[24] has raised awareness of the need for better software processes. Software processes are often discussed at the project management level, and its not uncommon for an organization to employ the services of a process engineer with the intent of wide-scale process improvement. Software processes describe the interaction among people and artifacts in carrying out the work involved in the software life-cycle. A software process encompasses the work that will be done (activities), what it will use and produce (input and output products), who will do it (agents), as well as, when and how it will be done (behavior). The past decade has seen increased demand for more powerful and robust automated software process systems. Tool vendors and the research community have responded with a variety of approaches. A review of the tool market place shows many groupware, process and workflow tools whose functionality ranges from graphical modeling or simple enactment to full support for defining, executing, analyzing, measuring, and tracking software processes. The Plethora of

tools, most of which have not been widely adopted, combines together with the increasingly distributed nature of software development today to form one of the challenges addressed by this paper. That is, the need to have interoperability among a heterogeneous set of process tools (which execute on distributed heterogeneous platforms.) The efforts of the Workflow Management Coalition (WfMC) [30] and the Object Management Group (OMG) are aimed at this challenge. Both organizations are identifying common interfaces that vendors can use for interoperability among their products. Other middleware efforts have identified process support services, for example, PCIS (Portable Common Interface Set) [9]. A follow-on project [18] integrates the Open Process Components of Gary [13] with other middleware components, such as version and configuration management. In this paper, we build on these efforts to show how processes can be distributed compositions of personal process components. Considerable research has addressed automating the software process. Some are addressing formalisms for expressing process [3]. Different formalisms such as Petri nets[12], rule-based formalisms[1,25], process programming languages[10], event-based representations [4,8,21], and object-oriented approaches[8,21] are

1 of 8

Overview of Personal Software Process

proposed for representing software processes. Other research includes comprehensive environments centered on process, such as ISTAR, in which all activities are modeled using a contractual model. In a process-centered environment, nearly all activity takes place within a defined process. Christie has elaborated several problems in the adoption of process automation [6]. Process-centered environments are typically all-or-nothing and difficult to adopt in steps. Management is justifiably reluctant to invest in dramatic change without a gradual migration path or concrete evidence of value-added. Benefits of enactment support or tracking require time consuming frontend resources for process definition. Some systems require definition of activities that don't have relevance to tracking and improvement. Adoption also places other stresses on an organization ranging from engineer's perception of excessive intrusion to the need for additional personnel who specialize in process engineering. In this paper, we present a process framework that shifts its approach toward composable process components. Project processes are created by brokering among the building blocks of engineer's defined personal processes. Software engineers are responsible for tracking measuring and analyzing their own processes distinct from organizational concerns.

managing, and improving their predictability, productivity, and quality. PSP consists of a family of seven personal processes that progressively introduce data and analysis techniques (Figure 1 on page 2) [16,28]. Engineers use these data and analysis outcomes to determine their performance and to measure the effectiveness of their methods. Humphrey's initial result (applied to 50 students and three industrial software organization) indicates an average test defects improvement of over ten times and productivity improvements of better than 25% [28].
PSP3 Cyclic Development

Cyclic Personal Process

Personal Quality Managem ent

PSP2 Code Reviews Design Reviews

PSP2.1 Design Templates

Personal Planning Process

PSP1 Size Estimating Test Report

PSP1.1 Task Planning Schedule Planning

PSP0 Baseline Personal Process Current Process Time Recording Defect Recording Defect Type Standard

PSP0.1 Coding Standard Size Measurement Process Improvement Proposal

2.0 Overview of Personal Software Process
Current software professionals utilize private techniques and practices that were learned from peers or through personal experiences. Few software engineers are aware of, or consistently use methods that lend themselves to personal process improvement. A personal software development process is a concept introduced to address improvement needs of an individual. Watts Humphrey of the Software Engineering Institute has formalized a personal software development process called Personal Software Process (PSP). Today, there are various realizations of PSP to aid software engineers in applying the process. The realizations range from case tools and web-based repository browsers to formal training classes.

FIGURE 1.

PSP Process Evolution

2.1

Personal Software Process

Figure 1 on page 2 shows the PSP progression in which each PSP step includes all the elements of prior steps together with additions. The PSP process steps are Baseline Personal Process (PSP0, PSP0.1), Personal Planning Process (PSP1, PSP1.1), Personal Quality Management (PSP2, PSP2.1), and Cyclic Personal Process (PSP3). Starting in The Baseline Personal Process, the software engineer creates the foundations for measurement and improvement. PSP0 is the software engineers current software development process extended to provide measurements (time and defect trackings). PSP0 covers three phases: planning, development (design, code, compile, and test), and postmortem. PSP0.1 includes coding standards, size measurements, and a Process Improvement Proposal (PIP). Personal Planning Process (PSP1, PSP1.1) adds planning to the baseline. Here, the software engineer pre-

Personal Software Process (PSP) [15,16,17,28] is designed to assist software engineers in controlling,

Tracking Personal Processes in Group Projects

2 of 8

Applying PSP to Group Projects

pares the basis for project tracking, which include software estimates and development plans. The goal is to learn the relationship between program size and resources, as well as how to make realistic schedules. PSP1 enhances PSP0 and PSP0.1 to include size and resource estimation and a test report using Proxy Based Estimation (PROBE) as a method to estimate sizes and development times. Personal Quality Management (PSP2, PSP2.1) provides defect management by tracking the relationship between time spent in reviews and the phases during which defects are injected and removed. Prior project defect data are used to realize review checklists and selfassessments. PSP2.1 extends PSP2 with design specifications and accompanying analyses. The goal is to provide the criteria for design completion. Cycle Personal Process (PSP3) introduces techniques for developing large-scale projects. The approach calls for sub-dividing into personal processes. Development is done in incremental steps starting with a base module.

Project Tables are automated forms, such as Logs, Summaries, and Templates. The log tables support tracking time, defects and issues. The Project Summary table records the estimated and actual totals for the project and for all projects to date. The Cycle Summary table supports the project summaries by capturing the planned and actual size, time, and defects for each cycle.

2.3

ECEN 4553 Database Browser

ECEN 4553 PSP Database (PSP Database Browser) [5] is a web-based database that also automates many of the PSP forms, scripts, calculations, and reports. The database is organized to capture a set of related data for an individual software engineer. The core of the database is the concept of a job, which is a software engineer's activity. Once the job is defined, the software engineer can log time against the job, log defects against the job, and specify a detailed project plan for the job. Although the PSP Database Browser does not strictly adhere to all of Watts Humphrey's Personal Software Process data, it collects planned and actual data for each job. ECEN 4553 PSP Database Browser automates a subset of Watts Humphrey's Personal Software Process with a web-based user interface.

2.2

Personal Software Process Studio

Personal Software Process Studio (PSP Studio or PSPS) [11] is a case tool developed at East Tennessee State University to assist in using the Personal Software Process. PSPS does so by automating the planning and postmortem artifacts. In particular, PSPS provides the following features: Data Measurement, Historical Database, Convenient Access to Tables, Statistical Calculations, and Guidance through the Process. The Data Measurement feature allows developers to accurately (similar to a stopwatch) measure development times, track defects, and measure program sizes. The Historical Database feature allows developers to store all of the developers historical PSP data in a reliable and secure database. Convenient Access to Tables provides a window with tab access to the forms. The Statistical Calculations feature automatically maintains totals and performs the statistical calculations. The Guidance through the Process feature provides on-line direction for using the PSP. PSP Studio groups all of the automated paper works, forms, and calculations into two categories: Process Tables and Project Tables. Process Tables guide or improve the individual software engineer process with an online process outline, access to standard tables for defect, LOC and coding standards, and access to a process improvement proposal.

3.0 Applying PSP to Group Projects
ISO 9000 [7,22] and the Capability Maturity Model (CMM) [23,24] assist organizations in improving their processes. Personal Software Process [15,16,17,28], on the other hand, provides an improvement technique for software engineers, in the context of individually developed software. Seamless integration of the PSP within a software organization cannot be achieved, since individuals rarely cycle through all phases of development on a software project. Engineers can, however, apply PSP analysis techniques to their individual activities on group projects. The resulting metrics can be the basis for personal process improvement, without having the "big brother is watching over me" complex that is common to organizationally imposed quality and improvement efforts. This section discusses our approach to providing well integrated organizational and personal process improvement. At ASU, we have been developing software to support the use of planning and postmortem phases of the PSP and to support their application to various life-cycle activities. For example, in an organizational setting, an individual may be assigned to testing. The test engineer

Tracking Personal Processes in Group Projects

3 of 8

Applying PSP to Group Projects

would develop their own test process that includes planning and postmortem. The resulting personal test process becomes part of an organizational or project process. The "integratable" personal processes (personal test process) collect product measures, use defect analysis and consider resource usage as a means of improving that process segment. The artifacts and the automation we have developed are discussed in Section 3.2.

3.1

Process Components

The Open Process Component Toolset (OPC) [14,19,20] is a set of tools developed at ASU to support process definitions and enactment. OPC's basic premise is that a process is a process component and may consist of one or more process components. Process components may be compositions of subcomponents whose underlying representations may differ. For example, a Process Weaver component, called create_design, may be composed with a TeamWare Flow component called review_design. Thus, the Integrated Process is defined as a process component consisting of three process components: the Planning Process Component (Figure 2 on page 4), the Personal Software Activities Component, and the Postmortem Component. The Software Activities component may be any process component such as, testing, design, coding, or review. Discussion of these components and the support we have implemented for Planning and Postmortem artifacts can be found in the following sections (Section 3.1.1, Section 3.1.2, and Section 3.1.3). 3.1.1 The Planning Process The Planning Process Component defines the individual engineer's plans for the software activity. The process is assigned to the project planner, takes as inputs the customer requirements (written or oral) and produces as output an initial version of the planning artifacts, a requirements specification, a cost estimate report, and a size estimate report. Additionally, an engineering notebook for the project is created and initialized based on the activity schedule. At this phase, the project activity schedule and the project plan summary forms only contain planning information such as estimated total size, the project development duration, and defects injected and removed. Further descriptions of the project activity schedule, the project plan summary, and the engineering notebook are discussed in Section 3.2.

FIGURE 2.

Planning Process Component

As shown in Figure 2 on page 4, the Planning Process Component is composed of its children process components: Identify Requirement, Perform Size Estimation, Perform Cost Estimation, and Construct Plan. Each child process component is defined to perform a specific task to help planning the software activity and laying the groundwork for analysis. For instance, the Identify Requirement Process Component generates the requirement specification (SRS) given the customer requirements (Cust Req). Figure 2 on page 4 shows the OPC definition tool's graphical depiction of the Planning Process. The model includes nodes for Processes (process components that have subcomponents), Activities (process components without subs), Roles and Products. Directed edges depict relationships such as can_perform, is_input_to, has_output and has_sub. For example, the Requirement Specification (Product) is_input_to Perform Cost Estimation (Activity), and the Identify Requirement (Activity) has_output which is the Requirement Specification. 3.1.2 Personal Software Activities In the PSP, the planning and postmortem activities depend on a personal software process that includes the phases, planning, design, code, code review, compile, test, and postmortem. In our application of the PSP to group projects, we provide the capability to replace design, code, code review, compile and test with other activities. Our approach is to provide the background for the planning and postmortem phases as applied to any software related activities. In a group project, an individual engineer may

4 of 8

Tracking Personal Processes in Group Projects

Applying PSP to Group Projects

not be involved in coding, compiling and testing, but may instead work on design and design reviews, or may instead be a test engineer whose involvement does not go beyond planning, developing, executing and reporting on tests. Our assumption is that the analysis techniques that consider resources (labor, primarily), product measures and quality assessment all equally apply to any other software related activities, whether directly developing code or not. Process improvement should be a center of focus for all participants in a software process. At ASU, we have been using this approach to Integrating Personal Processes for group software projects in a classroom setting and for group independent study projects. Thus far, uses are for small applications in which most project members get involved with all of the life-cycle activities. The primary challenge in generalizing the approach to large group efforts has to do with product and quality measures. PSP relies on Source Lines of Code as the basis for product measures. Software defect management is the basis for quality, planning and process improvement. Engineers using PSP, record defects by type, phase injected and phase removed. PSP uses yield (percentage of defects removed before compiling), appraisal cost of quality and failure cost of quality as the primary input for quality management and process improvement. These are a good starting point for the practicing software engineer, however, one must define product measures and defects in a manner appropriate to the activity. For example, a test engineer may use test cases generated as the primary product measure. For example, test cases may be defined to be triples (input condition, action, expected result) independent of how the test case is realized in performing tests. Defect types for a test engineer may include: unsatisfied test requirement, and resulting software defects for which there existed a test case.

We have used OPC to depict Postmortem. The process is assigned to the process engineer and accomplishes its objective of producing the project plan summary by using the initialized project activity schedule, the project defect log, and the initialized project plan summary as input products. Unlike the Planning Process Component, Postmortem does not use children process components to accomplish its goal.

3.2

Automated Support for Process Artifacts

The Integrated Personal Process uses four artifacts: the Engineering Notebook, the Project Activity Schedule & Log, the Project Defect Log, and the Project Plan Summary. We have implemented each artifact as a standalone application. When using the worklist handler of OPC, enacting one of the Integrated Personal Process Planning or Postmortem activities may cause the invocation of one or all of these applications according to the process input and output specifications. All four artifacts use a single repository interface to store and manipulate data. The interface is implemented in Java, using synchronization to support multiple concurrent access. Highlights of these artifacts are detailed in the following sections. 3.2.1 The Engineering Notebook The Engineering Notebook is an application which implements some concept of the Personal Software Process Engineering Notebook. The Engineering Notebook objective is to create an engineering notebook that tracks a software engineers daily time usage. More specifically, as shown in Figure 3 on page 6, the Engineering Notebook allows a software engineer to define and record, for a given project, its activities, the time spent on the activities, and product lists of the activities. An activity is a unit of work which takes an engineers time (e.g. interruptions, coding, breaks, lunch, designing, etc.); it is any work performed by a software engineer. The time spent on each activity is recorded in an increment of hours; for instance, a job that takes 15 minutes could be recorded as 0.25 hours, but our usage generally limits granularity to one tenth of an hour (6 minutes). The product list entry allows the engineer to list products produced by the activity. The initial engineering notebook is derived from infromation in the Project Activity Schedule & Log.

3.1.3 The Postmortem Process The Postmortem Process Component defines a process for analyzing the performance (postmortem analysis) of a completed project. Postmortem analysis gathers product measures, performs actual resource usage analysis, performs actual defect analysis, and performs summary quality analysis.

Tracking Personal Processes in Group Projects

5 of 8

Applying PSP to Group Projects

to allow add-on functions. For example, the user can add the tools for estimation or a LOC Counter. The add-on tools are specified using MIME types. 3.2.3 Defect Log The Defect Log (DL) is an application which automates Defect Recording [15,16] to aid in tracking defects injected and removed. The defect data are stored in the defect log, which are used as input to generating the plan summary (Section 3.2.4) in postmortem analysis. The Defect Log is realized as a tabular application where the rows represent the defects and their information and the columns are classifications of the defects. As shown in Figure 4 on page 6, the DL allows its user to specify the date, the defect type, the injected phase, the removed phase, the fixed time, and a description.

FIGURE 3.

Engineering Notebook Main Window

Modification to the times in the engineering notebook causes the transfer of the times spent and the product list to the Project Activity Schedule & Log. 3.2.2 Activity Schedule & Log The Activity Schedule & Log (ASL) is an application, which aids in developing project plans. The ASL application allows the project planner to identify the activities, phases, agents, and times for the activity. When the user completes the activity specifications, ASL places the schedule in a persistent repository. An activity is a task of the project. A project may have a set of activities (process components) representing the work of all group members assigned activities on the project. Similarly, all engineers working on a project will have their own activity schedules, which reflect the lowerlevel activities necessary to complete their input to the group. The lower-level activities are subject to analysis and improvement as defined above. A project planner may also have an Activity Schedule and Log to coordinate the activities and products of a group of engineers. We envision that the project planner can use the Process Broker [27] (which is currently under development), to determine the kind of activities that the project may need and to check for the availability of those process components. To do this, the project planner first specifies the characteristics of the current project to the Process Broker. The Process Broker uses its locating and matching semantic engine and its repository of process components to determine the projects that best fit the specified criteria. For each activity, the project planner estimates a begin and end time, the development duration, the project size, and the output products. These are estimated values, thus, the project planner can use experience to determine the values, some tools, or historical project data. OPC is designed

FIGURE 4.

Project Defect Log Main Window

The date that the defect was discovered and the defect description can be anything that the user enters. The DL default defect types are: Documentation, Syntax, Build/ Package, Assignment, Interface, Checking, Data, Function, System, and Environment. These can be modified to allow application of planning and postmortem to any software activity. Additionally, the DL also provides default phases including: Planning, Design, Code, Review, Compile, Test, and Postmortem. Analogous to the defect types, these can be changed to accommodate the activity. Finally, the time required to correct the defect is recorded in hours and tenths. 3.2.4 Project Plan Summary The Plan Summary (PS) [15,16] is an application, to aid in planning and tracking a software activity. The plan summary is initialized in the planning activity and is com-

6 of 8

Tracking Personal Processes in Group Projects

Current and Future Work

pleted in postmortem. In our implementation, information in the plan summary is derived from the Activity Schedule & Log. The plan summary can be saved and named so that an engineer who participates in several software activities (reviews, testing, and coding, for example) can track data specific to the activity.

www.eas.asu.edu/~yfppg/ [3.] Armenise, P., Bandinelli, S. Ghezzi, C., and Morzenti, A. A Survey and Assessment of Software Process Representation Formalisms. International Journal of Software Engineering and Knowledge Engineering, vol. 3, no. 3, pp. 401-426. 1993. Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS-034-95, Computer Science Department, Columbia U. 1995. L. Carter, ECEN 4553 PSP Database Tool, (University of Colorado at Boulder, Department of Electrical & Computer Engineering, ece-www.colorado.edu/~ecen4553/Reference/ psp/examples.html). A. Christie, et al. A Study into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. F. Coallier, "How ISO 9001 Fits into the Software World", (IEEE Software, January 1994, pp. 98-100). Conradi, R., et al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. J.C. Derniame, et al. "Life-Cycle Process Support in PCIS, Or It Is Time to Think about Software Process Formalisms Standardization", in Proc. of the PCTE'94 Conf. PCTE Technical Journal No.2, PIMB Assn, November 1994. J.C. Derniame, and Gruhn, V. Development of Process-Centered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127150. 1994. East Tennessee State University, Personal Software Process Studio, (East Tennessee State University, Computer and Information Science, www-cs.etsu.edu/softeng/psp/dlpsps.html). W. Emmerich, and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proceedings of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. K. Gary, "Process Interoperability with Open Process Components", Arizona State University, Computer Science and Engineering Department, Ph.D. Dissertation, expected August 1998.

4.0 Current and Future Work
OPC provides an initial set of tools for defining and enacting process components. The underlying implementation of OPC provides the framework for wrapping various process tools for interoperability. We have achieved initial wrappings of two products, and hope to soon demonstrate interoperability between these products in the near future. Thus, a process component can be defined in terms of sub components each under the direction of a different enactment engine. We have used our integrated personal software process approach in classroom projects and in group independent studies. The tools described in this paper will be introduced to these projects beginning in the Fall semester 1998. Enactment using OPC is controlled by a worklist handler tool, which connects to a repository of process components. Process components are all represented using Java objects. Until the tool wrappers are fully functional, enactment involves launching an application associated with the input and output products as specified with MIME types. A few important distinctions differentiate our approach to integrating personal processes. Engineers are not asked to carry out a defined process that they themselves have not developed. Engineers are motivated to use process improvement techniques, since they directly and solely apply to their own activities. Product and defect measures are defined by the engineer and thus problems of consistency do not arise. Engineer define their own personal process for the software activities they perform. These may defined or applied from definitions they obtain from other engineers.

[4.]

[5.]

[6.]

[7.]

[8.]

[9.]

[10.]

[11.]

5.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proceedings of the 2nd EWSPT `92, Trondheim, Norway. September, 1992. Arizona State University, Open Process Component Toolkit, (Computer Science and Engineering Department, YFPPG Group, http://

[12.]

[13.]

[2.]

Tracking Personal Processes in Group Projects

7 of 8

References

[14.]

K. Gary, T. Lindquist, L. Sauer, and H. Koehnemann, "Automated Process Support for Organizational and Personal Processes", (Proceedings of the International Conference on Supporting Group Work (GROUP `97), the Integration Challenge, Phoenix, Arizona, USA, 16-19 Nov 1997). W. S. Humphrey, Introduction to the Personal Software Process (Reading, MA: Addison-Wesley, 1997). W. S. Humphrey, A Discipline for Software Engineering (Reading, MA: Addison-Wesley, 1995). W. S. Humphrey, "The Personal Process in Software Engineering", (Software Process Newsletter, Technical Council on Software Engineering, IEEE Computer Society, Vol. 13, No. 1, September 1994, pp. 1-3, http://www.sei.cmu.edu/products/publications/95.reports/95.ar.psp.swe.html). The US-France Technology Research and Development Project, PCIS2 Architecture Specification Version 1.0, (Lindquist, TE editor) SPAWAR Systems Command, San Diego CA, January 1998. T. Lindquist, "A Toolset Supporting Distributed Process Components", (Arizona State University, Computer Science & Engineering Department, Technical Report, TR-97-034, 1997). T. Lindquist, and J. Derniame, "Towards Distributed and Composable Process Components", (Proceedings of the European Workshop on Software Process Technology, June 1997). Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Mark C. Paulk, "How ISO 9001 Compares with the CMM", (IEEE Software, January 1993, pp. 74-83). Mark C. Paulk, Bill Curtis, and Mary Beth Chrissis, "Capability Maturity Model, Version 1.1", (IEEE Software, July 1993, pp. 18-27). Mark C. Paulk et al., "Capability Maturity Model for Software, Version 1.1", (Technical Report, CMU/SEI-93-TR-24, Software Engineering Institute, 1993). Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. W. Royce, "Managing the Development of Large Software Systems: Concepts and Techniques", (WESCON Technical Papers, Vol. 14, Los Ana-

gles, WESCON, August 1970). [27.] L. Sauer, "Brokering of Process Components", (Arizona State University, Computer Science & Engineering Department, Ph.D. Dissertation Proposal, December 1997). Software Engineering Institute, Personal Software Process (PSP), (SEI Technology, http:// www.sei.cmu.edu/technology/psp/). Software Engineering Institute, A Specification for Automated Support for the PSP, (SEI Technology, http://www.sei.cmu.edu/technology/pspAuto/indexh.html). The Workflow Management Coalition. The Reference Model. WfMC Document Number TC001003, January 1995.

[15.]

[28.]

[16.] [17.]

[29.]

[30.]

[18.]

[19.]

[20.]

[21.]

[22.]

[23.]

[24.]

[25.]

[26.]

8 of 8

Tracking Personal Processes in Group Projects

Towards Target-Level Testing and Debugging Tools for Embedded Software
Harry Koehnemann, Arizona State University
Dr. Timothy Lindquist, Arizona State University

Abstract

The current process for testing and debugging
embedded sojware is ine~ective at revealing errors. There
are currently huge costs associated with the validation of
embedded applications. Despite the huge costs, the most
dl~cult errors to reveal and locate are found extremely late
in the testing process, making them even more costly to
repm‚Äùr. This paper first presents a discussion of embedded
testing research andpractice. This discussion raises a need
to improve the existing process and tools for embe&@i
testing as well as enable better processes and tools for the
jWure. To fmilitate
this improvement, architectural and
software capabilities which support testing and &bugging
with minimal intrusion on the executing system must be
developed. Execution visibility and control must come
@om the underlying system, which should ofJer interjbces
to testing and debugging tools in the same numner it offers
them to a compiler. Finally we propose txtenswns to the
underlying system, which consists of adiiitions to both the
architecture and run-time system that will help reulize
target-level tools.
1. Introduction
Software validation involves many activities that
take place throughout
the lifecycle
of soft w are
development.
A substantial
portion
of the validation
process is software testing, which is the development
of
test procedures and the generation and execution of test
eases.
Notice we are not only concerned
with the
generation of a test case, but are also concerned with how
that test is executed. Therefore, a test case is not simply
composed of inputs to a system, but rdso includes any
environmental
factors.
Other research has examined the
issues behind test case selection, but few are addressing the
problems that surround the execution of those test cases.
The goal of this paper is to identify
the problems
associated with test case execution for embedded systems
and to propose solutions for making embedded testing more
effective at revealing errors.
1.1

Testing
and Debugging
Process
Many of the activities, tools, and methods used
during software testing are shared by software debugging.
Software testing is concerned with executing a piece of
software in order to reveal errors, while software debugging
is concerned with locating and correcting the cause of an
error once it has been revealed.
Though
these two
activities are often referenced separately, their ac$ivi$ies are
tightly coupled and share many common features.
Permission to copy without
fee all or part of this material is granted
provided that the copies are not made or distributed
for direct commercial
advantage, the ACM copyright notice and the title of the publication
and its
date appear, and notice is given that copying
is by permission
of the
Association
for Computing
Machinery.
To copy otherwise
or republish,
requires a fee and/or specific permiss~m.

During debugging, a developer must recreate the
exact execution scenario that revealed the fault during
testing.
Not only must the code execute
the same
instruction sequences, but all environmental
variants must
be accounted for during the debugging session. In addition,
the tools assisting
in the debugging
process
must
providing a &veloper
with a certain degree of execution
visibility
and control while not impacting
the execution
behavior of the program.
1.2

Embedded
Systems
The testing and debugging
process is greatly
restricted by embedded systems. Embedded applications are
among the most
complex
software
systems
being
developed today. Such software is often constrained by
‚óè Concaumnt designs
‚óè RcaI-time
constraints
‚óè l%lbedded
target Imvilrmments
‚óè Distributed
hardware ambitectures
‚óè Device control dependencies
Each of these properties of embedded software severely
restrict
execution
visibility
and
control,
which
conseqmdy
restricts the testing and debugging process.
Our current methods and tools for software testing and
debugging require a great deal of computing
resources.
Such resources are not available on the target environment.
Therefore, a large gap exists between the methods and tools
used during evaluation on the host and those used on the
target. Unfortunately,
mauy errors are only revealed during
testing in the target environment.
Because of the above issues, concerns are raised
over the effectiveness of software validation for embedded
systems.
Embedded
applications
are responsible
for
controlling physical devices and their correct execution is
critical in avoiding and/or recovetig
from device failure.
Often these physical devices control life-critical
processes,
making the embedded software a key element of a lifecritical system. Software failure can lead to system failure,
which in turn could lead to loss of life.
In addition, designers are increasing their use of
embedded software to conuol the physical elements of large
systems. This rate of increase is likely to increase as the
cost for embedded controllers becomes cheaper and more
attractive when compared with other mechanical techniques.
Computer
networks
are fast replacing
point-to-point
wiring,
due to the networks
light
weight,
easy
cotilgurability
and expansibility,
and lower
design
complexity.
The advancement
in the complexity
of
problems
addressed
by software
in these types of

01993

ACM

0-89791-621

-2/93/0009--0288

1.50

applications
may soon be limited
satisfy reliability needs and concerns.

by our inability

to

2. Software
Testing
The software testing phase is concerned with
executing a software program in order to reveal errors.
Software testing for embedded systems takes place in four
basic stages:
1) Module Level Testing
2) Integration Testing
3) System Testing
4) Hardware/Software
Integmtion Testing
The first three testing stages are typical of any software
product.
Testing begins with exercising each soft ware
module and concludes when the software is shown to meet
system specifications
by passing some rigorous
set of
system tests. The fourth phase is unique to embedded
systems. The software must not only be correct, but must
also interface properly with the devices it is controlling.
Testing
literature
contains
countless
methodologies,
techniques,
and tools that support the
software
testing process.
They range from software
verification
and program proving
to random test case
selection.
All testing
methods
indirectly
apply
to
embedded systems, as they do all software.
Of particular
interest to this paper are those techniques that address the
problems identified for embedded software - concurrency,
real-time
constraints,
embedded
environment,
etc.
Unfortunately,
there exists little research into the unique
with testing
embedded
software.
problems SSSOCilltd

techniques, must examine a large set of statw and therefore
must constrain itself to small, simple programs.
Research in dynamic testing of concurrent Ada
programs has largely focused on the detection of deadlocks
~emb85],
the saving of event
histories
KeDo85,
Maug85],
and other tec.huiques that passively watch a
program execute then allow the execution sequences to be
replayed after a failure has been detected.
Hanson Eans78] was among the first to discuss
run-time
control of concment
programs.
In order to
regulate
the sequences of events, he assigned each
concurrent event in the test program a unique time value.
He then introduced a test clock that regulated the system
during execution. A given event could only execute if it‚Äôs
time was greater than that of the clock.
Tai ~ai86, Tai91] extended Hanson‚Äôs work to the
Ada programnn ‚Äúng language.
His method takes an Ada
program P and a rendezvous ordering R and produces a new
Ada program P‚Äô such that the intertask communication
in
P‚Äô is always R. A similar approach was used in Koeh89]
to apply these techniques to testing and debugging tools.
This work addressed the facl that in order to,
testa specific
program state, values in a program
may need to be
modified during run-time.
Modification
o~f the program
state is a capability provided by any debugging tool and is
a required property of a tool debugging tasked programs. It
is important to note that both techniques explicitly perform
rendezvous scheduling, removing those decisions from the
run-time system and placing control in the hands of the
tool.

2.1

2.2

Testing
Concurrent
Systems
Concurrency increases the difficulty
of software
testing,
Given a concurrent program and some set of
input, there exists an unmanageably
large set of legal
execution sequences the program will take. Furthermore,
subsequent execution
with the same input may yield
different,
yet correct results due to differences
in the
operating environment.
This is all complicated
by Ada‚Äôs
nondetermins tic select construct. Therefore, when testing
concurrent software, we are not only concerned with a valid
resuk but must also be concerned with how the program
arrived at that result.
Since multiple executions of a concurrent program
may yield different results, it is not enough to ensure that
the system produces the correct output for a given input.
One must also ensure that the system always produces an
acceptable output for each execution sequence that is legal
under the language definition.
Without sufficient control
over program execution,
there is no way of ensuring a
given test is exerasing the code it was intended to test.
Taylor and Osterweil
~ay180]
examined static
analysis of concument programs,
However, this research
considered processes in isolation and does not consider
interprocess cmrummication.
Taylor later extended this
work to Ada and a subset of the Ada rendezvous mechanism
~ay183]. Through this static aualysis technique, one could
determine aIl parallel actions and states that could block a
task from executing.
This method, as with most static

Non-intrusive
testing
Intrusion plays a significant role in the testing and
debugging of embedded software. Any technique used to
raise execution visibility
or provide for program control
must not interfere with the behavior of the teat program.
Embedded applications have strict timing requirements and
any intrusion on a test execution will likely make that test
void.
Intrusion
is typical for host-based
testing, but
becomes a large problem
for target-level
testing and
debugging activities.
The above approaches
address the need for
visibility,
control, and predictability
for testing concurrent
software.
However,
they are all intrusive
and use
instrumentation
(inserting probes into a usem program and
rewriting
certain constructs before submission
to the
compiler) to gather run-time information
and to control
~gram
exmtion.
After the probes are added, the user‚Äôs
object code is linked with the rest of the debugging system
and then executed under test. This additional
code has a
serious impact on the execution behavior of the program.
Instrumentation
is not appropriate
for testing real-time,
embedded applications.
A non-intrusive
debugger for Ada is proposed in
[Gil188]. A separate processor executes the testing system
and communicates with the target processor through some
special purpose hardware.
Lyttle and Ford ~ytt90]
have
also implemented a non-intrusive
embedded debugger for
Ada. Their tool provides monitoring,
breakpoints,
and

289

of hardware and the run-time system is called upon to
bridge the impending gap. No argument is made as to the
rate of increase identified
by the line slopes; nor is an
argument ma& that these increases are even linear.

display facilities
for executing embedded applications.
While these efforts provide an excellent start towards targetlevel tools, they do have severe limitations.
These
implementation
do not deal with high level activities such
as task interactions and are only concerned with items that
can be translated from monitoring
system bus activity. As
discussed later in Chapter 5, techniques dependent on bus
activity will likely fail for future architecture designs. In
addition,
many of the error detwted
in the target
environment are indeed concerned with high-level activities
(process scheduling
and interactions,
fault handling,
interrupt response, etc.).
Other real-time,
embedded
tools have been
proposed for crossdevelopment
environments.
They can
typically
be classified into one of the following
three
categories: 1) ROM monitors, 2) Emulators, and 3) Bus
monitors.
These types of tools will be further discussed
later in this paper.

c
o
m
P
1
e
x

i~
t

Y

constructs

Hardware

Time
Figure 2.1

2.3 Impact

of the Underlying
System
One of the large problems with testing concurrent
systems
is dealing
with
abstraction.
The Ada
programming
language
abstracts concurrent
activities
through task objects ~D83].
Tasks allow a developer to
abstract the concepts of concurrency
and interprocess
cxmmmnication
and discuss them at a high level. The
burden of implementation
is then placed on the compiler,
and typically the run-time system.
While abstraction
is a powerful
design tool, it
leads to significant
prthlems
during the testing phase of
software development.
Implementation
details become
buried in the underlying system. At the development level,
this high degree of abstraction is appropriate.
However,
abstraction complicates the testing process. Not only are
we concerned with implementation
details, but we must
aIso control them to demonstrate that certain properties
about a program will hold for every legal
execution
scenario.
Without
sufficient
control
over program
execution, there is no way of ensuring that a spedc
test is
exerasing the code it was intended to evaluate. In addition,
cmmxt operation
in one environment
(host) does not
necessarily imply comet operation in another (target) due
to implementation
difference in the underlying system.
The underlying
system is composed of two parts,
the features of the hardware architecture and the operations
provided by the run-time system. As language constructs
become more abstract, compilers are required to generate
more code to implement them. There is no longer a trivial
mapping from language construct to machine instruction.
Rather, the compiler must provide an algorithmic
solution
in order to implement these high level constructs. Those
solutions
exist as operations
in the run-time
system.
Rather than generate code for these constructs, the compiler
generates a call to a run-time system operation or servim.
As the constmcts
become
more
abstract,
compilers
develop
an increasing
dependency
on the
underlying
system. This increase in shown in figure 2.1.
As new constructs
are introduced
to programming
languages, their increase in abstraction is greater than that

L=

Language

Embedded
systems raise many problems
for
software testing and debugging.
Such software typically
must deal with concurrency,
real-time
constraints,
au
embedded
target environment,
distributed
hardware
architectures,
and a great deal of hardware-software
interfaces for controlling
externaI devices. These issues
tdone do not provide a complete view of the problems
SyStetUS are
extcotttttered by embedded testing. bbedded
typically developed on custom hardware configurations
m@ng
that each system introduces
it‚Äôs own unique
problems. Tools and techniques that apply to one are not
generally applicable on another, which leads to ad hoc
approaches to integration and system testing of embedded
software. The program is executed for some length of time
and continual y bombarded with inputs in an attempt to
show it adheres to some speeifkation,
3.1

Current
state of embedded
testing
As described
earlier, the testing
process for
embedded systems consists of 4 phases that conclude with
Hardware/Software
(H/S) Integration
During
H/S
integration
testing, device and timing related errors are
reveakd. These errors eneompass problems such as:
‚óè incorrect
handling of interrupts
‚óè distributed
communication problems
‚óè incorrect
ordering of concumen t events
‚óè resource contention
‚óè incorrect
use of device protocols and timing
‚Äú incomect response to failures or transients
These errors are often extremely difficult
problems to fix
and often require significant modifications
to the software
system.
In addition,
software is for~d
to conform
to
custom hardware that may itself have errors. As stated
above, H/S integration
is the last phase of testing for an
embedded system. Since errors are much cheaper to fix the
earlier they are revealed,. why would one wait until the last
phase of product development to reveal the most diftlcult to

290

locate, costly errors to fix? Our goal should be to reveal
these errors as early as possible.
Unfortunately,
target
level testing tools have yet to become a reality.
The target processor of au embedded computer is
typically minimal in function and size. It is only a small
portion of a larger system, whose goals are to minimize
cost and space. Therefore, target hardware of au embedded
systems will not support software development
nor any
development tools. To resolve this problem, the source is
developed on a larger host platform and cross axnpilers and
linkers are used to generate code and download it to the
target processor.
Consequently,
two environments
exist in our
development process, the host environment
and the target
environment,
each having
completely
different
functionality
and interface to a user. Tools that run on the
host provide a high level interface and give users detailed
information
on and control over their program execution.
However, little is provided on the target. Typically,
the
best information
obtainable is a low-level execution trace
provided by an in-circuit emulator.

3.2

Current
Solutions
Approaches to dealing with the above problems
can be divided
into hardware
solution
and software
solutions. The hardware solutions are attempts at gaining
execution visibility
and program control and include the
bus monitors, ROM monitors, and in-circuit emulators. A
bus monitor gains visibility
of an executing program by
observing
data and instructions
transferred
across the
system bus. With a ROM monitor,
debugger code is
placed into ROM on the target board. When a break point
is encountered,
control is transfered to the debug code
which can accept commands from the user to examine and
change the program‚Äôs state. Finally, an in-circuit emulator
connects with a host system across an ethernet connection.
At the other end, a probe replaces the processor on the
target board. The emulator then simulates the behavior of
the processor in (ideally)
real-time,
which allows the
emulator to tell the outaide world what it‚Äôs doing while it‚Äôs
doing it.
The
hardware
solutions
have
mini
m al
effectiveness for software development.
They can only
gather information
based on low-level machine data. The
developer must then create the mapping between low-level
system eventa and the entities defiied in the program. That
-ping
is the implementation
strategy chosen by a given
compilation
system and becomes severely complicated for
abstractions such as tasks Maintaining
an understanding of
the mapping is extremely difficult and cumbersome.
The software solutions cart be viewed as attempts
to reduce the tremendous costs of testing on the target.
Several factors determine how a pitxe of software is tested
1) Level of criticality of software module
Each software module is assigned a different level of
criticality
based on it‚Äôs importance
to the overall
operation of the system.
2) Test platform availability

Typically,
there will exist several test environments
available to test a piece of soft ware, each providing
a
closer approximation
to the actual target environment:
‚óè Host-baaed sours
level debugger
‚óè Host-based
instruction set simulator
‚óè Target emulator
‚óè Integrated
validation faality
3) Test Classification
The tests to be performed
can be categorized
to
determine what they are attempting
to demonstrate.
The goal of a test plays a large role in determining
the
platform on which it will execute. Some examples are
shown below.
‚óè Algorithmic
‚óè Inter-module
‚óè Intra-module
‚óè Performance
‚óè HE
integration
‚óè InttX-cabinet
Each of these factors play a role in assigning program
modules to the various test platforms
based on some
criteria that might contain the following:
‚óè Type of software
‚óè Hardware requirements
‚óè Test chssifkation
‚óè Platform
availability
‚óè Coverage requirements
‚óè Test support software
availability (drivers, stubs)
‚óè Certification
Requirements
‚óè Level of effort required for test
This criteria takes into account the 3 factors discussed
above as well as additional ones.
The software solutions are an attempt to minimize
the time spent testing
in the target
environment.
Validation facilities are expensive to build and time utilimd
for testing is expensive. This is due to the f;act that target
level testing occurs extremely late in the development
lifecycle and only a small window is allocated for HAS
integration
testing.
However,
the target is the only
location that can reveal tin
errors. It is ironic that our
current solutions attempt to reduce the anmunt of target
testing, but will likely lead to extensive modifications
and
thercfom extensive retesting.

4.

Problems with Embedded Irestinq
The solutions proposedaboveare not effective at

revealing errors. Effective implies that a technique reveals
a high percentage of the errors and that it does so in a costef!iaent manner. Instead, what the above tools provide is a
minimal, low-level view of the execution of a program and
those tools become available
at a very late stage in
development.
Below is a list of problems associated with
current approaches to embedded testing
4.1

Expense
of Testing
Process
Target
testing
requires
expensive,
custom
validation facilities.
The expense of these target facilities
is incurred for every project, since little
reuse across
projects is ever realized. The effort required to build these
validation
facilities
means that every test execution is
expensive, making retests extremely costly. Yet, hardware
often arrives late and full of errors, forcing software to be

291

modified
and subsequently retested.
This late arrival of
hardware also impacts the cost of an error, since certain
errom are only revealed during I-IN integrations testing.
Perhaps the largest factor associated with the high
costs of testing will be the questions and concerns that
certification processes are beginning to raise about sofhvare
tools. Typically, development tools have not been required
to meet any validation
criteria and certairdy not the strict
criteria imposed on the development system. This luxury
may soon disappear
as the role tools play in the
development
process comes under tighter scrutiny.
The
huge expense of validation
facilities
will
increase
dmmaticauy.

4.2 Level

of Functionality
on Target
The level of functionality
found on a target
machine is minimal and does not support tools. This lack
of functionality
greatly limits the effectiveness of testing,
since more time and effort is required to locate an error.
While a host system provides a high-level
interface and
discussed software in terms of the high-level language, the
target typically deals in machine instructions and physical
addresses. Translating these low-level entities requires time
and a great deal of tedious, error-prone activities.

4.3

Errora
revealed
late
in
development
lifecycle
Embedded
system designs
often incorporate
custom ASIC parta that are typicaIly not available until
very late in the development
process,
delaying
the
availability
of any target validation facility.
In addition,
errors designed into the ASICa are extremely expensive to
fix, requiring
new masks be created and complete
refabrication.
InsteaA errors in ASICs and other hardware
problems are resolved by modifying
the software.
As
stated before, this greatly delays the time which errors are
revealed, which in turn increasing the cost of software
testing.
4.4

Poor
teat selection
criteria
AIl to often, tool availability
diclates the quality
of a testing process.
Tests cases and scenarios
are
determined by what will work on available platforms and
which test are achedulable rather than being determined by
some theoretical
test cxitcria.
A prime example is the
FAA‚Äôs requirements ~AAS51 that 1) all testing be done in
the target environment
and 2) testing include statement
coverage.
Of course, test coverage is not currently
measured on the target.
Unfortunately,
it is cheaper for a company
to
spend it‚Äôs resources preparing an argument to obtain some
form of ‚Äòwaiver‚Äù than to actually perform a test. In time,
the argument approach will no longer be accepted and the
solutions
for embedded
testing must be in place to
accommodate
this
change.
It will
only take one
implementation
that performs statement coverage on the
target to force every embedded, real-time software developer
to perform statement coverage on the target to meet such a
certification requirement.

4.S Potential
use in advancing
architectures
Perhaps the largest problem
facing embedded
testing is that the current solutions cannot be applied to
future h~dware
architectures.
Future architectures
are
Proposing
‚óè wider addreas Spaces
‚óè higher -Sor
speeds
‚óè huge numbers of pins
‚óè internal
pipes
‚óè multiple
execution units
‚óè large internal
caches
‚óè multi-dip
modules
Such complexities
cast a dark shadow over the hardware
solutions previously discussed. With internal caching and
parallel activity being done on the chip, one will no longer
be able to gain processor state information
from simply
monitoring
the system bus. And as on-chip functions
become more complex, emulator vendors will no longer be
able to see into the chip through the pins making them
obsolete as well.
In [Chi191] an even stronger claim is made that
the debugging capabilities provided by the chip will need to
become more sophisticated. In future architectures, perhaps
the only possibility
to view and control the execution of
hardware is to gain that information
from the hardware
itself.

The previous
sections raised issues about the
effectiveness of our testing process and claimed that tcating
is currently being limited by tool functionality.
l%e goal
of this paper is to identify shortcomings in the embedded
testing proccas and propose a solution to those problems.
The view taken by the authors is that tool support for
embedded systems is lacking. Further, those approaches
currently used for gaining execution visibility
and control
will soon be obsolete for future architectures.
We propose
adding facilities to the underlying system to better support
testing and debugging tools for embedded software.
As stated previously,
the underlying
system is
composed of the hardware architecture and the run-time
system (RTS). Both are composed of data structures and
operations that implement
common system abstractions
such as processes, semaphores,
ports, timers, memory
heaps, and faultdexceptions.
It should be noted that there
is no distinct line between features of hardware and features
of the RTS. In fact, as these features and abstractions
become more standardized,
newer architectures
are
attempting to incorporate
them into their instruction
sets
~Nl%92].
In addition,
the implementation,of
a feature
may span parts of the architecture,
RTS, and compiler
generated code (i.e. fauhdexceptions).
5.1

Model
Debugging
System
Below is an illustration
of a debugging
system
(Figure 5.1). The data path from the debugging/testing
tool represents symbol table information
that allows the
tool to map machine level information
to source level

292

Test/Debug
Compiler
Generated
Code

\

Tool

1

A
e
x
t
e

3

Ada
Compilation

‚Äús-

Figure 5.1
constmcts.
The
ASIS
toolkit
provides
easy
required for this physical connection.
The
implementation
for this facility.
ASIS is a proposed
sw-tions describe ‚Äòtie architecture
additions
standard interface between an Ada library and any tool
interfaces in more detail.
requiring compilation information.
Of more interest is the communication
path
between the target processor and the testing tool. A tool
sits external to the rest of the embedded system, while the
RTS resides internally on the target board. At frost glance,
this conceptual
path seems rather difficult
to realize.
However, the implementation
becomes easier if thought
about as a typical host debugging system. Any debugging
system has a least two processes executing, one running
the test program and one running the &bugga.
These two
p=
Sa common physical machine, which allows
one process to gain information
about the other.
The
debugger procem simple requires data and computation
time, which it shares with the test program.
This same scenario is rcqnired
for embedded
debugging, except that the debugger process is split. Part
of the debugger process runs on the target machine and part
runs on the host. The goal is to minimize the portion that
must be run on the target so that it does not intrude on
execution
of the test program.
To realize this nonintrusive
execution
of the debug software,
the target
1) Execute debug code only at a break poinL
2) Run the debugger as a separate process, or
3) Provide a separate execution unit to execute the
debugger.
The details of these options are explored in depth later in
this paper.
The problem now lies with the interfa=
between
the embedded part of the debugger (intermddebuggcr)
and
the portion that lies on the host (external-debugger).
The
solution requires hardware additions that will be discussed
later in this paper. A high level view is given in figure
5.2. In this figure, the tool makes logical calls to services
provided by the RTS. These calls are actually implemented
by the debugging system through data passed between the
internal and external debuggers.
Hardware additions arc

1

I

next two
and RTS

I

Figure 5.2

-=

The past decade has seen hug{: advances in
microprocessor
designs.
Several of these advancements
were listed previously and include pipelining
and separate
functional
units.
The concept
of partitioning
a
microprocessor in order to perform parallel activities is of
great interest to this work.
It was noted
earlier
that these parallel
amputations
severely restrict current methods for testing
and debugging embedded systems, since on{e must simulate
a great amount of computations.
However,
debugging
tools can also use architectural
parallelism
to their
advantage. If a hardware design is partitioned successfully
to allow certain activities to occur concurrently,
then the
testing and debugging methodologies
might wish to add

293

their own computational
requirements to the list of parallel
activities.
This section will explore additions to hardware
architectures.
No claim is made as to the costs associated
with these features.
They assured y will require space
(transistors) and possibly even add to the execution cycles
required to implement certain instructions.
6.1 Hardware
Partitioning
of Memory
One primary concern for industry is reducing the
huge volume of retests associated with development.
The
current testing process ensures that errors are revealed late,
which forces retesting
large portions
of the system.
Despite correcting
these problems, industry will still be
faced with software that is constantly changing. Software
is deceivingly
easy to change and often the element of a
system assigned to unknown or ‚Äúrisky‚Äù aspects during
design.
Changing software is extremely expensive late in
the development
for critical
systems.
Such systems
typically
have requirement
that an error raised in one
portion of the system won‚Äôt interfere
with the correct
operation
of the rest of the system.
Current software
certification
agencies ~AA85]
have several software
restricdons including
‚óè Any modikation
made to a software module forces the
retesting of all other modules operating on that same
physical device.
. All software on a device must be developed under the
highest level of criticality of any module that will
execute on the same device.
Without
the ability
of hardware to guarantee software
boundaries, such requirements must be enforced. However,
these requirements
add a great deal of costs to software
development.
Consequently,
software is often physically
partitioned
based on critical level, rather than design
factors.
Partitioning
software modules based on critical
levels greatly interferes
with the design process.
One
would rather partition
modules based on factors such as
processor utilization
and inter-module
exmnmnication
requirements.
In fact, load balancing and p17XXsSmigration
are techniques
that would not be usable by embedded
system developers unless all software is developed at the
highest critical level.
The solution
to these issues is hardware
partitioning.
Each process should have it‚Äôs own protected
address space that is not accessible by any other process.
In addition, sets of processes may wish to share memory.
The processor should tdso provide the capability to restrict
access to segments of memory based on some criteria.
6.2 Computational
Facilities
for
Debugger.
The debugging
system is partitioned
into an
internal debugger and au external debugger. The internal
debugger must physically
exist on the target board and
communicate
with the external debugger through some
dedicated medium. The internal debugger will also require
execution from the target without
interfering
with the

operation of the application
program.
There are two
possible scenarios
‚óè The internal
debugger runs as a regular process on the
-Or
The architecture provides separate facilities to execute
the internal debugger code
In either case, control is transfered to the debugger when a
breakpoint is encountered
In the fiit
scemuio, the debugger is executed by
the processor as any other process.
If the debugger
executes as a low-level process, it would not interfere with
the operation of the rest of the system. However, this is
not a feasible approach.
Most intern-sting errors occur
during peak system loads, which would mean that the
debugger could only execute when the probability
of an
error occurring was low. Another approach wouId be to
execute the internal debugger as a periodic process of high
priority and design the entire system to take this process
into account when determining issues such as scheduling.
The second scenario requires the target processor
to provide some form of computational
facilities.
This
extra execution will certainly
require some amount of
utilization
of architecture
resources such as internal
registers and bus accesses.
The simplest
example of
architecture facilities would be a machine that contirtuaU y
dumps some representation of the instruction it is currently
executing.
This would require a dedicated bus to the
external world (proposed later in this section) and that
additional circuitry be attached to the computation units to
gain access to the current instruction.
The problem
with fis
approach
is that the
processor is not aware of what data is required by the tools
at the other end. Therefore, it must dump everything.
At
high processor speeds, the amount of information
being
sent could become overwhelming.
However, the data could
be faltered and then captured so that a tool could parse it
later and recreate an execution history of the program. The
hardware required for filtering
is not trivial and requires
great speed and storage capaaty to maintain pace with the
target processor.
lle next step is to allow software to dictate the
information
sent by the processor. The functional
unit of
the hardware sed.ing messagea could be implemented
as a
state machine, emitting
different
messages based on its
current state. The default state would be all processor
transactions. Basically, in this eontiguration,
the processor
is performing
the filtering
rather than the external
debugger.
This addition
should
not add much in
complexity to the hardware architecture and would greatly
reduee the wmplexity
of the external debugging hardware.
The final step is to take the (now
stateful)
functional
unit and make it programmable.
Instead of a
state machine, it now becomes a complete functional
unit
within the processor itself.
The internal debugger code
would then be loaded into this portion of the prowssor at
boot time and reside there for the entire execution,
transmitting
and receiving
messages to and from the
external debugger.
‚óè

294

6.3 Hardware
Break
Points
Software break points are intrusive and require
instructions be inserted into the code of the test program.
Conditioned
break points present a more significant
problem, since they require a computation every time they
are encountered to determine if the proper conditions are
met to halt execution.
Such breakpoints are unacceptable
for d-time
programs.
To resolve this issue, architectures need to provide
the capability
to set breakpointa in hardware.
A set of
registers would be classified
as BreakPoint
Registers
(BPR), which the processor would check against the
operands for each instruction.
Two types of breakpoints
are required, data and instruction.
Each data BPRs inside
the processor would be compared with the address of every
data operand for each instruction.
Instruction BPRs would
be compared with instruction
addresses or type. When a
match occurs, a breakpoint
fault would be raised and
control trsnafered to the internal debugger
Upon returning
from a break, the processor is
required
to restart execution
precisely
where it had
terminated.
The state of the processor consists of all it‚Äôs
internal registers, including
any pipeline information
and
cache memory. These values must be saved automatically
when a break is encountered.
Another issues is that of conditional breakpoints.
Such breakpoints
require computations
by the processor
that run in the background behind the program under test.
The evaluation of the conditional expression must begin far
enough in advance so that it may complete before the
processor has passed the breakpoint
location.
This
evaluation will require memory accesses, raising additional
problems. The current value of operands in the expressions
must be available to the processor, which might involve
accessing it from memory or cache. Any accesses to
memory must be scheduled in such a manner that they do
not block any resources required by the program under test.
F@dly, the value used must be valid and not in danger of
-g

before the breakpoint.
While the problems raised above seem difficult,
they are not insurmountable.
The extend
debugger must
compile the conditional expression and download the code.
At that point it can determine the scheduktbili~
of this
evaluation by comparing the @e for the conditional to the
other code that will occur in parallel. The user could then
be notified of problems with their additional
breakpoint.
The hardware is responsible for detecting any collisions in
parallel activity
and must not assume the debugger is
always accurate. Any debugger activity intruding
on the
behavior of the test program is important information
and
must be flagged by the processor.
The primary additions required for hardware break
points are additional registers from the architecture and the
logic necessary to compare them with the operands of the
current instruction.
To support conditional
breakpoints,
the processor must provide background
computational
support.
This support could come from a portion of the
processor dedicated to conditional breakpoints, or the code

could be downloaded to the internal debugger, given the
internal debugger support described previously.
6.4 Architectural
Support
for Abstractions
As common programming
paradigms
become
more refined, architectures will begin to inax-porate them
into their instruction
sets. It would be unlikely
that the
only abstractions supported by architectures would remain
simple data types (integer,
real) and their associated
operations (add, subtract, convert). Other abstractions such
as processes,
semaphores,
ports,
timers,
memory
management,
and faults
that are found
in typical
applications
should be supported as well, along with
associated operations on those abstractions.
M&ing
hardware to another level of abstraction
provides huge advantages for testing tools.
As stated
earlier, the architecture must be the basis for emulation
capabilities
and providing
execution visibility.
As the
hardware becomes more aware of programming elements, it
gains the abdity to send more meaningful messages to the
external world. A context switch between processes could
be sent with a single message, rather than the hundreds of
machine instructions it takes to implement the switch.
As the processor becom-es the single point of
visibility,
awareness of the progrdng
environment
becomes important.
A processor with a high-level
understanding
of program ,entities can emit fewer, more
meaningful
- messag-es than a processor
that only
comprehends low-level instructions.
6.5 Dedicated
Bus
Embedded
testing and debugging
require
an
interface that aliows the processor to communicate with the
external world without interfering with the behavior of the
system under test. This physical connection should reside
on the target and interface
extemall y tlhrough
some
detachable
mechanism.
The separation
technique
is
important,
since the external debugging system will be
detached from this connection once the system is placed
into operation.
The execution behavior alf the program
should be independent of whether or not any external tool
is attached.
Assuming an adequate physical connection,
the
next detmminah ‚Äúon is the protocol across it. ‚ÄòIle following
issues must be addressed
1) At what rate will messrwes need to be sent?
‚ÄòProcessor speed raises i~teresting problems, since future
speeds might be too quick for external
processing
techniques.
A solution to this problem was discussed
previously where the processor became aware of highlevel program elements.
The goal is to decrease the
number of messages required relative to the number of
machine cycles. 2) How much data is associated with a message?
If an architecture is required to emit large volumes of data
for messages, there may be instanms where the processor
must be suspended to allow the internal
debugging
hardware to catch up to the current processor state.
Higher level messages may compound
the problem,
since more maningful
messages might require more

295

This paper does not address the question of how
these interfaces should be UtdiZSd. Such SllSWerS should be
given by methodologies
and techniques for detecting and
locating
errors in embedded,
real-time
systems.
As
discussed earlier, the lack of these methods has led to
difficulties
for determining
adequate RTS services for
testing and debugging tools, which has forced a different
approach to determine the required operations.
Since the
RTS is in essence offering au implementation
of high-level
abstractions,
services that provide
visibility
into the
implementation
of RTS abstractions should adequately
fidfdl the needs of most testing and debugging techniques.
A standard currently exists for implementing
these
abstractions
in the MRTSI
[ARTE89]
and CIFO
[ARTE91].
In addition, most of the needs for testing and
debugging can be fulfiiled
by these standards. This is not
surprising, since our solution is based on implementation
visibility,
and the MRTSI and CEO are providing
an
implementation
interface. However, it is important to note
that this approach also indicates that implementations
that
support these staudards should require minimal additions to
and debugging tools as prOpOSed
by
akw SUppCWt testing
this paper. Below is a small discussion surrounding each
of these abstractions
and a list of shortcomings
in the
MRTSI and CIFO for testing and debugging.

information.
There is likely a tradeoff between message
level and data volume.
3) Is the connection bidirectional?
Visibility
concerns dictate that state information
travel
However,
methods
requiring
out of the processor.
control
of the executing
program require that state
information
travel the other direction.
Protoczds must be
in place to handle contention across the bus and those
must be extremely well defined, due to the extreme data
rate that could will be emmuntered across the bus.
4) Who is the active element in sending message9?
Either the processor or the RTS must determine the
information
sent from the processor.
The processor
cannot provide all the state information needed, while the
RTS will likely not be able to maintain adequate speeds
for sending messages.
These questions play a role in determining
the interface
between the internal and external debuggers.
A likely
solution would be a master-slave relation, where either the
internal or external debugger regulated the other. This
scenario does not seem likely, since each has such critical
processing concerns. Therefore, each will likely execute
independently,
while communication
is handled via some
bus and protocol.
There does exist a master-slave relationship
in
respect to the bus, howevex.
During program executiw,
the internal
debugger
must ‚Äòownn the bus, since it‚Äôs
processing concerns are the greatest.
It must meet the
message sending deadlines without altering computations
in other parts of the system.
There are points dting
execution where the extend
debugger must aeiz cmtrol.
If the internal debugger cannot allocate the bus to meet the
demands of the extcmal debuggm, the user must be notifkd
that their requested operation
cannot be accomplished
during a real-time execution.
The final determination
is that of the active
element
within
the processor.
There are two basic
approaches to detemnining control of the internal debugging
activities.
In the fiit
the processor is active and becomes
responsible for sending messages to the extend debugger.
‚Äòfhesecondappmachuse
aaspecialdebugge
rportionofthe
RTS to emit messagea, which is loaded into a dedicated
functional
unit within
the architecture.
Tools require
information
maintained
by both the architecture and the
RTS. Perhaps the solution lies between the two where
both the RTS and architecture have the ability to dump
messages, depending on the cmrcnt mquiremcnts dictated by
the external tool.

7.1

Processes
Concurrency
is a common abstraction
used in
embedded systems. A design can be decomposed without
concern for computational
resources, which can then be
determined
by a scheduler
during
run-time.
A&
irqplements concurren cy through tasks and task types. The
CIFO and MRTSI
provide
extensive
tasking
support
includlng
identifieation,
creation
and activation,
communication
through rendezvous, concurmat access to
shared entities,
and support for scheduling
control.
Elements of interest that are not provided by the CIFO or
MRTSI include
‚óè Task State - A developer
must have the ability to query
and modify the task state for each task in their system.
However, a modification
could leave the RTS in an
inanaistent
state. For example, changing a task‚Äôs
state from ‚Äúdelaying‚Äù to %unning‚Äù without removing
it from the &lay queue would place the RTS into a
state that could not be achieved
through
normal
execution. However, the same modification
ability is
available on typical debugging systems and should be
offered by em beddcddebwrgera as Wd.
‚óè Commm&ation
and Synchronization
- A developer
must have the ability tb view and modify eaeh entry
queue to determine the concurrent state of the system.
Again, modifications
could leave the RTS in an
unobtainable state.
‚óè Scheduling
Control
- In addition
to the extensive
operations provided by the CIFO for concurrency
control, a developer must have awess to the dispatch
port (or ports for muhiprqxssor
systems).

7. Run-Time
Svs tern Additions
The RTS requirements
deseribe an interface
between a tool and the underlying
system.
This is a
logical interface requiring substantial hardware support as
outlined
above.
An obvious goal is to minimize
the
required data and computational requhements of the internal
debugger as well as the required communications
between
the internal and external debuggers.

7.2

296

Interrupt

Management

One of our criticism of the current approach to
embedded testing is that timing errors are revealed late in
Interrupts are very related to
the development
process.
timing issues and their correctness is an important element
in embedded testing. Therefore, support for interrupts is
extremely
important
to target testing and debugging.
Faalities
provided through the CIFO and MRTSI would
allow developers
to bind various
interrupt
handling
routines, enable and disable certain interrupts, mask and
unmask interrupts,
and generate software interrupts
all
controlled dynamically duting program a program test.
7.3

Time
Management
As stated earlier,
important‚Äù to target testing
target tools require sfilaertt
time. Tools must be allowed
(although
such modifications
results) and the delay Iist of
by the RTS.

timing
issues are extremely
and debugging.
Therefore,
control over issues relating to
to view and modify the clock
might produce undefined
waiting processes maintained

7.4 Memory

Management
Dynamic
memory
is not typically
used by
due to diffldtk%
in dcmonstradng
embedded ti@iC4itiOliS
reliability.
However,
future
systems
will
likely
incorporate
algorithms
that requite dynamic storage. In
addition, memory ~agemcnt
for dynamic allocations is
part of a RTS and should therefore be included in RTS
visibility
and control discussions.
A tool will likely
require that ability to demonstrate an application programs
behavior when memory is exhausted.
The MRTSI would need to be extended to provide
operations that mim
a collection
making it smaller to
show execution behavior when memory is exhausted or
larger to demonstrate correct execution should a collection
be expanded by the developer.
Resizing is not cheap and
could require a gnat deal of computation and data transfers,
depending on an implementation.
7.5

Exception/Fault
Handling
Proper handling of exceptional events is evaluated
during hardwaresoftware
integration
testing. Therefore,
tools require a great deal of cattrol over exceptions and
One must be able to raise an
recovery mechanisms.
exception
or fault during program execution and also
modify handler binding during execution.
Another question of interest might be to locate the
handler for a given fault or exception at a given program
location.
Such information
is not easily gained from the
underlying
system.
The compiler
is responsible
for
handling exception propagation [ARTE89], so &k
‚Äú ‚Äú g
the handler from only RTS information
might be an
impossibility
and is at best resolved uniquely for each
compilation
system.

architectural and RTS additions. The architectural additions
will certainly be costly in both time and space, requiring
space (transistors)
on the chip and access to internal
registers and busses that could cause contention and slow
the execution
of other instructions
provided
by the
architecture.
However, the RTS additions
are minimal.
We defined
the needs of testing
as making
the
implementation
details of common system abstractions
visible and then determined the functionality
required to
view and control them.
The ARTEWG‚ÄôS
MRTSI
and
CIFO provided an outstanding basis for this approach.
The RTS additions are admittedly
weak.
Our
initial goal was to have the methodologies
and techniques
used for testing embedded, real-time
systems drive the
operations
required
by the RTS.
Unfort.tmatcl y, such
methods do not yet exist. As stated earlier, testing and
debugging of embedded, real-time software remains a black
art, with ad hoc methods and techniques. While there has
been much research into the concurrency and distribution
issues, none has examined real-time constraints, embedded
environments,
and other issues relating
to embedded
systems Perhaps the MRTSI and CIFO are sufficient
for
implementing
target level testing and debugging
tools.
However, this question cannot fully be resolved until more
formal methods exist.
Our next step is to evaluate the additions
and
determine their feasibility.
Questions relating the cost of
these additions to au architecture and RTS in terms of time
and space must be answered.
Also, a more complete
mapping should exist between the added feattues and the
impact they have on the desired features.
One can then
make a valid comparison between a feature and the costs
associated with it.
‚Äòl‚Äôhe embedded contmllermark~
is currently huge,
but has only begun to require the computational
powers
~SOCiKltti With lUiCrOpKXXWOrS. Embedded i@k.i3tiOttS
have traditional
been event driven rather than computation
dependent. Due to their light weigh~ easy con@mbility
and expansibility,
and lower design complexity,
computers
are quickly being chosen over mechanical techniques for
controlling
devices. As this transition continues, the size
and complexity
of embedded
programs
will
grow.
Controllers will not only have strict timing requirements,
but also have significant
computational
needs as well.
This combination
requires new approaches to our current
testing process for embedded systems and therefore, more
effective tools to aid in testing and debugging embedded
applications.
References

[ARTES9]

Ada Run-time
Environment
Working
Oroup,
‚ÄúA Model
Run-Time
System
Interface for A&m Ada Letters, January,
1989.

[ARTE91]

Ada Run-time
Environment
Working
Group, ‚ÄúCatslogue of Interface Features

s

co nclusions
The goal of this paper is two fold.

The first goal
is to identify defkienaes
in embedded system testing and
raise questions about the future of current tools.
The
second is to propose a solution to these problems through

297

and Options
for the Ada Runtime
Environment,‚Äù
Special Edition of Ada
Letters, Fall 1991 (fI).

lyxn6J

Tai, K.C., ‚Äú&producing
Testing of Ada
Tasking Programs,‚Äù IEEB Transactions
on Software Engineering,
1986.

[CHIL91]

Child,
Jeffrey,
‚Äú32-bit
Emulators
Struggle with Processor Complexities,n
Computer Design, May 1,1991.

~A191]

DD83]

Department
of Defense,
Reference
Manual
for the Ada Programming
Language,
ANSI/MIL-STD1815a,
United States DoD, 1983.

Tai, K.C., Carver, R.H., and Obaid,
E.E., ‚ÄúDebugging
Concurrent
Ada
Programs by Deterministic
Execution,‚Äù
IEEE
Transactions
on
Software
Engineering, January, 1991.

~AYL80]

Taylor,
R.N.
and Osterweil,
L. J.,
‚ÄúAnomaly
Detection
in Concurrent
Software by Static Data Flow Analysis,‚Äù
IEEE
Transactions
on
Software
Engineering, May, 1980.

~AYIJ33]

Taylor,
R.N., ‚ÄúA General
Purpose
Algorithm
for Analyzing
Concurrent
Programs,‚Äù
Communications
of the
ACM, ~y,
1983.

Federal Aviation
Association,
Software
Consideration
in Airlx)me
Systems and
Equipment
Certification,
RTCA/DO178A, 1985.
[GILL88]

Gilles, Jeff aud Ford, Ray, ‚ÄúA Guided
Tour Through
a Window
Oriented
Debugging Environment
for Embedded
Real Time
Ada
Systems,‚Äù
IEEE
Transactions
on Software Engineering,
1988.

&IAm78]

Hansen, B., ~eproduable
Testing of
Monitors,‚Äù
Software-practice
and
Experience, Volume 8,1978.
Hembold,
D. and Luckham,
D.,
‚ÄúDebugging
Ada Tasking
Rograms,‚Äù
IEEE software, March, 1985.

Iw‚ÄôfJ=l

Intel Corporation,
i960
Architecture
programmer‚Äôs
Manual, 1993.

KOEH91]

Koehnemann,
H.E. and LindquisL
T.E.,
‚ÄúRuntime Control of Ada Rendezvous for
Testing and ~U@llg,‚Äù
Procedm - gs of
the 24th Hawaii International Conference
on System Sciences, Volume II, 1991.

Extended
Reference

LeDoux, C, and Parker, D.S., ‚ÄúSaving
Traces for Ada Debugging,‚Äù
Ada in Use
Proceedings
of the Paris Conference,
1985.
Km]

Lyttle, D. and Ford, R., ‚ÄúA Symbolic
Debugger for Red-Time Embedded Ada
Software,‚Äù
Software
- Practice
and
Experience, May 1990.

@fAUG851

Mauger, C. and Pammett K., ‚ÄúAn EventDriven Debugger for Ati‚Äù
Ada in Use:
Proceedings
of the Paris Conference,
1985.

298

1

On time-reversibility of linear stochastic models
Tryphon T. Georgiou and Anders Lindquist

arXiv:1309.0165v1 [cs.SY] 31 Aug 2013

Abstract
Reversal of the time direction in stochastic systems driven by white noise has been central throughout the
development of stochastic realization theory, filtering and smoothing. Similar ideas were developed in connection
with certain problems in the theory of moments, where a duality induced by time reversal was introduced to
parametrize solutions. In this latter work it was shown that stochastic systems driven by arbitrary second-order
stationary processes can be similarly time-reversed. By combining these two sets of ideas we present herein a
generalization of time-reversal in stochastic realization theory.

I. I NTRODUCTION
Time reversal of stochastic systems is central in stochastic realization theory (see, e.g., [1], [2], [3],
[4], [5], [6], [7], [8]), filtering (see [9]), smoothing (see [10], [11], [12]) and system identification. The
principal construction is to model a stochastic process as the output of a linear system driven by a noise
process which is assumed to be white in discrete time, and orthogonal-increment in continuous time. In
studying the dependence between past and future of the process, it is natural to decompose the interface
between past and future in a time-symmetric manner. This gives rise to systems representations of the
process running in either time direction, forward or backward in time.
In a different context (see [13]) a certain duality between the two time-directions in modeling a
stochastic process was introduced in order to characterize solutions to moment problems. In this new
setting the noise-process was general (not necessarily white), and the correspondence between the driving
inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics.
In the present note, we combine these two sets of ideas to develop a general framework where two
time-opposite stochastic systems model a given stochastic process. We study the relationship between
these systems and the corresponding processes. In particular, we recover as a special case certain results
of stochastic realization theory ([1], [5], [10]) from the 1970‚Äôs using a novel procedure.
In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving generating processes, in opposite time-directions, via causal
and anti-causal mappings, respectively. In Section III we utilize this mechanism in the context of general
output processes and, similarly, introduce a pair of time-opposite models. Finally, in Section IV, we draw
connection to literature on time reversibility and related issues in physics, and we indicate directions for
future research.
II. S TATE

DYNAMICS AND ALL - PASS EXTENSION

In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. As
usual, in discrete-time these take the form of a set of difference equations
x(t + 1) = Ax(t) + Bu(t)

(1)

This research was supported by grants from AFOSR, NSF, VR, and the SSF.
Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota, tryphon@umn.edu
Department of Automation, Shanghai Jiao Tong University, Shanghai, China, and Center for Industrial and Applied Mathematics and
ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden, alq@kth.se

2

where t ‚àà Z, A ‚àà Rn√ón , B ‚àà Rn√óp , n, p ‚àà N, A has all eigenvalues in the open unit disc D = {z | |z| < 1},
and u(t), x(t) are stationary vector-valued stochastic processes. The system of equations is assumed to be
reachable, i.e.,


rank B, AB, . . . An‚àí1 B = n,
(2)
and non-trivial in the sense that B is full rank.

In continuous-time, state-dynamics take the form of a system of stochastic differential equations
dx(t) = Ax(t)dt + Bdu(t)

(3)

where, here, u(t), x(t) are stationary continuous-time vector-valued stochastic processes. Reachability
(which in this case, is equivalent to controllability) of the pair (A, B) is also assumed throughout and the
condition for this is identical to the one for discrete-time given above (as is well known). In continuous
time, stability of the system of equations is equivalent to A having only eigenvalues with negative real
part, and will be assumed throughout along with the condition that B has full rank.
In either case, discrete-time or continuous-time, it is possible to define an output equation so that
the overall system is all-pass. This is done next. The assumptions of stationarity and constant parameter
matrices is made for simplicity of notation and brevity and can be easily removed.
A. All-pass extension in discrete-time
Consider the discrete-time Lyapunov equation
P = AP A‚Ä≤ + BB ‚Ä≤ .

(4)

Since A has all eigenvalues inside the unit disc of the complex plane and (2) holds, (4) has as solution a
matrix P which is positive definite. The state transformation
1

Œæ = P ‚àí 2 x,

(5)

and
1

1

1

F = P ‚àí 2 AP 2 , G = P ‚àí 2 B,

(6)

Œæ(t + 1) = F Œæ(t) + Gu(t).

(7)

brings (1) into

For this new system, the corresponding Lyapunov equation X = F XF ‚Ä≤ + GG‚Ä≤ has In as solution,
where In denotes the (n √ó n) identity matrix. This fact, namely, that
In = F F ‚Ä≤ + GG‚Ä≤
implies that this [F, G] can be embedded as part of an orthogonal matrix


F G
U=
,
H J

(8)

(9)

i.e., such that UU ‚Ä≤ = U ‚Ä≤ U = In+p .
Define the transfer function
U(z) := H(zIn ‚àí F )‚àí1 G + J

(10)

3

corresponding to
Œæ(t + 1) = F Œæ(t) + Gu(t)
uÃÑ(t) = HŒæ(t) + Ju(t).

(11a)
(11b)

x(t + 1) = Ax(t) + Bu(t)
uÃÑ(t) = BÃÑ ‚Ä≤ x(t) + Ju(t),

(12a)
(12b)

This is also the transfer function of

1

where BÃÑ := P ‚àí 2 H ‚Ä≤, since the two systems are related by a similarity transformation. Hence,
U(z) = BÃÑ ‚Ä≤ (zIn ‚àí A)‚àí1 B + J.

(13)

We claim that U(z) is an all-pass transfer function (with respect to the unit disc), i.e., that U(z) is a
transfer function of a stable system (obvious) and that
U(z)U(z ‚àí1 )‚Ä≤ = U(z ‚àí1 )‚Ä≤ U(z) = Ip .

(14)

The latter claim is immediate after we observe that, since U ‚Ä≤ U = In+p ,

 

Œæ(t + 1)
Œæ(t)
‚Ä≤
U
=
,
uÃÑ(t)
u(t)
and hence,
Œæ(t) = F ‚Ä≤ Œæ(t + 1) + H ‚Ä≤ uÃÑ(t)
u(t) = G‚Ä≤ Œæ(t + 1) + J ‚Ä≤ uÃÑ(t)

(15a)
(15b)

or, equivalently,
1

x(t) = P A‚Ä≤P ‚àí1 x(t + 1) + P 2 H ‚Ä≤ u(t)
u(t) = B ‚Ä≤ P ‚àí1 x(t + 1) + J ‚Ä≤ uÃÑ(t).

(16a)
(16b)

xÃÑ(t) := P ‚àí1x(t + 1),

(17)

xÃÑ(t ‚àí 1) = A‚Ä≤ xÃÑ(t) + BÃÑ uÃÑ(t)
u(t) = B ‚Ä≤ xÃÑ(t) + J ‚Ä≤ uÃÑ(t)

(18a)
(18b)

U(z)‚àó = B ‚Ä≤ (z ‚àí1 In ‚àí A‚Ä≤ )‚àí1 BÃÑ + J ‚Ä≤ .

(19)

Setting
(16) can be written

with transfer function

Either of the above systems inverts the dynamical relation u ‚Üí uÃÑ (in (12) or (11)).

u(t)‚ú≤

Fig. 1.

Realization (12) in the forward time-direction.

U

uÃÑ(t)‚ú≤

4

u(t)

U‚àó

‚úõ

Fig. 2.

uÃÑ(t)

‚úõ

Realization (18) in the backward time-direction.

B. All-pass extension in continuous-time
Consider the continuous-time Lyapunov equation
AP + P A‚Ä≤ + BB ‚Ä≤ = 0.

(20)

Since A has all its eigenvalues in the left half of the complex plane and since (2) holds, (20) has as
solution a positive definite matrix P . Once again, applying (5-6), the system in (3) becomes
dŒæ(t) = F Œæ(t)dt + Gdu(t).

(21a)

We now seek a completion by adding an output equation
duÃÑ(t) = HŒæ(t)dt + Jdu(t)

(21b)

U(s) := H(sIn ‚àí F )‚àí1 G + J

(22)

so that the transfer function

is all-pass (with respect to the imaginary axis), i.e.,
U(s)U(‚àís)‚Ä≤ = U(‚àís)‚Ä≤ U(s) = Ip .

(23)

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and
hence,
F + F ‚Ä≤ + GG‚Ä≤ = 0.
Utilizing this relationship we note that
(sIn ‚àí F )‚àí1 GG‚Ä≤ (‚àísIn ‚àí F ‚Ä≤ )‚àí1
= (sIn ‚àí F )‚àí1 (sIn ‚àí F ‚àí sIn ‚àí F ‚Ä≤ )(‚àísIn ‚àí F ‚Ä≤ )‚àí1
= (sIn ‚àí F )‚àí1 + (‚àísIn ‚àí F ‚Ä≤ )‚àí1 ,
and we calculate that
U(s)U(‚àís)‚Ä≤
= (H(sIn ‚àí F )‚àí1 G + J)(G‚Ä≤ (‚àísIn ‚àí F ‚Ä≤ )‚àí1 H ‚Ä≤ + J ‚Ä≤ )
= JJ ‚Ä≤ + H(sIn ‚àí F )‚àí1 (GJ ‚Ä≤ + H ‚Ä≤ )
(JG‚Ä≤ + H)(‚àísIn ‚àí F ‚Ä≤ )‚àí1 H ‚Ä≤ .
For the product to equal the identity,
JJ ‚Ä≤ = Ip
H = ‚àíJG‚Ä≤ .
Thus, we may take
J = Ip
H = ‚àíG‚Ä≤ ,

(24)

5

and the forward dynamics
dŒæ(t) = F Œæ(t)dt + Gdu(t)
duÃÑ(t) = ‚àíG‚Ä≤ Œæ(t)dt + du(t).

(25a)
(25b)

Substituting F = ‚àíF ‚Ä≤ ‚àí GG‚Ä≤ from (24) into (25a) we obtain the reverse-time dynamics
dŒæ(t) = ‚àíF ‚Ä≤ Œæ(t)dt + GduÃÑ(t)
du(t) = G‚Ä≤ Œæ(t)dt + duÃÑ(t).

(26a)
(26b)

xÃÑ(t) := P ‚àí1x(t)

(27)

dxÃÑ(t) = ‚àíA‚Ä≤ xÃÑ(t)dt + BÃÑduÃÑ(t)
du(t) = B ‚Ä≤ xÃÑ(t)dt + duÃÑ(t),

(28a)
(28b)

U(s)‚àó = B ‚Ä≤ (sIn + A‚Ä≤ )‚àí1 BÃÑ + Ip ,

(29)

BÃÑ := P ‚àí1 B.

(30)

Now defining
and using (5) and (6), (26) becomes

with transfer function

where
Furthermore, the forward dynamics (25) can be expressed in the form
dx(t) = Ax(t)dt + Bdu(t)
duÃÑ(t) = BÃÑ ‚Ä≤ x(t)dt + du(t)

(31a)
(31b)

U(s) = BÃÑ ‚Ä≤ (sIn ‚àí A‚Ä≤ )‚àí1 B + Ip .

(32)

with transfer function

III. T IME - REVERSAL

OF LINEAR STOCHASTIC SYSTEMS

The development so far allows us to draw a connection between two linear stochastic systems having
the same output and driven by a pair of arbitrary, but dual, stationary processes u(t) and uÃÑ(t), one evolving
forward in time and one evolving backward in time. When one of these two processes is white noise (or,
orthogonal increment process, in continuous-time), then so is the other. For this special case we recover
results of [1] and [5] in stochastic realization theory.
A. Time-reversal of discrete-time stochastic systems
Consider a stochastic linear system
x(t + 1) = Ax(t) + Bu(t)
y(t) = Cx(t) + Du(t)

(33a)
(33b)

with an m-dimensional output process y, and x, u, A, B are defined as in Section II-A. All processes are
stationary and the system can be thought as evolving forward in time from the remote past (t = ‚àí‚àû). In
particular,


x(t + 1)
is Ftu -measurable
y(t)

6

for all t ‚àà Z, where Ftu is the œÉ-algebra generated by {u(s) | s ‚â§ t}. Next we construct a stochastic
system
xÃÑ(t ‚àí 1) = A‚Ä≤ xÃÑ(t) + BÃÑ uÃÑ(t)
y(t) = CÃÑ xÃÑ(t) + DÃÑuÃÑ(t),

(34a)
(34b)

which evolves backward in time from the remote future (t = ‚àû), and for which


xÃÑ(t ‚àí 1)
is FÃÑtuÃÑ -measurable
y(t)
for all t ‚àà Z, where FÃÑtuÃÑ is the œÉ-algebra generated by {uÃÑ(s) | s ‚â• t}. The processes xÃÑ, x, uÃÑ, u relate as in
the previous section. More specifically, as shown in Section II-A,
uÃÑ(t) is Ftu -measurable
while
u(t) is FÃÑtuÃÑ -measurable
for all t, as examplified in Figures 1 and 2.
In fact, the all-pass extension (12) of (33a) yields
uÃÑ(t) = BÃÑ ‚Ä≤ x(t) + Ju(t)

(35)

It follows from (18b) that (35) can be inverted to yield
u(t) = B ‚Ä≤ xÃÑ(t) + J ‚Ä≤ uÃÑ(t),

(36)

where xÃÑ(t) = P ‚àí1 x(t + 1), and that we have the reverse-time recursion
xÃÑ(t ‚àí 1) = A‚Ä≤ xÃÑ(t) + BÃÑ uÃÑ(t).

(37a)

Then inserting (36) and
x(t) = P xÃÑ(t ‚àí 1) = P A‚Ä≤ xÃÑ(t) + P BÃÑ uÃÑ(t)
into (33b), we obtain
y(t) = CÃÑ xÃÑ(t) + DÃÑuÃÑ(t),

(37b)

CÃÑ := CP A‚Ä≤ + DB ‚Ä≤ .

(38)

where DÃÑ := DJ ‚Ä≤ and
Then, (37) is precisely what we wanted to establish.
Moreover, the transfer functions
W(z) = C(zIn ‚àí A)‚àí1 B + D

(39)

WÃÑ(z) = CÃÑ(z ‚àí1 In ‚àí A‚Ä≤ )‚àí1 BÃÑ + DÃÑ

(40)

W(z) = WÃÑ(z)U(z).

(41)

of (33) and

of (34) satisfy

In the context of stochastic realization theory, discussed next, U(z) is called structural function ([3], [4]).

7

u(t)‚ú≤

Fig. 3.

W

The forward stochastic system (33).

y(t)

WÃÑ

‚úõ

Fig. 4.

y(t)‚ú≤

uÃÑ(t)

‚úõ

The backward stochastic system (34)

1) Time-reversal of stochastic realizations.: Given an m-dimensional stationary process y, consider a
minimal stochastic realization (33), evolving forward in time, where now u is a normalized white noise
process, i.e.,
E{u(t)u(s)‚Ä≤} = Ip Œ¥t‚àís .
Since U, given by (13), is all-pass, uÃÑ is also a normalized white noise process, i.e.,
E{uÃÑ(t)uÃÑ(s)‚Ä≤ } = Ip Œ¥t‚àís .
From the reverse-time recursion (34a)
xÃÑ(t) =

‚àû
X

(A‚Ä≤ )k‚àí(t+1) BÃÑ uÃÑ(k).

k=t+1

Since, uÃÑ is a white noise process, E{xÃÑ(t)uÃÑ(s)‚Ä≤ } = 0 for all s ‚â§ t. Consequently, (34) is a backward
stochastic realization in the sense of stochastic realization theory.
B. Time-reversal of continuous-time stochastic systems
We now turn to the continuous-time case. Let
dx = Axdt + Bdu
dy = Cxdt + Ddu

(42a)
(42b)

be a stochastic system with x, u, A, B as in Section II-B, evolving forward in time from the remote past
(t = ‚àí‚àû). All processes have stationary increments and


x(t)
is Ftu -measurable
y(t)
for all t ‚àà R, where Ftu is the œÉ-algebra generated by {u(s) | s ‚â§ t}.
The all-pass extension of Section II-B yields
duÃÑ = du ‚àí BÃÑ ‚Ä≤ xdt

(43)

dxÃÑ = ‚àíA‚Ä≤ xÃÑdt + BÃÑduÃÑ
du = B ‚Ä≤ xÃÑdt + duÃÑ,

(44a)
(44b)

as well as the reverse-time relation

8

where xÃÑ(t) = P ‚àí1 x(t). Inserting (44b) into
dy = CP xÃÑdt + Ddu
yields
dy = CÃÑ xÃÑdt + DduÃÑ,
where
CÃÑ = CP + DB ‚Ä≤ .

(45)

dxÃÑ = ‚àíA‚Ä≤ xÃÑdt + BÃÑduÃÑ
dy = CÃÑ xÃÑdt + DduÃÑ.

(46a)
(46b)

Thus, the reverse-time system is

From this, we deduce that


xÃÑ(t)
y(t)



is FÃÑtuÃÑ -measurable

for all t ‚àà R. We also note that the transfer function
W(s) = C(sIn ‚àí A)‚àí1 B + D
of (42) and the transfer function
WÃÑ(s) = CÃÑ(sIn + A‚Ä≤ )‚àí1 BÃÑ + D
of (46) also satisfy
W(s) = WÃÑ(s)U(s)
as in discrete-time.
1) Time-reversal of stochastic realizations.: In continuous-time stochastic realization theory, (42) is a
forward minimal stochastic realization of an m-dimensional process y with stationary increments provided
u is a normalized orthogonal-increment process satisfying
E{du(t)du(t)‚Ä≤} = Ip dt.
Since U(s) is all-pass,
duÃÑ = du ‚àí BÃÑ ‚Ä≤ xdt

(47)

also defines a stationary orthogonal-increment process uÃÑ such that
E{duÃÑ(t)duÃÑ(t)‚Ä≤ } = Ip dt.
It remains to show that (46) is a backward stochastic realization, that is, at each time t the past increments
of uÃÑ are orthogonal to xÃÑ(t). But this follows from the fact that
Z ‚àû
‚Ä≤
xÃÑ(t) =
e‚àíA (t‚àís) BÃÑduÃÑ(s)
t

and uÃÑ has orthogonal increments.

9

IV. C ONCLUDING

REMARKS

The direction of time in physical laws and the fact that physical laws are symmetric with respect to
time have occupied some of the most prominent minds in science and mathematics ([14], [15], [16]).
These early consideration were motivated by no less an issue than that of the very nature of the quantum.
Indeed, Erwin SchroÃàdinger‚Äôs aim appears to have been to draw a classical analog to his famous equation.
A large body of work followed.
In particular, closer to our immediate interests, dual time-reversed models have been employed to
model, in different time-directions, Brownian or SchroÃàdinger bridges (see [17], [18]), a subject which is
related to reciprocal processes ([19], [20], [21], [22]). The topic of time reversibility has also been central
to thermodynamics, and in recent years studies have sought to elucidate its relation to systems theory (see
[23], [24]). Possible connections between this body of work and our present paper will be the subject of
future work.
The thesis of the present work is that under mild assumptions on a stochastic process, any model that
consists of a linear stable dynamical system driven by an appropriate input process can be reversed in
time. In fact, a reverse-time dual system along with the corresponding input process can be obtained via
an all-pass extension of the state equation. The correspondence between the two input processes can be
expressed in terms of each other by a causal and an anti-causal map, respectively.
The formalism of our paper can easily be extended to a non-stationary setting at a price of increased
notational, but not conceptual, complexity. Informally, and in order to underscore the point, if u(t) is
a non-stationary process and the linear system is time-varying, under suitable conditions, a reverse-time
system and a process uÃÑ(t) can be similarly constructed via a time-varying orthogonal transformation.
R EFERENCES
[1] A. Lindquist and G. Picci, ‚ÄúOn the stochastic realization problem,‚Äù SIAM J. Control Optim., vol. 17, no. 3, pp. 365‚Äì389, 1979.
[2] ‚Äî‚Äî, ‚ÄúForward and backward semimartingale models for Gaussian processes with stationary increments,‚Äù Stochastics, vol. 15, no. 1,
pp. 1‚Äì50, 1985.
[3] ‚Äî‚Äî, ‚ÄúRealization theory for multivariate stationary Gaussian processes,‚Äù SIAM J. Control Optim., vol. 23, no. 6, pp. 809‚Äì857, 1985.
[4] ‚Äî‚Äî, ‚ÄúA geometric approach to modelling and estimation of linear stochastic systems,‚Äù J. Math. Systems Estim. Control, vol. 1, no. 3,
pp. 241‚Äì333, 1991.
[5] M. Pavon, ‚ÄúStochastic realization and invariant directions of the matrix Riccati equation,‚Äù SIAM Journal on Control and Optimization,
vol. 18, no. 2, pp. 155‚Äì180, 1980.
[6] A. Lindquist and M. Pavon, ‚ÄúOn the structure of state-space models for discrete-time stochastic vector processes,‚Äù IEEE Trans. Automat.
Control, vol. 29, no. 5, pp. 418‚Äì432, 1984.
[7] G. Michaletzky, J. Bokor, and P. VaÃÅrlaki, Representability of stochastic systems. Budapest: AkadeÃÅmiai KiadoÃÅ, 1998.
[8] G. Michaletzky and A. Ferrante, ‚ÄúSplitting subspaces and acausal spectral factors,‚Äù J. Math. Systems Estim. Control, vol. 5, no. 3, pp.
1‚Äì26, 1995.
[9] A. Lindquist, ‚ÄúA new algorithm for optimal filtering of discrete-time stationary processes,‚Äù SIAM J. Control, vol. 12, pp. 736‚Äì746,
1974.
[10] F. Badawi, A. Lindquist, and M. Pavon, ‚ÄúOn the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary
linear stochastic systems,‚Äù in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on,
vol. 18. IEEE, 1979, pp. 505‚Äì510.
[11] F. A. Badawi, A. Lindquist, and M. Pavon, ‚ÄúA stochastic realization approach to the smoothing problem,‚Äù IEEE Trans. Automat. Control,
vol. 24, no. 6, pp. 878‚Äì888, 1979.
[12] A. Ferrante and G. Picci, ‚ÄúMinimal realization and dynamic properties of optimal smoothers,‚Äù Automatic Control, IEEE Transactions
on, vol. 45, no. 11, pp. 2028‚Äì2046, 2000.
[13] T. T. Georgiou, ‚ÄúThe CaratheÃÅodory‚ÄìFejeÃÅr‚ÄìPisarenko decomposition and its multivariable counterpart,‚Äù Automatic Control, IEEE
Transactions on, vol. 52, no. 2, pp. 212‚Äì228, 2007.
[14] E. SchroÃàdinger, UÃàber die Umkehrung der Naturgesetze. Akad. d. Wissenschaften, 1931.
[15] A. Kolmogorov, Selected Works of AN Kolmogorov: Probability theory and mathematical statistics. Springer, 1992, vol. 26.
[16] A. Shiryayev, ‚ÄúOn the reversibility of the statistical laws of nature,‚Äù in Selected Works of AN Kolmogorov. Springer, 1992, pp. 209‚Äì215.
[17] M. Pavon and A. Wakolbinger, ‚ÄúOn free energy, stochastic control, and SchroÃàdinger processes,‚Äù in Modeling, Estimation and Control
of Systems with Uncertainty. Springer, 1991, pp. 334‚Äì348.
[18] P. Dai Pra and M. Pavon, ‚ÄúOn the Markov processes of SchroÃàdinger, the Feynman-Kac formula and stochastic control,‚Äù in Realization
and Modelling in System Theory. Springer, 1990, pp. 497‚Äì504.

10

[19] B. Jamison, ‚ÄúReciprocal processes,‚Äù Probability Theory and Related Fields, vol. 30, no. 1, pp. 65‚Äì86, 1974.
[20] A. Krener, ‚ÄúReciprocal processes and the stochastic realization problem for acausal systems,‚Äù in Modelling, Identification and Robust
Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197‚Äì211.
[21] B. C. Levy, R. Frezza, and A. J. Krener, ‚ÄúModeling and estimation of discrete-time gaussian reciprocal processes,‚Äù Automatic Control,
IEEE Transactions on, vol. 35, no. 9, pp. 1013‚Äì1023, 1990.
[22] P. Dai Pra, ‚ÄúA stochastic control approach to reciprocal diffusion processes,‚Äù Applied mathematics and Optimization, vol. 23, no. 1,
pp. 313‚Äì329, 1991.
[23] W. M. Haddad, V. Chellaboina, and S. G. Nersesov, ‚ÄúTime-reversal symmetry, poincareÃÅ recurrence, irreversibility, and the entropic
arrow of time: From mechanics to system thermodynamics,‚Äù Nonlinear Analysis: Real World Applications, vol. 9, no. 2, pp. 250‚Äì271,
2008.
[24] ‚Äî‚Äî, Thermodynamics: A dynamical systems approach. Princeton University Press, 2009.

SOCRADES: A Web Service based Shop Floor
Integration Infrastructure
Luciana Moreira SaÃÅ de Souza, Patrik Spiess, Dominique Guinard,
Moritz KoÃàhler, Stamatis Karnouskos, and Domnic Savio
SAP Research
Vincenz-Priessnitz-Strasse 1, D-76131, Karlsruhe, Germany
Kreuzplatz 20, CH-8008, Zurich, Switzerland
{luciana.moreira.sa.de.souza, patrik.spiess, dominique.guinard,
mo.koehler, stamatis.karnouskos, domnic.savio} @sap.com

Abstract. On the one hand, enterprises manufacturing any kinds of
goods require agile production technology to be able to fully accommodate their customers‚Äô demand for flexibility. On the other hand, Smart
Objects, such as networked intelligent machines or tagged raw materials,
exhibit ever increasing capabilities, up to the point where they offer their
smart behaviour as web services. The two trends towards higher flexibility and more capable objects will lead to a service-oriented infrastructure
where complex processes will span over all types of systems ‚Äî from the
backend enterprise system down to the Smart Objects. To fully support
this, we present SOCRADES, an integration architecture that can serve
the requirements of future manufacturing. SOCRADES provides generic
components upon which sophisticated production processes can be modelled. In this paper we in particular give a list of requirements, the design,
and the reference implementation of that integration architecture.

1

Introduction

In the manufacturing domain, constant improvements and innovation in the
business processes are key factors in order to keep enterprises competitive in the
market. Manufacturing businesses are standing on the brink of a new era, one
that will considerably transform the way business processes are handled.
With the introduction of ubiquitous computing on the shop floor1 , an entirely new dynamic network of networked devices can be created - an Internet
of Things (IoT) for manufacturing. The Internet of Things is a concept which
first appeared shortly after 2000. Until now, several approaches to describe the
IoT have been undertaken of which most have focused on RFID technologies
and their application ([5, 13]).
Only recently, new technologies such as Smart Embedded Devices and Sensor
Networks have entered the scene and can be considered as part of the IoT [11].
1

In manufacturing, the shop floor is the location where machines are located and
products produced.

Smart Embedded Devices are embedded electronic systems which can sense their
internal state and are able to communicate it through data networks. In contrast
to this, Sensor Networks not only can measure internal states of their nodes,
but also external states of the environment. We group these three technologies RFID, Smart Embedded Devices, and Sensor Networks - under the notion Smart
Objects.
Smart Objects are the nerve cells, which are interconnected through the
Internet and thus build the IoT. RFID has already been proved to open fundamentally new ways of executing business processes, and the technology has
already been adopted by several key players in the industry. Therefore the focus
of this paper is on Smart Embedded Devices and Sensor Networks and their
effects on automatic business process execution.
Although client-server architectures still play an important role in the field
of business software systems, the Service Oriented Architecture (SOA) is on the
move and it is foreseeable that this architectural paradigm will be predominant
in the future. The integration of devices into the business IT-landscape through
SOA is a promising approach to connect physical objects and to make them available to IT-systems. This can be achieved by running instances of web services
on these devices, which moves the integration of back end applications, such as
Enterprise Resource Planning (ERP) systems, with the devices one step forward,
enabling them to interact and create an Internet of Services that collaborates
and empowers the future service-based factory.
Enabling efficient collaboration between device-level SOA and services and
applications that constitute the enterprise back-end on the other hand, is a
challenging task. The introduction of web service concepts at a level as low as
the production device or facility automation makes this integration significantly
less complex. But there are still differences between device-level SOA and the one
that is used in the back end. To name but a few of them, device-level services are
of higher granularity, exhibit a lower reliability (especially if they are connected
wirelessly) and higher dynamicity and are more focused on technical issues than
on business aspects.
These differences can be overcome by introducing a middleware between the
back end applications and the services that are offered by devices, service mediators, and gateways. This middleware adds the required reliability, provides
means to deal with services appearing and disappearing, and allows intermediate service composition to raise the technical interfaces of low-level services to
business-relevant ones.
In this paper we present the SOCRADES middleware for business integration; an architecture focused on coupling web service enabled devices with enterprise applications such as ERP Systems. Our approach combines existing
technologies and proposes new concepts for the management of services running
on the devices.
This paper is organized as follows: in section 2 we discuss the current state of
the art in coupling technologies for shop floor and enterprise applications. Section
3 presents the requirements for our approach, followed by section 4 where we

discuss our approach. We propose a prototype for evaluating our approach in
section 5 and perform an analysis in section 6. Section 7 concludes this paper.

2

Related Work

Manufacturing companies need agile production systems that can support reconfigurability and flexibility to economically manufacture products. These systems must be able to inform resource planning systems like SAP ERP in advance,
about the upcoming breakdown of a whole production processes or parts of them,
so that adaptation in the workflow can be elaborated.
Currently Manufacturing Execution Systems (MES) are bridging the gap
between the shop floor and ERP systems that run in the back end. The International Systems and Automation Society - 95 (ISA-95) derivative from the
Instrumentation Systems and Automation Society define the standards for this
interface [1]. Although MES systems exist as gateways between the enterprise
world and the shop floor, they have to be tailored to the individual group of
devices and protocols that exist on this shop floor.
By integrating web services on the shop floor, devices have the possibility
of interacting seamlessly with the back end system ([9, 8]). Currently products
like SIMATIC WinCC Smart Access [2] from Siemens Automation use SOAP
for accessing tag based data from devices like display panels to PC‚Äôs. However,
they neither provide mechanisms to discover other web-service enabled devices,
nor mechanisms for maintaining a catalogue of discovered devices.
The domain of Holonic Manufacturing Execution Systems (HMS) [6] is also
relevant to our work. HMS are used in the context of collaborative computing,
and use web service concepts to integrate different sources and destinations inside a production environment. They do, however, not offer support to process
orchestration or service composition.
Amongst others, European Commission funded projects like SIRENA [4]
showed the feasibility and benefit of embedding web services in production devices. However, since these were only initial efforts for proving the concept, not
much attention has been given to issues such as device supervision, device life cycle management, or catalogues for maintaining the status of discovered devices,
etc. The consortium of the SOCRADES project has integrated partners, code
and concepts from SIRENA, and aims to further design and implement a more
sophisticated infrastructure of web-service enabled devices. SODA (www.sodaitea.org) aims at creating a comprehensive, scalable, easy to deploy ecosystem
built on top of the foundations laid by the SIRENA project.
The SODA ecosystem will comprise a comprehensive tool suite and will target industry standard platforms supported by wired and wireless communications. Although EU projects like SIRENA showed the feasibility and benefit of
embedding web services in devices used for production, they do not offer an
infrastructure or a framework for device supervision or device life cycle. They
neither do provide a catalogue for maintaining the status of discovered devices
[4]. Changes due to the current development are moving towards a more promis-

ing approach of integrating shop floor devices and ERP systems more strongly
[14].
Some authors are criticizing the use of RPC-style interaction in ubiquitous
computing [12] (we consider the smart manufacturing devices a special case
of that). We believe this does not concern our approach, since web services also
allow for interaction with asynchronous, one-way messages and publish-subscribe
communication.
SAP xApp Manufacturing Integration and Intelligence (SAP xMII) is a manufacturing intelligence portal that uses a web server to extract data from multiple sources, aggregate it at the server, transform it into business context and
personalize the delivered results to the users [7]. The user community can include existing personal computers running internet browsers, wireless PDAs or
other UIs. Using database connectivity, any legacy device can expose itself to
the enterprise systems using this technology.
The drawback of this product is that every device has to communicate to the
system using a driver that is tailored to the database connectivity. In this way,
SAP xMII limits itself to devices or gateway solutions that support database
connectivity.
In [10], we proposed a service-oriented architecture to bridge between shop
floor devices and enterprise applications. In this paper however, building on both
our previous work and SAP xMII, we show how the already available functionality of xMII can be leveraged and extended to provide an even richer integration
platform. The added functionality comprises integration of web service enabled
devices, making them accessible through xMII, and supporting the software life
cycle of embedded services. This enables real-world devices to seamlessly participate in business processes that span over several systems from the back end
through the middleware right down to the Smart Objects.

3

System Requirements

As embedded technology advances, more functionality that currently is hosted
on powerful back end systems and intermediate supervisory devices can now be
pushed down to the shop floor level. Although this functionality can be transferred to devices that have only a fraction of the capabilities of more complex
systems, their distributed orchestration in conjunction with the fact that they
execute very task-specific processing, allows us to realise approaches that can
outperform centralised systems in means of functionality. By embedding web
services on devices, these can become part of a modern Enterprise SOA communication infrastructure.
The first step to come closer to realize this vision, is to create a list of requirements. We have come up with this list through interviews with project
partners and customers from the application domain, as well as a series of technical workshops with partners form the solution domain. As usually done in
software engineering, we separated the list into functional and non-functional
requirements.

Functional Requirements
‚Äì WS based direct access to devices: Back end services must be able to
discover and directly communicate with devices, and consume the services
they offer. This implies the capability of event notifications from the device
side, to which other services can subscribe to.
‚Äì WS based direct access to back end services: Most efforts in the
research domain today focus on how to open the shop floor functionality to
the back end systems. The next challenge is to open back end systems to the
shop floor. E.g. devices must be able to subscribe to events and use enterprise
services. Having achieved that, business logic executing locally on shop floor
devices can now take decisions not only based on its local information, but
also on information from back end systems.
‚Äì Service Discovery: Having the services on devices will not be of much
use if they can not be dynamically discovered by other entities. Automatic
service discovery will allow us to access them in a dynamic way without
having explicit task knowledge and the need of a priori binding. The last
would also prevent the system from scaling and we could not create abstract
business process models.
‚Äì Brokered access to events: Events are a fundamental pillar of a service
based infrastructure. Therefore access to these has to be eased. As many
devices are expected to be mobile, and their online status often change (including the services they host), buffered service invocation should be in-place
to guarantee that any started process will continue when the device becomes
available again. Also, since not all applications expose web services, a pull
point should be realised that will offer access to infrastructure events by
polling.
‚Äì Service life cycle management: In future factories, various services are
expected to be installed, updated, deleted, started, and stopped. Therefore,
we need an open ways of managing their life cycle. Therefore the requirement
is to provide basic support in the infrastructure itself that can offer an open
way of handling these issues.
‚Äì Legacy device integration: Devices of older generations should be also
part of the new infrastructure. Although their role will be mostly providing
(and not consuming) information, we have to make sure that this information can be acquired and transformed to fit in the new WS-enabled factory.
Therefore the requirement is to implement gateways and service mediators
to allow integration of the non-ws enabled devices.
‚Äì Middleware historian: In an information-rich future factory, logging of
data, events, and the history of devices is needed. The middleware historian
is needed which offers information to middleware services, especially when
an analysis of up-to-now behavior of devices and services is needed.
‚Äì Middleware device management: Web service enabled devices, will contain both, static and dynamic data. This data can now be better and more
reliably integrated to back end systems offering a more accurate view of the
shop floor state. Furthermore by checking device data and enterprise inventory, incompatibilities can be discovered and tackled. Therefore we require

approaches that will effectively enable the full integration of device data and
their exploitation above the device-layer.
Non-Functional Requirements
‚Äì Security support: Shop floors are more or less closed environments with
limited and controlled communication among their components. However,
because of open (and partially wireless) communication networks, this is fundamentally changing. Issues like confidentiality, integrity, availability must
be tackled. In a web service mash-up - as the future factory is expected to
be -, devices must be able to a) authenticate themselves to external services
and b) authenticate/control access to services they offer.
‚Äì Semantics support: This requirement facilitates the basic blocks primarily for service composition but also for meaningful data understanding and
integration. Support for the usage of ontologies and semantic-web concepts
will also enhance collaboration as a formal description of concepts, terms,
and relationships within a manufacturing knowledge domain.
‚Äì Service composition: In a SOA infrastructure, service composition will
allow us to build more sophisticated services on top of generic ones, therefore allowing thin add-ons for enhanced functionality. This implies a mixed
environment where one could compose services a) at device level b) at back
end level and c) in a bidirectional cross-level way.
In the above list we have described both, functional and non-functional requirements. In our architecture these requirements will be realized through components, each one offering a unique functionality.

4
4.1

Architecture
Overview

In this chapter, we present a concrete integration architecture focusing on leveraging the benefits of existing technologies and taking them to a next level of integration through the use of DPWS and the SOCRADES middleware. The architecture proposed in Figure 1 is composed of four main layers: Device Layer, SOCRADES middleware (consisting of an application and a device services part),
xMII, and Enterprise Applications.
The Device Layer comprises the devices in the shop floor. These devices when
enabled with DPWS connect to the SOCRADES middleware for more advanced
features. Nevertheless, since they support web services, they provide the means
for a direct connection to Enterprise Applications. For the intermediate part
of the SOCRADES architecture, bridging between enterprise and device layer,
we identified an SAP product that partly covered our requirements: SAP xApp
Manufacturing Integration and Intelligence (SAP xMII). The features already
available in xMII are:

ENTERPRISE APPLICATIONS
HTML-GUI /
Applets

SAP
Protocols

Web Services

SOCRADES MIDDLEWARE APP SERVICES

xMII
Visualization Services

SAP Connectivity

Invoker

Applets
Display Controls
Displays

Asynchronous
Buffer

SAP Transaction
Access

GUI Widgets

Eventing

(Event)
Pull Point

Notification
Broker

Web Services
Cross-layer

Business Logic Services

Service
Catalogue

Business Process Monitoring
Alert

Shop floor
standard

Hardware
Vendor
Implementation

Data Services

DPWS
Back-end
Services

SOCRADES Connector
Web Services

SOCRADES MIDDLEWARE DEVICE SERVICES
Device Manager
and Monitor

Middleware
Historian

Service
Discovery

Service
Lifecycle
Management

Legacy Connector

Composed
Services
Runtime

Service Services
Mapper Repository

Service Access Control
Proprietary
Protocol

OPC UA
over DPWS

OPC UA
over DPWS

Gateway

DEVICE LAYER

Fig. 1. SOCRADES Integrated Architecture

‚Äì Connectivity to non web service enabled devices via various shop floor communication standards
‚Äì Graphical modelling and execution of business rules
‚Äì Visualization Services
‚Äì Connectivity to older SAP software through SAP-specific protocols
We decided not to re-implement that functionality but use it as a basis and
extend it by what we call the SOCRADES middleware. The SOCRADES middleware and xMII perform together a full integration of devices with ERP systems, adding functionalities such as graphical visualization of device data and
life cycle management of services running on the devices. In this setting, xMII
provides the handling of business logic, process monitoring and visualization of
the current status of the devices.
Finally, the connection with Enterprise Applications is realized in three ways.
SAP xMII can be used to generate rich web content that can be integrated into
the GUI of an enterprise system in mash-up style. Alternatively, it can be used
to establish the connection to older SAP systems using SAP-specific protocols.
Current, web service based enterprise software can access devices either via
web services of the SOCRADES middleware, benefiting from the additional functionality, or they can directly bind against the web services of DPWS-enabled
devices. The data delivered to Enterprise Applications is currently provided by
xMII. Nevertheless with the introduction of the SOCRADES middleware and

the use of DPWS, this data can be also delivered directly by the regarding devices, leaving to xMII only the task of delivering processed data that requires a
global view of the shop floor and of the business process.
4.2

Features and Components of the SOCRADES Middleware

The SOCRADES middleware is the bridging technology that enables the use of
features of existing software systems with DPWS enabled devices. Together with
SAP xMII, this middleware connects the shop floor to the top floor, providing additional functionality not available in either one of these layers. Although direct
access from an ERP system to devices is possible, the SOCRADES middleware
simplifies the management of the shop floor devices. In the following, we list
this additional functionality and show how the components of the architecture
implement them.
Brokered Access to Devices. Brokered access means to have an intermediate
party in the communication between web service clients and servers that adds
functionality. Example are asynchronous invocations, a pull point for handling
events, and a publish-subscribe mechanism for events. Asynchronous invocations
are useful when dealing with devices that are occasionally connected so that invocations have to be buffered until the device re-appears; they are implemented
by the Invoker component. Pull points enable applications to access events without having to expose a web service interface to receive them. The application can
instruct the pull point to buffer events and can obtain them by a web service call
whenever it is ready. Alternatively, to be notified immediately, the application
can expose a web service endpoint and register it at the notification broker for
any type of event.
Service Discovery: The service discovery components carries out the actual
service discovery on the shop floor level. This component is distributed and
replicated at each physical site because the DPWS discovery mechanism WSDiscovery relies on UDP multicast, a feature that may not be enabled globally
across all subsidiaries in a corporate network. All discovered devices from all
physically distributed sites and all the services that each device runs are then
in a central repository called Device Manager and Monitor, which acts as the
single access point where ERP systems can find all devices even when they have
no direct access to the shop floor network.
Device Supervision: Device Management and Monitor and DPWS Historian
provide the necessary static and dynamic information about each DPWS-enabled
physical device available in the system. The device manager holds any static
device data of all on-line and off-line devices while the device monitor contains
information about the current state of each device. The middleware historian can
be configured to log any event occurring at middleware level for later diagnosis

and analysis. Many low-level production systems feature historians, but they
are concerned with logging low-level data that might be irrelevant for businesslevel analysis. Only a middleware historian can capture high-level events that
are constructed within this architectural layer.

Service Life Cycle Management: Some hardware platforms allow exchanging the embedded software running on them via the network. In a service-enabled
shop floor this means that one can update services running on devices. The
management of these installed services is handled through the use of the Service
Mapper and Services Repository. These components together make a selection
of the software that should run in each device and perform the deployment.

Cross-Layer Service Catalogue: The cross-layer service catalogue comprises
two components. One is the Composed Services Runtime that executes service
composition descriptions, therefore realizing service composition at the middleware layer. The second component is the DPWS device for back end services
that allows DPWS devices to discover and use a relevant set of services of the
ERP system.
The Composed Services Runtime is used to enrich the services offered by the
shop floor devices with business context, such as associating an ID read from
an RFID tag with the corresponding order. A compound service can deliver this
data by both invoking a service on the RFID reader, and from a warehouse
application. A Composed Services Runtime, which is an execution engine for
such service composition descriptions, e.g., BPEL [3], is placed in the middleware
because only from there, all DPWS services on the shop floor as well as all back
end services can be reached.
Another requirement is that shop floor devices must be able to access enterprise application services, which can be achieved by making a relevant subset
available through the DPWS discovery. This way, devices that run DPWS clients
can invoke back end services in exactly the same way they invoke services on
their peer devices. Providing only the relevant back end services allows for some
access control and reduces overhead during discovery of devices. Co-locating
both sub-components in the same component has the advantage that also the
composed services that the Composed Services Runtime provides, can be made
available to the devices through the virtual DPWS device for back end services.

Security support: The (optional) security features supported by the middleware are role-based access control of devices communication to middleware and
back end services and vice versa. Event filtering based on roles is also possible. Both the devices as well as back end and middleware services have to be
authorized when they want to communicate. Access control is enforced by the
respective component. Additionally, message integrity and confidentiality is provided by the WS-Security standard.

To demonstrate the feasibility of our approach and to make some first evaluations, we implemented a simple manufacturing scenario. We used a first implementation of our architecture to connect two DPWS-enabled real-world devices
with an enterprise application.

5

Reference Implementation

In order to prove the feasibility of our concept, we have started realising a reference implementation. From a functional point of view, it demonstrates two
of the most important incentives for the use of standardized device level web
services in manufacturing: flexibility and integration with enterprise software.
Indeed, the scenario shows DPWS-enabled devices can be combined easily to
create higher-level services and behaviours that can then be integrated into topfloor applications.
The business benefits from adopting such an architecture are numerous:
‚Äì
‚Äì
‚Äì
‚Äì

5.1

lower cost of information delivery
increased flexibility and thus total cost of ownership (TCO) of machines.
increased visibility of the entire manufacturing process to the shop floor.
ability to model at the enterprise layer processes with only abstract view of
the underlying layer, therefore easing the creation of new applications and
services from non-domain experts.
Scenario

To support this idea we consider a simple setting with two DPWS devices:
‚Äì A robotic arm that can be operated through web service calls. Additionally
it offers status information to subscribers through the SOCRADES eventing
system.
‚Äì A wireless sensor node providing various information about the current environment, delivered as events. Furthermore, the sensor nodes provide actuators that are accessible through standard service calls.
The manufacturing process is created on the shop floor using a simple service composition scheme: from the atomic services offered by the arm (such
as start/stop, etc.) a simple manufacturing process p is created. The robot manipulates heat-sensitive chemicals. As a consequence it is identified that the
manufacturing process cannot continue if the temperature rises above 45 .
The robot may not have a temperature sensor (or this is malfunctioning),
but as mentioned before the manufacturing plant is equipped with a network
of wireless sensor nodes providing information about the environment. Thus,
in order to enforce the business rule, the chief operator uses a visual composition language to combine p with the temperature information published by the
service-enabled sensor node: t.
In pseudo code, such a rule looks like:

¬â

if (t > 45) then p.stopTransportProcess();
Furthermore, the operator instantiates a simple gauge fed with the temperature data (provided by t). For this purpose he uses a manufacturing intelligence
software and displays the gauge on a screen situated close the robot.
Finally, the sales manager can also leverage the service oriented architecture
of this factory. Indeed, the output of the business rule is connected to an ERP
system which provides up-to-date information about the execution of the current
orders. Whenever the process is stopped because the rule was triggered, an event
is sent to the ERP system through its web service interface. The ERP system
then updates the orders accordingly and informs the clients of a possible delay
in the delivery.
5.2

Components

This section describes the architecture of our prototype from an abstract point of
view. Its aim is to understand the functionality whose concrete implementation
will be described within the next section.
Functional Components The system comprises four main components as
shown on Figure 2 that we shall briefly describe:
‚Äì Smart Devices: Manufacturing devices, sensors and Smart Things (i.e.
Smart Objects) are the actors forming an Internet of Services in the factory
as well as outside of the factory. They all offer web service interfaces, either directly or through the use of gateways or service mediators. Through
these interfaces they offer functional services (e.g. start/stop, swap to manual/automatic mode) or status information (e.g. power consumption, mode
of operation, usage statistics, etc.).
‚Äì Composed Service: The component aggregates the services offered by
smart objects. Indeed, it is in charge of exposing coarse-grained services
to the upper layers. In the case of the robotic arm for instance, it will consume the open(), close() and move(...), methods and use them to offer
a doTransportProcess (...) service.
‚Äì Business Logic Services and Visualisation Services: In our prototype,
the business logic services are supported by a service composition engine and
visualized using a visualization toolkit. The former component is used to
model business rules or higher-level processes, known as business logic services in our architecture. As an example the operator can use it to create the
business rules exposed above. The latter component is used to build a plantfloor visualisation of the devices‚Äô status and the overall process execution.
As an example the operator can instantiate and use a set of widgets such as
gauges and graphs to monitor the status of the machines. The production
manager can also use it to obtain real-time graphs of the process execution
and status.

‚Äì Enterprise Applications: This is the place of high-end business software
such as ERPs or PLMs. The idea at this level is to visualize processes rather
than the machines executing the processes. This layer is connected to the
plant-floor devices through the other layers. As such it can report machines
failures and plant-floor information on the process visualization and workflow. Furthermore, business actions (e.g. inform customers about a possible
delay) can be executed based on this information.

Fig. 2. The DPWS service bus.

Cross-Component Communication In a mash-up, the architecture is not
layered but rather flat, enabling any functional component to talk to any other.
Such architectures need a common denominator in order for the components to
be able to invoke services on one another. In our case the common denominator
is the enhanced DPWS we developed. Each component is DPWS-enabled and
thus, consumes DPWS services and exposes a DPWS interface to invoke the
operations it offers. The service invocations can be done either synchronously or
asynchronously via the web service eventing system. For instance the temperature is gathered via a subscription to the temperature service (asynchronous)
whereas the transport process is stopped by invoking an operation on the process
middleware. Figure 2 depicts the architecture by representing the components
connected to a common (DPWS) ESB (Enterprise Service Bus).
5.3

Implementation

The system described in this paper is a reference implementation of concepts
described in the architecture rather than a stand-alone concept. Therefore it
uses and extends several software and hardware components rather than writing
them from scratch. In this section we will briefly describe what these components
are and how they interact together, taking a bottom up approach.

Functional Components
‚Äì Smart Devices: The wireless sensor network providing temperature information is implemented using the Sun Microsystems‚Äô SunSPOT sensor
nodes. Since the nodes are not web services enabled, we had to implement a gateway (as described in our architecture), that would capture the
temperature readings and provide it via DPWS as services one can subscribe to. The gateway component hides the communication protocol between the SunSPOTs and exposes their functionalities as device level web
services (DPWS). More concretely the SunSPOT offer services for sensing
the environment (e.g. getTemperature()) or providing output directly on
the nodes (e.g. turnLightOn(Color)). The robotic arm was implemented
as a clamp offering DPWS services for both monitoring and control. The
clamp makes these operations available as DPWS SOAP calls on a PLC
(Programmable Logic Controller) over gateway. For monitoring services (e.g.
getPowerConsumption()) the calls are issued directly on the gateway standing for the clamp. For control services the idea is slightly different.
‚Äì Composed Service: Typical operations at the clamp level are openClamp()
and closeClamp(). In order to consistently use these operations on the topfloor we need to add some business semantics already on the shop floor. This
is the role of composed services which aggregate an number of coarse-grained
operations (e.g. openClamp()) and turn them into higher level services. This
way the start(), openClamp(), closeClamp(), move(x), stop() operations
are combined to offer the startTransportProcess() service.
‚Äì Business Logic Services and Vizualisation Services: Services offered
by both the sensors and the clamp are combined to create a business rule.
The creation of this business logic service is supported by xMII, SAP‚Äôs Manufacturing Integration and Intelligence software. As mentioned before, the
aim of this software is firstly to offer a mean for gathering monitoring data
from different device aggregators on the shop floor such as MESs (Manufacturing Execution Systems). This functionality is depicted on Figure 3.
Since the SOCRADES infrastructure proposes to DPWS-enable all the devices on the plant-floor, we can enhance the model by directly connecting
the devices to xMII. Additionally, xMII offers a business intelligence tool.
Using its data visualization services we create a visualization of processrelated and monitoring data. Finally, we use the visual composition tool
offered by xMII to create the rule. Whenever this rule is triggered the
stopTransportProcess()operation is invoked on the middleware to stop
the clamp.
‚Äì Enterprise Applications: Whenever the business rule is triggered, xMII
invokes the updateOrderStatus()on the ERP. As mentioned before this
latter component displays the failure and its consequences (i.e. a delay in
the production) in the orders‚Äô list. Additionally, if the alert lasts for a while,
it informs the customer by email providing him with information about a
probable delay.

Fig. 3. xMII indirect device connectivity.

Fig. 4. Direct connectivity to the DPWS devices.

Cross-Component Communication Figure 5 presents the communication
amongst the components whenever the business rule is triggered. At first the
SunSPOT dispatches the temperature change by placing a SOAP message on
the DPWS service bus. The xMII is subscribed to this event and thus, receives
the message and feeds it to its rules engine. Since the reported temperature
is above the threshold xMII fires the rule. As a consequence it invokes the
stopTransportProcess()operation on the Process Service middleware. This
component contacts the clamp and stops it. Furthermore, xMII triggers the
updateOrderStatus()operation on the ERP. This latter system update the
status of the concerned order accordingly and decides whether to contact the
customer to inform him by email about the delay.

Fig. 5. Interactions when the business rule is triggered.

6

System Analysis

In this section we will discuss the properties of our architecture and give decision
makers a framework at hand through which they can assess the concrete value of
our system for their organisation. Since the work we are presenting in this paper
is part of ongoing research, we think it is helpful to have such a framework, in
particular to assess future work.
In the field of Systems Management several standards exist [ref. Standards,
ITIL, etc.] which aim to support a structured dealing with IT systems. One
framework in particular helpful for central corporate functions such as produc-

tion is the ISO model FCAPS (Fault, Configuration, Administration, Performance, Security). Although being a framework for network management, it is
relevant for our architecture because it is enabling low level networked interaction between Smart Objects. Here we will give a first attempt to evaluate the
architecture.
‚Äì Fault Management: Since our system will be part of the manufacturing
IT-landscape we need to manage both, faults of particular parts of the manufacturing process and faults in our system. Due to the tight integration
these types of faults inherently become the same. In particular the SOA
based approach of device integration enables the user to identify faults in
his production process, at a level never seen before. It also gives the possibility to build redundancy at system critical stages which ensures fast recovery
from local failures. Finally the flexibility given by our SOA approach lets the
user decide to what extend he wants to introduce capabilities of quick fault
recovery, depending on his individual needs.
‚Äì Configuration Management: Mainly the two components Service Lifecycle Management and Cross-Layer Service Catalogue support dynamic configuration management. However, at the current point of view we see code
updated to Smart Devices as a major challenge which until today has not
been resolved sufficiently. Configuration also includes the composition of
services into higher-level services. In a future version, our Service Discovery
module will use semantic annotation of services to find appropriate service
instances for online service composition. Using ontologies to specify the behaviour and parameters of web services in their interface descriptions and
metadata allows flexible service composition. Especially in the very well defined domain of manufacturing we can make use of existing ontologies that
describe production processes.
‚Äì Administrative Management: The Device Manager provides the necessary static and dynamic information about each Smart Device. Through the
strict use of web-service interfaces, it will be possible to easily integrate devices into management dash-boards. Through this technically we allow easy
and user friendly access to Smart Devices. However, taking the possibly very
large number of devices into account, we belief that our middle-ware has deficiencies in offering this user friendly administration. Although this problem
is subject to other fields of research such as sensor networks, (e.g, macro
programming), we will dedicate our research efforts to the problem.
‚Äì Performance Management: Already now we can say that local components of our system will scale well in regards to total amount of Smart
Objects and their level of interaction. This can be justified since all interaction occurs locally and only a limited amount of Smart Objects is needed
to fulfil a particular task. However, it is still an open question, if our system
will scale well on a global scale and to what extend it will need to be modularized. For example we will need to investigate whether central components
such as device and service registries should operate on a plant level or on

a corporate level, which could mean that these parts would have to handle
several millions or even billions of devices at the same time.
‚Äì Security Management: As mentioned in the security support section of
the architecture, our system can make use of well established security features which already are part of web-service technologies and their protocols
such as DPWS. It is most likely that we will have to take into account industry specific security requirements, and it will be interesting to see, if we
can deliver a security specification which satisfies all manufacturing setups.

7

Conclusions

In this paper we have presented SOCRADES, a Web Service based Shop Floor
Integration Infrastructure. With SOCRADES we are offering an architecture including a middleware which support connecting Smart Devices, i.e. intelligent
production machines from manufacturing shop floors, to high-level back-end systems such as an ERP system. Our integration strategy is to use web services
as the main connector technology. This approach is motivated by the emerging importance of Enterprise Service Oriented Architectures, which are enabled
through web services.
Our work has three main contributions: First, we elaborated and structured
a set of requirements for the integration problem. Second, we are proposing
a concrete architecture containing of components which realized the required
functionality of the system. Our third contribution is a reference implementation
of the SOCRADES architecture. In this implementation we have demonstrated
the full integration of two Smart Devices into and enterprise system. We showed
that it is possible to connect Smart Devices to an ERP system, and describe
how this is done.
Our next steps include integrating a prototype in a bigger setup and testing
it with live production systems.

8

Acknowledgments

The authors would like to thank the European Commission and the partners
of the European IST FP6 project ‚ÄùService-Oriented Cross-layer infRAstructure
for Distributed smart Embedded devices‚Äù (SOCRADES - www.socrades.eu), for
their support.

References
1. Instrumentation Systems and Automation Society. http://www.isa.org/.
2. SIMATIC WinCC flexible. http://www.siemens.com/simatic-wincc-flexible/.
3. Web Services Business Process Execution Language Version 2.0 (OASIS Standard),
April 2007. http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.html.

4. H. Bohn, A. Bobek, and F. Golatowski. SIRENA - Service Infrastructure for Realtime Embedded Networked Devices: A service oriented framework for different
domains. In International Conference on Systems and International Conference on
Mobile Communications and Learning Technologies (ICNICONSMCL‚Äô06), page 43,
Washington, DC, USA, 2006. IEEE Computer Society.
5. E. Fleisch and F. Mattern, editors. Das Internet der Dinge: Ubiquitous Computing und RFID in der Praxis:Visionen, Technologien, Anwendungen, Handlungsanleitungen. Springer, 2005.
6. L. Gaxiola, M. de J. Ramƒ±ÃÅrez, G. Jimenez, and A. Molina. Proposal of Holonic
Manufacturing Execution Systems Based on Web Service Technologies for Mexican
SMEs. In HoloMAS, pages 156‚Äì166, 2003.
7. G.Gorbach. Pursuing manufacturing excellence through Real-Time performance
management and continuous improvement. ARC Whitepaper, April 2006.
8. F. Jammes, A. Mensch, and H. Smit. Service-Oriented Device Communications
using the Devices Profile for Web Services. In MPAC ‚Äô05: Proceedings of the 3rd
international workshop on Middleware for pervasive and ad-hoc computing, pages
1‚Äì8, New York, NY, USA, 2005. ACM Press.
9. F. Jammes and H. Smit. Service-oriented paradigms in industrial automation.
IEEE Transactions on Industrial Informatics, 1:62‚Äì70, 2005.
10. S. Karnouskos, O. Baecker, L. M. S. de Souza, and P. Spiess. Integration of SOAready Networked Embedded Devices in Enterprise Systems via a Cross-Layered
Web Service Infrastructure. In 12th IEEE Conference on Emerging Technologies
and Factory Automation, 2007.
11. A. Reinhardt. A Machine-To-Machine ‚ÄùInternet Of Things‚Äù. Business Week, April
2004.
12. U. Saif and D. J. Greaves. Communication Primitives for Ubiquitous Systems or
RPC Considered Harmful. In 21st International Conference of Distributed Computing Systems (Workshop on Smart Appliances and Wearable Computing), Los
Alamitos, CA, USA, 2001. IEEE Computer Society.
13. C. R. Schoenberger. RFID: The Internet of Things. Forbes, (18), March 2002.
14. E. Zeeb, A. Bobek, H. Bohn, and F. Golatowski. Service-Oriented Architectures for
Embedded Systems Using Devices Profile for Web Services. In 21st International
Conference on Advanced Information Networking and Applications Workshops.,
2007.

1

Optimal estimation with missing observations
via balanced time-symmetric stochastic models

arXiv:1503.06014v2 [math.OC] 18 Aug 2015

Tryphon T. Georgiou, Fellow, IEEE and Anders Lindquist, Life Fellow, IEEE

Abstract‚ÄîWe consider data fusion for the purpose of smoothing and interpolation based on observation records with missing
data. Stochastic processes are generated by linear stochastic
models. The paper begins by drawing a connection between
time reversal in stochastic systems and all-pass extensions. A
particular normalization (choice of basis) between the two timedirections allows the two to share the same orthonormalized
state process and simplifies the mathematics of data fusion.
In this framework we derive symmetric and balanced MayneFraser-like formulas that apply simultaneously to smoothing and
interpolation.

I. I NTRODUCTION
Data fusion is the process of integrating different data sets,
or statistics, into a more accurate representation for a quantity
of interest. A case in point in the context of systems and
control is provided by the Mayne-Fraser two-filter formula
[1], [2] in which the estimates generated by two different
filters are merged into a combined more reliable estimate
in fixed-interval smoothing. The purpose of this paper is to
develop such a two-filter formula that is universally applicable
to smoothing and interpolation based on general records with
missing observations.
In [3], [4] the Mayne-Fraser formula was analyzed in the
context of stochastic realization theory and was shown that
it can be formulated in terms a forward and a backward
Kalman filter. In a subsequent series of papers, Pavon [5],
[6] addressed in a similar manner the hitherto challenging
problem of interpolation [7], [8], [9], [10]. This latter problem consists of reconstructing missing values of a stochastic
process over a given interval. In departure from the earlier
statistical literature, [5], [6] considered a stationary process
with rational spectral density and, therefore, reliazable as the
output of a linear stochastic system. Interpolation was then
cast as seeking an estimate of the state process based on an
incomplete observation record. A basic tool in these works
is the concept of time-reversal in stochastic systems which
has been central in stochastic realization theory (see, e.g.,
[11], [12], [13], [14], [5], [6], [15], [16], [17]). For a recent
Research supported by grants from AFOSR, NSF, VR, and the SSF.
T.T. Georgiou is with the Department of Electrical & Computer
Engineering, University of Minnesota, Minneapolis, Minnesota; email:
tryphon@umn.edu and A. Lindquist is with the Department of Automation
and the Department of Mathematics, Shanghai Jiao Tong University, Shanghai,
China, and the Center for Industrial and Applied Mathematics and ACCESS
Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden;
email: alq@kth.se

overview of smoothing and interpolation theory in the context
of stochastic realization theory see [18, Chapter 15].
In the present paper we are taking this program several
steps further. Given intermittent observations of the output
of a linear stochastic system over a finite interval, we want
to determine the linear least-squares estimate of the state
of the system in an arbitrary point in the interior of the
interval, which may either be in a subinterval of missing
data or in one where observations are available. Hence, this
combines smoothing and interpolation over general patterns of
available observations. Our main interest is in continuous-time
(possibly time-varying) systems. However, the absence of data
over subintervals, depending on the information pattern, may
necessitate a hybrid approach involving discrete-time filtering
steps.
In studying the statistics of a process over an interval, it is
natural to decompose the interface between past and future in
a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward
or backward in time. This point was fundamental in early work
in stochastic realization; see [18] and references therein. In a
different context [19] a certain duality between the two timedirections in modeling a stochastic process was introduced in
order to characterize solutions to moment problems. In this
new setting the noise-process was general (not necessarily
white), and the correspondence between the driving inputs to
the two time-opposite models was shown to be captured by
suitable dual all-pass dynamics.
Here, we begin by combining these two sets of ideas
to develop a general framework where two time-opposite
stochastic systems model a given stochastic process. We study
the relationship between these systems and the corresponding
processes. In particular, we recover as a special case certain
results of stochastic realization theory [11], [5], [6], [4] from
the 1970‚Äôs using a novel procedure. This theory provides a
normalized and balanced version of the forward-backward
duality which is essential for our new formulation of the
two-filter Mayne-Fraser-like formula uniformly applicable to
intervals with or without observations.
The paper is structured as follows. In Section II we
explain how a lifting of state-dynamics into an all-pass system
allows direct correspondence between sample-paths of driving

2

generating processes, in opposite time-directions, via causal
and anti-causal mappings, respectively. This is most easily
understood and explained in discrete-time and hence we begin
with that. In Section III we utilize this mechanism in the
context of general output processes and, similarly, introduce a
pair of time-opposite models. These two introductory sections,
II and III, deal with stationary models for simplicity and
are largely based on [20]. The corresponding generalizations
to time-varying systems are given in Section IV and in the
appendix, in continuous and discrete-time, respectively. In
Section V we explain Kalman filtering for problems with
missing information in the continuous-time setting. In this,
we first consider the case where increments of the output
process across intervals of no information are unavailable as
a simplified preliminary, after which we focus on the central
problem where the output process is the object of observation.
Section VI deals with the geometry of information fusion.
In Section VII we present a generalized balanced two-filter
formula that applies uniformly over intervals where data is or
is not available. We summarize the computational steps of this
approach in Section VIII. Finally, we highlight the use of the
two-filter formula with a numerical example given in Section
IX and provide concluding remarks in Section X.

this is identical to the one for discrete-time given above (as
is well known). In continuous time, stability of the system
of equations is equivalent to A having only eigenvalues with
negative real part.
In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system
is all-pass. This is done next.

A. All-pass extension in discrete-time
Consider the discrete-time Lyapunov equation
P = AP A0 + BB 0 .

Since A has all eigenvalues inside the unit disc of the complex
plane and (3) holds, (6) has as solution a matrix P which is
positive definite. The state transformation
1

Œæ = P ‚àí 2 x,

x(t + 1) = Ax(t) + Bw(t)
n√ón

(1)

n√óp

where t ‚àà Z, A ‚àà R
,B ‚àà R
, A has all eigenvalues
in the open unit disc D = {z | |z| < 1}, and w(t), x(t) are
(centered) stationary vector-valued stochastic processes with
w(t) normalized white noise; i.e.,
E{w(t)w(s)0 } = Ip Œ¥ts ,

(2)

where E denotes mathematical expectation. The system of
equations is assumed to be reachable, i.e.,


rank B, AB, . . . An‚àí1 B = n.
(3)
In continuous-time, state-dynamics take the form of a
system of stochastic differential equations
dx(t) = Ax(t)dt + Bdw(t)

(4)

where, here, x(t) is a stationary continuous-time vector-valued
stochastic process and w(t) is a vector-valued process with
orthogonal increments with the property
E{dwdw0 } = Ip dt,

(5)

where Ip is the p √ó p identity matrix. Reachability of the
pair (A, B) is also assumed throughout and the condition for

(7)

and
1

1

1

F = P ‚àí 2 AP 2 , G = P ‚àí 2 B,

(8)

Œæ(t + 1) = F Œæ(t) + Gw(t).

(9)

brings (1) into

II. S TATE DYNAMICS AND ALL - PASS EXTENSION
In this paper we consider discrete-time as well as
continuous-time stochastic linear state-dynamics. We begin by
explaining basic ideas in a stationary setting. In discrete-time
systems take the form of a set of difference equations

(6)

For this new system, the corresponding Lyapunov equation
X = F XF 0 + GG0 has In as solution, where In denotes the
(n √ó n) identity matrix. This fact, namely, that
In = F F 0 + GG0

(10)

implies that this [F, G] can be embedded as part of an
orthogonal matrix


F G
U=
,
(11)
H J
i.e., a matrix such that U U 0 = U 0 U = In+p .
Define the transfer function
U(z) := H(zIn ‚àí F )‚àí1 G + J

(12)

corresponding to
Œæ(t + 1) = F Œæ(t) + Gw(t)

(13a)

wÃÑ(t) = HŒæ(t) + Jw(t).

(13b)

This is also the transfer function of
x(t + 1) = Ax(t) + Bw(t)

(14a)

0

(14b)

wÃÑ(t) = BÃÑ x(t) + Jw(t),
‚àí 21

where BÃÑ := P H 0 , since the two systems are related by a
similarity transformation. Hence,
U(z) = BÃÑ 0 (zIn ‚àí A)‚àí1 B + J.

(15)

3

Now, using the identity
We claim that U(z) is a stable all-pass transfer function (with
respect to the unit disc), i.e., that U(z) is a transfer function
of a stable system and that
U(z)U(z ‚àí1 )0 = U(z ‚àí1 )0 U(z) = Ip .

(16)

The latter claim is immediate after we observe that, since
U U = In+p ,

 

Œæ(t)
Œæ(t + 1)
,
=
U0
w(t)
wÃÑ(t)
0

and hence,

In ‚àí F F 0 = (zIn ‚àí F )(z ‚àí1 In ‚àí F 0 )
+ (zIn ‚àí F )F 0 + F (z ‚àí1 In ‚àí F 0 ),
(10) and GJ 0 = ‚àíF H 0 , obtained from U U 0 = In+p , this
yields
U(z)U(z ‚àí1 )0 = HH 0 + JJ 0 = In+p ,
as claimed.

B. All-pass extension in continuous-time
Consider the continuous-time Lyapunov equation

Œæ(t) = F 0 Œæ(t + 1) + H 0 wÃÑ(t)
0

0

w(t) = G Œæ(t + 1) + J wÃÑ(t)

(17b)

or, equivalently,
1

x(t) = P A0 P ‚àí1 x(t + 1) + P 2 H 0 wÃÑ(t)
0

w(t) = B P

‚àí1

0

x(t + 1) + J wÃÑ(t).

(18a)

(19)

dŒæ(t) = F Œæ(t)dt + Gdw(t).

dwÃÑ(t) = HŒæ(t)dt + Jdw(t)
(20a)

w(t) = B 0 xÃÑ(t) + J 0 wÃÑ(t)

(20b)

U(z)‚àó = B 0 (z ‚àí1 In ‚àí A0 )‚àí1 BÃÑ + J 0 .

(21)

Either of the above systems inverts the dynamical relation
w ‚Üí wÃÑ (in (14) or (13)).

-

(23b)

so that the transfer function
U(s) := H(sIn ‚àí F )‚àí1 G + J

(24)

is all-pass (with respect to the imaginary axis), i.e.,

with transfer function

w(t)

(23a)

We now seek a completion by adding an output equation

(18) can be written
xÃÑ(t ‚àí 1) = A0 xÃÑ(t) + BÃÑ wÃÑ(t)

(22)

Since A has all its eigenvalues in the left half of the complex
plane and since (3) holds, (22) has as solution a positive
definite matrix P . Once again, applying (7-8), the system in
(4) becomes

(18b)

Setting
xÃÑ(t) := P ‚àí1 x(t + 1),

AP + P A0 + BB 0 = 0.

(17a)

U

U(s)U(‚àís)0 = U(‚àís)0 U(s) = Ip .

For this new system, the corresponding Lyapunov equation
has as solution the identity matrix and hence,
F + F 0 + GG0 = 0.

wÃÑ(t)
-

Fig. 1: Realization (14) in the forward time-direction.

(25)

(26)

Utilizing this relationship we note that
(sIn ‚àí F )‚àí1 GG0 (‚àísIn ‚àí F 0 )‚àí1
= (sIn ‚àí F )‚àí1 (sIn ‚àí F ‚àí sIn ‚àí F 0 )(‚àísIn ‚àí F 0 )‚àí1

w(t)


U‚àó

wÃÑ(t)


Fig. 2: Realization (20) in the backward time-direction.
An algebraic proof of (16) is also quite immediate. In fact,
U(z)U(z ‚àí1 )0


0
= H(zIn ‚àí F )‚àí1 G + J H(z ‚àí1 In ‚àí F )‚àí1 G + J
=H(zIn ‚àí F )‚àí1 GG0 (z ‚àí1 In ‚àí F 0 )‚àí1 H 0 + JJ 0
+ H(zIn ‚àí F )‚àí1 GJ 0 + JG0 (z ‚àí1 In ‚àí F 0 )‚àí1 H

= (sIn ‚àí F )‚àí1 + (‚àísIn ‚àí F 0 )‚àí1 ,
and we calculate that
U(s)U(‚àís)0
= (H(sIn ‚àí F )‚àí1 G + J)(G0 (‚àísIn ‚àí F 0 )‚àí1 H 0 + J 0 )
= JJ 0 + H(sIn ‚àí F )‚àí1 (GJ 0 + H 0 )
(JG0 + H)(‚àísIn ‚àí F 0 )‚àí1 H 0 .
For the product to equal the identity,
JJ 0 = Ip
H = ‚àíJG0 .

4

Thus, we may take

A. Time-reversal of discrete-time stochastic systems
Consider a stochastic linear system

J = Ip
H = ‚àíG0 ,

x(t + 1) = Ax(t) + Bw(t)

(35a)

y(t) = Cx(t) + Dw(t)

(35b)

and the forward dynamics
dŒæ(t) = F Œæ(t)dt + Gdw(t)

(27a)

dwÃÑ(t) = ‚àíG0 Œæ(t)dt + dw(t).

(27b)

Substituting F = ‚àíF 0 ‚àí GG0 from (26) into (27a) we obtain
the reverse-time dynamics
dŒæ(t) = ‚àíF 0 Œæ(t)dt + GdwÃÑ(t)
0

dw(t) = G Œæ(t)dt + dwÃÑ(t).

(28a)
(28b)

Now defining
xÃÑ(t) := P ‚àí1 x(t)

(29)

and using (7) and (8), (28) becomes
dxÃÑ(t) = ‚àíA0 xÃÑ(t)dt + BÃÑdwÃÑ(t)
0

dw(t) = B xÃÑ(t)dt + dwÃÑ(t),

(30a)
(30b)

with transfer function

with an m-dimensional output process y, and x, u, A, B are
defined as in Section II-A. All processes are stationary and
the system can be thought as evolving forward in time from
the remote past (t = ‚àí‚àû).
To formalize this, we introduce some notation. Let H be
the Hilbert space spanned by {wk (t); t ‚àà Z, k = 1, 2, . . . , n},
endowed with the inner product hŒª, ¬µi = E{Œª¬µ}, and let
+
H‚àí
t (w) and Ht (w) be the (closed) subspaces spanned by
{wk (s); s ‚â§ t ‚àí 1, k = 1, . . . , m} and {wk (s); s ‚â• t, k =
+
1, . . . , m}, respectively. Define H‚àí
t (y) and Ht (y) accordingly in terms of the output process process y. Then the
stochastic system (35) evolves forward in time in the sense
that
‚àí
+
H‚àí
(36)
t (z) ‚äÇ Ht (w) ‚ä• Ht (w),
where A ‚ä• B means that elements of the subspaces A and
B are mutually orthogonal, and where H‚àí
t (z) is formed as
above in terms of


x(t + 1)
z(t) =
;
y(t)

U(s)‚àó = Ip + B 0 (sIn + A0 )‚àí1 BÃÑ,

(31)

BÃÑ := P ‚àí1 B.

(32)

xÃÑ(t ‚àí 1) = A0 xÃÑ(t) + BÃÑ wÃÑ(t)

(37a)

Furthermore, the forward dynamics (27) can be expressed in
the form

y(t) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t),

(37b)

see [18, Chapter 6] for more details.

where

Next we construct a stochastic system

dx(t) = Ax(t)dt + Bdw(t)
0

dwÃÑ(t) = BÃÑ x(t)dt + dw(t)

(33a)
(33b)

with transfer function

which evolves backward in time from the remote future
(t = ‚àû) in the sense that the processes xÃÑ, x, wÃÑ, w relate as in
the previous section. More specifically, as shown in Section
II-A, H‚àí (wÃÑ) ‚äÇ H‚àí (w) and H+ (w) ‚äÇ H+ (wÃÑ) for all t, as
examplified in Figures 1 and 2.
In fact, the all-pass extension (14) of (35a) yields

U(s) = Ip ‚àí BÃÑ 0 (sIn ‚àí A)‚àí1 B.

(34)

wÃÑ(t) = BÃÑ 0 x(t) + Jw(t)

(38)

It follows from (20b) that (38) can be inverted to yield
III. T IME - REVERSAL OF STATIONARY LINEAR
STOCHASTIC SYSTEMS

The development so far allows us to draw a connection
between two linear stochastic systems having the same output
and driven by a pair of arbitrary, but dual, stationary processes
w(t) and wÃÑ(t), one evolving forward in time and one evolving
backward in time. When one of these two processes is white
noise (or, orthogonal increment process, in continuous-time),
then so is the other. For this special case we recover results
of [11] and [5], [6] in stochastic realization theory.

w(t) = B 0 xÃÑ(t) + J 0 wÃÑ(t),

(39)

where xÃÑ(t) = P ‚àí1 x(t + 1), and that we have the reverse-time
recursion
xÃÑ(t ‚àí 1) = A0 xÃÑ(t) + BÃÑ wÃÑ(t).
(40a)
Then inserting (39) and
x(t) = P xÃÑ(t ‚àí 1) = P A0 xÃÑ(t) + P BÃÑ wÃÑ(t)
into (35b), we obtain
y(t) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t),

(40b)

5

where DÃÑ := CP BÃÑ + DJ 0 and
CÃÑ := CP A0 + DB 0 .

(41)

Then, (40) is precisely what we wanted to establish.
The white noise w is normalized in the sense of (2). Since
U, given by (15), is all-pass, wÃÑ is also a normalized white
noise process, i.e.,
E{wÃÑ(t)wÃÑ(s)0 } = Ip Œ¥t‚àís .
From the reverse-time recursion (37a)
xÃÑ(t) =

‚àû
X

+
same inner product as above, and let H‚àí
t (du) and Ht (du)
be the (closed) subspaces spanned by the increments of the
components of U on (‚àí‚àû, t] and [t, ‚àû), respectively. Define
+
H‚àí
t (dy) and Ht (dy) accordingly in terms of the output
process y. All processes have stationary increments and the
stochastic system (45) evolves forward in time in the sense
that
‚àí
+
H‚àí
(46)
t (dz) ‚äÇ Ht (dw) ‚ä• Ht (dw),

where H‚àí
t (dz) is formed in terms of


x(t)
.
z(t) =
y(t)

(47)

The all-pass extension of Section II-B yields

(A0 )k‚àí(t+1) BÃÑ wÃÑ(k).

dwÃÑ = dw ‚àí BÃÑ 0 xdt

k=t+1
0

Since, wÃÑ is a white noise process, E{xÃÑ(t)wÃÑ(s) } = 0 for all
s ‚â§ t. Consequently, (37) is a backward stochastic realization
in the sense defined above.

as well as the reverse-time relation
dxÃÑ = ‚àíA0 xÃÑdt + BÃÑdwÃÑ
0

dw = B xÃÑdt + dwÃÑ,

Moreover, the transfer functions
W(z) = C(zIn ‚àí A)‚àí1 B + D

(42)

(48)

(49a)
(49b)

where xÃÑ(t) = P ‚àí1 x(t). Inserting (49b) into
dy = CP xÃÑdt + Ddw

of (35) and
WÃÑ(z) = CÃÑ(z ‚àí1 In ‚àí A0 )‚àí1 BÃÑ + DÃÑ

(43)

of (37) satisfy

yields
dy = CÃÑ xÃÑdt + DdwÃÑ,
where

W(z) = WÃÑ(z)U(z).

In the context of stochastic realization theory, U(z) is called
structural function ([13], [14]).

w(t)
-

W

-

y(t)

WÃÑ

(50)

Thus, the reverse-time system is

y(t)

dxÃÑ = ‚àíA0 xÃÑdt + BÃÑdwÃÑ

(51a)

dy = CÃÑ xÃÑdt + DdwÃÑ.

(51b)

From this, we deduce that the system (45) has the backward
property
+
‚àí
H+
(52)
t (dzÃÑ) ‚äÇ Ht (dwÃÑ) ‚ä• Ht (dwÃÑ),

Fig. 3: The forward stochastic system (35).



CÃÑ = CP + DB 0 .

(44)

where H+
t (dzÃÑ) is formed as above in terms of


xÃÑ(t)
zÃÑ(t) =
.
y(t)

wÃÑ(t)


We also note that the transfer function
Fig. 4: The backward stochastic system (37)

W(s) = C(sIn ‚àí A)‚àí1 B + D
of (45) and the transfer function

B. Time-reversal of continuous-time stochastic systems

WÃÑ(s) = CÃÑ(sIn + A0 )‚àí1 BÃÑ + D

We now turn to the continuous-time case. Let

of (51) also satisfy

dx = Axdt + Bdw

(45a)

dy = Cxdt + Ddw

(45b)

be a stochastic system with x, w, A, B as in Section II-B,
evolving forward in time from the remote past (t = ‚àí‚àû).
Now let H be the Hilbert space spanned by the increments
of the components of w on the real line R, endowed with the

W(s) = WÃÑ(s)U(s)
as in discrete-time.
Note that the orthogonal-increment process w is normalized in the sense of (5). Since U(s) is all-pass,
dwÃÑ = du ‚àí BÃÑ 0 xdt

(53)

6

also defines a stationary orthogonal-increment process wÃÑ such
that
{dwÃÑ(t)dwÃÑ(t)0 } = Ip dt.
It remains to show that (51) is a backward stochastic realization, that is, at each time t the past increments of wÃÑ are
orthogonal to xÃÑ(t). But this follows from the fact that
Z ‚àû
0
e‚àíA (t‚àís) BÃÑdwÃÑ(s)
xÃÑ(t) =

1

1

1

P (t)‚àí 2 PÃá P (t)‚àí 2 = ‚àíR(t) ‚àí R(t)0 ,
and hence the (55) yields
F (t) + F (t)0 + G(t)G(t)0 = 0.

(61)

Using (61) to eliminate F in (57), we obtain

t

and wÃÑ has orthogonal increments.

1

Differentiating P (t)‚àí 2 P (t)P (t)‚àí 2 = In , we obtain

dŒæ = ‚àíF (t)0 Œæ(t)dt + G(t)dwÃÑ,

(62)

dwÃÑ = dw ‚àí G(t)0 Œæ(t)dt,

(63)

where

IV. T IME REVERSAL OF NON - STATIONARY STOCHASTIC

which can also be written

SYSTEMS

dwÃÑ = dw ‚àí BÃÑ(t)0 x(t)dt,

In a similar manner non-stationary stochastic systems
admit unitary extensions which in turn allows us to construct
dual time-reversed stochastic models that share the same state
process. The case of discrete-time dynamics is documented
in the appendix, whereas the continuous-time counterpart is
explained next as prelude to smoothing and interpolation that
will follow.

A. Unitary extension
The covariance matrix function P (t) := E{x(t)x(t)0 } of
the time-varying state representation
dx = A(t)x(t)dt + B(t)dw,

x(0) = x0

(54)

with x0 a zero-mean stochastic vector with covariance matrix
P0 = E{x0 x00 }, satisfies the matrix-valued differential equation
PÃá (t) = A(t)P (t) + P (t)A(t)0 + B(t)B(t)0
(55)
with P (0) = P0 . Throughout we assume total reachability [18,
Section 15.2], and therefore P (t) > 0 for all t > 0.
A unitary extension of (54) is somewhat more complicated
than in the discrete time case. In fact, differentiating
1

Œæ(t) = P (t)‚àí 2 x(t)

(64)

where BÃÑ(t) := P (t)‚àí1 B(t).
Proposition 1: A process wÃÑ satisfying (63) has orthogonal
increments with the normalized property (5). Moreover,
E{[wÃÑ(t) ‚àí wÃÑ(s)]Œæ(t)0 } = 0

(65)

for all s ‚â§ t.
Proof: As is well-known, the solution of (57) can be
written in the form
Z t
Œæ(t) = Œ¶(t, s)Œæ(s) +
Œ¶(t, œÑ )G(œÑ )dw,
(66)
s

where Œ¶(t, s) is the transition matrix with the property
‚àÇŒ¶
(t, s) = F (t)Œ¶(t, s), Œ¶(s, s) = In
(67a)
‚àÇt
‚àÇŒ¶
(t, s) = ‚àíŒ¶(t, s)F (s), Œ¶(t, t) = In
(67b)
‚àÇs
Let s ‚â§ t. Then, in view of (63), a straight-forward calculation
yields
wÃÑ(t) ‚àí wÃÑ(s) = w(t) ‚àí w(s)
Z
‚àí M (t, s)Œæ(s) ‚àí

t

M (t, œÑ )G(œÑ )dw,

(68)

s

where
Z

(56)

M (t, s) =

t

G(œÑ )0 Œ¶(œÑ, s)dœÑ.

(69)

s

we obtain

Therefore,
dŒæ = F (t)Œæ(t)dt + G(t)dw,

(57)
E{[wÃÑ(t) ‚àí wÃÑ(s)][wÃÑ(t) ‚àí wÃÑ(s))0 } = Ip (t ‚àí s) + ‚àÜ(t, s),

where
1

1

F (t) = P (t)‚àí 2 A(t)P (t) 2 + R(t),
G(t) = P (t)
with

‚àí 12

where
(58a)

B(t)

(58b)


1
d
‚àí 12
R(t) =
P (t)
P (t) 2 .
dt

(59)



1

Z

t

‚àÜ(t, s) = M (t, s)M (t, s) +
M (t, œÑ )G(œÑ )G(œÑ )0 M (t, œÑ )0 dœÑ
s
Z t
‚àí
[M (t, œÑ )G(œÑ ) + G(œÑ )0 M (t, œÑ )0 ] dœÑ.
s

However, ‚àÜ(t, s) is identically zero. To see this, first note that

In fact,
dŒæ = P (t)‚àí 2 dx + R(t)Œæ(t)dt.

0

(60)

‚àÇM
(t, s) = ‚àíM (t, s)F (s) ‚àí G(s)0 .
‚àÇs

(70)

7

Then, in view of (61), a simple calculation shows that
‚àÇ‚àÜ
(t, s) ‚â° 0.
‚àÇs
Since ‚àÜ(t, t) = 0, the assertion follows. Hence the incremental
covariance is normalized.

B. Time reversal in continuous-time systems

dx = A(t)x(t)dt + B(t)dw,

x(0) = x0

(75a)

Next, we show that wÃÑ(t) has orthogonal increments. To
this end, choose arbitrary times s ‚â§ t ‚â§ a ‚â§ b on the interval
[0, T ], where we choose a and b fixed, and show that

dy = C(t)x(t)dt + D(t)dw,

y(0) = 0

(75b)

Q(t, s) := E{[wÃÑ(b) ‚àí wÃÑ(a)][wÃÑ(t) ‚àí wÃÑ(s))0 }

Next we derive the backward stochastic system corresponding to the non-stationary forward stochastic system

defined on the finite interval [0, T ], where x0 (with covariance
P0 ) and the normalized Wiener process w are uncorrelated.
To this end, apply the transformation
xÃÑ(t) = P (t)‚àí1 x(t)

is identically zero for all s ‚â§ t. Using (68) and
wÃÑ(b) ‚àí wÃÑ(a) = w(b) ‚àí w(s) ‚àí M (b, a)Œ¶(a, s)Œæ(s)
Z b
Z b
‚àí M (b, a)
Œ¶(a, œÑ )G(œÑ )dw ‚àí
M (b, œÑ )dw
s

a

computed analogously, we obtain
"

Z

(77)

b

Œ¶(a, œÑ )G(œÑ )dœÑ
#

b
0

Œ¶(a, œÑ )G(œÑ )G(œÑ ) M (t, œÑ )dœÑ .
s

Then, again using (61), we see that
‚àÇM
(t, s) ‚â° 0,
‚àÇs
so, since Q(t, t) = 0, we see that Q(t, s) is identically zero,
establishing that wÃÑ(t) has orthogonal increments.
Finally, we use the same trick to show (65). In fact, for
s ‚â§ t, (66) and (68) yield
E{[wÃÑ(t) ‚àí wÃÑ(s))Œæ(t)0 } = ‚àíM (t, s)Œ¶(t, s)0
Z t
Z t
+
G(œÑ )0 Œ¶(t, œÑ )0 dœÑ ‚àí
M (t, œÑ )G(œÑ )G(œÑ )0 )Œ¶(t, œÑ )0 dœÑ,
s

dy = CÃÑ(t)xÃÑ(t) + D(t)dwÃÑ,
where

s

+

together with (74b) to (75b) to obtain

CÃÑ(t) = C(t)P (t) + D(t)B(t).

Q(t, s) = M (b, a) Œ¶(a, s)M (t, s)0 ‚àí
Z

(76)

This together with (74a) yields the the backward system
corresponding to (75), namely
dxÃÑ = ‚àíA(t)0 xÃÑ(t)dt + BÃÑ(t)dwÃÑ

(78a)

dy = CÃÑ(t)xÃÑ(t)dt + D(t)dwÃÑ.

(78b)

with end-point condition xÃÑ(T ) = P (T )
the Wiener process wÃÑ.

‚àí1

x(T ) uncorelated to

The backward realization (78) was derived in [3], but in
cumbersome way, requiring the proof that wÃÑ(t) is a normalized
process with orthogonal increments to be suppressed. What
is new here is imposing the unitary map between w and wÃÑ,
making the analysis much simpler and more natural.
V. K ALMAN FILTERING WITH MISSING OBSERVATIONS

s

the partial derivative of which with respect to s is identical
zero; this is seen by again using (61). Therefore, since (65) is
zero for s = t, it is identical zero for all s ‚â§ t, as claimed.
This concludes the proof of Proposition 1.
Consequently, (57) and (64) form a forward unitary system
dx = A(t)x(t)dt + B(t)dw

(71a)

dwÃÑ = dw ‚àí BÃÑ(t)0 x(t)dt,

(71b)

The corresponding backward unitary system is obtained
through the transformation
1

xÃÑ(t) = P (t) 2 Œæ(t),

(72)

which yields
1

dxÃÑ = P (t)‚àí 2 dŒæ + R(t)Œæ(t)dt.

(73)

This together with (62) and (63) yields
dxÃÑ = ‚àíA(t)0 xÃÑ(t)dt + BÃÑ(t)dwÃÑ
0

dw = B(t) xÃÑ(t)dt + dwÃÑ,

(74a)
(74b)

We consider the linear stochastic system (75) which does
not have a purely deterministic component that enables exact
estimation of components of x from y, an assumption that we
retain in the rest of the paper. In the engineering literature is
often the case that the stochastic system (75) represented as
xÃá(t) = A(t)x(t) + B(t)wÃá(t),
yÃá(t) = C(t)x(t) + D(t)wÃá(t)

x(0) = x0

(79a)
(79b)

where the formal ‚Äúderivative‚Äù wÃá is white noise, i.e.,
E{wÃá(t)wÃá(s)0 } = IŒ¥(t ‚àí s) with Œ¥(t ‚àí s) being the Dirac
‚Äúfunction‚Äù. Of course xÃá, yÃá and wÃá are to be interpreted
as generalized stochastic processes. From a mathematically
rigorous point of view, observing yÃá makes little sense since,
for any fixed t, yÃá(t) has infinite variance and contains no
information about the state process x. However, observations
of yÃá could be interpreted as observations of the increments dy
of y in a precise meaning to be defined next. On the other
hand, one can think of (75) as a system of type


x(t)
dz = M (t)z(t)dt + N (t)dw(t), where z(t) =
,
y(t)

8

and one would like to determine the optimal linear leastsquares estimate of x(t) given past observed values of y.

with R(t) = D(t)D(t)0 and initial conditions x‚àí (0) = 0 and
Q(0) = P0 . Here Q‚àí (t) is the error covariance

Generally this distinction between observing y or dy is
not important. However, when there is loss of information
over an interval (t1 , t2 ), there are two different information
patterns depending on whether dy or y is observed. The
difference consists in whether ‚àÜy := y(t2 ) ‚àí y(t1 ) is part
of the observation record or not. These two cases will be
dealt with separately in subsections below. In fact, the former,
which is common in engineering applications, is provided as
a simplified preliminary, whereas our main interest is in the
latter. To this end, we first introduce some notation.

Q‚àí (t) := E{[x(t) ‚àí x‚àí (t)](x(t) ‚àí x‚àí (t)]0 },

Consider the stochastic system (75) on a finite interval
[0, T ]. As before, let H be the Hilbert space spanned by
{wk (t) ‚àí wk (s); s, t ‚àà [0, T ], k = 1, 2, . . . , m}, endowed
with the inner product hŒª, ¬µi = E{Œª¬µ}. For any Œª ‚àà H
and any subspace A, let EA denote the orthogonal projection
of Œª onto A. We denote by H[t1 ,t2 ] (dy) the (closed) subspace generated by the components of the increments of the
observation process y over the window [t1 , t2 ]. In particular,
we shall also use the notations H‚àí
t (dy) := H[0,t] (dy) and
H+
t (dy) := H[t,T ] (dy).
Suppose that the output process or its increments are
available for observation only on some subintervals of [0, T ],
‚ó¶
namely Ik , k = 1, 2, . . . , ŒΩ. Next we want to define H as the
proper subspace of H[0,T ] (dy) spanned by the observed data.
In the case that only the increments dy or, equivalently, the
‚Äúderivative‚Äù yÃá is observed, we simply define
‚ó¶

H := HI1 (dy) ‚à® HI2 (dy) ‚à® ¬∑ ¬∑ ¬∑ ‚à® HIŒΩ (dy),
In the case that the process y is observed, we need to expand
‚ó¶
H by adding the subspaces spanned by the increments ‚àÜy over
the complementary intervals without observation. In either
case, we define
‚ó¶

‚ó¶

‚àí
H‚àí
t := H ‚à© Ht (dy) and

‚ó¶

‚ó¶

+
H+
t := H ‚à© Ht (dy).

(80)

Then Kalman filtering with missing observations amounts to
determining a recursion for x‚àí where
‚ó¶

‚àí

a0 x‚àí (t) = EHt a0 x(t),

for all a ‚àà Rn .

(81)

A. Observing dy only

dx‚àí = A(t)x‚àí (t)dt + K‚àí (t)(dy(t) ‚àí C(t)x‚àí (t)dt)
(82a)
0
QÃá‚àí (t) = AQ‚àí + Q‚àí A0 ‚àí K‚àí RK‚àí
+ BB 0

which, by the nondeterministic assumption, is positive definite
for all t.
Next suppose the observation process becomes unavailable
over the interval [t1 , t2 ) ‚äÇ [0, T ]. Then the Kalman filter needs
to be modified accordingly. In fact, for any t ‚àà [t1 , t2 ), (81)
‚ó¶

‚àí
holds with the space of observations H‚àí
t := Ht1 (dy), and
consequently
‚àí

a0 x‚àí (t) = EHt1 (dy) a0 x(t) = a0 Œ¶(t, t1 )x‚àí (t1 ).
This corresponds to setting K‚àí (t) = 0 in (82) on the interval
[t1 , t) so that
dx‚àí = A(t)x‚àí (t)dt
(84a)
with initial condition x‚àí (t1 ) given by (82a). The error covariance Q‚àí is then given by the Lyapunov equation
QÃá‚àí (t) = AQ‚àí + Q‚àí A0 + BB 0

(84b)

with initial the condition Q‚àí (t1 ) given by the value produced
in the previous interval.
Then suppose observations of dy become available again
on the interval [t2 , t3 ). Then, for any t ‚àà [t2 , t3 ), we have
‚ó¶

H+
t = H[0,t1 ] ‚à® H[t2 ,t] ,
so the Kalman estimate is generated by (82) but now with
initial conditions x‚àí (t2 ) and Q‚àí (t2 ) being those computed
in the previous step without observation. In the case there are
more intervals, one proceeds similarly by alternating between
filters (82) and (84) depending on whether increments dy are
available or not.
In an identical manner, a cascade of backward Kalman
filters generates a process xÃÑ+ (t) based on the backward
stochastic realization (78) and the observation windows [t, T ].
Assuming that there are observations in a final interval ending
at t = T , on that interval the Kalman estimate
‚ó¶

+

a0 xÃÑ+ (t) = EHt a0 xÃÑ(t),

(85)

‚ó¶

with initial observation space H+
t := H[t,T ] , is generated by
the backward Kalman filter
dxÃÑ+ = ‚àíA(t)0 xÃÑ+ (t)dt

When observations are available on the interval [0, t1 ], the
Kalman filter on that interval is given by

K‚àí = (Q‚àí C 0 + BD0 )R‚àí1

(83)

+ KÃÑ+ (t)(dy(t) ‚àí CÃÑ(t)xÃÑ+ (t)dt)
0

0

‚àí1

KÃÑ+ = ‚àí(QÃÑ+ CÃÑ ‚àí BÃÑD )R
QÃÑÀô + = ‚àíA0 QÃÑ+ ‚àí QÃÑ+ A + KÃÑ+ R(t)KÃÑ+ (t)0 ‚àí BÃÑ BÃÑ 0

(86a)
(86b)
(86c)

(82b)

and initial conditions xÃÑ+ (T ) = 0 and QÃÑ+ (T ) = PÃÑ (T ) for xÃÑ+
and the error covariance

(82c)

QÃÑ+ (t) := E{[xÃÑ(t) ‚àí xÃÑ+ (t)][xÃÑ(t) ‚àí xÃÑ+ (t)]0 },

(87)

9

which like Q‚àí (t) is positive definite for all t. During periods
of no observations of dy, we then set the gain KÃÑ+ = 0. This
update is obtained from the backward time stochastic model
(74) in an identical manner to that of (84).
Consequently, both the underlying process as well as the
filter can run in either time-direction. This duality becomes
essential in subsequent sections where we will be concerned
with smoothing and interpolation.
B. Observing y
Now consider the case that y, and note merely dy, is
available for observation on all intervals Ik , k = 1, 2, . . . , ŒΩ.
Under this scenario and with a continuous-time process the
dynamics of Kalman filtering become hybrid, requiring both
continuous-time filtering when data is available as well as a
discrete-time update across intervals where measurements are
not available.
Then on the first interval [0, t1 ] the Kalman estimate (82)
will still be valid. However, when t reaches the endpoint t2
of the interval of no information and an observation of y is
obtained again, the subspace of observed data becomes
‚ó¶

where ‚àÜy := y(t2 ) ‚àí y(t1 ). Computing x(t2 ) across the window (t1 , t2 ] as a function of x(t1 ) and the noise components
we have that
Z t2
x(t2 ) = Œ¶(t2 , t1 ) x(t1 ) +
Œ¶(t2 , s)Bdw(s)
| {z }
t1
|
{z
}
Ad
u1 (t1 )

while
t2

y(t2 ) = y(t1 ) +

Z

t2

C(t)x(t)dt +
t1

D(t)dw(t).
t1

Therefore,
Z

t2

‚àÜy =
t1

|

C(t)Œ¶(t, t1 )dt) x(t1 ) + u2 (t1 )
{z
}
Cd

where
Z

t2

u2 (t1 ) =

Z

t1

C(t)

Œ¶(t, s)B(s)dw(s)dt
Z t2
+
D(s)dw(s)
t1

Z t2 Z t2
=
C(t)Œ¶(t, s)dtB(s) + D(s) dw(s).
t1
| t
{z
}
t1


u(t1 ) =

  
Bd
u1 (t1 )
=
v(t1 )
u2 (t1 )
Dd

and Bd and Dd are chosen so that


Bd
Dd




Bd0 , Dd0 =

Z

t2

t1

Œ¶(t2 , s)BB 0 Œ¶(t, s) Œ¶(t2 , s)BM (s)0
M (s)B 0 Œ¶(t2 , s)0
M (s)M (s)0


ds

while E{v(t1 )v(t1 )0 } = I.
Hence, across the window of missing data the Kalman state
estimate x‚àí is now generated by a discrete-time Kalman-filter
step
x‚àí (t2 ) = Ad x‚àí (t1 ) + Kd (‚àÜy ‚àí Cd x‚àí (t1 ))
Kd =

(Ad Q(t1 )Cd0

+

(89a)

Bd Dd0 )

√ó (Cd Q(t1 )Cd0 + Dd Dd0 )‚àí1

(89b)

with initial conditions x‚àí (t1 ) and Q(t1 ) given by (82) and
the error covariance at t2 by
Q(t2 ) = Ad Q(t1 )A0d ‚àí Kd (Cd Q(t1 )Cd0
+ Dd Dd0 )Kd0 + Bd Bd0 .

(89c)

In the next interval [t2 , t3 ], where observations of y are
available, the new Kalman estimate (81) with

Ht‚àí2 = H‚àí
t1 ‚à® H(‚àÜy),

Z

where

t

M (s)

Thus, we obtain the discrete-time update
x(t2 ) = Ad x(t1 ) + Bd v(t1 )

(88a)

‚àÜy = Cd x(t1 ) + Dd v(t1 )

(88b)

‚ó¶

H+
t = H[0,t1 ] ‚à® H(‚àÜy) ‚à® H[t2 ,t]
is again generated by the continuous-time Kalman filter (82)
starting from x‚àí (t2 ) and Q(t2 ) given by (89).
Again given an observation pattern, where intermittently
y becomes unavailable for observation, the Kalman estimate
(81) can be generated in precisely this manner by a cascade
of continuous and discrete-time Kalman filters.
Remark 2: The observation pattern of a continuous-time
stochastic model, where y becomes unavailable over particular
time-windows, is closely related to hybrid stochastic models
where continuous-time diffusion is punctuated by discrete-time
transitions. Indeed, unless interpolation of the statistics within
windows of unavailable data is the goal, the end points of such
intervals can be identified and the same hybrid model utilized
to capture the dynamics.
Remark 3: A common engineering scenario is the case
where the signal is lost while the observation noise is still
present. This amounts to having C ‚â° 0 over the corresponding
window, and the Kalman estimates are obtained by merely
running the filters (82) and (86) in the two time directions with
the modified condition on C. This situation does not cover
the information patterns discussed above since, whenever
BD0 6= 0, the Kalman gains do not vanish and information
about the state process is available even when C is zero.

10

C. Smoothing
Given these intermittent forward and backward Kalman
estimates, we shall derive a formula for the smoothing estimate
‚ó¶

a0 xÃÇ(t) := EH a0 x(t),

a ‚àà Rn ,

EX+ (t) a0 x‚àí (t) = E{a0 x‚àí (t)xÃÑ+ (t)}PÃÑ+ (t)‚àí1 xÃÑ+ (t)
(90)

valid for both the cases discussed above, where
‚ó¶

H :=

‚ó¶

‚àí
Ht

‚à®

‚ó¶

+
Ht

‚äÇ H[0,T ] (dy)

(91)

is the complete subspace of observations. This is discussed
next.

Consider the system (75), and let X(t) be the (finitedimensional) subspace in H spanned by the components of the
stochastic state vector x(t). Then it can be shown [18, Chapter
7] that H[0,t] (dy) ‚ä• H[t,T ] (dy) | Xt , where A ‚ä• B | X
denotes the conditional orthogonality
X

hŒ± ‚àí E Œ±, Œ≤ ‚àí E Œ≤i = 0 for all Œ± ‚àà A, Œ≤ ‚àà B.

(92)

Next, let X‚àí (t) and X+ (t) be the subspaces spanned by the
components of the (intermittent) Kalman estimates x‚àí (t) and
‚ó¶
xÃÑ+ (t), respectively. Then since X‚àí (t) ‚äÇ H‚àí
t ‚äÇ H[0,t] (dy)
and X+ (t) ‚äÇ

‚ó¶

+
Ht

‚äÇ H[t,T ] (dy), we have
X‚àí (t) ‚ä• X+ (t) | X(t),

which is equivalent to
EX+ (t) a0 x‚àí (t) = EX+ (t) EX(t) a0 x‚àí (t),

a ‚àà Rn

(93a)

EX+ |X‚àí

‚àí‚Üí

EX |X‚àí&

where x+ (t) := PÃÑ+ (t)‚àí1 xÃÑ+ (t) is the dual basis in X+ (t) such
that E{x+ (t)xÃÑ+ (t)0 } = I. Moreover,
EX(t) a0 x‚àí (t) = E{a0 x‚àí (t)x(t)0 }P (t)‚àí1 x(t)
= a0 E{x‚àí (t)x(t)0 }xÃÑ(t) = a0 P‚àí (t)xÃÑ(t),

= b0 E{xÃÑ(t)xÃÑ+ (t)}x+ (t)
= b0 PÃÑ+ (t)x+ (t),
by condition (ii), and consequently
EX+ (t) E X(t) a0 x‚àí (t) = a0 P‚àí (t)PÃÑ+ (t)x+ (t).

Remark 5: The proof of condition (iii) in Lemma 4 could
be simplified if xÃÑ+ were a regular backward Kalman estimate
without intermittent loss of information. In this case, x+ =
PÃÑ+‚àí1 xÃÑ+ would be generated by a forward stochastic realization
belonging to the same class as (75) and E{xÃÑ+ (t)x‚àí (t)0 } =
PÃÑ+ (t) E{x+ (t)x‚àí (t)} = PÃÑ+ (t) E{x‚àí (t)x‚àí (t)}.



a ‚àà Rn ,

(96)

(93b)

X

where H
t is the subspace
H
t = X‚àí (t) ‚à® X+ (t).

commutes, where the argument t has been suppressed.
Lemma 4: Let x(t), xÃÑ(t), x‚àí (t) and xÃÑ+ (t) be defined as
above. Then, for each t ‚àà [0, T ],
(i) E{x(t)x‚àí (t)0 } = P‚àí (t)
(ii) E{xÃÑ(t)xÃÑ+ (t)0 } = PÃÑ+ (t)
(iii) E{xÃÑ+ (t)x‚àí (t)0 } = PÃÑ+ (t)P‚àí (t),
where P‚àí (t) := E{x‚àí (t)x‚àí (t)0 } is the state covariance of the
Kalman estimate x‚àí (t) and P+ (t) := E{xÃÑ+ (t)xÃÑ+ (t)0 } is the
covariance of the backward Kalman estimate xÃÑ+ (t).
Proof: By the definition of the Kalman filter, (81) holds,
and consequently the components of the estimation error
x(t) ‚àí x‚àí (t) are orthogonal to H‚àí
t and hence to the components of x‚àí (t). Therefore,
E{x(t)x‚àí (t)0 } = E{x‚àí (t)x‚àí (t)0 } = P‚àí (t),

(95)

Then condition (iii) follows from (93a), (94) and (95).

a0 xÃÇ(t) = E Ht a0 x(t),

X+
%EX+ |X

EX+ (t) b0 xÃÑ(t) = E{b0 xÃÑ(t)xÃÑ+ (t)}PÃÑ+ (t)‚àí1 xÃÑ+ (t)

Lemma 6: For each t ‚àà [0, T ], the smoothing estimate
xÃÇ(t), defined by (90), is given by

[18, Proposition 2.4.2]. Therefore the diagram
X‚àí

(94)

= a0 E{x‚àí (t)xÃÑ+ (t)0 }x+ (t),

where we have used condition (i) and (76). Next, set b := P‚àí a
and form

VI. G EOMETRY OF FUSION

X

proving condition (i). Condition (ii) follows from a symmetric
argument. To prove (iii) we use condition (93). To this end,
first note that, by the usual projection formula,

(97)
‚ó¶

Proof: Following [14], [3], [18], define N‚àí (t) := H‚àí
t 	
‚ó¶
+
+
X‚àí (t) and N (t) := Ht 	 X+ (t). Then
‚ó¶

‚àí

+
H = N (t) ‚äï Ht ‚äï N (t).
‚ó¶

Now, a0 (x(t) ‚àí x‚àí (t)) is orthogonal to H‚àí
t and hence to
N‚àí (t). Also a0 x‚àí (t) ‚ä• N‚àí (t). Hence a0 x(t) ‚ä• N‚àí (t) as
well. In the same way we see that a0 x(t) ‚ä• N+ (t). Therefore
(96) follows.
Consequently, the information from the two Kalman filters
can be fused into the smoothing estimate
xÃÇ(t) = L‚àí (t)x‚àí (t) + LÃÑ+ (t)xÃÑ+ (t)
for some matrix functions L‚àí and LÃÑ+ .

(98)

11

VII. U NIVERSAL TWO - FILTER FORMULA
To obtain a robust and particularly simple smoothing formula that works also with an intermittent observation pattern,
we assume that the stochastic system (75) has already been
transformed via (58) so that, for all t ‚àà [0, T ],
x(t) = xÃÑ(t)

(99)

Then (102) follows from (98) and (107). To prove (104)
eliminate LÃÑ+ in (106) to obtain
L‚àí (I ‚àí P‚àí PÃÑ+ ) = QÃÑ+ ,
which together with (107) yields
‚àí1
Q‚àí1 = Q‚àí1
‚àí (I ‚àí P‚àí PÃÑ+ )QÃÑ+ .

However,

and therefore

I ‚àí P‚àí PÃÑ+ = QÃÑ+ + Q‚àí ‚àí Q‚àí QÃÑ+ ,
0

P (t) = E{x(t)x(t) } = I = PÃÑ (t).

(100)

Then the error covariances in the filtering formulas of Section V are
Q‚àí = I ‚àí P‚àí

and QÃÑ+ = I ‚àí PÃÑ+ .

(101)

Consequently, x(t), xÃÑ(t), P‚àí (t) and PÃÑ+ (t) are all bounded in
norm by one for all t ‚àà [0, T ].
Theorem 7: Suppose that (99) holds. For every t ‚àà [0, T ],
we have the formula

xÃÇ(t) = Q(t) Q‚àí (t)‚àí1 x‚àí (t) + QÃÑ+ (t)‚àí1 xÃÑ+ (t)
(102)
for the smoothing estimate (90), where the estimation error

0	
Q(t) := E (x(t) ‚àí xÃÇ(t)) (x(t) ‚àí xÃÇ(t))
(103)
is given by
Q(t)‚àí1 = Q‚àí (t)‚àí1 + QÃÑ+ (t)‚àí1 ‚àí I,

and hence (104) follows.
In the special case with no loss of observation this is a
normalized version of the Mayne-Frazer two-filter formula [1],
[2], which however in [1], [2] was formulated in terms of
x‚àí and x+ rather than xÃÑ+ , where x+ is the state process of
the forward stochastic system of the backward Kalman filter.
(For the corresponding formula in terms of x‚àí and xÃÑ+ , see
[3], [18]; also cf. [21], where an independent derivation was
given.) With a single interval of loss of observation the formula
(102) reduces to a version of the interpolation formulas in
[6]. The remarkable fact, discovered here, is that the same
formula (102) holds for any intermittent observations structure
and by a cascade of continuous and discrete-time forward and
backward Kalman filters, as needed depending on the assumed
information pattern.

(104)

and where x‚àí , xÃÑ+ , Q‚àí and Q+ are given by (82) and (86)
with boundary conditions x‚àí (0) = xÃÑ+ (T ) = 0 and Q‚àí (0) =
Q+ (T ) = I.
Proof: Clearly the matrix functions L‚àí and LÃÑ+ in (98)
can be determined from the orthogonality relations
E{[x(t) ‚àí xÃÇ(t)]x‚àí (t)0 } = 0

(105a)

E{[x(t) ‚àí xÃÇ(t)]xÃÑ+ (t)0 } = 0.

(105b)

and
By Lemma 4, (105) yields
P‚àí ‚àí L‚àí P‚àí ‚àí LÃÑ+ PÃÑ+ P‚àí = 0
PÃÑ+ ‚àí L‚àí P‚àí PÃÑ+ ‚àí LÃÑ+ PÃÑ+ = 0,

VIII. R ECAP OF COMPUTATIONAL STEPS
Given a system (75) with state covariance (55), make the
normalizing substitution
1

1

A(t) ‚Üê P (t)‚àí 2 A(t)P (t) 2 + R(t)
1

B(t) ‚Üê P (t)‚àí 2 B(t)

C(t) ‚Üê C(t)P (t)
h
i
1
1
with R(t) = ddt P (t)‚àí 2 P (t) 2 . Next, we compute the
intermittent forward and backward Kalman filter estimates x‚àí
and xÃÑ+ , respectively, along the lines of Section V, where,
due to the normalization, Q‚àí (0) = QÃÑ+ (T ) = In . Then the
smoothing estimate is given by

xÃÇ(t) = Q(t) Q‚àí (t)‚àí1 x‚àí (t) + QÃÑ+ (t)‚àí1 xÃÑ+ (t) ,
where

which, in view of the fact that P‚àí and PÃÑ+ are positive definite,
yields
L‚àí + LÃÑ+ PÃÑ+ = I
(106a)
L‚àí P‚àí + LÃÑ+ = I

(106b)

Again by orthogonality and Lemma 4,
Q = E {(x ‚àí xÃÇ) x0 } = I ‚àí L‚àí P‚àí ‚àí LÃÑ+ PÃÑ+ ,
which, in view of (106) and the relations (101), yields
L‚àí =

QQ‚àí1
‚àí

and

LÃÑ+ =

QQÃÑ‚àí1
+ .

(107)

(108)

1
2

Q(t) = Q‚àí (t)‚àí1 + QÃÑ+ (t)‚àí1 ‚àí I

‚àí1

.

IX. A N EXAMPLE
We now illustrate the results of the paper on a specific
numerical example. We consider the continuous-time diffusion
process
dx1 (t)

= x2 (t)dt

dx2 (t)

= ‚àí0.3x1 (t)dt ‚àí 0.7x2 (t)dt + dw(t)

dy(t)

= x1 (t)dt + dv(t)

12

where w and v are thought to be independent standard Wiener
processes. Here, x1 is thought of as position and x2 as velocity
of a particle that is steered by stochastic excitation in dw, in the
presence of a restoring force 0.3x1 and frictional force 0.7x2 .
Then dy/dt represents measurement of the position and dv/dt
represents measurement noise (white).

1

y

0
-50

0
0

5

10

15

20

25

30

35

40

45

dy

0.5

-3
0

5

10

15

20

25

30

35

40

0

5

10

15

20

25

30

35

40

45

x1
x2

10

15

20

25

30

35

40

45

x 2,est
x2

0
-2

0
0

5

10

15

20

25

30

35

40

45

Fig. 5: Sample paths of output process, increment, and state
processes

-4

0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 7: Kalman estimates in the backward time direction
2

KF forward estimate x - missing data in blue intervals
1

2

1

1

0

0

-1

-1

-2

x est,1
x1

-2
0

5

10

15

20

25

30

35

40

-3

0

5

10

15

20

25

30

35

40

45

Smoothed state estimates
4
x 2,smooth
x2

2

x est,2
x2

2

x 1,smooth
x1

45

KF forward estimate x 2 - missing data in blue intervals

4

0

0

-2

-2
-4

5

KF backward estimate x 2 - missing data in blue intervals

2

5

-3

0

4

0

-5

x 1,est
x1

45

5

-5

-1
-2

0
-0.5

KF backward estimate x 1 - missing data in blue intervals

2

Output process and states

50

of intervals, data are not made available for state estimation;
these intervals where data are not to be used are marked by
a thick blue baseline in the figures. In Figure 5 we display
sample paths of the output process y, increments dy, and stateprocesses x1 and x2 .

-4
0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 6: Kalman estimates in the forward time direction
Numerical simulation over [0, T ] with T = 45 (units of
time) produces a time-function y(t) which is sampled with
integer multiples of ‚àÜt = 0.01 (units). The interval [0, T ] is
partitioned into
[0, T ] = ‚à™9i=1 [ti‚àí1 , ti ]
where t0 = 0 and ti ‚àí ti‚àí1 = i (units). Measurements of y
are made available for purposes of state estimation over the
intervals [ti‚àí1 , ti ] for i = 1, 3, 5, 9. Over the complement set

0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 8: Interpolation/smoothed estimates by fusion of Kalman
forward and backward estimates
The process increments dy over [ti‚àí1 , ti ] for i = 1, 3, 5, 9
as well as the increments ‚àÜy across the [ti‚àí1 , ti ] for i =
2, 4, 6, 8 are used in the two-filter formula for the purpose
of smoothing. The Kalman estimates for the states in the
forward and backwards in time directions, x‚àí (t) and xÃÑ+ (t)
are shown in Figures 6 and 7, respectively. The fusion of the
two using (102) is shown in Figure 8. It is worth observing the
nature and fidelity of the estimates. In the forward direction,
across intervals where data is not available, x‚àí becomes

13

increasing more unreliable whereas the opposite is true for
xÃÑ+ , as expected. The smoothing estimate is generally an
improvement to those of the two Kalman filters as seen in
Figure 8. In particular, it is worth noting x2 (in subplot 2),
where, over windows of available observations, estimates have
considerably less variance in the middle of the interval where
the weights (Q(t)Q‚àí (t)‚àí1 and Q(t)QÃÑ+ (t)‚àí1 ) in (102) are
equalized, whereas sample paths become increasing rugged
at the two ends where one of the two Kalman estimates has
significantly higher variance, and the corresponding mixing
coefficient becomes relatively smaller.
X. C ONCLUDING REMARKS
Historically the problem of interpolation has been considered from the beginning of the study of stochastic processes
[22], [23]. Early accounts and treatments were cumbersome
and non-explicit as the problem was considered difficult [7],
[8], [9], [10]. In a manner that echoes the development of
Kalman filtering, the problem became transparent and computable for ouput processes of linear stochastic systems [5],
[6], [18].
This paper builds on developments in stochastic realization
theory [11], [24] and presents a unified and generalized twofilter formula for smoothing and interpolation in continuous
time for the case of intermittent availability of data over
an operating window. The analysis considers two alternative
information patterns where increments of the output process or
the output process itself is recorded when information becomes
available. The second information pattern appears most natural
to us in this continuous-time setting, and this is our main
problem. Nevertheless, in either case, two Kalman filters run in
opposite time-directions, designed on the basis of a forward
and a backward model for the process, respectively. Fusion
of the respective estimates is effected via linear mixing in
a manner similar to the Mayne-Fraser formula and applies to
both smoothing and interpolation intermixed. In earlier works,
smoothing and interpolation have been considered separate
problems [18, Chapter 15]. The balancing normalization also
simplifies the mixing formula and makes it completely time
symmetric.
The theory relies on time-reversal of stochastic models.
We provide a new derivation of such a reversal which has the
convenient property of being balanced. It is based on lossless
imbedding of linear systems and effects the time reversal
through a unitary transformation. Interestingly, time symmetry
in statistical and physical laws have occupied some of the most
prominent minds in science and mathematics. In particular,
closer to our immediate interests, dual time-reversed models
have been employed to model, in different time-directions,
Brownian or SchroÃàdinger bridges [25], [26], a subject which is
related to reciprocal processes [27], [28], [29], [30]. A natural

extension of the present work in fact is in the direction of
general reciprocal dynamics [28], [29] and the question of
whether similar two-filter formula are possible.
A PPENDIX : T IME REVERSAL OF NON - STATIONARY
DISCRETE - TIME SYSTEMS
Next, instead of (1), consider the non-stationary state
dynamics
x(t + 1) = A(t)x(t) + B(t)w(t),

x(0) = x0 ,

(109)

on a finite time-window [0, T ], where, for simplicity we
now assume that the covariance matrix P0 := P (0) of
the zero-mean stochastic vector x0 is positive definite, i.e.,
P0 = E{x0 x00 } > 0. Then the state covariance matrix
P (t) := E{x(t)x(t)0 } will satisfy the Lyapunov difference
equation
P (t + 1) = A(t)P (t)A(t)0 + B(t)B(t)0 .

(110)

The state transformation
1

Œæ(t) = P (t)‚àí 2 x(t)

(111)

brings the system (109) into the form
Œæ(t + 1) = F (t)Œæ(t) + G(t)w(t),

(112)

where now E{Œæ(t)Œæ(t)0 } = In for all t and
1

1

F (t) = P (t + 1)‚àí 2 A(t)P (t) 2 ,
G(t) = P (t + 1)

‚àí 12

B.

(113a)
(113b)

The Lyapunov difference equation then reduces to
In = F (t)F (t)0 + G(t)G(t)0

(114)

allowing us to embed [F, G] as part of a time-varying orthogonal matrix


F (t) G(t)
U (t) =
.
(115)
H(t) J(t)
This amounts to extending (112) to
Œæ(t + 1) = F (t)Œæ(t) + G(t)w(t)

(116a)

wÃÑ(t) = H(t)Œæ(t) + J(t)w(t),

(116b)

or, in the equivalent form




Œæ(t + 1)
Œæ(t)
= U (t)
.
wÃÑ(t)
w(t)

(117)

Hence, since E{Œæ(t)Œæ(t)0 } = In and E{w(t)w(t)0 } = Ip , and
assuming that E {Œæ(t)w(t)0 } = 0,
(

0 )
Œæ(t + 1) Œæ(t + 1)
E
= U (t)U (t)0 = In+p , (118)
wÃÑ(t)
wÃÑ(t)
which yields
E{Œæ(t + 1)wÃÑ(t)0 } = 0,
0

E{wÃÑ(t)uÃÑ(t) } = Ip .

(119a)
(119b)

14

where x0 and the normalized white-noise process w are
uncorrelated and E{x0 x00 } = P0 . In fact, inserting the transformations (122) and (123a) into (124b) yields

Moreover, from (116) we have
uÃÑ(t + k) = H(t + k)Œ¶(t + k, t)Œæ(t)
+

t+k‚àí1
X

H(t + k)Œ¶(t + k, j + 1)G(j)w(j) + J(t)w(t)

y(t) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t),

j=t

for k > 0, where
(
F (s ‚àí 1)F (s ‚àí 2) ¬∑ ¬∑ ¬∑ F (t) for s > t
Œ¶(s, t) =
In for s = t.

where

Therefore, since F (t)H(t)0 + G(t)J(t)0 = 0 by the unitarity
of U (t),

From that we have the backward system

CÃÑ = C(t)P (t)A(t)0 + D(t)B(t)0
DÃÑ = C(t)P (t)BÃÑ(t) + D(t)J(t)

0

E{uÃÑ(t + k)uÃÑ(t) }
= H(t + k)Œ¶(t + k, t + 1)[F (t)H(t)0 + G(t)J(t)0 ] = 0.
Consequently, uÃÑ is a white noise process. Finally, premultiplying (117) by U (t)0 , we then obtain
Œæ(t) = F (t)0 Œæ(t + 1) + H(t)0 wÃÑ(t)
0

0

w(t) = G(t) Œæ(t + 1) + J(t) wÃÑ(t),

(120a)

0

(125)
(126)

xÃÑ(t ‚àí 1) = A(t)0 xÃÑ(t) + BÃÑ(t)wÃÑ(t)

(127a)

y(t) = CÃÑ(t)xÃÑ(t) + DÃÑ(t)wÃÑ(t)

(127b)

with the boundary condition xÃÑ(T ‚àí 1) = P (T )‚àí1 x(T ) being
uncorrelated to the white-noise process wÃÑ.

R EFERENCES

(120b)

which, in view of (119), is a backward stochastic system.

[1] D. Q. Mayne, ‚ÄúA solution of the smoothing problem for linear dynamic
systems,‚Äù Automatica, vol. 4, pp. 73‚Äì92, 1966.

Using the transformation (111), (116) yields the forward
representation

[2] D. Fraser and J. Potter, ‚ÄúThe optimum linear smoother as a combination
of two optimum linear filters,‚Äù Automatic Control, IEEE Transactions
on, vol. 14, no. 4, pp. 387‚Äì390, 1969.

x(t + 1) = A(t)x(t) + B(t)w(t)

[3] F. A. Badawi, A. Lindquist, and M. Pavon, ‚ÄúA stochastic realization
approach to the smoothing problem,‚Äù IEEE Trans. Automat. Control,
vol. 24, no. 6, pp. 878‚Äì888, 1979.

wÃÑ(t) = BÃÑ(t)0 x(t) + J(t)w(t),

(121a)
(121b)

1

where BÃÑ(t) := P (t)‚àí 2 H(t)0 . Likewise (120) and
xÃÑ(t) = P (t + 1)‚àí1 x(t + 1),

(122)

yields the backward representation
xÃÑ(t ‚àí 1) = A(t)0 xÃÑ(t) + BÃÑ(t)wÃÑ(t)
w(t) = B(t)0 xÃÑ(t) + J(t)0 wÃÑ(t).

(123a)
(123b)

Remark 8: When considered on the doubly infinite time
axis, equation (117) defines an isometry. Indeed, assuming that
the input is squarely summable, the fact that U (t) is unitary
for all t directly implies that
N
X

2

2

kwÃÑk + kŒæ(t + 1)k =

‚àí‚àû

N
X

Then, Œæ(t) ‚Üí 0 as t ‚Üí ‚àû, provided Œ¶(t, s) ‚Üí 0 as s ‚Üí ‚àí‚àû.
It follows that

t=‚àí‚àû

kwÃÑ(t)k2 =

‚àû
X

kw(t)k2 .

t=‚àí‚àû

We are now in a position to derive a backward version of
a non-stationary stochastic system
x(t + 1) = A(t)x(t) + B(t)w(t),
y(t) = C(t)x(t) + D(t)w(t)

x(0) = x0

[5] M. Pavon, ‚ÄúNew results on the interpolation problem for continuoustime stationary increments processes,‚Äù SIAM journal on Control and
Optimization, vol. 22, no. 1, pp. 133‚Äì142, 1984.
[6] ‚Äî‚Äî, ‚ÄúOptimal interpolation for linear stochastic systems,‚Äù SIAM journal on Control and Optimization, vol. 22, no. 4, pp. 618‚Äì629, 1984.
[7] K. Karhunen, Zur Interpolation von stationaÃàren zufaÃàlligen Funktionen.
Suomalainen tiedeakatemia, 1952.
[8] Y. Rozanov, Stationary random processes. Holden-Day, San Francisco,
1967.
[9] P. Masani, ‚ÄúReview: Yu.A. Rozanov, stationary random processes,‚Äù The
Annals of Mathematical Statistics, vol. 42, no. 4, pp. 1463‚Äì1467, 1971.

kw(t)k2 .

‚àí‚àû

‚àû
X

[4] F. Badawi, A. Lindquist, and M. Pavon, ‚ÄúOn the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear
stochastic systems,‚Äù in Decision and Control including the Symposium
on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE,
1979, pp. 505‚Äì510.

(124a)
(124b)

[10] H. Dym and H. P. McKean, Gaussian processes, function theory, and
the inverse spectral problem. Courier Dover Publications, 2008.
[11] A. Lindquist and G. Picci, ‚ÄúOn the stochastic realization problem,‚Äù SIAM
J. Control Optim., vol. 17, no. 3, pp. 365‚Äì389, 1979.
[12] ‚Äî‚Äî, ‚ÄúForward and backward semimartingale models for Gaussian
processes with stationary increments,‚Äù Stochastics, vol. 15, no. 1, pp.
1‚Äì50, 1985.
[13] ‚Äî‚Äî, ‚ÄúRealization theory for multivariate stationary Gaussian processes,‚Äù SIAM J. Control Optim., vol. 23, no. 6, pp. 809‚Äì857, 1985.
[14] ‚Äî‚Äî, ‚ÄúA geometric approach to modelling and estimation of linear
stochastic systems,‚Äù J. Math. Systems Estim. Control, vol. 1, no. 3, pp.
241‚Äì333, 1991.

15

[15] A. Lindquist and M. Pavon, ‚ÄúOn the structure of state-space models
for discrete-time stochastic vector processes,‚Äù IEEE Trans. Automat.
Control, vol. 29, no. 5, pp. 418‚Äì432, 1984.
[16] G. Michaletzky, J. Bokor, and P. VaÃÅrlaki, Representability of stochastic
systems. Budapest: AkadeÃÅmiai KiadoÃÅ, 1998.
[17] G. Michaletzky and A. Ferrante, ‚ÄúSplitting subspaces and acausal
spectral factors,‚Äù J. Math. Systems Estim. Control, vol. 5, no. 3, pp.
1‚Äì26, 1995.
[18] A. Lindquist and G. Picci, Linear Stochastic Systems: A Geometric
Approach to Modeling, Estimation and Identification. Springer-Verlag,
Berlin Heidelberg, 2015.
[19] T. T. Georgiou, ‚ÄúThe CaratheÃÅodory‚ÄìFejeÃÅr‚ÄìPisarenko decomposition and
its multivariable counterpart,‚Äù Automatic Control, IEEE Transactions on,
vol. 52, no. 2, pp. 212‚Äì228, 2007.

vol. 4, no. 4, pp. 173‚Äì178, 1949.
[24] M. Pavon, ‚ÄúStochastic realization and invariant directions of the matrix
Riccati equation,‚Äù SIAM Journal on Control and Optimization, vol. 18,
no. 2, pp. 155‚Äì180, 1980.
[25] M. Pavon and A. Wakolbinger, ‚ÄúOn free energy, stochastic control, and
SchroÃàdinger processes,‚Äù in Modeling, Estimation and Control of Systems
with Uncertainty. Springer, 1991, pp. 334‚Äì348.
[26] P. Dai Pra and M. Pavon, ‚ÄúOn the Markov processes of SchroÃàdinger,
the Feynman-Kac formula and stochastic control,‚Äù in Realization and
Modelling in System Theory. Springer, 1990, pp. 497‚Äì504.
[27] B. Jamison, ‚ÄúReciprocal processes,‚Äù Probability Theory and Related
Fields, vol. 30, no. 1, pp. 65‚Äì86, 1974.

[20] T. Georgiou and A. Lindquist, ‚ÄúOn time-reversibility of linear stochastic
models,‚Äù arXiv preprint arXiv:1309.0165, 2013.

[28] A. Krener, ‚ÄúReciprocal processes and the stochastic realization problem
for acausal systems,‚Äù in Modelling, Identification and Robust Control,
C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986,
pp. 197‚Äì211.

[21] J. E. Wall Jr, A. S. Willsky, and N. R. Sandell Jr, ‚ÄúOn the fixedinterval smoothing problem,‚Äù Stochastics: An International Journal of
Probability and Stochastic Processes, vol. 5, no. 1-2, pp. 1‚Äì41, 1981.

[29] B. C. Levy, R. Frezza, and A. J. Krener, ‚ÄúModeling and estimation of
discrete-time Gaussian reciprocal processes,‚Äù Automatic Control, IEEE
Transactions on, vol. 35, no. 9, pp. 1013‚Äì1023, 1990.

[22] A. N. Kolmogorov, Stationary sequences in Hilbert space. John Crerar
Library National Translations Center, 1978.

[30] P. Dai Pra, ‚ÄúA stochastic control approach to reciprocal diffusion
processes,‚Äù Applied mathematics and Optimization, vol. 23, no. 1, pp.
313‚Äì329, 1991.

[23] A. M. Yaglom, ‚ÄúOn problems about the linear interpolation of stationary
random sequences and processes,‚Äù Uspekhi Matematicheskikh Nauk,

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

A Java Cryptography Service Provider Implementing One-Time Pad
Timothy E. Lindquist, Mohamed Diarra, and Bruce R. Millard
Electronics and Computer Engineering Technology
Arizona State University East
http://www.east.asu.edu/ctas/ecet
mailto:Tim@asu.edu

Abstract
Security is a challenging aspect of communications
today that touches many areas including memory space,
processing speed, code development and maintenance
issues. When it comes to dealing with lightweight computing devices, each of these problems is amplified. In
an attempt to address some of these problems, SUN‚Äôs
Java 2 Standard Edition version 1.4 includes the Java
Cryptography Architecture (JCA). The JCA provides a
single encryption API for application developers within
a framework where multiple service providers may
implement different algorithms. To the extent possible
application developers have available multiple encryption technologies through a framework of common
classes, interfaces and methods.
The One Time Pad encryption method is a simple and
reliable cryptographic algorithm whose characteristics
make it attractive for communication with limited computing devices. The major difficulty of the One-Time pad
is key distribution.In this paper, we present an implementation of One-Time Pad as a JCA service provider,
and demonstrate its usefulness on Palm devices.

1. Problem
Dependence on the communications infrastructure
continues to grow as the size of computing devices
decreases. The growing dependence on Internet accessibility to services that do not reside in a local machine
brings with it the need for secure communications. The
target of this work are relatively small devices and their
related systems, such as Windows CE, PalmTE, Handspring and cell phones used to access Internet services.
While several large computer service organizations have
spent millions of dollars recovering from cyber attacks,
the potential economic impact of insecure e-commerce
communications on limited devices is huge[1], [3].

Java continues to enjoy dominance in server-side
technologies, however, a small but growing number of
limited device applications are developed in Java. Nevertheless, Sun Microsystems Inc., added Java Cryptography Extension (JCE) and JCA (to the Java T M 2
Development Kit Standard Edition v1.4 (J2SDK), and
has created a substantial market for applications running
on J2ME (Java 2 Micro Edition). Other vendors are
offering Java runtimes for limited devices. These versions bring Java to client application developers [9],
[11], and raise the issue of appropriate Java-based security mechanisms.
J2ME does not include JCE and JCA, however The
Legion Of The Bouncy Castle has developed a lightweight Cryptography API and a Provider for JCE and
JCA [14]. Neither provider offers implementation of the
One-Time Pad cryptography service [14].
The simplicity of the One-Time Pad method and the
fact that it does not require high processor speed, make
it ideal for lightweight computing devices.

1.1

Context

This paper focuses on integrating the JCA cryptography service provider, starting by defining the engine
classes and then implementing the One-Time Pad
method. We include simple evaluation programs to test
the provider. The problem of pad distribution is one of
the tasks taken-on in order to have successful deployment.
Implementations of the one-time pad encryption0.9.4 are readily available. For example, one product is
available for Windows command line launching. The
source code written in ANSI-C and DOS executable are
available for download at http://www.vidwest.com/otp/
[1].
The Security documentation provided with J2SDK
includes detailed information on the implementation of

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

1

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

the Provider for the JCA [12]. The documentation for ‚Äúa
Provider for JCE and JCA‚Äù of The Legion Of The
Bouncy Castle is also available [14].
The possibility of using the One-Time pad for data
encryption and decryption for security purposes on
lightweight computing devices was covered at the 35th
Hawaii International Conference on System Science
2002 [2].

Sender

Message

2. One Time Pad
The one-time pad algorithm is among the simplest in
the world of cryptography and is considered by some to
be unbreakable. It is nothing more than an exclusive OR
between the message (to be encrypted) and the pad (a
random key - sequence of bits). The principles that govern the encryption technique are not that simple to
apply. First, the key must be random, which by itself is a
big challenge. Second, parts of the key that have already
been used to do encryption must not be available for
other encryption. The key (Pad) must be a sequence of
random bits as long as the message to be encrypted. The
sender exclusive-OR's the message with the pad and
sends the result through a communication channel. The
one time requirement that makes it unbreakable and difficult at the same time is that after use, the sender must
get rid of part of the pad, and not use it again. At the
other end of the communication, the receiver must have
an identical copy of the pad. The receiver decrypts the
cipher text to obtain the original message by doing an
exclusive-OR of the incoming cipher with its copy of
the pad [1], [3], [4]. The receiver should also destroy the
pad after use. See Figure 1.

2.1

Advantages of the One Time Pad

If the pad is actually random and has been distributed
securely to the receiver, then no third party can decrypt
the message. Even guessing part of the key will not
allow a third party to determine the remainder. This is
why some people claim that one-time pad is unbreakable. While there are a number of very good pseudo-random number generators, so far any attempt to generate a
truly random key with computers appears to generate
the same sequence after a certain point. Several
approaches avoid this problem by personalizing the key.
Another advantage of this technique resides in the simplicity of its algorithm. It does not involve complex
operations that challenge the computational speed of
some relatively small processors.

XOR

Encrypted
message
transmitted
normally

OTP
Receiver

Encrypted
message
transmitted
normally

XOR

Original
Message

OTP

Figure 1. One-time pad (OTP) cryptography.

2.2

Disadvantages of the One Time Pad

The key must be as large as the message being
encrypted; this fact is sometimes inconvenient especially in the case of large messages. The principle of the
one-time pad is to have a unique key for each communication, which makes the generation and management of
keys problematic as the number of recipients and frequency of use escalates. The last challenging aspect of
the One Time Pad is the Key distribution. In fact the key
should remain undisclosed (secret) to any other party
besides the communicating parties. Extensions to the
One-Time Pad provider discussed in this paper center on
portable memory devices [15][16] that are frequently
synchronized with more capable machine (laptops or
desktops) for key exchange. However, our implementation is aimed at handheld devices in general. General
purpose (non-proprietary) portable memory interfaces
for handheld devices don‚Äôt exist yet, so another
approach is necessary. Some possibilities are discussed
in the Key Management section below. Each application
must deal with this issue in its own effective way(s).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

2

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3. Java Cryptography Architecture
The Java security model has been evolving to adjust
to new security issues. The JCA is a framework providing cryptography functionality development capabilities
for a Java platform. It was introduced early in Java‚Äôs
evolution as an add-on package. The first release of the
Security API was an extension of JCA including API‚Äôs
for encryption, key exchange, and coding message
authentication. Prior to J2SDK 1.4, JCE was optional, in
part due to export restrictions. The ‚ÄúJava Secure Socket
Extension‚Äù (JSSE) and ‚ÄúJava Authentication and Authorization Service‚Äù (JAAS) security features have also
been integrated into the J2SDK, version 1.4. Two new
security features have been introduced: ‚ÄúJava GSS-API‚Äù
(Java Generic Security Services Application Program
Interface) that can be used for securely exchanging messages between communicating applications using the
Kerberos V5 mechanism and ‚ÄúJava Certification Path
API‚Äù that includes classes and methods in the java.security.cert package. These classes allow the developer to
build and validate certificate chains.
The java cryptography architecture includes a provider architecture [2], [5], [6], [7], [10], [12]. The notion
of Cryptography Service Provider (CSP), or just provider, has been introduced in JCA. The provider archit e ct u r e a l l o w s f o r m u l t i p l e an d i n t er o p er ab l e
cryptography implementations. An application developer can create or specify his/her own cryptography service provider. The service provider interface (SPI)
presents a single interface for implementors. Classes,
methods and properties are accessible to applications
through the JCA application program interface (API).
The SPI allows a cryptography service provider to plugin implementations for java applications. A provider can
be used to implement any security service. Several providers can be available and they may or may not provide
similar cryptography services and algorithms. Figure 2
depicts the layers of Java Cryptography Architecture,
and is taken from Sun Java documentation [6].
A given installation of J2SDK may have several
cryptography service providers installed, which may
provide implementations of different algorithms and/or
may provide multiple implementations of a single cryptography algorithm. Each provider has a name that is
used by application programmers to specify the desired
provider. It is also possible to specify the order of preference of providers. The default provider that comes
with the J2SDK is the Sun provider, which includes a

wide variety of cryptographic algorithms and tools [2],
[5], [6], [7], [10], [12].

Figure 2. Java cryptography architecture

3.1

Java Cryptographic Service Providers

The Java Cryptography Architecture, which
includes the provider(s), has two main design principles,
First, is independence from implementation and interoperability: This derives from using the services without
knowing their implementation details [12]. Second, is
algorithm independence and extensibility; meaning that
new service providers and/or algorithms can be added
without effecting existing providers. Together these provide a modular architecture that allows for encryption to
be done by an implementation of a specific algorithm
and subsequent decryption to be done by another implementation.

3.2

Provider Implementing One Time Pad

The primary question when building such a provider
is: does the nature of the one-time pad allow it to be
implemented in a pluggable architecture? Figure 1
shows the interoperability between Sender and Receiver
as independent systems in communication. The element
they are required to have in common is the key. The
implementation of the algorithm does not matter. As far
as extensibility is concerned, it is up to the provider programmer to remain independent of other cryptographic
services.
A Cryptographic Service provider is a package or set
of packages providing concrete implementations of a
subset of the cryptography portion of the Java security
SPI. The Java Security Guide in J2SDK, v1.4 documentation lists a series of nine steps to follow for implementing a Provider. This paper follows those steps as
guidelines for its development.
Two aspects in the structures of cryptographic service were needed to write the implementation code: the
Engine Class and the Service Provider Interface (SPI).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

3

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3.3

Engine Class

An Engine Class is an abstraction of a cryptographic
service. It defines the service without a concrete implementation of the particular associated algorithms. Applications access instances of the engine class through the
API, to carry out available operations. Every engine
class has a corresponding service provider interface,
which provides abstract classes accessing the engine
class features. The service provider interface indicates
all the methods that the actual cryptographic service
provider should implement for a particular cryptographic algorithm or type. A service provider interface
is named with its engine class name followed by ‚ÄúSpi‚Äù
[12].
For each service that a provider implements, we must
define the engine class, and then write its service provider interface. For the One Time Pad technique, the
service provider interface‚Äôs abstract class is called OneTimePadSpi. The engine class for OneTimePadSpi in
compliance to the nomenclature of JCA is called OneTimePad. The engine class is a concrete subclass of the
service provider interface, implementing all the abstract
methods.
The provider class is a final subclass of java.security.provider. Our provider is named ASUEcetProvider.
The provider name is used by applications to access our
one-time pad service [12].

3.4

Provider‚Äôs Information

The provider class provides access to various properties of the service, including the version, and other information about the service(s) it provides such as
algorithm, type, and techniques [2], [12]. The value provided for this argument in this project is: ‚ÄúASUEcetProvider v1.0, implementing One Time Pad (OTP)
cryptographic technique, Arizona State University East,
Electronic and Computer Engineering Technology. May
2003‚Äù

3.5

Install and Configure the Provider

The provider needs to be correctly installed and configured for the application program to utilize its cryptographic service(s). There are two different ways to
install a provider. The first method consists of creating a
JAR file (Java Archive File) or ZIP file containing all
the class files belonging to the Cryptography Service
Provider. The JAR file is added to the CLASSPATH
environment variable. The exact steps of doing this last
action, depends upon the local operating system [2],
[12]. The second approach deploys the provider‚Äôs JAR
file of classes as an extension (optional package) to

Java. The file can be bundled with a particular application (with a manifest indicating relative URLs), or it can
be installed in the Java Runtime Environment to be
shared by all running applications.

3.6

Registering the Service

Configuring the service provider enables client
access to the service(s) by registering the provider and
defining default preferences where more than one provider is registered for the same service algorithm.
Static Registration consists of editing the java.security file (located in ‚Äúlib\security‚Äù subdirectory of the
Java Runtime Environment) to add the provider name to
the list of approved providers. For each available provider for a given algorithm, there is a corresponding line
in the java.security file with the form:
security.provider.<n>=<providerClassName>
Where ‚Äún‚Äù is the preference number for the provider.
For example the line:
security.provider.2=asue.provider.ASUEcetProvider
registers our provider with an order of preference 2.
Dynamic Registration can be done by a client application upon requesting service(s) from a provider. The
client application calls a class method, such as:
Security.addProvider (Provider providerName).

3.7

Test Programs and Documentation

Several test programs were written to exercise three
aspects of the service provider. For client applications to
be able to request service(s) provided by a specific provider, the provider should be successfully registered
with the security API. A simple test program can verify
registration by creating an instance of the provider and
accessing its name, version, and info (getName (), getVersion (), and getInfo () methods.
After making sure that the provider is accessible
from the security API, we need to retrieve the provided
service(s) by calling its ‚ÄúgetAlgorithm ()‚Äù method.
Finally, we check the functionality provided by the
service by writing sender and receiver applications that
use the service.
In addition to providing sample service test programs, our implementation provides documentation.
Our documentation is generated by the Javadoc tool
from the source code and it targets application programmers.

4. Key Management
The generation and distribution of the random keys
for this method is of primary concern. Since the size of
the key must match the size of the message being

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

4

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

encrypted, we limit our application to transmission of
relatively small messages. The key can be any random
array of bits, the key type used in JCA has not been
used.
The OneTimePad class has been designed, so that
when a new random key is generated, it is stored in an
external file that could be later sent to the receivers.
There are many approaches to providing random keys
for the One-Time Pad. The primary problem with using
the One-Time Pad is the amount of random bits needed.
Every message sent needs a key of the same length. If
every message is sent encrypted, the required key space
becomes large very quickly. The alternative of only
encrypting sensitive data suffers from forcing the application to make these choices, and only delays the issue
of when and how to replenish the key once all the bits
have been used on earlier messages.
Handheld devices, the target for this work, generally
only send small messages requiring encryption. This
allows for many messages using a small key of say 1
MB. A nightly synchronization using a recharging cradle can be used to also replenish the One-Time Pad key.
For devices that don‚Äôt have a connection to a host while
charging, another approach is portable memory devices
[15][16]. With a 2 GB pad, the replenishment cycle
would be much longer. Generally long enough to add a
few jpeg or gif pictures a day. The primary problem
with portable memory and handheld devices is that of
interfacing requirements. Currently, no general-purpose
interfaces (e.g., USB) are available for handhelds. Portable memory devices, to be useful, must come with general purpose interfaces, such as USB. An alternative to
the cradle and portable memory approach is to update or
replenish the pad on-line. In this approach, a One-Time
Pad is used until almost exhausted. Then the remaining
pad is used to exchange a block-cipher key for a more
computationally complex cryptography algorithm, such
as RC4 or AES. The One-Time Pad can then be generated by the server and sent to the handheld using the
complex cipher. The encryption process would then
return to the One-Time Pad methodology. This
approach is expensive on both network and CPU utilization. It may only be acceptable on larger handhelds such
as Windows CE (Pocket PC) machines. A final alternative would be to use PRNG (pseudo random number
generator) that is cryptographically appropriate. An
example PRNG is ISAAC [17]. ISAAC is a relatively
fast random number generator with a very long cycle
(i.e., guaranteed 240 with an average 28295). Generating
random pads then requires knowing the seed. Using the
prior approach of a trailing seed from the last pad as a
seed for the new pad would permit, possibly, near realtime generation of One-Time Pads.

5. Running on Limited Devices
The fact that JCA and JCE are not part of J2ME, limits applicability to our intended application space. One
approach is to configure limited client applications by
embedding the provider directly in the deployed J2ME
application.
Another approach is to use the lightweight cryptography API defined by The Legion Of The Bouncy Castle to
develop a provider based on their design principle of A
provider for the JCE and JCA [14]. This solution results
with a provider not fitting exactly the Java Cryptography Architecture, but which is usable on J2ME devices,
such as PalmOS [14].

6. Conclusions and Enhancements
As long as electronic communication continues to
expand, security will remain an issue. Cryptography is
one of the most effective tools available to address these
issues. One-time pad is among the most powerful existing cryptographic techniques, providing it is used within
the constraints of its applicability. Primarily the constraints are key management, compute limited devices
and encryption tasks that do not require encrypting large
files. The Java Cryptography Architecture offers the
potential of a single interface for applications that
allows plug-in of any number of participating service
providers. The approach allows evolution of security
approaches with the promise of minimal impact on
applications.
Security remains an open field on every computing
platform. As platforms continue to evolve to smaller and
better connected devices, they will meet the information
needs of a broader range of consumers. Its importance to
provide frameworks for developing secure distributed
and web-based applications on such mobile devices.

7. References
[1.]

d@vidwest.net (2000, November 17). ‚ÄúOne Time Pad
Encryption v0.9.4‚Äù Retrieved January 25, 2002 from
the World Wide Web: http://www.vidwest.com/otp/

[2.]

Gong, L. (1998). ‚ÄúJavaTM 2 Platform Security Architecture Version 1.1‚Äù. Sun Microsystems, Inc. Retrieved
February 20, 2003 from the World Wide Web: http://java.sun.com/j2se/1.4/docs/guide/security/spec/securityspec.doc.html

[3.]

Jenkin M. & Dymond P. (2002) ‚ÄúSecure communication between lightweight computing devices over the
Internet‚Äù. HICSS 35 January 2002.

[4.]

Kahn D. (1967) The Codebreakers, New York, NY,
MacMillan.

[5.]

Knudsen J. (1998) Java Cryptography, Sebastopol, CA,

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

5

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

O‚ÄôReilly.
[6.]

[7.]

[8.]

[9.]

Lindquist T. (2003, January 14). ‚ÄúCet427/598 Distributed Object Systems‚Äù. Retrieved February 02, 2003
from the World Wide Web:
http://pooh.east.asu.edu/Cet427/ClassNotes/Security/
cnSecurity.html
McGraw, G. and Felten, E. (1996) Java Security: Hostile Applets, Holes, and Antidotes. New York, NY.
John Wiley & Sons.
McGraw, G. (1998) ‚ÄúTesting for security during development: why we should scrap penetrate and patch‚Äù.
IEEE Aerospace and Electronic Systems, 13(4):13-15,
April 1998.
Muchow J. (2001) Core J2METM Technology and
MIDP, Upper Saddle River, NJ, Prentice Hall.

[10.]

Oaks, S. (1998) Java Security, Sebastopol, CA, O'Reilly & Associates.

[11.]

Rubin, A, Geer, D. and Ranum, M. (1997) The Web Security Sourcebook. New York, NY, John Wiley &
Sons.

[12.]

Sun Microsystems, Inc (2002). ‚ÄúJava 2 Platform, Standard Edition, v 1.4.0 API Specification‚Äù. Retried January 12, 2002 from World Wide Web: http://
java.sun.com/docs/

[13.]

Sundsted T. (2001). ‚ÄúJava, J2ME, and Cryptography‚Äù
Retrieved March 20, 2003 from the World Wide Web:
http://www.itworld.com/nl/java_sec/10262001/

[14.]

The Legion Of The Bouncy Castle (2000). ‚ÄúThe Bouncy Castle Crypto APIs‚Äù Retrieved March 20, 2003 from
the World Wide Web: http://www.bouncycastle.org/index.html

[15.]

Zbar, Jeff, ‚ÄúPortable Memory Gets Small,‚Äù retrieved
Sept. 2003; http://www.beststuff.com/article.php3?story_id=4395.

[16.]

Lexar Media, ‚ÄúSamsung Sampling 2Gb NAND Flash
Memory Devices to Lexar Media,‚Äù retrieved Sept.
2003; http://www.digitalfilm.com/newsroom/press/
press_02_25_02a.html.

[17.]

Jenkins, Bob, ‚ÄúISAAC: a fast cryptographic random
number generator,‚Äù retrieved Sept. 2003; http://
www.burtleburtle.net/bob/rand/isaacafa.html.

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

FRAMEWORKS FOR SECURING LIMITED-DEVICE APPLICATIONS
Timothy Lindquist
Aarthi Ramamurthy
Ramon Anguamea
Timothy.Lindquist@asu.edu Aarthi.Ramamurthy@asu.edu Ramon.Anguamea@asu.edu
Division of Computing Studies
Arizona State University
Abstract

In this paper, we compare the features
available for developing secure distributed
applications for limited devices, such as smart
phones. We limit our scope to examine
frameworks for Java. This work is part of a
continuing project which is considering
capabilities and performance for application
development on these platforms. The paper
considers performance as it relates to various
approaches to securing applications.
The paper addresses two separate concerns.
First is protecting access to resources by an
executing application. The facilities for defining,
limiting and controlling applications during
their development, installation and execution are
described. Second, we discuss approaches
available for securing communication among
application components running on servers or
limited devices.

1. Introduction
In this paper we consider limited devices
that are connected to the Internet and other
communication media, for example, handheld
devices such as intelligent cell phones and PDAs
with Internet connections or platforms which
combine these functionalities. These devices
have limited memory, limited processing power,
no hard disk storage, small display screens, and
limited human input capability. We consider
only those having communication facilities
(WiFi or EV-DO)..
The operating environments for these
devices are comprised of three base components:
local operating system, network operating
system and language runtime environment. The
leading operating systems for these devices are
Symbian, Palm and Windows Mobile 6.
Connected limited-device configuration and
mobile
information
device
profile
(CLDC1.1/MIDP2) are the Java frameworks
designed for resource constrained devices, such
as phones and PDA‚Äôs. CLDC1.1/MIDP2 as

realized by the IBM J9 runtime and SUN
Wireless toolkit pre-defined classes, is the
security environment we evaluated for this paper.
Various development environments are available
depending upon platform and language. For
Microsoft Windows Mobile 6 the application
development environment for the .NET
languages, such as C#, is the .NET Framework
together with Visual Studio 2005 with the
Compact Framework 2.0. Several alternatives are
available for configurations utilizing Java, in part
depending on the Java runtime environment
being used. Sun‚Äôs CLDC HotSpot and IBM‚Äôs J9
are two popular Java runtime environment
choices. Add-on packages and various
configurations are available to support different
security approaches, device capabilities and
networking needs.

2. Background
The connectivity of computing devices to
the Internet, has enabled malicious attacks. The
motivation for attacks varies from willful
espionage to experimentation. Equally important
to protection from attack is the ability to prevent
harm from mistakes in coding, configuration or
user operations. Protection and detection are
difficult in handheld devices because of limited
capability.
Trust
is
confidence
in
expected
functionality. When running an application the
user must trust that it produces valid information
and that privacy, integrity, or confidentiality will
not be compromised. There are several security
policies, protocols and mechanisms that are of
particular interest to the limited devices.
Languages such as Java, C#, and other scripting
languages are widely used in distributed
applications and provide varying degrees
security support. Java and C# both permit
examination of compiled intermediate code for
unsafe actions. Both the Java CLDC/MIDP and
Mobile 6 execution environments support an
array of cryptographic functions that can be used

1530-1605/08 $25.00 ¬© 2008 IEEE

1

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

by communication protocols and by the system
elements to aid in access control at the device
and application level.

3. Java CLDC/MIDP
Three different limited device configurations
exist for the Java 2 Micro Edition (J2ME). For
more capable devices such as set-top boxes and
high-end wireless devices the Connected Device
Configuration (CDC) defines an API whose
functionality is close to J2SE, but is reduced as
appropriate for the limited hardware and
applications. At the lowest level of functionality
is the JavaCard API (for Smart-cards/Sim-cards).
JavaCard as can include functionality for
asynchronous security operations, such as
encryption, decryption, digital signature,
verification and others for limited devices whose
computing capacity is unable to perform such
operations without disrupting user-functionality.
The Connected Limited Device Configuration
(CLDC) is defined for PDA and wireless phone
devices. A device such as a PDA or smart-phone
running Java applications would include a virtual
software stack with the following components:
‚Ä¢ Mobile Information Device Profile
(MIDP2) that supports the application
life-time model, persistent storage,
network resources and the user-interface.
‚Ä¢ CLDC1.1 that supports the core Java
language, IO and networking classes,
security features and internationalization
facilities.
‚Ä¢ The selected Java runtime environment
‚Ä¢ The device operating system and related
services

3.1 Application Security Model
The J2SE model for securing the operations in
an executing virtual machine changed
dramatically as Java evolved. Java originally,
used the sandbox model for application security.
Initial versions of Java provided full trust to
classes loaded locally and prohibited all sensitive
operations from any code obtained dynamically.
Java1.2 introduced support for a continuum of
access control. Access to system resources (such
as files, sockets, runtime, properties, security
permissions, serializable, reflection, and window
toolkit) is granted based on domains. A domain
is defined to include:
‚Ä¢ a set of permissions (resources),

‚Ä¢
‚Ä¢
‚Ä¢

a set of operations associated with
each permission,
codebase indicating the code origin,
a digital signature of the code which
allows identification of the signer
and verification that the code has not
been modified.

The codebase indicates the file or URL from
which the code is loaded. If signed, the alias of
the public key can also be used to define a
domain. Each class loaded into a Java virtual
machine has an associated protection domain,
which defines the access it has to resources.
When execution encounters an operation that
requires a system resource, all classes
representing the currently executing methods
(contents of the runtime stack) are checked to
assure all have access to the resource. The Figure
below is taken from the On-line Java Tutorial
and shows how an execution can include a range
of protection domains ranging from no access to
resources to full access.

Figure 1.

Controlling Access to Java Resources

In J2SE (Java2), security domains are defined
by a policy file granting permissions to the
domain. For example, suppose the company
GrowthStocksExpress publishes an applet on
their (hypothetical) web site at the URL:
http://GSE.com/applets
Assuming the applet needs connections to one
or more hosts having a domain address ending
with GSE.com on ports beginning at 2575, a
policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase
"http://GSE.com/applets"
{
permission java.net.SocketPermission

2

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

"*.GSE.com:2575-", "accept, connect, listen, resolve";
};

A policy may consist of one or more grants each
defining different domains. Each domain may
have one or more associated permissions.
CLDC/MIDP2 security. The application
security model for CLDC/MIDP2 draws on the
model for J2SE, in that it includes domains and
signed code. CLDC/MIDP2, however has a
simpler model, in part because of the constraints
imposed by the configuration and profile. The
following CLDC/MIDP2 constraints are most
significant
‚Ä¢ Java Native Interface (JNI). JNI provides
J2SE applications access to native code
running on the platform. CLDC provides
similar capabilities in Kilo Native
Interface (KNI), but prohibits dynamically
loading and calling arbitrary native
functions.
‚Ä¢ No reflection, remote method invocation
or serialization. In J2SE, an RMI server or
client can cause remote code to be
automatically downloaded and executed
to satisfy argument or return (sub)classes.
When a serializable RMI parameter is
provided an argument of an extended
type, the RMI system will attempt to load
(if necessary from an http codebase) the
needed class.
‚Ä¢ No user-defined class loaders. Related to
the constraint above, the developer cannot
define a class loader in CLDC. The classloader in CLDC cannot be extended or
replaced
by
the
developer.
A
CLDC/MIDP2 application can only load
classes from its own (signed) Java
archive. As a result, the developer cannot
extend or modify any classes in the CLDC
configuration, MIDP2 profile, or which
are provided by the runtime environment
vendor.
‚Ä¢ CLDC supports multi-threading, but it
does not provide facilities to build
daemons or thread-groups.
MIDP2 security protects access to
sensitive API‚Äôs by permissions. Protecting
resources includes the concept of a domain,
which is conceptually similar to J2SE. The
full scope of protection includes the following
elements:
‚Ä¢ Protection domains (4) that are
statically defined in a policy file (by

‚Ä¢

‚Ä¢

the vendor) and associated to
resource permissions; for example,
socket, http, https, PushRegistry. The
protection domains are Minimum,
Maximum
(or
Trusted)
and
Untrusted.
Certificate and archive signature.
The jar file containing application
class files and other resources can be
digitally signed.
Level of access ‚Äì either Allowed or
User.

Unlike J2SE, the 4 protection domains are
device-specific and defined by the runtime
vendor. They can be modified only as
provided by the vendor. Each of the four
domains is associated with a set of
permissions together with a level of access.
The 4 protection domains are defined by the
runtime vendor.
‚Ä¢ Minimum. None of the permissions
are allowed.
‚Ä¢ Maximum. All of the permissions are
allowed.
‚Ä¢ Trusted. All of the permissions are
allowed.
‚Ä¢ Untrusted. To be allowed, the user
must provide consent.
The permissions defined by the MIDP2
specification include: http, socket, https, ssl,
datagram, serversocket, datagramreceiver, and
PushRegistry (invoke other applications). These
permissions may be grouped together by the
vendor into meaningful subsets and assigned to
domains based on the subsets; for example,
NetworkAccess.
Within a domain, the level of access may be
different for different permission (sets). The
accesses are:
‚Ä¢ Allowed. The permission (set) is
allowed without involving the device
user.
‚Ä¢ User level. The application‚Äôs access
to the permission(set) depends on
explicit authorization from the
device user.
With user level of access, a dialog box is
presented to the user indicating information
about the permission and asking the user

3

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

whether access should be granted. User level
access can be specified on one of 3 modes:
‚Ä¢ Oneshot. The user must be prompted
for each operation on the protected
resource.
‚Ä¢ Session. When the user grants
access, it applies to all operations on
the resource during a single
execution of the MIDlet.
‚Ä¢ Blanket. When the user grants
access, it applies to all operations on
the resource during any execution of
the MIDlet.
CLDC/MIDP2 provides MIDlet access to the
Record Management System (RMS), which
provides persistent storage for application data
via a record store. MIDP2 provides shared access
to the record store of other MIDlet suites, and
provides that access should be provided as readwrite or read-only.
Low-level security is provided by the J2ME
Java virtual machine. A virtual machine
supporting CLDC must reject invalid class files.
This is accomplished by a two-step process. At
development time, classes are pre-verified by a
tool which adds special attributes to class files to
facilitate runtime class verification on the device.
Much of the verification process can be handled
statically by the pre-verifier. At runtime, the
virtual machine rejects classes that have not been
pre-verified.

3.2 Security and Trust API
MIDP2 provides HTTPS and SSL for secure
communications with other devices. But, runtime
environment
providers
are
increasingly
providing additional options. For example,
IBM‚Äôs J9 version 5.7 provides web service
security package which allows web method calls
using encrypted SOAP envelopes or digitally
signed method calls for authentication and
information integrity.
Security and Trust Services API (SATSA)
provides access to more comprehensive hash
code,
digital
signature/verification,
key/certificate management, as well as
encryption and decryption. SATSA is designed
as 4 optional components. The primary purpose
is to provide access to a SmartCard Java device,
which provides security functionality in an
asynchronous manner that does not disrupt
applications supporting the device user.

SmartCard includes the Java Card Protection
Profile. The protection profile supports both
open and closed cards. Open cards provide the
end-user with the ability to install or activate
new applications on the card. Closed cards have
applications set by the vendor at the time the
card is personalized for the end-user. A good
example of a closed card may be a banking card
that supports personal electronic purchases and
bank account functions. Open cards that allow
new applications to be downloaded and installed
on the card present special security risk that
would exclude open cards that include banking
applications. Nevertheless, applications for open
cards that support other aspects of security may
become increasingly important. An example may
be securely communicating information outside
of direct e-commerce applications. Data integrity
and authentication are becoming increasingly
important
as
electronic
communication
proliferates.
The Java Card Protection Profile defines
four different configurations for a Java Card
based on open and closed cards. The minimum
configuration corresponds to a closed card in
which no applications can be installed on the
card after it‚Äôs been issued to an end-user. The
three
remaining
configurations
provide
additional functionality that‚Äôs available through
the evolution of the Java Card specification, such
as RMI (a limited version), logical channels,
applet deletion, object deletion, external
memory, biometry, and contactless interface.
The Java virtual machine for the device includes
an API (RTE API) that may contain classes for
performing security operations on information
and for certificate and key management.
SATSA runs on the limited-device, not on
the Java Card. SATSA provides an interface to
card security functionality, or when there is no
associated smart card, provides security
operations for the limited-device. SATSA has
four optional packages.
‚Ä¢ SATSA-APDU provides low-level
stream/socket-based protocol for communicating
between the limited-device and the card.
‚Ä¢ SATSA-JCRMI provides an RMI
interface that allows an application running on
the limited-device to call methods running in
applications on the Card. This interface would be
used to instead of SATSA-APDU to avoid the
overhead of programming with a low-level
socket data protocol.
‚Ä¢ SATSA-PKI allows limited device
applications to use the smart card to digitally
sign information or to verify digital signatures.

4

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

PKI also provides for key and certificate
management.
‚Ä¢ SATSA-CRYPTO. When a Java Card is
not available, the CRYPTO API is used to
compute security operations directly on the
limited-device.
Use of APDU, JCRMI, and/or PKI is
accomplished using threading on the limiteddevice. Threading allows security operations to
take place on the card while other applications
continue to run on the device supporting the enduser. In this scenario, SATSA is appropriate for
limited-devices with constrained processing
power. Independent of processing capability,
using a Java Card may be necessary to provide
assurance level that is appropriate to the
application. The open-device nature of cell
phones and PDA‚Äôs make it difficult to certify
trustworthiness of applications on the device.
Instead, we can isolate all high-risk user-specific
information and computations to a certified
secure Java Card.

HTML page, it returns an XML message in
Simple Object Application Protocol (SOAP)
format.
The service description - specified in Web
Services Description Language (WSDL) - this
description defines the web methods (functions)
that a service will accept - the inputs that go into
these methods, and the format of the output that
can be expected in return. This is used in
generating a web service client proxy class for
the limited device.
The web service registry - is a directory of
web services. The directory is optional because a
web service need not be listed in a registry to be
used. The registry provides a catalogue of
available services - similar to Java Naming and
Directory Service (JNDI).
The web service client proxy ‚Äì The proxy
negotiates the communication between a limited
device client and the web service. It marshals
arguments, signs or encrypts as appropriate,
posts the message and interprets the result.

4. Secure Web Services

4.1 Types of Security Services

Web services provide an XML-based
service protocol for communicating among
components of a distributed application. Web
services differ from prior similar technologies,
such as Microsoft DCOM, Object Management
Group CORBA and Java Remote Method
Invocation through reliance on http protocol and
XML.

The following are the security services that
may be required by a distributed limited device
application.
Authentication: Ensures that the sender and
receiver are who they claim to be. Mechanisms
such as username/password, smart cards, and
Public Key Infrastructure (PKI) can be used to
assure authentication.
Authorization or Access Control: Ensures that
an authenticated entity can access only those
services they are allowed to access. Access
control lists are used to implement this.
Confidentiality: This assures that information
in storage and in-transit are accessible only for
reading by authorized parties. Encryption is used
to
assure
message
confidentiality.
Integrity: Ensures that information, either in
storage or in-transit cannot be modified
intentionally unintentionally. Digital signatures
are used to assure message integrity.
Non-repudiation: Requires that neither the
sender nor the receiver of a message be able to
legitimately claim they didn't send/receive the
message.

Figure 2. Web Services Architecture [9]
In Figure 2, the service, is performed by a web
server acting as a container for executing the
service code. This is generally just a web like
page that gets posted similar to the way other
http web requests are done. Instead of returning a

4.2 Transport Level Security
The most popular security scheme for web
services is SSL (Secure Socket Layer), which is
typically used with http, and is supported by

5

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

CLDC/MIDP2.
However using SSL for
securing web services has a number of
limitations. The inadequacy of SSL can be easily
explained by a simple example.
Consider a Web Service that can be provided
indirectly to a user. A user accesses a website
which indirectly invokes another remote web
service. In this case, we have two security
contexts:
1. Between the user and the website
2. Between the user and the web service
The second security context requires the
security of SOAP request/reply message
(between the web site and the web service) to be
assured over more than one client-server
connection. SSL is inadequate to provide this
type of security mainly because of the fact that
while it encrypts the data stream, it does not
support end-to-end confidentiality.
The shortcomings of SSL (https) should be
considered when being used for a distributed
application to reside on a limited-device.
SSL is designed to provide point-to-point
security. Often, Web services require end-to-end
security, where multiple intermediary nodes
could exist between the two endpoints. In a
typical Web services environment XML-based
business documents route through multiple
intermediary nodes.
Https in its current form does not support
non-repudiation well. Non-repudiation is critical
for business Web services and, for that matter,
any business transaction.
Finally, SSL does not provide element-wise
signing and encryption. For example, if there is a
large purchase order XML document, yet only a
single element, say, a credit card element needs
to be encrypted. Signing or encrypting a single
element is difficult with transport level security.

4.3 XML Signature
XML based security schemes, provide unified
and comprehensive security functionalities for
Web Services. The important ones being, XML
Signature, XML Encryption, WS-Security (Web
ServicesSecurity).
The W3C (World Wide Web Consortium)
and the IETF (Internet Engineering Task Force)
jointly coordinated to generate the XML digital
signature technology. The XML digital signature
specification [10] defines XML syntax for
representing digital signatures over any data
type. It also specifies the procedures for
computing and verifying such signatures.

Another important area that XML digital
signature addresses is the canonicalization of
XML documents. Canonicalization enables the
generation of the identical message digest and
thus identical digital signatures for XML
documents that are syntactically equivalent but
different in appearance due to, for example, a
different number of white spaces present in the
documents.
The advantages of using XML digital
signature can be summarized as below.
‚Ä¢
‚Ä¢

‚Ä¢

‚Ä¢
‚Ä¢

It accounts for and takes advantages of two
existing and popular technologies, viz., the
Internet and XML.
XML digital signature provides a flexible
means of signing. For example, individual
item or multiple items of an XML document
can be signed. This becomes extremely
useful in a scenario where each person in a
workflow is responsible ONLY for certain
work.
It supports diverse sets of Internet
transaction models. For instance, the
document signed can be local or even a
remote object, as long as those objects can
be referenced through a URI (Uniform
Resource Identifier). A signature can be
either enveloped or enveloping, which
means the signature can be either embedded
in a document being signed or reside outside
the document.
It provides important security features like
authentication, data integrity (tamperproofing), and non-repudiation.
XML digital signature also allows multiple
signing levels for the same content, thus
allowing flexible signing semantics. For
example, the same content can be
semantically signed, cosigned, witnessed,
and notarized by different people.

Figure 3. WSE ‚Äì Input/Output filters [1]

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

Web Service Deployment Descriptor
(WSDD) and Handlers play the pivotal role in
the implementation of digital signature in Java.
A deployment descriptor specifies aspects such
as handlers and communication protocol. The
Handler is a java class (implementing the Input
and output filters of Figure 3) that provides a
MessageContext through which access is
provided to the input/output stream of XML \In
the Apache AXIS framework, MessageContext
is a structure, that contains: 1) a request message,
2) a response message, and 3) a number of
properties.
All the SOAP message manipulation is done
within the handler class.

6. Conclusions and Issues
The authors have continuing efforts in this
area which include obtaining devices, software
development environments, and simulators /
emulators related to securing limited-devices.
Our approach considers the operating
environment on the device as well as their
applications.
We are in the midst of consolidation of
small hand-held devices to provide integrated
functionality. Common applications including
personal organizers, cell-phones, and multimedia players can effectively be placed on a
single platform. While users who desire more
than one of these functionalities are exploring
integrated solutions, the industry is pushing
separation (partly for financial reasons.)
Consolidated functionality brings a higher
diversity of applications onto limited devices, as
does special purpose applications (for example
autonomous vehicle control). Either way,
security concerns increase.
The use of smart cards in the United States
is just beginning after lagging behind use in
some other regions. The integration of smart
cards (SIM-Cards) on cell phones is an
indication of this trend. Enabling high-risk
applications, such as banking and purchasing,
by leveraging smart cards integrated with other
devices presents an attractive alternative. Of
course the concern for security places new
demands on platforms in which security has not
historically been a high priority.
Performance has been the primary
impediment to the use of more strongly objectoriented languages such as Java for limited
device applications. Securing a distributed

application complicates the issue. Important
considerations include:
‚Ä¢ Underlying
architecture
processor
performance,
ancillary
processing
capability such as SmartCard,
‚Ä¢ Frameworks supporting securing the
application, as well as communications,
‚Ä¢ Use of security mechanisms appropriate
to application needs (authentication,
integrity, confidentiality),
‚Ä¢ Proper use of available frameworks
including proper handling of passwords,
certificates, keys, digital signatures, and
encrypted information.
Frameworks discussed in the paper are an
important enabler to developing more secure
distributed limited-device applications. Further
usage reports and benchmarking for security
mechanisms would better support developers.

References
[1.] Tim Ewald, ‚ÄúProgramming with Web
Services Enhancements 1.0 for
Microsoft.NET‚Äù, Available, see:
http://msdn.microsoft.com/webservices/buil
ding/wse/default.aspx?pull=/library/enus/dnwse/html/progwse.asp
[2.] Sun Microsystems Java Security and
Crypto Implementation,
http://www.cs.wustl.edu/~luther/Classes/Cs
502/WHITE-PAPERS/jcsi.html
[3.] Knudsen, Jonathan; Understanding MIDP
2.0‚Äôs Security Architecture.
http://developers.sun.com/techtopics/mobilit
y/midp/articles/permissions/
[4.] WebSphere Everyplace Micro Environment
v5.7; MIDP Installation guide for J9 Palm
runtime environment. Available online from
IBM.
[5.] Mourad Debbabi, Mohamed Saleh,
Chmseddine Talhi and Sami Zhioua:
‚ÄúSecurity Evaluation of J2ME CLDC
Embedded Java Platform‚Äù, in Journal of
Object Technology, (5,2) Mar-Apr 2006. pp.
125-54.
[6.] Security and Trust Service APIs for Java
Platform Micro Edition Developers Guide.
Available from http://www.java.sun.com/
[7.] Pannu, K.; Lindquist, TE; Whitehouse, RO;
and Li, YH; ‚ÄúJava Performance on Limited
Devices‚Äù; Proc The 2005 International
Conference on Embedded Systems and
Applications, CSREA Press, Las Vegas,
June, 2005.

7

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

[8.] Lindquist, TE, Diarra, M, and Millard, BR;
‚ÄúA Java Cryptography Service Provider
Implementing One-Time Pad‚Äù; Proc. 37th
Annual Hawaii Int'l Conf on Systems
Sciences, ACM, IEEE Computer Society,
January 2004.
[9.] Online Documentation on Web Services,

Available from: http://www.servicearchitecture.com/webservices/articles/web_services_explained.ht
ml
[10.] XML Digital Signature Specification, W3C
Recommendations, Available from:
http://www.w3.org/TR/xmldsig-core

8

Tracking Personal Processes in Group Projects
Ly Danielle Sauer, Timothy E. Lindquist, Jeremy Cairney
Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287-5406
{sauer, lindquist, cairney}@asu.edu
February 8, 1999
Abstract
Software engineering continues to develop methods for process improvement and quality. The Personal Software
Process is one way to introduce software engineers to aspects of process tracking, assessment and improvement. In
this paper, we describe the software tools that we‚Äôve constructed to support the planning and postmortem of software
activities. We describe an approach that allows the personal software process to be used in group projects, while still
allowing the individual engineer to employ personal process quality and improvement techniques on their own activities. The tools supporting planning and postmortem are used in the context of a workflow system developed at ASU,
called Open Process Components, whose aim is to componentize software services and provide interoperability
among various approaches. These tools and approaches explore software development in the increasingly distributed
environment in which the software engineer is responsible for their own assessment and improvement.

1.0 Introduction
Measuring, guiding and refining an organization‚Äôs software process improves effectiveness of development
resources and provides a level of control on software
quality. The development of the SEI Capability Maturity
Model[24] has raised awareness of the need for better
software processes. Software processes are often discussed at the project management level, and its not
uncommon for an organization to employ the services of
a process engineer with the intent of wide-scale process
improvement.
Software processes describe the interaction among people and artifacts in carrying out the work involved in the
software life-cycle. A software process encompasses the
work that will be done (activities), what it will use and
produce (input and output products), who will do it
(agents), as well as, when and how it will be done
(behavior). The past decade has seen increased demand
for more powerful and robust automated software process systems. Tool vendors and the research community
have responded with a variety of approaches.
A review of the tool market place shows many groupware, process and workflow tools whose functionality
ranges from graphical modeling or simple enactment to
full support for defining, executing, analyzing, measuring, and tracking software processes. The Plethora of

tools, most of which have not been widely adopted,
combines together with the increasingly distributed
nature of software development today to form one of the
challenges addressed by this paper. That is, the need to
have interoperability among a heterogeneous set of process tools (which execute on distributed heterogeneous
platforms.)
The efforts of the Workflow Management Coalition
(WfMC) [30] and the Object Management Group
(OMG) are aimed at this challenge. Both organizations
are identifying common interfaces that vendors can use
for interoperability among their products. Other middleware efforts have identified process support services, for
example, PCIS (Portable Common Interface Set) [9]. A
follow-on project [18] integrates the Open Process
Components of Gary [13] with other middleware components, such as version and configuration management. In this paper, we build on these efforts to show
how processes can be distributed compositions of personal process components.
Considerable research has addressed automating the
software process. Some are addressing formalisms for
expressing process [3]. Different formalisms such as
Petri nets[12], rule-based formalisms[1,25], process
programming languages[10], event-based representations [4,8,21], and object-oriented approaches[8,21] are

1 of 8

Overview of Personal Software Process

proposed for representing software processes. Other
research includes comprehensive environments centered
on process, such as ISTAR, in which all activities are
modeled using a contractual model. In a process-centered environment, nearly all activity takes place within
a defined process.
Christie has elaborated several problems in the adoption
of process automation [6]. Process-centered environments are typically all-or-nothing and difficult to adopt
in steps. Management is justifiably reluctant to invest in
dramatic change without a gradual migration path or
concrete evidence of value-added. Benefits of enactment support or tracking require time consuming frontend resources for process definition. Some systems
require definition of activities that don‚Äôt have relevance
to tracking and improvement. Adoption also places
other stresses on an organization ranging from engineer‚Äôs perception of excessive intrusion to the need for
additional personnel who specialize in process engineering.
In this paper, we present a process framework that shifts
its approach toward composable process components.
Project processes are created by brokering among the
building blocks of engineer‚Äôs defined personal processes. Software engineers are responsible for tracking
measuring and analyzing their own processes distinct
from organizational concerns.

2.0 Overview of Personal Software Process
Current software professionals utilize private techniques
and practices that were learned from peers or through
personal experiences. Few software engineers are aware
of, or consistently use methods that lend themselves to
personal process improvement. A personal software
development process is a concept introduced to address
improvement needs of an individual.
Watts Humphrey of the Software Engineering Institute
has formalized a personal software development process
called Personal Software Process (PSP). Today, there
are various realizations of PSP to aid software engineers
in applying the process. The realizations range from
case tools and web-based repository browsers to formal
training classes.

2.1

Personal Software Process

Personal Software Process (PSP) [15,16,17,28] is
designed to assist software engineers in controlling,

Tracking Personal Processes in Group Projects

managing, and improving their predictability, productivity, and quality. PSP consists of a family of seven personal processes that progressively introduce data and
analysis techniques (Figure 1 on page 2) [16,28]. Engineers use these data and analysis outcomes to determine
their performance and to measure the effectiveness of
their methods. Humphrey‚Äôs initial result (applied to 50
students and three industrial software organization) indicates an average test defects improvement of over ten
times and productivity improvements of better than 25%
[28].
PSP3

Cyclic
Personal
Process

Personal
Quality
Managem
ent

PSP2

PSP2.1

Code Reviews
Design Reviews

Design Templates

PSP1.1

PSP1

Personal
Planning
Process

Size Estimating
Test Report

PSP0
Baseline
Personal
Process

Cyclic
Development

Current Process
Time Recording
Defect Recording
Defect Type Standard

FIGURE 1.

Task Planning
Schedule Planning

PSP0.1
Coding Standard
Size Measurement
Process Improvement
Proposal

PSP Process Evolution

Figure 1 on page 2 shows the PSP progression in which
each PSP step includes all the elements of prior steps
together with additions. The PSP process steps are Baseline Personal Process (PSP0, PSP0.1), Personal Planning Process (PSP1, PSP1.1), Personal Quality
Management (PSP2, PSP2.1), and Cyclic Personal Process (PSP3). Starting in The Baseline Personal Process,
the software engineer creates the foundations for measurement and improvement. PSP0 is the software engineers current software development process extended to
provide measurements (time and defect trackings).
PSP0 covers three phases: planning, development
(design, code, compile, and test), and postmortem.
PSP0.1 includes coding standards, size measurements,
and a Process Improvement Proposal (PIP).
Personal Planning Process (PSP1, PSP1.1) adds planning to the baseline. Here, the software engineer pre-

2 of 8

Applying PSP to Group Projects

pares the basis for project tracking, which include
software estimates and development plans. The goal is
to learn the relationship between program size and
resources, as well as how to make realistic schedules.
PSP1 enhances PSP0 and PSP0.1 to include size and
resource estimation and a test report using Proxy Based
Estimation (PROBE) as a method to estimate sizes and
development times.
Personal Quality Management (PSP2, PSP2.1) provides
defect management by tracking the relationship between
time spent in reviews and the phases during which
defects are injected and removed. Prior project defect
data are used to realize review checklists and selfassessments. PSP2.1 extends PSP2 with design specifications and accompanying analyses. The goal is to provide the criteria for design completion.
Cycle Personal Process (PSP3) introduces techniques
for developing large-scale projects. The approach calls
for sub-dividing into personal processes. Development
is done in incremental steps starting with a base module.

2.2

Personal Software Process Studio

Personal Software Process Studio (PSP Studio or PSPS)
[11] is a case tool developed at East Tennessee State
University to assist in using the Personal Software Process. PSPS does so by automating the planning and
postmortem artifacts. In particular, PSPS provides the
following features: Data Measurement, Historical Database, Convenient Access to Tables, Statistical Calculations, and Guidance through the Process. The Data
Measurement feature allows developers to accurately
(similar to a stopwatch) measure development times,
track defects, and measure program sizes. The Historical Database feature allows developers to store all of
the developers historical PSP data in a reliable and
secure database. Convenient Access to Tables provides a
window with tab access to the forms. The Statistical
Calculations feature automatically maintains totals and
performs the statistical calculations. The Guidance
through the Process feature provides on-line direction
for using the PSP.
PSP Studio groups all of the automated paper works,
forms, and calculations into two categories: Process
Tables and Project Tables. Process Tables guide or
improve the individual software engineer process with
an online process outline, access to standard tables for
defect, LOC and coding standards, and access to a
process improvement proposal.

Tracking Personal Processes in Group Projects

Project Tables are automated forms, such as Logs, Summaries, and Templates. The log tables support tracking
time, defects and issues. The Project Summary table
records the estimated and actual totals for the project
and for all projects to date. The Cycle Summary table
supports the project summaries by capturing the planned
and actual size, time, and defects for each cycle.

2.3

ECEN 4553 Database Browser

ECEN 4553 PSP Database (PSP Database Browser) [5]
is a web-based database that also automates many of the
PSP forms, scripts, calculations, and reports. The database is organized to capture a set of related data for an
individual software engineer. The core of the database is
the concept of a job, which is a software engineer‚Äôs
activity. Once the job is defined, the software engineer
can log time against the job, log defects against the job,
and specify a detailed project plan for the job.
Although the PSP Database Browser does not strictly
adhere to all of Watts Humphrey‚Äôs Personal Software
Process data, it collects planned and actual data for each
job. ECEN 4553 PSP Database Browser automates a
subset of Watts Humphrey‚Äôs Personal Software Process
with a web-based user interface.

3.0 Applying PSP to Group Projects
ISO 9000 [7,22] and the Capability Maturity Model
(CMM) [23,24] assist organizations in improving their
processes. Personal Software Process [15,16,17,28], on
the other hand, provides an improvement technique for
software engineers, in the context of individually developed software. Seamless integration of the PSP within a
software organization cannot be achieved, since individuals rarely cycle through all phases of development on a
software project. Engineers can, however, apply PSP
analysis techniques to their individual activities on
group projects. The resulting metrics can be the basis for
personal process improvement, without having the ‚Äúbig
brother is watching over me‚Äù complex that is common
to organizationally imposed quality and improvement
efforts. This section discusses our approach to providing
well integrated organizational and personal process
improvement.
At ASU, we have been developing software to support
the use of planning and postmortem phases of the PSP
and to support their application to various life-cycle
activities. For example, in an organizational setting, an
individual may be assigned to testing. The test engineer

3 of 8

Applying PSP to Group Projects

would develop their own test process that includes planning and postmortem. The resulting personal test process
becomes part of an organizational or project process. The
‚Äúintegratable‚Äù personal processes (personal test process)
collect product measures, use defect analysis and consider
resource usage as a means of improving that process segment. The artifacts and the automation we have developed
are discussed in Section 3.2.

3.1

Process Components

The Open Process Component Toolset (OPC) [14,19,20] is
a set of tools developed at ASU to support process definitions and enactment. OPC‚Äôs basic premise is that a process
is a process component and may consist of one or more
process components. Process components may be compositions of subcomponents whose underlying representations may differ. For example, a Process Weaver
component, called create_design, may be composed with a
TeamWare Flow component called review_design. Thus,
the Integrated Process is defined as a process component
consisting of three process components: the Planning Process Component (Figure 2 on page 4), the Personal Software Activities Component, and the Postmortem
Component. The Software Activities component may be
any process component such as, testing, design, coding, or
review.
Discussion of these components and the support we have
implemented for Planning and Postmortem artifacts can be
found in the following sections (Section 3.1.1, Section
3.1.2, and Section 3.1.3).
3.1.1 The Planning Process
The Planning Process Component defines the individual
engineer‚Äôs plans for the software activity. The process is
assigned to the project planner, takes as inputs the customer requirements (written or oral) and produces as output an initial version of the planning artifacts, a
requirements specification, a cost estimate report, and a
size estimate report. Additionally, an engineering notebook for the project is created and initialized based on the
activity schedule. At this phase, the project activity schedule and the project plan summary forms only contain planning information such as estimated total size, the project
development duration, and defects injected and removed.
Further descriptions of the project activity schedule, the
project plan summary, and the engineering notebook are
discussed in Section 3.2.

4 of 8

FIGURE 2.

Planning Process Component

As shown in Figure 2 on page 4, the Planning Process
Component is composed of its children process components: Identify Requirement, Perform Size Estimation,
Perform Cost Estimation, and Construct Plan. Each child
process component is defined to perform a specific task to
help planning the software activity and laying the groundwork for analysis. For instance, the Identify Requirement
Process Component generates the requirement specification (SRS) given the customer requirements (Cust Req).
Figure 2 on page 4 shows the OPC definition tool‚Äôs graphical depiction of the Planning Process. The model includes
nodes for Processes (process components that have subcomponents), Activities (process components without
subs), Roles and Products. Directed edges depict relationships such as can_perform, is_input_to, has_output and
has_sub. For example, the Requirement Specification
(Product) is_input_to Perform Cost Estimation (Activity),
and the Identify Requirement (Activity) has_output which
is the Requirement Specification.
3.1.2 Personal Software Activities
In the PSP, the planning and postmortem activities depend
on a personal software process that includes the phases,
planning, design, code, code review, compile, test, and
postmortem. In our application of the PSP to group
projects, we provide the capability to replace design, code,
code review, compile and test with other activities. Our
approach is to provide the background for the planning
and postmortem phases as applied to any software related
activities. In a group project, an individual engineer may

Tracking Personal Processes in Group Projects

Applying PSP to Group Projects

not be involved in coding, compiling and testing, but
may instead work on design and design reviews, or may
instead be a test engineer whose involvement does not
go beyond planning, developing, executing and reporting on tests. Our assumption is that the analysis techniques that consider resources (labor, primarily),
product measures and quality assessment all equally
apply to any other software related activities, whether
directly developing code or not. Process improvement
should be a center of focus for all participants in a software process.
At ASU, we have been using this approach to Integrating Personal Processes for group software projects in a
classroom setting and for group independent study
projects. Thus far, uses are for small applications in
which most project members get involved with all of the
life-cycle activities.
The primary challenge in generalizing the approach to
large group efforts has to do with product and quality
measures. PSP relies on Source Lines of Code as the
basis for product measures. Software defect management is the basis for quality, planning and process
improvement. Engineers using PSP, record defects by
type, phase injected and phase removed. PSP uses yield
(percentage of defects removed before compiling),
appraisal cost of quality and failure cost of quality as the
primary input for quality management and process
improvement. These are a good starting point for the
practicing software engineer, however, one must define
product measures and defects in a manner appropriate to
the activity. For example, a test engineer may use test
cases generated as the primary product measure. For
example, test cases may be defined to be triples (input
condition, action, expected result) independent of how
the test case is realized in performing tests. Defect types
for a test engineer may include: unsatisfied test requirement, and resulting software defects for which there
existed a test case.

3.1.3 The Postmortem Process
The Postmortem Process Component defines a process
for analyzing the performance (postmortem analysis) of
a completed project. Postmortem analysis gathers product measures, performs actual resource usage analysis,
performs actual defect analysis, and performs summary
quality analysis.

Tracking Personal Processes in Group Projects

We have used OPC to depict Postmortem. The process
is assigned to the process engineer and accomplishes its
objective of producing the project plan summary by
using the initialized project activity schedule, the project
defect log, and the initialized project plan summary as
input products. Unlike the Planning Process Component, Postmortem does not use children process components to accomplish its goal.

3.2

Automated Support for Process Artifacts

The Integrated Personal Process uses four artifacts: the
Engineering Notebook, the Project Activity Schedule &
Log, the Project Defect Log, and the Project Plan Summary. We have implemented each artifact as a standalone application. When using the worklist handler of
OPC, enacting one of the Integrated Personal Process
Planning or Postmortem activities may cause the invocation of one or all of these applications according to the
process input and output specifications.
All four artifacts use a single repository interface to
store and manipulate data. The interface is implemented
in Java, using synchronization to support multiple concurrent access. Highlights of these artifacts are detailed
in the following sections.
3.2.1 The Engineering Notebook
The Engineering Notebook is an application which
implements some concept of the Personal Software Process Engineering Notebook. The Engineering Notebook
objective is to create an engineering notebook that
tracks a software engineers daily time usage. More specifically, as shown in Figure 3 on page 6, the Engineering Notebook allows a software engineer to define and
record, for a given project, its activities, the time spent
on the activities, and product lists of the activities.
An activity is a unit of work which takes an engineers
time (e.g. interruptions, coding, breaks, lunch, designing, etc.); it is any work performed by a software engineer. The time spent on each activity is recorded in an
increment of hours; for instance, a job that takes 15 minutes could be recorded as 0.25 hours, but our usage generally limits granularity to one tenth of an hour (6
minutes). The product list entry allows the engineer to
list products produced by the activity. The initial engineering notebook is derived from infromation in the
Project Activity Schedule & Log.

5 of 8

Applying PSP to Group Projects

to allow add-on functions. For example, the user can add
the tools for estimation or a LOC Counter. The add-on
tools are specified using MIME types.
3.2.3 Defect Log

FIGURE 3.

Engineering Notebook Main Window

Modification to the times in the engineering notebook
causes the transfer of the times spent and the product list
to the Project Activity Schedule & Log.

The Defect Log (DL) is an application which automates
Defect Recording [15,16] to aid in tracking defects
injected and removed. The defect data are stored in the
defect log, which are used as input to generating the plan
summary (Section 3.2.4) in postmortem analysis. The
Defect Log is realized as a tabular application where the
rows represent the defects and their information and the
columns are classifications of the defects. As shown in
Figure 4 on page 6, the DL allows its user to specify the
date, the defect type, the injected phase, the removed
phase, the fixed time, and a description.

3.2.2 Activity Schedule & Log
The Activity Schedule & Log (ASL) is an application,
which aids in developing project plans. The ASL application allows the project planner to identify the activities,
phases, agents, and times for the activity. When the user
completes the activity specifications, ASL places the
schedule in a persistent repository.
An activity is a task of the project. A project may have a
set of activities (process components) representing the
work of all group members assigned activities on the
project. Similarly, all engineers working on a project will
have their own activity schedules, which reflect the lowerlevel activities necessary to complete their input to the
group. The lower-level activities are subject to analysis
and improvement as defined above.
A project planner may also have an Activity Schedule and
Log to coordinate the activities and products of a group of
engineers. We envision that the project planner can use the
Process Broker [27] (which is currently under development), to determine the kind of activities that the project
may need and to check for the availability of those process
components. To do this, the project planner first specifies
the characteristics of the current project to the Process
Broker. The Process Broker uses its locating and matching
semantic engine and its repository of process components
to determine the projects that best fit the specified criteria.
For each activity, the project planner estimates a begin and
end time, the development duration, the project size, and
the output products. These are estimated values, thus, the
project planner can use experience to determine the values, some tools, or historical project data. OPC is designed

6 of 8

FIGURE 4.

Project Defect Log Main Window

The date that the defect was discovered and the defect
description can be anything that the user enters. The DL
default defect types are: Documentation, Syntax, Build/
Package, Assignment, Interface, Checking, Data, Function, System, and Environment. These can be modified to
allow application of planning and postmortem to any software activity. Additionally, the DL also provides default
phases including: Planning, Design, Code, Review, Compile, Test, and Postmortem. Analogous to the defect types,
these can be changed to accommodate the activity. Finally,
the time required to correct the defect is recorded in hours
and tenths.
3.2.4 Project Plan Summary
The Plan Summary (PS) [15,16] is an application, to aid in
planning and tracking a software activity. The plan summary is initialized in the planning activity and is com-

Tracking Personal Processes in Group Projects

Current and Future Work

pleted in postmortem. In our implementation,
information in the plan summary is derived from the
Activity Schedule & Log. The plan summary can be
saved and named so that an engineer who participates in
several software activities (reviews, testing, and coding,
for example) can track data specific to the activity.

4.0 Current and Future Work
OPC provides an initial set of tools for defining and
enacting process components. The underlying implementation of OPC provides the framework for wrapping
various process tools for interoperability. We have
achieved initial wrappings of two products, and hope to
soon demonstrate interoperability between these products in the near future. Thus, a process component can
be defined in terms of sub components each under the
direction of a different enactment engine.
We have used our integrated personal software process
approach in classroom projects and in group independent studies. The tools described in this paper will be
introduced to these projects beginning in the Fall semester 1998. Enactment using OPC is controlled by a
worklist handler tool, which connects to a repository of
process components. Process components are all represented using Java objects. Until the tool wrappers are
fully functional, enactment involves launching an application associated with the input and output products as
specified with MIME types.
A few important distinctions differentiate our approach
to integrating personal processes. Engineers are not
asked to carry out a defined process that they themselves have not developed. Engineers are motivated to
use process improvement techniques, since they directly
and solely apply to their own activities. Product and
defect measures are defined by the engineer and thus
problems of consistency do not arise. Engineer define
their own personal process for the software activities
they perform. These may defined or applied from definitions they obtain from other engineers.

5.0 References
[1.]

[2.]

Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing
Evolving Knowledge in Software Process. Proceedings of the 2nd EWSPT ‚Äò92, Trondheim,
Norway. September, 1992.
Arizona State University, Open Process Component Toolkit, (Computer Science and Engineering Department, YFPPG Group, http://

Tracking Personal Processes in Group Projects

www.eas.asu.edu/~yfppg/
[3.]

Armenise, P., Bandinelli, S. Ghezzi, C., and
Morzenti, A. A Survey and Assessment of Software Process Representation Formalisms. International Journal of Software Engineering
and Knowledge Engineering, vol. 3, no. 3, pp.
401-426. 1993.

[4.]

Ben-Shaul, I. and Kaiser, G. An Interoperability
Model for Process-Centered Software Engineering Environments and its Implementation
in Oz. Technical Report CUCS-034-95, Computer Science Department, Columbia U. 1995.

[5.]

L. Carter, ECEN 4553 PSP Database Tool,
(University of Colorado at Boulder, Department of Electrical & Computer Engineering,
ece-www.colorado.edu/~ecen4553/Reference/
psp/examples.html).

[6.]

A. Christie, et al. A Study into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow and
Process Automation in Information Systems,
Athens, GA, May, 1996.

[7.]

F. Coallier, ‚ÄúHow ISO 9001 Fits into the Software World‚Äù, (IEEE Software, January 1994,
pp. 98-100).

[8.]

Conradi, R., et al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software
Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp.
33-70. John Wiley. 1994.

[9.]

J.C. Derniame, et al. ‚ÄúLife-Cycle Process Support in PCIS, Or It Is Time to Think about Software Process Formalisms Standardization‚Äù, in
Proc. of the PCTE‚Äô94 Conf. PCTE Technical
Journal No.2, PIMB Assn, November 1994.

[10.]

J.C. Derniame, and Gruhn, V. Development of
Process-Centered IPSEs in the ALF Project.
Journal of Systems Integration, vol. 4, pp. 127150. 1994.

[11.]

East Tennessee State University, Personal Software Process Studio, (East Tennessee State
University, Computer and Information Science,
www-cs.etsu.edu/softeng/psp/dlpsps.html).

[12.]

W. Emmerich, and Gruhn, V. FUNSOFT nets:
A Petri-net Based Software Process Modeling
Language. Proceedings of the 6th International
Workshop on Software Specification and Design, Como, Italy. September 1991.

[13.]

K. Gary, ‚ÄúProcess Interoperability with Open
Process Components‚Äù, Arizona State University, Computer Science and Engineering Department, Ph.D. Dissertation, expected August
1998.

7 of 8

References

[14.]

K. Gary, T. Lindquist, L. Sauer, and H. Koehnemann, ‚ÄúAutomated Process Support for Organizational and Personal Processes‚Äù, (Proceedings of
the International Conference on Supporting
Group Work (GROUP ‚Äò97), the Integration Challenge, Phoenix, Arizona, USA, 16-19 Nov 1997).

[15.]

W. S. Humphrey, Introduction to the Personal
Software Process (Reading, MA: Addison-Wesley, 1997).

[16.]

W. S. Humphrey, A Discipline for Software Engineering (Reading, MA: Addison-Wesley, 1995).

[17.]

W. S. Humphrey, ‚ÄúThe Personal Process in Software Engineering‚Äù, (Software Process Newsletter, Technical Council on Software Engineering,
IEEE Computer Society, Vol. 13, No. 1, September 1994, pp. 1-3, http://www.sei.cmu.edu/products/publications/95.reports/95.ar.psp.swe.html).

[18.]

The US-France Technology Research and Development Project, PCIS2 Architecture Specification
Version 1.0, (Lindquist, TE editor) SPAWAR
Systems Command, San Diego CA, January
1998.

[19.]

T. Lindquist, ‚ÄúA Toolset Supporting Distributed
Process Components‚Äù, (Arizona State University,
Computer Science & Engineering Department,
Technical Report, TR-97-034, 1997).

[20.]

T. Lindquist, and J. Derniame, ‚ÄúTowards Distributed and Composable Process Components‚Äù,
(Proceedings of the European Workshop on Software Process Technology, June 1997).

[21.]

Melo, W.L. and Belkhatir, N. TEMPO: A Support
for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots
(Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland.
1994.

[22.]

Mark C. Paulk, ‚ÄúHow ISO 9001 Compares with
the CMM‚Äù, (IEEE Software, January 1993, pp.
74-83).

[23.]

Mark C. Paulk, Bill Curtis, and Mary Beth Chrissis, ‚ÄúCapability Maturity Model, Version 1.1‚Äù,
(IEEE Software, July 1993, pp. 18-27).

[24.]

Mark C. Paulk et al., ‚ÄúCapability Maturity Model
for Software, Version 1.1‚Äù, (Technical Report,
CMU/SEI-93-TR-24, Software Engineering Institute, 1993).

[25.]

Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on
Software Engineering, pp. 262-279. May, 1992.

[26.]

W. Royce, ‚ÄúManaging the Development of Large
Software Systems: Concepts and Techniques‚Äù,
(WESCON Technical Papers, Vol. 14, Los Ana-

8 of 8

gles, WESCON, August 1970).
[27.]

L. Sauer, ‚ÄúBrokering of Process Components‚Äù,
(Arizona State University, Computer Science &
Engineering Department, Ph.D. Dissertation Proposal, December 1997).

[28.]

Software Engineering Institute, Personal Software Process (PSP), (SEI Technology, http://
www.sei.cmu.edu/technology/psp/).

[29.]

Software Engineering Institute, A Specification
for Automated Support for the PSP, (SEI Technology, http://www.sei.cmu.edu/technology/pspAuto/indexh.html).

[30.]

The Workflow Management Coalition. The Reference Model. WfMC Document Number TC001003, January 1995.

Tracking Personal Processes in Group Projects

CANONICAL CORRELATION ANALYSIS, APPROXIMATE
COVARIANCE EXTENSION, AND IDENTIFICATION OF
STATIONARY TIME SERIES*
ANDERS LINDQUIST‚Ä† AND GIORGIO PICCI‚Ä°

Abstract. In this paper we analyze a class of state-space identiÔ¨Åcation algorithms
for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called ‚Äúsubspace methods‚Äù
can be described as covariance estimation followed by stochastic realization. The
methods oÔ¨Äer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identiÔ¨Åcation into the solution of a Riccati
equation but introduce at the same time some nontrivial mathematical problems
related to positivity. The reason for this is that an essential part of the problem
is equivalent to the well-known rational covariance extension problem. Therefore
the usual deterministic arguments based on factorization of a Hankel matrix are
not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identiÔ¨Åcation
procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not
explicitly reported.
In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We
review the basic ideas of stochastic realization theory in the context of identiÔ¨Åcation, discuss the concept of stochastic balancing and of stochastic model reduction
by principal subsystem truncation. The model reduction method of Desai and Pal,
based on truncated balanced stochastic realizations, is partially justiÔ¨Åed, showing
that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing
conditions under which the ‚Äùsubspace identiÔ¨Åcation‚Äù methods produce bona Ô¨Åde
stochastic systems.

1. Introduction
Recently there has been a renewed interest in state-space identiÔ¨Åcation algorithms
for time series based on a two steps procedure which in principle can be described
as estimation of a rational covariance model from observed data followed by stochastic realization. The method oÔ¨Äers the major advantage of converting the nonlinear
parameter estimation phase which is necessary in traditional ARMA models identiÔ¨Åcation into a partial realization problem, involving a Hankel matrix of estimated
‚àó This research was supported in part by grants from TFR, the GoÃàran Gustafsson Foundation,
the SCIENCE project ‚ÄúSystem IdentiÔ¨Åcation‚Äù and LADSEB-CNR.
‚Ä† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
‚Ä° Dipartimento di Elettronica e Informatica, Universita‚Äô di Padova, 35131 Padova, Italy
1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which eÔ¨Écient numerical solution techniques are available. In this framework
we can naturally accommodate multivariate processes and there are indications that
the algorithms may work also with data containing purely deterministic components
(van Overschee and De Moor, 1993). A drawback, however, to be emphasized in
this paper, is that, unlike, say, least-squares identiÔ¨Åcation of ARMA models, these
methods do not work for arbitrary data.
This type of procedure was apparently Ô¨Årst advocated by Faurre (1969); see also
Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work,
based on canonical correlation analysis (Akaike, 1975) (or some other singular-value
decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki
(1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor,
1993). Numerical experience shows that the computation time needed to get the Ô¨Ånal model parameters estimates compares very favorably with traditional iterative
prediction error methods for ARMA models.
On the other hand there is a price to be paid for this simpliÔ¨Åcation. These methods
introduce some nontrivial mathematical problems related to positivity. The reason
for this is that an essential part of the problem is equivalent to the well-known rational
covariance extension problem. Therefore the usual deterministic realization arguments
based on factorization of a Hankel matrix are not valid for generic data, something
that is habitually overlooked in the literature. Note that positivity is the natural
condition insuring solvability of the Riccati equation required to compute state-space
models of the signal from the covariance estimates.
Central in the procedures described above is the following classical problem of
identiÔ¨Åcation of a covariance sequence. Let
{Œõ0 , Œõ1 , . . . , ŒõŒΩ }

(1.1)

be a Ô¨Ånite set of sample m √ó m covariance matrices estimated in some unspeciÔ¨Åed
way from a certain m-dimensional sequence of observations
{y0 , y1 , y2 , . . . yT },

(1.2)

and consider the problem of Ô¨Ånding a minimal1 triplet of matrices (A, C, CÃÑ) such that
CAk‚àí1 CÃÑ  = Œõk

k = 1, 2, . . . , ŒΩ

(1.3)

and such that the inÔ¨Ånite sequence
{Œõ0 , Œõ1 , Œõ2 , . . . },

(1.4)

obtained from (1.1) by setting Œõk := CAk‚àí1 CÃÑ  for k = ŒΩ + 1, ŒΩ + 2, . . . , is a bona Ô¨Åde
covariance sequence.
In the literature the last condition is generally ignored. The remaining problem of
Ô¨Ånding a minimal triplet (A, C, CÃÑ) satisfying (1.3) is called the minimal partial realization problem. The triplet (A, C, CÃÑ) is usually computed by minimal factorization
1

Here (A, C, CÃÑ) is minimal if (A, C) is completely observable and (A, CÃÑ  ) is completely reachable.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

of a block Hankel matrix corresponding to the data (1.1) as follows:
Ô£Æ
Ô£π Ô£Æ
Ô£πÔ£Æ
Ô£π
CÃÑ
Œõ1 Œõ2
C
Œõ3 ¬∑ ¬∑ ¬∑
Œõj
Ô£ØŒõ2 Œõ3
Œõ4 ¬∑ ¬∑ ¬∑ Œõj+1 Ô£∫ Ô£Ø CA Ô£∫ Ô£Ø CÃÑA Ô£∫
Ô£∫ ,
H=Ô£Ø
= Ô£Ø . Ô£∫Ô£Ø
..
.. Ô£∫
..
..
..
Ô£∞ ...
Ô£ª
.
.
. Ô£ª Ô£∞ .. Ô£ª Ô£∞
.
.
Œõi Œõi+1 Œõi+2 ¬∑ ¬∑ ¬∑ Œõi+j‚àí1
CAi‚àí1
CÃÑ(A )j‚àí1

3

(1.5)

where i + j ‚àí 1 = ŒΩ and the Hankel matrix H is chosen as close to square as possible
by taking |i ‚àí j| ‚â§ 1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j) such
that i + j ‚àí 1 = ŒΩ, and hence the minimal factorization must be made for a choice
of (i, j) in which the Hankel matrix (1.5) has maximal rank. The inÔ¨Ånite sequence
{Œõ0 , Œõ1 , Œõ2 , . . . } obtained in this way by setting Œõk := CAk‚àí1 CÃÑ  for k = ŒΩ +1, ŒΩ +2, . . .
is called a minimal rational extension of the Ô¨Ånite sequence (1.1) and is in general not
a covariance sequence. The dimension r of a minimal rational extension is called the
(algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to
the McMillan degree of the m √ó m rational matrix
1
(1.6)
Z(z) = C(zI ‚àí A)‚àí1 CÃÑ  + Œõ0 ,
2
and the elements of the inÔ¨Ånite sequence (1.4) are the coeÔ¨Écients of the Laurent
expansion
1
Z(z) = Œõ0 + Œõ1 z ‚àí1 + Œõ2 z ‚àí2 + . . .
2

(1.7)

about z = ‚àû.
The underlying identiÔ¨Åcation problem is however a great deal more complicated
than the classical partial realization problem. In fact, the requirement that (1.4) be a
bona Ô¨Åde covariance sequence amounts to (1.4) being a positive sequence in the sense
that, for every t ‚àà Z+ , the block Toeplitz matrices Tt ,
Ô£Æ
Ô£π
Œõ2 ¬∑ ¬∑ ¬∑ Œõt
Œõ0 Œõ1
Ô£ØŒõ1 Œõ0
Œõ1 ¬∑ ¬∑ ¬∑ Œõt‚àí1 Ô£∫
Tt = Ô£Ø
,
(1.8)
..
.. Ô£∫
.
.
...
Ô£∞ ..
..
.
. Ô£ª
Œõt Œõt‚àí1 Œõt‚àí2 ¬∑ ¬∑ ¬∑

Œõ0

formed from the inÔ¨Ånite sequence (1.4), be positive deÔ¨Ånite or, equivalently, that the
matrix function
Œ¶(z) := Z(z) + Z(1/z)

(1.9)

be positive semideÔ¨Ånite on the unit circle, i.e.
Œ¶(eiŒ∏ ) ‚â• 0 Œ∏ ‚àà [0, 2œÄ).

(1.10)

This property is equivalent to Œ¶ being a spectral density matrix. In fact, it will be the
spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial
covariance sequence unless TŒΩ > 0, but this is not enough.
From the point of view of identiÔ¨Åcation there seem to be two possible routes to
determine a model (A, C, CÃÑ) from the Ô¨Ånite covariance sequence (1.1). One that
has been proposed in the literature is do minimal factorization (1.5) of a Ô¨Ånite block
Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown
in this paper, there is no a priori guarantee that this method will yield a positive
extension. This fact has nothing to do with sample variability (random Ô¨Çuctuations)
of the covariance estimates (1.1), and to emphasize this point we initially assume
that all strings of data (1.2) are inÔ¨Ånitely long. A theoretically sounder identiÔ¨Åcation
method, which will not be considered in this paper, could instead be to do positive
extension Ô¨Årst and then to use a stochastic model reduction procedure on the triplet
(A, C, CÃÑ) of the positive extended sequence.
The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this
diÔ¨Éculty into consideration have been pointed out by the authors of this paper at
many scientiÔ¨Åc meetings in the last ten years. This has had no apparent eÔ¨Äect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in
which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identiÔ¨Åcation procedure of Aoki (1990) and
demonstrate that there is a hidden, and not easily tested, assumption without which
the procedure will not be guaranteed to succeed. The punch line is that none of the
subspace identiÔ¨Åcation methods under consideration can be expected to always work
for generic data but that some not entirely natural conditions on the data are needed.
The analysis of the basic theoretical issues behind subspace identiÔ¨Åcation is carried
out in the geometric framework of stochastic realization theory; see, e.g., Lindquist
and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this
theory and adapt them to the problem of identiÔ¨Åcation. To this end, we Ô¨Årst discuss
an idealized situation in which the time series (1.2) is inÔ¨Ånitely long i.e. T = ‚àû, and
the available covariance data are given by the ergodic limit
1 

yt+k yt+j
= Œõk‚àíj
T ‚Üí‚àû T + 1
t=0
T

lim

(1.11)

for all k and j. Then the sample estimates in the sequence (1.1) are bona Ô¨Åde covariance matrices and the Toeplitz matrix TŒΩ formed from the data will be positive
deÔ¨Ånite and symmetric. We introduce a Hilbert space of observed (inÔ¨Ånite) strings of
data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic
system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical
problem of identiÔ¨Åcation. In this way we establish a correspondence which turns operations on random quantities deÔ¨Åned on abstract probability spaces into prototypes of
statistical algorithms involving computations based on the observed data. Canonical
correlations and balanced stochastic realizations are then analyzed in this setting in
Section 4, and the basic concepts and principles used in the subspace identiÔ¨Åcation
methods, as well as in the model reduction procedures of Desai and Pal, are translated
into the more natural context of geometric stochastic realization theory.
Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such
objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact inÔ¨Ånite covariance sequence
(1.4). Consequently, the diÔ¨Écult question of positivity is not an issue here. Nor is
it the Ô¨Ånite sample size per se which is the problem, but the fact that only a Ô¨Ånite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is Ô¨Ånite.
Therefore we separate these issues by discussing stochastic realization theory from
Ô¨Ånite covariance data in Section 5 and subspace identiÔ¨Åcation in Section 6. In this
framework we show that the method of van Overschee and De Moor (1993) is valid
under some rather stringent assumptions. We stress that we are only concerned with
identiÔ¨Åcation procedures for state space modeling of time series. ‚ÄúSubspace identiÔ¨Åcation‚Äù methods for deterministic systems with measurable inputs or for spectral
factors do not involve positivity, but stability may still be a problem. However, the
algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part,
so the problem of positivity arises here too.
Another idea behind the subspace identiÔ¨Åcation methods considered in this paper is
to disregard modes corresponding to ‚Äúsmall‚Äù canonical correlation coeÔ¨Écients. This
is called balanced truncation and is in fact a stochastic model reduction procedure.
In all such procedures there must be a guarantee that the reduced-degree matrix
function (1.6) is positive real, and therefore the preservation of positivity in such
reductions is a main concern of this paper. Section 7 is devoted to such issues. The
model reduction procedure of Desai and Pal (1982) was never theoretically justiÔ¨Åed
in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 .
Here we shall demonstrate that this reduction procedure produces a positive real, but
not in general balanced, reduced model structure. In fact, the singular values of the
truncated system are usually not equal to the r Ô¨Årst singular values of the original
system.
It is an interesting fact that the procedure of Desai and Pal does produce balanced
truncations for continuous-time stochastic systems. A partial result in this direction
was given by Harshavardana, Jonckheere and Silverman (1984), who showed that
the truncated function is positive real and conjectured that it is balanced. We shall
demonstrate that it is indeed balanced, a result that is actually already contained in
the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time
depends on the fact that the spectral factors of the truncated approximate spectrum
behave diÔ¨Äerently than in continuous time. While in continuous time the realizations
of the reduced spectral factors are proper subsystems, obtained by partitioning the
matrices of the realizations of the factors of Œ¶, this is not the case in discrete time,
contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced
truncation procedure is available in discrete time, but the systems matrices are no
longer submatrices of those of the original system, and therefore it is not equivalent
to the truncation procedure used in subspace identiÔ¨Åcation.
Several of the results of this paper have previously been announced in Lindquist
and Picci (1994a)3 and in Lindquist and Picci (1994b).

2

In Desai et al. (1985) a diÔ¨Äerent model reduction procedure, which is not relevant to subspace
identiÔ¨Åcation, is considered, namely ‚Äúdeterministic‚Äù model reduction of the minimum phase spectral
factors.
3
We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some
erroneous statements, was accidentally published in place of the paper Ô¨Ånally submitted for publication. The correct version can be obtained from the authors.

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances
The solution to the minimal partial realization problem , i.e., the problem to Ô¨Ånd
the triplet (A, C, CÃÑ) satisfying (1.1) is in general not unique. This lack of uniqueness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and
Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1)
equals that of
{Œõ0 , Œõ1 , . . . , ŒõŒΩ‚àí1 }
so that we can use a Hankel matrix (1.5) based
allowing us to deÔ¨Åne the shifted Hankel matrix
Ô£Æ
Œõ3
Œõ4
Œõ2
Ô£Ø Œõ3
Œõ4
Œõ5
œÉ(H) = Ô£Ø
..
.
.
Ô£∞ ..
..
.
Œõi+1 Œõi+2 Œõi+3

(2.1)

on this data, i.e., with i + j = ŒΩ,
Ô£π
¬∑ ¬∑ ¬∑ Œõj+1
¬∑ ¬∑ ¬∑ Œõj+2 Ô£∫
.. Ô£∫
...
. Ô£ª
¬∑ ¬∑ ¬∑ ŒõŒΩ

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) produces a minimal solution (A, C, CÃÑ) which is unique up to a similarity transformation.
As Ô¨Årst pointed out by Zeiger and McEwen (1974), the minimal factorization on
which the Ho-Kalman procedure is based may be performed by singular-value decomposition, thereby Ô¨Åxing (A, C, CÃÑ) uniquely; see also Kung (1978). In fact, the Hankel
matrix H may be factored as
H = U Œ£V 

U  U = I = V  V,

(2.3)

where Œ£ is the square n √ó n diagonal matrix of the nonzero singular values taken in
decreasing order. Setting ‚Ñ¶ := U Œ£1/2 and ‚Ñ¶ÃÑ := V Œ£1/2 this leads to a factorization
H = ‚Ñ¶‚Ñ¶ÃÑ

‚Ñ¶ ‚Ñ¶ = Œ£ = ‚Ñ¶ÃÑ ‚Ñ¶ÃÑ

(2.4)

of the type (1.5). Then a minimal realization (A, C, CÃÑ) is obtained by solving
‚Ñ¶A‚Ñ¶ÃÑ = œÉ(H),

C ‚Ñ¶ÃÑ = œÅ1 (H) and CÃÑ‚Ñ¶ = œÅ1 (H  ),

where œÉ(H) is the shifted Hankel matrix (2.2) and œÅ1 (H) is the Ô¨Årst block row of H.
It follows that the triplet (A, C, CÃÑ) must be given by
A = Œ£‚àí1/2 U  œÉ(H)V Œ£‚àí1/2 ,
C = œÅ1 (H)V Œ£‚àí1/2 ,
CÃÑ = œÅ1 (H  )U Œ£‚àí1/2 ,

(2.5a)
(2.5b)
(2.5c)

a form to which we refer as Ô¨Ånite-interval balanced, since it is balanced in the sense
that ‚Ñ¶ ‚Ñ¶ and ‚Ñ¶ÃÑ ‚Ñ¶ÃÑ are both equal to Œ£, and that
Ô£π
Ô£Æ
Ô£π
Ô£Æ
CÃÑ
C
Ô£Ø CÃÑA Ô£∫
Ô£Ø CA Ô£∫
Ô£∫
Ô£Ø
Ô£∫.
‚Ñ¶ÃÑ
=
(2.6)
‚Ñ¶=Ô£Ø
..
Ô£ª
Ô£∞
Ô£∞ ... Ô£ª
.
CAi‚àí1

CÃÑ(A )j‚àí1

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identiÔ¨Åcation of time
series. The problem with such a strategy is that this algorithm is a deterministic
realization procedure and hence does not a priori insure that (1.6) is positive real, or
even stable for that matter, even if the Toeplitz matrix TŒΩ is positive deÔ¨Ånite. In fact,
it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space
of covariance data (1.1) for which A is not stable, and a fortiori the same holds for
positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in
Aoki (1990) is based on the following hidden assumption which is not entirely natural.
Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H.
Therefore, not only must we know that there exists an underlying Ô¨Ånite-dimensional
system, but we must also have some upper bound for its dimension. A conservative
upper bound which will always suÔ¨Éce is [ ŒΩ2 ].
Is this assumption natural? If the covariance data are really generated exactly from
a ‚Äútrue‚Äù stochastic system and there is a reliable estimate of its order which is no
more than half of the length of the covariance sequence, then the assumption will hold.
However, and this is an important point of this paper, one cannot expect Assumption
2.1 to hold for an arbitrary covariance sequence (1.1).
To clarify this point, let us agree to call {Œõ0 , Œõ1 , Œõ2 , . . . } a minimal rational extension of {Œõ0 , Œõ1 , . . . , ŒõŒΩ } if the rational function (1.7) has minimal degree. By
deÔ¨Ånition this is the algebraic degree of {Œõ0 , Œõ1 , . . . , ŒõŒΩ }. A rational extension is
called positive if, for every ¬µ > ŒΩ, the block Toeplitz matrices T¬µ formed from the
corresponding inÔ¨Ånite sequence (1.4) are positive deÔ¨Ånite. An extension with this
property is called a positive rational extension. It is well known that the extension
{Œõ0 , Œõ1 , Œõ2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function
Z(z) is analytic in the closed unit disc and the matrix function
Œ¶(z) = Z(z) + Z(1/z)

(2.7)

is nonnegative deÔ¨Ånite on the unit circle, making Œ¶ a spectral density matrix. A
minimal positive rational extension of the Ô¨Ånite sequence (1.1) is one for which the
dimension of the triplet (A, C, CÃÑ) in (1.6) is as small as possible.
DeÔ¨Ånition 2.2. The positive degree p of the Ô¨Ånite covariance sequence {Œõ0 , Œõ1 , . . . , ŒõŒΩ }
is the dimension of any minimal positive extension.
A well-known example of a positive extension is the maximum entropy extension
(Whittle, 1963) corresponding to the spectral density Œ¶(z) := W (z)W (1/z) , where
the spectral factor W (z) is (modulo a multiplicative constant matrix) the inverse of the
Levinson-SzegoÃà matrix polynomial of order ŒΩ corresponding to the Ô¨Ånite covariance
sequence (1.1). Since the rational function W (z) generically has the McMillan degree
equal to mŒΩ, it follows from spectral factorization theory (Anderson, 1958) that Z(z)
has also degree mŒΩ. Consequently, the positive degree p is bounded from below by
the algebraic degree r and from above by mŒΩ.
As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use
algebraic rather than positive extensions, usually computed by minimal factorization
a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1
may also be formulated in the following way.


Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree.
This assumption prescribes a property of the covariance sequence (1.1) which is not
generic. We can illustrate this point by considering the rational extension problem
for a Ô¨Ånite scalar covariance sequence (1.1). The positive degree p lies between the
algebraic degree r and ŒΩ. Note that neither the case p = ŒΩ nor the case p < ŒΩ
are ‚Äùrare events‚Äù, because there are open sets of covariance sequences (1.1) of both
categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each ¬µ
such that ŒΩ2 ‚â§ ¬µ ‚â§ ŒΩ there is an open set of covariance data in RŒΩ for which p = ¬µ.
If the upper limit p = ŒΩ is attained there are inÔ¨Ånitely many nonequivalent minimal
triplets (A, C, CÃÑ) providing a positive extension, one of which is the maximum entropy
extension. In fact, it can be shown that these ŒΩ-dimensional extensions form an
Euclidean space (Byrnes and Lindquist, 1989). This shows that the Ô¨Ånite data (1.1)
never contains enough information to establish a ‚Äùtrue‚Äù underlying system. A similar
statement can be made in the case when p < ŒΩ.
Example 2.3. Consider the case m = 1 and ŒΩ = 2, i.e., consider a scalar partial
covariance sequence {Œõ0 , Œõ1 , Œõ2 }. If Œõ1 = Œõ2 = 0, we have r = p = 0. Otherwise,
we always have r = 1, whereas the positive degree can be either one or two. In fact,
setting Œ≥0 := Œõ1 /Œõ0 and Œ≥1 := (Œõ21 + Œõ2 )/(1 ‚àí Œõ21 ), it can be shown (Georgiou, 1987;
also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1
if and only if
|Œ≥0 |
|Œ≥1 | <
1 + |Œ≥0 |
and p = 2 otherwise.
In fact, it is not hard to construct examples for which the gap between algebraic
and positive rank is arbitrarily large, as the following theorem shows.
Theorem 2.4. Let n ‚àà Z+ be Ô¨Åxed. Then for an arbitrarily large ŒΩ there is a stable
rational function Z(z) of degree n, such that the Toeplitz matrix TŒΩ formed as in ( 1.8)
from the coeÔ¨Écients of the Laurent expansion ( 1.7), is positive deÔ¨Ånite while TŒΩ+1 is
indeÔ¨Ånite.
Consequently, you cannot test the positivity of a rational extension of (1.1) by
checking a Ô¨Ånite Toeplitz matrix, however large is its dimension. The proof of Theorem
2.4 is given in Appendix A.
Let us now return to the identiÔ¨Åcation procedure of Aoki (1990). In practice the
rank of H will always be full, and to compute a partial realization of reasonable
dimension the basic idea is to partition Œ£ as


	
Œ£1 0
,
(2.8)
Œ£=
0 Œ£2

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in Œ£2 are smaller than those in Œ£1 , perhaps close to zero,
and then take Œ£2 = 0 so that H is approximated by
	


Œ£1 0 
(2.9)
V = U1 Œ£1 V1 .
H1 = U
0 0
The matrix H1 is a best approximation (given the rank) of H in (the induced) !2 ‚Äì
norm, but it is in general not Hankel and hence can not be used to determine a
reduced order system. Of course, one may instead use Hankel-norm approximation
(Adamjan, Arov and Krein, 1971), which produces another best approximation of
H in !2 -norm that is Hankel and has the same rank as H1 . However, if Œ£2 is ‚Äúvery
small‚Äù compared to Œ£1 , then H1 is close to H and hence approximately Hankel. For
this reason, Aoki‚Äôs procedure (Aoki, 1990) is based on the original data H and œÉ(H).
Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I, the
same type of calculation as above yields the reduced triplet (Ar , Cr , CÃÑr ) given by
‚àí1/2

Ar = Œ£1

‚àí1/2

U1 œÉ(H)V1 Œ£1
‚àí1/2

Cr = œÅ1 (H)V1 Œ£1

,

,

‚àí1/2

CÃÑr = œÅ1 (H  )U1 Œ£1

(2.10a)
(2.10b)

.

(2.10c)

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal
subsystem truncation in the sense that, if H is produced by a Ô¨Ånite-dimensional system
with (A, C, CÃÑ) having Ô¨Ånite-interval balanced form (2.5), we have
Ar = A11 ,
where



	
A11 A12
A=
A21 A22

Cr = C1 ,

CÃÑr = CÃÑ1 ,



C = C1 C2



CÃÑ = CÃÑ1 CÃÑ2 .

(2.11)

(2.12)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for œÉ(H),
œÅ1 (H) and œÅ1 (H  ) and inserting in (2.10).
However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that
1
(2.13)
Z1 (z) = C1 (zI ‚àí A11 )‚àí1 CÃÑ1 + Œõ0
2
is positive real, provided of course that Z, deÔ¨Åned by (1.6), is positive real. The
question of stability was answered in the aÔ¨Érmative in Pernebo and Silverman (1982)
and is addressed in Aoki (1990). The crucial question of positivity, however, is not
discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be
proven for a somewhat modiÔ¨Åed procedure described below.
In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall
consider a normalized Hankel matrix
‚àíT
HÃÇ = L‚àí1
+ HL‚àí ,

(2.14)

where L‚àí and L+ are lower triangular Cholesky factors of the Toeplitz matrices T‚àí
and T+ of (1.1) and the corresponding sequence of transposed covariances respectively;
see Section 4 below. This is also the Hankel matrix considered in van Overschee and
De Moor (1993). Taking the singular value decomposition of HÃÇ instead of H, the

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coeÔ¨Écients, i.e., the cosines of the
angles between the past and the future of the process y. The systems matrices can
be determined in a manner analogous to (2.5), but now
‚Ñ¶ T+‚àí1 ‚Ñ¶ = Œ£ÃÇ = ‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑ

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as
above. To see this, consider the singular value decomposition HÃÇ = UÃÇ Œ£ÃÇUÃÇ  so that H =
(L+ UÃÇ )Œ£ÃÇ(L‚àí VÃÇ ) . Since H = ‚Ñ¶‚Ñ¶ÃÑ and this factorization is unique modulo coordinate
transformation in state space, we may take ‚Ñ¶ = L+ UÃÇ Œ£ÃÇ1/2 and ‚Ñ¶ÃÑ = L‚àí VÃÇ Œ£ÃÇ1/2 . Then
(2.15) follows from UÃÇ  UÃÇ = I = VÃÇ  VÃÇ . As we shall see next, (2.15) corresponds to a
more natural type of balancing corresponding to a Hankel operator describing the
interface between the past and the future of the time series y.
3. Stochastic realization theory in the Hilbert space of a sample function
In this section we introduce a mathematical framework which is suitable for the identiÔ¨Åcation problem described above. We deÔ¨Åne a Hilbert space of observed (inÔ¨Ånite)
strings of data {yt }. This framework turns out to be isomorphic to that of geometric
stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985,
1991) also for the statistical problem of identiÔ¨Åcation. In this way we also establish a
correspondence which converts operations on random quantities deÔ¨Åned on abstract
probability spaces into prototypes of statistical algorithms involving computations
based on the observed data.
In identiÔ¨Åcation we have access only to a Ô¨Ånite string of data
{y0 , y1 , y2 , . . . , yT }.

(3.1)

Here T may be quite large but, of course, always Ô¨Ånite. To begin with, we shall,
however, consider the idealized situation that we are given a doubly inÔ¨Ånite sequence
of m-dimensional data
{. . . , y‚àí3 , y‚àí2 , y‚àí1 , y0 , y1 , y2 , y3 . . . }

(3.2)

together with a corresponding covariance sequence {Œõk }k‚â•0 , each matrix Œõk of the
sequence being computed from the data (3.2) by an ergodic limit of the type (1.11).
In Section 5 we then modify the theory to handle the situation of Ô¨Ånite data (3.1).
For each k ‚àà Z deÔ¨Åne the m √ó ‚àû matrix
y(t) := [yt , yt+1 , yt+2 , . . . ]

(3.3)

and consider the sequence y := {y(t)}t‚ààZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of
all Ô¨Ånite linear combinations

ak ‚àà Rm , tk ‚àà Z
ak y(tk );

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product deÔ¨Åned by linear
extension of the bilinear form
t0 +T
1 

a yt+k yt+j
b = a Œõk‚àíj b,
a y(k), b y(j) := lim
T ‚Üí‚àû T + 1
t=t




(3.4)

0

which clearly does not depend on t0 . This inner product is nondegenerate if the
Toeplitz matrix Tk , constructed from the covariance data {Œõ0 , Œõ1 , . . . , Œõk }, is a positive deÔ¨Ånite symmetric matrix for all k. Here we shall assume that the sequence
{Tk }k‚â•0 is actually coercive, i.e., Tk > cI for some c > 0 and all k ‚â• 0. (See Assumption 3.2 below for an alternative characterization.) We also deÔ¨Åne a shift operator U
on the family of semi-inÔ¨Ånite matrices (3.3), by setting
Ua y(t) = a y(t + 1)

t ‚àà Z,

a ‚àà Rm ,

deÔ¨Åning a linear map which is isometric with respect to the inner product (3.4) and
extendable by linearity to all of Y . In particular the sequence of matrices {y(k)}
corresponding to the time series y is propagated in time by the action of the operator
U, i.e.,
yi (t) = Ut yi (0),

i = 1, 2, . . . , m,

t ‚àà Z,

(3.5)

where yi denotes the i:th row component of y. Then, closing the vector space Y in
the inner product (3.4), we obtain a Hilbert space H(y) := cl Y . The shift operator
U is extended by continuity to all of H(y) and is a unitary operator there.
As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results
in the geometric theory of stochastic realization can be carried over to the present
framework by merely identifying the time series y with a stationary stochastic process
y. In particular, the subspaces H ‚àí and H + of H(y) generated by the elements (3.3)
for t < 0 and t ‚â• 0 respectively can be regarded as the past and future subspaces
of the stationary process y. For reasons of uniformity of notation the inner product
(3.4) will also be denoted
Œæ, Œ∑ = E{ŒæŒ∑},

(3.6)

as the frameworks are completely equivalent. Here we allow E{¬∑} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity
condition introduced above insures that ‚à©t‚ààZ Ut H ‚àí = 0 and ‚à©t‚ààZ Ut H + = 0, i.e., y is
a purely nondeterministic sequence.
As we have pointed out above, the subspace identiÔ¨Åcation methods of Aoki (1990)
and van Overschee and De Moor (1993) are based on the assumption that the available
data is generated by an underlying stochastic system of Ô¨Ånite dimension. More specifically, using the notations introduced above, we assume that the data are generated
by a linear system of the type

x(t + 1) = Ax(t) + Bw(t)
(3.7)
y(t) = Cx(t) + Dw(t)

12

ANDERS LINDQUIST AND GIORGIO PICCI

deÔ¨Åned for all t ‚àà Z, where w is some vector-valued normalized white noise time
series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability
matrix. Throughout this paper we shall 	assume
(without restriction) that (A, B, C)


B
is a minimal triplet and that the matrix
has linearly independent columns.
D
The system is assumed to be in statistical steady state so that the n-dimensional
state x and the m-dimensional output y are uniquely deÔ¨Åned by (3.7) as linear causal
functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and
y(s) will depend only on the diÔ¨Äerence t ‚àí s. We shall think of the system (3.7) as a
representation of the output time series y. The state and input variables x and w are
introduced in order to display the special structure of the dynamic model of y and
are by no means unique. Such a representation is called a state-space realization of y.
Remark 3.1. Despite the fact that the model (3.7) is deÔ¨Åned in terms of sample
sequences, all equalities must be understood in the sense of Hilbert space metric, just
as in the case of models based on random variables.
The number of state variables n is called the dimension of the realization. A
realization is minimal if there is no other realization of y of smaller dimension. In
this case the covariance matrix of the state vector,
(3.8)
P = E{x(t)x(t) }
	 

B
is positive deÔ¨Ånite. Moreover as the matrix
is taken with linearly independent
D
columns, the number of (scalar) white noise inputs p is also as small as possible.
Clearly, the covariance sequence {Œõ0 , Œõ1 , Œõ2 , . . . } of the output {y(t)} of a minimal
model (3.7) is a rational sequence of degree n, i.e., represented as

Œõk = CAk‚àí1 CÃÑ  k = 0, 1, 2, . . . where CÃÑ  = AP C  + BD
.
(3.9)
Œõ0 = CP C  + DD
In the following we shall need to assume that the corresponding spectral density Œ¶(z)
satisÔ¨Åes the following condition.
Assumption 3.2. The spectral density Œ¶ of the output process of the underlying
system (3.7) is coercive in the sense that
Œ¶(eiŒ∏ ) > 0 for all Œ∏ ‚àà [0, 2œÄ].

(3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent
sequences. Recall that a positive real function Z such that Œ¶(z) := Z(z) + Z(z ‚àí1 )
satisÔ¨Åes (3.10) is called strictly positive real.
Let H(w) be the Hilbert space generated by w, i.e. the closure of the linear space
spanned by the family {wi (t), i = 1 . . . p, t ‚àà Z} with respect to the metric induced
by the inner product Œæ, Œ∑ = E{Œæ Œ∑} where E{¬∑} is deÔ¨Åned by (3.6). Let H + and H ‚àí
be the subspaces of H(w) generated by the components of future {y(0), y(1), y(2) . . . }
and past outputs {y(‚àí1), y(‚àí2), y(‚àí3) . . . }, respectively.
4

This means that E{w(t)w(s) } = IŒ¥ts where Œ¥ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace
X := {a x(0) | a ‚àà Rn }

(3.11)

is invariant under coordinate changes of the type (A, B, C) ‚Üí (T AT ‚àí1 , T B, CT ‚àí1 )
and is a coordinate-free representation of the realization (3.7). Such an object is called
a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next deÔ¨Åne the
stationary Hankel operator of y, H : H + ‚Üí H ‚àí as
‚àí

H := E H |H +

(3.12)

‚àí

where E H Œª is the orthogonal projection of Œª onto H ‚àí . The splitting subspace
property of X is equivalent to the commutativity of the diagram
H

H + ‚àí‚Üí H ‚àí
O‚àó 
C
X
i.e. to the factorization
H = CO‚àó ,
+

(3.13)
‚àí

where the operators O := E H |X and C := E H |X are the observability respectively
constructibility operators relative to the splitting subspace X. It can be shown that
the splitting subspace X is minimal if and only if O and C are both injective. (See,
e.g., Lindquist and Picci (1991).)
The system (3.7) is a forward or causal realization of y in the sense that the subspace
+
H (w), generated by the future of w, is orthogonal to X and H ‚àí , i.e. to the present
state and past output. Corresponding to (3.7) there is another realization

xÃÑ(t ‚àí 1) = A xÃÑ(t) + BÃÑ wÃÑ(t ‚àí 1)
(3.14)
y(t ‚àí 1) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t ‚àí 1)
which is backward or anticausal in the sense that the subspace H ‚àí (wÃÑ), generated by
the past of wÃÑ, is orthogonal to X and H + . Like x(0), xÃÑ(0) is a basis in X, i.e.
X := {a xÃÑ(0) | a ‚àà Rn }.

(3.15)

In fact, xÃÑ(0) is the dual basis of x(0) in the sense that E{x(0)xÃÑ(0) } = I. Hence
PÃÑ = P ‚àí1

xÃÑ(0) = P ‚àí1 x(0).

(3.16)

The particular notations used in (3.7) and (3.14) reÔ¨Çect the special meaning of the
parameters (A, C, CÃÑ). Computing the covariance matrix of the output using the dual
realizations (3.7) and (3.14), it is in fact readily seen that (A, C, CÃÑ) is precisely a
triplet realizing the positive real part (1.6) of the spectral density matrix Œ¶(z) of the
time series y. There are inÔ¨Ånitely many minimal factorizations (3.13), one for each
Markovian splitting subspace, but the basis in each state space X can be chosen so
that the triplets (A, C, CÃÑ) are the same for each minimal X. This is called a uniform
choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward
predictor spaces
‚àí

X‚àí = E H H +

X+ = E H H ‚àí ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12)
and of its adjoint, respectively.
Fixing a uniform choice of bases, and thus the triplets (A, C, CÃÑ), the splitting
subspace X‚àí has the forward stochastic realization

x‚àí (t + 1) = Ax‚àí (t) + B‚àí w‚àí (t)
(3.18)
y(t) = Cx‚àí (t) + D‚àí w‚àí (t)
with state covariance P‚àí , and X+ has the backward realization

xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ wÃÑ+ (t ‚àí 1)
y(t ‚àí 1) = CÃÑ xÃÑ+ (t) + DÃÑ+ wÃÑ+ (t ‚àí 1)

(3.19)

with state covariance PÃÑ+ .
These two stochastic realizations will play an important role in what follows. In
fact, an important interpretation of these realizations is that
‚àí1
[y(t) ‚àí Cx‚àí (t)]
x‚àí (t + 1) = Ax‚àí (t) + B‚àí D‚àí

is the unique steady-state Kalman Ô¨Ålter of any minimal realization (3.7) of y in the
Ô¨Åxed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8)
corresponding to the forward counterpart of (3.19), i.e., P+ = (PÃÑ+ )‚àí1 , then
P ‚àí ‚â§ P ‚â§ P+

(3.20)

for the state covariance of any minimal realization (3.7).
In the same way
‚àí1
[y(t ‚àí 1) ‚àí C xÃÑ+ (t)]
xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ DÃÑ+

is the backward steady-state Kalman Ô¨Ålter of all minimal backward realizations (3.14),
and
PÃÑ+ ‚â§ PÃÑ ‚â§ PÃÑ‚àí
for an arbitrary backward minimal realization (3.14), where PÃÑ‚àí is the backward counterpart of P‚àí .
4. Canonical correlations and balanced stochastic realization
In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a Ô¨Ånite-dimensional realization
of the type (3.7). Equivalently, we study certain factorizations of the inÔ¨Ånite Hankel matrix of the corresponding inÔ¨Ånite covariance sequence {Œõ0 , Œõ1 , Œõ2 , . . . }. Some
portions of this section can be found in an equivalent but somewhat diÔ¨Äerent setting
in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and
set notations. This will be done in the geometric framework of Section 3, thereby
providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall
introduce orthonormal bases in H ‚àí and H + . To this end it will be useful to represent
past and future outputs as inÔ¨Ånite vectors in the form,
Ô£Æ
Ô£π
Ô£Æ
Ô£π
y(‚àí1)
y(0)
Ô£Øy(‚àí2)Ô£∫
Ô£Øy(1)Ô£∫
Ô£∫
Ô£Ø
Ô£∫
y
=
(4.1)
y‚àí = Ô£Ø
+
Ô£∞y(‚àí3)Ô£ª
Ô£∞y(2)Ô£ª
..
..
.
.
Let L‚àí and L+ be the lower triangular Cholesky factors of the inÔ¨Ånite block Toeplitz
matrices


} = L‚àí L‚àí
T+ := E{y+ y+
} = L+ L+
T‚àí := E{y‚àí y‚àí
and let
ŒΩ := L‚àí1
‚àí y‚àí
be the corresponding orthonormal
implies that
Ô£Æ
Œõ1
Ô£Ø
Œõ2

}=Ô£Ø
H‚àû := E{y+ y‚àí
Ô£∞Œõ3
..
.

ŒΩÃÑ := L‚àí1
+ y+

(4.2)

bases in H ‚àí and H + respectively. Now, (3.9)
Œõ2 Œõ3
Œõ3 Œõ4
Œõ4 Œõ5
..
..
.
.

Ô£π Ô£Æ
Ô£πÔ£Æ
Ô£π
CÃÑ
C
...
. . .Ô£∫ Ô£Ø CA Ô£∫ Ô£Ø CÃÑA Ô£∫
Ô£∫ Ô£Ø
Ô£∫Ô£Ø
Ô£∫
. . .Ô£ª = Ô£∞CA2 Ô£ª Ô£∞CÃÑ(A )2 Ô£ª ,
..
..
...
.
.

(4.3)

and therefore we have the following representation result, which can be found in Desai
et al. (1985).
Proposition 4.1. Let y be realized by a Ô¨Ånite dimensional model of the form (3.7).
Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator
H is

‚àíT
‚àí1
 ‚àíT
HÃÇ‚àû = L‚àí1
+ E{y+ y‚àí }L‚àí = L+ ‚Ñ¶‚Ñ¶ÃÑ L‚àí ,

where

Ô£π
C
Ô£Ø CA Ô£∫
Ô£∫
‚Ñ¶=Ô£Ø
Ô£∞CA2 Ô£ª
..
.

(4.4)

Ô£Æ

Ô£Æ

and

Ô£π
CÃÑ
Ô£Ø CÃÑA Ô£∫
Ô£∫
‚Ñ¶ÃÑ = Ô£Ø
Ô£∞CÃÑ(A )2 Ô£ª .
..
.

(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization
(4.3) for H‚àû , irrespective of which X (i.e. which minimal realization of y) is chosen.
Recall that the adjoint O‚àó of the observability operator O is deÔ¨Åned as the unique
linear operator H + ‚Üí X such that OŒæ, Œª = Œæ, O‚àó Œª for all Œæ ‚àà X and Œª ‚àà H + .
Orthogonality implies that
+

E H Œæ, Œª = Œæ, Œª = Œæ, E X Œª,
and therefore O‚àó = E X |H + . In the same way, we see that C ‚àó = E X |H ‚àí . The Ô¨Åniterank linear operators O‚àó O and C ‚àó C are deÔ¨Åned on X and are the coordinate-free
representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O‚àó O is full rank and constructible if and only if
C ‚àó C is full rank. The following representations show that these gramians are related

16

ANDERS LINDQUIST AND GIORGIO PICCI

to P‚àí and PÃÑ+ , the state covariances of the forward and backward steady-state Kalman
Ô¨Ålters (Picci and Pinzoni, 1994).
Proposition 4.2. Let x(0) and xÃÑ(0) be the conjugate basis vectors in a minimal splitting subspace X as deÔ¨Åned above. Then, in a uniform choice of bases,
O‚àó O a xÃÑ(0) = a PÃÑ+ x(0)

(4.6)

C ‚àó C a x(0) = a P‚àí xÃÑ(0),

(4.7)

and
i.e., C ‚àó C and O‚àó O have matrix representations P‚àí and PÃÑ+ , respectively, independently
of X.
Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal,
‚àí

E H a x(0) = a x‚àí (0),
and therefore

C ‚àó C a x(0) = E X a x‚àí (0) = E X a P‚àí xÃÑ‚àí (0).
But, since the bases xÃÑ(0) and xÃÑ‚àí (0) are chosen uniformly,
E X a xÃÑ‚àí (0) = a xÃÑ(0) a ‚àà Rn ,

and consequently (4.7) follows. The proof of (4.6) is analogous.
The factorization (4.4) can also be derived from (3.13) and the following useful
matrix representations of the observability and constructibility operators.
Proposition 4.3. Let x(0) and xÃÑ(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
O a xÃÑ(0) = a ‚Ñ¶ L‚àíT
+ ŒΩÃÑ

O‚àó b ŒΩÃÑ = b L‚àí1
+ ‚Ñ¶x(0)

(4.8)

C a x(0) = a ‚Ñ¶ÃÑ L‚àíT
‚àí ŒΩ

C ‚àó b ŒΩ = b L‚àí1
‚àí ‚Ñ¶ÃÑxÃÑ(0),

(4.9)

and

where ‚Ñ¶ and ‚Ñ¶ÃÑ are given by (4.5).
Proof. Since, in view of (3.7),
y+ = ‚Ñ¶x(0) + terms which are orthogonal to X,
and ŒΩÃÑ = L‚àí1
+ y+ , we have
E{ŒΩÃÑx(0) } = L‚àí1
+ ‚Ñ¶P.

(4.10)

Consequently, for any a ‚àà Rn , the usual projection formula5 yields
O a x(0) = E H a x(0) = a E{x(0)ŒΩÃÑ  }ŒΩÃÑ
+

and

O‚àó b ŒΩÃÑ = E X b ŒΩÃÑ = b E{ŒΩÃÑx(0) }P ‚àí1 x(0),
from which (4.8) follows. A symmetric argument yields (4.9).
If Œæ ‚àà H(w) and the subspace Z ‚äÇ H(w) is spanned by the components of the full-rank random
vector z, then E Z Œæ = E{Œæz  }(E{zz  })‚àí1 z.
5

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should
note that the matrix representations of O‚àó and C ‚àó are the transposes of those of O
and C if and only if x(0) is an orthogonal basis, i.e., P = PÃÑ = I. Moreover, it follows
from (4.8) that
O‚àó Oa xÃÑ(0) = a ‚Ñ¶ T+‚àí1 ‚Ñ¶x(0),
showing that ‚Ñ¶ T+‚àí1 ‚Ñ¶ is a matrix representation of O‚àó O, in harmony with the analysis
at the end of Section 2. In the same way, (4.9) yields
C ‚àó Ca x(0) = a ‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑxÃÑ(0),
and hence ‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑ is a matrix representation of C ‚àó C. Together with Proposition 4.2
this yields the following explicit formulas for P‚àí and PÃÑ+ :
‚Ñ¶ T+‚àí1 ‚Ñ¶ = PÃÑ+

‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑ = P‚àí .

(4.11)

Now, let {œÉ1 , œÉ2 , œÉ3 , . . . } be the singular values of the Hankel operator H. Since
rank H = n, œÉi = 0 for i > n. The nonzero singular values
1 ‚â• œÉ1 ‚â• œÉ2 ‚â• œÉ3 . . . ‚â• œÉn > 0

(4.12)

are the cosines of the angles between the subspaces H‚àí and H+ ; they are known as the
canonical correlation coeÔ¨Écients of y (Hotelling, 1936, Anderson, 1958). Obviously
œÉ1 < 1 if and and only if H‚àí ‚à© H+ = 0. The squares of the canonical correlation
coeÔ¨Écients are the eigenvalues of H‚àó H, i.e.,
H‚àó H Œæi = œÉi2 Œæi ,
which, in view of (3.13) may be written
O‚àó OC ‚àó C(O‚àó Œæi ) = œÉi2 (O‚àó Œæi ),
and therefore, as was also demonstrated in Picci and Pinzoni (1994),
Œª{O‚àó OC ‚àó C} = {œÉ12 , œÉ22 , . . . , œÉn2 },

(4.13)

i.e., œÉ12 , œÉ22 , . . . , œÉn2 are the eigenvalues of O‚àó OC ‚àó C. But, in view of Proposition 4.2,
this is precisely the coordinate-free version of the invariance condition
{œÉ12 , œÉ22 , . . . , œÉn2 } = Œª{P‚àí PÃÑ+ }

(4.14)

of Desai and Pal (1984).
This suggests that an appropriate uniform choice of bases would be the one that
makes P‚àí and PÃÑ+ equal and equal to the diagonal matrix of nonzero canonical correlation coeÔ¨Écients.
In fact, in view of Proposition 4.1, the inÔ¨Ånite normalized Hankel matrix HÃÇ‚àû is the
matrix representation of the operator H in the orthonormal bases (4.2). Therefore
HÃÇ‚àû has the singular-value decomposition
HÃÇ‚àû = U‚àû Œ£‚àû V‚àû = U Œ£V  ,

(4.15)

where Œ£ is the diagonal n√ón matrix consisting of the canonical correlation coeÔ¨Écients
Œ£ = diag{œÉ1 , œÉ2 , œÉ3 , . . . , œÉn },

(4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and Œ£‚àû is the inÔ¨Ånite matrix
Œ£‚àû

	


Œ£ 0
=
.
0 0

Moreover U‚àû and V‚àû are inÔ¨Ånite orthogonal matrices, and U and V are ‚àû √ó n
submatrices of U‚àû and V‚àû with the the property that
U  U = I = V  V.

(4.17)


We now rotate the the orthonormal bases (4.2) in H + and H ‚àí to obtain u := U‚àû
ŒΩÃÑ


and v := V‚àû ŒΩ respectively. Note that E{uv } = Œ£‚àû . What makes these orthonormal
bases useful is that they are adapted to the orthogonal decompositions6

H ‚àí ‚à® H + = [H ‚àí ‚à© (H + )‚ä• ] ‚äï H  ‚äï [H + ‚à© (H ‚àí )‚ä• ],

(4.18)

where H  := X‚àí ‚à® X+ is the so-called frame space (Lindquist and Picci (1985, 1991),
in the sense that
X‚àí = span{v1 , v2 , . . . , vn }

X+ = span{u1 , u2 , . . . , un }.

This is true since X‚àí is precisely the subspace of random variables in H ‚àí having
nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H ‚àí . Since therefore
{vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H ‚àí ‚à© (H + )‚ä• and H + ‚à© (H ‚àí )‚ä• ,
respectively, these spaces will play no role in what follows.
Now deÔ¨Åne the n-dimensional vectors
Ô£Æ 1/2 Ô£π
Ô£Æ 1/2 Ô£π
œÉ1 u1
œÉ1 v1
Ô£Ø œÉ 1/2 u Ô£∫
Ô£Ø œÉ 1/2 v Ô£∫
2Ô£∫
2Ô£∫
Ô£Ø
Ô£Ø
zÃÑ = Ô£Ø 2 . Ô£∫ = Œ£1/2 U  L‚àí1
(4.19)
z = Ô£Ø 2 . Ô£∫ = Œ£1/2 V  L‚àí1
‚àí y‚àí
+ y+
.
.
Ô£∞ . Ô£ª
Ô£∞ . Ô£ª
1/2

1/2

œÉn vn

œÉn un

From what we have seen before, z is a basis in X‚àí and zÃÑ is a basis in X+ , and they
have the properties
E{zz  } = Œ£ = E{zÃÑ zÃÑ  }.

(4.20)

In fact, we even have more as seen from the following ampliÔ¨Åcation7 of a theorem by
Desai and Pal (1984) (Theorem 1).
Theorem 4.4. The basis vectors
x‚àí (0) = z

xÃÑ+ (0) = zÃÑ

(4.21)

in X‚àí and X+ respectively belong to the same uniform choice of basis, i.e. to the
same choice of triplets (A, C, CÃÑ), and in this uniform choice
P‚àí = Œ£ = PÃÑ+ .

(4.22)

The symbols ‚à® and ‚äï denote vector sum and orthogonal vector sum of subspaces.
A priori there is no reason why choosing bases in X‚àí and X+ would lead to the same (A, C, CÃÑ).
This important property is explicitly mentioned in Theorem 4.4.
6
7

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coeÔ¨Écients {œÉ1 , œÉ2 , œÉ3 , . . . , œÉn } are distinct, this is, modulo
multiplication with a signature matrix 8 , the only uniform choice of bases for which
( 4.22) holds.
Such a choice of (A, C, CÃÑ) is know as stochastically balanced, and, in the case of
distinct canonical correlation coeÔ¨Écients, it deÔ¨Ånes a canonical form with respect to
state space isomorphism in (1.6) by Ô¨Åxing the sign in, say, the Ô¨Årst element in each
row of C. Such canonical forms have also been studied by Ober (1991).
Proof. It follows from (4.4) and (4.15) that
E{zÃÑz  } = Œ£2 .

(4.23)

Now, choose (A, C, CÃÑ) so that xÃÑ+ (0) = zÃÑ, and let the bases in the other splitting
subspaces be chosen accordingly so that the choice of bases is uniform. We want
to show that x‚àí (0) = z. To this end, Ô¨Årst note that x+ (0) = Œ£‚àí1 xÃÑ+ (0) and that
x‚àí (0) = E X‚àí x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula
and the fact that z is a basis in X‚àí ,
x‚àí (0) = Œ£‚àí1 E{zÃÑz  }Œ£‚àí1 z,
which, in view of (4.23), yields x‚àí (0) = z as claimed. Hence (4.22) follows from
(4.20).
Next, suppose that (QAQ‚àí1 , CQ‚àí1 , CÃÑQ ) is another uniform choice of bases which
is also stochastically balanced. Since then x‚àí (0) = Qz and, as is readily seen from
the backward system (3.14), xÃÑ+ (0) = Q‚àíT zÃÑ so that P‚àí = QŒ£Q and PÃÑ+ = Q‚àíT Œ£Q‚àí1 ,
(4.22) yields
QŒ£Q = Œ£ and Q‚àíT Œ£Q‚àí1 = Œ£,
from which we have
QŒ£2 = Œ£2 Q.
Since Œ£ has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959)
that there is a scalar polynomial œï(z) such that Q = œï(Œ£2 ). Hence Q is diagonal and
commutes with Œ£ so that, by QŒ£Q = Œ£, we have
QQ = I.
Consequently, since Q is diagonal, it must be a signature matrix.
In view of (4.21) and (3.16), the Ô¨Årst of relations (4.9) and (4.8) respectively yield
z = ‚Ñ¶ÃÑ T‚àí‚àí1 y‚àí

zÃÑ = ‚Ñ¶ T+‚àí1 y+ .

(4.24)

Consequently, in view of (4.20), (2.15) holds also for the case of an inÔ¨Ånite Hankel
matrix. This can of course also be seen from (4.11).
Note that the normalization of the block Hankel matrix H‚àû is necessary in order for
the singular values to become the canonical correlation coeÔ¨Écients, i.e., the singular
values of H. In fact, if we were to use the unnormalized matrix representation (4.3)
of H instead, as may seem simpler and more natural, the transpose of (4.3) would
not be the matrix representation of H‚àó in the same bases, a property which is crucial
in the singular value decomposition above. This is because (4.3) corresponds to the
bases y‚àí in H ‚àí and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ¬±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the Ô¨Ånite-dimensional case studied in
Section 2, and therefore the normalized Hankel matrix HÃÇ, deÔ¨Åned in Section 2, is
preferable to the unnormalized H.
Formulas, such as (2.5), expressing A, C, CÃÑ in terms of the Hankel matrix H‚àû , can
be easily derived from basic principles. In fact, standard calculations based on the
forward model (3.7) and the backward model (3.14) yield
A = E{x(1)x(0) }P ‚àí1
C = E{y(0)x(0) }P ‚àí1 ,
CÃÑ = E{y(‚àí1)xÃÑ(0) }PÃÑ ‚àí1 = E{y(‚àí1)x(0) }

(4.25a)
(4.25b)
(4.25c)

for any dual pair of bases x(0) and xÃÑ(0).
Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases
(4.19) can be computed by means of the formulas
‚àíT
‚àí1/2
,
A = Œ£‚àí1/2 U  L‚àí1
+ œÉ(H‚àû )L‚àí V Œ£

(4.26a)

‚àí1/2
C = œÅ1 (H‚àû )L‚àíT
,
‚àí VŒ£

(4.26b)


‚àí1/2
œÅ1 (H‚àû
)L‚àíT
,
+ UŒ£

(4.26c)

CÃÑ =

where H‚àû is the unnormalized Hankel matrix (4.3), œÉ(H‚àû ) is obtained from H‚àû by
deleting the Ô¨Årst block row, and œÅ1 (H‚àû ) is the Ô¨Årst block row.
Proof. First, in (4.25a) and (4.25b), we take x(0) to be x‚àí (0). By the Kalman Ô¨Ålter
representation a [x+ (1) ‚àí x‚àí (1)] ‚ä• UH ‚àí ‚äÉ H ‚àí for all a ‚àà Rn ,
E{x‚àí (1)x‚àí (0) } = E{x+ (1)x‚àí (0) } = PÃÑ+‚àí1 E{xÃÑ+ (1)x‚àí (0) }.
But (A, C, CÃÑ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19),
1/2  ‚àí1
U L+ œÉ(y+ ), where œÉ(y+ )
P‚àí = Œ£ = PÃÑ+ , x‚àí (0) = Œ£1/2 V  L‚àí1
‚àí y‚àí and xÃÑ+ (1) = Œ£
is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),

‚àíT
‚àí1/2
,
A = Œ£‚àí1/2 U  L‚àí1
+ E{œÉ(y+ )y‚àí }L‚àí V Œ£

which is identical to (4.26a). Likewise, from (4.26b),
‚àí1/2
,
C = E{y(0)y‚àí }L‚àíT
‚àí VŒ£

which yields (4.26b). Finally, taking xÃÑ(0) to be xÃÑ+ (0) in (4.25c), a symmetric argument yields (4.26c).
Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H‚àû factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from Ô¨Ånite covariance data
In this section we modify the realization theory of Section 4 to the case that only a
Ô¨Ånite segment
{y(0), y(1), y(2), . . . , y(ŒΩ)},

(5.1)

of the time series {y(t)} is available. We still deÔ¨Åne each y(t) as the semi-inÔ¨Ånite
string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact
partial covariance sequence
{Œõ0 , Œõ1 , Œõ2 . . . , ŒõŒΩ }.

(5.2)

The corresponding realization problem, which is purely theoretical and is intended to
prepare for the more realistic identiÔ¨Åcation situation with Ô¨Ånite strings of observed
data (Section 6), is therefore the partial stochastic realization problem mentioned in
Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the
output of some minimal ‚Äútrue‚Äù system (3.7) of dimension n and that ŒΩ is large enough
for n to equal the positive degree of the partial sequence (5.2).
Now, suppose that ŒΩ = 2œÑ ‚àí 1, and partition the data into two matrices
Ô£Æ
Ô£Æ
Ô£π
Ô£π
y(0)
y(œÑ )
Ô£Ø y(1) Ô£∫
Ô£Ø y(œÑ + 1) Ô£∫
+
Ô£∫
Ô£Ø
Ô£∫,
y
=
(5.3)
yœÑ‚àí = Ô£Ø
.
..
œÑ
Ô£∞
Ô£ª
Ô£∞
Ô£ª
..
.
y(œÑ ‚àí 1)

y(2œÑ ‚àí 1)

representing the past and the future respectively, and deÔ¨Åne the corresponding (Ô¨Ånitedimensional) subspaces YœÑ‚àí and YœÑ+ spanned by the rows of yœÑ‚àí and yœÑ+ respectively as
explained in Section 3. Since the data size œÑ will be important in the considerations
that will follow, we denote the Ô¨Ånite block Hankel matrix H of Section 2, relative to
the data (5.3), by HœÑ , i.e.,
HœÑ = E{yœÑ+ (yœÑ‚àí ) }.

(5.4)

Let œÑ0 be the smallest integer œÑ such that rank HœÑ = n. It is well-known that œÑ0 is
the maximum of the observability and constructibility indicies of (A, C, CÃÑ), so n is an
upper bound for œÑ0 . As pointed out in the beginning of Section 2, we need œÑ > œÑ0 to
be certain that the factorization of HœÑ yields a unique (A, C, CÃÑ).
Next we shall consider the class of minimal splitting subspaces for YœÑ‚àí and YœÑ+ , i.e.,
the subspaces XœÑ admitting a canonical factorization
H

œÑ
YœÑ‚àí
YœÑ+ ‚àí‚Üí
OœÑ‚àó 
 CœÑ
XœÑ

of the Ô¨Ånite-interval Hankel operator
‚àí

HœÑ := E YœÑ |YœÑ+ .

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces,
‚àí

XÃÇœÑ ‚àí = E YœÑ YœÑ+

+

and XÃÇœÑ + = E YœÑ YœÑ‚àí ,

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred
to Appendix D.
Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary
time series {y(t)}. Then, if œÑ > œÑ0 ,
XœÑ := UœÑ X

(5.6)

is a minimal splitting subspace for YœÑ‚àí and YœÑ+ , and
‚àí

XÃÇœÑ ‚àí = E YœÑ XœÑ ,

+

XÃÇœÑ + = E YœÑ XœÑ .

(5.7)

Conversely, any basis xÃÇ(œÑ ) in XÃÇœÑ ‚àí has a unique representation9
‚àí

xÃÇ(œÑ ) = E YœÑ x(œÑ ),

(5.8)

ÀÜ (œÑ ) in XÃÇœÑ + has a unique representation
where x(œÑ ) is a basis in XœÑ , and any basis xÃÑ
+

ÀÜ (œÑ ) = E YœÑ xÃÑ(œÑ ),
xÃÑ

(5.9)

X

with xÃÑ(œÑ ) a basis in XœÑ . As X varies over the family
of all minimal Markovian
splitting subspaces, the corresponding x(0) [xÃÑ(0)] constitute a uniform choice of bases.
The stochastic realizations corresponding to the Ô¨Ånite-interval predictor spaces XÃÇœÑ ‚àí
and XÃÇœÑ + are nonstationary. However, taking advantage of the representations (5.8)
and (5.9), we shall be able to express these realizations in such a way that they can
be parameterized by the stationary triplet (A, C, CÃÑ) corresponding to one uniform
choice of bases, both for the forward and the backward settings. In fact, if the bases
ÀÜ (œÑ ) are chosen so that x(œÑ ) and xÃÑ(œÑ ) in representations (5.8) and (5.9) are
xÃÇ(œÑ ) and xÃÑ
dual bases in XœÑ , i.e., E{x(œÑ )xÃÑ(œÑ )} = I, then the same choice of (A, C, CÃÑ) is used for
all X ‚àà . Such a choice of bases in XÃÇœÑ ‚àí and XÃÇœÑ + is called coherent.
The realizations generated by these coherent bases are precisely the (transient)
forward and backward Kalman Ô¨Ålters. In fact, the vector xÃÇ(œÑ ) is the one-step predictor
of x(œÑ ) based on YœÑ‚àí and, as shown in Appendix C, it evolves in time as the Kalman
Ô¨Ålter

X

xÃÇ(t + 1) = AxÃÇ(t) + K(t)[y(t) ‚àí C xÃÇ(t)];

xÃÇ(0) = 0,

(5.10)

where the gain K(t) is given by
K(t) = (CÃÑ  ‚àí AP‚àí (t)C  )(Œõ0 ‚àí CP‚àí (t)C  )‚àí1

(5.11)

and the Ô¨Ålter estimate covariance
P‚àí (t) = E{xÃÇ(t)xÃÇ(t) }

(5.12)

is the solution of the matrix Riccati equation

P‚àí (t + 1) = AP‚àí (t)A + (CÃÑ  ‚àí AP‚àí (t)C  )(Œõ0 ‚àí CP‚àí (t)C  )‚àí1 (CÃÑ  ‚àí AP‚àí (t)C  )
P‚àí (0)) = 0.
(5.13)
9

With slight misuse of notations, the orthogonal projection operator applied to a vector will
denote the vector of the projections of the components.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the
components of
+

ÀÜ (œÑ ) = E YœÑ xÃÑ(œÑ )
xÃÑ

(5.14)

form a basis in XÃÇœÑ + and are generated by the backward Kalman Ô¨Ålter
ÀÜ (t ‚àí 1) = A xÃÑ
ÀÜ (t) + KÃÑ(t)[y(t ‚àí 1) ‚àí CÃÑ xÃÑ
ÀÜ (t)];
xÃÑ

ÀÜ (2œÑ ‚àí 1) = 0,
xÃÑ

(5.15)

with
KÃÑ(t) = (C  ‚àí A PÃÑ+ (t)CÃÑ  )(Œõ0 ‚àí CÃÑP‚àí (t)CÃÑ  )‚àí1 ,

(5.16)

ÀÜ (t)xÃÑ
ÀÜ (t) }
PÃÑ+ (t) = E{xÃÑ

(5.17)

where

is obtained by solving the matrix Riccati equation

PÃÑ+ (t ‚àí 1) = A PÃÑ+ (t)A + (C  ‚àí A PÃÑ+ (t)CÃÑ  )(Œõ0 ‚àí CÃÑ PÃÑ+ (t)CÃÑ  )‚àí1 (C  ‚àí A PÃÑ+ (t)CÃÑ  )
PÃÑ+ (2œÑ ‚àí 1) = 0.
(5.18)
Now, it is well-known that both
ŒΩ(t) = (Œõ0 ‚àí CP‚àí (t)C  )‚àí1/2 [y(t) ‚àí C xÃÇ(t)]

(5.19)

ÀÜ (t)]
ŒΩÃÑ(t) = (Œõ0 ‚àí CÃÑ PÃÑ+ (t)CÃÑ  )‚àí1/2 [y(t ‚àí 1) ‚àí CÃÑ xÃÑ

(5.20)

and

are normalized white noises, called the forward respectively the backward (transient)
innovation processes. Consequently, we may write the Kalman Ô¨Ålter (5.10) as

xÃÇ(t + 1) = AxÃÇ(t) + B‚àí (t)ŒΩ(t)
(5.21)
y(t) = C xÃÇ(t) + D‚àí (t)ŒΩ(t)
where D‚àí (t) := (Œõ0 ‚àí CP‚àí (t)C  )1/2 and B‚àí (t) := K(t)D‚àí (t). Likewise, the backward
Kalman Ô¨Ålter (5.10) may be written

ÀÜ (t) + BÃÑ+ (t)ŒΩÃÑ(t ‚àí 1)
ÀÜ (t ‚àí 1) = A xÃÑ
xÃÑ
(5.22)
ÀÜ (t) + DÃÑ+ (t)ŒΩÃÑ(t ‚àí 1)
y(t ‚àí 1) = CÃÑ xÃÑ
where DÃÑ+ (t) := (Œõ0 ‚àí CÃÑ PÃÑ+ (t)CÃÑ  )1/2 and BÃÑ+ (t) := KÃÑ(t)DÃÑ+ (t). Comparing with (3.7)
and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7)
and (3.14) are time-varying and describe the output y only on the interval [0, 2œÑ ‚àí 1].
In fact, since
P ‚àí P‚àí (t) = E{[x(t) ‚àí xÃÇ(t)][x(t) ‚àí xÃÇ(t)] } ‚â• 0,
and, for the same reason, PÃÑ ‚àí PÃÑ+ (t) ‚â• 0, we have
P‚àí (t) ‚â§ P ‚â§ P+ (t) := PÃÑ+ (t)‚àí1 ,

(5.23)

so we see that the predictor spaces XÃÇœÑ ‚àí and XÃÇœÑ + are extremal splitting subspaces,
just as X‚àí and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the Ô¨Ånite-interval counterparts of equations (4.25)
are given by
A = E{xÃÇ(œÑ + 1)xÃÇ(œÑ ) }P‚àí (œÑ )‚àí1
C = E{y(œÑ )xÃÇ(œÑ ) }P‚àí (œÑ )‚àí1 ,
ÀÜ (œÑ ) }PÃÑ+ (œÑ )‚àí1 = E{y(œÑ ‚àí 1)xÃÇ(œÑ ) }
CÃÑ = E{y(œÑ ‚àí 1)xÃÑ

(5.24a)
(5.24b)
(5.24c)

In complete analogy with the stationary framework in Section 4, the canonical
correlation coeÔ¨Écients
1 ‚â• œÉ1 (œÑ ) ‚â• œÉ2 (œÑ ) ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉn (œÑ ) > 0

(5.25)

between the Ô¨Ånite past YœÑ‚àí and the Ô¨Ånite future YœÑ+ are now deÔ¨Åned as the singular
values of the operator HœÑ given by (5.5). To determine these we need a matrix representation of HœÑ in some orthonormal bases. Using the pair (5.19)‚Äì(5.20) of transient
innovation processes for this purpose, we obtain the normalized matrix (2.14), which
we shall here denote HÃÇœÑ . Singular value decomposition yields
HÃÇœÑ = UœÑ Œ£œÑ VœÑ ,

(5.26)

where UœÑ UœÑ = I = VœÑ VœÑ , and Œ£œÑ is the diagonal matrix of canonical correlation
coeÔ¨Écients. As in Section 4 it is seen that

1/2
‚àí1 ‚àí
z(œÑ ) = Œ£œÑ VœÑ (L‚àí
œÑ ) yœÑ
(5.27)
1/2
‚àí1 +
zÃÑ(œÑ ) = Œ£œÑ UœÑ (L+
œÑ ) yœÑ
are bases in XÃÇœÑ ‚àí and XÃÇœÑ + respectively and that
E{z(œÑ )z(œÑ ) } = Œ£œÑ = E{zÃÑœÑ zÃÑœÑ }.

(5.28)

+
Here L‚àí
œÑ and LœÑ are the Ô¨Ånite-interval counterparts of L‚àí and L+ respectively, and
they are of course submatrices of these. Note that HœÑ , as deÔ¨Åned by (5.4), is now
given by
‚àí 
H œÑ = L+
œÑ HÃÇœÑ (LœÑ ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z(œÑ ) and zÃÑ(œÑ ) are coherent bases, and the
corresponding triplet (A, C, CÃÑ) is a Ô¨Ånite-interval stochastically balanced realization,
i.e.,
P‚àí (œÑ ) = Œ£œÑ = PÃÑ+ (œÑ ).

(5.30)

The following Ô¨Ånite-interval modiÔ¨Åcation of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the
Ô¨Ånite block hankel matrix HœÑ , and the proof is analogous.
Proposition 5.2. The Ô¨Ånite-interval stochastically balanced triplet (AœÑ , CœÑ , CÃÑœÑ ), obÀÜ (œÑ ) = zÃÑ(œÑ ), is given by
tained from (5.24) by choosing the bases xÃÇ(œÑ ) = z(œÑ ) and xÃÑ
‚àí1
‚àí ‚àíT
UœÑ (L+
VœÑ Œ£‚àí1/2
,
AœÑ = Œ£‚àí1/2
œÑ
œÑ ) œÉ(HœÑ )(LœÑ )
œÑ
‚àíT
VœÑ Œ£‚àí1/2
,
CœÑ = œÅ1 (HœÑ )(L‚àí
œÑ )
œÑ

+ ‚àíT
‚àí1/2
CÃÑœÑ = œÅ1 (HœÑ )(LœÑ ) UœÑ Œ£œÑ ,

(5.31a)
(5.31b)
(5.31c)

where the operators œÉ(¬∑) and œÅ1 (¬∑) are deÔ¨Åned as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

Note that the triplet (AœÑ , CœÑ , CÃÑœÑ ) actually varies with œÑ , but that, for each œÑ , it
is similar to the stochastically balanced triplet (A, C, CÃÑ) of Section 4, i.e., there is a
nonsingular matrix QœÑ so that
‚àí1

(AœÑ , CœÑ , CÃÑœÑ ) = (QœÑ AQ‚àí1
œÑ , CQœÑ , CÃÑQœÑ ).

(5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the
stationary predictor spaces X‚àí and X+ will have the state covariances
P‚àí = QœÑ Œ£QœÑ

‚àí1
and PÃÑ+ = Q‚àíT
œÑ Œ£QœÑ ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state
covariances are not diagonal and equal is a manifestation of the fact that the triplet
(AœÑ , CœÑ , CÃÑœÑ ) is not stochastically balanced in the sense of Section 4. It is well known
that P‚àí (t) and PÃÑ+ (t) tend monotonically to P‚àí and PÃÑ+ , respectively, as t ‚Üí ‚àû, and
therefore we have the following ordering
P‚àí (œÑ ) := Œ£œÑ ‚â§ P‚àí ‚â§ (PÃÑ+ )‚àí1 ‚â§ (PÃÑ+ (œÑ ))‚àí1 := Œ£‚àí1
œÑ .
Since the number n of nonzero singular values (5.25) is in general too large too
yield a reasonable model, we must consider what happens when some of the smallest
singular values are set equal to zero. The truncation procedure employed by van
Overschee and De Moor (1993) is equivalent to the principal subsystem truncation
presented in Section 2, except that, and this is very important, the singular-value
decomposition is performed on the normalized block Hankel matrix HÃÇœÑ , which is the
natural matrix representation of the operator œÑ . It will be shown in Section 7 that
such a truncation will preserve positivity in the stationary case (Theorem 7.3). In
order to carry this result over to the case of Ô¨Ånite œÑ , we need to assume that the
spectral density Œ¶ of the time series {y(t)} is coercive so that Assumption 3.2 is
fulÔ¨Ålled, i.e., that the function Z is strictly positive real.
The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D,
shows that principal subsystem truncation preserves positivity provided œÑ is chosen
large enough.

H

Theorem 5.3. Suppose that the spectral density Œ¶ of the time series {y(t)} is coercive. Then, there is an integer œÑ1 > œÑ0 such that, for œÑ ‚â• œÑ1 , the principal subsystem
truncation ((AœÑ )11 , (CœÑ )1 , (CÃÑœÑ )1 ) of (AœÑ , CœÑ , CÃÑœÑ ) is a minimal realization of a strictly
positive real function (2.13).
6. Subspace identiÔ¨Åcation
The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have
access to an inÔ¨Ånite sequence (3.2) of data. In reality we will have a Ô¨Ånite string of
observed data
{y0 , y1 , y2 , . . . , yN },

(6.1)

where, however, N may be quite large. More speciÔ¨Åcally, we assume that N is sufÔ¨Åciently large that replacing the ergodic limits (1.11) by truncated sums yields good
approximations of
{Œõ0 , Œõ1 , Œõ2 . . . , ŒõŒΩ },

(6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course, ŒΩ << N . This is equivalent to saying that T := N ‚àí ŒΩ is suÔ¨Éciently
large for
1  

a yt+k yt+j
b
T + 1 t=0
T

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we
shall use the Ô¨Ånite-interval realization theory of Section 5 as if we had a Ô¨Ånite time
series
{y(0), y(1), y(2), . . . , y(ŒΩ)},

(6.4)

while substituting the semi-inÔ¨Ånite string (3.3) of data by
y(t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . , ŒΩ.

(6.5)

In particular, in this case the inner product becomes merely that of a Ô¨Ånite-dimensional
Euclidean space so that the block Hankel matrix HœÑ can be written
HœÑ =
where

Ô£Æ
Ô£π
yœÑ ‚àí1 yœÑ . . . yT +œÑ ‚àí1
Ô£ØyœÑ ‚àí2 yœÑ ‚àí1 . . . yT +œÑ ‚àí2 Ô£∫
yœÑ‚àí = Ô£Ø
..
.. Ô£∫
..
Ô£∞ ...
.
.
. Ô£ª
y0
y1 . . .
yT

1
y + (y ‚àí )
T +1 œÑ œÑ
Ô£Æ

Ô£π
yœÑ +1 . . .
yT +œÑ
yœÑ
Ô£Ø yœÑ +1 yœÑ +2 . . . yT +œÑ +1 Ô£∫
and yœÑ+ = Ô£Ø
.
..
.. Ô£∫
..
Ô£∞ ...
.
.
. Ô£ª
y2œÑ ‚àí1 y2œÑ . . . yT +2œÑ ‚àí1

Consequently, the identiÔ¨Åcation of a minimal stationary state-space innovation
model describing the data (6.1) can be performed in the following steps.
(1) Perform canonical correlation analysis on the data yœÑ‚àí , yœÑ+ to obtain, from
ÀÜ + (œÑ ) = zÃÑ(œÑ ) and, from (5.26), the
(5.27), the state vectors xÃÇ‚àí (œÑ ) = z(œÑ ) and xÃÑ
corresponding common state covariance matrix Œ£œÑ , i.e., the diagonal matrix of
the (Ô¨Ånite interval) canonical correlation coeÔ¨Écients (5.25).
(2) Given the singular value decomposition (5.26), compute via (5.31) a minimal
realization (A, C, CÃÑ). This realization will be in Ô¨Ånite-interval balanced form,
i.e., (5.30) will hold instead of (4.22).
(3) To obtain a state space model (3.7) for y we need to compute the matrices B
and D. Note that such matrices will exist if and only if (A, C, CÃÑ, Œõ0 ) deÔ¨Ånes
a positive real function (1.6), or, in other words, if and only if there is a
symmetric positive deÔ¨Ånite P = P  such that


	
P ‚àí AP A CÃÑ  ‚àí AP C 
‚â• 0.
(6.6)
M (P ) :=
CÃÑ ‚àí CP A Œõ0 ‚àí CP C 
[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6),
B and D can be determined (in a nonunique way) by a full rank factorization
of M (P ), i.e.,
	 


B  
B D = M (P ).
(6.7)
D

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P‚àí = E{x‚àí (t)x‚àí (t) } has been
determined. Obtaining P‚àí amounts to Ô¨Ånding the minimal solution of the
algebraic Riccati equation
P = AP A + (CÃÑ  ‚àí AP C  )(Œõ0 ‚àí CP C  )‚àí1 (CÃÑ  ‚àí AP C  )

(6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t ‚Üí ‚àû
with initial condition P‚àí (œÑ ) = Œ£œÑ . (The corresponding dual procedures yield
PÃÑ+ .) Again, in both cases, a positive deÔ¨Ånite P‚àí can be found if and only
if (A, C, CÃÑ, Œõ0 ) deÔ¨Ånes a positive real function (1.6). In fact, in general,
{P‚àí (t)}t‚â•0 may not even converge unless this positivity condition is fulÔ¨Ålled
and may in fact exhibit dynamical behavior with several of the characteristics
of chaotic dynamics (Byrnes et al., 1991, 1994).
Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that,
for œÑ Ô¨Åxed but suÔ¨Éciently large (see Section 2), we will have rank HœÑ = n as T ‚Üí ‚àû,
and the triplet (A, C, CÃÑ) will be uniquely determined from the data and similar to the
triplet (A, C, CÃÑ) of the ‚Äútrue‚Äù generating system. Hence, in particular, in the limit as
T ‚Üí ‚àû, at least in theory positivity will be guaranteed. If nÃÇ is an upper bound for
the order of the ‚Äútrue‚Äù system, we may choose œÑ to be any integer larger than nÃÇ.
In practice, however, T is Ô¨Ånite, and even if we had a true system generating exact
data, the spectral estimate Œ¶T , although converging to the true spectrum Œ¶ as T ‚Üí ‚àû
may in principle fail to be positive for any Ô¨Ånite T if there are frequencies œâ for which
Œ¶(eiœâ ) = 0. Positivity for a suitably large T can however be guaranteed if the ‚Äútrue‚Äù
spectrum is coercive. The following proposition, which also applies to Aoki‚Äôs method
discussed in Section 2, is proved in Appendix D.
Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulÔ¨Ålled. Then, there is a T0 ‚àà Z+ such that, for T ‚â• T0 , the triplet (A, C, CÃÑ) deÔ¨Åned by
(5.31) yields a function (1.6) which is strictly positive real.
However, in practice, rank HœÑ normally will keep increasing with œÑ , even for very
large T , so that one must resort to some kind of truncation of the Hankel singular
values. As we have pointed out in Section 5, setting all canonical correlation coeÔ¨Écients œÉr+1 (œÑ ), œÉr+2 (œÑ ), . . . equal to zero for some suitable r, as is done in, for example,
van Overschee and De Moor (1993), is equivalent to principal subsystem truncation.
An important issue is therefore under what conditions such a procedure will insure
positivity. Here we must distinguish between problems generated by the sample Ô¨Çuctuations of the data due to Ô¨Ånite sample size T , as considered in Proposition 6.1, and
the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an inÔ¨Ånite string of data generated by a ‚Äútrue‚Äù
high-dimensional system, such a truncation procedure may fail if œÑ is smaller than
that dimension.
Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following
result, which justiÔ¨Åes this approximation procedure, provided the rather stringent
Assumption 2.1 holds and we have coercivity, and provided T and œÑ are suÔ¨Éciently
large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulÔ¨Ålled.
Then, there are positive integers T0 and œÑ1 > œÑ0 such that, for T ‚â• T0 and œÑ ‚â• œÑ1 , the
triplet (A11 , C1 , CÃÑ1 ), obtained from (2.12) by taking H := HœÑ in (2.10), is a minimal
realization of a strictly positive real function (2.13).
We note that, in van Overschee and De Moor (1993), the large Hankel matrix
HÃÉœÑ = (yœÑ+ ) (E{yœÑ+ (yœÑ+ ) })‚àí1 E{yœÑ+ (yœÑ‚àí ) }(E{yœÑ‚àí (yœÑ‚àí ) })‚àí1 yœÑ‚àí
is used in place of HÃÇœÑ . This leads to a procedure which is equivalent to the one
described above. Moreover, the computation of a second singular-value decomposition
in van Overschee and De Moor (1993), based on HœÑ +1 := E{yœÑ++1 (yœÑ‚àí+1 ) }, together
with a subsequent change of bases, is actually redundant, as can be deduced from
the following proposition. In fact, a considerable amount of computation is needed in
van Overschee and De Moor (1993) to compensate for the fact that taking z(œÑ + 1),
computed from a second singular-value decomposition, as a basis in XÃÇ(œÑ +1)‚àí would
lead to a Kalman Ô¨Ålter model with time-varying parameters.
ÀÜ (œÑ ) in the Ô¨Ånite-interval
Proposition 6.3. To each coherent pair of bases xÃÇ(œÑ ) and xÃÑ
predictor spaces XÃÇœÑ ‚àí and XÃÇœÑ + , there corresponds a minimal factorization
HœÑ = ‚Ñ¶œÑ ‚Ñ¶ÃÑœÑ

(6.9)

of the block Hankel matrix HœÑ . Here
‚àí

‚Ñ¶œÑ xÃÇ(œÑ ) = E YœÑ yœÑ+

and

+

ÀÜ (œÑ ) = E YœÑ yœÑ‚àí .
‚Ñ¶ÃÑœÑ xÃÑ

(6.10)

Conversely, given a minimal factorization (6.9),
xÃÇ(œÑ ) = ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí

and

ÀÜ (œÑ ) = ‚Ñ¶œÑ (TœÑ+ )‚àí1 yœÑ+
xÃÑ

(6.11)

is a coherent pair of bases in XÃÇœÑ ‚àí and XÃÇœÑ + .
ÀÜ (œÑ ) be a coherent choice of bases in XÃÇœÑ ‚àí and XÃÇœÑ + . Then, for
Proof. Let xÃÇ(œÑ ) and xÃÑ
any XœÑ as deÔ¨Åned in Theorem 5.1, there is a unique pair (x(œÑ ), xÃÑ(œÑ )) of dual bases
such that (5.8) and (5.9) hold. Let ‚Ñ¶œÑ and ‚Ñ¶ÃÑœÑ be the matrices deÔ¨Åned via
E XœÑ yœÑ+ = ‚Ñ¶œÑ x(œÑ ) and E XœÑ yœÑ‚àí = ‚Ñ¶ÃÑœÑ xÃÑ(œÑ ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of XœÑ with respect to
YœÑ‚àí and YœÑ+ yields
E{yœÑ+ (yœÑ‚àí ) } = E{E XœÑ yœÑ+ (E XœÑ yœÑ‚àí )) },
‚àí

+

which, in view of (6.12), is the same as (6.9). Applying E YœÑ and E YœÑ to respectively
the Ô¨Årst and second equations of (6.12), the splitting property yields (6.10).
As for the converse statement, equations (6.11) follow from the construction in the
proof of Theorem 5.1, from which it also follows that the resulting bases xÃÇ(œÑ ) and
ÀÜ (œÑ ) are constructed from the same (A, C, CÃÑ) and therefore coherent.
xÃÑ
As soon as the parameters (A, C, CÃÑ) have been Ô¨Åxed by a particular choice of x(œÑ )
in the representation (5.8) in Theorem 5.1, we must choose xÃÇ(œÑ + 1) as
xÃÇ(œÑ + 1) = E YœÑ +1 Ux(œÑ )

(6.13)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More speciÔ¨Åcally Proposition 6.3
implies that ‚Ñ¶œÑ and ‚Ñ¶ÃÑœÑ are uniquely determined once x(œÑ ) has been selected. Hence
(A, C, CÃÑ) is uniquely determined by the Ho-Kalman algorithm so that
	


CÃÑ
‚Ñ¶ÃÑœÑ +1 =
‚Ñ¶ÃÑœÑ A
is prescribed, as is
xÃÇ(œÑ + 1) = ‚Ñ¶ÃÑœÑ (TœÑ‚àí+1 )‚àí1 yœÑ‚àí+1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining
xÃÇ(œÑ + 1) by an extra singular-value decomposition, as in van Overschee and De Moor
(1993), is actually redundant. If we actually were to determine xÃÇ(œÑ + 1) as described
above, we would better compute ‚Ñ¶ÃÑœÑ +1 from ‚Ñ¶ÃÑœÑ +1 = ‚Ñ¶‚àíL
œÑ HœÑ +1 , where the left inverse
is very easily obtained from the singular-value decomposition of HœÑ .
We stress that Assumption 2.1, although quite limiting, is absolutely crucial in
insuring that the subspace identiÔ¨Åcation algorithms mentioned above will actually
work. Note that for generic data these algorithms may break down for any Ô¨Åxed œÑ .
The same is true for all other subspace methods which deal with identiÔ¨Åcation of
covariance models (or equivalent) involving stochastic signals.
On the other hand, Assumption 2.1 introduces a quite unrealistic condition which,
as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure
to estimate T0 and œÑ1 in Proposition 6.2, as the proof is based only on continuity
arguments.
7. Stochastic model reduction
As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step
in order to keep the dimension of the model at a reasonable level. To justify any
such procedure one must either assume that there is an underlying ‚Äútrue‚Äù system of
suÔ¨Éciently low order, i.e., invoke Assumption 2.1, or to perform rational covariance
extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995),
Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an inÔ¨Ånite
one. The latter can be done in many ways, one of which is the maximum entropy
extension.
In either case, the truncation problem is equivalent to approximating a positive
real matrix function
1
Z(z) = C(zI ‚àí A)‚àí1 CÃÑ  + Œõ0 ,
2

(7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of
lower degree. In this section we shall investigate how this can be done and also how
such an approximation aÔ¨Äects the canonical correlation structure.
One main question to be addressed is whether the principal subsystem truncation
(2.11) preserves positive realness and balancing, and hence the leading canonical
correlation coeÔ¨Écients, as originally claimed by Desai and Pal (1982). As it turns
out, the answer is aÔ¨Érmative to the Ô¨Årst but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identiÔ¨Åcation approximation obtained
by setting some canonical correlation coeÔ¨Écients equal to zero.
It is instructive to Ô¨Årst consider the continuous-time counterpart of this problem
since the latter is simpler and exhibits more desirable properties. Also, it has been
widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true.
It is well-known [see, e.g., Faurre et al. (1979)] that an m √ó m matrix function Z
with minimal realization
1
(7.2)
Z(s) = C(sI ‚àí A)‚àí1 CÃÑ  + R,
2
is positive real with respect to the right half plane if and only if there is a symmetric
matrix P > 0 such that


	
‚àíAP ‚àí P A CÃÑ  ‚àí P C 
‚â• 0,
(7.3)
M (P ) :=
CÃÑ ‚àí CP
R
where here we assume that R is positive deÔ¨Ånite and symmetric. In this case there
are two solutions of (7.3), P‚àí and P+ , with the property that any other solution of
(7.3) satisÔ¨Åes
P ‚àí ‚â§ P ‚â§ P+ .

(7.4)

These extremal solutions play the same role as P‚àí and P+ in the discrete-time setting,
and
rank M (P‚àí ) = m = rank M (P+ ).

(7.5)

If the state-space coordinates are chosen so that both P‚àí and PÃÑ+ := P+‚àí1 are diagonal
and equal, and thus, by (4.14), equal to the diagonal matrix Œ£ of canonical correlation
coeÔ¨Écients, we say that (A, C, CÃÑ) is stochastically balanced.
Now, suppose that Œ£ is partitioned as in (2.8) with œÉr+1 < œÉr , and consider the
corresponding principal subsystem truncation (2.12). Using the stochastic realization
framework, Harshavaradana, Jonckheere and Silverman (1984) showed that
1
Z1 (s) = C1 (sI ‚àí A11 )‚àí1 CÃÑ1 + R,
2

(7.6)

is a minimal realization of a positive real function and conjectured that (A11 , C1 , CÃÑ1 )
is stochastically balanced. We shall next show that this conjecture is true, as has
already been done by Ober (1991) in a framework of canonical forms.
First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield
Ô£π
Ô£Æ
‚àíA11 Œ£1 ‚àí Œ£1 A11 ‚àó CÃÑ1 ‚àí Œ£1 C1
Ô£ª ‚â• 0,
Ô£∞
‚àó
‚àó
‚àó
(7.7)
‚àó
R
CÃÑ1 ‚àí C1 Œ£1
where blocks which play no role in the analysis are marked by an asterisk. Consequently,


	
‚àíA11 Œ£1 ‚àí Œ£1 A11 CÃÑ1 ‚àí Œ£1 C1
‚â• 0.
(7.8)
M1 (Œ£1 ) =
CÃÑ1 ‚àí C1 Œ£1
R

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982),
Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane,
(7.6) is positive real, but it remains to prove that (A11 , C1 , CÃÑ1 ) is a minimal realization.
This was done in Harshavaradhana et al. (1984). It is important to observe here that,
contrary to the situation in the discrete-time setting, rank M1 (Œ£1 ) = rank M (Œ£) = m
‚àí1
and rank M1 (Œ£‚àí1
1 ) = rank M (Œ£ ) = m, important facts that will be seen to imply
that the reduced system is stochastically balanced.
Recall that in the continuous-time setting the spectral density Œ¶(s) = Z(s)+Z(‚àís)
is coercive if, for some < > 0, we have Œ¶(s) ‚â• <I for all s on the imaginary axis. This
is equivalent to the condition that R > 0 and Œ¶ has no zeros on the imaginary axis
(Faurre et al., 1979, Theorem 4.17).
Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with Œ¶(s) :=
Z(s) + Z(‚àís) coercive, and let (A, C, CÃÑ) be in stochastically balanced form. Then,
if œÉr+1 < œÉr , the reduced system (A11 , C1 , CÃÑ1 ) deÔ¨Ånes a positive real function (7.6)
for which it is a minimal realization in stochastically balanced form, and Œ¶1 (s) :=
Z1 (s) + Z1 (‚àís) is coercive.
Proof. We have already shown that Z1 is positive real, and we refer the reader to
Harshavaradhana et al. (1984) for the proof that (A11 , C1 , CÃÑ1 ) is a minimal realization
of Z1 . It remains to show that Œ¶1 is coercive and that (A11 , C1 , CÃÑ1 ) is stochastically
‚àí1
, where P1‚àí and P1+ are solutions to the algebraic
balanced, i.e., that P1‚àí = Œ£1 = P1+
Riccati equation
A11 P1 + P1 A11 + (CÃÑ  ‚àí P1 C1 )R‚àí1 (CÃÑ  ‚àí P1 C1 ) = 0

(7.9)

such that any other solution P1 of (7.9) satisÔ¨Åes P1‚àí ‚â§ P1 ‚â§ P1+ . To this end,
‚àí1
note that since M1 (Œ£1 ) and M1 (Œ£‚àí1
1 ) have rank m, both Œ£1 and Œ£1 satisfy (7.9).
Therefore, as is well-known (Molinari, 1977) and easy to show, Q := Œ£‚àí1
1 ‚àíŒ£1 satisÔ¨Åes
Œì1 Q + QŒì1 + QC1 R‚àí1 C1 Q = 0,

(7.10)

Œì1 = A11 ‚àí (CÃÑ  ‚àí Œ£1 C1 )R‚àí1 C1 .

(7.11)

where
Since Œ¶ is coercive, Œ£‚àí1 ‚àí Œ£ = P+ ‚àí P‚àí > 0 (Faurre et al., 1979, Theorem 4.17) so
that œÉ1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to
Œì1 Q‚àí1 + Q‚àí1 Œì1 + C1 R‚àí1 C1 = 0.

(7.12)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , Œì1 ). Since,
in addition, the Lyapunov equation (7.12) has a positive deÔ¨Ånite solution Q‚àí1 , Œì1
must be a stability matrix. Therefore Œ£1 is the minimal (stabilizing) solution P1‚àí of
‚àí1
= Œ£1 .
(7.9). In the same way, using the backward setting, we show that PÃÑ1+ := P1+
Consequently, (A11 , C1 , CÃÑ1 ) is stochastically balanced. Since P1+ ‚àí P1‚àí > 0, Œ¶1 is
coercive.
Let us now return to the discrete-time setting. Let us recall that, if (A, C, CÃÑ, 12 Œõ0 )
is a minimal realization of (7.1), the matrix function Z is positive real if and only if
the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given
the positive real rational function (7.1) with the property that Œ¶(z) = Z(z) + Z(z ‚àí1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y, the state covariance P of any minimal
stochastic realization (3.7) of y satisÔ¨Åes (6.6) and the matrices B, D in (3.7) satisfy
(6.7). Consequently, as pointed out in Section 5, the matrices B and D can be
determined via a matrix factorization of M (P ) once P has been determined.
Now, if (A, C, CÃÑ) is in stochastically balanced form, Theorem 4.4 implies that
M (Œ£) ‚â• 0. In view of (4.16) and (2.12), M (Œ£) may be written
Ô£π
Ô£Æ
Œ£1 ‚àí A11 Œ£1 A11 ‚àí A12 Œ£2 A12 ‚àó CÃÑ1 ‚àí A11 Œ£1 C1 ‚àí A12 Œ£2 C2
Ô£ª,
Ô£∞
‚àó
‚àó
‚àó




CÃÑ1 ‚àí C1 Œ£1 A11 ‚àí C2 Œ£2 A12 ‚àó Œõ0 ‚àí C1 Œ£1 C1 ‚àí C2 Œ£2 C2
where, as before, the blocks which do not enter the analysis are marked with an
asterisk. Since M (Œ£) ‚â• 0, this implies that
	 
 	 

A
A
(7.13)
M1 (Œ£1 ) ‚àí 12 Œ£2 12 ‚â• 0,
C2
C2
where

	


Œ£1 ‚àí A11 Œ£1 A11 CÃÑ1 ‚àí A11 Œ£1 C1
M1 (Œ£1 ) =
CÃÑ1 ‚àí C1 Œ£1 A11 Œõ0 ‚àí C1 Œ£1 C1

(7.14)

is the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , CÃÑ1 ). Therefore, M (Œ£1 ) ‚â• 0, so if we can show that A11 is stable, i.e., has all its eigenvalues
strictly inside the unit circle, it follows that
1
(7.15)
Z1 (z) = C1 (zI ‚àí A11 )‚àí1 CÃÑ1 + Œõ0 ,
2
is positive real. As we shall see below this is true without the requirement needed in
continuous time that œÉr+1 < œÉr .
For (A11 , C1 , CÃÑ1 ) also to be balanced, Œ£1 would have to be the minimal solution P1‚àí
of M1 (P1 ) ‚â• 0, which in turn would require that rank M1 (Œ£1 ) = rank M (Œ£) = m.
Due to the extra positive semideÔ¨Ånite term in (7.13), however, this will in general not
be the case and therefore Œ£1 ‚â• P1‚àí will correspond to an external realization, as will
Œ£‚àí1
1 ‚â§ P1+ ; see Lindquist and Picci (1991).
To show that (A11 , C1 , CÃÑ1 ) is minimal we need to assume that Œ¶ is coercive, or,
equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979,
Theorem A4.4) that this implies that
P+ ‚àí P‚àí > 0.

(7.16)

In fact, if Œõ0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to
coercivity. Coercivity also implies that
Œõ0 ‚àí CP‚àí C  > 0.

(7.17)

Remark 7.2. With (A, C, CÃÑ) in balanced form, P‚àí = Œ£ = PÃÑ+ and, in view of (3.16),
P+ = Œ£‚àí1 . Hence (7.16) becomes Œ£‚àí1 > Œ£, which obviously holds if and only if
œÉ1 < 1, which in turn is equivalent to H ‚àí ‚à© H + = 0. Consequently, given the full
rank condition Œõ0 > 0, coercivity is equivalent to the past and the future spaces of y
having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

Theorem 7.3. Let (7.1) be positive real, and let (A, C, CÃÑ) be in stochastically balanced form. Then the reduced-degree function (7.15) obtained via principal subsystem
decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so
is Z1 , and (A11 , C1 , CÃÑ1 , 12 Œõ0 ) is a minimal realization of Z1 .
For the proof we need the following lemma, the proof of which is given in Appendix
D.
Lemma 7.4. Let the matrix function Z be given by (7.1), where Œõ0 > 0, but where
(C, A) and (CÃÑ, A ) are not necessarily observable, and suppose that (6.6) has two
positive deÔ¨Ånite symmetric solutions, P1 and P2 , such that
P2 ‚àí P1 > 0.

(7.18)

Then Z is strictly positive real.
Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is
stable. To this end, we note that P is the reachability gramian of (3.7). In particular,
if (A, C, CÃÑ) is stochastically balanced, the reachability gramian of the system (3.18)
equals Œ£ so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable.
By Remark 7.2, coercivity of Œ¶ implies that Œ£‚àí1 ‚àí Œ£ > 0, from which it follows
that Œ£‚àí1
1 ‚àí Œ£1 > 0 and that Œõ0 > 0. Moreover, By construction, M1 (Œ£1 ) ‚â• 0 and
‚àí1
M1 (Œ£1 ) ‚â• 0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is.
To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows
by symmetry. By regularity condition (7.17),
Œõ0 ‚àí C1 Œ£1 C1 ‚â• Œõ0 ‚àí CŒ£C  > 0,
and consequently, since M1 (Œ£1 ) ‚â• 0, Œ£1 satisÔ¨Åes the algebraic Riccati inequality
A11 P1 A11 ‚àí P1 + (CÃÑ1 ‚àí A11 P1 C1 )(Œõ0 ‚àí C1 P1 C1 )‚àí1 (CÃÑ1 ‚àí A11 P1 C1 ) ‚â• 0, (7.19)
but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable.
Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density Œ¶1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a
unique symmetric P1‚àí > 0 which satisÔ¨Åes (7.19) with equality and for which
Œì1‚àí := A11 ‚àí (CÃÑ1 ‚àí A11 P1‚àí C1 )(Œõ0 ‚àí C1 P1‚àí C1 )‚àí1 C1
is stable. It is well-known (Faurre et al., 1979) that P1‚àí is the minimal symmetric
solution of the linear matrix inequality M1 (P1 ) ‚â• 0, i.e., that any other symmetric
‚àí1
solution P1 satisÔ¨Åes P1 ‚â• P1‚àí . We also know that M1 (Œ£‚àí1
1 ) ‚â• 0. Next, since Œ£1 ‚àí
Œ£1 > 0, a fortiori it holds that Q := Œ£‚àí1
1 ‚àí P1‚àí > 0. A tedious but straight-forward
calculation shows that Q satisÔ¨Åes
Œì1‚àí (Q‚àí1 ‚àí C1 R‚àí1 C1 )‚àí1 Œì1‚àí ‚àí Q ‚â• 0,
from which it follows that
Q‚àí1 ‚àí C1 R‚àí1 C1 ‚àí Œì1‚àí Q‚àí1 Œì1‚àí ‚â§ 0.
Cf. Faurre et al. (1979), pp. 85 and 95.

(7.20)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a ‚àà
and a Œª ‚àà C such that [C1 , ŒªI ‚àí A11 ]a = 0. and therefore, in view of (7.20),

Cr

(1 ‚àí |Œª|2 )a‚àó Q‚àí1 a ‚â§ 0.
But Œª is an eigenvalue of the stable matrix A11 , implying that |Œª| < 1, so we must
have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable.
A remaining question is whether there is some balanced order-reduction procedure
in discrete time which preserves both positivity and balancing. That this is the case
in continuous time implies that the answer is aÔ¨Érmative, but the reduced system
cannot be a simple principal subsystem truncation.
Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, CÃÑ) be in stochastically
balanced form. Moreover, given a decomposition ( 2.12) such that œÉr+1 < œÉr , let
Ar
Cr
CÃÑr
Œõr0

=
=
=
=

A11 ‚àí A12 (I + A22 )‚àí1 A21
C1 ‚àí C2 (I + A22 )‚àí1 A21
CÃÑ1 ‚àí CÃÑ2 (I + A22 )‚àí1 A12
Œõ0 ‚àí C2 (I + A22 )‚àí1 CÃÑ2 ‚àí CÃÑ2 (I + A22 )‚àí1 C2

Then (Ar , Cr , CÃÑr , Œõr0 ) is a minimal realization of a strictly positive real function
1
Zr (z) = Cr (zI ‚àí Ar )‚àí1 CÃÑr + Œõr0 .
2

(7.21)

Moreover, (Ar , Cr , CÃÑr , Œõr0 ) is stochastically balanced with canonical correlation coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉr .
To understand why this reduced-order system does preserve both positivity and
balancing, note that for
Ô£π
Ô£Æ
I ‚àíA12 (I + A22 )‚àí1 0
I
0Ô£ª
T = Ô£∞0
‚àí1
I
0 ‚àíC2 (I + A22 )
we obtain

Ô£Æ
Ô£π
Œ£1 ‚àí Ar Œ£1 Ar ‚àó CÃÑr ‚àí Ar Œ£1 Cr
Ô£ª,
‚àó
‚àó
‚àó
T M (Œ£)T  = Ô£∞


CÃÑr ‚àí Cr Œ£1 Ar ‚àó Œõr0 ‚àí Cr Œ£1 Cr

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the
reduced-order system, Mr (œÉ1 ) ‚â• 0 and rank Mr (Œ£1 ) ‚â§ rank M (Œ£).
To prove Theorem 7.5 we observe that (Ar , Cr , CÃÑr , Œõr0 ) is precisely what one obtains
if one transforms (A, C, CÃÑ, Œõ0 ) by the appropriate linear fractional transform to the
continuous-time setting and then, after reduction, back to discrete time again as
suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions
The purpose of this paper is to analyze a class of popular subspace identiÔ¨Åcation
procedures for state space models in the theoretical framework of rational covariance
extension, balanced model reduction, and geometric theory for splitting subspaces.
We have pointed out that these methods are based on the hidden Assumption 2.1
which is not entirely natural and which is in general untestable.
The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially
equivalent to the Ho-Kalman algorithm in which the basic factorization is performed
by singular-value decomposition of a block Hankel matrix of Ô¨Ånite covariance data,
as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and
De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in
orthonormal coordinates and allows for theoretical veriÔ¨Åcation of the truncation step.
A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of
the data equals the algebraic degree. To achieve this, one must assume that the data
are generated exactly by an underlying system and that the amount of data is suÔ¨Écient for constructing an accurate partial covariance sequence the length of which is
suÔ¨Écient in relation to the dimension of the underlying system. Hence it is absolutely
crucial that a reliable upper bound of the dimension of the ‚Äútrue‚Äù underlying system
is available.
We stress that these stringent assumptions are not satisÔ¨Åed for generic data, as was
pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that
the positive degree has no generic value. In fact, just for the moment considering
the single-output case, for each p such that r ‚â§ p ‚â§ ŒΩ there is a nonempty open
set of partial covariance sequences having positive degree p in the space of sequences
of length ŒΩ. Secondly, for any r, it is possible to construct examples of long partial
covariance sequences having algebraic degree r but having arbitrarily large positive
degree (Theorem 2.4).
In Section 7 we proved an open question concerning the preservation of positivity
in the original (discrete-time) model reduction procedure of Desai and Pal (1984).
Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to
the principal subsystem truncation used in van Overschee and De Moor (1993), but
not to the one in Aoki (1990). We prove that positivity is preserved provided that
the original data satisÔ¨Åes Assumption 2.1, justifying setting the smaller canonical
correlation coeÔ¨Écients equal to zero. Unlike the situation in continuous time, this
truncation does not preserve balancing. The validity of the corresponding procedure
of Aoki (1990) has not been settled.
The contribution of this paper is to provide theoretical understanding of these
identiÔ¨Åcation algorithms and to point out possible pitfalls of such procedures. Hence
the primary purpose is not to suggest alternative procedures. Nevertheless, we would
like to point out that a two-stage procedure equivalent to covariance extension followed
by model reduction would work on any Ô¨Ånite string of data, thus elimination the need
for Assumptions 2.1. However, we leave open the question of how such a procedure
should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension
or some other.
Acknowledgment. We would like to thank the referees and the associate editor for
the careful review of our paper and for many useful suggestions, which have led to
considerable improvements of this paper.
References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel
operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31‚Äì73.
2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner.
3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley.
4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM
J. Control, 13, 162‚Äì173.
5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag.
6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems
and Control Letters, 2, 2301‚Äì2312.
7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization
of modelling Ô¨Ålter. Inter. J. of Control, 50, 2301‚Äì2321.
8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted
for publication.
9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization
of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40.
10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in
Kalman Ô¨Åltering. IEEE Trans. Autom. Control, 36, 563‚Äì579.
11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast Ô¨Åltering
algorithms. SIAM J. Control and Optimization, 32, 744‚Äì789.
12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and
balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105‚Äì1112.
13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE
Trans. Automatic Control, AC-29, 1097‚Äì1100.
14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821‚Äì839.
15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer.
16. Faurre, P. (1969). IdentiÔ¨Åcation par minimisation d‚Äôune representation Markovienne de processus
aleatoires. Symposium on Optimization, Nice.
17. Faurre, P. and Chataigner (1971). IdentiÔ¨Åcation en temp reel et en temp diÔ¨Äeree par factorisation
de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt.
18. Faurre, P., M. Clerget, and F. Germain (1979). OpeÃÅrateurs Rationnels Positifs, Dunod.
19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de reÃÅalisation stochastique. C. R. Academie
Sciences Paris 268.
20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York.
21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE
Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438‚Äì449.
22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and
their L‚àû error bounds. Intern. J. Control, 39, 1115‚Äì1193.
23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and
its Applications, 50, 277‚Äì319.
24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York.
25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling
of time series. Econometric Reviews 11, 379‚Äì396.
26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321‚Äì377.
27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and
approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744‚Äì746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference,
Tel Aviv, Israel.
29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory,
McGraw-Hill.
30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta
Polytech. Scand., MA31, 9‚Äì39.
31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, IdentiÔ¨Åcation
and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499‚Äì513.
32. Kung, S. Y. (1978). A new identiÔ¨Åcation and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705‚Äì714.
33. Larimore, W. E. (1990). System identiÔ¨Åcation, reduced-order Ô¨Åltering and modeling via canonical
variate analysis. Proc. 29th Conf. Decison and Control, pp. 445‚Äì451.
34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809‚Äì857.
35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear
stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241‚Äì333.
36. Lindquist, A. and G. Picci (1994a). On ‚Äúsubspace methods‚Äù identiÔ¨Åcation. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds.,
Akademie Verlag, 315‚Äì320.
37. Lindquist, A. and G. Picci (1994b). On ‚Äúsubspace methods‚Äù identiÔ¨Åcation and stochastic model
reduction. Proceedings 10th IFAC Symposium on System IdentiÔ¨Åcation, Copenhagen, June 1994,
Volume 2, 397‚Äì403.
38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE
Trans. Automatic Control, 20, 396‚Äì399.
39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica,
13, 347‚Äì357.
40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and
Optimization, 29, 1251‚Äì1287.
41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identiÔ¨Åcation
problem. Automatica, 29 , 649-660.
42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identiÔ¨Åcation of
combined deterministic-stochastic systems. Automatica, 30, 75‚Äì93.
43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identiÔ¨Åcation
algorithms and its interpretation. Proceedings 10th IFAC Symposium on System IdentiÔ¨Åcation,
Copenhagen, June 1994, Volume 2, 145‚Äì156.
44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382‚Äì387.
45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes.
Linear Algebra and its Applications, 205-206, 957-1003.
46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day.
47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II.
Journal fur die reine und angewandte Mathematik, 148, 122‚Äì145.
48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space
approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401‚Äì
421.
49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation.
IEEE Trans. Automatic Control, AC-16, 621‚Äì634.
50. Whittle, P. (1963). On the Ô¨Åtting of multivariate autoregressions and the approximate canonical
factorization of a spectral density matrix. Biometrica, 50, 129‚Äì134.
51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its
Applications, Cambridge U.P.
52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via
Ho‚Äôs algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4.
We Ô¨Årst give a proof for the special case n = 1. Consider a scalar function
1z+b
(A.1)
2z +a
with a scalar sequence (1.4) such that Œõ0 = 1. Now it is well-known [see, e.g., Schur
(1918), Akhiezer (1965)] that TŒΩ is positive deÔ¨Ånite if and only if
Z(z) =

|Œ≥t | < 1 t = 0, 1, 2, . . . , ŒΩ ‚àí 1

(A.2)

where {Œ≥0 , Œ≥1 , Œ≥2 , . . . } are the so called Schur parameters. There is a bijective relation
between partial sequences (1.1) and partial sequences {Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1 } of the same
length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the
Schur parameters of (A.1) are generated by the nonlinear dynamical system

Œ±t
Œ±0 = 12 (a + b)
Œ±t+1 = 1‚àíŒ≥
2
t
(A.3)
t Œ±t
Œ≥0 = 12 (b ‚àí a)
Œ≥t+1 = ‚àíŒ≥
1‚àíŒ≥ 2
t

and that Tt becomes singular precisely when there is Ô¨Ånite escape. It was also shown
in Byrnes et al. (1991) that {Œ±t } is generated by a linear system

 	

	 

	
2/Œ∫ ‚àí1 ut
ut+1
=
,
(A.4)
vt+1
vt
1
0
where Œ±t = vt /ut and Œ∫ := (a + b)(1 + ab)‚àí1 . If Œ∫ is greater than one in modulus, the
coeÔ¨Écient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant
scalar factor, similar to
	


cos Œ∏ sin Œ∏
,
‚àí sin Œ∏ cos Œ∏
‚àö
where Œ∏ := arctan Œ∫2 ‚àí 1. Hence Œ±t is the slope of a line through the origin in R2
which rotates counter-clockwise with the constant angle Œ∏ in each time step. Consequently,
arctan Œ±t+1 = arctan Œ±t + Œ∏.
Moreover, assuming that Œ±0 > 0, the Schur condition Œ≥t < 1 will fail as soon as Œ±t+1
negative or inÔ¨Ånite, as can be seen from the Ô¨Årst of recursions (A.3). Hence (A.2)
holds if and only if
œÄ
(A.5)
arctan Œ±ŒΩ < .
2
Therefore for a small < > 0, take‚àöa = 1 ‚àí < and b = 1 + <, yielding a stable Z. Then
2

4 ‚àí <2 . We may choose < so that
Œ∫ = 2‚àí
2 > 1 and Œ∏ = arctan 2‚àí2
œë
œë
<Œ∏< ,
ŒΩ+1
ŒΩ
where œë := œÄ2 ‚àí arctan Œ±0 . Then (A.5) holds so that TŒΩ > 0, but we also have
œÄ
arctan Œ±ŒΩ+1 >
2
so that TŒΩ+1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function
1 œàn (z) + 12 (a + b)œàn‚àí1 (z)
Z(z) =
2 œïn (z) + 12 (a + b)œïn‚àí1 (z)
where {œït } and {œàt } are the SzegoÃà polynomials of the Ô¨Årst and second kind respectively (Akhiezer, 1965). The function Z has the property that its Ô¨Årst n Schur
parameters, {Œ≥0 , Œ≥1 , . . . , Œ≥n‚àí1 }, are precisely the data which uniquely determines œïn ,
œïn‚àí1 , œàn and œàn‚àí1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in
Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated
by

Œ±0 = 12 (a + b)
Œ±t+1 = 1‚àíŒ≥Œ±2 t
t+n‚àí1

Œ≥t+1 =

‚àíŒ≥t Œ±t
2
1‚àíŒ≥t+n‚àí1

Hence, we have reduced the problem to the case n = 1. If we choose the initial
Schur parameters suÔ¨Éciently small so that œïn (z) and œïn‚àí1 (z) are approximately z n
and z n‚àí1 ,
œïn (z) + Œ±0 œïn‚àí1 (z)
is stable if we choose a := 1 ‚àí 2< and b := 1 + < for some small < > 0. Then Œ∫ > 1
and the proof for the case n = 1 carries through with a trivial modiÔ¨Åcation.
Appendix B. The Hilbert space of a sample function
Let y = {y(t)}t‚â•0 be a zero-mean wide-sense-stationary stochastic process deÔ¨Åned on
a probability space {‚Ñ¶, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t, œâ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit
exists, the m √ó m matrix function k ‚Üí Œõk obtained from a particular trajectory is
then a bona-Ô¨Åde covariance function. [The continuous-time analog of this property
was observed already by Wiener (1933)]. If moreover the sample limit is (almost
surely) independent of the particular trajectory and hence necessarily coincides with
the ‚Äùensemble‚Äù covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in
the book of Hannan (1970). It is obvious from BirkhoÔ¨Ä‚Äôs ergodic theorem that any
(zero-mean) strictly stationary ergodic process is also second-order ergodic.
In this Appendix we shall show that the properties of the Hilbert space structure
associated to a stationary time series y, deÔ¨Åned on page 10, are identical to those of
the Hilbert space induced by a second-order ergodic process.10
The two frameworks, i.e., the statistical ‚Äútime-series‚Äù structure and the ‚Äúprobabilistic‚Äù structure, are in fact isomorphic. To see this, pick a ‚Äúrepresentative‚Äù trajectory
of y, i.e. one in the subset of ‚Ñ¶ (of probability one) for which the limit (1.11) exists.
Clearly there will be no loss of generality in assuming that the probability space ‚Ñ¶ of
y is the ‚Äúsample space‚Äù, of all possible trajectories of y, i.e. the set of all semi-inÔ¨Ånite
sequences œâ = {œâ0 , œâ1 , œâ2 , . . . }, œât ‚àà Rm . With this choice, A will be the usual œÉalgebra of cylinder subsets of ‚Ñ¶ and the t:th random variable of the process, y(t), is
just the canonical projection function
y(t, œâ) : œâ ‚Üí œât .
For a process of this kind the Hilbert space H(y) is the closure in L2 (‚Ñ¶, A, P ) of the linear
vector space generated by the scalar random variables œâ ‚Üí yi (t, œâ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence
of m √ó ‚àû matrices y := {y(k)}k‚â•0 as in (3.3). For œâ in the subset of ‚Ñ¶ where the
time averages converge, deÔ¨Åne the map Tœâ ,
Tœâ : a y(t) ‚Üí a y(t) t ‚â• 0 a ‚àà Rm
associating the i:th scalar components of each m-dimensional random vector y(t)
of the process to the corresponding i:th (inÔ¨Ånite) row of the m √ó ‚àû matrix y(t)
constructed from the corresponding sample path {y(t, œâ); t ‚àà Z}. By second-order
ergodicity, the set of all such œâ ‚àà ‚Ñ¶ will have probability measure one and the map
Tœâ will in fact be norm preserving, since by construction we have
Œõt‚àís = Ey(t)y(s) = Ey(t)y(s) ,
where Œõt is the covariance matrix of y. The map Tœâ can then be extended by linearity
and continuity to a unitary linear operator Tœâ : H(y) ‚Üí H(y) which commutes with
the action of the natural shift operators (both of which we denote U), in these two
Hilbert spaces:
U

H(y) ‚àí‚ÜíH(y)
‚Üì Tœâ
Tœâ ‚Üì
U

H(y) ‚àí‚ÜíH(y)
This isomorphism allows us to employ exactly the same formalism and notations
used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991)
in the present statistical setup, where we build estimates of the parameters of models
describing the data in terms of an observed time series instead of stochastic processes.
This provides a remarkable conceptual unity and admits a straightforward derivation
in the style of stochastic realization theory of the formulas in the paper van Overschee
and De Moor (1993), there obtained with considerable eÔ¨Äort through lengthy and
formal manipulations.
Appendix C. The invariant form of the Kalman Ô¨Ålter
Given a stationary stochastic system (3.7), the Kalman Ô¨Ålter is usually determined
via the matrix Riccati equation
Q(t + 1) = AQ(t)A ‚àí [AQ(t)C  + BD ][CQ(t)C  + DD ]‚àí1 [AQ(t)C  + BD ] + BB 
(C.1)
where Q(0) = P := E{x(0)x(0) }. Here
Q(t) = E{[x(t) ‚àí xÃÇ(t)][x(t) ‚àí xÃÇ(t)] },

(C.2)

and the Kalman gain is given by
K(t) = [AQ(t)C  + BD ][CQ(t)C  + DD ]‚àí1 .

(C.3)

These equations of course depend on P , B and D, which vary as the splitting subspace
X varies over , whereas (A, C, CÃÑ) is invariant if a uniform choice of bases is made.
However, as shall see, the gain K depends only on the triplet (A, C, CÃÑ) and hence
one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

X

on (A, C, CÃÑ), and hence are invariant over . Clearly, in view of Theorem 5.1, P‚àí (t),
as deÔ¨Åned by (5.12), has this property. Moreover,
Q(t) = P ‚àí P‚àí (t),
and, consequently, in view of (3.9), and the Lyapunov equation
P = AP A + BB  ,
P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11).
A symmetric argument yields the backward equations.
It is easy to see that as Q(t) ‚Üí Q‚àû monotonously, P‚àí (t) ‚Üí P‚àí , and hence P ‚â• P‚àí ,
as should be.
Appendix D. Some deferred proofs
Proof of Theorem 5.1. Since X is a splitting subspace for the inÔ¨Ånite past H ‚àí and the
inÔ¨Ånite future H + , by stationarity, XœÑ splits HœÑ‚àí := U œÑ H ‚àí and HœÑ+ := U œÑ H + . But
YœÑ‚àí ‚äÇ HœÑ‚àí and YœÑ+ ‚äÇ HœÑ+ , and hence XœÑ splits YœÑ‚àí and YœÑ+ also. (See, e.g., Lindquist
and Picci (1985, 1991).) Now, using the projection formula in the footnote of page
16, we have for any b yœÑ+ ‚àà YœÑ+
Ô£Æ
Ô£πÔ£Æ
Ô£π‚àí1
ŒõœÑ
Œõ1 Œõ2 . . .
Œõ0 Œõ1 . . . ŒõœÑ
Ô£ØŒõ2 Œõ3 . . . ŒõœÑ +1 Ô£∫ Ô£ØŒõ1 Œõ0 . . . ŒõœÑ ‚àí1 Ô£∫
‚àí
Ô£Ø .
E YœÑ b yœÑ+ = b Ô£Ø
yœÑ‚àí
..
.. Ô£∫
..
.. Ô£∫
..
..
Ô£∞ ...
Ô£ª
Ô£∞
Ô£ª
.
.
.
.
.
.
.
.
ŒõœÑ ŒõœÑ +1 ¬∑ ¬∑ ¬∑ Œõ2œÑ ‚àí1
ŒõœÑ ŒõœÑ ‚àí1 ¬∑ ¬∑ ¬∑ Œõ0
= b ‚Ñ¶œÑ ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí
= b  ‚Ñ¶œÑ Œæ
where ‚Ñ¶œÑ and ‚Ñ¶ÃÑœÑ are appropriate Ô¨Ånite-dimensional observability and constructibility
matrices (2.6) of full rank. If œÑ > œÑ0 , there is a minimal factorization H = ‚Ñ¶œÑ ‚Ñ¶ÃÑœÑ such
that Œæ := ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí has n components, and
E{ŒæŒæ  } = ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 ‚Ñ¶ÃÑœÑ > 0.
Therefore, since the components of Œæ belong to XÃÇœÑ ‚àí , dim XÃÇœÑ ‚àí ‚â• n = dim XœÑ so, since
XÃÇœÑ ‚àí is minimal, XœÑ must also be minimal and XÃÇœÑ ‚àí be spanned by the components
of Œæ.
Next, from the backward system (3.14) we see that
yœÑ‚àí = ‚Ñ¶ÃÑœÑ xÃÑ(œÑ ) + terms ortogonal to XœÑ ,
and therefore, by the same projection formula,
‚àí

E YœÑ a x(œÑ ) = a E{x(œÑ )xÃÑ(œÑ ) }‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí = a Œæ.
Consequently, E YœÑ XœÑ = {a Œæ | a ‚àà Rn } = XÃÇœÑ ‚àí , establishing the Ô¨Årst of identities
(5.7). The second follows from a symmetric argument.
The representation formula (5.8) follows from the minimality of XœÑ as a splitting
subspace for YœÑ+ and YœÑ‚àí , which, in particular, implies that the constructibility operator,
YœÑ‚àí
: XœÑ ‚Üí XÃÇœÑ ‚àí
Ct := E|X
œÑ
‚àí

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n,
there is a unique random variable xk (œÑ ) ‚àà XœÑ whose projection onto YœÑ‚àí is xÃÇk (œÑ ).
To show that x(0) form a uniform choice of bases as X varies over , Ô¨Årst take X
to be the stationary backward predictor space X+ and let x+ (œÑ ) be the unique basis
‚àí
be arbitrary. Then, since
in UœÑ X+ such that xÃÇ(œÑ ) = E YœÑ x+ (œÑ ). Now, let X ‚àà
‚àí
œÑ
œÑ
+
XœÑ is a splitting subspace for YœÑ and U X+ ‚äÇ U H (Lindquist and Picci, 1991,
Proposition 2.1(vi)), we have

X

X

‚àí

‚àí

xÃÇ(œÑ ) = E YœÑ x+ (œÑ ) = E YœÑ E XœÑ x+ (œÑ ),
and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all
X ‚àà , which is a well-known characterization of uniform choice of bases; see Section
6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields
the corresponding statement for (5.9).

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a
corresponding partial covariance sequence , where ŒΩ is large enough for the Hankel
matrix H, deÔ¨Åned by (1.5), to have rank n. Let (A, C, CÃÑ) be the triplet determined
from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the
covariance data by estimates
{Œõ0T , Œõ1T , . . . , ŒõŒΩT }
of type (6.3), and let (AT , CT , CÃÑT ) be the corresponding triplet obtained via (2.5).
We want to prove that
1
ZT (z) := CT (zI ‚àí AT )‚àí1 CÃÑT + Œõ0T
2
is
strictly
positive
real
for
a
suÔ¨Éciently
large
T
.
Now,
if
	 ‚àí1 deg

 ZT = deg Z, replace Œ£ by
	






Œ£
0
Œ£ 0
in (2.5) in the appropriate
, U by U 0 , V by V 0 , and Œ£‚àí1 by
0 0
0 0
calculation so that (A, C, CÃÑ) and (AT , CT , CÃÑT ) have the same dimensions. This will
not aÔ¨Äect Z and ZT . By continuity, (AT , CT , CÃÑT , Œõ0T ) can be made arbitrarily close
to (A, C, CÃÑ, Œõ0 ) in any norm by choosing T suÔ¨Éciently large. Thus the same holds
for
max !Z(eiŒ∏ ) ‚àí ZT (eiŒ∏ )!
Œ∏‚àà[0,2œÄ]

and hence, since Œ¶(z) := Z(z) + Z(z ‚àí1 ) satisÔ¨Åes (3.10), so will Œ¶T (z) := ZT (z) +
ZT (z ‚àí1 ) for suÔ¨Éciently large T . Moreover, since |Œª(A)| < 1, we have |Œª(AT )| < 1 by
continuity for suÔ¨Éciently large T . Consequently, there is a T0 such that ZT is strictly
positive real for T ‚â• T0 .
Proof of Theorem 5.3. Let Z, deÔ¨Åned by (1.6), be strictly positive real, and let (A, C, CÃÑ)
be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , deÔ¨Åned by
(7.15) in terms of the principal subsystem truncation (A11 , C1 , CÃÑ1 ), is also strictly
positive real. We want to prove that this property is carried over to rational matrix
function
1
ZœÑ 1 (z) = (CœÑ )1 (zI ‚àí (AœÑ )11 )‚àí1 (CÃÑœÑ )1 ) + Œõ0
2
for œÑ suÔ¨Éciently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let QœÑ be deÔ¨Åned by (5.32). Since the canonical correlation coeÔ¨Écients (5.25) tend to the canonical correlation coeÔ¨Écients (4.12) as œÑ ‚Üí ‚àû, Œ£œÑ ‚Üí Œ£.
Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P‚àí (t)
tends to QœÑ Œ£QœÑ as t ‚Üí ‚àû if the initial condition is taken to be P‚àí (œÑ ) = Œ£œÑ . Consequently, for any < > 0, there is a suÔ¨Éciently large œÑ such that !Œ£œÑ ‚àí Œ£! < 2 and
!Œ£œÑ ‚àí QœÑ Œ£QœÑ ! < 2 so that !Œ£ ‚àí QœÑ Œ£QœÑ ! < <. Hence QœÑ tends to a limit Q‚àû with
the property Œ£ = Q‚àû Œ£Q‚àû . Using the same argument in the backward direction, the
‚àí1
second of relations (5.33) shows that Q‚àû also satisÔ¨Åes Œ£ = Q‚àíT
‚àû Œ£Q‚àû . Consequently,
by the same argument as in the proof of Theorem 4.4, Q‚àû is a signature matrix, and
hence in particular diagonal. Therefore,
‚àí1

((AœÑ )11 , (CœÑ )1 , (CÃÑœÑ )1 ) ‚Üí ((Q‚àû )11 A(Q‚àû )‚àí1
11 , C(Q‚àû )11 , CÃÑ(Q‚àû )11 ) as œÑ ‚Üí ‚àû,

where (Q‚àû )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, ZœÑ 1 ‚Üí Z1 . Hence, since Z1 is positive real, then so is ZœÑ 1 for
œÑ suÔ¨Éciently large.
Proof of Lemma 7.4. Let us Ô¨Årst consider the case when (A, C, CÃÑ) is a minimal triplet.
Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P‚àí and P+ respectively, which, in
particular, have the property that P‚àí ‚â§ P1 and P2 ‚â§ P+ . Then, in view of (7.18),
P+ ‚àí P‚àí > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem
A4.4).
Next, let us reduce the general case to the case considered above. If (C, A) is
not observable, change the coordinates in state space, through a transformation
(A, C, CÃÑ) ‚Üí (QAQ‚àí1 , CQ‚àí1 , QCÃÑ  ), so that
	






AÃÇ 0
ÀÜ ‚àó ,
A=
C = CÃÇ 0
CÃÑ = CÃÑ
‚àó ‚àó
where (CÃÇ, AÃÇ) is observable. Then, if P1 and P2 have the corresponding representations
	
	




PÃÇ1 ‚àó
PÃÇ2 ‚àó
P2 =
,
P1 =
‚àó ‚àó
‚àó ‚àó
it is easy to see that PÃÇ1 and PÃÇ2 satisfy the reduced version of the linear matrix
ÀÜ and that, in this new
inequality (6.6) obtained by exchanging (A, C, CÃÑ) for (AÃÇ, CÃÇ, CÃÑ)
ÀÜ , AÃÇ ) is not observable, we proceed
setting, (7.18) holds, i.e., PÃÇ2 ‚àí PÃÇ1 > 0. If (CÃÑ
by removing these unobservable modes. First note that PÃÇ1‚àí1 and PÃÇ2‚àí1 satisfy the
ÀÜ by (AÃÇ , CÃÑ
ÀÜ , CÃÇ). Then,
dual linear matrix inequality obtained by exchanging (AÃÇ, CÃÇ, CÃÑ)
changing coordinates in state space so that
	  





AÃÉ 0

ÀÜ
Àú
AÃÇ =
CÃÑ = CÃÑ ‚àó
CÃÇ = CÃÉ 0 ,
‚àó ‚àó
ÀÜ , AÃÉ ) observable, and deÔ¨Åning
with (CÃÑ
	 ‚àí1 

PÃÉ
‚àó
‚àí1
PÃÇ1 = 1
‚àó ‚àó

PÃÇ2‚àí1

	 ‚àí1 

PÃÉ
‚àó
= 2
,
‚àó ‚àó

44

ANDERS LINDQUIST AND GIORGIO PICCI

Àú , 1 Œõ ) is a minimal realization of Z. Moreover, PÃÉ and PÃÉ satisfy
we see that (AÃÉ, CÃÉ, CÃÑ
1
2
2 0
the corresponding linear matrix inequality (6.6) and have the property (7.18) in this
setting. Hence the problem is reduced to the case already studied above.
Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans, mapping
formed to the continuous-time setting via a bilinear transformation s = z‚àí1
z+1
the unit disc onto the left half plane so that


1+s
(D.1)
Zc (s) = Zd
1‚àís
is positive real in the continuous-time sense if and only if Zd is positive real in the
discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al.
(1979)] that, if (Ad , Cd , CÃÑd , 12 Œõ0 ) and (Ac , Cc , CÃÑc , 12 R) are realizations of Zd and Zc
respectively, we have
Ô£±
Ac = (Ad + I)‚àí1 (Ad ‚àí I)
Ô£¥
Ô£¥
Ô£¥
Ô£≤C = ‚àö2C (A + I)‚àí1
c
‚àö d d
(D.2)
Ô£¥
CÃÑc = 2CÃÑd (Ad + I)‚àí1
Ô£¥
Ô£¥
Ô£≥
R = Œõ0 ‚àí Cd (Ad + I)‚àí1 CÃÑd ‚àí CÃÑd (Ad + I)‚àí1 Cd
and inversely

Ô£±
Ad = (I ‚àí Ac )‚àí1 (I + Ac )
Ô£¥
Ô£¥
Ô£¥
Ô£≤C = ‚àö2C (I ‚àí A )‚àí1
d
c
‚àö c
 ‚àí1
Ô£¥
=
2
CÃÑ
(I
‚àí
A
CÃÑ
d
c
Ô£¥
c)
Ô£¥
Ô£≥
Œõ0 = R + Cc (I ‚àí Ac )‚àí1 CÃÑc + CÃÑc (I ‚àí Ac )‚àí1 Cc

(D.3)

Under this transformation the observability gramian and the constructibility gramian
(i.e., the observability gramian of (CÃÑ, A )) are preserved so that (Ad , Cd , CÃÑd , 12 Œõ0 ) is
a minimal realization if and only if (Ac , Cc , CÃÑc , 12 R) is; see, e.g., p. 1119 in Glover
(1984). Moreover, coercivity is preserved, and the solution sets of the corresponding
linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the
reachability gramian of a spectral factor and this gramian is also preserved.)
Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact,
transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting,
all the requirements of Theorem 7.1 are satisÔ¨Åed. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order
positive real function thus obtained via (D.3) back to discrete time, the desired result
is obtained.

1688

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Universal Regulators for Optimal Tracking
in Discrete-Time Systems Affected
by Harmonic Disturbances
Anders Lindquist, Fellow, IEEE, and Vladimir A. Yakubovich, Member, IEEE

Abstract‚Äî The authors consider the problem of controlling a
discrete-time linear system by output feedback so as to have a
second output z t track an observed reference signal r t . First, as a
preliminary, we consider the problem of asymptotic tracking, i.e.,
to design a regulator such that jz t 0 rt j ! 0. This problem has
been studied intensely in the literature, mainly in the continuoustime case. It is known that only under very special conditions
does there exist a linear regulator which achieves this design
goal and which is universal in the sense that it works for all
reference signals and does not depend on them. On the other
hand, if rt is a harmonic signal with known frequencies but
with unknown amplitudes and phases, there exist such regulators
under mild conditions, provided the dimension of rt is no larger
than the number of controls. This is true even if the plant itself
is corrupted by an unobserved additive harmonic disturbance wt
of the same type as rt , if the dimension of wt is no larger than
the number of outputs available for feedback control.
However, if the first dimensionality condition is not satisfied,
asymptotic tracking is not possible, but a steady-state tracking
error remains. Therefore, the authors turn to another approach
to the tracking problem, which also allows for damping of
other system and control variables, and this is our main result.
The measure of performance is given by a natural quadratic
cost function. The object is to design an optimal regulator
which is universal in the sense that it does not depend on the
unknown amplitudes and phases of rt and wt and is optimal
for all choices of rt and wt . The authors prove that an optimal
universal regulator exists in a wide class of stabilizing and
possibly nonlinear regulators under natural technical conditions
and that this regulator is in fact linear, provided that the second
dimensionality condition above is satisfied. On the other hand, if
it is not satisfied, the existence of an optimal universal regulator is
not a generic property, so as a rule no optimal universal regulator
exists.
The authors provide complete solutions of all the problems
described above.
Index Terms‚ÄîInternal model principle, optimal tracking, optimal universal regulators, sinusoidal disturbance.

Fig. 1. Feedback configuration.

(1b)
(1c)
, two vector outputs
and
,
with a state
and an
and two vector inputs, namely a control
which we shall take to be
unobserved disturbance
harmonic with known frequencies but unknown amplitudes
and phases. More precisely
(2)
where the frequencies
(3)
,
,
are known, but the complex vector amplitudes
,
, in which the phases have been absorbed, are either
completely unknown or zero. Consequently, some frequencies
and have been included for
(3) may not be represented in
notational purposes to be explained shortly.
In this paper we consider the problem to control the system
(1) by feedback from the output so as to have the output
track an observed -dimensional real reference signal

I. INTRODUCTION

C

(4)

ONSIDER a discrete-time linear control system
(1a)

Manuscript received January 4, 1996; revised February 1, 1997 and April
6, 1998. Recommended by Associate Editor, S. Weiland. This work was
supported in part by grants from NFR, INTAS, and NUTEK.
A. Lindquist is with the Division of Optimization and Systems Theory,
Royal Institute of Technology, 100 44 Stockholm, Sweden.
V. A. Yakubovich is with the Department of Mathematics and Mechanics,
St. Petersburg University, St. Petersburg 198904, Russia.
Publisher Item Identifier S 0018-9286(99)07132-9.

which is harmonic with the known frequencies (3) but with
,
,
,
which are
complex vector amplitudes
either completely unknown or zero so that certain frequencies
(3) may not occur in . The feedback configuration of this
problem is described in the flow diagram as shown in Fig. 1.
Many important engineering problems could be modeled in
this way. Some examples are connected to industrial machines
and helicopters [2], [9]‚Äì[12], [27], [28], control of aircraft in

0018‚Äì9286/99$10.00 Ô£© 1999 IEEE

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

the presence of wind shear [19], [23], [31], and control of the
roll motion of a ship [14].
For notational convenience we use a common set of freand , forcing us to set certain complex
quencies (3) for
vector amplitudes equal to zero. To formalize this we introduce
,
of for which
the index sets
and
, respectively, are nonzero and arbitrary. Then
and

(5)

Without loss of generality we assume that

Accordingly, we define the class
of disturbances and the
and ,
class of reference signals consisting of all signals
and
respectively, obtained by letting
vary arbitrarily subject to the constraint that the signals (5)
are real.
, and
are constant
We assume that , , , ,
real matrices of appropriate dimensions such that
is stabilizable and
is detectable. Without loss of
generality we may also assume that
and

(6)

In fact, if the first condition is not satisfied, some components
of could be eliminated. Moreover, if has linearly dependent columns, these could be combined without restriction.
and
.
Clearly, (6) implies that
Now, a possible criterion of performance for the tracking
problem described above is given by
(7)
but, to allow for damping of internal system variables and
the energy of control, we shall also consider a more general
criterion of the type
(8)
where

is a real quadratic form
(9)

with properties to be specified in Section V. [To ensure that the
, we must of course introduce some
infimum of is not
condition on the quadratic form (9).] We note that the second
functional (8) becomes a measure not only of the tracking
accuracy but also of the forced oscillations in the closed-loop
system. For the classes of admissible regulators to be defined
next, these cost functions do not depend on initial conditions.
, a regulator
The object is to find, for suitable ,

1689

satisfies the weak stability condition
as

2) optimal in the sense that the cost function (8) is
minimized;
3) universal in the sense that it simultaneously solves the
complete family of optimization problems corresponding to different values of the complex vector amplitudes
and
and thus does not depend
on these amplitudes.
Such a regulator will be referred to as an optimal universal
regulator (OUR), and the class of regulators (10) satisfying
conditions 1) and 2) will be denoted . The stability condition
(11) may at first sight seem somewhat unnatural, but, as we
shall see in Section VI, it is the natural mathematical condition
for which statements of necessity
defining the largest class
and sufficiency can be made.
Removing the last term of (8) related to tracking we obtain
some special cases of this problem which were studied in [21]
and in [22] for the cases of complete and incomplete state
information, respectively.
In this paper we show that, under suitable technical con, the problem stated above has
ditions and provided
a solution in , and this solution happens to be a linear
stabilizing regulator of type
(12)
is the backward shift
and
,
where
, and
are real matrix polynomials, of dimensions
,
, and
, respectively, with the property that
and
and
are proper rational
functions so that the regulator is nonanticipatory in the sense
does not depend on future values of
and , in
that
the subclass of
harmony with (10). We shall denote by
such linear regulators. Existence of an OUR in the subclass
itself can be established under somewhat milder technical
is important.
conditions. The dimensionality condition
As in [22], it can be shown that if it fails, then the existence of
an optimal universal regulator becomes a nongeneric property.
It means that no optimal universal regulator exists from a
.
practical point of view if
The cost function (7) would of course be minimized if we
could control (1a) so that
as

(13)

In fact, it would be zero. Therefore, asymptotic tracking
appears as a special case in our analysis. This problem has been
studied intensely in the literature, at least in the continuoustime case; see, e.g., [1], [4]‚Äì[8], [13], [16], and the references
therein. The connection to this earlier work, developed in
continuous time, is made evident by noting that the disturbance
and reference signals (5) can be modeled as the output of a
critically stable system

(10)
which is:
sat1) stabilizing in the sense that any process
isfying the closed-loop system equations (1), (10) also

(11)

with

having all its eigenvalues on the unit circle.

1690

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Therefore, we begin by developing our optimization procedure in this well-known setting of asymptotic tracking,
thereby obtaining alternative formulations in the discrete-time
case. Using a very short and simple proof, we are able
to give a complete solution to the problem of finding all
universal tracking regulators, i.e., all regulators which achieve
asymptotic tracking (13) for all values of the complex vector
and
and which do not
amplitudes
depend on these amplitudes. This will be done in Section IV.
As a preliminary for this, and to set up notations, in Section III
), and we
we first consider an undisturbed system (
characterize all regulators (12) achieving the design objective
(13) for all reference signals , not only harmonic ones, and
all initial conditions; we shall refer to this property as Tuniversal. The solution of this problem is certainly known,
but we include it for conceptual reasons.
, i.e., the dimension of
is larger than
However, if
the number of outputs available for feedback, no universal
tracking regulator exists, so a nonzero tracking error remains.
To damp this error we turn to our main problem, namely to
characterize all optimal universal regulators, as defined above.
Also, we may want to use a criterion (8) even if asymptotic
tracing is possible, if it is desirable to damp the control energy
and/or some particular internal system variables. This is the
topic of Section V, where optimality in the linear class is
studied. In Section VI we show that these linear universal
regulators are optimal also in the wider class of nonlinear
regulators satisfying (11), provided slightly stronger technical
conditions are satisfied. The complete solution is given. We
note that a similar but different optimization problem, over a
finite horizon, is considered in [26].
Obviously, there is no a priori guarantee that a regulator
which minimizes (8) will also satisfy other design specifications, and hence we look for complete solutions with many
free parameters which then can be tuned by loop shaping. In
fact, all our results are based on a parameterization derived
in Section II, which is akin to that of Youla and KucÃÜera
and which generalizes some parameterizations previously presented in [21] and [22].
Finally, in Section VII, we give some simple numerical
examples.
II. LINEAR STABILIZING AND REALIZABLE REGULATORS

(15b)
(15c)
so, in particular,
(16)
where

is the

matrix polynomial
(17)
,

Similarly, the transfer functions
respectively, are given by

from

to

which stabilize the control system (1) and which are realizable
in a sense to be defined shortly. As before, is the backward
, and
,
, and
are real matrix
shift
,
, and
, respectively.
polynomials of dimensions
Let us consider a bit closer the meaning of (14) being
,
,
stabilizing. To this end, note that the transfer functions
from
to , , and , respectively, in the closed-loop
system (1), (14) satisfy

which together with (16) yields
(19)
We shall say that the regulator (14) is stabilizing if the matrix
is stable, i.e.,
for
.
polynomial
Next we consider the condition that the regulator be realizable. Clearly (14) must be nonanticipatory in the sense that
does not depend on future values of
and . To ensure
this, we must assume that
and

are proper

(20)

.
requiring in particular that
Let us investigate what properties must have for (20) to
be satisfied. To this end, let us introduce the rational transfer
functions

(21)
from the control signal to the outputs
Then it is easy to see that

and

, respectively.

(22)
and that

(23)
Writing (22) in the alternative form

we see that (20) implies that
is strictly proper and
is proper. In fact,
is strictly proper, making
as well as its inverse proper. Then, it follows from
and
are strictly proper also. Consequently
(23) that
where

(15a)

,

(18)

In order to design universal regulators we need a parameterization of all linear regulators
(14)

and

is finite

(24)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

so that
and
depend on
for
only and on
for
only. We shall say that the regulator (14) is realizable
if condition (24) is satisfied. At the end of this section we
shall demonstrate that any stabilizing and realizable regulator
satisfies (20) so that the nonanticipatory property is implied
(Corollary 2.3).
We say that two regulators

and

are equivalent if there are stable
and
such that

matrix polynomials
(25)

Hence we allow the systems matrices , , and to have
stable common factors, as coprimeness is not required. Clearly,
,
,
, and
as can be seen from (22) and (23),
are invariant under this equivalence and so are the regulator
transfer functions (20).
is a stable matrix, i.e.,
From now on, we assume that
for all
. Since
is stabilizable
is detectable, this is no restriction. In fact, it is
and
well-known that the system (1) can be replaced by a similar
system having a stable -matrix but, in general, a larger
dimension. (See any standard text, such as [1] and [18].) Only
under special conditions [15], including the case of complete
state observation, is it possible to do this by constant feedback,
but the system can always be stabilized by a dynamic observer.
Then, extending the state space by including this observer, a
system with stable -matrix is obtained. For these reasons we
shall from now on, without loss of generality, assume that
in (1) is a stable matrix.
The following theorem, generalizing a similar result in
[22], provides a parameterization akin to the well-known
Youla‚ÄìKucÃÜera parameterization. (We note that if is not stable, also the latter parameterization requires an observer-based
prestabilization, increasing the dimension of the regulator; see,
e.g., [32, p. 226].)
be a stable matrix with
Theorem 2.1: Let
being its characteristic polynomial, and let
and
be the matrix polynomials
(26)
be an arbitrary stable scalar polynomial
Moreover, let
and
be arbitrary matrix polynomials of
and let
and
, respectively, such that
dimensions

1691

is stabilizing and realizable, and for this regulator
(30)
and
(31)
is given by (17). Conversely, any stabilizing and
where
realizable regulator (28) is equivalent to one constructed in
this way.
Before turning to the proof of this parameterization, let us
is a
briefly explain the nature of relation (31). Although
for the regulator defined via (29), this is in
factor in
general not the case for an arbitrary regulator belonging the
same equivalence class. In fact, while the closed-loop transfer
and the regulator transfer functions
and
function
are invariant under the equivalence (25),
is not.
Taking the Schur complement, it immediately follows from
(17) that
(32)
is given by (21). Since, in general, the second factor
where
in
is not a polynomial, is of course not a factor in
general. Nevertheless, it will turn out to be useful to represent
each equivalence class by a regulator that has this property.
Proof of Theorem 2.1: In view of (29), we have
(33)
and consequently (30) follows from (22) and (31) follows from
is a stable matrix poly(32). By construction, therefore,
nomial, establishing that the regulator is stabilizing. Moreover,
is strictly proper and
is proper, i.e.,
in view of (27),
and
is finite. It then follows from (23)
and
are strictly proper, and hence the regulator
that
is realizable.
,
,
To prove the converse statement, suppose that
is an arbitrary stabilizing and realizable regulator. Then (32)
may be written

where

is the

matrix polynomial
(34)

which is stable and full rank, since
stable and nontrivial. It follows from (22) that

is
(35)

(27)

and
are the closed-loop transfer functions
where
, ,
. Therefore, setting
corresponding to the regulator

(28)

and

(29)

where
, (35) shows that

Then the regulator

with
is the adjoint matrix polynomial of
and
are given by (30). Since
,

1692

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

,
is a realizable regulator, it follows from (24) that the
and
degree conditions (27) hold. Consequently, defining
via (29), it follows from the first part of the theorem that
, ,
is a stabilizing and realizable regulator with the
and
as
,
,
same closed-loop transfer functions
. It remains to show that
, ,
and
,
,
are
equivalent. To this end, note that

Also it follows from (34) that

Consequently

i.e.,

and

are equivalent as required.

If
so that
, the representation of stabilizing
regulators can be simplified considerably, since and can be
chosen so that cancellations occur. Since this formulation has a
different form and, moreover, will be used later, we state it as
a corollary. Note that, in view of the converse statement, this
corollary is strictly speaking not a special case of Theorem 2.1.
It is in fact a generalization of [21, Lemma 4.3], but the proof
here is new.
Corollary 2.2: Let be a stable matrix, and suppose that
. Let
be an arbitrary real scalar stable polynomial,
and
be arbitrary real matrix polynomials,
and let
and
, respectively, such that
of dimensions
(36)
Then the regulator
(37)

Conversely, by Theorem 2.1, any stabilizing and realizable
regulator (37) is equivalent to some regulator
of
the type described in Theorem 2.1, where we set
everywhere. It remains to show that
is also a
regulator of the type described in the corollary. To this end,
. This implies that
,
define
and hence the equations of Theorem 2.1 become those of the
replaced by . Hence
is also a
corollary with
regulator in the sense of the corollary.
In the beginning of this section we demonstrated that the
realizability condition (24) is a consequence of nonanticipatory
condition (20). Next we show that the converse is also true,
has full rank as assumed in (6).
provided
. Then, for any
Corollary 2.3: Suppose that
stabilizing regulator (28), the realizability condition (24) and
the nonanticipatory condition (20) are equivalent.
Proof: The proof is immediate in the special case
. In fact, for a regulator (37) with
and
given by (38),
condition (20) is a direct consequence of the degree condition
(36). For any other stabilizing regulator (37), it follows from
the definition of equivalence.
The general case follows from the fact that (28) is a subclass
of (37). In fact, writing (28) as

it follows from what has already been proved that
is proper. Since
has full rank, this implies that
is proper follows directly.
proper. That
III.

is

-UNIVERSAL REGULATORS

As a preliminary for the analysis in Sections IV and V,
in this section we consider the problem of controlling the
undisturbed system
(40a)
(40b)
(40c)

with
(38)
is stabilizing and realizable, and, for this regulator
(39)
satisfies (31). Conversely, any stabilizing and
and
realizable regulator (37) is equivalent to one constructed in
this way.
Proof: Let the polynomials and be chosen as in the
and
statement of the corollary, and take
to be the corresponding polynomials
and
in Theorem 2.1. Then, since
, the degree conditions (27) are satisfied for
and .
Moreover, the corresponding regulator polynomials matrices
and
, become
(29), which we denote
and
, where
and
are given by (38). Then,
, the regulator
,
,
is stabilizing and
setting
realizable by Theorem 2.1. Thanks to cancellation, therefore,
is a stabilizing and realizable regulator for the
problem of Corollary 2.2, as claimed.

so that it tracks a given
by feedback from the output
in the sense that
reference signal
as

(41)

As explained in Section II it is no restriction to assume that
is stable if it is assumed that
is stabilizable and
is detectable. The solution of this problem is simple
and certainly known, but we include it for completeness and
for conceptual reasons.
More precisely, we want to find a stabilizing and realizable
regulator of the form
(42)
which is universal for the asymptotic tracking problem in the
sense that (41) holds for all solutions of (40), (42), and all
reference signals . More specifically we shall refer to this
property as T-universal.
Clearly, for (42) to be stabilizing and realizable, the matrix
,
, and
must satisfy the specifipolynomials
cations of Theorem 2.1. It remains to investigate under what

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

conditions the tracking criterion (41) is satisfied and under
what conditions this regulator is T-universal.
We begin by deriving a necessary condition for Tuniversality. Consider a reference signal of the type
(43)
and
are fixed but arbitrary. Then the
where
closed-loop system (40), (42) has solutions

1693

delays between
and . Indeed, the condition (48) for Tuniversality imposes some rather stringent conditions on the
is
and
is
,
system (40). In particular, since
, and
must have full rank.
(48) implies that
Theorem 3.2: Suppose that is stable. Then there exists a
T-universal regulator for the tracking problem if and only if
matrix function
with no
there is a proper rational
which satisfies the equation
poles in the region
(49)

(44)
with

which, in particular, implies that
In this case, let be a stable scalar polynomial such that

.
(50)

(45)
,
where
are defined by (21). Moreover,

, and

and
(46)

But the tracking condition (41) requires that
as
and, since is arbitrary, this implies that
follows from (44) and (46) that

. Therefore, it
(47)

Now, in order for the regulator (42) be T-universal, (47) must
hold for all , that is, for all and . Consequently, we must
have
(48)
on the unit circle and, by analytic continuation, in the rest of
the complex plane.
Lemma 3.1: A stabilizing and realizable regulator (42) is
T-universal if and only if the identity (48) holds.
Proof: We have already proved that (48) is a necessary
condition for (42) to be T-universal, so it remains to prove
that it is also sufficient. To this end, first assume that there are
such that
for all . Then
positive numbers ,
has a -transform

which converges for
. It follows from (45) and
is the transfer function from
to ,
(46) that
with a -transform
and hence (40), (42) has a solution
. But, if (48) holds, then
and hence
for all . Because of stability any other solution
tends asymptotically to this solution, and therefore (41) holds.
If increases so fast that it does not have a -transform, set
for
and
for
, and
be the corresponding -solution. Then it is easy to see
let
for
. Since
is arbitrary, the
that
conclusion follows.
As a corollary we see that
must be full rank, or
else (48) will be violated. This implies that there are no

be a
matrix
is a matrix polynomial, and let
polynomial satisfying the first degree constraint (27). Then, the
and
given by (29), is a T-universal
regulator (28), with
regulator for the tracking problem, and any other T-universal
regulator is equivalent to one obtained in this way.
Proof: First, suppose that there exists a T-universal regulator of the form (42). Then, according to Lemma 3.1, there
to (49) with the prescribed properties,
exists a solution
. In fact, in view of (22), (32) and the fact that
namely
is stable, it follows that
has no poles in the region
. Moreover, since the regulator is realizable,
is
proper.
which is proper
Next, suppose that (49) has a solution
, and let , , and be
with no poles in the region
defined as in the theorem. [Note that in order to satisfy the
first of degree conditions (27) we may need to choose and
which are not coprime.] Then, by Theorem 2.1, the regulator
given by (29) is stabilizing and realizable and
(28) with
(51)
. Consequently, it follows from
i.e., in view of (50),
(49) and Lemma 3.1 that the regulator is T-universal.
It remains to prove the last statement of the theorem. To
this end, suppose that the regulator
(52)
is T-universal. Then, in particular, it is stabilizing and realizable, and thus, by Theorem 2.1, there are some , , and
with the properties specified in Theorem 2.1 such that the
given by (29) is equivalent to (52).
regulator (28) with
is invariant under this equivalence. Therefore, since
Now,
(48) holds for the regulator (52) by Lemma 3.1, (48) also holds
for (28). However, by Theorem 2.1, (51) holds, and hence there
, satisfying (49) and (50).
is an , namely
In general, a solution to (49) cannot be expected to be
, only one solution is possible, namely
unique, but if

and this would require that
is a stable, proper rational
must be minimum phase with
function, implying that
must be
no zeros at infinity. In particular,
nonsingular.

1694

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Corollary 3.3: Suppose that
is stable and the transfer
is square, i.e.,
.
function
Then there is a T-universal regulator for the tracking problem
is proper with no poles in the region
if and only if
. In this case, let
be a stable scalar polynomial
is a matrix polynomial and
is
such that
matrix polynomial satisfying the degree requirement
a
and
are defined by (29) and by
(27). Then, if
(53)
the regulator (28) is a T-universal regulator, and any other
T-universal regulator is equivalent to one obtained in this way.
A T-universal regulator exists only under rather special
conditions. However, if we restrict our attention to harmonic
reference signals (4), these conditions can be considerably
relaxed and we may also allow for external harmonic disturbances. This is the topic of the next section.

respectively, satisfying the degree requirements (27) and the
interpolation conditions
for
for

(59b)

and
are given by (29), the regulator (28) is a
Then, if
universal tracking regulator, and any other universal tracking
regulator (28) is equivalent to one obtained in this way.
Proof: Whenever a linear stabilizing regulator is applied
tends exponentially to the
to system (1), the process
harmonic steady-state solution
(60)
where
(61a)

IV. UNIVERSAL TRACKING REGULATORS
IN HARMONICALLY DISTURBED SYSTEMS

(61b)

We now return to the situation described in Section I,
where the control system takes the form (1) with a harmonic
disturbance (2) and where there is a harmonic reference signal
to be empty, for
(4). Although we may allow the index set
.
tracking we must take
The first question to be answered is when it is possible to
find a regulator (12) in such that
as

(59a)

,
,
, and
being the closed-loop transfer functions
,
defined in Section II. In fact, for any regulator in ,
defined by (17), is a stable matrix polynomial. In the same
tends exponentially to
way, in view of (1c),

(54)

(62)

which is universal in the sense that (54) holds for all values
and
and does not depend on these
of
vector amplitudes. We shall refer to such a regulator as a
universal tracking regulator. For convenience, in the sequel
we use the notation

Now, the basic idea is that the tracking condition (54) is
achieved precisely when the cost function (7) is zero. It is
easy to see that
(63)

(55)
is stable, and let
Theorem 4.1: Suppose that the matrix
and
be the matrix polynomials defined by (26).
be the
matrix function defined by
Moreover, let
the
matrix polynomial
(21) and

To see this, observe that if
sequences

and

are two harmonic vector

and

(56)
Then, for a universal tracking regulator to exist in
necessary that the rank condition
for all

, it is

with
distinct as in (3), and
appropriate dimensions, then

is an arbitrary matrix of

(57)

holds, and it is sufficient that both rank conditions (57) and
for all

(64)

(58)

, and
hold. In particular, (57) requires that
. More precisely, let
be
(58) that
and
an arbitrary stable scalar real polynomial, and let
be matrix polynomials, of dimensions
and
,

Moreover, in view of (61b) and (62)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1695

Remark 4.3‚ÄîInternal Model Principle: The situation most
often studied in the literature is when
, i.e.,
,
, and
, and when the regulator (28) takes the form

and consequently (63) equals zero for all values of
and
if and only if
(65a)

for
for

(65b)

Theorem 2.1 states that the regulator (28) is stabilizing if
and
are defined by (29) for some stable scalar real
and some real matrix polynomials
polynomial
and
satisfying (27) and that any other stabilizing and
realizable regulator (28) is equivalent to one obtained in this
way. Moreover
(66)
which inserted into (65) yields precisely (59).
If the rank conditions (57) and (58) hold, the interpolation
conditions (59) have a solution, and the general solution is

obtained by setting
. We assume that the rank
.
conditions (57) and (58) are satisfied so that
For robustness it is desirable to include a model of the
disturbance dynamics in the regulator. This is the internal
model principle. Following [3], we replace the matrix fracby the (reachable) matrix fraction
tion representation
so that
. The harmonic
representation
dynamics is then included in the regulator dynamics by setting
, where
and
is a stable matrix polynomial. Then, by (29)

which, in view of the fact that

, yields

for
for
where, for
,
and
are arbitrary matrices
and
. Here
such that
the degree of the stable polynomial is chosen sufficiently
high to satisfy the degree constraints (67). On the other hand,
the rank condition (57) is also necessary for the existence of a
is stable, (65b)
universal tracking regulator. In fact, since
for some
.
cannot hold if
Remark 4.2: The two rank conditions (57) and (58) in
Theorem 4.1, which of course can be stated in terms of zeros
of certain transfer functions, have different status. If (57) is
violated, the interpolation condition (59b) cannot hold, so there
could be no universal tracking regulator. On the other hand,
if (57) holds but (58) does not, interpolation condition (59a)
could still be valid, as the rank of the right member could be
less than . However, this is a nongeneric situation, and hence
it cannot be expected to occur in practice. In fact, if
and
, the following equation must hold:

which will occur only on a lower-dimensional algebraic set in
the parameter space.
Theorem 4.1 provides a complete solution of a problem
studied in various degrees of generality in [4]‚Äì[8], [13], [16]
and of course is consistent with the solutions given there,
although given in a different form and in continuous time.
, rank condition (58) becomes void and only (57), a
If
considerably weaker version of condition (49) in Section III,
remains. Hence, for universal tracking regulators to exist the
is necessary, and if there are external
condition
, in practice, we must also have
.
disturbances
Consequently, as also noted in [4], [7], [8], [13], and [16],
asymptotic tracking is only possible under certain specific
conditions.

where we have assumed that
has no zeros in the points
. (Otherwise we include a simple feedback loop to
clearly satisfy the interpolation
move the zeros.) These
,
and
conditions (59). In fact, since
, by (29), these can be written
for
for
Consequently, we see that the internal-model-principle regulators form a subclass of the ones considered above.
, which
The rank condition (58) becomes void if
is equivalent to the case with complete state information, i.e.,
. Then the formulas for the regulator
the case when
also simplify considerably.
so that
.
Theorem 4.4: Suppose that
Moreover, suppose that is stable and that condition (6) holds.
Then, there exists a universal tracking regulator (37) in if
and only if the rank condition (57) holds. In fact, let
be a stable scalar real polynomial, and let
and
be
matrix polynomials satisfying the degree constraints (36) and
the interpolation conditions
for
for

(67a)
(67b)

and
are given by (38), the regulator (37) is a
Then, if
universal tracking regulator, and any other universal tracking
regulator (37) is equivalent to one obtained in this way.

1696

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof: The proof follows the same lines as that of
Theorem 4.1, except that (39) from Corollary 2.2 is used in
,
exists and (67a)
lieu of (66). Since
can be solved.
When
, there are no universal tracking regulators,
and in order to damp the steady-state tracking error we shall
therefore next turn to an optimization procedure. This is the
topic of the next section.
V. LINEAR QUADRATIC OPTIMIZATION
FOR TRACKING AND DAMPING
We now return to the optimization problem stated in
Section I. In this section we consider only linear regulators.
Later, in Section VI, we demonstrate that under slightly
stronger technical conditions the optimal universal regulators
presented here are actually optimal in the much larger class
, which includes nonlinear regulators.
Let us recall that the problem under consideration is to
control the disturbed system (1) by feedback from the output
so as to minimize the cost function
(68)

for all

,

satisfying
(73)

such that
. It can be shown [21]
for all
that if this condition fails in a strong way, i.e., there are
, , and ,
, such that
, then there
such that
. In
is an external disturbance
this section, however, we shall only need the weak frequency
, ,
,
domain condition that (72) and (73) hold for
, defined as in (55).
Both of these conditions are invariant under the action of
the feedback group

where is a nonsingular matrix and is an arbitrary matrix of
appropriate dimensions. Moreover, since has no eigenvalues
on the unit circle, the inverse
(74)
exists for all on the unit circle, and hence
where
is the Hermitian
that
matrix function

so

(75)
is the quadratic form defined by (9). Hence,
where
we may not only want to damp the tracking error, but also
some internal systems variables. As before, both the disturand the reference signal are harmonic and given by
bance
(5), where only the frequencies are known. The optimization is
performed over the class of stabilizing and realizable linear
regulators (12). The problem under consideration is: 1) to find
the conditions under which there are optimal regulators which
are universal in the sense that they are optimal for all choices
of the amplitudes of (5) and independent of these and 2) to
characterize the class of all such universal optimal regulators.
To address this problem, let us first take a closer look at
the cost function (68). A straightforward reformulation taking
(1c) into consideration yields

In this notation the strong frequency domain condition may
be written
for all

is the real quadratic form
(70)

with the real matrices

,

, and

given by

(71)
The quadratic form (70) need not be nonnegative definite but
must of course satisfy some condition ensuring that
. As we shall see, a sufficient condition for this is the
strong frequency domain condition, i.e., that there is a
such that
(72)

(76)

and the weak one as
for

(77)

We now state the main result of this section. It will be
strengthened in Section VI, where we show that under mild
is
technical conditions the optimal universal regulator in
also optimal in the wider class .
,
, and
be the matrix
Theorem 5.1: Let
polynomials defined by (26) and (56). Suppose that the matrix
is stable and that the weak frequency domain condition
(77) holds, and suppose that
for all

(69)
where

on the unit circle

(78)

. Then,
i.e., in particular that
which is universal in
there exists an optimal regulator in
and
the sense that it is optimal for all values of
and does not depend on these vector amplitudes.
be an arbitrary stable scalar real
More precisely, let
and
be matrix polynomials of
polynomial, and let
and
, respectively, satisfying the degree
dimensions
requirements (27) and the interpolation conditions

with

and

for

(79a)

for

(79b)

given by
(80)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

where
. Then the regulator (28) is an
and
universal regulator, which is optimal in , provided
are given by (29) and any other universal regulator (28), which
is optimal in , is equivalent to one obtained in this way.
is nonsingular for
Since, by assumption,
, (79a) has the solution

1697

(60) of
. In fact, we have the following lemma. The
proof follows from a simple completion-of-squares argument
and is deferred to Appendix A.
be any solution to the closed-loop
Lemma 5.5: Let
system (1), (12), where (12) is a stabilizing and realizable regulator, and suppose that the weak frequency domain condition
(77) holds. Then the cost function (68) exists as a usual limit,
and it takes the value

(81)
, and these are precisely all solutions of (79a).
for
and
Clearly, there are always matrix polynomials
satisfying (81), (79b) and the degree constraints (27), provided
is chosen
the degree of the stable scalar polynomial
sufficiently large.
, there exist optimal regulators, but,
Remark 5.2: If
as explained in Remark 4.2, universality is not a generic
property; therefore, for all practical purposes, there are no
.
optimal universal regulators if
Remark 5.3: Before proceeding to the proof of Theorem
5.1, let us make certain that it is consistent with the results
of Section IV. To this end, let us consider a cost function (7),
. Then
i.e., suppose that

(82)

where, for
(83)
with

and

given by (80) and

by
(84)

where
where the
matrix function
is given by (21). If
, the weak frequency domain condition cannot hold,
so Theorem 5.1 does not apply. Instead, Theorem 4.1 should
, the weak frequency domain condition is
be used. If
a consequence of condition (57), and it is easy to check that
the optimal cost will be zero, as required by Theorem 4.1.
Moreover, interpolation conditions (59) and (79) are identical.
, no universal tracking regulator exists by
Finally, if
Theorem 4.1, and the optimal cost will be nonzero in general.
Remark 5.4‚ÄîGeneralized Internal Model Principle: As in
, so that
Remark 4.3, let us consider the case when
,
,
, and
, and
in the regulator (28). For simplicity, also assume that
.
and
and
, the interpolation
If
conditions (79) can be written

(85)
,
In the expression (82) for the cost function , only
, ,
depend on the regulator to be chosen. They are
defined by (61b), i.e.,
(86)
of external disturbances
Recall that we consider the class
for
and
for
with arbitrary
and the class of reference signals with
for
and
for
.
Consequently, if we could find a stabilizing and realizsatisfy the
able regulator (12) such that
optimality conditions
(87)

for
, as can be seen from (29), (80), and
,
, and
. All of these
the fact that
interpolation conditions are satisfied if the second set is, and
in this case (29) implies that

which could be interpreted as a generalized internal model
principle for the optimization problem.
The basic idea behind the proof of Theorem 5.1 is, as for
Theorem 4.1, that whenever a linear stabilizing regulator is
tends exponenapplied to the system (1), the process
tially to the harmonic steady-state solution (60). Therefore, the
cost function (68) depends only on the harmonic component

which, in view of (86), is the same as
(88)
then this regulator would be optimal. If, in addition, this regu,
lator does not depend on the amplitudes
and
,
and the conditions (88) hold for all
and
, i.e., all disturbances in
and all
reference signals in , then this optimal regulator is also
universal. This condition holds if and only if
for

(89a)

for

(89b)

1698

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof of Theorem 5.1: Theorem 2.1 states that the reguand
are defined by (29) for
lator (28) is stabilizing if
and some real matrix
some stable scalar real polynomial
satisfying (27), and that any other stabilizing
polynomial
and realizable regulator (28) is equivalent to one obtained in
this way. Moreover

Since, by assumption,
is a nonsingular matrix of
, (91a) has the solution
dimension

(92)
. There are always matrix polynomials
and
satisfying (92), (91b), and the degree constraints (36),
is
provided the degree of the stable scalar polynomial
chosen sufficiently large.
for

(90)
We have demonstrated above that (89) is a necessary condition
for the regulator (28) to be an optimal universal regulator, and
inserting (90) into (89) yields precisely (79). Clearly, as we
have already discussed, there are always matrix polynomials
and
satisfying these conditions and the degree
constraints (27), provided the degree of the stable scalar
is chosen sufficiently large, and provided
polynomial
condition (78) is satisfied.
It remains to prove the converse statement. For any optimal
, the value of the cost function
universal regulator
, defined by (84). It follows from (82) and the
(68) equals
, for
, that (87) holds for
fact that
, and
. Therefore, (89) follows from
all
is equivalent to
(88). By Theorem 2.1, the regulator
given by (29) for some
satisfying the
(28) with
requirements of Theorem 5.1. This regulator is also optimal
since equivalent regulators have the same cost . It is also
does not depend on
universal because
and
.
Corollary 5.6: The optimal value of the cost function (68)
, defined by (82) and (83).
in the class is
Note that, although an optimal universal regulator will not
and
, the cost function (84)
depend on
will.
In the special case of complete state information, i.e.,
, condition (78) is always satisfied. In view of Corollary 2.2,
Theorem 5.1 can be considerably simplified in this case, so we
state it separately. The proof is the same as for Theorem 5.1,
except that we now use the equations of Corollary 2.2.
so that
.
Theorem 5.7: Suppose that
Moreover, suppose that is stable and that condition (6) holds.
Then, if the weak frequency domain condition (77) holds, there
exists a universal regulator (37), which is optimal in . In fact,
let
be a stable scalar real polynomial, and let
and
be matrix polynomials satisfying the degree constraints
(36) and the interpolation conditions
for

(91a)

for

(91b)

and
are
where and are defined as in (80). Then, if
given by (38), the regulator (37) is a universal regulator, which
is optimal in . Conversely, any other universal regulator (37),
which is optimal in , is equivalent to one obtained in this
way. Finally, the optimal value of the cost function (68) is
given by (84).

VI. OPTIMALITY IN THE CLASS OF NONLINEAR REGULATORS
In this section we show that the universal optimal linear
regulators described in Theorems 5.1 and 5.7 are actually
optimal in a wide class of nonlinear regulators. We now define
this class.
of
Given the control system (1), consider the class
nonlinear regulators
(93)
of
which is stabilizing in the sense that any solution
the closed-loop system consisting of (1) and (93) satisfies the
condition
as

(94)

This stability condition is quite weak but will suffice for our
purposes. Of course, a weaker condition has the advantage of
allowing for a larger class of controls.
We consider the same problem as in Section V, except that
.
we now optimize over all regulators in . Clearly,
The only price we have to pay for this generalization is that
the weak frequency domain condition needs to be replaced by
the strong one.
be stable, and suppose that the rank
Theorem 6.1: Let
condition (78) holds. Then, if the strong frequency domain
condition (76) holds, the linear optimal universal regulators of
Theorem 5.1 are optimal in the class .
It turns out that Theorem 6.1 is a simple consequence of the
corresponding result for complete state information. In fact, the
class of stabilizing and realizable regulators
with
is a subclass of the class of stabilizing and realizable regulators

in that only a special structure of is required. But, as seen in
Section V, an optimal universal regulator in the former class
is optimal also in the latter, since the same optimal value
is achieved (Corollary 5.6 and Theorem 5.7). (The only
difference between the cases of complete and incomplete state
information is that a higher degree regulator may be required
in the latter case to achieve the optimum.) Consequently, if
we can prove the following theorem, we have also proved
Theorem 6.1.

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

Theorem 6.2: Let
be stable, and suppose that
and that
. Then, if the strong frequency domain
condition (76) holds, the linear optimal universal regulators of
Theorem 5.7 are optimal in the class .
In order to prove this theorem we consider an optimization
problem which unlike that in Section V does not require that
a linear regulator has been applied. More precisely, let us
first consider the problem of finding a process
which minimizes the cost function (8), subject to the constraints (94) and
(95)
and
are arbitrary bounded and
where now
complex-valued vector sequences.
It is well known (see, e.g., [20], [21], [24], [25], and [29])
that if the strong frequency domain condition (76) holds and
is stabilizable, then the algebraic Riccati equation

1699

The optimal value of the cost function is
(103)
where

(104)
exists, any optimal process
If the limit
is produced in this way.
Note that the control (101) cannot in general be used in
and . Even
practice, since it depends on future values of
in the harmonic case when this dependence can be resolved,
this control law has serious disadvantages [21, Sec. III]. It is
developed here as an instrument of proof.
Next, let us return to our original problem and take
and
to be harmonic, given by (5). Then a simple
calculation, using (99) and (100), yields the representation

(96)
has a unique symmetric solution
matrix

with

which renders the feedback

(105)

where

where
(97)
stable in the sense that all eigenvalues of lie strictly inside
the unit circle. We shall refer to this solution as the stabilizing
solution of (96). For this solution we also have that
(98)
is positive definite.1
Then we have the following result, which should be compared to [21, Th. 2.3], the proof of which we defer to
Appendix B.
be stabilizable and suppose that
Lemma 6.3: Let
the strong frequency domain condition (76) holds so that (96)
has a stabilizing solution . Moreover, let

We are now in a position to prove Theorem 6.1.
Proof of Theorem 6.1: Clearly, for any regulator in ,
(103) is a lower bound for the cost . Therefore, if we can
demonstrate that there is a regulator in which achieves the
same value (103) of the cost , this regulator must be optimal
also in , and so must all regulators which are optimal in .
so that
To this end, let us introduce a new control
(106)
transforming the system (1a) to
(107)

(99)
where

We want to find a stabilizing and realizable regulator
(100)

Then the problem to minimize the cost function (8) subject
to constraints (94) and (95) is solved by a process
such that

(108)
so that the closed-loop system (106)‚Äì(108) has a solution
satisfying (101) for some with the property (102).
Then, by Lemma 6.3, the regulator (106), (108), i.e.,
(109)

(101)
where is given by (97) and
such that

is any vector sequence

(102)
1 Note that there is a misprint in [21, p. 788]: In Theorem 2.1, replace
‚Äústatements hold‚Äù for ‚Äústatements are equivalent.‚Äù

is optimal in . Therefore, the optimal linear regulators of
Theorem 2.1 must be optimal also in .
of the closedSince (108) is stabilizing, the solution
loop system (107), (108) tends exponentially to a harmonic
solution

1700

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

which of course yields the same value to
so that
if we can choose

as

. Now,

for

The matrix polynomials (26) are
and

(110)

, then
has the property (102)
and hence
and (106) becomes (101) as required.
such that (110) holds, we
To show that there are
first apply Corollary 2.2 to the system (107), where takes
and
that of . In fact, by Corollary 2.2,
the place of
there is a stable scalar polynomial and matrix polynomials
such that
and
so that
are given by
and

Let us first take
a T-universal regulator

and consider the problem to find
(112)

tends asymptotically to . By Corollary 3.3, a
so that
T-universal regulator exists if and only if
and

stable

(113)

where

and
In fact,
. In this case, (112) is a Tuniversal regulator if and only if
However,
tends exponentially to the harmonic solution
Since therefore

.

and
is given by (105), the optimality condition (110) will
and
if
be satisfied for all

(114)
and
such that
is stable and
for some polynomials
or is equivalent to one obtained in this
. Of course,
way. This corresponds to the choice
asymptotic tracking is achieved for all choices of reference
signal .
If, instead, we consider a reference signal
(115)

Since

is full rank, in view of the discussion in Section V
can be chosen to satisfy these interpolation
conditions.
VII. SOME SIMPLE NUMERICAL EXAMPLES

for

To illustrate the results of this paper, let us consider the
system

is the control,
where
characteristic polynomial

and

where the frequencies , are given, but the amplitudes ,
and the phases ,
are unknown, the class of regulators
(112) which achieve asymptotic tracking is much larger, and
condition (113) need not be satisfied but can be exchanged for
(116)

In fact, by Theorem 4.1, in this case we may choose any
stabilizing regulator

(111)

(117)

are outputs, and the

provided is stable and the degree constraint (27) and the
interpolation conditions
for

is stable with

. Defining the state

the plant equations (111) can be written in state-space form
(1), where

so that

is the characteristic polynomial of

, and

are satisfied. The same regulator is obtained by applying
Theorem 5.1, now observing that (116) is the weak frequency
domain condition; see Remark 5.3. This allows for more tuning
parameters to satisfy other design specifications. Of course, if
condition (113) is fulfilled, the T-universal regulator can still
be used.
,
, and
As a numerical example, suppose that
, and let
and
. Then condition (113)
is satisfied, so a T-universal regulator exists. Such a regulator
and
in
is obtained by, for example, setting
and
and the initial conditions are
(114). If
, this yields the error depicted in Fig. 2. The

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

Fig. 2.

Fig. 3.

dashed line in the same figure is the tracking error obtained
.
by setting
, while and remain the same.
Next, let us take
Then becomes unstable, so a T-universal regulator fails to
exist. Although condition (113) fails, we could still obtain
asymptotic tracking by using a universal tracking regulator,
constructed as in Theorem 4.1, provided condition (116) holds,
and we shall present a simulation for this case in the end of
the section.
We now add an harmonic disturbance

straightforward calculation yields

(118)
and ,
in the system (111), where , are given, but ,
are unknown. Suppose we want to determine an optimal
universal regulator for the cost function

1701

for any on the unit circle. In order to construct an optimal
universal regulator we need to choose a stable polynomial

of degree at least five. The parameters , , , , , as
well as will be available for tuning in order to improve the
overall design. Then, defining the real numbers , , , ,
, , ,
via
for
for

(119)
it is easily seen that the polynomials
Since the matrices

,

, and

in (71) become

a simple calculation yields

will satisfy the interpolation conditions (79a) if and only if its
coefficients satisfy the linear system of equations

for (75), and therefore the strong frequency domain condition
, so any optimal universal
(76) is always satisfied if
of possibly
regulator (112) is optimal in the larger class
, the
nonlinear regulators described in Section VI. If
strong frequency domain condition will fail if and only if the
has a root on the unit circle, while the weak
polynomial
frequency condition (77) will still hold provided we avoid
choosing any of the frequencies in (115) and (118) so that
,
,
, or
is such a root.
Next, let us consider the interpolation condition (79).
defined by (56) is identically one, and a
Clearly,

Consequently, by Theorem 5.1, (117) is an optimal universal
and
are determined in this way.
regulator if
,
, and
For an example, take as before
. Moreover, we choose a disturbance (118) with
and
, while the harmonic
frequencies
,
reference signal (115) has the same frequencies
as in the first simulation. In Fig. 3 we illustrate the
tracking error of the optimal universal regulator corresponding
to a polynomial with roots 0.3 0.3 , 0.3 0.2 , 0.5, and
. The amplitudes in (115) and (118) have been taken

1702

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Fig. 4.

to be
,
, and
, and the initial
. As before, the dashed line is
conditions are
. Remember
the tracking error obtained by setting
, the control energy is also damped, so
that, since
there is a certain tradeoff here. We remark that it is important
to tune the free parameters to obtain good properties of the
regulator. In particular, the transients, which do not affect the
cost function, can change dramatically with different choices
of free parameters.
and
instead,
Now, setting
while keeping all the other parameters the same, we obtain
the errors in Fig. 4. As seen, the error goes asymptotically
to zero, despite the fact that condition (113) is not fulfilled so
that a T-universal regulator does not exist. In fact, by Theorem
4.1, this is a universal tracking regulator which exists since
on the unit circle. In order to speed up the
convergence, the roots of have been reset at 0.7 0.1 , 0.3
0.2 , and 0.8. Since now we do not have the disturbance
and
, we could choose another
frequencies
to possibly get a universal tracking regulator with a
better transient.

reference signal is no larger than the dimension
of the
control, such a regulator exists under mild conditions. This
is in harmony with other results in the literature [4]‚Äì[8], [13],
[16], where, however, the continuous-time case is considered.
We provided complete solutions of these problems in discrete
time, and our proof is considerably simpler.
If the system is also corrupted by a harmonic disturbance
, asymptotic tracking may still be possible provided the
dimension of the disturbance is no larger than the dimension
of the output available for feedback. However, if a certain
,
rank condition fails, which in particular is the case if
asymptotic tracking is not possible, but a steady-state error
will remain. Therefore, we considered next an optimal control
problem to damp the steady-state tracking error, also giving
the option to damp internal system variables. We characterized
the class of all optimal regulators which are universal in the
sense that they are optimal for all choices of the amplitudes
of and . Such regulators were shown to exist if the weak
. On the other
frequency domain condition holds and
, there are always algebraic conditions on the
hand, if
system parameters, implying that universality is not a generic
property in this case.
We have also shown that all optimal universal regulators
can be chosen as linear even if the optimization is over a
very large class of nonlinear regulators, provided the strong
frequency domain condition holds. We have given complete
characterizations of all linear optimal universal regulators in
terms of parameterizations containing many free parameters.
This allows for a considerable amount of design freedom,
which can be used to satisfy other design specifications via
loop shaping. Indeed, we stress that our solutions are optimal
in the sense stated in this paper only, and that other desirable
design specifications may not be satisfied for an arbitrary
universal optimal regulator.
APPENDIX A
PROOF OF LEMMA 5.5
and
tend exponentially to the harmonic comSince
ponents (60), only these contribute to the cost function (70);
consequently, the usual limit (rather than just limsup) does
where
exist in (69), and it is given by

VIII. CONCLUSIONS
In this paper we have given complete characterizations of
regulators which satisfy certain tracking specifications and
which are universal in the sense that they are independent
of disturbances and tracking signals and apply regardless of
the values of these.
As a preliminary, we considered a problem of asymptotic
tracking of an arbitrary signal , and we characterized all
regulators which are universal with respect to the choice of
. We showed that such universal regulators exist only under
very special conditions. These conditions can be considerably
relaxed if the reference signal is exchanged for a harmonic
signal with known frequencies but unknown amplitudes and
phases, and we want the regulator to be universal in the
sense that it achieves asymptotic tracking for all choices
of the
of amplitudes and phases. Then, if the dimension

(A1)
. In fact, this follows from the argument
for
leading to (64). Now, in view of the constraint (1a)
(A2)
and therefore (A1) takes the form
(A3)
if the weak frequency domain condition (77)
where
is given by (85), and
is fulfilled. Here
(A4)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

Therefore, assuming that the weak frequency domain condition
for
, we may
(77) holds so that
complete squares in (A3) to obtain
(A5)
where

1703

By virtue of condition (94) and the boundedness of

where of course the last term tends to zero as
.
, the cost function
Consequently, for any admissible
(B1) becomes

(A6)
From this the equations of the lemma follow readily.
(B5)

APPENDIX B
PROOF OF LEMMA 6.3
The proof is similar, mutatis mutandis, to the one given in
[21, Sec. II]. Recall from (69) that the cost function can be
written
(B1)
where

Therefore, since
(B6)
for any admissible control. Clearly, equality would be achieved
to satisfy (101) since
does not
if we could take
contribute to by virtue of (102). Hence it remains to prove
that such a process satisfies the stability condition (94). To this
end, insert (101) in (95) to obtain
(B7)

(B2)
being the quadratic form (70). Next, introduce
with
the Lyapunov function
(B3)
is the unique stabilizing solution of (96),
where
is given by (100) and
satisfies (104). Then, along
the trajectory of (95)

and
are bounded,
satisfies
Since
is a stability matrix,
satisfies the
(102) and
weak stability condition (94). The last statement follows
immediately from (B5) and (B6).
ACKNOWLEDGMENT
The authors would like to thank the anonymous referees and
the associate editor for several useful suggestions. They would
also like to thank X. Hu for technical advice and stimulating
discussions.

(B4)
is given by (99).
where
In fact, inserting (95) and completing squares in the left
member of (B4) yields the right member of (B4) plus a number
of terms which are either quadratic in , linear in , or
constant with respect to . The quadratic terms cancel due
to the fact that satisfies the algebraic Riccati equation (96),
and the constant terms cancel due to (104). Finally, the linear
terms cancel provided

which has the unique bounded solution (100), since
is a
stable matrix.
and
, where
Now, set
is an admissible process, and sum (B4) from
to
to obtain

REFERENCES
[1] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic
Methods. London, U.K.: Prentice-Hall, 1989.
[2] S. Bittanti, F. Lorito, and S. Strada, ‚ÄúAn LQ approach to active control
of vibrations in helicopters,‚Äù Trans. ASME, J. Dynamical Systems,
Measurement and Contr., vol. 118, pp. 482‚Äì488, 1996.
[3] C. T. Chen, Linear System Theory and Design. New York: Holt,
Rinehart and Winston, 1984.
[4] E. J. Davison and A. Goldenberg, ‚ÄúRobust control of a general servomechanism problem: The servo compensator,‚Äù Automatica, vol. 11, pp.
461‚Äì471, 1975.
[5] E. J. Davison and B. R. Copeland, ‚ÄúGain margin and time lag tolerance
constraints applied to the stabilization problem and robust servomechanism problem,‚Äù IEEE Trans. Automat. Contr., vol. AC-30, pp. 229‚Äì239,
1985.
[6] E. J. Davison and B. M. Scherzinger, ‚ÄúPerfect control of the robust
servomechanism problem,‚Äù IEEE Trans. Automat. Contr., vol. AC-32,
pp. 689‚Äì702, 1987.
[7] B. A. Francis, ‚ÄúThe linear multivariable regulator problem,‚Äù SIAM J.
Contr. Optim., vol. 15, pp. 486‚Äì505, 1977.
[8] B. A. Francis and W. M. Wonham, ‚ÄúThe internal model principle of
control theory,‚Äù Automatica, vol. 12, pp. 457‚Äì465, 1977.
[9] K. V. Frolov and F. A. Furman, Applied Theory of Vibration Protected
Systems. Moscow, Russia: Mashinostroenie, 1980 (in Russian).
[10] K. V. Frolov, Vibration in Engineering. Moscow, Russia: Mashinostroenie, 1981 (in Russian).
[11] M. D. Genkin, V. G. Elezov, and V. D. Iablonski, Methods of Controlled
Vibration Protection of Engines. Moscow, Russia: Nauka, 1985 (in
Russian).

1704

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

[12] D. Guicking, Active Noise and Vibration Control, reference bibliography,
Third Physical Institute, Univ. Goettingen, Jan. 1990.
[13] A. Isidori and C. I. Byrnes, ‚ÄúOutput regulation of nonlinear systems,‚Äù
IEEE Trans. Automat. Contr., vol. 35, pp. 131‚Äì140, 1990.
[14] C. G. KaÃàllstroÃàm and P. Ottosson, ‚ÄúThe generation and control of roll
motion of ships in closed turns,‚Äù in Proc. 4th Int. Symp. Ship Operation
Automat., Geneva, Switzerland, 1982, pp. 1‚Äì12.
[15] H. Kimura, ‚ÄúPole assignment by output feedback: A longstanding open
problem,‚Äù in Proc. 33rd Conf. Decision and Control, Lake Buena Vista,
FL, Dec. 1994.
[16] A. Krener, ‚ÄúThe construction of optimal linear and nonlinear regulators,‚Äù
in Systems, Models and Feedback: Theory and Applications, A. Isidori
and T. J. Tarn, Eds. Boston, MA: BirkhaÃàuser, 1992, pp. 301‚Äì322.
[17] V. KucÃÜera, ‚ÄúThe discrete Riccati equation of optimal control,‚Äù Kybernetika, vol. 8, pp. 430‚Äì447, 1972.
[18] H. Kwakernaak and R. Sivan, Modern Signals and Systems. Englewood Cliffs, NJ: Prentice-Hall, 1991.
[19] G. Leitmann and S. Pandey, ‚ÄúAircraft control under conditions of
windshear,‚Äù in Proc. 29th Conf. Decision and Control, Honolulu, HI,
1990, pp. 747‚Äì752.
[20] P. Lancaster, A. C. M. Ran, and L. Rodman, ‚ÄúHermitian solution of the
discrete algebraic Riccati equation,‚Äù Int. J. Contr., vol. 44, pp. 777‚Äì802,
1986.
[21] A. Lindquist and V. A. Yakubovich, ‚ÄúOptimal damping of forced
oscillations in discrete-time systems,‚Äù IEEE Trans. Automat. Contr., vol.
42, pp. 786‚Äì802, 1997.
, ‚ÄúOptimal damping of forced oscillations by output feedback,‚Äù
[22]
in Stochastic Differential and Difference Equations, Progress in Systems
and Control Theory, vol. 23, I. CsiszaÃÅr and G. Michaletzky, Eds.
Boston, MA: BirkhaÃàuser, 1997, pp. 203‚Äì231.
[23] A. Miele, ‚ÄúOptimal trajectories and guidance trajectories for aircraft
flight through windshears,‚Äù in Proc. 29th Conf. Decision and Control,
Honolulu, HI, 1990, pp. 737‚Äì746.
[24] B. P. Molinari, ‚ÄúThe stabilizing solution of the discrete algebraic Riccati
equation,‚Äù IEEE Trans. Automat. Contr., vol. AC-20, pp. 396‚Äì399, 1975.
[25] V. M. Popov, Hyperstability of Control Systems. Berlin, Germany:
Springer, 1973.
[26] A. V. Savkin and I. R. Petersen, ‚ÄúRobust control with rejection of
harmonic disturbances,‚Äù IEEE Trans. Automat. Contr., vol. 40, pp.
1968‚Äì1971, 1995.
[27] R. Shoureshi, L. Brackney, N. Kubota, and G. Batta, ‚ÄúA modern control
approach to active noise control,‚Äù Trans. ASME, J. Dynamical Syst.s,
Measurement and Contr., vol. 115, pp. 673‚Äì678, 1993.
[28] V. Z. Weytz, M. Z. Kolovski, and A. E. Koguza, Dynamics of Controlled
Machine Units. Moscow, Russia: Nauka, 1984 (in Russian).
[29] V. A. Yakubovich, ‚ÄúA frequency theorem in control theory,‚Äù Sibirskij
Mat. Zh., vol. 4, pp. 386‚Äì419, 1973 (in Russian); English translation in
Sibirian Math. J.
[30]
, ‚ÄúUniversal regulators in linear-quadratic optimization problems,‚Äù in Trends in Control, A. Isidori, Ed. New York: Springer-Verlag,
1995, pp. 53‚Äì68.
[31] Y. Zhao and A. E. Bryson, ‚ÄúAircraft control in a downburst on takeoff
and landing,‚Äù in Proc. 29th Conf. Decision and Control, Honolulu, HI,
1990, pp. 753‚Äì757.
[32] K. Zhou, Essentials of Robust Control. Englewood Cliffs, NJ: PrenticeHall, 1998.

Anders Lindquist (M‚Äô77‚ÄìSM‚Äô86‚ÄìF‚Äô89) received
the M.S. and Ph.D. degrees from the Royal Institute
of Technology, Stockholm, Sweden, and in 1972
he was appointed a Docent of Optimization and
Systems Theory there.
From 1972 to 1974, he held visiting positions at
the University of Florida, Brown University, and
State University of New York, Albany. In 1974,
he became an Associate Professor and in 1980
a Professor of Mathematics at the University of
Kentucky, where he remained until 1983. He is
presently a Professor at the Royal Institute of Technology, where in 1982
he was appointed to the Chair of Optimization and Systems Theory, as well
as an Affiliate Professor at Washington University, St. Louis. He has also held
visiting positions at University of Padova, Italy, University of Arizona, USSR
Academy of Sciences, Moscow, East China Normal University, Shanghai, and
Technion, Haifa, Israel. He is the author of many papers in the area of systems
and control, especially stochastic control, filtering, stochastic systems theory,
realization theory, robust control, and applications of nonlinear dynamics in
estimation and control.
Dr. Lindquist is a Member of the Royal Swedish Academy of Engineering
Sciences, a Foreign Member of the Russian Academy of Natural Sciences,
and an Honorary Member of the Hungarian Operations Research Society. He
has also served on many editorial and advisory boards.

Vladimir A. Yakubovich (M‚Äô97) was born in
Novosibirsk, Russia, in 1926. He graduated from
Moscow University in 1949. He received the
Candidate of Science degree (Ph.D.) in 1953 and
the Doctor of Science degree in 1959, both from
Leningrad University.
After having worked for some time in industry
as an Engineer, he was admitted to the Leningrad
University, where he has remained. He is the author
of more than 270 papers and coauthor of seven
books in different areas of applied mathematics and
control theory. He has worked in parametric resonance theory, in the theory
of stability of nonlinear systems, and in optimization theory.
Dr. Yakubovich has served on many scientific committees and editorial
boards. He is a member of several scientific societies in Russia. He was
awarded the Norbert Wiener Prize in 1991, a prize from the international
editorial company ‚ÄúNauka‚Äù for best publication in its journals in 1995,
and the IEEE Control Systems Award in 1996. Since 1991, he has been a
Corresponding Member of the Russian Academy of Sciences and since 1994
a Member of the Russian Academy of Natural Sciences.

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Security Considerations for Distributed Web-Based e-commerce Applications in Java
Timothy E. Lindquist Electronics and Computer Engineering Technology Arizona State University East http://www.east.asu.edu/ctas/ecet Tim@asu.edu Abstract
Today's distributed e-commerce applications typically rely upon various technologies in their realization, including the web, scripting languages, server-side processing and an underlying database. The combination of these technologies creates a system that requires attention to the security issues of each component and the system as a whole. In considering the overall system, issues arise from the interactions of security frameworks available for each component. In this paper, we consider the approach and related issues for distributed e-commerce applications developed with Java. The flexible nature of Java allows migration of objects (compiled code with state) through features such as RMI and Applets. Security for distributed applications developed in Java has issues and lessons applicable to systems of components built on different technologies.
DBMS

legacy appl

view objects/ clients IGURE 1.

business logic/ middle-tier/ server objects

transaction monitor

3-Tier Client-Server Architecture

1. Problem
Web-based e-commerce and distributed applications are changing the way we buy goods, access information and learn. Use of email and other related technology increasingly facilitates collaboration and is more commonly being used for official communications. Official communications via the internet are too often done in insecure mode. Web-based e-commerce applications commonly employ multiple tiers (3-tier client server architecture) and a combination of technologies such as HTML, XML, JavaScript, Java (JSP, Servlets), ASP, dynamic html, CGI, and relational databases, as shown in Figure 1. Each of these technologies have separate and in some cases incompatible approaches to protection against intrusion. For web-based applications, the communication between clients and the middle-tier is via web protocol http. Clients may employ any number of technologies such as applets, html, xml, and scripts. The middle-tier business

logic often employs any of a number of CGI work-arounds such as Netscape's NSAPI, Microsoft's ISAPI, WebObjects, ASPs, Java J2EE, servlets and JSP. The combination of different technologies at each tier, presents special challenges to security of the overall application. Development time and cost pressures often short-change security concerns. Problems range from software design constraints that prevent adequate security to insufficient testing to exercise common attacks. Often performance concerns limit the extent to which assurance can be implemented in a web-based application. Emerging technologies and applications are also presenting new challenges to secure applications. Distributed Object technologies have been maturing for the past several years and are being increasingly utilized in web-based developments. Although some researchers have supported an approach where an object-web replaces the largely datacentered web of today, this has not materialized. Nonetheless, object technologies such as CORBA, DCOM and Java RMI enjoy increased usage in distributed web-based applications. Additional frameworks such as JINI, JavaSpaces, JNDI, and EJB support distributed Java Objects.

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

1

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Distributed objects are network aware objects. Applications running on remote machines may communicate with each other at the object-level using this technology. With a remote reference to a Java RMI object, methods can be requested of the object (messages) in the same manner as if the object were local to the executing application. The SUN implementation of Java (sdk/jdk1.2 and up) includes a flexible and feature rich approach to security providing the basis for distributed Java applications.

private key

message

message

public key

algorithm

algorithm

signature

signature

verify

2. Security Concerns
Sender The platform independence of Java has lead to easy movement of (compiled) code across the internet. While the approaches of OMG CORBA (see: http:// www.omg.org) and Microsoft do provide multi-language solutions, they do not provide the same code migration capabilities as is available with Java. Interacting remote Java objects may easily be written in a manner that requires dynamic movement and execution of code (class files) across the internet. Security concerns include authentication, integrity and encryption/decryption. These may all come into play whenever information (code or data) is moved (across a network or within a single machine). 1. Authenticity allows the receiver of information to know with certainty the identity of the sender. 2. Integrity allows the receiver to know with certainty that information transmitted by the sender has not been modified or tampered with enroute. 3. Encryption is the process of taking data (called clear text) and a short key and producing cipher data that is meaningless to anyone who does not know the key. Decryption is the process of taking cipher data and a short key to produce the corresponding clear text. Each of these basic security concerns come into play with distributed applications, for controlling executing applications as well as access to information. Figure 2 shows how authentication and integrity can be provided using digital signatures. The sender uses his own private key (which must be kept protected utilizing access control) and together with a message to generate a digital signature, which is unique to the message and private key. The message and signature are transferred to the receiver. The receiver must have a public key (usually received separately) corresponding to the sender's private key. The public key can be used to verify a signature, but cannot be used to generate a signature. Upon receipt, the message and signature are verified assuring both authenticity and integrity of the exchanged message.
IGURE 2.

Receiver

3-Tier Client-Server Architecture

3. Digital Signatures
Keys are generated in pairs. The private key is used to generate the signature and is kept confidential to whoever is doing the signing. The public key is used by the receiver to verify authenticity of the message. The signer should distribute the public key to anyone who will receive signed information. The issue as to whether the public key actually corresponds to the sender is resolved with certificates. A certificate represents a chain of trust leading from the sender to the receiver, indicating that the public key belongs to whom you want to believe it belongs. If the sender and receiver both trust the same certificating agency then the chain may be of length one. Each link in the chain is a certifying agency (such as VeriSign or Entrust) which certifies that the entity prior to it in the chain (the owner of the private key or another certifying agency) is who they say they are. Users should understand how certificates are signed and managed. Current web browsers display information about the certificate and who signed it, but few users ever look beyond the lock icon on their web browsers. This provides some opportunity for anyone with a signed certificate to use a man-in-the-middle attack. Simple possession of a certificate says nothing of integrity, quality or functionality of code or other information conveyed by the certificate holder. Another complication of digital signatures is management of a certificate revocation list. Once a key is known to be compromised, there must be some way to inform users that it should no longer be trusted. The SUN implementation of Java comes with a primitive set of tools for manipulating keys, certificates and digital signatures. It also includes the framework classes (in the package java.security) for program creation and verification of digital signatures. Figure 3 includes a sample of Java that may exist for the sender. The example generates a public and private key pair

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

2

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

//generate the private and matching public key KeyPairGenerator keyGen=KeyPairGenerator.getInstance("DSA", "SUN"); SecureRandom random = SecureRandom.getInstance( "SHA1PRNG", "SUN"); keyGen.initialize(1024, random); KeyPair pair = keyGen.generateKeyPair(); PrivateKey priv = pair.getPrivate(); PublicKey pub = pair.getPublic(); //create the signature object Signature dsa = Signature.getInstance("SHA1withDSA", "SUN"); dsa.initSign(priv); //read the datafile; FileInputStream fis = new FileInputStream(args[0]); BufferedInputStream bufin = new BufferedInputStream(fis); byte[] buffer = new byte[1024]; int len; while (bufin.available() != 0) { len = bufin.read(buffer); dsa.update(buffer, 0, len); } bufin.close(); //generate the signature byte[] realSig = dsa.sign(); //save the signed data in a file FileOutputStream sigfos = new FileOutputStream("sigOf"+args[0]); sigfos.write(realSig); sigfos.close();

4. Securing Java Applications
Many aspects of Java's design lend well to distributed applications. One such example is serialization. The ability to externalize objects from one executing Java program (virtual machine) and to read them into another is a process Java calls serialization. Serializable objects may be transmitted through the internet without loss of object properties, including methods. To accomplish object externalization, it is often necessary to move the compiled code along with object data. Several mechanisms exist within Java to do this either implicitly or explicitly under programmer control. Applets and Remote Method Invocation (RMI) are two such mechanisms. Applets are small Java programs communicated from a web-server and executed by a virtual machine running in the browser. RMI, provides the programmer with an object view of internet objects so that method calls, for example can formulated as though the object were in the same virtual machine. RMI capabilities are similar to Microsoft DCOM and the Object Management Group's Common Object Request Broker Architecture (CORBA). Java's platform independence is critical to realizing these dynamic capabilities. Compiled java code (class files) can move to a variety of platforms and be executed without loss of meaning. This powerful capability, which has not been realized to the same level and extent by any other language efforts, was first made generally available by Java implementations. Java's reflection capabilities, allow a program to discover and access the properties available in an object. This allows internet available objects to be manipulated in a manner not necessarily know by the program at the time it was compiled. In addition to facilitating distribution, Serialization, RMI, and Reflection are leading to a view of internet enabled software service objects. These provide the critical infrastructure for e-commerce services, such as financial, investment, and retail purchase. The current e-commerce solutions utilize the web and represent a composition of diverse technologies: 1. User interface through html, xml, Java, JavaScript, Flash and so on, 2. Server functions through dynamic html, JSP, ASP, J2EE, CGI or servlets, 3. Legacy data through RMI, ODBC or JDBC connectivity to a relational database. The challenge is to formulate a secure impenetrable application in light of the combination of a variety of technologies and capabilities. The Java model for securing the operations in an executing virtual machine has progressed significantly since the

FIGURE 3.

Snipet of Java to sign a file

and uses the private key to generate a digital signature for a data file. The signature is saved to file. This code represents simplistically what must be done by the sender. The public key, the signature file and the data file are all transmitted to the receiver, where a similar program verifies the signature using the data and public key. The primary vulnerability of this approach rests in communicating the public key. An attack may replace the data, signature file and public key if they are all three transmitted together. Certificates are the most common mechanism used to assure the public key authentically identifies the sender. Good practice dictates that the public key be transmitted separately in an assureable manner. The public key (certificate) is stored by the receiver for later use to authenticate multiple subsequent transmissions. This mechanism can be used to verify the authenticity and integrity of either data or program code that is transferred in distributed e-commerce applications. The approach verifies that information came from the purported sender and was not modified in transmission. Encryption is necessary to protect information from reading by others during transmission, as discussed below. Security within an executing Java application is based upon authenticity and integrity using digital signatures.

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

3

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

initial introduction of the sandbox model for Java applications. Initial versions of Java provided full trust to classes loaded locally and prohibited all sensitive operations from any code obtained dynamically. Java now supports a continuum of access control. Access to system resources (such as files, sockets, runtime, properties, security permissions, serializable, reflection, and window toolkit) is granted based on domains. A domain includes a set of permissions together with a codebase and an indication of who signed the code. The codebase indicates the file or URL from which the code is loaded. If signed, the alias of the public key can also be used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which defines the access it has to resources.

{

permission java.net.SocketPermission "*.GSE.com:2575-", "accept, connect, listen, resolve";

};

A policy may consist of one or more grants each defining different domains. Each domain may have one or more associated permissions. When a protected operation is attempted, the virtual machine's security manager performs a security check. It looks at the classes of all methods currently on the runtime call stack. Each associated protection domain is queried to determine whether the operation is allowed. An operation is performed only if all methods on the runtime stack have the appropriate permission. Signing executable is an important application of authentication and integrity technology. As the number of distributed applications grows and those applications increasingly rely upon migration of code, we need assurance that we are granting permissions to trustworthy code. Today, code signing is largely platform dependent. For example, applets executed with the Netscape or Internet Explorer Java virtual machines require use of Netscape or Microsoft tools to sign the code. Applets designed to run with the SUN plug-in virtual machine must be signed with the SUN tools. This lack of consistency only accentuates the problems arising from utilizing multiple technologies to realize an e-commerce application.

5. Cryptography
FIGURE 4. Controlling Access to Java Resources

Figure 4, is taken from the On-line Java Tutorial, http://java.sun.com/docs/books/tutorial/ and shows the interaction between the security domains defined in Java2 and the original sandbox model. In the Java 2 SDK version 1.4, the standard platform has been further augmented to integrate the Java authentication and authorization service (JAAS). Doing so takes a step closer to integrating user login services with authentication mechanisms. See: http://java.sun.com/products/jaas/ In Java 2, security domains are defined by a policy granting permissions to the domain. For example, suppose the company GrowthStocksExpress publishes an applet on their hypothetical web site at the URL: http://GSE.com/applets Assuming the applet needs connections to one or more hosts having a domain address ending with GSE.com on ports beginning at 2575, a policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase "http://GSE.com/applets"

The SUN implementation of the Java Cryptography Architecture (Java Crypto Extensions) is freely available for developing applications that rely on encryption. Similarly, if the application requires an encrypting web server, Apache-SSL is one of the freely available web servers based on OpenSSL. It can be freely obtained and used commercially. See: http://www.apache-ssl.org/ In addition to authentication and integrity, distributed ecommerce applications require cryptographic services. Authentication and integrity assure the identity of the sender and that information was not changed in transmission, but they do not protect against reading during transmission. Encryption is a concern for financial transactions or other communication where personal identification information must be transmitted. To guard against this type of intrusion, many encryption / decryption algorithms and implementations exist. Encryption is the process of taking clear text and converting it into cypher data that is unreadable to anyone who does not know the key. Decryption reconstructs clear text from the cypher data, using the key. The sender performs encryption before transmission and the receiver decrypts to

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

4

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

reconstruct the information. Several implementations of various strength exist. Primary issues relate to strength and performance - time and space to encrypt and decrypt. Figure 5 is taken from the Java Tutorial and shows the service provider architecture that is used in the security frameworks provided by SUN with Java. The API (application program interface) provides a common interface for developing e-commerce or other distributed applications. To the extent possible, various alternative approaches to security are cast into a single interface. Engine classes abstractly define cryptographic services. Providers (implementing security services) write to the lower level SPI (service providers interface). For an example implementation see JCSI [5]. SUN also provides a default implementation which is distributed with the downloadable extensions. Where multiple implementations exist, initialization methods select the appropriate implementation based on parameters. This same approach is used, for example in Java's database connectivity, JDBC. Where multiple drivers exist, selection is wired-into the API through initialization methods. Although this architecture is a powerful approach that adds considerable value to the Java framework, in practice it is very difficult to achieve a single common interface that works equally well for all implementations.

Authenticity and integrity are just that and no more. Signed Java can be relied upon regarding who signed it and that it has not been disturbed in transmission. The fact that a digital signature has been verified tells the user nothing about the goodness of the code or the security of the system that is delivered in signed form. These are elements of trust in the individual or company that signed the code. To further the problem, security problems do and will continue to result from problems in the infrastructure upon which the Java implementation is built. For example, denial of service attacks, file access, host system intrusion and underlying problems with TCP/IP all arise to the applications built on these technologies. Nevertheless, e-commerce applications must be secure and the best way to build in security is to use best software practices and processes for their development. Specification and design of a secure distributed Java application should include security risks, requirements and underlying constraints. Development should proceed with a security risk assessment, followed by design and reviews from risk perspective. Security testing, which is necessarily different from specification testing, should consider likely avenues of problems and exercise documented successful attacks on similar systems. For further reading on security problems with Java and related technology, see: http://www.cigital.com/javasecurity/articles-1.html http://www.w3.org/pub/Conferences/WWW4/Papers/ 197/40.html and the Java security website: http://www.rstcorp.com/java-security.html For further reading in Security and Encryption, see Peter Guttmann's web site [4], which contains references to various research publications as well as software and other internet resources related to security and encryption.

7. References
IGURE 5. Service Provider Architecture

6. Closing Remarks
For a language that has developed and whose use has spread so rapidly, Java's features are remarkably complete and consistent. Nevertheless, security in Java applications is a difficult task. Java security mechanisms are complex and as such are likely to be inappropriately used by developers. The Java security model, together with the Java cryptography architecture are powerful tools that are integrated well into the language both in terms of controlling applications and in terms of defining security frameworks that are amenable to realization by multiple implementations.

[1.] McGraw, Gary and Felton, Ed; Securing Java, John Wiley and Sons Inc., 1999, see: http://www.securingjava.com/ [2.] Griscom, Daniel; Code Signing for Java Applets; see: http://www.suitable.com/Doc_CodeSigning.shtml [3.] Campione, Mary, et al., The Java Tutorial, SUN, Addison Wesley, December 2000, http://java.sun.com/docs/books/tutorial/ [4.] Guttmann, Peter; Security and Encryption-Related Resources and Links, http://www.cs.auckland.ac.nz/~pgut001/links.html [5.] Sun Microsystems Java Security and Crypto Implementation, http://www.cs.wustl.edu/~luther/Classes/Cs502/ WHITE-PAPERS/jcsi.html

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

5

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION
OF TIME SERIES*
JORGE MARI‚Ä†, ANDERS DAHLEÃÅN‚Ä†, AND ANDERS LINDQUIST‚Ä†

Abstract. In this paper we consider a three-step procedure for identiÔ¨Åcation of
time series, based on covariance extension and model reduction, and we present a
complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy
model is determined, which is Ô¨Ånally approximated by a lower-order model by
stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all
three steps has been lacking. Supposing the data is generated from a true Ô¨Ånitedimensional system which is minimum phase, it is shown that the transfer function
of the estimated system tends in H‚àû to the true transfer function as the data length
tends to inÔ¨Ånity, if the covariance extension and the model reduction is done properly. The proposed identiÔ¨Åcation procedure, and some variations of it, are evaluated
by simulations.

1. Introduction
In recent years there has been quite some interest in a certain type of procedures
for identiÔ¨Åcation of time series known as subspace methods [1, 42, 41, 28, 29]. These
identiÔ¨Åcation procedures are based on geometric projection methods, and they could
be understood in the context of splitting geometry and partial stochastic realization
theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9],
these procedures are algebraically equivalent to minimal factorization of a Hankel
matrix of covariance estimates, and they make no distinction between stochastic and
deterministic partial realizations. Therefore they may fail because of loss of positive
realness in the spectral estimation phase.
In an attempt to overcome these problems we analyze an alternative approach
to time series identiÔ¨Åcation proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the
maximum-entropy method, leading to a high order autoregressive (AR) process, and
Ô¨Ånally stochastically balanced truncation. This method shares certain features with
stochastic subspace identiÔ¨Åcation methods, the most obvious one being that it is
based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
‚àó This research was supported by a grant from the Swedish Research Council for Engineering
Sciences (TFR).
‚Ä† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
1

2

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for
maximum-likelihood (ML) methods, are needed.
The idea of approximating an autoregressive moving-average (ARMA) process by
an AR process is by no means new. Its origins can be traced back to the Wold
decomposition [55] where L2 -convergence of high-order AR models to general analytic
models is shown. Pioneers in the use of this concept for systems identiÔ¨Åcation are
Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations
were studied by Berk [2] and later reÔ¨Åned in [36, 34, 33, 7]. The interesting paper [7]
contains nice proofs of some of the convergence results needed in this paper, but, for
the sake of completeness and insight, we provide new proofs based on some properties
of fast Ô¨Åltering algorithms [5] and simple methods of complex analysis and SzegoÃã
polynomials. The power of the theory of SzegoÃã polynomials and Toeplitz matrices in
analyzing stochastic processes is reported in [24], but, except for elementary theory,
it has not been much used in systems identiÔ¨Åcation [39]. This is even more true for
the newer results [16, 40, 37, 27] on orthogonal polynomials.
The idea of using model reduction for systems identiÔ¨Åcation appears in the thesis
by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency
weighted reduction. Instead, we use stochastically balanced truncation, for which we
develop a simple computational procedure, exploiting the special structure of the AR
model. We also show the advantage of this reduction procedure by theoretical analysis
and simulations. In fact, a comprehensive study comprising all the steps mentioned
above together with a qualitative and quantitative analysis of the entire identiÔ¨Åcation
strategy has been lacking, and that is what we oÔ¨Äer in this paper.
The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identiÔ¨Åcation
procedure contributes to the estimation error. In Section 3 we show that the transfer
function of the maximum-entropy Ô¨Ålter, constructed from true covariances, tends to
that of the true Ô¨Ålter in H‚àû norm at a geometric rate determined by the largest
modulus of the zeros of the true Ô¨Ålter as the order of the maximum-entropy Ô¨Ålter
becomes large. However the order of the approximation is too high, and therefore
model reduction is performed. This is studied in Section 4. A stochastic balancing
procedure, based only on linear-algebra operations so that no Riccati equations need
to be solved, is provided together with the analysis of the model-reduction error.
Both deterministically and stochastically balanced truncation lead to good results.
However, when the covariances are estimated from statistical data, stochastic model
reduction is found to be superior. In particular, variances are considerably closer to
the CrameÃÅr-Rao bounds. In Section 5 we state our statistical convergence theorems,
proving that the total error tends to zero as the length of the data string tends to
inÔ¨Ånity, provided the degree of the AR model tends to inÔ¨Ånity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using
stochastic subspace identiÔ¨Åcation [43] is included. For clarity of exposition, all the
proofs have been deferred to two appendices, Appendix A dealing with the asymptotic
properties of the maximum-entropy Ô¨Ålter, and Appendix B devoted to the statistical
error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identiÔ¨Åcation
Time series identiÔ¨Åcation in the form studied here amounts to estimating the matrices
(A, B, C, D) in some n-dimensional linear stochastic system

x(t + 1) = Ax(t) + Bw(t)
(2.1)
y(t)
= Cx(t) + Dw(t)
driven by normalized white noise {w(t)}, from a data string of observations
{y0 , y1 , y2 , . . . , yN }

(2.2)

of the output process {y(t)}, which here will be taken to be scalar.
The basic idea behind our approach is very simple: given estimates of a partial
sequence
c 0 , c 1 , c 2 , . . . , cŒΩ

(2.3)

of the covariances ck = E{y(t+k)y(t)}, which satisÔ¨Åes the condition that the Toeplitz
matrix
Ô£Æ
Ô£π
c2 ¬∑ ¬∑ ¬∑ cŒΩ
c0 c1
Ô£Ø c1 c0
c1 ¬∑ ¬∑ ¬∑ cŒΩ‚àí1 Ô£∫
Ô£Ø
Ô£∫
Ô£Ø
c
c
c0 ¬∑ ¬∑ ¬∑ cŒΩ‚àí2 Ô£∫
1
TŒΩ+1 := Ô£Ø 2
(2.4)
..
.. Ô£∫
..
...
Ô£∞ ...
Ô£ª
.
.
.
cŒΩ cŒΩ‚àí1 cŒΩ‚àí2 ¬∑ ¬∑ ¬∑ c0
is positive deÔ¨Ånite, Ô¨Årst construct a high-order model continuing (2.3) by covariance
extension. This model has all the required positivity properties, but the order is too
high. Then reduce the order by means of a positivity-preserving model reduction
procedure to be speciÔ¨Åed below. That this simple recipe will in fact provide a good
identiÔ¨Åcation method is by no means a trivial matter but is based on some rather
deep results, which will be presented here.
More speciÔ¨Åcally, the approach consists of three steps, for which there are several
possible variants that will be discussed below. The rigorous mathematical analysis,
however, will be carried out for the following procedure, for which we shall give
theoretical bounds.
(i) Estimate a partial covariance sequence
cÃÇ0 , cÃÇ1 , cÃÇ2 , . . . , cÃÇŒΩ

(2.5)

from the time-series data (2.2) via the ergodic estimate
N ‚àík
1 	
yt+k yt
cÃÇk =
N + 1 t=0

k = 0, 1, . . . , ŒΩ.

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer
function
zŒΩ
,
(2.7)
WÃÇŒΩ (z) =
œÜÃÇŒΩ (z)
where œÜÃÇŒΩ (z) is the normalized SzegoÃà polynomial of degree ŒΩ, to be introduced
in Section 3, computed from the estimated covariance data (2.5).

4

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

(iii) Determine a reduced-degree approximation WÃÇ (z) of WÃÇŒΩ (z) via a stochastic
model reduction procedure [11] to be described in more detail in Section 4.
In this procedure, the idea is that ŒΩ >> n, the order of the system to be identiÔ¨Åed,
and ideally nÃÇ := deg WÃÇ equals the degree n of the true system (2.1). However, the
method will produce a valid model even if this is not the case or even if there is
no ‚Äútrue‚Äù underlying model. This is in contrast to stochastic subspace identiÔ¨Åcation
models, which may fail to produce any model at all [9].
There are possibilities for variations of the procedure described above. In Step (i)
we could use alternative covariance estimates or Burg‚Äôs estimation of Schur parameters
[3], the only requirements being that the estimated Toeplitz matrix TÃÇŒΩ+1 of (2.5) is
positive deÔ¨Ånite and that cÃÇk ‚Üí ck a.s. as N ‚Üí ‚àû. In Step (ii) we could instead use
approximate covariance extension or covariance extension with prescribed zeros, for
which there is now a complete parameterization [5] and an algorithm [4]. (In the latter
case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction
methods could be used. For example, an important model reduction paradigm is the
one based on optimal Hankel norm approximation [21].
Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable
transfer function
W (z) = C(zI ‚àí A)‚àí1 B + D,

(2.8)

of McMillan degree n. We also assume that W (z) is minimum-phase so that both
zeros and poles are located in the open unit disc. Then, we need to be able to measure
how the estimated model, with transfer function WÃÇ (z), converges to the true one as
N ‚Üí ‚àû. In this paper we have chosen to use distance between W (z) and WÃÇ (z) in
‚àû
norm as a measure of proximity between the true and estimated model. From an
engineering point of view this could be called worst case identiÔ¨Åcation. The modern
literature in robust control makes extensive use of the worst case philosophy; see for
example [20, 52]. There are also other reasons for using the ‚àû , as discussed in [35].
Returning, then, to the identiÔ¨Åcation approach outlined above, the estimation error
can be decomposed into three parts, one corresponding to each of the steps (i), (ii)
and (iii). Hence we have the error bound

L

L

W ‚àí WÃÇ ‚àû ‚â§ W ‚àí WŒΩ ‚àû + WŒΩ ‚àí WÃÇŒΩ ‚àû + WÃÇŒΩ ‚àí WÃÇ ‚àû ,

(2.9)

where WŒΩ is the AR model corresponding to the true covariances (2.3) and WÃÇŒΩ is
the one determined from the estimated covariances (2.6). To prove convergence to
zero of the estimation error (2.9), we shall need to assume that W is minimum-phase,
and hence WÃÇ should have the same property, which moreover is desirable in many
applications. Our procedure insures this.
Estimating the Ô¨Årst term in (2.9) is a problem in stochastic partial realization
theory and function theory and will be dealt with in the next section. The third term
concerns model reduction which will be studied, in the particular setting required
here, in Sections 4 and 5. In Section 5, Ô¨Ånally, we consider the second term together
with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence
Step (ii) in the identiÔ¨Åcation procedure outlined in Section 2 is based on rational
covariance extension. To understand this, let us consider the covariance extension
problem from a more general point of view. Given a partial covariance sequence
c 0 , c 1 , c 2 , . . . , cŒΩ ,

(3.1)

covariance extension amounts to Ô¨Ånding an inÔ¨Ånite extension cŒΩ+1 , cŒΩ+2 , cŒΩ+3 , . . . of
this sequence such that the function
V (z) := 12 c0 + c1 z ‚àí1 + c2 z ‚àí2 + . . .

is strictly positive real, i.e., it is an analytic function in the complement Dc of the
open unit disc D, which maps Dc to the open right complex half-plane. Then
Œ¶(z) := V (z) + V (z ‚àí1 )
is a spectral density for a process having c0 , c1 , . . . , cŒΩ as its Ô¨Årst ŒΩ covariances and
which is coercive in the sense that
Œ¶(eiŒ∏ ) > 0 for all Œ∏.
Spectral factorization is then to Ô¨Ånd a stable transfer function W (z) such that
|W (eiŒ∏ )|2 = Œ¶(eiŒ∏ ).
In particular, we are interested in Ô¨Ånding covariance extensions for which V (z), and
hence W (z), have at most degree ŒΩ.
For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is
classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one
correspondence between inÔ¨Ånite covariance sequences
c0 , c1 , c2 , c3 , . . .

(3.2)

and a sequence of Schur parameters, or reÔ¨Çection coeÔ¨Écients,
Œ≥0 , Œ≥1 , Œ≥2 , Œ≥3 , . . . ,

(3.3)

with the property |Œ≥t | < 1 for all t. In fact, Ô¨Åxing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and Œ≥0 , Œ≥1 , . . . , Œ≥m‚àí1 for
each m. The Schur parameters can be determined from the covariances via the SzegoÃà
polynomials
œït (z) = z t + œït1 z t‚àí1 + ¬∑ ¬∑ ¬∑ + œïtt t = 0, 1, 2 . . . ,
computed by means of the SzegoÃà-Levinson recursion
 






 
 


z
‚àíŒ≥t œït (z)
œï0 (z)
1
œït+1 (z)
=
;
=
,
(3.4)
‚àízŒ≥t 1
1
œï‚àót+1 (z)
œï‚àót (z)
œï‚àó0 (z)
where

œï‚àót (z) := z t œït (z ‚àí1 )
is the reciprocal polynomial of œït (z), and the Schur parameters are computed via

= r1t tj=0 œït,t‚àíj cj+1
Œ≥t
(3.5)
rt+1 = rt (1 ‚àí |Œ≥t |2 ), r0 = c0 .

6

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Hence Œ≥t = ‚àíœït+1 (0), a fact that we shall use below.
In the problem to Ô¨Ånd a covariance extension for (3.1), therefore, Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1
are Ô¨Åxed and the inÔ¨Ånite continuation Œ≥ŒΩ , Œ≥ŒΩ+1 , . . . can be chosen freely. In particular,
if we take Œ≥t = 0 for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . . We obtain the maximum entropy solution
WŒΩ (z) =

zŒΩ
,
œÜŒΩ (z)

(3.6)

where œÜŒΩ (z) is the normalized SzegoÃà polynomial
1
œÜŒΩ (z) := ‚àö œïŒΩ (z).
rŒΩ

(3.7)

Thus, in this particular case, the solution to the covariance extension problem turns
out to be rational of degree at most ŒΩ as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations
are needed. In fact, it has recently been shown [5] that there is exactly one such
solution for each choice of zeros of WŒΩ (z), thus proving a long-standing conjecture
by Georgiou [18], who had established existence. Nevertheless, as we shall see next,
rationality implies that the Schur parameters tend geometrically to zero, provided
W (z) has no zeros on the unit circle.
In this section we shall demonstrate that the rational transfer function (2.8) can be
approximated arbitrarily closely in L‚àû by the transfer function WŒΩ (z) of a maximum
entropy Ô¨Ålter for suÔ¨Éciently large ŒΩ and that this ŒΩ depends on the maximum modulus
of the zeros of W (z). We shall Ô¨Årst present a heuristic argument in support of this
conclusion.
To this end, let (3.2) be the inÔ¨Ånite covariance sequence of the output process y in
(2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via
the SzegoÃà-Levinson algorithm presented above. Then we have the following special
case of Corollary 2.1 in [5].
Lemma 3.1. Let the spectral density
Œ¶(eiŒ∏ ) = |W (eiŒ∏ )|2

(3.8)

be coercive in the sense that it is positive for all Œ∏ and let (3.3) be the corresponding
inÔ¨Ånite sequence of Schur parameters. Moreover, let Œ≥ ‚àà (0, 1) be greater than the
maximum of the moduli of the zeros of W (z). Then
|Œ≥t | = O(Œ≥ t ),

(3.9)

i.e., |Œ≥t | ‚â§ M Œ≥ t for some M ‚àà R and for suÔ¨Éciently large t.
Remark 3.2. Since (3.9) holds for all Œ≥ greater than the the maximum of the moduli
of the zeros of W (z), we have in fact that |Œ≥t | = o(Œ≥ t ), i.e., limt‚Üí‚àû |Œ≥t |Œ≥ ‚àít = 0.
For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis
of certain fast algorithms for Kalman Ô¨Åltering [6].

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the
spectral density
z(z ‚àí 1)2
Œ¶(z) = ‚àí 2
(z + z + 2)(2z 2 + z + 1)
is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters
are seen to be ‚àí1/2, ‚àí2/3, ‚àí2/5, ‚àí2/7, ‚àí2/9, ‚àí2/11, . . . , which tend to zero but not
geometrically. On the other hand, there are coercive, analytic but nonrational models
which also exhibit geometric convergence rate. A classical example [23] is obtained
2
when ck = Œ∏k for some Œ∏ ‚àà (‚àí1, 1). The Schur parameters in this case form an exact
geometric sequence, Œ≥k = (‚àíŒ∏)k+1 , k ‚â• 0.
Lemma 3.1 implies that, for a suÔ¨Éciently large ŒΩ which depends on Œ≥, the Schur
parameters Œ≥t are close to zero for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . . But, the Schur parameters
of WŒΩ are exactly zero for t = ŒΩ, ŒΩ + 1, ŒΩ + 2, . . . , and hence geometric convergence
would insure that WŒΩ is a good approximation of W (z) for suÔ¨Éciently large ŒΩ. We
shall prove that this is indeed the case.
Theorem 3.4. Suppose W (z) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let Œ≥ ‚àà (0, 1) be greater than the maximum of the moduli of the
zeros of W (z). Then
lim WŒΩ ‚àí W ‚àû = 0,

ŒΩ‚Üí‚àû

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that
WŒΩ ‚àí W ‚àû ‚â§ M Œ≥ ŒΩ .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to Ô¨Årst showing
that
lim WŒΩ‚àí1 ‚àí W ‚àí1 ‚àû = 0.

ŒΩ‚Üí‚àû

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we
give an alternative proof of this fact based on SzegoÃà theory, and also show that the
convergence is geometric. In fact, we can choose Œ≥ arbitrarily close to the maximum
modulus of the zeros of W .
However, as we shall see next, we can actually prove more. To this end, let us Ô¨Årst
observe that, since WŒΩ‚àí1 and W ‚àí1 have their poles in the open unit disc D and thus
are bounded and analytic in the complement Dc of D, they belong to the Hardy space
‚àû
of functions which are analytic and bounded in {z ‚àà C | |z| > 1}. Hence the
H‚àí
‚àû
, and
convergence (3.12) is in H‚àí
z ‚àíŒΩ œÜŒΩ (z) ‚Üí W ‚àí1 (z)

(3.13)

uniformly in each compact subset of Dc . Now, W ‚àí1 is analytic in {z ‚àà C | |z| ‚â• Œ≥},
a region that is strictly larger than Dc . This in itself of course does not insure that
the convergence (3.13) extends to this larger region. In fact, even if z ‚àíŒΩ œÜŒΩ (z) did
converge in {z ‚àà C | Œ≥ ‚â§ |z| ‚â§ 1}, it could fail to converge to W ‚àí1 (z) there. The fact
that it really does converge uniformly to this limit is another consequence of Lemma
3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and
hence an estimate of the convergence rate Œ≥, is given in [35].
Theorem 3.5. Suppose W (z) is a minimum-phase rational function having all its
poles in the open unit disc D and all its zeros in

DœÅ := {z ‚àà C | |z| ‚â§ œÅ} ‚äÇ D

where 0 < œÅ < 1,

and let {œÜŒΩ (z)}‚àû
0 be the normalized SzegoÃà polynomial (3.7) determined from the covariances in the spectral density
|W (e )| = c0 + 2
iŒ∏

2

‚àû
	

ck cos kŒ∏.

k=1

Then, as ŒΩ ‚Üí ‚àû, z ‚àíŒΩ œÜŒΩ (z) ‚Üí W ‚àí1 (z) uniformly in every compact subset of
{z ‚àà C | |z| > œÅ}, the complement of DœÅ .

DcœÅ :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of œÜŒΩ (z) and hence of the poles of the high-order AR
model with transfer function WŒΩ (z). It is known that, if the Toeplitz matrix TŒΩ+1
is positive deÔ¨Ånite, all roots of œÜŒΩ (z) are located in the open unit disc D, but little
has been reported in the literature on their behavior as ŒΩ ‚Üí ‚àû. This behavior is
illustrated in Figure 3.1.
Original system

Original system

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0

0.5

1

‚àí1
‚àí1

Original and AR(24)
1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí0.5

0

0.5

0

0.5

1

Original and AR(24)

1

‚àí1
‚àí1

‚àí0.5

1

‚àí1
‚àí1

‚àí0.5

0

0.5

1

Figure 3.1: Distribution of zeros of œÜŒΩ (z).

The top two diagrams show the zero-pole positions, within the boundaries of the
unit circle, of two minimum phase spectral factors W , both of degree Ô¨Åve. Also
indicated is a circle of radius equal to the maximum modulus of the zeros of these
spectral factors. The little circles ‚Äú‚ó¶‚Äù represent zeros and the ‚Äú+‚Äù sign represent
poles. The lower two Ô¨Ågures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the
exact covariance sequence. The poles of the latter models are indicated with ‚Äú√ó‚Äù.
The left part of Figure 3.1 illustrates what may happen if all the poles of W (z) are
located in {z ‚àà C | |z| < œÅ}, where œÅ is chosen to be the maximum of the moduli of
the zeros of W (z). The roots of œÜŒΩ (z) tend to cluster inside a circle of radius œÅ as
ŒΩ ‚Üí ‚àû. This phenomenon is in a sense predictable, since the constant term of the
SzegoÃà polynomials is œïn+1 (0) = ‚àíŒ≥n , which equals the product of the roots and, by
Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to œÅ.
This does not preclude that other types of crowns may occur, because subsequences
of {Œ≥n } could decay faster than the overall rate Œ≥, as follows from [5]. Very general
statements about the distribution of zeros of orthogonal polynomials, derived with
the help of potential-theoretic methods, can be found in [37, 27].
To the right in Figure 3.1 we see what happens in the case that W has poles with
moduli larger than œÅ. Then, for ŒΩ suÔ¨Éciently large, the normalized SzegoÃà polynomial
œÜŒΩ (z) has roots in {z ‚àà C | œÅ ‚â§ |z| < 1}, but exactly as many as the poles of W in
this region and approximately at the same place as these. This is of course due to
the uniform convergence of z ‚àíŒΩ œÜŒΩ (z) to W ‚àí1 (z) in every compact subset of DcœÅ . The
other roots of œÜŒΩ (z) behave exactly as in the previous case and tend to accumulate in
a crown inside and very close to the circle {z ‚àà C | |z| = œÅ}.
‚àû
approximation WŒΩ of W which can be made
We have thus constructed an H‚àí
arbitrarily good by choosing ŒΩ suÔ¨Éciently large. However, WŒΩ will have much larger
degree and, except for the poles outside the circle {z ‚àà C | |z| = œÅ}, a completely
diÔ¨Äerent zero-pole pattern. We shall rectify this situation by model reduction. In
fact, for the moment considering the perfect modeling problem to identify the rational
transfer function (2.8) given an exact partial covariance sequence (3.1), the last step
in our procedure consists in approximating WŒΩ by a rational function Wred of smaller
degree, ideally of the same degree as W .
The simplest model reduction procedure is deterministically balanced truncation
(DBT), Ô¨Årst introduced by Moore [38]. Though easy to implement, it may fail to
yield a minimum-phase approximation, a requirement which is important in certain
contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced
truncation (SBT), Ô¨Årst introduced by Desai and Pal [10], which is based on a diÔ¨Äerent
balancing strategy to be explained in detail in Section 4.
Original system

Reduction by DBT

Reduction by SBT

1

1

1

0.5

0.5

0.5

0

0

0

‚àí0.5

‚àí0.5

‚àí0.5

‚àí1
‚àí1

0

1

‚àí1
‚àí1

0

1

‚àí1
‚àí1

0

1

Figure 3.2: Zero-pole pattern of W (z) and Wred (z) for diÔ¨Äerent model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This Ô¨Åfth-order

10

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

model has Ô¨Årst been approximated by WŒΩ of degree ŒΩ = 24, producing the pole-zero
pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens
when the model is reduced back to order Ô¨Åve by either deterministically balanced
truncation or stochastically balanced truncation. The zeros are denoted by ‚Äú‚ó¶‚Äù and
the poles by ‚Äú+‚Äù. Both reduction procedures give good approximations when applied
to exact covariance data. However, as we shall see in Section 5, the advantages of SBT
becomes apparent when applied to statistical data. Also, as explained in Remark 4.5,
there are theoretical reasons to prefer stochastic model reduction.
4. Model reduction
In the present setting, model reduction amounts to replacing a stochastic system
(2.1) of dimension ŒΩ by one of some dimension r < ŒΩ in such a way that most of
its statistical features are retained. In particular, we want to remove the part of the
system which corresponds to the weakest correlation between past and future. This
idea can be formalized in the following way.
Basic concepts. In the Hilbert space generated by the random variables {y(t) |
‚àí‚àû < t < ‚àû} in the inner product u, v = E{uv}, let H ‚àí be the subspace generated
by the past, i.e., {y(t) | t < 0}, and H + that generated by the future {y(t) | t ‚â• 0}.
Consider the Hankel operator H : H + ‚Üí H ‚àí and its adjoint H‚àó : H ‚àí ‚Üí H + deÔ¨Åned
as

H = EH

‚àí

|H +

and

H‚àó = E H

+

|H ‚àí ,

(4.1)

‚àí

where E H denotes orthogonal projection onto the past space H ‚àí . More precisely,
H sends Œæ ‚àà H + to E H ‚àí Œæ ‚àà H ‚àí and H‚àó sends Œ∑ ‚àà H ‚àí to E H + Œ∑ ‚àà H +. Since the
process y is the output of a minimal stochastic system of dimension ŒΩ, rank H = ŒΩ by
Kronecker‚Äôs Theorem [56], and hence H has exactly ŒΩ singular values, œÉ1 , œÉ2 , . . . , œÉŒΩ ,
which are positive, as usually listed so that œÉ1 ‚â• œÉ2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉŒΩ . These singular
values are the canonical correlation coeÔ¨Écients and hence the cosines of the angles
between the principal directions of the past space H ‚àí and the future space H + . They
are therefore less than one, and the part of the stochastic system corresponding to
singular values which are close to zero have a weak coupling between past and future,
i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic
model reduction is to truncate the system so that this part is removed.
To each singular value œÉk there is an associated Schmidt pair (Œæk , Œ∑k ) with Œæk ‚àà H +
and Œ∑k ‚àà H ‚àí such that

HŒæk = œÉk Œ∑k ,

H‚àóŒ∑k = œÉk Œæk ,

and such that the sequences Œæ1 , Œæ2 , Œæ3 , . . . and Œ∑1 , Œ∑2 , Œ∑3 , . . . of singular vectors are
orthonormal. The singular vectors corresponding to nonzero singular values span the
predictor spaces
X‚àí := span{Œ∑1 , Œ∑2 , . . . , Œ∑ŒΩ },

X+ := span{Œæ1 , Œæ2 , . . . , ŒæŒΩ }.

Clearly, X‚àí ‚äÇ H ‚àí and X+ ‚äÇ H + .
The process y has one representation (2.1) for each minimal spectral factor W ,
having W as its transfer function. Such representations are called minimal stochastic
realizations and the corresponding subspaces X := {a x(0) | a ‚àà RŒΩ } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X‚àí is the splitting subspace of the stochastic
realization

x‚àí (t + 1) = Ax‚àí (t) + B‚àí w‚àí (t)
(4.2)
y(t)
= Cx‚àí (t) + D‚àí w‚àí (t)
with the transfer function W‚àí (z), the minimum-phase spectral factor; and X+ is the
splitting subspace of

x+ (t + 1) = Ax+ (t) + B+ w+ (t)
(4.3)
y(t)
= Cx+ (t) + D+ w+ (t)
with transfer function W+ (z), the maximum-phase spectral factor, having all its zeros
in Dc . Note that A and C are the same in both realizations (uniform choice of bases).
Each realization has a counterpart which evolves backwards in time and has the
same splitting subspace. For example, the backward realization of X+ ,

xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ wÃÑ+ (t)
,
(4.4)
y(t)
= CÃÑ xÃÑ+ (t) + DÃÑ+ wÃÑ+ (t)
has transfer function WÃÑ+ (z), the coanalytic minimum-phase spectral factor, having all
its poles and zeros in Dc . In the present case with scalar y, we have WÃÑ+ (z) = W‚àí (z ‚àí1 ).
Now, in order to identify the part of the system which has the weakest coupling
between past and future, and hence will be removed in the model reduction, we need
to balance the system in the sense of Desai and Pal, as we shall explain next. To this
end, we make a coordinate transformation
(A, C, CÃÑ) ‚Üí (SAS ‚àí1 , CS ‚àí1 , CÃÑS  ),

(4.5)

in the minimal realization of
1
(4.6)
V (z) = C(zI ‚àí A)‚àí1 CÃÑ  + c0 ,
2
the strictly positive real part of the spectral density of y, so that the state covariances
P‚àí := E{x‚àí (t)x‚àí (t) } and PÃÑ+ = E{xÃÑ+ (t)xÃÑ+ (t) } coincide with the diagonal ŒΩ √ó ŒΩ
matrix Œ£ of nonzero canonical correlation coeÔ¨Écients, i.e.,
P‚àí = PÃÑ+ = Œ£ := diag(œÉ1 , œÉ2 , . . . , œÉŒΩ ).

(4.7)

1

This is done by choosing S so that Sx‚àí (0) = Œ£ 2 Œ∑, where Œ∑ = (Œ∑1 , Œ∑2 , . . . , Œ∑ŒΩ ) , and
1
(S  )‚àí1 xÃÑ+ (0) = Œ£ 2 Œæ, where Œæ := (Œæ1 , Œæ2 , . . . , ŒæŒΩ ) .
To compute the canonical correlation coeÔ¨Écients, we Ô¨Årst observe that the eigenvalues of the product P‚àí PÃÑ+ are precisely the squares of the canonical correlation
coeÔ¨Écients, i.e.,
Œª(P‚àí PÃÑ+ ) = Œª(P‚àí P+‚àí1 ) = {œÉ12 , œÉ22 , . . . , œÉŒΩ2 },

(4.8)

where we have used the fact that the state covariance of (4.3) is P+ = PÃÑ+‚àí1 . Therefore
the canonical correlation coeÔ¨Écients can then be determined via (4.8) by solving the
Lyapunov equations

P‚àí = AP‚àí A + B‚àí B‚àí


and P+ = AP+ A + B+ B+
.

(4.9)

12

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

The point is now to identify the canonical correlation coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉr corresponding to the part of the system one wants to keep. The part corresponding to
œÉr+1 , œÉr+2 , . . . , œÉŒΩ will be disposed of. This amounts to partitioning Œ£ as



Œ£1
,
(4.10)
Œ£=
Œ£2
where Œ£1 is r √ó r.
In order to reduce model (2.1) we make the coordinate transformation (A, B, C) ‚Üí
(SAS ‚àí1 , SB, CS ‚àí1 ), with the same balancing transformation S. Then, partition the
new triplet (A, B, C) conformally with (4.10) as


 




B1
A11 A12
(4.11)
, B=
, C = C1 C2 ,
A=
A21 A22
B2
and perform a principal subsystem truncation to obtain the transfer function of a
reduced-order system
Wred (z) = C1 (zI ‚àí A11 )‚àí1 B1 + D

(4.12)

of degree r. If Œ£2 is close to zero, while Œ£1 is not, the rank of H is close to r, and the
discarded part of the system gives a negligible contribution to y.
Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy Ô¨Ålter
‚àö ŒΩ
rŒΩ z
(4.13)
W‚àí (z) := WŒΩ (z) =
œïŒΩ (z)
of order ŒΩ, which, for the moment we denote W‚àí (z) to emphasize its character as the
minimum-phase spectral factor of the spectral density
rŒΩ
.
œïŒΩ (z)œïŒΩ (z ‚àí1 )
Remark 4.1. Without loss of generality we assume that œïŒΩ (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller ŒΩ for which this condition holds.
In fact, œïŒΩ (0) = Œ≥ŒΩ‚àí1 , and if Œ≥ŒΩ‚àíp = Œ≥ŒΩ‚àíp+1 = ¬∑ ¬∑ ¬∑ = Œ≥ŒΩ‚àí1 = 0 and Œ≥ŒΩ‚àíp‚àí1 = 0 for some
p = 1, 2, . . . , ŒΩ, then œïŒΩ (z) = z ŒΩ‚àíp œïŒΩ‚àíp (z) by (3.4), and hence (3.6) can be replaced
by WŒΩ (z) = WŒΩ‚àíp (z), and for WŒΩ‚àíp (z) the required condition holds.
The maximum-phase spectral factor W+ (z) has all its zeros at inÔ¨Ånity, and hence
‚àö
rŒΩ

‚àí1
W+ (z) = h (zI ‚àí F ) b =
,
(4.14)
œïŒΩ (z)
where (F, b, g) is the (observable) canonical form
Ô£π
Ô£Æ
Ô£π
Ô£Æ
0
0
1
¬∑¬∑¬∑
0
..
.. Ô£∫
. Ô£∫
...
Ô£Ø ...
.
. Ô£∫, b = Ô£Ø
Ô£Ø .. Ô£∫ ,
F =Ô£Ø
Ô£∞ 0 Ô£ª
Ô£∞ 0
0
¬∑¬∑¬∑
1 Ô£ª
‚àö
‚àíœïŒΩŒΩ ‚àíœïŒΩ,ŒΩ‚àí1 ¬∑ ¬∑ ¬∑ ‚àíœïŒΩ1
rŒΩ

Ô£Æ Ô£π
1
Ô£Ø0Ô£∫
Ô£∫
h=Ô£Ø
Ô£∞ ... Ô£ª ,
0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

œïŒΩ1 , œïŒΩ2 , . . . , œïŒΩŒΩ being the coeÔ¨Écients of the SzegoÃà polynomial œïŒΩ (z). In this basis,
it follows from (4.9) that

  œÄ

1
iŒ∏
‚àí1  ‚àíiŒ∏
 ‚àí1
(e I ‚àí A) bb (e I ‚àí A ) dŒ∏
[P+ ]jk =
2œÄ ‚àíœÄ
jk
 œÄ
1
r
ŒΩ
=
e‚àí(j‚àík)iŒ∏
dŒ∏ = cj‚àík ,
2œÄ ‚àíœÄ
œïŒΩ (eiŒ∏ )œïŒΩ (e‚àíiŒ∏ )
and hence P+ = TŒΩ . It is well-known and easy to prove that Œ¶ŒΩ TŒΩ Œ¶ŒΩ = RŒΩ , where

Œ¶ŒΩ+1

Ô£Æ
œïŒΩŒΩ
Ô£Ø ..
Ô£Ø .
Ô£Ø
= Ô£ØœïŒΩ2
Ô£Ø
Ô£∞œïŒΩ1
1

œïŒΩ‚àí1,ŒΩ‚àí1
..
.

œïŒΩ‚àí2,ŒΩ‚àí2
..
.

œïŒΩ‚àí1,1
1

1

¬∑¬∑¬∑

Ô£π
1
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ
rŒΩ‚àí1
Ô£Ø
Ô£Ø
and RŒΩ = Ô£Ø
Ô£∞

Ô£π
Ô£∫
Ô£∫
Ô£∫ , (4.16)
Ô£ª

rŒΩ‚àí2
..

.
r0

and consequently
PÃÑ+ = TŒΩ‚àí1 = Œ¶ŒΩ RŒΩ‚àí1 Œ¶ŒΩ .
It remains to determine P‚àí . From (4.13) is easy to see that
‚àö
W‚àí (z) = ‚àíœïŒΩ (zI ‚àí F )‚àí1 b + rŒΩ ,
where



œïŒΩ := œïŒΩŒΩ œïŒΩ,ŒΩ‚àí1 ¬∑ ¬∑ ¬∑ œïŒΩ1 ,

(4.17)

(4.18)

(4.19)

but, in order to determine P‚àí , this realization needs to be transformed so that the A
and C matrices are the same as in (4.14) (uniform choice of bases). More precisely,
we need to perform a transformation
(F, b, ‚àíœïŒΩ ) ‚Üí (QF Q‚àí1 , Qb, ‚àíœïŒΩ Q‚àí1 ) =: (F, Qb, h ).
Then P‚àí is the solution of the Lyapunov equation P‚àí = F P‚àí F  + Qbb Q , and therefore, since TŒΩ = F TŒΩ F  + bb and QF = F Q and consequently
QTŒΩ Q = F QTŒΩ Q F  + Qbb Q ,
we have
P‚àí = QTŒΩ Q .
To determine Q, notice that ‚àíœïŒΩ = h Q and QF
Ô£π Ô£Æ
Ô£Æ
h
‚àíœïŒΩ

Ô£Ø ‚àíœïŒΩ F Ô£∫ Ô£Ø h F
Ô£∫=Ô£Ø .
Ô£Ø
..
Ô£ª Ô£∞ ..
Ô£∞
.
‚àíœïŒΩ F ŒΩ‚àí1

(4.20)
= F Q to form
Ô£π
Ô£∫
Ô£∫ Q = Q.
Ô£ª

(4.21)

h F ŒΩ‚àí1

Next, deÔ¨Åne the symmetric matrix
M := RŒΩ‚àí1/2 Œ¶ŒΩ QTŒΩ Q Œ¶ŒΩ RŒΩ‚àí1/2 .

(4.22)

14

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

In view of (4.20) and (4.17), det(zI ‚àí M ) = det(zI ‚àí P‚àí PÃÑ+ ), and hence, by (4.8), M
has the eigenvalues œÉ12 , œÉ22 , . . . , œÉŒΩ2 , and the singular-value decomposition
M = U Œ£2 U  ,

(4.23)

where U  U = U U  = I. It is then well-known and simple to check that
S := Œ£‚àí1/2 U  RŒΩ‚àí1/2 Œ¶ŒΩ

(4.24)

is the required balancing transformation (4.5) such that SP‚àí S  = (S  )‚àí1 PÃÑ+ S ‚àí1 = Œ£.
Proposition 4.2. Given the partial covariance sequence
ck = E{y(t + k)y(t)},

k = 0, 1, . . . , ŒΩ,

let œï1 (z), œï2 (z), . . . , œïŒΩ (z) and r0 , r1 , . . . , rŒΩ be the corresponding SzegoÃà polynomials
and error variances. Supposing that Œ≥ŒΩ‚àí1 = ‚àíœïŒΩ (0) = 0, let (F, b, h) be given by
(4.15), RŒΩ and Œ¶ŒΩ by (4.16) and Q by (4.21). Moreover, let U and Œ£ be deÔ¨Åned by
the singular value decomposition (4.23) of (4.22). Then, the canonical correlation
coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉŒΩ are the diagonal elements of Œ£, as described in (4.7), and
the stochastically balanced realization of WŒΩ is given by
‚àö
(4.25)
(A, B, C, D) = (SF S ‚àí1 , SQb, h S ‚àí1 , rŒΩ ),
where S is deÔ¨Åned by (4.24).
Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to
yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase
property. In fact, we have the following result, the proof of which is given in Section A.
Theorem 4.3. Let Wred be the SBT approximation of degree r of WŒΩ , and set

ŒΩ
ŒΩ‚àí1
	
‚àö  1 + |Œ≥k |
œÉk
9 := 2
and Œ∫ := c0
,
(4.26)
1
‚àí
œÉ
1
‚àí
|Œ≥
k
k|
k=r+1
k=0
where Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1 are the Schur parameters of c0 , c1 , c2 , . . . , cŒΩ . Then
c0 (1 ‚àí 9)Œ∫‚àí1 ‚â§ |Wred (eiŒ∏ )| ‚â§ (1 + 9)Œ∫ for all Œ∏,

(4.27)

and, if 9 < 1, Wred is minimum phase. Finally, the approximation error has the bound
WŒΩ ‚àí Wred ‚àû ‚â§ 9Œ∫.

(4.28)

A properly executed SBT procedure should imply that the canonical correlation
coeÔ¨Écients œÉr+1 , . . . , œÉŒΩ , and hence 9, are close to zero, insuring the minimum-phase
condition.
Remark 4.4. Stochastic model reduction can also be carried out by instead performing principal subsystem truncation on (A, C, CÃÑ) in VŒΩ (z) = C(zI ‚àí A)‚àí1 CÃÑ + 12 c0 ,
where A and C are given by (4.25) and CÃÑ  = S(c1 , c2 , . . . , cn ). It was shown in
[32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally,
the spectral density Œ¶red (z) := Vred (z) + Vred (z ‚àí1 ) is factorized to yield a minimumphase spectral factor WÃÉ . This is in a sense a more natural procedure, but we do
not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small Œ£2 it yields almost the same result. In fact, it is shown in [53], that
|WÃÉ (eiŒ∏ )|2 = |Wred (eiŒ∏ )|2 + H(eiŒ∏ )Œ£2 H(e‚àíiŒ∏ ), where H(z) = C1 (zI ‚àí A11 )‚àí1 A12 .
Remark 4.5. There are good reasons to prefer stochastic over deterministic model
reduction, as seen from the following heuristics. In fact, it can be seen that
VŒΩ (z) =

c0 œàŒΩ (z)
,
2 œïŒΩ (z)

(4.29)

where œàŒΩ (z) is the SzegoÃà polynomial of the second kind (obtained by exchanging ‚àíŒ≥t
for Œ≥t in the recursion (3.4)). Now, the matrix representation of the Hankel operator
H in the innovation bases of the past and the future, provided by w‚àí and wÃÑ+ respecis the inÔ¨Ånite Hankel matrix of the sequence
tively, is given by L‚àí1 (L‚àí1 ) , where
c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T‚àû ;
see, e.g., [32, p. 714]. It is easy to see that œàŒΩ (z) has the same asymptotic behavior as
œïŒΩ (z), i.e., the roots tend to cluster uniformly inside the circle z = œÅ as ŒΩ ‚Üí ‚àû, and
hence these roots are close to canceling in (4.29). Consequently, the corresponding
Hankel matrix is close to having low rank. This massive ‚Äúalmost cancellation‚Äù does
not occur in WŒΩ (z), and hence the corresponding inÔ¨Ånite Hankel matrix, constructed
from the Laurent coeÔ¨Écients of WŒΩ (z), may have a less distinct separation between
Œ£1 and Œ£2 . On the other hand, since the Schur parameters tend geometrically to
zero, the lower part of L tends to the identity, and hence the asymptotic behavior of
the canonical correlation coeÔ¨Écients is very much like that of the singular values of
. Therefore we may expect SBT to have better statistical behavior than DBT. In
Section 6 we shall see that this is the case.

H

H

H

H

5. IdentiÔ¨Åcation from statistical data
We now return to our original problem of time series identiÔ¨Åcation: Given a data
string (2.2) of observations of the output process y of some n-dimensional linear
stochastic system (2.1) with minimum-phase transfer function W (z), given by (2.8),
Ô¨Ånd an estimate (AÃÇ, BÃÇ, CÃÇ, DÃÇ) of the matrices (A, B, C, D).
The identiÔ¨Åcation method proceeds as follows. Given the covariance estimates (2.5),
we compute the corresponding maximum entropy Ô¨Ålter (2.7), a balanced realization
(4.25), and the canonical correlation coeÔ¨Écients
œÉÃÇ1 , œÉÃÇ2 , œÉÃÇ3 , . . . , œÉÃÇŒΩ ,

(5.1)

determined as in Proposition 4.2 from the covariance estimates cÃÇ0 , cÃÇ1 , . . . , cÃÇŒΩ .
Based on (5.1), choose an integer nÃÇ such that œÉÃÇnÃÇ+1 , œÉÃÇnÃÇ+2 , . . . , œÉÃÇŒΩ are close to zero or
at least distinctively smaller than œÉÃÇ1 , œÉÃÇ2 , . . . , œÉÃÇnÃÇ . Then, the balanced realization (4.25)
is truncated accordingly as in (4.11) to yield a nÃÇ-dimensional triplet (A11 , B1 , C1 ) and
a transfer function
WÃÇ (z) = C1 (zI ‚àí A11 )‚àí1 B1 + D.

(5.2)

Then, (A11 , B1 , C1 , D) is the required estimate (AÃÇ, BÃÇ, CÃÇ, DÃÇ).
As pointed out in Section 2, we have a bound
W ‚àí WÃÇ ‚àû ‚â§ W ‚àí WŒΩ ‚àû + WŒΩ ‚àí WÃÇŒΩ ‚àû + WÃÇŒΩ ‚àí WÃÇ ‚àû ,

(5.3)

16

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the Ô¨Årst term W ‚àí WŒΩ ‚àû , which
does not depend on the statistical data (2.2) but only on the underlying system (2.1),
tends to zero geometrically with a rate Œ≥ ‚àà (0, 1) as ŒΩ ‚Üí ‚àû. The other two terms
depend on the data (2.2), and here N must grow at a faster rate than ŒΩ. In fact, we
shall assume that
ŒΩ = ŒΩ(N ) = O(log N ),

(5.4)

which in particular requires that limN ‚Üí‚àû NŒΩ = 0. We also need to assume that the
white noise process in (2.1) satisÔ¨Åes a mild technical condition, namely
E{w(t)4 } < ‚àû.

(5.5)

This condition is, of course, satisÔ¨Åed if w is Gaussian.
Next, we present our main convergence theorem.
Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then,
to each length N of the data string (2.2), there is a ŒΩ(N ), tending to inÔ¨Ånity with N
at the rate (5.4), such that any sequence of estimated transfer functions WÃÇ of Ô¨Åxed
degree nÃÇ ‚â• n, determined, for each N and corresponding ŒΩ = ŒΩ(N ), by the procedure
described above, satisÔ¨Åes
W ‚àí WÃÇ ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû. For suÔ¨Éciently large ŒΩ(N ), the transfer function WÃÇ has
minimum phase.
We have already proven that the Ô¨Årst term in (5.3) tends to zero, so Theorem 5.1
follows from the next two theorems, each corresponding to one of the remaining terms
in (5.3). As for the second term, we have the following result, the proof of which is
deferred to Appendix B.
Theorem 5.2. Suppose the system (2.1) satisÔ¨Åes the conditions of Theorem 5.1. Let
WŒΩ be the maximum-entropy Ô¨Ålter (3.6) determined from the partial covariance sequence (3.1) of y and let WÃÇŒΩ be the corresponding function determined from the ergodic
estimates (2.5). Then, if ŒΩ(N ) is deÔ¨Åned as in Theorem 5.1,
WŒΩ(N ) ‚àí WÃÇŒΩ(N ) ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû.
There are several results of this type in the literature [2, 36, 7, 33]. In particular,
3
Berk [2] proved that, provided ŒΩN ‚Üí 0 as N ‚Üí ‚àû and Œ¶ is coercive (i.e. positive
 iŒ∏ ) ‚Üí Œ¶(eiŒ∏ ) in probability.
on the unit circle), the estimated AR spectral density Œ¶(e
Under the same hypotheses, Caines and Baykal-GuÃàrsoy [7] showed that if N ‚â• ŒΩ 5+Œ∑
for some Œ∑ > 0, then WÃÇŒΩ‚àí1 ‚àí W ‚àí1 ‚àû ‚Üí 0 almost surely as ŒΩ ‚Üí ‚àû. However, in both
cases, ergodic estimates are used which are not quite the same as (2.5).
Finally, we consider the last term in (5.3). The proof of the following theorem is
given in Appendix B.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function ŒΩ(N ) are deÔ¨Åned as in
Theorem 5.1. Moreover, for each N , let WÃÇŒΩ(N ) be deÔ¨Åned as in Theorem 5.2 and WÃÇ
as in Theorem 5.1. Then, for suÔ¨Éciently large ŒΩ(N ), WÃÇ has minimum phase, and
WÃÇŒΩ(N ) ‚àí WÃÇ ‚àû ‚Üí 0
almost surely as ŒΩ(N ) ‚Üí ‚àû.
6. Simulations
Performing model reduction on WÃÇŒΩ , rather than on the maximum-entropy Ô¨Ålter of
exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier
and more accurate order determination, as the heuristics of Remark 4.5 suggest.
There are also alternative order determination statistical tests based on the canonical
correlation coeÔ¨Écients [17, 26, 46]. But, even more importantly, there is less bias,
and the error variances are closer to the CrameÃÅr-Rao bound.
Since we are approximating rational models with AR models the method will be
biased for Ô¨Ånite amount of data, unless the model generating the data really is an
AR model. The consistency result given in Theorem 5.1 implies that the method is
asymptotically unbiased and therefore we consider the CrameÃÅr-Rao bound for unbiased methods; see [44, pp. 137‚Äì138]. The CrameÃÅr-Rao bound for biased estimation
requires knowledge about the bias as a function of the parameter to be estimated.
As already mentioned, the method will be unbiased and even statistically eÔ¨Écient for
Gaussian AR processes if the model reduction step is omitted. Despite the fact that
an algorithm based on covariance estimates (2.6) is not asymptotically eÔ¨Écient for
general ARMA models [44, p. 144], our method can be used to provide a starting
guess for other algorithms, for example the maximum likelihood method.
6
SBT dashed line, DBT dotted line.
5

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identiÔ¨Åcation estimates.

To illustrate our procedure, let us consider data generated by passing white noise
through a ‚Äútrue system‚Äù with transfer function
W (z) =

z 5 ‚àí 0.0550z 4 ‚àí 0.1497z 3 ‚àí 0.2159z 2 + 0.1717z ‚àí 0.0495
.
z 5 ‚àí 0.7031z 4 + 0.3029z 3 + 0.1103z 2 ‚àí 0.1461z + 0.2845

18

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy
model WÃÇŒΩ of degree ŒΩ = 24 determined from estimated covariances. Based on 100
test runs, the empirical means and standard deviations are determined. Figure 6.1
illustrates the statistical bias as a function of the length N of the data string when
using stochastic (dashed curve) and deterministic (dotted curve) model reduction
respectively.
For the same test runs, Figure 6.2 illustrates the corresponding standard deviations
together with the CrameÃÅr-Rao bound (solid curve). More precisely, the Ô¨Ågures depict
the sums of the moduli of the biases and standard deviations respectively for the
coeÔ¨Écients of the numerator and denominator polynomials of WÃÇ (z).
5
CRB solid line, SBT dashed line, DBT dotted line.
4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the CrameÃÅr-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z)
with poles and zeros closer to the unit circle is considered. The poles and zeros of
WÃÇ (z) are determined for 100 runs and a data length N = 500. As before, ŒΩ = 24.
Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with
the poles and zeros of W (z), which are denoted by ‚Äú‚ó¶‚Äù.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

1

‚àí1
‚àí1

‚àí0.5

0
poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT.
To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in
Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 24.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

1

‚àí1
‚àí1

‚àí0.5

0
poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identiÔ¨Åcation.

Figure 6.5 describes the result obtained when applying stochastic subspace identiÔ¨Åcation to the same data. More precisely, Algorithm # 2 in [43] is used. In order
to make the experiments comparable, we have chosen a Hankel matrix of dimension
13 √ó 13, which corresponds to ŒΩ = 25 in our procedure.
Note that the estimates are much less focused, and many zeros tend to cluster on the
unit circle, implying that coercivity becomes critical. This is related to the positivity
issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the
subspace identiÔ¨Åcation method performs worse than our SBT identiÔ¨Åcation method,
yielding larger biases and standard deviations, but performs better than when DBT
is used.

20

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as
illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will
disappear as ŒΩ and N are increased. In Figure 6.6 we show the same experiment for
ŒΩ = 64 and N = 2000.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and ŒΩ = 64.

1

1

0.5

0.5

0

0

‚àí0.5

‚àí0.5

‚àí1
‚àí1

‚àí0.5

0
zeros

0.5

‚àí1
‚àí1

1

‚àí0.5

0
poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and ŒΩ = 40 using Burg‚Äôs method.

In practice, there is a trade-oÔ¨Ä between the quality of the ergodic estimates, which
roughly speaking depend on |Œªmax (A)|, the ‚àû -error tolerance, which is a function of
|Œªmax (A ‚àí BD‚àí1 C)|, and the numerical accuracy of the computations. For example,
if the zeros of W (z) are far from the unit circle and ŒΩ is chosen very large, the error
may increase.
In the present example, it turns out that using Burg‚Äôs method [3] in lieu of the
ergodic estimate (2.6) yields better estimates for smaller ŒΩ and N , as illustrated in
Figure 6.7 which shows the case N = 500 and ŒΩ = 40.
A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There
we give the empirical bias and standard deviation for the coeÔ¨Écients of the numerator
and the denominator, respectively, of the estimated transfer functions together with
the CrameÃÅr-Rao bound. It is the authors experience that Burg‚Äôs method gives at
least as good results as when using the ergodic covariance estimate (2.6), unless the
intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

Parameter
True value
Bias:
CE:
Burg:
Std.dev.:
CE:
Burg:
CRB:

21

œÉw2
b1
b2
b3
b4
b5
1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491
0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895
0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734
0.2332 0.1314 0.0508 0.0611 0.0722 0.0802
0.0712 0.0411 0.0381 0.0339 0.0339 0.0356
0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and ŒΩ = 40 using covariance estimation (CE) or Burg estimation and , in
both cases, followed by SBT.

Parameter
a1
a2
a3
a4
a5
True value
-0.6281 0.3597 0.2634 -0.5322 0.7900
Bias:
CE:
0.0087 -0.0044 -0.0003 0.0066 -0.0152
Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125
Std.dev.:
CE:
0.0274 0.0304 0.0371 0.0305 0.0304
Burg: 0.0336 0.0307 0.0358 0.0324 0.0306
0.0293 0.0321 0.0342 0.0322 0.0290
CRB:
Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and ŒΩ = 40 using covariance estimation (CE) or Burg estimation and,
in both cases, followed by SBT.

7. Conclusions
We have presented a three-step procedure for identiÔ¨Åcation of time series, which is easy
to understand and implement. Just like for subspace identiÔ¨Åcation methods, robust
linear-algebra algorithms can be used and no nonconvex optimization computations
are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identiÔ¨Åcation, as our extensive simulations indicate.
In particular, its good performance has been conÔ¨Årmed by Monte Carlo simulations.
The paper only covers the scalar case, but the multivariate case is presently being
worked out.
The three steps, covariance estimation, covariance extension and model reduction
have each been studied separately before. This is an advantage which should make the
method easy to grasp. However, a comprehensive study of the entire identiÔ¨Åcation
strategy, giving appropriate bounds, has been missing and this is what we oÔ¨Äered
here.
The observation that the Schur parameters converge geometrically simpliÔ¨Åes our
application of SzegoÃà theory and allows us to give a complete account of the asymptotic
behavior of maximum entropy models of growing order. This analysis provides us with
a clear indication as to when the identiÔ¨Åcation strategy is good and when it might face
diÔ¨Éculties, based purely on the closeness of the maximum modulus zero to the unit
circle. The parsimony permeating other system identiÔ¨Åcation methods should not be
a reason for refraining from high-order modeling as an intermediate step. In fact,
such a strategy might be desirable, since we have shown that the poles of the ‚Äútrue‚Äù
system which lie outside a circle in the complex plane containing all of its zeros are

22

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the
perimeter of this circle, providing a justiÔ¨Åcation for choosing stochastically balanced
model reduction, rather than deterministically balanced truncation, in the last step.
With this reduction procedure, we have conÔ¨Årmed better statistical properties with
variances closer to the CrameÃÅr Rao bound. The procedure could also be modiÔ¨Åed by
exchanging exact covariance extension for approximate one, as outlined in [35].
Even though, in general, stochastic balancing would require the solution of a pair
of Riccati equations, this is not the case for the particular maximum entropy models
used here. In fact, the balancing procedure only requires linear algebra, and hence
an intelligent use of the Levinson algorithm may substantially reduce the number of
arithmetic operations.
Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N , ŒΩ and nÃÇ, we gave worst-case guaranteed
bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive
stochastic system, but the method returns a valid model also for generic data. In fact,
in contrast to many stochastic subspace identiÔ¨Åcation [9], all steps of the procedure
preserve the positive real property.
Appendix A. Asymptotic behavior of the maximum entropy Ô¨Ålter
Theorem 3.4 is actually a modiÔ¨Åcation to the rational setting of a theorem due to
SzegoÃà [47], and the proof is modeled after [19], which in turn includes aspects already
present in the work of Schur [45]. See also [48], [49] and [16] for more facts on
orthogonal polynomials. However, rationality and coercivity allows us to present a
simpliÔ¨Åed and self-contained proof of a version of SzegoÃà‚Äôs classical theorem, to which
we also are able to add geometric convergence. The derivation of Caines and BaykalGuÃàrsoy [7] is shorter, but we feel that our approach is more systematic and gives
additional insight into the mechanism of identiÔ¨Åcation.
To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas.
‚àíŒΩ
œÜŒΩ (z)|
Lemma A.1. Let {œÜŒΩ (z)}‚àû
0 be the normalized SzegoÃà polynomials (3.7). Then |z
c
is uniformly bounded from above and away from zero in the complement D of the open
unit disc, i.e., there are positive numbers Œ±, Œ≤ ‚àà R such that

Œ± ‚â§ |z ‚àíŒΩ œÜŒΩ (z)| ‚â§ Œ≤
for all ŒΩ and all z ‚àà Dc .
Proof. In view of the SzegoÃà-Levinson recursion (3.4),



œï‚àót (z)
œït+1 (z) = œït (z) z ‚àí Œ≥ÃÑt
,
œït (z)
and hence
z

‚àíŒΩ

œïŒΩ (z) =

ŒΩ‚àí1


k=0


œï‚àók (z)
1 ‚àí z Œ≥ÃÑk
.
œïk (z)
‚àí1

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , zŒΩ are the roots of œïŒΩ (z), it is immediately seen that
œï‚àóŒΩ (z)  1 ‚àí z zÃÑk
,
=
œïŒΩ (z) k=1 z ‚àí zk
ŒΩ

which is a Blaschke product, analytic in Dc and having modulus one on the unit circle,
and thus modulus less than or equal to one in Dc . Hence, since |z ‚àí1 | ‚â§ 1 in Dc ,
ŒΩ‚àí1


(1 ‚àí |Œ≥k |) ‚â§ |z

‚àíŒΩ

œïŒΩ (z)| ‚â§

k=0

ŒΩ‚àí1


(1 + |Œ≥k |)

(A.1)

k=0

for all z ‚àà Dc . But, these products converge to positivenumbers as ŒΩ ‚Üí ‚àû. This
follows from the absolute convergence of the inÔ¨Ånite sum ‚àû
k=0 |Œ≥k |, a fact that, in the
present context, stems from Lemma 3.1. From (3.5) we also have 0 < r‚àû ‚â§ rŒΩ ‚â§ r0 ,
and consequently the lemma follows.
Remark A.2. An equivalent statement of this lemma is that the maximum entropy
solution WŒΩ (z), deÔ¨Åned by (3.6), is uniformly bounded from above and away from
zero for all ŒΩ and z ‚àà Dc .
Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of
functions
fŒΩ (z) := z ‚àíŒΩ œÜŒΩ (z)
converges uniformly to an analytic function f‚àû in
statement of Theorem 3.5.

DcœÅ,

where

DcœÅ

is deÔ¨Åned in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the
purely algebraic relation
ŒΩ
	

œÜk (z)œÜk (w) =

k=0

œÜ‚àóŒΩ (z)œÜ‚àóŒΩ (w) ‚àí z wÃÑœÜŒΩ (z)œÜŒΩ (w)
,
1 ‚àí z wÃÑ

(A.2)

which is called the ChristoÔ¨Äel-Darboux-SzegoÃà formula. In particular, setting w = 0
and exchanging z for z ‚àí1 in (A.2), (3.5) and (3.7) yield
fŒΩ (z)
1 	
Œ≥k‚àí1
‚àí
œÜk (z ‚àí1 ) ‚àö .
‚àö =
rŒΩ
c0 k=1
rk
ŒΩ

(A.3)

Observe that œÜk (z ‚àí1 ) is analytic and bounded in DcœÅ , and hence in Dc , and therefore it
‚àû
. Moreover, by the maximum modulus principle, it attains its maximum
belongs to ‚àí
c
value in D on the unit circle where, by Lemma A.1, it is bounded by Œ≤. Hence

H

|œÜk (z ‚àí1 )| ‚â§ Œ≤

for z ‚àà Dc and for all k.

Therefore, in view of (A.3) and the fact that rk ‚â• r‚àû , we have


ŒΩ
	
 fŒΩ (z) f¬µ (z) 
 ‚àö ‚àí ‚àö  ‚â§ ‚àöŒ≤
|Œ≥k‚àí1 |,
 rŒΩ
r¬µ 
r‚àû k=¬µ+1

(A.4)

(A.5)

24

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for suÔ¨Éciently large ŒΩ and ¬µ.
‚àû
. The same holds for fŒΩ (z). In
This establishes (A.3) as a Cauchy sequence in ‚àí
c
fact, since rŒΩ ‚â§ c0 , for all z ‚àà D


‚àö  fŒΩ (z) f¬µ (z) 
|fŒΩ (z) ‚àí f¬µ (z)| ‚â§ c0  ‚àö ‚àí ‚àö 
rŒΩ
rŒΩ




 1
‚àö  fŒΩ (z) f¬µ (z) 
1 

‚â§ c0  ‚àö ‚àí ‚àö  + |f¬µ (z)|  ‚àö ‚àí ‚àö  .
(A.6)
rŒΩ
r¬µ
rŒΩ
r¬µ

H

But, by Lemma A.1, |f¬µ (z)| ‚â§ Œ≤ for all ŒΩ and z ‚àà Dc , and therefore, in view of (A.5),
we obtain



ŒΩ
 1

c0 	
1
|Œ≥k‚àí1 | + Œ≤  ‚àö ‚àí ‚àö  for all z ‚àà Dc . (A.7)
|fŒΩ (z) ‚àí f¬µ (z)| ‚â§ Œ≤
r‚àû k=¬µ+1
rŒΩ
r¬µ
Since rŒΩ ‚Üí r‚àû as ŒΩ ‚Üí ‚àû, we see that, for each 9 > 0, |fŒΩ (z) ‚àí f¬µ (z)| < 9 for
suÔ¨Éciently large ŒΩ and ¬µ. Consequently, fŒΩ tends uniformly in Dc to a function
‚àû
.
f‚àû ‚àà H‚àí
The uniform convergence and the analyticity can be extended to any compact
subset of DcœÅ . To see this, Ô¨Årst note that z ‚àà D if and only if z ‚àí1 ‚àà Dc . Therefore, by
Lemma A.1,
|œÜk (z ‚àí1 )| ‚â§ Œ≤|z|‚àík for z ‚àà D,
and consequently, since rŒΩ ‚â§ rk , (A.3) yields
ŒΩ‚àí1
	
1
|fŒΩ (z)| ‚â§ ‚àö + Œ≤|z|‚àí1
|Œ≥k ||z|‚àík .
c0
k=0

Similarly, instead of (A.5) we have


ŒΩ‚àí1
	
 fŒΩ (z) f¬µ (z) 
 ‚àö ‚àí ‚àö  ‚â§ ‚àöŒ≤ |z|‚àí1
|Œ≥k ||z|‚àík .
 rŒΩ
r¬µ 
r‚àû
k=¬µ

(A.8)

(A.9)

Now, for any compact subset K ‚àà DcœÅ , there is a Œ≥ ‚àà (œÅ, 1) and an 9 > 0 such
that |z| > Œ≥ + 9 for all z ‚àà K. Hence, by Lemma 3.1, |Œ≥k ||z|‚àík ‚â§ M Œ≥ÃÇ k where
Œ≥ÃÇ := Œ≥(Œ≥ + 9)‚àí1 < 1. Consequently, by (A.8), fŒΩ (z) is uniformly bounded in K, and
(A.9) can be made arbitrarily small for suÔ¨Éciently large ŒΩ and ¬µ. Therefore, by (A.6),
fŒΩ tends uniformly in K to the analytic function f‚àû .
Lemma A.4. Let Œ≥ be a real number such that œÅ < Œ≥ < 1. Then fŒΩ ‚àíf‚àû ‚àû = O(Œ≥ ŒΩ ).
Proof. It follows from (A.7) that


 
‚àû

‚àö 	
Œ≤
r‚àû 
c0
|Œ≥k‚àí1 | + 1 ‚àí
|fŒΩ (z) ‚àí f‚àû (z)| ‚â§ ‚àö
r‚àû
rŒΩ 
k=ŒΩ+1

for all z ‚àà Dc .
(A.10)

By Lemma 3.1, the Ô¨Årst term is O(Œ≥ ŒΩ ). It remains to show that the same holds for
the second term. To this end, Ô¨Årst note that, by (3.5),

‚àû 

r‚àû
=1‚àí
1 ‚àí Œ≥k2 .
1‚àí
rŒΩ
k=ŒΩ

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

But, by Lemma 3.1, |Œ≥k | ‚â§ M Œ≥ k for some M . Therefore, since
each x ‚àà [0, 1],

‚àû

r‚àû
‚â§ 1 ‚àí (1 ‚àí M Œ≥ k ) = O(Œ≥ t )
1‚àí
rt
k=t

‚àö

25

1 ‚àí x2 ‚â• 1 ‚àí x for

for t large enough. This concludes the proof.
Recalling the deÔ¨Ånition (3.6) of WŒΩ , we note that Lemma A.4 may be written
WŒΩ‚àí1 ‚àí f‚àû ‚àû = O(Œ≥ ŒΩ ).
‚àí1
in the same manner.
As it turns out, by coercivity, this implies that WŒΩ ‚Üí f‚àû

Lemma A.5. Let WŒΩ be the transfer function (3.6) of the maximum entropy Ô¨Ålter.
Then
‚àí1
‚àû = O(Œ≥ ŒΩ ),
WŒΩ ‚àí f‚àû
where f‚àû is the limit function of Lemma A.3.
Proof. Note that the limit function f‚àû has the same uniform bounds as fŒΩ in Lemma
A.1. In particular, |f‚àû (z)| ‚â• Œ±, |f‚àû (z)|‚àí1 ‚â§ Œ±‚àí1 , and |WŒΩ (z)| ‚â§ Œ±‚àí1 for all z ‚àà Dc .
Consequently,
‚àí1
‚àí1
WŒΩ ‚àí f‚àû
‚àû ‚â§ WŒΩ ‚àû f‚àû
‚àû WŒΩ‚àí1 ‚àí f‚àû ‚àû ‚â§ Œ±‚àí2 WŒΩ‚àí1 ‚àí f‚àû ‚àû ,

so the required result follows from Lemma A.4.
Lemma A.6. Let W be the rational minimum-phase function deÔ¨Åned above, and let
f‚àû be the limit function in Lemma A.3. Then W (z) = f‚àû (z)‚àí1 for all z ‚àà DcœÅ .
Proof. Let Œ¶ŒΩ (eiŒ∏ ) := |WŒΩ (eiŒ∏ )|2 be the spectral density of the maximum entropy
process. Then, in view of the interpolation condition,
 œÄ
 œÄ
1
1
ikŒ∏
iŒ∏
e Œ¶(e )dŒ∏ = ck =
eikŒ∏ Œ¶ŒΩ (eiŒ∏ )dŒ∏ for k = 0, 1, . . . , ŒΩ, (A.11)
2œÄ ‚àíœÄ
2œÄ ‚àíœÄ
from which we have pointwise convergence of the Fourier coeÔ¨Écients of Œ¶ŒΩ (eiŒ∏ ) to
those of Œ¶(eiŒ∏ ) as ŒΩ ‚Üí ‚àû, and hence Œ¶ŒΩ (eiŒ∏ ) ‚Üí Œ¶(eiŒ∏ ) in the 2 sense. However, by
Lemma A.5, Œ¶ŒΩ (eiŒ∏ ) ‚Üí |f‚àû (eiŒ∏ )|‚àí2 in ‚àû norm, and hence a fortiori in 2 norm, as
ŒΩ ‚Üí ‚àû. Since, in addition, not only Œ¶(eiŒ∏ ) but also f‚àû is analytic in a neighborhood
of the unit circle (Lemma A.3), we have

L

L

L

Œ¶(eiŒ∏ ) = |f‚àû (eiŒ∏ )|‚àí2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer.
In particular, WŒΩ is an outer spectral factor of Œ¶ŒΩ (eiŒ∏ ) satisfying

  œÄ it

e +z
1
it 2
log |WŒΩ (e )| dt .
WŒΩ (z) = exp
4œÄ ‚àíœÄ eit ‚àí z
But Lemma A.5, Equation (A.12) and the fact that Œ¶(eiŒ∏ ) = |W (eiŒ∏ )|2 ,

  œÄ it

1
e +z
it 2
log |W (e )| dt = W (z),
WŒΩ (z) ‚Üí exp
4œÄ ‚àíœÄ eit ‚àí z
the outer spectral factor of Œ¶. But, by Lemma A.3, WŒΩ (z) ‚Üí f‚àû (z)‚àí1 in
therefore f‚àû (z) = W ‚àí1 (z) as claimed.

DcœÅ, and

26

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6.
Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6.
Proof of Theorem 4.3. Following [53] we see that
WŒΩ‚àí1 (WŒΩ ‚àí Wred )‚àû ‚â§ 9,

(A.13)

and consequently
|WŒΩ (eiŒ∏ ) ‚àí Wred (eiŒ∏ )| ‚â§ 9|WŒΩ (eiŒ∏ )|
holds for all Œ∏, from which we have
(1 ‚àí 9)|WŒΩ (eiŒ∏ )| ‚â§ |Wred (eiŒ∏ )| ‚â§ (1 + 9)|WŒΩ (eiŒ∏ )|.
However, in view of (3.6) and (3.7), it follows from (A.1) that
‚àö
‚àö
rŒΩ
rŒΩ
iŒ∏
‚â§ |WŒΩ (e )| ‚â§ ŒΩ‚àí1
,
ŒΩ‚àí1
k=0 (1 + |Œ≥k |)
k=0 (1 ‚àí |Œ≥k |)
which together with (3.5) yields
c0
‚â§ |WŒΩ (eiŒ∏ )| ‚â§ Œ∫
Œ∫

(A.14)

for all Œ∏. This establishes (4.27). To see that Wred is minimum phase if 9 < 1, note
that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by RoucheÃÅ‚Äôs
Theorem, Wred has the same number of zeros in Dc (including ‚àû) as WŒΩ . Hence,
since WŒΩ is minimum phase, so is Wred .
To establish the bound (4.28) note that
WŒΩ ‚àí Wred ‚àû ‚â§ WŒΩ ‚àû WŒΩ‚àí1 (WŒΩ ‚àí Wred )‚àû .
From (A.14) we have WŒΩ ‚àû ‚â§ Œ∫, and hence (4.28) follows from (A.13).
Appendix B. Statistical convergence proofs
Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corresponding SzegoÃà polynomial œïÃÇŒΩ (z) and predictor error variance rÃÇŒΩ from (3.4) and (3.5),
and form the maximum-entropy Ô¨Ålter
‚àö ŒΩ
rÃÇŒΩ z
.
WÃÇŒΩ (z) =
œïÃÇŒΩ (z)
To determine WŒΩ ‚àí WÃÇŒΩ ‚àû let z ‚àà Dc and form
‚àö ŒΩ
‚àö ŒΩ
rŒΩ z
rÃÇŒΩ z
WŒΩ (z) ‚àí WÃÇŒΩ (z) =
‚àí
œïŒΩ (z)
œïÃÇŒΩ (z)
‚àö
‚àö
‚àö
( rŒΩ ‚àí rÃÇŒΩ )z ‚àíŒΩ œïŒΩ (z) ‚àí rŒΩ z ‚àíŒΩ (œïŒΩ (z) ‚àí œïÃÇŒΩ (z))
.
=
z ‚àíŒΩ œïŒΩ (z)z ‚àíŒΩ œïÃÇŒΩ (z)
Since r‚àû > 0, by (3.7) and Lemma A.1,
‚àö
‚àö
0 < ¬µ := r‚àû Œ± ‚â§ |z ‚àíŒΩ œïŒΩ (z)| ‚â§ c0 Œ≤ =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1),
|z

‚àíŒΩ

œïÃÇŒΩ (z)| ‚â• ¬µÃÇŒΩ :=

ŒΩ‚àí1


(1 ‚àí |Œ≥ÃÇk |),

k=0

where Œ≥ÃÇ0 , Œ≥ÃÇ1 , . . . , Œ≥ÃÇŒΩ‚àí1 are the Schur parameters corresponding to the estimated covariances (2.6). Therefore, by the maximum-modulus principle,

‚àö
‚àö
1
{M | rŒΩ ‚àí rÃÇŒΩ | + c0 |œïŒΩ (z) ‚àí œïÃÇŒΩ (z)|},
|WŒΩ (z) ‚àí WÃÇŒΩ (z)| ‚â§ max
|z|=1 ¬µ¬µÃÇŒΩ
where we have also used the fact that rŒΩ ‚â§ c0 . But, for |z| = 1,
|œïŒΩ (z) ‚àí œïÃÇŒΩ (z)| ‚â§ œïŒΩ ‚àí œïÃÇŒΩ 1 ,
where œïŒΩ and œïÃÇŒΩ are the ŒΩ-vectors formed as in (4.19) and ¬∑1 is the B1 norm. Recall
that œïŒΩ is the unique solution of the normal equations


TŒΩ œïŒΩ = ‚àícŒΩ where cŒΩ := cŒΩ cŒΩ‚àí1 . . . c1 ,
(B.1)
where TŒΩ is the Toeplitz matrix deÔ¨Åned by (2.4), and that
rŒΩ = c0 + cŒΩ œïŒΩ .

(B.2)

Also, the analogous relations hold for œïÃÇŒΩ and rÃÇŒΩ . Then,
rŒΩ ‚àí rÃÇŒΩ = (c0 ‚àí cÃÇ0 ) + (cŒΩ ‚àí cÃÇŒΩ ) œïŒΩ + cÃÇŒΩ (œïŒΩ ‚àí œïÃÇŒΩ )
and hence
|rŒΩ ‚àí rÃÇŒΩ | ‚â§ |c0 ‚àí cÃÇ0 | + cŒΩ ‚àí cÃÇŒΩ 1 œïŒΩ ‚àû + cÃÇŒΩ ‚àû œïŒΩ ‚àí œïÃÇŒΩ 1 .
Finally,


‚àö
|rŒΩ ‚àí rÃÇŒΩ |
|r ‚àí rÃÇ |
‚àö ‚â§ ŒΩ‚àö ŒΩ ,
| rŒΩ ‚àí rÃÇŒΩ | ‚â§ ‚àö
r‚àû
rŒΩ + rÃÇŒΩ

and consequently, since x1 ‚â§ ŒΩx‚àû for any x ‚àà RŒΩ ,

M
‚àö {|c0 ‚àí cÃÇ0 | + œïŒΩ ‚àû ŒΩcŒΩ ‚àí cÃÇŒΩ ‚àû }
¬µ¬µÃÇŒΩ r‚àû



M cÃÇŒΩ ‚àû
1 ‚àö
c0 + ‚àö
+
ŒΩœïŒΩ ‚àí œïÃÇŒΩ ‚àû .
¬µ¬µÃÇŒΩ
r‚àû

WŒΩ ‚àí WÃÇŒΩ ‚àû ‚â§

(B.3)

Recall now that œïŒΩ and œïÃÇŒΩ are each solutions of a normal equation (B.1). More
precisely, TŒΩ œïŒΩ = ‚àícŒΩ and TÃÇŒΩ œïÃÇŒΩ = ‚àícÃÇŒΩ . Since ck = CAk‚àí1 CÃÑ  for k > 0, where all
eigenvalues of A are less than one in modulus, ck ‚Üí 0 exponentially, we have
cŒΩ ‚àû ‚â§ K1

and TŒΩ ‚àû ‚â§ c0 + 2

ŒΩ‚àí1
	

|ck | ‚â§ K2

k=1

for some constants K1 and K2 . Moreover, from [8] we have
TŒΩ‚àí1 ‚àû ‚â§

ŒΩ‚àí1
1  1 + |Œ≥k |
‚â§ K3
c0 k=0 1 ‚àí |Œ≥k |

28

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence
œïŒΩ ‚àû ‚â§ TŒΩ‚àí1 ‚àû cŒΩ ‚àû ‚â§ K1 K3
and the condition number
Œ∫(TŒΩ ) := TŒΩ ‚àû TŒΩ‚àí1 ‚àû ‚â§ K := K2 K3
is bounded for all ŒΩ.
Now, it is known [25] that for each data length N in (2.2), there is a ŒΩ(N ) of order
O(log N ) such that


log log N
,
(B.4)
max |ck ‚àí cÃÇk | = O
0‚â§k‚â§ŒΩ(N )
N
and therefore, for any a ‚àà R,
ŒΩ a |c0 ‚àí cÃÇ0 | ‚Üí 0 and ŒΩ a cŒΩ ‚àí cÃÇŒΩ ‚àû ‚Üí 0 as ŒΩ = ŒΩ(N ) ‚Üí ‚àû.

(B.5)

Consequently the Ô¨Årst term in the bound (B.3) tends to zero as N ‚Üí ‚àû and
ŒΩ(N ) ‚Üí ‚àû provided it is done at the speciÔ¨Åed relative rates and provided ¬µÃÇŒΩ is
bounded away from zero. However, the estimate (2.6) has the property that the
corresponding Toeplitz matrix TÃÇŒΩ is positive deÔ¨Ånite for each Ô¨Ånite ŒΩ, and this in turn
is equivalent to |Œ≥ÃÇk | < 1 for k = 0, 1, . . . , ŒΩ ‚àí 1 so that ¬µÃÇŒΩ > 0. Since, in addition
¬µÃÇŒΩ ‚Üí ¬µ > 0 as ŒΩ(N ) ‚Üí ‚àû by (B.4) and continuity, the second requirement is also
fulÔ¨Ålled. To simplify notations, we have suppressed the index N in the quantities
marked with a hat, which of course depend on the data (2.2) and hence also on N .
Next we show that also the second term in (B.3) tends to zero. Since cÃÇŒΩ ‚àû ‚â§
cŒΩ ‚àû + cŒΩ ‚àí cÃÇŒΩ ‚àû is bounded, it thus remains to demonstrate that
ŒΩœïŒΩ(N ) ‚àí œïÃÇŒΩ(N ) ‚àû ‚Üí 0 as ŒΩ(N ) ‚Üí ‚àû.
This follows from the more general fact, needed for the proof of Corollary B.1, that
ŒΩ a œïŒΩ(N ) ‚àí œïÃÇŒΩ(N ) ‚àû ‚Üí 0 as ŒΩ(N ) ‚Üí ‚àû

(B.6)

for any a ‚àà R. To prove this, Ô¨Årst note that
TŒΩ ‚àí TÃÇŒΩ ‚àû ‚â§ |c0 ‚àí cÃÇ0 | + 2ŒΩcŒΩ ‚àí cÃÇŒΩ ‚àû ,
and hence TŒΩ ‚àí TÃÇŒΩ ‚àû ‚Üí 0. Therefore œÅŒΩ := TŒΩ ‚àí TÃÇŒΩ ‚àû TŒΩ‚àí1 ‚àû < 1 for ŒΩ := ŒΩ(N )
suÔ¨Éciently large, and, provided cŒΩ = 0, the standard perturbation estimate [22] yields


1
TŒΩ ‚àí TÃÇŒΩ ‚àû cŒΩ ‚àí cÃÇŒΩ ‚àû
œïŒΩ ‚àí œïÃÇŒΩ ‚àû
‚â§
Œ∫(TŒΩ )
+
,
(B.7)
œïŒΩ ‚àû
1 ‚àí œÅŒΩ
TŒΩ ‚àû
cŒΩ ‚àû
and consequently, since TŒΩ ‚àû ‚â• c0 > 0, it follows from (B.5) that (B.6) tends to
zero in the required manner.
If cŒΩ = 0, œïŒΩ = 0, and hence
œïŒΩ ‚àí œïÃÇŒΩ ‚àû = œïÃÇŒΩ ‚àû ‚â§ TÃÇŒΩ‚àí1 ‚àû cÃÇŒΩ ‚àû = TÃÇŒΩ‚àí1 ‚àû cŒΩ ‚àí cÃÇŒΩ ‚àû ,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case cŒΩ = 0. In fact, since ¬µÃÇ is
bounded away from zero, by continuity, for each 9 > 0, there is a N0 such that
TÃÇŒΩ‚àí1 ‚àû

ŒΩ‚àí1
ŒΩ‚àí1
1  1 + |Œ≥k |
1  1 + |Œ≥ÀÜk |
‚â§
+ 9 ‚â§ K3 + 9
‚â§
cÃÇ0 k=0 1 ‚àí |Œ≥ÀÜk |
c0 k=0 1 ‚àí |Œ≥k |

for ŒΩ ‚â• N0 .

Corollary B.1. If ŒΩ(N ) is deÔ¨Åned as in Theorem 5.1, then, for any a ‚àà R,
ŒΩ a WŒΩ ‚àí WÃÇŒΩ ‚àû ‚Üí 0

almost surely as ŒΩ := ŒΩ(N ) ‚Üí ‚àû.

To prove Theorem 5.3, we Ô¨Årst note that the Hankel operator H, deÔ¨Åned by (4.1),
has a nice representation in the space 2 of square-integrable functions. In fact, let
2
2
of functions with vanishing negative Fourier coeÔ¨Écients,
+ be the subspace in
hence being analytic in the unit disc D. In this setting, H has the representation
2
2
‚Üí 2 +
given by
HŒò : +

L

L

H

H

L H

HŒò f = P ‚ä• Œòf,

(B.8)

where P ‚ä• is the orthogonal projection onto the orthogonal complement
2
2
, and where Œò is the ‚àû -function
+ in

H

L

L

Œò(z) = W‚àí (z)WÃÑ+ (z)‚àí1 .

L H
2

2
+

of

(B.9)

Here W‚àí (z) and WÃÑ+ (z) are the analytic and coanalytic minimum-phase spectral factors deÔ¨Åned in Section 4. (See, e.g., [30, 31].) In the present scalar case, WÃÑ+ (z) =
W‚àí (z ‚àí1 ). In fact, the phase function Œò is the transfer function of an all-pass Ô¨Ålter
transforming the white noise w‚àí in (4.2) to the white noise w+ in (4.3) [30, p. 834].
ÀÜ + be the stochastic measures such that
Let dwÃÇ‚àí and dwÃÑ
 œÄ
 œÄ
iŒ∏t
ÀÜ+
e dwÃÇ‚àí and wÃÑ+ (t) =
eiŒ∏t dwÃÑ
w‚àí (t) =
‚àíœÄ

Then

‚àíœÄ


H

+

œÄ

=

H‚àí =
and consequently
H := E

f ‚Üî f dwÃÇ‚àí .

H‚àí

‚àíœÄ
œÄ

H

L H
2

‚àíœÄ



2 ÀÜ
+ dwÃÑ +

œÄ

=
‚àíœÄ

H

2
iŒ∏t
+ Œò(e )dwÃÇ‚àí

2
+ dwÃÇ‚àí

|H + corresponds to HŒò under the isomorphism deÔ¨Åned by





Proof of Theorem 5.3. It follows from Theorem 5.2 that |WÃÇŒΩ (eiŒ∏ )| ‚àí |WŒΩ (eiŒ∏ )| ‚Üí 0
uniformly in Œ∏ as ŒΩ ‚Üí ‚àû, and hence, by Lemma A.1, there are positive real numbers
¬µ1 and ¬µ2 such that
¬µ1 ‚â§ |WÃÇŒΩ (eiŒ∏ )| ‚â§ ¬µ2
for all Œ∏ and suÔ¨Éciently large ŒΩ. Therefore, since
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ WÃÇŒΩ ‚àû WÃÇŒΩ‚àí1 (WÃÇŒΩ ‚àí WÃÇ )‚àû ,

(B.10)

30

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

(A.13) and (4.26) imply that
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ 2¬µ2

ŒΩ
	

œÉÃÇk
,
1 ‚àí œÉÃÇk
k=nÃÇ+1

(B.11)

for suÔ¨Éciently large ŒΩ, where œÉÃÇ1 , œÉÃÇ2 , . . . , œÉÃÇŒΩ are the singular values (5.1) determined
from the covariance estimates (2.6).
It is well-known (see, e.g., [56, p. 204]) that the singular value œÉk of the Hankel
operator HŒò , deÔ¨Åned by (B.8) equals the inÔ¨Åmum of HŒò ‚àí K over all operators
2
2
‚Üí 2 +
of Ô¨Ånite rank at most k. Recall that Œò(z) = WŒΩ (z)/WŒΩ (z ‚àí1 ).
K : +
The singular value œÉÃÇk of HŒòÃÇ , where ŒòÃÇ(z) = WÃÇŒΩ (z)/WÃÇŒΩ (z ‚àí1 ), is described analogously.
Therefore, since

H

L H

HŒòÃÇ ‚àí K ‚â§ HŒòÃÇ ‚àí HŒò  + HŒò ‚àí K ‚â§ ŒòÃÇ ‚àí Œò‚àû + HŒò ‚àí K,
we have œÉÃÇk ‚â§ ŒòÃÇ ‚àí Œò‚àû + œÉk . But, for k > n, œÉk = 0, and hence œÉÃÇk ‚â§ ŒòÃÇ ‚àí Œò‚àû .
Consequently, (B.11) yields
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ M1 ŒΩŒòÃÇ ‚àí Œò‚àû ,

(B.12)

where M1 := 2¬µ2 (1 ‚àí œÉÃÇnÃÇ+1 )‚àí1 . However,


‚àí1 ‚àí1
‚àí1
‚àí1
WÃÇŒΩ (z) ‚àí W (z) ‚àí Œò(z)[WÃÇŒΩ (z ) ‚àí W (z )] ,
ŒòÃÇ(z) ‚àí Œò(z) = WÃÇŒΩ (z )
so, since WÃÇŒΩ (z ‚àí1 )‚àû is uniformly bounded by (B.10), and Œò‚àû is constant,
ŒòÃÇ ‚àí Œò‚àû ‚â§ M2 W ‚àí WÃÇŒΩ ‚àû ,
which together with (B.12) yields
WÃÇŒΩ ‚àí WÃÇ ‚àû ‚â§ M1 M2 ŒΩW ‚àí WŒΩ ‚àû + M1 M2 ŒΩWŒΩ ‚àí WÃÇŒΩ ‚àû
for suÔ¨Éciently large ŒΩ. Consequently the theorem follows from Theorem 3.4 and
Corollary B.1.
Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A.
Gombani, W. B. Gragg, G. Picci and T. SoÃàderstroÃàm for stimulating discussions and
for providing us with appropriate references. We are also indebted to the anonymous
referees for several useful suggestions.
References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987.
2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489‚Äì502, 1974.
3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975.
4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational
covariance extension problem. SIAM Journal on Control and Optimization, 37:211‚Äì229, 1999.
5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of
all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841‚Äì1857, 1995.
6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast Ô¨Åltering algorithms.
SIAM Journal on Control and Optimization, 32:744‚Äì789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-GuÃàrsoy. On the L‚àû consistency of L2 estimators. Systems & Control
Letters, 12:71‚Äì76, 1989.
8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978.
9. A. DahleÃÅn, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace
identiÔ¨Åcation methods may fail. Systems and Control Letters, 34:303‚Äì312, 1998.
10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced
stochatic realizations. In Proc. 21st IEEE CDC, pages 1105‚Äì1112, 1983.
11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans.
Automatic Control, AC-29:1097‚Äì1100, 1984.
12. J. Durbin. EÔ¨Écient estimation of parameters in moving average models. Biometrika, 46:306‚Äì316,
1959.
13. J. Durbin. The Ô¨Åtting of time-series models. Rev. Inst. Int. Stat., pages 223‚Äì243, 1960.
14. P. Duren. Theory of Hp spaces. Academic Press, 1970.
15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear.
16. G. Freud. Orthogonale Polynome. BirkhaÃàuser Verlag, 1969.
17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic
Control, AC-32:358‚Äì361, 1987.
18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac.,
Speech and Signal Processing, ASSP-35:438‚Äì449, 1987.
19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961.
20. M. Gevers. Towards a joint design of identiÔ¨Åcation and control. In J. Willems and H. Trentelman,
editors, Essays on Control: Perspectives in the Theory and its Applications, 1993.
21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their
l‚àû -error bounds. Int. J. Contr., 39:1115‚Äì1193, 1984.
22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989.
23. W. B. Gragg. Positive deÔ¨Ånite Toeplitz matrices, the Arnoldi process for isometric operators,
and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in
Linear Algebra, pages 16‚Äì32. Moscow U. P., 1982.
24. U. Grenander and G. SzegoÃà. Toeplitz forms and their applications. Univ. California Press, 1958.
25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons,
1988.
26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples.
Biometrika, pages 297‚Äì307, 1989.
27. W. Jones and E. SaÔ¨Ä. SzegoÃà polynomials and frequency analysis. In Approximation Theory,
pages 341‚Äì352. Dekker Inc., 1992.
28. S. Y. Kung. A new identiÔ¨Åcation method and model reduction algorithm via singular value
decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705‚Äì714, 1978.
29. W. E. Larimore. System identiÔ¨Åcation, reduced ordered Ô¨Åltering and modeling via canonical
variate analysis. In Proc. of the American Control Conference, 1983.
30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes.
SIAM J. Control and Optimization, 23:809‚Äì857, 1985.
31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic
systems. J. of Math. Systems, Estimation and Control, 1:241‚Äì333, 1991.
32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension,
and identiÔ¨Åcation of stationary time series. Automatica, 32(5):709‚Äì733, 1996.
33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating
transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412‚Äì440, 1992.
34. L. Ljung and Z. Yuan. Asymptotic properties of black box identiÔ¨Åcation of transfer functions.
IEEE Trans. Automatic Control, AC-26:514‚Äì530, 1985.
35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis,
Royal Instiute of Technology, 1998.
36. D. Q. Mayne and F. Firoozan. Linear identiÔ¨Åcation of ARMA processes. Automatica, 18:461‚Äì466,
1982.

32

J. MARI, A. DAHLEÃÅN, AND A. LINDQUIST

37. H. Mhaskar and E. SaÔ¨Ä. The distribution of zeros of asymptotically extremal polynomials. J.
Approx. Theory, 3:279‚Äì300, 1991.
38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66‚Äì73, 1978.
39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward,
editors, Approximation Theory VI, 1989.
40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991.
41. P. Van Overschee. Subspace IdentiÔ¨Åcation, Theory - Implementation - Application. PhD thesis,
Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De
Moor.
42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identiÔ¨Åcation problem.
In Proc. 30th Conference on Decision and Control, Brighton, 1991.
43. P. Van Overschee and B. De Moor. Subspace IdentiÔ¨Åcation for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996.
44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994.
45. I. Schur. UÃàber Potenzreihen, die im Innern des Einheitskreises beschraÃànkt sind. J. fuÃàr die Reine
und Angewandte Mathematik, 147:205‚Äì232, 1917.
46. J. Sorelius, T. SoÃàderstroÃàm, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identiÔ¨Åcation. In Proc. SYSID ‚Äô97, 1997.
47. G. SzegoÃà. BeitraÃàge zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift,
6:167‚Äì202, 1920.
48. G. SzegoÃà. UÃàber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232‚Äì244, 1921.
49. G. SzegoÃà. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications,
1939 (4th edition 1975).
50. B. Wahlberg. On the IdentiÔ¨Åcation and Approximation of Linear Systems. PhD thesis, LinkoÃàping
University, 1987. LinkoÃàping Studies in Science and technology. Dissertations No. 163.
51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive
approximations. Journal of Time Series Analysis, 10:283‚Äì299, 1989.
52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like
identiÔ¨Åcation techniques. IEEE Trans. Automatic Control, 37:900‚Äì912, 1992.
53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation.
Systems and Control Letters, 14:307‚Äì317, 1990.
54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423‚Äì
434, 1953.
55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938.
56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu
Abstract
Software engineering education is a technologically challenging, rapidly evolving
discipline. Like all STEM educators, software engineering educators are bombarded with
a constant stream of new tools and techniques (MOOCs! Active learning! Inverted
classrooms!) while under national pressure to produce outstanding STEM graduates.
Software engineering educators are also pressured on the discipline side; a constant
evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the
technology, guidance on the adoption of project-centric curricula is needed. This paper
focuses on vertical integration of project experiences in undergraduate software
engineering degree programs or course sequences. The Software Enterprise, now in its
9 th year, has grown from an upper-division course sequence to a vertical integration
program feature. The Software Enterprise is presented as an implementation of a project
spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those
in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software
engineering and computer science education focus on content taxonomies and bodies of
knowledge. This is not a bad thing, but taken in isolation may lead educators to believe
content coverage is more important than applied learning experiences. There is literature
on project-based learning within computing as a means to learn soft skills and complex
technical competencies. However, project experiences tend to be disjoint [5]; there may
be a freshman project or a capstone project or a semester project assigned by an
individual instructor. Yearlong capstone projects are offered at most institutions as a
synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do
it all the time?
Project experiences, while pervasive in computing programs, are not a central
integrating feature. Sheppard et al. [6] suggests that engineering curricular design should
move away from a linear, deductive model and move instead toward a networked model:
‚ÄúThe ideal learning trajectory is a spiral, with all components revisited at increasing
levels of sophistication and interconnection‚Äù ([6] p. 191). The general engineering degree
program at Arizona State University (ASU) was designed from its inception in 2005 [7]
to be a flexible, project-centric curriculum that embodied such integration (even before
[6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division
course sequence to integrate contextualized project experiences with software engineering
fundamental concepts. The computing and engineering programs at ASU‚Äôs Polytechnic
campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

Board of Regents (ABOR) approved a new Bachelor‚Äôs degree in software engineering
(BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo
accreditation review shortly thereafter.
At the course level the Software Enterprise defines a delivery structure integrating
established learning techniques around a project-based contextualized learning
experience. At the degree program level, the Enterprise weaves project experiences
throughout the BS SE degree program, integrating program outcomes at each year of the
major. There are several publications on the manner in which the Software Enterprise is
conducted within a project course (for example, [8][9]]), and we summarize this in-course
integration pedagogy in section 2. The intent of this work-in-progress paper is to describe
extending the Enterprise as a spiral curricular design feature we refer to as the project
spine, and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a
student‚Äôs competencies from understanding to comprehension to applied knowledge by
co-locating preparation, discussion, practice, reflection, and contextualized learning
activities in time. In this model, learners prepare for a module by doing readings,
tutorials, or research before a class meeting time. The class discusses the module‚Äôs
concepts, in a lecture or seminar-style setting. The students then practice with a tool or
technique that reinforces the concepts in the next class meeting. At this point students
reflect to internalize the concepts and elicit student expectations, or hypotheses, for the
utility of the concept. Then, students apply the concept in the context of a team-oriented,
scalable project, and finally reflect again to (in)validate their earlier hypotheses. These
activities take place in a single three-week sprint, resulting is a highly iterative
methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right)
The Software Enterprise represents an innovation derived from existing scholarship in
that it assembles best practices such as preparation, reflection, practice (labs), and
project-centered learning in a rapid integration model that accelerates applied learning.
Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle
[10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on
maturing the delivery process, creating new or packaging existing learning materials to fit
the delivery model, and to explore ways to assess project-centered learning.

3. The Software Enterprise Project Spine
An innovation in the new BS in Software
Engineering at ASU has been the vertical adoption of
the Software Enterprise. Enterprise courses are now
required from the sophomore to senior years. This
innovation represents what [6] calls a professional
spine, as the Enterprise serves as an integrator of
learning outcomes for a given year in the major. We
refer to our project-centered realization as a project
spine, where foundational concepts are tied to project
work throughout the undergraduate program. There is
significant
computing
literature
on
projects
(embedded, mobile, gaming, etc.) to achieve learning
or retention outcomes. However, computing lacks a
framework for integrating concepts in a project spine.
The Enterprise is an implementation that moves
students from basic comprehension to applied
Figure 2. ASU Project Spine
knowledge to critical analysis outcomes. In the BS SE
at ASU, program outcomes are described at 4 levels: describe, apply, select, and
internalize. Students must achieve level 3 (select between alternatives) in at least 1
outcome and achieve level 2 (apply) in all others. The program outcomes for the BS SE
include Design, Computing Practice, Critical Thinking, Professionalism, Perspective,
Problem Solving, Communication, and Technical Competence. An example leveled
outcome description for Perspective is given in Table 1. The Enterprise accelerates level
3 outcomes by providing contextualized integrated experiences fostering decision-making
in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes.
Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in
business, global, economic, environmental, and societal contexts.
Level 1. Understands technological change and development have both positive & negative effects.
Level 2. Identifies and evaluates the assumptions made by others in their description of the role and
impact of engineering and computing on the world.
Level 3. Selects from different scenarios for the future and appropriately adapts them to match current
technical, social, economic and political concerns.
Level 4. Has formed a constructive model for the future of our society, and makes life and
career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical
competencies by assigning projects inclusive of the technical material covered in the
regular computing courses. So for example, junior projects (Software Enterprise III and
IV) emphasize technical complexities in Networks, Distributed Computing, and
Databases, while senior projects emphasize technical complexities in Web and Mobile
computing. The technical ‚Äúfocus area‚Äù courses are chosen more based on faculty expertise
and recruitment goals than software engineering outcomes; one can envision many
different areas represented by upper division courses here. These do help address the
concern that an accredited software engineering degree has an application area. A risk we
have not yet addressed is if the technical area impacts the software engineering process,
such as with a soon-to-be-introduced embedded systems focus area.

There are 2 additional aspects of integration to the project spine. As summarized in
section 2, the Enterprise integrates software engineering concepts throughout the project
experiences. Students in the sophomore year learn the Personal Software Process [11] as a
means to build individual understanding of time management, defect management, and
estimation skills. They then focus on Quality, including but not limited to testing. In the
junior year Enterprise students focus on Design (human-centered and system design
principles) followed by best practices in software construction, taken primarily from
eXtreme Programming. In the senior year students focus on Requirements Engineering
then Process and Project Management. The final aspect of integration is with soft-skill
outcomes such as Communication, Teamwork, and Professionalism (see Table 1).
Throughout the spine the project experiences are crafted to ensure variations on pedagogy
to address these outcomes. For example, in the freshman year students receive explicit
instruction in teamwork. In the senior year the emphasis is on formal documentation as a
means of communication. In the junior year, students work on service learning projects of
high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of
program adoption. There are examples of program design and lessons learned [5][12][13],
or reflections and recommendations on the software engineering education landscape
[14][15][16][17][18]. These are worthwhile guides but do not offer examples on
evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on
‚ÄúProgram Implementation and Assessment‚Äù which discusses a number of key factors in
program adoption, but is geared toward accreditation and not evaluation instruments. A
survey instrument is presented in [19] but is designed for comparison of a large number
of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate
programs in software engineering but more as an aggregate counting exercise in
knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software
Engineering project conducted a survey of graduate degree programs [20] and then
produced a comparison report [21] of graduate programs to the GSwE2009 reference
model, which includes data on program characteristics and in-depth profiles from 3
institutions. A recent study is Conry‚Äôs [23] survey of accredited software engineering
degree programs. Conry summarizes institutional, administrative, and curricular
(knowledge area) aspects in describing the 19 accredited programs as of October 2009.
Certainly program adoption measures from other engineering programs are also relevant,
though software engineering programs are unique due to the forces discussed in section 1.
Our next steps for the Enterprise-as-project-spine involve defining measures for
adoption impact, and determining how this concept fits with established patterns for
curricular maps in software engineering programs. We plan to use quantitative and
qualitative instruments to evaluate adoption. Quantitative data, such as program size,
institution type, faculty and student backgrounds, can be collected via available resources
(departmental archives or online) and direct surveys. Qualitative data can be collected
through survey instruments and interviews of all stakeholders (faculty participants,
administrators, and advisors). Different instruments may be used at different times to
evaluate ‚Äúin-stream‚Äù attitudes versus post-adoption reflections. Defining and validating
these instruments is a significant area of work going forward.
The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in
software engineering. Taxonomies are useful and the sign of an emerging discipline. We

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas,
and plan to elaborate on these mappings. Specifically, we intend to produce CS2013
course exemplars. Further, the SE2004 report includes a section on program curricular
patterns, and we will propose new patterns based on the project spine concept, which we
hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the
New Century. The National Academies Press, Washington D.C., 2005.
[2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of
Knowledge (SWEBOK). Los Alamitos, CA, 2004.
[3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society
Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at
http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013.
[4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society.
Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software
Engineering. Joint Task Force on Computing Curricula, 2004.
[5] Shepard, T. ‚ÄúAn Efficient Set of Software Degree Programs for One Domain.‚Äù In Proceedings of the
International Conference on Software Engineering (ICSE) 2001.
[6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the
Future of the Field, Jossey-Bass, San Francisco, 2008.
[7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. ‚ÄúA Flexible
Curriculum for a Multi-disciplinary Undergraduate Engineering Degree.‚Äù Proceedings of the Frontiers in
Education Conference 2005.
[8] Gary, K. ‚ÄúThe Software Enterprise: Practicing Best Practices in Software Engineering Education‚Äù, The
International Journal of Engineering Education Special Issue on Trends in Software Engineering Education,
Volume 24, Number 4, July 2008, pp. 705-716.
[9] Gary, K., ‚ÄúThe Software Enterprise: Preparing Industry-ready Software Engineers‚Äù Software Engineering:
Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group
Publishing. October 2008.
[10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984.
[11] Humphrey, W.S. Introduction to the Personal Software Process, Addison-Wesley, Boston, 1997.
[12] Lutz, M. and Naveda, J.F. ‚ÄúThe Road Less Traveled: A Baccalaureate Degree in Software Engineering.‚Äù
Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997.
[13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor‚Äôs Program.
IEEE Software November/December 2006.
[14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. ‚ÄúGuidance for the
development of software engineering education programs.‚Äù The Journal of Systems and Software,
49(1999):163-169. 1999.
[15] Ghezzi, C. and Mandrioli. ‚ÄúThe Challenges of Software Engineering Education.‚Äù In Proceedings of the
International Conference on Software Engineering (ICSE) 2006.
[16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. ‚ÄúImproving software practice through
education: Challenges and future trends.‚Äù Proceedings of the Future of Software Engineering Conference, 2007.
[17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the
Future of Software Engineering, Limerick Ireland, 2000.
[18] Mead, N. (2009). Software Engineering Education: How far We‚Äôve Come and How far We Have to Go.
Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009.
[19] Modesitt, K., Bagert, D.J., and Werth, L. ‚ÄúAcademic Software Engineering: What is it and What Could it be?
Results of the First International Survey for SE Programs.‚Äù Proceedings of the International Conference on
Software Engineering (ICSE) 2001.
[20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering
Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008.
[21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master‚Äôs Programs in
Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013.
[22] Bagert, D.J. & Chenoweth, S.V. ‚ÄúFuture Growth of Software Engineering Baccalaureate Programs in the United
States‚Äù, Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005.
[23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of
the American Society for Engineering Education, Louisville, KY, 2010.
[24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). ‚ÄúRevision of the SE2004
Curriculum Model.‚Äù Panel at the ACM Conference of the Special Interest Group on Computer Science
Education (SIGCSE), Denver, CO, 2013.

Automated Process Support for Organizational and Personal
Processes
Kevin Gary, Tim Lindquist, Harry Koehnemann. Ly Sauer
Arizona State University
Computer Science Department
Mail Stop 5406
Tempe, AZ 85287-5406
yfppg@asu.edu
ABSTRACT
We propose two views on process: an organizational view
and a personal process view. Information technology applies
Automated Workflow technology to define, execute, and
track an organization‚Äôs automated business processes. Calendaring tools provide a form of personal process view
through scheduled work items. However, the personal, or
individual, view of the process space has largely been
ignored. We maintain that as organizations become increasingly decentralized, a single organization‚Äôs process space is
becoming difficult to recognize. Individuals of the organization are asked to do work that spans organizational, functional, and even geographic boundaries. An integrated view
of organizational workflows and personal processes is
needed to address these new demands. In this paper we
argue for the need to integrate organizational and personal
processes. We then propose a component-based process
modeling approach and supporting process architecture that
integrates these process spaces. Finally, we describe our
recent efforts at developing Java prototype process tools that
realize the proposed modeling technique and supporting
architecture.
Keywords: Workflow, Personal Process, Components

1.0 Introduction
Recent changes in industry and technology are imposing
more demanding technical requirements on information
technology. In industry, organizations are downsizing and
becoming increasingly decentralized, often causing projects
to be managed across multiple organizations or functional

units. Current technology is growing at a rate that can be
difficult to track. Industry investments in desktop tools and
groupware must be leveraged against growth in local and
wide-area networks (LANs and WANs). In particular, the
growth of the Internet makes it possible to envision computer support of global, decentralized, business processes.
These changes in industry and technology escalate the pressures put on information technology research. Providing
computer support for widely distributed organizations using
new technologies such as the Internet, Groupware, Calendar
Management, and Automated Workflow is at least an IT
systems analyst‚Äôs headache. Determining the best way to
integrate these tools to ensure maximum productivity is at
best an IT manager‚Äôs nightmare. In our view, the ability to
define, execute, and track business processes is central to
the ability to integrate these technologies in a widely distributed setting and make their use productive. Therefore in our
research we focus on automated process support. In the
business domain, automating business processes is known
as Automated Workflow.
Workflow is the study of modeling and enacting business
processes by human and computer agents. Automated
Workflow adds an emphasis on applying current computer
and information technology in a workflow environment,
with the desire of automating parts of workflows, or supporting entire workflows.
Automated Workflow has traditionally focused on defining
and automating business processes from the organization‚Äôs
standpoint. Little regard is given to managing overlapping
workflows in an individual workspace, or for even considering the personal processes of an individual when considering the productivity of the organization. The current
solution is to drop a set of personal productivity tools, such
as calendaring tools, in the lap of the individual and let her/
him work it out.

In order to achieve greater productivity from both workflow
and personal productivity tools, a more integrated view of
organizational and personal processes must be considered.
An integrated view allows individuals the ability to develop
their own productive work practices in support of an organization‚Äôs processes, and allows for a more natural handling
of processes spanning multiple organizations and individuals. We are in the beginning stages of our research into the
utility of providing such an integrated view. In this paper we
propose an open architecture for integrating organizational
workflows and personal productivity processes. We motivate the need for an integrated approach, and present a component-based approach to process modeling that provides
the interoperability required to achieve the integration. We
also present a suite of tools being developed at Arizona
State University that realize this architecture.
The rest of this paper is organized as follows. Section 2.0
discusses relevant issues in current workflow and calendaring technology. Section 3.0 argues for an integrated view of
organizational and personal process spaces, presents a component-based approach to process modeling, and proposes a
general process support architecture. Section 4.0 presents
tool prototypes realizing this architecture that were recently
developed at Arizona State University. We conclude in
Section 5.0 with a summary and discuss future avenues for
our research.

2.0 Background
Approaches to developing workflow systems have both
commercial and academic origins. Commercial systems
have evolved from work on forms-based image processing
systems and groupware[14]. The line between workflow
and other types of systems is often blurred, with groupware,
scheduling, database, and email tools providing some workflow functionality. In addition, several commercial products
that advertise workflow capabilities fall far short of providing full-fledged support for defining and enacting business
processes. Academic research has focused mainly on process modeling and database transaction issues[9]. Process
modeling research has led to the development of workflow
representations based on a variety of formalisms. Database
transaction research focuses on extending traditional transaction semantics to support long duration[2][9] and/or cooperative transaction[5][10] models. The result is a
proliferation of approaches and issues relating to workflow.
Current efforts are attempting to get researchers and vendors to converge on a common foundation for workflow.
The Workflow Management Coalition (WfMC) was formed
in August 1993 to promote workflow technology. The
WfMC has proposed a reference model[22] and a set of
interfaces,
called
WAPIs1
based
on
that
model[24][25][26][27] as an attempt at standardizing archi-

tectural elements of workflow systems and the interactions
between those elements. The Process Interchange Format
(PIF) Working Group was formed to explore the potential to
provide automatic translations between process representation formalisms[15]. Finally, Microsoft is pushing their
Messaging API (MAPI) as a defacto standard for implementing workflow systems. Microsoft has recently teamed
with Wang to develop the MAPI-WF specification[17], an
extension of MAPI for supporting workflow-specific services.
The WfMC is presently the most significant of the efforts
attempting to standardize workflow systems. The WfMC
Reference Model (Figure 1) identifies the basic architectural components of a workflow environment. At the center
of the model is a Workflow Enactment Service (WES),
comprised of one or more Workflow Engines. A WES provides services through the WAPIs to workflow-related tools.
These include Process Definition Tools for defining processes, Workflow Client Applications for handling user
requests for work, Third-party Applications that need to
communicate data and operations to the WES, other WESs
for providing interoperability between enactment services,
and Administration and Monitoring Tools for data gathering
for process improvement activities.
The WfMC Reference Model identifies common workflow
system components and interfaces. The WAPI interface
specifications define a set of low-level protocols for synchronously and asynchronously exchanging workflow data
between the tools and the WES. Our basic problem with this
approach is that these protocols are too low-level; they
imply a restrictive workflow model. Workflow representations that cannot easily convert their process data to conform with this underlying model cannot obtain conformance
with the model. This is one of the issues our research
addresses.
Recent standardization efforts also address the area of calendaring protocols. One popular calendaring protocol
(adopted by Netscape‚Äôs Calendar Server[18]) is the vCalendar protocol[12]. In the vCalendar protocol, calendaring and
scheduling entities, called events, are transported between
applications that can understand the protocol. This approach
is similar to the effort of the WfMC protocols in that it
defines a low-level data interchange format that tools must
understand to conform to the protocol. Other, more industry-wide standardization efforts are being sponsored by the
Internet Engineering Task Force (IETF) based in part on the
vCalendar specification. The IETF has recently sponsored
the development of three separate calendaring protocols, the
Calendaring Interoperability Protocol (CIP), the Core
Object Specification (COS), and the Internet Calendar
1. For Workflow API and Interchange

3.1 Organization vs. Personal Process Space
Automated Workflow is the specification and execution of a
business process of an organization[9]. Workflows are modeled as a collection of process steps, or tasks, assigned to
individuals taking on particular roles. Many modern workflow systems work in this way; the process is considered
from a single organization‚Äôs viewpoint. This viewpoint is
illustrated in Figure 2.
Organization A
Workflow 1
Workflow 2
FIGURE 1. WfMC Reference Model ([23])
Access Protocol (ICAP). These protocols specify interface
and other requirements on calendaring systems exchanging
calendaring data.
The standardization efforts in both workflow and calendaring focus on low-level data interchange and protocols for
exchanging such data in a client-server environment. While
this is a widely accepted standardization approach, we fear
that a stable data format is difficult to obtain due to the
maturing of the underlying models in each domain. This is
especially true in workflow. In the calendaring domain, a
problematic issue is that calendaring formats and tools support only rudimentary dependencies between tasks. These
issues are compounded when integrating workflow and calendaring systems. Workflow systems can write events to
calendar tools, but are not aware of the personal views of
the process of the participating individuals. Likewise, calendaring systems provide a personalized view of work, but do
not possess sophisticated enough models to negotiate with
workflow systems over the ability to do assigned work.

3.0 Integrated Process Support
We advocate an integrated view of an organization‚Äôs process
space and the personal process spaces of its individual
workers. In this view, the organization‚Äôs workflows are integrated with individual personal process spaces. Section 3.1
discusses this idea in more detail. To support this integrated
view, we advocate a component-based approach to process
modeling that avoids a reliance on low-level data interchange formats. This approach is called Open Process Components, and is described in Section 3.2. Finally, we propose
a generic architecture in Section 3.3 that derives from our
integrated view of process. In Section 4.0 we present some
Java prototype tools based on our ideas.

Workflow 3

Workflow 4
Workflow 5

Organization B
FIGURE 2. Organizational Process Perspective
Figure 2 shows the process space of two organizations,
generically labeled A and B. These organizations share two
workflows: Workflow 3 and Workflow 4. Interoperability of
the underlying process models and process support architecture is required to allow these organizations to share these
workflows.
The workflow systems we have experienced or seen in the
literature take this organization-centered approach to automating business processes. For example, Action Workflow
from Action Technologies[1] operates on a cyclical model
where workflow units interoperate to produce customer satisfaction. Different participants are viewed as customers,
performers, or observers at each workflow stage of the
cycle. While Action Workflow provides client-side functionality to obtain task lists for individuals, it does not provide a structured way for individuals to define personal
processes and integrate them into the scope of organizational processes. Another example is the application of
groupware-oriented tools such as Lotus Notes to workflow[20]. Notes provides much of the needed infrastructure
for managing data and transactions within a workflow.
However, again there is no structured way to define personal processes and integrate them into organizational processes. Instead, the approach is again organization-centered,
where workflows are defined at the organizational scope,

and personal tasks derived from the workflow model. Still
other workflow platforms, such as InConcert[16], emphasize collaborative aspects of workflow execution. Collaborative work is closer in spirit to the idea of integrated
process spaces, but differs in that the emphasis is on mechanisms supporting shared access to data. Users still act on
tasks delegated to them by the organizational workflow
model.
To keep pace with industry trends and technology impacts,
this organization-centered viewpoint will have to change in
at least the following ways:

‚Ä¢ Interoperability between workflows developed across
business functional units and/or organizations must be
supported.

Figure 3 shows an agent-centered viewpoint of the process
space. Jill is an agent working for Organization A, Bob
works for Organization B. Jill participates in Organization
A‚Äôs workflow 1 and 3. Bob participates in Organization B‚Äôs
workflows 3 and 5. In order to accomplish tasks in workflow 1, Jill employs her Personal Process 1. Likewise, Bob
employs his Personal Process 3 in carrying out tasks relevant to Workflow 5. In addition, Bob employs Personal Process 3 to carry out similar tasks in the shared Workflow 3.
Jill does not have a relevant personal process defined for her
assigned tasks in Workflow 3. Finally, each individual may
have personal processes defined that are outside the scope
of an explicit workflow for either organization. These may
be processes defined solely by the individual‚Äôs personal productivity initiative.
Organization A‚Äôs Space

‚Ä¢ The potential for wide-area distributed participation
must be supported.

Jill

Personal
Process 1

Personal
Process 2

‚Ä¢ Individuals must have the ability to define, execute, and
track the personal processes they perform to be productive within the context of an organization‚Äôs business processes and goals.
The work of the Workflow Management Coalition as well
as research efforts such as our Open Process Components
Framework (see Section 3.2) address the first two issues
directly. However, there has not been a lot of consideration
for the last issue. At best, current workflow systems notify
individuals of new work items through email or custom client applications. Some even have the ability to write to personal calendaring software through interfaces such as
Microsoft and Wang‚Äôs MAPI-WF[17]. But the viewpoint
still originates with the organizational process. An agentcentered viewpoint, showing the distribution of workflows
an individual participates in, and the set of personal processes an individual employs, is not considered.
The need for supporting the personal process view is just
beginning to be recognized in more dynamic process areas
such as Software Engineering[11]. In the software process
domain, the work of the software developer is considered
dynamic in the sense that the developer must be creative in
seeking the solutions to design, implementation, and maintenance dilemmas[4]. As workflow extends to more complex and skilled tasks, automated workflow systems will be
required to encompass more than just the straightforward
document-routing capabilities of image processing systems.
Future demands will include the ability to support more of
the skilled, or knowledge work, that people perform in the
organization. In order to do this, workflow systems must
relax the prescriptive constraints it places on performers of
the workflow, and allow these workers to perform their own
personal processes to carry out the work.

Personal Space

Workflow 1
Workflow 3
Workflow 5
Personal
Process 4
Personal
Process 3
Organization B‚Äôs Space

Bob

Personal Space

FIGURE 3. Personal Process Perspective
There are several reasons for arguing for an integrated view
of organizational and personal processes. Figure 3 shows
the overlap of the personal and organizational process
space. Defining and executing business processes is motivated in part by the need to ensure business goals are
achieved. Workflows are largely assumed to be static, repetitive processes that involve rote decision-making in support
of well-defined business goals[9] 1. To expand the scope of
processes automated workflow systems can support, more
dynamic workflows that include personal processes should
be considered. Another motivating reason comes from the
diverse set of relationships in which both organizations and
individuals participate. Individual workers, particularly at
1. We refer to Georgakopoulos, Hornick, and Sheth‚Äôs[9]
trade press characterization of administrative and production workflows. Our research is closer to ad hoc
workflows, though our point is they can be better understood through an integrated view of the process space.

highly skilled levels, perform in a wide variety of diverse
business functions. Downsizing and decentralization of
organizations coupled with increasing outsourcing of work
makes it unrealistic to take the single organization
approach. The business processes of multiple organizations
must be integrated with the personal processes of the participants.
In order to accomplish this integration, we propose a component-based approach to process modeling and an open
architecture for supporting personal and organizational process spaces.

work artifact that is either consumed as input by an Activity
or produced as output. A Role is a process-specific definition of the skill set required to perform an Activity. A Role
is process-specific as opposed to organization-specific,
meaning management must decide how to map organizational roles to process-specific roles. This mapping is the
relationship between Roles and Agents. The meta-model
described briefly here is adopted from the PCIS LCPS metamodel[7]. However, the concepts are similar in a variety of
general descriptions of process in the literature[5][9][15][22]. In the OPC Framework, this set of process entities and relationships form the basis for meaningful
component interactions.

3.2 Component-based Process Modeling
Organizations developing standards in workflow and calendaring focus on low-level data interchange protocols to be
applied in a client-server environment. The development of
such protocols, particularly the protocols related to workflow definition interchange1, are too restrictive to ensure
widespread adoption. Instead, we propose an object-oriented component-based approach to process modeling and
execution. In our research we are developing a componentbased framework for process modeling called the Open Process Components (OPC) Framework. It is not the focus of
this paper to delve into the details of the OPC Framework,
but we do provide a brief discussion relevant to the process
support architecture presented in Section 3.3. Further details
may be found in [8].
There is a need for a unifying framework for representing
and manipulating workflow abstractions. We take an objectoriented approach we call Open Process Components. Entities of the workflow domain are represented as objects, with
manipulations of those objects defined as object behaviors.
The approach is component-based, from the perspective that
interfaces are well-defined so that components interact in
meaningful ways. The OPC Framework provides a foundation for constructing component-based process models in an
extendable fashion.
There are three important aspects to the OPC Framework
that allow it to support component-based process modeling.
The first is a meta-model that identifies basic process entities and relationships between entities. Basic process entities include Process, Activity, Product, Role, and Agent. A
Process is a decomposable entity into subprocesses and subactivities. This allows development of process models in a
top-down fashion. An Activity is an executable fragment of
a process model; it represents a refinement of a portion of a
process model down to an executable state. A Product is a

1. More specifically, the Workflow Process Definition Language proposed in WAPI 1[24].

The second important aspect of the OPC Framework is a
state-based encapsulation of execution interfaces. By this
we mean each component in a process model possesses a
process state, and this state is manipulable by a set of interfaces to the component that are available during various
stages of executing the process model. Example interfaces
include start, suspend, resume, abort, completeWithFailure
and completeWithSuccess. Each component maintains an
explicit, independent state during execution of the process
model, and the state of process execution at any point in
time is the combination of states of the components
involved in the process.
The final salient feature of the OPC Framework is a threetiered object-oriented class hierarchy for defining components. An object-oriented methodology provides several
advantages: encapsulation of heterogeneous process representations, an economy of representation through inheritance, and the ability to specialize component definitions
through subclassing. From a process modeling perspective,
one major advantage of the hierarchy is its ability to be
extended. New component definitions and abstractions can
be added within the framework without modifying preexisting definitions. A second important advantage is that
specialized component definitions allow heterogeneous process modeling formalisms to interoperate with one another.
For example, a Petri-net based process model fragment can
interoperate with a process model fragment developed in a
scripting language by encapsulating each as a component
under the framework. This is especially beneficial in the
organizational/personal context of processes we consider in
this paper since it should not be assumed that homogeneous
process models are generated across these contexts.
As a brief example, consider the ad hoc workflow depicted
in Figure 4, taken from [9]. This workflow represents a
paper review process. In a component-based process model,
each task in the workflow is represented as an activity component. Interactions between the components is governed
by the set of interfaces each component supports. The benefit is that the implementation of each component is separated from these interfaces. Different process modeling and

enactment services can be used to define and execute the
details of each task. This differs from existing systems
where homogeneous models and supporting services are
employed.
The workflow in Figure 4 is a relevant example of the utility
of integrated organizational and personal process spaces.
Consider for example the ‚ÄúReview‚Äù tasks in the workflow.
These are assigned to separate persons fulfilling the role of
Reviewer. However, there is not sufficient detail in this definition to automate the support of review activities for each
reviewer. Furthermore, it is not appropriate to believe that
this organizational workflow should provide such detail.
Instead, it is more natural that each reviewer perform a personalized review process that meets the requirements of the
organizational workflow. Therefore, if Jill and Bob were
Reviewers in this workflow, each would carry out the
review according to her/his own personal process for
reviewing papers, employing familiar tools and methods for
producing the needed results.

3.3 A Process Support Architecture
To support the integration of the organizational and personal
process spaces, we propose an architecture that extends traditional workflow client-server architectures to include support for the personal process space. Figure 6 shows the
proposed general architecture.

Process
Definition Tools

Personal
Process Servers

Workflow
Servers

Calendar
Manager

Select
reviewers

Review
request 1

Review
request 2

Review
request n

Worklist
Handler

Calendar
Tool

FIGURE 6. Process Support Architecture
Distribute
Papers
Review 1

Review 2

Review 3

Produce
joint reviews
Forward
review

The architecture in Figure 6 integrates organizations‚Äô workflow servers and personal process servers with calendaring
technology to produce a time-oriented view of work for the
end-user. Arcs indicate the bidirectional flow of components over the architecture. This architecture extends traditional workflow architectures, such as the Workflow
Management Coalition‚Äôs Reference Model[22], to include
the end user‚Äôs personal process space. The components of
this architecture are:

‚Ä¢ Process Definition Tools
FIGURE 4. Example Workflow
(taken from [9)
Component-based process modeling is at the heart of our
research and relevant to the topics discussed in the rest of
this paper. However, the elements of organizational versus
personal process spaces and process architecture we discuss
do not necessarily rely on a component-based approach.
One can readily envision modifications to existing tools
such as Action Workflow or Lotus Notes discussed earlier
that would address process space integration. We encourage
the reader to consider process modeling approaches and
process space integration issues as independently as possible.

Process Definition Tools are used to create componentbased process models. These tools may query Personal
Process and Workflow servers in order to reuse existing
process component definitions.

‚Ä¢ Workflow Servers
One or more servers create the organizational process
space(s). These servers manage component-based workflow models created for organizational units by Process
Definition Tools.

‚Ä¢ Personal Process Servers
Similar to a Workflow Server, a Personal Process Server
manages process definitions for individuals, created
from components by Process Definition Tools.

‚Ä¢ Calendar Manager
The Calendar Manager is the organizer of an individual‚Äôs process space. The Calendar Manager manages
instances of process models from the individual‚Äôs perspective.

‚Ä¢ Worklist Handler/Calendar Tool
This is a client-side tool that presents the individual with
her/his work to do. This may be in the form of a task list,
or may be a time-oriented view depending on process
constraints and personal scheduling preferences.
This general architecture clearly shows the separation and
integration of organizational and personal process spaces.
The distinct servers manage personal and organization processes. This distinction is a logical one; in practice a single
implemented server may include the functionality to manage both process spaces. Integration of the spaces comes
from the Process Definition Tools and the Calendar Manager. A Process Definition Tool creates component-based
process models. By accessing the process definitions on
both servers, the tool is able to create and reuse organizational process that utilizes process specifications of relevant
individuals. The Calendar Manager integrates instances of
organizational and personal processes from the individual‚Äôs
perspective. The Calendar Manager has the ability to accept
or decline work requests from process servers, or manage
changes to the individual‚Äôs process space when forced to do
so. This tool is the focal point of the individual‚Äôs process
space. Finally, the Worklist Handler/Calendar Tool is a
combination of a workflow client and a personal calendaring tool. This client-side tool has the ability to host process
components and support the enactment of such components
in order to carry out the actual work.
The architecture we propose is an integration of current
workflow architectures such as the WfMC‚Äôs Reference
Model[22] and calendaring environments such as
Netscape‚Äôs Calendar Server[18]. However, current architectures do not take such an integrated view. We know of no
tool that allows for process models to be created that integrate a workflow model and a personal process model. The
proposed process definition tool allows for this integration.
We know of no environment that provides a componentized
personal view of process like the proposed Calendar Manager. One can envision workflow servers writing to an individual‚Äôs calendar through an interface such as Microsoft and
Wang‚Äôs MAPI-WF interface[17]. However, this requires
that the workflow server have explicit knowledge and
access rights to individuals‚Äô calendars. The proposed Calendar Manager explicitly manages an individual‚Äôs workspace,
negotiating between servers and individual preferences to
present the personal process space to the end user. The
existence of such a tool enables a component-based architecture that does not require Personal Process and Workflow

Servers to communicate directly to negotiate over rights to
assign work to an individual.
The proposed architecture is process model independent. It
does not favor any particular representation of process.
However, we again advocate the use of component-based
process models. Component-based process modeling allows
for easier integration of organizational and personal process
spaces in the Process Definition Tools and Calendar Managers. Without components, there would be a push on each
tool to support low-level protocols allowing for heterogeneous process models to be integrated. This is just the type
of interoperability that is deficient in current workflow systems, and a major motivating force behind the componentbased approach to process modeling described in
Section 3.2.
We have developed a set of Java tools realizing the proposed architecture. In the next section we present our
progress with this project.

4.0 The Current Prototype
The YFPPG Research Group at Arizona State University
has sponsored a series of Master‚Äôs projects during the
Spring 1997 semester for developing a toolset in Java for
component-based process modeling and enactment. This
toolset conforms closely to the general architecture presented in Section 3.3. The specific architecture is shown in
Figure 7.

Repository
Browser
Process
Component
Repository

Components
Editor
Worklist
Handler

Calendar
Manager
Server

Calendar
Client

FIGURE 7. OPC Support Architecture
The components of this architecture are:

‚Ä¢ Process Component Repository
This is implemented as a Java RMI[22] server that uses
Java Serialization facilities to distribute process objects
to client tools. The repository stores component-based
process definitions and distributed components for

enactment. Multiple named repositories, each storing
multiple process models, can be managed by a single
server.

‚Ä¢ Calendar Manager Server
Java RMI and CORBA1 versions of this server exist.
This server stores time-oriented appointments as well as
task lists for individuals.

‚Ä¢ Repository Browser
The Repository Browser is a process administration and
management tool that allows users to browse through the
current objects in a repository. This is implemented as a
Java RMI client.

‚Ä¢ Components Editor
The Components Editor is another Java RMI client. It
allows users to graphically create component-based process models through component creation and reuse.
Figure 8 shows the Components Editor GUI with our
example process definition from Figure 4.

FIGURE 8. Components Editor

‚Ä¢ Worklist Handler
This client-side tool obtains work items for a user from a
repository. The work items are actually Java objects that
are serialized and obtained through Java RMI calls.
Once the Worklist Handler obtains these objects, it can
execute them, changing the state of the process model
and invoking tools on work products. Figure 9 shows a
Worklist Handler for Bob.

‚Ä¢ Calendar Client
The Calendar Client obtains the appointments and task
lists for an individual from a Calendar Manager Server.
In addition, the Calendar Client can bring up a Worklist
Handler to access the Process Component Repository.
Java RMI and CORBA versions of this tool exist.

1. Iona Technologies‚Äô OrbixWeb[13] was used to implement the CORBA-enabled calendar server and client.

FIGURE 9. Worklist Handler
At this point in the development of our toolset we have yet
to implement the full envisioned functionality of the Calendar Manager Server. The overlap of the organizational and
personal process space occurs in the Calendar Client, which
is responsible for providing the integrated view of the two
spaces. The next step is to implement the full negotiation
between the two servers, as we discuss in the next section.
We have already learned several lessons during the development and use of this toolset. On the plus side, these tools
successfully demonstrate the integration of organizational
and personal process spaces. These tools are also demonstrations of forward-looking component distribution technologies such as Java RMI[22] and CORBA[19]. Finally,
these tools demonstrate the utility of component-based process modeling. There have been some hiccups however.
Managing migrating components in a distributed environment is a difficult configuration management problem. It
has proven troublesome to track distributed process components‚Äô states and synchronize updates to process models
stored in the repository. Despite these problems, we are
excited by the possibilities of distributed, component-based
process modeling, and are initiating a new set of projects to

update the current environment. Readers interested in
obtaining the prototypes or tracking progress of this project
may visit the YFPPG website at http://www.eas.asu.edu/
~yfppg.

6.0 References
[1.]

Action Technologies, Inc. Coordination Software:
Enabling the Horizontal Corporation. Action Technologies, Inc. White Paper. July, 1994.

5.0 Summary and Future Work

[2.]

Alonso, G. and Schek, H. Research Issues in Large
Workflow Management Systems. Proceedings of the
NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996.

[3.]

Armitage, J. and Kellner, M. A Conceptual Schema
for Process Definitions and Models. Proceedings of
the Third International Conference on the Software
Process (ICSP3), pp. 153-165, Reston, VA. October,
1994.

[4.]

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study
into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow
and Process Automation in Information Systems,
Athens, GA, May, 1996.

[5.]

Conradi, R., Liu, C., and Hagaseth, M. Planning Support for Cooperating Transactions in EPOS. Information Sciences, vol. 20, no. 4, pp. 317-336. 1995.

[6.]

Curtis, B., Kellner, M., and Over, J. Process Modeling. Communications of the ACM, vol. 35, no. 9, pp.
75-90, September, 1992.

[7.]

Derniame, J.C. Life Cycle Process Support in PCIS.
Proceedings of the PCTE ‚Äò94 Conference. 1994.

[8.]

Gary, K., Lindquist, T., and Koehnemann, H. Component-based Process Modeling. Technical Report TR97-022, Computer Science Department, Arizona State
University. May, 1997.

[9.]

Georgakopoulos, D., Hornick, M., and Sheth, A. An
Overview of Workflow Management: From Process
Modeling to Workflow Automation Infrastructure.
Distributed and Parallel Databases, vol. 3, pp. 119153. 1995.

In this paper we have advocated an integrated view of organizational workflows and personal process spaces. In this
view, both the perspective of the organization and the perspective of the individual are considered when integrating
process spaces. This view allows organizational goals to be
pursued while allowing individual workers the flexibility to
define how to accomplish such goals. Such flexibility will
be required in the not-too-distant future due to the increasing demands on current workflow systems and the current
pace of technology.
In this paper we proposed a generic architecture for process
support that logically integrates functionality needed for
both perspectives. We suggest a component-based process
modeling approach to further reduce the dependencies
between workflow and calendaring systems by avoiding the
need for low-level, brittle data interchange protocols.
Finally, we described a set of prototype tools based on component-based process modeling that realizes the generic
architecture. Despite the success or failure of our efforts, we
hope that the argument for integrated organizational and
personal process spaces will have an effect on future considerations in the converging areas of workflow and groupware
research.
Given the relatively early stage of this research, there are
several avenues we intend to pursue in this area. First, further research is needed to fully understand the nature of the
negotiation between organizational and personal process
spaces that takes place in the Calendar Manager. We are
pursuing research in this area under the topic Process Component Brokering, where such negotiation is carried out by
having the Calendar Manager provide a brokering service
that identifies personal process components that meet organizational process requirements. Second, we are looking at
ways to integrate automated planning and scheduling techniques for workflow and personal processes. The result will
be enhanced Calendar Managers that negotiate with organizations Workflow Servers to optimize the overlap between
organizational and personal process execution. Finally, we
plan to validate the proposed architecture by employing our
tools in real workflow settings, and extending our work into
more dynamic process areas. Specifically, we are looking at
ways to support Personal Software Processes and Distributed Learning processes between mentors and students.

[10.] Godart, C., Canals, G., Charoy, F., and Molli, P. An
Introduction to Cooperative Software Development in
COO. International Conference on System Integration, 1994.
[11.] Humphrey, W. The Personal Process in Software Engineering. Proceedings of the Third International
Conference on the Software Process (ICSP-3). IEEE
Press. October, 1994.
[12.] Internet Mail Consortium. vCalendar V1.0 Specification. Available at http://www.imc.org/pdi/pdiproddev.html
[13.] Iona Technologies. OrbixWeb 2.0 Programming
Guide. November 1995.
[14.] Khoshafian, S., and Buckiewicz, M. Introduction to
Groupware, Workflow, and Workgroup Computing.

J. Wiley and Sons, New York. 1995.
[15.] Lee, J. Gruniger, M., Jin, Y., Malone, T., and Yost, G.
The PIF Process Interchange Format and Framework.
Available at http://www.aiai.ed.ac.uk/pif/index.html.
May 24, 1996.
[16.] McCarthy, D. and Sarin, S. Workflow and Transactions in InConcert. IEEE Bulletin of the Technical
Committee on Data Engineering, vol. 16 no. 2. June
1993.
[17.] Microsoft Corporation and Wang Laboratories, Inc.
Microsoft MAPI Workflow Framework Concepts and
Facilities (White Paper). Available at http://
www.wang.com/sbu/w9602210.htm. February 21,
1996.
[18.] Netscape Communications Corporation. Netscape
Calendar Server 1.0 and 2.0. Available at http://
home.netscape.com/comprod/server_central/product/calendar/calendar2_data.html.
[19.] Object Management Group. Corba 2.0 Specification.
Available at http://www.omg.org/corbask.htm. July
1995.
[20.] Reinwald, B and Mohan, C. Structured Workflow
Management with Lotus Notes release 4. Proceedings
of the 41st IEEE CompCon digest of papers, pp.451457, Santa Clara, CA. February, 1996.
[21.] Riddle, W. E. Fundamental Process Modeling Con-

cepts. Proceedings of the NSF Workshop on Workflow and Process Automation in Information
Systems. May, 1996.
[22.] Sun Microsystems, Inc. Remote Method Invocation
Specification.
Available
at
http://www.javasoft.com:80/products/jdk/1.1/docs/guide/rmi/spec/
rmiTOC.doc.html
[23.] The Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003,
January 1995.
[24.] The Workflow Management Coalition. Interface 1:
Process Definition Interchange. WfMC Document
Number TC-1016, Version 1.0 Beta. May 29, 1996.
[25.] The Workflow Management Coalition. Interface 2
Specification. WfMC Document Number TC-1009,
Version 1.0. November 20, 1995.
[26.] The Workflow Management Coalition. Interoperability Abstract Specification. WfMC Document Number
TC-1012, Version 1.0. October 20, 1996.
[27.] The Workflow Management Coalition. Draft Audit
Specification. WfMC Document Number TC-1015.
August 14, 1996.

Automated Process Support for Organizational and Personal
Processes
Kevin Gary, Tim Lindquist, Harry Koehnemann. Ly Sauer
Arizona State University
Computer Science Department
Mail Stop 5406
Tempe, AZ 85287-5406
yfppg@asu.edu
ABSTRACT
We propose two views on process: an organizational view
and a personal process view. Information technology applies
Automated Workflow technology to define, execute, and
track an organization‚Äôs automated business processes. Calendaring tools provide a form of personal process view
through scheduled work items. However, the personal, or
individual, view of the process space has largely been
ignored. We maintain that as organizations become increasingly decentralized, a single organization‚Äôs process space is
becoming difficult to recognize. Individuals of the organization are asked to do work that spans organizational, functional, and even geographic boundaries. An integrated view
of organizational workflows and personal processes is
needed to address these new demands. In this paper we
argue for the need to integrate organizational and personal
processes. We then propose a component-based process
modeling approach and supporting process architecture that
integrates these process spaces. Finally, we describe our
recent efforts at developing Java prototype process tools that
realize the proposed modeling technique and supporting
architecture.
Keywords: Workflow, Personal Process, Components

1.0 Introduction
Recent changes in industry and technology are imposing
more demanding technical requirements on information
technology. In industry, organizations are downsizing and
becoming increasingly decentralized, often causing projects
to be managed across multiple organizations or functional

units. Current technology is growing at a rate that can be
difficult to track. Industry investments in desktop tools and
groupware must be leveraged against growth in local and
wide-area networks (LANs and WANs). In particular, the
growth of the Internet makes it possible to envision computer support of global, decentralized, business processes.
These changes in industry and technology escalate the pressures put on information technology research. Providing
computer support for widely distributed organizations using
new technologies such as the Internet, Groupware, Calendar
Management, and Automated Workflow is at least an IT
systems analyst‚Äôs headache. Determining the best way to
integrate these tools to ensure maximum productivity is at
best an IT manager‚Äôs nightmare. In our view, the ability to
define, execute, and track business processes is central to
the ability to integrate these technologies in a widely distributed setting and make their use productive. Therefore in our
research we focus on automated process support. In the
business domain, automating business processes is known
as Automated Workflow.
Workflow is the study of modeling and enacting business
processes by human and computer agents. Automated
Workflow adds an emphasis on applying current computer
and information technology in a workflow environment,
with the desire of automating parts of workflows, or supporting entire workflows.
Automated Workflow has traditionally focused on defining
and automating business processes from the organization‚Äôs
standpoint. Little regard is given to managing overlapping
workflows in an individual workspace, or for even considering the personal processes of an individual when considering the productivity of the organization. The current
solution is to drop a set of personal productivity tools, such
as calendaring tools, in the lap of the individual and let her/
him work it out.

In order to achieve greater productivity from both workflow
and personal productivity tools, a more integrated view of
organizational and personal processes must be considered.
An integrated view allows individuals the ability to develop
their own productive work practices in support of an organization‚Äôs processes, and allows for a more natural handling
of processes spanning multiple organizations and individuals. We are in the beginning stages of our research into the
utility of providing such an integrated view. In this paper we
propose an open architecture for integrating organizational
workflows and personal productivity processes. We motivate the need for an integrated approach, and present a component-based approach to process modeling that provides
the interoperability required to achieve the integration. We
also present a suite of tools being developed at Arizona
State University that realize this architecture.
The rest of this paper is organized as follows. Section 2.0
discusses relevant issues in current workflow and calendaring technology. Section 3.0 argues for an integrated view of
organizational and personal process spaces, presents a component-based approach to process modeling, and proposes a
general process support architecture. Section 4.0 presents
tool prototypes realizing this architecture that were recently
developed at Arizona State University. We conclude in
Section 5.0 with a summary and discuss future avenues for
our research.

2.0 Background
Approaches to developing workflow systems have both
commercial and academic origins. Commercial systems
have evolved from work on forms-based image processing
systems and groupware[14]. The line between workflow
and other types of systems is often blurred, with groupware,
scheduling, database, and email tools providing some workflow functionality. In addition, several commercial products
that advertise workflow capabilities fall far short of providing full-fledged support for defining and enacting business
processes. Academic research has focused mainly on process modeling and database transaction issues[9]. Process
modeling research has led to the development of workflow
representations based on a variety of formalisms. Database
transaction research focuses on extending traditional transaction semantics to support long duration[2][9] and/or cooperative transaction[5][10] models. The result is a
proliferation of approaches and issues relating to workflow.
Current efforts are attempting to get researchers and vendors to converge on a common foundation for workflow.
The Workflow Management Coalition (WfMC) was formed
in August 1993 to promote workflow technology. The
WfMC has proposed a reference model[22] and a set of
interfaces,
called
WAPIs1
based
on
that
model[24][25][26][27] as an attempt at standardizing archi-

tectural elements of workflow systems and the interactions
between those elements. The Process Interchange Format
(PIF) Working Group was formed to explore the potential to
provide automatic translations between process representation formalisms[15]. Finally, Microsoft is pushing their
Messaging API (MAPI) as a defacto standard for implementing workflow systems. Microsoft has recently teamed
with Wang to develop the MAPI-WF specification[17], an
extension of MAPI for supporting workflow-specific services.
The WfMC is presently the most significant of the efforts
attempting to standardize workflow systems. The WfMC
Reference Model (Figure 1) identifies the basic architectural components of a workflow environment. At the center
of the model is a Workflow Enactment Service (WES),
comprised of one or more Workflow Engines. A WES provides services through the WAPIs to workflow-related tools.
These include Process Definition Tools for defining processes, Workflow Client Applications for handling user
requests for work, Third-party Applications that need to
communicate data and operations to the WES, other WESs
for providing interoperability between enactment services,
and Administration and Monitoring Tools for data gathering
for process improvement activities.
The WfMC Reference Model identifies common workflow
system components and interfaces. The WAPI interface
specifications define a set of low-level protocols for synchronously and asynchronously exchanging workflow data
between the tools and the WES. Our basic problem with this
approach is that these protocols are too low-level; they
imply a restrictive workflow model. Workflow representations that cannot easily convert their process data to conform with this underlying model cannot obtain conformance
with the model. This is one of the issues our research
addresses.
Recent standardization efforts also address the area of calendaring protocols. One popular calendaring protocol
(adopted by Netscape‚Äôs Calendar Server[18]) is the vCalendar protocol[12]. In the vCalendar protocol, calendaring and
scheduling entities, called events, are transported between
applications that can understand the protocol. This approach
is similar to the effort of the WfMC protocols in that it
defines a low-level data interchange format that tools must
understand to conform to the protocol. Other, more industry-wide standardization efforts are being sponsored by the
Internet Engineering Task Force (IETF) based in part on the
vCalendar specification. The IETF has recently sponsored
the development of three separate calendaring protocols, the
Calendaring Interoperability Protocol (CIP), the Core
Object Specification (COS), and the Internet Calendar
1. For Workflow API and Interchange

3.1 Organization vs. Personal Process Space
Automated Workflow is the specification and execution of a
business process of an organization[9]. Workflows are modeled as a collection of process steps, or tasks, assigned to
individuals taking on particular roles. Many modern workflow systems work in this way; the process is considered
from a single organization‚Äôs viewpoint. This viewpoint is
illustrated in Figure 2.
Organization A
Workflow 1
Workflow 2
FIGURE 1. WfMC Reference Model ([23])
Access Protocol (ICAP). These protocols specify interface
and other requirements on calendaring systems exchanging
calendaring data.
The standardization efforts in both workflow and calendaring focus on low-level data interchange and protocols for
exchanging such data in a client-server environment. While
this is a widely accepted standardization approach, we fear
that a stable data format is difficult to obtain due to the
maturing of the underlying models in each domain. This is
especially true in workflow. In the calendaring domain, a
problematic issue is that calendaring formats and tools support only rudimentary dependencies between tasks. These
issues are compounded when integrating workflow and calendaring systems. Workflow systems can write events to
calendar tools, but are not aware of the personal views of
the process of the participating individuals. Likewise, calendaring systems provide a personalized view of work, but do
not possess sophisticated enough models to negotiate with
workflow systems over the ability to do assigned work.

3.0 Integrated Process Support
We advocate an integrated view of an organization‚Äôs process
space and the personal process spaces of its individual
workers. In this view, the organization‚Äôs workflows are integrated with individual personal process spaces. Section 3.1
discusses this idea in more detail. To support this integrated
view, we advocate a component-based approach to process
modeling that avoids a reliance on low-level data interchange formats. This approach is called Open Process Components, and is described in Section 3.2. Finally, we propose
a generic architecture in Section 3.3 that derives from our
integrated view of process. In Section 4.0 we present some
Java prototype tools based on our ideas.

Workflow 3

Workflow 4
Workflow 5

Organization B
FIGURE 2. Organizational Process Perspective
Figure 2 shows the process space of two organizations,
generically labeled A and B. These organizations share two
workflows: Workflow 3 and Workflow 4. Interoperability of
the underlying process models and process support architecture is required to allow these organizations to share these
workflows.
The workflow systems we have experienced or seen in the
literature take this organization-centered approach to automating business processes. For example, Action Workflow
from Action Technologies[1] operates on a cyclical model
where workflow units interoperate to produce customer satisfaction. Different participants are viewed as customers,
performers, or observers at each workflow stage of the
cycle. While Action Workflow provides client-side functionality to obtain task lists for individuals, it does not provide a structured way for individuals to define personal
processes and integrate them into the scope of organizational processes. Another example is the application of
groupware-oriented tools such as Lotus Notes to workflow[20]. Notes provides much of the needed infrastructure
for managing data and transactions within a workflow.
However, again there is no structured way to define personal processes and integrate them into organizational processes. Instead, the approach is again organization-centered,
where workflows are defined at the organizational scope,

and personal tasks derived from the workflow model. Still
other workflow platforms, such as InConcert[16], emphasize collaborative aspects of workflow execution. Collaborative work is closer in spirit to the idea of integrated
process spaces, but differs in that the emphasis is on mechanisms supporting shared access to data. Users still act on
tasks delegated to them by the organizational workflow
model.
To keep pace with industry trends and technology impacts,
this organization-centered viewpoint will have to change in
at least the following ways:

‚Ä¢ Interoperability between workflows developed across
business functional units and/or organizations must be
supported.

Figure 3 shows an agent-centered viewpoint of the process
space. Jill is an agent working for Organization A, Bob
works for Organization B. Jill participates in Organization
A‚Äôs workflow 1 and 3. Bob participates in Organization B‚Äôs
workflows 3 and 5. In order to accomplish tasks in workflow 1, Jill employs her Personal Process 1. Likewise, Bob
employs his Personal Process 3 in carrying out tasks relevant to Workflow 5. In addition, Bob employs Personal Process 3 to carry out similar tasks in the shared Workflow 3.
Jill does not have a relevant personal process defined for her
assigned tasks in Workflow 3. Finally, each individual may
have personal processes defined that are outside the scope
of an explicit workflow for either organization. These may
be processes defined solely by the individual‚Äôs personal productivity initiative.
Organization A‚Äôs Space

‚Ä¢ The potential for wide-area distributed participation
must be supported.

Jill

Personal
Process 1

Personal
Process 2

‚Ä¢ Individuals must have the ability to define, execute, and
track the personal processes they perform to be productive within the context of an organization‚Äôs business processes and goals.
The work of the Workflow Management Coalition as well
as research efforts such as our Open Process Components
Framework (see Section 3.2) address the first two issues
directly. However, there has not been a lot of consideration
for the last issue. At best, current workflow systems notify
individuals of new work items through email or custom client applications. Some even have the ability to write to personal calendaring software through interfaces such as
Microsoft and Wang‚Äôs MAPI-WF[17]. But the viewpoint
still originates with the organizational process. An agentcentered viewpoint, showing the distribution of workflows
an individual participates in, and the set of personal processes an individual employs, is not considered.
The need for supporting the personal process view is just
beginning to be recognized in more dynamic process areas
such as Software Engineering[11]. In the software process
domain, the work of the software developer is considered
dynamic in the sense that the developer must be creative in
seeking the solutions to design, implementation, and maintenance dilemmas[4]. As workflow extends to more complex and skilled tasks, automated workflow systems will be
required to encompass more than just the straightforward
document-routing capabilities of image processing systems.
Future demands will include the ability to support more of
the skilled, or knowledge work, that people perform in the
organization. In order to do this, workflow systems must
relax the prescriptive constraints it places on performers of
the workflow, and allow these workers to perform their own
personal processes to carry out the work.

Personal Space

Workflow 1
Workflow 3
Workflow 5
Personal
Process 4
Personal
Process 3
Organization B‚Äôs Space

Bob

Personal Space

FIGURE 3. Personal Process Perspective
There are several reasons for arguing for an integrated view
of organizational and personal processes. Figure 3 shows
the overlap of the personal and organizational process
space. Defining and executing business processes is motivated in part by the need to ensure business goals are
achieved. Workflows are largely assumed to be static, repetitive processes that involve rote decision-making in support
of well-defined business goals[9] 1. To expand the scope of
processes automated workflow systems can support, more
dynamic workflows that include personal processes should
be considered. Another motivating reason comes from the
diverse set of relationships in which both organizations and
individuals participate. Individual workers, particularly at
1. We refer to Georgakopoulos, Hornick, and Sheth‚Äôs[9]
trade press characterization of administrative and production workflows. Our research is closer to ad hoc
workflows, though our point is they can be better understood through an integrated view of the process space.

highly skilled levels, perform in a wide variety of diverse
business functions. Downsizing and decentralization of
organizations coupled with increasing outsourcing of work
makes it unrealistic to take the single organization
approach. The business processes of multiple organizations
must be integrated with the personal processes of the participants.
In order to accomplish this integration, we propose a component-based approach to process modeling and an open
architecture for supporting personal and organizational process spaces.

work artifact that is either consumed as input by an Activity
or produced as output. A Role is a process-specific definition of the skill set required to perform an Activity. A Role
is process-specific as opposed to organization-specific,
meaning management must decide how to map organizational roles to process-specific roles. This mapping is the
relationship between Roles and Agents. The meta-model
described briefly here is adopted from the PCIS LCPS metamodel[7]. However, the concepts are similar in a variety of
general descriptions of process in the literature[5][9][15][22]. In the OPC Framework, this set of process entities and relationships form the basis for meaningful
component interactions.

3.2 Component-based Process Modeling
Organizations developing standards in workflow and calendaring focus on low-level data interchange protocols to be
applied in a client-server environment. The development of
such protocols, particularly the protocols related to workflow definition interchange1, are too restrictive to ensure
widespread adoption. Instead, we propose an object-oriented component-based approach to process modeling and
execution. In our research we are developing a componentbased framework for process modeling called the Open Process Components (OPC) Framework. It is not the focus of
this paper to delve into the details of the OPC Framework,
but we do provide a brief discussion relevant to the process
support architecture presented in Section 3.3. Further details
may be found in [8].
There is a need for a unifying framework for representing
and manipulating workflow abstractions. We take an objectoriented approach we call Open Process Components. Entities of the workflow domain are represented as objects, with
manipulations of those objects defined as object behaviors.
The approach is component-based, from the perspective that
interfaces are well-defined so that components interact in
meaningful ways. The OPC Framework provides a foundation for constructing component-based process models in an
extendable fashion.
There are three important aspects to the OPC Framework
that allow it to support component-based process modeling.
The first is a meta-model that identifies basic process entities and relationships between entities. Basic process entities include Process, Activity, Product, Role, and Agent. A
Process is a decomposable entity into subprocesses and subactivities. This allows development of process models in a
top-down fashion. An Activity is an executable fragment of
a process model; it represents a refinement of a portion of a
process model down to an executable state. A Product is a

1. More specifically, the Workflow Process Definition Language proposed in WAPI 1[24].

The second important aspect of the OPC Framework is a
state-based encapsulation of execution interfaces. By this
we mean each component in a process model possesses a
process state, and this state is manipulable by a set of interfaces to the component that are available during various
stages of executing the process model. Example interfaces
include start, suspend, resume, abort, completeWithFailure
and completeWithSuccess. Each component maintains an
explicit, independent state during execution of the process
model, and the state of process execution at any point in
time is the combination of states of the components
involved in the process.
The final salient feature of the OPC Framework is a threetiered object-oriented class hierarchy for defining components. An object-oriented methodology provides several
advantages: encapsulation of heterogeneous process representations, an economy of representation through inheritance, and the ability to specialize component definitions
through subclassing. From a process modeling perspective,
one major advantage of the hierarchy is its ability to be
extended. New component definitions and abstractions can
be added within the framework without modifying preexisting definitions. A second important advantage is that
specialized component definitions allow heterogeneous process modeling formalisms to interoperate with one another.
For example, a Petri-net based process model fragment can
interoperate with a process model fragment developed in a
scripting language by encapsulating each as a component
under the framework. This is especially beneficial in the
organizational/personal context of processes we consider in
this paper since it should not be assumed that homogeneous
process models are generated across these contexts.
As a brief example, consider the ad hoc workflow depicted
in Figure 4, taken from [9]. This workflow represents a
paper review process. In a component-based process model,
each task in the workflow is represented as an activity component. Interactions between the components is governed
by the set of interfaces each component supports. The benefit is that the implementation of each component is separated from these interfaces. Different process modeling and

enactment services can be used to define and execute the
details of each task. This differs from existing systems
where homogeneous models and supporting services are
employed.
The workflow in Figure 4 is a relevant example of the utility
of integrated organizational and personal process spaces.
Consider for example the ‚ÄúReview‚Äù tasks in the workflow.
These are assigned to separate persons fulfilling the role of
Reviewer. However, there is not sufficient detail in this definition to automate the support of review activities for each
reviewer. Furthermore, it is not appropriate to believe that
this organizational workflow should provide such detail.
Instead, it is more natural that each reviewer perform a personalized review process that meets the requirements of the
organizational workflow. Therefore, if Jill and Bob were
Reviewers in this workflow, each would carry out the
review according to her/his own personal process for
reviewing papers, employing familiar tools and methods for
producing the needed results.

3.3 A Process Support Architecture
To support the integration of the organizational and personal
process spaces, we propose an architecture that extends traditional workflow client-server architectures to include support for the personal process space. Figure 6 shows the
proposed general architecture.

Process
Definition Tools

Personal
Process Servers

Workflow
Servers

Calendar
Manager

Select
reviewers

Review
request 1

Review
request 2

Review
request n

Worklist
Handler

Calendar
Tool

FIGURE 6. Process Support Architecture
Distribute
Papers
Review 1

Review 2

Review 3

Produce
joint reviews
Forward
review

The architecture in Figure 6 integrates organizations‚Äô workflow servers and personal process servers with calendaring
technology to produce a time-oriented view of work for the
end-user. Arcs indicate the bidirectional flow of components over the architecture. This architecture extends traditional workflow architectures, such as the Workflow
Management Coalition‚Äôs Reference Model[22], to include
the end user‚Äôs personal process space. The components of
this architecture are:

‚Ä¢ Process Definition Tools
FIGURE 4. Example Workflow
(taken from [9)
Component-based process modeling is at the heart of our
research and relevant to the topics discussed in the rest of
this paper. However, the elements of organizational versus
personal process spaces and process architecture we discuss
do not necessarily rely on a component-based approach.
One can readily envision modifications to existing tools
such as Action Workflow or Lotus Notes discussed earlier
that would address process space integration. We encourage
the reader to consider process modeling approaches and
process space integration issues as independently as possible.

Process Definition Tools are used to create componentbased process models. These tools may query Personal
Process and Workflow servers in order to reuse existing
process component definitions.

‚Ä¢ Workflow Servers
One or more servers create the organizational process
space(s). These servers manage component-based workflow models created for organizational units by Process
Definition Tools.

‚Ä¢ Personal Process Servers
Similar to a Workflow Server, a Personal Process Server
manages process definitions for individuals, created
from components by Process Definition Tools.

‚Ä¢ Calendar Manager
The Calendar Manager is the organizer of an individual‚Äôs process space. The Calendar Manager manages
instances of process models from the individual‚Äôs perspective.

‚Ä¢ Worklist Handler/Calendar Tool
This is a client-side tool that presents the individual with
her/his work to do. This may be in the form of a task list,
or may be a time-oriented view depending on process
constraints and personal scheduling preferences.
This general architecture clearly shows the separation and
integration of organizational and personal process spaces.
The distinct servers manage personal and organization processes. This distinction is a logical one; in practice a single
implemented server may include the functionality to manage both process spaces. Integration of the spaces comes
from the Process Definition Tools and the Calendar Manager. A Process Definition Tool creates component-based
process models. By accessing the process definitions on
both servers, the tool is able to create and reuse organizational process that utilizes process specifications of relevant
individuals. The Calendar Manager integrates instances of
organizational and personal processes from the individual‚Äôs
perspective. The Calendar Manager has the ability to accept
or decline work requests from process servers, or manage
changes to the individual‚Äôs process space when forced to do
so. This tool is the focal point of the individual‚Äôs process
space. Finally, the Worklist Handler/Calendar Tool is a
combination of a workflow client and a personal calendaring tool. This client-side tool has the ability to host process
components and support the enactment of such components
in order to carry out the actual work.
The architecture we propose is an integration of current
workflow architectures such as the WfMC‚Äôs Reference
Model[22] and calendaring environments such as
Netscape‚Äôs Calendar Server[18]. However, current architectures do not take such an integrated view. We know of no
tool that allows for process models to be created that integrate a workflow model and a personal process model. The
proposed process definition tool allows for this integration.
We know of no environment that provides a componentized
personal view of process like the proposed Calendar Manager. One can envision workflow servers writing to an individual‚Äôs calendar through an interface such as Microsoft and
Wang‚Äôs MAPI-WF interface[17]. However, this requires
that the workflow server have explicit knowledge and
access rights to individuals‚Äô calendars. The proposed Calendar Manager explicitly manages an individual‚Äôs workspace,
negotiating between servers and individual preferences to
present the personal process space to the end user. The
existence of such a tool enables a component-based architecture that does not require Personal Process and Workflow

Servers to communicate directly to negotiate over rights to
assign work to an individual.
The proposed architecture is process model independent. It
does not favor any particular representation of process.
However, we again advocate the use of component-based
process models. Component-based process modeling allows
for easier integration of organizational and personal process
spaces in the Process Definition Tools and Calendar Managers. Without components, there would be a push on each
tool to support low-level protocols allowing for heterogeneous process models to be integrated. This is just the type
of interoperability that is deficient in current workflow systems, and a major motivating force behind the componentbased approach to process modeling described in
Section 3.2.
We have developed a set of Java tools realizing the proposed architecture. In the next section we present our
progress with this project.

4.0 The Current Prototype
The YFPPG Research Group at Arizona State University
has sponsored a series of Master‚Äôs projects during the
Spring 1997 semester for developing a toolset in Java for
component-based process modeling and enactment. This
toolset conforms closely to the general architecture presented in Section 3.3. The specific architecture is shown in
Figure 7.

Repository
Browser
Process
Component
Repository

Components
Editor
Worklist
Handler

Calendar
Manager
Server

Calendar
Client

FIGURE 7. OPC Support Architecture
The components of this architecture are:

‚Ä¢ Process Component Repository
This is implemented as a Java RMI[22] server that uses
Java Serialization facilities to distribute process objects
to client tools. The repository stores component-based
process definitions and distributed components for

enactment. Multiple named repositories, each storing
multiple process models, can be managed by a single
server.

‚Ä¢ Calendar Manager Server
Java RMI and CORBA1 versions of this server exist.
This server stores time-oriented appointments as well as
task lists for individuals.

‚Ä¢ Repository Browser
The Repository Browser is a process administration and
management tool that allows users to browse through the
current objects in a repository. This is implemented as a
Java RMI client.

‚Ä¢ Components Editor
The Components Editor is another Java RMI client. It
allows users to graphically create component-based process models through component creation and reuse.
Figure 8 shows the Components Editor GUI with our
example process definition from Figure 4.

FIGURE 8. Components Editor

‚Ä¢ Worklist Handler
This client-side tool obtains work items for a user from a
repository. The work items are actually Java objects that
are serialized and obtained through Java RMI calls.
Once the Worklist Handler obtains these objects, it can
execute them, changing the state of the process model
and invoking tools on work products. Figure 9 shows a
Worklist Handler for Bob.

‚Ä¢ Calendar Client
The Calendar Client obtains the appointments and task
lists for an individual from a Calendar Manager Server.
In addition, the Calendar Client can bring up a Worklist
Handler to access the Process Component Repository.
Java RMI and CORBA versions of this tool exist.

1. Iona Technologies‚Äô OrbixWeb[13] was used to implement the CORBA-enabled calendar server and client.

FIGURE 9. Worklist Handler
At this point in the development of our toolset we have yet
to implement the full envisioned functionality of the Calendar Manager Server. The overlap of the organizational and
personal process space occurs in the Calendar Client, which
is responsible for providing the integrated view of the two
spaces. The next step is to implement the full negotiation
between the two servers, as we discuss in the next section.
We have already learned several lessons during the development and use of this toolset. On the plus side, these tools
successfully demonstrate the integration of organizational
and personal process spaces. These tools are also demonstrations of forward-looking component distribution technologies such as Java RMI[22] and CORBA[19]. Finally,
these tools demonstrate the utility of component-based process modeling. There have been some hiccups however.
Managing migrating components in a distributed environment is a difficult configuration management problem. It
has proven troublesome to track distributed process components‚Äô states and synchronize updates to process models
stored in the repository. Despite these problems, we are
excited by the possibilities of distributed, component-based
process modeling, and are initiating a new set of projects to

update the current environment. Readers interested in
obtaining the prototypes or tracking progress of this project
may visit the YFPPG website at http://www.eas.asu.edu/
~yfppg.

6.0 References
[1.]

Action Technologies, Inc. Coordination Software:
Enabling the Horizontal Corporation. Action Technologies, Inc. White Paper. July, 1994.

5.0 Summary and Future Work

[2.]

Alonso, G. and Schek, H. Research Issues in Large
Workflow Management Systems. Proceedings of the
NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996.

[3.]

Armitage, J. and Kellner, M. A Conceptual Schema
for Process Definitions and Models. Proceedings of
the Third International Conference on the Software
Process (ICSP3), pp. 153-165, Reston, VA. October,
1994.

[4.]

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study
into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow
and Process Automation in Information Systems,
Athens, GA, May, 1996.

[5.]

Conradi, R., Liu, C., and Hagaseth, M. Planning Support for Cooperating Transactions in EPOS. Information Sciences, vol. 20, no. 4, pp. 317-336. 1995.

[6.]

Curtis, B., Kellner, M., and Over, J. Process Modeling. Communications of the ACM, vol. 35, no. 9, pp.
75-90, September, 1992.

[7.]

Derniame, J.C. Life Cycle Process Support in PCIS.
Proceedings of the PCTE ‚Äò94 Conference. 1994.

[8.]

Gary, K., Lindquist, T., and Koehnemann, H. Component-based Process Modeling. Technical Report TR97-022, Computer Science Department, Arizona State
University. May, 1997.

[9.]

Georgakopoulos, D., Hornick, M., and Sheth, A. An
Overview of Workflow Management: From Process
Modeling to Workflow Automation Infrastructure.
Distributed and Parallel Databases, vol. 3, pp. 119153. 1995.

In this paper we have advocated an integrated view of organizational workflows and personal process spaces. In this
view, both the perspective of the organization and the perspective of the individual are considered when integrating
process spaces. This view allows organizational goals to be
pursued while allowing individual workers the flexibility to
define how to accomplish such goals. Such flexibility will
be required in the not-too-distant future due to the increasing demands on current workflow systems and the current
pace of technology.
In this paper we proposed a generic architecture for process
support that logically integrates functionality needed for
both perspectives. We suggest a component-based process
modeling approach to further reduce the dependencies
between workflow and calendaring systems by avoiding the
need for low-level, brittle data interchange protocols.
Finally, we described a set of prototype tools based on component-based process modeling that realizes the generic
architecture. Despite the success or failure of our efforts, we
hope that the argument for integrated organizational and
personal process spaces will have an effect on future considerations in the converging areas of workflow and groupware
research.
Given the relatively early stage of this research, there are
several avenues we intend to pursue in this area. First, further research is needed to fully understand the nature of the
negotiation between organizational and personal process
spaces that takes place in the Calendar Manager. We are
pursuing research in this area under the topic Process Component Brokering, where such negotiation is carried out by
having the Calendar Manager provide a brokering service
that identifies personal process components that meet organizational process requirements. Second, we are looking at
ways to integrate automated planning and scheduling techniques for workflow and personal processes. The result will
be enhanced Calendar Managers that negotiate with organizations Workflow Servers to optimize the overlap between
organizational and personal process execution. Finally, we
plan to validate the proposed architecture by employing our
tools in real workflow settings, and extending our work into
more dynamic process areas. Specifically, we are looking at
ways to support Personal Software Processes and Distributed Learning processes between mentors and students.

[10.] Godart, C., Canals, G., Charoy, F., and Molli, P. An
Introduction to Cooperative Software Development in
COO. International Conference on System Integration, 1994.
[11.] Humphrey, W. The Personal Process in Software Engineering. Proceedings of the Third International
Conference on the Software Process (ICSP-3). IEEE
Press. October, 1994.
[12.] Internet Mail Consortium. vCalendar V1.0 Specification. Available at http://www.imc.org/pdi/pdiproddev.html
[13.] Iona Technologies. OrbixWeb 2.0 Programming
Guide. November 1995.
[14.] Khoshafian, S., and Buckiewicz, M. Introduction to
Groupware, Workflow, and Workgroup Computing.

J. Wiley and Sons, New York. 1995.
[15.] Lee, J. Gruniger, M., Jin, Y., Malone, T., and Yost, G.
The PIF Process Interchange Format and Framework.
Available at http://www.aiai.ed.ac.uk/pif/index.html.
May 24, 1996.
[16.] McCarthy, D. and Sarin, S. Workflow and Transactions in InConcert. IEEE Bulletin of the Technical
Committee on Data Engineering, vol. 16 no. 2. June
1993.
[17.] Microsoft Corporation and Wang Laboratories, Inc.
Microsoft MAPI Workflow Framework Concepts and
Facilities (White Paper). Available at http://
www.wang.com/sbu/w9602210.htm. February 21,
1996.
[18.] Netscape Communications Corporation. Netscape
Calendar Server 1.0 and 2.0. Available at http://
home.netscape.com/comprod/server_central/product/calendar/calendar2_data.html.
[19.] Object Management Group. Corba 2.0 Specification.
Available at http://www.omg.org/corbask.htm. July
1995.
[20.] Reinwald, B and Mohan, C. Structured Workflow
Management with Lotus Notes release 4. Proceedings
of the 41st IEEE CompCon digest of papers, pp.451457, Santa Clara, CA. February, 1996.
[21.] Riddle, W. E. Fundamental Process Modeling Con-

cepts. Proceedings of the NSF Workshop on Workflow and Process Automation in Information
Systems. May, 1996.
[22.] Sun Microsystems, Inc. Remote Method Invocation
Specification.
Available
at
http://www.javasoft.com:80/products/jdk/1.1/docs/guide/rmi/spec/
rmiTOC.doc.html
[23.] The Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003,
January 1995.
[24.] The Workflow Management Coalition. Interface 1:
Process Definition Interchange. WfMC Document
Number TC-1016, Version 1.0 Beta. May 29, 1996.
[25.] The Workflow Management Coalition. Interface 2
Specification. WfMC Document Number TC-1009,
Version 1.0. November 20, 1995.
[26.] The Workflow Management Coalition. Interoperability Abstract Specification. WfMC Document Number
TC-1012, Version 1.0. October 20, 1996.
[27.] The Workflow Management Coalition. Draft Audit
Specification. WfMC Document Number TC-1015.
August 14, 1996.

INDUSTRY TRENDS

Moving Java into
Mobile Phones
George Lawton

Telecom are now selling Java phones.
And, said Ben Wang, manager of systems development for Sprint PCS, 80
percent of the new phones the company sells will be Java enabled after the
big rollout next month. Nokia alone
plans to ship 50 million Java phones
this year and 100 million next year. In
fact, 15 handset makers either are or
soon will be selling 50 models of Java
phones.

Advantages

A

s mobile technology matures,
handheld-device vendors are
looking for ways to make
their products more functional, and Java is one
approach they are turning to. This is
particularly the case with smart cellular phones, which are using Java to
help add new capabilities.
In smart phones, Java functions as a
layer between the operating system
and the hardware, or runs parallel to
the OS within a separate chip.
In the past, the key constraint to running Java on mobile devices has been
their processing, memory, and powerconsumption limitations. However, new
mobile hardware and software developments are reducing these limitations.
Thus, industry observers expect Java
use in mobile devices, which is already
supported by many vendors, to
explode during the coming years. Nick
Jones, a fellow at Gartner Inc., a market research firm, said Java will
become a de facto standard on midrange and high-end cellular phones. He
predicted that at least 80 percent of
mobile phones will support Java by
2006, although some may also run on
other technologies, such as Microsoft‚Äôs
Pocket PC operating system.
According to Jones, mobile-device
manufacturers‚Äô desire for an aftermarket is driving interest in Java as a mechanism for easily adding software to
devices.
Java also permits applications to
work across platforms. This is important in the mobile-phone market,

which features many platforms.
However, questions about Java‚Äôs performance and a dearth of Java-based
applications for cellular phones, particularly in Europe and the US, remain
as obstacles to the technology‚Äôs widespread adoption in mobile devices.

DRIVING JAVA USE IN HANDHELDS
Work on Java-enabled handheld
devices began several years ago, but
completion of the Java 2 Platform
Mobile Edition (J2ME) and support
from device vendors and cellularphone-service providers have driven the
recent level of interest, explained Eric
Chu, Sun Microsystems‚Äô group product manager for industry marketing.

Adoption levels
Korea‚Äôs LG Telecom in became the
first service provider to deploy Java in
September 2000. Since then, users have
deployed between 18 million and 20
million Java-enabled telephones, said
Sun spokesperson Marie Domingo.
Companies such as Nextel in the US,
NTT DoCoMo in Japan, and British

According to Sun‚Äôs Chu, one of
Java‚Äôs major benefits for cellular phones
is support for packet-based networks
running TCP/IP. Using TCP/IP makes
it easier to write applications that communicate directly with the phone,
rather than relying on an intermediate
technology such as the wireless application protocol (WAP). Also, Chu said,
Java, unlike WAP, supports pictures
and colors. In addition, he explained,
the Java environment provides good
security because it includes a sandbox
that limits downloaded code‚Äôs access to
the rest of a host system.
Moreover, Java‚Äôs ability to work with
different platforms is important in the
fragmented cellular-phone market. This
capability lets a Java-enabled phone run
applications and services written for
other mobile platforms and also lets
software vendors save time and money
by writing a single, Java-based version
of an application to run on multiple
platforms. And Java-enabled phones
and servers could communicate directly
with each other, thereby enhancing
interactive applications.
Java enables smart-phone users to
download applications directly from
the Internet. Similarly, Java lets users
download Java applets that customize
their devices in various ways, such as
with special ring tones or improved
caller ID. This lets users get new features more easily. In the past, users had
to buy new phones, run new applications remotely using WAP, or download
programs first downloaded to a PC.
Meanwhile, there are many Java
developers, which makes it easier for
June 2002

17

I n d u s t r y Tr e n d s

Java 2 Platform
Micro Edition (J2ME)

Optional
packages
Optional
packages
Java 2
Platform
Enterprise
Edition
(J2EE)

Personal
basis profile

Java 2
Platform
Standard
Edition
(J2SE)

Personal
profile

Foundation profile

MIDP

CDC

CLDC

JVM

KVM
Source: Sun Microsystems

Figure 1. Sun‚Äôs three primary Java platforms are each designed primarily to run on a different type of machine. The Java 2 Platform Enterprise Edition is designed for servers; the
Java 2 Platform Standard Edition for workstations, PCs, and laptops; and the Java 2 Platform Micro Edition for PDAs, smart cellular phones, and other smaller systems. J2EE and
J2SE use the full Java virtual machine (JVM). J2ME also works with the slimmed-down K
virtual machine (KVM), the connected limited device configuration (CLDC), and the mobile
information device profile (MIDP).

vendors of Java-enabled mobile devices to find people to write their software.

MAKING JAVA WORK
IN HANDHELDS
Sun, which designed and manages
development of Java, is in the forefront
of making the technology work in handheld devices. However, other vendors
have also become active in this area.

Sun Microsystems
Sun and a group of partners created
J2ME to make Java work on smaller
devices. J2ME includes some core Java
instructions and APIs but runs more
easily on small devices because it has a
smaller footprint than the Java 2
Platform Standard Edition (J2SE) or
Enterprise Edition (J2EE), shown in
Figure 1, and has only those features
relevant for the targeted devices. For
example, J2ME‚Äôs graphics and database-access capabilities are less sophisticated.
18

Computer

J2ME generally incorporates the
connected limited device configuration
(CLDC), which is implemented on top
of operating systems and serves as an
interface between the OS and Javabased applications. The CLDC generally uses the K virtual machine (KVM),
a slimmed-down, less-functional version of the Java virtual machine (JVM)
for small devices. The J2ME mobile
information device profile (MIDP) sits
on top of the CLDC and provides a set
of APIs that define how mobile phones
will interface with applications.

Other vendors
Several vendors besides Sun are creating Java-based technologies for handheld devices. Hewlett-Packard makes
the MicroChaiVM (http://www.hp.
com/products1/embedded/products/dev
tools/microchai_vm.html), a cloned
JVM that doesn‚Äôt have Sun‚Äôs licensing
fees and usage restrictions. Several vendors, including Ericsson and HP, plan
to use MicroChaiVM-based phones.

Other approaches help Java technologies designed for larger computers
work on mobile devices.
For example, SavaJe developed the
SavaJe OS, which supports Java applications in a mobile environment by
optimizing J2SE libraries for common
mobile CPUs. Mathew Catino,
SavaJe‚Äôs cofounder and vice president
of marketing, said Java applications
typically spend 80 to 90 percent of
their time executing the libraries.
Therefore, he explained, optimizing
the libraries enables applications to run
10 to 20 times faster.
Zeosoft has developed ZeoSphere
Developer, which permits the creation
of mobile applications that support
Enterprise Java Beans, Sun‚Äôs Java-based
software-component architecture. This
could simplify the development of complex enterprise applications that communicate and run across servers (via
J2EE), PCs (via J2SE), and mobile
devices (via J2ME).

Software development tools
Application developers can use existing tools to create Java programs for
handheld devices by limiting their code
to libraries and APIs supported by
J2ME.
However, J2ME includes only a limited number of development libraries,
noted Jacob Christfort, chief technology officer of Oracle‚Äôs Mobile Division.
Also, said Gartner‚Äôs Jones, enterprises might shy away from J2ME
because of the poor user interface designed for small device screens, the
primitive threading model, and minimal native data-handling facilities. In
essence, he explained, the design
approach that lets J2ME work on small
devices sometimes makes it inappropriate for large-scale enterprise uses.
To address these concerns, several
vendors have released or will soon
release development toolkits or toolkit
extensions to help developers more
easily meet enterprise applications‚Äô
needs. The new approaches include
Sun‚Äôs Forte for Java Programming
Tools, the Oracle 9i Application Server

Server-side handheld Java
Another Java-enabling approach
would link handheld devices to Java
applications and services on servers.
AT&T Wireless, BEA Systems, IBM,
Nokia, NTT DoCoMo, Sun, and other
companies have created the Java-based
Open Mobile Architecture for linking
cellular phones and servers. The project
would augment J2EE, designed primarily for servers, so that it would support standards that mobile devices can
use with Internet-based information.
The standards include XHTML (for
displaying Web pages on mobile
devices), SyncML (for synchronizing
data between mobile devices and other
machines), WAP 2.0 (to access Internet
content and services), and the multimedia messaging service (for handheld
messaging).

ETM9 interface
Instruction
TCM
interface

Data
TCM
interface
ARM9EJ-S
core

Instruction
cache

Memory
management
unit
Write buffer

Control logic and bus interface unit

Coprocessor
interface

AHB interface
Instruction

Data

Source: ARM Ltd.

Figure 2. ARM Ltd.‚Äôs ARM926EJ-S chip includes the company‚Äôs Jazelle technology in its
ARM9EJ-S Java-enabled processor core. In addition, the chip includes separate ETM
(embedded trace macrocell), data TCM (tightly coupled memory), and AHB (advanced
high-performance bus) interfaces.

Java technology can be implemented
in software or in hardware on either a
specialized Java acceleration chip or a
core within the main processor.
Software implementations tend to
run less efficiently because systems must
translate each Java instruction into
native instructions that the CPU can
run. Separate hardware chips are more
efficient but represent additional device
components and cost. Java cores integrate some of both approaches.

Components Group, said his company
has developed techniques for speeding
up the software process, which used to
bog down when the CPU switched
from instructions it could accelerate to
instructions it couldn‚Äôt.
In addition, Intel and other software-based Java proponents say the
latest mobile processors can run Java
fast enough to compete with hardware-based approaches.
Analyst Markus Levy with MicroDesign Resources, a semiconductorindustry research firm, disagreed. He
said, ‚ÄúPeople are spending a lot of
energy fine-tuning the software-based
approaches. For some people that may
be good enough, but if you really want
the most efficient implementation you
need a hardware-based approach.‚Äù

Software approach

Java hardware

In the software approach, a device‚Äôs
CPU runs the Java code. David Rogers,
marketing manager for Intel‚Äôs PCA

Companies such as ARC Cores,
ARM Ltd., Aurora VLSI, Digital
Communications Technologies, inSili-

IMPLEMENTATION IN
HARDWARE AND SOFTWARE

Data
cache

Memory
management
unit

A R M96EJ- S

Wireless architecture toolkit, and the
Sprint PCS Wireless Toolkit.
Because of J2ME‚Äôs shortcomings,
Jones said, corporate applications will
probably be based on the larger-footprint J2SE as mobile devices get more
processing power.
Regardless, said John Montgomery,
product manager with Microsoft‚Äôs
.NET Development Group, current
Java tools are too primitive and difficult to use for most developers.

con, and Zucotto Wireless are developing hardware that runs Java, either
as Java coprocessing cores for integration into CPUs or as stand-alone Java
chips.
Both hardware-based approaches
promise to increase Java-based application performance and, by running
more efficiently, reduce power demands on battery-dependent cellular
phones.
Different companies‚Äô chips execute
different subsets of the Java instructions. For example, ARM‚Äôs Jazelle
chip, shown in Figure 2, executes
about 68.2 percent of all possible Java
instructions, while Aurora‚Äôs DeCaf
runs about 95 percent. Running a bigger set of Java instructions provides
more functionality but makes a chip
cost more and consume more power.
Joan Pendleton, Aurora‚Äôs cofounder
and chief architect, said there are two
classes of acceleration. The first, used
by most vendors, translates Java byteJune 2002

19

I n d u s t r y Tr e n d s

code into native processor instructions.
The second directly executes Java bytecode, which offers better performance
but requires a larger footprint because
of the additional circuitry necessary to
run the software in hardware.
Levy predicted that Java cores will be
more popular than stand-alone Java
processors. This approach‚Äôs primary
constraint is that developers must use
a system-on-chip approach to create
their products. Putting multiple functions on a chip is more expensive to
develop, but the elimination of additional chips reduces device costs. Standalone Java chips are less expensive to
design but lead to higher device costs.

CONCERNS AND CHALLENGES
Mobile Java is still a relatively new
technology. Many industry watchers
say the technology has kinks that still
need to be worked out.
For example, Gartner‚Äôs Jones expressed concern about vendors‚Äô differing Java implementations. He said
some developers are complaining about
having to manually optimize their Java
games for different cellular phones.
And although there are many Java
developers, there are fewer who have
experience working with J2ME and
writing code for small, resource-constrained devices.
Overall, said Microsoft‚Äôs Montgomery, ‚ÄúJ2ME is an interesting set of
engineering compromises, but I would
argue exactly the wrong set of compromises. It is too big for the smallest
devices but too small to have the features
you want on the smartest devices.‚Äù

Performance
Jones said that mobile Java can be
somewhat slow because the KVM is
not particularly fast. However, he
added, the KVM should become faster
in the future, particularly as phones
with more memory can run just-in-time
compiler technology, which enhances
performance. ‚ÄúIn five years,‚Äù he said,
‚Äú[performance] will be a nonissue.‚Äù
Another problem, said Levy, is a lack
of standards to objectively measure
20

Computer

performance across platforms. Levy
has thus launched a Java-processor
group within the Embedded Microprocessor Benchmark Consortium
(http://www.eembc.org/). The group
expects to release its first benchmark
by next month.

Not enough applications
There are currently some mobileJava applications, including games and
weather and traffic maps. However,
Jones said, there are not enough desirable mobile-Java applications yet. The
reason is not the technology, he said,
but instead the lack of an effective business model and a commercial infrastructure that would enable developers
to profit from their work.

Industry observers
say mobile Java
still has kinks that
must be worked out.
The growth of publishing intermediaries that would certify and sell
mobile-Java software may eliminate
this problem.

HANDHELDS AND
THE FUTURE OF JAVA
Jones said Java is doing well on
back-end servers because Java-based
applications can easily be redeployed
as companies buy new servers. However, he noted, client-side Java use
has faded considerably because many
enterprise-application developers turned
to Visual Basic to work within the corporate environment, which is typically
Microsoft-based. Thus, the battle for
the mobile platform is important to
Sun.
However, Sun‚Äôs Java initiatives for
cellular phones are facing stiff competition from various sources, including
Microsoft‚Äôs wireless efforts, the Symbian operating system, Linux, and Qualcomm‚Äôs binary runtime environment for
wireless (http://www.qualcomm.com/
brew/).

‚ÄúWe are still in a phase of market
confusion and have not yet gotten to a
state of market consolidation,‚Äù Jones
explained.

ccording to Jones, J2ME will
attract more application developers as it becomes a richer and
less constrained environment. A survey by Evans Data, a market research
firm, found that wireless developers
who have used Java expect to use the
technology a bit more in 2003 than
they will this year.
Java will also become even more
attractive as smart phones get more
processing power and vendors design
better APIs for color screens, higher
quality sound, intellectual-property
protection, and user-location capabilities, he added. However, he cautioned,
these extra features would give vendors
more opportunity to create their own
Java implementations, which could
fragment the application-development
environment.
Sprint PCS‚Äôs Wang said the initial
focus of mobile Java will be on games,
multimedia, and ring tones. Over time,
Levy added, Java will become a de
facto standard built into smart phones.
SavaJe‚Äôs Catino predicted that
Microsoft and Java-based technologies
are likely to coexist in phones during
the coming years. Third-party vendors
could help this process by developing
software-integration techniques that
would combine the two environments
in devices. ‚ñ†

A

George Lawton is a freelance technology writer based in Brisbane, California. Contact him at glawton@
glawton.com.

Editor: Lee Garber, Computer, 10662 Los
Vaqueros Circle, PO Box 3014, Los Alamitos,
CA 90720-1314; l.garber@computer.org

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu
Abstract
Software engineering education is a technologically challenging, rapidly evolving
discipline. Like all STEM educators, software engineering educators are bombarded with
a constant stream of new tools and techniques (MOOCs! Active learning! Inverted
classrooms!) while under national pressure to produce outstanding STEM graduates.
Software engineering educators are also pressured on the discipline side; a constant
evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the
technology, guidance on the adoption of project-centric curricula is needed. This paper
focuses on vertical integration of project experiences in undergraduate software
engineering degree programs or course sequences. The Software Enterprise, now in its
9 th year, has grown from an upper-division course sequence to a vertical integration
program feature. The Software Enterprise is presented as an implementation of a project
spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those
in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software
engineering and computer science education focus on content taxonomies and bodies of
knowledge. This is not a bad thing, but taken in isolation may lead educators to believe
content coverage is more important than applied learning experiences. There is literature
on project-based learning within computing as a means to learn soft skills and complex
technical competencies. However, project experiences tend to be disjoint [5]; there may
be a freshman project or a capstone project or a semester project assigned by an
individual instructor. Yearlong capstone projects are offered at most institutions as a
synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do
it all the time?
Project experiences, while pervasive in computing programs, are not a central
integrating feature. Sheppard et al. [6] suggests that engineering curricular design should
move away from a linear, deductive model and move instead toward a networked model:
‚ÄúThe ideal learning trajectory is a spiral, with all components revisited at increasing
levels of sophistication and interconnection‚Äù ([6] p. 191). The general engineering degree
program at Arizona State University (ASU) was designed from its inception in 2005 [7]
to be a flexible, project-centric curriculum that embodied such integration (even before
[6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division
course sequence to integrate contextualized project experiences with software engineering
fundamental concepts. The computing and engineering programs at ASU‚Äôs Polytechnic
campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

Board of Regents (ABOR) approved a new Bachelor‚Äôs degree in software engineering
(BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo
accreditation review shortly thereafter.
At the course level the Software Enterprise defines a delivery structure integrating
established learning techniques around a project-based contextualized learning
experience. At the degree program level, the Enterprise weaves project experiences
throughout the BS SE degree program, integrating program outcomes at each year of the
major. There are several publications on the manner in which the Software Enterprise is
conducted within a project course (for example, [8][9]]), and we summarize this in-course
integration pedagogy in section 2. The intent of this work-in-progress paper is to describe
extending the Enterprise as a spiral curricular design feature we refer to as the project
spine, and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a
student‚Äôs competencies from understanding to comprehension to applied knowledge by
co-locating preparation, discussion, practice, reflection, and contextualized learning
activities in time. In this model, learners prepare for a module by doing readings,
tutorials, or research before a class meeting time. The class discusses the module‚Äôs
concepts, in a lecture or seminar-style setting. The students then practice with a tool or
technique that reinforces the concepts in the next class meeting. At this point students
reflect to internalize the concepts and elicit student expectations, or hypotheses, for the
utility of the concept. Then, students apply the concept in the context of a team-oriented,
scalable project, and finally reflect again to (in)validate their earlier hypotheses. These
activities take place in a single three-week sprint, resulting is a highly iterative
methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right)
The Software Enterprise represents an innovation derived from existing scholarship in
that it assembles best practices such as preparation, reflection, practice (labs), and
project-centered learning in a rapid integration model that accelerates applied learning.
Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle
[10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on
maturing the delivery process, creating new or packaging existing learning materials to fit
the delivery model, and to explore ways to assess project-centered learning.

3. The Software Enterprise Project Spine
An innovation in the new BS in Software
Engineering at ASU has been the vertical adoption of
the Software Enterprise. Enterprise courses are now
required from the sophomore to senior years. This
innovation represents what [6] calls a professional
spine, as the Enterprise serves as an integrator of
learning outcomes for a given year in the major. We
refer to our project-centered realization as a project
spine, where foundational concepts are tied to project
work throughout the undergraduate program. There is
significant
computing
literature
on
projects
(embedded, mobile, gaming, etc.) to achieve learning
or retention outcomes. However, computing lacks a
framework for integrating concepts in a project spine.
The Enterprise is an implementation that moves
students from basic comprehension to applied
Figure 2. ASU Project Spine
knowledge to critical analysis outcomes. In the BS SE
at ASU, program outcomes are described at 4 levels: describe, apply, select, and
internalize. Students must achieve level 3 (select between alternatives) in at least 1
outcome and achieve level 2 (apply) in all others. The program outcomes for the BS SE
include Design, Computing Practice, Critical Thinking, Professionalism, Perspective,
Problem Solving, Communication, and Technical Competence. An example leveled
outcome description for Perspective is given in Table 1. The Enterprise accelerates level
3 outcomes by providing contextualized integrated experiences fostering decision-making
in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes.
Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in
business, global, economic, environmental, and societal contexts.
Level 1. Understands technological change and development have both positive & negative effects.
Level 2. Identifies and evaluates the assumptions made by others in their description of the role and
impact of engineering and computing on the world.
Level 3. Selects from different scenarios for the future and appropriately adapts them to match current
technical, social, economic and political concerns.
Level 4. Has formed a constructive model for the future of our society, and makes life and
career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical
competencies by assigning projects inclusive of the technical material covered in the
regular computing courses. So for example, junior projects (Software Enterprise III and
IV) emphasize technical complexities in Networks, Distributed Computing, and
Databases, while senior projects emphasize technical complexities in Web and Mobile
computing. The technical ‚Äúfocus area‚Äù courses are chosen more based on faculty expertise
and recruitment goals than software engineering outcomes; one can envision many
different areas represented by upper division courses here. These do help address the
concern that an accredited software engineering degree has an application area. A risk we
have not yet addressed is if the technical area impacts the software engineering process,
such as with a soon-to-be-introduced embedded systems focus area.

There are 2 additional aspects of integration to the project spine. As summarized in
section 2, the Enterprise integrates software engineering concepts throughout the project
experiences. Students in the sophomore year learn the Personal Software Process [11] as a
means to build individual understanding of time management, defect management, and
estimation skills. They then focus on Quality, including but not limited to testing. In the
junior year Enterprise students focus on Design (human-centered and system design
principles) followed by best practices in software construction, taken primarily from
eXtreme Programming. In the senior year students focus on Requirements Engineering
then Process and Project Management. The final aspect of integration is with soft-skill
outcomes such as Communication, Teamwork, and Professionalism (see Table 1).
Throughout the spine the project experiences are crafted to ensure variations on pedagogy
to address these outcomes. For example, in the freshman year students receive explicit
instruction in teamwork. In the senior year the emphasis is on formal documentation as a
means of communication. In the junior year, students work on service learning projects of
high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of
program adoption. There are examples of program design and lessons learned [5][12][13],
or reflections and recommendations on the software engineering education landscape
[14][15][16][17][18]. These are worthwhile guides but do not offer examples on
evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on
‚ÄúProgram Implementation and Assessment‚Äù which discusses a number of key factors in
program adoption, but is geared toward accreditation and not evaluation instruments. A
survey instrument is presented in [19] but is designed for comparison of a large number
of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate
programs in software engineering but more as an aggregate counting exercise in
knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software
Engineering project conducted a survey of graduate degree programs [20] and then
produced a comparison report [21] of graduate programs to the GSwE2009 reference
model, which includes data on program characteristics and in-depth profiles from 3
institutions. A recent study is Conry‚Äôs [23] survey of accredited software engineering
degree programs. Conry summarizes institutional, administrative, and curricular
(knowledge area) aspects in describing the 19 accredited programs as of October 2009.
Certainly program adoption measures from other engineering programs are also relevant,
though software engineering programs are unique due to the forces discussed in section 1.
Our next steps for the Enterprise-as-project-spine involve defining measures for
adoption impact, and determining how this concept fits with established patterns for
curricular maps in software engineering programs. We plan to use quantitative and
qualitative instruments to evaluate adoption. Quantitative data, such as program size,
institution type, faculty and student backgrounds, can be collected via available resources
(departmental archives or online) and direct surveys. Qualitative data can be collected
through survey instruments and interviews of all stakeholders (faculty participants,
administrators, and advisors). Different instruments may be used at different times to
evaluate ‚Äúin-stream‚Äù attitudes versus post-adoption reflections. Defining and validating
these instruments is a significant area of work going forward.
The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in
software engineering. Taxonomies are useful and the sign of an emerging discipline. We

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas,
and plan to elaborate on these mappings. Specifically, we intend to produce CS2013
course exemplars. Further, the SE2004 report includes a section on program curricular
patterns, and we will propose new patterns based on the project spine concept, which we
hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the
New Century. The National Academies Press, Washington D.C., 2005.
[2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of
Knowledge (SWEBOK). Los Alamitos, CA, 2004.
[3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society
Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at
http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013.
[4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society.
Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software
Engineering. Joint Task Force on Computing Curricula, 2004.
[5] Shepard, T. ‚ÄúAn Efficient Set of Software Degree Programs for One Domain.‚Äù In Proceedings of the
International Conference on Software Engineering (ICSE) 2001.
[6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the
Future of the Field, Jossey-Bass, San Francisco, 2008.
[7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. ‚ÄúA Flexible
Curriculum for a Multi-disciplinary Undergraduate Engineering Degree.‚Äù Proceedings of the Frontiers in
Education Conference 2005.
[8] Gary, K. ‚ÄúThe Software Enterprise: Practicing Best Practices in Software Engineering Education‚Äù, The
International Journal of Engineering Education Special Issue on Trends in Software Engineering Education,
Volume 24, Number 4, July 2008, pp. 705-716.
[9] Gary, K., ‚ÄúThe Software Enterprise: Preparing Industry-ready Software Engineers‚Äù Software Engineering:
Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group
Publishing. October 2008.
[10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984.
[11] Humphrey, W.S. Introduction to the Personal Software Process, Addison-Wesley, Boston, 1997.
[12] Lutz, M. and Naveda, J.F. ‚ÄúThe Road Less Traveled: A Baccalaureate Degree in Software Engineering.‚Äù
Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997.
[13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor‚Äôs Program.
IEEE Software November/December 2006.
[14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. ‚ÄúGuidance for the
development of software engineering education programs.‚Äù The Journal of Systems and Software,
49(1999):163-169. 1999.
[15] Ghezzi, C. and Mandrioli. ‚ÄúThe Challenges of Software Engineering Education.‚Äù In Proceedings of the
International Conference on Software Engineering (ICSE) 2006.
[16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. ‚ÄúImproving software practice through
education: Challenges and future trends.‚Äù Proceedings of the Future of Software Engineering Conference, 2007.
[17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the
Future of Software Engineering, Limerick Ireland, 2000.
[18] Mead, N. (2009). Software Engineering Education: How far We‚Äôve Come and How far We Have to Go.
Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009.
[19] Modesitt, K., Bagert, D.J., and Werth, L. ‚ÄúAcademic Software Engineering: What is it and What Could it be?
Results of the First International Survey for SE Programs.‚Äù Proceedings of the International Conference on
Software Engineering (ICSE) 2001.
[20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering
Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008.
[21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master‚Äôs Programs in
Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013.
[22] Bagert, D.J. & Chenoweth, S.V. ‚ÄúFuture Growth of Software Engineering Baccalaureate Programs in the United
States‚Äù, Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005.
[23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of
the American Society for Engineering Education, Louisville, KY, 2010.
[24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). ‚ÄúRevision of the SE2004
Curriculum Model.‚Äù Panel at the ACM Conference of the Special Interest Group on Computer Science
Education (SIGCSE), Denver, CO, 2013.

SOCRADES: A Web Service based Shop Floor
Integration Infrastructure
Luciana Moreira SaÃÅ de Souza, Patrik Spiess, Dominique Guinard,
Moritz KoÃàhler, Stamatis Karnouskos, and Domnic Savio
SAP Research
Vincenz-Priessnitz-Strasse 1, D-76131, Karlsruhe, Germany
Kreuzplatz 20, CH-8008, Zurich, Switzerland
{luciana.moreira.sa.de.souza, patrik.spiess, dominique.guinard,
mo.koehler, stamatis.karnouskos, domnic.savio} @sap.com

Abstract. On the one hand, enterprises manufacturing any kinds of
goods require agile production technology to be able to fully accommodate their customers‚Äô demand for flexibility. On the other hand, Smart
Objects, such as networked intelligent machines or tagged raw materials,
exhibit ever increasing capabilities, up to the point where they offer their
smart behaviour as web services. The two trends towards higher flexibility and more capable objects will lead to a service-oriented infrastructure
where complex processes will span over all types of systems ‚Äî from the
backend enterprise system down to the Smart Objects. To fully support
this, we present SOCRADES, an integration architecture that can serve
the requirements of future manufacturing. SOCRADES provides generic
components upon which sophisticated production processes can be modelled. In this paper we in particular give a list of requirements, the design,
and the reference implementation of that integration architecture.

1

Introduction

In the manufacturing domain, constant improvements and innovation in the
business processes are key factors in order to keep enterprises competitive in the
market. Manufacturing businesses are standing on the brink of a new era, one
that will considerably transform the way business processes are handled.
With the introduction of ubiquitous computing on the shop floor1 , an entirely new dynamic network of networked devices can be created - an Internet
of Things (IoT) for manufacturing. The Internet of Things is a concept which
first appeared shortly after 2000. Until now, several approaches to describe the
IoT have been undertaken of which most have focused on RFID technologies
and their application ([5, 13]).
Only recently, new technologies such as Smart Embedded Devices and Sensor
Networks have entered the scene and can be considered as part of the IoT [11].
1

In manufacturing, the shop floor is the location where machines are located and
products produced.

Smart Embedded Devices are embedded electronic systems which can sense their
internal state and are able to communicate it through data networks. In contrast
to this, Sensor Networks not only can measure internal states of their nodes,
but also external states of the environment. We group these three technologies RFID, Smart Embedded Devices, and Sensor Networks - under the notion Smart
Objects.
Smart Objects are the nerve cells, which are interconnected through the
Internet and thus build the IoT. RFID has already been proved to open fundamentally new ways of executing business processes, and the technology has
already been adopted by several key players in the industry. Therefore the focus
of this paper is on Smart Embedded Devices and Sensor Networks and their
effects on automatic business process execution.
Although client-server architectures still play an important role in the field
of business software systems, the Service Oriented Architecture (SOA) is on the
move and it is foreseeable that this architectural paradigm will be predominant
in the future. The integration of devices into the business IT-landscape through
SOA is a promising approach to connect physical objects and to make them available to IT-systems. This can be achieved by running instances of web services
on these devices, which moves the integration of back end applications, such as
Enterprise Resource Planning (ERP) systems, with the devices one step forward,
enabling them to interact and create an Internet of Services that collaborates
and empowers the future service-based factory.
Enabling efficient collaboration between device-level SOA and services and
applications that constitute the enterprise back-end on the other hand, is a
challenging task. The introduction of web service concepts at a level as low as
the production device or facility automation makes this integration significantly
less complex. But there are still differences between device-level SOA and the one
that is used in the back end. To name but a few of them, device-level services are
of higher granularity, exhibit a lower reliability (especially if they are connected
wirelessly) and higher dynamicity and are more focused on technical issues than
on business aspects.
These differences can be overcome by introducing a middleware between the
back end applications and the services that are offered by devices, service mediators, and gateways. This middleware adds the required reliability, provides
means to deal with services appearing and disappearing, and allows intermediate service composition to raise the technical interfaces of low-level services to
business-relevant ones.
In this paper we present the SOCRADES middleware for business integration; an architecture focused on coupling web service enabled devices with enterprise applications such as ERP Systems. Our approach combines existing
technologies and proposes new concepts for the management of services running
on the devices.
This paper is organized as follows: in section 2 we discuss the current state of
the art in coupling technologies for shop floor and enterprise applications. Section
3 presents the requirements for our approach, followed by section 4 where we

discuss our approach. We propose a prototype for evaluating our approach in
section 5 and perform an analysis in section 6. Section 7 concludes this paper.

2

Related Work

Manufacturing companies need agile production systems that can support reconfigurability and flexibility to economically manufacture products. These systems must be able to inform resource planning systems like SAP ERP in advance,
about the upcoming breakdown of a whole production processes or parts of them,
so that adaptation in the workflow can be elaborated.
Currently Manufacturing Execution Systems (MES) are bridging the gap
between the shop floor and ERP systems that run in the back end. The International Systems and Automation Society - 95 (ISA-95) derivative from the
Instrumentation Systems and Automation Society define the standards for this
interface [1]. Although MES systems exist as gateways between the enterprise
world and the shop floor, they have to be tailored to the individual group of
devices and protocols that exist on this shop floor.
By integrating web services on the shop floor, devices have the possibility
of interacting seamlessly with the back end system ([9, 8]). Currently products
like SIMATIC WinCC Smart Access [2] from Siemens Automation use SOAP
for accessing tag based data from devices like display panels to PC‚Äôs. However,
they neither provide mechanisms to discover other web-service enabled devices,
nor mechanisms for maintaining a catalogue of discovered devices.
The domain of Holonic Manufacturing Execution Systems (HMS) [6] is also
relevant to our work. HMS are used in the context of collaborative computing,
and use web service concepts to integrate different sources and destinations inside a production environment. They do, however, not offer support to process
orchestration or service composition.
Amongst others, European Commission funded projects like SIRENA [4]
showed the feasibility and benefit of embedding web services in production devices. However, since these were only initial efforts for proving the concept, not
much attention has been given to issues such as device supervision, device life cycle management, or catalogues for maintaining the status of discovered devices,
etc. The consortium of the SOCRADES project has integrated partners, code
and concepts from SIRENA, and aims to further design and implement a more
sophisticated infrastructure of web-service enabled devices. SODA (www.sodaitea.org) aims at creating a comprehensive, scalable, easy to deploy ecosystem
built on top of the foundations laid by the SIRENA project.
The SODA ecosystem will comprise a comprehensive tool suite and will target industry standard platforms supported by wired and wireless communications. Although EU projects like SIRENA showed the feasibility and benefit of
embedding web services in devices used for production, they do not offer an
infrastructure or a framework for device supervision or device life cycle. They
neither do provide a catalogue for maintaining the status of discovered devices
[4]. Changes due to the current development are moving towards a more promis-

ing approach of integrating shop floor devices and ERP systems more strongly
[14].
Some authors are criticizing the use of RPC-style interaction in ubiquitous
computing [12] (we consider the smart manufacturing devices a special case
of that). We believe this does not concern our approach, since web services also
allow for interaction with asynchronous, one-way messages and publish-subscribe
communication.
SAP xApp Manufacturing Integration and Intelligence (SAP xMII) is a manufacturing intelligence portal that uses a web server to extract data from multiple sources, aggregate it at the server, transform it into business context and
personalize the delivered results to the users [7]. The user community can include existing personal computers running internet browsers, wireless PDAs or
other UIs. Using database connectivity, any legacy device can expose itself to
the enterprise systems using this technology.
The drawback of this product is that every device has to communicate to the
system using a driver that is tailored to the database connectivity. In this way,
SAP xMII limits itself to devices or gateway solutions that support database
connectivity.
In [10], we proposed a service-oriented architecture to bridge between shop
floor devices and enterprise applications. In this paper however, building on both
our previous work and SAP xMII, we show how the already available functionality of xMII can be leveraged and extended to provide an even richer integration
platform. The added functionality comprises integration of web service enabled
devices, making them accessible through xMII, and supporting the software life
cycle of embedded services. This enables real-world devices to seamlessly participate in business processes that span over several systems from the back end
through the middleware right down to the Smart Objects.

3

System Requirements

As embedded technology advances, more functionality that currently is hosted
on powerful back end systems and intermediate supervisory devices can now be
pushed down to the shop floor level. Although this functionality can be transferred to devices that have only a fraction of the capabilities of more complex
systems, their distributed orchestration in conjunction with the fact that they
execute very task-specific processing, allows us to realise approaches that can
outperform centralised systems in means of functionality. By embedding web
services on devices, these can become part of a modern Enterprise SOA communication infrastructure.
The first step to come closer to realize this vision, is to create a list of requirements. We have come up with this list through interviews with project
partners and customers from the application domain, as well as a series of technical workshops with partners form the solution domain. As usually done in
software engineering, we separated the list into functional and non-functional
requirements.

Functional Requirements
‚Äì WS based direct access to devices: Back end services must be able to
discover and directly communicate with devices, and consume the services
they offer. This implies the capability of event notifications from the device
side, to which other services can subscribe to.
‚Äì WS based direct access to back end services: Most efforts in the
research domain today focus on how to open the shop floor functionality to
the back end systems. The next challenge is to open back end systems to the
shop floor. E.g. devices must be able to subscribe to events and use enterprise
services. Having achieved that, business logic executing locally on shop floor
devices can now take decisions not only based on its local information, but
also on information from back end systems.
‚Äì Service Discovery: Having the services on devices will not be of much
use if they can not be dynamically discovered by other entities. Automatic
service discovery will allow us to access them in a dynamic way without
having explicit task knowledge and the need of a priori binding. The last
would also prevent the system from scaling and we could not create abstract
business process models.
‚Äì Brokered access to events: Events are a fundamental pillar of a service
based infrastructure. Therefore access to these has to be eased. As many
devices are expected to be mobile, and their online status often change (including the services they host), buffered service invocation should be in-place
to guarantee that any started process will continue when the device becomes
available again. Also, since not all applications expose web services, a pull
point should be realised that will offer access to infrastructure events by
polling.
‚Äì Service life cycle management: In future factories, various services are
expected to be installed, updated, deleted, started, and stopped. Therefore,
we need an open ways of managing their life cycle. Therefore the requirement
is to provide basic support in the infrastructure itself that can offer an open
way of handling these issues.
‚Äì Legacy device integration: Devices of older generations should be also
part of the new infrastructure. Although their role will be mostly providing
(and not consuming) information, we have to make sure that this information can be acquired and transformed to fit in the new WS-enabled factory.
Therefore the requirement is to implement gateways and service mediators
to allow integration of the non-ws enabled devices.
‚Äì Middleware historian: In an information-rich future factory, logging of
data, events, and the history of devices is needed. The middleware historian
is needed which offers information to middleware services, especially when
an analysis of up-to-now behavior of devices and services is needed.
‚Äì Middleware device management: Web service enabled devices, will contain both, static and dynamic data. This data can now be better and more
reliably integrated to back end systems offering a more accurate view of the
shop floor state. Furthermore by checking device data and enterprise inventory, incompatibilities can be discovered and tackled. Therefore we require

approaches that will effectively enable the full integration of device data and
their exploitation above the device-layer.
Non-Functional Requirements
‚Äì Security support: Shop floors are more or less closed environments with
limited and controlled communication among their components. However,
because of open (and partially wireless) communication networks, this is fundamentally changing. Issues like confidentiality, integrity, availability must
be tackled. In a web service mash-up - as the future factory is expected to
be -, devices must be able to a) authenticate themselves to external services
and b) authenticate/control access to services they offer.
‚Äì Semantics support: This requirement facilitates the basic blocks primarily for service composition but also for meaningful data understanding and
integration. Support for the usage of ontologies and semantic-web concepts
will also enhance collaboration as a formal description of concepts, terms,
and relationships within a manufacturing knowledge domain.
‚Äì Service composition: In a SOA infrastructure, service composition will
allow us to build more sophisticated services on top of generic ones, therefore allowing thin add-ons for enhanced functionality. This implies a mixed
environment where one could compose services a) at device level b) at back
end level and c) in a bidirectional cross-level way.
In the above list we have described both, functional and non-functional requirements. In our architecture these requirements will be realized through components, each one offering a unique functionality.

4
4.1

Architecture
Overview

In this chapter, we present a concrete integration architecture focusing on leveraging the benefits of existing technologies and taking them to a next level of integration through the use of DPWS and the SOCRADES middleware. The architecture proposed in Figure 1 is composed of four main layers: Device Layer, SOCRADES middleware (consisting of an application and a device services part),
xMII, and Enterprise Applications.
The Device Layer comprises the devices in the shop floor. These devices when
enabled with DPWS connect to the SOCRADES middleware for more advanced
features. Nevertheless, since they support web services, they provide the means
for a direct connection to Enterprise Applications. For the intermediate part
of the SOCRADES architecture, bridging between enterprise and device layer,
we identified an SAP product that partly covered our requirements: SAP xApp
Manufacturing Integration and Intelligence (SAP xMII). The features already
available in xMII are:

ENTERPRISE APPLICATIONS
HTML-GUI /
Applets

SAP
Protocols

Web Services

SOCRADES MIDDLEWARE APP SERVICES

xMII
Visualization Services

SAP Connectivity

Invoker

Applets
Display Controls
Displays

Asynchronous
Buffer

SAP Transaction
Access

GUI Widgets

Eventing

(Event)
Pull Point

Notification
Broker

Web Services
Cross-layer

Business Logic Services

Service
Catalogue

Business Process Monitoring
Alert

Shop floor
standard

Hardware
Vendor
Implementation

Data Services

DPWS
Back-end
Services

SOCRADES Connector
Web Services

SOCRADES MIDDLEWARE DEVICE SERVICES
Device Manager
and Monitor

Middleware
Historian

Service
Discovery

Service
Lifecycle
Management

Legacy Connector

Composed
Services
Runtime

Service Services
Mapper Repository

Service Access Control
Proprietary
Protocol

OPC UA
over DPWS

OPC UA
over DPWS

Gateway

DEVICE LAYER

Fig. 1. SOCRADES Integrated Architecture

‚Äì Connectivity to non web service enabled devices via various shop floor communication standards
‚Äì Graphical modelling and execution of business rules
‚Äì Visualization Services
‚Äì Connectivity to older SAP software through SAP-specific protocols
We decided not to re-implement that functionality but use it as a basis and
extend it by what we call the SOCRADES middleware. The SOCRADES middleware and xMII perform together a full integration of devices with ERP systems, adding functionalities such as graphical visualization of device data and
life cycle management of services running on the devices. In this setting, xMII
provides the handling of business logic, process monitoring and visualization of
the current status of the devices.
Finally, the connection with Enterprise Applications is realized in three ways.
SAP xMII can be used to generate rich web content that can be integrated into
the GUI of an enterprise system in mash-up style. Alternatively, it can be used
to establish the connection to older SAP systems using SAP-specific protocols.
Current, web service based enterprise software can access devices either via
web services of the SOCRADES middleware, benefiting from the additional functionality, or they can directly bind against the web services of DPWS-enabled
devices. The data delivered to Enterprise Applications is currently provided by
xMII. Nevertheless with the introduction of the SOCRADES middleware and

the use of DPWS, this data can be also delivered directly by the regarding devices, leaving to xMII only the task of delivering processed data that requires a
global view of the shop floor and of the business process.
4.2

Features and Components of the SOCRADES Middleware

The SOCRADES middleware is the bridging technology that enables the use of
features of existing software systems with DPWS enabled devices. Together with
SAP xMII, this middleware connects the shop floor to the top floor, providing additional functionality not available in either one of these layers. Although direct
access from an ERP system to devices is possible, the SOCRADES middleware
simplifies the management of the shop floor devices. In the following, we list
this additional functionality and show how the components of the architecture
implement them.
Brokered Access to Devices. Brokered access means to have an intermediate
party in the communication between web service clients and servers that adds
functionality. Example are asynchronous invocations, a pull point for handling
events, and a publish-subscribe mechanism for events. Asynchronous invocations
are useful when dealing with devices that are occasionally connected so that invocations have to be buffered until the device re-appears; they are implemented
by the Invoker component. Pull points enable applications to access events without having to expose a web service interface to receive them. The application can
instruct the pull point to buffer events and can obtain them by a web service call
whenever it is ready. Alternatively, to be notified immediately, the application
can expose a web service endpoint and register it at the notification broker for
any type of event.
Service Discovery: The service discovery components carries out the actual
service discovery on the shop floor level. This component is distributed and
replicated at each physical site because the DPWS discovery mechanism WSDiscovery relies on UDP multicast, a feature that may not be enabled globally
across all subsidiaries in a corporate network. All discovered devices from all
physically distributed sites and all the services that each device runs are then
in a central repository called Device Manager and Monitor, which acts as the
single access point where ERP systems can find all devices even when they have
no direct access to the shop floor network.
Device Supervision: Device Management and Monitor and DPWS Historian
provide the necessary static and dynamic information about each DPWS-enabled
physical device available in the system. The device manager holds any static
device data of all on-line and off-line devices while the device monitor contains
information about the current state of each device. The middleware historian can
be configured to log any event occurring at middleware level for later diagnosis

and analysis. Many low-level production systems feature historians, but they
are concerned with logging low-level data that might be irrelevant for businesslevel analysis. Only a middleware historian can capture high-level events that
are constructed within this architectural layer.

Service Life Cycle Management: Some hardware platforms allow exchanging the embedded software running on them via the network. In a service-enabled
shop floor this means that one can update services running on devices. The
management of these installed services is handled through the use of the Service
Mapper and Services Repository. These components together make a selection
of the software that should run in each device and perform the deployment.

Cross-Layer Service Catalogue: The cross-layer service catalogue comprises
two components. One is the Composed Services Runtime that executes service
composition descriptions, therefore realizing service composition at the middleware layer. The second component is the DPWS device for back end services
that allows DPWS devices to discover and use a relevant set of services of the
ERP system.
The Composed Services Runtime is used to enrich the services offered by the
shop floor devices with business context, such as associating an ID read from
an RFID tag with the corresponding order. A compound service can deliver this
data by both invoking a service on the RFID reader, and from a warehouse
application. A Composed Services Runtime, which is an execution engine for
such service composition descriptions, e.g., BPEL [3], is placed in the middleware
because only from there, all DPWS services on the shop floor as well as all back
end services can be reached.
Another requirement is that shop floor devices must be able to access enterprise application services, which can be achieved by making a relevant subset
available through the DPWS discovery. This way, devices that run DPWS clients
can invoke back end services in exactly the same way they invoke services on
their peer devices. Providing only the relevant back end services allows for some
access control and reduces overhead during discovery of devices. Co-locating
both sub-components in the same component has the advantage that also the
composed services that the Composed Services Runtime provides, can be made
available to the devices through the virtual DPWS device for back end services.

Security support: The (optional) security features supported by the middleware are role-based access control of devices communication to middleware and
back end services and vice versa. Event filtering based on roles is also possible. Both the devices as well as back end and middleware services have to be
authorized when they want to communicate. Access control is enforced by the
respective component. Additionally, message integrity and confidentiality is provided by the WS-Security standard.

To demonstrate the feasibility of our approach and to make some first evaluations, we implemented a simple manufacturing scenario. We used a first implementation of our architecture to connect two DPWS-enabled real-world devices
with an enterprise application.

5

Reference Implementation

In order to prove the feasibility of our concept, we have started realising a reference implementation. From a functional point of view, it demonstrates two
of the most important incentives for the use of standardized device level web
services in manufacturing: flexibility and integration with enterprise software.
Indeed, the scenario shows DPWS-enabled devices can be combined easily to
create higher-level services and behaviours that can then be integrated into topfloor applications.
The business benefits from adopting such an architecture are numerous:
‚Äì
‚Äì
‚Äì
‚Äì

5.1

lower cost of information delivery
increased flexibility and thus total cost of ownership (TCO) of machines.
increased visibility of the entire manufacturing process to the shop floor.
ability to model at the enterprise layer processes with only abstract view of
the underlying layer, therefore easing the creation of new applications and
services from non-domain experts.
Scenario

To support this idea we consider a simple setting with two DPWS devices:
‚Äì A robotic arm that can be operated through web service calls. Additionally
it offers status information to subscribers through the SOCRADES eventing
system.
‚Äì A wireless sensor node providing various information about the current environment, delivered as events. Furthermore, the sensor nodes provide actuators that are accessible through standard service calls.
The manufacturing process is created on the shop floor using a simple service composition scheme: from the atomic services offered by the arm (such
as start/stop, etc.) a simple manufacturing process p is created. The robot manipulates heat-sensitive chemicals. As a consequence it is identified that the
manufacturing process cannot continue if the temperature rises above 45 .
The robot may not have a temperature sensor (or this is malfunctioning),
but as mentioned before the manufacturing plant is equipped with a network
of wireless sensor nodes providing information about the environment. Thus,
in order to enforce the business rule, the chief operator uses a visual composition language to combine p with the temperature information published by the
service-enabled sensor node: t.
In pseudo code, such a rule looks like:

¬â

if (t > 45) then p.stopTransportProcess();
Furthermore, the operator instantiates a simple gauge fed with the temperature data (provided by t). For this purpose he uses a manufacturing intelligence
software and displays the gauge on a screen situated close the robot.
Finally, the sales manager can also leverage the service oriented architecture
of this factory. Indeed, the output of the business rule is connected to an ERP
system which provides up-to-date information about the execution of the current
orders. Whenever the process is stopped because the rule was triggered, an event
is sent to the ERP system through its web service interface. The ERP system
then updates the orders accordingly and informs the clients of a possible delay
in the delivery.
5.2

Components

This section describes the architecture of our prototype from an abstract point of
view. Its aim is to understand the functionality whose concrete implementation
will be described within the next section.
Functional Components The system comprises four main components as
shown on Figure 2 that we shall briefly describe:
‚Äì Smart Devices: Manufacturing devices, sensors and Smart Things (i.e.
Smart Objects) are the actors forming an Internet of Services in the factory
as well as outside of the factory. They all offer web service interfaces, either directly or through the use of gateways or service mediators. Through
these interfaces they offer functional services (e.g. start/stop, swap to manual/automatic mode) or status information (e.g. power consumption, mode
of operation, usage statistics, etc.).
‚Äì Composed Service: The component aggregates the services offered by
smart objects. Indeed, it is in charge of exposing coarse-grained services
to the upper layers. In the case of the robotic arm for instance, it will consume the open(), close() and move(...), methods and use them to offer
a doTransportProcess (...) service.
‚Äì Business Logic Services and Visualisation Services: In our prototype,
the business logic services are supported by a service composition engine and
visualized using a visualization toolkit. The former component is used to
model business rules or higher-level processes, known as business logic services in our architecture. As an example the operator can use it to create the
business rules exposed above. The latter component is used to build a plantfloor visualisation of the devices‚Äô status and the overall process execution.
As an example the operator can instantiate and use a set of widgets such as
gauges and graphs to monitor the status of the machines. The production
manager can also use it to obtain real-time graphs of the process execution
and status.

‚Äì Enterprise Applications: This is the place of high-end business software
such as ERPs or PLMs. The idea at this level is to visualize processes rather
than the machines executing the processes. This layer is connected to the
plant-floor devices through the other layers. As such it can report machines
failures and plant-floor information on the process visualization and workflow. Furthermore, business actions (e.g. inform customers about a possible
delay) can be executed based on this information.

Fig. 2. The DPWS service bus.

Cross-Component Communication In a mash-up, the architecture is not
layered but rather flat, enabling any functional component to talk to any other.
Such architectures need a common denominator in order for the components to
be able to invoke services on one another. In our case the common denominator
is the enhanced DPWS we developed. Each component is DPWS-enabled and
thus, consumes DPWS services and exposes a DPWS interface to invoke the
operations it offers. The service invocations can be done either synchronously or
asynchronously via the web service eventing system. For instance the temperature is gathered via a subscription to the temperature service (asynchronous)
whereas the transport process is stopped by invoking an operation on the process
middleware. Figure 2 depicts the architecture by representing the components
connected to a common (DPWS) ESB (Enterprise Service Bus).
5.3

Implementation

The system described in this paper is a reference implementation of concepts
described in the architecture rather than a stand-alone concept. Therefore it
uses and extends several software and hardware components rather than writing
them from scratch. In this section we will briefly describe what these components
are and how they interact together, taking a bottom up approach.

Functional Components
‚Äì Smart Devices: The wireless sensor network providing temperature information is implemented using the Sun Microsystems‚Äô SunSPOT sensor
nodes. Since the nodes are not web services enabled, we had to implement a gateway (as described in our architecture), that would capture the
temperature readings and provide it via DPWS as services one can subscribe to. The gateway component hides the communication protocol between the SunSPOTs and exposes their functionalities as device level web
services (DPWS). More concretely the SunSPOT offer services for sensing
the environment (e.g. getTemperature()) or providing output directly on
the nodes (e.g. turnLightOn(Color)). The robotic arm was implemented
as a clamp offering DPWS services for both monitoring and control. The
clamp makes these operations available as DPWS SOAP calls on a PLC
(Programmable Logic Controller) over gateway. For monitoring services (e.g.
getPowerConsumption()) the calls are issued directly on the gateway standing for the clamp. For control services the idea is slightly different.
‚Äì Composed Service: Typical operations at the clamp level are openClamp()
and closeClamp(). In order to consistently use these operations on the topfloor we need to add some business semantics already on the shop floor. This
is the role of composed services which aggregate an number of coarse-grained
operations (e.g. openClamp()) and turn them into higher level services. This
way the start(), openClamp(), closeClamp(), move(x), stop() operations
are combined to offer the startTransportProcess() service.
‚Äì Business Logic Services and Vizualisation Services: Services offered
by both the sensors and the clamp are combined to create a business rule.
The creation of this business logic service is supported by xMII, SAP‚Äôs Manufacturing Integration and Intelligence software. As mentioned before, the
aim of this software is firstly to offer a mean for gathering monitoring data
from different device aggregators on the shop floor such as MESs (Manufacturing Execution Systems). This functionality is depicted on Figure 3.
Since the SOCRADES infrastructure proposes to DPWS-enable all the devices on the plant-floor, we can enhance the model by directly connecting
the devices to xMII. Additionally, xMII offers a business intelligence tool.
Using its data visualization services we create a visualization of processrelated and monitoring data. Finally, we use the visual composition tool
offered by xMII to create the rule. Whenever this rule is triggered the
stopTransportProcess()operation is invoked on the middleware to stop
the clamp.
‚Äì Enterprise Applications: Whenever the business rule is triggered, xMII
invokes the updateOrderStatus()on the ERP. As mentioned before this
latter component displays the failure and its consequences (i.e. a delay in
the production) in the orders‚Äô list. Additionally, if the alert lasts for a while,
it informs the customer by email providing him with information about a
probable delay.

Fig. 3. xMII indirect device connectivity.

Fig. 4. Direct connectivity to the DPWS devices.

Cross-Component Communication Figure 5 presents the communication
amongst the components whenever the business rule is triggered. At first the
SunSPOT dispatches the temperature change by placing a SOAP message on
the DPWS service bus. The xMII is subscribed to this event and thus, receives
the message and feeds it to its rules engine. Since the reported temperature
is above the threshold xMII fires the rule. As a consequence it invokes the
stopTransportProcess()operation on the Process Service middleware. This
component contacts the clamp and stops it. Furthermore, xMII triggers the
updateOrderStatus()operation on the ERP. This latter system update the
status of the concerned order accordingly and decides whether to contact the
customer to inform him by email about the delay.

Fig. 5. Interactions when the business rule is triggered.

6

System Analysis

In this section we will discuss the properties of our architecture and give decision
makers a framework at hand through which they can assess the concrete value of
our system for their organisation. Since the work we are presenting in this paper
is part of ongoing research, we think it is helpful to have such a framework, in
particular to assess future work.
In the field of Systems Management several standards exist [ref. Standards,
ITIL, etc.] which aim to support a structured dealing with IT systems. One
framework in particular helpful for central corporate functions such as produc-

tion is the ISO model FCAPS (Fault, Configuration, Administration, Performance, Security). Although being a framework for network management, it is
relevant for our architecture because it is enabling low level networked interaction between Smart Objects. Here we will give a first attempt to evaluate the
architecture.
‚Äì Fault Management: Since our system will be part of the manufacturing
IT-landscape we need to manage both, faults of particular parts of the manufacturing process and faults in our system. Due to the tight integration
these types of faults inherently become the same. In particular the SOA
based approach of device integration enables the user to identify faults in
his production process, at a level never seen before. It also gives the possibility to build redundancy at system critical stages which ensures fast recovery
from local failures. Finally the flexibility given by our SOA approach lets the
user decide to what extend he wants to introduce capabilities of quick fault
recovery, depending on his individual needs.
‚Äì Configuration Management: Mainly the two components Service Lifecycle Management and Cross-Layer Service Catalogue support dynamic configuration management. However, at the current point of view we see code
updated to Smart Devices as a major challenge which until today has not
been resolved sufficiently. Configuration also includes the composition of
services into higher-level services. In a future version, our Service Discovery
module will use semantic annotation of services to find appropriate service
instances for online service composition. Using ontologies to specify the behaviour and parameters of web services in their interface descriptions and
metadata allows flexible service composition. Especially in the very well defined domain of manufacturing we can make use of existing ontologies that
describe production processes.
‚Äì Administrative Management: The Device Manager provides the necessary static and dynamic information about each Smart Device. Through the
strict use of web-service interfaces, it will be possible to easily integrate devices into management dash-boards. Through this technically we allow easy
and user friendly access to Smart Devices. However, taking the possibly very
large number of devices into account, we belief that our middle-ware has deficiencies in offering this user friendly administration. Although this problem
is subject to other fields of research such as sensor networks, (e.g, macro
programming), we will dedicate our research efforts to the problem.
‚Äì Performance Management: Already now we can say that local components of our system will scale well in regards to total amount of Smart
Objects and their level of interaction. This can be justified since all interaction occurs locally and only a limited amount of Smart Objects is needed
to fulfil a particular task. However, it is still an open question, if our system
will scale well on a global scale and to what extend it will need to be modularized. For example we will need to investigate whether central components
such as device and service registries should operate on a plant level or on

a corporate level, which could mean that these parts would have to handle
several millions or even billions of devices at the same time.
‚Äì Security Management: As mentioned in the security support section of
the architecture, our system can make use of well established security features which already are part of web-service technologies and their protocols
such as DPWS. It is most likely that we will have to take into account industry specific security requirements, and it will be interesting to see, if we
can deliver a security specification which satisfies all manufacturing setups.

7

Conclusions

In this paper we have presented SOCRADES, a Web Service based Shop Floor
Integration Infrastructure. With SOCRADES we are offering an architecture including a middleware which support connecting Smart Devices, i.e. intelligent
production machines from manufacturing shop floors, to high-level back-end systems such as an ERP system. Our integration strategy is to use web services
as the main connector technology. This approach is motivated by the emerging importance of Enterprise Service Oriented Architectures, which are enabled
through web services.
Our work has three main contributions: First, we elaborated and structured
a set of requirements for the integration problem. Second, we are proposing
a concrete architecture containing of components which realized the required
functionality of the system. Our third contribution is a reference implementation
of the SOCRADES architecture. In this implementation we have demonstrated
the full integration of two Smart Devices into and enterprise system. We showed
that it is possible to connect Smart Devices to an ERP system, and describe
how this is done.
Our next steps include integrating a prototype in a bigger setup and testing
it with live production systems.

8

Acknowledgments

The authors would like to thank the European Commission and the partners
of the European IST FP6 project ‚ÄùService-Oriented Cross-layer infRAstructure
for Distributed smart Embedded devices‚Äù (SOCRADES - www.socrades.eu), for
their support.

References
1. Instrumentation Systems and Automation Society. http://www.isa.org/.
2. SIMATIC WinCC flexible. http://www.siemens.com/simatic-wincc-flexible/.
3. Web Services Business Process Execution Language Version 2.0 (OASIS Standard),
April 2007. http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.html.

4. H. Bohn, A. Bobek, and F. Golatowski. SIRENA - Service Infrastructure for Realtime Embedded Networked Devices: A service oriented framework for different
domains. In International Conference on Systems and International Conference on
Mobile Communications and Learning Technologies (ICNICONSMCL‚Äô06), page 43,
Washington, DC, USA, 2006. IEEE Computer Society.
5. E. Fleisch and F. Mattern, editors. Das Internet der Dinge: Ubiquitous Computing und RFID in der Praxis:Visionen, Technologien, Anwendungen, Handlungsanleitungen. Springer, 2005.
6. L. Gaxiola, M. de J. Ramƒ±ÃÅrez, G. Jimenez, and A. Molina. Proposal of Holonic
Manufacturing Execution Systems Based on Web Service Technologies for Mexican
SMEs. In HoloMAS, pages 156‚Äì166, 2003.
7. G.Gorbach. Pursuing manufacturing excellence through Real-Time performance
management and continuous improvement. ARC Whitepaper, April 2006.
8. F. Jammes, A. Mensch, and H. Smit. Service-Oriented Device Communications
using the Devices Profile for Web Services. In MPAC ‚Äô05: Proceedings of the 3rd
international workshop on Middleware for pervasive and ad-hoc computing, pages
1‚Äì8, New York, NY, USA, 2005. ACM Press.
9. F. Jammes and H. Smit. Service-oriented paradigms in industrial automation.
IEEE Transactions on Industrial Informatics, 1:62‚Äì70, 2005.
10. S. Karnouskos, O. Baecker, L. M. S. de Souza, and P. Spiess. Integration of SOAready Networked Embedded Devices in Enterprise Systems via a Cross-Layered
Web Service Infrastructure. In 12th IEEE Conference on Emerging Technologies
and Factory Automation, 2007.
11. A. Reinhardt. A Machine-To-Machine ‚ÄùInternet Of Things‚Äù. Business Week, April
2004.
12. U. Saif and D. J. Greaves. Communication Primitives for Ubiquitous Systems or
RPC Considered Harmful. In 21st International Conference of Distributed Computing Systems (Workshop on Smart Appliances and Wearable Computing), Los
Alamitos, CA, USA, 2001. IEEE Computer Society.
13. C. R. Schoenberger. RFID: The Internet of Things. Forbes, (18), March 2002.
14. E. Zeeb, A. Bobek, H. Bohn, and F. Golatowski. Service-Oriented Architectures for
Embedded Systems Using Devices Profile for Web Services. In 21st International
Conference on Advanced Information Networking and Applications Workshops.,
2007.

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Security Considerations for Distributed Web-Based e-commerce
Applications in Java
Timothy E. Lindquist
Electronics and Computer Engineering Technology
Arizona State University East
http://www.east.asu.edu/ctas/ecet
Tim@asu.edu
Abstract
Today‚Äôs distributed e-commerce applications typically
rely upon various technologies in their realization, including the web, scripting languages, server-side processing
and an underlying database. The combination of these
technologies creates a system that requires attention to the
security issues of each component and the system as a
whole. In considering the overall system, issues arise from
the interactions of security frameworks available for each
component. In this paper, we consider the approach and
related issues for distributed e-commerce applications
developed with Java. The flexible nature of Java allows
migration of objects (compiled code with state) through
features such as RMI and Applets. Security for distributed
applications developed in Java has issues and lessons
applicable to systems of components built on different technologies.

1. Problem
Web-based e-commerce and distributed applications are
changing the way we buy goods, access information and
learn. Use of email and other related technology increasingly facilitates collaboration and is more commonly being
used for official communications. Official communications
via the internet are too often done in insecure mode.
Web-based e-commerce applications commonly employ
multiple tiers (3-tier client server architecture) and a combination of technologies such as HTML, XML, JavaScript,
Java (JSP, Servlets), ASP, dynamic html, CGI, and relational databases, as shown in Figure 1. Each of these technologies have separate and in some cases incompatible
approaches to protection against intrusion.
For web-based applications, the communication
between clients and the middle-tier is via web protocol
http. Clients may employ any number of technologies such
as applets, html, xml, and scripts. The middle-tier business

DBMS

legacy appl

view objects/
clients
IGURE 1.

business logic/
middle-tier/
server objects

3-Tier Client-Server Architecture

logic often employs any of a number of CGI work-arounds
such as Netscape‚Äôs NSAPI, Microsoft‚Äôs ISAPI, WebObjects, ASPs, Java J2EE, servlets and JSP. The combination
of different technologies at each tier, presents special challenges to security of the overall application.
Development time and cost pressures often short-change
security concerns. Problems range from software design
constraints that prevent adequate security to insufficient
testing to exercise common attacks. Often performance
concerns limit the extent to which assurance can be implemented in a web-based application.
Emerging technologies and applications are also presenting new challenges to secure applications. Distributed
Object technologies have been maturing for the past several
years and are being increasingly utilized in web-based
developments. Although some researchers have supported
an approach where an object-web replaces the largely datacentered web of today, this has not materialized. Nonetheless, object technologies such as CORBA, DCOM and Java
RMI enjoy increased usage in distributed web-based applications. Additional frameworks such as JINI, JavaSpaces,
JNDI, and EJB support distributed Java Objects.

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35‚Äô02)
0-7695-1435-9/02 $17.00 ¬© 2002 IEEE

transaction
monitor

1

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Distributed objects are network aware objects. Applications running on remote machines may communicate with
each other at the object-level using this technology. With a
remote reference to a Java RMI object, methods can be
requested of the object (messages) in the same manner as if
the object were local to the executing application. The SUN
implementation of Java (sdk/jdk1.2 and up) includes a flexible and feature rich approach to security providing the
basis for distributed Java applications.

private key

message

algorithm

signature

message

public key

algorithm

signature

verify

2. Security Concerns
Sender
The platform independence of Java has lead to easy
movement of (compiled) code across the internet. While
the approaches of OMG CORBA (see: http://
www.omg.org) and Microsoft do provide multi-language
solutions, they do not provide the same code migration
capabilities as is available with Java. Interacting remote
Java objects may easily be written in a manner that requires
dynamic movement and execution of code (class files)
across the internet.
Security concerns include authentication, integrity and
encryption/decryption. These may all come into play whenever information (code or data) is moved (across a network
or within a single machine).
1. Authenticity allows the receiver of information to
know with certainty the identity of the sender.
2. Integrity allows the receiver to know with certainty
that information transmitted by the sender has not been
modified or tampered with enroute.
3. Encryption is the process of taking data (called clear
text) and a short key and producing cipher data that is
meaningless to anyone who does not know the key.
Decryption is the process of taking cipher data and a
short key to produce the corresponding clear text.
Each of these basic security concerns come into play
with distributed applications, for controlling executing
applications as well as access to information. Figure 2
shows how authentication and integrity can be provided
using digital signatures.
The sender uses his own private key (which must be
kept protected utilizing access control) and together with a
message to generate a digital signature, which is unique to
the message and private key. The message and signature
are transferred to the receiver. The receiver must have a
public key (usually received separately) corresponding to
the sender‚Äôs private key. The public key can be used to verify a signature, but cannot be used to generate a signature.
Upon receipt, the message and signature are verified assuring both authenticity and integrity of the exchanged message.

IGURE 2.

3. Digital Signatures
Keys are generated in pairs. The private key is used to
generate the signature and is kept confidential to whoever
is doing the signing. The public key is used by the receiver
to verify authenticity of the message. The signer should
distribute the public key to anyone who will receive signed
information.
The issue as to whether the public key actually corresponds to the sender is resolved with certificates. A certificate represents a chain of trust leading from the sender to
the receiver, indicating that the public key belongs to whom
you want to believe it belongs. If the sender and receiver
both trust the same certificating agency then the chain may
be of length one. Each link in the chain is a certifying
agency (such as VeriSign or Entrust) which certifies that
the entity prior to it in the chain (the owner of the private
key or another certifying agency) is who they say they are.
Users should understand how certificates are signed and
managed. Current web browsers display information about
the certificate and who signed it, but few users ever look
beyond the lock icon on their web browsers. This provides
some opportunity for anyone with a signed certificate to
use a man-in-the-middle attack. Simple possession of a certificate says nothing of integrity, quality or functionality of
code or other information conveyed by the certificate
holder.
Another complication of digital signatures is management of a certificate revocation list. Once a key is known to
be compromised, there must be some way to inform users
that it should no longer be trusted.
The SUN implementation of Java comes with a primitive set of tools for manipulating keys, certificates and digital signatures. It also includes the framework classes (in the
package java.security) for program creation and verification of digital signatures.
Figure 3 includes a sample of Java that may exist for the
sender. The example generates a public and private key pair

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35‚Äô02)
0-7695-1435-9/02 $17.00 ¬© 2002 IEEE

Receiver

3-Tier Client-Server Architecture

2

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

//generate the private and matching public key
KeyPairGenerator keyGen=KeyPairGenerator.getInstance("DSA",
"SUN");
SecureRandom random = SecureRandom.getInstance(
"SHA1PRNG", "SUN");
keyGen.initialize(1024, random);
KeyPair pair = keyGen.generateKeyPair();
PrivateKey priv = pair.getPrivate();
PublicKey pub = pair.getPublic();
//create the signature object
Signature dsa = Signature.getInstance("SHA1withDSA", "SUN");
dsa.initSign(priv);
//read the datafile;
FileInputStream fis = new FileInputStream(args[0]);
BufferedInputStream bufin = new BufferedInputStream(fis);
byte[] buffer = new byte[1024];
int len;
while (bufin.available() != 0) {
len = bufin.read(buffer);
dsa.update(buffer, 0, len);
}
bufin.close();
//generate the signature
byte[] realSig = dsa.sign();
//save the signed data in a file
FileOutputStream sigfos = new FileOutputStream("sigOf"+args[0]);
sigfos.write(realSig);
sigfos.close();

FIGURE 3.

Snipet of Java to sign a file

and uses the private key to generate a digital signature for a
data file. The signature is saved to file. This code represents
simplistically what must be done by the sender. The public
key, the signature file and the data file are all transmitted to
the receiver, where a similar program verifies the signature
using the data and public key. The primary vulnerability of
this approach rests in communicating the public key. An
attack may replace the data, signature file and public key if
they are all three transmitted together. Certificates are the
most common mechanism used to assure the public key
authentically identifies the sender. Good practice dictates
that the public key be transmitted separately in an assureable manner. The public key (certificate) is stored by the
receiver for later use to authenticate multiple subsequent
transmissions.
This mechanism can be used to verify the authenticity
and integrity of either data or program code that is transferred in distributed e-commerce applications. The
approach verifies that information came from the purported
sender and was not modified in transmission. Encryption is
necessary to protect information from reading by others
during transmission, as discussed below. Security within an
executing Java application is based upon authenticity and
integrity using digital signatures.

4. Securing Java Applications
Many aspects of Java‚Äôs design lend well to distributed
applications. One such example is serialization. The ability
to externalize objects from one executing Java program
(virtual machine) and to read them into another is a process
Java calls serialization. Serializable objects may be transmitted through the internet without loss of object properties, including methods.
To accomplish object externalization, it is often necessary to move the compiled code along with object data.
Several mechanisms exist within Java to do this either
implicitly or explicitly under programmer control. Applets
and Remote Method Invocation (RMI) are two such mechanisms. Applets are small Java programs communicated
from a web-server and executed by a virtual machine running in the browser. RMI, provides the programmer with an
object view of internet objects so that method calls, for
example can formulated as though the object were in the
same virtual machine. RMI capabilities are similar to
Microsoft DCOM and the Object Management Group‚Äôs
Common Object Request Broker Architecture (CORBA).
Java‚Äôs platform independence is critical to realizing
these dynamic capabilities. Compiled java code (class files)
can move to a variety of platforms and be executed without
loss of meaning. This powerful capability, which has not
been realized to the same level and extent by any other language efforts, was first made generally available by Java
implementations.
Java‚Äôs reflection capabilities, allow a program to discover and access the properties available in an object. This
allows internet available objects to be manipulated in a
manner not necessarily know by the program at the time it
was compiled. In addition to facilitating distribution, Serialization, RMI, and Reflection are leading to a view of
internet enabled software service objects. These provide
the critical infrastructure for e-commerce services, such as
financial, investment, and retail purchase.
The current e-commerce solutions utilize the web and
represent a composition of diverse technologies:
1. User interface through html, xml, Java, JavaScript,
Flash and so on,
2. Server functions through dynamic html, JSP, ASP,
J2EE, CGI or servlets,
3. Legacy data through RMI, ODBC or JDBC connectivity to a relational database.
The challenge is to formulate a secure impenetrable
application in light of the combination of a variety of technologies and capabilities.
The Java model for securing the operations in an executing virtual machine has progressed significantly since the

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35‚Äô02)
0-7695-1435-9/02 $17.00 ¬© 2002 IEEE

3

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

initial introduction of the sandbox model for Java applications. Initial versions of Java provided full trust to classes
loaded locally and prohibited all sensitive operations from
any code obtained dynamically.
Java now supports a continuum of access control.
Access to system resources (such as files, sockets, runtime,
properties, security permissions, serializable, reflection,
and window toolkit) is granted based on domains. A
domain includes a set of permissions together with a codebase and an indication of who signed the code. The codebase indicates the file or URL from which the code is
loaded. If signed, the alias of the public key can also be
used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which
defines the access it has to resources.

{

permission java.net.SocketPermission
"*.GSE.com:2575-",
"accept, connect, listen, resolve";

};

A policy may consist of one or more grants each defining different domains. Each domain may have one or more
associated permissions.
When a protected operation is attempted, the virtual
machine‚Äôs security manager performs a security check. It
looks at the classes of all methods currently on the runtime
call stack. Each associated protection domain is queried to
determine whether the operation is allowed. An operation
is performed only if all methods on the runtime stack have
the appropriate permission.
Signing executable is an important application of
authentication and integrity technology. As the number of
distributed applications grows and those applications
increasingly rely upon migration of code, we need assurance that we are granting permissions to trustworthy code.
Today, code signing is largely platform dependent. For
example, applets executed with the Netscape or Internet
Explorer Java virtual machines require use of Netscape or
Microsoft tools to sign the code. Applets designed to run
with the SUN plug-in virtual machine must be signed with
the SUN tools. This lack of consistency only accentuates
the problems arising from utilizing multiple technologies to
realize an e-commerce application.

5. Cryptography
FIGURE 4.

Controlling Access to Java Resources

Figure 4, is taken from the On-line Java Tutorial,
http://java.sun.com/docs/books/tutorial/
and shows the interaction between the security domains
defined in Java2 and the original sandbox model. In the
Java 2 SDK version 1.4, the standard platform has been
further augmented to integrate the Java authentication and
authorization service (JAAS). Doing so takes a step closer
to integrating user login services with authentication mechanisms. See:
http://java.sun.com/products/jaas/
In Java 2, security domains are defined by a policy
granting permissions to the domain. For example, suppose
the company GrowthStocksExpress publishes an applet
on their hypothetical web site at the URL:
http://GSE.com/applets
Assuming the applet needs connections to one or more
hosts having a domain address ending with GSE.com on
ports beginning at 2575, a policy for clients who access the
applet may be:
grant

signedBy "GrowthStockExpress",
codeBase "http://GSE.com/applets"

The SUN implementation of the Java Cryptography
Architecture (Java Crypto Extensions) is freely available
for developing applications that rely on encryption. Similarly, if the application requires an encrypting web server,
Apache-SSL is one of the freely available web servers
based on OpenSSL. It can be freely obtained and used
commercially. See:
http://www.apache-ssl.org/
In addition to authentication and integrity, distributed ecommerce applications require cryptographic services.
Authentication and integrity assure the identity of the
sender and that information was not changed in transmission, but they do not protect against reading during transmission. Encryption is a concern for financial transactions
or other communication where personal identification
information must be transmitted. To guard against this type
of intrusion, many encryption / decryption algorithms and
implementations exist.
Encryption is the process of taking clear text and converting it into cypher data that is unreadable to anyone who
does not know the key. Decryption reconstructs clear text
from the cypher data, using the key. The sender performs
encryption before transmission and the receiver decrypts to

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35‚Äô02)
0-7695-1435-9/02 $17.00 ¬© 2002 IEEE

4

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

reconstruct the information. Several implementations of
various strength exist. Primary issues relate to strength and
performance - time and space to encrypt and decrypt.
Figure 5 is taken from the Java Tutorial and shows the
service provider architecture that is used in the security
frameworks provided by SUN with Java. The API (application program interface) provides a common interface for
developing e-commerce or other distributed applications.
To the extent possible, various alternative approaches to
security are cast into a single interface. Engine classes
abstractly define cryptographic services.
Providers (implementing security services) write to the
lower level SPI (service providers interface). For an example implementation see JCSI [5]. SUN also provides a
default implementation which is distributed with the downloadable extensions. Where multiple implementations
exist, initialization methods select the appropriate implementation based on parameters. This same approach is
used, for example in Java‚Äôs database connectivity, JDBC.
Where multiple drivers exist, selection is wired-into the
API through initialization methods.
Although this architecture is a powerful approach that
adds considerable value to the Java framework, in practice
it is very difficult to achieve a single common interface that
works equally well for all implementations.

Authenticity and integrity are just that and no more.
Signed Java can be relied upon regarding who signed it and
that it has not been disturbed in transmission. The fact that
a digital signature has been verified tells the user nothing
about the goodness of the code or the security of the system
that is delivered in signed form. These are elements of trust
in the individual or company that signed the code.
To further the problem, security problems do and will
continue to result from problems in the infrastructure upon
which the Java implementation is built. For example, denial
of service attacks, file access, host system intrusion and
underlying problems with TCP/IP all arise to the applications built on these technologies.
Nevertheless, e-commerce applications must be secure
and the best way to build in security is to use best software
practices and processes for their development. Specification and design of a secure distributed Java application
should include security risks, requirements and underlying
constraints. Development should proceed with a security
risk assessment, followed by design and reviews from risk
perspective. Security testing, which is necessarily different
from specification testing, should consider likely avenues
of problems and exercise documented successful attacks on
similar systems.
For further reading on security problems with Java and
related technology, see:
http://www.cigital.com/javasecurity/articles-1.html
http://www.w3.org/pub/Conferences/WWW4/Papers/
197/40.html
and the Java security website:
http://www.rstcorp.com/java-security.html
For further reading in Security and Encryption, see Peter
Guttmann‚Äôs web site [4], which contains references to various research publications as well as software and other
internet resources related to security and encryption.

7. References
IGURE 5.

Service Provider Architecture

6. Closing Remarks
For a language that has developed and whose use has
spread so rapidly, Java‚Äôs features are remarkably complete
and consistent. Nevertheless, security in Java applications
is a difficult task. Java security mechanisms are complex
and as such are likely to be inappropriately used by developers. The Java security model, together with the Java
cryptography architecture are powerful tools that are integrated well into the language both in terms of controlling
applications and in terms of defining security frameworks
that are amenable to realization by multiple implementations.

[1.] McGraw, Gary and Felton, Ed; Securing Java, John
Wiley and Sons Inc., 1999, see:
http://www.securingjava.com/
[2.] Griscom, Daniel; Code Signing for Java Applets; see:
http://www.suitable.com/Doc_CodeSigning.shtml
[3.] Campione, Mary, et al., The Java Tutorial, SUN, Addison Wesley, December 2000,
http://java.sun.com/docs/books/tutorial/
[4.] Guttmann, Peter; Security and Encryption-Related
Resources and Links,
http://www.cs.auckland.ac.nz/~pgut001/links.html
[5.] Sun Microsystems Java Security and Crypto Implementation,
http://www.cs.wustl.edu/~luther/Classes/Cs502/
WHITE-PAPERS/jcsi.html

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35‚Äô02)
0-7695-1435-9/02 $17.00 ¬© 2002 IEEE

5

1

Optimal estimation with missing observations
via balanced time-symmetric stochastic models

arXiv:1503.06014v2 [math.OC] 18 Aug 2015

Tryphon T. Georgiou, Fellow, IEEE and Anders Lindquist, Life Fellow, IEEE

Abstract‚ÄîWe consider data fusion for the purpose of smoothing and interpolation based on observation records with missing
data. Stochastic processes are generated by linear stochastic
models. The paper begins by drawing a connection between
time reversal in stochastic systems and all-pass extensions. A
particular normalization (choice of basis) between the two timedirections allows the two to share the same orthonormalized
state process and simplifies the mathematics of data fusion.
In this framework we derive symmetric and balanced MayneFraser-like formulas that apply simultaneously to smoothing and
interpolation.

I. I NTRODUCTION
Data fusion is the process of integrating different data sets,
or statistics, into a more accurate representation for a quantity
of interest. A case in point in the context of systems and
control is provided by the Mayne-Fraser two-filter formula
[1], [2] in which the estimates generated by two different
filters are merged into a combined more reliable estimate
in fixed-interval smoothing. The purpose of this paper is to
develop such a two-filter formula that is universally applicable
to smoothing and interpolation based on general records with
missing observations.
In [3], [4] the Mayne-Fraser formula was analyzed in the
context of stochastic realization theory and was shown that
it can be formulated in terms a forward and a backward
Kalman filter. In a subsequent series of papers, Pavon [5],
[6] addressed in a similar manner the hitherto challenging
problem of interpolation [7], [8], [9], [10]. This latter problem consists of reconstructing missing values of a stochastic
process over a given interval. In departure from the earlier
statistical literature, [5], [6] considered a stationary process
with rational spectral density and, therefore, reliazable as the
output of a linear stochastic system. Interpolation was then
cast as seeking an estimate of the state process based on an
incomplete observation record. A basic tool in these works
is the concept of time-reversal in stochastic systems which
has been central in stochastic realization theory (see, e.g.,
[11], [12], [13], [14], [5], [6], [15], [16], [17]). For a recent
Research supported by grants from AFOSR, NSF, VR, and the SSF.
T.T. Georgiou is with the Department of Electrical & Computer
Engineering, University of Minnesota, Minneapolis, Minnesota; email:
tryphon@umn.edu and A. Lindquist is with the Department of Automation
and the Department of Mathematics, Shanghai Jiao Tong University, Shanghai,
China, and the Center for Industrial and Applied Mathematics and ACCESS
Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden;
email: alq@kth.se

overview of smoothing and interpolation theory in the context
of stochastic realization theory see [18, Chapter 15].
In the present paper we are taking this program several
steps further. Given intermittent observations of the output
of a linear stochastic system over a finite interval, we want
to determine the linear least-squares estimate of the state
of the system in an arbitrary point in the interior of the
interval, which may either be in a subinterval of missing
data or in one where observations are available. Hence, this
combines smoothing and interpolation over general patterns of
available observations. Our main interest is in continuous-time
(possibly time-varying) systems. However, the absence of data
over subintervals, depending on the information pattern, may
necessitate a hybrid approach involving discrete-time filtering
steps.
In studying the statistics of a process over an interval, it is
natural to decompose the interface between past and future in
a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward
or backward in time. This point was fundamental in early work
in stochastic realization; see [18] and references therein. In a
different context [19] a certain duality between the two timedirections in modeling a stochastic process was introduced in
order to characterize solutions to moment problems. In this
new setting the noise-process was general (not necessarily
white), and the correspondence between the driving inputs to
the two time-opposite models was shown to be captured by
suitable dual all-pass dynamics.
Here, we begin by combining these two sets of ideas
to develop a general framework where two time-opposite
stochastic systems model a given stochastic process. We study
the relationship between these systems and the corresponding
processes. In particular, we recover as a special case certain
results of stochastic realization theory [11], [5], [6], [4] from
the 1970‚Äôs using a novel procedure. This theory provides a
normalized and balanced version of the forward-backward
duality which is essential for our new formulation of the
two-filter Mayne-Fraser-like formula uniformly applicable to
intervals with or without observations.
The paper is structured as follows. In Section II we
explain how a lifting of state-dynamics into an all-pass system
allows direct correspondence between sample-paths of driving

2

generating processes, in opposite time-directions, via causal
and anti-causal mappings, respectively. This is most easily
understood and explained in discrete-time and hence we begin
with that. In Section III we utilize this mechanism in the
context of general output processes and, similarly, introduce a
pair of time-opposite models. These two introductory sections,
II and III, deal with stationary models for simplicity and
are largely based on [20]. The corresponding generalizations
to time-varying systems are given in Section IV and in the
appendix, in continuous and discrete-time, respectively. In
Section V we explain Kalman filtering for problems with
missing information in the continuous-time setting. In this,
we first consider the case where increments of the output
process across intervals of no information are unavailable as
a simplified preliminary, after which we focus on the central
problem where the output process is the object of observation.
Section VI deals with the geometry of information fusion.
In Section VII we present a generalized balanced two-filter
formula that applies uniformly over intervals where data is or
is not available. We summarize the computational steps of this
approach in Section VIII. Finally, we highlight the use of the
two-filter formula with a numerical example given in Section
IX and provide concluding remarks in Section X.

this is identical to the one for discrete-time given above (as
is well known). In continuous time, stability of the system
of equations is equivalent to A having only eigenvalues with
negative real part.
In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system
is all-pass. This is done next.

A. All-pass extension in discrete-time
Consider the discrete-time Lyapunov equation
P = AP A0 + BB 0 .

Since A has all eigenvalues inside the unit disc of the complex
plane and (3) holds, (6) has as solution a matrix P which is
positive definite. The state transformation
1

Œæ = P ‚àí 2 x,

x(t + 1) = Ax(t) + Bw(t)
n√ón

(1)

n√óp

where t ‚àà Z, A ‚àà R
,B ‚àà R
, A has all eigenvalues
in the open unit disc D = {z | |z| < 1}, and w(t), x(t) are
(centered) stationary vector-valued stochastic processes with
w(t) normalized white noise; i.e.,
E{w(t)w(s)0 } = Ip Œ¥ts ,

(2)

where E denotes mathematical expectation. The system of
equations is assumed to be reachable, i.e.,


rank B, AB, . . . An‚àí1 B = n.
(3)
In continuous-time, state-dynamics take the form of a
system of stochastic differential equations
dx(t) = Ax(t)dt + Bdw(t)

(4)

where, here, x(t) is a stationary continuous-time vector-valued
stochastic process and w(t) is a vector-valued process with
orthogonal increments with the property
E{dwdw0 } = Ip dt,

(5)

where Ip is the p √ó p identity matrix. Reachability of the
pair (A, B) is also assumed throughout and the condition for

(7)

and
1

1

1

F = P ‚àí 2 AP 2 , G = P ‚àí 2 B,

(8)

Œæ(t + 1) = F Œæ(t) + Gw(t).

(9)

brings (1) into

II. S TATE DYNAMICS AND ALL - PASS EXTENSION
In this paper we consider discrete-time as well as
continuous-time stochastic linear state-dynamics. We begin by
explaining basic ideas in a stationary setting. In discrete-time
systems take the form of a set of difference equations

(6)

For this new system, the corresponding Lyapunov equation
X = F XF 0 + GG0 has In as solution, where In denotes the
(n √ó n) identity matrix. This fact, namely, that
In = F F 0 + GG0

(10)

implies that this [F, G] can be embedded as part of an
orthogonal matrix


F G
U=
,
(11)
H J
i.e., a matrix such that U U 0 = U 0 U = In+p .
Define the transfer function
U(z) := H(zIn ‚àí F )‚àí1 G + J

(12)

corresponding to
Œæ(t + 1) = F Œæ(t) + Gw(t)

(13a)

wÃÑ(t) = HŒæ(t) + Jw(t).

(13b)

This is also the transfer function of
x(t + 1) = Ax(t) + Bw(t)

(14a)

0

(14b)

wÃÑ(t) = BÃÑ x(t) + Jw(t),
‚àí 21

where BÃÑ := P H 0 , since the two systems are related by a
similarity transformation. Hence,
U(z) = BÃÑ 0 (zIn ‚àí A)‚àí1 B + J.

(15)

3

Now, using the identity
We claim that U(z) is a stable all-pass transfer function (with
respect to the unit disc), i.e., that U(z) is a transfer function
of a stable system and that
U(z)U(z ‚àí1 )0 = U(z ‚àí1 )0 U(z) = Ip .

(16)

The latter claim is immediate after we observe that, since
U U = In+p ,

 

Œæ(t)
Œæ(t + 1)
,
=
U0
w(t)
wÃÑ(t)
0

and hence,

In ‚àí F F 0 = (zIn ‚àí F )(z ‚àí1 In ‚àí F 0 )
+ (zIn ‚àí F )F 0 + F (z ‚àí1 In ‚àí F 0 ),
(10) and GJ 0 = ‚àíF H 0 , obtained from U U 0 = In+p , this
yields
U(z)U(z ‚àí1 )0 = HH 0 + JJ 0 = In+p ,
as claimed.

B. All-pass extension in continuous-time
Consider the continuous-time Lyapunov equation

Œæ(t) = F 0 Œæ(t + 1) + H 0 wÃÑ(t)
0

0

w(t) = G Œæ(t + 1) + J wÃÑ(t)

(17b)

or, equivalently,
1

x(t) = P A0 P ‚àí1 x(t + 1) + P 2 H 0 wÃÑ(t)
0

w(t) = B P

‚àí1

0

x(t + 1) + J wÃÑ(t).

(18a)

(19)

dŒæ(t) = F Œæ(t)dt + Gdw(t).

dwÃÑ(t) = HŒæ(t)dt + Jdw(t)
(20a)

w(t) = B 0 xÃÑ(t) + J 0 wÃÑ(t)

(20b)

U(z)‚àó = B 0 (z ‚àí1 In ‚àí A0 )‚àí1 BÃÑ + J 0 .

(21)

Either of the above systems inverts the dynamical relation
w ‚Üí wÃÑ (in (14) or (13)).

-

(23b)

so that the transfer function
U(s) := H(sIn ‚àí F )‚àí1 G + J

(24)

is all-pass (with respect to the imaginary axis), i.e.,

with transfer function

w(t)

(23a)

We now seek a completion by adding an output equation

(18) can be written
xÃÑ(t ‚àí 1) = A0 xÃÑ(t) + BÃÑ wÃÑ(t)

(22)

Since A has all its eigenvalues in the left half of the complex
plane and since (3) holds, (22) has as solution a positive
definite matrix P . Once again, applying (7-8), the system in
(4) becomes

(18b)

Setting
xÃÑ(t) := P ‚àí1 x(t + 1),

AP + P A0 + BB 0 = 0.

(17a)

U

U(s)U(‚àís)0 = U(‚àís)0 U(s) = Ip .

For this new system, the corresponding Lyapunov equation
has as solution the identity matrix and hence,
F + F 0 + GG0 = 0.

wÃÑ(t)
-

Fig. 1: Realization (14) in the forward time-direction.

(25)

(26)

Utilizing this relationship we note that
(sIn ‚àí F )‚àí1 GG0 (‚àísIn ‚àí F 0 )‚àí1
= (sIn ‚àí F )‚àí1 (sIn ‚àí F ‚àí sIn ‚àí F 0 )(‚àísIn ‚àí F 0 )‚àí1

w(t)


U‚àó

wÃÑ(t)


Fig. 2: Realization (20) in the backward time-direction.
An algebraic proof of (16) is also quite immediate. In fact,
U(z)U(z ‚àí1 )0


0
= H(zIn ‚àí F )‚àí1 G + J H(z ‚àí1 In ‚àí F )‚àí1 G + J
=H(zIn ‚àí F )‚àí1 GG0 (z ‚àí1 In ‚àí F 0 )‚àí1 H 0 + JJ 0
+ H(zIn ‚àí F )‚àí1 GJ 0 + JG0 (z ‚àí1 In ‚àí F 0 )‚àí1 H

= (sIn ‚àí F )‚àí1 + (‚àísIn ‚àí F 0 )‚àí1 ,
and we calculate that
U(s)U(‚àís)0
= (H(sIn ‚àí F )‚àí1 G + J)(G0 (‚àísIn ‚àí F 0 )‚àí1 H 0 + J 0 )
= JJ 0 + H(sIn ‚àí F )‚àí1 (GJ 0 + H 0 )
(JG0 + H)(‚àísIn ‚àí F 0 )‚àí1 H 0 .
For the product to equal the identity,
JJ 0 = Ip
H = ‚àíJG0 .

4

Thus, we may take

A. Time-reversal of discrete-time stochastic systems
Consider a stochastic linear system

J = Ip
H = ‚àíG0 ,

x(t + 1) = Ax(t) + Bw(t)

(35a)

y(t) = Cx(t) + Dw(t)

(35b)

and the forward dynamics
dŒæ(t) = F Œæ(t)dt + Gdw(t)

(27a)

dwÃÑ(t) = ‚àíG0 Œæ(t)dt + dw(t).

(27b)

Substituting F = ‚àíF 0 ‚àí GG0 from (26) into (27a) we obtain
the reverse-time dynamics
dŒæ(t) = ‚àíF 0 Œæ(t)dt + GdwÃÑ(t)
0

dw(t) = G Œæ(t)dt + dwÃÑ(t).

(28a)
(28b)

Now defining
xÃÑ(t) := P ‚àí1 x(t)

(29)

and using (7) and (8), (28) becomes
dxÃÑ(t) = ‚àíA0 xÃÑ(t)dt + BÃÑdwÃÑ(t)
0

dw(t) = B xÃÑ(t)dt + dwÃÑ(t),

(30a)
(30b)

with transfer function

with an m-dimensional output process y, and x, u, A, B are
defined as in Section II-A. All processes are stationary and
the system can be thought as evolving forward in time from
the remote past (t = ‚àí‚àû).
To formalize this, we introduce some notation. Let H be
the Hilbert space spanned by {wk (t); t ‚àà Z, k = 1, 2, . . . , n},
endowed with the inner product hŒª, ¬µi = E{Œª¬µ}, and let
+
H‚àí
t (w) and Ht (w) be the (closed) subspaces spanned by
{wk (s); s ‚â§ t ‚àí 1, k = 1, . . . , m} and {wk (s); s ‚â• t, k =
+
1, . . . , m}, respectively. Define H‚àí
t (y) and Ht (y) accordingly in terms of the output process process y. Then the
stochastic system (35) evolves forward in time in the sense
that
‚àí
+
H‚àí
(36)
t (z) ‚äÇ Ht (w) ‚ä• Ht (w),
where A ‚ä• B means that elements of the subspaces A and
B are mutually orthogonal, and where H‚àí
t (z) is formed as
above in terms of


x(t + 1)
z(t) =
;
y(t)

U(s)‚àó = Ip + B 0 (sIn + A0 )‚àí1 BÃÑ,

(31)

BÃÑ := P ‚àí1 B.

(32)

xÃÑ(t ‚àí 1) = A0 xÃÑ(t) + BÃÑ wÃÑ(t)

(37a)

Furthermore, the forward dynamics (27) can be expressed in
the form

y(t) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t),

(37b)

see [18, Chapter 6] for more details.

where

Next we construct a stochastic system

dx(t) = Ax(t)dt + Bdw(t)
0

dwÃÑ(t) = BÃÑ x(t)dt + dw(t)

(33a)
(33b)

with transfer function

which evolves backward in time from the remote future
(t = ‚àû) in the sense that the processes xÃÑ, x, wÃÑ, w relate as in
the previous section. More specifically, as shown in Section
II-A, H‚àí (wÃÑ) ‚äÇ H‚àí (w) and H+ (w) ‚äÇ H+ (wÃÑ) for all t, as
examplified in Figures 1 and 2.
In fact, the all-pass extension (14) of (35a) yields

U(s) = Ip ‚àí BÃÑ 0 (sIn ‚àí A)‚àí1 B.

(34)

wÃÑ(t) = BÃÑ 0 x(t) + Jw(t)

(38)

It follows from (20b) that (38) can be inverted to yield
III. T IME - REVERSAL OF STATIONARY LINEAR
STOCHASTIC SYSTEMS

The development so far allows us to draw a connection
between two linear stochastic systems having the same output
and driven by a pair of arbitrary, but dual, stationary processes
w(t) and wÃÑ(t), one evolving forward in time and one evolving
backward in time. When one of these two processes is white
noise (or, orthogonal increment process, in continuous-time),
then so is the other. For this special case we recover results
of [11] and [5], [6] in stochastic realization theory.

w(t) = B 0 xÃÑ(t) + J 0 wÃÑ(t),

(39)

where xÃÑ(t) = P ‚àí1 x(t + 1), and that we have the reverse-time
recursion
xÃÑ(t ‚àí 1) = A0 xÃÑ(t) + BÃÑ wÃÑ(t).
(40a)
Then inserting (39) and
x(t) = P xÃÑ(t ‚àí 1) = P A0 xÃÑ(t) + P BÃÑ wÃÑ(t)
into (35b), we obtain
y(t) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t),

(40b)

5

where DÃÑ := CP BÃÑ + DJ 0 and
CÃÑ := CP A0 + DB 0 .

(41)

Then, (40) is precisely what we wanted to establish.
The white noise w is normalized in the sense of (2). Since
U, given by (15), is all-pass, wÃÑ is also a normalized white
noise process, i.e.,
E{wÃÑ(t)wÃÑ(s)0 } = Ip Œ¥t‚àís .
From the reverse-time recursion (37a)
xÃÑ(t) =

‚àû
X

+
same inner product as above, and let H‚àí
t (du) and Ht (du)
be the (closed) subspaces spanned by the increments of the
components of U on (‚àí‚àû, t] and [t, ‚àû), respectively. Define
+
H‚àí
t (dy) and Ht (dy) accordingly in terms of the output
process y. All processes have stationary increments and the
stochastic system (45) evolves forward in time in the sense
that
‚àí
+
H‚àí
(46)
t (dz) ‚äÇ Ht (dw) ‚ä• Ht (dw),

where H‚àí
t (dz) is formed in terms of


x(t)
.
z(t) =
y(t)

(47)

The all-pass extension of Section II-B yields

(A0 )k‚àí(t+1) BÃÑ wÃÑ(k).

dwÃÑ = dw ‚àí BÃÑ 0 xdt

k=t+1
0

Since, wÃÑ is a white noise process, E{xÃÑ(t)wÃÑ(s) } = 0 for all
s ‚â§ t. Consequently, (37) is a backward stochastic realization
in the sense defined above.

as well as the reverse-time relation
dxÃÑ = ‚àíA0 xÃÑdt + BÃÑdwÃÑ
0

dw = B xÃÑdt + dwÃÑ,

Moreover, the transfer functions
W(z) = C(zIn ‚àí A)‚àí1 B + D

(42)

(48)

(49a)
(49b)

where xÃÑ(t) = P ‚àí1 x(t). Inserting (49b) into
dy = CP xÃÑdt + Ddw

of (35) and
WÃÑ(z) = CÃÑ(z ‚àí1 In ‚àí A0 )‚àí1 BÃÑ + DÃÑ

(43)

of (37) satisfy

yields
dy = CÃÑ xÃÑdt + DdwÃÑ,
where

W(z) = WÃÑ(z)U(z).

In the context of stochastic realization theory, U(z) is called
structural function ([13], [14]).

w(t)
-

W

-

y(t)

WÃÑ

(50)

Thus, the reverse-time system is

y(t)

dxÃÑ = ‚àíA0 xÃÑdt + BÃÑdwÃÑ

(51a)

dy = CÃÑ xÃÑdt + DdwÃÑ.

(51b)

From this, we deduce that the system (45) has the backward
property
+
‚àí
H+
(52)
t (dzÃÑ) ‚äÇ Ht (dwÃÑ) ‚ä• Ht (dwÃÑ),

Fig. 3: The forward stochastic system (35).



CÃÑ = CP + DB 0 .

(44)

where H+
t (dzÃÑ) is formed as above in terms of


xÃÑ(t)
zÃÑ(t) =
.
y(t)

wÃÑ(t)


We also note that the transfer function
Fig. 4: The backward stochastic system (37)

W(s) = C(sIn ‚àí A)‚àí1 B + D
of (45) and the transfer function

B. Time-reversal of continuous-time stochastic systems

WÃÑ(s) = CÃÑ(sIn + A0 )‚àí1 BÃÑ + D

We now turn to the continuous-time case. Let

of (51) also satisfy

dx = Axdt + Bdw

(45a)

dy = Cxdt + Ddw

(45b)

be a stochastic system with x, w, A, B as in Section II-B,
evolving forward in time from the remote past (t = ‚àí‚àû).
Now let H be the Hilbert space spanned by the increments
of the components of w on the real line R, endowed with the

W(s) = WÃÑ(s)U(s)
as in discrete-time.
Note that the orthogonal-increment process w is normalized in the sense of (5). Since U(s) is all-pass,
dwÃÑ = du ‚àí BÃÑ 0 xdt

(53)

6

also defines a stationary orthogonal-increment process wÃÑ such
that
{dwÃÑ(t)dwÃÑ(t)0 } = Ip dt.
It remains to show that (51) is a backward stochastic realization, that is, at each time t the past increments of wÃÑ are
orthogonal to xÃÑ(t). But this follows from the fact that
Z ‚àû
0
e‚àíA (t‚àís) BÃÑdwÃÑ(s)
xÃÑ(t) =

1

1

1

P (t)‚àí 2 PÃá P (t)‚àí 2 = ‚àíR(t) ‚àí R(t)0 ,
and hence the (55) yields
F (t) + F (t)0 + G(t)G(t)0 = 0.

(61)

Using (61) to eliminate F in (57), we obtain

t

and wÃÑ has orthogonal increments.

1

Differentiating P (t)‚àí 2 P (t)P (t)‚àí 2 = In , we obtain

dŒæ = ‚àíF (t)0 Œæ(t)dt + G(t)dwÃÑ,

(62)

dwÃÑ = dw ‚àí G(t)0 Œæ(t)dt,

(63)

where

IV. T IME REVERSAL OF NON - STATIONARY STOCHASTIC

which can also be written

SYSTEMS

dwÃÑ = dw ‚àí BÃÑ(t)0 x(t)dt,

In a similar manner non-stationary stochastic systems
admit unitary extensions which in turn allows us to construct
dual time-reversed stochastic models that share the same state
process. The case of discrete-time dynamics is documented
in the appendix, whereas the continuous-time counterpart is
explained next as prelude to smoothing and interpolation that
will follow.

A. Unitary extension
The covariance matrix function P (t) := E{x(t)x(t)0 } of
the time-varying state representation
dx = A(t)x(t)dt + B(t)dw,

x(0) = x0

(54)

with x0 a zero-mean stochastic vector with covariance matrix
P0 = E{x0 x00 }, satisfies the matrix-valued differential equation
PÃá (t) = A(t)P (t) + P (t)A(t)0 + B(t)B(t)0
(55)
with P (0) = P0 . Throughout we assume total reachability [18,
Section 15.2], and therefore P (t) > 0 for all t > 0.
A unitary extension of (54) is somewhat more complicated
than in the discrete time case. In fact, differentiating
1

Œæ(t) = P (t)‚àí 2 x(t)

(64)

where BÃÑ(t) := P (t)‚àí1 B(t).
Proposition 1: A process wÃÑ satisfying (63) has orthogonal
increments with the normalized property (5). Moreover,
E{[wÃÑ(t) ‚àí wÃÑ(s)]Œæ(t)0 } = 0

(65)

for all s ‚â§ t.
Proof: As is well-known, the solution of (57) can be
written in the form
Z t
Œæ(t) = Œ¶(t, s)Œæ(s) +
Œ¶(t, œÑ )G(œÑ )dw,
(66)
s

where Œ¶(t, s) is the transition matrix with the property
‚àÇŒ¶
(t, s) = F (t)Œ¶(t, s), Œ¶(s, s) = In
(67a)
‚àÇt
‚àÇŒ¶
(t, s) = ‚àíŒ¶(t, s)F (s), Œ¶(t, t) = In
(67b)
‚àÇs
Let s ‚â§ t. Then, in view of (63), a straight-forward calculation
yields
wÃÑ(t) ‚àí wÃÑ(s) = w(t) ‚àí w(s)
Z
‚àí M (t, s)Œæ(s) ‚àí

t

M (t, œÑ )G(œÑ )dw,

(68)

s

where
Z

(56)

M (t, s) =

t

G(œÑ )0 Œ¶(œÑ, s)dœÑ.

(69)

s

we obtain

Therefore,
dŒæ = F (t)Œæ(t)dt + G(t)dw,

(57)
E{[wÃÑ(t) ‚àí wÃÑ(s)][wÃÑ(t) ‚àí wÃÑ(s))0 } = Ip (t ‚àí s) + ‚àÜ(t, s),

where
1

1

F (t) = P (t)‚àí 2 A(t)P (t) 2 + R(t),
G(t) = P (t)
with

‚àí 12

where
(58a)

B(t)

(58b)


1
d
‚àí 12
R(t) =
P (t)
P (t) 2 .
dt

(59)



1

Z

t

‚àÜ(t, s) = M (t, s)M (t, s) +
M (t, œÑ )G(œÑ )G(œÑ )0 M (t, œÑ )0 dœÑ
s
Z t
‚àí
[M (t, œÑ )G(œÑ ) + G(œÑ )0 M (t, œÑ )0 ] dœÑ.
s

However, ‚àÜ(t, s) is identically zero. To see this, first note that

In fact,
dŒæ = P (t)‚àí 2 dx + R(t)Œæ(t)dt.

0

(60)

‚àÇM
(t, s) = ‚àíM (t, s)F (s) ‚àí G(s)0 .
‚àÇs

(70)

7

Then, in view of (61), a simple calculation shows that
‚àÇ‚àÜ
(t, s) ‚â° 0.
‚àÇs
Since ‚àÜ(t, t) = 0, the assertion follows. Hence the incremental
covariance is normalized.

B. Time reversal in continuous-time systems

dx = A(t)x(t)dt + B(t)dw,

x(0) = x0

(75a)

Next, we show that wÃÑ(t) has orthogonal increments. To
this end, choose arbitrary times s ‚â§ t ‚â§ a ‚â§ b on the interval
[0, T ], where we choose a and b fixed, and show that

dy = C(t)x(t)dt + D(t)dw,

y(0) = 0

(75b)

Q(t, s) := E{[wÃÑ(b) ‚àí wÃÑ(a)][wÃÑ(t) ‚àí wÃÑ(s))0 }

Next we derive the backward stochastic system corresponding to the non-stationary forward stochastic system

defined on the finite interval [0, T ], where x0 (with covariance
P0 ) and the normalized Wiener process w are uncorrelated.
To this end, apply the transformation
xÃÑ(t) = P (t)‚àí1 x(t)

is identically zero for all s ‚â§ t. Using (68) and
wÃÑ(b) ‚àí wÃÑ(a) = w(b) ‚àí w(s) ‚àí M (b, a)Œ¶(a, s)Œæ(s)
Z b
Z b
‚àí M (b, a)
Œ¶(a, œÑ )G(œÑ )dw ‚àí
M (b, œÑ )dw
s

a

computed analogously, we obtain
"

Z

(77)

b

Œ¶(a, œÑ )G(œÑ )dœÑ
#

b
0

Œ¶(a, œÑ )G(œÑ )G(œÑ ) M (t, œÑ )dœÑ .
s

Then, again using (61), we see that
‚àÇM
(t, s) ‚â° 0,
‚àÇs
so, since Q(t, t) = 0, we see that Q(t, s) is identically zero,
establishing that wÃÑ(t) has orthogonal increments.
Finally, we use the same trick to show (65). In fact, for
s ‚â§ t, (66) and (68) yield
E{[wÃÑ(t) ‚àí wÃÑ(s))Œæ(t)0 } = ‚àíM (t, s)Œ¶(t, s)0
Z t
Z t
+
G(œÑ )0 Œ¶(t, œÑ )0 dœÑ ‚àí
M (t, œÑ )G(œÑ )G(œÑ )0 )Œ¶(t, œÑ )0 dœÑ,
s

dy = CÃÑ(t)xÃÑ(t) + D(t)dwÃÑ,
where

s

+

together with (74b) to (75b) to obtain

CÃÑ(t) = C(t)P (t) + D(t)B(t).

Q(t, s) = M (b, a) Œ¶(a, s)M (t, s)0 ‚àí
Z

(76)

This together with (74a) yields the the backward system
corresponding to (75), namely
dxÃÑ = ‚àíA(t)0 xÃÑ(t)dt + BÃÑ(t)dwÃÑ

(78a)

dy = CÃÑ(t)xÃÑ(t)dt + D(t)dwÃÑ.

(78b)

with end-point condition xÃÑ(T ) = P (T )
the Wiener process wÃÑ.

‚àí1

x(T ) uncorelated to

The backward realization (78) was derived in [3], but in
cumbersome way, requiring the proof that wÃÑ(t) is a normalized
process with orthogonal increments to be suppressed. What
is new here is imposing the unitary map between w and wÃÑ,
making the analysis much simpler and more natural.
V. K ALMAN FILTERING WITH MISSING OBSERVATIONS

s

the partial derivative of which with respect to s is identical
zero; this is seen by again using (61). Therefore, since (65) is
zero for s = t, it is identical zero for all s ‚â§ t, as claimed.
This concludes the proof of Proposition 1.
Consequently, (57) and (64) form a forward unitary system
dx = A(t)x(t)dt + B(t)dw

(71a)

dwÃÑ = dw ‚àí BÃÑ(t)0 x(t)dt,

(71b)

The corresponding backward unitary system is obtained
through the transformation
1

xÃÑ(t) = P (t) 2 Œæ(t),

(72)

which yields
1

dxÃÑ = P (t)‚àí 2 dŒæ + R(t)Œæ(t)dt.

(73)

This together with (62) and (63) yields
dxÃÑ = ‚àíA(t)0 xÃÑ(t)dt + BÃÑ(t)dwÃÑ
0

dw = B(t) xÃÑ(t)dt + dwÃÑ,

(74a)
(74b)

We consider the linear stochastic system (75) which does
not have a purely deterministic component that enables exact
estimation of components of x from y, an assumption that we
retain in the rest of the paper. In the engineering literature is
often the case that the stochastic system (75) represented as
xÃá(t) = A(t)x(t) + B(t)wÃá(t),
yÃá(t) = C(t)x(t) + D(t)wÃá(t)

x(0) = x0

(79a)
(79b)

where the formal ‚Äúderivative‚Äù wÃá is white noise, i.e.,
E{wÃá(t)wÃá(s)0 } = IŒ¥(t ‚àí s) with Œ¥(t ‚àí s) being the Dirac
‚Äúfunction‚Äù. Of course xÃá, yÃá and wÃá are to be interpreted
as generalized stochastic processes. From a mathematically
rigorous point of view, observing yÃá makes little sense since,
for any fixed t, yÃá(t) has infinite variance and contains no
information about the state process x. However, observations
of yÃá could be interpreted as observations of the increments dy
of y in a precise meaning to be defined next. On the other
hand, one can think of (75) as a system of type


x(t)
dz = M (t)z(t)dt + N (t)dw(t), where z(t) =
,
y(t)

8

and one would like to determine the optimal linear leastsquares estimate of x(t) given past observed values of y.

with R(t) = D(t)D(t)0 and initial conditions x‚àí (0) = 0 and
Q(0) = P0 . Here Q‚àí (t) is the error covariance

Generally this distinction between observing y or dy is
not important. However, when there is loss of information
over an interval (t1 , t2 ), there are two different information
patterns depending on whether dy or y is observed. The
difference consists in whether ‚àÜy := y(t2 ) ‚àí y(t1 ) is part
of the observation record or not. These two cases will be
dealt with separately in subsections below. In fact, the former,
which is common in engineering applications, is provided as
a simplified preliminary, whereas our main interest is in the
latter. To this end, we first introduce some notation.

Q‚àí (t) := E{[x(t) ‚àí x‚àí (t)](x(t) ‚àí x‚àí (t)]0 },

Consider the stochastic system (75) on a finite interval
[0, T ]. As before, let H be the Hilbert space spanned by
{wk (t) ‚àí wk (s); s, t ‚àà [0, T ], k = 1, 2, . . . , m}, endowed
with the inner product hŒª, ¬µi = E{Œª¬µ}. For any Œª ‚àà H
and any subspace A, let EA denote the orthogonal projection
of Œª onto A. We denote by H[t1 ,t2 ] (dy) the (closed) subspace generated by the components of the increments of the
observation process y over the window [t1 , t2 ]. In particular,
we shall also use the notations H‚àí
t (dy) := H[0,t] (dy) and
H+
t (dy) := H[t,T ] (dy).
Suppose that the output process or its increments are
available for observation only on some subintervals of [0, T ],
‚ó¶
namely Ik , k = 1, 2, . . . , ŒΩ. Next we want to define H as the
proper subspace of H[0,T ] (dy) spanned by the observed data.
In the case that only the increments dy or, equivalently, the
‚Äúderivative‚Äù yÃá is observed, we simply define
‚ó¶

H := HI1 (dy) ‚à® HI2 (dy) ‚à® ¬∑ ¬∑ ¬∑ ‚à® HIŒΩ (dy),
In the case that the process y is observed, we need to expand
‚ó¶
H by adding the subspaces spanned by the increments ‚àÜy over
the complementary intervals without observation. In either
case, we define
‚ó¶

‚ó¶

‚àí
H‚àí
t := H ‚à© Ht (dy) and

‚ó¶

‚ó¶

+
H+
t := H ‚à© Ht (dy).

(80)

Then Kalman filtering with missing observations amounts to
determining a recursion for x‚àí where
‚ó¶

‚àí

a0 x‚àí (t) = EHt a0 x(t),

for all a ‚àà Rn .

(81)

A. Observing dy only

dx‚àí = A(t)x‚àí (t)dt + K‚àí (t)(dy(t) ‚àí C(t)x‚àí (t)dt)
(82a)
0
QÃá‚àí (t) = AQ‚àí + Q‚àí A0 ‚àí K‚àí RK‚àí
+ BB 0

which, by the nondeterministic assumption, is positive definite
for all t.
Next suppose the observation process becomes unavailable
over the interval [t1 , t2 ) ‚äÇ [0, T ]. Then the Kalman filter needs
to be modified accordingly. In fact, for any t ‚àà [t1 , t2 ), (81)
‚ó¶

‚àí
holds with the space of observations H‚àí
t := Ht1 (dy), and
consequently
‚àí

a0 x‚àí (t) = EHt1 (dy) a0 x(t) = a0 Œ¶(t, t1 )x‚àí (t1 ).
This corresponds to setting K‚àí (t) = 0 in (82) on the interval
[t1 , t) so that
dx‚àí = A(t)x‚àí (t)dt
(84a)
with initial condition x‚àí (t1 ) given by (82a). The error covariance Q‚àí is then given by the Lyapunov equation
QÃá‚àí (t) = AQ‚àí + Q‚àí A0 + BB 0

(84b)

with initial the condition Q‚àí (t1 ) given by the value produced
in the previous interval.
Then suppose observations of dy become available again
on the interval [t2 , t3 ). Then, for any t ‚àà [t2 , t3 ), we have
‚ó¶

H+
t = H[0,t1 ] ‚à® H[t2 ,t] ,
so the Kalman estimate is generated by (82) but now with
initial conditions x‚àí (t2 ) and Q‚àí (t2 ) being those computed
in the previous step without observation. In the case there are
more intervals, one proceeds similarly by alternating between
filters (82) and (84) depending on whether increments dy are
available or not.
In an identical manner, a cascade of backward Kalman
filters generates a process xÃÑ+ (t) based on the backward
stochastic realization (78) and the observation windows [t, T ].
Assuming that there are observations in a final interval ending
at t = T , on that interval the Kalman estimate
‚ó¶

+

a0 xÃÑ+ (t) = EHt a0 xÃÑ(t),

(85)

‚ó¶

with initial observation space H+
t := H[t,T ] , is generated by
the backward Kalman filter
dxÃÑ+ = ‚àíA(t)0 xÃÑ+ (t)dt

When observations are available on the interval [0, t1 ], the
Kalman filter on that interval is given by

K‚àí = (Q‚àí C 0 + BD0 )R‚àí1

(83)

+ KÃÑ+ (t)(dy(t) ‚àí CÃÑ(t)xÃÑ+ (t)dt)
0

0

‚àí1

KÃÑ+ = ‚àí(QÃÑ+ CÃÑ ‚àí BÃÑD )R
QÃÑÀô + = ‚àíA0 QÃÑ+ ‚àí QÃÑ+ A + KÃÑ+ R(t)KÃÑ+ (t)0 ‚àí BÃÑ BÃÑ 0

(86a)
(86b)
(86c)

(82b)

and initial conditions xÃÑ+ (T ) = 0 and QÃÑ+ (T ) = PÃÑ (T ) for xÃÑ+
and the error covariance

(82c)

QÃÑ+ (t) := E{[xÃÑ(t) ‚àí xÃÑ+ (t)][xÃÑ(t) ‚àí xÃÑ+ (t)]0 },

(87)

9

which like Q‚àí (t) is positive definite for all t. During periods
of no observations of dy, we then set the gain KÃÑ+ = 0. This
update is obtained from the backward time stochastic model
(74) in an identical manner to that of (84).
Consequently, both the underlying process as well as the
filter can run in either time-direction. This duality becomes
essential in subsequent sections where we will be concerned
with smoothing and interpolation.
B. Observing y
Now consider the case that y, and note merely dy, is
available for observation on all intervals Ik , k = 1, 2, . . . , ŒΩ.
Under this scenario and with a continuous-time process the
dynamics of Kalman filtering become hybrid, requiring both
continuous-time filtering when data is available as well as a
discrete-time update across intervals where measurements are
not available.
Then on the first interval [0, t1 ] the Kalman estimate (82)
will still be valid. However, when t reaches the endpoint t2
of the interval of no information and an observation of y is
obtained again, the subspace of observed data becomes
‚ó¶

where ‚àÜy := y(t2 ) ‚àí y(t1 ). Computing x(t2 ) across the window (t1 , t2 ] as a function of x(t1 ) and the noise components
we have that
Z t2
x(t2 ) = Œ¶(t2 , t1 ) x(t1 ) +
Œ¶(t2 , s)Bdw(s)
| {z }
t1
|
{z
}
Ad
u1 (t1 )

while
t2

y(t2 ) = y(t1 ) +

Z

t2

C(t)x(t)dt +
t1

D(t)dw(t).
t1

Therefore,
Z

t2

‚àÜy =
t1

|

C(t)Œ¶(t, t1 )dt) x(t1 ) + u2 (t1 )
{z
}
Cd

where
Z

t2

u2 (t1 ) =

Z

t1

C(t)

Œ¶(t, s)B(s)dw(s)dt
Z t2
+
D(s)dw(s)
t1

Z t2 Z t2
=
C(t)Œ¶(t, s)dtB(s) + D(s) dw(s).
t1
| t
{z
}
t1


u(t1 ) =

  
Bd
u1 (t1 )
=
v(t1 )
u2 (t1 )
Dd

and Bd and Dd are chosen so that


Bd
Dd




Bd0 , Dd0 =

Z

t2

t1

Œ¶(t2 , s)BB 0 Œ¶(t, s) Œ¶(t2 , s)BM (s)0
M (s)B 0 Œ¶(t2 , s)0
M (s)M (s)0


ds

while E{v(t1 )v(t1 )0 } = I.
Hence, across the window of missing data the Kalman state
estimate x‚àí is now generated by a discrete-time Kalman-filter
step
x‚àí (t2 ) = Ad x‚àí (t1 ) + Kd (‚àÜy ‚àí Cd x‚àí (t1 ))
Kd =

(Ad Q(t1 )Cd0

+

(89a)

Bd Dd0 )

√ó (Cd Q(t1 )Cd0 + Dd Dd0 )‚àí1

(89b)

with initial conditions x‚àí (t1 ) and Q(t1 ) given by (82) and
the error covariance at t2 by
Q(t2 ) = Ad Q(t1 )A0d ‚àí Kd (Cd Q(t1 )Cd0
+ Dd Dd0 )Kd0 + Bd Bd0 .

(89c)

In the next interval [t2 , t3 ], where observations of y are
available, the new Kalman estimate (81) with

Ht‚àí2 = H‚àí
t1 ‚à® H(‚àÜy),

Z

where

t

M (s)

Thus, we obtain the discrete-time update
x(t2 ) = Ad x(t1 ) + Bd v(t1 )

(88a)

‚àÜy = Cd x(t1 ) + Dd v(t1 )

(88b)

‚ó¶

H+
t = H[0,t1 ] ‚à® H(‚àÜy) ‚à® H[t2 ,t]
is again generated by the continuous-time Kalman filter (82)
starting from x‚àí (t2 ) and Q(t2 ) given by (89).
Again given an observation pattern, where intermittently
y becomes unavailable for observation, the Kalman estimate
(81) can be generated in precisely this manner by a cascade
of continuous and discrete-time Kalman filters.
Remark 2: The observation pattern of a continuous-time
stochastic model, where y becomes unavailable over particular
time-windows, is closely related to hybrid stochastic models
where continuous-time diffusion is punctuated by discrete-time
transitions. Indeed, unless interpolation of the statistics within
windows of unavailable data is the goal, the end points of such
intervals can be identified and the same hybrid model utilized
to capture the dynamics.
Remark 3: A common engineering scenario is the case
where the signal is lost while the observation noise is still
present. This amounts to having C ‚â° 0 over the corresponding
window, and the Kalman estimates are obtained by merely
running the filters (82) and (86) in the two time directions with
the modified condition on C. This situation does not cover
the information patterns discussed above since, whenever
BD0 6= 0, the Kalman gains do not vanish and information
about the state process is available even when C is zero.

10

C. Smoothing
Given these intermittent forward and backward Kalman
estimates, we shall derive a formula for the smoothing estimate
‚ó¶

a0 xÃÇ(t) := EH a0 x(t),

a ‚àà Rn ,

EX+ (t) a0 x‚àí (t) = E{a0 x‚àí (t)xÃÑ+ (t)}PÃÑ+ (t)‚àí1 xÃÑ+ (t)
(90)

valid for both the cases discussed above, where
‚ó¶

H :=

‚ó¶

‚àí
Ht

‚à®

‚ó¶

+
Ht

‚äÇ H[0,T ] (dy)

(91)

is the complete subspace of observations. This is discussed
next.

Consider the system (75), and let X(t) be the (finitedimensional) subspace in H spanned by the components of the
stochastic state vector x(t). Then it can be shown [18, Chapter
7] that H[0,t] (dy) ‚ä• H[t,T ] (dy) | Xt , where A ‚ä• B | X
denotes the conditional orthogonality
X

hŒ± ‚àí E Œ±, Œ≤ ‚àí E Œ≤i = 0 for all Œ± ‚àà A, Œ≤ ‚àà B.

(92)

Next, let X‚àí (t) and X+ (t) be the subspaces spanned by the
components of the (intermittent) Kalman estimates x‚àí (t) and
‚ó¶
xÃÑ+ (t), respectively. Then since X‚àí (t) ‚äÇ H‚àí
t ‚äÇ H[0,t] (dy)
and X+ (t) ‚äÇ

‚ó¶

+
Ht

‚äÇ H[t,T ] (dy), we have
X‚àí (t) ‚ä• X+ (t) | X(t),

which is equivalent to
EX+ (t) a0 x‚àí (t) = EX+ (t) EX(t) a0 x‚àí (t),

a ‚àà Rn

(93a)

EX+ |X‚àí

‚àí‚Üí

EX |X‚àí&

where x+ (t) := PÃÑ+ (t)‚àí1 xÃÑ+ (t) is the dual basis in X+ (t) such
that E{x+ (t)xÃÑ+ (t)0 } = I. Moreover,
EX(t) a0 x‚àí (t) = E{a0 x‚àí (t)x(t)0 }P (t)‚àí1 x(t)
= a0 E{x‚àí (t)x(t)0 }xÃÑ(t) = a0 P‚àí (t)xÃÑ(t),

= b0 E{xÃÑ(t)xÃÑ+ (t)}x+ (t)
= b0 PÃÑ+ (t)x+ (t),
by condition (ii), and consequently
EX+ (t) E X(t) a0 x‚àí (t) = a0 P‚àí (t)PÃÑ+ (t)x+ (t).

Remark 5: The proof of condition (iii) in Lemma 4 could
be simplified if xÃÑ+ were a regular backward Kalman estimate
without intermittent loss of information. In this case, x+ =
PÃÑ+‚àí1 xÃÑ+ would be generated by a forward stochastic realization
belonging to the same class as (75) and E{xÃÑ+ (t)x‚àí (t)0 } =
PÃÑ+ (t) E{x+ (t)x‚àí (t)} = PÃÑ+ (t) E{x‚àí (t)x‚àí (t)}.



a ‚àà Rn ,

(96)

(93b)

X

where H
t is the subspace
H
t = X‚àí (t) ‚à® X+ (t).

commutes, where the argument t has been suppressed.
Lemma 4: Let x(t), xÃÑ(t), x‚àí (t) and xÃÑ+ (t) be defined as
above. Then, for each t ‚àà [0, T ],
(i) E{x(t)x‚àí (t)0 } = P‚àí (t)
(ii) E{xÃÑ(t)xÃÑ+ (t)0 } = PÃÑ+ (t)
(iii) E{xÃÑ+ (t)x‚àí (t)0 } = PÃÑ+ (t)P‚àí (t),
where P‚àí (t) := E{x‚àí (t)x‚àí (t)0 } is the state covariance of the
Kalman estimate x‚àí (t) and P+ (t) := E{xÃÑ+ (t)xÃÑ+ (t)0 } is the
covariance of the backward Kalman estimate xÃÑ+ (t).
Proof: By the definition of the Kalman filter, (81) holds,
and consequently the components of the estimation error
x(t) ‚àí x‚àí (t) are orthogonal to H‚àí
t and hence to the components of x‚àí (t). Therefore,
E{x(t)x‚àí (t)0 } = E{x‚àí (t)x‚àí (t)0 } = P‚àí (t),

(95)

Then condition (iii) follows from (93a), (94) and (95).

a0 xÃÇ(t) = E Ht a0 x(t),

X+
%EX+ |X

EX+ (t) b0 xÃÑ(t) = E{b0 xÃÑ(t)xÃÑ+ (t)}PÃÑ+ (t)‚àí1 xÃÑ+ (t)

Lemma 6: For each t ‚àà [0, T ], the smoothing estimate
xÃÇ(t), defined by (90), is given by

[18, Proposition 2.4.2]. Therefore the diagram
X‚àí

(94)

= a0 E{x‚àí (t)xÃÑ+ (t)0 }x+ (t),

where we have used condition (i) and (76). Next, set b := P‚àí a
and form

VI. G EOMETRY OF FUSION

X

proving condition (i). Condition (ii) follows from a symmetric
argument. To prove (iii) we use condition (93). To this end,
first note that, by the usual projection formula,

(97)
‚ó¶

Proof: Following [14], [3], [18], define N‚àí (t) := H‚àí
t 	
‚ó¶
+
+
X‚àí (t) and N (t) := Ht 	 X+ (t). Then
‚ó¶

‚àí

+
H = N (t) ‚äï Ht ‚äï N (t).
‚ó¶

Now, a0 (x(t) ‚àí x‚àí (t)) is orthogonal to H‚àí
t and hence to
N‚àí (t). Also a0 x‚àí (t) ‚ä• N‚àí (t). Hence a0 x(t) ‚ä• N‚àí (t) as
well. In the same way we see that a0 x(t) ‚ä• N+ (t). Therefore
(96) follows.
Consequently, the information from the two Kalman filters
can be fused into the smoothing estimate
xÃÇ(t) = L‚àí (t)x‚àí (t) + LÃÑ+ (t)xÃÑ+ (t)
for some matrix functions L‚àí and LÃÑ+ .

(98)

11

VII. U NIVERSAL TWO - FILTER FORMULA
To obtain a robust and particularly simple smoothing formula that works also with an intermittent observation pattern,
we assume that the stochastic system (75) has already been
transformed via (58) so that, for all t ‚àà [0, T ],
x(t) = xÃÑ(t)

(99)

Then (102) follows from (98) and (107). To prove (104)
eliminate LÃÑ+ in (106) to obtain
L‚àí (I ‚àí P‚àí PÃÑ+ ) = QÃÑ+ ,
which together with (107) yields
‚àí1
Q‚àí1 = Q‚àí1
‚àí (I ‚àí P‚àí PÃÑ+ )QÃÑ+ .

However,

and therefore

I ‚àí P‚àí PÃÑ+ = QÃÑ+ + Q‚àí ‚àí Q‚àí QÃÑ+ ,
0

P (t) = E{x(t)x(t) } = I = PÃÑ (t).

(100)

Then the error covariances in the filtering formulas of Section V are
Q‚àí = I ‚àí P‚àí

and QÃÑ+ = I ‚àí PÃÑ+ .

(101)

Consequently, x(t), xÃÑ(t), P‚àí (t) and PÃÑ+ (t) are all bounded in
norm by one for all t ‚àà [0, T ].
Theorem 7: Suppose that (99) holds. For every t ‚àà [0, T ],
we have the formula

xÃÇ(t) = Q(t) Q‚àí (t)‚àí1 x‚àí (t) + QÃÑ+ (t)‚àí1 xÃÑ+ (t)
(102)
for the smoothing estimate (90), where the estimation error

0	
Q(t) := E (x(t) ‚àí xÃÇ(t)) (x(t) ‚àí xÃÇ(t))
(103)
is given by
Q(t)‚àí1 = Q‚àí (t)‚àí1 + QÃÑ+ (t)‚àí1 ‚àí I,

and hence (104) follows.
In the special case with no loss of observation this is a
normalized version of the Mayne-Frazer two-filter formula [1],
[2], which however in [1], [2] was formulated in terms of
x‚àí and x+ rather than xÃÑ+ , where x+ is the state process of
the forward stochastic system of the backward Kalman filter.
(For the corresponding formula in terms of x‚àí and xÃÑ+ , see
[3], [18]; also cf. [21], where an independent derivation was
given.) With a single interval of loss of observation the formula
(102) reduces to a version of the interpolation formulas in
[6]. The remarkable fact, discovered here, is that the same
formula (102) holds for any intermittent observations structure
and by a cascade of continuous and discrete-time forward and
backward Kalman filters, as needed depending on the assumed
information pattern.

(104)

and where x‚àí , xÃÑ+ , Q‚àí and Q+ are given by (82) and (86)
with boundary conditions x‚àí (0) = xÃÑ+ (T ) = 0 and Q‚àí (0) =
Q+ (T ) = I.
Proof: Clearly the matrix functions L‚àí and LÃÑ+ in (98)
can be determined from the orthogonality relations
E{[x(t) ‚àí xÃÇ(t)]x‚àí (t)0 } = 0

(105a)

E{[x(t) ‚àí xÃÇ(t)]xÃÑ+ (t)0 } = 0.

(105b)

and
By Lemma 4, (105) yields
P‚àí ‚àí L‚àí P‚àí ‚àí LÃÑ+ PÃÑ+ P‚àí = 0
PÃÑ+ ‚àí L‚àí P‚àí PÃÑ+ ‚àí LÃÑ+ PÃÑ+ = 0,

VIII. R ECAP OF COMPUTATIONAL STEPS
Given a system (75) with state covariance (55), make the
normalizing substitution
1

1

A(t) ‚Üê P (t)‚àí 2 A(t)P (t) 2 + R(t)
1

B(t) ‚Üê P (t)‚àí 2 B(t)

C(t) ‚Üê C(t)P (t)
h
i
1
1
with R(t) = ddt P (t)‚àí 2 P (t) 2 . Next, we compute the
intermittent forward and backward Kalman filter estimates x‚àí
and xÃÑ+ , respectively, along the lines of Section V, where,
due to the normalization, Q‚àí (0) = QÃÑ+ (T ) = In . Then the
smoothing estimate is given by

xÃÇ(t) = Q(t) Q‚àí (t)‚àí1 x‚àí (t) + QÃÑ+ (t)‚àí1 xÃÑ+ (t) ,
where

which, in view of the fact that P‚àí and PÃÑ+ are positive definite,
yields
L‚àí + LÃÑ+ PÃÑ+ = I
(106a)
L‚àí P‚àí + LÃÑ+ = I

(106b)

Again by orthogonality and Lemma 4,
Q = E {(x ‚àí xÃÇ) x0 } = I ‚àí L‚àí P‚àí ‚àí LÃÑ+ PÃÑ+ ,
which, in view of (106) and the relations (101), yields
L‚àí =

QQ‚àí1
‚àí

and

LÃÑ+ =

QQÃÑ‚àí1
+ .

(107)

(108)

1
2

Q(t) = Q‚àí (t)‚àí1 + QÃÑ+ (t)‚àí1 ‚àí I

‚àí1

.

IX. A N EXAMPLE
We now illustrate the results of the paper on a specific
numerical example. We consider the continuous-time diffusion
process
dx1 (t)

= x2 (t)dt

dx2 (t)

= ‚àí0.3x1 (t)dt ‚àí 0.7x2 (t)dt + dw(t)

dy(t)

= x1 (t)dt + dv(t)

12

where w and v are thought to be independent standard Wiener
processes. Here, x1 is thought of as position and x2 as velocity
of a particle that is steered by stochastic excitation in dw, in the
presence of a restoring force 0.3x1 and frictional force 0.7x2 .
Then dy/dt represents measurement of the position and dv/dt
represents measurement noise (white).

1

y

0
-50

0
0

5

10

15

20

25

30

35

40

45

dy

0.5

-3
0

5

10

15

20

25

30

35

40

0

5

10

15

20

25

30

35

40

45

x1
x2

10

15

20

25

30

35

40

45

x 2,est
x2

0
-2

0
0

5

10

15

20

25

30

35

40

45

Fig. 5: Sample paths of output process, increment, and state
processes

-4

0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 7: Kalman estimates in the backward time direction
2

KF forward estimate x - missing data in blue intervals
1

2

1

1

0

0

-1

-1

-2

x est,1
x1

-2
0

5

10

15

20

25

30

35

40

-3

0

5

10

15

20

25

30

35

40

45

Smoothed state estimates
4
x 2,smooth
x2

2

x est,2
x2

2

x 1,smooth
x1

45

KF forward estimate x 2 - missing data in blue intervals

4

0

0

-2

-2
-4

5

KF backward estimate x 2 - missing data in blue intervals

2

5

-3

0

4

0

-5

x 1,est
x1

45

5

-5

-1
-2

0
-0.5

KF backward estimate x 1 - missing data in blue intervals

2

Output process and states

50

of intervals, data are not made available for state estimation;
these intervals where data are not to be used are marked by
a thick blue baseline in the figures. In Figure 5 we display
sample paths of the output process y, increments dy, and stateprocesses x1 and x2 .

-4
0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 6: Kalman estimates in the forward time direction
Numerical simulation over [0, T ] with T = 45 (units of
time) produces a time-function y(t) which is sampled with
integer multiples of ‚àÜt = 0.01 (units). The interval [0, T ] is
partitioned into
[0, T ] = ‚à™9i=1 [ti‚àí1 , ti ]
where t0 = 0 and ti ‚àí ti‚àí1 = i (units). Measurements of y
are made available for purposes of state estimation over the
intervals [ti‚àí1 , ti ] for i = 1, 3, 5, 9. Over the complement set

0

5

10

15

20

25

30

35

40

45

Time [sec]

Fig. 8: Interpolation/smoothed estimates by fusion of Kalman
forward and backward estimates
The process increments dy over [ti‚àí1 , ti ] for i = 1, 3, 5, 9
as well as the increments ‚àÜy across the [ti‚àí1 , ti ] for i =
2, 4, 6, 8 are used in the two-filter formula for the purpose
of smoothing. The Kalman estimates for the states in the
forward and backwards in time directions, x‚àí (t) and xÃÑ+ (t)
are shown in Figures 6 and 7, respectively. The fusion of the
two using (102) is shown in Figure 8. It is worth observing the
nature and fidelity of the estimates. In the forward direction,
across intervals where data is not available, x‚àí becomes

13

increasing more unreliable whereas the opposite is true for
xÃÑ+ , as expected. The smoothing estimate is generally an
improvement to those of the two Kalman filters as seen in
Figure 8. In particular, it is worth noting x2 (in subplot 2),
where, over windows of available observations, estimates have
considerably less variance in the middle of the interval where
the weights (Q(t)Q‚àí (t)‚àí1 and Q(t)QÃÑ+ (t)‚àí1 ) in (102) are
equalized, whereas sample paths become increasing rugged
at the two ends where one of the two Kalman estimates has
significantly higher variance, and the corresponding mixing
coefficient becomes relatively smaller.
X. C ONCLUDING REMARKS
Historically the problem of interpolation has been considered from the beginning of the study of stochastic processes
[22], [23]. Early accounts and treatments were cumbersome
and non-explicit as the problem was considered difficult [7],
[8], [9], [10]. In a manner that echoes the development of
Kalman filtering, the problem became transparent and computable for ouput processes of linear stochastic systems [5],
[6], [18].
This paper builds on developments in stochastic realization
theory [11], [24] and presents a unified and generalized twofilter formula for smoothing and interpolation in continuous
time for the case of intermittent availability of data over
an operating window. The analysis considers two alternative
information patterns where increments of the output process or
the output process itself is recorded when information becomes
available. The second information pattern appears most natural
to us in this continuous-time setting, and this is our main
problem. Nevertheless, in either case, two Kalman filters run in
opposite time-directions, designed on the basis of a forward
and a backward model for the process, respectively. Fusion
of the respective estimates is effected via linear mixing in
a manner similar to the Mayne-Fraser formula and applies to
both smoothing and interpolation intermixed. In earlier works,
smoothing and interpolation have been considered separate
problems [18, Chapter 15]. The balancing normalization also
simplifies the mixing formula and makes it completely time
symmetric.
The theory relies on time-reversal of stochastic models.
We provide a new derivation of such a reversal which has the
convenient property of being balanced. It is based on lossless
imbedding of linear systems and effects the time reversal
through a unitary transformation. Interestingly, time symmetry
in statistical and physical laws have occupied some of the most
prominent minds in science and mathematics. In particular,
closer to our immediate interests, dual time-reversed models
have been employed to model, in different time-directions,
Brownian or SchroÃàdinger bridges [25], [26], a subject which is
related to reciprocal processes [27], [28], [29], [30]. A natural

extension of the present work in fact is in the direction of
general reciprocal dynamics [28], [29] and the question of
whether similar two-filter formula are possible.
A PPENDIX : T IME REVERSAL OF NON - STATIONARY
DISCRETE - TIME SYSTEMS
Next, instead of (1), consider the non-stationary state
dynamics
x(t + 1) = A(t)x(t) + B(t)w(t),

x(0) = x0 ,

(109)

on a finite time-window [0, T ], where, for simplicity we
now assume that the covariance matrix P0 := P (0) of
the zero-mean stochastic vector x0 is positive definite, i.e.,
P0 = E{x0 x00 } > 0. Then the state covariance matrix
P (t) := E{x(t)x(t)0 } will satisfy the Lyapunov difference
equation
P (t + 1) = A(t)P (t)A(t)0 + B(t)B(t)0 .

(110)

The state transformation
1

Œæ(t) = P (t)‚àí 2 x(t)

(111)

brings the system (109) into the form
Œæ(t + 1) = F (t)Œæ(t) + G(t)w(t),

(112)

where now E{Œæ(t)Œæ(t)0 } = In for all t and
1

1

F (t) = P (t + 1)‚àí 2 A(t)P (t) 2 ,
G(t) = P (t + 1)

‚àí 12

B.

(113a)
(113b)

The Lyapunov difference equation then reduces to
In = F (t)F (t)0 + G(t)G(t)0

(114)

allowing us to embed [F, G] as part of a time-varying orthogonal matrix


F (t) G(t)
U (t) =
.
(115)
H(t) J(t)
This amounts to extending (112) to
Œæ(t + 1) = F (t)Œæ(t) + G(t)w(t)

(116a)

wÃÑ(t) = H(t)Œæ(t) + J(t)w(t),

(116b)

or, in the equivalent form




Œæ(t + 1)
Œæ(t)
= U (t)
.
wÃÑ(t)
w(t)

(117)

Hence, since E{Œæ(t)Œæ(t)0 } = In and E{w(t)w(t)0 } = Ip , and
assuming that E {Œæ(t)w(t)0 } = 0,
(

0 )
Œæ(t + 1) Œæ(t + 1)
E
= U (t)U (t)0 = In+p , (118)
wÃÑ(t)
wÃÑ(t)
which yields
E{Œæ(t + 1)wÃÑ(t)0 } = 0,
0

E{wÃÑ(t)uÃÑ(t) } = Ip .

(119a)
(119b)

14

where x0 and the normalized white-noise process w are
uncorrelated and E{x0 x00 } = P0 . In fact, inserting the transformations (122) and (123a) into (124b) yields

Moreover, from (116) we have
uÃÑ(t + k) = H(t + k)Œ¶(t + k, t)Œæ(t)
+

t+k‚àí1
X

H(t + k)Œ¶(t + k, j + 1)G(j)w(j) + J(t)w(t)

y(t) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t),

j=t

for k > 0, where
(
F (s ‚àí 1)F (s ‚àí 2) ¬∑ ¬∑ ¬∑ F (t) for s > t
Œ¶(s, t) =
In for s = t.

where

Therefore, since F (t)H(t)0 + G(t)J(t)0 = 0 by the unitarity
of U (t),

From that we have the backward system

CÃÑ = C(t)P (t)A(t)0 + D(t)B(t)0
DÃÑ = C(t)P (t)BÃÑ(t) + D(t)J(t)

0

E{uÃÑ(t + k)uÃÑ(t) }
= H(t + k)Œ¶(t + k, t + 1)[F (t)H(t)0 + G(t)J(t)0 ] = 0.
Consequently, uÃÑ is a white noise process. Finally, premultiplying (117) by U (t)0 , we then obtain
Œæ(t) = F (t)0 Œæ(t + 1) + H(t)0 wÃÑ(t)
0

0

w(t) = G(t) Œæ(t + 1) + J(t) wÃÑ(t),

(120a)

0

(125)
(126)

xÃÑ(t ‚àí 1) = A(t)0 xÃÑ(t) + BÃÑ(t)wÃÑ(t)

(127a)

y(t) = CÃÑ(t)xÃÑ(t) + DÃÑ(t)wÃÑ(t)

(127b)

with the boundary condition xÃÑ(T ‚àí 1) = P (T )‚àí1 x(T ) being
uncorrelated to the white-noise process wÃÑ.

R EFERENCES

(120b)

which, in view of (119), is a backward stochastic system.

[1] D. Q. Mayne, ‚ÄúA solution of the smoothing problem for linear dynamic
systems,‚Äù Automatica, vol. 4, pp. 73‚Äì92, 1966.

Using the transformation (111), (116) yields the forward
representation

[2] D. Fraser and J. Potter, ‚ÄúThe optimum linear smoother as a combination
of two optimum linear filters,‚Äù Automatic Control, IEEE Transactions
on, vol. 14, no. 4, pp. 387‚Äì390, 1969.

x(t + 1) = A(t)x(t) + B(t)w(t)

[3] F. A. Badawi, A. Lindquist, and M. Pavon, ‚ÄúA stochastic realization
approach to the smoothing problem,‚Äù IEEE Trans. Automat. Control,
vol. 24, no. 6, pp. 878‚Äì888, 1979.

wÃÑ(t) = BÃÑ(t)0 x(t) + J(t)w(t),

(121a)
(121b)

1

where BÃÑ(t) := P (t)‚àí 2 H(t)0 . Likewise (120) and
xÃÑ(t) = P (t + 1)‚àí1 x(t + 1),

(122)

yields the backward representation
xÃÑ(t ‚àí 1) = A(t)0 xÃÑ(t) + BÃÑ(t)wÃÑ(t)
w(t) = B(t)0 xÃÑ(t) + J(t)0 wÃÑ(t).

(123a)
(123b)

Remark 8: When considered on the doubly infinite time
axis, equation (117) defines an isometry. Indeed, assuming that
the input is squarely summable, the fact that U (t) is unitary
for all t directly implies that
N
X

2

2

kwÃÑk + kŒæ(t + 1)k =

‚àí‚àû

N
X

Then, Œæ(t) ‚Üí 0 as t ‚Üí ‚àû, provided Œ¶(t, s) ‚Üí 0 as s ‚Üí ‚àí‚àû.
It follows that

t=‚àí‚àû

kwÃÑ(t)k2 =

‚àû
X

kw(t)k2 .

t=‚àí‚àû

We are now in a position to derive a backward version of
a non-stationary stochastic system
x(t + 1) = A(t)x(t) + B(t)w(t),
y(t) = C(t)x(t) + D(t)w(t)

x(0) = x0

[5] M. Pavon, ‚ÄúNew results on the interpolation problem for continuoustime stationary increments processes,‚Äù SIAM journal on Control and
Optimization, vol. 22, no. 1, pp. 133‚Äì142, 1984.
[6] ‚Äî‚Äî, ‚ÄúOptimal interpolation for linear stochastic systems,‚Äù SIAM journal on Control and Optimization, vol. 22, no. 4, pp. 618‚Äì629, 1984.
[7] K. Karhunen, Zur Interpolation von stationaÃàren zufaÃàlligen Funktionen.
Suomalainen tiedeakatemia, 1952.
[8] Y. Rozanov, Stationary random processes. Holden-Day, San Francisco,
1967.
[9] P. Masani, ‚ÄúReview: Yu.A. Rozanov, stationary random processes,‚Äù The
Annals of Mathematical Statistics, vol. 42, no. 4, pp. 1463‚Äì1467, 1971.

kw(t)k2 .

‚àí‚àû

‚àû
X

[4] F. Badawi, A. Lindquist, and M. Pavon, ‚ÄúOn the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear
stochastic systems,‚Äù in Decision and Control including the Symposium
on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE,
1979, pp. 505‚Äì510.

(124a)
(124b)

[10] H. Dym and H. P. McKean, Gaussian processes, function theory, and
the inverse spectral problem. Courier Dover Publications, 2008.
[11] A. Lindquist and G. Picci, ‚ÄúOn the stochastic realization problem,‚Äù SIAM
J. Control Optim., vol. 17, no. 3, pp. 365‚Äì389, 1979.
[12] ‚Äî‚Äî, ‚ÄúForward and backward semimartingale models for Gaussian
processes with stationary increments,‚Äù Stochastics, vol. 15, no. 1, pp.
1‚Äì50, 1985.
[13] ‚Äî‚Äî, ‚ÄúRealization theory for multivariate stationary Gaussian processes,‚Äù SIAM J. Control Optim., vol. 23, no. 6, pp. 809‚Äì857, 1985.
[14] ‚Äî‚Äî, ‚ÄúA geometric approach to modelling and estimation of linear
stochastic systems,‚Äù J. Math. Systems Estim. Control, vol. 1, no. 3, pp.
241‚Äì333, 1991.

15

[15] A. Lindquist and M. Pavon, ‚ÄúOn the structure of state-space models
for discrete-time stochastic vector processes,‚Äù IEEE Trans. Automat.
Control, vol. 29, no. 5, pp. 418‚Äì432, 1984.
[16] G. Michaletzky, J. Bokor, and P. VaÃÅrlaki, Representability of stochastic
systems. Budapest: AkadeÃÅmiai KiadoÃÅ, 1998.
[17] G. Michaletzky and A. Ferrante, ‚ÄúSplitting subspaces and acausal
spectral factors,‚Äù J. Math. Systems Estim. Control, vol. 5, no. 3, pp.
1‚Äì26, 1995.
[18] A. Lindquist and G. Picci, Linear Stochastic Systems: A Geometric
Approach to Modeling, Estimation and Identification. Springer-Verlag,
Berlin Heidelberg, 2015.
[19] T. T. Georgiou, ‚ÄúThe CaratheÃÅodory‚ÄìFejeÃÅr‚ÄìPisarenko decomposition and
its multivariable counterpart,‚Äù Automatic Control, IEEE Transactions on,
vol. 52, no. 2, pp. 212‚Äì228, 2007.

vol. 4, no. 4, pp. 173‚Äì178, 1949.
[24] M. Pavon, ‚ÄúStochastic realization and invariant directions of the matrix
Riccati equation,‚Äù SIAM Journal on Control and Optimization, vol. 18,
no. 2, pp. 155‚Äì180, 1980.
[25] M. Pavon and A. Wakolbinger, ‚ÄúOn free energy, stochastic control, and
SchroÃàdinger processes,‚Äù in Modeling, Estimation and Control of Systems
with Uncertainty. Springer, 1991, pp. 334‚Äì348.
[26] P. Dai Pra and M. Pavon, ‚ÄúOn the Markov processes of SchroÃàdinger,
the Feynman-Kac formula and stochastic control,‚Äù in Realization and
Modelling in System Theory. Springer, 1990, pp. 497‚Äì504.
[27] B. Jamison, ‚ÄúReciprocal processes,‚Äù Probability Theory and Related
Fields, vol. 30, no. 1, pp. 65‚Äì86, 1974.

[20] T. Georgiou and A. Lindquist, ‚ÄúOn time-reversibility of linear stochastic
models,‚Äù arXiv preprint arXiv:1309.0165, 2013.

[28] A. Krener, ‚ÄúReciprocal processes and the stochastic realization problem
for acausal systems,‚Äù in Modelling, Identification and Robust Control,
C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986,
pp. 197‚Äì211.

[21] J. E. Wall Jr, A. S. Willsky, and N. R. Sandell Jr, ‚ÄúOn the fixedinterval smoothing problem,‚Äù Stochastics: An International Journal of
Probability and Stochastic Processes, vol. 5, no. 1-2, pp. 1‚Äì41, 1981.

[29] B. C. Levy, R. Frezza, and A. J. Krener, ‚ÄúModeling and estimation of
discrete-time Gaussian reciprocal processes,‚Äù Automatic Control, IEEE
Transactions on, vol. 35, no. 9, pp. 1013‚Äì1023, 1990.

[22] A. N. Kolmogorov, Stationary sequences in Hilbert space. John Crerar
Library National Translations Center, 1978.

[30] P. Dai Pra, ‚ÄúA stochastic control approach to reciprocal diffusion
processes,‚Äù Applied mathematics and Optimization, vol. 23, no. 1, pp.
313‚Äì329, 1991.

[23] A. M. Yaglom, ‚ÄúOn problems about the linear interpolation of stationary
random sequences and processes,‚Äù Uspekhi Matematicheskikh Nauk,

1688

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Universal Regulators for Optimal Tracking
in Discrete-Time Systems Affected
by Harmonic Disturbances
Anders Lindquist, Fellow, IEEE, and Vladimir A. Yakubovich, Member, IEEE

Abstract‚Äî The authors consider the problem of controlling a
discrete-time linear system by output feedback so as to have a
second output z t track an observed reference signal r t . First, as a
preliminary, we consider the problem of asymptotic tracking, i.e.,
to design a regulator such that jz t 0 rt j ! 0. This problem has
been studied intensely in the literature, mainly in the continuoustime case. It is known that only under very special conditions
does there exist a linear regulator which achieves this design
goal and which is universal in the sense that it works for all
reference signals and does not depend on them. On the other
hand, if rt is a harmonic signal with known frequencies but
with unknown amplitudes and phases, there exist such regulators
under mild conditions, provided the dimension of rt is no larger
than the number of controls. This is true even if the plant itself
is corrupted by an unobserved additive harmonic disturbance wt
of the same type as rt , if the dimension of wt is no larger than
the number of outputs available for feedback control.
However, if the first dimensionality condition is not satisfied,
asymptotic tracking is not possible, but a steady-state tracking
error remains. Therefore, the authors turn to another approach
to the tracking problem, which also allows for damping of
other system and control variables, and this is our main result.
The measure of performance is given by a natural quadratic
cost function. The object is to design an optimal regulator
which is universal in the sense that it does not depend on the
unknown amplitudes and phases of rt and wt and is optimal
for all choices of rt and wt . The authors prove that an optimal
universal regulator exists in a wide class of stabilizing and
possibly nonlinear regulators under natural technical conditions
and that this regulator is in fact linear, provided that the second
dimensionality condition above is satisfied. On the other hand, if
it is not satisfied, the existence of an optimal universal regulator is
not a generic property, so as a rule no optimal universal regulator
exists.
The authors provide complete solutions of all the problems
described above.
Index Terms‚ÄîInternal model principle, optimal tracking, optimal universal regulators, sinusoidal disturbance.

Fig. 1. Feedback configuration.

(1b)
(1c)
, two vector outputs
and
,
with a state
and an
and two vector inputs, namely a control
which we shall take to be
unobserved disturbance
harmonic with known frequencies but unknown amplitudes
and phases. More precisely
(2)
where the frequencies
(3)
,
,
are known, but the complex vector amplitudes
,
, in which the phases have been absorbed, are either
completely unknown or zero. Consequently, some frequencies
and have been included for
(3) may not be represented in
notational purposes to be explained shortly.
In this paper we consider the problem to control the system
(1) by feedback from the output so as to have the output
track an observed -dimensional real reference signal

I. INTRODUCTION

C

(4)

ONSIDER a discrete-time linear control system
(1a)

Manuscript received January 4, 1996; revised February 1, 1997 and April
6, 1998. Recommended by Associate Editor, S. Weiland. This work was
supported in part by grants from NFR, INTAS, and NUTEK.
A. Lindquist is with the Division of Optimization and Systems Theory,
Royal Institute of Technology, 100 44 Stockholm, Sweden.
V. A. Yakubovich is with the Department of Mathematics and Mechanics,
St. Petersburg University, St. Petersburg 198904, Russia.
Publisher Item Identifier S 0018-9286(99)07132-9.

which is harmonic with the known frequencies (3) but with
,
,
,
which are
complex vector amplitudes
either completely unknown or zero so that certain frequencies
(3) may not occur in . The feedback configuration of this
problem is described in the flow diagram as shown in Fig. 1.
Many important engineering problems could be modeled in
this way. Some examples are connected to industrial machines
and helicopters [2], [9]‚Äì[12], [27], [28], control of aircraft in

0018‚Äì9286/99$10.00 Ô£© 1999 IEEE

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

the presence of wind shear [19], [23], [31], and control of the
roll motion of a ship [14].
For notational convenience we use a common set of freand , forcing us to set certain complex
quencies (3) for
vector amplitudes equal to zero. To formalize this we introduce
,
of for which
the index sets
and
, respectively, are nonzero and arbitrary. Then
and

(5)

Without loss of generality we assume that

Accordingly, we define the class
of disturbances and the
and ,
class of reference signals consisting of all signals
and
respectively, obtained by letting
vary arbitrarily subject to the constraint that the signals (5)
are real.
, and
are constant
We assume that , , , ,
real matrices of appropriate dimensions such that
is stabilizable and
is detectable. Without loss of
generality we may also assume that
and

(6)

In fact, if the first condition is not satisfied, some components
of could be eliminated. Moreover, if has linearly dependent columns, these could be combined without restriction.
and
.
Clearly, (6) implies that
Now, a possible criterion of performance for the tracking
problem described above is given by
(7)
but, to allow for damping of internal system variables and
the energy of control, we shall also consider a more general
criterion of the type
(8)
where

is a real quadratic form
(9)

with properties to be specified in Section V. [To ensure that the
, we must of course introduce some
infimum of is not
condition on the quadratic form (9).] We note that the second
functional (8) becomes a measure not only of the tracking
accuracy but also of the forced oscillations in the closed-loop
system. For the classes of admissible regulators to be defined
next, these cost functions do not depend on initial conditions.
, a regulator
The object is to find, for suitable ,

1689

satisfies the weak stability condition
as

2) optimal in the sense that the cost function (8) is
minimized;
3) universal in the sense that it simultaneously solves the
complete family of optimization problems corresponding to different values of the complex vector amplitudes
and
and thus does not depend
on these amplitudes.
Such a regulator will be referred to as an optimal universal
regulator (OUR), and the class of regulators (10) satisfying
conditions 1) and 2) will be denoted . The stability condition
(11) may at first sight seem somewhat unnatural, but, as we
shall see in Section VI, it is the natural mathematical condition
for which statements of necessity
defining the largest class
and sufficiency can be made.
Removing the last term of (8) related to tracking we obtain
some special cases of this problem which were studied in [21]
and in [22] for the cases of complete and incomplete state
information, respectively.
In this paper we show that, under suitable technical con, the problem stated above has
ditions and provided
a solution in , and this solution happens to be a linear
stabilizing regulator of type
(12)
is the backward shift
and
,
where
, and
are real matrix polynomials, of dimensions
,
, and
, respectively, with the property that
and
and
are proper rational
functions so that the regulator is nonanticipatory in the sense
does not depend on future values of
and , in
that
the subclass of
harmony with (10). We shall denote by
such linear regulators. Existence of an OUR in the subclass
itself can be established under somewhat milder technical
is important.
conditions. The dimensionality condition
As in [22], it can be shown that if it fails, then the existence of
an optimal universal regulator becomes a nongeneric property.
It means that no optimal universal regulator exists from a
.
practical point of view if
The cost function (7) would of course be minimized if we
could control (1a) so that
as

(13)

In fact, it would be zero. Therefore, asymptotic tracking
appears as a special case in our analysis. This problem has been
studied intensely in the literature, at least in the continuoustime case; see, e.g., [1], [4]‚Äì[8], [13], [16], and the references
therein. The connection to this earlier work, developed in
continuous time, is made evident by noting that the disturbance
and reference signals (5) can be modeled as the output of a
critically stable system

(10)
which is:
sat1) stabilizing in the sense that any process
isfying the closed-loop system equations (1), (10) also

(11)

with

having all its eigenvalues on the unit circle.

1690

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Therefore, we begin by developing our optimization procedure in this well-known setting of asymptotic tracking,
thereby obtaining alternative formulations in the discrete-time
case. Using a very short and simple proof, we are able
to give a complete solution to the problem of finding all
universal tracking regulators, i.e., all regulators which achieve
asymptotic tracking (13) for all values of the complex vector
and
and which do not
amplitudes
depend on these amplitudes. This will be done in Section IV.
As a preliminary for this, and to set up notations, in Section III
), and we
we first consider an undisturbed system (
characterize all regulators (12) achieving the design objective
(13) for all reference signals , not only harmonic ones, and
all initial conditions; we shall refer to this property as Tuniversal. The solution of this problem is certainly known,
but we include it for conceptual reasons.
, i.e., the dimension of
is larger than
However, if
the number of outputs available for feedback, no universal
tracking regulator exists, so a nonzero tracking error remains.
To damp this error we turn to our main problem, namely to
characterize all optimal universal regulators, as defined above.
Also, we may want to use a criterion (8) even if asymptotic
tracing is possible, if it is desirable to damp the control energy
and/or some particular internal system variables. This is the
topic of Section V, where optimality in the linear class is
studied. In Section VI we show that these linear universal
regulators are optimal also in the wider class of nonlinear
regulators satisfying (11), provided slightly stronger technical
conditions are satisfied. The complete solution is given. We
note that a similar but different optimization problem, over a
finite horizon, is considered in [26].
Obviously, there is no a priori guarantee that a regulator
which minimizes (8) will also satisfy other design specifications, and hence we look for complete solutions with many
free parameters which then can be tuned by loop shaping. In
fact, all our results are based on a parameterization derived
in Section II, which is akin to that of Youla and KucÃÜera
and which generalizes some parameterizations previously presented in [21] and [22].
Finally, in Section VII, we give some simple numerical
examples.
II. LINEAR STABILIZING AND REALIZABLE REGULATORS

(15b)
(15c)
so, in particular,
(16)
where

is the

matrix polynomial
(17)
,

Similarly, the transfer functions
respectively, are given by

from

to

which stabilize the control system (1) and which are realizable
in a sense to be defined shortly. As before, is the backward
, and
,
, and
are real matrix
shift
,
, and
, respectively.
polynomials of dimensions
Let us consider a bit closer the meaning of (14) being
,
,
stabilizing. To this end, note that the transfer functions
from
to , , and , respectively, in the closed-loop
system (1), (14) satisfy

which together with (16) yields
(19)
We shall say that the regulator (14) is stabilizing if the matrix
is stable, i.e.,
for
.
polynomial
Next we consider the condition that the regulator be realizable. Clearly (14) must be nonanticipatory in the sense that
does not depend on future values of
and . To ensure
this, we must assume that
and

are proper

(20)

.
requiring in particular that
Let us investigate what properties must have for (20) to
be satisfied. To this end, let us introduce the rational transfer
functions

(21)
from the control signal to the outputs
Then it is easy to see that

and

, respectively.

(22)
and that

(23)
Writing (22) in the alternative form

we see that (20) implies that
is strictly proper and
is proper. In fact,
is strictly proper, making
as well as its inverse proper. Then, it follows from
and
are strictly proper also. Consequently
(23) that
where

(15a)

,

(18)

In order to design universal regulators we need a parameterization of all linear regulators
(14)

and

is finite

(24)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

so that
and
depend on
for
only and on
for
only. We shall say that the regulator (14) is realizable
if condition (24) is satisfied. At the end of this section we
shall demonstrate that any stabilizing and realizable regulator
satisfies (20) so that the nonanticipatory property is implied
(Corollary 2.3).
We say that two regulators

and

are equivalent if there are stable
and
such that

matrix polynomials
(25)

Hence we allow the systems matrices , , and to have
stable common factors, as coprimeness is not required. Clearly,
,
,
, and
as can be seen from (22) and (23),
are invariant under this equivalence and so are the regulator
transfer functions (20).
is a stable matrix, i.e.,
From now on, we assume that
for all
. Since
is stabilizable
is detectable, this is no restriction. In fact, it is
and
well-known that the system (1) can be replaced by a similar
system having a stable -matrix but, in general, a larger
dimension. (See any standard text, such as [1] and [18].) Only
under special conditions [15], including the case of complete
state observation, is it possible to do this by constant feedback,
but the system can always be stabilized by a dynamic observer.
Then, extending the state space by including this observer, a
system with stable -matrix is obtained. For these reasons we
shall from now on, without loss of generality, assume that
in (1) is a stable matrix.
The following theorem, generalizing a similar result in
[22], provides a parameterization akin to the well-known
Youla‚ÄìKucÃÜera parameterization. (We note that if is not stable, also the latter parameterization requires an observer-based
prestabilization, increasing the dimension of the regulator; see,
e.g., [32, p. 226].)
be a stable matrix with
Theorem 2.1: Let
being its characteristic polynomial, and let
and
be the matrix polynomials
(26)
be an arbitrary stable scalar polynomial
Moreover, let
and
be arbitrary matrix polynomials of
and let
and
, respectively, such that
dimensions

1691

is stabilizing and realizable, and for this regulator
(30)
and
(31)
is given by (17). Conversely, any stabilizing and
where
realizable regulator (28) is equivalent to one constructed in
this way.
Before turning to the proof of this parameterization, let us
is a
briefly explain the nature of relation (31). Although
for the regulator defined via (29), this is in
factor in
general not the case for an arbitrary regulator belonging the
same equivalence class. In fact, while the closed-loop transfer
and the regulator transfer functions
and
function
are invariant under the equivalence (25),
is not.
Taking the Schur complement, it immediately follows from
(17) that
(32)
is given by (21). Since, in general, the second factor
where
in
is not a polynomial, is of course not a factor in
general. Nevertheless, it will turn out to be useful to represent
each equivalence class by a regulator that has this property.
Proof of Theorem 2.1: In view of (29), we have
(33)
and consequently (30) follows from (22) and (31) follows from
is a stable matrix poly(32). By construction, therefore,
nomial, establishing that the regulator is stabilizing. Moreover,
is strictly proper and
is proper, i.e.,
in view of (27),
and
is finite. It then follows from (23)
and
are strictly proper, and hence the regulator
that
is realizable.
,
,
To prove the converse statement, suppose that
is an arbitrary stabilizing and realizable regulator. Then (32)
may be written

where

is the

matrix polynomial
(34)

which is stable and full rank, since
stable and nontrivial. It follows from (22) that

is
(35)

(27)

and
are the closed-loop transfer functions
where
, ,
. Therefore, setting
corresponding to the regulator

(28)

and

(29)

where
, (35) shows that

Then the regulator

with
is the adjoint matrix polynomial of
and
are given by (30). Since
,

1692

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

,
is a realizable regulator, it follows from (24) that the
and
degree conditions (27) hold. Consequently, defining
via (29), it follows from the first part of the theorem that
, ,
is a stabilizing and realizable regulator with the
and
as
,
,
same closed-loop transfer functions
. It remains to show that
, ,
and
,
,
are
equivalent. To this end, note that

Also it follows from (34) that

Consequently

i.e.,

and

are equivalent as required.

If
so that
, the representation of stabilizing
regulators can be simplified considerably, since and can be
chosen so that cancellations occur. Since this formulation has a
different form and, moreover, will be used later, we state it as
a corollary. Note that, in view of the converse statement, this
corollary is strictly speaking not a special case of Theorem 2.1.
It is in fact a generalization of [21, Lemma 4.3], but the proof
here is new.
Corollary 2.2: Let be a stable matrix, and suppose that
. Let
be an arbitrary real scalar stable polynomial,
and
be arbitrary real matrix polynomials,
and let
and
, respectively, such that
of dimensions
(36)
Then the regulator
(37)

Conversely, by Theorem 2.1, any stabilizing and realizable
regulator (37) is equivalent to some regulator
of
the type described in Theorem 2.1, where we set
everywhere. It remains to show that
is also a
regulator of the type described in the corollary. To this end,
. This implies that
,
define
and hence the equations of Theorem 2.1 become those of the
replaced by . Hence
is also a
corollary with
regulator in the sense of the corollary.
In the beginning of this section we demonstrated that the
realizability condition (24) is a consequence of nonanticipatory
condition (20). Next we show that the converse is also true,
has full rank as assumed in (6).
provided
. Then, for any
Corollary 2.3: Suppose that
stabilizing regulator (28), the realizability condition (24) and
the nonanticipatory condition (20) are equivalent.
Proof: The proof is immediate in the special case
. In fact, for a regulator (37) with
and
given by (38),
condition (20) is a direct consequence of the degree condition
(36). For any other stabilizing regulator (37), it follows from
the definition of equivalence.
The general case follows from the fact that (28) is a subclass
of (37). In fact, writing (28) as

it follows from what has already been proved that
is proper. Since
has full rank, this implies that
is proper follows directly.
proper. That
III.

is

-UNIVERSAL REGULATORS

As a preliminary for the analysis in Sections IV and V,
in this section we consider the problem of controlling the
undisturbed system
(40a)
(40b)
(40c)

with
(38)
is stabilizing and realizable, and, for this regulator
(39)
satisfies (31). Conversely, any stabilizing and
and
realizable regulator (37) is equivalent to one constructed in
this way.
Proof: Let the polynomials and be chosen as in the
and
statement of the corollary, and take
to be the corresponding polynomials
and
in Theorem 2.1. Then, since
, the degree conditions (27) are satisfied for
and .
Moreover, the corresponding regulator polynomials matrices
and
, become
(29), which we denote
and
, where
and
are given by (38). Then,
, the regulator
,
,
is stabilizing and
setting
realizable by Theorem 2.1. Thanks to cancellation, therefore,
is a stabilizing and realizable regulator for the
problem of Corollary 2.2, as claimed.

so that it tracks a given
by feedback from the output
in the sense that
reference signal
as

(41)

As explained in Section II it is no restriction to assume that
is stable if it is assumed that
is stabilizable and
is detectable. The solution of this problem is simple
and certainly known, but we include it for completeness and
for conceptual reasons.
More precisely, we want to find a stabilizing and realizable
regulator of the form
(42)
which is universal for the asymptotic tracking problem in the
sense that (41) holds for all solutions of (40), (42), and all
reference signals . More specifically we shall refer to this
property as T-universal.
Clearly, for (42) to be stabilizing and realizable, the matrix
,
, and
must satisfy the specifipolynomials
cations of Theorem 2.1. It remains to investigate under what

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

conditions the tracking criterion (41) is satisfied and under
what conditions this regulator is T-universal.
We begin by deriving a necessary condition for Tuniversality. Consider a reference signal of the type
(43)
and
are fixed but arbitrary. Then the
where
closed-loop system (40), (42) has solutions

1693

delays between
and . Indeed, the condition (48) for Tuniversality imposes some rather stringent conditions on the
is
and
is
,
system (40). In particular, since
, and
must have full rank.
(48) implies that
Theorem 3.2: Suppose that is stable. Then there exists a
T-universal regulator for the tracking problem if and only if
matrix function
with no
there is a proper rational
which satisfies the equation
poles in the region
(49)

(44)
with

which, in particular, implies that
In this case, let be a stable scalar polynomial such that

.
(50)

(45)
,
where
are defined by (21). Moreover,

, and

and
(46)

But the tracking condition (41) requires that
as
and, since is arbitrary, this implies that
follows from (44) and (46) that

. Therefore, it
(47)

Now, in order for the regulator (42) be T-universal, (47) must
hold for all , that is, for all and . Consequently, we must
have
(48)
on the unit circle and, by analytic continuation, in the rest of
the complex plane.
Lemma 3.1: A stabilizing and realizable regulator (42) is
T-universal if and only if the identity (48) holds.
Proof: We have already proved that (48) is a necessary
condition for (42) to be T-universal, so it remains to prove
that it is also sufficient. To this end, first assume that there are
such that
for all . Then
positive numbers ,
has a -transform

which converges for
. It follows from (45) and
is the transfer function from
to ,
(46) that
with a -transform
and hence (40), (42) has a solution
. But, if (48) holds, then
and hence
for all . Because of stability any other solution
tends asymptotically to this solution, and therefore (41) holds.
If increases so fast that it does not have a -transform, set
for
and
for
, and
be the corresponding -solution. Then it is easy to see
let
for
. Since
is arbitrary, the
that
conclusion follows.
As a corollary we see that
must be full rank, or
else (48) will be violated. This implies that there are no

be a
matrix
is a matrix polynomial, and let
polynomial satisfying the first degree constraint (27). Then, the
and
given by (29), is a T-universal
regulator (28), with
regulator for the tracking problem, and any other T-universal
regulator is equivalent to one obtained in this way.
Proof: First, suppose that there exists a T-universal regulator of the form (42). Then, according to Lemma 3.1, there
to (49) with the prescribed properties,
exists a solution
. In fact, in view of (22), (32) and the fact that
namely
is stable, it follows that
has no poles in the region
. Moreover, since the regulator is realizable,
is
proper.
which is proper
Next, suppose that (49) has a solution
, and let , , and be
with no poles in the region
defined as in the theorem. [Note that in order to satisfy the
first of degree conditions (27) we may need to choose and
which are not coprime.] Then, by Theorem 2.1, the regulator
given by (29) is stabilizing and realizable and
(28) with
(51)
. Consequently, it follows from
i.e., in view of (50),
(49) and Lemma 3.1 that the regulator is T-universal.
It remains to prove the last statement of the theorem. To
this end, suppose that the regulator
(52)
is T-universal. Then, in particular, it is stabilizing and realizable, and thus, by Theorem 2.1, there are some , , and
with the properties specified in Theorem 2.1 such that the
given by (29) is equivalent to (52).
regulator (28) with
is invariant under this equivalence. Therefore, since
Now,
(48) holds for the regulator (52) by Lemma 3.1, (48) also holds
for (28). However, by Theorem 2.1, (51) holds, and hence there
, satisfying (49) and (50).
is an , namely
In general, a solution to (49) cannot be expected to be
, only one solution is possible, namely
unique, but if

and this would require that
is a stable, proper rational
must be minimum phase with
function, implying that
must be
no zeros at infinity. In particular,
nonsingular.

1694

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Corollary 3.3: Suppose that
is stable and the transfer
is square, i.e.,
.
function
Then there is a T-universal regulator for the tracking problem
is proper with no poles in the region
if and only if
. In this case, let
be a stable scalar polynomial
is a matrix polynomial and
is
such that
matrix polynomial satisfying the degree requirement
a
and
are defined by (29) and by
(27). Then, if
(53)
the regulator (28) is a T-universal regulator, and any other
T-universal regulator is equivalent to one obtained in this way.
A T-universal regulator exists only under rather special
conditions. However, if we restrict our attention to harmonic
reference signals (4), these conditions can be considerably
relaxed and we may also allow for external harmonic disturbances. This is the topic of the next section.

respectively, satisfying the degree requirements (27) and the
interpolation conditions
for
for

(59b)

and
are given by (29), the regulator (28) is a
Then, if
universal tracking regulator, and any other universal tracking
regulator (28) is equivalent to one obtained in this way.
Proof: Whenever a linear stabilizing regulator is applied
tends exponentially to the
to system (1), the process
harmonic steady-state solution
(60)
where
(61a)

IV. UNIVERSAL TRACKING REGULATORS
IN HARMONICALLY DISTURBED SYSTEMS

(61b)

We now return to the situation described in Section I,
where the control system takes the form (1) with a harmonic
disturbance (2) and where there is a harmonic reference signal
to be empty, for
(4). Although we may allow the index set
.
tracking we must take
The first question to be answered is when it is possible to
find a regulator (12) in such that
as

(59a)

,
,
, and
being the closed-loop transfer functions
,
defined in Section II. In fact, for any regulator in ,
defined by (17), is a stable matrix polynomial. In the same
tends exponentially to
way, in view of (1c),

(54)

(62)

which is universal in the sense that (54) holds for all values
and
and does not depend on these
of
vector amplitudes. We shall refer to such a regulator as a
universal tracking regulator. For convenience, in the sequel
we use the notation

Now, the basic idea is that the tracking condition (54) is
achieved precisely when the cost function (7) is zero. It is
easy to see that
(63)

(55)
is stable, and let
Theorem 4.1: Suppose that the matrix
and
be the matrix polynomials defined by (26).
be the
matrix function defined by
Moreover, let
the
matrix polynomial
(21) and

To see this, observe that if
sequences

and

are two harmonic vector

and

(56)
Then, for a universal tracking regulator to exist in
necessary that the rank condition
for all

, it is

with
distinct as in (3), and
appropriate dimensions, then

is an arbitrary matrix of

(57)

holds, and it is sufficient that both rank conditions (57) and
for all

(64)

(58)

, and
hold. In particular, (57) requires that
. More precisely, let
be
(58) that
and
an arbitrary stable scalar real polynomial, and let
be matrix polynomials, of dimensions
and
,

Moreover, in view of (61b) and (62)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1695

Remark 4.3‚ÄîInternal Model Principle: The situation most
often studied in the literature is when
, i.e.,
,
, and
, and when the regulator (28) takes the form

and consequently (63) equals zero for all values of
and
if and only if
(65a)

for
for

(65b)

Theorem 2.1 states that the regulator (28) is stabilizing if
and
are defined by (29) for some stable scalar real
and some real matrix polynomials
polynomial
and
satisfying (27) and that any other stabilizing and
realizable regulator (28) is equivalent to one obtained in this
way. Moreover
(66)
which inserted into (65) yields precisely (59).
If the rank conditions (57) and (58) hold, the interpolation
conditions (59) have a solution, and the general solution is

obtained by setting
. We assume that the rank
.
conditions (57) and (58) are satisfied so that
For robustness it is desirable to include a model of the
disturbance dynamics in the regulator. This is the internal
model principle. Following [3], we replace the matrix fracby the (reachable) matrix fraction
tion representation
so that
. The harmonic
representation
dynamics is then included in the regulator dynamics by setting
, where
and
is a stable matrix polynomial. Then, by (29)

which, in view of the fact that

, yields

for
for
where, for
,
and
are arbitrary matrices
and
. Here
such that
the degree of the stable polynomial is chosen sufficiently
high to satisfy the degree constraints (67). On the other hand,
the rank condition (57) is also necessary for the existence of a
is stable, (65b)
universal tracking regulator. In fact, since
for some
.
cannot hold if
Remark 4.2: The two rank conditions (57) and (58) in
Theorem 4.1, which of course can be stated in terms of zeros
of certain transfer functions, have different status. If (57) is
violated, the interpolation condition (59b) cannot hold, so there
could be no universal tracking regulator. On the other hand,
if (57) holds but (58) does not, interpolation condition (59a)
could still be valid, as the rank of the right member could be
less than . However, this is a nongeneric situation, and hence
it cannot be expected to occur in practice. In fact, if
and
, the following equation must hold:

which will occur only on a lower-dimensional algebraic set in
the parameter space.
Theorem 4.1 provides a complete solution of a problem
studied in various degrees of generality in [4]‚Äì[8], [13], [16]
and of course is consistent with the solutions given there,
although given in a different form and in continuous time.
, rank condition (58) becomes void and only (57), a
If
considerably weaker version of condition (49) in Section III,
remains. Hence, for universal tracking regulators to exist the
is necessary, and if there are external
condition
, in practice, we must also have
.
disturbances
Consequently, as also noted in [4], [7], [8], [13], and [16],
asymptotic tracking is only possible under certain specific
conditions.

where we have assumed that
has no zeros in the points
. (Otherwise we include a simple feedback loop to
clearly satisfy the interpolation
move the zeros.) These
,
and
conditions (59). In fact, since
, by (29), these can be written
for
for
Consequently, we see that the internal-model-principle regulators form a subclass of the ones considered above.
, which
The rank condition (58) becomes void if
is equivalent to the case with complete state information, i.e.,
. Then the formulas for the regulator
the case when
also simplify considerably.
so that
.
Theorem 4.4: Suppose that
Moreover, suppose that is stable and that condition (6) holds.
Then, there exists a universal tracking regulator (37) in if
and only if the rank condition (57) holds. In fact, let
be a stable scalar real polynomial, and let
and
be
matrix polynomials satisfying the degree constraints (36) and
the interpolation conditions
for
for

(67a)
(67b)

and
are given by (38), the regulator (37) is a
Then, if
universal tracking regulator, and any other universal tracking
regulator (37) is equivalent to one obtained in this way.

1696

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof: The proof follows the same lines as that of
Theorem 4.1, except that (39) from Corollary 2.2 is used in
,
exists and (67a)
lieu of (66). Since
can be solved.
When
, there are no universal tracking regulators,
and in order to damp the steady-state tracking error we shall
therefore next turn to an optimization procedure. This is the
topic of the next section.
V. LINEAR QUADRATIC OPTIMIZATION
FOR TRACKING AND DAMPING
We now return to the optimization problem stated in
Section I. In this section we consider only linear regulators.
Later, in Section VI, we demonstrate that under slightly
stronger technical conditions the optimal universal regulators
presented here are actually optimal in the much larger class
, which includes nonlinear regulators.
Let us recall that the problem under consideration is to
control the disturbed system (1) by feedback from the output
so as to minimize the cost function
(68)

for all

,

satisfying
(73)

such that
. It can be shown [21]
for all
that if this condition fails in a strong way, i.e., there are
, , and ,
, such that
, then there
such that
. In
is an external disturbance
this section, however, we shall only need the weak frequency
, ,
,
domain condition that (72) and (73) hold for
, defined as in (55).
Both of these conditions are invariant under the action of
the feedback group

where is a nonsingular matrix and is an arbitrary matrix of
appropriate dimensions. Moreover, since has no eigenvalues
on the unit circle, the inverse
(74)
exists for all on the unit circle, and hence
where
is the Hermitian
that
matrix function

so

(75)
is the quadratic form defined by (9). Hence,
where
we may not only want to damp the tracking error, but also
some internal systems variables. As before, both the disturand the reference signal are harmonic and given by
bance
(5), where only the frequencies are known. The optimization is
performed over the class of stabilizing and realizable linear
regulators (12). The problem under consideration is: 1) to find
the conditions under which there are optimal regulators which
are universal in the sense that they are optimal for all choices
of the amplitudes of (5) and independent of these and 2) to
characterize the class of all such universal optimal regulators.
To address this problem, let us first take a closer look at
the cost function (68). A straightforward reformulation taking
(1c) into consideration yields

In this notation the strong frequency domain condition may
be written
for all

is the real quadratic form
(70)

with the real matrices

,

, and

given by

(71)
The quadratic form (70) need not be nonnegative definite but
must of course satisfy some condition ensuring that
. As we shall see, a sufficient condition for this is the
strong frequency domain condition, i.e., that there is a
such that
(72)

(76)

and the weak one as
for

(77)

We now state the main result of this section. It will be
strengthened in Section VI, where we show that under mild
is
technical conditions the optimal universal regulator in
also optimal in the wider class .
,
, and
be the matrix
Theorem 5.1: Let
polynomials defined by (26) and (56). Suppose that the matrix
is stable and that the weak frequency domain condition
(77) holds, and suppose that
for all

(69)
where

on the unit circle

(78)

. Then,
i.e., in particular that
which is universal in
there exists an optimal regulator in
and
the sense that it is optimal for all values of
and does not depend on these vector amplitudes.
be an arbitrary stable scalar real
More precisely, let
and
be matrix polynomials of
polynomial, and let
and
, respectively, satisfying the degree
dimensions
requirements (27) and the interpolation conditions

with

and

for

(79a)

for

(79b)

given by
(80)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

where
. Then the regulator (28) is an
and
universal regulator, which is optimal in , provided
are given by (29) and any other universal regulator (28), which
is optimal in , is equivalent to one obtained in this way.
is nonsingular for
Since, by assumption,
, (79a) has the solution

1697

(60) of
. In fact, we have the following lemma. The
proof follows from a simple completion-of-squares argument
and is deferred to Appendix A.
be any solution to the closed-loop
Lemma 5.5: Let
system (1), (12), where (12) is a stabilizing and realizable regulator, and suppose that the weak frequency domain condition
(77) holds. Then the cost function (68) exists as a usual limit,
and it takes the value

(81)
, and these are precisely all solutions of (79a).
for
and
Clearly, there are always matrix polynomials
satisfying (81), (79b) and the degree constraints (27), provided
is chosen
the degree of the stable scalar polynomial
sufficiently large.
, there exist optimal regulators, but,
Remark 5.2: If
as explained in Remark 4.2, universality is not a generic
property; therefore, for all practical purposes, there are no
.
optimal universal regulators if
Remark 5.3: Before proceeding to the proof of Theorem
5.1, let us make certain that it is consistent with the results
of Section IV. To this end, let us consider a cost function (7),
. Then
i.e., suppose that

(82)

where, for
(83)
with

and

given by (80) and

by
(84)

where
where the
matrix function
is given by (21). If
, the weak frequency domain condition cannot hold,
so Theorem 5.1 does not apply. Instead, Theorem 4.1 should
, the weak frequency domain condition is
be used. If
a consequence of condition (57), and it is easy to check that
the optimal cost will be zero, as required by Theorem 4.1.
Moreover, interpolation conditions (59) and (79) are identical.
, no universal tracking regulator exists by
Finally, if
Theorem 4.1, and the optimal cost will be nonzero in general.
Remark 5.4‚ÄîGeneralized Internal Model Principle: As in
, so that
Remark 4.3, let us consider the case when
,
,
, and
, and
in the regulator (28). For simplicity, also assume that
.
and
and
, the interpolation
If
conditions (79) can be written

(85)
,
In the expression (82) for the cost function , only
, ,
depend on the regulator to be chosen. They are
defined by (61b), i.e.,
(86)
of external disturbances
Recall that we consider the class
for
and
for
with arbitrary
and the class of reference signals with
for
and
for
.
Consequently, if we could find a stabilizing and realizsatisfy the
able regulator (12) such that
optimality conditions
(87)

for
, as can be seen from (29), (80), and
,
, and
. All of these
the fact that
interpolation conditions are satisfied if the second set is, and
in this case (29) implies that

which could be interpreted as a generalized internal model
principle for the optimization problem.
The basic idea behind the proof of Theorem 5.1 is, as for
Theorem 4.1, that whenever a linear stabilizing regulator is
tends exponenapplied to the system (1), the process
tially to the harmonic steady-state solution (60). Therefore, the
cost function (68) depends only on the harmonic component

which, in view of (86), is the same as
(88)
then this regulator would be optimal. If, in addition, this regu,
lator does not depend on the amplitudes
and
,
and the conditions (88) hold for all
and
, i.e., all disturbances in
and all
reference signals in , then this optimal regulator is also
universal. This condition holds if and only if
for

(89a)

for

(89b)

1698

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof of Theorem 5.1: Theorem 2.1 states that the reguand
are defined by (29) for
lator (28) is stabilizing if
and some real matrix
some stable scalar real polynomial
satisfying (27), and that any other stabilizing
polynomial
and realizable regulator (28) is equivalent to one obtained in
this way. Moreover

Since, by assumption,
is a nonsingular matrix of
, (91a) has the solution
dimension

(92)
. There are always matrix polynomials
and
satisfying (92), (91b), and the degree constraints (36),
is
provided the degree of the stable scalar polynomial
chosen sufficiently large.
for

(90)
We have demonstrated above that (89) is a necessary condition
for the regulator (28) to be an optimal universal regulator, and
inserting (90) into (89) yields precisely (79). Clearly, as we
have already discussed, there are always matrix polynomials
and
satisfying these conditions and the degree
constraints (27), provided the degree of the stable scalar
is chosen sufficiently large, and provided
polynomial
condition (78) is satisfied.
It remains to prove the converse statement. For any optimal
, the value of the cost function
universal regulator
, defined by (84). It follows from (82) and the
(68) equals
, for
, that (87) holds for
fact that
, and
. Therefore, (89) follows from
all
is equivalent to
(88). By Theorem 2.1, the regulator
given by (29) for some
satisfying the
(28) with
requirements of Theorem 5.1. This regulator is also optimal
since equivalent regulators have the same cost . It is also
does not depend on
universal because
and
.
Corollary 5.6: The optimal value of the cost function (68)
, defined by (82) and (83).
in the class is
Note that, although an optimal universal regulator will not
and
, the cost function (84)
depend on
will.
In the special case of complete state information, i.e.,
, condition (78) is always satisfied. In view of Corollary 2.2,
Theorem 5.1 can be considerably simplified in this case, so we
state it separately. The proof is the same as for Theorem 5.1,
except that we now use the equations of Corollary 2.2.
so that
.
Theorem 5.7: Suppose that
Moreover, suppose that is stable and that condition (6) holds.
Then, if the weak frequency domain condition (77) holds, there
exists a universal regulator (37), which is optimal in . In fact,
let
be a stable scalar real polynomial, and let
and
be matrix polynomials satisfying the degree constraints
(36) and the interpolation conditions
for

(91a)

for

(91b)

and
are
where and are defined as in (80). Then, if
given by (38), the regulator (37) is a universal regulator, which
is optimal in . Conversely, any other universal regulator (37),
which is optimal in , is equivalent to one obtained in this
way. Finally, the optimal value of the cost function (68) is
given by (84).

VI. OPTIMALITY IN THE CLASS OF NONLINEAR REGULATORS
In this section we show that the universal optimal linear
regulators described in Theorems 5.1 and 5.7 are actually
optimal in a wide class of nonlinear regulators. We now define
this class.
of
Given the control system (1), consider the class
nonlinear regulators
(93)
of
which is stabilizing in the sense that any solution
the closed-loop system consisting of (1) and (93) satisfies the
condition
as

(94)

This stability condition is quite weak but will suffice for our
purposes. Of course, a weaker condition has the advantage of
allowing for a larger class of controls.
We consider the same problem as in Section V, except that
.
we now optimize over all regulators in . Clearly,
The only price we have to pay for this generalization is that
the weak frequency domain condition needs to be replaced by
the strong one.
be stable, and suppose that the rank
Theorem 6.1: Let
condition (78) holds. Then, if the strong frequency domain
condition (76) holds, the linear optimal universal regulators of
Theorem 5.1 are optimal in the class .
It turns out that Theorem 6.1 is a simple consequence of the
corresponding result for complete state information. In fact, the
class of stabilizing and realizable regulators
with
is a subclass of the class of stabilizing and realizable regulators

in that only a special structure of is required. But, as seen in
Section V, an optimal universal regulator in the former class
is optimal also in the latter, since the same optimal value
is achieved (Corollary 5.6 and Theorem 5.7). (The only
difference between the cases of complete and incomplete state
information is that a higher degree regulator may be required
in the latter case to achieve the optimum.) Consequently, if
we can prove the following theorem, we have also proved
Theorem 6.1.

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

Theorem 6.2: Let
be stable, and suppose that
and that
. Then, if the strong frequency domain
condition (76) holds, the linear optimal universal regulators of
Theorem 5.7 are optimal in the class .
In order to prove this theorem we consider an optimization
problem which unlike that in Section V does not require that
a linear regulator has been applied. More precisely, let us
first consider the problem of finding a process
which minimizes the cost function (8), subject to the constraints (94) and
(95)
and
are arbitrary bounded and
where now
complex-valued vector sequences.
It is well known (see, e.g., [20], [21], [24], [25], and [29])
that if the strong frequency domain condition (76) holds and
is stabilizable, then the algebraic Riccati equation

1699

The optimal value of the cost function is
(103)
where

(104)
exists, any optimal process
If the limit
is produced in this way.
Note that the control (101) cannot in general be used in
and . Even
practice, since it depends on future values of
in the harmonic case when this dependence can be resolved,
this control law has serious disadvantages [21, Sec. III]. It is
developed here as an instrument of proof.
Next, let us return to our original problem and take
and
to be harmonic, given by (5). Then a simple
calculation, using (99) and (100), yields the representation

(96)
has a unique symmetric solution
matrix

with

which renders the feedback

(105)

where

where
(97)
stable in the sense that all eigenvalues of lie strictly inside
the unit circle. We shall refer to this solution as the stabilizing
solution of (96). For this solution we also have that
(98)
is positive definite.1
Then we have the following result, which should be compared to [21, Th. 2.3], the proof of which we defer to
Appendix B.
be stabilizable and suppose that
Lemma 6.3: Let
the strong frequency domain condition (76) holds so that (96)
has a stabilizing solution . Moreover, let

We are now in a position to prove Theorem 6.1.
Proof of Theorem 6.1: Clearly, for any regulator in ,
(103) is a lower bound for the cost . Therefore, if we can
demonstrate that there is a regulator in which achieves the
same value (103) of the cost , this regulator must be optimal
also in , and so must all regulators which are optimal in .
so that
To this end, let us introduce a new control
(106)
transforming the system (1a) to
(107)

(99)
where

We want to find a stabilizing and realizable regulator
(100)

Then the problem to minimize the cost function (8) subject
to constraints (94) and (95) is solved by a process
such that

(108)
so that the closed-loop system (106)‚Äì(108) has a solution
satisfying (101) for some with the property (102).
Then, by Lemma 6.3, the regulator (106), (108), i.e.,
(109)

(101)
where is given by (97) and
such that

is any vector sequence

(102)
1 Note that there is a misprint in [21, p. 788]: In Theorem 2.1, replace
‚Äústatements hold‚Äù for ‚Äústatements are equivalent.‚Äù

is optimal in . Therefore, the optimal linear regulators of
Theorem 2.1 must be optimal also in .
of the closedSince (108) is stabilizing, the solution
loop system (107), (108) tends exponentially to a harmonic
solution

1700

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

which of course yields the same value to
so that
if we can choose

as

. Now,

for

The matrix polynomials (26) are
and

(110)

, then
has the property (102)
and hence
and (106) becomes (101) as required.
such that (110) holds, we
To show that there are
first apply Corollary 2.2 to the system (107), where takes
and
that of . In fact, by Corollary 2.2,
the place of
there is a stable scalar polynomial and matrix polynomials
such that
and
so that
are given by
and

Let us first take
a T-universal regulator

and consider the problem to find
(112)

tends asymptotically to . By Corollary 3.3, a
so that
T-universal regulator exists if and only if
and

stable

(113)

where

and
In fact,
. In this case, (112) is a Tuniversal regulator if and only if
However,
tends exponentially to the harmonic solution
Since therefore

.

and
is given by (105), the optimality condition (110) will
and
if
be satisfied for all

(114)
and
such that
is stable and
for some polynomials
or is equivalent to one obtained in this
. Of course,
way. This corresponds to the choice
asymptotic tracking is achieved for all choices of reference
signal .
If, instead, we consider a reference signal
(115)

Since

is full rank, in view of the discussion in Section V
can be chosen to satisfy these interpolation
conditions.
VII. SOME SIMPLE NUMERICAL EXAMPLES

for

To illustrate the results of this paper, let us consider the
system

is the control,
where
characteristic polynomial

and

where the frequencies , are given, but the amplitudes ,
and the phases ,
are unknown, the class of regulators
(112) which achieve asymptotic tracking is much larger, and
condition (113) need not be satisfied but can be exchanged for
(116)

In fact, by Theorem 4.1, in this case we may choose any
stabilizing regulator

(111)

(117)

are outputs, and the

provided is stable and the degree constraint (27) and the
interpolation conditions
for

is stable with

. Defining the state

the plant equations (111) can be written in state-space form
(1), where

so that

is the characteristic polynomial of

, and

are satisfied. The same regulator is obtained by applying
Theorem 5.1, now observing that (116) is the weak frequency
domain condition; see Remark 5.3. This allows for more tuning
parameters to satisfy other design specifications. Of course, if
condition (113) is fulfilled, the T-universal regulator can still
be used.
,
, and
As a numerical example, suppose that
, and let
and
. Then condition (113)
is satisfied, so a T-universal regulator exists. Such a regulator
and
in
is obtained by, for example, setting
and
and the initial conditions are
(114). If
, this yields the error depicted in Fig. 2. The

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

Fig. 2.

Fig. 3.

dashed line in the same figure is the tracking error obtained
.
by setting
, while and remain the same.
Next, let us take
Then becomes unstable, so a T-universal regulator fails to
exist. Although condition (113) fails, we could still obtain
asymptotic tracking by using a universal tracking regulator,
constructed as in Theorem 4.1, provided condition (116) holds,
and we shall present a simulation for this case in the end of
the section.
We now add an harmonic disturbance

straightforward calculation yields

(118)
and ,
in the system (111), where , are given, but ,
are unknown. Suppose we want to determine an optimal
universal regulator for the cost function

1701

for any on the unit circle. In order to construct an optimal
universal regulator we need to choose a stable polynomial

of degree at least five. The parameters , , , , , as
well as will be available for tuning in order to improve the
overall design. Then, defining the real numbers , , , ,
, , ,
via
for
for

(119)
it is easily seen that the polynomials
Since the matrices

,

, and

in (71) become

a simple calculation yields

will satisfy the interpolation conditions (79a) if and only if its
coefficients satisfy the linear system of equations

for (75), and therefore the strong frequency domain condition
, so any optimal universal
(76) is always satisfied if
of possibly
regulator (112) is optimal in the larger class
, the
nonlinear regulators described in Section VI. If
strong frequency domain condition will fail if and only if the
has a root on the unit circle, while the weak
polynomial
frequency condition (77) will still hold provided we avoid
choosing any of the frequencies in (115) and (118) so that
,
,
, or
is such a root.
Next, let us consider the interpolation condition (79).
defined by (56) is identically one, and a
Clearly,

Consequently, by Theorem 5.1, (117) is an optimal universal
and
are determined in this way.
regulator if
,
, and
For an example, take as before
. Moreover, we choose a disturbance (118) with
and
, while the harmonic
frequencies
,
reference signal (115) has the same frequencies
as in the first simulation. In Fig. 3 we illustrate the
tracking error of the optimal universal regulator corresponding
to a polynomial with roots 0.3 0.3 , 0.3 0.2 , 0.5, and
. The amplitudes in (115) and (118) have been taken

1702

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Fig. 4.

to be
,
, and
, and the initial
. As before, the dashed line is
conditions are
. Remember
the tracking error obtained by setting
, the control energy is also damped, so
that, since
there is a certain tradeoff here. We remark that it is important
to tune the free parameters to obtain good properties of the
regulator. In particular, the transients, which do not affect the
cost function, can change dramatically with different choices
of free parameters.
and
instead,
Now, setting
while keeping all the other parameters the same, we obtain
the errors in Fig. 4. As seen, the error goes asymptotically
to zero, despite the fact that condition (113) is not fulfilled so
that a T-universal regulator does not exist. In fact, by Theorem
4.1, this is a universal tracking regulator which exists since
on the unit circle. In order to speed up the
convergence, the roots of have been reset at 0.7 0.1 , 0.3
0.2 , and 0.8. Since now we do not have the disturbance
and
, we could choose another
frequencies
to possibly get a universal tracking regulator with a
better transient.

reference signal is no larger than the dimension
of the
control, such a regulator exists under mild conditions. This
is in harmony with other results in the literature [4]‚Äì[8], [13],
[16], where, however, the continuous-time case is considered.
We provided complete solutions of these problems in discrete
time, and our proof is considerably simpler.
If the system is also corrupted by a harmonic disturbance
, asymptotic tracking may still be possible provided the
dimension of the disturbance is no larger than the dimension
of the output available for feedback. However, if a certain
,
rank condition fails, which in particular is the case if
asymptotic tracking is not possible, but a steady-state error
will remain. Therefore, we considered next an optimal control
problem to damp the steady-state tracking error, also giving
the option to damp internal system variables. We characterized
the class of all optimal regulators which are universal in the
sense that they are optimal for all choices of the amplitudes
of and . Such regulators were shown to exist if the weak
. On the other
frequency domain condition holds and
, there are always algebraic conditions on the
hand, if
system parameters, implying that universality is not a generic
property in this case.
We have also shown that all optimal universal regulators
can be chosen as linear even if the optimization is over a
very large class of nonlinear regulators, provided the strong
frequency domain condition holds. We have given complete
characterizations of all linear optimal universal regulators in
terms of parameterizations containing many free parameters.
This allows for a considerable amount of design freedom,
which can be used to satisfy other design specifications via
loop shaping. Indeed, we stress that our solutions are optimal
in the sense stated in this paper only, and that other desirable
design specifications may not be satisfied for an arbitrary
universal optimal regulator.
APPENDIX A
PROOF OF LEMMA 5.5
and
tend exponentially to the harmonic comSince
ponents (60), only these contribute to the cost function (70);
consequently, the usual limit (rather than just limsup) does
where
exist in (69), and it is given by

VIII. CONCLUSIONS
In this paper we have given complete characterizations of
regulators which satisfy certain tracking specifications and
which are universal in the sense that they are independent
of disturbances and tracking signals and apply regardless of
the values of these.
As a preliminary, we considered a problem of asymptotic
tracking of an arbitrary signal , and we characterized all
regulators which are universal with respect to the choice of
. We showed that such universal regulators exist only under
very special conditions. These conditions can be considerably
relaxed if the reference signal is exchanged for a harmonic
signal with known frequencies but unknown amplitudes and
phases, and we want the regulator to be universal in the
sense that it achieves asymptotic tracking for all choices
of the
of amplitudes and phases. Then, if the dimension

(A1)
. In fact, this follows from the argument
for
leading to (64). Now, in view of the constraint (1a)
(A2)
and therefore (A1) takes the form
(A3)
if the weak frequency domain condition (77)
where
is given by (85), and
is fulfilled. Here
(A4)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

Therefore, assuming that the weak frequency domain condition
for
, we may
(77) holds so that
complete squares in (A3) to obtain
(A5)
where

1703

By virtue of condition (94) and the boundedness of

where of course the last term tends to zero as
.
, the cost function
Consequently, for any admissible
(B1) becomes

(A6)
From this the equations of the lemma follow readily.
(B5)

APPENDIX B
PROOF OF LEMMA 6.3
The proof is similar, mutatis mutandis, to the one given in
[21, Sec. II]. Recall from (69) that the cost function can be
written
(B1)
where

Therefore, since
(B6)
for any admissible control. Clearly, equality would be achieved
to satisfy (101) since
does not
if we could take
contribute to by virtue of (102). Hence it remains to prove
that such a process satisfies the stability condition (94). To this
end, insert (101) in (95) to obtain
(B7)

(B2)
being the quadratic form (70). Next, introduce
with
the Lyapunov function
(B3)
is the unique stabilizing solution of (96),
where
is given by (100) and
satisfies (104). Then, along
the trajectory of (95)

and
are bounded,
satisfies
Since
is a stability matrix,
satisfies the
(102) and
weak stability condition (94). The last statement follows
immediately from (B5) and (B6).
ACKNOWLEDGMENT
The authors would like to thank the anonymous referees and
the associate editor for several useful suggestions. They would
also like to thank X. Hu for technical advice and stimulating
discussions.

(B4)
is given by (99).
where
In fact, inserting (95) and completing squares in the left
member of (B4) yields the right member of (B4) plus a number
of terms which are either quadratic in , linear in , or
constant with respect to . The quadratic terms cancel due
to the fact that satisfies the algebraic Riccati equation (96),
and the constant terms cancel due to (104). Finally, the linear
terms cancel provided

which has the unique bounded solution (100), since
is a
stable matrix.
and
, where
Now, set
is an admissible process, and sum (B4) from
to
to obtain

REFERENCES
[1] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic
Methods. London, U.K.: Prentice-Hall, 1989.
[2] S. Bittanti, F. Lorito, and S. Strada, ‚ÄúAn LQ approach to active control
of vibrations in helicopters,‚Äù Trans. ASME, J. Dynamical Systems,
Measurement and Contr., vol. 118, pp. 482‚Äì488, 1996.
[3] C. T. Chen, Linear System Theory and Design. New York: Holt,
Rinehart and Winston, 1984.
[4] E. J. Davison and A. Goldenberg, ‚ÄúRobust control of a general servomechanism problem: The servo compensator,‚Äù Automatica, vol. 11, pp.
461‚Äì471, 1975.
[5] E. J. Davison and B. R. Copeland, ‚ÄúGain margin and time lag tolerance
constraints applied to the stabilization problem and robust servomechanism problem,‚Äù IEEE Trans. Automat. Contr., vol. AC-30, pp. 229‚Äì239,
1985.
[6] E. J. Davison and B. M. Scherzinger, ‚ÄúPerfect control of the robust
servomechanism problem,‚Äù IEEE Trans. Automat. Contr., vol. AC-32,
pp. 689‚Äì702, 1987.
[7] B. A. Francis, ‚ÄúThe linear multivariable regulator problem,‚Äù SIAM J.
Contr. Optim., vol. 15, pp. 486‚Äì505, 1977.
[8] B. A. Francis and W. M. Wonham, ‚ÄúThe internal model principle of
control theory,‚Äù Automatica, vol. 12, pp. 457‚Äì465, 1977.
[9] K. V. Frolov and F. A. Furman, Applied Theory of Vibration Protected
Systems. Moscow, Russia: Mashinostroenie, 1980 (in Russian).
[10] K. V. Frolov, Vibration in Engineering. Moscow, Russia: Mashinostroenie, 1981 (in Russian).
[11] M. D. Genkin, V. G. Elezov, and V. D. Iablonski, Methods of Controlled
Vibration Protection of Engines. Moscow, Russia: Nauka, 1985 (in
Russian).

1704

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

[12] D. Guicking, Active Noise and Vibration Control, reference bibliography,
Third Physical Institute, Univ. Goettingen, Jan. 1990.
[13] A. Isidori and C. I. Byrnes, ‚ÄúOutput regulation of nonlinear systems,‚Äù
IEEE Trans. Automat. Contr., vol. 35, pp. 131‚Äì140, 1990.
[14] C. G. KaÃàllstroÃàm and P. Ottosson, ‚ÄúThe generation and control of roll
motion of ships in closed turns,‚Äù in Proc. 4th Int. Symp. Ship Operation
Automat., Geneva, Switzerland, 1982, pp. 1‚Äì12.
[15] H. Kimura, ‚ÄúPole assignment by output feedback: A longstanding open
problem,‚Äù in Proc. 33rd Conf. Decision and Control, Lake Buena Vista,
FL, Dec. 1994.
[16] A. Krener, ‚ÄúThe construction of optimal linear and nonlinear regulators,‚Äù
in Systems, Models and Feedback: Theory and Applications, A. Isidori
and T. J. Tarn, Eds. Boston, MA: BirkhaÃàuser, 1992, pp. 301‚Äì322.
[17] V. KucÃÜera, ‚ÄúThe discrete Riccati equation of optimal control,‚Äù Kybernetika, vol. 8, pp. 430‚Äì447, 1972.
[18] H. Kwakernaak and R. Sivan, Modern Signals and Systems. Englewood Cliffs, NJ: Prentice-Hall, 1991.
[19] G. Leitmann and S. Pandey, ‚ÄúAircraft control under conditions of
windshear,‚Äù in Proc. 29th Conf. Decision and Control, Honolulu, HI,
1990, pp. 747‚Äì752.
[20] P. Lancaster, A. C. M. Ran, and L. Rodman, ‚ÄúHermitian solution of the
discrete algebraic Riccati equation,‚Äù Int. J. Contr., vol. 44, pp. 777‚Äì802,
1986.
[21] A. Lindquist and V. A. Yakubovich, ‚ÄúOptimal damping of forced
oscillations in discrete-time systems,‚Äù IEEE Trans. Automat. Contr., vol.
42, pp. 786‚Äì802, 1997.
, ‚ÄúOptimal damping of forced oscillations by output feedback,‚Äù
[22]
in Stochastic Differential and Difference Equations, Progress in Systems
and Control Theory, vol. 23, I. CsiszaÃÅr and G. Michaletzky, Eds.
Boston, MA: BirkhaÃàuser, 1997, pp. 203‚Äì231.
[23] A. Miele, ‚ÄúOptimal trajectories and guidance trajectories for aircraft
flight through windshears,‚Äù in Proc. 29th Conf. Decision and Control,
Honolulu, HI, 1990, pp. 737‚Äì746.
[24] B. P. Molinari, ‚ÄúThe stabilizing solution of the discrete algebraic Riccati
equation,‚Äù IEEE Trans. Automat. Contr., vol. AC-20, pp. 396‚Äì399, 1975.
[25] V. M. Popov, Hyperstability of Control Systems. Berlin, Germany:
Springer, 1973.
[26] A. V. Savkin and I. R. Petersen, ‚ÄúRobust control with rejection of
harmonic disturbances,‚Äù IEEE Trans. Automat. Contr., vol. 40, pp.
1968‚Äì1971, 1995.
[27] R. Shoureshi, L. Brackney, N. Kubota, and G. Batta, ‚ÄúA modern control
approach to active noise control,‚Äù Trans. ASME, J. Dynamical Syst.s,
Measurement and Contr., vol. 115, pp. 673‚Äì678, 1993.
[28] V. Z. Weytz, M. Z. Kolovski, and A. E. Koguza, Dynamics of Controlled
Machine Units. Moscow, Russia: Nauka, 1984 (in Russian).
[29] V. A. Yakubovich, ‚ÄúA frequency theorem in control theory,‚Äù Sibirskij
Mat. Zh., vol. 4, pp. 386‚Äì419, 1973 (in Russian); English translation in
Sibirian Math. J.
[30]
, ‚ÄúUniversal regulators in linear-quadratic optimization problems,‚Äù in Trends in Control, A. Isidori, Ed. New York: Springer-Verlag,
1995, pp. 53‚Äì68.
[31] Y. Zhao and A. E. Bryson, ‚ÄúAircraft control in a downburst on takeoff
and landing,‚Äù in Proc. 29th Conf. Decision and Control, Honolulu, HI,
1990, pp. 753‚Äì757.
[32] K. Zhou, Essentials of Robust Control. Englewood Cliffs, NJ: PrenticeHall, 1998.

Anders Lindquist (M‚Äô77‚ÄìSM‚Äô86‚ÄìF‚Äô89) received
the M.S. and Ph.D. degrees from the Royal Institute
of Technology, Stockholm, Sweden, and in 1972
he was appointed a Docent of Optimization and
Systems Theory there.
From 1972 to 1974, he held visiting positions at
the University of Florida, Brown University, and
State University of New York, Albany. In 1974,
he became an Associate Professor and in 1980
a Professor of Mathematics at the University of
Kentucky, where he remained until 1983. He is
presently a Professor at the Royal Institute of Technology, where in 1982
he was appointed to the Chair of Optimization and Systems Theory, as well
as an Affiliate Professor at Washington University, St. Louis. He has also held
visiting positions at University of Padova, Italy, University of Arizona, USSR
Academy of Sciences, Moscow, East China Normal University, Shanghai, and
Technion, Haifa, Israel. He is the author of many papers in the area of systems
and control, especially stochastic control, filtering, stochastic systems theory,
realization theory, robust control, and applications of nonlinear dynamics in
estimation and control.
Dr. Lindquist is a Member of the Royal Swedish Academy of Engineering
Sciences, a Foreign Member of the Russian Academy of Natural Sciences,
and an Honorary Member of the Hungarian Operations Research Society. He
has also served on many editorial and advisory boards.

Vladimir A. Yakubovich (M‚Äô97) was born in
Novosibirsk, Russia, in 1926. He graduated from
Moscow University in 1949. He received the
Candidate of Science degree (Ph.D.) in 1953 and
the Doctor of Science degree in 1959, both from
Leningrad University.
After having worked for some time in industry
as an Engineer, he was admitted to the Leningrad
University, where he has remained. He is the author
of more than 270 papers and coauthor of seven
books in different areas of applied mathematics and
control theory. He has worked in parametric resonance theory, in the theory
of stability of nonlinear systems, and in optimization theory.
Dr. Yakubovich has served on many scientific committees and editorial
boards. He is a member of several scientific societies in Russia. He was
awarded the Norbert Wiener Prize in 1991, a prize from the international
editorial company ‚ÄúNauka‚Äù for best publication in its journals in 1995,
and the IEEE Control Systems Award in 1996. Since 1991, he has been a
Corresponding Member of the Russian Academy of Sciences and since 1994
a Member of the Russian Academy of Natural Sciences.

