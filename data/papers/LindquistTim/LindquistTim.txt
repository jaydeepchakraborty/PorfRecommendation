1

Optimal estimation with missing observations via balanced time-symmetric stochastic models
Tryphon T. Georgiou, Fellow, IEEE and Anders Lindquist, Life Fellow, IEEE

arXiv:1503.06014v2 [math.OC] 18 Aug 2015

Abstract--We consider data fusion for the purpose of smoothing and interpolation based on observation records with missing data. Stochastic processes are generated by linear stochastic models. The paper begins by drawing a connection between time reversal in stochastic systems and all-pass extensions. A particular normalization (choice of basis) between the two timedirections allows the two to share the same orthonormalized state process and simplifies the mathematics of data fusion. In this framework we derive symmetric and balanced MayneFraser-like formulas that apply simultaneously to smoothing and interpolation.

overview of smoothing and interpolation theory in the context of stochastic realization theory see [18, Chapter 15]. In the present paper we are taking this program several steps further. Given intermittent observations of the output of a linear stochastic system over a finite interval, we want to determine the linear least-squares estimate of the state of the system in an arbitrary point in the interior of the interval, which may either be in a subinterval of missing data or in one where observations are available. Hence, this combines smoothing and interpolation over general patterns of available observations. Our main interest is in continuous-time (possibly time-varying) systems. However, the absence of data over subintervals, depending on the information pattern, may necessitate a hybrid approach involving discrete-time filtering steps. In studying the statistics of a process over an interval, it is natural to decompose the interface between past and future in a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward or backward in time. This point was fundamental in early work in stochastic realization; see [18] and references therein. In a different context [19] a certain duality between the two timedirections in modeling a stochastic process was introduced in order to characterize solutions to moment problems. In this new setting the noise-process was general (not necessarily white), and the correspondence between the driving inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics. Here, we begin by combining these two sets of ideas to develop a general framework where two time-opposite stochastic systems model a given stochastic process. We study the relationship between these systems and the corresponding processes. In particular, we recover as a special case certain results of stochastic realization theory [11], [5], [6], [4] from the 1970's using a novel procedure. This theory provides a normalized and balanced version of the forward-backward duality which is essential for our new formulation of the two-filter Mayne-Fraser-like formula uniformly applicable to intervals with or without observations. The paper is structured as follows. In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving

I. I NTRODUCTION Data fusion is the process of integrating different data sets, or statistics, into a more accurate representation for a quantity of interest. A case in point in the context of systems and control is provided by the Mayne-Fraser two-filter formula [1], [2] in which the estimates generated by two different filters are merged into a combined more reliable estimate in fixed-interval smoothing. The purpose of this paper is to develop such a two-filter formula that is universally applicable to smoothing and interpolation based on general records with missing observations. In [3], [4] the Mayne-Fraser formula was analyzed in the context of stochastic realization theory and was shown that it can be formulated in terms a forward and a backward Kalman filter. In a subsequent series of papers, Pavon [5], [6] addressed in a similar manner the hitherto challenging problem of interpolation [7], [8], [9], [10]. This latter problem consists of reconstructing missing values of a stochastic process over a given interval. In departure from the earlier statistical literature, [5], [6] considered a stationary process with rational spectral density and, therefore, reliazable as the output of a linear stochastic system. Interpolation was then cast as seeking an estimate of the state process based on an incomplete observation record. A basic tool in these works is the concept of time-reversal in stochastic systems which has been central in stochastic realization theory (see, e.g., [11], [12], [13], [14], [5], [6], [15], [16], [17]). For a recent
Research supported by grants from AFOSR, NSF, VR, and the SSF. T.T. Georgiou is with the Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota; email: tryphon@umn.edu and A. Lindquist is with the Department of Automation and the Department of Mathematics, Shanghai Jiao Tong University, Shanghai, China, and the Center for Industrial and Applied Mathematics and ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden; email: alq@kth.se

2

generating processes, in opposite time-directions, via causal and anti-causal mappings, respectively. This is most easily understood and explained in discrete-time and hence we begin with that. In Section III we utilize this mechanism in the context of general output processes and, similarly, introduce a pair of time-opposite models. These two introductory sections, II and III, deal with stationary models for simplicity and are largely based on [20]. The corresponding generalizations to time-varying systems are given in Section IV and in the appendix, in continuous and discrete-time, respectively. In Section V we explain Kalman filtering for problems with missing information in the continuous-time setting. In this, we first consider the case where increments of the output process across intervals of no information are unavailable as a simplified preliminary, after which we focus on the central problem where the output process is the object of observation. Section VI deals with the geometry of information fusion. In Section VII we present a generalized balanced two-filter formula that applies uniformly over intervals where data is or is not available. We summarize the computational steps of this approach in Section VIII. Finally, we highlight the use of the two-filter formula with a numerical example given in Section IX and provide concluding remarks in Section X. II. S TATE DYNAMICS AND ALL - PASS EXTENSION In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. We begin by explaining basic ideas in a stationary setting. In discrete-time systems take the form of a set of difference equations x(t + 1) = Ax(t) + Bw(t)
n×n n×p

this is identical to the one for discrete-time given above (as is well known). In continuous time, stability of the system of equations is equivalent to A having only eigenvalues with negative real part. In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system is all-pass. This is done next.

A. All-pass extension in discrete-time Consider the discrete-time Lyapunov equation P = AP A + BB . (6)

Since A has all eigenvalues inside the unit disc of the complex plane and (3) holds, (6) has as solution a matrix P which is positive definite. The state transformation  = P - 2 x, and F = P - 2 AP 2 , G = P - 2 B, brings (1) into  (t + 1) = F  (t) + Gw(t). (9)
1 1 1 1

(7)

(8)

For this new system, the corresponding Lyapunov equation X = F XF + GG has In as solution, where In denotes the (n × n) identity matrix. This fact, namely, that In = F F + GG (10)

(1)

where t  Z, A  R ,B  R , A has all eigenvalues in the open unit disc D = {z | |z | < 1}, and w(t), x(t) are (centered) stationary vector-valued stochastic processes with w(t) normalized white noise; i.e., E{w(t)w(s) } = Ip ts , (2)

implies that this [F, G] can be embedded as part of an orthogonal matrix U= F H G J , (11)

i.e., a matrix such that U U = U U = In+p . Define the transfer function U(z ) := H (zIn - F )-1 G + J corresponding to  (t + 1) = F  (t) + Gw(t) w ¯ (t) = H (t) + Jw(t). This is also the transfer function of x(t + 1) = Ax(t) + Bw(t) ¯ x(t) + Jw(t), w ¯ (t) = B
1 -2

where E denotes mathematical expectation. The system of equations is assumed to be reachable, i.e., rank B, AB, . . . An-1 B = n. (3)

(12)

In continuous-time, state-dynamics take the form of a system of stochastic differential equations dx(t) = Ax(t)dt + Bdw(t) (4)

(13a) (13b)

where, here, x(t) is a stationary continuous-time vector-valued stochastic process and w(t) is a vector-valued process with orthogonal increments with the property E{dwdw } = Ip dt, (5)

(14a) (14b)

¯ := P H , since the two systems are related by a where B similarity transformation. Hence, ¯ (zIn - A)-1 B + J. U( z ) = B (15)

where Ip is the p × p identity matrix. Reachability of the pair (A, B ) is also assumed throughout and the condition for

3

Now, using the identity We claim that U(z ) is a stable all-pass transfer function (with respect to the unit disc), i.e., that U(z ) is a transfer function of a stable system and that U(z )U(z
-1

In - F F = (zIn - F )(z -1 In - F ) + (zIn - F )F + F (z -1 In - F ), (10) and GJ = -F H , obtained from U U = In+p , this yields U(z )U(z -1 ) = HH + JJ = In+p , as claimed.

) = U(z

-1

) U(z ) = Ip .

(16)

The latter claim is immediate after we observe that, since U U = In+p , U and hence,  (t) = F  (t + 1) + H w ¯ (t) w(t) = G  (t + 1) + J w ¯ (t) or, equivalently, x(t) = P A P -1 x(t + 1) + P 2 H w ¯ (t) w(t) = B P Setting x ¯(t) := P -1 x(t + 1), (18) can be written ¯w x ¯(t - 1) = A x ¯(t) + B ¯ (t) w(t) = B x ¯(t) + J w ¯ (t) with transfer function ¯ +J . U(z ) = B (z -1 In - A )-1 B (21) (20a) (20b) (19)
-1
1

 (t + 1) w ¯ (t)

=

 (t) w(t)

,

B. All-pass extension in continuous-time Consider the continuous-time Lyapunov equation (17a) (17b) AP + P A + BB = 0. (22)

(18a) (18b)

Since A has all its eigenvalues in the left half of the complex plane and since (3) holds, (22) has as solution a positive definite matrix P . Once again, applying (7-8), the system in (4) becomes d (t) = F  (t)dt + Gdw(t). (23a)

x(t + 1) + J w ¯ (t).

We now seek a completion by adding an output equation dw ¯ (t) = H (t)dt + Jdw(t) so that the transfer function U(s) := H (sIn - F )-1 G + J is all-pass (with respect to the imaginary axis), i.e., U(s)U(-s) = U(-s) U(s) = Ip . (25) (24) (23b)

Either of the above systems inverts the dynamical relation ww ¯ (in (14) or (13)).

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and hence, F + F + GG = 0. Utilizing this relationship we note that (sIn - F )-1 GG (-sIn - F )-1 = (sIn - F )-1 (sIn - F - sIn - F )(-sIn - F )-1 = (sIn - F )-1 + (-sIn - F )-1 , and we calculate that U(s)U(-s) = (H (sIn - F )-1 G + J )(G (-sIn - F )-1 H + J ) = JJ + H (sIn - F )-1 (GJ + H ) (JG + H )(-sIn - F )-1 H . (26)

w(t)
-

U

w ¯ (t)
-

Fig. 1: Realization (14) in the forward time-direction.

w(t)


U

w ¯ (t)


Fig. 2: Realization (20) in the backward time-direction. An algebraic proof of (16) is also quite immediate. In fact, U(z )U(z -1 ) = H (zIn - F )-1 G + J + H (zIn - F )
-1

H (z -1 In - F )-1 G + J
-1

For the product to equal the identity, JJ = Ip

=H (zIn - F )-1 GG (z -1 In - F )-1 H + JJ GJ + JG (z In - F )
-1

H

H = -JG .

4

Thus, we may take J = Ip H = -G , and the forward dynamics d (t) = F  (t)dt + Gdw(t) dw ¯ (t) = -G  (t)dt + dw(t). (27a) (27b)

A. Time-reversal of discrete-time stochastic systems Consider a stochastic linear system x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (35a) (35b)

with an m-dimensional output process y , and x, u, A, B are defined as in Section II-A. All processes are stationary and the system can be thought as evolving forward in time from the remote past (t = -). To formalize this, we introduce some notation. Let H be the Hilbert space spanned by {wk (t); t  Z, k = 1, 2, . . . , n}, endowed with the inner product , µ = E{µ}, and let + H- t (w ) and Ht (w ) be the (closed) subspaces spanned by {wk (s); s  t - 1, k = 1, . . . , m} and {wk (s); s  t, k = + 1, . . . , m}, respectively. Define H- t (y ) and Ht (y ) accordingly in terms of the output process process y . Then the stochastic system (35) evolves forward in time in the sense that - + H- (36) t (z )  Ht (w )  Ht (w ), where A  B means that elements of the subspaces A and B are mutually orthogonal, and where H- t (z ) is formed as above in terms of z (t) = x(t + 1) ; y (t)

Substituting F = -F - GG from (26) into (27a) we obtain the reverse-time dynamics d (t) = -F  (t)dt + Gdw ¯ (t) dw(t) = G  (t)dt + dw ¯ (t). Now defining x ¯(t) := P -1 x(t) and using (7) and (8), (28) becomes ¯ w dx ¯(t) = -A x ¯(t)dt + Bd ¯ (t) dw(t) = B x ¯(t)dt + dw ¯ (t), with transfer function ¯ U(s) = Ip + B (sIn + A )-1 B, where ¯ := P -1 B. B (32) (31) (30a) (30b) (29) (28a) (28b)

see [18, Chapter 6] for more details. Next we construct a stochastic system ¯w x ¯(t - 1) = A x ¯(t) + B ¯ (t) ¯ ¯ y (t) = C x ¯(t) + Dw ¯ (t), (37a) (37b)

Furthermore, the forward dynamics (27) can be expressed in the form dx(t) = Ax(t)dt + Bdw(t) ¯ x(t)dt + dw(t) dw ¯ (t) = B with transfer function ¯ (sIn - A)-1 B. U(s) = Ip - B (34) (33a) (33b)

which evolves backward in time from the remote future (t = ) in the sense that the processes x ¯, x, w, ¯ w relate as in the previous section. More specifically, as shown in Section II-A, H- (w ¯ )  H- (w) and H+ (w)  H+ (w ¯ ) for all t, as examplified in Figures 1 and 2. In fact, the all-pass extension (14) of (35a) yields ¯ x(t) + Jw(t) w ¯ (t) = B It follows from (20b) that (38) can be inverted to yield (38)

III. T IME - REVERSAL OF STATIONARY LINEAR
STOCHASTIC SYSTEMS

w(t) = B x ¯(t) + J w ¯ (t),

(39)

The development so far allows us to draw a connection between two linear stochastic systems having the same output and driven by a pair of arbitrary, but dual, stationary processes w(t) and w ¯ (t), one evolving forward in time and one evolving backward in time. When one of these two processes is white noise (or, orthogonal increment process, in continuous-time), then so is the other. For this special case we recover results of [11] and [5], [6] in stochastic realization theory.

where x ¯(t) = P -1 x(t + 1), and that we have the reverse-time recursion ¯w x ¯(t - 1) = A x ¯(t) + B ¯ (t). (40a) Then inserting (39) and ¯w x(t) = P x ¯(t - 1) = P A x ¯(t) + P B ¯ (t) into (35b), we obtain ¯x ¯w y (t) = C ¯(t) + D ¯ (t), (40b)

5

¯ := CP B ¯ + DJ and where D ¯ := CP A + DB . C Then, (40) is precisely what we wanted to establish. The white noise w is normalized in the sense of (2). Since U, given by (15), is all-pass, w ¯ is also a normalized white noise process, i.e., E{w ¯ (t)w ¯ (s) } = Ip t-s . From the reverse-time recursion (37a)


(41)

+ same inner product as above, and let H- t (du) and Ht (du) be the (closed) subspaces spanned by the increments of the components of U on (-, t] and [t, ), respectively. Define + H- t (dy ) and Ht (dy ) accordingly in terms of the output process y . All processes have stationary increments and the stochastic system (45) evolves forward in time in the sense that - + H- (46) t (dz )  Ht (dw )  Ht (dw ),

where H- t (dz ) is formed in terms of z (t) = x(t) . y (t) (47)

x ¯(t) =

¯w (A )k-(t+1) B ¯ (k ).
k=t+1

The all-pass extension of Section II-B yields ¯ xdt dw ¯ = dw - B as well as the reverse-time relation ¯ w dx ¯ = -A x ¯dt + Bd ¯ dw = B x ¯dt + dw, ¯ (42) where x ¯(t) = P -1 x(t). Inserting (49b) into dy = CP x ¯dt + Ddw (49a) (49b) (48)

Since, w ¯ is a white noise process, E{x ¯(t)w ¯ (s) } = 0 for all s  t. Consequently, (37) is a backward stochastic realization in the sense defined above. Moreover, the transfer functions W(z ) = C (zIn - A)-1 B + D of (35) and ¯ (z ) = C ¯ (z -1 In - A )-1 B ¯ +D ¯ W of (37) satisfy ¯ (z )U(z ). W(z ) = W (44) (43)

yields ¯x dy = C ¯dt + Ddw, ¯ where ¯ = CP + DB . C Thus, the reverse-time system is ¯ w dx ¯ = -A x ¯dt + Bd ¯ ¯ dy = C x ¯dt + Ddw. ¯ (51a) (51b) (50)

In the context of stochastic realization theory, U(z ) is called structural function ([13], [14]).

w(t)
-

W

y (t)
-

Fig. 3: The forward stochastic system (35).

From this, we deduce that the system (45) has the backward property H+ ¯)  H+ ¯ )  H- ¯ ), (52) t (dz t (dw t (dw where H+ ¯) is formed as above in terms of t (dz

y (t)


¯ W

w ¯ (t)


z ¯(t) =

x ¯(t) . y (t)

We also note that the transfer function Fig. 4: The backward stochastic system (37) W(s) = C (sIn - A)-1 B + D of (45) and the transfer function B. Time-reversal of continuous-time stochastic systems We now turn to the continuous-time case. Let dx = Axdt + Bdw dy = Cxdt + Ddw (45a) (45b) as in discrete-time. Note that the orthogonal-increment process w is normalized in the sense of (5). Since U(s) is all-pass, ¯ xdt dw ¯ = du - B (53) ¯ (s) = C ¯ (sIn + A )-1 B ¯ +D W of (51) also satisfy ¯ (s)U(s) W(s) = W

be a stochastic system with x, w, A, B as in Section II-B, evolving forward in time from the remote past (t = -). Now let H be the Hilbert space spanned by the increments of the components of w on the real line R, endowed with the

6

also defines a stationary orthogonal-increment process w ¯ such that {dw ¯ (t)dw ¯ (t) } = Ip dt. It remains to show that (51) is a backward stochastic realization, that is, at each time t the past increments of w ¯ are orthogonal to x ¯(t). But this follows from the fact that


Differentiating P (t)- 2 P (t)P (t)- 2 = In , we obtain
1 1  P (t)- 2 = -R(t) - R(t) , P (t)- 2 P

1

1

and hence the (55) yields F (t) + F (t) + G(t)G(t) = 0. Using (61) to eliminate F in (57), we obtain d = -F (t)  (t)dt + G(t)dw, ¯ where dw ¯ = dw - G(t)  (t)dt, (63) (62) (61)

x ¯(t) =
t

e

-A (t-s)

¯ w Bd ¯ (s)

and w ¯ has orthogonal increments.

IV. T IME REVERSAL OF NON - STATIONARY STOCHASTIC
SYSTEMS

which can also be written ¯ (t) x(t)dt, dw ¯ = dw - B ¯ (t) := P (t)-1 B (t). where B Proposition 1: A process w ¯ satisfying (63) has orthogonal increments with the normalized property (5). Moreover, E{[w ¯ (t) - w ¯ (s)] (t) } = 0 for all s  t. Proof: As is well-known, the solution of (57) can be written in the form
t

(64)

In a similar manner non-stationary stochastic systems admit unitary extensions which in turn allows us to construct dual time-reversed stochastic models that share the same state process. The case of discrete-time dynamics is documented in the appendix, whereas the continuous-time counterpart is explained next as prelude to smoothing and interpolation that will follow.

(65)

A. Unitary extension The covariance matrix function P (t) := E{x(t)x(t) } of the time-varying state representation dx = A(t)x(t)dt + B (t)dw, x(0) = x0 (54)

 (t) = (t, s) (s) +
s

(t,  )G( )dw,

(66)

where (t, s) is the transition matrix with the property  (t, s) = F (t)(t, s), (s, s) = In (67a) t  (t, s) = -(t, s)F (s), (t, t) = In (67b) s Let s  t. Then, in view of (63), a straight-forward calculation yields w ¯ (t) - w ¯ (s) = w(t) - w(s)
t

with x0 a zero-mean stochastic vector with covariance matrix P0 = E{x0 x0 }, satisfies the matrix-valued differential equation  (t) = A(t)P (t) + P (t)A(t) + B (t)B (t) P (55) with P (0) = P0 . Throughout we assume total reachability [18, Section 15.2], and therefore P (t) > 0 for all t > 0. A unitary extension of (54) is somewhat more complicated than in the discrete time case. In fact, differentiating  (t) = P (t)- 2 x(t) we obtain d = F (t) (t)dt + G(t)dw, where F (t) = P (t)- 2 A(t)P (t) 2 + R(t), G(t) = P (t) with R(t) = In fact, d = P (t)- 2 dx + R(t) (t)dt.
1 1 1 1

- M (t, s) (s) -
s

M (t,  )G( )dw,

(68)

where M (t, s) =
s

t

(56) Therefore, (57)

G( ) (, s)d.

(69)

E{[w ¯ (t) - w ¯ (s)][w ¯ (t) - w ¯ (s)) } = Ip (t - s) + (t, s), where (58a) (58b) (t, s) = M (t, s)M (t, s) +
s t
1 1 d P (t)- 2 P (t) 2 . dt

t

-1 2

B (t)

M (t,  )G( )G( ) M (t,  ) d

- (59)
s

[M (t,  )G( ) + G( ) M (t,  ) ] d.

However, (t, s) is identically zero. To see this, first note that (60) M (t, s) = -M (t, s)F (s) - G(s) . s (70)

7

Then, in view of (61), a simple calculation shows that  (t, s)  0. s Since (t, t) = 0, the assertion follows. Hence the incremental covariance is normalized. Next, we show that w ¯ (t) has orthogonal increments. To this end, choose arbitrary times s  t  a  b on the interval [0, T ], where we choose a and b fixed, and show that Q(t, s) := E{[w ¯ (b) - w ¯ (a)][w ¯ (t) - w ¯ (s)) } is identically zero for all s  t. Using (68) and w ¯ (b) - w ¯ (a) = w(b) - w(s) - M (b, a)(a, s) (s)
b b

B. Time reversal in continuous-time systems Next we derive the backward stochastic system corresponding to the non-stationary forward stochastic system dx = A(t)x(t)dt + B (t)dw, dy = C (t)x(t)dt + D(t)dw, x(0) = x0 y (0) = 0 (75a) (75b)

defined on the finite interval [0, T ], where x0 (with covariance P0 ) and the normalized Wiener process w are uncorrelated. To this end, apply the transformation x ¯(t) = P (t)-1 x(t) together with (74b) to (75b) to obtain ¯ (t)¯ dy = C x(t) + D(t)dw, ¯ where ¯ (t) = C (t)P (t) + D(t)B (t). C (77) This together with (74a) yields the the backward system corresponding to (75), namely ¯ (t)dw dx ¯ = -A(t) x ¯(t)dt + B ¯ ¯ dy = C (t)¯ x(t)dt + D(t)dw. ¯ with end-point condition x ¯(T ) = P (T ) the Wiener process w ¯.
-1

(76)

- M (b, a)
s

(a,  )G( )dw -
a

M (b,  )dw

computed analogously, we obtain
b

Q(t, s) = M (b, a) (a, s)M (t, s) -
s b

(a,  )G( )d

+
s

(a,  )G( )G( ) M (t,  )d .

(78a) (78b)

Then, again using (61), we see that M (t, s)  0, s so, since Q(t, t) = 0, we see that Q(t, s) is identically zero, establishing that w ¯ (t) has orthogonal increments. Finally, we use the same trick to show (65). In fact, for s  t, (66) and (68) yield E{[w ¯ (t) - w ¯ (s)) (t) } = -M (t, s)(t, s)
t t

x(T ) uncorelated to

The backward realization (78) was derived in [3], but in cumbersome way, requiring the proof that w ¯ (t) is a normalized process with orthogonal increments to be suppressed. What is new here is imposing the unitary map between w and w ¯, making the analysis much simpler and more natural. V. K ALMAN FILTERING WITH MISSING OBSERVATIONS We consider the linear stochastic system (75) which does not have a purely deterministic component that enables exact estimation of components of x from y , an assumption that we retain in the rest of the paper. In the engineering literature is often the case that the stochastic system (75) represented as x  (t) = A(t)x(t) + B (t)w  (t), y  (t) = C (t)x(t) + D(t)w  (t) x(0) = x0 (79a) (79b)

+
s

G( ) (t,  ) d -
s

M (t,  )G( )G( ) )(t,  ) d,

the partial derivative of which with respect to s is identical zero; this is seen by again using (61). Therefore, since (65) is zero for s = t, it is identical zero for all s  t, as claimed. This concludes the proof of Proposition 1. Consequently, (57) and (64) form a forward unitary system dx = A(t)x(t)dt + B (t)dw ¯ (t) x(t)dt, dw ¯ = dw - B (71a) (71b)

The corresponding backward unitary system is obtained through the transformation x ¯(t) = P (t) 2  (t), which yields dx ¯ = P (t)- 2 d + R(t) (t)dt. This together with (62) and (63) yields ¯ (t)dw dx ¯ = -A(t) x ¯(t)dt + B ¯ dw = B (t) x ¯(t)dt + dw, ¯ (74a) (74b)
1 1

(72)

(73)

where the formal "derivative" w  is white noise, i.e., E{w  (t)w  (s) } = I (t - s) with  (t - s) being the Dirac "function". Of course x , y  and w  are to be interpreted as generalized stochastic processes. From a mathematically rigorous point of view, observing y  makes little sense since, for any fixed t, y  (t) has infinite variance and contains no information about the state process x. However, observations of y  could be interpreted as observations of the increments dy of y in a precise meaning to be defined next. On the other hand, one can think of (75) as a system of type dz = M (t)z (t)dt + N (t)dw(t), where z (t) = x(t) , y (t)

8

and one would like to determine the optimal linear leastsquares estimate of x(t) given past observed values of y . Generally this distinction between observing y or dy is not important. However, when there is loss of information over an interval (t1 , t2 ), there are two different information patterns depending on whether dy or y is observed. The difference consists in whether y := y (t2 ) - y (t1 ) is part of the observation record or not. These two cases will be dealt with separately in subsections below. In fact, the former, which is common in engineering applications, is provided as a simplified preliminary, whereas our main interest is in the latter. To this end, we first introduce some notation. Consider the stochastic system (75) on a finite interval [0, T ]. As before, let H be the Hilbert space spanned by {wk (t) - wk (s); s, t  [0, T ], k = 1, 2, . . . , m}, endowed with the inner product , µ = E{µ}. For any   H and any subspace A, let EA denote the orthogonal projection of  onto A. We denote by H[t1 ,t2 ] (dy ) the (closed) subspace generated by the components of the increments of the observation process y over the window [t1 , t2 ]. In particular, we shall also use the notations H- t (dy ) := H[0,t] (dy ) and H+ t (dy ) := H[t,T ] (dy ). Suppose that the output process or its increments are available for observation only on some subintervals of [0, T ],  namely Ik , k = 1, 2, . . . ,  . Next we want to define H as the proper subspace of H[0,T ] (dy ) spanned by the observed data. In the case that only the increments dy or, equivalently, the "derivative" y  is observed, we simply define H := HI1 (dy )  HI2 (dy )  · · ·  HI (dy ), In the case that the process y is observed, we need to expand  H by adding the subspaces spanned by the increments y over the complementary intervals without observation. In either case, we define
- H- t := H  Ht (dy ) and   + H+ t := H  Ht (dy ).   

with R(t) = D(t)D(t) and initial conditions x- (0) = 0 and Q(0) = P0 . Here Q- (t) is the error covariance Q- (t) := E{[x(t) - x- (t)](x(t) - x- (t)] }, (83)

which, by the nondeterministic assumption, is positive definite for all t. Next suppose the observation process becomes unavailable over the interval [t1 , t2 )  [0, T ]. Then the Kalman filter needs to be modified accordingly. In fact, for any t  [t1 , t2 ), (81)
- holds with the space of observations H- t := Ht1 (dy ), and consequently 

a x- (t) = EHt1 (dy) a x(t) = a (t, t1 )x- (t1 ). This corresponds to setting K- (t) = 0 in (82) on the interval [t1 , t) so that dx- = A(t)x- (t)dt (84a) with initial condition x- (t1 ) given by (82a). The error covariance Q- is then given by the Lyapunov equation  - (t) = AQ- + Q- A + BB Q (84b)

-

with initial the condition Q- (t1 ) given by the value produced in the previous interval. Then suppose observations of dy become available again on the interval [t2 , t3 ). Then, for any t  [t2 , t3 ), we have H+ t = H[0,t1 ]  H[t2 ,t] , so the Kalman estimate is generated by (82) but now with initial conditions x- (t2 ) and Q- (t2 ) being those computed in the previous step without observation. In the case there are more intervals, one proceeds similarly by alternating between filters (82) and (84) depending on whether increments dy are available or not. In an identical manner, a cascade of backward Kalman filters generates a process x ¯+ (t) based on the backward stochastic realization (78) and the observation windows [t, T ]. Assuming that there are observations in a final interval ending at t = T , on that interval the Kalman estimate




(80)

Then Kalman filtering with missing observations amounts to determining a recursion for x- where


ax ¯+ (t) = EHt a x ¯(t),


+

(85)

a x- (t) = EHt a x(t),

-

for all a  Rn .

(81)

with initial observation space H+ t := H[t,T ] , is generated by the backward Kalman filter dx ¯+ = -A(t) x ¯+ (t)dt ¯ + (t)(dy (t) - C ¯ (t)¯ +K x+ (t)dt) ¯ + = -(Q ¯+C ¯ - BD ¯ ) R -1 K  = -A Q ¯ ¯+ - Q ¯+A + K ¯ + R(t)K ¯ + (t) - B ¯B ¯ Q +

A. Observing dy only When observations are available on the interval [0, t1 ], the Kalman filter on that interval is given by dx- = A(t)x- (t)dt + K- (t)(dy (t) - C (t)x- (t)dt) (82a) K- = (Q- C + BD )R
-1

(86a) (86b) (86c)

(82b) (82c)

¯ + (T ) = P ¯ (T ) for x and initial conditions x ¯+ (T ) = 0 and Q ¯+ and the error covariance ¯ + (t) := E{[¯ Q x(t) - x ¯+ (t)][¯ x(t) - x ¯+ (t)] }, (87)

 - (t) = AQ- + Q- A - K- RK- + BB Q

9

which like Q- (t) is positive definite for all t. During periods ¯ + = 0. This of no observations of dy , we then set the gain K update is obtained from the backward time stochastic model (74) in an identical manner to that of (84). Consequently, both the underlying process as well as the filter can run in either time-direction. This duality becomes essential in subsequent sections where we will be concerned with smoothing and interpolation. B. Observing y Now consider the case that y , and note merely dy , is available for observation on all intervals Ik , k = 1, 2, . . . ,  . Under this scenario and with a continuous-time process the dynamics of Kalman filtering become hybrid, requiring both continuous-time filtering when data is available as well as a discrete-time update across intervals where measurements are not available. Then on the first interval [0, t1 ] the Kalman estimate (82) will still be valid. However, when t reaches the endpoint t2 of the interval of no information and an observation of y is obtained again, the subspace of observed data becomes
- Ht = H- t1  H(y ), 2 

where u(t1 ) = u1 (t1 ) u2 (t1 ) = Bd v (t1 ) Dd

and Bd and Dd are chosen so that
Bd Dd
t2

Bd , Dd =

t1

(t2 , s)BB (t, s) (t2 , s)BM (s) M (s)B (t2 , s) M (s)M (s)

ds

while E {v (t1 )v (t1 ) } = I . Hence, across the window of missing data the Kalman state estimate x- is now generated by a discrete-time Kalman-filter step x- (t2 ) = Ad x- (t1 ) + Kd (y - Cd x- (t1 )) Kd = (Ad Q(t1 )Cd + Bd Dd ) × (Cd Q(t1 )Cd + Dd Dd )-1 (89b) (89a)

with initial conditions x- (t1 ) and Q(t1 ) given by (82) and the error covariance at t2 by Q(t2 ) = Ad Q(t1 )Ad - Kd (Cd Q(t1 )Cd + Dd Dd )Kd + Bd Bd . (89c)

In the next interval [t2 , t3 ], where observations of y are available, the new Kalman estimate (81) with H+ t = H[0,t1 ]  H(y )  H[t2 ,t] is again generated by the continuous-time Kalman filter (82) starting from x- (t2 ) and Q(t2 ) given by (89). Again given an observation pattern, where intermittently y becomes unavailable for observation, the Kalman estimate (81) can be generated in precisely this manner by a cascade of continuous and discrete-time Kalman filters. Remark 2: The observation pattern of a continuous-time stochastic model, where y becomes unavailable over particular time-windows, is closely related to hybrid stochastic models where continuous-time diffusion is punctuated by discrete-time transitions. Indeed, unless interpolation of the statistics within windows of unavailable data is the goal, the end points of such intervals can be identified and the same hybrid model utilized to capture the dynamics. Remark 3: A common engineering scenario is the case where the signal is lost while the observation noise is still present. This amounts to having C  0 over the corresponding window, and the Kalman estimates are obtained by merely running the filters (82) and (86) in the two time directions with the modified condition on C . This situation does not cover the information patterns discussed above since, whenever BD = 0, the Kalman gains do not vanish and information about the state process is available even when C is zero.


where y := y (t2 ) - y (t1 ). Computing x(t2 ) across the window (t1 , t2 ] as a function of x(t1 ) and the noise components we have that
t2

x(t2 ) = (t2 , t1 ) x(t1 ) +
t1 Ad

(t2 , s)Bdw(s)
u1 (t1 )

while
t2 t2

y (t2 ) = y (t1 ) +
t1

C (t)x(t)dt +
t1

D(t)dw(t).

Therefore,
t2

y =
t1

C (t)(t, t1 )dt) x(t1 ) + u2 (t1 )
Cd

where
t2 t1

u2 (t1 ) =
t1

C (t)
t

(t, s)B (s)dw(s)dt
t2

+
t1 t2 t2

D(s)dw(s)

=
t1 t

C (t)(t, s)dtB (s) + D(s) dw(s).
M (s)

Thus, we obtain the discrete-time update x(t2 ) = Ad x(t1 ) + Bd v (t1 ) y = Cd x(t1 ) + Dd v (t1 ) (88a) (88b)

10

C. Smoothing Given these intermittent forward and backward Kalman estimates, we shall derive a formula for the smoothing estimate


proving condition (i). Condition (ii) follows from a symmetric argument. To prove (iii) we use condition (93). To this end, first note that, by the usual projection formula, ¯+ (t)-1 x EX+ (t) a x- (t) = E{a x- (t)¯ x+ (t)}P ¯+ (t) (94)

ax ^(t) := EH a x(t),

a  Rn ,

(90)

= a E{x- (t)¯ x+ (t) }x+ (t),

valid for both the cases discussed above, where H :=
 - Ht 



+ Ht



 H[0,T ] (dy )

(91)

¯+ (t)-1 x where x+ (t) := P ¯+ (t) is the dual basis in X+ (t) such that E{x+ (t)¯ x+ (t) } = I . Moreover, EX(t) a x- (t) = E {a x- (t)x(t) }P (t)-1 x(t) = a E{x- (t)x(t) }x ¯(t) = a P- (t)¯ x(t), where we have used condition (i) and (76). Next, set b := P- a and form ¯+ (t)-1 x EX+ (t) b x ¯(t) = E{b x ¯(t)¯ x+ (t)}P ¯+ (t) = b E{x ¯(t)¯ x+ (t)}x+ (t) ¯ = b P+ (t)x+ (t), by condition (ii), and consequently ¯+ (t)x+ (t). EX+ (t) E X(t) a x- (t) = a P- (t)P Then condition (iii) follows from (93a), (94) and (95). Remark 5: The proof of condition (iii) in Lemma 4 could be simplified if x ¯+ were a regular backward Kalman estimate without intermittent loss of information. In this case, x+ = ¯ -1 x P + ¯+ would be generated by a forward stochastic realization belonging to the same class as (75) and E{x ¯+ (t)x- (t) } = ¯ ¯ P+ (t) E{x+ (t)x- (t)} = P+ (t) E{x- (t)x- (t)}. Lemma 6: For each t  [0, T ], the smoothing estimate x ^(t), defined by (90), is given by ax ^(t) = E Ht a x(t), a  Rn , (96) (95)

is the complete subspace of observations. This is discussed next. VI. G EOMETRY OF FUSION Consider the system (75), and let X(t) be the (finitedimensional) subspace in H spanned by the components of the stochastic state vector x(t). Then it can be shown [18, Chapter 7] that H[0,t] (dy )  H[t,T ] (dy ) | Xt , where A  B | X denotes the conditional orthogonality  - E ,  - E  = 0 for all   A,   B.
X X

(92)

Next, let X- (t) and X+ (t) be the subspaces spanned by the components of the (intermittent) Kalman estimates x- (t) and  x ¯+ (t), respectively. Then since X- (t)  H- t  H[0,t] (dy ) and X+ (t) 
+ Ht 

 H[t,T ] (dy ), we have X- (t)  X+ (t) | X(t),

which is equivalent to EX+ (t) a x- (t) = EX+ (t) EX(t) a x- (t), [18, Proposition 2.4.2]. Therefore the diagram X-
EX |X- EX+ |X-

a  Rn

(93a)

-

X+
EX+ |X

(93b) where Ht is the subspace Ht = X- (t)  X+ (t).


X commutes, where the argument t has been suppressed. Lemma 4: Let x(t), x ¯(t), x- (t) and x ¯+ (t) be defined as above. Then, for each t  [0, T ], (i) E{x(t)x- (t) } = P- (t) ¯+ (t) (ii) E{x ¯(t)¯ x+ (t) } = P ¯+ (t)P- (t), (iii) E{x ¯+ (t)x- (t) } = P where P- (t) := E{x- (t)x- (t) } is the state covariance of the Kalman estimate x- (t) and P+ (t) := E{x ¯+ (t)¯ x+ (t) } is the covariance of the backward Kalman estimate x ¯+ (t). Proof: By the definition of the Kalman filter, (81) holds, and consequently the components of the estimation error x(t) - x- (t) are orthogonal to H- t and hence to the components of x- (t). Therefore, E{x(t)x- (t) } = E{x- (t)x- (t) } = P- (t),

(97)

Proof: Following [14], [3], [18], define N- (t) := H- t  X- (t) and N+ (t) := H+ X+ (t). Then t
- + H = N (t)  Ht  N (t). 

Now, a (x(t) - x- (t)) is orthogonal to H- t and hence to N- (t). Also a x- (t)  N- (t). Hence a x(t)  N- (t) as well. In the same way we see that a x(t)  N+ (t). Therefore (96) follows. Consequently, the information from the two Kalman filters can be fused into the smoothing estimate ¯ + (t)¯ x ^(t) = L- (t)x- (t) + L x+ (t) ¯+. for some matrix functions L- and L (98)



11

VII. U NIVERSAL TWO - FILTER FORMULA To obtain a robust and particularly simple smoothing formula that works also with an intermittent observation pattern, we assume that the stochastic system (75) has already been transformed via (58) so that, for all t  [0, T ], x(t) = x ¯(t) and therefore ¯ (t). P (t) = E{x(t)x(t) } = I = P (100) (99)

Then (102) follows from (98) and (107). To prove (104) ¯ + in (106) to obtain eliminate L ¯+ ) = Q ¯+, L - ( I - P- P which together with (107) yields
1 ¯ ¯ -1 Q-1 = Q- - (I - P- P+ )Q+ .

However, ¯+ = Q ¯ + + Q- - Q- Q ¯+, I - P- P and hence (104) follows. In the special case with no loss of observation this is a normalized version of the Mayne-Frazer two-filter formula [1], [2], which however in [1], [2] was formulated in terms of x- and x+ rather than x ¯+ , where x+ is the state process of the forward stochastic system of the backward Kalman filter. (For the corresponding formula in terms of x- and x ¯+ , see [3], [18]; also cf. [21], where an independent derivation was given.) With a single interval of loss of observation the formula (102) reduces to a version of the interpolation formulas in [6]. The remarkable fact, discovered here, is that the same formula (102) holds for any intermittent observations structure and by a cascade of continuous and discrete-time forward and backward Kalman filters, as needed depending on the assumed information pattern. VIII. R ECAP OF COMPUTATIONAL STEPS Given a system (75) with state covariance (55), make the normalizing substitution A(t)  P (t)- 2 A(t)P (t) 2 + R(t) B (t)  P (t)- 2 B (t) C (t)  C (t)P (t)
1 1 2 1 1 1 1

Then the error covariances in the filtering formulas of Section V are Q- = I - P- ¯+ = I - P ¯+ . and Q (101)

¯+ (t) are all bounded in Consequently, x(t), x ¯(t), P- (t) and P norm by one for all t  [0, T ]. Theorem 7: Suppose that (99) holds. For every t  [0, T ], we have the formula ¯ + (t)-1 x x ^(t) = Q(t) Q- (t)-1 x- (t) + Q ¯+ (t) (102)

for the smoothing estimate (90), where the estimation error Q(t) := E (x(t) - x ^(t)) (x(t) - x ^(t)) is given by ¯ + (t)-1 - I, Q(t)-1 = Q- (t)-1 + Q (104) (103)

and where x- , x ¯+ , Q- and Q+ are given by (82) and (86) with boundary conditions x- (0) = x ¯+ (T ) = 0 and Q- (0) = Q+ (T ) = I . ¯ + in (98) Proof: Clearly the matrix functions L- and L can be determined from the orthogonality relations E{[x(t) - x ^(t)]x- (t) } = 0 and E{[x(t) - x ^(t)]¯ x+ (t) } = 0. By Lemma 4, (105) yields ¯+P ¯+ P- = 0 P- - L- P- - L ¯+ - L- P- P ¯+ - L ¯+P ¯+ = 0, P ¯+ are positive definite, which, in view of the fact that P- and P yields ¯+P ¯+ = I L- + L (106a) ¯+ = I L- P- + L Again by orthogonality and Lemma 4, ¯+P ¯+ , Q = E {(x - x ^ ) x } = I - L- P - - L which, in view of (106) and the relations (101), yields L- =
1 QQ- -

(108)

(105a) (105b)

-2 with R(t) = d P (t) 2 . Next, we compute the dt P (t) intermittent forward and backward Kalman filter estimates x- and x ¯+ , respectively, along the lines of Section V, where, ¯ + (T ) = In . Then the due to the normalization, Q- (0) = Q smoothing estimate is given by

¯ + (t)-1 x x ^(t) = Q(t) Q- (t)-1 x- (t) + Q ¯+ (t) , where ¯ + (t)-1 - I Q(t) = Q- (t)-1 + Q IX. A N EXAMPLE We now illustrate the results of the paper on a specific numerical example. We consider the continuous-time diffusion process dx1 (t) dx2 (t) (107) dy (t) = x2 (t)dt = -0.3x1 (t)dt - 0.7x2 (t)dt + dw(t) = x1 (t)dt + dv (t)
-1

.

(106b)

and

¯+ = L

¯ -1 . QQ +

12

where w and v are thought to be independent standard Wiener processes. Here, x1 is thought of as position and x2 as velocity of a particle that is steered by stochastic excitation in dw, in the presence of a restoring force 0.3x1 and frictional force 0.7x2 . Then dy/dt represents measurement of the position and dv/dt represents measurement noise (white).
50 0 -50 0.5 0 5 10 15 20 25 30 35 40 45

of intervals, data are not made available for state estimation; these intervals where data are not to be used are marked by a thick blue baseline in the figures. In Figure 5 we display sample paths of the output process y , increments dy , and stateprocesses x1 and x2 .
2 1 0 -1 -2 -3 0 5 10 15 20 25 30 35
x 1,est x1

KF backward estimate x 1 - missing data in blue intervals

Output process and states

dy

y

0 -0.5 5 0 5 10 15 20 25 30 35 40 45

40

45

4 2
0 5 10 15 20 25 30 35 40 45

KF backward estimate x 2 - missing data in blue intervals
x 2,est x2

x1

0 -5 5

0 -2

x2

0 -5 0 5 10 15 20 25 30 35 40 45

-4

0

5

10

15

20

25

30

35

40

45

Fig. 5: Sample paths of output process, increment, and state processes
KF forward estimate x - missing data in blue intervals
2 1 0 -1 -2 -3 0 5 10 15 20 25 30 35 40
x est,1 x1
1

Time [sec]

Fig. 7: Kalman estimates in the backward time direction
2 1 0 -1 -2 -3 0 5 10 15 20 25 30 35
x 1,smooth x1

40

45

45 4 2 0 -2 -4 0 5 10

Smoothed state estimates
x 2,smooth x2

4 2 0 -2 -4 0

KF forward estimate x 2 - missing data in blue intervals
x est,2 x2

15

20

25

30

35

40

45

5

10

15

20

25

30

35

40

45

Time [sec]

Time [sec]

Fig. 6: Kalman estimates in the forward time direction Numerical simulation over [0, T ] with T = 45 (units of time) produces a time-function y (t) which is sampled with integer multiples of t = 0.01 (units). The interval [0, T ] is partitioned into [0, T ] = 9 i=1 [ti-1 , ti ] where t0 = 0 and ti - ti-1 = i (units). Measurements of y are made available for purposes of state estimation over the intervals [ti-1 , ti ] for i = 1, 3, 5, 9. Over the complement set

Fig. 8: Interpolation/smoothed estimates by fusion of Kalman forward and backward estimates The process increments dy over [ti-1 , ti ] for i = 1, 3, 5, 9 as well as the increments y across the [ti-1 , ti ] for i = 2, 4, 6, 8 are used in the two-filter formula for the purpose of smoothing. The Kalman estimates for the states in the forward and backwards in time directions, x- (t) and x ¯+ (t) are shown in Figures 6 and 7, respectively. The fusion of the two using (102) is shown in Figure 8. It is worth observing the nature and fidelity of the estimates. In the forward direction, across intervals where data is not available, x- becomes

13

increasing more unreliable whereas the opposite is true for x ¯+ , as expected. The smoothing estimate is generally an improvement to those of the two Kalman filters as seen in Figure 8. In particular, it is worth noting x2 (in subplot 2), where, over windows of available observations, estimates have considerably less variance in the middle of the interval where ¯ + (t)-1 ) in (102) are the weights (Q(t)Q- (t)-1 and Q(t)Q equalized, whereas sample paths become increasing rugged at the two ends where one of the two Kalman estimates has significantly higher variance, and the corresponding mixing coefficient becomes relatively smaller. X. C ONCLUDING REMARKS Historically the problem of interpolation has been considered from the beginning of the study of stochastic processes [22], [23]. Early accounts and treatments were cumbersome and non-explicit as the problem was considered difficult [7], [8], [9], [10]. In a manner that echoes the development of Kalman filtering, the problem became transparent and computable for ouput processes of linear stochastic systems [5], [6], [18]. This paper builds on developments in stochastic realization theory [11], [24] and presents a unified and generalized twofilter formula for smoothing and interpolation in continuous time for the case of intermittent availability of data over an operating window. The analysis considers two alternative information patterns where increments of the output process or the output process itself is recorded when information becomes available. The second information pattern appears most natural to us in this continuous-time setting, and this is our main problem. Nevertheless, in either case, two Kalman filters run in opposite time-directions, designed on the basis of a forward and a backward model for the process, respectively. Fusion of the respective estimates is effected via linear mixing in a manner similar to the Mayne-Fraser formula and applies to both smoothing and interpolation intermixed. In earlier works, smoothing and interpolation have been considered separate problems [18, Chapter 15]. The balancing normalization also simplifies the mixing formula and makes it completely time symmetric. The theory relies on time-reversal of stochastic models. We provide a new derivation of such a reversal which has the convenient property of being balanced. It is based on lossless imbedding of linear systems and effects the time reversal through a unitary transformation. Interestingly, time symmetry in statistical and physical laws have occupied some of the most prominent minds in science and mathematics. In particular, closer to our immediate interests, dual time-reversed models have been employed to model, in different time-directions, Brownian or Schr¨ odinger bridges [25], [26], a subject which is related to reciprocal processes [27], [28], [29], [30]. A natural

extension of the present work in fact is in the direction of general reciprocal dynamics [28], [29] and the question of whether similar two-filter formula are possible. A PPENDIX : T IME REVERSAL OF NON - STATIONARY DISCRETE - TIME SYSTEMS Next, instead of (1), consider the non-stationary state dynamics x(t + 1) = A(t)x(t) + B (t)w(t), x(0) = x0 , (109)

on a finite time-window [0, T ], where, for simplicity we now assume that the covariance matrix P0 := P (0) of the zero-mean stochastic vector x0 is positive definite, i.e., P0 = E{x0 x0 } > 0. Then the state covariance matrix P (t) := E{x(t)x(t) } will satisfy the Lyapunov difference equation P (t + 1) = A(t)P (t)A(t) + B (t)B (t) . The state transformation  (t) = P (t)- 2 x(t) brings the system (109) into the form  (t + 1) = F (t) (t) + G(t)w(t), where now E{ (t) (t) } = In for all t and F (t) = P (t + 1)- 2 A(t)P (t) 2 , G(t) = P (t + 1)
-1 2
1 1 1

(110)

(111)

(112)

(113a) (113b)

B.

The Lyapunov difference equation then reduces to In = F (t)F (t) + G(t)G(t) (114)

allowing us to embed [F, G] as part of a time-varying orthogonal matrix U (t) = F (t) G(t) H (t) J (t) . (115)

This amounts to extending (112) to  (t + 1) = F (t) (t) + G(t)w(t) w ¯ (t) = H (t) (t) + J (t)w(t), or, in the equivalent form  (t + 1)  (t) = U (t) . w ¯ (t) w(t) (117) (116a) (116b)

Hence, since E{ (t) (t) } = In and E{w(t)w(t) } = Ip , and assuming that E { (t)w(t) } = 0, E  (t + 1) w ¯ (t)  (t + 1) w ¯ (t) = U (t)U (t) = In+p , (118)

which yields E{ (t + 1)w ¯ (t) } = 0, E{w ¯ (t)¯ u(t) } = Ip . (119a) (119b)

14

Moreover, from (116) we have u ¯(t + k ) = H (t + k )(t + k, t) (t)
t+k-1

where x0 and the normalized white-noise process w are uncorrelated and E{x0 x0 } = P0 . In fact, inserting the transformations (122) and (123a) into (124b) yields ¯x ¯w y (t) = C ¯(t) + D ¯ (t), where ¯ = C (t)P (t)A(t) + D(t)B (t) C ¯ = C (t)P (t)B ¯ (t) + D(t)J (t) D From that we have the backward system ¯ (t)w x ¯(t - 1) = A(t) x ¯(t) + B ¯ (t) ¯ ¯ y (t) = C (t)¯ x(t) + D(t)w ¯ (t) (127a) (127b) (125) (126)

+
j =t

H (t + k )(t + k, j + 1)G(j )w(j ) + J (t)w(t)

for k > 0, where (s, t) = F (s - 1)F (s - 2) · · · F (t) for s > t In for s = t.

Therefore, since F (t)H (t) + G(t)J (t) = 0 by the unitarity of U (t), E{u ¯(t + k )¯ u(t) } = H (t + k )(t + k, t + 1)[F (t)H (t) + G(t)J (t) ] = 0. Consequently, u ¯ is a white noise process. Finally, premultiplying (117) by U (t) , we then obtain  (t) = F (t)  (t + 1) + H (t) w ¯ (t) w(t) = G(t)  (t + 1) + J (t) w ¯ (t), (120a) (120b)

with the boundary condition x ¯(T - 1) = P (T )-1 x(T ) being uncorrelated to the white-noise process w ¯.

R EFERENCES
[1] D. Q. Mayne, "A solution of the smoothing problem for linear dynamic systems," Automatica, vol. 4, pp. 73­92, 1966. [2] D. Fraser and J. Potter, "The optimum linear smoother as a combination of two optimum linear filters," Automatic Control, IEEE Transactions on, vol. 14, no. 4, pp. 387­390, 1969. [3] F. A. Badawi, A. Lindquist, and M. Pavon, "A stochastic realization approach to the smoothing problem," IEEE Trans. Automat. Control, vol. 24, no. 6, pp. 878­888, 1979. [4] F. Badawi, A. Lindquist, and M. Pavon, "On the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear stochastic systems," in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE, 1979, pp. 505­510. [5] M. Pavon, "New results on the interpolation problem for continuoustime stationary increments processes," SIAM journal on Control and Optimization, vol. 22, no. 1, pp. 133­142, 1984. [6] ----, "Optimal interpolation for linear stochastic systems," SIAM journal on Control and Optimization, vol. 22, no. 4, pp. 618­629, 1984. [7] K. Karhunen, Zur Interpolation von station¨ aren zuf¨ alligen Funktionen. Suomalainen tiedeakatemia, 1952. [8] Y. Rozanov, Stationary random processes. Holden-Day, San Francisco, 1967. [9] P. Masani, "Review: Yu.A. Rozanov, stationary random processes," The Annals of Mathematical Statistics, vol. 42, no. 4, pp. 1463­1467, 1971. [10] H. Dym and H. P. McKean, Gaussian processes, function theory, and the inverse spectral problem. Courier Dover Publications, 2008. [11] A. Lindquist and G. Picci, "On the stochastic realization problem," SIAM J. Control Optim., vol. 17, no. 3, pp. 365­389, 1979. [12] ----, "Forward and backward semimartingale models for Gaussian processes with stationary increments," Stochastics, vol. 15, no. 1, pp. 1­50, 1985. [13] ----, "Realization theory for multivariate stationary Gaussian processes," SIAM J. Control Optim., vol. 23, no. 6, pp. 809­857, 1985. [14] ----, "A geometric approach to modelling and estimation of linear stochastic systems," J. Math. Systems Estim. Control, vol. 1, no. 3, pp. 241­333, 1991.

which, in view of (119), is a backward stochastic system. Using the transformation (111), (116) yields the forward representation x(t + 1) = A(t)x(t) + B (t)w(t) ¯ (t) x(t) + J (t)w(t), w ¯ (t) = B
1 ¯ (t) := P (t)- 2 H (t) . Likewise (120) and where B

(121a) (121b)

x ¯(t) = P (t + 1)-1 x(t + 1), yields the backward representation ¯ (t)w x ¯(t - 1) = A(t) x ¯(t) + B ¯ (t) w(t) = B (t) x ¯(t) + J (t) w ¯ (t).

(122)

(123a) (123b)

Remark 8: When considered on the doubly infinite time axis, equation (117) defines an isometry. Indeed, assuming that the input is squarely summable, the fact that U (t) is unitary for all t directly implies that
N N

w ¯
-

2

+  (t + 1)

2

=
-

w(t) 2 .

Then,  (t)  0 as t  , provided (t, s)  0 as s  -. It follows that
 

w ¯ (t)
t=-

2

=
t=-

w(t) 2 .

We are now in a position to derive a backward version of a non-stationary stochastic system x(t + 1) = A(t)x(t) + B (t)w(t), y (t) = C (t)x(t) + D(t)w(t) x(0) = x0 (124a) (124b)

15

[15] A. Lindquist and M. Pavon, "On the structure of state-space models for discrete-time stochastic vector processes," IEEE Trans. Automat. Control, vol. 29, no. 5, pp. 418­432, 1984. [16] G. Michaletzky, J. Bokor, and P. V´ arlaki, Representability of stochastic systems. Budapest: Akad´ emiai Kiad´ o, 1998. [17] G. Michaletzky and A. Ferrante, "Splitting subspaces and acausal spectral factors," J. Math. Systems Estim. Control, vol. 5, no. 3, pp. 1­26, 1995. [18] A. Lindquist and G. Picci, Linear Stochastic Systems: A Geometric Approach to Modeling, Estimation and Identification. Springer-Verlag, Berlin Heidelberg, 2015. [19] T. T. Georgiou, "The Carath´ eodory­Fej´ er­Pisarenko decomposition and its multivariable counterpart," Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 212­228, 2007. [20] T. Georgiou and A. Lindquist, "On time-reversibility of linear stochastic models," arXiv preprint arXiv:1309.0165, 2013. [21] J. E. Wall Jr, A. S. Willsky, and N. R. Sandell Jr, "On the fixedinterval smoothing problem," Stochastics: An International Journal of Probability and Stochastic Processes, vol. 5, no. 1-2, pp. 1­41, 1981. [22] A. N. Kolmogorov, Stationary sequences in Hilbert space. John Crerar Library National Translations Center, 1978. [23] A. M. Yaglom, "On problems about the linear interpolation of stationary random sequences and processes," Uspekhi Matematicheskikh Nauk,

vol. 4, no. 4, pp. 173­178, 1949. [24] M. Pavon, "Stochastic realization and invariant directions of the matrix Riccati equation," SIAM Journal on Control and Optimization, vol. 18, no. 2, pp. 155­180, 1980. [25] M. Pavon and A. Wakolbinger, "On free energy, stochastic control, and Schr¨ odinger processes," in Modeling, Estimation and Control of Systems with Uncertainty. Springer, 1991, pp. 334­348. [26] P. Dai Pra and M. Pavon, "On the Markov processes of Schr¨ odinger, the Feynman-Kac formula and stochastic control," in Realization and Modelling in System Theory. Springer, 1990, pp. 497­504. [27] B. Jamison, "Reciprocal processes," Probability Theory and Related Fields, vol. 30, no. 1, pp. 65­86, 1974. [28] A. Krener, "Reciprocal processes and the stochastic realization problem for acausal systems," in Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197­211. [29] B. C. Levy, R. Frezza, and A. J. Krener, "Modeling and estimation of discrete-time Gaussian reciprocal processes," Automatic Control, IEEE Transactions on, vol. 35, no. 9, pp. 1013­1023, 1990. [30] P. Dai Pra, "A stochastic control approach to reciprocal diffusion processes," Applied mathematics and Optimization, vol. 23, no. 1, pp. 313­329, 1991.

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian Arizona State University Mesa, AZ 85212 USA {kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu Abstract
Software engineering education is a technologically challenging, rapidly evolving discipline. Like all STEM educators, software engineering educators are bombarded with a constant stream of new tools and techniques (MOOCs! Active learning! Inverted classrooms!) while under national pressure to produce outstanding STEM graduates. Software engineering educators are also pressured on the discipline side; a constant evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the technology, guidance on the adoption of project-centric curricula is needed. This paper focuses on vertical integration of project experiences in undergraduate software engineering degree programs or course sequences. The Software Enterprise, now in its 9 th year, has grown from an upper-division course sequence to a vertical integration program feature. The Software Enterprise is presented as an implementation of a project spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software engineering and computer science education focus on content taxonomies and bodies of knowledge. This is not a bad thing, but taken in isolation may lead educators to believe content coverage is more important than applied learning experiences. There is literature on project-based learning within computing as a means to learn soft skills and complex technical competencies. However, project experiences tend to be disjoint [5]; there may be a freshman project or a capstone project or a semester project assigned by an individual instructor. Yearlong capstone projects are offered at most institutions as a synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do it all the time ? Project experiences, while pervasive in computing programs, are not a central integrating feature. Sheppard et al. [6] suggests that engineering curricular design should move away from a linear, deductive model and move instead toward a networked model: "The ideal learning trajectory is a spiral, with all components revisited at increasing levels of sophistication and interconnection" ([6] p. 191). The general engineering degree program at Arizona State University (ASU) was designed from its inception in 2005 [7] to be a flexible, project-centric curriculum that embodied such integration (even before [6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division course sequence to integrate contextualized project experiences with software engineering fundamental concepts. The computing and engineering programs at ASU's Polytechnic campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

Board of Regents (ABOR) approved a new Bachelor's degree in software engineering (BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo accreditation review shortly thereafter. At the course level the Software Enterprise defines a delivery structure integrating established learning techniques around a project-based contextualized learning experience. At the degree program level, the Enterprise weaves project experiences throughout the BS SE degree program, integrating program outcomes at each year of the major. There are several publications on the manner in which the Software Enterprise is conducted within a project course (for example, [8][9]]), and we summarize this in-course integration pedagogy in section 2. The intent of this work-in-progress paper is to describe extending the Enterprise as a spiral curricular design feature we refer to as the project spine , and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a student's competencies from understanding to comprehension to applied knowledge by co-locating preparation , discussion , practice , reflection , and contextualized learning activities in time. In this model, learners prepare for a module by doing readings, tutorials, or research before a class meeting time. The class discusses the module's concepts, in a lecture or seminar-style setting. The students then practice with a tool or technique that reinforces the concepts in the next class meeting. At this point students reflect to internalize the concepts and elicit student expectations, or hypotheses , for the utility of the concept. Then, students apply the concept in the context of a team-oriented, scalable project, and finally reflect again to (in)validate their earlier hypotheses. These activities take place in a single three-week sprint , resulting is a highly iterative methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right) The Software Enterprise represents an innovation derived from existing scholarship in that it assembles best practices such as preparation, reflection, practice (labs), and project-centered learning in a rapid integration model that accelerates applied learning. Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle [10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on maturing the delivery process, creating new or packaging existing learning materials to fit the delivery model, and to explore ways to assess project-centered learning.

3. The Software Enterprise Project Spine
An innovation in the new BS in Software Engineering at ASU has been the vertical adoption of the Software Enterprise. Enterprise courses are now required from the sophomore to senior years. This innovation represents what [6] calls a professional spine , as the Enterprise serves as an integrator of learning outcomes for a given year in the major. We refer to our project-centered realization as a project spine , where foundational concepts are tied to project work throughout the undergraduate program . There is significant computing literature on projects (embedded, mobile, gaming, etc.) to achieve learning or retention outcomes. However, computing lacks a framework for integrating concepts in a project spine. The Enterprise is an implementation that moves students from basic comprehension to applied Figure 2. ASU Project Spine knowledge to critical analysis outcomes. In the BS SE at ASU, program outcomes are described at 4 levels: describe , apply , select , and internalize . Students must achieve level 3 ( select between alternatives) in at least 1 outcome and achieve level 2 ( apply ) in all others. The program outcomes for the BS SE include Design, Computing Practice, Critical Thinking, Professionalism, Perspective, Problem Solving, Communication, and Technical Competence . An example leveled outcome description for Perspective is given in Table 1. The Enterprise accelerates level 3 outcomes by providing contextualized integrated experiences fostering decision-making in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes. Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in business, global, economic, environmental, and societal contexts. Level 1. Understands technological change and development have both positive & negative effects. Level 2. Identifies and evaluates the assumptions made by others in their description of the role and impact of engineering and computing on the world. Level 3. Selects from different scenarios for the future and appropriately adapts them to match current technical, social, economic and political concerns. Level 4 . Has formed a constructive model for the future of our society, and makes life and career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical competencies by assigning projects inclusive of the technical material covered in the regular computing courses. So for example, junior projects (Software Enterprise III and IV) emphasize technical complexities in Networks, Distributed Computing, and Databases, while senior projects emphasize technical complexities in Web and Mobile computing. The technical "focus area" courses are chosen more based on faculty expertise and recruitment goals than software engineering outcomes; one can envision many different areas represented by upper division courses here. These do help address the concern that an accredited software engineering degree has an application area. A risk we have not yet addressed is if the technical area impacts the software engineering process, such as with a soon-to-be-introduced embedded systems focus area.

There are 2 additional aspects of integration to the project spine. As summarized in section 2, the Enterprise integrates software engineering concepts throughout the project experiences. Students in the sophomore year learn the Personal Software Process [11] as a means to build individual understanding of time management, defect management, and estimation skills. They then focus on Quality, including but not limited to testing. In the junior year Enterprise students focus on Design (human-centered and system design principles) followed by best practices in software construction, taken primarily from eXtreme Programming. In the senior year students focus on Requirements Engineering then Process and Project Management. The final aspect of integration is with soft-skill outcomes such as Communication , Teamwork , and Professionalism (see Table 1). Throughout the spine the project experiences are crafted to ensure variations on pedagogy to address these outcomes. For example, in the freshman year students receive explicit instruction in teamwork. In the senior year the emphasis is on formal documentation as a means of communication. In the junior year, students work on service learning projects of high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of program adoption. There are examples of program design and lessons learned [5][12][13], or reflections and recommendations on the software engineering education landscape [14][15][16][17][18]. These are worthwhile guides but do not offer examples on evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on "Program Implementation and Assessment" which discusses a number of key factors in program adoption, but is geared toward accreditation and not evaluation instruments. A survey instrument is presented in [19] but is designed for comparison of a large number of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate programs in software engineering but more as an aggregate counting exercise in knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software Engineering project conducted a survey of graduate degree programs [20] and then produced a comparison report [21] of graduate programs to the GSwE2009 reference model, which includes data on program characteristics and in-depth profiles from 3 institutions. A recent study is Conry's [23] survey of accredited software engineering degree programs. Conry summarizes institutional, administrative, and curricular (knowledge area) aspects in describing the 19 accredited programs as of October 2009. Certainly program adoption measures from other engineering programs are also relevant, though software engineering programs are unique due to the forces discussed in section 1. Our next steps for the Enterprise-as-project-spine involve defining measures for adoption impact, and determining how this concept fits with established patterns for curricular maps in software engineering programs. We plan to use quantitative and qualitative instruments to evaluate adoption. Quantitative data, such as program size, institution type, faculty and student backgrounds, can be collected via available resources (departmental archives or online) and direct surveys. Qualitative data can be collected through survey instruments and interviews of all stakeholders (faculty participants, administrators, and advisors). Different instruments may be used at different times to evaluate "in-stream" attitudes versus post-adoption reflections. Defining and validating these instruments is a significant area of work going forward. The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in software engineering. Taxonomies are useful and the sign of an emerging discipline. We

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas, and plan to elaborate on these mappings. Specifically, we intend to produce CS2013 course exemplars. Further, the SE2004 report includes a section on program curricular patterns, and we will propose new patterns based on the project spine concept, which we hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the New Century. The National Academies Press, Washington D.C., 2005. [2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of Knowledge (SWEBOK). Los Alamitos, CA, 2004. [3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013. [4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society. Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering. Joint Task Force on Computing Curricula, 2004. [5] Shepard, T. "An Efficient Set of Software Degree Programs for One Domain." In Proceedings of the International Conference on Software Engineering (ICSE) 2001. [6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the Future of the Field, Jossey-Bass, San Francisco, 2008. [7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. "A Flexible Curriculum for a Multi-disciplinary Undergraduate Engineering Degree." Proceedings of the Frontiers in Education Conference 2005. [8] Gary, K. "The Software Enterprise: Practicing Best Practices in Software Engineering Education", The International Journal of Engineering Education Special Issue on Trends in Software Engineering Education, Volume 24, Number 4, July 2008, pp. 705-716. [9] Gary, K., "The Software Enterprise: Preparing Industry-ready Software Engineers" Software Engineering: Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group Publishing. October 2008. [10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984. [11] Humphrey, W.S. Introduction to the Personal Software Process , Addison-Wesley, Boston, 1997. [12] Lutz, M. and Naveda, J.F. "The Road Less Traveled: A Baccalaureate Degree in Software Engineering." Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997. [13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor's Program. IEEE Software November/December 2006. [14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. "Guidance for the development of software engineering education programs." The Journal of Systems and Software, 49(1999):163-169. 1999. [15] Ghezzi, C. and Mandrioli. "The Challenges of Software Engineering Education." In Proceedings of the International Conference on Software Engineering (ICSE) 2006. [16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. "Improving software practice through education: Challenges and future trends." Proceedings of the Future of Software Engineering Conference, 2007. [17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the Future of Software Engineering, Limerick Ireland, 2000. [18] Mead, N. (2009). Software Engineering Education: How far We've Come and How far We Have to Go. Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009. [19] Modesitt, K., Bagert, D.J., and Werth, L. "Academic Software Engineering: What is it and What Could it be? Results of the First International Survey for SE Programs." Proceedings of the International Conference on Software Engineering (ICSE) 2001. [20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008. [21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master's Programs in Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013. [22] Bagert, D.J. & Chenoweth, S.V. "Future Growth of Software Engineering Baccalaureate Programs in the United States", Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005. [23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of the American Society for Engineering Education, Louisville, KY, 2010. [24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). "Revision of the SE2004 Curriculum Model." Panel at the ACM Conference of the Special Interest Group on Computer Science Education (SIGCSE), Denver, CO, 2013.

SOCRADES: A Web Service based Shop Floor Integration Infrastructure
Luciana Moreira S´ a de Souza, Patrik Spiess, Dominique Guinard, Moritz K¨ ohler, Stamatis Karnouskos, and Domnic Savio
SAP Research Vincenz-Priessnitz-Strasse 1, D-76131, Karlsruhe, Germany Kreuzplatz 20, CH-8008, Zurich, Switzerland {luciana.moreira.sa.de.souza, patrik.spiess, dominique.guinard, mo.koehler, stamatis.karnouskos, domnic.savio} @sap.com

Abstract. On the one hand, enterprises manufacturing any kinds of goods require agile production technology to be able to fully accommodate their customers' demand for flexibility. On the other hand, Smart Objects, such as networked intelligent machines or tagged raw materials, exhibit ever increasing capabilities, up to the point where they offer their smart behaviour as web services. The two trends towards higher flexibility and more capable objects will lead to a service-oriented infrastructure where complex processes will span over all types of systems -- from the backend enterprise system down to the Smart Objects. To fully support this, we present SOCRADES, an integration architecture that can serve the requirements of future manufacturing. SOCRADES provides generic components upon which sophisticated production processes can be modelled. In this paper we in particular give a list of requirements, the design, and the reference implementation of that integration architecture.

1

Introduction

In the manufacturing domain, constant improvements and innovation in the business processes are key factors in order to keep enterprises competitive in the market. Manufacturing businesses are standing on the brink of a new era, one that will considerably transform the way business processes are handled. With the introduction of ubiquitous computing on the shop floor1 , an entirely new dynamic network of networked devices can be created - an Internet of Things (IoT) for manufacturing. The Internet of Things is a concept which first appeared shortly after 2000. Until now, several approaches to describe the IoT have been undertaken of which most have focused on RFID technologies and their application ([5, 13]). Only recently, new technologies such as Smart Embedded Devices and Sensor Networks have entered the scene and can be considered as part of the IoT [11].
1

In manufacturing, the shop floor is the location where machines are located and products produced.

Smart Embedded Devices are embedded electronic systems which can sense their internal state and are able to communicate it through data networks. In contrast to this, Sensor Networks not only can measure internal states of their nodes, but also external states of the environment. We group these three technologies RFID, Smart Embedded Devices, and Sensor Networks - under the notion Smart Objects. Smart Objects are the nerve cells, which are interconnected through the Internet and thus build the IoT. RFID has already been proved to open fundamentally new ways of executing business processes, and the technology has already been adopted by several key players in the industry. Therefore the focus of this paper is on Smart Embedded Devices and Sensor Networks and their effects on automatic business process execution. Although client-server architectures still play an important role in the field of business software systems, the Service Oriented Architecture (SOA) is on the move and it is foreseeable that this architectural paradigm will be predominant in the future. The integration of devices into the business IT-landscape through SOA is a promising approach to connect physical objects and to make them available to IT-systems. This can be achieved by running instances of web services on these devices, which moves the integration of back end applications, such as Enterprise Resource Planning (ERP) systems, with the devices one step forward, enabling them to interact and create an Internet of Services that collaborates and empowers the future service-based factory. Enabling efficient collaboration between device-level SOA and services and applications that constitute the enterprise back-end on the other hand, is a challenging task. The introduction of web service concepts at a level as low as the production device or facility automation makes this integration significantly less complex. But there are still differences between device-level SOA and the one that is used in the back end. To name but a few of them, device-level services are of higher granularity, exhibit a lower reliability (especially if they are connected wirelessly) and higher dynamicity and are more focused on technical issues than on business aspects. These differences can be overcome by introducing a middleware between the back end applications and the services that are offered by devices, service mediators, and gateways. This middleware adds the required reliability, provides means to deal with services appearing and disappearing, and allows intermediate service composition to raise the technical interfaces of low-level services to business-relevant ones. In this paper we present the SOCRADES middleware for business integration; an architecture focused on coupling web service enabled devices with enterprise applications such as ERP Systems. Our approach combines existing technologies and proposes new concepts for the management of services running on the devices. This paper is organized as follows: in section 2 we discuss the current state of the art in coupling technologies for shop floor and enterprise applications. Section 3 presents the requirements for our approach, followed by section 4 where we

discuss our approach. We propose a prototype for evaluating our approach in section 5 and perform an analysis in section 6. Section 7 concludes this paper.

2

Related Work

Manufacturing companies need agile production systems that can support reconfigurability and flexibility to economically manufacture products. These systems must be able to inform resource planning systems like SAP ERP in advance, about the upcoming breakdown of a whole production processes or parts of them, so that adaptation in the workflow can be elaborated. Currently Manufacturing Execution Systems (MES) are bridging the gap between the shop floor and ERP systems that run in the back end. The International Systems and Automation Society - 95 (ISA-95) derivative from the Instrumentation Systems and Automation Society define the standards for this interface [1]. Although MES systems exist as gateways between the enterprise world and the shop floor, they have to be tailored to the individual group of devices and protocols that exist on this shop floor. By integrating web services on the shop floor, devices have the possibility of interacting seamlessly with the back end system ([9, 8]). Currently products like SIMATIC WinCC Smart Access [2] from Siemens Automation use SOAP for accessing tag based data from devices like display panels to PC's. However, they neither provide mechanisms to discover other web-service enabled devices, nor mechanisms for maintaining a catalogue of discovered devices. The domain of Holonic Manufacturing Execution Systems (HMS) [6] is also relevant to our work. HMS are used in the context of collaborative computing, and use web service concepts to integrate different sources and destinations inside a production environment. They do, however, not offer support to process orchestration or service composition. Amongst others, European Commission funded projects like SIRENA [4] showed the feasibility and benefit of embedding web services in production devices. However, since these were only initial efforts for proving the concept, not much attention has been given to issues such as device supervision, device life cycle management, or catalogues for maintaining the status of discovered devices, etc. The consortium of the SOCRADES project has integrated partners, code and concepts from SIRENA, and aims to further design and implement a more sophisticated infrastructure of web-service enabled devices. SODA (www.sodaitea.org) aims at creating a comprehensive, scalable, easy to deploy ecosystem built on top of the foundations laid by the SIRENA project. The SODA ecosystem will comprise a comprehensive tool suite and will target industry standard platforms supported by wired and wireless communications. Although EU projects like SIRENA showed the feasibility and benefit of embedding web services in devices used for production, they do not offer an infrastructure or a framework for device supervision or device life cycle. They neither do provide a catalogue for maintaining the status of discovered devices [4]. Changes due to the current development are moving towards a more promis-

ing approach of integrating shop floor devices and ERP systems more strongly [14]. Some authors are criticizing the use of RPC-style interaction in ubiquitous computing [12] (we consider the smart manufacturing devices a special case of that). We believe this does not concern our approach, since web services also allow for interaction with asynchronous, one-way messages and publish-subscribe communication. SAP xApp Manufacturing Integration and Intelligence (SAP xMII) is a manufacturing intelligence portal that uses a web server to extract data from multiple sources, aggregate it at the server, transform it into business context and personalize the delivered results to the users [7]. The user community can include existing personal computers running internet browsers, wireless PDAs or other UIs. Using database connectivity, any legacy device can expose itself to the enterprise systems using this technology. The drawback of this product is that every device has to communicate to the system using a driver that is tailored to the database connectivity. In this way, SAP xMII limits itself to devices or gateway solutions that support database connectivity. In [10], we proposed a service-oriented architecture to bridge between shop floor devices and enterprise applications. In this paper however, building on both our previous work and SAP xMII, we show how the already available functionality of xMII can be leveraged and extended to provide an even richer integration platform. The added functionality comprises integration of web service enabled devices, making them accessible through xMII, and supporting the software life cycle of embedded services. This enables real-world devices to seamlessly participate in business processes that span over several systems from the back end through the middleware right down to the Smart Objects.

3

System Requirements

As embedded technology advances, more functionality that currently is hosted on powerful back end systems and intermediate supervisory devices can now be pushed down to the shop floor level. Although this functionality can be transferred to devices that have only a fraction of the capabilities of more complex systems, their distributed orchestration in conjunction with the fact that they execute very task-specific processing, allows us to realise approaches that can outperform centralised systems in means of functionality. By embedding web services on devices, these can become part of a modern Enterprise SOA communication infrastructure. The first step to come closer to realize this vision, is to create a list of requirements. We have come up with this list through interviews with project partners and customers from the application domain, as well as a series of technical workshops with partners form the solution domain. As usually done in software engineering, we separated the list into functional and non-functional requirements.

Functional Requirements ­ WS based direct access to devices: Back end services must be able to discover and directly communicate with devices, and consume the services they offer. This implies the capability of event notifications from the device side, to which other services can subscribe to. ­ WS based direct access to back end services: Most efforts in the research domain today focus on how to open the shop floor functionality to the back end systems. The next challenge is to open back end systems to the shop floor. E.g. devices must be able to subscribe to events and use enterprise services. Having achieved that, business logic executing locally on shop floor devices can now take decisions not only based on its local information, but also on information from back end systems. ­ Service Discovery: Having the services on devices will not be of much use if they can not be dynamically discovered by other entities. Automatic service discovery will allow us to access them in a dynamic way without having explicit task knowledge and the need of a priori binding. The last would also prevent the system from scaling and we could not create abstract business process models. ­ Brokered access to events: Events are a fundamental pillar of a service based infrastructure. Therefore access to these has to be eased. As many devices are expected to be mobile, and their online status often change (including the services they host), buffered service invocation should be in-place to guarantee that any started process will continue when the device becomes available again. Also, since not all applications expose web services, a pull point should be realised that will offer access to infrastructure events by polling. ­ Service life cycle management: In future factories, various services are expected to be installed, updated, deleted, started, and stopped. Therefore, we need an open ways of managing their life cycle. Therefore the requirement is to provide basic support in the infrastructure itself that can offer an open way of handling these issues. ­ Legacy device integration: Devices of older generations should be also part of the new infrastructure. Although their role will be mostly providing (and not consuming) information, we have to make sure that this information can be acquired and transformed to fit in the new WS-enabled factory. Therefore the requirement is to implement gateways and service mediators to allow integration of the non-ws enabled devices. ­ Middleware historian: In an information-rich future factory, logging of data, events, and the history of devices is needed. The middleware historian is needed which offers information to middleware services, especially when an analysis of up-to-now behavior of devices and services is needed. ­ Middleware device management: Web service enabled devices, will contain both, static and dynamic data. This data can now be better and more reliably integrated to back end systems offering a more accurate view of the shop floor state. Furthermore by checking device data and enterprise inventory, incompatibilities can be discovered and tackled. Therefore we require

approaches that will effectively enable the full integration of device data and their exploitation above the device-layer. Non-Functional Requirements ­ Security support: Shop floors are more or less closed environments with limited and controlled communication among their components. However, because of open (and partially wireless) communication networks, this is fundamentally changing. Issues like confidentiality, integrity, availability must be tackled. In a web service mash-up - as the future factory is expected to be -, devices must be able to a) authenticate themselves to external services and b) authenticate/control access to services they offer. ­ Semantics support: This requirement facilitates the basic blocks primarily for service composition but also for meaningful data understanding and integration. Support for the usage of ontologies and semantic-web concepts will also enhance collaboration as a formal description of concepts, terms, and relationships within a manufacturing knowledge domain. ­ Service composition: In a SOA infrastructure, service composition will allow us to build more sophisticated services on top of generic ones, therefore allowing thin add-ons for enhanced functionality. This implies a mixed environment where one could compose services a) at device level b) at back end level and c) in a bidirectional cross-level way. In the above list we have described both, functional and non-functional requirements. In our architecture these requirements will be realized through components, each one offering a unique functionality.

4
4.1

Architecture
Overview

In this chapter, we present a concrete integration architecture focusing on leveraging the benefits of existing technologies and taking them to a next level of integration through the use of DPWS and the SOCRADES middleware. The architecture proposed in Figure 1 is composed of four main layers: Device Layer, SOCRADES middleware (consisting of an application and a device services part), xMII, and Enterprise Applications. The Device Layer comprises the devices in the shop floor. These devices when enabled with DPWS connect to the SOCRADES middleware for more advanced features. Nevertheless, since they support web services, they provide the means for a direct connection to Enterprise Applications. For the intermediate part of the SOCRADES architecture, bridging between enterprise and device layer, we identified an SAP product that partly covered our requirements: SAP xApp Manufacturing Integration and Intelligence (SAP xMII). The features already available in xMII are:

ENTERPRISE APPLICATIONS
HTML-GUI / Applets SAP Protocols Web Services

xMII
Visualization Services Applets Display Controls Displays GUI Widgets SAP Transaction Access SAP Connectivity

SOCRADES MIDDLEWARE APP SERVICES
Invoker Asynchronous Buffer Eventing Notification Broker (Event) Pull Point

Web Services Cross-layer
Service Catalogue

Business Logic Services
Business Process Monitoring
Alert

Composed Services Runtime SOCRADES Connector Web Services DPWS Back-end Services

Legacy Connector Shop floor standard

Data Services

SOCRADES MIDDLEWARE DEVICE SERVICES
Device Manager and Monitor Middleware Historian Service Discovery
Service Lifecycle Management

Hardware Vendor Implementation

Service Services Mapper Repository

Service Access Control Proprietary Protocol OPC UA over DPWS OPC UA over DPWS Gateway

DEVICE LAYER

Fig. 1. SOCRADES Integrated Architecture

­ Connectivity to non web service enabled devices via various shop floor communication standards ­ Graphical modelling and execution of business rules ­ Visualization Services ­ Connectivity to older SAP software through SAP-specific protocols We decided not to re-implement that functionality but use it as a basis and extend it by what we call the SOCRADES middleware. The SOCRADES middleware and xMII perform together a full integration of devices with ERP systems, adding functionalities such as graphical visualization of device data and life cycle management of services running on the devices. In this setting, xMII provides the handling of business logic, process monitoring and visualization of the current status of the devices. Finally, the connection with Enterprise Applications is realized in three ways. SAP xMII can be used to generate rich web content that can be integrated into the GUI of an enterprise system in mash-up style. Alternatively, it can be used to establish the connection to older SAP systems using SAP-specific protocols. Current, web service based enterprise software can access devices either via web services of the SOCRADES middleware, benefiting from the additional functionality, or they can directly bind against the web services of DPWS-enabled devices. The data delivered to Enterprise Applications is currently provided by xMII. Nevertheless with the introduction of the SOCRADES middleware and

the use of DPWS, this data can be also delivered directly by the regarding devices, leaving to xMII only the task of delivering processed data that requires a global view of the shop floor and of the business process. 4.2 Features and Components of the SOCRADES Middleware

The SOCRADES middleware is the bridging technology that enables the use of features of existing software systems with DPWS enabled devices. Together with SAP xMII, this middleware connects the shop floor to the top floor, providing additional functionality not available in either one of these layers. Although direct access from an ERP system to devices is possible, the SOCRADES middleware simplifies the management of the shop floor devices. In the following, we list this additional functionality and show how the components of the architecture implement them. Brokered Access to Devices. Brokered access means to have an intermediate party in the communication between web service clients and servers that adds functionality. Example are asynchronous invocations, a pull point for handling events, and a publish-subscribe mechanism for events. Asynchronous invocations are useful when dealing with devices that are occasionally connected so that invocations have to be buffered until the device re-appears; they are implemented by the Invoker component. Pull points enable applications to access events without having to expose a web service interface to receive them. The application can instruct the pull point to buffer events and can obtain them by a web service call whenever it is ready. Alternatively, to be notified immediately, the application can expose a web service endpoint and register it at the notification broker for any type of event. Service Discovery: The service discovery components carries out the actual service discovery on the shop floor level. This component is distributed and replicated at each physical site because the DPWS discovery mechanism WSDiscovery relies on UDP multicast, a feature that may not be enabled globally across all subsidiaries in a corporate network. All discovered devices from all physically distributed sites and all the services that each device runs are then in a central repository called Device Manager and Monitor, which acts as the single access point where ERP systems can find all devices even when they have no direct access to the shop floor network. Device Supervision: Device Management and Monitor and DPWS Historian provide the necessary static and dynamic information about each DPWS-enabled physical device available in the system. The device manager holds any static device data of all on-line and off-line devices while the device monitor contains information about the current state of each device. The middleware historian can be configured to log any event occurring at middleware level for later diagnosis

and analysis. Many low-level production systems feature historians, but they are concerned with logging low-level data that might be irrelevant for businesslevel analysis. Only a middleware historian can capture high-level events that are constructed within this architectural layer.

Service Life Cycle Management: Some hardware platforms allow exchanging the embedded software running on them via the network. In a service-enabled shop floor this means that one can update services running on devices. The management of these installed services is handled through the use of the Service Mapper and Services Repository. These components together make a selection of the software that should run in each device and perform the deployment.

Cross-Layer Service Catalogue: The cross-layer service catalogue comprises two components. One is the Composed Services Runtime that executes service composition descriptions, therefore realizing service composition at the middleware layer. The second component is the DPWS device for back end services that allows DPWS devices to discover and use a relevant set of services of the ERP system. The Composed Services Runtime is used to enrich the services offered by the shop floor devices with business context, such as associating an ID read from an RFID tag with the corresponding order. A compound service can deliver this data by both invoking a service on the RFID reader, and from a warehouse application. A Composed Services Runtime, which is an execution engine for such service composition descriptions, e.g., BPEL [3], is placed in the middleware because only from there, all DPWS services on the shop floor as well as all back end services can be reached. Another requirement is that shop floor devices must be able to access enterprise application services, which can be achieved by making a relevant subset available through the DPWS discovery. This way, devices that run DPWS clients can invoke back end services in exactly the same way they invoke services on their peer devices. Providing only the relevant back end services allows for some access control and reduces overhead during discovery of devices. Co-locating both sub-components in the same component has the advantage that also the composed services that the Composed Services Runtime provides, can be made available to the devices through the virtual DPWS device for back end services.

Security support: The (optional) security features supported by the middleware are role-based access control of devices communication to middleware and back end services and vice versa. Event filtering based on roles is also possible. Both the devices as well as back end and middleware services have to be authorized when they want to communicate. Access control is enforced by the respective component. Additionally, message integrity and confidentiality is provided by the WS-Security standard.

To demonstrate the feasibility of our approach and to make some first evaluations, we implemented a simple manufacturing scenario. We used a first implementation of our architecture to connect two DPWS-enabled real-world devices with an enterprise application.

5

Reference Implementation

In order to prove the feasibility of our concept, we have started realising a reference implementation. From a functional point of view, it demonstrates two of the most important incentives for the use of standardized device level web services in manufacturing: flexibility and integration with enterprise software. Indeed, the scenario shows DPWS-enabled devices can be combined easily to create higher-level services and behaviours that can then be integrated into topfloor applications. The business benefits from adopting such an architecture are numerous: ­ ­ ­ ­ lower cost of information delivery increased flexibility and thus total cost of ownership (TCO) of machines. increased visibility of the entire manufacturing process to the shop floor. ability to model at the enterprise layer processes with only abstract view of the underlying layer, therefore easing the creation of new applications and services from non-domain experts. Scenario

5.1

To support this idea we consider a simple setting with two DPWS devices: ­ A robotic arm that can be operated through web service calls. Additionally it offers status information to subscribers through the SOCRADES eventing system. ­ A wireless sensor node providing various information about the current environment, delivered as events. Furthermore, the sensor nodes provide actuators that are accessible through standard service calls. The manufacturing process is created on the shop floor using a simple service composition scheme: from the atomic services offered by the arm (such as start/stop, etc.) a simple manufacturing process p is created. The robot manipulates heat-sensitive chemicals. As a consequence it is identified that the manufacturing process cannot continue if the temperature rises above 45 . The robot may not have a temperature sensor (or this is malfunctioning), but as mentioned before the manufacturing plant is equipped with a network of wireless sensor nodes providing information about the environment. Thus, in order to enforce the business rule, the chief operator uses a visual composition language to combine p with the temperature information published by the service-enabled sensor node: t. In pseudo code, such a rule looks like:



if (t > 45) then p.stopTransportProcess(); Furthermore, the operator instantiates a simple gauge fed with the temperature data (provided by t). For this purpose he uses a manufacturing intelligence software and displays the gauge on a screen situated close the robot. Finally, the sales manager can also leverage the service oriented architecture of this factory. Indeed, the output of the business rule is connected to an ERP system which provides up-to-date information about the execution of the current orders. Whenever the process is stopped because the rule was triggered, an event is sent to the ERP system through its web service interface. The ERP system then updates the orders accordingly and informs the clients of a possible delay in the delivery. 5.2 Components

This section describes the architecture of our prototype from an abstract point of view. Its aim is to understand the functionality whose concrete implementation will be described within the next section. Functional Components The system comprises four main components as shown on Figure 2 that we shall briefly describe: ­ Smart Devices: Manufacturing devices, sensors and Smart Things (i.e. Smart Objects) are the actors forming an Internet of Services in the factory as well as outside of the factory. They all offer web service interfaces, either directly or through the use of gateways or service mediators. Through these interfaces they offer functional services (e.g. start/stop, swap to manual/automatic mode) or status information (e.g. power consumption, mode of operation, usage statistics, etc.). ­ Composed Service: The component aggregates the services offered by smart objects. Indeed, it is in charge of exposing coarse-grained services to the upper layers. In the case of the robotic arm for instance, it will consume the open(), close() and move(...), methods and use them to offer a doTransportProcess (...) service. ­ Business Logic Services and Visualisation Services: In our prototype, the business logic services are supported by a service composition engine and visualized using a visualization toolkit. The former component is used to model business rules or higher-level processes, known as business logic services in our architecture. As an example the operator can use it to create the business rules exposed above. The latter component is used to build a plantfloor visualisation of the devices' status and the overall process execution. As an example the operator can instantiate and use a set of widgets such as gauges and graphs to monitor the status of the machines. The production manager can also use it to obtain real-time graphs of the process execution and status.

­ Enterprise Applications: This is the place of high-end business software such as ERPs or PLMs. The idea at this level is to visualize processes rather than the machines executing the processes. This layer is connected to the plant-floor devices through the other layers. As such it can report machines failures and plant-floor information on the process visualization and workflow. Furthermore, business actions (e.g. inform customers about a possible delay) can be executed based on this information.

Fig. 2. The DPWS service bus.

Cross-Component Communication In a mash-up, the architecture is not layered but rather flat, enabling any functional component to talk to any other. Such architectures need a common denominator in order for the components to be able to invoke services on one another. In our case the common denominator is the enhanced DPWS we developed. Each component is DPWS-enabled and thus, consumes DPWS services and exposes a DPWS interface to invoke the operations it offers. The service invocations can be done either synchronously or asynchronously via the web service eventing system. For instance the temperature is gathered via a subscription to the temperature service (asynchronous) whereas the transport process is stopped by invoking an operation on the process middleware. Figure 2 depicts the architecture by representing the components connected to a common (DPWS) ESB (Enterprise Service Bus). 5.3 Implementation

The system described in this paper is a reference implementation of concepts described in the architecture rather than a stand-alone concept. Therefore it uses and extends several software and hardware components rather than writing them from scratch. In this section we will briefly describe what these components are and how they interact together, taking a bottom up approach.

Functional Components ­ Smart Devices: The wireless sensor network providing temperature information is implemented using the Sun Microsystems' SunSPOT sensor nodes. Since the nodes are not web services enabled, we had to implement a gateway (as described in our architecture), that would capture the temperature readings and provide it via DPWS as services one can subscribe to. The gateway component hides the communication protocol between the SunSPOTs and exposes their functionalities as device level web services (DPWS). More concretely the SunSPOT offer services for sensing the environment (e.g. getTemperature()) or providing output directly on the nodes (e.g. turnLightOn(Color)). The robotic arm was implemented as a clamp offering DPWS services for both monitoring and control. The clamp makes these operations available as DPWS SOAP calls on a PLC (Programmable Logic Controller) over gateway. For monitoring services (e.g. getPowerConsumption()) the calls are issued directly on the gateway standing for the clamp. For control services the idea is slightly different. ­ Composed Service: Typical operations at the clamp level are openClamp() and closeClamp(). In order to consistently use these operations on the topfloor we need to add some business semantics already on the shop floor. This is the role of composed services which aggregate an number of coarse-grained operations (e.g. openClamp()) and turn them into higher level services. This way the start(), openClamp(), closeClamp(), move(x), stop() operations are combined to offer the startTransportProcess() service. ­ Business Logic Services and Vizualisation Services: Services offered by both the sensors and the clamp are combined to create a business rule. The creation of this business logic service is supported by xMII, SAP's Manufacturing Integration and Intelligence software. As mentioned before, the aim of this software is firstly to offer a mean for gathering monitoring data from different device aggregators on the shop floor such as MESs (Manufacturing Execution Systems). This functionality is depicted on Figure 3. Since the SOCRADES infrastructure proposes to DPWS-enable all the devices on the plant-floor, we can enhance the model by directly connecting the devices to xMII. Additionally, xMII offers a business intelligence tool. Using its data visualization services we create a visualization of processrelated and monitoring data. Finally, we use the visual composition tool offered by xMII to create the rule. Whenever this rule is triggered the stopTransportProcess()operation is invoked on the middleware to stop the clamp. ­ Enterprise Applications: Whenever the business rule is triggered, xMII invokes the updateOrderStatus()on the ERP. As mentioned before this latter component displays the failure and its consequences (i.e. a delay in the production) in the orders' list. Additionally, if the alert lasts for a while, it informs the customer by email providing him with information about a probable delay.

Fig. 3. xMII indirect device connectivity.

Fig. 4. Direct connectivity to the DPWS devices.

Cross-Component Communication Figure 5 presents the communication amongst the components whenever the business rule is triggered. At first the SunSPOT dispatches the temperature change by placing a SOAP message on the DPWS service bus. The xMII is subscribed to this event and thus, receives the message and feeds it to its rules engine. Since the reported temperature is above the threshold xMII fires the rule. As a consequence it invokes the stopTransportProcess()operation on the Process Service middleware. This component contacts the clamp and stops it. Furthermore, xMII triggers the updateOrderStatus()operation on the ERP. This latter system update the status of the concerned order accordingly and decides whether to contact the customer to inform him by email about the delay.

Fig. 5. Interactions when the business rule is triggered.

6

System Analysis

In this section we will discuss the properties of our architecture and give decision makers a framework at hand through which they can assess the concrete value of our system for their organisation. Since the work we are presenting in this paper is part of ongoing research, we think it is helpful to have such a framework, in particular to assess future work. In the field of Systems Management several standards exist [ref. Standards, ITIL, etc.] which aim to support a structured dealing with IT systems. One framework in particular helpful for central corporate functions such as produc-

tion is the ISO model FCAPS (Fault, Configuration, Administration, Performance, Security). Although being a framework for network management, it is relevant for our architecture because it is enabling low level networked interaction between Smart Objects. Here we will give a first attempt to evaluate the architecture. ­ Fault Management: Since our system will be part of the manufacturing IT-landscape we need to manage both, faults of particular parts of the manufacturing process and faults in our system. Due to the tight integration these types of faults inherently become the same. In particular the SOA based approach of device integration enables the user to identify faults in his production process, at a level never seen before. It also gives the possibility to build redundancy at system critical stages which ensures fast recovery from local failures. Finally the flexibility given by our SOA approach lets the user decide to what extend he wants to introduce capabilities of quick fault recovery, depending on his individual needs. ­ Configuration Management: Mainly the two components Service Lifecycle Management and Cross-Layer Service Catalogue support dynamic configuration management. However, at the current point of view we see code updated to Smart Devices as a major challenge which until today has not been resolved sufficiently. Configuration also includes the composition of services into higher-level services. In a future version, our Service Discovery module will use semantic annotation of services to find appropriate service instances for online service composition. Using ontologies to specify the behaviour and parameters of web services in their interface descriptions and metadata allows flexible service composition. Especially in the very well defined domain of manufacturing we can make use of existing ontologies that describe production processes. ­ Administrative Management: The Device Manager provides the necessary static and dynamic information about each Smart Device. Through the strict use of web-service interfaces, it will be possible to easily integrate devices into management dash-boards. Through this technically we allow easy and user friendly access to Smart Devices. However, taking the possibly very large number of devices into account, we belief that our middle-ware has deficiencies in offering this user friendly administration. Although this problem is subject to other fields of research such as sensor networks, (e.g, macro programming), we will dedicate our research efforts to the problem. ­ Performance Management: Already now we can say that local components of our system will scale well in regards to total amount of Smart Objects and their level of interaction. This can be justified since all interaction occurs locally and only a limited amount of Smart Objects is needed to fulfil a particular task. However, it is still an open question, if our system will scale well on a global scale and to what extend it will need to be modularized. For example we will need to investigate whether central components such as device and service registries should operate on a plant level or on

a corporate level, which could mean that these parts would have to handle several millions or even billions of devices at the same time. ­ Security Management: As mentioned in the security support section of the architecture, our system can make use of well established security features which already are part of web-service technologies and their protocols such as DPWS. It is most likely that we will have to take into account industry specific security requirements, and it will be interesting to see, if we can deliver a security specification which satisfies all manufacturing setups.

7

Conclusions

In this paper we have presented SOCRADES, a Web Service based Shop Floor Integration Infrastructure. With SOCRADES we are offering an architecture including a middleware which support connecting Smart Devices, i.e. intelligent production machines from manufacturing shop floors, to high-level back-end systems such as an ERP system. Our integration strategy is to use web services as the main connector technology. This approach is motivated by the emerging importance of Enterprise Service Oriented Architectures, which are enabled through web services. Our work has three main contributions: First, we elaborated and structured a set of requirements for the integration problem. Second, we are proposing a concrete architecture containing of components which realized the required functionality of the system. Our third contribution is a reference implementation of the SOCRADES architecture. In this implementation we have demonstrated the full integration of two Smart Devices into and enterprise system. We showed that it is possible to connect Smart Devices to an ERP system, and describe how this is done. Our next steps include integrating a prototype in a bigger setup and testing it with live production systems.

8

Acknowledgments

The authors would like to thank the European Commission and the partners of the European IST FP6 project "Service-Oriented Cross-layer infRAstructure for Distributed smart Embedded devices" (SOCRADES - www.socrades.eu), for their support.

References
1. Instrumentation Systems and Automation Society. http://www.isa.org/. 2. SIMATIC WinCC flexible. http://www.siemens.com/simatic-wincc-flexible/. 3. Web Services Business Process Execution Language Version 2.0 (OASIS Standard), April 2007. http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.html.

4. H. Bohn, A. Bobek, and F. Golatowski. SIRENA - Service Infrastructure for Realtime Embedded Networked Devices: A service oriented framework for different domains. In International Conference on Systems and International Conference on Mobile Communications and Learning Technologies (ICNICONSMCL'06), page 43, Washington, DC, USA, 2006. IEEE Computer Society. 5. E. Fleisch and F. Mattern, editors. Das Internet der Dinge: Ubiquitous Computing und RFID in der Praxis:Visionen, Technologien, Anwendungen, Handlungsanleitungen. Springer, 2005. 6. L. Gaxiola, M. de J. Ram´ irez, G. Jimenez, and A. Molina. Proposal of Holonic Manufacturing Execution Systems Based on Web Service Technologies for Mexican SMEs. In HoloMAS, pages 156­166, 2003. 7. G.Gorbach. Pursuing manufacturing excellence through Real-Time performance management and continuous improvement. ARC Whitepaper, April 2006. 8. F. Jammes, A. Mensch, and H. Smit. Service-Oriented Device Communications using the Devices Profile for Web Services. In MPAC '05: Proceedings of the 3rd international workshop on Middleware for pervasive and ad-hoc computing, pages 1­8, New York, NY, USA, 2005. ACM Press. 9. F. Jammes and H. Smit. Service-oriented paradigms in industrial automation. IEEE Transactions on Industrial Informatics, 1:62­70, 2005. 10. S. Karnouskos, O. Baecker, L. M. S. de Souza, and P. Spiess. Integration of SOAready Networked Embedded Devices in Enterprise Systems via a Cross-Layered Web Service Infrastructure. In 12th IEEE Conference on Emerging Technologies and Factory Automation, 2007. 11. A. Reinhardt. A Machine-To-Machine "Internet Of Things". Business Week, April 2004. 12. U. Saif and D. J. Greaves. Communication Primitives for Ubiquitous Systems or RPC Considered Harmful. In 21st International Conference of Distributed Computing Systems (Workshop on Smart Appliances and Wearable Computing), Los Alamitos, CA, USA, 2001. IEEE Computer Society. 13. C. R. Schoenberger. RFID: The Internet of Things. Forbes, (18), March 2002. 14. E. Zeeb, A. Bobek, H. Bohn, and F. Golatowski. Service-Oriented Architectures for Embedded Systems Using Devices Profile for Web Services. In 21st International Conference on Advanced Information Networking and Applications Workshops., 2007.

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Security Considerations for Distributed Web-Based e-commerce Applications in Java
Timothy E. Lindquist Electronics and Computer Engineering Technology Arizona State University East http://www.east.asu.edu/ctas/ecet Tim@asu.edu Abstract
Today's distributed e-commerce applications typically rely upon various technologies in their realization, including the web, scripting languages, server-side processing and an underlying database. The combination of these technologies creates a system that requires attention to the security issues of each component and the system as a whole. In considering the overall system, issues arise from the interactions of security frameworks available for each component. In this paper, we consider the approach and related issues for distributed e-commerce applications developed with Java. The flexible nature of Java allows migration of objects (compiled code with state) through features such as RMI and Applets. Security for distributed applications developed in Java has issues and lessons applicable to systems of components built on different technologies.
DBMS

legacy appl

view objects/ clients IGURE 1.

business logic/ middle-tier/ server objects

transaction monitor

3-Tier Client-Server Architecture

1. Problem
Web-based e-commerce and distributed applications are changing the way we buy goods, access information and learn. Use of email and other related technology increasingly facilitates collaboration and is more commonly being used for official communications. Official communications via the internet are too often done in insecure mode. Web-based e-commerce applications commonly employ multiple tiers (3-tier client server architecture) and a combination of technologies such as HTML, XML, JavaScript, Java (JSP, Servlets), ASP, dynamic html, CGI, and relational databases, as shown in Figure 1. Each of these technologies have separate and in some cases incompatible approaches to protection against intrusion. For web-based applications, the communication between clients and the middle-tier is via web protocol http. Clients may employ any number of technologies such as applets, html, xml, and scripts. The middle-tier business

logic often employs any of a number of CGI work-arounds such as Netscape's NSAPI, Microsoft's ISAPI, WebObjects, ASPs, Java J2EE, servlets and JSP. The combination of different technologies at each tier, presents special challenges to security of the overall application. Development time and cost pressures often short-change security concerns. Problems range from software design constraints that prevent adequate security to insufficient testing to exercise common attacks. Often performance concerns limit the extent to which assurance can be implemented in a web-based application. Emerging technologies and applications are also presenting new challenges to secure applications. Distributed Object technologies have been maturing for the past several years and are being increasingly utilized in web-based developments. Although some researchers have supported an approach where an object-web replaces the largely datacentered web of today, this has not materialized. Nonetheless, object technologies such as CORBA, DCOM and Java RMI enjoy increased usage in distributed web-based applications. Additional frameworks such as JINI, JavaSpaces, JNDI, and EJB support distributed Java Objects.

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

1

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Distributed objects are network aware objects. Applications running on remote machines may communicate with each other at the object-level using this technology. With a remote reference to a Java RMI object, methods can be requested of the object (messages) in the same manner as if the object were local to the executing application. The SUN implementation of Java (sdk/jdk1.2 and up) includes a flexible and feature rich approach to security providing the basis for distributed Java applications.

private key

message

message

public key

algorithm

algorithm

signature

signature

verify

2. Security Concerns
Sender The platform independence of Java has lead to easy movement of (compiled) code across the internet. While the approaches of OMG CORBA (see: http:// www.omg.org) and Microsoft do provide multi-language solutions, they do not provide the same code migration capabilities as is available with Java. Interacting remote Java objects may easily be written in a manner that requires dynamic movement and execution of code (class files) across the internet. Security concerns include authentication, integrity and encryption/decryption. These may all come into play whenever information (code or data) is moved (across a network or within a single machine). 1. Authenticity allows the receiver of information to know with certainty the identity of the sender. 2. Integrity allows the receiver to know with certainty that information transmitted by the sender has not been modified or tampered with enroute. 3. Encryption is the process of taking data (called clear text) and a short key and producing cipher data that is meaningless to anyone who does not know the key. Decryption is the process of taking cipher data and a short key to produce the corresponding clear text. Each of these basic security concerns come into play with distributed applications, for controlling executing applications as well as access to information. Figure 2 shows how authentication and integrity can be provided using digital signatures. The sender uses his own private key (which must be kept protected utilizing access control) and together with a message to generate a digital signature, which is unique to the message and private key. The message and signature are transferred to the receiver. The receiver must have a public key (usually received separately) corresponding to the sender's private key. The public key can be used to verify a signature, but cannot be used to generate a signature. Upon receipt, the message and signature are verified assuring both authenticity and integrity of the exchanged message.
IGURE 2.

Receiver

3-Tier Client-Server Architecture

3. Digital Signatures
Keys are generated in pairs. The private key is used to generate the signature and is kept confidential to whoever is doing the signing. The public key is used by the receiver to verify authenticity of the message. The signer should distribute the public key to anyone who will receive signed information. The issue as to whether the public key actually corresponds to the sender is resolved with certificates. A certificate represents a chain of trust leading from the sender to the receiver, indicating that the public key belongs to whom you want to believe it belongs. If the sender and receiver both trust the same certificating agency then the chain may be of length one. Each link in the chain is a certifying agency (such as VeriSign or Entrust) which certifies that the entity prior to it in the chain (the owner of the private key or another certifying agency) is who they say they are. Users should understand how certificates are signed and managed. Current web browsers display information about the certificate and who signed it, but few users ever look beyond the lock icon on their web browsers. This provides some opportunity for anyone with a signed certificate to use a man-in-the-middle attack. Simple possession of a certificate says nothing of integrity, quality or functionality of code or other information conveyed by the certificate holder. Another complication of digital signatures is management of a certificate revocation list. Once a key is known to be compromised, there must be some way to inform users that it should no longer be trusted. The SUN implementation of Java comes with a primitive set of tools for manipulating keys, certificates and digital signatures. It also includes the framework classes (in the package java.security) for program creation and verification of digital signatures. Figure 3 includes a sample of Java that may exist for the sender. The example generates a public and private key pair

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

2

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

//generate the private and matching public key KeyPairGenerator keyGen=KeyPairGenerator.getInstance("DSA", "SUN"); SecureRandom random = SecureRandom.getInstance( "SHA1PRNG", "SUN"); keyGen.initialize(1024, random); KeyPair pair = keyGen.generateKeyPair(); PrivateKey priv = pair.getPrivate(); PublicKey pub = pair.getPublic(); //create the signature object Signature dsa = Signature.getInstance("SHA1withDSA", "SUN"); dsa.initSign(priv); //read the datafile; FileInputStream fis = new FileInputStream(args[0]); BufferedInputStream bufin = new BufferedInputStream(fis); byte[] buffer = new byte[1024]; int len; while (bufin.available() != 0) { len = bufin.read(buffer); dsa.update(buffer, 0, len); } bufin.close(); //generate the signature byte[] realSig = dsa.sign(); //save the signed data in a file FileOutputStream sigfos = new FileOutputStream("sigOf"+args[0]); sigfos.write(realSig); sigfos.close();

4. Securing Java Applications
Many aspects of Java's design lend well to distributed applications. One such example is serialization. The ability to externalize objects from one executing Java program (virtual machine) and to read them into another is a process Java calls serialization. Serializable objects may be transmitted through the internet without loss of object properties, including methods. To accomplish object externalization, it is often necessary to move the compiled code along with object data. Several mechanisms exist within Java to do this either implicitly or explicitly under programmer control. Applets and Remote Method Invocation (RMI) are two such mechanisms. Applets are small Java programs communicated from a web-server and executed by a virtual machine running in the browser. RMI, provides the programmer with an object view of internet objects so that method calls, for example can formulated as though the object were in the same virtual machine. RMI capabilities are similar to Microsoft DCOM and the Object Management Group's Common Object Request Broker Architecture (CORBA). Java's platform independence is critical to realizing these dynamic capabilities. Compiled java code (class files) can move to a variety of platforms and be executed without loss of meaning. This powerful capability, which has not been realized to the same level and extent by any other language efforts, was first made generally available by Java implementations. Java's reflection capabilities, allow a program to discover and access the properties available in an object. This allows internet available objects to be manipulated in a manner not necessarily know by the program at the time it was compiled. In addition to facilitating distribution, Serialization, RMI, and Reflection are leading to a view of internet enabled software service objects. These provide the critical infrastructure for e-commerce services, such as financial, investment, and retail purchase. The current e-commerce solutions utilize the web and represent a composition of diverse technologies: 1. User interface through html, xml, Java, JavaScript, Flash and so on, 2. Server functions through dynamic html, JSP, ASP, J2EE, CGI or servlets, 3. Legacy data through RMI, ODBC or JDBC connectivity to a relational database. The challenge is to formulate a secure impenetrable application in light of the combination of a variety of technologies and capabilities. The Java model for securing the operations in an executing virtual machine has progressed significantly since the

FIGURE 3.

Snipet of Java to sign a file

and uses the private key to generate a digital signature for a data file. The signature is saved to file. This code represents simplistically what must be done by the sender. The public key, the signature file and the data file are all transmitted to the receiver, where a similar program verifies the signature using the data and public key. The primary vulnerability of this approach rests in communicating the public key. An attack may replace the data, signature file and public key if they are all three transmitted together. Certificates are the most common mechanism used to assure the public key authentically identifies the sender. Good practice dictates that the public key be transmitted separately in an assureable manner. The public key (certificate) is stored by the receiver for later use to authenticate multiple subsequent transmissions. This mechanism can be used to verify the authenticity and integrity of either data or program code that is transferred in distributed e-commerce applications. The approach verifies that information came from the purported sender and was not modified in transmission. Encryption is necessary to protect information from reading by others during transmission, as discussed below. Security within an executing Java application is based upon authenticity and integrity using digital signatures.

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

3

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

initial introduction of the sandbox model for Java applications. Initial versions of Java provided full trust to classes loaded locally and prohibited all sensitive operations from any code obtained dynamically. Java now supports a continuum of access control. Access to system resources (such as files, sockets, runtime, properties, security permissions, serializable, reflection, and window toolkit) is granted based on domains. A domain includes a set of permissions together with a codebase and an indication of who signed the code. The codebase indicates the file or URL from which the code is loaded. If signed, the alias of the public key can also be used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which defines the access it has to resources.

{

permission java.net.SocketPermission "*.GSE.com:2575-", "accept, connect, listen, resolve";

};

A policy may consist of one or more grants each defining different domains. Each domain may have one or more associated permissions. When a protected operation is attempted, the virtual machine's security manager performs a security check. It looks at the classes of all methods currently on the runtime call stack. Each associated protection domain is queried to determine whether the operation is allowed. An operation is performed only if all methods on the runtime stack have the appropriate permission. Signing executable is an important application of authentication and integrity technology. As the number of distributed applications grows and those applications increasingly rely upon migration of code, we need assurance that we are granting permissions to trustworthy code. Today, code signing is largely platform dependent. For example, applets executed with the Netscape or Internet Explorer Java virtual machines require use of Netscape or Microsoft tools to sign the code. Applets designed to run with the SUN plug-in virtual machine must be signed with the SUN tools. This lack of consistency only accentuates the problems arising from utilizing multiple technologies to realize an e-commerce application.

5. Cryptography
FIGURE 4. Controlling Access to Java Resources

Figure 4, is taken from the On-line Java Tutorial, http://java.sun.com/docs/books/tutorial/ and shows the interaction between the security domains defined in Java2 and the original sandbox model. In the Java 2 SDK version 1.4, the standard platform has been further augmented to integrate the Java authentication and authorization service (JAAS). Doing so takes a step closer to integrating user login services with authentication mechanisms. See: http://java.sun.com/products/jaas/ In Java 2, security domains are defined by a policy granting permissions to the domain. For example, suppose the company GrowthStocksExpress publishes an applet on their hypothetical web site at the URL: http://GSE.com/applets Assuming the applet needs connections to one or more hosts having a domain address ending with GSE.com on ports beginning at 2575, a policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase "http://GSE.com/applets"

The SUN implementation of the Java Cryptography Architecture (Java Crypto Extensions) is freely available for developing applications that rely on encryption. Similarly, if the application requires an encrypting web server, Apache-SSL is one of the freely available web servers based on OpenSSL. It can be freely obtained and used commercially. See: http://www.apache-ssl.org/ In addition to authentication and integrity, distributed ecommerce applications require cryptographic services. Authentication and integrity assure the identity of the sender and that information was not changed in transmission, but they do not protect against reading during transmission. Encryption is a concern for financial transactions or other communication where personal identification information must be transmitted. To guard against this type of intrusion, many encryption / decryption algorithms and implementations exist. Encryption is the process of taking clear text and converting it into cypher data that is unreadable to anyone who does not know the key. Decryption reconstructs clear text from the cypher data, using the key. The sender performs encryption before transmission and the receiver decrypts to

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

4

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

reconstruct the information. Several implementations of various strength exist. Primary issues relate to strength and performance - time and space to encrypt and decrypt. Figure 5 is taken from the Java Tutorial and shows the service provider architecture that is used in the security frameworks provided by SUN with Java. The API (application program interface) provides a common interface for developing e-commerce or other distributed applications. To the extent possible, various alternative approaches to security are cast into a single interface. Engine classes abstractly define cryptographic services. Providers (implementing security services) write to the lower level SPI (service providers interface). For an example implementation see JCSI [5]. SUN also provides a default implementation which is distributed with the downloadable extensions. Where multiple implementations exist, initialization methods select the appropriate implementation based on parameters. This same approach is used, for example in Java's database connectivity, JDBC. Where multiple drivers exist, selection is wired-into the API through initialization methods. Although this architecture is a powerful approach that adds considerable value to the Java framework, in practice it is very difficult to achieve a single common interface that works equally well for all implementations.

Authenticity and integrity are just that and no more. Signed Java can be relied upon regarding who signed it and that it has not been disturbed in transmission. The fact that a digital signature has been verified tells the user nothing about the goodness of the code or the security of the system that is delivered in signed form. These are elements of trust in the individual or company that signed the code. To further the problem, security problems do and will continue to result from problems in the infrastructure upon which the Java implementation is built. For example, denial of service attacks, file access, host system intrusion and underlying problems with TCP/IP all arise to the applications built on these technologies. Nevertheless, e-commerce applications must be secure and the best way to build in security is to use best software practices and processes for their development. Specification and design of a secure distributed Java application should include security risks, requirements and underlying constraints. Development should proceed with a security risk assessment, followed by design and reviews from risk perspective. Security testing, which is necessarily different from specification testing, should consider likely avenues of problems and exercise documented successful attacks on similar systems. For further reading on security problems with Java and related technology, see: http://www.cigital.com/javasecurity/articles-1.html http://www.w3.org/pub/Conferences/WWW4/Papers/ 197/40.html and the Java security website: http://www.rstcorp.com/java-security.html For further reading in Security and Encryption, see Peter Guttmann's web site [4], which contains references to various research publications as well as software and other internet resources related to security and encryption.

7. References
IGURE 5. Service Provider Architecture

6. Closing Remarks
For a language that has developed and whose use has spread so rapidly, Java's features are remarkably complete and consistent. Nevertheless, security in Java applications is a difficult task. Java security mechanisms are complex and as such are likely to be inappropriately used by developers. The Java security model, together with the Java cryptography architecture are powerful tools that are integrated well into the language both in terms of controlling applications and in terms of defining security frameworks that are amenable to realization by multiple implementations.

[1.] McGraw, Gary and Felton, Ed; Securing Java, John Wiley and Sons Inc., 1999, see: http://www.securingjava.com/ [2.] Griscom, Daniel; Code Signing for Java Applets; see: http://www.suitable.com/Doc_CodeSigning.shtml [3.] Campione, Mary, et al., The Java Tutorial, SUN, Addison Wesley, December 2000, http://java.sun.com/docs/books/tutorial/ [4.] Guttmann, Peter; Security and Encryption-Related Resources and Links, http://www.cs.auckland.ac.nz/~pgut001/links.html [5.] Sun Microsystems Java Security and Crypto Implementation, http://www.cs.wustl.edu/~luther/Classes/Cs502/ WHITE-PAPERS/jcsi.html

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

5

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION OF TIME SERIES*
´ , AND ANDERS LINDQUIST JORGE MARI, ANDERS DAHLEN

Abstract. In this paper we consider a three-step procedure for identification of time series, based on covariance extension and model reduction, and we present a complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy model is determined, which is finally approximated by a lower-order model by stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all three steps has been lacking. Supposing the data is generated from a true finitedimensional system which is minimum phase, it is shown that the transfer function of the estimated system tends in H to the true transfer function as the data length tends to infinity, if the covariance extension and the model reduction is done properly. The proposed identification procedure, and some variations of it, are evaluated by simulations.

1. Introduction In recent years there has been quite some interest in a certain type of procedures for identification of time series known as subspace methods [1, 42, 41, 28, 29]. These identification procedures are based on geometric projection methods, and they could be understood in the context of splitting geometry and partial stochastic realization theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9], these procedures are algebraically equivalent to minimal factorization of a Hankel matrix of covariance estimates, and they make no distinction between stochastic and deterministic partial realizations. Therefore they may fail because of loss of positive realness in the spectral estimation phase. In an attempt to overcome these problems we analyze an alternative approach to time series identification proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the maximum-entropy method, leading to a high order autoregressive (AR) process, and finally stochastically balanced truncation. This method shares certain features with stochastic subspace identification methods, the most obvious one being that it is based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
 This research was supported by a grant from the Swedish Research Council for Engineering Sciences (TFR).  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden 1

2

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for maximum-likelihood (ML) methods, are needed. The idea of approximating an autoregressive moving-average (ARMA) process by an AR process is by no means new. Its origins can be traced back to the Wold decomposition [55] where L2 -convergence of high-order AR models to general analytic models is shown. Pioneers in the use of this concept for systems identification are Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations were studied by Berk [2] and later refined in [36, 34, 33, 7]. The interesting paper [7] contains nice proofs of some of the convergence results needed in this paper, but, for the sake of completeness and insight, we provide new proofs based on some properties of fast filtering algorithms [5] and simple methods of complex analysis and Szeg o polynomials. The power of the theory of Szeg o polynomials and Toeplitz matrices in analyzing stochastic processes is reported in [24], but, except for elementary theory, it has not been much used in systems identification [39]. This is even more true for the newer results [16, 40, 37, 27] on orthogonal polynomials. The idea of using model reduction for systems identification appears in the thesis by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency weighted reduction. Instead, we use stochastically balanced truncation, for which we develop a simple computational procedure, exploiting the special structure of the AR model. We also show the advantage of this reduction procedure by theoretical analysis and simulations. In fact, a comprehensive study comprising all the steps mentioned above together with a qualitative and quantitative analysis of the entire identification strategy has been lacking, and that is what we offer in this paper. The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identification procedure contributes to the estimation error. In Section 3 we show that the transfer function of the maximum-entropy filter, constructed from true covariances, tends to that of the true filter in H norm at a geometric rate determined by the largest modulus of the zeros of the true filter as the order of the maximum-entropy filter becomes large. However the order of the approximation is too high, and therefore model reduction is performed. This is studied in Section 4. A stochastic balancing procedure, based only on linear-algebra operations so that no Riccati equations need to be solved, is provided together with the analysis of the model-reduction error. Both deterministically and stochastically balanced truncation lead to good results. However, when the covariances are estimated from statistical data, stochastic model reduction is found to be superior. In particular, variances are considerably closer to the Cram´ er-Rao bounds. In Section 5 we state our statistical convergence theorems, proving that the total error tends to zero as the length of the data string tends to infinity, provided the degree of the AR model tends to infinity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using stochastic subspace identification [43] is included. For clarity of exposition, all the proofs have been deferred to two appendices, Appendix A dealing with the asymptotic properties of the maximum-entropy filter, and Appendix B devoted to the statistical error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identification Time series identification in the form studied here amounts to estimating the matrices (A, B, C, D) in some n-dimensional linear stochastic system x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) driven by normalized white noise {w(t)}, from a data string of observations {y0 , y1 , y2 , . . . , yN } (2.2) of the output process {y (t)}, which here will be taken to be scalar. The basic idea behind our approach is very simple: given estimates of a partial sequence c 0 , c 1 , c 2 , . . . , c (2.3) of the covariances ck = E{y (t + k )y (t)}, which satisfies the condition that the Toeplitz matrix   c2 · · · c c0 c1  c1 c0 c 1 · · · c  -1     c c c 0 · · · c  -2  1 T +1 :=  2 (2.4)  . . . ... .  . . . . . . . . c  c  -1 c  -2 · · · c0 is positive definite, first construct a high-order model continuing (2.3) by covariance extension. This model has all the required positivity properties, but the order is too high. Then reduce the order by means of a positivity-preserving model reduction procedure to be specified below. That this simple recipe will in fact provide a good identification method is by no means a trivial matter but is based on some rather deep results, which will be presented here. More specifically, the approach consists of three steps, for which there are several possible variants that will be discussed below. The rigorous mathematical analysis, however, will be carried out for the following procedure, for which we shall give theoretical bounds. (i) Estimate a partial covariance sequence ^1 , c ^2 , . . . , c ^ c ^0 , c from the time-series data (2.2) via the ergodic estimate 1 c ^k = N +1
N -k

(2.1)

(2.5)

yt+k yt
t=0

k = 0, 1, . . . , .

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer function  ^  (z ) = z , (2.7) W ^ (z )  ^ (z ) is the normalized Szeg¨ where  o polynomial of degree  , to be introduced in Section 3, computed from the estimated covariance data (2.5).

4

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

^ (z ) of W ^  (z ) via a stochastic (iii) Determine a reduced-degree approximation W model reduction procedure [11] to be described in more detail in Section 4. In this procedure, the idea is that  >> n, the order of the system to be identified, ^ equals the degree n of the true system (2.1). However, the and ideally n ^ := deg W method will produce a valid model even if this is not the case or even if there is no "true" underlying model. This is in contrast to stochastic subspace identification models, which may fail to produce any model at all [9]. There are possibilities for variations of the procedure described above. In Step (i) we could use alternative covariance estimates or Burg's estimation of Schur parameters ^ +1 of (2.5) is [3], the only requirements being that the estimated Toeplitz matrix T positive definite and that c ^k  ck a.s. as N  . In Step (ii) we could instead use approximate covariance extension or covariance extension with prescribed zeros, for which there is now a complete parameterization [5] and an algorithm [4]. (In the latter case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction methods could be used. For example, an important model reduction paradigm is the one based on optimal Hankel norm approximation [21]. Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable transfer function W (z ) = C (zI - A)-1 B + D, (2.8)

L

of McMillan degree n. We also assume that W (z ) is minimum-phase so that both zeros and poles are located in the open unit disc. Then, we need to be able to measure ^ (z ), converges to the true one as how the estimated model, with transfer function W ^ (z ) in N  . In this paper we have chosen to use distance between W (z ) and W  norm as a measure of proximity between the true and estimated model. From an engineering point of view this could be called worst case identification. The modern literature in robust control makes extensive use of the worst case philosophy; see for example [20, 52]. There are also other reasons for using the  , as discussed in [35]. Returning, then, to the identification approach outlined above, the estimation error can be decomposed into three parts, one corresponding to each of the steps (i), (ii) and (iii). Hence we have the error bound

L

^ W -W



 W - W



^ + W - W



^ - W ^ + W

,

(2.9)

^  is where W is the AR model corresponding to the true covariances (2.3) and W the one determined from the estimated covariances (2.6). To prove convergence to zero of the estimation error (2.9), we shall need to assume that W is minimum-phase, ^ should have the same property, which moreover is desirable in many and hence W applications. Our procedure insures this. Estimating the first term in (2.9) is a problem in stochastic partial realization theory and function theory and will be dealt with in the next section. The third term concerns model reduction which will be studied, in the particular setting required here, in Sections 4 and 5. In Section 5, finally, we consider the second term together with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence Step (ii) in the identification procedure outlined in Section 2 is based on rational covariance extension. To understand this, let us consider the covariance extension problem from a more general point of view. Given a partial covariance sequence c 0 , c 1 , c 2 , . . . , c , (3.1)

covariance extension amounts to finding an infinite extension c +1 , c +2 , c +3 , . . . of this sequence such that the function is strictly positive real, i.e., it is an analytic function in the complement Dc of the open unit disc D, which maps Dc to the open right complex half-plane. Then (z ) := V (z ) + V (z -1 ) is a spectral density for a process having c0 , c1 , . . . , c as its first  covariances and which is coercive in the sense that (ei ) > 0 for all . Spectral factorization is then to find a stable transfer function W (z ) such that |W (ei )|2 = (ei ). In particular, we are interested in finding covariance extensions for which V (z ), and hence W (z ), have at most degree  . For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one correspondence between infinite covariance sequences c0 , c1 , c2 , c3 , . . . and a sequence of Schur parameters, or reflection coefficients, 0 , 1 , 2 , 3 , . . . , (3.3) with the property |t | < 1 for all t. In fact, fixing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and 0 , 1 , . . . , m-1 for each m. The Schur parameters can be determined from the covariances via the Szeg¨ o polynomials t (z ) = z t + t1 z t-1 + · · · + tt t = 0, 1, 2 . . . , computed by means of the Szeg¨ o-Levinson recursion z -t t+1 (z ) = ( z ) - z 1  t t+1 where t (z ) ;  t (z ) 0 (z ) 1 = , ( z ) 1  0 (3.4) (3.2) c + c 1 z -1 + c 2 z -2 + . . . V (z ) := 1 2 0

t -1  t (z ) := z t (z ) is the reciprocal polynomial of t (z ), and the Schur parameters are computed via

= r1t t t j =0 t,t-j cj +1 rt+1 = rt (1 - |t |2 ), r0 = c0 .

(3.5)

6

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Hence t = -t+1 (0), a fact that we shall use below. In the problem to find a covariance extension for (3.1), therefore, 0 , 1 , . . . ,  -1 are fixed and the infinite continuation  ,  +1 , . . . can be chosen freely. In particular, if we take t = 0 for t = ,  + 1,  + 2, . . . . We obtain the maximum entropy solution W (z ) = z ,  (z ) (3.6)

where  (z ) is the normalized Szeg¨ o polynomial 1  (z ) :=   (z ). r (3.7)

Thus, in this particular case, the solution to the covariance extension problem turns out to be rational of degree at most  as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations are needed. In fact, it has recently been shown [5] that there is exactly one such solution for each choice of zeros of W (z ), thus proving a long-standing conjecture by Georgiou [18], who had established existence. Nevertheless, as we shall see next, rationality implies that the Schur parameters tend geometrically to zero, provided W (z ) has no zeros on the unit circle. In this section we shall demonstrate that the rational transfer function (2.8) can be approximated arbitrarily closely in L by the transfer function W (z ) of a maximum entropy filter for sufficiently large  and that this  depends on the maximum modulus of the zeros of W (z ). We shall first present a heuristic argument in support of this conclusion. To this end, let (3.2) be the infinite covariance sequence of the output process y in (2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via the Szeg¨ o-Levinson algorithm presented above. Then we have the following special case of Corollary 2.1 in [5]. Lemma 3.1. Let the spectral density (ei ) = |W (ei )|2 (3.8)

be coercive in the sense that it is positive for all  and let (3.3) be the corresponding infinite sequence of Schur parameters. Moreover, let   (0, 1) be greater than the maximum of the moduli of the zeros of W (z ). Then |t | = O( t ), i.e., |t |  M  t for some M  R and for sufficiently large t. Remark 3.2. Since (3.9) holds for all  greater than the the maximum of the moduli of the zeros of W (z ), we have in fact that |t | = o( t ), i.e., limt |t | -t = 0. For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis of certain fast algorithms for Kalman filtering [6]. (3.9)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the spectral density z (z - 1)2 (z ) = - 2 (z + z + 2)(2z 2 + z + 1) is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters are seen to be -1/2, -2/3, -2/5, -2/7, -2/9, -2/11, . . . , which tend to zero but not geometrically. On the other hand, there are coercive, analytic but nonrational models which also exhibit geometric convergence rate. A classical example [23] is obtained 2 when ck = k for some   (-1, 1). The Schur parameters in this case form an exact geometric sequence, k = (-)k+1 , k  0. Lemma 3.1 implies that, for a sufficiently large  which depends on  , the Schur parameters t are close to zero for t = ,  + 1,  + 2, . . . . But, the Schur parameters of W are exactly zero for t = ,  + 1,  + 2, . . . , and hence geometric convergence would insure that W is a good approximation of W (z ) for sufficiently large  . We shall prove that this is indeed the case. Theorem 3.4. Suppose W (z ) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let   (0, 1) be greater than the maximum of the moduli of the zeros of W (z ). Then
 

lim W - W



= 0,

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that W - W


 M  .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to first showing that
  -1 - W -1 lim W 

= 0.

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we give an alternative proof of this fact based on Szeg¨ o theory, and also show that the convergence is geometric. In fact, we can choose  arbitrarily close to the maximum modulus of the zeros of W . However, as we shall see next, we can actually prove more. To this end, let us first -1 and W -1 have their poles in the open unit disc D and thus observe that, since W are bounded and analytic in the complement Dc of D, they belong to the Hardy space  of functions which are analytic and bounded in {z  C | |z | > 1}. Hence the H-  , and convergence (3.12) is in H- z -  (z )  W -1 (z ) (3.13) uniformly in each compact subset of Dc . Now, W -1 is analytic in {z  C | |z |   }, a region that is strictly larger than Dc . This in itself of course does not insure that the convergence (3.13) extends to this larger region. In fact, even if z -  (z ) did converge in {z  C |   |z |  1}, it could fail to converge to W -1 (z ) there. The fact that it really does converge uniformly to this limit is another consequence of Lemma 3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and hence an estimate of the convergence rate  , is given in [35]. Theorem 3.5. Suppose W (z ) is a minimum-phase rational function having all its poles in the open unit disc D and all its zeros in

D := {z  C | |z |  }  D

where 0 <  < 1,

and let { (z )} o polynomial (3.7) determined from the co0 be the normalized Szeg¨ variances in the spectral density


|W (e )| = c0 + 2
i 2 k=1

ck cos k.

Then, as   , z -  (z )  W -1 (z ) uniformly in every compact subset of {z  C | |z | > }, the complement of D .

Dc  :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of  (z ) and hence of the poles of the high-order AR model with transfer function W (z ). It is known that, if the Toeplitz matrix T +1 is positive definite, all roots of  (z ) are located in the open unit disc D, but little has been reported in the literature on their behavior as   . This behavior is illustrated in Figure 3.1.
Original system 1 1 Original system

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0

0.5

1

-1 -1

-0.5

0

0.5

1

Original and AR(24) 1 1

Original and AR(24)

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0

0.5

1

-1 -1

-0.5

0

0.5

1

Figure 3.1: Distribution of zeros of  (z ).

The top two diagrams show the zero-pole positions, within the boundaries of the unit circle, of two minimum phase spectral factors W , both of degree five. Also indicated is a circle of radius equal to the maximum modulus of the zeros of these spectral factors. The little circles "" represent zeros and the "+" sign represent poles. The lower two figures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the exact covariance sequence. The poles of the latter models are indicated with "×". The left part of Figure 3.1 illustrates what may happen if all the poles of W (z ) are located in {z  C | |z | < }, where  is chosen to be the maximum of the moduli of the zeros of W (z ). The roots of  (z ) tend to cluster inside a circle of radius  as   . This phenomenon is in a sense predictable, since the constant term of the Szeg¨ o polynomials is n+1 (0) = -n , which equals the product of the roots and, by Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to . This does not preclude that other types of crowns may occur, because subsequences of {n } could decay faster than the overall rate  , as follows from [5]. Very general statements about the distribution of zeros of orthogonal polynomials, derived with the help of potential-theoretic methods, can be found in [37, 27]. To the right in Figure 3.1 we see what happens in the case that W has poles with moduli larger than . Then, for  sufficiently large, the normalized Szeg¨ o polynomial  (z ) has roots in {z  C |   |z | < 1}, but exactly as many as the poles of W in this region and approximately at the same place as these. This is of course due to the uniform convergence of z -  (z ) to W -1 (z ) in every compact subset of Dc  . The other roots of  (z ) behave exactly as in the previous case and tend to accumulate in a crown inside and very close to the circle {z  C | |z | = }.  approximation W of W which can be made We have thus constructed an H- arbitrarily good by choosing  sufficiently large. However, W will have much larger degree and, except for the poles outside the circle {z  C | |z | = }, a completely different zero-pole pattern. We shall rectify this situation by model reduction. In fact, for the moment considering the perfect modeling problem to identify the rational transfer function (2.8) given an exact partial covariance sequence (3.1), the last step in our procedure consists in approximating W by a rational function Wred of smaller degree, ideally of the same degree as W . The simplest model reduction procedure is deterministically balanced truncation (DBT), first introduced by Moore [38]. Though easy to implement, it may fail to yield a minimum-phase approximation, a requirement which is important in certain contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced truncation (SBT), first introduced by Desai and Pal [10], which is based on a different balancing strategy to be explained in detail in Section 4.
Original system 1 0.5 0 -0.5 -1 -1 1 0.5 0 -0.5 -1 -1 Reduction by DBT 1 0.5 0 -0.5 -1 -1 Reduction by SBT

0

1

0

1

0

1

Figure 3.2: Zero-pole pattern of W (z ) and Wred (z ) for different model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This fifth-order

10

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

model has first been approximated by W of degree  = 24, producing the pole-zero pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens when the model is reduced back to order five by either deterministically balanced truncation or stochastically balanced truncation. The zeros are denoted by "" and the poles by "+". Both reduction procedures give good approximations when applied to exact covariance data. However, as we shall see in Section 5, the advantages of SBT becomes apparent when applied to statistical data. Also, as explained in Remark 4.5, there are theoretical reasons to prefer stochastic model reduction. 4. Model reduction In the present setting, model reduction amounts to replacing a stochastic system (2.1) of dimension  by one of some dimension r <  in such a way that most of its statistical features are retained. In particular, we want to remove the part of the system which corresponds to the weakest correlation between past and future. This idea can be formalized in the following way. Basic concepts. In the Hilbert space generated by the random variables {y (t) | - < t < } in the inner product u, v = E{uv }, let H - be the subspace generated by the past, i.e., {y (t) | t < 0}, and H + that generated by the future {y (t) | t  0}. Consider the Hankel operator H : H +  H - and its adjoint H : H -  H + defined as

H = EH
-

-

|H +

and

H = E H

+

|H - ,

(4.1)

where E H denotes orthogonal projection onto the past space H - . More precisely, H sends   H + to E H -   H - and H sends   H - to E H +   H +. Since the process y is the output of a minimal stochastic system of dimension  , rank H =  by Kronecker's Theorem [56], and hence H has exactly  singular values, 1 , 2 , . . . ,  , which are positive, as usually listed so that 1  2  · · ·   . These singular values are the canonical correlation coefficients and hence the cosines of the angles between the principal directions of the past space H - and the future space H + . They are therefore less than one, and the part of the stochastic system corresponding to singular values which are close to zero have a weak coupling between past and future, i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic model reduction is to truncate the system so that this part is removed. To each singular value k there is an associated Schmidt pair (k , k ) with k  H + and k  H - such that

Hk = k k ,

Hk = k k ,

and such that the sequences 1 , 2 , 3 , . . . and 1 , 2 , 3 , . . . of singular vectors are orthonormal. The singular vectors corresponding to nonzero singular values span the predictor spaces X- := span{1 , 2 , . . . ,  }, X+ := span{1 , 2 , . . . ,  }. Clearly, X-  H - and X+  H + . The process y has one representation (2.1) for each minimal spectral factor W , having W as its transfer function. Such representations are called minimal stochastic realizations and the corresponding subspaces X := {a x(0) | a  R } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X- is the splitting subspace of the stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) (4.2)

with the transfer function W- (z ), the minimum-phase spectral factor; and X+ is the splitting subspace of x+ (t + 1) = Ax+ (t) + B+ w+ (t) y (t) = Cx+ (t) + D+ w+ (t) (4.3)

with transfer function W+ (z ), the maximum-phase spectral factor, having all its zeros in Dc . Note that A and C are the same in both realizations (uniform choice of bases). Each realization has a counterpart which evolves backwards in time and has the same splitting subspace. For example, the backward realization of X+ , ¯+ w ¯+ (t) + B ¯+ (t) x ¯+ (t - 1) = A x , ¯ ¯ ¯+ (t) y (t) = Cx ¯+ (t) + D+ w (4.4)

¯ + (z ), the coanalytic minimum-phase spectral factor, having all has transfer function W ¯ + (z ) = W- (z -1 ). its poles and zeros in Dc . In the present case with scalar y , we have W Now, in order to identify the part of the system which has the weakest coupling between past and future, and hence will be removed in the model reduction, we need to balance the system in the sense of Desai and Pal, as we shall explain next. To this end, we make a coordinate transformation ¯ ), ¯ )  (SAS -1 , CS -1 , CS (A, C, C in the minimal realization of ¯ + 1 c0 , (4.6) V (z ) = C (zI - A)-1 C 2 the strictly positive real part of the spectral density of y , so that the state covariances ¯+ = E{x ¯+ (t)¯ x+ (t) } coincide with the diagonal  ×  P- := E{x- (t)x- (t) } and P matrix  of nonzero canonical correlation coefficients, i.e., ¯+ =  := diag(1 , 2 , . . . ,  ). P- = P
1

(4.5)

(4.7)

This is done by choosing S so that Sx- (0) =  2  , where  = (1 , 2 , . . . ,  ) , and 1 (S )-1 x ¯+ (0) =  2  , where  := (1 , 2 , . . . ,  ) . To compute the canonical correlation coefficients, we first observe that the eigen¯+ are precisely the squares of the canonical correlation values of the product P- P coefficients, i.e.,
-1 2 2 2 ¯+ ) = (P- P+ ) = {1 , 2 , . . . ,  }, (P- P

(4.8)

-1 ¯+ where we have used the fact that the state covariance of (4.3) is P+ = P . Therefore the canonical correlation coefficients can then be determined via (4.8) by solving the Lyapunov equations

P- = AP- A + B- B-

and P+ = AP+ A + B+ B+ .

(4.9)

12

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

The point is now to identify the canonical correlation coefficients 1 , 2 , . . . , r corresponding to the part of the system one wants to keep. The part corresponding to r+1 , r+2 , . . . ,  will be disposed of. This amounts to partitioning  as = 1 2 , (4.10)

where 1 is r × r. In order to reduce model (2.1) we make the coordinate transformation (A, B, C )  (SAS -1 , SB, CS -1 ), with the same balancing transformation S . Then, partition the new triplet (A, B, C ) conformally with (4.10) as A= A11 A12 , A21 A22 B= B1 , B2 C = C1 C2 , (4.11)

and perform a principal subsystem truncation to obtain the transfer function of a reduced-order system Wred (z ) = C1 (zI - A11 )-1 B1 + D (4.12) of degree r. If 2 is close to zero, while 1 is not, the rank of H is close to r, and the discarded part of the system gives a negligible contribution to y . Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy filter   r z (4.13) W- (z ) := W (z ) =  (z ) of order  , which, for the moment we denote W- (z ) to emphasize its character as the minimum-phase spectral factor of the spectral density r .  (z ) (z -1 ) Remark 4.1. Without loss of generality we assume that  (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller  for which this condition holds. In fact,  (0) =  -1 , and if  -p =  -p+1 = · · · =  -1 = 0 and  -p-1 = 0 for some p = 1, 2, . . . ,  , then  (z ) = z  -p  -p (z ) by (3.4), and hence (3.6) can be replaced by W (z ) = W -p (z ), and for W -p (z ) the required condition holds. The maximum-phase spectral factor W+ (z ) has all its zeros at infinity, and hence  r -1 W+ (z ) = h (zI - F ) b = , (4.14)  (z ) where (F, b, g ) is the (observable) canonical form     0 0 1 ··· 0 . . ...  .  . . . . . .  .  . , b =  .  F =  0 ,  0 0 ··· 1   - -, -1 · · · - 1 r   1 0 , h= . . . 0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

 1 ,  2 , . . . ,  being the coefficients of the Szeg¨ o polynomial  (z ). In this basis, it follows from (4.9) that [P+ ]jk = 1 2
 - 

(ei I - A)-1 bb (e-i I - A )-1 d
jk

1 = 2
   .  .  . =   2    1 1

r e-(j -k)i d = cj -k ,  (ei ) (e-i ) -
···  1        r -1   and R =    r -2 .. . r0    , (4.16) 

and hence P+ = T . It is well-known and easy to prove that  T  = R , where
 -1, -1 . . .  -1,1 1  -2, -2 . . . 1

 +1

and consequently
-1 -1 ¯+ = T P =  R  .

(4.17)

It remains to determine P- . From (4.13) is easy to see that  W- (z ) = - (zI - F )-1 b + r , where  :=  , -1 · · ·  1 ,

(4.18)

(4.19)

but, in order to determine P- , this realization needs to be transformed so that the A and C matrices are the same as in (4.14) (uniform choice of bases). More precisely, we need to perform a transformation (F, b, - )  (QF Q-1 , Qb, - Q-1 ) =: (F, Qb, h ). Then P- is the solution of the Lyapunov equation P- = F P- F + Qbb Q , and therefore, since T = F T F + bb and QF = F Q and consequently QT Q = F QT Q F + Qbb Q , we have P- = QT Q . To determine Q, notice that - = h Q and QF    h -  - F   h F = .  .   .  . . . - F  -1 Next, define the symmetric matrix
- 1/2 - 1/ 2  QT Q  R . M := R

(4.20) = F Q to form    Q = Q.  (4.21)

h F  -1

(4.22)

14

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

¯+ ), and hence, by (4.8), M In view of (4.20) and (4.17), det(zI - M ) = det(zI - P- P 2 2 2 has the eigenvalues 1 , 2 , . . . ,  , and the singular-value decomposition M = U 2 U , where U U = U U = I . It is then well-known and simple to check that
- 1/2  S := -1/2 U R

(4.23) (4.24)

¯+ S -1 = . is the required balancing transformation (4.5) such that SP- S = (S )-1 P Proposition 4.2. Given the partial covariance sequence ck = E{y (t + k )y (t)}, k = 0, 1, . . . , , o polynomials let 1 (z ), 2 (z ), . . . ,  (z ) and r0 , r1 , . . . , r be the corresponding Szeg¨ and error variances. Supposing that  -1 = - (0) = 0, let (F, b, h) be given by (4.15), R and  by (4.16) and Q by (4.21). Moreover, let U and  be defined by the singular value decomposition (4.23) of (4.22). Then, the canonical correlation coefficients 1 , 2 , . . . ,  are the diagonal elements of , as described in (4.7), and the stochastically balanced realization of W is given by  (4.25) (A, B, C, D) = (SF S -1 , SQb, h S -1 , r ), where S is defined by (4.24). Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase property. In fact, we have the following result, the proof of which is given in Section A. Theorem 4.3. Let Wred be the SBT approximation of degree r of W , and set k := 2 1 - k k=r+1
  -1

and

 :=



c0
k=0

1 + |k | , 1 - |k |

(4.26)

where 0 , 1 , . . . ,  -1 are the Schur parameters of c0 , c1 , c2 , . . . , c . Then c0 (1 - )-1  |Wred (ei )|  (1 + ) for all , W - Wred  . (4.27) (4.28) and, if < 1, Wred is minimum phase. Finally, the approximation error has the bound


A properly executed SBT procedure should imply that the canonical correlation coefficients r+1 , . . . ,  , and hence , are close to zero, insuring the minimum-phase condition. Remark 4.4. Stochastic model reduction can also be carried out by instead per¯ + 1 c0 , ¯ ) in V (z ) = C (zI - A)-1 C forming principal subsystem truncation on (A, C, C 2 ¯ = S (c1 , c2 , . . . , cn ). It was shown in where A and C are given by (4.25) and C [32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally, the spectral density red (z ) := Vred (z ) + Vred (z -1 ) is factorized to yield a minimum~ . This is in a sense a more natural procedure, but we do phase spectral factor W not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small 2 it yields almost the same result. In fact, it is shown in [53], that ~ (ei )|2 = |Wred (ei )|2 + H (ei )2 H (e-i ), where H (z ) = C1 (zI - A11 )-1 A12 . |W Remark 4.5. There are good reasons to prefer stochastic over deterministic model reduction, as seen from the following heuristics. In fact, it can be seen that V (z ) = c0  (z ) , 2  (z ) (4.29)

where  (z ) is the Szeg¨ o polynomial of the second kind (obtained by exchanging -t for t in the recursion (3.4)). Now, the matrix representation of the Hankel operator ¯+ respecH in the innovation bases of the past and the future, provided by w- and w is the infinite Hankel matrix of the sequence tively, is given by L-1 (L-1 ) , where c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T ; see, e.g., [32, p. 714]. It is easy to see that  (z ) has the same asymptotic behavior as  (z ), i.e., the roots tend to cluster uniformly inside the circle z =  as   , and hence these roots are close to canceling in (4.29). Consequently, the corresponding Hankel matrix is close to having low rank. This massive "almost cancellation" does not occur in W (z ), and hence the corresponding infinite Hankel matrix, constructed from the Laurent coefficients of W (z ), may have a less distinct separation between 1 and 2 . On the other hand, since the Schur parameters tend geometrically to zero, the lower part of L tends to the identity, and hence the asymptotic behavior of the canonical correlation coefficients is very much like that of the singular values of . Therefore we may expect SBT to have better statistical behavior than DBT. In Section 6 we shall see that this is the case.

H

H

H

H

5. Identification from statistical data We now return to our original problem of time series identification: Given a data string (2.2) of observations of the output process y of some n-dimensional linear stochastic system (2.1) with minimum-phase transfer function W (z ), given by (2.8), ^ B, ^ C, ^ D ^ ) of the matrices (A, B, C, D). find an estimate (A, The identification method proceeds as follows. Given the covariance estimates (2.5), we compute the corresponding maximum entropy filter (2.7), a balanced realization (4.25), and the canonical correlation coefficients ^2 ,  ^3 , . . . ,  ^ ,  ^1 ,  (5.1)

^1 , . . . , c ^ . determined as in Proposition 4.2 from the covariance estimates c ^0 , c Based on (5.1), choose an integer n ^ such that  ^n ^n ^ are close to zero or ^ +1 ,  ^ +2 , . . . ,  ^2 , . . . ,  ^n . Then, the balanced realization (4.25) at least distinctively smaller than  ^1 ,  ^ is truncated accordingly as in (4.11) to yield a n ^ -dimensional triplet (A11 , B1 , C1 ) and a transfer function ^ (z ) = C1 (zI - A11 )-1 B1 + D. W ^ B, ^ C, ^ D ^ ). Then, (A11 , B1 , C1 , D) is the required estimate (A, As pointed out in Section 2, we have a bound ^ W -W


(5.2)

 W - W



^ + W - W



^ - W ^ + W

,

(5.3)

16

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the first term W - W  , which does not depend on the statistical data (2.2) but only on the underlying system (2.1), tends to zero geometrically with a rate   (0, 1) as   . The other two terms depend on the data (2.2), and here N must grow at a faster rate than  . In fact, we shall assume that  =  (N ) = O(log N ), (5.4)

 = 0. We also need to assume that the which in particular requires that limN  N white noise process in (2.1) satisfies a mild technical condition, namely

E{w(t)4 } < . This condition is, of course, satisfied if w is Gaussian. Next, we present our main convergence theorem.

(5.5)

Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then, to each length N of the data string (2.2), there is a  (N ), tending to infinity with N ^ of fixed at the rate (5.4), such that any sequence of estimated transfer functions W degree n ^  n, determined, for each N and corresponding  =  (N ), by the procedure described above, satisfies ^ 0 W -W ^ has almost surely as  (N )  . For sufficiently large  (N ), the transfer function W minimum phase. We have already proven that the first term in (5.3) tends to zero, so Theorem 5.1 follows from the next two theorems, each corresponding to one of the remaining terms in (5.3). As for the second term, we have the following result, the proof of which is deferred to Appendix B. Theorem 5.2. Suppose the system (2.1) satisfies the conditions of Theorem 5.1. Let W be the maximum-entropy filter (3.6) determined from the partial covariance se^  be the corresponding function determined from the ergodic quence (3.1) of y and let W estimates (2.5). Then, if  (N ) is defined as in Theorem 5.1, ^  (N ) W  (N ) - W almost surely as  (N )  . There are several results of this type in the literature [2, 36, 7, 33]. In particular, 3  0 as N   and  is coercive (i.e. positive Berk [2] proved that, provided  N on the unit circle), the estimated AR spectral density (ei )  (ei ) in probability. Under the same hypotheses, Caines and Baykal-G¨ ursoy [7] showed that if N   5+ -1 ^ for some  > 0, then W - W -1   0 almost surely as   . However, in both cases, ergodic estimates are used which are not quite the same as (2.5). Finally, we consider the last term in (5.3). The proof of the following theorem is given in Appendix B.


0

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function  (N ) are defined as in ^ ^  (N ) be defined as in Theorem 5.2 and W Theorem 5.1. Moreover, for each N , let W ^ has minimum phase, and as in Theorem 5.1. Then, for sufficiently large  (N ), W ^ ^  (N ) - W W almost surely as  (N )  . 6. Simulations ^  , rather than on the maximum-entropy filter of Performing model reduction on W exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier and more accurate order determination, as the heuristics of Remark 4.5 suggest. There are also alternative order determination statistical tests based on the canonical correlation coefficients [17, 26, 46]. But, even more importantly, there is less bias, and the error variances are closer to the Cram´ er-Rao bound. Since we are approximating rational models with AR models the method will be biased for finite amount of data, unless the model generating the data really is an AR model. The consistency result given in Theorem 5.1 implies that the method is asymptotically unbiased and therefore we consider the Cram´ er-Rao bound for unbiased methods; see [44, pp. 137­138]. The Cram´ er-Rao bound for biased estimation requires knowledge about the bias as a function of the parameter to be estimated. As already mentioned, the method will be unbiased and even statistically efficient for Gaussian AR processes if the model reduction step is omitted. Despite the fact that an algorithm based on covariance estimates (2.6) is not asymptotically efficient for general ARMA models [44, p. 144], our method can be used to provide a starting guess for other algorithms, for example the maximum likelihood method.
6 SBT dashed line, DBT dotted line. 5



0

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identification estimates.

To illustrate our procedure, let us consider data generated by passing white noise through a "true system" with transfer function W (z ) = z 5 - 0.0550z 4 - 0.1497z 3 - 0.2159z 2 + 0.1717z - 0.0495 . z 5 - 0.7031z 4 + 0.3029z 3 + 0.1103z 2 - 0.1461z + 0.2845

18

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy ^  of degree  = 24 determined from estimated covariances. Based on 100 model W test runs, the empirical means and standard deviations are determined. Figure 6.1 illustrates the statistical bias as a function of the length N of the data string when using stochastic (dashed curve) and deterministic (dotted curve) model reduction respectively. For the same test runs, Figure 6.2 illustrates the corresponding standard deviations together with the Cram´ er-Rao bound (solid curve). More precisely, the figures depict the sums of the moduli of the biases and standard deviations respectively for the ^ (z ). coefficients of the numerator and denominator polynomials of W
5 CRB solid line, SBT dashed line, DBT dotted line. 4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the Cram´ er-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z ) with poles and zeros closer to the unit circle is considered. The poles and zeros of ^ (z ) are determined for 100 runs and a data length N = 500. As before,  = 24. W Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with the poles and zeros of W (z ), which are denoted by "".

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and  = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT. To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and  = 24.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identification.

Figure 6.5 describes the result obtained when applying stochastic subspace identification to the same data. More precisely, Algorithm # 2 in [43] is used. In order to make the experiments comparable, we have chosen a Hankel matrix of dimension 13 × 13, which corresponds to  = 25 in our procedure. Note that the estimates are much less focused, and many zeros tend to cluster on the unit circle, implying that coercivity becomes critical. This is related to the positivity issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the subspace identification method performs worse than our SBT identification method, yielding larger biases and standard deviations, but performs better than when DBT is used.

20

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will disappear as  and N are increased. In Figure 6.6 we show the same experiment for  = 64 and N = 2000.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and  = 64.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and  = 40 using Burg's method.

In practice, there is a trade-off between the quality of the ergodic estimates, which roughly speaking depend on |max (A)|, the  -error tolerance, which is a function of |max (A - BD-1 C )|, and the numerical accuracy of the computations. For example, if the zeros of W (z ) are far from the unit circle and  is chosen very large, the error may increase. In the present example, it turns out that using Burg's method [3] in lieu of the ergodic estimate (2.6) yields better estimates for smaller  and N , as illustrated in Figure 6.7 which shows the case N = 500 and  = 40. A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There we give the empirical bias and standard deviation for the coefficients of the numerator and the denominator, respectively, of the estimated transfer functions together with the Cram´ er-Rao bound. It is the authors experience that Burg's method gives at least as good results as when using the ergodic covariance estimate (2.6), unless the intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

21

Parameter True value Bias: CE: Burg: Std.dev.: CE: Burg: CRB:

2 w b1 b2 b3 b4 b5 1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491 0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895 0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734 0.2332 0.1314 0.0508 0.0611 0.0722 0.0802 0.0712 0.0411 0.0381 0.0339 0.0339 0.0356 0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and  = 40 using covariance estimation (CE) or Burg estimation and , in both cases, followed by SBT.

Parameter a1 a2 a3 a4 a5 True value -0.6281 0.3597 0.2634 -0.5322 0.7900 Bias: CE: 0.0087 -0.0044 -0.0003 0.0066 -0.0152 Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125 Std.dev.: CE: 0.0274 0.0304 0.0371 0.0305 0.0304 Burg: 0.0336 0.0307 0.0358 0.0324 0.0306 0.0293 0.0321 0.0342 0.0322 0.0290 CRB: Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and  = 40 using covariance estimation (CE) or Burg estimation and, in both cases, followed by SBT.

7. Conclusions We have presented a three-step procedure for identification of time series, which is easy to understand and implement. Just like for subspace identification methods, robust linear-algebra algorithms can be used and no nonconvex optimization computations are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identification, as our extensive simulations indicate. In particular, its good performance has been confirmed by Monte Carlo simulations. The paper only covers the scalar case, but the multivariate case is presently being worked out. The three steps, covariance estimation, covariance extension and model reduction have each been studied separately before. This is an advantage which should make the method easy to grasp. However, a comprehensive study of the entire identification strategy, giving appropriate bounds, has been missing and this is what we offered here. The observation that the Schur parameters converge geometrically simplifies our application of Szeg¨ o theory and allows us to give a complete account of the asymptotic behavior of maximum entropy models of growing order. This analysis provides us with a clear indication as to when the identification strategy is good and when it might face difficulties, based purely on the closeness of the maximum modulus zero to the unit circle. The parsimony permeating other system identification methods should not be a reason for refraining from high-order modeling as an intermediate step. In fact, such a strategy might be desirable, since we have shown that the poles of the "true" system which lie outside a circle in the complex plane containing all of its zeros are

22

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the perimeter of this circle, providing a justification for choosing stochastically balanced model reduction, rather than deterministically balanced truncation, in the last step. With this reduction procedure, we have confirmed better statistical properties with variances closer to the Cram´ er Rao bound. The procedure could also be modified by exchanging exact covariance extension for approximate one, as outlined in [35]. Even though, in general, stochastic balancing would require the solution of a pair of Riccati equations, this is not the case for the particular maximum entropy models used here. In fact, the balancing procedure only requires linear algebra, and hence an intelligent use of the Levinson algorithm may substantially reduce the number of arithmetic operations. Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N ,  and n ^ , we gave worst-case guaranteed bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive stochastic system, but the method returns a valid model also for generic data. In fact, in contrast to many stochastic subspace identification [9], all steps of the procedure preserve the positive real property. Appendix A. Asymptotic behavior of the maximum entropy filter Theorem 3.4 is actually a modification to the rational setting of a theorem due to Szeg¨ o [47], and the proof is modeled after [19], which in turn includes aspects already present in the work of Schur [45]. See also [48], [49] and [16] for more facts on orthogonal polynomials. However, rationality and coercivity allows us to present a simplified and self-contained proof of a version of Szeg¨ o's classical theorem, to which we also are able to add geometric convergence. The derivation of Caines and BaykalG¨ ursoy [7] is shorter, but we feel that our approach is more systematic and gives additional insight into the mechanism of identification. To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas. o polynomials (3.7). Then |z -  (z )| Lemma A.1. Let { (z )} 0 be the normalized Szeg¨ is uniformly bounded from above and away from zero in the complement Dc of the open unit disc, i.e., there are positive numbers ,   R such that   |z -  (z )|   for all  and all z  Dc . Proof. In view of the Szeg¨ o-Levinson recursion (3.4), t+1 (z ) = t (z ) z -  ¯t and hence z -  (z ) =
 -1

 t (z ) , t (z )  k (z ) . k (z )

¯k 1 - z -1 

k=0

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , z are the roots of  (z ), it is immediately seen that 1 - zz ¯k   (z ) , =  (z ) k=1 z - zk which is a Blaschke product, analytic in Dc and having modulus one on the unit circle, and thus modulus less than or equal to one in Dc . Hence, since |z -1 |  1 in Dc ,
 -1 

(1 - |k |)  |z
k=0

-

 -1

 (z )| 
k=0

(1 + |k |)

(A.1)

for all z  Dc . But, these products converge to positive numbers as   . This follows from the absolute convergence of the infinite sum  k=0 |k |, a fact that, in the present context, stems from Lemma 3.1. From (3.5) we also have 0 < r  r  r0 , and consequently the lemma follows. Remark A.2. An equivalent statement of this lemma is that the maximum entropy solution W (z ), defined by (3.6), is uniformly bounded from above and away from zero for all  and z  Dc . Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of functions f (z ) := z -  (z ) converges uniformly to an analytic function f in statement of Theorem 3.5.

Dc ,

where

Dc 

is defined in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the purely algebraic relation


k (z )k (w) =
k=0

  ¯  (z ) (w)  (z ) (w ) - z w , 1 - zw ¯

(A.2)

which is called the Christoffel-Darboux-Szeg¨ o formula. In particular, setting w = 0 -1 and exchanging z for z in (A.2), (3.5) and (3.7) yield f (z ) 1 k-1 - k (z -1 )  .  = r c0 k=1 rk


(A.3)

c Observe that k (z -1 ) is analytic and bounded in Dc  , and hence in D , and therefore it  belongs to - . Moreover, by the maximum modulus principle, it attains its maximum value in Dc on the unit circle where, by Lemma A.1, it is bounded by  . Hence

H

|k (z -1 )|  

for z  Dc and for all k.

(A.4)

Therefore, in view of (A.3) and the fact that rk  r , we have f (z ) fµ (z )    -  r rµ r


|k-1 |,
k=µ+1

(A.5)

24

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for sufficiently large  and µ.  . The same holds for f (z ). In This establishes (A.3) as a Cauchy sequence in - c fact, since r  c0 , for all z  D

H

|f (z ) - fµ (z )| 

f (z ) fµ (z ) c0  -  r r  f (z ) fµ (z ) 1 1  c0  -  + |fµ (z )|  -  . r rµ r rµ



(A.6)

But, by Lemma A.1, |fµ (z )|   for all  and z  Dc , and therefore, in view of (A.5), we obtain |f (z ) - fµ (z )|   c0 r 1 1 |k-1 | +   -  r rµ k=µ+1


for all z  Dc . (A.7)

Since r  r as   , we see that, for each > 0, |f (z ) - fµ (z )| < for sufficiently large  and µ. Consequently, f tends uniformly in Dc to a function  . f  H- The uniform convergence and the analyticity can be extended to any compact -1  Dc . Therefore, by subset of Dc  . To see this, first note that z  D if and only if z Lemma A.1, |k (z -1 )|   |z |-k for z  D, and consequently, since r  rk , (A.3) yields 1 |f (z )|   +  |z |-1 |k ||z |-k . c0 k=0 Similarly, instead of (A.5) we have f (z ) fµ (z )  |k ||z |-k .   |z |-1  -  r rµ r k =µ
 -1  -1

(A.8)

(A.9)

> 0 such Now, for any compact subset K  Dc  , there is a   (, 1) and an -k ^ k where that |z | >  + for all z  K . Hence, by Lemma 3.1, |k ||z |  M   ^ :=  ( + )-1 < 1. Consequently, by (A.8), f (z ) is uniformly bounded in K , and (A.9) can be made arbitrarily small for sufficiently large  and µ. Therefore, by (A.6), f tends uniformly in K to the analytic function f . Lemma A.4. Let  be a real number such that  <  < 1. Then f -f Proof. It follows from (A.7) that   c0 |k-1 | + 1 - |f (z ) - f (z )|   r k= +1
 

= O(  ).

r r

for all z  Dc . (A.10)

By Lemma 3.1, the first term is O(  ). It remains to show that the same holds for the second term. To this end, first note that, by (3.5), 1- r =1- r k =
 2 1 - k .

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

25

But, by Lemma 3.1, |k |  M  k for some M . Therefore, since each x  [0, 1],  r  1 - (1 - M  k ) = O( t ) 1- rt k =t for t large enough. This concludes the proof.



1 - x2  1 - x for

Recalling the definition (3.6) of W , we note that Lemma A.4 may be written
-1 W - f 

= O(  ).

-1 in the same manner. As it turns out, by coercivity, this implies that W  f

Lemma A.5. Let W be the transfer function (3.6) of the maximum entropy filter. Then -1  W - f  = O ( ), where f is the limit function of Lemma A.3. Proof. Note that the limit function f has the same uniform bounds as f in Lemma A.1. In particular, |f (z )|  , |f (z )|-1  -1 , and |W (z )|  -1 for all z  Dc . Consequently,
-1 W - f 

 W



-1 f



-1 W - f



-1   -2 W - f

,

so the required result follows from Lemma A.4. Lemma A.6. Let W be the rational minimum-phase function defined above, and let f be the limit function in Lemma A.3. Then W (z ) = f (z )-1 for all z  Dc . Proof. Let  (ei ) := |W (ei )|2 be the spectral density of the maximum entropy process. Then, in view of the interpolation condition, 1 2


e
-

ik

1 (e )d = ck = 2
i

 -

eik  (ei )d

for k = 0, 1, . . . , , (A.11)

from which we have pointwise convergence of the Fourier coefficients of  (ei ) to those of (ei ) as   , and hence  (ei )  (ei ) in the 2 sense. However, by Lemma A.5,  (ei )  |f (ei )|-2 in  norm, and hence a fortiori in 2 norm, as   . Since, in addition, not only (ei ) but also f is analytic in a neighborhood of the unit circle (Lemma A.3), we have

L

L

L

(ei ) = |f (ei )|-2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer. In particular, W is an outer spectral factor of  (ei ) satisfying W (z ) = exp 1 4
 -  -

eit + z log |W (eit )|2 dt . eit - z

But Lemma A.5, Equation (A.12) and the fact that (ei ) = |W (ei )|2 , W (z )  exp 1 4 eit + z log |W (eit )|2 dt = W (z ), it e -z

the outer spectral factor of . But, by Lemma A.3, W (z )  f (z )-1 in therefore f (z ) = W -1 (z ) as claimed.

Dc  , and

26

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6. Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6. Proof of Theorem 4.3. Following [53] we see that
-1 (W - Wred ) W 

 ,

(A.13)

and consequently |W (ei ) - Wred (ei )|  |W (ei )| holds for all , from which we have (1 - )|W (ei )|  |Wred (ei )|  (1 + )|W (ei )|. However, in view of (3.6) and (3.7), it follows from (A.1) that   r r i  |W (e )|   -1 ,  -1 k=0 (1 + |k |) k=0 (1 - |k |) which together with (3.5) yields c0  |W (ei )|    (A.14)

for all . This establishes (4.27). To see that Wred is minimum phase if < 1, note e's that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by Rouch´ c Theorem, Wred has the same number of zeros in D (including ) as W . Hence, since W is minimum phase, so is Wred . To establish the bound (4.28) note that W - Wred From (A.14) we have W
 

 W



-1 W (W - Wred )

.

 , and hence (4.28) follows from (A.13).

Appendix B. Statistical convergence proofs Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corre^ from (3.4) and (3.5), sponding Szeg¨ o polynomial  ^ (z ) and predictor error variance r and form the maximum-entropy filter   r ^ z ^ . W (z ) =  ^ (z ) ^ To determine W - W let z  Dc and form     r z r ^ z ^ W (z ) - W (z ) = -  (z )  ^ (z )    ^ )z -  (z ) - r z - ( (z ) -  ^ (z )) ( r - r . = z -  (z )z -  ^ (z )


Since r > 0, by (3.7) and Lemma A.1,   0 < µ := r   |z -  (z )|  c0  =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1), |z
-  -1

 ^ (z )|  µ ^ :=
k=0

(1 - | ^k |),

^1 , . . . ,  ^ -1 are the Schur parameters corresponding to the estimated cowhere  ^0 ,  variances (2.6). Therefore, by the maximum-modulus principle, ^  (z )|  max 1 {M |r - |W (z ) - W |z |=1 µµ ^ r ^ | +  c0 | (z ) -  ^ (z )|},

where we have also used the fact that r  c0 . But, for |z | = 1, ^  1, ^ (z )|   -  | (z ) -  ^  are the  -vectors formed as in (4.19) and · where  and  that  is the unique solution of the normal equations T  = -c where c := c c -1 . . .
1

is the

1

norm. Recall (B.1)

c1 ,

where T is the Toeplitz matrix defined by (2.4), and that r  = c 0 + c   . ^ . Then, Also, the analogous relations hold for  ^ and r ^  )  + c ^ ( -  ^ ) ^ = (c0 - c ^0 ) + (c - c r - r and hence ^ ^ |  |c0 - c ^0 | + c - c |r - r Finally,  | r -
1 1

(B.2)





^ + c



^  1.  - 

^ | ^ | |r - r |r - r  r ^ |     , r r + r ^


and consequently, since x ^ W - W


 x M 

for any x  R ,

^ {|c0 - c ^0 | +    c - c µµ ^  r ^  M c 1  ^  . c0 +  +   -  µµ ^ r



}

(B.3)

^  are each solutions of a normal equation (B.1). More Recall now that  and  ¯ for k > 0, where all ^  ^  = -c ^ . Since ck = CAk-1 C precisely, T  = -c and T eigenvalues of A are less than one in modulus, ck  0 exponentially, we have
 -1

c



 K1

and

T



 c0 + 2
k=1

|ck |  K2

for some constants K1 and K2 . Moreover, from [8] we have
-1 T 



1 c0

 -1

1 + |k |  K3 1 - |  k| k=0

28

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence  and the condition number (T ) := T
 -1 T   -1  T 

c



 K 1 K3

 K := K2 K3

is bounded for all  . Now, it is known [25] that for each data length N in (2.2), there is a  (N ) of order O(log N ) such that ^k | = O max |ck - c log log N N , (B.4)

0 k   (N )

and therefore, for any a  R, ^ ^0 |  0 and  a c - c  a |c0 - c


 0 as  =  (N )  .

(B.5)

Consequently the first term in the bound (B.3) tends to zero as N   and  (N )   provided it is done at the specified relative rates and provided µ ^ is bounded away from zero. However, the estimate (2.6) has the property that the ^ is positive definite for each finite  , and this in turn corresponding Toeplitz matrix T ^ > 0. Since, in addition is equivalent to | ^k | < 1 for k = 0, 1, . . . ,  - 1 so that µ µ ^  µ > 0 as  (N )   by (B.4) and continuity, the second requirement is also fulfilled. To simplify notations, we have suppressed the index N in the quantities marked with a hat, which of course depend on the data (2.2) and hence also on N . ^   Next we show that also the second term in (B.3) tends to zero. Since c ^  is bounded, it thus remains to demonstrate that c   + c - c ^  (N )    (N ) - 


 0 as  (N )  .

This follows from the more general fact, needed for the proof of Corollary B.1, that ^  (N )  a  (N ) - 


 0 as  (N )  

(B.6)

for any a  R. To prove this, first note that ^ T - T


^  |c0 - c ^0 | + 2 c - c

,

-1 ^   0. Therefore  := T - T ^  T and hence T - T  < 1 for  :=  (N ) sufficiently large, and, provided c = 0, the standard perturbation estimate [22] yields

^  -   





1 (T ) 1 - 

^ T - T T 



+

^ c - c c 



,

(B.7)

and consequently, since T   c0 > 0, it follows from (B.5) that (B.6) tends to zero in the required manner. If c = 0,  = 0, and hence ^  - 


^ = 



-1 ^  T



^ c



-1 ^ = T



^ c - c

,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case c = 0. In fact, since µ ^ is bounded away from zero, by continuity, for each > 0, there is a N0 such that ^ -1 T  for   N0 .


1  c ^0

 -1

1 1 + | ^ k|  1 - | ^ c0 k| k=0

 -1

1 + |k | +  K3 + 1 - |k | k=0

Corollary B.1. If  (N ) is defined as in Theorem 5.1, then, for any a  R, ^  a W - W


0

almost surely as  :=  (N )  .

To prove Theorem 5.3, we first note that the Hankel operator H, defined by (4.1), has a nice representation in the space 2 of square-integrable functions. In fact, let 2 2 of functions with vanishing negative Fourier coefficients, + be the subspace in hence being analytic in the unit disc D. In this setting, H has the representation 2 2  2 H : + + given by

H

L

L

H

L H

H f = P  f, where P  is the orthogonal projection onto the orthogonal complement 2 2 , and where  is the  -function + in

H

L

L

L H
2

(B.8)
2 +

of

¯ + (z )-1 . (z ) = W- (z )W

(B.9)

¯ + (z ) are the analytic and coanalytic minimum-phase spectral facHere W- (z ) and W ¯ + (z ) = tors defined in Section 4. (See, e.g., [30, 31].) In the present scalar case, W -1 W- (z ). In fact, the phase function  is the transfer function of an all-pass filter transforming the white noise w- in (4.2) to the white noise w+ in (4.3) [30, p. 834]. ^ ¯ + be the stochastic measures such that Let dw ^- and dw
 

w- (t) = Then

-

eit dw ^-


and w ¯+ (t) =


-

^ eit dw ¯+

H+ = H- = and consequently H := E f  f dw ^- .
H- -  -

H

2 ^ ¯+ + dw

=
-

H

2 it ^- + (e )dw

L H
2

2 ^- + dw

|H + corresponds to H under the isomorphism defined by

^  (ei )| - |W (ei )|  0 Proof of Theorem 5.3. It follows from Theorem 5.2 that |W uniformly in  as   , and hence, by Lemma A.1, there are positive real numbers µ1 and µ2 such that ^  (ei )|  µ2 µ1  |W for all  and sufficiently large  . Therefore, since -1 ^ ^ - W ^  W ^  W ^ ^) W (W - W
,

(B.10)

30

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

(A.13) and (4.26) imply that ^ - W ^ W


 2µ2

 ^k , 1- ^k k=^ n+1



(B.11)

^2 , . . . ,  ^ are the singular values (5.1) determined for sufficiently large  , where  ^1 ,  from the covariance estimates (2.6). It is well-known (see, e.g., [56, p. 204]) that the singular value k of the Hankel operator H , defined by (B.8) equals the infimum of H - K over all operators 2 2 -1  2 K : + + of finite rank at most k . Recall that (z ) = W (z )/W (z ). ^ ^ ^ -1 The singular value  ^k of H ^ , where (z ) = W (z )/W (z ), is described analogously. Therefore, since

H

L H

^ H ^ - K  H ^ - H  + H - K   - 



+ H - K ,
.

^ -   + k . But, for k > n, k = 0, and hence  ^ - we have  ^k   ^k   Consequently, (B.11) yields ^ ^ - W W


^ -  M1  

,

(B.12)

-1 ^n where M1 := 2µ2 (1 -  ^ +1 ) . However,

^  (z -1 ) - W (z -1 )] , ^  (z ) - W (z ) - (z )[W ^ z ) - (z ) = W ^  (z -1 )-1 W ( ^  (z -1 ) so, since W


is uniformly bounded by (B.10), and  ^ - 




is constant,

^  M2 W - W

,

which together with (B.12) yields ^ ^ - W W


 M1 M 2  W - W 



^ + M1 M2  W  - W



for sufficiently large  . Consequently the theorem follows from Theorem 3.4 and Corollary B.1. Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A. Gombani, W. B. Gragg, G. Picci and T. S¨ oderstr¨ om for stimulating discussions and for providing us with appropriate references. We are also indebted to the anonymous referees for several useful suggestions. References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987. 2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489­502, 1974. 3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975. 4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational covariance extension problem. SIAM Journal on Control and Optimization, 37:211­229, 1999. 5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841­1857, 1995. 6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast filtering algorithms. SIAM Journal on Control and Optimization, 32:744­789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-G¨ ursoy. On the L consistency of L2 estimators. Systems & Control Letters, 12:71­76, 1989. 8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978. 9. A. Dahl´ en, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace identification methods may fail. Systems and Control Letters, 34:303­312, 1998. 10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced stochatic realizations. In Proc. 21st IEEE CDC, pages 1105­1112, 1983. 11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29:1097­1100, 1984. 12. J. Durbin. Efficient estimation of parameters in moving average models. Biometrika, 46:306­316, 1959. 13. J. Durbin. The fitting of time-series models. Rev. Inst. Int. Stat., pages 223­243, 1960. 14. P. Duren. Theory of Hp spaces. Academic Press, 1970. 15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear. 16. G. Freud. Orthogonale Polynome. Birkh¨ auser Verlag, 1969. 17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic Control, AC-32:358­361, 1987. 18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac., Speech and Signal Processing, ASSP-35:438­449, 1987. 19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961. 20. M. Gevers. Towards a joint design of identification and control. In J. Willems and H. Trentelman, editors, Essays on Control: Perspectives in the Theory and its Applications, 1993. 21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their l -error bounds. Int. J. Contr., 39:1115­1193, 1984. 22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989. 23. W. B. Gragg. Positive definite Toeplitz matrices, the Arnoldi process for isometric operators, and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in Linear Algebra, pages 16­32. Moscow U. P., 1982. 24. U. Grenander and G. Szeg¨ o. Toeplitz forms and their applications. Univ. California Press, 1958. 25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons, 1988. 26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples. Biometrika, pages 297­307, 1989. 27. W. Jones and E. Saff. Szeg¨ o polynomials and frequency analysis. In Approximation Theory, pages 341­352. Dekker Inc., 1992. 28. S. Y. Kung. A new identification method and model reduction algorithm via singular value decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705­714, 1978. 29. W. E. Larimore. System identification, reduced ordered filtering and modeling via canonical variate analysis. In Proc. of the American Control Conference, 1983. 30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes. SIAM J. Control and Optimization, 23:809­857, 1985. 31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic systems. J. of Math. Systems, Estimation and Control, 1:241­333, 1991. 32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension, and identification of stationary time series. Automatica, 32(5):709­733, 1996. 33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412­440, 1992. 34. L. Ljung and Z. Yuan. Asymptotic properties of black box identification of transfer functions. IEEE Trans. Automatic Control, AC-26:514­530, 1985. 35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis, Royal Instiute of Technology, 1998. 36. D. Q. Mayne and F. Firoozan. Linear identification of ARMA processes. Automatica, 18:461­466, 1982.

32

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

37. H. Mhaskar and E. Saff. The distribution of zeros of asymptotically extremal polynomials. J. Approx. Theory, 3:279­300, 1991. 38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66­73, 1978. 39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward, editors, Approximation Theory VI, 1989. 40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991. 41. P. Van Overschee. Subspace Identification, Theory - Implementation - Application. PhD thesis, Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De Moor. 42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identification problem. In Proc. 30th Conference on Decision and Control, Brighton, 1991. 43. P. Van Overschee and B. De Moor. Subspace Identification for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996. 44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994. ¨ 45. I. Schur. Uber Potenzreihen, die im Innern des Einheitskreises beschr¨ ankt sind. J. f¨ ur die Reine und Angewandte Mathematik, 147:205­232, 1917. 46. J. Sorelius, T. S¨ oderstr¨ om, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identification. In Proc. SYSID '97, 1997. 47. G. Szeg¨ o. Beitr¨ age zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift, 6:167­202, 1920. ¨ 48. G. Szeg¨ o. Uber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232­244, 1921. 49. G. Szeg¨ o. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications, 1939 (4th edition 1975). 50. B. Wahlberg. On the Identification and Approximation of Linear Systems. PhD thesis, Link¨ oping University, 1987. Link¨ oping Studies in Science and technology. Dissertations No. 163. 51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive approximations. Journal of Time Series Analysis, 10:283­299, 1989. 52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like identification techniques. IEEE Trans. Automatic Control, 37:900­912, 1992. 53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation. Systems and Control Letters, 14:307­317, 1990. 54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423­ 434, 1953. 55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938. 56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

1688

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Universal Regulators for Optimal Tracking in Discrete-Time Systems Affected by Harmonic Disturbances
Anders Lindquist, Fellow, IEEE, and Vladimir A. Yakubovich, Member, IEEE
Abstract-- The authors consider the problem of controlling a discrete-time linear system by output feedback so as to have a second output z t track an observed reference signal r t . First, as a preliminary, we consider the problem of asymptotic tracking, i.e., to design a regulator such that jz t 0 rt j ! 0. This problem has been studied intensely in the literature, mainly in the continuoustime case. It is known that only under very special conditions does there exist a linear regulator which achieves this design goal and which is universal in the sense that it works for all reference signals and does not depend on them. On the other hand, if rt is a harmonic signal with known frequencies but with unknown amplitudes and phases, there exist such regulators under mild conditions, provided the dimension of rt is no larger than the number of controls. This is true even if the plant itself is corrupted by an unobserved additive harmonic disturbance wt of the same type as rt , if the dimension of wt is no larger than the number of outputs available for feedback control. However, if the first dimensionality condition is not satisfied, asymptotic tracking is not possible, but a steady-state tracking error remains. Therefore, the authors turn to another approach to the tracking problem, which also allows for damping of other system and control variables, and this is our main result. The measure of performance is given by a natural quadratic cost function. The object is to design an optimal regulator which is universal in the sense that it does not depend on the unknown amplitudes and phases of rt and wt and is optimal for all choices of rt and wt . The authors prove that an optimal universal regulator exists in a wide class of stabilizing and possibly nonlinear regulators under natural technical conditions and that this regulator is in fact linear, provided that the second dimensionality condition above is satisfied. On the other hand, if it is not satisfied, the existence of an optimal universal regulator is not a generic property, so as a rule no optimal universal regulator exists. The authors provide complete solutions of all the problems described above. Index Terms-- Internal model principle, optimal tracking, optimal universal regulators, sinusoidal disturbance.

Fig. 1. Feedback configuration.

(1b) (1c) , two vector outputs and , with a state and an and two vector inputs, namely a control which we shall take to be unobserved disturbance harmonic with known frequencies but unknown amplitudes and phases. More precisely (2) where the frequencies (3) , , are known, but the complex vector amplitudes , , in which the phases have been absorbed, are either completely unknown or zero. Consequently, some frequencies and have been included for (3) may not be represented in notational purposes to be explained shortly. In this paper we consider the problem to control the system (1) by feedback from the output so as to have the output track an observed -dimensional real reference signal (4)

I. INTRODUCTION

C

ONSIDER a discrete-time linear control system (1a)

Manuscript received January 4, 1996; revised February 1, 1997 and April 6, 1998. Recommended by Associate Editor, S. Weiland. This work was supported in part by grants from NFR, INTAS, and NUTEK. A. Lindquist is with the Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden. V. A. Yakubovich is with the Department of Mathematics and Mechanics, St. Petersburg University, St. Petersburg 198904, Russia. Publisher Item Identifier S 0018-9286(99)07132-9.

which is harmonic with the known frequencies (3) but with , , , which are complex vector amplitudes either completely unknown or zero so that certain frequencies (3) may not occur in . The feedback configuration of this problem is described in the flow diagram as shown in Fig. 1. Many important engineering problems could be modeled in this way. Some examples are connected to industrial machines and helicopters [2], [9]­[12], [27], [28], control of aircraft in

0018­9286/99$10.00 © 1999 IEEE

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1689

the presence of wind shear [19], [23], [31], and control of the roll motion of a ship [14]. For notational convenience we use a common set of freand , forcing us to set certain complex quencies (3) for vector amplitudes equal to zero. To formalize this we introduce , of for which the index sets and , respectively, are nonzero and arbitrary. Then and Without loss of generality we assume that (5)

satisfies the weak stability condition as (11)

Accordingly, we define the class of disturbances and the and , class of reference signals consisting of all signals and respectively, obtained by letting vary arbitrarily subject to the constraint that the signals (5) are real. , and are constant We assume that , , , , real matrices of appropriate dimensions such that is stabilizable and is detectable. Without loss of generality we may also assume that and (6)

In fact, if the first condition is not satisfied, some components of could be eliminated. Moreover, if has linearly dependent columns, these could be combined without restriction. and . Clearly, (6) implies that Now, a possible criterion of performance for the tracking problem described above is given by (7) but, to allow for damping of internal system variables and the energy of control, we shall also consider a more general criterion of the type (8) where is a real quadratic form (9) with properties to be specified in Section V. [To ensure that the , we must of course introduce some infimum of is not condition on the quadratic form (9).] We note that the second functional (8) becomes a measure not only of the tracking accuracy but also of the forced oscillations in the closed-loop system. For the classes of admissible regulators to be defined next, these cost functions do not depend on initial conditions. , a regulator The object is to find, for suitable , (10) which is: sat1) stabilizing in the sense that any process isfying the closed-loop system equations (1), (10) also

2) optimal in the sense that the cost function (8) is minimized; 3) universal in the sense that it simultaneously solves the complete family of optimization problems corresponding to different values of the complex vector amplitudes and and thus does not depend on these amplitudes. Such a regulator will be referred to as an optimal universal regulator (OUR), and the class of regulators (10) satisfying conditions 1) and 2) will be denoted . The stability condition (11) may at first sight seem somewhat unnatural, but, as we shall see in Section VI, it is the natural mathematical condition for which statements of necessity defining the largest class and sufficiency can be made. Removing the last term of (8) related to tracking we obtain some special cases of this problem which were studied in [21] and in [22] for the cases of complete and incomplete state information, respectively. In this paper we show that, under suitable technical con, the problem stated above has ditions and provided a solution in , and this solution happens to be a linear stabilizing regulator of type (12) is the backward shift and , where , and are real matrix polynomials, of dimensions , , and , respectively, with the property that and and are proper rational functions so that the regulator is nonanticipatory in the sense does not depend on future values of and , in that the subclass of harmony with (10). We shall denote by such linear regulators. Existence of an OUR in the subclass itself can be established under somewhat milder technical is important. conditions. The dimensionality condition As in [22], it can be shown that if it fails, then the existence of an optimal universal regulator becomes a nongeneric property. It means that no optimal universal regulator exists from a . practical point of view if The cost function (7) would of course be minimized if we could control (1a) so that as (13) In fact, it would be zero. Therefore, asymptotic tracking appears as a special case in our analysis. This problem has been studied intensely in the literature, at least in the continuoustime case; see, e.g., [1], [4]­[8], [13], [16], and the references therein. The connection to this earlier work, developed in continuous time, is made evident by noting that the disturbance and reference signals (5) can be modeled as the output of a critically stable system

with

having all its eigenvalues on the unit circle.

1690

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Therefore, we begin by developing our optimization procedure in this well-known setting of asymptotic tracking, thereby obtaining alternative formulations in the discrete-time case. Using a very short and simple proof, we are able to give a complete solution to the problem of finding all universal tracking regulators, i.e., all regulators which achieve asymptotic tracking (13) for all values of the complex vector and and which do not amplitudes depend on these amplitudes. This will be done in Section IV. As a preliminary for this, and to set up notations, in Section III ), and we we first consider an undisturbed system ( characterize all regulators (12) achieving the design objective (13) for all reference signals , not only harmonic ones, and all initial conditions; we shall refer to this property as Tuniversal. The solution of this problem is certainly known, but we include it for conceptual reasons. , i.e., the dimension of is larger than However, if the number of outputs available for feedback, no universal tracking regulator exists, so a nonzero tracking error remains. To damp this error we turn to our main problem, namely to characterize all optimal universal regulators, as defined above. Also, we may want to use a criterion (8) even if asymptotic tracing is possible, if it is desirable to damp the control energy and/or some particular internal system variables. This is the topic of Section V, where optimality in the linear class is studied. In Section VI we show that these linear universal regulators are optimal also in the wider class of nonlinear regulators satisfying (11), provided slightly stronger technical conditions are satisfied. The complete solution is given. We note that a similar but different optimization problem, over a finite horizon, is considered in [26]. Obviously, there is no a priori guarantee that a regulator which minimizes (8) will also satisfy other design specifications, and hence we look for complete solutions with many free parameters which then can be tuned by loop shaping. In fact, all our results are based on a parameterization derived in Section II, which is akin to that of Youla and Ku cera and which generalizes some parameterizations previously presented in [21] and [22]. Finally, in Section VII, we give some simple numerical examples. II. LINEAR STABILIZING AND REALIZABLE REGULATORS In order to design universal regulators we need a parameterization of all linear regulators (14) which stabilize the control system (1) and which are realizable in a sense to be defined shortly. As before, is the backward , and , , and are real matrix shift , , and , respectively. polynomials of dimensions Let us consider a bit closer the meaning of (14) being , , stabilizing. To this end, note that the transfer functions from to , , and , respectively, in the closed-loop system (1), (14) satisfy (15a)

(15b) (15c) so, in particular, (16) where is the matrix polynomial (17) Similarly, the transfer functions respectively, are given by , from to and ,

(18) which together with (16) yields (19) We shall say that the regulator (14) is stabilizing if the matrix is stable, i.e., for . polynomial Next we consider the condition that the regulator be realizable. Clearly (14) must be nonanticipatory in the sense that does not depend on future values of and . To ensure this, we must assume that and are proper (20)

. requiring in particular that Let us investigate what properties must have for (20) to be satisfied. To this end, let us introduce the rational transfer functions

(21) from the control signal to the outputs Then it is easy to see that and , respectively.

(22) and that

(23) Writing (22) in the alternative form

we see that (20) implies that is strictly proper and is proper. In fact, is strictly proper, making as well as its inverse proper. Then, it follows from and are strictly proper also. Consequently (23) that where is finite (24)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1691

so that and depend on for only and on for only. We shall say that the regulator (14) is realizable if condition (24) is satisfied. At the end of this section we shall demonstrate that any stabilizing and realizable regulator satisfies (20) so that the nonanticipatory property is implied (Corollary 2.3). We say that two regulators

is stabilizing and realizable, and for this regulator (30) and (31) is given by (17). Conversely, any stabilizing and where realizable regulator (28) is equivalent to one constructed in this way. Before turning to the proof of this parameterization, let us is a briefly explain the nature of relation (31). Although for the regulator defined via (29), this is in factor in general not the case for an arbitrary regulator belonging the same equivalence class. In fact, while the closed-loop transfer and the regulator transfer functions and function are invariant under the equivalence (25), is not. Taking the Schur complement, it immediately follows from (17) that (32) is given by (21). Since, in general, the second factor where in is not a polynomial, is of course not a factor in general. Nevertheless, it will turn out to be useful to represent each equivalence class by a regulator that has this property. Proof of Theorem 2.1: In view of (29), we have (33) and consequently (30) follows from (22) and (31) follows from is a stable matrix poly(32). By construction, therefore, nomial, establishing that the regulator is stabilizing. Moreover, is strictly proper and is proper, i.e., in view of (27), and is finite. It then follows from (23) and are strictly proper, and hence the regulator that is realizable. , , To prove the converse statement, suppose that is an arbitrary stabilizing and realizable regulator. Then (32) may be written

and

are equivalent if there are stable and such that

matrix polynomials (25)

Hence we allow the systems matrices , , and to have stable common factors, as coprimeness is not required. Clearly, , , , and as can be seen from (22) and (23), are invariant under this equivalence and so are the regulator transfer functions (20). is a stable matrix, i.e., From now on, we assume that for all . Since is stabilizable is detectable, this is no restriction. In fact, it is and well-known that the system (1) can be replaced by a similar system having a stable -matrix but, in general, a larger dimension. (See any standard text, such as [1] and [18].) Only under special conditions [15], including the case of complete state observation, is it possible to do this by constant feedback, but the system can always be stabilized by a dynamic observer. Then, extending the state space by including this observer, a system with stable -matrix is obtained. For these reasons we shall from now on, without loss of generality, assume that in (1) is a stable matrix. The following theorem, generalizing a similar result in [22], provides a parameterization akin to the well-known Youla­Ku cera parameterization. (We note that if is not stable, also the latter parameterization requires an observer-based prestabilization, increasing the dimension of the regulator; see, e.g., [32, p. 226].) be a stable matrix with Theorem 2.1: Let being its characteristic polynomial, and let and be the matrix polynomials (26) be an arbitrary stable scalar polynomial Moreover, let and be arbitrary matrix polynomials of and let and , respectively, such that dimensions (27) Then the regulator (28) with

where

is the

matrix polynomial (34)

which is stable and full rank, since stable and nontrivial. It follows from (22) that

is (35)

and are the closed-loop transfer functions where , , . Therefore, setting corresponding to the regulator

and

(29)

where , (35) shows that

is the adjoint matrix polynomial of and are given by (30). Since ,

1692

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

, is a realizable regulator, it follows from (24) that the and degree conditions (27) hold. Consequently, defining via (29), it follows from the first part of the theorem that , , is a stabilizing and realizable regulator with the and as , , same closed-loop transfer functions . It remains to show that , , and , , are equivalent. To this end, note that

Also it follows from (34) that

Consequently

i.e.,

and

are equivalent as required.

If so that , the representation of stabilizing regulators can be simplified considerably, since and can be chosen so that cancellations occur. Since this formulation has a different form and, moreover, will be used later, we state it as a corollary. Note that, in view of the converse statement, this corollary is strictly speaking not a special case of Theorem 2.1. It is in fact a generalization of [21, Lemma 4.3], but the proof here is new. Corollary 2.2: Let be a stable matrix, and suppose that . Let be an arbitrary real scalar stable polynomial, and be arbitrary real matrix polynomials, and let and , respectively, such that of dimensions (36) Then the regulator (37) with (38) is stabilizing and realizable, and, for this regulator (39) satisfies (31). Conversely, any stabilizing and and realizable regulator (37) is equivalent to one constructed in this way. Proof: Let the polynomials and be chosen as in the and statement of the corollary, and take to be the corresponding polynomials and in Theorem 2.1. Then, since , the degree conditions (27) are satisfied for and . Moreover, the corresponding regulator polynomials matrices and , become (29), which we denote and , where and are given by (38). Then, , the regulator , , is stabilizing and setting realizable by Theorem 2.1. Thanks to cancellation, therefore, is a stabilizing and realizable regulator for the problem of Corollary 2.2, as claimed.

Conversely, by Theorem 2.1, any stabilizing and realizable regulator (37) is equivalent to some regulator of the type described in Theorem 2.1, where we set everywhere. It remains to show that is also a regulator of the type described in the corollary. To this end, . This implies that , define and hence the equations of Theorem 2.1 become those of the replaced by . Hence is also a corollary with regulator in the sense of the corollary. In the beginning of this section we demonstrated that the realizability condition (24) is a consequence of nonanticipatory condition (20). Next we show that the converse is also true, has full rank as assumed in (6). provided . Then, for any Corollary 2.3: Suppose that stabilizing regulator (28), the realizability condition (24) and the nonanticipatory condition (20) are equivalent. Proof: The proof is immediate in the special case . In fact, for a regulator (37) with and given by (38), condition (20) is a direct consequence of the degree condition (36). For any other stabilizing regulator (37), it follows from the definition of equivalence. The general case follows from the fact that (28) is a subclass of (37). In fact, writing (28) as

it follows from what has already been proved that is proper. Since has full rank, this implies that is proper follows directly. proper. That III. -UNIVERSAL REGULATORS

is

As a preliminary for the analysis in Sections IV and V, in this section we consider the problem of controlling the undisturbed system (40a) (40b) (40c) so that it tracks a given by feedback from the output in the sense that reference signal as (41)

As explained in Section II it is no restriction to assume that is stable if it is assumed that is stabilizable and is detectable. The solution of this problem is simple and certainly known, but we include it for completeness and for conceptual reasons. More precisely, we want to find a stabilizing and realizable regulator of the form (42) which is universal for the asymptotic tracking problem in the sense that (41) holds for all solutions of (40), (42), and all reference signals . More specifically we shall refer to this property as T-universal. Clearly, for (42) to be stabilizing and realizable, the matrix , , and must satisfy the specifipolynomials cations of Theorem 2.1. It remains to investigate under what

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1693

conditions the tracking criterion (41) is satisfied and under what conditions this regulator is T-universal. We begin by deriving a necessary condition for Tuniversality. Consider a reference signal of the type (43) and are fixed but arbitrary. Then the where closed-loop system (40), (42) has solutions

delays between and . Indeed, the condition (48) for Tuniversality imposes some rather stringent conditions on the is and is , system (40). In particular, since , and must have full rank. (48) implies that Theorem 3.2: Suppose that is stable. Then there exists a T-universal regulator for the tracking problem if and only if matrix function with no there is a proper rational which satisfies the equation poles in the region (49)

(44) with (45) , where are defined by (21). Moreover, , and and (46) But the tracking condition (41) requires that as and, since is arbitrary, this implies that follows from (44) and (46) that . Therefore, it (47) Now, in order for the regulator (42) be T-universal, (47) must hold for all , that is, for all and . Consequently, we must have (48) on the unit circle and, by analytic continuation, in the rest of the complex plane. Lemma 3.1: A stabilizing and realizable regulator (42) is T-universal if and only if the identity (48) holds. Proof: We have already proved that (48) is a necessary condition for (42) to be T-universal, so it remains to prove that it is also sufficient. To this end, first assume that there are such that for all . Then positive numbers , has a -transform

which, in particular, implies that In this case, let be a stable scalar polynomial such that

. (50)

be a matrix is a matrix polynomial, and let polynomial satisfying the first degree constraint (27). Then, the and given by (29), is a T-universal regulator (28), with regulator for the tracking problem, and any other T-universal regulator is equivalent to one obtained in this way. Proof: First, suppose that there exists a T-universal regulator of the form (42). Then, according to Lemma 3.1, there to (49) with the prescribed properties, exists a solution . In fact, in view of (22), (32) and the fact that namely is stable, it follows that has no poles in the region . Moreover, since the regulator is realizable, is proper. which is proper Next, suppose that (49) has a solution , and let , , and be with no poles in the region defined as in the theorem. [Note that in order to satisfy the first of degree conditions (27) we may need to choose and which are not coprime.] Then, by Theorem 2.1, the regulator given by (29) is stabilizing and realizable and (28) with (51) . Consequently, it follows from i.e., in view of (50), (49) and Lemma 3.1 that the regulator is T-universal. It remains to prove the last statement of the theorem. To this end, suppose that the regulator (52) is T-universal. Then, in particular, it is stabilizing and realizable, and thus, by Theorem 2.1, there are some , , and with the properties specified in Theorem 2.1 such that the given by (29) is equivalent to (52). regulator (28) with is invariant under this equivalence. Therefore, since Now, (48) holds for the regulator (52) by Lemma 3.1, (48) also holds for (28). However, by Theorem 2.1, (51) holds, and hence there , satisfying (49) and (50). is an , namely In general, a solution to (49) cannot be expected to be , only one solution is possible, namely unique, but if

which converges for . It follows from (45) and is the transfer function from to , (46) that with a -transform and hence (40), (42) has a solution . But, if (48) holds, then and hence for all . Because of stability any other solution tends asymptotically to this solution, and therefore (41) holds. If increases so fast that it does not have a -transform, set for and for , and be the corresponding -solution. Then it is easy to see let for . Since is arbitrary, the that conclusion follows. As a corollary we see that must be full rank, or else (48) will be violated. This implies that there are no

and this would require that is a stable, proper rational must be minimum phase with function, implying that must be no zeros at infinity. In particular, nonsingular.

1694

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Corollary 3.3: Suppose that is stable and the transfer is square, i.e., . function Then there is a T-universal regulator for the tracking problem is proper with no poles in the region if and only if . In this case, let be a stable scalar polynomial is a matrix polynomial and is such that matrix polynomial satisfying the degree requirement a and are defined by (29) and by (27). Then, if (53) the regulator (28) is a T-universal regulator, and any other T-universal regulator is equivalent to one obtained in this way. A T-universal regulator exists only under rather special conditions. However, if we restrict our attention to harmonic reference signals (4), these conditions can be considerably relaxed and we may also allow for external harmonic disturbances. This is the topic of the next section. IV. UNIVERSAL TRACKING REGULATORS IN HARMONICALLY DISTURBED SYSTEMS We now return to the situation described in Section I, where the control system takes the form (1) with a harmonic disturbance (2) and where there is a harmonic reference signal to be empty, for (4). Although we may allow the index set . tracking we must take The first question to be answered is when it is possible to find a regulator (12) in such that as (54)

respectively, satisfying the degree requirements (27) and the interpolation conditions for for (59a) (59b)

and are given by (29), the regulator (28) is a Then, if universal tracking regulator, and any other universal tracking regulator (28) is equivalent to one obtained in this way. Proof: Whenever a linear stabilizing regulator is applied tends exponentially to the to system (1), the process harmonic steady-state solution (60) where (61a) (61b) , , , and being the closed-loop transfer functions , defined in Section II. In fact, for any regulator in , defined by (17), is a stable matrix polynomial. In the same tends exponentially to way, in view of (1c),

(62) Now, the basic idea is that the tracking condition (54) is achieved precisely when the cost function (7) is zero. It is easy to see that (63) To see this, observe that if sequences and are two harmonic vector

which is universal in the sense that (54) holds for all values and and does not depend on these of vector amplitudes. We shall refer to such a regulator as a universal tracking regulator. For convenience, in the sequel we use the notation (55) is stable, and let Theorem 4.1: Suppose that the matrix and be the matrix polynomials defined by (26). be the matrix function defined by Moreover, let the matrix polynomial (21) and (56) Then, for a universal tracking regulator to exist in necessary that the rank condition for all , it is (57)

and with distinct as in (3), and appropriate dimensions, then is an arbitrary matrix of

holds, and it is sufficient that both rank conditions (57) and for all (58) Moreover, in view of (61b) and (62)

(64)

, and hold. In particular, (57) requires that . More precisely, let be (58) that and an arbitrary stable scalar real polynomial, and let be matrix polynomials, of dimensions and ,

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1695

and consequently (63) equals zero for all values of and if and only if for for (65a) (65b)

Remark 4.3--Internal Model Principle: The situation most often studied in the literature is when , i.e., , , and , and when the regulator (28) takes the form

Theorem 2.1 states that the regulator (28) is stabilizing if and are defined by (29) for some stable scalar real and some real matrix polynomials polynomial and satisfying (27) and that any other stabilizing and realizable regulator (28) is equivalent to one obtained in this way. Moreover (66) which inserted into (65) yields precisely (59). If the rank conditions (57) and (58) hold, the interpolation conditions (59) have a solution, and the general solution is

obtained by setting . We assume that the rank . conditions (57) and (58) are satisfied so that For robustness it is desirable to include a model of the disturbance dynamics in the regulator. This is the internal model principle. Following [3], we replace the matrix fracby the (reachable) matrix fraction tion representation so that . The harmonic representation dynamics is then included in the regulator dynamics by setting , where and is a stable matrix polynomial. Then, by (29)

which, in view of the fact that for for where, for , and are arbitrary matrices and . Here such that the degree of the stable polynomial is chosen sufficiently high to satisfy the degree constraints (67). On the other hand, the rank condition (57) is also necessary for the existence of a is stable, (65b) universal tracking regulator. In fact, since for some . cannot hold if Remark 4.2: The two rank conditions (57) and (58) in Theorem 4.1, which of course can be stated in terms of zeros of certain transfer functions, have different status. If (57) is violated, the interpolation condition (59b) cannot hold, so there could be no universal tracking regulator. On the other hand, if (57) holds but (58) does not, interpolation condition (59a) could still be valid, as the rank of the right member could be less than . However, this is a nongeneric situation, and hence it cannot be expected to occur in practice. In fact, if and , the following equation must hold:

, yields

where we have assumed that has no zeros in the points . (Otherwise we include a simple feedback loop to clearly satisfy the interpolation move the zeros.) These , and conditions (59). In fact, since , by (29), these can be written for for Consequently, we see that the internal-model-principle regulators form a subclass of the ones considered above. , which The rank condition (58) becomes void if is equivalent to the case with complete state information, i.e., . Then the formulas for the regulator the case when also simplify considerably. so that . Theorem 4.4: Suppose that Moreover, suppose that is stable and that condition (6) holds. Then, there exists a universal tracking regulator (37) in if and only if the rank condition (57) holds. In fact, let be a stable scalar real polynomial, and let and be matrix polynomials satisfying the degree constraints (36) and the interpolation conditions for for (67a) (67b)

which will occur only on a lower-dimensional algebraic set in the parameter space. Theorem 4.1 provides a complete solution of a problem studied in various degrees of generality in [4]­[8], [13], [16] and of course is consistent with the solutions given there, although given in a different form and in continuous time. , rank condition (58) becomes void and only (57), a If considerably weaker version of condition (49) in Section III, remains. Hence, for universal tracking regulators to exist the is necessary, and if there are external condition , in practice, we must also have . disturbances Consequently, as also noted in [4], [7], [8], [13], and [16], asymptotic tracking is only possible under certain specific conditions.

and are given by (38), the regulator (37) is a Then, if universal tracking regulator, and any other universal tracking regulator (37) is equivalent to one obtained in this way.

1696

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof: The proof follows the same lines as that of Theorem 4.1, except that (39) from Corollary 2.2 is used in , exists and (67a) lieu of (66). Since can be solved. When , there are no universal tracking regulators, and in order to damp the steady-state tracking error we shall therefore next turn to an optimization procedure. This is the topic of the next section. V. LINEAR QUADRATIC OPTIMIZATION FOR TRACKING AND DAMPING We now return to the optimization problem stated in Section I. In this section we consider only linear regulators. Later, in Section VI, we demonstrate that under slightly stronger technical conditions the optimal universal regulators presented here are actually optimal in the much larger class , which includes nonlinear regulators. Let us recall that the problem under consideration is to control the disturbed system (1) by feedback from the output so as to minimize the cost function (68) is the quadratic form defined by (9). Hence, where we may not only want to damp the tracking error, but also some internal systems variables. As before, both the disturand the reference signal are harmonic and given by bance (5), where only the frequencies are known. The optimization is performed over the class of stabilizing and realizable linear regulators (12). The problem under consideration is: 1) to find the conditions under which there are optimal regulators which are universal in the sense that they are optimal for all choices of the amplitudes of (5) and independent of these and 2) to characterize the class of all such universal optimal regulators. To address this problem, let us first take a closer look at the cost function (68). A straightforward reformulation taking (1c) into consideration yields

for all

,

satisfying (73)

such that . It can be shown [21] for all that if this condition fails in a strong way, i.e., there are , , and , , such that , then there such that . In is an external disturbance this section, however, we shall only need the weak frequency , , , domain condition that (72) and (73) hold for , defined as in (55). Both of these conditions are invariant under the action of the feedback group

where is a nonsingular matrix and is an arbitrary matrix of appropriate dimensions. Moreover, since has no eigenvalues on the unit circle, the inverse (74) exists for all on the unit circle, and hence where is the Hermitian that matrix function so

(75) In this notation the strong frequency domain condition may be written for all and the weak one as for (77) on the unit circle (76)

We now state the main result of this section. It will be strengthened in Section VI, where we show that under mild is technical conditions the optimal universal regulator in also optimal in the wider class . , , and be the matrix Theorem 5.1: Let polynomials defined by (26) and (56). Suppose that the matrix is stable and that the weak frequency domain condition (77) holds, and suppose that for all (78)

(69) where is the real quadratic form (70) with the real matrices , , and given by

(71) The quadratic form (70) need not be nonnegative definite but must of course satisfy some condition ensuring that . As we shall see, a sufficient condition for this is the strong frequency domain condition, i.e., that there is a such that (72)

. Then, i.e., in particular that which is universal in there exists an optimal regulator in and the sense that it is optimal for all values of and does not depend on these vector amplitudes. be an arbitrary stable scalar real More precisely, let and be matrix polynomials of polynomial, and let and , respectively, satisfying the degree dimensions requirements (27) and the interpolation conditions for for with and given by (80) (79a) (79b)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1697

where . Then the regulator (28) is an and universal regulator, which is optimal in , provided are given by (29) and any other universal regulator (28), which is optimal in , is equivalent to one obtained in this way. is nonsingular for Since, by assumption, , (79a) has the solution

(60) of . In fact, we have the following lemma. The proof follows from a simple completion-of-squares argument and is deferred to Appendix A. be any solution to the closed-loop Lemma 5.5: Let system (1), (12), where (12) is a stabilizing and realizable regulator, and suppose that the weak frequency domain condition (77) holds. Then the cost function (68) exists as a usual limit, and it takes the value (82)

(81) , and these are precisely all solutions of (79a). for and Clearly, there are always matrix polynomials satisfying (81), (79b) and the degree constraints (27), provided is chosen the degree of the stable scalar polynomial sufficiently large. , there exist optimal regulators, but, Remark 5.2: If as explained in Remark 4.2, universality is not a generic property; therefore, for all practical purposes, there are no . optimal universal regulators if Remark 5.3: Before proceeding to the proof of Theorem 5.1, let us make certain that it is consistent with the results of Section IV. To this end, let us consider a cost function (7), . Then i.e., suppose that

where, for (83) with and given by (80) and by (84)

where where the matrix function is given by (21). If , the weak frequency domain condition cannot hold, so Theorem 5.1 does not apply. Instead, Theorem 4.1 should , the weak frequency domain condition is be used. If a consequence of condition (57), and it is easy to check that the optimal cost will be zero, as required by Theorem 4.1. Moreover, interpolation conditions (59) and (79) are identical. , no universal tracking regulator exists by Finally, if Theorem 4.1, and the optimal cost will be nonzero in general. Remark 5.4--Generalized Internal Model Principle: As in , so that Remark 4.3, let us consider the case when , , , and , and in the regulator (28). For simplicity, also assume that . and and , the interpolation If conditions (79) can be written

(85) , In the expression (82) for the cost function , only , , depend on the regulator to be chosen. They are defined by (61b), i.e., (86) of external disturbances Recall that we consider the class for and for with arbitrary and the class of reference signals with for and for . Consequently, if we could find a stabilizing and realizsatisfy the able regulator (12) such that optimality conditions (87)

for , as can be seen from (29), (80), and , , and . All of these the fact that interpolation conditions are satisfied if the second set is, and in this case (29) implies that

which, in view of (86), is the same as (88) then this regulator would be optimal. If, in addition, this regu, lator does not depend on the amplitudes and , and the conditions (88) hold for all and , i.e., all disturbances in and all reference signals in , then this optimal regulator is also universal. This condition holds if and only if for for (89a) (89b)

which could be interpreted as a generalized internal model principle for the optimization problem. The basic idea behind the proof of Theorem 5.1 is, as for Theorem 4.1, that whenever a linear stabilizing regulator is tends exponenapplied to the system (1), the process tially to the harmonic steady-state solution (60). Therefore, the cost function (68) depends only on the harmonic component

1698

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof of Theorem 5.1: Theorem 2.1 states that the reguand are defined by (29) for lator (28) is stabilizing if and some real matrix some stable scalar real polynomial satisfying (27), and that any other stabilizing polynomial and realizable regulator (28) is equivalent to one obtained in this way. Moreover (90) We have demonstrated above that (89) is a necessary condition for the regulator (28) to be an optimal universal regulator, and inserting (90) into (89) yields precisely (79). Clearly, as we have already discussed, there are always matrix polynomials and satisfying these conditions and the degree constraints (27), provided the degree of the stable scalar is chosen sufficiently large, and provided polynomial condition (78) is satisfied. It remains to prove the converse statement. For any optimal , the value of the cost function universal regulator , defined by (84). It follows from (82) and the (68) equals , for , that (87) holds for fact that , and . Therefore, (89) follows from all is equivalent to (88). By Theorem 2.1, the regulator given by (29) for some satisfying the (28) with requirements of Theorem 5.1. This regulator is also optimal since equivalent regulators have the same cost . It is also does not depend on universal because and . Corollary 5.6: The optimal value of the cost function (68) , defined by (82) and (83). in the class is Note that, although an optimal universal regulator will not and , the cost function (84) depend on will. In the special case of complete state information, i.e., , condition (78) is always satisfied. In view of Corollary 2.2, Theorem 5.1 can be considerably simplified in this case, so we state it separately. The proof is the same as for Theorem 5.1, except that we now use the equations of Corollary 2.2. so that . Theorem 5.7: Suppose that Moreover, suppose that is stable and that condition (6) holds. Then, if the weak frequency domain condition (77) holds, there exists a universal regulator (37), which is optimal in . In fact, let be a stable scalar real polynomial, and let and be matrix polynomials satisfying the degree constraints (36) and the interpolation conditions for for (91a) (91b)

Since, by assumption, is a nonsingular matrix of , (91a) has the solution dimension

(92) . There are always matrix polynomials and satisfying (92), (91b), and the degree constraints (36), is provided the degree of the stable scalar polynomial chosen sufficiently large. for VI. OPTIMALITY IN THE CLASS OF NONLINEAR REGULATORS In this section we show that the universal optimal linear regulators described in Theorems 5.1 and 5.7 are actually optimal in a wide class of nonlinear regulators. We now define this class. of Given the control system (1), consider the class nonlinear regulators (93) of which is stabilizing in the sense that any solution the closed-loop system consisting of (1) and (93) satisfies the condition as (94)

This stability condition is quite weak but will suffice for our purposes. Of course, a weaker condition has the advantage of allowing for a larger class of controls. We consider the same problem as in Section V, except that . we now optimize over all regulators in . Clearly, The only price we have to pay for this generalization is that the weak frequency domain condition needs to be replaced by the strong one. be stable, and suppose that the rank Theorem 6.1: Let condition (78) holds. Then, if the strong frequency domain condition (76) holds, the linear optimal universal regulators of Theorem 5.1 are optimal in the class . It turns out that Theorem 6.1 is a simple consequence of the corresponding result for complete state information. In fact, the class of stabilizing and realizable regulators with is a subclass of the class of stabilizing and realizable regulators

and are where and are defined as in (80). Then, if given by (38), the regulator (37) is a universal regulator, which is optimal in . Conversely, any other universal regulator (37), which is optimal in , is equivalent to one obtained in this way. Finally, the optimal value of the cost function (68) is given by (84).

in that only a special structure of is required. But, as seen in Section V, an optimal universal regulator in the former class is optimal also in the latter, since the same optimal value is achieved (Corollary 5.6 and Theorem 5.7). (The only difference between the cases of complete and incomplete state information is that a higher degree regulator may be required in the latter case to achieve the optimum.) Consequently, if we can prove the following theorem, we have also proved Theorem 6.1.

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1699

Theorem 6.2: Let be stable, and suppose that and that . Then, if the strong frequency domain condition (76) holds, the linear optimal universal regulators of Theorem 5.7 are optimal in the class . In order to prove this theorem we consider an optimization problem which unlike that in Section V does not require that a linear regulator has been applied. More precisely, let us first consider the problem of finding a process which minimizes the cost function (8), subject to the constraints (94) and (95) and are arbitrary bounded and where now complex-valued vector sequences. It is well known (see, e.g., [20], [21], [24], [25], and [29]) that if the strong frequency domain condition (76) holds and is stabilizable, then the algebraic Riccati equation

The optimal value of the cost function is (103) where

(104) exists, any optimal process If the limit is produced in this way. Note that the control (101) cannot in general be used in and . Even practice, since it depends on future values of in the harmonic case when this dependence can be resolved, this control law has serious disadvantages [21, Sec. III]. It is developed here as an instrument of proof. Next, let us return to our original problem and take and to be harmonic, given by (5). Then a simple calculation, using (99) and (100), yields the representation with where (105)

(96) has a unique symmetric solution matrix where (97) stable in the sense that all eigenvalues of lie strictly inside the unit circle. We shall refer to this solution as the stabilizing solution of (96). For this solution we also have that (98) is positive definite.1 Then we have the following result, which should be compared to [21, Th. 2.3], the proof of which we defer to Appendix B. be stabilizable and suppose that Lemma 6.3: Let the strong frequency domain condition (76) holds so that (96) has a stabilizing solution . Moreover, let (99) where (100) Then the problem to minimize the cost function (8) subject to constraints (94) and (95) is solved by a process such that (101) where is given by (97) and such that is any vector sequence is optimal in . Therefore, the optimal linear regulators of Theorem 2.1 must be optimal also in . of the closedSince (108) is stabilizing, the solution loop system (107), (108) tends exponentially to a harmonic solution We want to find a stabilizing and realizable regulator (108) so that the closed-loop system (106)­(108) has a solution satisfying (101) for some with the property (102). Then, by Lemma 6.3, the regulator (106), (108), i.e., (109) which renders the feedback

We are now in a position to prove Theorem 6.1. Proof of Theorem 6.1: Clearly, for any regulator in , (103) is a lower bound for the cost . Therefore, if we can demonstrate that there is a regulator in which achieves the same value (103) of the cost , this regulator must be optimal also in , and so must all regulators which are optimal in . so that To this end, let us introduce a new control (106) transforming the system (1a) to (107)

(102)
1 Note that there is a misprint in [21, p. 788]: In Theorem 2.1, replace "statements hold" for "statements are equivalent."

1700

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

which of course yields the same value to so that if we can choose for

as

. Now, (110)

The matrix polynomials (26) are and Let us first take a T-universal regulator and consider the problem to find (112) tends asymptotically to . By Corollary 3.3, a so that T-universal regulator exists if and only if and where stable (113)

, then has the property (102) and hence and (106) becomes (101) as required. such that (110) holds, we To show that there are first apply Corollary 2.2 to the system (107), where takes and that of . In fact, by Corollary 2.2, the place of there is a stable scalar polynomial and matrix polynomials such that and so that are given by and and

In fact, . In this case, (112) is a Tuniversal regulator if and only if However, tends exponentially to the harmonic solution Since therefore . (114) and such that is stable and for some polynomials or is equivalent to one obtained in this . Of course, way. This corresponds to the choice asymptotic tracking is achieved for all choices of reference signal . If, instead, we consider a reference signal (115) is full rank, in view of the discussion in Section V can be chosen to satisfy these interpolation conditions. VII. SOME SIMPLE NUMERICAL EXAMPLES To illustrate the results of this paper, let us consider the system (111) is the control, where characteristic polynomial and are outputs, and the Since where the frequencies , are given, but the amplitudes , and the phases , are unknown, the class of regulators (112) which achieve asymptotic tracking is much larger, and condition (113) need not be satisfied but can be exchanged for for (116)

and is given by (105), the optimality condition (110) will and if be satisfied for all

In fact, by Theorem 4.1, in this case we may choose any stabilizing regulator (117) provided is stable and the degree constraint (27) and the interpolation conditions for

is stable with

. Defining the state

the plant equations (111) can be written in state-space form (1), where

so that

is the characteristic polynomial of

, and

are satisfied. The same regulator is obtained by applying Theorem 5.1, now observing that (116) is the weak frequency domain condition; see Remark 5.3. This allows for more tuning parameters to satisfy other design specifications. Of course, if condition (113) is fulfilled, the T-universal regulator can still be used. , , and As a numerical example, suppose that , and let and . Then condition (113) is satisfied, so a T-universal regulator exists. Such a regulator and in is obtained by, for example, setting and and the initial conditions are (114). If , this yields the error depicted in Fig. 2. The

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1701

Fig. 2.

Fig. 3.

dashed line in the same figure is the tracking error obtained . by setting , while and remain the same. Next, let us take Then becomes unstable, so a T-universal regulator fails to exist. Although condition (113) fails, we could still obtain asymptotic tracking by using a universal tracking regulator, constructed as in Theorem 4.1, provided condition (116) holds, and we shall present a simulation for this case in the end of the section. We now add an harmonic disturbance (118) and , in the system (111), where , are given, but , are unknown. Suppose we want to determine an optimal universal regulator for the cost function (119)

straightforward calculation yields

for any on the unit circle. In order to construct an optimal universal regulator we need to choose a stable polynomial

of degree at least five. The parameters , , , , , as well as will be available for tuning in order to improve the overall design. Then, defining the real numbers , , , , , , , via for for it is easily seen that the polynomials

Since the matrices

,

, and

in (71) become will satisfy the interpolation conditions (79a) if and only if its coefficients satisfy the linear system of equations

a simple calculation yields

for (75), and therefore the strong frequency domain condition , so any optimal universal (76) is always satisfied if of possibly regulator (112) is optimal in the larger class , the nonlinear regulators described in Section VI. If strong frequency domain condition will fail if and only if the has a root on the unit circle, while the weak polynomial frequency condition (77) will still hold provided we avoid choosing any of the frequencies in (115) and (118) so that , , , or is such a root. Next, let us consider the interpolation condition (79). defined by (56) is identically one, and a Clearly,

Consequently, by Theorem 5.1, (117) is an optimal universal and are determined in this way. regulator if , , and For an example, take as before . Moreover, we choose a disturbance (118) with and , while the harmonic frequencies , reference signal (115) has the same frequencies as in the first simulation. In Fig. 3 we illustrate the tracking error of the optimal universal regulator corresponding to a polynomial with roots 0.3 0.3 , 0.3 0.2 , 0.5, and . The amplitudes in (115) and (118) have been taken

1702

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Fig. 4.

to be , , and , and the initial . As before, the dashed line is conditions are . Remember the tracking error obtained by setting , the control energy is also damped, so that, since there is a certain tradeoff here. We remark that it is important to tune the free parameters to obtain good properties of the regulator. In particular, the transients, which do not affect the cost function, can change dramatically with different choices of free parameters. and instead, Now, setting while keeping all the other parameters the same, we obtain the errors in Fig. 4. As seen, the error goes asymptotically to zero, despite the fact that condition (113) is not fulfilled so that a T-universal regulator does not exist. In fact, by Theorem 4.1, this is a universal tracking regulator which exists since on the unit circle. In order to speed up the convergence, the roots of have been reset at 0.7 0.1 , 0.3 0.2 , and 0.8. Since now we do not have the disturbance and , we could choose another frequencies to possibly get a universal tracking regulator with a better transient. VIII. CONCLUSIONS In this paper we have given complete characterizations of regulators which satisfy certain tracking specifications and which are universal in the sense that they are independent of disturbances and tracking signals and apply regardless of the values of these. As a preliminary, we considered a problem of asymptotic tracking of an arbitrary signal , and we characterized all regulators which are universal with respect to the choice of . We showed that such universal regulators exist only under very special conditions. These conditions can be considerably relaxed if the reference signal is exchanged for a harmonic signal with known frequencies but unknown amplitudes and phases, and we want the regulator to be universal in the sense that it achieves asymptotic tracking for all choices of the of amplitudes and phases. Then, if the dimension

reference signal is no larger than the dimension of the control, such a regulator exists under mild conditions. This is in harmony with other results in the literature [4]­[8], [13], [16], where, however, the continuous-time case is considered. We provided complete solutions of these problems in discrete time, and our proof is considerably simpler. If the system is also corrupted by a harmonic disturbance , asymptotic tracking may still be possible provided the dimension of the disturbance is no larger than the dimension of the output available for feedback. However, if a certain , rank condition fails, which in particular is the case if asymptotic tracking is not possible, but a steady-state error will remain. Therefore, we considered next an optimal control problem to damp the steady-state tracking error, also giving the option to damp internal system variables. We characterized the class of all optimal regulators which are universal in the sense that they are optimal for all choices of the amplitudes of and . Such regulators were shown to exist if the weak . On the other frequency domain condition holds and , there are always algebraic conditions on the hand, if system parameters, implying that universality is not a generic property in this case. We have also shown that all optimal universal regulators can be chosen as linear even if the optimization is over a very large class of nonlinear regulators, provided the strong frequency domain condition holds. We have given complete characterizations of all linear optimal universal regulators in terms of parameterizations containing many free parameters. This allows for a considerable amount of design freedom, which can be used to satisfy other design specifications via loop shaping. Indeed, we stress that our solutions are optimal in the sense stated in this paper only, and that other desirable design specifications may not be satisfied for an arbitrary universal optimal regulator. APPENDIX A PROOF OF LEMMA 5.5 and tend exponentially to the harmonic comSince ponents (60), only these contribute to the cost function (70); consequently, the usual limit (rather than just limsup) does where exist in (69), and it is given by

(A1) . In fact, this follows from the argument for leading to (64). Now, in view of the constraint (1a) (A2) and therefore (A1) takes the form (A3) if the weak frequency domain condition (77) where is given by (85), and is fulfilled. Here (A4)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1703

Therefore, assuming that the weak frequency domain condition for , we may (77) holds so that complete squares in (A3) to obtain (A5) where (A6) From this the equations of the lemma follow readily. APPENDIX B PROOF OF LEMMA 6.3 The proof is similar, mutatis mutandis, to the one given in [21, Sec. II]. Recall from (69) that the cost function can be written (B1) where

By virtue of condition (94) and the boundedness of

where of course the last term tends to zero as . , the cost function Consequently, for any admissible (B1) becomes

(B5) Therefore, since (B6) for any admissible control. Clearly, equality would be achieved to satisfy (101) since does not if we could take contribute to by virtue of (102). Hence it remains to prove that such a process satisfies the stability condition (94). To this end, insert (101) in (95) to obtain (B7) and are bounded, satisfies Since is a stability matrix, satisfies the (102) and weak stability condition (94). The last statement follows immediately from (B5) and (B6). ACKNOWLEDGMENT The authors would like to thank the anonymous referees and the associate editor for several useful suggestions. They would also like to thank X. Hu for technical advice and stimulating discussions. REFERENCES
[1] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic Methods. London, U.K.: Prentice-Hall, 1989. [2] S. Bittanti, F. Lorito, and S. Strada, "An LQ approach to active control of vibrations in helicopters," Trans. ASME, J. Dynamical Systems, Measurement and Contr., vol. 118, pp. 482­488, 1996. [3] C. T. Chen, Linear System Theory and Design. New York: Holt, Rinehart and Winston, 1984. [4] E. J. Davison and A. Goldenberg, "Robust control of a general servomechanism problem: The servo compensator," Automatica, vol. 11, pp. 461­471, 1975. [5] E. J. Davison and B. R. Copeland, "Gain margin and time lag tolerance constraints applied to the stabilization problem and robust servomechanism problem," IEEE Trans. Automat. Contr., vol. AC-30, pp. 229­239, 1985. [6] E. J. Davison and B. M. Scherzinger, "Perfect control of the robust servomechanism problem," IEEE Trans. Automat. Contr., vol. AC-32, pp. 689­702, 1987. [7] B. A. Francis, "The linear multivariable regulator problem," SIAM J. Contr. Optim., vol. 15, pp. 486­505, 1977. [8] B. A. Francis and W. M. Wonham, "The internal model principle of control theory," Automatica, vol. 12, pp. 457­465, 1977. [9] K. V. Frolov and F. A. Furman, Applied Theory of Vibration Protected Systems. Moscow, Russia: Mashinostroenie, 1980 (in Russian). [10] K. V. Frolov, Vibration in Engineering. Moscow, Russia: Mashinostroenie, 1981 (in Russian). [11] M. D. Genkin, V. G. Elezov, and V. D. Iablonski, Methods of Controlled Vibration Protection of Engines. Moscow, Russia: Nauka, 1985 (in Russian).

(B2) being the quadratic form (70). Next, introduce with the Lyapunov function (B3) is the unique stabilizing solution of (96), where is given by (100) and satisfies (104). Then, along the trajectory of (95)

(B4) is given by (99). where In fact, inserting (95) and completing squares in the left member of (B4) yields the right member of (B4) plus a number of terms which are either quadratic in , linear in , or constant with respect to . The quadratic terms cancel due to the fact that satisfies the algebraic Riccati equation (96), and the constant terms cancel due to (104). Finally, the linear terms cancel provided

which has the unique bounded solution (100), since is a stable matrix. and , where Now, set is an admissible process, and sum (B4) from to to obtain

1704

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

[12] D. Guicking, Active Noise and Vibration Control, reference bibliography, Third Physical Institute, Univ. Goettingen, Jan. 1990. [13] A. Isidori and C. I. Byrnes, "Output regulation of nonlinear systems," IEEE Trans. Automat. Contr., vol. 35, pp. 131­140, 1990. [14] C. G. K¨ allstr¨ om and P. Ottosson, "The generation and control of roll motion of ships in closed turns," in Proc. 4th Int. Symp. Ship Operation Automat., Geneva, Switzerland, 1982, pp. 1­12. [15] H. Kimura, "Pole assignment by output feedback: A longstanding open problem," in Proc. 33rd Conf. Decision and Control, Lake Buena Vista, FL, Dec. 1994. [16] A. Krener, "The construction of optimal linear and nonlinear regulators," in Systems, Models and Feedback: Theory and Applications, A. Isidori and T. J. Tarn, Eds. Boston, MA: Birkh¨ auser, 1992, pp. 301­322. [17] V. Ku cera, "The discrete Riccati equation of optimal control," Kybernetika, vol. 8, pp. 430­447, 1972. [18] H. Kwakernaak and R. Sivan, Modern Signals and Systems. Englewood Cliffs, NJ: Prentice-Hall, 1991. [19] G. Leitmann and S. Pandey, "Aircraft control under conditions of windshear," in Proc. 29th Conf. Decision and Control, Honolulu, HI, 1990, pp. 747­752. [20] P. Lancaster, A. C. M. Ran, and L. Rodman, "Hermitian solution of the discrete algebraic Riccati equation," Int. J. Contr., vol. 44, pp. 777­802, 1986. [21] A. Lindquist and V. A. Yakubovich, "Optimal damping of forced oscillations in discrete-time systems," IEEE Trans. Automat. Contr., vol. 42, pp. 786­802, 1997. , "Optimal damping of forced oscillations by output feedback," [22] in Stochastic Differential and Difference Equations, Progress in Systems and Control Theory, vol. 23, I. Csisz´ ar and G. Michaletzky, Eds. Boston, MA: Birkh¨ auser, 1997, pp. 203­231. [23] A. Miele, "Optimal trajectories and guidance trajectories for aircraft flight through windshears," in Proc. 29th Conf. Decision and Control, Honolulu, HI, 1990, pp. 737­746. [24] B. P. Molinari, "The stabilizing solution of the discrete algebraic Riccati equation," IEEE Trans. Automat. Contr., vol. AC-20, pp. 396­399, 1975. [25] V. M. Popov, Hyperstability of Control Systems. Berlin, Germany: Springer, 1973. [26] A. V. Savkin and I. R. Petersen, "Robust control with rejection of harmonic disturbances," IEEE Trans. Automat. Contr., vol. 40, pp. 1968­1971, 1995. [27] R. Shoureshi, L. Brackney, N. Kubota, and G. Batta, "A modern control approach to active noise control," Trans. ASME, J. Dynamical Syst.s, Measurement and Contr., vol. 115, pp. 673­678, 1993. [28] V. Z. Weytz, M. Z. Kolovski, and A. E. Koguza, Dynamics of Controlled Machine Units. Moscow, Russia: Nauka, 1984 (in Russian). [29] V. A. Yakubovich, "A frequency theorem in control theory," Sibirskij Mat. Zh., vol. 4, pp. 386­419, 1973 (in Russian); English translation in Sibirian Math. J. [30] , "Universal regulators in linear-quadratic optimization problems," in Trends in Control, A. Isidori, Ed. New York: Springer-Verlag, 1995, pp. 53­68. [31] Y. Zhao and A. E. Bryson, "Aircraft control in a downburst on takeoff and landing," in Proc. 29th Conf. Decision and Control, Honolulu, HI, 1990, pp. 753­757. [32] K. Zhou, Essentials of Robust Control. Englewood Cliffs, NJ: PrenticeHall, 1998.

Anders Lindquist (M'77­SM'86­F'89) received the M.S. and Ph.D. degrees from the Royal Institute of Technology, Stockholm, Sweden, and in 1972 he was appointed a Docent of Optimization and Systems Theory there. From 1972 to 1974, he held visiting positions at the University of Florida, Brown University, and State University of New York, Albany. In 1974, he became an Associate Professor and in 1980 a Professor of Mathematics at the University of Kentucky, where he remained until 1983. He is presently a Professor at the Royal Institute of Technology, where in 1982 he was appointed to the Chair of Optimization and Systems Theory, as well as an Affiliate Professor at Washington University, St. Louis. He has also held visiting positions at University of Padova, Italy, University of Arizona, USSR Academy of Sciences, Moscow, East China Normal University, Shanghai, and Technion, Haifa, Israel. He is the author of many papers in the area of systems and control, especially stochastic control, filtering, stochastic systems theory, realization theory, robust control, and applications of nonlinear dynamics in estimation and control. Dr. Lindquist is a Member of the Royal Swedish Academy of Engineering Sciences, a Foreign Member of the Russian Academy of Natural Sciences, and an Honorary Member of the Hungarian Operations Research Society. He has also served on many editorial and advisory boards.

Vladimir A. Yakubovich (M'97) was born in Novosibirsk, Russia, in 1926. He graduated from Moscow University in 1949. He received the Candidate of Science degree (Ph.D.) in 1953 and the Doctor of Science degree in 1959, both from Leningrad University. After having worked for some time in industry as an Engineer, he was admitted to the Leningrad University, where he has remained. He is the author of more than 270 papers and coauthor of seven books in different areas of applied mathematics and control theory. He has worked in parametric resonance theory, in the theory of stability of nonlinear systems, and in optimization theory. Dr. Yakubovich has served on many scientific committees and editorial boards. He is a member of several scientific societies in Russia. He was awarded the Norbert Wiener Prize in 1991, a prize from the international editorial company "Nauka" for best publication in its journals in 1995, and the IEEE Control Systems Award in 1996. Since 1991, he has been a Corresponding Member of the Russian Academy of Sciences and since 1994 a Member of the Russian Academy of Natural Sciences.

Tracking Personal Processes in Group Projects
Ly Danielle Sauer, Timothy E. Lindquist, Jeremy Cairney Computer Science and Engineering Arizona State University Tempe, Arizona 85287-5406 {sauer, lindquist, cairney}@asu.edu February 8, 1999 Abstract
Software engineering continues to develop methods for process improvement and quality. The Personal Software Process is one way to introduce software engineers to aspects of process tracking, assessment and improvement. In this paper, we describe the software tools that we've constructed to support the planning and postmortem of software activities. We describe an approach that allows the personal software process to be used in group projects, while still allowing the individual engineer to employ personal process quality and improvement techniques on their own activities. The tools supporting planning and postmortem are used in the context of a workflow system developed at ASU, called Open Process Components, whose aim is to componentize software services and provide interoperability among various approaches. These tools and approaches explore software development in the increasingly distributed environment in which the software engineer is responsible for their own assessment and improvement.

1.0 Introduction
Measuring, guiding and refining an organization's software process improves effectiveness of development resources and provides a level of control on software quality. The development of the SEI Capability Maturity Model[24] has raised awareness of the need for better software processes. Software processes are often discussed at the project management level, and its not uncommon for an organization to employ the services of a process engineer with the intent of wide-scale process improvement. Software processes describe the interaction among people and artifacts in carrying out the work involved in the software life-cycle. A software process encompasses the work that will be done (activities), what it will use and produce (input and output products), who will do it (agents), as well as, when and how it will be done (behavior). The past decade has seen increased demand for more powerful and robust automated software process systems. Tool vendors and the research community have responded with a variety of approaches. A review of the tool market place shows many groupware, process and workflow tools whose functionality ranges from graphical modeling or simple enactment to full support for defining, executing, analyzing, measuring, and tracking software processes. The Plethora of

tools, most of which have not been widely adopted, combines together with the increasingly distributed nature of software development today to form one of the challenges addressed by this paper. That is, the need to have interoperability among a heterogeneous set of process tools (which execute on distributed heterogeneous platforms.) The efforts of the Workflow Management Coalition (WfMC) [30] and the Object Management Group (OMG) are aimed at this challenge. Both organizations are identifying common interfaces that vendors can use for interoperability among their products. Other middleware efforts have identified process support services, for example, PCIS (Portable Common Interface Set) [9]. A follow-on project [18] integrates the Open Process Components of Gary [13] with other middleware components, such as version and configuration management. In this paper, we build on these efforts to show how processes can be distributed compositions of personal process components. Considerable research has addressed automating the software process. Some are addressing formalisms for expressing process [3]. Different formalisms such as Petri nets[12], rule-based formalisms[1,25], process programming languages[10], event-based representations [4,8,21], and object-oriented approaches[8,21] are

1 of 8

Overview of Personal Software Process

proposed for representing software processes. Other research includes comprehensive environments centered on process, such as ISTAR, in which all activities are modeled using a contractual model. In a process-centered environment, nearly all activity takes place within a defined process. Christie has elaborated several problems in the adoption of process automation [6]. Process-centered environments are typically all-or-nothing and difficult to adopt in steps. Management is justifiably reluctant to invest in dramatic change without a gradual migration path or concrete evidence of value-added. Benefits of enactment support or tracking require time consuming frontend resources for process definition. Some systems require definition of activities that don't have relevance to tracking and improvement. Adoption also places other stresses on an organization ranging from engineer's perception of excessive intrusion to the need for additional personnel who specialize in process engineering. In this paper, we present a process framework that shifts its approach toward composable process components. Project processes are created by brokering among the building blocks of engineer's defined personal processes. Software engineers are responsible for tracking measuring and analyzing their own processes distinct from organizational concerns.

managing, and improving their predictability, productivity, and quality. PSP consists of a family of seven personal processes that progressively introduce data and analysis techniques (Figure 1 on page 2) [16,28]. Engineers use these data and analysis outcomes to determine their performance and to measure the effectiveness of their methods. Humphrey's initial result (applied to 50 students and three industrial software organization) indicates an average test defects improvement of over ten times and productivity improvements of better than 25% [28].
PSP3 Cyclic Development

Cyclic Personal Process

Personal Quality Managem ent

PSP2 Code Reviews Design Reviews

PSP2.1 Design Templates

Personal Planning Process

PSP1 Size Estimating Test Report

PSP1.1 Task Planning Schedule Planning

PSP0 Baseline Personal Process Current Process Time Recording Defect Recording Defect Type Standard

PSP0.1 Coding Standard Size Measurement Process Improvement Proposal

2.0 Overview of Personal Software Process
Current software professionals utilize private techniques and practices that were learned from peers or through personal experiences. Few software engineers are aware of, or consistently use methods that lend themselves to personal process improvement. A personal software development process is a concept introduced to address improvement needs of an individual. Watts Humphrey of the Software Engineering Institute has formalized a personal software development process called Personal Software Process (PSP). Today, there are various realizations of PSP to aid software engineers in applying the process. The realizations range from case tools and web-based repository browsers to formal training classes.

FIGURE 1.

PSP Process Evolution

2.1

Personal Software Process

Figure 1 on page 2 shows the PSP progression in which each PSP step includes all the elements of prior steps together with additions. The PSP process steps are Baseline Personal Process (PSP0, PSP0.1), Personal Planning Process (PSP1, PSP1.1), Personal Quality Management (PSP2, PSP2.1), and Cyclic Personal Process (PSP3). Starting in The Baseline Personal Process, the software engineer creates the foundations for measurement and improvement. PSP0 is the software engineers current software development process extended to provide measurements (time and defect trackings). PSP0 covers three phases: planning, development (design, code, compile, and test), and postmortem. PSP0.1 includes coding standards, size measurements, and a Process Improvement Proposal (PIP). Personal Planning Process (PSP1, PSP1.1) adds planning to the baseline. Here, the software engineer pre-

Personal Software Process (PSP) [15,16,17,28] is designed to assist software engineers in controlling,

Tracking Personal Processes in Group Projects

2 of 8

Applying PSP to Group Projects

pares the basis for project tracking, which include software estimates and development plans. The goal is to learn the relationship between program size and resources, as well as how to make realistic schedules. PSP1 enhances PSP0 and PSP0.1 to include size and resource estimation and a test report using Proxy Based Estimation (PROBE) as a method to estimate sizes and development times. Personal Quality Management (PSP2, PSP2.1) provides defect management by tracking the relationship between time spent in reviews and the phases during which defects are injected and removed. Prior project defect data are used to realize review checklists and selfassessments. PSP2.1 extends PSP2 with design specifications and accompanying analyses. The goal is to provide the criteria for design completion. Cycle Personal Process (PSP3) introduces techniques for developing large-scale projects. The approach calls for sub-dividing into personal processes. Development is done in incremental steps starting with a base module.

Project Tables are automated forms, such as Logs, Summaries, and Templates. The log tables support tracking time, defects and issues. The Project Summary table records the estimated and actual totals for the project and for all projects to date. The Cycle Summary table supports the project summaries by capturing the planned and actual size, time, and defects for each cycle.

2.3

ECEN 4553 Database Browser

ECEN 4553 PSP Database (PSP Database Browser) [5] is a web-based database that also automates many of the PSP forms, scripts, calculations, and reports. The database is organized to capture a set of related data for an individual software engineer. The core of the database is the concept of a job, which is a software engineer's activity. Once the job is defined, the software engineer can log time against the job, log defects against the job, and specify a detailed project plan for the job. Although the PSP Database Browser does not strictly adhere to all of Watts Humphrey's Personal Software Process data, it collects planned and actual data for each job. ECEN 4553 PSP Database Browser automates a subset of Watts Humphrey's Personal Software Process with a web-based user interface.

2.2

Personal Software Process Studio

Personal Software Process Studio (PSP Studio or PSPS) [11] is a case tool developed at East Tennessee State University to assist in using the Personal Software Process. PSPS does so by automating the planning and postmortem artifacts. In particular, PSPS provides the following features: Data Measurement, Historical Database, Convenient Access to Tables, Statistical Calculations, and Guidance through the Process. The Data Measurement feature allows developers to accurately (similar to a stopwatch) measure development times, track defects, and measure program sizes. The Historical Database feature allows developers to store all of the developers historical PSP data in a reliable and secure database. Convenient Access to Tables provides a window with tab access to the forms. The Statistical Calculations feature automatically maintains totals and performs the statistical calculations. The Guidance through the Process feature provides on-line direction for using the PSP. PSP Studio groups all of the automated paper works, forms, and calculations into two categories: Process Tables and Project Tables. Process Tables guide or improve the individual software engineer process with an online process outline, access to standard tables for defect, LOC and coding standards, and access to a process improvement proposal.

3.0 Applying PSP to Group Projects
ISO 9000 [7,22] and the Capability Maturity Model (CMM) [23,24] assist organizations in improving their processes. Personal Software Process [15,16,17,28], on the other hand, provides an improvement technique for software engineers, in the context of individually developed software. Seamless integration of the PSP within a software organization cannot be achieved, since individuals rarely cycle through all phases of development on a software project. Engineers can, however, apply PSP analysis techniques to their individual activities on group projects. The resulting metrics can be the basis for personal process improvement, without having the "big brother is watching over me" complex that is common to organizationally imposed quality and improvement efforts. This section discusses our approach to providing well integrated organizational and personal process improvement. At ASU, we have been developing software to support the use of planning and postmortem phases of the PSP and to support their application to various life-cycle activities. For example, in an organizational setting, an individual may be assigned to testing. The test engineer

Tracking Personal Processes in Group Projects

3 of 8

Applying PSP to Group Projects

would develop their own test process that includes planning and postmortem. The resulting personal test process becomes part of an organizational or project process. The "integratable" personal processes (personal test process) collect product measures, use defect analysis and consider resource usage as a means of improving that process segment. The artifacts and the automation we have developed are discussed in Section 3.2.

3.1

Process Components

The Open Process Component Toolset (OPC) [14,19,20] is a set of tools developed at ASU to support process definitions and enactment. OPC's basic premise is that a process is a process component and may consist of one or more process components. Process components may be compositions of subcomponents whose underlying representations may differ. For example, a Process Weaver component, called create_design, may be composed with a TeamWare Flow component called review_design. Thus, the Integrated Process is defined as a process component consisting of three process components: the Planning Process Component (Figure 2 on page 4), the Personal Software Activities Component, and the Postmortem Component. The Software Activities component may be any process component such as, testing, design, coding, or review. Discussion of these components and the support we have implemented for Planning and Postmortem artifacts can be found in the following sections (Section 3.1.1, Section 3.1.2, and Section 3.1.3). 3.1.1 The Planning Process The Planning Process Component defines the individual engineer's plans for the software activity. The process is assigned to the project planner, takes as inputs the customer requirements (written or oral) and produces as output an initial version of the planning artifacts, a requirements specification, a cost estimate report, and a size estimate report. Additionally, an engineering notebook for the project is created and initialized based on the activity schedule. At this phase, the project activity schedule and the project plan summary forms only contain planning information such as estimated total size, the project development duration, and defects injected and removed. Further descriptions of the project activity schedule, the project plan summary, and the engineering notebook are discussed in Section 3.2.

FIGURE 2.

Planning Process Component

As shown in Figure 2 on page 4, the Planning Process Component is composed of its children process components: Identify Requirement, Perform Size Estimation, Perform Cost Estimation, and Construct Plan. Each child process component is defined to perform a specific task to help planning the software activity and laying the groundwork for analysis. For instance, the Identify Requirement Process Component generates the requirement specification (SRS) given the customer requirements (Cust Req). Figure 2 on page 4 shows the OPC definition tool's graphical depiction of the Planning Process. The model includes nodes for Processes (process components that have subcomponents), Activities (process components without subs), Roles and Products. Directed edges depict relationships such as can_perform, is_input_to, has_output and has_sub. For example, the Requirement Specification (Product) is_input_to Perform Cost Estimation (Activity), and the Identify Requirement (Activity) has_output which is the Requirement Specification. 3.1.2 Personal Software Activities In the PSP, the planning and postmortem activities depend on a personal software process that includes the phases, planning, design, code, code review, compile, test, and postmortem. In our application of the PSP to group projects, we provide the capability to replace design, code, code review, compile and test with other activities. Our approach is to provide the background for the planning and postmortem phases as applied to any software related activities. In a group project, an individual engineer may

4 of 8

Tracking Personal Processes in Group Projects

Applying PSP to Group Projects

not be involved in coding, compiling and testing, but may instead work on design and design reviews, or may instead be a test engineer whose involvement does not go beyond planning, developing, executing and reporting on tests. Our assumption is that the analysis techniques that consider resources (labor, primarily), product measures and quality assessment all equally apply to any other software related activities, whether directly developing code or not. Process improvement should be a center of focus for all participants in a software process. At ASU, we have been using this approach to Integrating Personal Processes for group software projects in a classroom setting and for group independent study projects. Thus far, uses are for small applications in which most project members get involved with all of the life-cycle activities. The primary challenge in generalizing the approach to large group efforts has to do with product and quality measures. PSP relies on Source Lines of Code as the basis for product measures. Software defect management is the basis for quality, planning and process improvement. Engineers using PSP, record defects by type, phase injected and phase removed. PSP uses yield (percentage of defects removed before compiling), appraisal cost of quality and failure cost of quality as the primary input for quality management and process improvement. These are a good starting point for the practicing software engineer, however, one must define product measures and defects in a manner appropriate to the activity. For example, a test engineer may use test cases generated as the primary product measure. For example, test cases may be defined to be triples (input condition, action, expected result) independent of how the test case is realized in performing tests. Defect types for a test engineer may include: unsatisfied test requirement, and resulting software defects for which there existed a test case.

We have used OPC to depict Postmortem. The process is assigned to the process engineer and accomplishes its objective of producing the project plan summary by using the initialized project activity schedule, the project defect log, and the initialized project plan summary as input products. Unlike the Planning Process Component, Postmortem does not use children process components to accomplish its goal.

3.2

Automated Support for Process Artifacts

The Integrated Personal Process uses four artifacts: the Engineering Notebook, the Project Activity Schedule & Log, the Project Defect Log, and the Project Plan Summary. We have implemented each artifact as a standalone application. When using the worklist handler of OPC, enacting one of the Integrated Personal Process Planning or Postmortem activities may cause the invocation of one or all of these applications according to the process input and output specifications. All four artifacts use a single repository interface to store and manipulate data. The interface is implemented in Java, using synchronization to support multiple concurrent access. Highlights of these artifacts are detailed in the following sections. 3.2.1 The Engineering Notebook The Engineering Notebook is an application which implements some concept of the Personal Software Process Engineering Notebook. The Engineering Notebook objective is to create an engineering notebook that tracks a software engineers daily time usage. More specifically, as shown in Figure 3 on page 6, the Engineering Notebook allows a software engineer to define and record, for a given project, its activities, the time spent on the activities, and product lists of the activities. An activity is a unit of work which takes an engineers time (e.g. interruptions, coding, breaks, lunch, designing, etc.); it is any work performed by a software engineer. The time spent on each activity is recorded in an increment of hours; for instance, a job that takes 15 minutes could be recorded as 0.25 hours, but our usage generally limits granularity to one tenth of an hour (6 minutes). The product list entry allows the engineer to list products produced by the activity. The initial engineering notebook is derived from infromation in the Project Activity Schedule & Log.

3.1.3 The Postmortem Process The Postmortem Process Component defines a process for analyzing the performance (postmortem analysis) of a completed project. Postmortem analysis gathers product measures, performs actual resource usage analysis, performs actual defect analysis, and performs summary quality analysis.

Tracking Personal Processes in Group Projects

5 of 8

Applying PSP to Group Projects

to allow add-on functions. For example, the user can add the tools for estimation or a LOC Counter. The add-on tools are specified using MIME types. 3.2.3 Defect Log The Defect Log (DL) is an application which automates Defect Recording [15,16] to aid in tracking defects injected and removed. The defect data are stored in the defect log, which are used as input to generating the plan summary (Section 3.2.4) in postmortem analysis. The Defect Log is realized as a tabular application where the rows represent the defects and their information and the columns are classifications of the defects. As shown in Figure 4 on page 6, the DL allows its user to specify the date, the defect type, the injected phase, the removed phase, the fixed time, and a description.

FIGURE 3.

Engineering Notebook Main Window

Modification to the times in the engineering notebook causes the transfer of the times spent and the product list to the Project Activity Schedule & Log. 3.2.2 Activity Schedule & Log The Activity Schedule & Log (ASL) is an application, which aids in developing project plans. The ASL application allows the project planner to identify the activities, phases, agents, and times for the activity. When the user completes the activity specifications, ASL places the schedule in a persistent repository. An activity is a task of the project. A project may have a set of activities (process components) representing the work of all group members assigned activities on the project. Similarly, all engineers working on a project will have their own activity schedules, which reflect the lowerlevel activities necessary to complete their input to the group. The lower-level activities are subject to analysis and improvement as defined above. A project planner may also have an Activity Schedule and Log to coordinate the activities and products of a group of engineers. We envision that the project planner can use the Process Broker [27] (which is currently under development), to determine the kind of activities that the project may need and to check for the availability of those process components. To do this, the project planner first specifies the characteristics of the current project to the Process Broker. The Process Broker uses its locating and matching semantic engine and its repository of process components to determine the projects that best fit the specified criteria. For each activity, the project planner estimates a begin and end time, the development duration, the project size, and the output products. These are estimated values, thus, the project planner can use experience to determine the values, some tools, or historical project data. OPC is designed

FIGURE 4.

Project Defect Log Main Window

The date that the defect was discovered and the defect description can be anything that the user enters. The DL default defect types are: Documentation, Syntax, Build/ Package, Assignment, Interface, Checking, Data, Function, System, and Environment. These can be modified to allow application of planning and postmortem to any software activity. Additionally, the DL also provides default phases including: Planning, Design, Code, Review, Compile, Test, and Postmortem. Analogous to the defect types, these can be changed to accommodate the activity. Finally, the time required to correct the defect is recorded in hours and tenths. 3.2.4 Project Plan Summary The Plan Summary (PS) [15,16] is an application, to aid in planning and tracking a software activity. The plan summary is initialized in the planning activity and is com-

6 of 8

Tracking Personal Processes in Group Projects

Current and Future Work

pleted in postmortem. In our implementation, information in the plan summary is derived from the Activity Schedule & Log. The plan summary can be saved and named so that an engineer who participates in several software activities (reviews, testing, and coding, for example) can track data specific to the activity.

www.eas.asu.edu/~yfppg/ [3.] Armenise, P., Bandinelli, S. Ghezzi, C., and Morzenti, A. A Survey and Assessment of Software Process Representation Formalisms. International Journal of Software Engineering and Knowledge Engineering, vol. 3, no. 3, pp. 401-426. 1993. Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS-034-95, Computer Science Department, Columbia U. 1995. L. Carter, ECEN 4553 PSP Database Tool, (University of Colorado at Boulder, Department of Electrical & Computer Engineering, ece-www.colorado.edu/~ecen4553/Reference/ psp/examples.html). A. Christie, et al. A Study into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. F. Coallier, "How ISO 9001 Fits into the Software World", (IEEE Software, January 1994, pp. 98-100). Conradi, R., et al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. J.C. Derniame, et al. "Life-Cycle Process Support in PCIS, Or It Is Time to Think about Software Process Formalisms Standardization", in Proc. of the PCTE'94 Conf. PCTE Technical Journal No.2, PIMB Assn, November 1994. J.C. Derniame, and Gruhn, V. Development of Process-Centered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127150. 1994. East Tennessee State University, Personal Software Process Studio, (East Tennessee State University, Computer and Information Science, www-cs.etsu.edu/softeng/psp/dlpsps.html). W. Emmerich, and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proceedings of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. K. Gary, "Process Interoperability with Open Process Components", Arizona State University, Computer Science and Engineering Department, Ph.D. Dissertation, expected August 1998.

4.0 Current and Future Work
OPC provides an initial set of tools for defining and enacting process components. The underlying implementation of OPC provides the framework for wrapping various process tools for interoperability. We have achieved initial wrappings of two products, and hope to soon demonstrate interoperability between these products in the near future. Thus, a process component can be defined in terms of sub components each under the direction of a different enactment engine. We have used our integrated personal software process approach in classroom projects and in group independent studies. The tools described in this paper will be introduced to these projects beginning in the Fall semester 1998. Enactment using OPC is controlled by a worklist handler tool, which connects to a repository of process components. Process components are all represented using Java objects. Until the tool wrappers are fully functional, enactment involves launching an application associated with the input and output products as specified with MIME types. A few important distinctions differentiate our approach to integrating personal processes. Engineers are not asked to carry out a defined process that they themselves have not developed. Engineers are motivated to use process improvement techniques, since they directly and solely apply to their own activities. Product and defect measures are defined by the engineer and thus problems of consistency do not arise. Engineer define their own personal process for the software activities they perform. These may defined or applied from definitions they obtain from other engineers.

[4.]

[5.]

[6.]

[7.]

[8.]

[9.]

[10.]

[11.]

5.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proceedings of the 2nd EWSPT `92, Trondheim, Norway. September, 1992. Arizona State University, Open Process Component Toolkit, (Computer Science and Engineering Department, YFPPG Group, http://

[12.]

[13.]

[2.]

Tracking Personal Processes in Group Projects

7 of 8

References

[14.]

K. Gary, T. Lindquist, L. Sauer, and H. Koehnemann, "Automated Process Support for Organizational and Personal Processes", (Proceedings of the International Conference on Supporting Group Work (GROUP `97), the Integration Challenge, Phoenix, Arizona, USA, 16-19 Nov 1997). W. S. Humphrey, Introduction to the Personal Software Process (Reading, MA: Addison-Wesley, 1997). W. S. Humphrey, A Discipline for Software Engineering (Reading, MA: Addison-Wesley, 1995). W. S. Humphrey, "The Personal Process in Software Engineering", (Software Process Newsletter, Technical Council on Software Engineering, IEEE Computer Society, Vol. 13, No. 1, September 1994, pp. 1-3, http://www.sei.cmu.edu/products/publications/95.reports/95.ar.psp.swe.html). The US-France Technology Research and Development Project, PCIS2 Architecture Specification Version 1.0, (Lindquist, TE editor) SPAWAR Systems Command, San Diego CA, January 1998. T. Lindquist, "A Toolset Supporting Distributed Process Components", (Arizona State University, Computer Science & Engineering Department, Technical Report, TR-97-034, 1997). T. Lindquist, and J. Derniame, "Towards Distributed and Composable Process Components", (Proceedings of the European Workshop on Software Process Technology, June 1997). Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Mark C. Paulk, "How ISO 9001 Compares with the CMM", (IEEE Software, January 1993, pp. 74-83). Mark C. Paulk, Bill Curtis, and Mary Beth Chrissis, "Capability Maturity Model, Version 1.1", (IEEE Software, July 1993, pp. 18-27). Mark C. Paulk et al., "Capability Maturity Model for Software, Version 1.1", (Technical Report, CMU/SEI-93-TR-24, Software Engineering Institute, 1993). Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. W. Royce, "Managing the Development of Large Software Systems: Concepts and Techniques", (WESCON Technical Papers, Vol. 14, Los Ana-

gles, WESCON, August 1970). [27.] L. Sauer, "Brokering of Process Components", (Arizona State University, Computer Science & Engineering Department, Ph.D. Dissertation Proposal, December 1997). Software Engineering Institute, Personal Software Process (PSP), (SEI Technology, http:// www.sei.cmu.edu/technology/psp/). Software Engineering Institute, A Specification for Automated Support for the PSP, (SEI Technology, http://www.sei.cmu.edu/technology/pspAuto/indexh.html). The Workflow Management Coalition. The Reference Model. WfMC Document Number TC001003, January 1995.

[15.]

[28.]

[16.] [17.]

[29.]

[30.]

[18.]

[19.]

[20.]

[21.]

[22.]

[23.]

[24.]

[25.]

[26.]

8 of 8

Tracking Personal Processes in Group Projects

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann Computer Science and Engineering Department Arizona State University Tempe, Arizona 85287-5406 Jean-Claude Derniame Laboratoire lorrain de Recherche en Informatique et Applications LORIA : Bd des Aiguillettes BP 239 54 506 Vandoeuvre Cedex Abstract
Only recently has the research community started to consider how to make software process models interoperable and reusable. The task is difficult. Software processes are inherently creative and dynamic, difficult to define and repeat at an enactable level of detail. Additionally, interoperability and reusability have not been considered important issues. Recent interoperability and reusability solutions advocate the development of standard process model representations based on common concepts or generic schemas, which are used as a basis for translating between heterogeneous process representations. In this paper we propose an alternative approach through the development of process-based components. We present the Open Process Components Framework, a componentbased framework for software process modeling. In this approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of process representations, an explicit representation of process state, and an extendable set of class relationships. their benefits, systems based on these formalisms create enactable process models which are not interoperable nor reusable with one another. The prevailing solution is to advocate an intermediary standard process representation and provide translations for interoperability and reuse. We do not believe this approach is scalable and defeats the purpose of using heterogeneous process representations. We advocate an object-oriented, component-based philosophy for providing software process interoperability and reuse. This paper presents Open Process Components, a component-based framework for software process definition and enactment. In this framework, components are well-encapsulated representations of process entities that interact in meaningful ways. The framework is solidly founded on mature concepts in the software process field, and yet is extendable so that process models may be customized in a particular domain. A componentized view of process representations results in easier process definition, modularized process enactment, natural interoperability, and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable. Instances vary according to constantly changing demands of specific projects. Fully elaborating a software process model to an enactable level of granularity is often too tedious, time-consuming, and costly[4]. Motivated by the need for interoperability and reuse, we advocate applying component-based techniques to software process modeling. Constructing software process models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined components.

1.0 Introduction
Since Osterweil's proposal[12] for automating the software process a decade ago, there has been significant debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets [9], rule-based formalisms [1,8,13], process programming languages [15], event-based representations [3,6,10], object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research under Award number N00014-97-1-0872

A component-based approach:

2.1.2 Process Component States
The elements of the meta-model appear in most process models. Each model requires a different enactment service to interpret the representation and execute the process. Regardless of the formalism employed and the interpreter used, all models define actions on the entities within the process domain, which effect the states of those entities. The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes finite state machines as part of its basic abstractions. Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for process modeling is not unique[7,11]. The OPC framework defines a basic set of states and transitions for Process and Activity components. These include states such as executing, suspended , and aborted, with corresponding transitions between states defined by actions such as startProcess, suspendProcess, and abortProcess. Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object's unique behavior when interacting with other components within the framework. The current class definition for state transition graphs include operations to add and remove states and allowable transitions between states, making a component's state and behaviors affecting state explicit and manipulable.

· · · ·

avoids deep integration of semantic models handles the natural complexity of software processes, responds to dynamic software processes, and facilitates reuse, minimizing one-shot process models. Component-based process modeling requires a framework for developing components. The framework must identify process entities, define meaningful interactions between entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides a foundation for developing, integrating, maintaining, and reusing a variety of process representations. The framework defines basic abstractions of the problem space that can be specialized. Yet, the framework must make some commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this meta-model as a basis for translation between process models, we use it as a foundation for identifying elements of the process space for componentization, and for defining meaningful ways in which process components interact.
has_sub

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a layered, three-tier software architecture (Figure 2). The Framework Layer defines classes and interfaces modeling process entities derived from the OPC meta-model. The Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component Layer extends representations to particular domains. It is from this layer that actual Process component objects are instantiated. A process model in the OPC framework is a set of components, realized as objects of Component Layer classes, and a set of relations between those components, created under the constraints of the Framework Layer, implemented using Representation Layer semantics. Figure 2 shows example classes at each layer of abstraction for the meta-model element Process. The Framework Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation Layer is comprised of class definitions for specific process representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets (PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any other representations we wish to encapsulate. The Compo-

role
assigned_to can_perform

activity

has_input has_output

product

consists_of

has_version has_variant
has_sub

agent

process

FIGURE 1. The Open Process Components Framework Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and Agent), and artifacts (Product). The meta-model defines the "rules of engagement" for components. It identifies what component types interact with what other component types under what relationships. These relationships are not static; process components and component relationships are highly dynamic during the course of the component's life cycle.

Framework Layer

Process

Process
Representation Layer ECAProcess OrderedProcess PetriNetProcess RuleProcess

PDLProcess

Component Layer Bug Fix Code Module Integration Test Design

Code Module

Peer Review Stress Test

FIGURE 2. Object-oriented class diagram for Process components

nent Layer contains type definitions for actual process types. The dashed lines between layers in Figure 2 denotes that the Representation and Component Layers in fact can have many levels. This allows for multiple ways in which to extend and specialize the framework. The first step identifies a base set of classes and interfaces at the Framework Layer. The next step is to construct encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are encapsulated behind the interfaces inherited from the Framework Layer. For example, the implementation may come from a COTS process tool. Finally, components defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used since inheritance would tie the component's type to its implementation. Component Layer objects are configurable. Component Layer classes represent generic process models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product, Role, or Agent) fully specified and bound is part of an instantiated process model[5]. Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given representations. Delegating Representation Layer classes provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi's levels of process specialization[5].

software development support. PCIS2 services include Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA. The process support services in PCIS2 are based on the OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification, known as jFlow[11], submitted to the OMG. The jFlow specification is largely an "object-ization" of existing WfMC interfaces[16]. This is not a drawback, but one of the strengths of the OMG's approach to adopting and adapting existing technology. The jFlow specification improves upon the original WAPI specifications by defining appropriate interactions between objects to gain interoperability and maintainability of workflow systems. The PCIS2 specification is object-oriented from the ground up, but has borrowed some of the jFlow concepts in order to maintain compliance with emerging standards. PCIS2 and the jFlow specification differ in three areas. First, PCIS2 supports dynamic processes through ad hoc process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2 incorporates support for the metaprocess, by defining views on its services for controlling, defining, performing, and monitoring processes. jFlow only defines interfaces for performing (enacting) and monitoring workflows. It should also be pointed out that jFlow identifies concepts not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these specifications, they are largely complementary and both provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The authors propose the construction of software process components for producing process artifacts. A "software process component" is essentially a process model fragment written in some Process Modeling Language (PML). Components are dynamically combined to construct complete process models through interface types and their respective "connectors ports". The authors correctly motivate the need to eliminate monolithic process systems and instead provide reuse and integration capabilities for process representations. However, the approach lacks adherence to foundational concepts, such as those used in OPC (see Section 2.1). The three-tier layering of the OPC framework provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite their differences, the Pynode component approach is simi-

3.0 PCIS2 Process Services
The Open Process Components framework is currently used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

lar in philosophy and motivation to the OPC framework, and appears to be at roughly the same level of maturity. Results of these two experiments will be very useful to the software process modeling community. A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14]. The authors view the object space and the process space at different levels. The object space is data-oriented, whereas the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that brings together the object level and the process level together. Shams-Aliee and Warboys[14] also advocate modeling a process as a collection of objects or components. However, we find the distinction between the object level and the process level unnecessary. In particular, we do not agree that the object level is a data-oriented model. In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of these objects as defined by their interfaces. OPC merges objects and processes into components through an explicit representation of process state contained in the component. We propose a full object-oriented framework that includes class definitions, inheritance, and rules for component interaction. This merging of objects and processes into a complete component-based model allows OPC the full potential to achieve interoperability and reuse by being independent of any process modeling formalism.

Software Process (ICSP4). December, 1996. [3.] Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995. Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R. Towards a Reference Framework for Process Concepts. Proc. of EWSPT'92, pp. 3-17, Trondheim, Norway. September 1992. Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. Derniame, J.C. Life Cycle Process Support in PCIS. Proc. of the PCTE `94 Conference. 1994. Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994. Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proc. of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Object Management Group. jFlow Joint Submission. OMG Document Number bom/98-06-07. July 4, 1998. Osterweil, L. Software Processes are Software Too. Proc. of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987. Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings of the First World Conference on Design and Process Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for Design and Process Science, Austin, TX. December 1995. Sutton, S., Heimbigner, D., and Osterweil, L. Language Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium on Software Development Environments, Irvine, CA. December 1990. Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003, January 1995.

[4.]

[5.]

[6.]

[7.] [8.]

[9.]

[10.]

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software processes. This framework identifies common concepts in the research community and defines an object-oriented framework for applying these concepts. This framework is currently employed in the construction of a software architecture for support distributed software development. This approach, together with related efforts in the field of workflow, makes the important contribution that the software process automation field is maturing to the point that efforts such as the one described herein can be attempted. Despite whether the reader agrees with the design of this framework, providing interoperability and reusability will overcome one of the serious hurdles preventing wide scale deployment of software process automation technology.

[11.] [12.]

[13.]

[14.]

6.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proc. of EWSPT `92, Trondheim, Norway. September, 1992. Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

[15.]

[16.]

[2.]

Automated Process Support for Organizational and Personal Processes
Kevin Gary, Tim Lindquist, Harry Koehnemann. Ly Sauer Arizona State University Computer Science Department Mail Stop 5406 Tempe, AZ 85287-5406 yfppg@asu.edu
ABSTRACT We propose two views on process: an organizational view and a personal process view. Information technology applies Automated Workflow technology to define, execute, and track an organization's automated business processes. Calendaring tools provide a form of personal process view through scheduled work items. However, the personal, or individual, view of the process space has largely been ignored. We maintain that as organizations become increasingly decentralized, a single organization's process space is becoming difficult to recognize. Individuals of the organization are asked to do work that spans organizational, functional, and even geographic boundaries. An integrated view of organizational workflows and personal processes is needed to address these new demands. In this paper we argue for the need to integrate organizational and personal processes. We then propose a component-based process modeling approach and supporting process architecture that integrates these process spaces. Finally, we describe our recent efforts at developing Java prototype process tools that realize the proposed modeling technique and supporting architecture. Keywords: Workflow, Personal Process, Components units. Current technology is growing at a rate that can be difficult to track. Industry investments in desktop tools and groupware must be leveraged against growth in local and wide-area networks (LANs and WANs). In particular, the growth of the Internet makes it possible to envision computer support of global, decentralized, business processes. These changes in industry and technology escalate the pressures put on information technology research. Providing computer support for widely distributed organizations using new technologies such as the Internet, Groupware, Calendar Management, and Automated Workflow is at least an IT systems analyst's headache. Determining the best way to integrate these tools to ensure maximum productivity is at best an IT manager's nightmare. In our view, the ability to define, execute, and track business processes is central to the ability to integrate these technologies in a widely distributed setting and make their use productive. Therefore in our research we focus on automated process support. In the business domain, automating business processes is known as Automated Workflow. Workflow is the study of modeling and enacting business processes by human and computer agents. Automated Workflow adds an emphasis on applying current computer and information technology in a workflow environment, with the desire of automating parts of workflows, or supporting entire workflows. Automated Workflow has traditionally focused on defining and automating business processes from the organization's standpoint. Little regard is given to managing overlapping workflows in an individual workspace, or for even considering the personal processes of an individual when considering the productivity of the organization. The current solution is to drop a set of personal productivity tools, such as calendaring tools, in the lap of the individual and let her/ him work it out.

1.0 Introduction
Recent changes in industry and technology are imposing more demanding technical requirements on information technology. In industry, organizations are downsizing and becoming increasingly decentralized, often causing projects to be managed across multiple organizations or functional

In order to achieve greater productivity from both workflow and personal productivity tools, a more integrated view of organizational and personal processes must be considered. An integrated view allows individuals the ability to develop their own productive work practices in support of an organization's processes, and allows for a more natural handling of processes spanning multiple organizations and individuals. We are in the beginning stages of our research into the utility of providing such an integrated view. In this paper we propose an open architecture for integrating organizational workflows and personal productivity processes. We motivate the need for an integrated approach, and present a component-based approach to process modeling that provides the interoperability required to achieve the integration. We also present a suite of tools being developed at Arizona State University that realize this architecture. The rest of this paper is organized as follows. Section 2.0 discusses relevant issues in current workflow and calendaring technology. Section 3.0 argues for an integrated view of organizational and personal process spaces, presents a component-based approach to process modeling, and proposes a general process support architecture. Section 4.0 presents tool prototypes realizing this architecture that were recently developed at Arizona State University. We conclude in Section 5.0 with a summary and discuss future avenues for our research.

tectural elements of workflow systems and the interactions between those elements. The Process Interchange Format (PIF) Working Group was formed to explore the potential to provide automatic translations between process representation formalisms[15]. Finally, Microsoft is pushing their Messaging API (MAPI) as a defacto standard for implementing workflow systems. Microsoft has recently teamed with Wang to develop the MAPI-WF specification[17], an extension of MAPI for supporting workflow-specific services. The WfMC is presently the most significant of the efforts attempting to standardize workflow systems. The WfMC Reference Model (Figure 1) identifies the basic architectural components of a workflow environment. At the center of the model is a Workflow Enactment Service (WES), comprised of one or more Workflow Engines. A WES provides services through the WAPIs to workflow-related tools. These include Process Definition Tools for defining processes, Workflow Client Applications for handling user requests for work, Third-party Applications that need to communicate data and operations to the WES, other WESs for providing interoperability between enactment services, and Administration and Monitoring Tools for data gathering for process improvement activities. The WfMC Reference Model identifies common workflow system components and interfaces. The WAPI interface specifications define a set of low-level protocols for synchronously and asynchronously exchanging workflow data between the tools and the WES. Our basic problem with this approach is that these protocols are too low-level; they imply a restrictive workflow model. Workflow representations that cannot easily convert their process data to conform with this underlying model cannot obtain conformance with the model. This is one of the issues our research addresses. Recent standardization efforts also address the area of calendaring protocols. One popular calendaring protocol (adopted by Netscape's Calendar Server[18]) is the vCalendar protocol[12]. In the vCalendar protocol, calendaring and scheduling entities, called events, are transported between applications that can understand the protocol. This approach is similar to the effort of the WfMC protocols in that it defines a low-level data interchange format that tools must understand to conform to the protocol. Other, more industry-wide standardization efforts are being sponsored by the Internet Engineering Task Force (IETF) based in part on the vCalendar specification. The IETF has recently sponsored the development of three separate calendaring protocols, the Calendaring Interoperability Protocol (CIP), the Core Object Specification (COS), and the Internet Calendar 1. For Workflow API and Interchange

2.0 Background
Approaches to developing workflow systems have both commercial and academic origins. Commercial systems have evolved from work on forms-based image processing systems and groupware[14]. The line between workflow and other types of systems is often blurred, with groupware, scheduling, database, and email tools providing some workflow functionality. In addition, several commercial products that advertise workflow capabilities fall far short of providing full-fledged support for defining and enacting business processes. Academic research has focused mainly on process modeling and database transaction issues[9]. Process modeling research has led to the development of workflow representations based on a variety of formalisms. Database transaction research focuses on extending traditional transaction semantics to support long duration[2][9] and/or cooperative transaction[5][10] models. The result is a proliferation of approaches and issues relating to workflow. Current efforts are attempting to get researchers and vendors to converge on a common foundation for workflow. The Workflow Management Coalition (WfMC) was formed in August 1993 to promote workflow technology. The WfMC has proposed a reference model[22] and a set of interfaces, called WAPIs1 based on that model[24][25][26][27] as an attempt at standardizing archi-

3.1 Organization vs. Personal Process Space
Automated Workflow is the specification and execution of a business process of an organization[9]. Workflows are modeled as a collection of process steps, or tasks, assigned to individuals taking on particular roles. Many modern workflow systems work in this way; the process is considered from a single organization's viewpoint. This viewpoint is illustrated in Figure 2. Organization A Workflow 1 Workflow 2 FIGURE 1. WfMC Reference Model ([23]) Access Protocol (ICAP). These protocols specify interface and other requirements on calendaring systems exchanging calendaring data. The standardization efforts in both workflow and calendaring focus on low-level data interchange and protocols for exchanging such data in a client-server environment. While this is a widely accepted standardization approach, we fear that a stable data format is difficult to obtain due to the maturing of the underlying models in each domain. This is especially true in workflow. In the calendaring domain, a problematic issue is that calendaring formats and tools support only rudimentary dependencies between tasks. These issues are compounded when integrating workflow and calendaring systems. Workflow systems can write events to calendar tools, but are not aware of the personal views of the process of the participating individuals. Likewise, calendaring systems provide a personalized view of work, but do not possess sophisticated enough models to negotiate with workflow systems over the ability to do assigned work. Workflow 3 Workflow 4 Workflow 5

Organization B FIGURE 2. Organizational Process Perspective Figure 2 shows the process space of two organizations, generically labeled A and B. These organizations share two workflows: Workflow 3 and Workflow 4. Interoperability of the underlying process models and process support architecture is required to allow these organizations to share these workflows. The workflow systems we have experienced or seen in the literature take this organization-centered approach to automating business processes. For example, Action Workflow from Action Technologies[1] operates on a cyclical model where workflow units interoperate to produce customer satisfaction. Different participants are viewed as customers, performers, or observers at each workflow stage of the cycle. While Action Workflow provides client-side functionality to obtain task lists for individuals, it does not provide a structured way for individuals to define personal processes and integrate them into the scope of organizational processes. Another example is the application of groupware-oriented tools such as Lotus Notes to workflow[20]. Notes provides much of the needed infrastructure for managing data and transactions within a workflow. However, again there is no structured way to define personal processes and integrate them into organizational processes. Instead, the approach is again organization-centered, where workflows are defined at the organizational scope,

3.0 Integrated Process Support
We advocate an integrated view of an organization's process space and the personal process spaces of its individual workers. In this view, the organization's workflows are integrated with individual personal process spaces. Section 3.1 discusses this idea in more detail. To support this integrated view, we advocate a component-based approach to process modeling that avoids a reliance on low-level data interchange formats. This approach is called Open Process Components, and is described in Section 3.2. Finally, we propose a generic architecture in Section 3.3 that derives from our integrated view of process. In Section 4.0 we present some Java prototype tools based on our ideas.

and personal tasks derived from the workflow model. Still other workflow platforms, such as InConcert[16], emphasize collaborative aspects of workflow execution. Collaborative work is closer in spirit to the idea of integrated process spaces, but differs in that the emphasis is on mechanisms supporting shared access to data. Users still act on tasks delegated to them by the organizational workflow model. To keep pace with industry trends and technology impacts, this organization-centered viewpoint will have to change in at least the following ways:

· Interoperability between workflows developed across
business functional units and/or organizations must be supported.

Figure 3 shows an agent-centered viewpoint of the process space. Jill is an agent working for Organization A, Bob works for Organization B. Jill participates in Organization A's workflow 1 and 3. Bob participates in Organization B's workflows 3 and 5. In order to accomplish tasks in workflow 1, Jill employs her Personal Process 1. Likewise, Bob employs his Personal Process 3 in carrying out tasks relevant to Workflow 5. In addition, Bob employs Personal Process 3 to carry out similar tasks in the shared Workflow 3. Jill does not have a relevant personal process defined for her assigned tasks in Workflow 3. Finally, each individual may have personal processes defined that are outside the scope of an explicit workflow for either organization. These may be processes defined solely by the individual's personal productivity initiative.
Organization A's Space

· The potential for wide-area distributed participation
must be supported. Personal Process 1

Jill

Personal Space

· Individuals must have the ability to define, execute, and
track the personal processes they perform to be productive within the context of an organization's business processes and goals. The work of the Workflow Management Coalition as well as research efforts such as our Open Process Components Framework (see Section 3.2) address the first two issues directly. However, there has not been a lot of consideration for the last issue. At best, current workflow systems notify individuals of new work items through email or custom client applications. Some even have the ability to write to personal calendaring software through interfaces such as Microsoft and Wang's MAPI-WF[17]. But the viewpoint still originates with the organizational process. An agentcentered viewpoint, showing the distribution of workflows an individual participates in, and the set of personal processes an individual employs, is not considered. The need for supporting the personal process view is just beginning to be recognized in more dynamic process areas such as Software Engineering[11]. In the software process domain, the work of the software developer is considered dynamic in the sense that the developer must be creative in seeking the solutions to design, implementation, and maintenance dilemmas[4]. As workflow extends to more complex and skilled tasks, automated workflow systems will be required to encompass more than just the straightforward document-routing capabilities of image processing systems. Future demands will include the ability to support more of the skilled, or knowledge work, that people perform in the organization. In order to do this, workflow systems must relax the prescriptive constraints it places on performers of the workflow, and allow these workers to perform their own personal processes to carry out the work. Workflow 1 Workflow 3 Workflow 5

Personal Process 2

Personal Process 4 Personal Process 3
Organization B's Space

Bob

Personal Space

FIGURE 3. Personal Process Perspective There are several reasons for arguing for an integrated view of organizational and personal processes. Figure 3 shows the overlap of the personal and organizational process space. Defining and executing business processes is motivated in part by the need to ensure business goals are achieved. Workflows are largely assumed to be static, repetitive processes that involve rote decision-making in support of well-defined business goals[9] 1. To expand the scope of processes automated workflow systems can support, more dynamic workflows that include personal processes should be considered. Another motivating reason comes from the diverse set of relationships in which both organizations and individuals participate. Individual workers, particularly at 1. We refer to Georgakopoulos, Hornick, and Sheth's[9] trade press characterization of administrative and production workflows. Our research is closer to ad hoc workflows, though our point is they can be better understood through an integrated view of the process space.

highly skilled levels, perform in a wide variety of diverse business functions. Downsizing and decentralization of organizations coupled with increasing outsourcing of work makes it unrealistic to take the single organization approach. The business processes of multiple organizations must be integrated with the personal processes of the participants. In order to accomplish this integration, we propose a component-based approach to process modeling and an open architecture for supporting personal and organizational process spaces.

work artifact that is either consumed as input by an Activity or produced as output. A Role is a process-specific definition of the skill set required to perform an Activity. A Role is process-specific as opposed to organization-specific, meaning management must decide how to map organizational roles to process-specific roles. This mapping is the relationship between Roles and Agents. The meta-model described briefly here is adopted from the PCIS LCPS metamodel[7]. However, the concepts are similar in a variety of general descriptions of process in the literature[5][9][15][22]. In the OPC Framework, this set of process entities and relationships form the basis for meaningful component interactions. The second important aspect of the OPC Framework is a state-based encapsulation of execution interfaces. By this we mean each component in a process model possesses a process state, and this state is manipulable by a set of interfaces to the component that are available during various stages of executing the process model. Example interfaces include start, suspend, resume, abort, completeWithFailure and completeWithSuccess. Each component maintains an explicit, independent state during execution of the process model, and the state of process execution at any point in time is the combination of states of the components involved in the process. The final salient feature of the OPC Framework is a threetiered object-oriented class hierarchy for defining components. An object-oriented methodology provides several advantages: encapsulation of heterogeneous process representations, an economy of representation through inheritance, and the ability to specialize component definitions through subclassing. From a process modeling perspective, one major advantage of the hierarchy is its ability to be extended. New component definitions and abstractions can be added within the framework without modifying preexisting definitions. A second important advantage is that specialized component definitions allow heterogeneous process modeling formalisms to interoperate with one another. For example, a Petri-net based process model fragment can interoperate with a process model fragment developed in a scripting language by encapsulating each as a component under the framework. This is especially beneficial in the organizational/personal context of processes we consider in this paper since it should not be assumed that homogeneous process models are generated across these contexts. As a brief example, consider the ad hoc workflow depicted in Figure 4, taken from [9]. This workflow represents a paper review process. In a component-based process model, each task in the workflow is represented as an activity component. Interactions between the components is governed by the set of interfaces each component supports. The benefit is that the implementation of each component is separated from these interfaces. Different process modeling and

3.2 Component-based Process Modeling
Organizations developing standards in workflow and calendaring focus on low-level data interchange protocols to be applied in a client-server environment. The development of such protocols, particularly the protocols related to workflow definition interchange1, are too restrictive to ensure widespread adoption. Instead, we propose an object-oriented component-based approach to process modeling and execution. In our research we are developing a componentbased framework for process modeling called the Open Process Components (OPC) Framework. It is not the focus of this paper to delve into the details of the OPC Framework, but we do provide a brief discussion relevant to the process support architecture presented in Section 3.3. Further details may be found in [8]. There is a need for a unifying framework for representing and manipulating workflow abstractions. We take an objectoriented approach we call Open Process Components. Entities of the workflow domain are represented as objects, with manipulations of those objects defined as object behaviors. The approach is component-based, from the perspective that interfaces are well-defined so that components interact in meaningful ways. The OPC Framework provides a foundation for constructing component-based process models in an extendable fashion. There are three important aspects to the OPC Framework that allow it to support component-based process modeling. The first is a meta-model that identifies basic process entities and relationships between entities. Basic process entities include Process, Activity, Product, Role, and Agent. A Process is a decomposable entity into subprocesses and subactivities. This allows development of process models in a top-down fashion. An Activity is an executable fragment of a process model; it represents a refinement of a portion of a process model down to an executable state. A Product is a

1. More specifically, the Workflow Process Definition Language proposed in WAPI 1[24].

enactment services can be used to define and execute the details of each task. This differs from existing systems where homogeneous models and supporting services are employed. The workflow in Figure 4 is a relevant example of the utility of integrated organizational and personal process spaces. Consider for example the "Review" tasks in the workflow. These are assigned to separate persons fulfilling the role of Reviewer. However, there is not sufficient detail in this definition to automate the support of review activities for each reviewer. Furthermore, it is not appropriate to believe that this organizational workflow should provide such detail. Instead, it is more natural that each reviewer perform a personalized review process that meets the requirements of the organizational workflow. Therefore, if Jill and Bob were Reviewers in this workflow, each would carry out the review according to her/his own personal process for reviewing papers, employing familiar tools and methods for producing the needed results.

3.3 A Process Support Architecture
To support the integration of the organizational and personal process spaces, we propose an architecture that extends traditional workflow client-server architectures to include support for the personal process space. Figure 6 shows the proposed general architecture.

Process Definition Tools

Personal Process Servers

Workflow Servers

Calendar Manager

Select reviewers Review request 2 Distribute Papers Review 1 Review 2 Produce joint reviews Forward review Review 3

Review request 1

Review request n

Worklist Handler

Calendar Tool

FIGURE 6. Process Support Architecture The architecture in Figure 6 integrates organizations' workflow servers and personal process servers with calendaring technology to produce a time-oriented view of work for the end-user. Arcs indicate the bidirectional flow of components over the architecture. This architecture extends traditional workflow architectures, such as the Workflow Management Coalition's Reference Model[22], to include the end user's personal process space. The components of this architecture are:

· Process Definition Tools
FIGURE 4. Example Workflow (taken from [9) Component-based process modeling is at the heart of our research and relevant to the topics discussed in the rest of this paper. However, the elements of organizational versus personal process spaces and process architecture we discuss do not necessarily rely on a component-based approach. One can readily envision modifications to existing tools such as Action Workflow or Lotus Notes discussed earlier that would address process space integration. We encourage the reader to consider process modeling approaches and process space integration issues as independently as possible. Process Definition Tools are used to create componentbased process models. These tools may query Personal Process and Workflow servers in order to reuse existing process component definitions.

· Workflow Servers
One or more servers create the organizational process space(s). These servers manage component-based workflow models created for organizational units by Process Definition Tools.

· Personal Process Servers
Similar to a Workflow Server, a Personal Process Server manages process definitions for individuals, created from components by Process Definition Tools.

· Calendar Manager
The Calendar Manager is the organizer of an individual's process space. The Calendar Manager manages instances of process models from the individual's perspective.

Servers to communicate directly to negotiate over rights to assign work to an individual. The proposed architecture is process model independent. It does not favor any particular representation of process. However, we again advocate the use of component-based process models. Component-based process modeling allows for easier integration of organizational and personal process spaces in the Process Definition Tools and Calendar Managers. Without components, there would be a push on each tool to support low-level protocols allowing for heterogeneous process models to be integrated. This is just the type of interoperability that is deficient in current workflow systems, and a major motivating force behind the componentbased approach to process modeling described in Section 3.2. We have developed a set of Java tools realizing the proposed architecture. In the next section we present our progress with this project.

· Worklist Handler/Calendar Tool
This is a client-side tool that presents the individual with her/his work to do. This may be in the form of a task list, or may be a time-oriented view depending on process constraints and personal scheduling preferences. This general architecture clearly shows the separation and integration of organizational and personal process spaces. The distinct servers manage personal and organization processes. This distinction is a logical one; in practice a single implemented server may include the functionality to manage both process spaces. Integration of the spaces comes from the Process Definition Tools and the Calendar Manager. A Process Definition Tool creates component-based process models. By accessing the process definitions on both servers, the tool is able to create and reuse organizational process that utilizes process specifications of relevant individuals. The Calendar Manager integrates instances of organizational and personal processes from the individual's perspective. The Calendar Manager has the ability to accept or decline work requests from process servers, or manage changes to the individual's process space when forced to do so. This tool is the focal point of the individual's process space. Finally, the Worklist Handler/Calendar Tool is a combination of a workflow client and a personal calendaring tool. This client-side tool has the ability to host process components and support the enactment of such components in order to carry out the actual work. The architecture we propose is an integration of current workflow architectures such as the WfMC's Reference Model[22] and calendaring environments such as Netscape's Calendar Server[18]. However, current architectures do not take such an integrated view. We know of no tool that allows for process models to be created that integrate a workflow model and a personal process model. The proposed process definition tool allows for this integration. We know of no environment that provides a componentized personal view of process like the proposed Calendar Manager. One can envision workflow servers writing to an individual's calendar through an interface such as Microsoft and Wang's MAPI-WF interface[17]. However, this requires that the workflow server have explicit knowledge and access rights to individuals' calendars. The proposed Calendar Manager explicitly manages an individual's workspace, negotiating between servers and individual preferences to present the personal process space to the end user. The existence of such a tool enables a component-based architecture that does not require Personal Process and Workflow

4.0 The Current Prototype
The YFPPG Research Group at Arizona State University has sponsored a series of Master's projects during the Spring 1997 semester for developing a toolset in Java for component-based process modeling and enactment. This toolset conforms closely to the general architecture presented in Section 3.3. The specific architecture is shown in Figure 7.

Repository Browser Process Component Repository

Components Editor Worklist Handler

Calendar Manager Server

Calendar Client

FIGURE 7. OPC Support Architecture The components of this architecture are:

· Process Component Repository
This is implemented as a Java RMI[22] server that uses Java Serialization facilities to distribute process objects to client tools. The repository stores component-based process definitions and distributed components for

enactment. Multiple named repositories, each storing multiple process models, can be managed by a single server.

· Calendar Manager Server
Java RMI and CORBA1 versions of this server exist. This server stores time-oriented appointments as well as task lists for individuals.

· Repository Browser
The Repository Browser is a process administration and management tool that allows users to browse through the current objects in a repository. This is implemented as a Java RMI client.

· Components Editor
The Components Editor is another Java RMI client. It allows users to graphically create component-based process models through component creation and reuse. Figure 8 shows the Components Editor GUI with our example process definition from Figure 4. FIGURE 8. Components Editor

· Worklist Handler
This client-side tool obtains work items for a user from a repository. The work items are actually Java objects that are serialized and obtained through Java RMI calls. Once the Worklist Handler obtains these objects, it can execute them, changing the state of the process model and invoking tools on work products. Figure 9 shows a Worklist Handler for Bob.

· Calendar Client
The Calendar Client obtains the appointments and task lists for an individual from a Calendar Manager Server. In addition, the Calendar Client can bring up a Worklist Handler to access the Process Component Repository. Java RMI and CORBA versions of this tool exist.

FIGURE 9. Worklist Handler At this point in the development of our toolset we have yet to implement the full envisioned functionality of the Calendar Manager Server. The overlap of the organizational and personal process space occurs in the Calendar Client, which is responsible for providing the integrated view of the two spaces. The next step is to implement the full negotiation between the two servers, as we discuss in the next section. We have already learned several lessons during the development and use of this toolset. On the plus side, these tools successfully demonstrate the integration of organizational and personal process spaces. These tools are also demonstrations of forward-looking component distribution technologies such as Java RMI[22] and CORBA[19]. Finally, these tools demonstrate the utility of component-based process modeling. There have been some hiccups however. Managing migrating components in a distributed environment is a difficult configuration management problem. It has proven troublesome to track distributed process components' states and synchronize updates to process models stored in the repository. Despite these problems, we are excited by the possibilities of distributed, component-based process modeling, and are initiating a new set of projects to

1. Iona Technologies' OrbixWeb[13] was used to implement the CORBA-enabled calendar server and client.

update the current environment. Readers interested in obtaining the prototypes or tracking progress of this project may visit the YFPPG website at http://www.eas.asu.edu/ ~yfppg.

6.0 References
[1.] Action Technologies, Inc. Coordination Software: Enabling the Horizontal Corporation. Action Technologies, Inc. White Paper. July, 1994. Alonso, G. and Schek, H. Research Issues in Large Workflow Management Systems. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996. Armitage, J. and Kellner, M. A Conceptual Schema for Process Definitions and Models. Proceedings of the Third International Conference on the Software Process (ICSP3), pp. 153-165, Reston, VA. October, 1994. Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. Conradi, R., Liu, C., and Hagaseth, M. Planning Support for Cooperating Transactions in EPOS. Information Sciences, vol. 20, no. 4, pp. 317-336. 1995. Curtis, B., Kellner, M., and Over, J. Process Modeling. Communications of the ACM, vol. 35, no. 9, pp. 75-90, September, 1992. Derniame, J.C. Life Cycle Process Support in PCIS. Proceedings of the PCTE `94 Conference. 1994. Gary, K., Lindquist, T., and Koehnemann, H. Component-based Process Modeling. Technical Report TR97-022, Computer Science Department, Arizona State University. May, 1997. Georgakopoulos, D., Hornick, M., and Sheth, A. An Overview of Workflow Management: From Process Modeling to Workflow Automation Infrastructure. Distributed and Parallel Databases, vol. 3, pp. 119153. 1995.

5.0 Summary and Future Work
In this paper we have advocated an integrated view of organizational workflows and personal process spaces. In this view, both the perspective of the organization and the perspective of the individual are considered when integrating process spaces. This view allows organizational goals to be pursued while allowing individual workers the flexibility to define how to accomplish such goals. Such flexibility will be required in the not-too-distant future due to the increasing demands on current workflow systems and the current pace of technology. In this paper we proposed a generic architecture for process support that logically integrates functionality needed for both perspectives. We suggest a component-based process modeling approach to further reduce the dependencies between workflow and calendaring systems by avoiding the need for low-level, brittle data interchange protocols. Finally, we described a set of prototype tools based on component-based process modeling that realizes the generic architecture. Despite the success or failure of our efforts, we hope that the argument for integrated organizational and personal process spaces will have an effect on future considerations in the converging areas of workflow and groupware research. Given the relatively early stage of this research, there are several avenues we intend to pursue in this area. First, further research is needed to fully understand the nature of the negotiation between organizational and personal process spaces that takes place in the Calendar Manager. We are pursuing research in this area under the topic Process Component Brokering, where such negotiation is carried out by having the Calendar Manager provide a brokering service that identifies personal process components that meet organizational process requirements. Second, we are looking at ways to integrate automated planning and scheduling techniques for workflow and personal processes. The result will be enhanced Calendar Managers that negotiate with organizations Workflow Servers to optimize the overlap between organizational and personal process execution. Finally, we plan to validate the proposed architecture by employing our tools in real workflow settings, and extending our work into more dynamic process areas. Specifically, we are looking at ways to support Personal Software Processes and Distributed Learning processes between mentors and students.

[2.]

[3.]

[4.]

[5.]

[6.]

[7.] [8.]

[9.]

[10.] Godart, C., Canals, G., Charoy, F., and Molli, P. An Introduction to Cooperative Software Development in COO. International Conference on System Integration, 1994. [11.] Humphrey, W. The Personal Process in Software Engineering. Proceedings of the Third International Conference on the Software Process (ICSP-3). IEEE Press. October, 1994. [12.] Internet Mail Consortium. vCalendar V1.0 Specification. Available at http://www.imc.org/pdi/pdiproddev.html [13.] Iona Technologies. OrbixWeb 2.0 Programming Guide. November 1995. [14.] Khoshafian, S., and Buckiewicz, M. Introduction to Groupware, Workflow, and Workgroup Computing.

J. Wiley and Sons, New York. 1995. [15.] Lee, J. Gruniger, M., Jin, Y., Malone, T., and Yost, G. The PIF Process Interchange Format and Framework. Available at http://www.aiai.ed.ac.uk/pif/index.html. May 24, 1996. [16.] McCarthy, D. and Sarin, S. Workflow and Transactions in InConcert. IEEE Bulletin of the Technical Committee on Data Engineering, vol. 16 no. 2. June 1993. [17.] Microsoft Corporation and Wang Laboratories, Inc. Microsoft MAPI Workflow Framework Concepts and Facilities (White Paper). Available at http:// www.wang.com/sbu/w9602210.htm. February 21, 1996. [18.] Netscape Communications Corporation. Netscape Calendar Server 1.0 and 2.0. Available at http:// home.netscape.com/comprod/server_central/product/calendar/calendar2_data.html. [19.] Object Management Group. Corba 2.0 Specification. Available at http://www.omg.org/corbask.htm. July 1995. [20.] Reinwald, B and Mohan, C. Structured Workflow Management with Lotus Notes release 4. Proceedings of the 41st IEEE CompCon digest of papers, pp.451457, Santa Clara, CA. February, 1996. [21.] Riddle, W. E. Fundamental Process Modeling Con-

cepts. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996. [22.] Sun Microsystems, Inc. Remote Method Invocation Specification. Available at http://www.javasoft.com:80/products/jdk/1.1/docs/guide/rmi/spec/ rmiTOC.doc.html [23.] The Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003, January 1995. [24.] The Workflow Management Coalition. Interface 1: Process Definition Interchange. WfMC Document Number TC-1016, Version 1.0 Beta. May 29, 1996. [25.] The Workflow Management Coalition. Interface 2 Specification. WfMC Document Number TC-1009, Version 1.0. November 20, 1995. [26.] The Workflow Management Coalition. Interoperability Abstract Specification. WfMC Document Number TC-1012, Version 1.0. October 20, 1996. [27.] The Workflow Management Coalition. Draft Audit Specification. WfMC Document Number TC-1015. August 14, 1996.

1

On time-reversibility of linear stochastic models
Tryphon T. Georgiou and Anders Lindquist

Abstract Reversal of the time direction in stochastic systems driven by white noise has been central throughout the development of stochastic realization theory, filtering and smoothing. Similar ideas were developed in connection with certain problems in the theory of moments, where a duality induced by time reversal was introduced to parametrize solutions. In this latter work it was shown that stochastic systems driven by arbitrary second-order stationary processes can be similarly time-reversed. By combining these two sets of ideas we present herein a generalization of time-reversal in stochastic realization theory.

arXiv:1309.0165v1 [cs.SY] 31 Aug 2013

I. I NTRODUCTION Time reversal of stochastic systems is central in stochastic realization theory (see, e.g., [1], [2], [3], [4], [5], [6], [7], [8]), filtering (see [9]), smoothing (see [10], [11], [12]) and system identification. The principal construction is to model a stochastic process as the output of a linear system driven by a noise process which is assumed to be white in discrete time, and orthogonal-increment in continuous time. In studying the dependence between past and future of the process, it is natural to decompose the interface between past and future in a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward or backward in time. In a different context (see [13]) a certain duality between the two time-directions in modeling a stochastic process was introduced in order to characterize solutions to moment problems. In this new setting the noise-process was general (not necessarily white), and the correspondence between the driving inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics. In the present note, we combine these two sets of ideas to develop a general framework where two time-opposite stochastic systems model a given stochastic process. We study the relationship between these systems and the corresponding processes. In particular, we recover as a special case certain results of stochastic realization theory ([1], [5], [10]) from the 1970's using a novel procedure. In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving generating processes, in opposite time-directions, via causal and anti-causal mappings, respectively. In Section III we utilize this mechanism in the context of general output processes and, similarly, introduce a pair of time-opposite models. Finally, in Section IV, we draw connection to literature on time reversibility and related issues in physics, and we indicate directions for future research. II. S TATE
DYNAMICS AND ALL - PASS EXTENSION

In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. As usual, in discrete-time these take the form of a set of difference equations x(t + 1) = Ax(t) + Bu(t) (1)

This research was supported by grants from AFOSR, NSF, VR, and the SSF. Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota, tryphon@umn.edu Department of Automation, Shanghai Jiao Tong University, Shanghai, China, and Center for Industrial and Applied Mathematics and ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden, alq@kth.se

2

where t  Z, A  Rn×n , B  Rn×p , n, p  N, A has all eigenvalues in the open unit disc D = {z | |z | < 1}, and u(t), x(t) are stationary vector-valued stochastic processes. The system of equations is assumed to be reachable, i.e., rank B, AB, . . . An-1 B = n, and non-trivial in the sense that B is full rank. In continuous-time, state-dynamics take the form of a system of stochastic differential equations dx(t) = Ax(t)dt + Bdu(t) (3) (2)

where, here, u(t), x(t) are stationary continuous-time vector-valued stochastic processes. Reachability (which in this case, is equivalent to controllability) of the pair (A, B ) is also assumed throughout and the condition for this is identical to the one for discrete-time given above (as is well known). In continuous time, stability of the system of equations is equivalent to A having only eigenvalues with negative real part, and will be assumed throughout along with the condition that B has full rank. In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system is all-pass. This is done next. The assumptions of stationarity and constant parameter matrices is made for simplicity of notation and brevity and can be easily removed. A. All-pass extension in discrete-time Consider the discrete-time Lyapunov equation P = AP A + BB  . (4)

Since A has all eigenvalues inside the unit disc of the complex plane and (2) holds, (4) has as solution a matrix P which is positive definite. The state transformation  = P - 2 x, and F = P - 2 AP 2 , G = P - 2 B, brings (1) into  (t + 1) = F  (t) + Gu(t). (7)
1 1 1 1

(5)

(6)

For this new system, the corresponding Lyapunov equation X = F XF  + GG has In as solution, where In denotes the (n × n) identity matrix. This fact, namely, that In = F F  + GG implies that this [F, G] can be embedded as part of an orthogonal matrix U= i.e., such that UU  = U  U = In+p . Define the transfer function U(z ) := H (zIn - F )-1 G + J (10) F G H J , (9) (8)

3

corresponding to  (t + 1) = F  (t) + Gu(t) u ¯(t) = H (t) + Ju(t). This is also the transfer function of x(t + 1) = Ax(t) + Bu(t) ¯  x(t) + Ju(t), u ¯(t) = B
1 ¯ := P - 2 where B H , since the two systems are related by a similarity transformation. Hence,

(11a) (11b)

(12a) (12b)

¯  (zIn - A)-1 B + J. U(z ) = B

(13)

We claim that U(z ) is an all-pass transfer function (with respect to the unit disc), i.e., that U(z ) is a transfer function of a stable system (obvious) and that U(z )U(z -1 ) = U(z -1 ) U(z ) = Ip . The latter claim is immediate after we observe that, since U  U = In+p , U and hence,  (t) = F   (t + 1) + H  u ¯(t)   u(t) = G  (t + 1) + J u ¯(t) or, equivalently, x(t) = P AP -1 x(t + 1) + P 2 H  u(t) u(t) = B  P -1 x(t + 1) + J  u ¯(t). Setting x ¯(t) := P -1x(t + 1), (16) can be written ¯u x ¯(t - 1) = A x ¯(t) + B ¯(t)   u(t) = B x ¯(t) + J u ¯(t) with transfer function ¯ + J . U(z ) = B  (z -1 In - A )-1 B Either of the above systems inverts the dynamical relation u  u ¯ (in (12) or (11)). (19) (18a) (18b) (17)
1

(14)

 (t + 1) u ¯(t)

=

 (t) u(t)

,

(15a) (15b)

(16a) (16b)

u(t )

U

u ¯(t) 

Fig. 1.

Realization (12) in the forward time-direction.

4



u(t)

U



u ¯(t)

Fig. 2.

Realization (18) in the backward time-direction.

B. All-pass extension in continuous-time Consider the continuous-time Lyapunov equation AP + P A + BB  = 0. (20)

Since A has all its eigenvalues in the left half of the complex plane and since (2) holds, (20) has as solution a positive definite matrix P . Once again, applying (5-6), the system in (3) becomes d (t) = F  (t)dt + Gdu(t). We now seek a completion by adding an output equation du ¯(t) = H (t)dt + Jdu(t) so that the transfer function U(s) := H (sIn - F )-1 G + J is all-pass (with respect to the imaginary axis), i.e., U(s)U(-s) = U(-s) U(s) = Ip . (23) (22) (21b) (21a)

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and hence, F + F  + GG = 0. Utilizing this relationship we note that (sIn - F )-1 GG (-sIn - F  )-1 = (sIn - F )-1 (sIn - F - sIn - F  )(-sIn - F  )-1 = (sIn - F )-1 + (-sIn - F  )-1 , and we calculate that U(s)U(-s) = (H (sIn - F )-1 G + J )(G (-sIn - F  )-1 H  + J  ) = JJ  + H (sIn - F )-1 (GJ  + H  ) (JG + H )(-sIn - F  )-1 H  . For the product to equal the identity, JJ  = Ip H = -JG . Thus, we may take J = Ip H = -G , (24)

5

and the forward dynamics d (t) = F  (t)dt + Gdu(t) du ¯(t) = -G  (t)dt + du(t). Substituting F = -F  - GG from (24) into (25a) we obtain the reverse-time dynamics d (t) = -F   (t)dt + Gdu ¯(t)  du(t) = G  (t)dt + du ¯(t). Now defining x ¯(t) := P -1x(t) and using (5) and (6), (26) becomes ¯ u dx ¯(t) = -A x ¯(t)dt + Bd ¯(t)  du(t) = B x ¯(t)dt + du ¯(t), with transfer function ¯ + Ip , U(s) = B  (sIn + A )-1 B where ¯ := P -1 B. B (29) (30) (28a) (28b) (27) (26a) (26b) (25a) (25b)

Furthermore, the forward dynamics (25) can be expressed in the form dx(t) = Ax(t)dt + Bdu(t) ¯  x(t)dt + du(t) du ¯(t) = B with transfer function ¯  (sIn - A )-1 B + Ip . U(s) = B III. T IME - REVERSAL
OF LINEAR STOCHASTIC SYSTEMS

(31a) (31b)

(32)

The development so far allows us to draw a connection between two linear stochastic systems having the same output and driven by a pair of arbitrary, but dual, stationary processes u(t) and u ¯(t), one evolving forward in time and one evolving backward in time. When one of these two processes is white noise (or, orthogonal increment process, in continuous-time), then so is the other. For this special case we recover results of [1] and [5] in stochastic realization theory. A. Time-reversal of discrete-time stochastic systems Consider a stochastic linear system x(t + 1) = Ax(t) + Bu(t) y (t) = Cx(t) + Du(t) (33a) (33b)

with an m-dimensional output process y , and x, u, A, B are defined as in Section II-A. All processes are stationary and the system can be thought as evolving forward in time from the remote past (t = -). In particular, x(t + 1) is Ftu -measurable y (t)

6

for all t  Z, where Ftu is the  -algebra generated by {u(s) | s  t}. Next we construct a stochastic system ¯u x ¯(t - 1) = A x ¯(t) + B ¯(t) ¯x ¯u y (t) = C ¯(t) + D ¯(t), which evolves backward in time from the remote future (t = ), and for which x ¯(t - 1) y (t)
¯ ¯u is F t -measurable

(34a) (34b)

¯ ¯u for all t  Z, where F ¯(s) | s  t}. The processes x ¯, x, u ¯, u relate as in t is the  -algebra generated by {u the previous section. More specifically, as shown in Section II-A,

u ¯(t) is Ftu -measurable while
¯ ¯u u(t) is F t -measurable

for all t, as examplified in Figures 1 and 2. In fact, the all-pass extension (12) of (33a) yields ¯  x(t) + Ju(t) u ¯(t) = B It follows from (18b) that (35) can be inverted to yield u(t) = B  x ¯(t) + J  u ¯(t), where x ¯(t) = P -1 x(t + 1), and that we have the reverse-time recursion ¯u x ¯(t - 1) = A x ¯(t) + B ¯(t). Then inserting (36) and into (33b), we obtain ¯ := DJ  and where D ¯u x(t) = P x ¯(t - 1) = P A x ¯(t) + P B ¯(t) ¯x ¯u y (t) = C ¯(t) + D ¯(t), ¯ := CP A + DB  . C (37b) (38) (37a) (36) (35)

Then, (37) is precisely what we wanted to establish. Moreover, the transfer functions W(z ) = C (zIn - A)-1 B + D of (33) and ¯ (z ) = C ¯ (z -1 In - A )-1 B ¯ +D ¯ W of (34) satisfy ¯ (z )U(z ). W (z ) = W (41) (40) (39)

In the context of stochastic realization theory, discussed next, U(z ) is called structural function ([3], [4]).

7

u(t )

W

y (t) 

Fig. 3.

The forward stochastic system (33).



y (t)

¯ W



u ¯(t)

Fig. 4.

The backward stochastic system (34)

1) Time-reversal of stochastic realizations.: Given an m-dimensional stationary process y , consider a minimal stochastic realization (33), evolving forward in time, where now u is a normalized white noise process, i.e., E{u(t)u(s)} = Ip t-s . Since U, given by (13), is all-pass, u ¯ is also a normalized white noise process, i.e., E{u ¯(t)¯ u(s) } = Ip t-s . From the reverse-time recursion (34a)


x ¯(t) =

¯u (A )k-(t+1) B ¯ (k ).
k =t+1

Since, u ¯ is a white noise process, E{x ¯(t)¯ u(s) } = 0 for all s  t. Consequently, (34) is a backward stochastic realization in the sense of stochastic realization theory. B. Time-reversal of continuous-time stochastic systems We now turn to the continuous-time case. Let dx = Axdt + Bdu dy = Cxdt + Ddu (42a) (42b)

be a stochastic system with x, u, A, B as in Section II-B, evolving forward in time from the remote past (t = -). All processes have stationary increments and x(t) y (t) is Ftu -measurable

for all t  R, where Ftu is the  -algebra generated by {u(s) | s  t}. The all-pass extension of Section II-B yields ¯  xdt du ¯ = du - B as well as the reverse-time relation ¯ u dx ¯ = -A x ¯dt + Bd ¯  du = B x ¯dt + du ¯, (44a) (44b) (43)

8

where x ¯(t) = P -1 x(t). Inserting (44b) into dy = CP x ¯dt + Ddu yields ¯x dy = C ¯dt + Ddu ¯, where ¯ = CP + DB  . C Thus, the reverse-time system is ¯ u dx ¯ = -A x ¯dt + Bd ¯ ¯x dy = C ¯dt + Ddu ¯. From this, we deduce that x ¯(t) y (t)
¯ ¯u is F t -measurable

(45)

(46a) (46b)

for all t  R. We also note that the transfer function W(s) = C (sIn - A)-1 B + D of (42) and the transfer function ¯ ( s) = C ¯ (sIn + A )-1 B ¯ +D W of (46) also satisfy ¯ (s)U(s) W ( s) = W as in discrete-time. 1) Time-reversal of stochastic realizations.: In continuous-time stochastic realization theory, (42) is a forward minimal stochastic realization of an m-dimensional process y with stationary increments provided u is a normalized orthogonal-increment process satisfying E{du(t)du(t)} = Ip dt. Since U(s) is all-pass, ¯  xdt du ¯ = du - B also defines a stationary orthogonal-increment process u ¯ such that E{du ¯(t)du ¯(t) } = Ip dt. It remains to show that (46) is a backward stochastic realization, that is, at each time t the past increments of u ¯ are orthogonal to x ¯(t). But this follows from the fact that


(47)

x ¯(t) =
t

¯ u e-A (t-s) Bd ¯ ( s)


and u ¯ has orthogonal increments.

9

IV. C ONCLUDING

REMARKS

The direction of time in physical laws and the fact that physical laws are symmetric with respect to time have occupied some of the most prominent minds in science and mathematics ([14], [15], [16]). These early consideration were motivated by no less an issue than that of the very nature of the quantum. Indeed, Erwin Schr¨ odinger's aim appears to have been to draw a classical analog to his famous equation. A large body of work followed. In particular, closer to our immediate interests, dual time-reversed models have been employed to model, in different time-directions, Brownian or Schr¨ odinger bridges (see [17], [18]), a subject which is related to reciprocal processes ([19], [20], [21], [22]). The topic of time reversibility has also been central to thermodynamics, and in recent years studies have sought to elucidate its relation to systems theory (see [23], [24]). Possible connections between this body of work and our present paper will be the subject of future work. The thesis of the present work is that under mild assumptions on a stochastic process, any model that consists of a linear stable dynamical system driven by an appropriate input process can be reversed in time. In fact, a reverse-time dual system along with the corresponding input process can be obtained via an all-pass extension of the state equation. The correspondence between the two input processes can be expressed in terms of each other by a causal and an anti-causal map, respectively. The formalism of our paper can easily be extended to a non-stationary setting at a price of increased notational, but not conceptual, complexity. Informally, and in order to underscore the point, if u(t) is a non-stationary process and the linear system is time-varying, under suitable conditions, a reverse-time system and a process u ¯(t) can be similarly constructed via a time-varying orthogonal transformation. R EFERENCES
[1] A. Lindquist and G. Picci, "On the stochastic realization problem," SIAM J. Control Optim., vol. 17, no. 3, pp. 365­389, 1979. [2] ----, "Forward and backward semimartingale models for Gaussian processes with stationary increments," Stochastics, vol. 15, no. 1, pp. 1­50, 1985. [3] ----, "Realization theory for multivariate stationary Gaussian processes," SIAM J. Control Optim., vol. 23, no. 6, pp. 809­857, 1985. [4] ----, "A geometric approach to modelling and estimation of linear stochastic systems," J. Math. Systems Estim. Control, vol. 1, no. 3, pp. 241­333, 1991. [5] M. Pavon, "Stochastic realization and invariant directions of the matrix Riccati equation," SIAM Journal on Control and Optimization, vol. 18, no. 2, pp. 155­180, 1980. [6] A. Lindquist and M. Pavon, "On the structure of state-space models for discrete-time stochastic vector processes," IEEE Trans. Automat. Control, vol. 29, no. 5, pp. 418­432, 1984. [7] G. Michaletzky, J. Bokor, and P. V´ arlaki, Representability of stochastic systems. Budapest: Akad´ emiai Kiad´ o, 1998. [8] G. Michaletzky and A. Ferrante, "Splitting subspaces and acausal spectral factors," J. Math. Systems Estim. Control, vol. 5, no. 3, pp. 1­26, 1995. [9] A. Lindquist, "A new algorithm for optimal filtering of discrete-time stationary processes," SIAM J. Control, vol. 12, pp. 736­746, 1974. [10] F. Badawi, A. Lindquist, and M. Pavon, "On the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear stochastic systems," in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE, 1979, pp. 505­510. [11] F. A. Badawi, A. Lindquist, and M. Pavon, "A stochastic realization approach to the smoothing problem," IEEE Trans. Automat. Control, vol. 24, no. 6, pp. 878­888, 1979. [12] A. Ferrante and G. Picci, "Minimal realization and dynamic properties of optimal smoothers," Automatic Control, IEEE Transactions on, vol. 45, no. 11, pp. 2028­2046, 2000. [13] T. T. Georgiou, "The Carath´ eodory­Fej´ er­Pisarenko decomposition and its multivariable counterpart," Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 212­228, 2007. ¨ [14] E. Schr¨ odinger, Uber die Umkehrung der Naturgesetze. Akad. d. Wissenschaften, 1931. [15] A. Kolmogorov, Selected Works of AN Kolmogorov: Probability theory and mathematical statistics. Springer, 1992, vol. 26. [16] A. Shiryayev, "On the reversibility of the statistical laws of nature," in Selected Works of AN Kolmogorov. Springer, 1992, pp. 209­215. [17] M. Pavon and A. Wakolbinger, "On free energy, stochastic control, and Schr¨ odinger processes," in Modeling, Estimation and Control of Systems with Uncertainty. Springer, 1991, pp. 334­348. [18] P. Dai Pra and M. Pavon, "On the Markov processes of Schr¨ odinger, the Feynman-Kac formula and stochastic control," in Realization and Modelling in System Theory. Springer, 1990, pp. 497­504.

10

[19] B. Jamison, "Reciprocal processes," Probability Theory and Related Fields, vol. 30, no. 1, pp. 65­86, 1974. [20] A. Krener, "Reciprocal processes and the stochastic realization problem for acausal systems," in Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197­211. [21] B. C. Levy, R. Frezza, and A. J. Krener, "Modeling and estimation of discrete-time gaussian reciprocal processes," Automatic Control, IEEE Transactions on, vol. 35, no. 9, pp. 1013­1023, 1990. [22] P. Dai Pra, "A stochastic control approach to reciprocal diffusion processes," Applied mathematics and Optimization, vol. 23, no. 1, pp. 313­329, 1991. [23] W. M. Haddad, V. Chellaboina, and S. G. Nersesov, "Time-reversal symmetry, poincar´ e recurrence, irreversibility, and the entropic arrow of time: From mechanics to system thermodynamics," Nonlinear Analysis: Real World Applications, vol. 9, no. 2, pp. 250­271, 2008. [24] ----, Thermodynamics: A dynamical systems approach. Princeton University Press, 2009.

CANONICAL CORRELATION ANALYSIS, APPROXIMATE COVARIANCE EXTENSION, AND IDENTIFICATION OF STATIONARY TIME SERIES*
ANDERS LINDQUIST AND GIORGIO PICCI

Abstract. In this paper we analyze a class of state-space identification algorithms for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called "subspace methods" can be described as covariance estimation followed by stochastic realization. The methods offer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identification into the solution of a Riccati equation but introduce at the same time some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identification procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not explicitly reported. In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We review the basic ideas of stochastic realization theory in the context of identification, discuss the concept of stochastic balancing and of stochastic model reduction by principal subsystem truncation. The model reduction method of Desai and Pal, based on truncated balanced stochastic realizations, is partially justified, showing that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing conditions under which the "subspace identification" methods produce bona fide stochastic systems.

1. Introduction Recently there has been a renewed interest in state-space identification algorithms for time series based on a two steps procedure which in principle can be described as estimation of a rational covariance model from observed data followed by stochastic realization. The method offers the major advantage of converting the nonlinear parameter estimation phase which is necessary in traditional ARMA models identification into a partial realization problem, involving a Hankel matrix of estimated
 This research was supported in part by grants from TFR, the G¨ oran Gustafsson Foundation, the SCIENCE project "System Identification" and LADSEB-CNR.  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden  Dipartimento di Elettronica e Informatica, Universita' di Padova, 35131 Padova, Italy 1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which efficient numerical solution techniques are available. In this framework we can naturally accommodate multivariate processes and there are indications that the algorithms may work also with data containing purely deterministic components (van Overschee and De Moor, 1993). A drawback, however, to be emphasized in this paper, is that, unlike, say, least-squares identification of ARMA models, these methods do not work for arbitrary data. This type of procedure was apparently first advocated by Faurre (1969); see also Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work, based on canonical correlation analysis (Akaike, 1975) (or some other singular-value decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki (1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor, 1993). Numerical experience shows that the computation time needed to get the final model parameters estimates compares very favorably with traditional iterative prediction error methods for ARMA models. On the other hand there is a price to be paid for this simplification. These methods introduce some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic realization arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. Note that positivity is the natural condition insuring solvability of the Riccati equation required to compute state-space models of the signal from the covariance estimates. Central in the procedures described above is the following classical problem of identification of a covariance sequence. Let {0 , 1 , . . . ,  } (1.1)

be a finite set of sample m × m covariance matrices estimated in some unspecified way from a certain m-dimensional sequence of observations {y0 , y1 , y2 , . . . yT }, ¯ = k CAk-1 C and such that the infinite sequence {0 , 1 , 2 , . . . }, (1.4) (1.2)

¯ ) such that and consider the problem of finding a minimal1 triplet of matrices (A, C, C k = 1, 2, . . . ,  (1.3)

¯ for k =  + 1,  + 2, . . . , is a bona fide obtained from (1.1) by setting k := CAk-1 C covariance sequence. In the literature the last condition is generally ignored. The remaining problem of ¯ ) satisfying (1.3) is called the minimal partial realfinding a minimal triplet (A, C, C ¯ ) is usually computed by minimal factorization ization problem. The triplet (A, C, C
1

¯ ) is minimal if (A, C ) is completely observable and (A, C ¯ ) is completely reachable. Here (A, C, C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

3

of a block Hankel matrix corresponding to the data (1.1) as follows:      ¯ C 1 2 C 3 · · · j ¯ 2 3  4 · · · j +1   CA   CA      , H= = . . . . . .  .      . .. . . . . . . . . . . . ¯ (A )j -1 i i+1 i+2 · · · i+j -1 CAi-1 C

(1.5)

where i + j - 1 =  and the Hankel matrix H is chosen as close to square as possible by taking |i - j |  1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j ) such that i + j - 1 =  , and hence the minimal factorization must be made for a choice of (i, j ) in which the Hankel matrix (1.5) has maximal rank. The infinite sequence ¯ for k =  +1,  +2, . . . {0 , 1 , 2 , . . . } obtained in this way by setting k := CAk-1 C is called a minimal rational extension of the finite sequence (1.1) and is in general not a covariance sequence. The dimension r of a minimal rational extension is called the (algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to the McMillan degree of the m × m rational matrix ¯ + 1 0 , (1.6) Z (z ) = C (zI - A)-1 C 2 and the elements of the infinite sequence (1.4) are the coefficients of the Laurent expansion 1 Z (z ) = 0 + 1 z -1 + 2 z -2 + . . . 2 (1.7)

about z = . The underlying identification problem is however a great deal more complicated than the classical partial realization problem. In fact, the requirement that (1.4) be a bona fide covariance sequence amounts to (1.4) being a positive sequence in the sense that, for every t  Z+ , the block Toeplitz matrices Tt ,   2 · · · t 0 1 1 0 1 · · · t-1  , Tt =  (1.8) . . . . ...  . . . . . .  . . t t-1 t-2 · · · 0 formed from the infinite sequence (1.4), be positive definite or, equivalently, that the matrix function (z ) := Z (z ) + Z (1/z ) be positive semidefinite on the unit circle, i.e. (ei )  0   [0, 2 ). (1.10) (1.9)

This property is equivalent to  being a spectral density matrix. In fact, it will be the spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial covariance sequence unless T > 0, but this is not enough. From the point of view of identification there seem to be two possible routes to ¯ ) from the finite covariance sequence (1.1). One that determine a model (A, C, C has been proposed in the literature is do minimal factorization (1.5) of a finite block Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown in this paper, there is no a priori guarantee that this method will yield a positive extension. This fact has nothing to do with sample variability (random fluctuations) of the covariance estimates (1.1), and to emphasize this point we initially assume that all strings of data (1.2) are infinitely long. A theoretically sounder identification method, which will not be considered in this paper, could instead be to do positive extension first and then to use a stochastic model reduction procedure on the triplet ¯ ) of the positive extended sequence. (A, C, C The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this difficulty into consideration have been pointed out by the authors of this paper at many scientific meetings in the last ten years. This has had no apparent effect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identification procedure of Aoki (1990) and demonstrate that there is a hidden, and not easily tested, assumption without which the procedure will not be guaranteed to succeed. The punch line is that none of the subspace identification methods under consideration can be expected to always work for generic data but that some not entirely natural conditions on the data are needed. The analysis of the basic theoretical issues behind subspace identification is carried out in the geometric framework of stochastic realization theory; see, e.g., Lindquist and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this theory and adapt them to the problem of identification. To this end, we first discuss an idealized situation in which the time series (1.2) is infinitely long i.e. T = , and the available covariance data are given by the ergodic limit 1 T  T + 1 lim
T

yt+k yt+j = k-j
t=0

(1.11)

for all k and j . Then the sample estimates in the sequence (1.1) are bona fide covariance matrices and the Toeplitz matrix T formed from the data will be positive definite and symmetric. We introduce a Hilbert space of observed (infinite) strings of data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we establish a correspondence which turns operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. Canonical correlations and balanced stochastic realizations are then analyzed in this setting in Section 4, and the basic concepts and principles used in the subspace identification methods, as well as in the model reduction procedures of Desai and Pal, are translated into the more natural context of geometric stochastic realization theory. Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact infinite covariance sequence (1.4). Consequently, the difficult question of positivity is not an issue here. Nor is it the finite sample size per se which is the problem, but the fact that only a finite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is finite. Therefore we separate these issues by discussing stochastic realization theory from finite covariance data in Section 5 and subspace identification in Section 6. In this framework we show that the method of van Overschee and De Moor (1993) is valid under some rather stringent assumptions. We stress that we are only concerned with identification procedures for state space modeling of time series. "Subspace identification" methods for deterministic systems with measurable inputs or for spectral factors do not involve positivity, but stability may still be a problem. However, the algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part, so the problem of positivity arises here too. Another idea behind the subspace identification methods considered in this paper is to disregard modes corresponding to "small" canonical correlation coefficients. This is called balanced truncation and is in fact a stochastic model reduction procedure. In all such procedures there must be a guarantee that the reduced-degree matrix function (1.6) is positive real, and therefore the preservation of positivity in such reductions is a main concern of this paper. Section 7 is devoted to such issues. The model reduction procedure of Desai and Pal (1982) was never theoretically justified in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 . Here we shall demonstrate that this reduction procedure produces a positive real, but not in general balanced, reduced model structure. In fact, the singular values of the truncated system are usually not equal to the r first singular values of the original system. It is an interesting fact that the procedure of Desai and Pal does produce balanced truncations for continuous-time stochastic systems. A partial result in this direction was given by Harshavardana, Jonckheere and Silverman (1984), who showed that the truncated function is positive real and conjectured that it is balanced. We shall demonstrate that it is indeed balanced, a result that is actually already contained in the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time depends on the fact that the spectral factors of the truncated approximate spectrum behave differently than in continuous time. While in continuous time the realizations of the reduced spectral factors are proper subsystems, obtained by partitioning the matrices of the realizations of the factors of , this is not the case in discrete time, contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced truncation procedure is available in discrete time, but the systems matrices are no longer submatrices of those of the original system, and therefore it is not equivalent to the truncation procedure used in subspace identification. Several of the results of this paper have previously been announced in Lindquist and Picci (1994a)3 and in Lindquist and Picci (1994b).

In Desai et al. (1985) a different model reduction procedure, which is not relevant to subspace identification, is considered, namely "deterministic" model reduction of the minimum phase spectral factors. 3 We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some erroneous statements, was accidentally published in place of the paper finally submitted for publication. The correct version can be obtained from the authors.

2

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances The solution to the minimal partial realization problem , i.e., the problem to find ¯ ) satisfying (1.1) is in general not unique. This lack of uniquethe triplet (A, C, C ness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1) equals that of {0 , 1 , . . . ,  -1 } so that we can use a Hankel matrix (1.5) based allowing us to define the shifted Hankel matrix  3 4 2  3 4 5  (H ) =  . . .  . . . . . . i+1 i+2 i+3 (2.1)

on this data, i.e., with i + j =  ,  · · · j +1 · · · j +2   . ... . .  · · · 

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) pro¯ ) which is unique up to a similarity transformation. duces a minimal solution (A, C, C As first pointed out by Zeiger and McEwen (1974), the minimal factorization on which the Ho-Kalman procedure is based may be performed by singular-value decom¯ ) uniquely; see also Kung (1978). In fact, the Hankel position, thereby fixing (A, C, C matrix H may be factored as H = U V U U = I = V V, (2.3) where  is the square n × n diagonal matrix of the nonzero singular values taken in ¯ := V 1/2 this leads to a factorization decreasing order. Setting  := U 1/2 and  ¯ H =  ¯ ¯ == (2.4)

¯ ) is obtained by solving of the type (1.5). Then a minimal realization (A, C, C ¯ =  (H ), A ¯ = 1 (H ) and C ¯  = 1 (H ), C

where  (H ) is the shifted Hankel matrix (2.2) and 1 (H ) is the first block row of H . ¯ ) must be given by It follows that the triplet (A, C, C A = -1/2 U  (H )V -1/2 , C = 1 (H )V -1/2 , ¯ = 1 (H )U -1/2 , C (2.5a) (2.5b) (2.5c)

a form to which we refer as finite-interval balanced, since it is balanced in the sense ¯ are both equal to , and that ¯ that   and      ¯ C C ¯  CA   CA  ¯   .  = (2.6) = . .    . . .  . ¯ (A )j -1 CAi-1 C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identification of time series. The problem with such a strategy is that this algorithm is a deterministic realization procedure and hence does not a priori insure that (1.6) is positive real, or even stable for that matter, even if the Toeplitz matrix T is positive definite. In fact, it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space of covariance data (1.1) for which A is not stable, and a fortiori the same holds for positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in Aoki (1990) is based on the following hidden assumption which is not entirely natural. Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H . Therefore, not only must we know that there exists an underlying finite-dimensional system, but we must also have some upper bound for its dimension. A conservative ]. upper bound which will always suffice is [  2 Is this assumption natural? If the covariance data are really generated exactly from a "true" stochastic system and there is a reliable estimate of its order which is no more than half of the length of the covariance sequence, then the assumption will hold. However, and this is an important point of this paper, one cannot expect Assumption 2.1 to hold for an arbitrary covariance sequence (1.1). To clarify this point, let us agree to call {0 , 1 , 2 , . . . } a minimal rational extension of {0 , 1 , . . . ,  } if the rational function (1.7) has minimal degree. By definition this is the algebraic degree of {0 , 1 , . . . ,  }. A rational extension is called positive if, for every µ >  , the block Toeplitz matrices Tµ formed from the corresponding infinite sequence (1.4) are positive definite. An extension with this property is called a positive rational extension. It is well known that the extension {0 , 1 , 2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function Z (z ) is analytic in the closed unit disc and the matrix function (z ) = Z (z ) + Z (1/z ) (2.7)

is nonnegative definite on the unit circle, making  a spectral density matrix. A minimal positive rational extension of the finite sequence (1.1) is one for which the ¯ ) in (1.6) is as small as possible. dimension of the triplet (A, C, C Definition 2.2. The positive degree p of the finite covariance sequence {0 , 1 , . . . ,  } is the dimension of any minimal positive extension. A well-known example of a positive extension is the maximum entropy extension (Whittle, 1963) corresponding to the spectral density (z ) := W (z )W (1/z ) , where the spectral factor W (z ) is (modulo a multiplicative constant matrix) the inverse of the Levinson-Szeg¨ o matrix polynomial of order  corresponding to the finite covariance sequence (1.1). Since the rational function W (z ) generically has the McMillan degree equal to m , it follows from spectral factorization theory (Anderson, 1958) that Z (z ) has also degree m . Consequently, the positive degree p is bounded from below by the algebraic degree r and from above by m . As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use algebraic rather than positive extensions, usually computed by minimal factorization a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1 may also be formulated in the following way. Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree. This assumption prescribes a property of the covariance sequence (1.1) which is not generic. We can illustrate this point by considering the rational extension problem for a finite scalar covariance sequence (1.1). The positive degree p lies between the algebraic degree r and  . Note that neither the case p =  nor the case p <  are "rare events", because there are open sets of covariance sequences (1.1) of both categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each µ  µ   there is an open set of covariance data in R for which p = µ. such that  2 If the upper limit p =  is attained there are infinitely many nonequivalent minimal ¯ ) providing a positive extension, one of which is the maximum entropy triplets (A, C, C extension. In fact, it can be shown that these  -dimensional extensions form an Euclidean space (Byrnes and Lindquist, 1989). This shows that the finite data (1.1) never contains enough information to establish a "true" underlying system. A similar statement can be made in the case when p <  . Example 2.3. Consider the case m = 1 and  = 2, i.e., consider a scalar partial covariance sequence {0 , 1 , 2 }. If 1 = 2 = 0, we have r = p = 0. Otherwise, we always have r = 1, whereas the positive degree can be either one or two. In fact, 2 setting 0 := 1 /0 and 1 := (2 1 + 2 )/(1 - 1 ), it can be shown (Georgiou, 1987; also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1 if and only if |0 | |1 | < 1 + |0 | and p = 2 otherwise. In fact, it is not hard to construct examples for which the gap between algebraic and positive rank is arbitrarily large, as the following theorem shows. Theorem 2.4. Let n  Z+ be fixed. Then for an arbitrarily large  there is a stable rational function Z (z ) of degree n, such that the Toeplitz matrix T formed as in ( 1.8) from the coefficients of the Laurent expansion ( 1.7), is positive definite while T +1 is indefinite. Consequently, you cannot test the positivity of a rational extension of (1.1) by checking a finite Toeplitz matrix, however large is its dimension. The proof of Theorem 2.4 is given in Appendix A. Let us now return to the identification procedure of Aoki (1990). In practice the rank of H will always be full, and to compute a partial realization of reasonable dimension the basic idea is to partition  as = 1 0 , 0 2 (2.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in 2 are smaller than those in 1 , perhaps close to zero, and then take 2 = 0 so that H is approximated by H1 = U 1 0 V = U1 1 V1 . 0 0 (2.9)

The matrix H1 is a best approximation (given the rank) of H in (the induced) 2 ­ norm, but it is in general not Hankel and hence can not be used to determine a reduced order system. Of course, one may instead use Hankel-norm approximation (Adamjan, Arov and Krein, 1971), which produces another best approximation of H in 2 -norm that is Hankel and has the same rank as H1 . However, if 2 is "very small" compared to 1 , then H1 is close to H and hence approximately Hankel. For this reason, Aoki's procedure (Aoki, 1990) is based on the original data H and  (H ). Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I , the ¯r ) given by same type of calculation as above yields the reduced triplet (Ar , Cr , C Ar = 1
- 1/ 2

U1  (H )V1 1
- 1/2

- 1/ 2

,

(2.10a) (2.10b) (2.10c)

Cr = 1 (H )V1 1 , 1 /2 ¯r = 1 (H )U1 - C . 1

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal subsystem truncation in the sense that, if H is produced by a finite-dimensional system ¯ ) having finite-interval balanced form (2.5), we have with (A, C, C Ar = A11 , where A= A11 A12 A21 A22 C = C1 C2 ¯1 C ¯2 . ¯= C C (2.12) Cr = C1 , ¯r = C ¯1 , C (2.11)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for  (H ), 1 (H ) and 1 (H ) and inserting in (2.10). However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that ¯1 + 1 0 (2.13) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real, provided of course that Z , defined by (1.6), is positive real. The question of stability was answered in the affirmative in Pernebo and Silverman (1982) and is addressed in Aoki (1990). The crucial question of positivity, however, is not discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be proven for a somewhat modified procedure described below. In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall consider a normalized Hankel matrix
1 -T ^ = L- H + HL- ,

(2.14)

where L- and L+ are lower triangular Cholesky factors of the Toeplitz matrices T- and T+ of (1.1) and the corresponding sequence of transposed covariances respectively; see Section 4 below. This is also the Hankel matrix considered in van Overschee and ^ instead of H , the De Moor (1993). Taking the singular value decomposition of H

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coefficients, i.e., the cosines of the angles between the past and the future of the process y . The systems matrices can be determined in a manner analogous to (2.5), but now
-1 -1 ¯ ^ = ¯ T-  =  T+

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as ^ =U ^ ^U ^ so that H = above. To see this, consider the singular value decomposition H ¯ ^ ^ ^ (L+ U )(L- V ) . Since H =  and this factorization is unique modulo coordinate ¯ = L- V ^ ^ 1/2 and  ^ ^ 1/2 . Then transformation in state space, we may take  = L+ U ^ ^ ^ ^ (2.15) follows from U U = I = V V . As we shall see next, (2.15) corresponds to a more natural type of balancing corresponding to a Hankel operator describing the interface between the past and the future of the time series y . 3. Stochastic realization theory in the Hilbert space of a sample function In this section we introduce a mathematical framework which is suitable for the identification problem described above. We define a Hilbert space of observed (infinite) strings of data {yt }. This framework turns out to be isomorphic to that of geometric stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we also establish a correspondence which converts operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. In identification we have access only to a finite string of data {y0 , y1 , y2 , . . . , yT }. (3.1)

Here T may be quite large but, of course, always finite. To begin with, we shall, however, consider the idealized situation that we are given a doubly infinite sequence of m-dimensional data {. . . , y-3 , y-2 , y-1 , y0 , y1 , y2 , y3 . . . } (3.2)

together with a corresponding covariance sequence {k }k0 , each matrix k of the sequence being computed from the data (3.2) by an ergodic limit of the type (1.11). In Section 5 we then modify the theory to handle the situation of finite data (3.1). For each k  Z define the m ×  matrix y (t) := [yt , yt+1 , yt+2 , . . . ] (3.3)

and consider the sequence y := {y (t)}tZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of all finite linear combinations ak y (tk ); ak  Rm , tk  Z

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product defined by linear extension of the bilinear form 1 a y (k ), b y (j ) := lim T  T + 1
t0 +T

a yt+k yt+j b = a k-j b,
t=t0

(3.4)

which clearly does not depend on t0 . This inner product is nondegenerate if the Toeplitz matrix Tk , constructed from the covariance data {0 , 1 , . . . , k }, is a positive definite symmetric matrix for all k . Here we shall assume that the sequence {Tk }k0 is actually coercive, i.e., Tk > cI for some c > 0 and all k  0. (See Assumption 3.2 below for an alternative characterization.) We also define a shift operator U on the family of semi-infinite matrices (3.3), by setting Ua y (t) = a y (t + 1) t  Z, a  Rm ,

defining a linear map which is isometric with respect to the inner product (3.4) and extendable by linearity to all of Y . In particular the sequence of matrices {y (k )} corresponding to the time series y is propagated in time by the action of the operator U, i.e., yi (t) = Ut yi (0), i = 1, 2, . . . , m, t  Z, (3.5)

where yi denotes the i:th row component of y . Then, closing the vector space Y in the inner product (3.4), we obtain a Hilbert space H (y ) := cl Y . The shift operator U is extended by continuity to all of H (y ) and is a unitary operator there. As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results in the geometric theory of stochastic realization can be carried over to the present framework by merely identifying the time series y with a stationary stochastic process y. In particular, the subspaces H - and H + of H (y ) generated by the elements (3.3) for t < 0 and t  0 respectively can be regarded as the past and future subspaces of the stationary process y. For reasons of uniformity of notation the inner product (3.4) will also be denoted ,  = E { }, (3.6)

as the frameworks are completely equivalent. Here we allow E {·} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity condition introduced above insures that tZ Ut H - = 0 and tZ Ut H + = 0, i.e., y is a purely nondeterministic sequence. As we have pointed out above, the subspace identification methods of Aoki (1990) and van Overschee and De Moor (1993) are based on the assumption that the available data is generated by an underlying stochastic system of finite dimension. More specifically, using the notations introduced above, we assume that the data are generated by a linear system of the type x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (3.7)

12

ANDERS LINDQUIST AND GIORGIO PICCI

defined for all t  Z, where w is some vector-valued normalized white noise time series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability matrix. Throughout this paper we shall assume (without restriction) that (A, B, C ) B is a minimal triplet and that the matrix has linearly independent columns. D The system is assumed to be in statistical steady state so that the n-dimensional state x and the m-dimensional output y are uniquely defined by (3.7) as linear causal functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and y (s) will depend only on the difference t - s. We shall think of the system (3.7) as a representation of the output time series y . The state and input variables x and w are introduced in order to display the special structure of the dynamic model of y and are by no means unique. Such a representation is called a state-space realization of y . Remark 3.1. Despite the fact that the model (3.7) is defined in terms of sample sequences, all equalities must be understood in the sense of Hilbert space metric, just as in the case of models based on random variables. The number of state variables n is called the dimension of the realization. A realization is minimal if there is no other realization of y of smaller dimension. In this case the covariance matrix of the state vector, P = E {x(t)x(t) } is positive definite. Moreover as the matrix (3.8)

B is taken with linearly independent D columns, the number of (scalar) white noise inputs p is also as small as possible. Clearly, the covariance sequence {0 , 1 , 2 , . . . } of the output {y (t)} of a minimal model (3.7) is a rational sequence of degree n, i.e., represented as ¯ = AP C + BD ¯ k = 0, 1, 2, . . . where C k = CAk-1 C 0 = CP C + DD . (3.9)

In the following we shall need to assume that the corresponding spectral density (z ) satisfies the following condition. Assumption 3.2. The spectral density  of the output process of the underlying system (3.7) is coercive in the sense that (ei ) > 0 for all   [0, 2 ]. (3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent sequences. Recall that a positive real function Z such that (z ) := Z (z ) + Z (z -1 ) satisfies (3.10) is called strictly positive real. Let H (w) be the Hilbert space generated by w, i.e. the closure of the linear space spanned by the family {wi (t), i = 1 . . . p, t  Z} with respect to the metric induced by the inner product ,  = E {  } where E {·} is defined by (3.6). Let H + and H - be the subspaces of H (w) generated by the components of future {y (0), y (1), y (2) . . . } and past outputs {y (-1), y (-2), y (-3) . . . }, respectively.
4

This means that E {w(t)w(s) } = Its where ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace X := {a x(0) | a  Rn } (3.11)

is invariant under coordinate changes of the type (A, B, C )  (T AT -1 , T B, CT -1 ) and is a coordinate-free representation of the realization (3.7). Such an object is called a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next define the stationary Hankel operator of y , H : H +  H - as H := E H |H +
- -

(3.12)

where E H  is the orthogonal projection of  onto H - . The splitting subspace property of X is equivalent to the commutativity of the diagram H+ O i.e. to the factorization H = CO ,
+ -

- H - C X

H

(3.13)

where the operators O := E H |X and C := E H |X are the observability respectively constructibility operators relative to the splitting subspace X . It can be shown that the splitting subspace X is minimal if and only if O and C are both injective. (See, e.g., Lindquist and Picci (1991).) The system (3.7) is a forward or causal realization of y in the sense that the subspace + H (w), generated by the future of w, is orthogonal to X and H - , i.e. to the present state and past output. Corresponding to (3.7) there is another realization ¯w ¯(t) + B ¯ (t - 1) x ¯(t - 1) = A x ¯x ¯w y (t - 1) = C ¯(t) + D ¯ (t - 1) (3.14)

¯ ), generated by which is backward or anticausal in the sense that the subspace H - (w + ¯(0) is a basis in X , i.e. the past of w ¯ , is orthogonal to X and H . Like x(0), x X := {a x ¯(0) | a  Rn }. ¯ = P -1 P x ¯(0) = P -1 x(0). (3.15)

In fact, x ¯(0) is the dual basis of x(0) in the sense that E {x(0)¯ x(0) } = I . Hence (3.16)

The particular notations used in (3.7) and (3.14) reflect the special meaning of the ¯ ). Computing the covariance matrix of the output using the dual parameters (A, C, C ¯ ) is precisely a realizations (3.7) and (3.14), it is in fact readily seen that (A, C, C triplet realizing the positive real part (1.6) of the spectral density matrix (z ) of the time series y . There are infinitely many minimal factorizations (3.13), one for each Markovian splitting subspace, but the basis in each state space X can be chosen so ¯ ) are the same for each minimal X . This is called a uniform that the triplets (A, C, C choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward predictor spaces X- = E H H +
-

X+ = E H H - ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12) and of its adjoint, respectively. ¯ ), the splitting Fixing a uniform choice of bases, and thus the triplets (A, C, C subspace X- has the forward stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) with state covariance P- , and X+ has the backward realization ¯+ w x ¯+ (t - 1) = A x ¯+ (t) + B ¯+ (t - 1) ¯ +w ¯x ¯+ (t - 1) y (t - 1) = C ¯+ (t) + D (3.19) (3.18)

¯+ . with state covariance P These two stochastic realizations will play an important role in what follows. In fact, an important interpretation of these realizations is that
-1 [y (t) - Cx- (t)] x- (t + 1) = Ax- (t) + B- D-

is the unique steady-state Kalman filter of any minimal realization (3.7) of y in the fixed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8) ¯+ )-1 , then corresponding to the forward counterpart of (3.19), i.e., P+ = (P P -  P  P+ for the state covariance of any minimal realization (3.7). In the same way
-1 ¯+ ¯+ D ¯+ (t) + B [y (t - 1) - C x ¯+ (t)] x ¯+ (t - 1) = A x

(3.20)

is the backward steady-state Kalman filter of all minimal backward realizations (3.14), and ¯+  P ¯P ¯- P ¯- is the backward counfor an arbitrary backward minimal realization (3.14), where P terpart of P- . 4. Canonical correlations and balanced stochastic realization In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a finite-dimensional realization of the type (3.7). Equivalently, we study certain factorizations of the infinite Hankel matrix of the corresponding infinite covariance sequence {0 , 1 , 2 , . . . }. Some portions of this section can be found in an equivalent but somewhat different setting in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and set notations. This will be done in the geometric framework of Section 3, thereby providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall introduce orthonormal bases in H - and H + . To this end it will be useful to represent past and future outputs as infinite vectors in the form,     y (-1) y (0) y (-2) y (1)    y = (4.1) y- =  + y (-3) y (2) . . . . . . Let L- and L+ be the lower triangular Cholesky factors of the infinite block Toeplitz matrices T+ := E {y+ y+ } = L+ L+ T- := E {y- y- } = L- L- and let
1  := L- - y- 1  ¯ := L- + y+

(4.2)

be the corresponding orthonormal implies that  1 2 H := E {y+ y- } =  3 . . .

bases in H - and H + respectively. Now, (3.9) 2 3 3 4 4 5 . . . . . .    ¯  C C ... ¯  . . .  CA   CA = 2  ¯ 2 , . . . CA  C (A )  . . ... . . . .

(4.3)

and therefore we have the following representation result, which can be found in Desai et al. (1985). Proposition 4.1. Let y be realized by a finite dimensional model of the form (3.7). Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator H is
1 -T -1 ¯ -T ^  = L- H + E {y+ y- }L- = L+  L- ,

(4.4)

where

 C  CA   = CA2  . . . 

and

¯  C ¯  CA  ¯   = C ¯ (A )2  . . . .



(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization (4.3) for H , irrespective of which X (i.e. which minimal realization of y ) is chosen. Recall that the adjoint O of the observability operator O is defined as the unique linear operator H +  X such that O,  = , O  for all   X and   H + . Orthogonality implies that E H ,  = ,  = , E X  , and therefore O = E X |H + . In the same way, we see that C  = E X |H - . The finiterank linear operators O O and C  C are defined on X and are the coordinate-free representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O O is full rank and constructible if and only if C  C is full rank. The following representations show that these gramians are related
+

16

ANDERS LINDQUIST AND GIORGIO PICCI

¯+ , the state covariances of the forward and backward steady-state Kalman to P- and P filters (Picci and Pinzoni, 1994). Proposition 4.2. Let x(0) and x ¯(0) be the conjugate basis vectors in a minimal splitting subspace X as defined above. Then, in a uniform choice of bases, ¯+ x(0) ¯(0) = a P O O a x and C  C a x(0) = a P- x ¯(0), (4.7) ¯+ , respectively, independently i.e., C  C and O O have matrix representations P- and P of X . Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal, E H a x(0) = a x- (0), C  C a x(0) = E X a x- (0) = E X a P- x ¯- (0). But, since the bases x ¯(0) and x ¯- (0) are chosen uniformly, EX a x ¯- (0) = a x ¯(0) a  Rn , and consequently (4.7) follows. The proof of (4.6) is analogous. The factorization (4.4) can also be derived from (3.13) and the following useful matrix representations of the observability and constructibility operators. Proposition 4.3. Let x(0) and x ¯(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
T ¯(0) = a  L- ¯ Oax +  1 O b  ¯ = b L- + x(0)
-

(4.6)

and therefore

(4.8)

and
T ¯ L- C a x(0) = a  -  1¯ x(0), C  b  = b L- - ¯

(4.9)

¯ are given by (4.5). where  and  Proof. Since, in view of (3.7), y+ = x(0) + terms which are orthogonal to X,
1 and  ¯ = L- + y+ , we have 1 E { ¯x(0) } = L- + P.

(4.10)

Consequently, for any a  Rn , the usual projection formula5 yields O a x(0) = E H a x(0) = a E {x(0)¯  } ¯ and O b  ¯ = EX b  ¯ = b E { ¯x(0) }P -1 x(0), from which (4.8) follows. A symmetric argument yields (4.9).
If   H (w) and the subspace Z  H (w) is spanned by the components of the full-rank random vector z , then E Z  = E {z }(E {zz })-1 z .
5
+

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should note that the matrix representations of O and C  are the transposes of those of O ¯ = I . Moreover, it follows and C if and only if x(0) is an orthogonal basis, i.e., P = P from (4.8) that -1 ¯(0) = a  T+ x(0), O Oa x
-1 showing that  T+  is a matrix representation of O O, in harmony with the analysis at the end of Section 2. In the same way, (4.9) yields -1 ¯ ¯ T- ¯ x(0), C  C a x(0) = a  -1 ¯ ¯ T-  is a matrix representation of C  C . Together with Proposition 4.2 and hence  ¯+ : this yields the following explicit formulas for P- and P -1 ¯+ =P  T+ -1 ¯ ¯ T-   = P- .

(4.11)

Now, let {1 , 2 , 3 , . . . } be the singular values of the Hankel operator H. Since rank H = n, i = 0 for i > n. The nonzero singular values 1  1  2  3 . . .  n > 0 (4.12)

are the cosines of the angles between the subspaces H- and H+ ; they are known as the canonical correlation coefficients of y (Hotelling, 1936, Anderson, 1958). Obviously 1 < 1 if and and only if H-  H+ = 0. The squares of the canonical correlation coefficients are the eigenvalues of H H, i.e.,
2 i , H H i = i

which, in view of (3.13) may be written
2 (O i ), O OC  C (O i ) = i

and therefore, as was also demonstrated in Picci and Pinzoni (1994),
2 2 2 , 2 , . . . , n }, {O OC  C} = {1

(4.13)

2 2 2 , 2 , . . . , n are the eigenvalues of O OC  C . But, in view of Proposition 4.2, i.e., 1 this is precisely the coordinate-free version of the invariance condition 2 2 2 ¯+ } , 2 , . . . , n } = {P- P {1

(4.14)

of Desai and Pal (1984). This suggests that an appropriate uniform choice of bases would be the one that ¯+ equal and equal to the diagonal matrix of nonzero canonical corremakes P- and P lation coefficients. ^  is the In fact, in view of Proposition 4.1, the infinite normalized Hankel matrix H matrix representation of the operator H in the orthonormal bases (4.2). Therefore ^  has the singular-value decomposition H ^  = U   V = U  V , H   = diag{1 , 2 , 3 , . . . , n }, (4.15) where  is the diagonal n × n matrix consisting of the canonical correlation coefficients (4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and  is the infinite matrix  =  0 . 0 0

Moreover U and V are infinite orthogonal matrices, and U and V are  × n submatrices of U and V with the the property that U U = I = V V. (4.17) We now rotate the the orthonormal bases (4.2) in H + and H - to obtain u := U  ¯ and v := V  respectively. Note that E {uv } =  . What makes these orthonormal bases useful is that they are adapted to the orthogonal decompositions6 H -  H + = [H -  (H + ) ]  H  [H +  (H - ) ], (4.18)

where H := X-  X+ is the so-called frame space (Lindquist and Picci (1985, 1991), in the sense that X- = span{v1 , v2 , . . . , vn } X+ = span{u1 , u2 , . . . , un }. This is true since X- is precisely the subspace of random variables in H - having nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H - . Since therefore {vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H -  (H + ) and H +  (H - ) , respectively, these spaces will play no role in what follows. Now define the n-dimensional vectors  1/2   1/2  1 u1 1 v1   1/2 u    1/2 v  2 2   1 1 z ¯ =  2 .  = 1/2 U L- (4.19) z =  2 .  = 1/2 V L- - y- + y+ . .  .   .  n vn
1/2

n un

1/ 2

¯ is a basis in X+ , and they From what we have seen before, z is a basis in X- and z have the properties ¯z ¯ }. E {zz } =  = E {z (4.20)

In fact, we even have more as seen from the following amplification7 of a theorem by Desai and Pal (1984) (Theorem 1). Theorem 4.4. The basis vectors x- (0) = z x ¯+ (0) = z ¯ (4.21)

in X- and X+ respectively belong to the same uniform choice of basis, i.e. to the ¯ ), and in this uniform choice same choice of triplets (A, C, C ¯+ . P- =  = P
6 7

(4.22)

The symbols  and  denote vector sum and orthogonal vector sum of subspaces. ¯ ). A priori there is no reason why choosing bases in X- and X+ would lead to the same (A, C, C This important property is explicitly mentioned in Theorem 4.4.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coefficients {1 , 2 , 3 , . . . , n } are distinct, this is, modulo multiplication with a signature matrix 8 , the only uniform choice of bases for which ( 4.22) holds. ¯ ) is know as stochastically balanced, and, in the case of Such a choice of (A, C, C distinct canonical correlation coefficients, it defines a canonical form with respect to state space isomorphism in (1.6) by fixing the sign in, say, the first element in each row of C . Such canonical forms have also been studied by Ober (1991). Proof. It follows from (4.4) and (4.15) that E {z ¯z } = 2 . (4.23) ¯ ) so that x ¯, and let the bases in the other splitting Now, choose (A, C, C ¯+ (0) = z subspaces be chosen accordingly so that the choice of bases is uniform. We want ¯+ (0) and that to show that x- (0) = z . To this end, first note that x+ (0) = -1 x x- (0) = E X- x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula and the fact that z is a basis in X- , ¯z }-1 z, x- (0) = -1 E {z which, in view of (4.23), yields x- (0) = z as claimed. Hence (4.22) follows from (4.20). ¯ ) is another uniform choice of bases which Next, suppose that (QAQ-1 , CQ-1 , CQ is also stochastically balanced. Since then x- (0) = Qz and, as is readily seen from ¯+ = Q-T Q-1 , ¯ so that P- = QQ and P the backward system (3.14), x ¯+ (0) = Q-T z (4.22) yields QQ =  and Q-T Q-1 = , from which we have Q2 = 2 Q. Since  has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959) that there is a scalar polynomial (z ) such that Q = (2 ). Hence Q is diagonal and commutes with  so that, by QQ = , we have QQ = I. Consequently, since Q is diagonal, it must be a signature matrix. In view of (4.21) and (3.16), the first of relations (4.9) and (4.8) respectively yield -1 -1 ¯ T- y- z ¯ =  T+ y+ . (4.24) z= Consequently, in view of (4.20), (2.15) holds also for the case of an infinite Hankel matrix. This can of course also be seen from (4.11). Note that the normalization of the block Hankel matrix H is necessary in order for the singular values to become the canonical correlation coefficients, i.e., the singular values of H. In fact, if we were to use the unnormalized matrix representation (4.3) of H instead, as may seem simpler and more natural, the transpose of (4.3) would not be the matrix representation of H in the same bases, a property which is crucial in the singular value decomposition above. This is because (4.3) corresponds to the bases y- in H - and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the finite-dimensional case studied in ^ , defined in Section 2, is Section 2, and therefore the normalized Hankel matrix H preferable to the unnormalized H . ¯ in terms of the Hankel matrix H , can Formulas, such as (2.5), expressing A, C, C be easily derived from basic principles. In fact, standard calculations based on the forward model (3.7) and the backward model (3.14) yield A = E {x(1)x(0) }P -1 C = E {y (0)x(0) }P -1 , ¯ -1 = E {y (-1)x(0) } ¯ = E {y (-1)¯ C x(0) }P for any dual pair of bases x(0) and x ¯(0). Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases (4.19) can be computed by means of the formulas
1 -T - 1/ 2 , A = -1/2 U L- +  (H )L- V  T - 1/2 C = 1 (H )L- , - V - T - 1 / 2 ¯ = 1 (H )L+ U  C ,

(4.25a) (4.25b) (4.25c)

(4.26a) (4.26b) (4.26c)

where H is the unnormalized Hankel matrix (4.3),  (H ) is obtained from H by deleting the first block row, and 1 (H ) is the first block row. Proof. First, in (4.25a) and (4.25b), we take x(0) to be x- (0). By the Kalman filter representation a [x+ (1) - x- (1)]  UH -  H - for all a  Rn ,
-1 ¯+ E {x ¯+ (1)x- (0) }. E {x- (1)x- (0) } = E {x+ (1)x- (0) } = P

¯ ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19), But (A, C, C 1 1 ¯+ , x- (0) = 1/2 V L- ¯+ (1) = 1/2 U L- P- =  = P - y- and x +  (y+ ), where  (y+ ) is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),
1 -T - 1/ 2 , A = -1/2 U L- + E { (y+ )y- }L- V 

which is identical to (4.26a). Likewise, from (4.26b),
T - 1/ 2 , C = E {y (0)y- }L- - V

which yields (4.26b). Finally, taking x ¯(0) to be x ¯+ (0) in (4.25c), a symmetric argument yields (4.26c). Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from finite covariance data In this section we modify the realization theory of Section 4 to the case that only a finite segment {y (0), y (1), y (2), . . . , y ( )}, (5.1) of the time series {y (t)} is available. We still define each y (t) as the semi-infinite string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact partial covariance sequence {0 , 1 , 2 . . . ,  }. (5.2)

The corresponding realization problem, which is purely theoretical and is intended to prepare for the more realistic identification situation with finite strings of observed data (Section 6), is therefore the partial stochastic realization problem mentioned in Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the output of some minimal "true" system (3.7) of dimension n and that  is large enough for n to equal the positive degree of the partial sequence (5.2). Now, suppose that  = 2 - 1, and partition the data into two matrices     y (0) y ( )  y (1)   y ( + 1)  - +   , = y = (5.3) y . .      . . . . y ( - 1) y (2 - 1) representing the past and the future respectively, and define the corresponding (finite- + and y respectively as dimensional) subspaces Y- and Y+ spanned by the rows of y explained in Section 3. Since the data size  will be important in the considerations that will follow, we denote the finite block Hankel matrix H of Section 2, relative to the data (5.3), by H , i.e.,
+ - H = E {y (y ) }.

(5.4)

Let 0 be the smallest integer  such that rank H = n. It is well-known that 0 is ¯ ), so n is an the maximum of the observability and constructibility indicies of (A, C, C upper bound for 0 . As pointed out in the beginning of Section 2, we need  > 0 to ¯ ). be certain that the factorization of H yields a unique (A, C, C Next we shall consider the class of minimal splitting subspaces for Y- and Y+ , i.e., the subspaces X admitting a canonical factorization Y+  O of the finite-interval Hankel operator H := E Y |Y+ .
-  - Y- C X

H

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces, ^  - = E Y- Y+ X ^  + = E Y+ Y- , and X

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred to Appendix D. Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary time series {y (t)}. Then, if  > 0 , X := U X is a minimal splitting subspace for Y- and Y+ , and ^  - = E Y- X , X ^  + = E Y+ X . X (5.7) (5.6)

^  - has a unique representation9 Conversely, any basis x ^( ) in X x ^( ) = E Y x( ),
-

(5.8)

^  + has a unique representation ^ where x( ) is a basis in X , and any basis x ¯( ) in X ^ x ¯( ) = E Y x ¯( ),
+

with x ¯( ) a basis in X . As X varies over the family of all minimal Markovian splitting subspaces, the corresponding x(0) [¯ x(0)] constitute a uniform choice of bases. ^ - The stochastic realizations corresponding to the finite-interval predictor spaces X ^  + are nonstationary. However, taking advantage of the representations (5.8) and X and (5.9), we shall be able to express these realizations in such a way that they can ¯ ) corresponding to one uniform be parameterized by the stationary triplet (A, C, C choice of bases, both for the forward and the backward settings. In fact, if the bases ^ x ^( ) and x ¯( ) are chosen so that x( ) and x ¯( ) in representations (5.8) and (5.9) are ¯ ) is used for x( )} = I , then the same choice of (A, C, C dual bases in X , i.e., E {x( )¯ ^  + is called coherent. ^  - and X all X  . Such a choice of bases in X The realizations generated by these coherent bases are precisely the (transient) forward and backward Kalman filters. In fact, the vector x ^( ) is the one-step predictor of x( ) based on Y- and, as shown in Appendix C, it evolves in time as the Kalman filter

X

(5.9)

X

x ^(t + 1) = Ax ^(t) + K (t)[y (t) - C x ^(t)]; where the gain K (t) is given by

x ^(0) = 0,

(5.10)

¯ - AP- (t)C )(0 - CP- (t)C )-1 K (t) = (C and the filter estimate covariance ^(t)^ x(t) } P- (t) = E {x is the solution of the matrix Riccati equation

(5.11)

(5.12)

¯ - AP- (t)C )(0 - CP- (t)C )-1 (C ¯ - AP- (t)C ) P- (t + 1) = AP- (t)A + (C P- (0)) = 0. (5.13)
With slight misuse of notations, the orthogonal projection operator applied to a vector will denote the vector of the projections of the components.
9

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the components of ^ ¯( ) x ¯( ) = E Y x ^  + and are generated by the backward Kalman filter form a basis in X ¯ (t)[y (t - 1) - C ¯x ^ ^ ^ x ¯(t - 1) = A x ¯(t) + K ¯(t)]; with ¯+ (t)C ¯ )(0 - CP ¯ - (t)C ¯ )-1 , ¯ (t) = (C - A P K where ¯+ (t) = E {x ^ ^ P ¯(t)x ¯(t) } is obtained by solving the matrix Riccati equation ¯+ (t)A + (C - A P ¯+ (t)C ¯+ (t)C ¯ )(0 - C ¯P ¯+ (t)C ¯ )-1 (C - A P ¯) ¯+ (t - 1) = A P P ¯+ (2 - 1) = 0. P (5.18) Now, it is well-known that both ^(t)]  (t) = (0 - CP- (t)C )-1/2 [y (t) - C x and ¯P ¯+ (t)C ¯ )-1/2 [y (t - 1) - C ¯x ^  ¯(t) = (0 - C ¯(t)] (5.20) (5.19) (5.17) (5.16) ^ x ¯(2 - 1) = 0, (5.15)
+

(5.14)

are normalized white noises, called the forward respectively the backward (transient) innovation processes. Consequently, we may write the Kalman filter (5.10) as x ^(t + 1) = Ax ^(t) + B- (t) (t) y (t) = C x ^(t) + D- (t) (t) (5.21)

where D- (t) := (0 - CP- (t)C )1/2 and B- (t) := K (t)D- (t). Likewise, the backward Kalman filter (5.10) may be written ¯+ (t)¯ ^ ^ ¯(t) + B x ¯(t - 1) = A x  (t - 1) ¯ ¯ ^ y (t - 1) = C x ¯(t) + D+ (t)¯  (t - 1) (5.22)

¯ + (t) := (0 - C ¯P ¯+ (t)C ¯ )1/2 and B ¯+ (t) := K ¯ (t)D ¯ + (t). Comparing with (3.7) where D and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7) and (3.14) are time-varying and describe the output y only on the interval [0, 2 - 1]. In fact, since ^(t)][x(t) - x ^(t)] }  0, P - P- (t) = E {[x(t) - x ¯ ¯ and, for the same reason, P - P+ (t)  0, we have ¯+ (t)-1 , P- (t)  P  P+ (t) := P (5.23)

^  - and X ^  + are extremal splitting subspaces, so we see that the predictor spaces X just as X- and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the finite-interval counterparts of equations (4.25) are given by A = E {x ^( + 1)^ x( ) }P- ( )-1 C = E {y ( )^ x( ) }P- ( )-1 , ¯+ ( )-1 = E {y ( - 1)^ ¯ = E {y ( - 1)x ^ x( ) } C ¯( ) }P (5.24a) (5.24b) (5.24c)

In complete analogy with the stationary framework in Section 4, the canonical correlation coefficients 1  1 ( )  2 ( )  · · ·  n ( ) > 0 (5.25) between the finite past Y- and the finite future Y+ are now defined as the singular values of the operator H given by (5.5). To determine these we need a matrix representation of H in some orthonormal bases. Using the pair (5.19)­(5.20) of transient innovation processes for this purpose, we obtain the normalized matrix (2.14), which ^  . Singular value decomposition yields we shall here denote H ^  = U  V , H (5.26) where U U = I = V V , and  is the diagonal matrix of canonical correlation coefficients. As in Section 4 it is seen that
-1 - z ( ) =  V (L-  ) y 1/2 -1 + z ¯( ) =  U (L+  ) y 1/2

(5.27)

^  - and X ^  + respectively and that are bases in X E {z ( )z ( ) } =  = E {z ¯ z ¯ }. (5.28)
+ Here L-  and L are the finite-interval counterparts of L- and L+ respectively, and they are of course submatrices of these. Note that H , as defined by (5.4), is now given by - ^ H  = L+  H (L ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z ( ) and z ¯( ) are coherent bases, and the ¯ corresponding triplet (A, C, C ) is a finite-interval stochastically balanced realization, i.e., ¯+ ( ). P- ( ) =  = P (5.30)

The following finite-interval modification of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the finite block hankel matrix H , and the proof is analogous. ¯ ), obProposition 5.2. The finite-interval stochastically balanced triplet (A , C , C ^ tained from (5.24) by choosing the bases x ^( ) = z ( ) and x ¯( ) = z ¯( ), is given by
1 /2 -1 - -T 1/ 2 U (L+ V - , A = -   )  (H )(L )  -T 1/ 2 V - , C = 1 (H )(L-  )  + -T - 1 /2 ¯ C = 1 (H )(L ) U  ,

(5.31a) (5.31b) (5.31c)

where the operators  (·) and 1 (·) are defined as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

¯ ) actually varies with  , but that, for each  , it Note that the triplet (A , C , C ¯ ) of Section 4, i.e., there is a is similar to the stochastically balanced triplet (A, C, C nonsingular matrix Q so that ¯ ) = (Q AQ-1 , CQ-1 , CQ ¯ ). (A , C , C    (5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the stationary predictor spaces X- and X+ will have the state covariances P- = Q Q
T -1 ¯+ = Q- and P  Q ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state covariances are not diagonal and equal is a manifestation of the fact that the triplet ¯ ) is not stochastically balanced in the sense of Section 4. It is well known (A , C , C ¯+ , respectively, as t  , and ¯+ (t) tend monotonically to P- and P that P- (t) and P therefore we have the following ordering ¯+ )-1  (P ¯+ ( ))-1 := -1 . P- ( ) :=   P-  (P


Since the number n of nonzero singular values (5.25) is in general too large too yield a reasonable model, we must consider what happens when some of the smallest singular values are set equal to zero. The truncation procedure employed by van Overschee and De Moor (1993) is equivalent to the principal subsystem truncation presented in Section 2, except that, and this is very important, the singular-value ^  , which is the decomposition is performed on the normalized block Hankel matrix H natural matrix representation of the operator  . It will be shown in Section 7 that such a truncation will preserve positivity in the stationary case (Theorem 7.3). In order to carry this result over to the case of finite  , we need to assume that the spectral density  of the time series {y (t)} is coercive so that Assumption 3.2 is fulfilled, i.e., that the function Z is strictly positive real. The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D, shows that principal subsystem truncation preserves positivity provided  is chosen large enough.

H

Theorem 5.3. Suppose that the spectral density  of the time series {y (t)} is coercive. Then, there is an integer 1 > 0 such that, for   1 , the principal subsystem ¯ )1 ) of (A , C , C ¯ ) is a minimal realization of a strictly truncation ((A )11 , (C )1 , (C positive real function (2.13). 6. Subspace identification The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have access to an infinite sequence (3.2) of data. In reality we will have a finite string of observed data {y0 , y1 , y2 , . . . , yN }, (6.1)

where, however, N may be quite large. More specifically, we assume that N is sufficiently large that replacing the ergodic limits (1.11) by truncated sums yields good approximations of {0 , 1 , 2 . . . ,  }, (6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course,  << N . This is equivalent to saying that T := N -  is sufficiently large for 1 T +1
T

a yt+k yt+j b
t=0

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we shall use the finite-interval realization theory of Section 5 as if we had a finite time series {y (0), y (1), y (2), . . . , y ( )}, while substituting the semi-infinite string (3.3) of data by y (t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . ,  . (6.5) (6.4)

In particular, in this case the inner product becomes merely that of a finite-dimensional Euclidean space so that the block Hankel matrix H can be written H = where   y  -1 y  . . . yT + -1 y -2 y -1 . . . yT + -2  -  y = . . ..  . . . . . . .  . y0 y1 . . . yT 1 y + (y - ) T +1    y +1 . . . yT + y  y +1 y +2 . . . yT + +1  + . = and y . . ..  . . . . . . .  . y2 -1 y2 . . . yT +2 -1 

Consequently, the identification of a minimal stationary state-space innovation model describing the data (6.1) can be performed in the following steps. - + , y to obtain, from (1) Perform canonical correlation analysis on the data y ^ ¯+ ( ) = z ¯( ) and, from (5.26), the (5.27), the state vectors x ^- ( ) = z ( ) and x corresponding common state covariance matrix  , i.e., the diagonal matrix of the (finite interval) canonical correlation coefficients (5.25). (2) Given the singular value decomposition (5.26), compute via (5.31) a minimal ¯ ). This realization will be in finite-interval balanced form, realization (A, C, C i.e., (5.30) will hold instead of (4.22). (3) To obtain a state space model (3.7) for y we need to compute the matrices B ¯ 0 ) defines and D. Note that such matrices will exist if and only if (A, C, C, a positive real function (1.6), or, in other words, if and only if there is a symmetric positive definite P = P such that M (P ) := P - AP A ¯ - CP A C ¯ - AP C C 0 - CP C  0. (6.6)

[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6), B and D can be determined (in a nonunique way) by a full rank factorization of M (P ), i.e., B D B D = M (P ). (6.7)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P- = E {x- (t)x- (t) } has been determined. Obtaining P- amounts to finding the minimal solution of the algebraic Riccati equation ¯ - AP C )(0 - CP C )-1 (C ¯ - AP C ) P = AP A + (C (6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t   with initial condition P- ( ) =  . (The corresponding dual procedures yield ¯+ .) Again, in both cases, a positive definite P- can be found if and only P ¯ 0 ) defines a positive real function (1.6). In fact, in general, if (A, C, C, {P- (t)}t0 may not even converge unless this positivity condition is fulfilled and may in fact exhibit dynamical behavior with several of the characteristics of chaotic dynamics (Byrnes et al., 1991, 1994). Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that, for  fixed but sufficiently large (see Section 2), we will have rank H = n as T  , ¯ ) will be uniquely determined from the data and similar to the and the triplet (A, C, C ¯ triplet (A, C, C ) of the "true" generating system. Hence, in particular, in the limit as T  , at least in theory positivity will be guaranteed. If n ^ is an upper bound for the order of the "true" system, we may choose  to be any integer larger than n ^. In practice, however, T is finite, and even if we had a true system generating exact data, the spectral estimate T , although converging to the true spectrum  as T   may in principle fail to be positive for any finite T if there are frequencies  for which (ei ) = 0. Positivity for a suitably large T can however be guaranteed if the "true" spectrum is coercive. The following proposition, which also applies to Aoki's method discussed in Section 2, is proved in Appendix D. Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are ful¯ ) defined by filled. Then, there is a T0  Z+ such that, for T  T0 , the triplet (A, C, C (5.31) yields a function (1.6) which is strictly positive real. However, in practice, rank H normally will keep increasing with  , even for very large T , so that one must resort to some kind of truncation of the Hankel singular values. As we have pointed out in Section 5, setting all canonical correlation coefficients r+1 ( ), r+2 ( ), . . . equal to zero for some suitable r, as is done in, for example, van Overschee and De Moor (1993), is equivalent to principal subsystem truncation. An important issue is therefore under what conditions such a procedure will insure positivity. Here we must distinguish between problems generated by the sample fluctuations of the data due to finite sample size T , as considered in Proposition 6.1, and the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an infinite string of data generated by a "true" high-dimensional system, such a truncation procedure may fail if  is smaller than that dimension. Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following result, which justifies this approximation procedure, provided the rather stringent Assumption 2.1 holds and we have coercivity, and provided T and  are sufficiently large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulfilled. Then, there are positive integers T0 and 1 > 0 such that, for T  T0 and   1 , the ¯1 ), obtained from (2.12) by taking H := H in (2.10), is a minimal triplet (A11 , C1 , C realization of a strictly positive real function (2.13). We note that, in van Overschee and De Moor (1993), the large Hankel matrix
+ + + + - - - - ~  = (y ) (E {y (y ) })-1 E {y (y ) }(E {y (y ) })-1 y H

^  . This leads to a procedure which is equivalent to the one is used in place of H described above. Moreover, the computation of a second singular-value decomposition + - in van Overschee and De Moor (1993), based on H +1 := E {y +1 (y +1 ) }, together with a subsequent change of bases, is actually redundant, as can be deduced from the following proposition. In fact, a considerable amount of computation is needed in van Overschee and De Moor (1993) to compensate for the fact that taking z ( + 1), ^ ( +1)- would computed from a second singular-value decomposition, as a basis in X lead to a Kalman filter model with time-varying parameters. ^ Proposition 6.3. To each coherent pair of bases x ^( ) and x ¯( ) in the finite-interval ^ ^ predictor spaces X - and X + , there corresponds a minimal factorization ¯ H =   of the block Hankel matrix H . Here
+  x ^( ) = E Y y
- + - ¯x ^ ¯( ) = E Y y  .

(6.9)

and

(6.10)

Conversely, given a minimal factorization (6.9), ¯ (T - )-1 y - x ^( ) =     and
+ ^ x ¯( ) =  (T+ )-1 y

(6.11)

^ +. ^  - and X is a coherent pair of bases in X ^  - and X ^  + . Then, for ^ Proof. Let x ^( ) and x ¯( ) be a coherent choice of bases in X ¯( )) of dual bases any X as defined in Theorem 5.1, there is a unique pair (x( ), x ¯  be the matrices defined via such that (5.8) and (5.9) hold. Let  and 
+ - ¯x E X y =  x( ) and E X y = ¯( ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of X with respect to Y- and Y+ yields + - + - (y ) } = E {E X y (E X y )) }, E {y which, in view of (6.12), is the same as (6.9). Applying E Y and E Y to respectively the first and second equations of (6.12), the splitting property yields (6.10). As for the converse statement, equations (6.11) follow from the construction in the proof of Theorem 5.1, from which it also follows that the resulting bases x ^( ) and ¯ ^ x ¯( ) are constructed from the same (A, C, C ) and therefore coherent. ¯ ) have been fixed by a particular choice of x( ) As soon as the parameters (A, C, C in the representation (5.8) in Theorem 5.1, we must choose x ^( + 1) as x ^( + 1) = E Y +1 Ux( ) (6.13)
- +

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More specifically Proposition 6.3 ¯  are uniquely determined once x( ) has been selected. Hence implies that  and  ¯ ) is uniquely determined by the Ho-Kalman algorithm so that (A, C, C ¯ ¯  +1 = C  ¯  A is prescribed, as is
-1 - ¯  (T- x ^( + 1) =  +1 ) y +1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining x ^( + 1) by an extra singular-value decomposition, as in van Overschee and De Moor (1993), is actually redundant. If we actually were to determine x ^( + 1) as described -L ¯ ¯ above, we would better compute  +1 from  +1 =  H +1 , where the left inverse is very easily obtained from the singular-value decomposition of H . We stress that Assumption 2.1, although quite limiting, is absolutely crucial in insuring that the subspace identification algorithms mentioned above will actually work. Note that for generic data these algorithms may break down for any fixed  . The same is true for all other subspace methods which deal with identification of covariance models (or equivalent) involving stochastic signals. On the other hand, Assumption 2.1 introduces a quite unrealistic condition which, as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure to estimate T0 and 1 in Proposition 6.2, as the proof is based only on continuity arguments. 7. Stochastic model reduction As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step in order to keep the dimension of the model at a reasonable level. To justify any such procedure one must either assume that there is an underlying "true" system of sufficiently low order, i.e., invoke Assumption 2.1, or to perform rational covariance extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995), Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an infinite one. The latter can be done in many ways, one of which is the maximum entropy extension. In either case, the truncation problem is equivalent to approximating a positive real matrix function ¯ + 1 0 , Z (z ) = C (zI - A)-1 C 2 (7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of lower degree. In this section we shall investigate how this can be done and also how such an approximation affects the canonical correlation structure. One main question to be addressed is whether the principal subsystem truncation (2.11) preserves positive realness and balancing, and hence the leading canonical correlation coefficients, as originally claimed by Desai and Pal (1982). As it turns out, the answer is affirmative to the first but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identification approximation obtained by setting some canonical correlation coefficients equal to zero. It is instructive to first consider the continuous-time counterpart of this problem since the latter is simpler and exhibits more desirable properties. Also, it has been widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true. It is well-known [see, e.g., Faurre et al. (1979)] that an m × m matrix function Z with minimal realization ¯ + 1 R, (7.2) Z (s) = C (sI - A)-1 C 2 is positive real with respect to the right half plane if and only if there is a symmetric matrix P > 0 such that ¯ - PC -AP - P A C  0, (7.3) M (P ) := ¯ C - CP R where here we assume that R is positive definite and symmetric. In this case there are two solutions of (7.3), P- and P+ , with the property that any other solution of (7.3) satisfies P -  P  P+ . (7.4)

These extremal solutions play the same role as P- and P+ in the discrete-time setting, and rank M (P- ) = m = rank M (P+ ). (7.5)

-1 ¯+ := P+ If the state-space coordinates are chosen so that both P- and P are diagonal and equal, and thus, by (4.14), equal to the diagonal matrix  of canonical correlation ¯ ) is stochastically balanced. coefficients, we say that (A, C, C Now, suppose that  is partitioned as in (2.8) with r+1 < r , and consider the corresponding principal subsystem truncation (2.12). Using the stochastic realization framework, Harshavaradana, Jonckheere and Silverman (1984) showed that

¯1 + 1 R, Z1 (s) = C1 (sI - A11 )-1 C 2

(7.6)

¯1 ) is a minimal realization of a positive real function and conjectured that (A11 , C1 , C is stochastically balanced. We shall next show that this conjecture is true, as has already been done by Ober (1991) in a framework of canonical forms. First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield   ¯1 - 1 C1 -A11 1 - 1 A11  C   0,     (7.7) ¯  R C1 - C1 1 where blocks which play no role in the analysis are marked by an asterisk. Consequently, M1 (1 ) = ¯1 - 1 C1 -A11 1 - 1 A11 C  0. ¯ C1 - C1 1 R (7.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982), Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane, ¯1 ) is a minimal realization. (7.6) is positive real, but it remains to prove that (A11 , C1 , C This was done in Harshavaradhana et al. (1984). It is important to observe here that, contrary to the situation in the discrete-time setting, rank M1 (1 ) = rank M () = m 1 -1 and rank M1 (- 1 ) = rank M ( ) = m, important facts that will be seen to imply that the reduced system is stochastically balanced. Recall that in the continuous-time setting the spectral density (s) = Z (s)+ Z (-s) is coercive if, for some > 0, we have (s)  I for all s on the imaginary axis. This is equivalent to the condition that R > 0 and  has no zeros on the imaginary axis (Faurre et al., 1979, Theorem 4.17). Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with (s) := ¯ ) be in stochastically balanced form. Then, Z (s) + Z (-s) coercive, and let (A, C, C ¯1 ) defines a positive real function (7.6) if r+1 < r , the reduced system (A11 , C1 , C for which it is a minimal realization in stochastically balanced form, and 1 (s) := Z1 (s) + Z1 (-s) is coercive. Proof. We have already shown that Z1 is positive real, and we refer the reader to ¯1 ) is a minimal realization Harshavaradhana et al. (1984) for the proof that (A11 , C1 , C ¯1 ) is stochastically of Z1 . It remains to show that 1 is coercive and that (A11 , C1 , C -1 balanced, i.e., that P1- = 1 = P1+ , where P1- and P1+ are solutions to the algebraic Riccati equation ¯ - P1 C1 )R-1 (C ¯ - P1 C1 ) = 0 A11 P1 + P1 A11 + (C (7.9) such that any other solution P1 of (7.9) satisfies P1-  P1  P1+ . To this end, 1 -1 note that since M1 (1 ) and M1 (- 1 ) have rank m, both 1 and 1 satisfy (7.9). 1 Therefore, as is well-known (Molinari, 1977) and easy to show, Q := - 1 - 1 satisfies 1 Q + Q1 + QC1 R-1 C1 Q = 0, where ¯ - 1 C )R-1 C1 . 1 = A11 - (C 1 (7.11) Since  is coercive, -1 -  = P+ - P- > 0 (Faurre et al., 1979, Theorem 4.17) so that 1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to 1 Q-1 + Q-1 1 + C1 R-1 C1 = 0. (7.12) (7.10)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , 1 ). Since, in addition, the Lyapunov equation (7.12) has a positive definite solution Q-1 , 1 must be a stability matrix. Therefore 1 is the minimal (stabilizing) solution P1- of -1 ¯1+ := P1+ = 1 . (7.9). In the same way, using the backward setting, we show that P ¯1 ) is stochastically balanced. Since P1+ - P1- > 0, 1 is Consequently, (A11 , C1 , C coercive. ¯ 1 0 ) Let us now return to the discrete-time setting. Let us recall that, if (A, C, C, 2 is a minimal realization of (7.1), the matrix function Z is positive real if and only if the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given the positive real rational function (7.1) with the property that (z ) = Z (z ) + Z (z -1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y , the state covariance P of any minimal stochastic realization (3.7) of y satisfies (6.6) and the matrices B, D in (3.7) satisfy (6.7). Consequently, as pointed out in Section 5, the matrices B and D can be determined via a matrix factorization of M (P ) once P has been determined. ¯ ) is in stochastically balanced form, Theorem 4.4 implies that Now, if (A, C, C M ()  0. In view of (4.16) and (2.12), M () may be written   ¯1 - A11 1 C1 - A12 2 C2 1 - A11 1 A11 - A12 2 A12  C ,     ¯ C1 - C1 1 A11 - C2 2 A12  0 - C1 1 C1 - C2 2 C2 where, as before, the blocks which do not enter the analysis are marked with an asterisk. Since M ()  0, this implies that M1 (1 ) - where M1 (1 ) = ¯1 - A11 1 C1 1 - A11 1 A11 C ¯ C1 - C1 1 A11 0 - C1 1 C1 (7.14) A A12 2 12 C2 C2  0, (7.13)

¯1 ). Thereis the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , C fore, M (1 )  0, so if we can show that A11 is stable, i.e., has all its eigenvalues strictly inside the unit circle, it follows that ¯1 + 1 0 , (7.15) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real. As we shall see below this is true without the requirement needed in continuous time that r+1 < r . ¯1 ) also to be balanced, 1 would have to be the minimal solution P1- For (A11 , C1 , C of M1 (P1 )  0, which in turn would require that rank M1 (1 ) = rank M () = m. Due to the extra positive semidefinite term in (7.13), however, this will in general not be the case and therefore 1  P1- will correspond to an external realization, as will 1 - 1  P1+ ; see Lindquist and Picci (1991). ¯1 ) is minimal we need to assume that  is coercive, or, To show that (A11 , C1 , C equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979, Theorem A4.4) that this implies that P+ - P- > 0. (7.16)

In fact, if 0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to coercivity. Coercivity also implies that 0 - CP- C > 0. (7.17)

¯ ) in balanced form, P- =  = P ¯+ and, in view of (3.16), Remark 7.2. With (A, C, C -1 -1 P+ =  . Hence (7.16) becomes  > , which obviously holds if and only if 1 < 1, which in turn is equivalent to H -  H + = 0. Consequently, given the full rank condition 0 > 0, coercivity is equivalent to the past and the future spaces of y having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

¯ ) be in stochastically balTheorem 7.3. Let (7.1) be positive real, and let (A, C, C anced form. Then the reduced-degree function (7.15) obtained via principal subsystem decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so ¯1 , 1 0 ) is a minimal realization of Z1 . is Z1 , and (A11 , C1 , C 2 For the proof we need the following lemma, the proof of which is given in Appendix D. Lemma 7.4. Let the matrix function Z be given by (7.1), where 0 > 0, but where ¯ A ) are not necessarily observable, and suppose that (6.6) has two (C, A) and (C, positive definite symmetric solutions, P1 and P2 , such that P2 - P1 > 0. Then Z is strictly positive real. Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is stable. To this end, we note that P is the reachability gramian of (3.7). In particular, ¯ ) is stochastically balanced, the reachability gramian of the system (3.18) if (A, C, C equals  so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable. By Remark 7.2, coercivity of  implies that -1 -  > 0, from which it follows 1 that - 1 - 1 > 0 and that 0 > 0. Moreover, By construction, M1 (1 )  0 and -1 M1 (1 )  0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is. To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows by symmetry. By regularity condition (7.17), 0 - C1 1 C1  0 - C C > 0, and consequently, since M1 (1 )  0, 1 satisfies the algebraic Riccati inequality ¯1 - A11 P1 C1 )(0 - C1 P1 C1 )-1 (C ¯1 - A11 P1 C1 )  0, (7.19) A11 P1 A11 - P1 + (C but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable. Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density 1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a unique symmetric P1- > 0 which satisfies (7.19) with equality and for which ¯1 - A11 P1- C1 )(0 - C1 P1- C1 )-1 C1 1- := A11 - (C is stable. It is well-known (Faurre et al., 1979) that P1- is the minimal symmetric solution of the linear matrix inequality M1 (P1 )  0, i.e., that any other symmetric 1 -1 solution P1 satisfies P1  P1- . We also know that M1 (- 1 )  0. Next, since 1 - 1 1 > 0, a fortiori it holds that Q := - 1 - P1- > 0. A tedious but straight-forward calculation shows that Q satisfies 1- (Q-1 - C1 R-1 C1 )-1 1- - Q  0, from which it follows that Q-1 - C1 R-1 C1 - 1- Q-1 1-  0. Cf. Faurre et al. (1979), pp. 85 and 95. (7.20) (7.18)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a  and a   C such that [C1 , I - A11 ]a = 0. and therefore, in view of (7.20), (1 - ||2 )a Q-1 a  0.

Cr

But  is an eigenvalue of the stable matrix A11 , implying that || < 1, so we must have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable. A remaining question is whether there is some balanced order-reduction procedure in discrete time which preserves both positivity and balancing. That this is the case in continuous time implies that the answer is affirmative, but the reduced system cannot be a simple principal subsystem truncation. ¯ ) be in stochastically Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, C balanced form. Moreover, given a decomposition ( 2.12) such that r+1 < r , let Ar Cr ¯r C r0 = = = = A11 - A12 (I + A22 )-1 A21 C1 - C2 (I + A22 )-1 A21 ¯1 - C ¯2 (I + A22 )-1 A12 C ¯2 - C ¯2 (I + A22 )-1 C2 0 - C2 (I + A22 )-1 C

¯r , r0 ) is a minimal realization of a strictly positive real function Then (Ar , Cr , C ¯r + 1 r0 . Zr (z ) = Cr (zI - Ar )-1 C 2 (7.21)

¯r , r0 ) is stochastically balanced with canonical correlation coeffiMoreover, (Ar , Cr , C cients 1 , 2 , . . . , r . To understand why this reduced-order system does preserve both positivity and balancing, note that for   I -A12 (I + A22 )-1 0 I 0 T = 0 -1 I 0 -C2 (I + A22 ) we obtain   ¯r - Ar 1 Cr 1 - Ar 1 Ar  C ,    T M ()T =  ¯r - Cr 1 A  r0 - Cr 1 C C r r

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the reduced-order system, Mr (1 )  0 and rank Mr (1 )  rank M (). ¯r , r0 ) is precisely what one obtains To prove Theorem 7.5 we observe that (Ar , Cr , C ¯ 0 ) by the appropriate linear fractional transform to the if one transforms (A, C, C, continuous-time setting and then, after reduction, back to discrete time again as suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions The purpose of this paper is to analyze a class of popular subspace identification procedures for state space models in the theoretical framework of rational covariance extension, balanced model reduction, and geometric theory for splitting subspaces. We have pointed out that these methods are based on the hidden Assumption 2.1 which is not entirely natural and which is in general untestable. The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially equivalent to the Ho-Kalman algorithm in which the basic factorization is performed by singular-value decomposition of a block Hankel matrix of finite covariance data, as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in orthonormal coordinates and allows for theoretical verification of the truncation step. A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of the data equals the algebraic degree. To achieve this, one must assume that the data are generated exactly by an underlying system and that the amount of data is sufficient for constructing an accurate partial covariance sequence the length of which is sufficient in relation to the dimension of the underlying system. Hence it is absolutely crucial that a reliable upper bound of the dimension of the "true" underlying system is available. We stress that these stringent assumptions are not satisfied for generic data, as was pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that the positive degree has no generic value. In fact, just for the moment considering the single-output case, for each p such that r  p   there is a nonempty open set of partial covariance sequences having positive degree p in the space of sequences of length  . Secondly, for any r, it is possible to construct examples of long partial covariance sequences having algebraic degree r but having arbitrarily large positive degree (Theorem 2.4). In Section 7 we proved an open question concerning the preservation of positivity in the original (discrete-time) model reduction procedure of Desai and Pal (1984). Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to the principal subsystem truncation used in van Overschee and De Moor (1993), but not to the one in Aoki (1990). We prove that positivity is preserved provided that the original data satisfies Assumption 2.1, justifying setting the smaller canonical correlation coefficients equal to zero. Unlike the situation in continuous time, this truncation does not preserve balancing. The validity of the corresponding procedure of Aoki (1990) has not been settled. The contribution of this paper is to provide theoretical understanding of these identification algorithms and to point out possible pitfalls of such procedures. Hence the primary purpose is not to suggest alternative procedures. Nevertheless, we would like to point out that a two-stage procedure equivalent to covariance extension followed by model reduction would work on any finite string of data, thus elimination the need for Assumptions 2.1. However, we leave open the question of how such a procedure should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension or some other. Acknowledgment. We would like to thank the referees and the associate editor for the careful review of our paper and for many useful suggestions, which have led to considerable improvements of this paper. References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31­73. 2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner. 3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley. 4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM J. Control, 13, 162­173. 5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag. 6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems and Control Letters, 2, 2301­2312. 7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization of modelling filter. Inter. J. of Control, 50, 2301­2321. 8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted for publication. 9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40. 10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in Kalman filtering. IEEE Trans. Autom. Control, 36, 563­579. 11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast filtering algorithms. SIAM J. Control and Optimization, 32, 744­789. 12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105­1112. 13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29, 1097­1100. 14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821­839. 15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer. 16. Faurre, P. (1969). Identification par minimisation d'une representation Markovienne de processus aleatoires. Symposium on Optimization, Nice. 17. Faurre, P. and Chataigner (1971). Identification en temp reel et en temp differee par factorisation de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt. 18. Faurre, P., M. Clerget, and F. Germain (1979). Op´ erateurs Rationnels Positifs, Dunod. 19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de r´ ealisation stochastique. C. R. Academie Sciences Paris 268. 20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York. 21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438­449. 22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and their L error bounds. Intern. J. Control, 39, 1115­1193. 23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and its Applications, 50, 277­319. 24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York. 25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling of time series. Econometric Reviews 11, 379­396. 26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321­377. 27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744­746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference, Tel Aviv, Israel. 29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory, McGraw-Hill. 30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta Polytech. Scand., MA31, 9­39. 31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499­513. 32. Kung, S. Y. (1978). A new identification and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705­714. 33. Larimore, W. E. (1990). System identification, reduced-order filtering and modeling via canonical variate analysis. Proc. 29th Conf. Decison and Control, pp. 445­451. 34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809­857. 35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241­333. 36. Lindquist, A. and G. Picci (1994a). On "subspace methods" identification. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds., Akademie Verlag, 315­320. 37. Lindquist, A. and G. Picci (1994b). On "subspace methods" identification and stochastic model reduction. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 397­403. 38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE Trans. Automatic Control, 20, 396­399. 39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica, 13, 347­357. 40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and Optimization, 29, 1251­1287. 41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identification problem. Automatica, 29 , 649-660. 42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica, 30, 75­93. 43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identification algorithms and its interpretation. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 145­156. 44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382­387. 45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes. Linear Algebra and its Applications, 205-206, 957-1003. 46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day. 47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II. Journal fur die reine und angewandte Mathematik, 148, 122­145. 48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401­ 421. 49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation. IEEE Trans. Automatic Control, AC-16, 621­634. 50. Whittle, P. (1963). On the fitting of multivariate autoregressions and the approximate canonical factorization of a spectral density matrix. Biometrica, 50, 129­134. 51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its Applications, Cambridge U.P. 52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via Ho's algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4. We first give a proof for the special case n = 1. Consider a scalar function 1z+b (A.1) 2z +a with a scalar sequence (1.4) such that 0 = 1. Now it is well-known [see, e.g., Schur (1918), Akhiezer (1965)] that T is positive definite if and only if Z (z ) = |t | < 1 t = 0, 1, 2, . . . ,  - 1 (A.2) where {0 , 1 , 2 , . . . } are the so called Schur parameters. There is a bijective relation between partial sequences (1.1) and partial sequences {0 , 1 , . . . ,  -1 } of the same length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the Schur parameters of (A.1) are generated by the nonlinear dynamical system t+1 = t+1 =
t 2 1 - t - t  t 2 1 - t

0 = 1 (a + b) 2 0 = 1 (b - a) 2

(A.3)

and that Tt becomes singular precisely when there is finite escape. It was also shown in Byrnes et al. (1991) that {t } is generated by a linear system 2/ -1 ut+1 = vt+1 1 0 ut , vt (A.4)

where t = vt /ut and  := (a + b)(1 + ab)-1 . If  is greater than one in modulus, the coefficient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant scalar factor, similar to cos  sin  , - sin  cos   where  := arctan 2 - 1. Hence t is the slope of a line through the origin in R2 which rotates counter-clockwise with the constant angle  in each time step. Consequently, arctan t+1 = arctan t + . Moreover, assuming that 0 > 0, the Schur condition t < 1 will fail as soon as t+1 negative or infinite, as can be seen from the first of recursions (A.3). Hence (A.2) holds if and only if  (A.5) arctan  < . 2 Therefore for a small > 0, take a = 1 - and b = 1 + , yielding a stable Z . Then  2 4 - 2 . We may choose so that  = 2- 2 > 1 and  = arctan 2- 2   << , +1  - arctan 0 . Then (A.5) holds so that T > 0, but we also have where  :=  2  arctan  +1 > 2 so that T +1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function (a + b)n-1 (z ) 1 n (z ) + 1 2 Z (z ) = 1 2 n (z ) + 2 (a + b)n-1 (z ) o polynomials of the first and second kind rewhere {t } and {t } are the Szeg¨ spectively (Akhiezer, 1965). The function Z has the property that its first n Schur parameters, {0 , 1 , . . . , n-1 }, are precisely the data which uniquely determines n , n-1 , n and n-1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated by t 0 = 1 (a + b) t+1 = 1- 2 2 t+1 =
- t  t 2 1- t +n-1
t+n-1

Hence, we have reduced the problem to the case n = 1. If we choose the initial Schur parameters sufficiently small so that n (z ) and n-1 (z ) are approximately z n and z n-1 , n (z ) + 0 n-1 (z ) is stable if we choose a := 1 - 2 and b := 1 + for some small > 0. Then  > 1 and the proof for the case n = 1 carries through with a trivial modification. Appendix B. The Hilbert space of a sample function Let y = {y(t)}t0 be a zero-mean wide-sense-stationary stochastic process defined on a probability space {, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t,  ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit exists, the m × m matrix function k  k obtained from a particular trajectory is then a bona-fide covariance function. [The continuous-time analog of this property was observed already by Wiener (1933)]. If moreover the sample limit is (almost surely) independent of the particular trajectory and hence necessarily coincides with the "ensemble" covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in the book of Hannan (1970). It is obvious from Birkhoff's ergodic theorem that any (zero-mean) strictly stationary ergodic process is also second-order ergodic. In this Appendix we shall show that the properties of the Hilbert space structure associated to a stationary time series y , defined on page 10, are identical to those of the Hilbert space induced by a second-order ergodic process.10 The two frameworks, i.e., the statistical "time-series" structure and the "probabilistic" structure, are in fact isomorphic. To see this, pick a "representative" trajectory of y, i.e. one in the subset of  (of probability one) for which the limit (1.11) exists. Clearly there will be no loss of generality in assuming that the probability space  of y is the "sample space", of all possible trajectories of y, i.e. the set of all semi-infinite sequences  = {0 , 1 , 2 , . . . }, t  Rm . With this choice, A will be the usual  algebra of cylinder subsets of  and the t:th random variable of the process, y(t), is just the canonical projection function y(t,  ) :   t .
For a process of this kind the Hilbert space H (y) is the closure in L2 (, A, P ) of the linear vector space generated by the scalar random variables   yi (t,  ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence of m ×  matrices y := {y (k )}k0 as in (3.3). For  in the subset of  where the time averages converge, define the map T , T : a y(t)  a y (t) t  0 a  Rm associating the i:th scalar components of each m-dimensional random vector y(t) of the process to the corresponding i:th (infinite) row of the m ×  matrix y (t) constructed from the corresponding sample path {y(t,  ); t  Z}. By second-order ergodicity, the set of all such    will have probability measure one and the map T will in fact be norm preserving, since by construction we have t-s = E y(t)y(s) = Ey (t)y (s) , where t is the covariance matrix of y. The map T can then be extended by linearity and continuity to a unitary linear operator T : H (y)  H (y ) which commutes with the action of the natural shift operators (both of which we denote U), in these two Hilbert spaces: H (y) -H (y)  T T  H (y ) -H (y ) This isomorphism allows us to employ exactly the same formalism and notations used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991) in the present statistical setup, where we build estimates of the parameters of models describing the data in terms of an observed time series instead of stochastic processes. This provides a remarkable conceptual unity and admits a straightforward derivation in the style of stochastic realization theory of the formulas in the paper van Overschee and De Moor (1993), there obtained with considerable effort through lengthy and formal manipulations. Appendix C. The invariant form of the Kalman filter Given a stationary stochastic system (3.7), the Kalman filter is usually determined via the matrix Riccati equation Q(t + 1) = AQ(t)A - [AQ(t)C + BD ][CQ(t)C + DD ]-1 [AQ(t)C + BD ] + BB (C.1) where Q(0) = P := E{x(0)x(0) }. Here Q(t) = E{[x(t) - x ^(t)][x(t) - x ^(t)] }, and the Kalman gain is given by K (t) = [AQ(t)C + BD ][CQ(t)C + DD ]-1 . (C.3) (C.2)
U U

These equations of course depend on P , B and D, which vary as the splitting subspace ¯ ) is invariant if a uniform choice of bases is made. X varies over , whereas (A, C, C ¯ ) and hence However, as shall see, the gain K depends only on the triplet (A, C, C one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

¯ ), and hence are invariant over . Clearly, in view of Theorem 5.1, P- (t), on (A, C, C as defined by (5.12), has this property. Moreover, Q(t) = P - P- (t), and, consequently, in view of (3.9), and the Lyapunov equation P = AP A + BB , P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11). A symmetric argument yields the backward equations. It is easy to see that as Q(t)  Q monotonously, P- (t)  P- , and hence P  P- , as should be. Appendix D. Some deferred proofs Proof of Theorem 5.1. Since X is a splitting subspace for the infinite past H - and the - + := U  H - and H := U  H + . But infinite future H + , by stationarity, X splits H - - + + - + Y  H and Y  H , and hence X splits Y and Y also. (See, e.g., Lindquist and Picci (1985, 1991).) Now, using the projection formula in the footnote of page +  Y+ 16, we have for any b y   -1  1 2 . . . 0 1 . . .  2 3 . . .  +1  1 0 . . .  -1  - + -  .  y E Y b y =b  . . . . .. ..  .    . . . . . . . . . . . . . .   +1 · · · 2 -1   -1 · · · 0 - ¯  (T- )-1 y = b   = b   ¯  are appropriate finite-dimensional observability and constructibility where  and  ¯  such matrices (2.6) of full rank. If  > 0 , there is a minimal factorization H =   - -1 - ¯ that  :=  (T ) y has n components, and ¯  > 0. ¯  (T- )-1  E { } =  ^  - , dim X ^  -  n = dim X so, since Therefore, since the components of  belong to X ^ ^ X - is minimal, X must also be minimal and X - be spanned by the components of  . Next, from the backward system (3.14) we see that - ¯x = ¯( ) + terms ortogonal to X , y and therefore, by the same projection formula,
- ¯ (T - )-1 y - = a . E Y a x( ) = a E {x( )¯ x( ) }    - ^  - , establishing the first of identities Consequently, E Y X = {a  | a  Rn } = X (5.7). The second follows from a symmetric argument. The representation formula (5.8) follows from the minimality of X as a splitting subspace for Y+ and Y- , which, in particular, implies that the constructibility operator, -  ^ Ct := E|Y X : X  X -

X

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n, ^k ( ). there is a unique random variable xk ( )  X whose projection onto Y- is x To show that x(0) form a uniform choice of bases as X varies over , first take X to be the stationary backward predictor space X+ and let x+ ( ) be the unique basis - ^( ) = E Y x+ ( ). Now, let X  be arbitrary. Then, since in U X+ such that x -   + X is a splitting subspace for Y and U X+  U H (Lindquist and Picci, 1991, Proposition 2.1(vi)), we have

X

X

x ^( ) = E Y x+ ( ) = E Y E X x+ ( ), and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all X  , which is a well-known characterization of uniform choice of bases; see Section 6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields the corresponding statement for (5.9).

-

-

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a corresponding partial covariance sequence , where  is large enough for the Hankel ¯ ) be the triplet determined matrix H , defined by (1.5), to have rank n. Let (A, C, C from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the covariance data by estimates {0T , 1T , . . . , T } ¯T ) be the corresponding triplet obtained via (2.5). of type (6.3), and let (AT , CT , C We want to prove that ¯T + 1 0T ZT (z ) := CT (zI - AT )-1 C 2 is strictly positive real for a sufficiently large T . Now, if deg ZT = deg Z , replace  by -1 0  0 in (2.5) in the appropriate , U by U 0 , V by V 0 , and -1 by 0 0 0 0 ¯ ) and (AT , CT , C ¯T ) have the same dimensions. This will calculation so that (A, C, C ¯T , 0T ) can be made arbitrarily close not affect Z and ZT . By continuity, (AT , CT , C ¯ to (A, C, C, 0 ) in any norm by choosing T sufficiently large. Thus the same holds for max Z (ei ) - ZT (ei )
[0,2 ]

and hence, since (z ) := Z (z ) + Z (z -1 ) satisfies (3.10), so will T (z ) := ZT (z ) + ZT (z -1 ) for sufficiently large T . Moreover, since |(A)| < 1, we have |(AT )| < 1 by continuity for sufficiently large T . Consequently, there is a T0 such that ZT is strictly positive real for T  T0 . ¯) Proof of Theorem 5.3. Let Z , defined by (1.6), be strictly positive real, and let (A, C, C be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , defined by ¯1 ), is also strictly (7.15) in terms of the principal subsystem truncation (A11 , C1 , C positive real. We want to prove that this property is carried over to rational matrix function ¯ )1 ) + 1 0 Z 1 (z ) = (C )1 (zI - (A )11 )-1 (C 2 for  sufficiently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let Q be defined by (5.32). Since the canonical correlation coefficients (5.25) tend to the canonical correlation coefficients (4.12) as   ,   . Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P- (t) tends to Q Q as t   if the initial condition is taken to be P- ( ) =  . Consequently, for any > 0, there is a sufficiently large  such that  -  < 2 and  - Q Q < 2 so that  - Q Q < . Hence Q tends to a limit Q with the property  = Q Q . Using the same argument in the backward direction, the T -1 second of relations (5.33) shows that Q also satisfies  = Q-  Q . Consequently, by the same argument as in the proof of Theorem 4.4, Q is a signature matrix, and hence in particular diagonal. Therefore,
1 -1 ¯ ¯ )1 )  ((Q )11 A(Q )- ((A )11 , (C )1 , (C 11 , C (Q )11 , C (Q )11 ) as   ,

where (Q )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, Z 1  Z1 . Hence, since Z1 is positive real, then so is Z 1 for  sufficiently large. ¯ ) is a minimal triplet. Proof of Lemma 7.4. Let us first consider the case when (A, C, C Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P- and P+ respectively, which, in particular, have the property that P-  P1 and P2  P+ . Then, in view of (7.18), P+ - P- > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem A4.4). Next, let us reduce the general case to the case considered above. If (C, A) is not observable, change the coordinates in state space, through a transformation ¯ ), so that ¯ )  (QAQ-1 , CQ-1 , QC (A, C, C ^ 0 C= C A= ^ 0 A   ^ ¯= C ¯  , C

^ A ^) is observable. Then, if P1 and P2 have the corresponding representations where (C, P1 = ^1  P   P2 = ^2  P ,  

^2 satisfy the reduced version of the linear matrix ^1 and P it is easy to see that P ^) and that, in this new ¯ ) for (A, ^ C, ^ C ¯ inequality (6.6) obtained by exchanging (A, C, C ^ ^2 - P ^1 > 0. If (C, ¯ A ^ ) is not observable, we proceed setting, (7.18) holds, i.e., P -1 -1 ^2 ^1 and P satisfy the by removing these unobservable modes. First note that P ^ ^ ^ C, ^ C ¯ ) by (A ^ , C, ¯ C ^ ). Then, dual linear matrix inequality obtained by exchanging (A, changing coordinates in state space so that ^ ~ ¯= C ¯  C ~ ^ = A A  0  ^= C ~ 0 , C

^ ¯ A ~ ) observable, and defining with (C, ~ -1  P -1 ^1 P = 1   ~ -1  ^ - 1 = P2 P , 2  

44

ANDERS LINDQUIST AND GIORGIO PICCI

~ ~ C, ~ C, ¯ 1 0 ) is a minimal realization of Z . Moreover, P ~1 and P ~2 satisfy we see that (A, 2 the corresponding linear matrix inequality (6.6) and have the property (7.18) in this setting. Hence the problem is reduced to the case already studied above. Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans-1 , mapping formed to the continuous-time setting via a bilinear transformation s = z z +1 the unit disc onto the left half plane so that Zc (s) = Zd 1+s 1-s (D.1)

is positive real in the continuous-time sense if and only if Zd is positive real in the discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al. ¯d , 1 0 ) and (Ac , Cc , C ¯c , 1 R) are realizations of Zd and Zc (1979)] that, if (Ad , Cd , C 2 2 respectively, we have  Ac = (Ad + I )-1 (Ad - I )    C = 2C (A + I )-1 c  d d (D.2) ¯ ¯d (A + I )-1  Cc = 2C  d   ¯ -C ¯d (A + I )-1 C R = 0 - Cd (Ad + I )-1 C d d d and inversely  Ad = (I - Ac )-1 (I + Ac )    C = 2C (I - A )-1 d c  c -1 ¯ ¯  = 2 C ( I - A C d c  c)   ¯ +C ¯c (I - Ac )-1 Cc 0 = R + Cc (I - Ac )-1 C c

(D.3)

Under this transformation the observability gramian and the constructibility gramian ¯d , 1 0 ) is ¯ A )) are preserved so that (Ad , Cd , C (i.e., the observability gramian of (C, 2 ¯c , 1 R) is; see, e.g., p. 1119 in Glover a minimal realization if and only if (Ac , Cc , C 2 (1984). Moreover, coercivity is preserved, and the solution sets of the corresponding linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the reachability gramian of a spectral factor and this gramian is also preserved.) Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact, transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting, all the requirements of Theorem 7.1 are satisfied. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order positive real function thus obtained via (D.3) back to discrete time, the desired result is obtained.

Towards Target-Level Testing and Debugging Tools for Embedded Software
Harry Koehnemann, Arizona State University Dr. Timothy Lindquist, Arizona State University

Abstract

The current process for testing and debugging embedded sojware is ine~ective at revealing errors. There are currently huge costs associated with the validation of embedded applications. Despite the huge costs, the most dl~cult errors to reveal and locate are found extremely late in the testing process, making them even more costly to repm"r. This paper first presents a discussion of embedded testing research andpractice. This discussion raises a need to improve the existing process and tools for embe&@i testing as well as enable better processes and tools for the jWure. To fmilitate this improvement, architectural and software capabilities which support testing and &bugging with minimal intrusion on the executing system must be developed. Execution visibility and control must come @om the underlying system, which should ofJer interjbces to testing and debugging tools in the same numner it offers them to a compiler. Finally we propose txtenswns to the underlying system, which consists of adiiitions to both the architecture and run-time system that will help reulize target-level tools. 1. Introduction Software validation involves many activities that take place throughout the lifecycle of soft w are development. A substantial portion of the validation process is software testing, which is the development of test procedures and the generation and execution of test eases. Notice we are not only concerned with the generation of a test case, but are also concerned with how that test is executed. Therefore, a test case is not simply composed of inputs to a system, but rdso includes any environmental factors. Other research has examined the issues behind test case selection, but few are addressing the problems that surround the execution of those test cases. The goal of this paper is to identify the problems associated with test case execution for embedded systems and to propose solutions for making embedded testing more effective at revealing errors.
Testing and Debugging Process Many of the activities, tools, and methods used during software testing are shared by software debugging. Software testing is concerned with executing a piece of software in order to reveal errors, while software debugging is concerned with locating and correcting the cause of an error once it has been revealed. Though these two activities are often referenced separately, their ac$ivi$ies are tightly coupled and share many common features.
Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise or republish, requires a fee and/or specific permiss~m.

During debugging, a developer must recreate the exact execution scenario that revealed the fault during testing. Not only must the code execute the same instruction sequences, but all environmental variants must be accounted for during the debugging session. In addition, the tools assisting in the debugging process must providing a &veloper with a certain degree of execution visibility and control while not impacting the execution behavior of the program. Embedded Systems The testing and debugging process is greatly restricted by embedded systems. Embedded applications are among the most complex software systems being developed today. Such software is often constrained by q Concaumnt designs q RcaI-time constraints q l%lbedded target Imvilrmments q Distributed hardware ambitectures q Device control dependencies Each of these properties of embedded software severely restrict execution visibility and control, which conseqmdy restricts the testing and debugging process. Our current methods and tools for software testing and debugging require a great deal of computing resources. Such resources are not available on the target environment. Therefore, a large gap exists between the methods and tools used during evaluation on the host and those used on the target. Unfortunately, mauy errors are only revealed during testing in the target environment. Because of the above issues, concerns are raised over the effectiveness of software validation for embedded systems. Embedded applications are responsible for controlling physical devices and their correct execution is critical in avoiding and/or recovetig from device failure. Often these physical devices control life-critical processes, making the embedded software a key element of a lifecritical system. Software failure can lead to system failure, which in turn could lead to loss of life. In addition, designers are increasing their use of embedded software to conuol the physical elements of large systems. This rate of increase is likely to increase as the cost for embedded controllers becomes cheaper and more attractive when compared with other mechanical techniques. Computer networks are fast replacing point-to-point wiring, due to the networks light weight, easy cotilgurability and expansibility, and lower design complexity. The advancement in the complexity of problems addressed by software in these types of 1.2

1.1

01993

ACM

0-89791-621

-2/93/0009--0288

1.50

applications may soon be limited satisfy reliability needs and concerns.

by our inability

to

2. Software Testing The software testing phase is concerned with executing a software program in order to reveal errors. Software testing for embedded systems takes place in four basic stages: 1) Module Level Testing 2) Integration Testing 3) System Testing 4) Hardware/Software Integmtion Testing The first three testing stages are typical of any software product. Testing begins with exercising each soft ware module and concludes when the software is shown to meet system specifications by passing some rigorous set of system tests. The fourth phase is unique to embedded systems. The software must not only be correct, but must also interface properly with the devices it is controlling. Testing literature contains countless methodologies, techniques, and tools that support the software testing process. They range from software verification and program proving to random test case selection. All testing methods indirectly apply to embedded systems, as they do all software. Of particular interest to this paper are those techniques that address the problems identified for embedded software - concurrency, real-time constraints, embedded environment, etc. Unfortunately, there exists little research into the unique with testing embedded software. problems SSSOCilltd
Testing Concurrent Systems Concurrency increases the difficulty of software testing, Given a concurrent program and some set of input, there exists an unmanageably large set of legal execution sequences the program will take. Furthermore, subsequent execution with the same input may yield different, yet correct results due to differences in the operating environment. This is all complicated by Ada's nondetermins tic select construct. Therefore, when testing concurrent software, we are not only concerned with a valid resuk but must also be concerned with how the program arrived at that result. Since multiple executions of a concurrent program may yield different results, it is not enough to ensure that the system produces the correct output for a given input. One must also ensure that the system always produces an acceptable output for each execution sequence that is legal under the language definition. Without sufficient control over program execution, there is no way of ensuring a given test is exerasing the code it was intended to test. Taylor and Osterweil ~ay180] examined static analysis of concument programs, However, this research considered processes in isolation and does not consider interprocess cmrummication. Taylor later extended this work to Ada and a subset of the Ada rendezvous mechanism ~ay183]. Through this static aualysis technique, one could determine aIl parallel actions and states that could block a task from executing. This method, as with most static 2.1

techniques, must examine a large set of statw and therefore must constrain itself to small, simple programs. Research in dynamic testing of concurrent Ada programs has largely focused on the detection of deadlocks ~emb85], the saving of event histories KeDo85, Maug85], and other tec.huiques that passively watch a program execute then allow the execution sequences to be replayed after a failure has been detected. Hanson Eans78] was among the first to discuss run-time control of concment programs. In order to regulate the sequences of events, he assigned each concurrent event in the test program a unique time value. He then introduced a test clock that regulated the system during execution. A given event could only execute if it's time was greater than that of the clock. Tai ~ai86, Tai91] extended Hanson's work to the Ada programnn "ng language. His method takes an Ada program P and a rendezvous ordering R and produces a new Ada program P' such that the intertask communication in P' is always R. A similar approach was used in Koeh89] to apply these techniques to testing and debugging tools. This work addressed the facl that in order to, test a specific program state, values in a program may need to be modified during run-time. Modification o~f the program state is a capability provided by any debugging tool and is a required property of a tool debugging tasked programs. It is important to note that both techniques explicitly perform rendezvous scheduling, removing those decisions from the run-time system and placing control in the hands of the tool. Non-intrusive testing Intrusion plays a significant role in the testing and debugging of embedded software. Any technique used to raise execution visibility or provide for program control must not interfere with the behavior of the teat program. Embedded applications have strict timing requirements and any intrusion on a test execution will likely make that test void. Intrusion is typical for host-based testing, but becomes a large problem for target-level testing and debugging activities. The above approaches address the need for visibility, control, and predictability for testing concurrent software. However, they are all intrusive and use instrumentation (inserting probes into a usem program and rewriting certain constructs before submission to the compiler) to gather run-time information and to control ~gram exmtion. After the probes are added, the user's object code is linked with the rest of the debugging system and then executed under test. This additional code has a serious impact on the execution behavior of the program. Instrumentation is not appropriate for testing real-time, embedded applications. A non-intrusive debugger for Ada is proposed in [Gil188]. A separate processor executes the testing system and communicates with the target processor through some special purpose hardware. Lyttle and Ford ~ytt90] have also implemented a non-intrusive embedded debugger for Ada. Their tool provides monitoring, breakpoints, and

2.2

289

display facilities for executing embedded applications. While these efforts provide an excellent start towards targetlevel tools, they do have severe limitations. These implementation do not deal with high level activities such as task interactions and are only concerned with items that can be translated from monitoring system bus activity. As discussed later in Chapter 5, techniques dependent on bus activity will likely fail for future architecture designs. In addition, many of the error detwted in the target environment are indeed concerned with high-level activities (process scheduling and interactions, fault handling, interrupt response, etc.). Other real-time, embedded tools have been proposed for crossdevelopment environments. They can typically be classified into one of the following three categories: 1) ROM monitors, 2) Emulators, and 3) Bus monitors. These types of tools will be further discussed later in this paper. of the Underlying System One of the large problems with testing concurrent systems is dealing with abstraction. The Ada programming language abstracts concurrent activities through task objects ~D83]. Tasks allow a developer to abstract the concepts of concurrency and interprocess cxmmmnication and discuss them at a high level. The burden of implementation is then placed on the compiler, and typically the run-time system. While abstraction is a powerful design tool, it leads to significant prthlems during the testing phase of software development. Implementation details become buried in the underlying system. At the development level, this high degree of abstraction is appropriate. However, abstraction complicates the testing process. Not only are we concerned with implementation details, but we must aIso control them to demonstrate that certain properties about a program will hold for every legal execution scenario. Without sufficient control over program execution, there is no way of ensuring that a spedc test is exerasing the code it was intended to evaluate. In addition, cmmxt operation in one environment (host) does not necessarily imply comet operation in another (target) due to implementation difference in the underlying system. The underlying system is composed of two parts, the features of the hardware architecture and the operations provided by the run-time system. As language constructs become more abstract, compilers are required to generate more code to implement them. There is no longer a trivial mapping from language construct to machine instruction. Rather, the compiler must provide an algorithmic solution in order to implement these high level constructs. Those solutions exist as operations in the run-time system. Rather than generate code for these constructs, the compiler generates a call to a run-time system operation or servim. As the constmcts become more abstract, compilers develop an increasing dependency on the underlying system. This increase in shown in figure 2.1. As new constructs are introduced to programming languages, their increase in abstraction is greater than that

of hardware and the run-time system is called upon to bridge the impending gap. No argument is made as to the rate of increase identified by the line slopes; nor is an argument ma& that these increases are even linear.

c
o
m P 1 e x

i~ t

L=
Time Figure 2.1

Language

constructs

Hardware

Y

2.3 Impact

Embedded systems raise many problems for software testing and debugging. Such software typically must deal with concurrency, real-time constraints, au embedded target environment, distributed hardware architectures, and a great deal of hardware-software interfaces for controlling externaI devices. These issues tdone do not provide a complete view of the problems SyStetUS are extcotttttered by embedded testing. bbedded typically developed on custom hardware configurations m@ng that each system introduces it's own unique problems. Tools and techniques that apply to one are not generally applicable on another, which leads to ad hoc approaches to integration and system testing of embedded software. The program is executed for some length of time and continual y bombarded with inputs in an attempt to show it adheres to some speeifkation, Current state of embedded testing As described earlier, the testing process for embedded systems consists of 4 phases that conclude with Hardware/Software (H/S) Integration During H/S integration testing, device and timing related errors are reveakd. These errors eneompass problems such as: q incorrect handling of interrupts q distributed communication problems q incorrect ordering of concumen t events q resource contention q incorrect use of device protocols and timing " incomect response to failures or transients These errors are often extremely difficult problems to fix and often require significant modifications to the software system. In addition, software is for~d to conform to custom hardware that may itself have errors. As stated above, H/S integration is the last phase of testing for an embedded system. Since errors are much cheaper to fix the earlier they are revealed,. why would one wait until the last phase of product development to reveal the most diftlcult to 3.1

290

locate, costly errors to fix? Our goal should be to reveal these errors as early as possible. Unfortunately, target level testing tools have yet to become a reality. The target processor of au embedded computer is typically minimal in function and size. It is only a small portion of a larger system, whose goals are to minimize cost and space. Therefore, target hardware of au embedded systems will not support software development nor any development tools. To resolve this problem, the source is developed on a larger host platform and cross axnpilers and linkers are used to generate code and download it to the target processor. Consequently, two environments exist in our development process, the host environment and the target environment, each having completely different functionality and interface to a user. Tools that run on the host provide a high level interface and give users detailed information on and control over their program execution. However, little is provided on the target. Typically, the best information obtainable is a low-level execution trace provided by an in-circuit emulator. Current Solutions Approaches to dealing with the above problems can be divided into hardware solution and software solutions. The hardware solutions are attempts at gaining execution visibility and program control and include the bus monitors, ROM monitors, and in-circuit emulators. A bus monitor gains visibility of an executing program by observing data and instructions transferred across the system bus. With a ROM monitor, debugger code is placed into ROM on the target board. When a break point is encountered, control is transfered to the debug code which can accept commands from the user to examine and change the program's state. Finally, an in-circuit emulator connects with a host system across an ethernet connection. At the other end, a probe replaces the processor on the target board. The emulator then simulates the behavior of the processor in (ideally) real-time, which allows the emulator to tell the outaide world what it's doing while it's doing it. The hardware solutions have mini m al effectiveness for software development. They can only gather information based on low-level machine data. The developer must then create the mapping between low-level system eventa and the entities defiied in the program. That -ping is the implementation strategy chosen by a given compilation system and becomes severely complicated for abstractions such as tasks Maintaining an understanding of the mapping is extremely difficult and cumbersome. The software solutions cart be viewed as attempts to reduce the tremendous costs of testing on the target. Several factors determine how a pitxe of software is tested 1) Level of criticality of software module Each software module is assigned a different level of criticality based on it's importance to the overall operation of the system. 2) Test platform availability

3.2

Typically, there will exist several test environments available to test a piece of soft ware, each providing a closer approximation to the actual target environment: q Host-baaed sours level debugger q Host-based instruction set simulator q Target emulator q Integrated validation faality 3) Test Classification The tests to be performed can be categorized to determine what they are attempting to demonstrate. The goal of a test plays a large role in determining the platform on which it will execute. Some examples are shown below. q Algorithmic q Inter-module q Intra-module q Performance q HE integration q InttX-cabinet Each of these factors play a role in assigning program modules to the various test platforms based on some criteria that might contain the following: q Type of software q Hardware requirements q Test chssifkation q Platform availability q Coverage requirements q Test support software availability (drivers, stubs) q Certification Requirements q Level of effort required for test This criteria takes into account the 3 factors discussed above as well as additional ones. The software solutions are an attempt to minimize the time spent testing in the target environment. Validation facilities are expensive to build and time utilimd for testing is expensive. This is due to the f;act that target level testing occurs extremely late in the development lifecycle and only a small window is allocated for HAS integration testing. However, the target is the only location that can reveal tin errors. It is ironic that our current solutions attempt to reduce the anmunt of target testing, but will likely lead to extensive modifications and thercfom extensive retesting.

4.

Problems with Embedded Irestinq The solutions proposedaboveare not effective at

revealing errors. Effective implies that a technique reveals a high percentage of the errors and that it does so in a costef!iaent manner. Instead, what the above tools provide is a minimal, low-level view of the execution of a program and those tools become available at a very late stage in development. Below is a list of problems associated with current approaches to embedded testing Expense of Testing Process Target testing requires expensive, custom validation facilities. The expense of these target facilities is incurred for every project, since little reuse across projects is ever realized. The effort required to build these validation facilities means that every test execution is expensive, making retests extremely costly. Yet, hardware often arrives late and full of errors, forcing software to be 4.1

291

modified and subsequently retested. This late arrival of hardware also impacts the cost of an error, since certain errom are only revealed during I-IN integrations testing. Perhaps the largest factor associated with the high costs of testing will be the questions and concerns that certification processes are beginning to raise about sofhvare tools. Typically, development tools have not been required to meet any validation criteria and certairdy not the strict criteria imposed on the development system. This luxury may soon disappear as the role tools play in the development process comes under tighter scrutiny. The huge expense of validation facilities will increase dmmaticauy. of Functionality on Target The level of functionality found on a target machine is minimal and does not support tools. This lack of functionality greatly limits the effectiveness of testing, since more time and effort is required to locate an error. While a host system provides a high-level interface and discussed software in terms of the high-level language, the target typically deals in machine instructions and physical addresses. Translating these low-level entities requires time and a great deal of tedious, error-prone activities. Errora revealed late in development lifecycle Embedded system designs often incorporate custom ASIC parta that are typicaIly not available until very late in the development process, delaying the availability of any target validation facility. In addition, errors designed into the ASICa are extremely expensive to fix, requiring new masks be created and complete refabrication. InsteaA errors in ASICs and other hardware problems are resolved by modifying the software. As stated before, this greatly delays the time which errors are revealed, which in turn increasing the cost of software testing. 4.4 Poor teat selection criteria AIl to often, tool availability diclates the quality of a testing process. Tests cases and scenarios are determined by what will work on available platforms and which test are achedulable rather than being determined by some theoretical test cxitcria. A prime example is the FAA's requirements ~AAS51 that 1) all testing be done in the target environment and 2) testing include statement coverage. Of course, test coverage is not currently measured on the target. Unfortunately, it is cheaper for a company to spend it's resources preparing an argument to obtain some form of `waiver" than to actually perform a test. In time, the argument approach will no longer be accepted and the solutions for embedded testing must be in place to accommodate this change. It will only take one implementation that performs statement coverage on the target to force every embedded, real-time software developer to perform statement coverage on the target to meet such a certification requirement.

4.S Potential use in advancing architectures Perhaps the largest problem facing embedded testing is that the current solutions cannot be applied to future h~dware architectures. Future architectures are Proposing q wider addreas Spaces q higher -Sor speeds q huge numbers of pins q internal pipes q multiple execution units q large internal caches q multi-dip modules Such complexities cast a dark shadow over the hardware solutions previously discussed. With internal caching and parallel activity being done on the chip, one will no longer be able to gain processor state information from simply monitoring the system bus. And as on-chip functions become more complex, emulator vendors will no longer be able to see into the chip through the pins making them obsolete as well. In [Chi191] an even stronger claim is made that the debugging capabilities provided by the chip will need to become more sophisticated. In future architectures, perhaps the only possibility to view and control the execution of hardware is to gain that information from the hardware itself.

4.2 Level

4.3

The previous sections raised issues about the effectiveness of our testing process and claimed that tcating is currently being limited by tool functionality. l%e goal of this paper is to identify shortcomings in the embedded testing proccas and propose a solution to those problems. The view taken by the authors is that tool support for embedded systems is lacking. Further, those approaches currently used for gaining execution visibility and control will soon be obsolete for future architectures. We propose adding facilities to the underlying system to better support testing and debugging tools for embedded software. As stated previously, the underlying system is composed of the hardware architecture and the run-time system (RTS). Both are composed of data structures and operations that implement common system abstractions such as processes, semaphores, ports, timers, memory heaps, and faultdexceptions. It should be noted that there is no distinct line between features of hardware and features of the RTS. In fact, as these features and abstractions become more standardized, newer architectures are attempting to incorporate them into their instruction sets ~Nl%92]. In addition, the implementation,of a feature may span parts of the architecture, RTS, and compiler generated code (i.e. fauhdexceptions). 5.1 Model Debugging System Below is an illustration of a debugging system (Figure 5.1). The data path from the debugging/testing tool represents symbol table information that allows the tool to map machine level information to source level

292

Test/Debug Compiler Generated Code \ Tool 1

A
e x
t e

Ada Compilation

3

"snext two and RTS

Figure 5.1 constmcts. The ASIS toolkit provides easy required for this physical connection. The implementation for this facility. ASIS is a proposed sw-tions describe `tie architecture additions standard interface between an Ada library and any tool interfaces in more detail. requiring compilation information. Of more interest is the communication path between the target processor and the testing tool. A tool sits external to the rest of the embedded system, while the RTS resides internally on the target board. At frost glance, this conceptual path seems rather difficult to realize. However, the implementation becomes easier if thought about as a typical host debugging system. Any debugging system has a least two processes executing, one running the test program and one running the &bugga. These two p= Sa common physical machine, which allows one process to gain information about the other. The debugger procem simple requires data and computation time, which it shares with the test program. This same scenario is rcqnired for embedded debugging, except that the debugger process is split. Part of the debugger process runs on the target machine and part runs on the host. The goal is to minimize the portion that must be run on the target so that it does not intrude on execution of the test program. To realize this nonintrusive execution of the debug software, the target 1) Execute debug code only at a break poinL 2) Run the debugger as a separate process, or 3) Provide a separate execution unit to execute the debugger. The details of these options are explored in depth later in this paper. The problem now lies with the interfa= between the embedded part of the debugger (intermddebuggcr) and the portion that lies on the host (external-debugger). The solution requires hardware additions that will be discussed later in this paper. A high level view is given in figure 5.2. In this figure, the tool makes logical calls to services provided by the RTS. These calls are actually implemented by the debugging system through data passed between the internal and external debuggers. Hardware additions arc -=

1

I

I

Figure 5.2

The past decade has seen hug{: advances in microprocessor designs. Several of these advancements were listed previously and include pipelining and separate functional units. The concept of partitioning a microprocessor in order to perform parallel activities is of great interest to this work. It was noted earlier that these parallel amputations severely restrict current methods for testing and debugging embedded systems, since on{e must simulate a great amount of computations. However, debugging tools can also use architectural parallelism to their advantage. If a hardware design is partitioned successfully to allow certain activities to occur concurrently, then the testing and debugging methodologies might wish to add

293

their own computational requirements to the list of parallel activities. This section will explore additions to hardware architectures. No claim is made as to the costs associated with these features. They assured y will require space (transistors) and possibly even add to the execution cycles required to implement certain instructions. 6.1 Hardware Partitioning of Memory One primary concern for industry is reducing the huge volume of retests associated with development. The current testing process ensures that errors are revealed late, which forces retesting large portions of the system. Despite correcting these problems, industry will still be faced with software that is constantly changing. Software is deceivingly easy to change and often the element of a system assigned to unknown or "risky" aspects during design. Changing software is extremely expensive late in the development for critical systems. Such systems typically have requirement that an error raised in one portion of the system won't interfere with the correct operation of the rest of the system. Current software certification agencies ~AA85] have several software restricdons including q Any modikation made to a software module forces the retesting of all other modules operating on that same physical device. . All software on a device must be developed under the highest level of criticality of any module that will execute on the same device. Without the ability of hardware to guarantee software boundaries, such requirements must be enforced. However, these requirements add a great deal of costs to software development. Consequently, software is often physically partitioned based on critical level, rather than design factors. Partitioning software modules based on critical levels greatly interferes with the design process. One would rather partition modules based on factors such as processor utilization and inter-module exmnmnication requirements. In fact, load balancing and p17XXsSmigration are techniques that would not be usable by embedded system developers unless all software is developed at the highest critical level. The solution to these issues is hardware partitioning. Each process should have it's own protected address space that is not accessible by any other process. In addition, sets of processes may wish to share memory. The processor should tdso provide the capability to restrict access to segments of memory based on some criteria. 6.2 Computational Facilities for Debugger. The debugging system is partitioned into an internal debugger and au external debugger. The internal debugger must physically exist on the target board and communicate with the external debugger through some dedicated medium. The internal debugger will also require execution from the target without interfering with the

operation of the application program. There are two possible scenarios q The internal debugger runs as a regular process on the -Or The architecture provides separate facilities to execute the internal debugger code In either case, control is transfered to the debugger when a breakpoint is encountered In the fiit scemuio, the debugger is executed by the processor as any other process. If the debugger executes as a low-level process, it would not interfere with the operation of the rest of the system. However, this is not a feasible approach. Most intern-sting errors occur during peak system loads, which would mean that the debugger could only execute when the probability of an error occurring was low. Another approach wouId be to execute the internal debugger as a periodic process of high priority and design the entire system to take this process into account when determining issues such as scheduling. The second scenario requires the target processor to provide some form of computational facilities. This extra execution will certainly require some amount of utilization of architecture resources such as internal registers and bus accesses. The simplest example of architecture facilities would be a machine that contirtuaU y dumps some representation of the instruction it is currently executing. This would require a dedicated bus to the external world (proposed later in this section) and that additional circuitry be attached to the computation units to gain access to the current instruction. The problem with fis approach is that the processor is not aware of what data is required by the tools at the other end. Therefore, it must dump everything. At high processor speeds, the amount of information being sent could become overwhelming. However, the data could be faltered and then captured so that a tool could parse it later and recreate an execution history of the program. The hardware required for filtering is not trivial and requires great speed and storage capaaty to maintain pace with the target processor. lle next step is to allow software to dictate the information sent by the processor. The functional unit of the hardware sed.ing messagea could be implemented as a state machine, emitting different messages based on its current state. The default state would be all processor transactions. Basically, in this eontiguration, the processor is performing the filtering rather than the external debugger. This addition should not add much in complexity to the hardware architecture and would greatly reduee the wmplexity of the external debugging hardware. The final step is to take the (now stateful) functional unit and make it programmable. Instead of a state machine, it now becomes a complete functional unit within the processor itself. The internal debugger code would then be loaded into this portion of the prowssor at boot time and reside there for the entire execution, transmitting and receiving messages to and from the external debugger.
q

294

6.3 Hardware Break Points Software break points are intrusive and require instructions be inserted into the code of the test program. Conditioned break points present a more significant problem, since they require a computation every time they are encountered to determine if the proper conditions are met to halt execution. Such breakpoints are unacceptable for d-time programs. To resolve this issue, architectures need to provide the capability to set breakpointa in hardware. A set of registers would be classified as BreakPoint Registers (BPR), which the processor would check against the operands for each instruction. Two types of breakpoints are required, data and instruction. Each data BPRs inside the processor would be compared with the address of every data operand for each instruction. Instruction BPRs would be compared with instruction addresses or type. When a match occurs, a breakpoint fault would be raised and control trsnafered to the internal debugger Upon returning from a break, the processor is required to restart execution precisely where it had terminated. The state of the processor consists of all it's internal registers, including any pipeline information and cache memory. These values must be saved automatically when a break is encountered. Another issues is that of conditional breakpoints. Such breakpoints require computations by the processor that run in the background behind the program under test. The evaluation of the conditional expression must begin far enough in advance so that it may complete before the processor has passed the breakpoint location. This evaluation will require memory accesses, raising additional problems. The current value of operands in the expressions must be available to the processor, which might involve accessing it from memory or cache. Any accesses to memory must be scheduled in such a manner that they do not block any resources required by the program under test. F@dly, the value used must be valid and not in danger of before the breakpoint. While the problems raised above seem difficult, they are not insurmountable. The extend debugger must compile the conditional expression and download the code. At that point it can determine the scheduktbili~ of this evaluation by comparing the @e for the conditional to the other code that will occur in parallel. The user could then be notified of problems with their additional breakpoint. The hardware is responsible for detecting any collisions in parallel activity and must not assume the debugger is always accurate. Any debugger activity intruding on the behavior of the test program is important information and must be flagged by the processor. The primary additions required for hardware break points are additional registers from the architecture and the logic necessary to compare them with the operands of the current instruction. To support conditional breakpoints, the processor must provide background computational support. This support could come from a portion of the processor dedicated to conditional breakpoints, or the code -g

could be downloaded to the internal debugger, given the internal debugger support described previously. 6.4 Architectural Support for Abstractions As common programming paradigms become more refined, architectures will begin to inax-porate them into their instruction sets. It would be unlikely that the only abstractions supported by architectures would remain simple data types (integer, real) and their associated operations (add, subtract, convert). Other abstractions such as processes, semaphores, ports, timers, memory management, and faults that are found in typical applications should be supported as well, along with associated operations on those abstractions. M&ing hardware to another level of abstraction provides huge advantages for testing tools. As stated earlier, the architecture must be the basis for emulation capabilities and providing execution visibility. As the hardware becomes more aware of programming elements, it gains the abdity to send more meaningful messages to the external world. A context switch between processes could be sent with a single message, rather than the hundreds of machine instructions it takes to implement the switch. As the processor becom-es the single point of visibility, awareness of the progrdng environment becomes important. A processor with a high-level understanding of program ,entities can emit fewer, more meaningful - messag-es than a processor that only comprehends low-level instructions. 6.5 Dedicated Bus Embedded testing and debugging require an interface that aliows the processor to communicate with the external world without interfering with the behavior of the system under test. This physical connection should reside on the target and interface extemall y tlhrough some detachable mechanism. The separation technique is important, since the external debugging system will be detached from this connection once the system is placed into operation. The execution behavior alf the program should be independent of whether or not any external tool is attached. Assuming an adequate physical connection, the next detmminah "on is the protocol across it. `Ile following issues must be addressed 1) At what rate will messrwes need to be sent? `Processor speed raises i~teresting problems, since future speeds might be too quick for external processing techniques. A solution to this problem was discussed previously where the processor became aware of highlevel program elements. The goal is to decrease the number of messages required relative to the number of machine cycles. 2) How much data is associated with a message? If an architecture is required to emit large volumes of data for messages, there may be instanms where the processor must be suspended to allow the internal debugging hardware to catch up to the current processor state. Higher level messages may compound the problem, since more maningful messages might require more

295

information. There is likely a tradeoff between message level and data volume. 3) Is the connection bidirectional? Visibility concerns dictate that state information travel However, methods requiring out of the processor. control of the executing program require that state information travel the other direction. Protoczds must be in place to handle contention across the bus and those must be extremely well defined, due to the extreme data rate that could will be emmuntered across the bus. 4) Who is the active element in sending message9? Either the processor or the RTS must determine the information sent from the processor. The processor cannot provide all the state information needed, while the RTS will likely not be able to maintain adequate speeds for sending messages. These questions play a role in determining the interface between the internal and external debuggers. A likely solution would be a master-slave relation, where either the internal or external debugger regulated the other. This scenario does not seem likely, since each has such critical processing concerns. Therefore, each will likely execute independently, while communication is handled via some bus and protocol. There does exist a master-slave relationship in respect to the bus, howevex. During program executiw, the internal debugger must `ownn the bus, since it's processing concerns are the greatest. It must meet the message sending deadlines without altering computations in other parts of the system. There are points dting execution where the extend debugger must aeiz cmtrol. If the internal debugger cannot allocate the bus to meet the demands of the extcmal debuggm, the user must be notifkd that their requested operation cannot be accomplished during a real-time execution. The final determination is that of the active element within the processor. There are two basic approaches to detemnining control of the internal debugging activities. In the fiit the processor is active and becomes responsible for sending messages to the extend debugger. `fhesecondappmachuse aaspecialdebugge rportionofthe RTS to emit messagea, which is loaded into a dedicated functional unit within the architecture. Tools require information maintained by both the architecture and the RTS. Perhaps the solution lies between the two where both the RTS and architecture have the ability to dump messages, depending on the cmrcnt mquiremcnts dictated by the external tool. 7. Run-Time Svs tern Additions The RTS requirements deseribe an interface between a tool and the underlying system. This is a logical interface requiring substantial hardware support as outlined above. An obvious goal is to minimize the required data and computational requhements of the internal debugger as well as the required communications between the internal and external debuggers.

This paper does not address the question of how these interfaces should be UtdiZSd. Such SllSWerS should be given by methodologies and techniques for detecting and locating errors in embedded, real-time systems. As discussed earlier, the lack of these methods has led to difficulties for determining adequate RTS services for testing and debugging tools, which has forced a different approach to determine the required operations. Since the RTS is in essence offering au implementation of high-level abstractions, services that provide visibility into the implementation of RTS abstractions should adequately fidfdl the needs of most testing and debugging techniques. A standard currently exists for implementing these abstractions in the MRTSI [ARTE89] and CIFO [ARTE91]. In addition, most of the needs for testing and debugging can be fulfiiled by these standards. This is not surprising, since our solution is based on implementation visibility, and the MRTSI and CEO are providing an implementation interface. However, it is important to note that this approach also indicates that implementations that support these staudards should require minimal additions to and debugging tools as prOpOSed by akw SUppCWt testing this paper. Below is a small discussion surrounding each of these abstractions and a list of shortcomings in the MRTSI and CIFO for testing and debugging. Processes Concurrency is a common abstraction used in embedded systems. A design can be decomposed without concern for computational resources, which can then be determined by a scheduler during run-time. A& irqplements concurren cy through tasks and task types. The CIFO and MRTSI provide extensive tasking support includlng identifieation, creation and activation, communication through rendezvous, concurmat access to shared entities, and support for scheduling control. Elements of interest that are not provided by the CIFO or MRTSI include q Task State - A developer must have the ability to query and modify the task state for each task in their system. However, a modification could leave the RTS in an inanaistent state. For example, changing a task's state from "delaying" to %unning" without removing it from the &lay queue would place the RTS into a state that could not be achieved through normal execution. However, the same modification ability is available on typical debugging systems and should be offered by em beddcddebwrgera as Wd. q Commm&ation and Synchronization - A developer must have the ability tb view and modify eaeh entry queue to determine the concurrent state of the system. Again, modifications could leave the RTS in an unobtainable state. q Scheduling Control - In addition to the extensive operations provided by the CIFO for concurrency control, a developer must have awess to the dispatch port (or ports for muhiprqxssor systems). 7.2 Interrupt Management 7.1

296

One of our criticism of the current approach to embedded testing is that timing errors are revealed late in Interrupts are very related to the development process. timing issues and their correctness is an important element in embedded testing. Therefore, support for interrupts is extremely important to target testing and debugging. Faalities provided through the CIFO and MRTSI would allow developers to bind various interrupt handling routines, enable and disable certain interrupts, mask and unmask interrupts, and generate software interrupts all controlled dynamically duting program a program test. Time Management As stated earlier, important" to target testing target tools require sfilaertt time. Tools must be allowed (although such modifications results) and the delay Iist of by the RTS. 7.3

timing issues are extremely and debugging. Therefore, control over issues relating to to view and modify the clock might produce undefined waiting processes maintained

Management Dynamic memory is not typically used by due to diffldtk% in dcmonstradng embedded ti@iC4itiOliS reliability. However, future systems will likely incorporate algorithms that requite dynamic storage. In addition, memory ~agemcnt for dynamic allocations is part of a RTS and should therefore be included in RTS visibility and control discussions. A tool will likely require that ability to demonstrate an application programs behavior when memory is exhausted. The MRTSI would need to be extended to provide operations that mim a collection making it smaller to show execution behavior when memory is exhausted or larger to demonstrate correct execution should a collection be expanded by the developer. Resizing is not cheap and could require a gnat deal of computation and data transfers, depending on an implementation. Exception/Fault Handling Proper handling of exceptional events is evaluated during hardwaresoftware integration testing. Therefore, tools require a great deal of cattrol over exceptions and One must be able to raise an recovery mechanisms. exception or fault during program execution and also modify handler binding during execution. Another question of interest might be to locate the handler for a given fault or exception at a given program location. Such information is not easily gained from the underlying system. The compiler is responsible for handling exception propagation [ARTE89], so &k " " g the handler from only RTS information might be an impossibility and is at best resolved uniquely for each compilation system. 7.5

7.4 Memory

architectural and RTS additions. The architectural additions will certainly be costly in both time and space, requiring space (transistors) on the chip and access to internal registers and busses that could cause contention and slow the execution of other instructions provided by the architecture. However, the RTS additions are minimal. We defined the needs of testing as making the implementation details of common system abstractions visible and then determined the functionality required to view and control them. The ARTEWG'S MRTSI and CIFO provided an outstanding basis for this approach. The RTS additions are admittedly weak. Our initial goal was to have the methodologies and techniques used for testing embedded, real-time systems drive the operations required by the RTS. Unfort.tmatcl y, such methods do not yet exist. As stated earlier, testing and debugging of embedded, real-time software remains a black art, with ad hoc methods and techniques. While there has been much research into the concurrency and distribution issues, none has examined real-time constraints, embedded environments, and other issues relating to embedded systems Perhaps the MRTSI and CIFO are sufficient for implementing target level testing and debugging tools. However, this question cannot fully be resolved until more formal methods exist. Our next step is to evaluate the additions and determine their feasibility. Questions relating the cost of these additions to au architecture and RTS in terms of time and space must be answered. Also, a more complete mapping should exist between the added feattues and the impact they have on the desired features. One can then make a valid comparison between a feature and the costs associated with it. `l'he embedded contmllermark~ is currently huge, but has only begun to require the computational powers ~SOCiKltti With lUiCrOpKXXWOrS. Embedded i@k.i3tiOttS have traditional been event driven rather than computation dependent. Due to their light weigh~ easy con@mbility and expansibility, and lower design complexity, computers are quickly being chosen over mechanical techniques for controlling devices. As this transition continues, the size and complexity of embedded programs will grow. Controllers will not only have strict timing requirements, but also have significant computational needs as well. This combination requires new approaches to our current testing process for embedded systems and therefore, more effective tools to aid in testing and debugging embedded applications. References

[ARTES9]

The first goal is to identify defkienaes in embedded system testing and raise questions about the future of current tools. The second is to propose a solution to these problems through

co nclusions The goal of this paper is two fold.

s

Ada Run-time Environment Working Oroup, "A Model Run-Time System Interface for A&m Ada Letters, January, 1989. Ada Run-time Environment Working Group, "Catslogue of Interface Features

[ARTE91]

297

and Options for the Ada Runtime Environment," Special Edition of Ada Letters, Fall 1991 (fI). [CHIL91] Child, Jeffrey, "32-bit Emulators Struggle with Processor Complexities,n Computer Design, May 1,1991. Department of Defense, Reference Manual for the Ada Programming Language, ANSI/MIL-STD1815a, United States DoD, 1983. Federal Aviation Association, Software Consideration in Airlx)me Systems and Equipment Certification, RTCA/DO178A, 1985. [GILL88] Gilles, Jeff aud Ford, Ray, "A Guided Tour Through a Window Oriented Debugging Environment for Embedded Real Time Ada Systems," IEEE Transactions on Software Engineering, 1988. Hansen, B., ~eproduable Testing of Monitors," Software-practice and Experience, Volume 8,1978. Hembold, D. and Luckham, D., "Debugging Ada Tasking Rograms," IEEE software, March, 1985.

lyxn6J

Tai, K.C., "&producing Testing of Ada Tasking Programs," IEEB Transactions on Software Engineering, 1986. Tai, K.C., Carver, R.H., and Obaid, E.E., "Debugging Concurrent Ada Programs by Deterministic Execution," IEEE Transactions on Software Engineering, January, 1991. Taylor, R.N. and Osterweil, L. J., "Anomaly Detection in Concurrent Software by Static Data Flow Analysis," IEEE Transactions on Software Engineering, May, 1980. Taylor, R.N., "A General Purpose Algorithm for Analyzing Concurrent Programs," Communications of the ACM, ~y, 1983.

~A191]

DD83]

~AYL80]

~AYIJ33]

&IAm78]

Iw'fJ=l

Intel Corporation, i960 Architecture programmer's Manual, 1993.

Extended Reference

KOEH91]

Koehnemann, H.E. and LindquisL T.E., "Runtime Control of Ada Rendezvous for Testing and ~U@llg," Procedm - gs of the 24th Hawaii International Conference on System Sciences, Volume II, 1991. LeDoux, C, and Parker, D.S., "Saving Traces for Ada Debugging," Ada in Use Proceedings of the Paris Conference, 1985.

Km]

Lyttle, D. and Ford, R., "A Symbolic Debugger for Red-Time Embedded Ada Software," Software - Practice and Experience, May 1990. Mauger, C. and Pammett K., "An EventDriven Debugger for Ati" Ada in Use: Proceedings of the Paris Conference, 1985.

@fAUG851

298

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

FRAMEWORKS FOR SECURING LIMITED-DEVICE APPLICATIONS
Timothy Lindquist Aarthi Ramamurthy Ramon Anguamea Timothy.Lindquist@asu.edu Aarthi.Ramamurthy@asu.edu Ramon.Anguamea@asu.edu Division of Computing Studies Arizona State University Abstract
realized by the IBM J9 runtime and SUN Wireless toolkit pre-defined classes, is the security environment we evaluated for this paper. Various development environments are available depending upon platform and language. For Microsoft Windows Mobile 6 the application development environment for the .NET languages, such as C#, is the .NET Framework together with Visual Studio 2005 with the Compact Framework 2.0. Several alternatives are available for configurations utilizing Java, in part depending on the Java runtime environment being used. Sun's CLDC HotSpot and IBM's J9 are two popular Java runtime environment choices. Add-on packages and various configurations are available to support different security approaches, device capabilities and networking needs.

In this paper, we compare the features available for developing secure distributed applications for limited devices, such as smart phones. We limit our scope to examine frameworks for Java. This work is part of a continuing project which is considering capabilities and performance for application development on these platforms. The paper considers performance as it relates to various approaches to securing applications. The paper addresses two separate concerns. First is protecting access to resources by an executing application. The facilities for defining, limiting and controlling applications during their development, installation and execution are described. Second, we discuss approaches available for securing communication among application components running on servers or limited devices.

2. Background
The connectivity of computing devices to the Internet, has enabled malicious attacks. The motivation for attacks varies from willful espionage to experimentation. Equally important to protection from attack is the ability to prevent harm from mistakes in coding, configuration or user operations. Protection and detection are difficult in handheld devices because of limited capability. Trust is confidence in expected functionality. When running an application the user must trust that it produces valid information and that privacy, integrity, or confidentiality will not be compromised. There are several security policies, protocols and mechanisms that are of particular interest to the limited devices. Languages such as Java, C#, and other scripting languages are widely used in distributed applications and provide varying degrees security support. Java and C# both permit examination of compiled intermediate code for unsafe actions. Both the Java CLDC/MIDP and Mobile 6 execution environments support an array of cryptographic functions that can be used

1. Introduction
In this paper we consider limited devices that are connected to the Internet and other communication media, for example, handheld devices such as intelligent cell phones and PDAs with Internet connections or platforms which combine these functionalities. These devices have limited memory, limited processing power, no hard disk storage, small display screens, and limited human input capability. We consider only those having communication facilities (WiFi or EV-DO).. The operating environments for these devices are comprised of three base components: local operating system, network operating system and language runtime environment. The leading operating systems for these devices are Symbian, Palm and Windows Mobile 6. Connected limited-device configuration and mobile information device profile (CLDC1.1/MIDP2) are the Java frameworks designed for resource constrained devices, such as phones and PDA's. CLDC1.1/MIDP2 as

1530-1605/08 $25.00 © 2008 IEEE

1

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

by communication protocols and by the system elements to aid in access control at the device and application level.

· · ·

3. Java CLDC/MIDP
Three different limited device configurations exist for the Java 2 Micro Edition (J2ME). For more capable devices such as set-top boxes and high-end wireless devices the Connected Device Configuration (CDC) defines an API whose functionality is close to J2SE, but is reduced as appropriate for the limited hardware and applications. At the lowest level of functionality is the JavaCard API (for Smart-cards/Sim-cards). JavaCard as can include functionality for asynchronous security operations, such as encryption, decryption, digital signature, verification and others for limited devices whose computing capacity is unable to perform such operations without disrupting user-functionality. The Connected Limited Device Configuration (CLDC) is defined for PDA and wireless phone devices. A device such as a PDA or smart-phone running Java applications would include a virtual software stack with the following components: · Mobile Information Device Profile (MIDP2) that supports the application life-time model, persistent storage, network resources and the user-interface. · CLDC1.1 that supports the core Java language, IO and networking classes, security features and internationalization facilities. · The selected Java runtime environment · The device operating system and related services

a set of operations associated with each permission, codebase indicating the code origin, a digital signature of the code which allows identification of the signer and verification that the code has not been modified.

The codebase indicates the file or URL from which the code is loaded. If signed, the alias of the public key can also be used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which defines the access it has to resources. When execution encounters an operation that requires a system resource, all classes representing the currently executing methods (contents of the runtime stack) are checked to assure all have access to the resource. The Figure below is taken from the On-line Java Tutorial and shows how an execution can include a range of protection domains ranging from no access to resources to full access.

3.1 Application Security Model
The J2SE model for securing the operations in an executing virtual machine changed dramatically as Java evolved. Java originally, used the sandbox model for application security. Initial versions of Java provided full trust to classes loaded locally and prohibited all sensitive operations from any code obtained dynamically. Java1.2 introduced support for a continuum of access control. Access to system resources (such as files, sockets, runtime, properties, security permissions, serializable, reflection, and window toolkit) is granted based on domains. A domain is defined to include: · a set of permissions (resources),

Figure 1.

Controlling Access to Java Resources

In J2SE (Java2), security domains are defined by a policy file granting permissions to the domain. For example, suppose the company GrowthStocksExpress publishes an applet on their (hypothetical) web site at the URL: http://GSE.com/applets Assuming the applet needs connections to one or more hosts having a domain address ending with GSE.com on ports beginning at 2575, a policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase "http://GSE.com/applets" { permission java.net.SocketPermission

2

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

"*.GSE.com:2575-", "accept, connect, listen, resolve"; };

A policy may consist of one or more grants each defining different domains. Each domain may have one or more associated permissions. CLDC/MIDP2 security. The application security model for CLDC/MIDP2 draws on the model for J2SE, in that it includes domains and signed code. CLDC/MIDP2, however has a simpler model, in part because of the constraints imposed by the configuration and profile. The following CLDC/MIDP2 constraints are most significant · Java Native Interface (JNI). JNI provides J2SE applications access to native code running on the platform. CLDC provides similar capabilities in Kilo Native Interface (KNI), but prohibits dynamically loading and calling arbitrary native functions. · No reflection, remote method invocation or serialization. In J2SE, an RMI server or client can cause remote code to be automatically downloaded and executed to satisfy argument or return (sub)classes. When a serializable RMI parameter is provided an argument of an extended type, the RMI system will attempt to load (if necessary from an http codebase) the needed class. · No user-defined class loaders. Related to the constraint above, the developer cannot define a class loader in CLDC. The classloader in CLDC cannot be extended or replaced by the developer. A CLDC/MIDP2 application can only load classes from its own (signed) Java archive. As a result, the developer cannot extend or modify any classes in the CLDC configuration, MIDP2 profile, or which are provided by the runtime environment vendor. · CLDC supports multi-threading, but it does not provide facilities to build daemons or thread-groups. MIDP2 security protects access to sensitive API's by permissions. Protecting resources includes the concept of a domain, which is conceptually similar to J2SE. The full scope of protection includes the following elements: · Protection domains (4) that are statically defined in a policy file (by

·

·

the vendor) and associated to resource permissions; for example, socket, http, https, PushRegistry. The protection domains are Minimum, Maximum (or Trusted) and Untrusted. Certificate and archive signature. The jar file containing application class files and other resources can be digitally signed. Level of access ­ either Allowed or User.

Unlike J2SE, the 4 protection domains are device-specific and defined by the runtime vendor. They can be modified only as provided by the vendor. Each of the four domains is associated with a set of permissions together with a level of access. The 4 protection domains are defined by the runtime vendor. · Minimum. None of the permissions are allowed. · Maximum. All of the permissions are allowed. · Trusted. All of the permissions are allowed. · Untrusted. To be allowed, the user must provide consent. The permissions defined by the MIDP2 specification include: http, socket, https, ssl, datagram, serversocket, datagramreceiver, and PushRegistry (invoke other applications). These permissions may be grouped together by the vendor into meaningful subsets and assigned to domains based on the subsets; for example, NetworkAccess. Within a domain, the level of access may be different for different permission (sets). The accesses are: · Allowed. The permission (set) is allowed without involving the device user. · User level. The application's access to the permission(set) depends on explicit authorization from the device user. With user level of access, a dialog box is presented to the user indicating information about the permission and asking the user

3

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

whether access should be granted. User level access can be specified on one of 3 modes: · Oneshot. The user must be prompted for each operation on the protected resource. · Session. When the user grants access, it applies to all operations on the resource during a single execution of the MIDlet. · Blanket. When the user grants access, it applies to all operations on the resource during any execution of the MIDlet. CLDC/MIDP2 provides MIDlet access to the Record Management System (RMS), which provides persistent storage for application data via a record store. MIDP2 provides shared access to the record store of other MIDlet suites, and provides that access should be provided as readwrite or read-only. Low-level security is provided by the J2ME Java virtual machine. A virtual machine supporting CLDC must reject invalid class files. This is accomplished by a two-step process. At development time, classes are pre-verified by a tool which adds special attributes to class files to facilitate runtime class verification on the device. Much of the verification process can be handled statically by the pre-verifier. At runtime, the virtual machine rejects classes that have not been pre-verified.

3.2 Security and Trust API
MIDP2 provides HTTPS and SSL for secure communications with other devices. But, runtime environment providers are increasingly providing additional options. For example, IBM's J9 version 5.7 provides web service security package which allows web method calls using encrypted SOAP envelopes or digitally signed method calls for authentication and information integrity. Security and Trust Services API (SATSA) provides access to more comprehensive hash code, digital signature/verification, key/certificate management, as well as encryption and decryption. SATSA is designed as 4 optional components. The primary purpose is to provide access to a SmartCard Java device, which provides security functionality in an asynchronous manner that does not disrupt applications supporting the device user.

SmartCard includes the Java Card Protection Profile. The protection profile supports both open and closed cards. Open cards provide the end-user with the ability to install or activate new applications on the card. Closed cards have applications set by the vendor at the time the card is personalized for the end-user. A good example of a closed card may be a banking card that supports personal electronic purchases and bank account functions. Open cards that allow new applications to be downloaded and installed on the card present special security risk that would exclude open cards that include banking applications. Nevertheless, applications for open cards that support other aspects of security may become increasingly important. An example may be securely communicating information outside of direct e-commerce applications. Data integrity and authentication are becoming increasingly important as electronic communication proliferates. The Java Card Protection Profile defines four different configurations for a Java Card based on open and closed cards. The minimum configuration corresponds to a closed card in which no applications can be installed on the card after it's been issued to an end-user. The three remaining configurations provide additional functionality that's available through the evolution of the Java Card specification, such as RMI (a limited version), logical channels, applet deletion, object deletion, external memory, biometry, and contactless interface. The Java virtual machine for the device includes an API (RTE API) that may contain classes for performing security operations on information and for certificate and key management. SATSA runs on the limited-device, not on the Java Card. SATSA provides an interface to card security functionality, or when there is no associated smart card, provides security operations for the limited-device. SATSA has four optional packages. · SATSA-APDU provides low-level stream/socket-based protocol for communicating between the limited-device and the card. · SATSA-JCRMI provides an RMI interface that allows an application running on the limited-device to call methods running in applications on the Card. This interface would be used to instead of SATSA-APDU to avoid the overhead of programming with a low-level socket data protocol. · SATSA-PKI allows limited device applications to use the smart card to digitally sign information or to verify digital signatures.

4

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

PKI also provides for key and certificate management. · SATSA-CRYPTO. When a Java Card is not available, the CRYPTO API is used to compute security operations directly on the limited-device. Use of APDU, JCRMI, and/or PKI is accomplished using threading on the limiteddevice. Threading allows security operations to take place on the card while other applications continue to run on the device supporting the enduser. In this scenario, SATSA is appropriate for limited-devices with constrained processing power. Independent of processing capability, using a Java Card may be necessary to provide assurance level that is appropriate to the application. The open-device nature of cell phones and PDA's make it difficult to certify trustworthiness of applications on the device. Instead, we can isolate all high-risk user-specific information and computations to a certified secure Java Card.

HTML page, it returns an XML message in Simple Object Application Protocol (SOAP) format. The service description - specified in Web Services Description Language (WSDL) - this description defines the web methods (functions) that a service will accept - the inputs that go into these methods, and the format of the output that can be expected in return. This is used in generating a web service client proxy class for the limited device. The web service registry - is a directory of web services. The directory is optional because a web service need not be listed in a registry to be used. The registry provides a catalogue of available services - similar to Java Naming and Directory Service (JNDI). The web service client proxy ­ The proxy negotiates the communication between a limited device client and the web service. It marshals arguments, signs or encrypts as appropriate, posts the message and interprets the result.

4. Secure Web Services
Web services provide an XML-based service protocol for communicating among components of a distributed application. Web services differ from prior similar technologies, such as Microsoft DCOM, Object Management Group CORBA and Java Remote Method Invocation through reliance on http protocol and XML.

4.1 Types of Security Services
The following are the security services that may be required by a distributed limited device application. Authentication: Ensures that the sender and receiver are who they claim to be. Mechanisms such as username/password, smart cards, and Public Key Infrastructure (PKI) can be used to assure authentication. Authorization or Access Control: Ensures that an authenticated entity can access only those services they are allowed to access. Access control lists are used to implement this. Confidentiality: This assures that information in storage and in-transit are accessible only for reading by authorized parties. Encryption is used to assure message confidentiality. Integrity: Ensures that information, either in storage or in-transit cannot be modified intentionally unintentionally. Digital signatures are used to assure message integrity. Non-repudiation: Requires that neither the sender nor the receiver of a message be able to legitimately claim they didn't send/receive the message.

Figure 2. Web Services Architecture [9] In Figure 2, the service, is performed by a web server acting as a container for executing the service code. This is generally just a web like page that gets posted similar to the way other http web requests are done. Instead of returning a

4.2 Transport Level Security
The most popular security scheme for web services is SSL (Secure Socket Layer), which is typically used with http, and is supported by

5

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

CLDC/MIDP2. However using SSL for securing web services has a number of limitations. The inadequacy of SSL can be easily explained by a simple example. Consider a Web Service that can be provided indirectly to a user. A user accesses a website which indirectly invokes another remote web service. In this case, we have two security contexts: 1. Between the user and the website 2. Between the user and the web service The second security context requires the security of SOAP request/reply message (between the web site and the web service) to be assured over more than one client-server connection. SSL is inadequate to provide this type of security mainly because of the fact that while it encrypts the data stream, it does not support end-to-end confidentiality. The shortcomings of SSL (https) should be considered when being used for a distributed application to reside on a limited-device. SSL is designed to provide point-to-point security. Often, Web services require end-to-end security, where multiple intermediary nodes could exist between the two endpoints. In a typical Web services environment XML-based business documents route through multiple intermediary nodes. Https in its current form does not support non-repudiation well. Non-repudiation is critical for business Web services and, for that matter, any business transaction. Finally, SSL does not provide element-wise signing and encryption. For example, if there is a large purchase order XML document, yet only a single element, say, a credit card element needs to be encrypted. Signing or encrypting a single element is difficult with transport level security.

Another important area that XML digital signature addresses is the canonicalization of XML documents. Canonicalization enables the generation of the identical message digest and thus identical digital signatures for XML documents that are syntactically equivalent but different in appearance due to, for example, a different number of white spaces present in the documents. The advantages of using XML digital signature can be summarized as below. · · It accounts for and takes advantages of two existing and popular technologies, viz., the Internet and XML. XML digital signature provides a flexible means of signing. For example, individual item or multiple items of an XML document can be signed. This becomes extremely useful in a scenario where each person in a workflow is responsible ONLY for certain work. It supports diverse sets of Internet transaction models. For instance, the document signed can be local or even a remote object, as long as those objects can be referenced through a URI (Uniform Resource Identifier). A signature can be either enveloped or enveloping, which means the signature can be either embedded in a document being signed or reside outside the document. It provides important security features like authentication, data integrity (tamperproofing), and non-repudiation. XML digital signature also allows multiple signing levels for the same content, thus allowing flexible signing semantics. For example, the same content can be semantically signed, cosigned, witnessed, and notarized by different people.

·

· ·

4.3 XML Signature
XML based security schemes, provide unified and comprehensive security functionalities for Web Services. The important ones being, XML Signature, XML Encryption, WS-Security (Web ServicesSecurity). The W3C (World Wide Web Consortium) and the IETF (Internet Engineering Task Force) jointly coordinated to generate the XML digital signature technology. The XML digital signature specification [10] defines XML syntax for representing digital signatures over any data type. It also specifies the procedures for computing and verifying such signatures.

Figure 3. WSE ­ Input/Output filters [1]

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

Web Service Deployment Descriptor (WSDD) and Handlers play the pivotal role in the implementation of digital signature in Java. A deployment descriptor specifies aspects such as handlers and communication protocol. The Handler is a java class (implementing the Input and output filters of Figure 3) that provides a MessageContext through which access is provided to the input/output stream of XML \In the Apache AXIS framework, MessageContext is a structure, that contains: 1) a request message, 2) a response message, and 3) a number of properties. All the SOAP message manipulation is done within the handler class.

6. Conclusions and Issues
The authors have continuing efforts in this area which include obtaining devices, software development environments, and simulators / emulators related to securing limited-devices. Our approach considers the operating environment on the device as well as their applications. We are in the midst of consolidation of small hand-held devices to provide integrated functionality. Common applications including personal organizers, cell-phones, and multimedia players can effectively be placed on a single platform. While users who desire more than one of these functionalities are exploring integrated solutions, the industry is pushing separation (partly for financial reasons.) Consolidated functionality brings a higher diversity of applications onto limited devices, as does special purpose applications (for example autonomous vehicle control). Either way, security concerns increase. The use of smart cards in the United States is just beginning after lagging behind use in some other regions. The integration of smart cards (SIM-Cards) on cell phones is an indication of this trend. Enabling high-risk applications, such as banking and purchasing, by leveraging smart cards integrated with other devices presents an attractive alternative. Of course the concern for security places new demands on platforms in which security has not historically been a high priority. Performance has been the primary impediment to the use of more strongly objectoriented languages such as Java for limited device applications. Securing a distributed

application complicates the issue. Important considerations include: · Underlying architecture processor performance, ancillary processing capability such as SmartCard, · Frameworks supporting securing the application, as well as communications, · Use of security mechanisms appropriate to application needs (authentication, integrity, confidentiality), · Proper use of available frameworks including proper handling of passwords, certificates, keys, digital signatures, and encrypted information. Frameworks discussed in the paper are an important enabler to developing more secure distributed limited-device applications. Further usage reports and benchmarking for security mechanisms would better support developers.

References
[1.] Tim Ewald, "Programming with Web Services Enhancements 1.0 for Microsoft.NET", Available, see: http://msdn.microsoft.com/webservices/buil ding/wse/default.aspx?pull=/library/enus/dnwse/html/progwse.asp [2.] Sun Microsystems Java Security and Crypto Implementation, http://www.cs.wustl.edu/~luther/Classes/Cs 502/WHITE-PAPERS/jcsi.html [3.] Knudsen, Jonathan; Understanding MIDP 2.0's Security Architecture. http://developers.sun.com/techtopics/mobilit y/midp/articles/permissions/ [4.] WebSphere Everyplace Micro Environment v5.7; MIDP Installation guide for J9 Palm runtime environment. Available online from IBM. [5.] Mourad Debbabi, Mohamed Saleh, Chmseddine Talhi and Sami Zhioua: "Security Evaluation of J2ME CLDC Embedded Java Platform", in Journal of Object Technology, (5,2) Mar-Apr 2006. pp. 125-54. [6.] Security and Trust Service APIs for Java Platform Micro Edition Developers Guide. Available from http://www.java.sun.com/ [7.] Pannu, K.; Lindquist, TE; Whitehouse, RO; and Li, YH; "Java Performance on Limited Devices"; Proc The 2005 International Conference on Embedded Systems and Applications, CSREA Press, Las Vegas, June, 2005.

7

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

[8.] Lindquist, TE, Diarra, M, and Millard, BR; "A Java Cryptography Service Provider Implementing One-Time Pad"; Proc. 37th Annual Hawaii Int'l Conf on Systems Sciences, ACM, IEEE Computer Society, January 2004. [9.] Online Documentation on Web Services,

Available from: http://www.servicearchitecture.com/webservices/articles/web_services_explained.ht ml [10.] XML Digital Signature Specification, W3C Recommendations, Available from: http://www.w3.org/TR/xmldsig-core

8

ARTICLE IN PRESS

The Journal of Systems and Software xxx (2004) xxx­xxx www.elsevier.com/locate/jss

Automated support for service-based software development and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a Department of Computer Science and Engineering, Arizona State University­­Main, P.O. Box 875406, Tempe, AZ 85287-5406, USA Department of Electronics and Computer Engineering Technology, Arizona State University­­East 7001 E, Williams Field Road, Building 50, Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003

Abstract A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. Ó 2003 Published by Elsevier Inc.

1. Introduction A service-based development paradigm, or services model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can interact with one another and be providers or consumers of data and behavior. Some of the defining characteristics of service-based technologies include modularity, availability, description, implementation-independence, and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon the definition of the interface needed to access a service (description) while hiding the details of its implementation (implementation-independence). Since the client and service are decoupled, other concerns such as side effects become non-factors (modularity). One of the potential benefits of using a service-based approach for developing software is that at any given time, a wide variety of alternatives may be available that meet the needs of a given client (availability). As a result, any or all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. The technique utilizes an architecture description language to describe services and achieves run-time integration using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and involves the automatic generation of ``glue'' code for both services and applications. The Jini interconnection technology (Edwards, 1999) is used as a broker for facilitating service registration, lookup, and integration at runtime. The remainder of this paper is organized as follows. Section 2 describes background material in the areas of software architecture and the middleware technology we are using to enable dynamic integration (i.e. Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work, and Section 5 draws conclusions and suggests further investigations.

2. Background
Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751. E-mail address: gannod@asu.edu (G.C. Gannod). 0164-1212/$ - see front matter Ó 2003 Published by Elsevier Inc. doi:10.1016/j.jss.2003.05.002
*

This section describes background material on software architecture and Jini.

ARTICLE IN PRESS
2 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx

2.1. Software architecture A software architecture describes the overall organization of a software system in terms of its constituent elements, including computational units and their interrelationships (Shaw and Garlan, 1996). In general, an architecture is defined as a configuration of components and connectors. A component is an encapsulation of a computational unit and has an interface (e.g. port) that specifies the capabilities that the component can provide. Connectors encapsulate the ways that components interact. A connector is specified by the type of the connector, the roles defined by the connector type, and the constraints imposed on the roles of the connector. A connector defines a set of roles for the participants of the interaction specified by the connector. Components are connected by attaching their ports to the roles of connectors. Another important concept is an architectural style. An architectural style defines patterns and semantic constraints on a configuration of components and connectors. As such, a style can define a set or family of systems that share common architectural semantics (Medvidovic and Taylor, 1997).

3. Approach This section describes the service-based development approach including the techniques used for defining services, specifying client applications, realizing integration, and generating glue code. 3.1. Example Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on the health of systems in a network. This application utilizes a network sniffer service and a port monitoring service. The network sniffer service gives an administrator information about traffic on the network. The port monitoring service provides information about the open ports on the various machines on a network. Together, these services facilitate determining whether certain kinds of attacks (such as ping storms) are being directed to a machine or machines. The client application supports analysis of several networks, each of which is accessed using the buttons shown on the top portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services that utilize different models of execution (strict call return and data streams). The remainder of this section refers to architectural specifications that were used in the construction of this example. 3.2. Overview The methodology that we have developed follows closely the model suggested by Stal (2002) for web ser-

2.2. Jini The primary enabling feature of the work described in this paper is the existence of Jini (Edwards, 1999) for the delivery and management of services. In a typical Jini network, services are provided by devices that are connected to the network. A Jini technology layer provides distributed system services for activities such as discovery, lookup, remote event management, transaction management, service registration, and service leasing. When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is registered, a proxy (Gamma et al., 1995) is stored by the lookup service. The proxy can later be transported to the clients of the service. Other network members can discover the availability of the service via the lookup service. When a client application finds an appropriate device, the lookup service sets up the connection. In our approach to component integration, we use Jini to provide a standard method for registering and connecting a client to corresponding software components that are acting as services. One of the advantages of using this Jini-based integration technique is that it facilitates construction of applications ``on-the-fly'' whereby components can be used on an as-needed basis. One of the disadvantages is that clients of services must have some prior knowledge about how to use each respective service.

Fig. 1. Running example.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx 3

vices, although the technology that we are using to realize our approach is Jini. The approach itself focuses on two concerns with respect to software reuse. That is, it addresses both for reuse and with reuse concerns. With respect to for reuse, the approach involves the construction of services via the use of adapter and proxy synthesis. Specifically, the methodology involves two steps for creating services as follows: (1) specification of components as services, and (2) generation of services using proxies via the construction of appropriate adapters and glue code. These services are consequently registered and made available on a network. With respect to with reuse concerns, the approach involves the construction of applications using services as follows: (1) specification of a client to make use of services from a repository or network, (2) generation of the client (both manual construction of client application specific code and automated generation of glue code), and (3) execution of the client, including integration of the specified services at runtime. Within our approach, a user (e.g. developer) is responsible for writing the source code for the client application along with the specification of the architecture for a client. Among other things, the client specification contains a description of the basic services that the client application will need in order to be a complete system. All other source code, including code necessary to realize the connections between the client and employed services, is generated based on the specifications describing clients, services, and connectors. 3.3. Service generation In this section we describe some of the issues related to automating the creation of service wrappers. To support these activities, we have developed an automated tool that takes as input a software architecture and produces glue code. A primary source of reusable components that we employ in our approach are legacy command-line applications (Gannod et al., 2000). In order to generate services from legacy components, we take the approach of wrapping the components by utilizing the interface provided by the component. Since command-line applications have a well-defined input and output interface, the interface of the application as a service can be based entirely upon the knowledge of what the application intends to provide. 3.3.1. Specification and synthesis The concept of using an adapter for wrapping legacy software is not a new one (Gamma et al., 1995). As a migration strategy, component wrapping has many benefits in terms of re-engineering including a reduction in the amount of new code that must be created and a reduction in the amount of existing code that must be rewritten.

In regards to wrapping components, our approach uses two steps. First, a specification of the legacy software as an architectural component is created. These specifications provide vital information that is required to define the interface to the legacy software. Second, the appropriate adapter source code is synthesized based on the specification. 3.3.2. Specification requirements To aid in the development of an appropriate scheme for the wrapping activity, we defined the following requirements upon specifications. These requirements are as follows: (S1) a sufficient amount of information should be captured in the interface specification in order to minimize the amount of source code that must be manually constructed, (S2) a specification of the interface of the adapted component should be as loosely coupled as possible from the target implementation language, and (S3) the specification of the adapted component should be usable within a more general architectural context. The requirement S1 addresses the fact that we are interested in gaining a benefit from reusing legacy software. As a consequence, we must avoid modifying the source code of the legacy software. At the same time, we must provide an interface that is sufficient for use by a target application. To provide that interface, a sufficient amount of information is needed in order to automatically construct the adapter. Our selection of command-line applications addresses the modification concern of requirement S1 since source code is not available. As such, we are required to provide an interface that is based solely on the knowledge of how the application is used rather than how it works. Table 1 shows the properties used in the specification of services, clients and connectors. A service component specification consists of two parts: properties and ports. The properties section describes style of the service, while the ports section describes functions provided by the service. In addition, the service specifications indicate style-based information as well as conditions or commands that need to be true or executed, respectively, in order to establish an environment necessary to use the service. Finally, a key in terms of a ``service type'' (e.g. interface property) is used to support a service lookup, which is later utilized during application integration. The requirement S2 (i.e. the decoupling of a specification from a target implementation language) is based on the desire to apply the synthesis approach to a variety of target languages and implementations. In addition, this requirement facilitates enforcement of requirement S1 by ensuring that new source code is not artificially embedded in the specification. While satisfying this requirement is ideal, we found in our strategy that a certain amount of implementation dependence was

ARTICLE IN PRESS
4 Table 1 Properties Group Service properties Service port properties Attribute Component-Type Signature Return Cmd Pre Post Interface Path Port-Type Shared-GUI Part-of-client GUI-CodeFile Component-Type Shared-GUI Port-Type Interface Connector-Type Prop-type Description Architectural style this component adheres to The port's signature The port's return type The command-line program being wrapped Pre-processing command Post-processing command The generic interface implemented by this port Path to the wrapped command-line program The port's type based on the Component-Type Boolean indicating shared (true) or exclusive (false) GUI Identifies inclusion in client application The filename for client's GUI code Architectural style this component adheres to Boolean indicating shared (true) or exclusive (false) GUI The port's type based on the Component-Type The generic interface that this port can bind with Architectural style this connector adheres to The connectors role based on the Connector-Type G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx

Client properties

Client port properties Connector properties Connector role

necessary due to the fact that our implementation would make use of Jini. When a component has been wrapped using our technique, an interface is defined that facilitates the use of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also desirable to be able to use the specification of the adapted component within a more general architectural context. That is, it is advantageous to be able to use the specification as part of the software architecture specification for new systems. In using a content-rich specification, where interfaces are defined explicitly, the added benefit of providing information that can be integrated into an architectural specification of a target application is gained. In order to realize the requirements placed upon desired interface specifications for legacy software wrappers, we used the ACME (Garlan et al., 1997) architecture description language (ADL). Specifically, we used the properties section of the ACME ADL to specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an ADL that has been used for high-level architectural specification and interchange (Garlan et al., 1997). 3.3.3. Synthesis As stated earlier, the class of legacy systems that we are considering are command-line applications (Gannod et al., 2000). Given this constraint, we make the assumption that any client applications utilizing the wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We find this assumption to be reasonable due to the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage profiles. In our approach, the specification that is needed to generate wrappers contains properties associated with the ports as shown in Fig. 2. These properties include Signature, Command, Pre, Post, Path, Interface, and Return. In this case, the specification describes the NetworkSniffing and PortMonitor services, which are services created by wrapping tcpdump, and nmap, respectively. In the synthesis process, ACME specifica-

Fig. 2. ACME services section.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx 5

tions are combined with a standard template that implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we have constructed to automate this process generates the appropriate source code for facilitating interaction between a potential client and the wrapped component. At present, this is an automated tool that generates fully executable code for the wrapped application and does not require the user to modify or write any new code outside of option GUI code. Both the service and client synthesis steps utilize a template-based approach to synthesize code. That is, a standard file has been created that has stubs containing place holders that must be instantiated with either service or client specific parameters. Fig. 3 contains a portion of the ServiceTemplate file which contains all of the application and service independent source code and provides the routines necessary to integrate the legacy code into a Jini network. Specifically, the ServiceTemplate contains functions that implement the discover and join protocol for registering a service with the lookup service. The ServiceTemplate also contains tags that are place-holders for the automatically generated functions. For instance, in Fig. 3 the tag <put-ServerName> is a place-holder for the final name of the adapter component. In addition to the ServiceTemplate, there is also a reusable set of functions that can be utilized in an interface specification and consequently in the generated wrappers. For instance, the getOutputStream( ) routine (shown in Fig. 4) is available as a function for use within the Java code to provide standard stream input support.

The amount of automation that has been achieved through the approach described above is dependent on the degree of graphical user interface (GUI) support that is desired. For a service, the code synthesis step can be fully automated if no GUI support is desired. Otherwise, the amount of manual code construction is limited to GUI support. 3.4. Client generation Once the services are generated and stored in a repository, a client application can be architected. First we need to specify the client application taking into account the architectural style of each of the services. Once a client is specified, it can be verified and generated. In this subsection we look at the requirements for specifying the client and then describe synthesis of the client. 3.4.1. Specification Refer again to Table 1 which, in addition to the properties for service specifications, contains the properties of client application components and connectors. When dealing with integration at the component level, two issues arise (among others) that are of interest. First, the problem of architectural style mismatch (Shaw and Garlan, 1996) occurs when the underlying assumptions made by components conflict. Second, most modern applications provide a graphical user interface (GUI). As a result, integration of off-the-shelf components can leverage these user interfaces in order to take advantage of previously built technology. To cope with these issues we impose two requirements on the specification of client applications as follows: (C1) the specification of the components should capture the notion of architectural style so that the high-level interaction between clients and services can be verified, and (C2) the specification must facilitate the use of shared and exclusive GUI components. The requirement C1 addresses the fact that a component must provide a notion of architectural style. A component's style plays a very important role when it interacts with other components by imposing interaction constraints. Using a basic style attribute (by name) architectural mismatches can be determined by simple keyword matching. Requirement C2 addresses the fact that a service may provide a GUI that allows a user to access and control the service. In this context, there may be GUI components provided by services that are either sharable by other services or exclusive to the service. A sharable GUI component can be used by both the client as well as other integrated services while an exclusive GUI component can only be used by the service that provides the interface.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

ARTICLE IN PRESS
6 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx

3.4.2. Synthesis The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample specification of a client. The information contained within client specifications are used to support the synthesis of client code. This synthesis step utilizes two features; first, the information regarding connectors and attachments, such as those shown in Fig. 5 are used to determine the relationships between client applications and desired services. Second, information regarding GUIs provided by services is used to determine how to realize the GUI in a client application. In our framework, the wrappers for the various services can implement a common interface that allows the client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially used across multiple services and are identified using a name taken from a standard GUI vocabulary (for example ``ResultsWindow''). The name is then used to identify which GUI components can be shared across services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive component is independent and cannot be shared between services. The exclusive GUI components of the wrappers are used as is but may interact with one or more of the shared components. For both shared and exclusive components, the interaction with the client GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers. 3.5. Discussion As stated in Section 1, the service-oriented domain are characterized by modularity, availability, description, implementation-independence, and publication. As a result, services and service-based approaches are more coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a software architecture to specify applications that operate under these characteristics. As such, a software architecture in this context defines components, their interfaces, and the mechanisms by which services (as components) can be joined in order to fulfill needed software behavior. Consequently, services enable the use of a software architecture as an integration vehicle in which the architecture facilitates generation of glue code. It is the very fact that services adhere to the characteristics described above that the integration and code generation become possible at this level. However, the approach does lack in its ability to address needs that are more specific than what individual services provide. To cope with this, we are developing an approach that allows for the creation of federated services, where services are combined to meet some higher-level objective.

4. Related work Recently, the use of web services has gained attention with vendors releasing webservices toolkits that allow for building and using webservices. Webservices and .NET (Meyer, 2001) are based on the SOAP and XML (Seely and Sharkey, 2001) protocols. The Jini approach to service integration goes beyond what the webservices paradigm provides by defining how services can be used within a larger application context and providing support for code transportation. FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server that distributed messages to other tools that were interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this approach is a precursor to service-based development. Urnes and Graham (1999) describe an approach to facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the component space across a network. In our approach, services are potentially developed by different organizations and thus the choice of what to distribute is not

Fig. 5. Portion of ACME client specification.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx 7

available. The component model being addressed by Urnes and Graham, as such, is finer-grained and violates implementation-independence, a tenet of servicebased development. Grundy et al. (2000) discuss issues and experiences in constructing component-based software engineering environments. They created a variety of useful software engineering tools using their tool set (JViews, JComposer, etc.). They use ``plug and play'' and an event-based composition approach to achieve component integration. In this framework, components are more tightly coupled and their granularity is finegrained. In contrast, our approach is based on dynamic integration of coarse-grained services that are loosely coupled. Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and component gluing. This creates a clean separation of customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work provides a potential strategy for dealing with component mismatches, which is currently ignored in our approach.

Acknowledgements G. Gannod is supported in part by NSF CAREER grant CCR-0133956. References
Edwards, W.K., 1999. Core Jini. Prentice-Hall. Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services. Commun. ACM 45 (10), 77­80. Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison Wesley Longman. Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for legacy software. In: Proc. 7th Working Conf. Reverse Eng., IEEE, pp. 128­137. Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture description interchange language. In: Proc. CASCON'97, pp. 69­ 183. Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2). Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to develop a family of applications. IEE Proc. Software Eng. 144 (5­ 6), 237­248. Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92­97. Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration with pluggable composite adapters. Software Archit. Comp. Technol.. Reiss, S.P., 1990. Connecting tools using message passing in field environment. IEEE Software 7 (7), 57­66. Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services Development Using XML. Prentice-Hall. Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on an Emerging Discipline. Prentice-Hall. Stal, M., 2002. Web services: beyond component-based computing. Commun. ACM 45 (10), 71­76. Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In Proc. of Design, Specification and Verification of Interactive Systems. Gerald C. Gannod is an Assistant Professor in the Department of Computer Science and Engineering at Arizona State University and is a recipient of a 2002 NSF CAREER Award. He received the M.S. (1994) and Ph.D. (1998) degrees in Computer Science from Michigan State University. His research interests include software product lines, software reverse engineering, formal methods for software development, software architecture, and software for embedded systems. Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras (Chennai), India. His research interests include software engineering, distributed and object-oriented systems, software design, software architecture, service-oriented software engineering, and Wireless Application platforms. Timothy E. Lindquist is Professor and Chair in the Department of Electronics and Computer Engineering Technology at Arizona State University East Campus in Mesa, Arizona. He received the Ph.D. (1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

5. Conclusions The web-based services paradigm has gained attention recently with the development of technologies such as SOAP (Seely and Sharkey, 2001). The benefits of such technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these benefits by providing facilities for on-the-fly integration and component introspection. In this paper, we described an approach for addressing component integration via the use of services in the context of Jini interconnection technology. Specifically, the approach utilizes synthesis to generate code necessary to realize component integration. To facilitate integration, the ACME ADL is used to specify both services and target applications, and is used a medium for performing service compatibility checking. We are currently developing an environment that will assist in the creation of applications within the servicebased paradigm and will support service browsing to facilitate application design. In addition, we are investigating approaches for allowing services to collaborate beyond the scope of a client application in order to create federated groups of services. Furthermore, we are developing technologies similar to the ones described in this paper in order to support service-based application within the .NET and web service frameworks.

INDUSTRY TRENDS

Moving Java into Mobile Phones
George Lawton

Telecom are now selling Java phones. And, said Ben Wang, manager of systems development for Sprint PCS, 80 percent of the new phones the company sells will be Java enabled after the big rollout next month. Nokia alone plans to ship 50 million Java phones this year and 100 million next year. In fact, 15 handset makers either are or soon will be selling 50 models of Java phones.

Advantages

A

s mobile technology matures, handheld-device vendors are looking for ways to make their products more functional, and Java is one approach they are turning to. This is particularly the case with smart cellular phones, which are using Java to help add new capabilities. In smart phones, Java functions as a layer between the operating system and the hardware, or runs parallel to the OS within a separate chip. In the past, the key constraint to running Java on mobile devices has been their processing, memory, and powerconsumption limitations. However, new mobile hardware and software developments are reducing these limitations. Thus, industry observers expect Java use in mobile devices, which is already supported by many vendors, to explode during the coming years. Nick Jones, a fellow at Gartner Inc., a market research firm, said Java will become a de facto standard on midrange and high-end cellular phones. He predicted that at least 80 percent of mobile phones will support Java by 2006, although some may also run on other technologies, such as Microsoft's Pocket PC operating system. According to Jones, mobile-device manufacturers' desire for an aftermarket is driving interest in Java as a mechanism for easily adding software to devices. Java also permits applications to work across platforms. This is important in the mobile-phone market,

which features many platforms. However, questions about Java's performance and a dearth of Java-based applications for cellular phones, particularly in Europe and the US, remain as obstacles to the technology's widespread adoption in mobile devices.

DRIVING JAVA USE IN HANDHELDS
Work on Java-enabled handheld devices began several years ago, but completion of the Java 2 Platform Mobile Edition (J2ME) and support from device vendors and cellularphone-service providers have driven the recent level of interest, explained Eric Chu, Sun Microsystems' group product manager for industry marketing.

Adoption levels
Korea's LG Telecom in became the first service provider to deploy Java in September 2000. Since then, users have deployed between 18 million and 20 million Java-enabled telephones, said Sun spokesperson Marie Domingo. Companies such as Nextel in the US, NTT DoCoMo in Japan, and British

According to Sun's Chu, one of Java's major benefits for cellular phones is support for packet-based networks running TCP/IP. Using TCP/IP makes it easier to write applications that communicate directly with the phone, rather than relying on an intermediate technology such as the wireless application protocol (WAP). Also, Chu said, Java, unlike WAP, supports pictures and colors. In addition, he explained, the Java environment provides good security because it includes a sandbox that limits downloaded code's access to the rest of a host system. Moreover, Java's ability to work with different platforms is important in the fragmented cellular-phone market. This capability lets a Java-enabled phone run applications and services written for other mobile platforms and also lets software vendors save time and money by writing a single, Java-based version of an application to run on multiple platforms. And Java-enabled phones and servers could communicate directly with each other, thereby enhancing interactive applications. Java enables smart-phone users to download applications directly from the Internet. Similarly, Java lets users download Java applets that customize their devices in various ways, such as with special ring tones or improved caller ID. This lets users get new features more easily. In the past, users had to buy new phones, run new applications remotely using WAP, or download programs first downloaded to a PC. Meanwhile, there are many Java developers, which makes it easier for
June 2002

17

I n d u s t r y Tr e n d s

Java 2 Platform Micro Edition (J2ME)

Optional packages Optional packages Java 2 Platform Enterprise Edition (J2EE) Java 2 Platform Standard Edition (J2SE) Personal basis profile Personal profile MIDP CLDC KVM
Source: Sun Microsystems

Foundation profile CDC JVM

Figure 1. Sun's three primary Java platforms are each designed primarily to run on a different type of machine. The Java 2 Platform Enterprise Edition is designed for servers; the Java 2 Platform Standard Edition for workstations, PCs, and laptops; and the Java 2 Platform Micro Edition for PDAs, smart cellular phones, and other smaller systems. J2EE and J2SE use the full Java virtual machine (JVM). J2ME also works with the slimmed-down K virtual machine (KVM), the connected limited device configuration (CLDC), and the mobile information device profile (MIDP).

Other approaches help Java technologies designed for larger computers work on mobile devices. For example, SavaJe developed the SavaJe OS, which supports Java applications in a mobile environment by optimizing J2SE libraries for common mobile CPUs. Mathew Catino, SavaJe's cofounder and vice president of marketing, said Java applications typically spend 80 to 90 percent of their time executing the libraries. Therefore, he explained, optimizing the libraries enables applications to run 10 to 20 times faster. Zeosoft has developed ZeoSphere Developer, which permits the creation of mobile applications that support Enterprise Java Beans, Sun's Java-based software-component architecture. This could simplify the development of complex enterprise applications that communicate and run across servers (via J2EE), PCs (via J2SE), and mobile devices (via J2ME).

Software development tools
Application developers can use existing tools to create Java programs for handheld devices by limiting their code to libraries and APIs supported by J2ME. However, J2ME includes only a limited number of development libraries, noted Jacob Christfort, chief technology officer of Oracle's Mobile Division. Also, said Gartner's Jones, enterprises might shy away from J2ME because of the poor user interface designed for small device screens, the primitive threading model, and minimal native data-handling facilities. In essence, he explained, the design approach that lets J2ME work on small devices sometimes makes it inappropriate for large-scale enterprise uses. To address these concerns, several vendors have released or will soon release development toolkits or toolkit extensions to help developers more easily meet enterprise applications' needs. The new approaches include Sun's Forte for Java Programming Tools, the Oracle 9i Application Server

vendors of Java-enabled mobile devices to find people to write their software.

MAKING JAVA WORK IN HANDHELDS
Sun, which designed and manages development of Java, is in the forefront of making the technology work in handheld devices. However, other vendors have also become active in this area.

Sun Microsystems
Sun and a group of partners created J2ME to make Java work on smaller devices. J2ME includes some core Java instructions and APIs but runs more easily on small devices because it has a smaller footprint than the Java 2 Platform Standard Edition (J2SE) or Enterprise Edition (J2EE), shown in Figure 1, and has only those features relevant for the targeted devices. For example, J2ME's graphics and database-access capabilities are less sophisticated.
18
Computer

J2ME generally incorporates the connected limited device configuration (CLDC), which is implemented on top of operating systems and serves as an interface between the OS and Javabased applications. The CLDC generally uses the K virtual machine (KVM), a slimmed-down, less-functional version of the Java virtual machine (JVM) for small devices. The J2ME mobile information device profile (MIDP) sits on top of the CLDC and provides a set of APIs that define how mobile phones will interface with applications.

Other vendors
Several vendors besides Sun are creating Java-based technologies for handheld devices. Hewlett-Packard makes the MicroChaiVM (http://www.hp. com/products1/embedded/products/dev tools/microchai_vm.html), a cloned JVM that doesn't have Sun's licensing fees and usage restrictions. Several vendors, including Ericsson and HP, plan to use MicroChaiVM-based phones.

Wireless architecture toolkit, and the Sprint PCS Wireless Toolkit. Because of J2ME's shortcomings, Jones said, corporate applications will probably be based on the larger-footprint J2SE as mobile devices get more processing power. Regardless, said John Montgomery, product manager with Microsoft's .NET Development Group, current Java tools are too primitive and difficult to use for most developers.

ETM9 interface Instruction TCM interface Instruction cache Memory management unit ARM9EJ-S core Data TCM interface Data cache Memory management unit Write buffer Control logic and bus interface unit

Server-side handheld Java
Another Java-enabling approach would link handheld devices to Java applications and services on servers. AT&T Wireless, BEA Systems, IBM, Nokia, NTT DoCoMo, Sun, and other companies have created the Java-based Open Mobile Architecture for linking cellular phones and servers. The project would augment J2EE, designed primarily for servers, so that it would support standards that mobile devices can use with Internet-based information. The standards include XHTML (for displaying Web pages on mobile devices), SyncML (for synchronizing data between mobile devices and other machines), WAP 2.0 (to access Internet content and services), and the multimedia messaging service (for handheld messaging).

A R M96EJ- S

Coprocessor interface

AHB interface Instruction Data

Source: ARM Ltd.

Figure 2. ARM Ltd.'s ARM926EJ-S chip includes the company's Jazelle technology in its ARM9EJ-S Java-enabled processor core. In addition, the chip includes separate ETM (embedded trace macrocell), data TCM (tightly coupled memory), and AHB (advanced high-performance bus) interfaces.

IMPLEMENTATION IN HARDWARE AND SOFTWARE
Java technology can be implemented in software or in hardware on either a specialized Java acceleration chip or a core within the main processor. Software implementations tend to run less efficiently because systems must translate each Java instruction into native instructions that the CPU can run. Separate hardware chips are more efficient but represent additional device components and cost. Java cores integrate some of both approaches.

Components Group, said his company has developed techniques for speeding up the software process, which used to bog down when the CPU switched from instructions it could accelerate to instructions it couldn't. In addition, Intel and other software-based Java proponents say the latest mobile processors can run Java fast enough to compete with hardware-based approaches. Analyst Markus Levy with MicroDesign Resources, a semiconductorindustry research firm, disagreed. He said, "People are spending a lot of energy fine-tuning the software-based approaches. For some people that may be good enough, but if you really want the most efficient implementation you need a hardware-based approach."

Software approach
In the software approach, a device's CPU runs the Java code. David Rogers, marketing manager for Intel's PCA

Java hardware
Companies such as ARC Cores, ARM Ltd., Aurora VLSI, Digital Communications Technologies, inSili-

con, and Zucotto Wireless are developing hardware that runs Java, either as Java coprocessing cores for integration into CPUs or as stand-alone Java chips. Both hardware-based approaches promise to increase Java-based application performance and, by running more efficiently, reduce power demands on battery-dependent cellular phones. Different companies' chips execute different subsets of the Java instructions. For example, ARM's Jazelle chip, shown in Figure 2, executes about 68.2 percent of all possible Java instructions, while Aurora's DeCaf runs about 95 percent. Running a bigger set of Java instructions provides more functionality but makes a chip cost more and consume more power. Joan Pendleton, Aurora's cofounder and chief architect, said there are two classes of acceleration. The first, used by most vendors, translates Java byteJune 2002

19

I n d u s t r y Tr e n d s

code into native processor instructions. The second directly executes Java bytecode, which offers better performance but requires a larger footprint because of the additional circuitry necessary to run the software in hardware. Levy predicted that Java cores will be more popular than stand-alone Java processors. This approach's primary constraint is that developers must use a system-on-chip approach to create their products. Putting multiple functions on a chip is more expensive to develop, but the elimination of additional chips reduces device costs. Standalone Java chips are less expensive to design but lead to higher device costs.

performance across platforms. Levy has thus launched a Java-processor group within the Embedded Microprocessor Benchmark Consortium (http://www.eembc.org/). The group expects to release its first benchmark by next month.

"We are still in a phase of market confusion and have not yet gotten to a state of market consolidation," Jones explained.

Not enough applications
There are currently some mobileJava applications, including games and weather and traffic maps. However, Jones said, there are not enough desirable mobile-Java applications yet. The reason is not the technology, he said, but instead the lack of an effective business model and a commercial infrastructure that would enable developers to profit from their work.

A

CONCERNS AND CHALLENGES
Mobile Java is still a relatively new technology. Many industry watchers say the technology has kinks that still need to be worked out. For example, Gartner's Jones expressed concern about vendors' differing Java implementations. He said some developers are complaining about having to manually optimize their Java games for different cellular phones. And although there are many Java developers, there are fewer who have experience working with J2ME and writing code for small, resource-constrained devices. Overall, said Microsoft's Montgomery, "J2ME is an interesting set of engineering compromises, but I would argue exactly the wrong set of compromises. It is too big for the smallest devices but too small to have the features you want on the smartest devices."

Industry observers say mobile Java still has kinks that must be worked out.
The growth of publishing intermediaries that would certify and sell mobile-Java software may eliminate this problem.

HANDHELDS AND THE FUTURE OF JAVA
Jones said Java is doing well on back-end servers because Java-based applications can easily be redeployed as companies buy new servers. However, he noted, client-side Java use has faded considerably because many enterprise-application developers turned to Visual Basic to work within the corporate environment, which is typically Microsoft-based. Thus, the battle for the mobile platform is important to Sun. However, Sun's Java initiatives for cellular phones are facing stiff competition from various sources, including Microsoft's wireless efforts, the Symbian operating system, Linux, and Qualcomm's binary runtime environment for wireless (http://www.qualcomm.com/ brew/).

ccording to Jones, J2ME will attract more application developers as it becomes a richer and less constrained environment. A survey by Evans Data, a market research firm, found that wireless developers who have used Java expect to use the technology a bit more in 2003 than they will this year. Java will also become even more attractive as smart phones get more processing power and vendors design better APIs for color screens, higher quality sound, intellectual-property protection, and user-location capabilities, he added. However, he cautioned, these extra features would give vendors more opportunity to create their own Java implementations, which could fragment the application-development environment. Sprint PCS's Wang said the initial focus of mobile Java will be on games, multimedia, and ring tones. Over time, Levy added, Java will become a de facto standard built into smart phones. SavaJe's Catino predicted that Microsoft and Java-based technologies are likely to coexist in phones during the coming years. Third-party vendors could help this process by developing software-integration techniques that would combine the two environments in devices. I

Performance
Jones said that mobile Java can be somewhat slow because the KVM is not particularly fast. However, he added, the KVM should become faster in the future, particularly as phones with more memory can run just-in-time compiler technology, which enhances performance. "In five years," he said, "[performance] will be a nonissue." Another problem, said Levy, is a lack of standards to objectively measure
20
Computer

George Lawton is a freelance technology writer based in Brisbane, California. Contact him at glawton@ glawton.com.

Editor: Lee Garber, Computer, 10662 Los Vaqueros Circle, PO Box 3014, Los Alamitos, CA 90720-1314; l.garber@computer.org

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

A Java Cryptography Service Provider Implementing One-Time Pad
Timothy E. Lindquist, Mohamed Diarra, and Bruce R. Millard Electronics and Computer Engineering Technology Arizona State University East http://www.east.asu.edu/ctas/ecet mailto:Tim@asu.edu

Abstract
Security is a challenging aspect of communications today that touches many areas including memory space, processing speed, code development and maintenance issues. When it comes to dealing with lightweight computing devices, each of these problems is amplified. In an attempt to address some of these problems, SUN's Java 2 Standard Edition version 1.4 includes the Java Cryptography Architecture (JCA). The JCA provides a single encryption API for application developers within a framework where multiple service providers may implement different algorithms. To the extent possible application developers have available multiple encryption technologies through a framework of common classes, interfaces and methods. The One Time Pad encryption method is a simple and reliable cryptographic algorithm whose characteristics make it attractive for communication with limited computing devices. The major difficulty of the One-Time pad is key distribution.In this paper, we present an implementation of One-Time Pad as a JCA service provider, and demonstrate its usefulness on Palm devices.

Java continues to enjoy dominance in server-side technologies, however, a small but growing number of limited device applications are developed in Java. Nevertheless, Sun Microsystems Inc., added Java Cryptography Extension (JCE) and JCA (to the Java T M 2 Development Kit Standard Edition v1.4 (J2SDK), and has created a substantial market for applications running on J2ME (Java 2 Micro Edition). Other vendors are offering Java runtimes for limited devices. These versions bring Java to client application developers [9], [11], and raise the issue of appropriate Java-based security mechanisms. J2ME does not include JCE and JCA, however The Legion Of The Bouncy Castle has developed a lightweight Cryptography API and a Provider for JCE and JCA [14]. Neither provider offers implementation of the One-Time Pad cryptography service [14]. The simplicity of the One-Time Pad method and the fact that it does not require high processor speed, make it ideal for lightweight computing devices.

1.1

Context

1. Problem
Dependence on the communications infrastructure continues to grow as the size of computing devices decreases. The growing dependence on Internet accessibility to services that do not reside in a local machine brings with it the need for secure communications. The target of this work are relatively small devices and their related systems, such as Windows CE, PalmTE, Handspring and cell phones used to access Internet services. While several large computer service organizations have spent millions of dollars recovering from cyber attacks, the potential economic impact of insecure e-commerce communications on limited devices is huge[1], [3].

This paper focuses on integrating the JCA cryptography service provider, starting by defining the engine classes and then implementing the One-Time Pad method. We include simple evaluation programs to test the provider. The problem of pad distribution is one of the tasks taken-on in order to have successful deployment. Implementations of the one-time pad encryption0.9.4 are readily available. For example, one product is available for Windows command line launching. The source code written in ANSI-C and DOS executable are available for download at http://www.vidwest.com/otp/ [1]. The Security documentation provided with J2SDK includes detailed information on the implementation of

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

1

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

the Provider for the JCA [12]. The documentation for "a Provider for JCE and JCA" of The Legion Of The Bouncy Castle is also available [14]. The possibility of using the One-Time pad for data encryption and decryption for security purposes on lightweight computing devices was covered at the 35th Hawaii International Conference on System Science 2002 [2].

Sender

2. One Time Pad
The one-time pad algorithm is among the simplest in the world of cryptography and is considered by some to be unbreakable. It is nothing more than an exclusive OR between the message (to be encrypted) and the pad (a random key - sequence of bits). The principles that govern the encryption technique are not that simple to apply. First, the key must be random, which by itself is a big challenge. Second, parts of the key that have already been used to do encryption must not be available for other encryption. The key (Pad) must be a sequence of random bits as long as the message to be encrypted. The sender exclusive-OR's the message with the pad and sends the result through a communication channel. The one time requirement that makes it unbreakable and difficult at the same time is that after use, the sender must get rid of part of the pad, and not use it again. At the other end of the communication, the receiver must have an identical copy of the pad. The receiver decrypts the cipher text to obtain the original message by doing an exclusive-OR of the incoming cipher with its copy of the pad [1], [3], [4]. The receiver should also destroy the pad after use. See Figure 1.

Message

XOR

Encrypted message transmitted normally

OTP
Receiver

Encrypted message transmitted normally

XOR

Original Message

OTP

Figure 1. One-time pad (OTP) cryptography.

2.2

Disadvantages of the One Time Pad

2.1

Advantages of the One Time Pad

If the pad is actually random and has been distributed securely to the receiver, then no third party can decrypt the message. Even guessing part of the key will not allow a third party to determine the remainder. This is why some people claim that one-time pad is unbreakable. While there are a number of very good pseudo-random number generators, so far any attempt to generate a truly random key with computers appears to generate the same sequence after a certain point. Several approaches avoid this problem by personalizing the key. Another advantage of this technique resides in the simplicity of its algorithm. It does not involve complex operations that challenge the computational speed of some relatively small processors.

The key must be as large as the message being encrypted; this fact is sometimes inconvenient especially in the case of large messages. The principle of the one-time pad is to have a unique key for each communication, which makes the generation and management of keys problematic as the number of recipients and frequency of use escalates. The last challenging aspect of the One Time Pad is the Key distribution. In fact the key should remain undisclosed (secret) to any other party besides the communicating parties. Extensions to the One-Time Pad provider discussed in this paper center on portable memory devices [15][16] that are frequently synchronized with more capable machine (laptops or desktops) for key exchange. However, our implementation is aimed at handheld devices in general. General purpose (non-proprietary) portable memory interfaces for handheld devices don't exist yet, so another approach is necessary. Some possibilities are discussed in the Key Management section below. Each application must deal with this issue in its own effective way(s).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

2

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3. Java Cryptography Architecture
The Java security model has been evolving to adjust to new security issues. The JCA is a framework providing cryptography functionality development capabilities for a Java platform. It was introduced early in Java's evolution as an add-on package. The first release of the Security API was an extension of JCA including API's for encryption, key exchange, and coding message authentication. Prior to J2SDK 1.4, JCE was optional, in part due to export restrictions. The "Java Secure Socket Extension" (JSSE) and "Java Authentication and Authorization Service" (JAAS) security features have also been integrated into the J2SDK, version 1.4. Two new security features have been introduced: "Java GSS-API" (Java Generic Security Services Application Program Interface) that can be used for securely exchanging messages between communicating applications using the Kerberos V5 mechanism and "Java Certification Path API" that includes classes and methods in the java.security.cert package. These classes allow the developer to build and validate certificate chains. The java cryptography architecture includes a provider architecture [2], [5], [6], [7], [10], [12]. The notion of Cryptography Service Provider (CSP), or just provider, has been introduced in JCA. The provider archit e ct u r e a l l o w s f o r m u l t i p l e an d i n t er o p er ab l e cryptography implementations. An application developer can create or specify his/her own cryptography service provider. The service provider interface (SPI) presents a single interface for implementors. Classes, methods and properties are accessible to applications through the JCA application program interface (API). The SPI allows a cryptography service provider to plugin implementations for java applications. A provider can be used to implement any security service. Several providers can be available and they may or may not provide similar cryptography services and algorithms. Figure 2 depicts the layers of Java Cryptography Architecture, and is taken from Sun Java documentation [6]. A given installation of J2SDK may have several cryptography service providers installed, which may provide implementations of different algorithms and/or may provide multiple implementations of a single cryptography algorithm. Each provider has a name that is used by application programmers to specify the desired provider. It is also possible to specify the order of preference of providers. The default provider that comes with the J2SDK is the Sun provider, which includes a

wide variety of cryptographic algorithms and tools [2], [5], [6], [7], [10], [12].

Figure 2. Java cryptography architecture

3.1

Java Cryptographic Service Providers

The Java Cryptography Architecture, which includes the provider(s), has two main design principles, First, is independence from implementation and interoperability: This derives from using the services without knowing their implementation details [12]. Second, is algorithm independence and extensibility; meaning that new service providers and/or algorithms can be added without effecting existing providers. Together these provide a modular architecture that allows for encryption to be done by an implementation of a specific algorithm and subsequent decryption to be done by another implementation.

3.2

Provider Implementing One Time Pad

The primary question when building such a provider is: does the nature of the one-time pad allow it to be implemented in a pluggable architecture? Figure 1 shows the interoperability between Sender and Receiver as independent systems in communication. The element they are required to have in common is the key. The implementation of the algorithm does not matter. As far as extensibility is concerned, it is up to the provider programmer to remain independent of other cryptographic services. A Cryptographic Service provider is a package or set of packages providing concrete implementations of a subset of the cryptography portion of the Java security SPI. The Java Security Guide in J2SDK, v1.4 documentation lists a series of nine steps to follow for implementing a Provider. This paper follows those steps as guidelines for its development. Two aspects in the structures of cryptographic service were needed to write the implementation code: the Engine Class and the Service Provider Interface (SPI).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

3

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3.3

Engine Class

An Engine Class is an abstraction of a cryptographic service. It defines the service without a concrete implementation of the particular associated algorithms. Applications access instances of the engine class through the API, to carry out available operations. Every engine class has a corresponding service provider interface, which provides abstract classes accessing the engine class features. The service provider interface indicates all the methods that the actual cryptographic service provider should implement for a particular cryptographic algorithm or type. A service provider interface is named with its engine class name followed by "Spi" [12]. For each service that a provider implements, we must define the engine class, and then write its service provider interface. For the One Time Pad technique, the service provider interface's abstract class is called OneTimePadSpi. The engine class for OneTimePadSpi in compliance to the nomenclature of JCA is called OneTimePad. The engine class is a concrete subclass of the service provider interface, implementing all the abstract methods. The provider class is a final subclass of java.security.provider. Our provider is named ASUEcetProvider. The provider name is used by applications to access our one-time pad service [12].

Java. The file can be bundled with a particular application (with a manifest indicating relative URLs), or it can be installed in the Java Runtime Environment to be shared by all running applications.

3.6

Registering the Service

Configuring the service provider enables client access to the service(s) by registering the provider and defining default preferences where more than one provider is registered for the same service algorithm. Static Registration consists of editing the java.security file (located in " lib\security " subdirectory of the Java Runtime Environment) to add the provider name to the list of approved providers. For each available provider for a given algorithm, there is a corresponding line in the java.security file with the form: security.provider.<n>=<providerClassName> Where "n " is the preference number for the provider. For example the line: security.provider.2=asue.provider.ASUEcetProvider registers our provider with an order of preference 2. Dynamic Registration can be done by a client application upon requesting service(s) from a provider. The client application calls a class method, such as: Security.addProvider (Provider providerName).

3.7

Test Programs and Documentation

3.4

Provider's Information

The provider class provides access to various properties of the service, including the version, and other information about the service(s) it provides such as algorithm, type, and techniques [2], [12]. The value provided for this argument in this project is: "ASUEcetProvider v1.0, implementing One Time Pad (OTP) cryptographic technique, Arizona State University East, Electronic and Computer Engineering Technology. May 2003"

3.5

Install and Configure the Provider

The provider needs to be correctly installed and configured for the application program to utilize its cryptographic service(s). There are two different ways to install a provider. The first method consists of creating a JAR file (Java Archive File) or ZIP file containing all the class files belonging to the Cryptography Service Provider. The JAR file is added to the CLASSPATH environment variable. The exact steps of doing this last action, depends upon the local operating system [2], [12]. The second approach deploys the provider's JAR file of classes as an extension (optional package) to

Several test programs were written to exercise three aspects of the service provider. For client applications to be able to request service(s) provided by a specific provider, the provider should be successfully registered with the security API. A simple test program can verify registration by creating an instance of the provider and accessing its name, version, and info (getName (), getVersion (), and getInfo () methods. After making sure that the provider is accessible from the security API, we need to retrieve the provided service(s) by calling its "getAlgorithm ()" method. Finally, we check the functionality provided by the service by writing sender and receiver applications that use the service. In addition to providing sample service test programs, our implementation provides documentation. Our documentation is generated by the Javadoc tool from the source code and it targets application programmers.

4. Key Management
The generation and distribution of the random keys for this method is of primary concern. Since the size of the key must match the size of the message being

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

4

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

encrypted, we limit our application to transmission of relatively small messages. The key can be any random array of bits, the key type used in JCA has not been used. The OneTimePad class has been designed, so that when a new random key is generated, it is stored in an external file that could be later sent to the receivers. There are many approaches to providing random keys for the One-Time Pad. The primary problem with using the One-Time Pad is the amount of random bits needed. Every message sent needs a key of the same length. If every message is sent encrypted, the required key space becomes large very quickly. The alternative of only encrypting sensitive data suffers from forcing the application to make these choices, and only delays the issue of when and how to replenish the key once all the bits have been used on earlier messages. Handheld devices, the target for this work, generally only send small messages requiring encryption. This allows for many messages using a small key of say 1 MB. A nightly synchronization using a recharging cradle can be used to also replenish the One-Time Pad key. For devices that don't have a connection to a host while charging, another approach is portable memory devices [15][16]. With a 2 GB pad, the replenishment cycle would be much longer. Generally long enough to add a few jpeg or gif pictures a day. The primary problem with portable memory and handheld devices is that of interfacing requirements. Currently, no general-purpose interfaces (e.g., USB) are available for handhelds. Portable memory devices, to be useful, must come with general purpose interfaces, such as USB. An alternative to the cradle and portable memory approach is to update or replenish the pad on-line. In this approach, a One-Time Pad is used until almost exhausted. Then the remaining pad is used to exchange a block-cipher key for a more computationally complex cryptography algorithm, such as RC4 or AES. The One-Time Pad can then be generated by the server and sent to the handheld using the complex cipher. The encryption process would then return to the One-Time Pad methodology. This approach is expensive on both network and CPU utilization. It may only be acceptable on larger handhelds such as Windows CE (Pocket PC) machines. A final alternative would be to use PRNG (pseudo random number generator) that is cryptographically appropriate. An example PRNG is ISAAC [17]. ISAAC is a relatively fast random number generator with a very long cycle (i.e., guaranteed 240 with an average 28295). Generating random pads then requires knowing the seed. Using the prior approach of a trailing seed from the last pad as a seed for the new pad would permit, possibly, near realtime generation of One-Time Pads.

5. Running on Limited Devices
The fact that JCA and JCE are not part of J2ME, limits applicability to our intended application space. One approach is to configure limited client applications by embedding the provider directly in the deployed J2ME application. Another approach is to use the lightweight cryptography API defined by The Legion Of The Bouncy Castle to develop a provider based on their design principle of A provider for the JCE and JCA [14]. This solution results with a provider not fitting exactly the Java Cryptography Architecture, but which is usable on J2ME devices, such as PalmOS [14].

6. Conclusions and Enhancements
As long as electronic communication continues to expand, security will remain an issue. Cryptography is one of the most effective tools available to address these issues. One-time pad is among the most powerful existing cryptographic techniques, providing it is used within the constraints of its applicability. Primarily the constraints are key management, compute limited devices and encryption tasks that do not require encrypting large files. The Java Cryptography Architecture offers the potential of a single interface for applications that allows plug-in of any number of participating service providers. The approach allows evolution of security approaches with the promise of minimal impact on applications. Security remains an open field on every computing platform. As platforms continue to evolve to smaller and better connected devices, they will meet the information needs of a broader range of consumers. Its importance to provide frameworks for developing secure distributed and web-based applications on such mobile devices.

7. References
[1.] d@vidwest.net (2000, November 17). "One Time Pad Encryption v0.9.4" Retrieved January 25, 2002 from the World Wide Web: http://www.vidwest.com/otp/ Gong, L. (1998). "JavaTM 2 Platform Security Architecture Version 1.1". Sun Microsystems, Inc. Retrieved February 20, 2003 from the World Wide Web: http://java.sun.com/j2se/1.4/docs/guide/security/spec/securityspec.doc.html Jenkin M. & Dymond P. (2002) "Secure communication between lightweight computing devices over the Internet". HICSS 35 January 2002. Kahn D. (1967) The Codebreakers, New York, NY, MacMillan. Knudsen J. (1998) Java Cryptography, Sebastopol, CA,

[2.]

[3.]

[4.] [5.]

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

5

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

O'Reilly. [6.] Lindquist T. (2003, January 14). "Cet427/598 Distributed Object Systems". Retrieved February 02, 2003 from the World Wide Web: http://pooh.east.asu.edu/Cet427/ClassNotes/Security/ cnSecurity.html McGraw, G. and Felten, E. (1996) Java Security: Hostile Applets, Holes, and Antidotes. New York, NY. John Wiley & Sons. McGraw, G. (1998) "Testing for security during development: why we should scrap penetrate and patch". IEEE Aerospace and Electronic Systems, 13(4):13-15, April 1998. Muchow J. (2001) Core J2METM Technology and MIDP, Upper Saddle River, NJ, Prentice Hall. Oaks, S. (1998) Java Security, Sebastopol, CA, O'Reilly & Associates. Rubin, A, Geer, D. and Ranum, M. (1997) The Web Security Sourcebook. New York, NY, John Wiley & Sons.

[12.]

Sun Microsystems, Inc (2002). "Java 2 Platform, Standard Edition, v 1.4.0 API Specification". Retried January 12, 2002 from World Wide Web: http:// java.sun.com/docs/ Sundsted T. (2001). "Java, J2ME, and Cryptography" Retrieved March 20, 2003 from the World Wide Web: http://www.itworld.com/nl/java_sec/10262001/ The Legion Of The Bouncy Castle (2000). "The Bouncy Castle Crypto APIs" Retrieved March 20, 2003 from the World Wide Web: http://www.bouncycastle.org/index.html Zbar, Jeff, "Portable Memory Gets Small," retrieved Sept. 2003; http://www.beststuff.com/article.php3?story_id=4395. Lexar Media, "Samsung Sampling 2Gb NAND Flash Memory Devices to Lexar Media," retrieved Sept. 2003; http://www.digitalfilm.com/newsroom/press/ press_02_25_02a.html. Jenkins, Bob, "ISAAC: a fast cryptographic random number generator," retrieved Sept. 2003; http:// www.burtleburtle.net/bob/rand/isaacafa.html.

[13.]

[7.]

[14.]

[8.]

[15.]

[9.] [10.] [11.]

[16.]

[17.]

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

6

1

Optimal estimation with missing observations via balanced time-symmetric stochastic models
Tryphon T. Georgiou, Fellow, IEEE and Anders Lindquist, Life Fellow, IEEE

arXiv:1503.06014v2 [math.OC] 18 Aug 2015

Abstract--We consider data fusion for the purpose of smoothing and interpolation based on observation records with missing data. Stochastic processes are generated by linear stochastic models. The paper begins by drawing a connection between time reversal in stochastic systems and all-pass extensions. A particular normalization (choice of basis) between the two timedirections allows the two to share the same orthonormalized state process and simplifies the mathematics of data fusion. In this framework we derive symmetric and balanced MayneFraser-like formulas that apply simultaneously to smoothing and interpolation.

overview of smoothing and interpolation theory in the context of stochastic realization theory see [18, Chapter 15]. In the present paper we are taking this program several steps further. Given intermittent observations of the output of a linear stochastic system over a finite interval, we want to determine the linear least-squares estimate of the state of the system in an arbitrary point in the interior of the interval, which may either be in a subinterval of missing data or in one where observations are available. Hence, this combines smoothing and interpolation over general patterns of available observations. Our main interest is in continuous-time (possibly time-varying) systems. However, the absence of data over subintervals, depending on the information pattern, may necessitate a hybrid approach involving discrete-time filtering steps. In studying the statistics of a process over an interval, it is natural to decompose the interface between past and future in a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward or backward in time. This point was fundamental in early work in stochastic realization; see [18] and references therein. In a different context [19] a certain duality between the two timedirections in modeling a stochastic process was introduced in order to characterize solutions to moment problems. In this new setting the noise-process was general (not necessarily white), and the correspondence between the driving inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics. Here, we begin by combining these two sets of ideas to develop a general framework where two time-opposite stochastic systems model a given stochastic process. We study the relationship between these systems and the corresponding processes. In particular, we recover as a special case certain results of stochastic realization theory [11], [5], [6], [4] from the 1970's using a novel procedure. This theory provides a normalized and balanced version of the forward-backward duality which is essential for our new formulation of the two-filter Mayne-Fraser-like formula uniformly applicable to intervals with or without observations. The paper is structured as follows. In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving

I. I NTRODUCTION Data fusion is the process of integrating different data sets, or statistics, into a more accurate representation for a quantity of interest. A case in point in the context of systems and control is provided by the Mayne-Fraser two-filter formula [1], [2] in which the estimates generated by two different filters are merged into a combined more reliable estimate in fixed-interval smoothing. The purpose of this paper is to develop such a two-filter formula that is universally applicable to smoothing and interpolation based on general records with missing observations. In [3], [4] the Mayne-Fraser formula was analyzed in the context of stochastic realization theory and was shown that it can be formulated in terms a forward and a backward Kalman filter. In a subsequent series of papers, Pavon [5], [6] addressed in a similar manner the hitherto challenging problem of interpolation [7], [8], [9], [10]. This latter problem consists of reconstructing missing values of a stochastic process over a given interval. In departure from the earlier statistical literature, [5], [6] considered a stationary process with rational spectral density and, therefore, reliazable as the output of a linear stochastic system. Interpolation was then cast as seeking an estimate of the state process based on an incomplete observation record. A basic tool in these works is the concept of time-reversal in stochastic systems which has been central in stochastic realization theory (see, e.g., [11], [12], [13], [14], [5], [6], [15], [16], [17]). For a recent
Research supported by grants from AFOSR, NSF, VR, and the SSF. T.T. Georgiou is with the Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota; email: tryphon@umn.edu and A. Lindquist is with the Department of Automation and the Department of Mathematics, Shanghai Jiao Tong University, Shanghai, China, and the Center for Industrial and Applied Mathematics and ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden; email: alq@kth.se

2

generating processes, in opposite time-directions, via causal and anti-causal mappings, respectively. This is most easily understood and explained in discrete-time and hence we begin with that. In Section III we utilize this mechanism in the context of general output processes and, similarly, introduce a pair of time-opposite models. These two introductory sections, II and III, deal with stationary models for simplicity and are largely based on [20]. The corresponding generalizations to time-varying systems are given in Section IV and in the appendix, in continuous and discrete-time, respectively. In Section V we explain Kalman filtering for problems with missing information in the continuous-time setting. In this, we first consider the case where increments of the output process across intervals of no information are unavailable as a simplified preliminary, after which we focus on the central problem where the output process is the object of observation. Section VI deals with the geometry of information fusion. In Section VII we present a generalized balanced two-filter formula that applies uniformly over intervals where data is or is not available. We summarize the computational steps of this approach in Section VIII. Finally, we highlight the use of the two-filter formula with a numerical example given in Section IX and provide concluding remarks in Section X. II. S TATE DYNAMICS AND ALL - PASS EXTENSION In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. We begin by explaining basic ideas in a stationary setting. In discrete-time systems take the form of a set of difference equations x(t + 1) = Ax(t) + Bw(t)
n×n n×p

this is identical to the one for discrete-time given above (as is well known). In continuous time, stability of the system of equations is equivalent to A having only eigenvalues with negative real part. In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system is all-pass. This is done next.

A. All-pass extension in discrete-time Consider the discrete-time Lyapunov equation P = AP A + BB . (6)

Since A has all eigenvalues inside the unit disc of the complex plane and (3) holds, (6) has as solution a matrix P which is positive definite. The state transformation  = P - 2 x, and F = P - 2 AP 2 , G = P - 2 B, brings (1) into  (t + 1) = F  (t) + Gw(t). (9)
1 1 1 1

(7)

(8)

For this new system, the corresponding Lyapunov equation X = F XF + GG has In as solution, where In denotes the (n × n) identity matrix. This fact, namely, that In = F F + GG (10)

(1)

where t  Z, A  R ,B  R , A has all eigenvalues in the open unit disc D = {z | |z | < 1}, and w(t), x(t) are (centered) stationary vector-valued stochastic processes with w(t) normalized white noise; i.e., E{w(t)w(s) } = Ip ts , (2)

implies that this [F, G] can be embedded as part of an orthogonal matrix U= F H G J , (11)

i.e., a matrix such that U U = U U = In+p . Define the transfer function U(z ) := H (zIn - F )-1 G + J corresponding to  (t + 1) = F  (t) + Gw(t) w ¯ (t) = H (t) + Jw(t). This is also the transfer function of x(t + 1) = Ax(t) + Bw(t) ¯ x(t) + Jw(t), w ¯ (t) = B
1 -2

where E denotes mathematical expectation. The system of equations is assumed to be reachable, i.e., rank B, AB, . . . An-1 B = n. (3)

(12)

In continuous-time, state-dynamics take the form of a system of stochastic differential equations dx(t) = Ax(t)dt + Bdw(t) (4)

(13a) (13b)

where, here, x(t) is a stationary continuous-time vector-valued stochastic process and w(t) is a vector-valued process with orthogonal increments with the property E{dwdw } = Ip dt, (5)

(14a) (14b)

¯ := P H , since the two systems are related by a where B similarity transformation. Hence, ¯ (zIn - A)-1 B + J. U( z ) = B (15)

where Ip is the p × p identity matrix. Reachability of the pair (A, B ) is also assumed throughout and the condition for

3

Now, using the identity We claim that U(z ) is a stable all-pass transfer function (with respect to the unit disc), i.e., that U(z ) is a transfer function of a stable system and that U(z )U(z
-1

In - F F = (zIn - F )(z -1 In - F ) + (zIn - F )F + F (z -1 In - F ), (10) and GJ = -F H , obtained from U U = In+p , this yields U(z )U(z -1 ) = HH + JJ = In+p , as claimed.

) = U(z

-1

) U(z ) = Ip .

(16)

The latter claim is immediate after we observe that, since U U = In+p , U and hence,  (t) = F  (t + 1) + H w ¯ (t) w(t) = G  (t + 1) + J w ¯ (t) or, equivalently, x(t) = P A P -1 x(t + 1) + P 2 H w ¯ (t) w(t) = B P Setting x ¯(t) := P -1 x(t + 1), (18) can be written ¯w x ¯(t - 1) = A x ¯(t) + B ¯ (t) w(t) = B x ¯(t) + J w ¯ (t) with transfer function ¯ +J . U(z ) = B (z -1 In - A )-1 B (21) (20a) (20b) (19)
-1
1

 (t + 1) w ¯ (t)

=

 (t) w(t)

,

B. All-pass extension in continuous-time Consider the continuous-time Lyapunov equation (17a) (17b) AP + P A + BB = 0. (22)

(18a) (18b)

Since A has all its eigenvalues in the left half of the complex plane and since (3) holds, (22) has as solution a positive definite matrix P . Once again, applying (7-8), the system in (4) becomes d (t) = F  (t)dt + Gdw(t). (23a)

x(t + 1) + J w ¯ (t).

We now seek a completion by adding an output equation dw ¯ (t) = H (t)dt + Jdw(t) so that the transfer function U(s) := H (sIn - F )-1 G + J is all-pass (with respect to the imaginary axis), i.e., U(s)U(-s) = U(-s) U(s) = Ip . (25) (24) (23b)

Either of the above systems inverts the dynamical relation ww ¯ (in (14) or (13)).

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and hence, F + F + GG = 0. Utilizing this relationship we note that (sIn - F )-1 GG (-sIn - F )-1 = (sIn - F )-1 (sIn - F - sIn - F )(-sIn - F )-1 = (sIn - F )-1 + (-sIn - F )-1 , and we calculate that U(s)U(-s) = (H (sIn - F )-1 G + J )(G (-sIn - F )-1 H + J ) = JJ + H (sIn - F )-1 (GJ + H ) (JG + H )(-sIn - F )-1 H . (26)

w(t)
-

U

w ¯ (t)
-

Fig. 1: Realization (14) in the forward time-direction.

w(t)


U

w ¯ (t)


Fig. 2: Realization (20) in the backward time-direction. An algebraic proof of (16) is also quite immediate. In fact, U(z )U(z -1 ) = H (zIn - F )-1 G + J + H (zIn - F )
-1

H (z -1 In - F )-1 G + J
-1

For the product to equal the identity, JJ = Ip

=H (zIn - F )-1 GG (z -1 In - F )-1 H + JJ GJ + JG (z In - F )
-1

H

H = -JG .

4

Thus, we may take J = Ip H = -G , and the forward dynamics d (t) = F  (t)dt + Gdw(t) dw ¯ (t) = -G  (t)dt + dw(t). (27a) (27b)

A. Time-reversal of discrete-time stochastic systems Consider a stochastic linear system x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (35a) (35b)

with an m-dimensional output process y , and x, u, A, B are defined as in Section II-A. All processes are stationary and the system can be thought as evolving forward in time from the remote past (t = -). To formalize this, we introduce some notation. Let H be the Hilbert space spanned by {wk (t); t  Z, k = 1, 2, . . . , n}, endowed with the inner product , µ = E{µ}, and let + H- t (w ) and Ht (w ) be the (closed) subspaces spanned by {wk (s); s  t - 1, k = 1, . . . , m} and {wk (s); s  t, k = + 1, . . . , m}, respectively. Define H- t (y ) and Ht (y ) accordingly in terms of the output process process y . Then the stochastic system (35) evolves forward in time in the sense that - + H- (36) t (z )  Ht (w )  Ht (w ), where A  B means that elements of the subspaces A and B are mutually orthogonal, and where H- t (z ) is formed as above in terms of z (t) = x(t + 1) ; y (t)

Substituting F = -F - GG from (26) into (27a) we obtain the reverse-time dynamics d (t) = -F  (t)dt + Gdw ¯ (t) dw(t) = G  (t)dt + dw ¯ (t). Now defining x ¯(t) := P -1 x(t) and using (7) and (8), (28) becomes ¯ w dx ¯(t) = -A x ¯(t)dt + Bd ¯ (t) dw(t) = B x ¯(t)dt + dw ¯ (t), with transfer function ¯ U(s) = Ip + B (sIn + A )-1 B, where ¯ := P -1 B. B (32) (31) (30a) (30b) (29) (28a) (28b)

see [18, Chapter 6] for more details. Next we construct a stochastic system ¯w x ¯(t - 1) = A x ¯(t) + B ¯ (t) ¯ ¯ y (t) = C x ¯(t) + Dw ¯ (t), (37a) (37b)

Furthermore, the forward dynamics (27) can be expressed in the form dx(t) = Ax(t)dt + Bdw(t) ¯ x(t)dt + dw(t) dw ¯ (t) = B with transfer function ¯ (sIn - A)-1 B. U(s) = Ip - B (34) (33a) (33b)

which evolves backward in time from the remote future (t = ) in the sense that the processes x ¯, x, w, ¯ w relate as in the previous section. More specifically, as shown in Section II-A, H- (w ¯ )  H- (w) and H+ (w)  H+ (w ¯ ) for all t, as examplified in Figures 1 and 2. In fact, the all-pass extension (14) of (35a) yields ¯ x(t) + Jw(t) w ¯ (t) = B It follows from (20b) that (38) can be inverted to yield (38)

III. T IME - REVERSAL OF STATIONARY LINEAR
STOCHASTIC SYSTEMS

w(t) = B x ¯(t) + J w ¯ (t),

(39)

The development so far allows us to draw a connection between two linear stochastic systems having the same output and driven by a pair of arbitrary, but dual, stationary processes w(t) and w ¯ (t), one evolving forward in time and one evolving backward in time. When one of these two processes is white noise (or, orthogonal increment process, in continuous-time), then so is the other. For this special case we recover results of [11] and [5], [6] in stochastic realization theory.

where x ¯(t) = P -1 x(t + 1), and that we have the reverse-time recursion ¯w x ¯(t - 1) = A x ¯(t) + B ¯ (t). (40a) Then inserting (39) and ¯w x(t) = P x ¯(t - 1) = P A x ¯(t) + P B ¯ (t) into (35b), we obtain ¯x ¯w y (t) = C ¯(t) + D ¯ (t), (40b)

5

¯ := CP B ¯ + DJ and where D ¯ := CP A + DB . C Then, (40) is precisely what we wanted to establish. The white noise w is normalized in the sense of (2). Since U, given by (15), is all-pass, w ¯ is also a normalized white noise process, i.e., E{w ¯ (t)w ¯ (s) } = Ip t-s . From the reverse-time recursion (37a)


(41)

+ same inner product as above, and let H- t (du) and Ht (du) be the (closed) subspaces spanned by the increments of the components of U on (-, t] and [t, ), respectively. Define + H- t (dy ) and Ht (dy ) accordingly in terms of the output process y . All processes have stationary increments and the stochastic system (45) evolves forward in time in the sense that - + H- (46) t (dz )  Ht (dw )  Ht (dw ),

where H- t (dz ) is formed in terms of z (t) = x(t) . y (t) (47)

x ¯(t) =

¯w (A )k-(t+1) B ¯ (k ).
k=t+1

The all-pass extension of Section II-B yields ¯ xdt dw ¯ = dw - B as well as the reverse-time relation ¯ w dx ¯ = -A x ¯dt + Bd ¯ dw = B x ¯dt + dw, ¯ (42) where x ¯(t) = P -1 x(t). Inserting (49b) into dy = CP x ¯dt + Ddw (49a) (49b) (48)

Since, w ¯ is a white noise process, E{x ¯(t)w ¯ (s) } = 0 for all s  t. Consequently, (37) is a backward stochastic realization in the sense defined above. Moreover, the transfer functions W(z ) = C (zIn - A)-1 B + D of (35) and ¯ (z ) = C ¯ (z -1 In - A )-1 B ¯ +D ¯ W of (37) satisfy ¯ (z )U(z ). W(z ) = W (44) (43)

yields ¯x dy = C ¯dt + Ddw, ¯ where ¯ = CP + DB . C Thus, the reverse-time system is ¯ w dx ¯ = -A x ¯dt + Bd ¯ ¯ dy = C x ¯dt + Ddw. ¯ (51a) (51b) (50)

In the context of stochastic realization theory, U(z ) is called structural function ([13], [14]).

w(t)
-

W

y (t)
-

Fig. 3: The forward stochastic system (35).

From this, we deduce that the system (45) has the backward property H+ ¯)  H+ ¯ )  H- ¯ ), (52) t (dz t (dw t (dw where H+ ¯) is formed as above in terms of t (dz

y (t)


¯ W

w ¯ (t)


z ¯(t) =

x ¯(t) . y (t)

We also note that the transfer function Fig. 4: The backward stochastic system (37) W(s) = C (sIn - A)-1 B + D of (45) and the transfer function B. Time-reversal of continuous-time stochastic systems We now turn to the continuous-time case. Let dx = Axdt + Bdw dy = Cxdt + Ddw (45a) (45b) as in discrete-time. Note that the orthogonal-increment process w is normalized in the sense of (5). Since U(s) is all-pass, ¯ xdt dw ¯ = du - B (53) ¯ (s) = C ¯ (sIn + A )-1 B ¯ +D W of (51) also satisfy ¯ (s)U(s) W(s) = W

be a stochastic system with x, w, A, B as in Section II-B, evolving forward in time from the remote past (t = -). Now let H be the Hilbert space spanned by the increments of the components of w on the real line R, endowed with the

6

also defines a stationary orthogonal-increment process w ¯ such that {dw ¯ (t)dw ¯ (t) } = Ip dt. It remains to show that (51) is a backward stochastic realization, that is, at each time t the past increments of w ¯ are orthogonal to x ¯(t). But this follows from the fact that


Differentiating P (t)- 2 P (t)P (t)- 2 = In , we obtain
1 1  P (t)- 2 = -R(t) - R(t) , P (t)- 2 P

1

1

and hence the (55) yields F (t) + F (t) + G(t)G(t) = 0. Using (61) to eliminate F in (57), we obtain d = -F (t)  (t)dt + G(t)dw, ¯ where dw ¯ = dw - G(t)  (t)dt, (63) (62) (61)

x ¯(t) =
t

e

-A (t-s)

¯ w Bd ¯ (s)

and w ¯ has orthogonal increments.

IV. T IME REVERSAL OF NON - STATIONARY STOCHASTIC
SYSTEMS

which can also be written ¯ (t) x(t)dt, dw ¯ = dw - B ¯ (t) := P (t)-1 B (t). where B Proposition 1: A process w ¯ satisfying (63) has orthogonal increments with the normalized property (5). Moreover, E{[w ¯ (t) - w ¯ (s)] (t) } = 0 for all s  t. Proof: As is well-known, the solution of (57) can be written in the form
t

(64)

In a similar manner non-stationary stochastic systems admit unitary extensions which in turn allows us to construct dual time-reversed stochastic models that share the same state process. The case of discrete-time dynamics is documented in the appendix, whereas the continuous-time counterpart is explained next as prelude to smoothing and interpolation that will follow.

(65)

A. Unitary extension The covariance matrix function P (t) := E{x(t)x(t) } of the time-varying state representation dx = A(t)x(t)dt + B (t)dw, x(0) = x0 (54)

 (t) = (t, s) (s) +
s

(t,  )G( )dw,

(66)

where (t, s) is the transition matrix with the property  (t, s) = F (t)(t, s), (s, s) = In (67a) t  (t, s) = -(t, s)F (s), (t, t) = In (67b) s Let s  t. Then, in view of (63), a straight-forward calculation yields w ¯ (t) - w ¯ (s) = w(t) - w(s)
t

with x0 a zero-mean stochastic vector with covariance matrix P0 = E{x0 x0 }, satisfies the matrix-valued differential equation  (t) = A(t)P (t) + P (t)A(t) + B (t)B (t) P (55) with P (0) = P0 . Throughout we assume total reachability [18, Section 15.2], and therefore P (t) > 0 for all t > 0. A unitary extension of (54) is somewhat more complicated than in the discrete time case. In fact, differentiating  (t) = P (t)- 2 x(t) we obtain d = F (t) (t)dt + G(t)dw, where F (t) = P (t)- 2 A(t)P (t) 2 + R(t), G(t) = P (t) with R(t) = In fact, d = P (t)- 2 dx + R(t) (t)dt.
1 1 1 1

- M (t, s) (s) -
s

M (t,  )G( )dw,

(68)

where M (t, s) =
s

t

(56) Therefore, (57)

G( ) (, s)d.

(69)

E{[w ¯ (t) - w ¯ (s)][w ¯ (t) - w ¯ (s)) } = Ip (t - s) + (t, s), where (58a) (58b) (t, s) = M (t, s)M (t, s) +
s t
1 1 d P (t)- 2 P (t) 2 . dt

t

-1 2

B (t)

M (t,  )G( )G( ) M (t,  ) d

- (59)
s

[M (t,  )G( ) + G( ) M (t,  ) ] d.

However, (t, s) is identically zero. To see this, first note that (60) M (t, s) = -M (t, s)F (s) - G(s) . s (70)

7

Then, in view of (61), a simple calculation shows that  (t, s)  0. s Since (t, t) = 0, the assertion follows. Hence the incremental covariance is normalized. Next, we show that w ¯ (t) has orthogonal increments. To this end, choose arbitrary times s  t  a  b on the interval [0, T ], where we choose a and b fixed, and show that Q(t, s) := E{[w ¯ (b) - w ¯ (a)][w ¯ (t) - w ¯ (s)) } is identically zero for all s  t. Using (68) and w ¯ (b) - w ¯ (a) = w(b) - w(s) - M (b, a)(a, s) (s)
b b

B. Time reversal in continuous-time systems Next we derive the backward stochastic system corresponding to the non-stationary forward stochastic system dx = A(t)x(t)dt + B (t)dw, dy = C (t)x(t)dt + D(t)dw, x(0) = x0 y (0) = 0 (75a) (75b)

defined on the finite interval [0, T ], where x0 (with covariance P0 ) and the normalized Wiener process w are uncorrelated. To this end, apply the transformation x ¯(t) = P (t)-1 x(t) together with (74b) to (75b) to obtain ¯ (t)¯ dy = C x(t) + D(t)dw, ¯ where ¯ (t) = C (t)P (t) + D(t)B (t). C (77) This together with (74a) yields the the backward system corresponding to (75), namely ¯ (t)dw dx ¯ = -A(t) x ¯(t)dt + B ¯ ¯ dy = C (t)¯ x(t)dt + D(t)dw. ¯ with end-point condition x ¯(T ) = P (T ) the Wiener process w ¯.
-1

(76)

- M (b, a)
s

(a,  )G( )dw -
a

M (b,  )dw

computed analogously, we obtain
b

Q(t, s) = M (b, a) (a, s)M (t, s) -
s b

(a,  )G( )d

+
s

(a,  )G( )G( ) M (t,  )d .

(78a) (78b)

Then, again using (61), we see that M (t, s)  0, s so, since Q(t, t) = 0, we see that Q(t, s) is identically zero, establishing that w ¯ (t) has orthogonal increments. Finally, we use the same trick to show (65). In fact, for s  t, (66) and (68) yield E{[w ¯ (t) - w ¯ (s)) (t) } = -M (t, s)(t, s)
t t

x(T ) uncorelated to

The backward realization (78) was derived in [3], but in cumbersome way, requiring the proof that w ¯ (t) is a normalized process with orthogonal increments to be suppressed. What is new here is imposing the unitary map between w and w ¯, making the analysis much simpler and more natural. V. K ALMAN FILTERING WITH MISSING OBSERVATIONS We consider the linear stochastic system (75) which does not have a purely deterministic component that enables exact estimation of components of x from y , an assumption that we retain in the rest of the paper. In the engineering literature is often the case that the stochastic system (75) represented as x  (t) = A(t)x(t) + B (t)w  (t), y  (t) = C (t)x(t) + D(t)w  (t) x(0) = x0 (79a) (79b)

+
s

G( ) (t,  ) d -
s

M (t,  )G( )G( ) )(t,  ) d,

the partial derivative of which with respect to s is identical zero; this is seen by again using (61). Therefore, since (65) is zero for s = t, it is identical zero for all s  t, as claimed. This concludes the proof of Proposition 1. Consequently, (57) and (64) form a forward unitary system dx = A(t)x(t)dt + B (t)dw ¯ (t) x(t)dt, dw ¯ = dw - B (71a) (71b)

The corresponding backward unitary system is obtained through the transformation x ¯(t) = P (t) 2  (t), which yields dx ¯ = P (t)- 2 d + R(t) (t)dt. This together with (62) and (63) yields ¯ (t)dw dx ¯ = -A(t) x ¯(t)dt + B ¯ dw = B (t) x ¯(t)dt + dw, ¯ (74a) (74b)
1 1

(72)

(73)

where the formal "derivative" w  is white noise, i.e., E{w  (t)w  (s) } = I (t - s) with  (t - s) being the Dirac "function". Of course x , y  and w  are to be interpreted as generalized stochastic processes. From a mathematically rigorous point of view, observing y  makes little sense since, for any fixed t, y  (t) has infinite variance and contains no information about the state process x. However, observations of y  could be interpreted as observations of the increments dy of y in a precise meaning to be defined next. On the other hand, one can think of (75) as a system of type dz = M (t)z (t)dt + N (t)dw(t), where z (t) = x(t) , y (t)

8

and one would like to determine the optimal linear leastsquares estimate of x(t) given past observed values of y . Generally this distinction between observing y or dy is not important. However, when there is loss of information over an interval (t1 , t2 ), there are two different information patterns depending on whether dy or y is observed. The difference consists in whether y := y (t2 ) - y (t1 ) is part of the observation record or not. These two cases will be dealt with separately in subsections below. In fact, the former, which is common in engineering applications, is provided as a simplified preliminary, whereas our main interest is in the latter. To this end, we first introduce some notation. Consider the stochastic system (75) on a finite interval [0, T ]. As before, let H be the Hilbert space spanned by {wk (t) - wk (s); s, t  [0, T ], k = 1, 2, . . . , m}, endowed with the inner product , µ = E{µ}. For any   H and any subspace A, let EA denote the orthogonal projection of  onto A. We denote by H[t1 ,t2 ] (dy ) the (closed) subspace generated by the components of the increments of the observation process y over the window [t1 , t2 ]. In particular, we shall also use the notations H- t (dy ) := H[0,t] (dy ) and H+ t (dy ) := H[t,T ] (dy ). Suppose that the output process or its increments are available for observation only on some subintervals of [0, T ],  namely Ik , k = 1, 2, . . . ,  . Next we want to define H as the proper subspace of H[0,T ] (dy ) spanned by the observed data. In the case that only the increments dy or, equivalently, the "derivative" y  is observed, we simply define H := HI1 (dy )  HI2 (dy )  · · ·  HI (dy ), In the case that the process y is observed, we need to expand  H by adding the subspaces spanned by the increments y over the complementary intervals without observation. In either case, we define
- H- t := H  Ht (dy ) and   + H+ t := H  Ht (dy ).   

with R(t) = D(t)D(t) and initial conditions x- (0) = 0 and Q(0) = P0 . Here Q- (t) is the error covariance Q- (t) := E{[x(t) - x- (t)](x(t) - x- (t)] }, (83)

which, by the nondeterministic assumption, is positive definite for all t. Next suppose the observation process becomes unavailable over the interval [t1 , t2 )  [0, T ]. Then the Kalman filter needs to be modified accordingly. In fact, for any t  [t1 , t2 ), (81)
- holds with the space of observations H- t := Ht1 (dy ), and consequently 

a x- (t) = EHt1 (dy) a x(t) = a (t, t1 )x- (t1 ). This corresponds to setting K- (t) = 0 in (82) on the interval [t1 , t) so that dx- = A(t)x- (t)dt (84a) with initial condition x- (t1 ) given by (82a). The error covariance Q- is then given by the Lyapunov equation  - (t) = AQ- + Q- A + BB Q (84b)

-

with initial the condition Q- (t1 ) given by the value produced in the previous interval. Then suppose observations of dy become available again on the interval [t2 , t3 ). Then, for any t  [t2 , t3 ), we have H+ t = H[0,t1 ]  H[t2 ,t] , so the Kalman estimate is generated by (82) but now with initial conditions x- (t2 ) and Q- (t2 ) being those computed in the previous step without observation. In the case there are more intervals, one proceeds similarly by alternating between filters (82) and (84) depending on whether increments dy are available or not. In an identical manner, a cascade of backward Kalman filters generates a process x ¯+ (t) based on the backward stochastic realization (78) and the observation windows [t, T ]. Assuming that there are observations in a final interval ending at t = T , on that interval the Kalman estimate




(80)

Then Kalman filtering with missing observations amounts to determining a recursion for x- where


ax ¯+ (t) = EHt a x ¯(t),


+

(85)

a x- (t) = EHt a x(t),

-

for all a  Rn .

(81)

with initial observation space H+ t := H[t,T ] , is generated by the backward Kalman filter dx ¯+ = -A(t) x ¯+ (t)dt ¯ + (t)(dy (t) - C ¯ (t)¯ +K x+ (t)dt) ¯ + = -(Q ¯+C ¯ - BD ¯ ) R -1 K  = -A Q ¯ ¯+ - Q ¯+A + K ¯ + R(t)K ¯ + (t) - B ¯B ¯ Q +

A. Observing dy only When observations are available on the interval [0, t1 ], the Kalman filter on that interval is given by dx- = A(t)x- (t)dt + K- (t)(dy (t) - C (t)x- (t)dt) (82a) K- = (Q- C + BD )R
-1

(86a) (86b) (86c)

(82b) (82c)

¯ + (T ) = P ¯ (T ) for x and initial conditions x ¯+ (T ) = 0 and Q ¯+ and the error covariance ¯ + (t) := E{[¯ Q x(t) - x ¯+ (t)][¯ x(t) - x ¯+ (t)] }, (87)

 - (t) = AQ- + Q- A - K- RK- + BB Q

9

which like Q- (t) is positive definite for all t. During periods ¯ + = 0. This of no observations of dy , we then set the gain K update is obtained from the backward time stochastic model (74) in an identical manner to that of (84). Consequently, both the underlying process as well as the filter can run in either time-direction. This duality becomes essential in subsequent sections where we will be concerned with smoothing and interpolation. B. Observing y Now consider the case that y , and note merely dy , is available for observation on all intervals Ik , k = 1, 2, . . . ,  . Under this scenario and with a continuous-time process the dynamics of Kalman filtering become hybrid, requiring both continuous-time filtering when data is available as well as a discrete-time update across intervals where measurements are not available. Then on the first interval [0, t1 ] the Kalman estimate (82) will still be valid. However, when t reaches the endpoint t2 of the interval of no information and an observation of y is obtained again, the subspace of observed data becomes
- Ht = H- t1  H(y ), 2 

where u(t1 ) = u1 (t1 ) u2 (t1 ) = Bd v (t1 ) Dd

and Bd and Dd are chosen so that
Bd Dd
t2

Bd , Dd =

t1

(t2 , s)BB (t, s) (t2 , s)BM (s) M (s)B (t2 , s) M (s)M (s)

ds

while E {v (t1 )v (t1 ) } = I . Hence, across the window of missing data the Kalman state estimate x- is now generated by a discrete-time Kalman-filter step x- (t2 ) = Ad x- (t1 ) + Kd (y - Cd x- (t1 )) Kd = (Ad Q(t1 )Cd + Bd Dd ) × (Cd Q(t1 )Cd + Dd Dd )-1 (89b) (89a)

with initial conditions x- (t1 ) and Q(t1 ) given by (82) and the error covariance at t2 by Q(t2 ) = Ad Q(t1 )Ad - Kd (Cd Q(t1 )Cd + Dd Dd )Kd + Bd Bd . (89c)

In the next interval [t2 , t3 ], where observations of y are available, the new Kalman estimate (81) with H+ t = H[0,t1 ]  H(y )  H[t2 ,t] is again generated by the continuous-time Kalman filter (82) starting from x- (t2 ) and Q(t2 ) given by (89). Again given an observation pattern, where intermittently y becomes unavailable for observation, the Kalman estimate (81) can be generated in precisely this manner by a cascade of continuous and discrete-time Kalman filters. Remark 2: The observation pattern of a continuous-time stochastic model, where y becomes unavailable over particular time-windows, is closely related to hybrid stochastic models where continuous-time diffusion is punctuated by discrete-time transitions. Indeed, unless interpolation of the statistics within windows of unavailable data is the goal, the end points of such intervals can be identified and the same hybrid model utilized to capture the dynamics. Remark 3: A common engineering scenario is the case where the signal is lost while the observation noise is still present. This amounts to having C  0 over the corresponding window, and the Kalman estimates are obtained by merely running the filters (82) and (86) in the two time directions with the modified condition on C . This situation does not cover the information patterns discussed above since, whenever BD = 0, the Kalman gains do not vanish and information about the state process is available even when C is zero.


where y := y (t2 ) - y (t1 ). Computing x(t2 ) across the window (t1 , t2 ] as a function of x(t1 ) and the noise components we have that
t2

x(t2 ) = (t2 , t1 ) x(t1 ) +
t1 Ad

(t2 , s)Bdw(s)
u1 (t1 )

while
t2 t2

y (t2 ) = y (t1 ) +
t1

C (t)x(t)dt +
t1

D(t)dw(t).

Therefore,
t2

y =
t1

C (t)(t, t1 )dt) x(t1 ) + u2 (t1 )
Cd

where
t2 t1

u2 (t1 ) =
t1

C (t)
t

(t, s)B (s)dw(s)dt
t2

+
t1 t2 t2

D(s)dw(s)

=
t1 t

C (t)(t, s)dtB (s) + D(s) dw(s).
M (s)

Thus, we obtain the discrete-time update x(t2 ) = Ad x(t1 ) + Bd v (t1 ) y = Cd x(t1 ) + Dd v (t1 ) (88a) (88b)

10

C. Smoothing Given these intermittent forward and backward Kalman estimates, we shall derive a formula for the smoothing estimate


proving condition (i). Condition (ii) follows from a symmetric argument. To prove (iii) we use condition (93). To this end, first note that, by the usual projection formula, ¯+ (t)-1 x EX+ (t) a x- (t) = E{a x- (t)¯ x+ (t)}P ¯+ (t) (94)

ax ^(t) := EH a x(t),

a  Rn ,

(90)

= a E{x- (t)¯ x+ (t) }x+ (t),

valid for both the cases discussed above, where H :=
 - Ht 



+ Ht



 H[0,T ] (dy )

(91)

¯+ (t)-1 x where x+ (t) := P ¯+ (t) is the dual basis in X+ (t) such that E{x+ (t)¯ x+ (t) } = I . Moreover, EX(t) a x- (t) = E {a x- (t)x(t) }P (t)-1 x(t) = a E{x- (t)x(t) }x ¯(t) = a P- (t)¯ x(t), where we have used condition (i) and (76). Next, set b := P- a and form ¯+ (t)-1 x EX+ (t) b x ¯(t) = E{b x ¯(t)¯ x+ (t)}P ¯+ (t) = b E{x ¯(t)¯ x+ (t)}x+ (t) ¯ = b P+ (t)x+ (t), by condition (ii), and consequently ¯+ (t)x+ (t). EX+ (t) E X(t) a x- (t) = a P- (t)P Then condition (iii) follows from (93a), (94) and (95). Remark 5: The proof of condition (iii) in Lemma 4 could be simplified if x ¯+ were a regular backward Kalman estimate without intermittent loss of information. In this case, x+ = ¯ -1 x P + ¯+ would be generated by a forward stochastic realization belonging to the same class as (75) and E{x ¯+ (t)x- (t) } = ¯ ¯ P+ (t) E{x+ (t)x- (t)} = P+ (t) E{x- (t)x- (t)}. Lemma 6: For each t  [0, T ], the smoothing estimate x ^(t), defined by (90), is given by ax ^(t) = E Ht a x(t), a  Rn , (96) (95)

is the complete subspace of observations. This is discussed next. VI. G EOMETRY OF FUSION Consider the system (75), and let X(t) be the (finitedimensional) subspace in H spanned by the components of the stochastic state vector x(t). Then it can be shown [18, Chapter 7] that H[0,t] (dy )  H[t,T ] (dy ) | Xt , where A  B | X denotes the conditional orthogonality  - E ,  - E  = 0 for all   A,   B.
X X

(92)

Next, let X- (t) and X+ (t) be the subspaces spanned by the components of the (intermittent) Kalman estimates x- (t) and  x ¯+ (t), respectively. Then since X- (t)  H- t  H[0,t] (dy ) and X+ (t) 
+ Ht 

 H[t,T ] (dy ), we have X- (t)  X+ (t) | X(t),

which is equivalent to EX+ (t) a x- (t) = EX+ (t) EX(t) a x- (t), [18, Proposition 2.4.2]. Therefore the diagram X-
EX |X- EX+ |X-

a  Rn

(93a)

-

X+
EX+ |X

(93b) where Ht is the subspace Ht = X- (t)  X+ (t).


X commutes, where the argument t has been suppressed. Lemma 4: Let x(t), x ¯(t), x- (t) and x ¯+ (t) be defined as above. Then, for each t  [0, T ], (i) E{x(t)x- (t) } = P- (t) ¯+ (t) (ii) E{x ¯(t)¯ x+ (t) } = P ¯+ (t)P- (t), (iii) E{x ¯+ (t)x- (t) } = P where P- (t) := E{x- (t)x- (t) } is the state covariance of the Kalman estimate x- (t) and P+ (t) := E{x ¯+ (t)¯ x+ (t) } is the covariance of the backward Kalman estimate x ¯+ (t). Proof: By the definition of the Kalman filter, (81) holds, and consequently the components of the estimation error x(t) - x- (t) are orthogonal to H- t and hence to the components of x- (t). Therefore, E{x(t)x- (t) } = E{x- (t)x- (t) } = P- (t),

(97)

Proof: Following [14], [3], [18], define N- (t) := H- t  X- (t) and N+ (t) := H+ X+ (t). Then t
- + H = N (t)  Ht  N (t). 

Now, a (x(t) - x- (t)) is orthogonal to H- t and hence to N- (t). Also a x- (t)  N- (t). Hence a x(t)  N- (t) as well. In the same way we see that a x(t)  N+ (t). Therefore (96) follows. Consequently, the information from the two Kalman filters can be fused into the smoothing estimate ¯ + (t)¯ x ^(t) = L- (t)x- (t) + L x+ (t) ¯+. for some matrix functions L- and L (98)



11

VII. U NIVERSAL TWO - FILTER FORMULA To obtain a robust and particularly simple smoothing formula that works also with an intermittent observation pattern, we assume that the stochastic system (75) has already been transformed via (58) so that, for all t  [0, T ], x(t) = x ¯(t) and therefore ¯ (t). P (t) = E{x(t)x(t) } = I = P (100) (99)

Then (102) follows from (98) and (107). To prove (104) ¯ + in (106) to obtain eliminate L ¯+ ) = Q ¯+, L - ( I - P- P which together with (107) yields
1 ¯ ¯ -1 Q-1 = Q- - (I - P- P+ )Q+ .

However, ¯+ = Q ¯ + + Q- - Q- Q ¯+, I - P- P and hence (104) follows. In the special case with no loss of observation this is a normalized version of the Mayne-Frazer two-filter formula [1], [2], which however in [1], [2] was formulated in terms of x- and x+ rather than x ¯+ , where x+ is the state process of the forward stochastic system of the backward Kalman filter. (For the corresponding formula in terms of x- and x ¯+ , see [3], [18]; also cf. [21], where an independent derivation was given.) With a single interval of loss of observation the formula (102) reduces to a version of the interpolation formulas in [6]. The remarkable fact, discovered here, is that the same formula (102) holds for any intermittent observations structure and by a cascade of continuous and discrete-time forward and backward Kalman filters, as needed depending on the assumed information pattern. VIII. R ECAP OF COMPUTATIONAL STEPS Given a system (75) with state covariance (55), make the normalizing substitution A(t)  P (t)- 2 A(t)P (t) 2 + R(t) B (t)  P (t)- 2 B (t) C (t)  C (t)P (t)
1 1 2 1 1 1 1

Then the error covariances in the filtering formulas of Section V are Q- = I - P- ¯+ = I - P ¯+ . and Q (101)

¯+ (t) are all bounded in Consequently, x(t), x ¯(t), P- (t) and P norm by one for all t  [0, T ]. Theorem 7: Suppose that (99) holds. For every t  [0, T ], we have the formula ¯ + (t)-1 x x ^(t) = Q(t) Q- (t)-1 x- (t) + Q ¯+ (t) (102)

for the smoothing estimate (90), where the estimation error Q(t) := E (x(t) - x ^(t)) (x(t) - x ^(t)) is given by ¯ + (t)-1 - I, Q(t)-1 = Q- (t)-1 + Q (104) (103)

and where x- , x ¯+ , Q- and Q+ are given by (82) and (86) with boundary conditions x- (0) = x ¯+ (T ) = 0 and Q- (0) = Q+ (T ) = I . ¯ + in (98) Proof: Clearly the matrix functions L- and L can be determined from the orthogonality relations E{[x(t) - x ^(t)]x- (t) } = 0 and E{[x(t) - x ^(t)]¯ x+ (t) } = 0. By Lemma 4, (105) yields ¯+P ¯+ P- = 0 P- - L- P- - L ¯+ - L- P- P ¯+ - L ¯+P ¯+ = 0, P ¯+ are positive definite, which, in view of the fact that P- and P yields ¯+P ¯+ = I L- + L (106a) ¯+ = I L- P- + L Again by orthogonality and Lemma 4, ¯+P ¯+ , Q = E {(x - x ^ ) x } = I - L- P - - L which, in view of (106) and the relations (101), yields L- =
1 QQ- -

(108)

(105a) (105b)

-2 with R(t) = d P (t) 2 . Next, we compute the dt P (t) intermittent forward and backward Kalman filter estimates x- and x ¯+ , respectively, along the lines of Section V, where, ¯ + (T ) = In . Then the due to the normalization, Q- (0) = Q smoothing estimate is given by

¯ + (t)-1 x x ^(t) = Q(t) Q- (t)-1 x- (t) + Q ¯+ (t) , where ¯ + (t)-1 - I Q(t) = Q- (t)-1 + Q IX. A N EXAMPLE We now illustrate the results of the paper on a specific numerical example. We consider the continuous-time diffusion process dx1 (t) dx2 (t) (107) dy (t) = x2 (t)dt = -0.3x1 (t)dt - 0.7x2 (t)dt + dw(t) = x1 (t)dt + dv (t)
-1

.

(106b)

and

¯+ = L

¯ -1 . QQ +

12

where w and v are thought to be independent standard Wiener processes. Here, x1 is thought of as position and x2 as velocity of a particle that is steered by stochastic excitation in dw, in the presence of a restoring force 0.3x1 and frictional force 0.7x2 . Then dy/dt represents measurement of the position and dv/dt represents measurement noise (white).
50 0 -50 0.5 0 5 10 15 20 25 30 35 40 45

of intervals, data are not made available for state estimation; these intervals where data are not to be used are marked by a thick blue baseline in the figures. In Figure 5 we display sample paths of the output process y , increments dy , and stateprocesses x1 and x2 .
2 1 0 -1 -2 -3 0 5 10 15 20 25 30 35
x 1,est x1

KF backward estimate x 1 - missing data in blue intervals

Output process and states

dy

y

0 -0.5 5 0 5 10 15 20 25 30 35 40 45

40

45

4 2
0 5 10 15 20 25 30 35 40 45

KF backward estimate x 2 - missing data in blue intervals
x 2,est x2

x1

0 -5 5

0 -2

x2

0 -5 0 5 10 15 20 25 30 35 40 45

-4

0

5

10

15

20

25

30

35

40

45

Fig. 5: Sample paths of output process, increment, and state processes
KF forward estimate x - missing data in blue intervals
2 1 0 -1 -2 -3 0 5 10 15 20 25 30 35 40
x est,1 x1
1

Time [sec]

Fig. 7: Kalman estimates in the backward time direction
2 1 0 -1 -2 -3 0 5 10 15 20 25 30 35
x 1,smooth x1

40

45

45 4 2 0 -2 -4 0 5 10

Smoothed state estimates
x 2,smooth x2

4 2 0 -2 -4 0

KF forward estimate x 2 - missing data in blue intervals
x est,2 x2

15

20

25

30

35

40

45

5

10

15

20

25

30

35

40

45

Time [sec]

Time [sec]

Fig. 6: Kalman estimates in the forward time direction Numerical simulation over [0, T ] with T = 45 (units of time) produces a time-function y (t) which is sampled with integer multiples of t = 0.01 (units). The interval [0, T ] is partitioned into [0, T ] = 9 i=1 [ti-1 , ti ] where t0 = 0 and ti - ti-1 = i (units). Measurements of y are made available for purposes of state estimation over the intervals [ti-1 , ti ] for i = 1, 3, 5, 9. Over the complement set

Fig. 8: Interpolation/smoothed estimates by fusion of Kalman forward and backward estimates The process increments dy over [ti-1 , ti ] for i = 1, 3, 5, 9 as well as the increments y across the [ti-1 , ti ] for i = 2, 4, 6, 8 are used in the two-filter formula for the purpose of smoothing. The Kalman estimates for the states in the forward and backwards in time directions, x- (t) and x ¯+ (t) are shown in Figures 6 and 7, respectively. The fusion of the two using (102) is shown in Figure 8. It is worth observing the nature and fidelity of the estimates. In the forward direction, across intervals where data is not available, x- becomes

13

increasing more unreliable whereas the opposite is true for x ¯+ , as expected. The smoothing estimate is generally an improvement to those of the two Kalman filters as seen in Figure 8. In particular, it is worth noting x2 (in subplot 2), where, over windows of available observations, estimates have considerably less variance in the middle of the interval where ¯ + (t)-1 ) in (102) are the weights (Q(t)Q- (t)-1 and Q(t)Q equalized, whereas sample paths become increasing rugged at the two ends where one of the two Kalman estimates has significantly higher variance, and the corresponding mixing coefficient becomes relatively smaller. X. C ONCLUDING REMARKS Historically the problem of interpolation has been considered from the beginning of the study of stochastic processes [22], [23]. Early accounts and treatments were cumbersome and non-explicit as the problem was considered difficult [7], [8], [9], [10]. In a manner that echoes the development of Kalman filtering, the problem became transparent and computable for ouput processes of linear stochastic systems [5], [6], [18]. This paper builds on developments in stochastic realization theory [11], [24] and presents a unified and generalized twofilter formula for smoothing and interpolation in continuous time for the case of intermittent availability of data over an operating window. The analysis considers two alternative information patterns where increments of the output process or the output process itself is recorded when information becomes available. The second information pattern appears most natural to us in this continuous-time setting, and this is our main problem. Nevertheless, in either case, two Kalman filters run in opposite time-directions, designed on the basis of a forward and a backward model for the process, respectively. Fusion of the respective estimates is effected via linear mixing in a manner similar to the Mayne-Fraser formula and applies to both smoothing and interpolation intermixed. In earlier works, smoothing and interpolation have been considered separate problems [18, Chapter 15]. The balancing normalization also simplifies the mixing formula and makes it completely time symmetric. The theory relies on time-reversal of stochastic models. We provide a new derivation of such a reversal which has the convenient property of being balanced. It is based on lossless imbedding of linear systems and effects the time reversal through a unitary transformation. Interestingly, time symmetry in statistical and physical laws have occupied some of the most prominent minds in science and mathematics. In particular, closer to our immediate interests, dual time-reversed models have been employed to model, in different time-directions, Brownian or Schr¨ odinger bridges [25], [26], a subject which is related to reciprocal processes [27], [28], [29], [30]. A natural

extension of the present work in fact is in the direction of general reciprocal dynamics [28], [29] and the question of whether similar two-filter formula are possible. A PPENDIX : T IME REVERSAL OF NON - STATIONARY DISCRETE - TIME SYSTEMS Next, instead of (1), consider the non-stationary state dynamics x(t + 1) = A(t)x(t) + B (t)w(t), x(0) = x0 , (109)

on a finite time-window [0, T ], where, for simplicity we now assume that the covariance matrix P0 := P (0) of the zero-mean stochastic vector x0 is positive definite, i.e., P0 = E{x0 x0 } > 0. Then the state covariance matrix P (t) := E{x(t)x(t) } will satisfy the Lyapunov difference equation P (t + 1) = A(t)P (t)A(t) + B (t)B (t) . The state transformation  (t) = P (t)- 2 x(t) brings the system (109) into the form  (t + 1) = F (t) (t) + G(t)w(t), where now E{ (t) (t) } = In for all t and F (t) = P (t + 1)- 2 A(t)P (t) 2 , G(t) = P (t + 1)
-1 2
1 1 1

(110)

(111)

(112)

(113a) (113b)

B.

The Lyapunov difference equation then reduces to In = F (t)F (t) + G(t)G(t) (114)

allowing us to embed [F, G] as part of a time-varying orthogonal matrix U (t) = F (t) G(t) H (t) J (t) . (115)

This amounts to extending (112) to  (t + 1) = F (t) (t) + G(t)w(t) w ¯ (t) = H (t) (t) + J (t)w(t), or, in the equivalent form  (t + 1)  (t) = U (t) . w ¯ (t) w(t) (117) (116a) (116b)

Hence, since E{ (t) (t) } = In and E{w(t)w(t) } = Ip , and assuming that E { (t)w(t) } = 0, E  (t + 1) w ¯ (t)  (t + 1) w ¯ (t) = U (t)U (t) = In+p , (118)

which yields E{ (t + 1)w ¯ (t) } = 0, E{w ¯ (t)¯ u(t) } = Ip . (119a) (119b)

14

Moreover, from (116) we have u ¯(t + k ) = H (t + k )(t + k, t) (t)
t+k-1

where x0 and the normalized white-noise process w are uncorrelated and E{x0 x0 } = P0 . In fact, inserting the transformations (122) and (123a) into (124b) yields ¯x ¯w y (t) = C ¯(t) + D ¯ (t), where ¯ = C (t)P (t)A(t) + D(t)B (t) C ¯ = C (t)P (t)B ¯ (t) + D(t)J (t) D From that we have the backward system ¯ (t)w x ¯(t - 1) = A(t) x ¯(t) + B ¯ (t) ¯ ¯ y (t) = C (t)¯ x(t) + D(t)w ¯ (t) (127a) (127b) (125) (126)

+
j =t

H (t + k )(t + k, j + 1)G(j )w(j ) + J (t)w(t)

for k > 0, where (s, t) = F (s - 1)F (s - 2) · · · F (t) for s > t In for s = t.

Therefore, since F (t)H (t) + G(t)J (t) = 0 by the unitarity of U (t), E{u ¯(t + k )¯ u(t) } = H (t + k )(t + k, t + 1)[F (t)H (t) + G(t)J (t) ] = 0. Consequently, u ¯ is a white noise process. Finally, premultiplying (117) by U (t) , we then obtain  (t) = F (t)  (t + 1) + H (t) w ¯ (t) w(t) = G(t)  (t + 1) + J (t) w ¯ (t), (120a) (120b)

with the boundary condition x ¯(T - 1) = P (T )-1 x(T ) being uncorrelated to the white-noise process w ¯.

R EFERENCES
[1] D. Q. Mayne, "A solution of the smoothing problem for linear dynamic systems," Automatica, vol. 4, pp. 73­92, 1966. [2] D. Fraser and J. Potter, "The optimum linear smoother as a combination of two optimum linear filters," Automatic Control, IEEE Transactions on, vol. 14, no. 4, pp. 387­390, 1969. [3] F. A. Badawi, A. Lindquist, and M. Pavon, "A stochastic realization approach to the smoothing problem," IEEE Trans. Automat. Control, vol. 24, no. 6, pp. 878­888, 1979. [4] F. Badawi, A. Lindquist, and M. Pavon, "On the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear stochastic systems," in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE, 1979, pp. 505­510. [5] M. Pavon, "New results on the interpolation problem for continuoustime stationary increments processes," SIAM journal on Control and Optimization, vol. 22, no. 1, pp. 133­142, 1984. [6] ----, "Optimal interpolation for linear stochastic systems," SIAM journal on Control and Optimization, vol. 22, no. 4, pp. 618­629, 1984. [7] K. Karhunen, Zur Interpolation von station¨ aren zuf¨ alligen Funktionen. Suomalainen tiedeakatemia, 1952. [8] Y. Rozanov, Stationary random processes. Holden-Day, San Francisco, 1967. [9] P. Masani, "Review: Yu.A. Rozanov, stationary random processes," The Annals of Mathematical Statistics, vol. 42, no. 4, pp. 1463­1467, 1971. [10] H. Dym and H. P. McKean, Gaussian processes, function theory, and the inverse spectral problem. Courier Dover Publications, 2008. [11] A. Lindquist and G. Picci, "On the stochastic realization problem," SIAM J. Control Optim., vol. 17, no. 3, pp. 365­389, 1979. [12] ----, "Forward and backward semimartingale models for Gaussian processes with stationary increments," Stochastics, vol. 15, no. 1, pp. 1­50, 1985. [13] ----, "Realization theory for multivariate stationary Gaussian processes," SIAM J. Control Optim., vol. 23, no. 6, pp. 809­857, 1985. [14] ----, "A geometric approach to modelling and estimation of linear stochastic systems," J. Math. Systems Estim. Control, vol. 1, no. 3, pp. 241­333, 1991.

which, in view of (119), is a backward stochastic system. Using the transformation (111), (116) yields the forward representation x(t + 1) = A(t)x(t) + B (t)w(t) ¯ (t) x(t) + J (t)w(t), w ¯ (t) = B
1 ¯ (t) := P (t)- 2 H (t) . Likewise (120) and where B

(121a) (121b)

x ¯(t) = P (t + 1)-1 x(t + 1), yields the backward representation ¯ (t)w x ¯(t - 1) = A(t) x ¯(t) + B ¯ (t) w(t) = B (t) x ¯(t) + J (t) w ¯ (t).

(122)

(123a) (123b)

Remark 8: When considered on the doubly infinite time axis, equation (117) defines an isometry. Indeed, assuming that the input is squarely summable, the fact that U (t) is unitary for all t directly implies that
N N

w ¯
-

2

+  (t + 1)

2

=
-

w(t) 2 .

Then,  (t)  0 as t  , provided (t, s)  0 as s  -. It follows that
 

w ¯ (t)
t=-

2

=
t=-

w(t) 2 .

We are now in a position to derive a backward version of a non-stationary stochastic system x(t + 1) = A(t)x(t) + B (t)w(t), y (t) = C (t)x(t) + D(t)w(t) x(0) = x0 (124a) (124b)

15

[15] A. Lindquist and M. Pavon, "On the structure of state-space models for discrete-time stochastic vector processes," IEEE Trans. Automat. Control, vol. 29, no. 5, pp. 418­432, 1984. [16] G. Michaletzky, J. Bokor, and P. V´ arlaki, Representability of stochastic systems. Budapest: Akad´ emiai Kiad´ o, 1998. [17] G. Michaletzky and A. Ferrante, "Splitting subspaces and acausal spectral factors," J. Math. Systems Estim. Control, vol. 5, no. 3, pp. 1­26, 1995. [18] A. Lindquist and G. Picci, Linear Stochastic Systems: A Geometric Approach to Modeling, Estimation and Identification. Springer-Verlag, Berlin Heidelberg, 2015. [19] T. T. Georgiou, "The Carath´ eodory­Fej´ er­Pisarenko decomposition and its multivariable counterpart," Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 212­228, 2007. [20] T. Georgiou and A. Lindquist, "On time-reversibility of linear stochastic models," arXiv preprint arXiv:1309.0165, 2013. [21] J. E. Wall Jr, A. S. Willsky, and N. R. Sandell Jr, "On the fixedinterval smoothing problem," Stochastics: An International Journal of Probability and Stochastic Processes, vol. 5, no. 1-2, pp. 1­41, 1981. [22] A. N. Kolmogorov, Stationary sequences in Hilbert space. John Crerar Library National Translations Center, 1978. [23] A. M. Yaglom, "On problems about the linear interpolation of stationary random sequences and processes," Uspekhi Matematicheskikh Nauk,

vol. 4, no. 4, pp. 173­178, 1949. [24] M. Pavon, "Stochastic realization and invariant directions of the matrix Riccati equation," SIAM Journal on Control and Optimization, vol. 18, no. 2, pp. 155­180, 1980. [25] M. Pavon and A. Wakolbinger, "On free energy, stochastic control, and Schr¨ odinger processes," in Modeling, Estimation and Control of Systems with Uncertainty. Springer, 1991, pp. 334­348. [26] P. Dai Pra and M. Pavon, "On the Markov processes of Schr¨ odinger, the Feynman-Kac formula and stochastic control," in Realization and Modelling in System Theory. Springer, 1990, pp. 497­504. [27] B. Jamison, "Reciprocal processes," Probability Theory and Related Fields, vol. 30, no. 1, pp. 65­86, 1974. [28] A. Krener, "Reciprocal processes and the stochastic realization problem for acausal systems," in Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197­211. [29] B. C. Levy, R. Frezza, and A. J. Krener, "Modeling and estimation of discrete-time Gaussian reciprocal processes," Automatic Control, IEEE Transactions on, vol. 35, no. 9, pp. 1013­1023, 1990. [30] P. Dai Pra, "A stochastic control approach to reciprocal diffusion processes," Applied mathematics and Optimization, vol. 23, no. 1, pp. 313­329, 1991.

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian Arizona State University Mesa, AZ 85212 USA {kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu Abstract
Software engineering education is a technologically challenging, rapidly evolving discipline. Like all STEM educators, software engineering educators are bombarded with a constant stream of new tools and techniques (MOOCs! Active learning! Inverted classrooms!) while under national pressure to produce outstanding STEM graduates. Software engineering educators are also pressured on the discipline side; a constant evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the technology, guidance on the adoption of project-centric curricula is needed. This paper focuses on vertical integration of project experiences in undergraduate software engineering degree programs or course sequences. The Software Enterprise, now in its 9 th year, has grown from an upper-division course sequence to a vertical integration program feature. The Software Enterprise is presented as an implementation of a project spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software engineering and computer science education focus on content taxonomies and bodies of knowledge. This is not a bad thing, but taken in isolation may lead educators to believe content coverage is more important than applied learning experiences. There is literature on project-based learning within computing as a means to learn soft skills and complex technical competencies. However, project experiences tend to be disjoint [5]; there may be a freshman project or a capstone project or a semester project assigned by an individual instructor. Yearlong capstone projects are offered at most institutions as a synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do it all the time ? Project experiences, while pervasive in computing programs, are not a central integrating feature. Sheppard et al. [6] suggests that engineering curricular design should move away from a linear, deductive model and move instead toward a networked model: "The ideal learning trajectory is a spiral, with all components revisited at increasing levels of sophistication and interconnection" ([6] p. 191). The general engineering degree program at Arizona State University (ASU) was designed from its inception in 2005 [7] to be a flexible, project-centric curriculum that embodied such integration (even before [6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division course sequence to integrate contextualized project experiences with software engineering fundamental concepts. The computing and engineering programs at ASU's Polytechnic campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

Board of Regents (ABOR) approved a new Bachelor's degree in software engineering (BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo accreditation review shortly thereafter. At the course level the Software Enterprise defines a delivery structure integrating established learning techniques around a project-based contextualized learning experience. At the degree program level, the Enterprise weaves project experiences throughout the BS SE degree program, integrating program outcomes at each year of the major. There are several publications on the manner in which the Software Enterprise is conducted within a project course (for example, [8][9]]), and we summarize this in-course integration pedagogy in section 2. The intent of this work-in-progress paper is to describe extending the Enterprise as a spiral curricular design feature we refer to as the project spine , and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a student's competencies from understanding to comprehension to applied knowledge by co-locating preparation , discussion , practice , reflection , and contextualized learning activities in time. In this model, learners prepare for a module by doing readings, tutorials, or research before a class meeting time. The class discusses the module's concepts, in a lecture or seminar-style setting. The students then practice with a tool or technique that reinforces the concepts in the next class meeting. At this point students reflect to internalize the concepts and elicit student expectations, or hypotheses , for the utility of the concept. Then, students apply the concept in the context of a team-oriented, scalable project, and finally reflect again to (in)validate their earlier hypotheses. These activities take place in a single three-week sprint , resulting is a highly iterative methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right) The Software Enterprise represents an innovation derived from existing scholarship in that it assembles best practices such as preparation, reflection, practice (labs), and project-centered learning in a rapid integration model that accelerates applied learning. Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle [10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on maturing the delivery process, creating new or packaging existing learning materials to fit the delivery model, and to explore ways to assess project-centered learning.

3. The Software Enterprise Project Spine
An innovation in the new BS in Software Engineering at ASU has been the vertical adoption of the Software Enterprise. Enterprise courses are now required from the sophomore to senior years. This innovation represents what [6] calls a professional spine , as the Enterprise serves as an integrator of learning outcomes for a given year in the major. We refer to our project-centered realization as a project spine , where foundational concepts are tied to project work throughout the undergraduate program . There is significant computing literature on projects (embedded, mobile, gaming, etc.) to achieve learning or retention outcomes. However, computing lacks a framework for integrating concepts in a project spine. The Enterprise is an implementation that moves students from basic comprehension to applied Figure 2. ASU Project Spine knowledge to critical analysis outcomes. In the BS SE at ASU, program outcomes are described at 4 levels: describe , apply , select , and internalize . Students must achieve level 3 ( select between alternatives) in at least 1 outcome and achieve level 2 ( apply ) in all others. The program outcomes for the BS SE include Design, Computing Practice, Critical Thinking, Professionalism, Perspective, Problem Solving, Communication, and Technical Competence . An example leveled outcome description for Perspective is given in Table 1. The Enterprise accelerates level 3 outcomes by providing contextualized integrated experiences fostering decision-making in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes. Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in business, global, economic, environmental, and societal contexts. Level 1. Understands technological change and development have both positive & negative effects. Level 2. Identifies and evaluates the assumptions made by others in their description of the role and impact of engineering and computing on the world. Level 3. Selects from different scenarios for the future and appropriately adapts them to match current technical, social, economic and political concerns. Level 4 . Has formed a constructive model for the future of our society, and makes life and career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical competencies by assigning projects inclusive of the technical material covered in the regular computing courses. So for example, junior projects (Software Enterprise III and IV) emphasize technical complexities in Networks, Distributed Computing, and Databases, while senior projects emphasize technical complexities in Web and Mobile computing. The technical "focus area" courses are chosen more based on faculty expertise and recruitment goals than software engineering outcomes; one can envision many different areas represented by upper division courses here. These do help address the concern that an accredited software engineering degree has an application area. A risk we have not yet addressed is if the technical area impacts the software engineering process, such as with a soon-to-be-introduced embedded systems focus area.

There are 2 additional aspects of integration to the project spine. As summarized in section 2, the Enterprise integrates software engineering concepts throughout the project experiences. Students in the sophomore year learn the Personal Software Process [11] as a means to build individual understanding of time management, defect management, and estimation skills. They then focus on Quality, including but not limited to testing. In the junior year Enterprise students focus on Design (human-centered and system design principles) followed by best practices in software construction, taken primarily from eXtreme Programming. In the senior year students focus on Requirements Engineering then Process and Project Management. The final aspect of integration is with soft-skill outcomes such as Communication , Teamwork , and Professionalism (see Table 1). Throughout the spine the project experiences are crafted to ensure variations on pedagogy to address these outcomes. For example, in the freshman year students receive explicit instruction in teamwork. In the senior year the emphasis is on formal documentation as a means of communication. In the junior year, students work on service learning projects of high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of program adoption. There are examples of program design and lessons learned [5][12][13], or reflections and recommendations on the software engineering education landscape [14][15][16][17][18]. These are worthwhile guides but do not offer examples on evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on "Program Implementation and Assessment" which discusses a number of key factors in program adoption, but is geared toward accreditation and not evaluation instruments. A survey instrument is presented in [19] but is designed for comparison of a large number of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate programs in software engineering but more as an aggregate counting exercise in knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software Engineering project conducted a survey of graduate degree programs [20] and then produced a comparison report [21] of graduate programs to the GSwE2009 reference model, which includes data on program characteristics and in-depth profiles from 3 institutions. A recent study is Conry's [23] survey of accredited software engineering degree programs. Conry summarizes institutional, administrative, and curricular (knowledge area) aspects in describing the 19 accredited programs as of October 2009. Certainly program adoption measures from other engineering programs are also relevant, though software engineering programs are unique due to the forces discussed in section 1. Our next steps for the Enterprise-as-project-spine involve defining measures for adoption impact, and determining how this concept fits with established patterns for curricular maps in software engineering programs. We plan to use quantitative and qualitative instruments to evaluate adoption. Quantitative data, such as program size, institution type, faculty and student backgrounds, can be collected via available resources (departmental archives or online) and direct surveys. Qualitative data can be collected through survey instruments and interviews of all stakeholders (faculty participants, administrators, and advisors). Different instruments may be used at different times to evaluate "in-stream" attitudes versus post-adoption reflections. Defining and validating these instruments is a significant area of work going forward. The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in software engineering. Taxonomies are useful and the sign of an emerging discipline. We

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas, and plan to elaborate on these mappings. Specifically, we intend to produce CS2013 course exemplars. Further, the SE2004 report includes a section on program curricular patterns, and we will propose new patterns based on the project spine concept, which we hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the New Century. The National Academies Press, Washington D.C., 2005. [2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of Knowledge (SWEBOK). Los Alamitos, CA, 2004. [3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013. [4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society. Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering. Joint Task Force on Computing Curricula, 2004. [5] Shepard, T. "An Efficient Set of Software Degree Programs for One Domain." In Proceedings of the International Conference on Software Engineering (ICSE) 2001. [6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the Future of the Field, Jossey-Bass, San Francisco, 2008. [7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. "A Flexible Curriculum for a Multi-disciplinary Undergraduate Engineering Degree." Proceedings of the Frontiers in Education Conference 2005. [8] Gary, K. "The Software Enterprise: Practicing Best Practices in Software Engineering Education", The International Journal of Engineering Education Special Issue on Trends in Software Engineering Education, Volume 24, Number 4, July 2008, pp. 705-716. [9] Gary, K., "The Software Enterprise: Preparing Industry-ready Software Engineers" Software Engineering: Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group Publishing. October 2008. [10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984. [11] Humphrey, W.S. Introduction to the Personal Software Process , Addison-Wesley, Boston, 1997. [12] Lutz, M. and Naveda, J.F. "The Road Less Traveled: A Baccalaureate Degree in Software Engineering." Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997. [13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor's Program. IEEE Software November/December 2006. [14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. "Guidance for the development of software engineering education programs." The Journal of Systems and Software, 49(1999):163-169. 1999. [15] Ghezzi, C. and Mandrioli. "The Challenges of Software Engineering Education." In Proceedings of the International Conference on Software Engineering (ICSE) 2006. [16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. "Improving software practice through education: Challenges and future trends." Proceedings of the Future of Software Engineering Conference, 2007. [17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the Future of Software Engineering, Limerick Ireland, 2000. [18] Mead, N. (2009). Software Engineering Education: How far We've Come and How far We Have to Go. Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009. [19] Modesitt, K., Bagert, D.J., and Werth, L. "Academic Software Engineering: What is it and What Could it be? Results of the First International Survey for SE Programs." Proceedings of the International Conference on Software Engineering (ICSE) 2001. [20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008. [21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master's Programs in Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013. [22] Bagert, D.J. & Chenoweth, S.V. "Future Growth of Software Engineering Baccalaureate Programs in the United States", Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005. [23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of the American Society for Engineering Education, Louisville, KY, 2010. [24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). "Revision of the SE2004 Curriculum Model." Panel at the ACM Conference of the Special Interest Group on Computer Science Education (SIGCSE), Denver, CO, 2013.

SOCRADES: A Web Service based Shop Floor Integration Infrastructure
Luciana Moreira S´ a de Souza, Patrik Spiess, Dominique Guinard, Moritz K¨ ohler, Stamatis Karnouskos, and Domnic Savio
SAP Research Vincenz-Priessnitz-Strasse 1, D-76131, Karlsruhe, Germany Kreuzplatz 20, CH-8008, Zurich, Switzerland {luciana.moreira.sa.de.souza, patrik.spiess, dominique.guinard, mo.koehler, stamatis.karnouskos, domnic.savio} @sap.com

Abstract. On the one hand, enterprises manufacturing any kinds of goods require agile production technology to be able to fully accommodate their customers' demand for flexibility. On the other hand, Smart Objects, such as networked intelligent machines or tagged raw materials, exhibit ever increasing capabilities, up to the point where they offer their smart behaviour as web services. The two trends towards higher flexibility and more capable objects will lead to a service-oriented infrastructure where complex processes will span over all types of systems -- from the backend enterprise system down to the Smart Objects. To fully support this, we present SOCRADES, an integration architecture that can serve the requirements of future manufacturing. SOCRADES provides generic components upon which sophisticated production processes can be modelled. In this paper we in particular give a list of requirements, the design, and the reference implementation of that integration architecture.

1

Introduction

In the manufacturing domain, constant improvements and innovation in the business processes are key factors in order to keep enterprises competitive in the market. Manufacturing businesses are standing on the brink of a new era, one that will considerably transform the way business processes are handled. With the introduction of ubiquitous computing on the shop floor1 , an entirely new dynamic network of networked devices can be created - an Internet of Things (IoT) for manufacturing. The Internet of Things is a concept which first appeared shortly after 2000. Until now, several approaches to describe the IoT have been undertaken of which most have focused on RFID technologies and their application ([5, 13]). Only recently, new technologies such as Smart Embedded Devices and Sensor Networks have entered the scene and can be considered as part of the IoT [11].
1

In manufacturing, the shop floor is the location where machines are located and products produced.

Smart Embedded Devices are embedded electronic systems which can sense their internal state and are able to communicate it through data networks. In contrast to this, Sensor Networks not only can measure internal states of their nodes, but also external states of the environment. We group these three technologies RFID, Smart Embedded Devices, and Sensor Networks - under the notion Smart Objects. Smart Objects are the nerve cells, which are interconnected through the Internet and thus build the IoT. RFID has already been proved to open fundamentally new ways of executing business processes, and the technology has already been adopted by several key players in the industry. Therefore the focus of this paper is on Smart Embedded Devices and Sensor Networks and their effects on automatic business process execution. Although client-server architectures still play an important role in the field of business software systems, the Service Oriented Architecture (SOA) is on the move and it is foreseeable that this architectural paradigm will be predominant in the future. The integration of devices into the business IT-landscape through SOA is a promising approach to connect physical objects and to make them available to IT-systems. This can be achieved by running instances of web services on these devices, which moves the integration of back end applications, such as Enterprise Resource Planning (ERP) systems, with the devices one step forward, enabling them to interact and create an Internet of Services that collaborates and empowers the future service-based factory. Enabling efficient collaboration between device-level SOA and services and applications that constitute the enterprise back-end on the other hand, is a challenging task. The introduction of web service concepts at a level as low as the production device or facility automation makes this integration significantly less complex. But there are still differences between device-level SOA and the one that is used in the back end. To name but a few of them, device-level services are of higher granularity, exhibit a lower reliability (especially if they are connected wirelessly) and higher dynamicity and are more focused on technical issues than on business aspects. These differences can be overcome by introducing a middleware between the back end applications and the services that are offered by devices, service mediators, and gateways. This middleware adds the required reliability, provides means to deal with services appearing and disappearing, and allows intermediate service composition to raise the technical interfaces of low-level services to business-relevant ones. In this paper we present the SOCRADES middleware for business integration; an architecture focused on coupling web service enabled devices with enterprise applications such as ERP Systems. Our approach combines existing technologies and proposes new concepts for the management of services running on the devices. This paper is organized as follows: in section 2 we discuss the current state of the art in coupling technologies for shop floor and enterprise applications. Section 3 presents the requirements for our approach, followed by section 4 where we

discuss our approach. We propose a prototype for evaluating our approach in section 5 and perform an analysis in section 6. Section 7 concludes this paper.

2

Related Work

Manufacturing companies need agile production systems that can support reconfigurability and flexibility to economically manufacture products. These systems must be able to inform resource planning systems like SAP ERP in advance, about the upcoming breakdown of a whole production processes or parts of them, so that adaptation in the workflow can be elaborated. Currently Manufacturing Execution Systems (MES) are bridging the gap between the shop floor and ERP systems that run in the back end. The International Systems and Automation Society - 95 (ISA-95) derivative from the Instrumentation Systems and Automation Society define the standards for this interface [1]. Although MES systems exist as gateways between the enterprise world and the shop floor, they have to be tailored to the individual group of devices and protocols that exist on this shop floor. By integrating web services on the shop floor, devices have the possibility of interacting seamlessly with the back end system ([9, 8]). Currently products like SIMATIC WinCC Smart Access [2] from Siemens Automation use SOAP for accessing tag based data from devices like display panels to PC's. However, they neither provide mechanisms to discover other web-service enabled devices, nor mechanisms for maintaining a catalogue of discovered devices. The domain of Holonic Manufacturing Execution Systems (HMS) [6] is also relevant to our work. HMS are used in the context of collaborative computing, and use web service concepts to integrate different sources and destinations inside a production environment. They do, however, not offer support to process orchestration or service composition. Amongst others, European Commission funded projects like SIRENA [4] showed the feasibility and benefit of embedding web services in production devices. However, since these were only initial efforts for proving the concept, not much attention has been given to issues such as device supervision, device life cycle management, or catalogues for maintaining the status of discovered devices, etc. The consortium of the SOCRADES project has integrated partners, code and concepts from SIRENA, and aims to further design and implement a more sophisticated infrastructure of web-service enabled devices. SODA (www.sodaitea.org) aims at creating a comprehensive, scalable, easy to deploy ecosystem built on top of the foundations laid by the SIRENA project. The SODA ecosystem will comprise a comprehensive tool suite and will target industry standard platforms supported by wired and wireless communications. Although EU projects like SIRENA showed the feasibility and benefit of embedding web services in devices used for production, they do not offer an infrastructure or a framework for device supervision or device life cycle. They neither do provide a catalogue for maintaining the status of discovered devices [4]. Changes due to the current development are moving towards a more promis-

ing approach of integrating shop floor devices and ERP systems more strongly [14]. Some authors are criticizing the use of RPC-style interaction in ubiquitous computing [12] (we consider the smart manufacturing devices a special case of that). We believe this does not concern our approach, since web services also allow for interaction with asynchronous, one-way messages and publish-subscribe communication. SAP xApp Manufacturing Integration and Intelligence (SAP xMII) is a manufacturing intelligence portal that uses a web server to extract data from multiple sources, aggregate it at the server, transform it into business context and personalize the delivered results to the users [7]. The user community can include existing personal computers running internet browsers, wireless PDAs or other UIs. Using database connectivity, any legacy device can expose itself to the enterprise systems using this technology. The drawback of this product is that every device has to communicate to the system using a driver that is tailored to the database connectivity. In this way, SAP xMII limits itself to devices or gateway solutions that support database connectivity. In [10], we proposed a service-oriented architecture to bridge between shop floor devices and enterprise applications. In this paper however, building on both our previous work and SAP xMII, we show how the already available functionality of xMII can be leveraged and extended to provide an even richer integration platform. The added functionality comprises integration of web service enabled devices, making them accessible through xMII, and supporting the software life cycle of embedded services. This enables real-world devices to seamlessly participate in business processes that span over several systems from the back end through the middleware right down to the Smart Objects.

3

System Requirements

As embedded technology advances, more functionality that currently is hosted on powerful back end systems and intermediate supervisory devices can now be pushed down to the shop floor level. Although this functionality can be transferred to devices that have only a fraction of the capabilities of more complex systems, their distributed orchestration in conjunction with the fact that they execute very task-specific processing, allows us to realise approaches that can outperform centralised systems in means of functionality. By embedding web services on devices, these can become part of a modern Enterprise SOA communication infrastructure. The first step to come closer to realize this vision, is to create a list of requirements. We have come up with this list through interviews with project partners and customers from the application domain, as well as a series of technical workshops with partners form the solution domain. As usually done in software engineering, we separated the list into functional and non-functional requirements.

Functional Requirements ­ WS based direct access to devices: Back end services must be able to discover and directly communicate with devices, and consume the services they offer. This implies the capability of event notifications from the device side, to which other services can subscribe to. ­ WS based direct access to back end services: Most efforts in the research domain today focus on how to open the shop floor functionality to the back end systems. The next challenge is to open back end systems to the shop floor. E.g. devices must be able to subscribe to events and use enterprise services. Having achieved that, business logic executing locally on shop floor devices can now take decisions not only based on its local information, but also on information from back end systems. ­ Service Discovery: Having the services on devices will not be of much use if they can not be dynamically discovered by other entities. Automatic service discovery will allow us to access them in a dynamic way without having explicit task knowledge and the need of a priori binding. The last would also prevent the system from scaling and we could not create abstract business process models. ­ Brokered access to events: Events are a fundamental pillar of a service based infrastructure. Therefore access to these has to be eased. As many devices are expected to be mobile, and their online status often change (including the services they host), buffered service invocation should be in-place to guarantee that any started process will continue when the device becomes available again. Also, since not all applications expose web services, a pull point should be realised that will offer access to infrastructure events by polling. ­ Service life cycle management: In future factories, various services are expected to be installed, updated, deleted, started, and stopped. Therefore, we need an open ways of managing their life cycle. Therefore the requirement is to provide basic support in the infrastructure itself that can offer an open way of handling these issues. ­ Legacy device integration: Devices of older generations should be also part of the new infrastructure. Although their role will be mostly providing (and not consuming) information, we have to make sure that this information can be acquired and transformed to fit in the new WS-enabled factory. Therefore the requirement is to implement gateways and service mediators to allow integration of the non-ws enabled devices. ­ Middleware historian: In an information-rich future factory, logging of data, events, and the history of devices is needed. The middleware historian is needed which offers information to middleware services, especially when an analysis of up-to-now behavior of devices and services is needed. ­ Middleware device management: Web service enabled devices, will contain both, static and dynamic data. This data can now be better and more reliably integrated to back end systems offering a more accurate view of the shop floor state. Furthermore by checking device data and enterprise inventory, incompatibilities can be discovered and tackled. Therefore we require

approaches that will effectively enable the full integration of device data and their exploitation above the device-layer. Non-Functional Requirements ­ Security support: Shop floors are more or less closed environments with limited and controlled communication among their components. However, because of open (and partially wireless) communication networks, this is fundamentally changing. Issues like confidentiality, integrity, availability must be tackled. In a web service mash-up - as the future factory is expected to be -, devices must be able to a) authenticate themselves to external services and b) authenticate/control access to services they offer. ­ Semantics support: This requirement facilitates the basic blocks primarily for service composition but also for meaningful data understanding and integration. Support for the usage of ontologies and semantic-web concepts will also enhance collaboration as a formal description of concepts, terms, and relationships within a manufacturing knowledge domain. ­ Service composition: In a SOA infrastructure, service composition will allow us to build more sophisticated services on top of generic ones, therefore allowing thin add-ons for enhanced functionality. This implies a mixed environment where one could compose services a) at device level b) at back end level and c) in a bidirectional cross-level way. In the above list we have described both, functional and non-functional requirements. In our architecture these requirements will be realized through components, each one offering a unique functionality.

4
4.1

Architecture
Overview

In this chapter, we present a concrete integration architecture focusing on leveraging the benefits of existing technologies and taking them to a next level of integration through the use of DPWS and the SOCRADES middleware. The architecture proposed in Figure 1 is composed of four main layers: Device Layer, SOCRADES middleware (consisting of an application and a device services part), xMII, and Enterprise Applications. The Device Layer comprises the devices in the shop floor. These devices when enabled with DPWS connect to the SOCRADES middleware for more advanced features. Nevertheless, since they support web services, they provide the means for a direct connection to Enterprise Applications. For the intermediate part of the SOCRADES architecture, bridging between enterprise and device layer, we identified an SAP product that partly covered our requirements: SAP xApp Manufacturing Integration and Intelligence (SAP xMII). The features already available in xMII are:

ENTERPRISE APPLICATIONS
HTML-GUI / Applets SAP Protocols Web Services

xMII
Visualization Services Applets Display Controls Displays GUI Widgets SAP Transaction Access SAP Connectivity

SOCRADES MIDDLEWARE APP SERVICES
Invoker Asynchronous Buffer Eventing Notification Broker (Event) Pull Point

Web Services Cross-layer
Service Catalogue

Business Logic Services
Business Process Monitoring
Alert

Composed Services Runtime SOCRADES Connector Web Services DPWS Back-end Services

Legacy Connector Shop floor standard

Data Services

SOCRADES MIDDLEWARE DEVICE SERVICES
Device Manager and Monitor Middleware Historian Service Discovery
Service Lifecycle Management

Hardware Vendor Implementation

Service Services Mapper Repository

Service Access Control Proprietary Protocol OPC UA over DPWS OPC UA over DPWS Gateway

DEVICE LAYER

Fig. 1. SOCRADES Integrated Architecture

­ Connectivity to non web service enabled devices via various shop floor communication standards ­ Graphical modelling and execution of business rules ­ Visualization Services ­ Connectivity to older SAP software through SAP-specific protocols We decided not to re-implement that functionality but use it as a basis and extend it by what we call the SOCRADES middleware. The SOCRADES middleware and xMII perform together a full integration of devices with ERP systems, adding functionalities such as graphical visualization of device data and life cycle management of services running on the devices. In this setting, xMII provides the handling of business logic, process monitoring and visualization of the current status of the devices. Finally, the connection with Enterprise Applications is realized in three ways. SAP xMII can be used to generate rich web content that can be integrated into the GUI of an enterprise system in mash-up style. Alternatively, it can be used to establish the connection to older SAP systems using SAP-specific protocols. Current, web service based enterprise software can access devices either via web services of the SOCRADES middleware, benefiting from the additional functionality, or they can directly bind against the web services of DPWS-enabled devices. The data delivered to Enterprise Applications is currently provided by xMII. Nevertheless with the introduction of the SOCRADES middleware and

the use of DPWS, this data can be also delivered directly by the regarding devices, leaving to xMII only the task of delivering processed data that requires a global view of the shop floor and of the business process. 4.2 Features and Components of the SOCRADES Middleware

The SOCRADES middleware is the bridging technology that enables the use of features of existing software systems with DPWS enabled devices. Together with SAP xMII, this middleware connects the shop floor to the top floor, providing additional functionality not available in either one of these layers. Although direct access from an ERP system to devices is possible, the SOCRADES middleware simplifies the management of the shop floor devices. In the following, we list this additional functionality and show how the components of the architecture implement them. Brokered Access to Devices. Brokered access means to have an intermediate party in the communication between web service clients and servers that adds functionality. Example are asynchronous invocations, a pull point for handling events, and a publish-subscribe mechanism for events. Asynchronous invocations are useful when dealing with devices that are occasionally connected so that invocations have to be buffered until the device re-appears; they are implemented by the Invoker component. Pull points enable applications to access events without having to expose a web service interface to receive them. The application can instruct the pull point to buffer events and can obtain them by a web service call whenever it is ready. Alternatively, to be notified immediately, the application can expose a web service endpoint and register it at the notification broker for any type of event. Service Discovery: The service discovery components carries out the actual service discovery on the shop floor level. This component is distributed and replicated at each physical site because the DPWS discovery mechanism WSDiscovery relies on UDP multicast, a feature that may not be enabled globally across all subsidiaries in a corporate network. All discovered devices from all physically distributed sites and all the services that each device runs are then in a central repository called Device Manager and Monitor, which acts as the single access point where ERP systems can find all devices even when they have no direct access to the shop floor network. Device Supervision: Device Management and Monitor and DPWS Historian provide the necessary static and dynamic information about each DPWS-enabled physical device available in the system. The device manager holds any static device data of all on-line and off-line devices while the device monitor contains information about the current state of each device. The middleware historian can be configured to log any event occurring at middleware level for later diagnosis

and analysis. Many low-level production systems feature historians, but they are concerned with logging low-level data that might be irrelevant for businesslevel analysis. Only a middleware historian can capture high-level events that are constructed within this architectural layer.

Service Life Cycle Management: Some hardware platforms allow exchanging the embedded software running on them via the network. In a service-enabled shop floor this means that one can update services running on devices. The management of these installed services is handled through the use of the Service Mapper and Services Repository. These components together make a selection of the software that should run in each device and perform the deployment.

Cross-Layer Service Catalogue: The cross-layer service catalogue comprises two components. One is the Composed Services Runtime that executes service composition descriptions, therefore realizing service composition at the middleware layer. The second component is the DPWS device for back end services that allows DPWS devices to discover and use a relevant set of services of the ERP system. The Composed Services Runtime is used to enrich the services offered by the shop floor devices with business context, such as associating an ID read from an RFID tag with the corresponding order. A compound service can deliver this data by both invoking a service on the RFID reader, and from a warehouse application. A Composed Services Runtime, which is an execution engine for such service composition descriptions, e.g., BPEL [3], is placed in the middleware because only from there, all DPWS services on the shop floor as well as all back end services can be reached. Another requirement is that shop floor devices must be able to access enterprise application services, which can be achieved by making a relevant subset available through the DPWS discovery. This way, devices that run DPWS clients can invoke back end services in exactly the same way they invoke services on their peer devices. Providing only the relevant back end services allows for some access control and reduces overhead during discovery of devices. Co-locating both sub-components in the same component has the advantage that also the composed services that the Composed Services Runtime provides, can be made available to the devices through the virtual DPWS device for back end services.

Security support: The (optional) security features supported by the middleware are role-based access control of devices communication to middleware and back end services and vice versa. Event filtering based on roles is also possible. Both the devices as well as back end and middleware services have to be authorized when they want to communicate. Access control is enforced by the respective component. Additionally, message integrity and confidentiality is provided by the WS-Security standard.

To demonstrate the feasibility of our approach and to make some first evaluations, we implemented a simple manufacturing scenario. We used a first implementation of our architecture to connect two DPWS-enabled real-world devices with an enterprise application.

5

Reference Implementation

In order to prove the feasibility of our concept, we have started realising a reference implementation. From a functional point of view, it demonstrates two of the most important incentives for the use of standardized device level web services in manufacturing: flexibility and integration with enterprise software. Indeed, the scenario shows DPWS-enabled devices can be combined easily to create higher-level services and behaviours that can then be integrated into topfloor applications. The business benefits from adopting such an architecture are numerous: ­ ­ ­ ­ lower cost of information delivery increased flexibility and thus total cost of ownership (TCO) of machines. increased visibility of the entire manufacturing process to the shop floor. ability to model at the enterprise layer processes with only abstract view of the underlying layer, therefore easing the creation of new applications and services from non-domain experts. Scenario

5.1

To support this idea we consider a simple setting with two DPWS devices: ­ A robotic arm that can be operated through web service calls. Additionally it offers status information to subscribers through the SOCRADES eventing system. ­ A wireless sensor node providing various information about the current environment, delivered as events. Furthermore, the sensor nodes provide actuators that are accessible through standard service calls. The manufacturing process is created on the shop floor using a simple service composition scheme: from the atomic services offered by the arm (such as start/stop, etc.) a simple manufacturing process p is created. The robot manipulates heat-sensitive chemicals. As a consequence it is identified that the manufacturing process cannot continue if the temperature rises above 45 . The robot may not have a temperature sensor (or this is malfunctioning), but as mentioned before the manufacturing plant is equipped with a network of wireless sensor nodes providing information about the environment. Thus, in order to enforce the business rule, the chief operator uses a visual composition language to combine p with the temperature information published by the service-enabled sensor node: t. In pseudo code, such a rule looks like:



if (t > 45) then p.stopTransportProcess(); Furthermore, the operator instantiates a simple gauge fed with the temperature data (provided by t). For this purpose he uses a manufacturing intelligence software and displays the gauge on a screen situated close the robot. Finally, the sales manager can also leverage the service oriented architecture of this factory. Indeed, the output of the business rule is connected to an ERP system which provides up-to-date information about the execution of the current orders. Whenever the process is stopped because the rule was triggered, an event is sent to the ERP system through its web service interface. The ERP system then updates the orders accordingly and informs the clients of a possible delay in the delivery. 5.2 Components

This section describes the architecture of our prototype from an abstract point of view. Its aim is to understand the functionality whose concrete implementation will be described within the next section. Functional Components The system comprises four main components as shown on Figure 2 that we shall briefly describe: ­ Smart Devices: Manufacturing devices, sensors and Smart Things (i.e. Smart Objects) are the actors forming an Internet of Services in the factory as well as outside of the factory. They all offer web service interfaces, either directly or through the use of gateways or service mediators. Through these interfaces they offer functional services (e.g. start/stop, swap to manual/automatic mode) or status information (e.g. power consumption, mode of operation, usage statistics, etc.). ­ Composed Service: The component aggregates the services offered by smart objects. Indeed, it is in charge of exposing coarse-grained services to the upper layers. In the case of the robotic arm for instance, it will consume the open(), close() and move(...), methods and use them to offer a doTransportProcess (...) service. ­ Business Logic Services and Visualisation Services: In our prototype, the business logic services are supported by a service composition engine and visualized using a visualization toolkit. The former component is used to model business rules or higher-level processes, known as business logic services in our architecture. As an example the operator can use it to create the business rules exposed above. The latter component is used to build a plantfloor visualisation of the devices' status and the overall process execution. As an example the operator can instantiate and use a set of widgets such as gauges and graphs to monitor the status of the machines. The production manager can also use it to obtain real-time graphs of the process execution and status.

­ Enterprise Applications: This is the place of high-end business software such as ERPs or PLMs. The idea at this level is to visualize processes rather than the machines executing the processes. This layer is connected to the plant-floor devices through the other layers. As such it can report machines failures and plant-floor information on the process visualization and workflow. Furthermore, business actions (e.g. inform customers about a possible delay) can be executed based on this information.

Fig. 2. The DPWS service bus.

Cross-Component Communication In a mash-up, the architecture is not layered but rather flat, enabling any functional component to talk to any other. Such architectures need a common denominator in order for the components to be able to invoke services on one another. In our case the common denominator is the enhanced DPWS we developed. Each component is DPWS-enabled and thus, consumes DPWS services and exposes a DPWS interface to invoke the operations it offers. The service invocations can be done either synchronously or asynchronously via the web service eventing system. For instance the temperature is gathered via a subscription to the temperature service (asynchronous) whereas the transport process is stopped by invoking an operation on the process middleware. Figure 2 depicts the architecture by representing the components connected to a common (DPWS) ESB (Enterprise Service Bus). 5.3 Implementation

The system described in this paper is a reference implementation of concepts described in the architecture rather than a stand-alone concept. Therefore it uses and extends several software and hardware components rather than writing them from scratch. In this section we will briefly describe what these components are and how they interact together, taking a bottom up approach.

Functional Components ­ Smart Devices: The wireless sensor network providing temperature information is implemented using the Sun Microsystems' SunSPOT sensor nodes. Since the nodes are not web services enabled, we had to implement a gateway (as described in our architecture), that would capture the temperature readings and provide it via DPWS as services one can subscribe to. The gateway component hides the communication protocol between the SunSPOTs and exposes their functionalities as device level web services (DPWS). More concretely the SunSPOT offer services for sensing the environment (e.g. getTemperature()) or providing output directly on the nodes (e.g. turnLightOn(Color)). The robotic arm was implemented as a clamp offering DPWS services for both monitoring and control. The clamp makes these operations available as DPWS SOAP calls on a PLC (Programmable Logic Controller) over gateway. For monitoring services (e.g. getPowerConsumption()) the calls are issued directly on the gateway standing for the clamp. For control services the idea is slightly different. ­ Composed Service: Typical operations at the clamp level are openClamp() and closeClamp(). In order to consistently use these operations on the topfloor we need to add some business semantics already on the shop floor. This is the role of composed services which aggregate an number of coarse-grained operations (e.g. openClamp()) and turn them into higher level services. This way the start(), openClamp(), closeClamp(), move(x), stop() operations are combined to offer the startTransportProcess() service. ­ Business Logic Services and Vizualisation Services: Services offered by both the sensors and the clamp are combined to create a business rule. The creation of this business logic service is supported by xMII, SAP's Manufacturing Integration and Intelligence software. As mentioned before, the aim of this software is firstly to offer a mean for gathering monitoring data from different device aggregators on the shop floor such as MESs (Manufacturing Execution Systems). This functionality is depicted on Figure 3. Since the SOCRADES infrastructure proposes to DPWS-enable all the devices on the plant-floor, we can enhance the model by directly connecting the devices to xMII. Additionally, xMII offers a business intelligence tool. Using its data visualization services we create a visualization of processrelated and monitoring data. Finally, we use the visual composition tool offered by xMII to create the rule. Whenever this rule is triggered the stopTransportProcess()operation is invoked on the middleware to stop the clamp. ­ Enterprise Applications: Whenever the business rule is triggered, xMII invokes the updateOrderStatus()on the ERP. As mentioned before this latter component displays the failure and its consequences (i.e. a delay in the production) in the orders' list. Additionally, if the alert lasts for a while, it informs the customer by email providing him with information about a probable delay.

Fig. 3. xMII indirect device connectivity.

Fig. 4. Direct connectivity to the DPWS devices.

Cross-Component Communication Figure 5 presents the communication amongst the components whenever the business rule is triggered. At first the SunSPOT dispatches the temperature change by placing a SOAP message on the DPWS service bus. The xMII is subscribed to this event and thus, receives the message and feeds it to its rules engine. Since the reported temperature is above the threshold xMII fires the rule. As a consequence it invokes the stopTransportProcess()operation on the Process Service middleware. This component contacts the clamp and stops it. Furthermore, xMII triggers the updateOrderStatus()operation on the ERP. This latter system update the status of the concerned order accordingly and decides whether to contact the customer to inform him by email about the delay.

Fig. 5. Interactions when the business rule is triggered.

6

System Analysis

In this section we will discuss the properties of our architecture and give decision makers a framework at hand through which they can assess the concrete value of our system for their organisation. Since the work we are presenting in this paper is part of ongoing research, we think it is helpful to have such a framework, in particular to assess future work. In the field of Systems Management several standards exist [ref. Standards, ITIL, etc.] which aim to support a structured dealing with IT systems. One framework in particular helpful for central corporate functions such as produc-

tion is the ISO model FCAPS (Fault, Configuration, Administration, Performance, Security). Although being a framework for network management, it is relevant for our architecture because it is enabling low level networked interaction between Smart Objects. Here we will give a first attempt to evaluate the architecture. ­ Fault Management: Since our system will be part of the manufacturing IT-landscape we need to manage both, faults of particular parts of the manufacturing process and faults in our system. Due to the tight integration these types of faults inherently become the same. In particular the SOA based approach of device integration enables the user to identify faults in his production process, at a level never seen before. It also gives the possibility to build redundancy at system critical stages which ensures fast recovery from local failures. Finally the flexibility given by our SOA approach lets the user decide to what extend he wants to introduce capabilities of quick fault recovery, depending on his individual needs. ­ Configuration Management: Mainly the two components Service Lifecycle Management and Cross-Layer Service Catalogue support dynamic configuration management. However, at the current point of view we see code updated to Smart Devices as a major challenge which until today has not been resolved sufficiently. Configuration also includes the composition of services into higher-level services. In a future version, our Service Discovery module will use semantic annotation of services to find appropriate service instances for online service composition. Using ontologies to specify the behaviour and parameters of web services in their interface descriptions and metadata allows flexible service composition. Especially in the very well defined domain of manufacturing we can make use of existing ontologies that describe production processes. ­ Administrative Management: The Device Manager provides the necessary static and dynamic information about each Smart Device. Through the strict use of web-service interfaces, it will be possible to easily integrate devices into management dash-boards. Through this technically we allow easy and user friendly access to Smart Devices. However, taking the possibly very large number of devices into account, we belief that our middle-ware has deficiencies in offering this user friendly administration. Although this problem is subject to other fields of research such as sensor networks, (e.g, macro programming), we will dedicate our research efforts to the problem. ­ Performance Management: Already now we can say that local components of our system will scale well in regards to total amount of Smart Objects and their level of interaction. This can be justified since all interaction occurs locally and only a limited amount of Smart Objects is needed to fulfil a particular task. However, it is still an open question, if our system will scale well on a global scale and to what extend it will need to be modularized. For example we will need to investigate whether central components such as device and service registries should operate on a plant level or on

a corporate level, which could mean that these parts would have to handle several millions or even billions of devices at the same time. ­ Security Management: As mentioned in the security support section of the architecture, our system can make use of well established security features which already are part of web-service technologies and their protocols such as DPWS. It is most likely that we will have to take into account industry specific security requirements, and it will be interesting to see, if we can deliver a security specification which satisfies all manufacturing setups.

7

Conclusions

In this paper we have presented SOCRADES, a Web Service based Shop Floor Integration Infrastructure. With SOCRADES we are offering an architecture including a middleware which support connecting Smart Devices, i.e. intelligent production machines from manufacturing shop floors, to high-level back-end systems such as an ERP system. Our integration strategy is to use web services as the main connector technology. This approach is motivated by the emerging importance of Enterprise Service Oriented Architectures, which are enabled through web services. Our work has three main contributions: First, we elaborated and structured a set of requirements for the integration problem. Second, we are proposing a concrete architecture containing of components which realized the required functionality of the system. Our third contribution is a reference implementation of the SOCRADES architecture. In this implementation we have demonstrated the full integration of two Smart Devices into and enterprise system. We showed that it is possible to connect Smart Devices to an ERP system, and describe how this is done. Our next steps include integrating a prototype in a bigger setup and testing it with live production systems.

8

Acknowledgments

The authors would like to thank the European Commission and the partners of the European IST FP6 project "Service-Oriented Cross-layer infRAstructure for Distributed smart Embedded devices" (SOCRADES - www.socrades.eu), for their support.

References
1. Instrumentation Systems and Automation Society. http://www.isa.org/. 2. SIMATIC WinCC flexible. http://www.siemens.com/simatic-wincc-flexible/. 3. Web Services Business Process Execution Language Version 2.0 (OASIS Standard), April 2007. http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.html.

4. H. Bohn, A. Bobek, and F. Golatowski. SIRENA - Service Infrastructure for Realtime Embedded Networked Devices: A service oriented framework for different domains. In International Conference on Systems and International Conference on Mobile Communications and Learning Technologies (ICNICONSMCL'06), page 43, Washington, DC, USA, 2006. IEEE Computer Society. 5. E. Fleisch and F. Mattern, editors. Das Internet der Dinge: Ubiquitous Computing und RFID in der Praxis:Visionen, Technologien, Anwendungen, Handlungsanleitungen. Springer, 2005. 6. L. Gaxiola, M. de J. Ram´ irez, G. Jimenez, and A. Molina. Proposal of Holonic Manufacturing Execution Systems Based on Web Service Technologies for Mexican SMEs. In HoloMAS, pages 156­166, 2003. 7. G.Gorbach. Pursuing manufacturing excellence through Real-Time performance management and continuous improvement. ARC Whitepaper, April 2006. 8. F. Jammes, A. Mensch, and H. Smit. Service-Oriented Device Communications using the Devices Profile for Web Services. In MPAC '05: Proceedings of the 3rd international workshop on Middleware for pervasive and ad-hoc computing, pages 1­8, New York, NY, USA, 2005. ACM Press. 9. F. Jammes and H. Smit. Service-oriented paradigms in industrial automation. IEEE Transactions on Industrial Informatics, 1:62­70, 2005. 10. S. Karnouskos, O. Baecker, L. M. S. de Souza, and P. Spiess. Integration of SOAready Networked Embedded Devices in Enterprise Systems via a Cross-Layered Web Service Infrastructure. In 12th IEEE Conference on Emerging Technologies and Factory Automation, 2007. 11. A. Reinhardt. A Machine-To-Machine "Internet Of Things". Business Week, April 2004. 12. U. Saif and D. J. Greaves. Communication Primitives for Ubiquitous Systems or RPC Considered Harmful. In 21st International Conference of Distributed Computing Systems (Workshop on Smart Appliances and Wearable Computing), Los Alamitos, CA, USA, 2001. IEEE Computer Society. 13. C. R. Schoenberger. RFID: The Internet of Things. Forbes, (18), March 2002. 14. E. Zeeb, A. Bobek, H. Bohn, and F. Golatowski. Service-Oriented Architectures for Embedded Systems Using Devices Profile for Web Services. In 21st International Conference on Advanced Information Networking and Applications Workshops., 2007.

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Security Considerations for Distributed Web-Based e-commerce Applications in Java
Timothy E. Lindquist Electronics and Computer Engineering Technology Arizona State University East http://www.east.asu.edu/ctas/ecet Tim@asu.edu Abstract
Today's distributed e-commerce applications typically rely upon various technologies in their realization, including the web, scripting languages, server-side processing and an underlying database. The combination of these technologies creates a system that requires attention to the security issues of each component and the system as a whole. In considering the overall system, issues arise from the interactions of security frameworks available for each component. In this paper, we consider the approach and related issues for distributed e-commerce applications developed with Java. The flexible nature of Java allows migration of objects (compiled code with state) through features such as RMI and Applets. Security for distributed applications developed in Java has issues and lessons applicable to systems of components built on different technologies.
DBMS

legacy appl

view objects/ clients IGURE 1.

business logic/ middle-tier/ server objects

transaction monitor

3-Tier Client-Server Architecture

1. Problem
Web-based e-commerce and distributed applications are changing the way we buy goods, access information and learn. Use of email and other related technology increasingly facilitates collaboration and is more commonly being used for official communications. Official communications via the internet are too often done in insecure mode. Web-based e-commerce applications commonly employ multiple tiers (3-tier client server architecture) and a combination of technologies such as HTML, XML, JavaScript, Java (JSP, Servlets), ASP, dynamic html, CGI, and relational databases, as shown in Figure 1. Each of these technologies have separate and in some cases incompatible approaches to protection against intrusion. For web-based applications, the communication between clients and the middle-tier is via web protocol http. Clients may employ any number of technologies such as applets, html, xml, and scripts. The middle-tier business

logic often employs any of a number of CGI work-arounds such as Netscape's NSAPI, Microsoft's ISAPI, WebObjects, ASPs, Java J2EE, servlets and JSP. The combination of different technologies at each tier, presents special challenges to security of the overall application. Development time and cost pressures often short-change security concerns. Problems range from software design constraints that prevent adequate security to insufficient testing to exercise common attacks. Often performance concerns limit the extent to which assurance can be implemented in a web-based application. Emerging technologies and applications are also presenting new challenges to secure applications. Distributed Object technologies have been maturing for the past several years and are being increasingly utilized in web-based developments. Although some researchers have supported an approach where an object-web replaces the largely datacentered web of today, this has not materialized. Nonetheless, object technologies such as CORBA, DCOM and Java RMI enjoy increased usage in distributed web-based applications. Additional frameworks such as JINI, JavaSpaces, JNDI, and EJB support distributed Java Objects.

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

1

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Distributed objects are network aware objects. Applications running on remote machines may communicate with each other at the object-level using this technology. With a remote reference to a Java RMI object, methods can be requested of the object (messages) in the same manner as if the object were local to the executing application. The SUN implementation of Java (sdk/jdk1.2 and up) includes a flexible and feature rich approach to security providing the basis for distributed Java applications.

private key

message

message

public key

algorithm

algorithm

signature

signature

verify

2. Security Concerns
Sender The platform independence of Java has lead to easy movement of (compiled) code across the internet. While the approaches of OMG CORBA (see: http:// www.omg.org) and Microsoft do provide multi-language solutions, they do not provide the same code migration capabilities as is available with Java. Interacting remote Java objects may easily be written in a manner that requires dynamic movement and execution of code (class files) across the internet. Security concerns include authentication, integrity and encryption/decryption. These may all come into play whenever information (code or data) is moved (across a network or within a single machine). 1. Authenticity allows the receiver of information to know with certainty the identity of the sender. 2. Integrity allows the receiver to know with certainty that information transmitted by the sender has not been modified or tampered with enroute. 3. Encryption is the process of taking data (called clear text) and a short key and producing cipher data that is meaningless to anyone who does not know the key. Decryption is the process of taking cipher data and a short key to produce the corresponding clear text. Each of these basic security concerns come into play with distributed applications, for controlling executing applications as well as access to information. Figure 2 shows how authentication and integrity can be provided using digital signatures. The sender uses his own private key (which must be kept protected utilizing access control) and together with a message to generate a digital signature, which is unique to the message and private key. The message and signature are transferred to the receiver. The receiver must have a public key (usually received separately) corresponding to the sender's private key. The public key can be used to verify a signature, but cannot be used to generate a signature. Upon receipt, the message and signature are verified assuring both authenticity and integrity of the exchanged message.
IGURE 2.

Receiver

3-Tier Client-Server Architecture

3. Digital Signatures
Keys are generated in pairs. The private key is used to generate the signature and is kept confidential to whoever is doing the signing. The public key is used by the receiver to verify authenticity of the message. The signer should distribute the public key to anyone who will receive signed information. The issue as to whether the public key actually corresponds to the sender is resolved with certificates. A certificate represents a chain of trust leading from the sender to the receiver, indicating that the public key belongs to whom you want to believe it belongs. If the sender and receiver both trust the same certificating agency then the chain may be of length one. Each link in the chain is a certifying agency (such as VeriSign or Entrust) which certifies that the entity prior to it in the chain (the owner of the private key or another certifying agency) is who they say they are. Users should understand how certificates are signed and managed. Current web browsers display information about the certificate and who signed it, but few users ever look beyond the lock icon on their web browsers. This provides some opportunity for anyone with a signed certificate to use a man-in-the-middle attack. Simple possession of a certificate says nothing of integrity, quality or functionality of code or other information conveyed by the certificate holder. Another complication of digital signatures is management of a certificate revocation list. Once a key is known to be compromised, there must be some way to inform users that it should no longer be trusted. The SUN implementation of Java comes with a primitive set of tools for manipulating keys, certificates and digital signatures. It also includes the framework classes (in the package java.security) for program creation and verification of digital signatures. Figure 3 includes a sample of Java that may exist for the sender. The example generates a public and private key pair

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

2

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

//generate the private and matching public key KeyPairGenerator keyGen=KeyPairGenerator.getInstance("DSA", "SUN"); SecureRandom random = SecureRandom.getInstance( "SHA1PRNG", "SUN"); keyGen.initialize(1024, random); KeyPair pair = keyGen.generateKeyPair(); PrivateKey priv = pair.getPrivate(); PublicKey pub = pair.getPublic(); //create the signature object Signature dsa = Signature.getInstance("SHA1withDSA", "SUN"); dsa.initSign(priv); //read the datafile; FileInputStream fis = new FileInputStream(args[0]); BufferedInputStream bufin = new BufferedInputStream(fis); byte[] buffer = new byte[1024]; int len; while (bufin.available() != 0) { len = bufin.read(buffer); dsa.update(buffer, 0, len); } bufin.close(); //generate the signature byte[] realSig = dsa.sign(); //save the signed data in a file FileOutputStream sigfos = new FileOutputStream("sigOf"+args[0]); sigfos.write(realSig); sigfos.close();

4. Securing Java Applications
Many aspects of Java's design lend well to distributed applications. One such example is serialization. The ability to externalize objects from one executing Java program (virtual machine) and to read them into another is a process Java calls serialization. Serializable objects may be transmitted through the internet without loss of object properties, including methods. To accomplish object externalization, it is often necessary to move the compiled code along with object data. Several mechanisms exist within Java to do this either implicitly or explicitly under programmer control. Applets and Remote Method Invocation (RMI) are two such mechanisms. Applets are small Java programs communicated from a web-server and executed by a virtual machine running in the browser. RMI, provides the programmer with an object view of internet objects so that method calls, for example can formulated as though the object were in the same virtual machine. RMI capabilities are similar to Microsoft DCOM and the Object Management Group's Common Object Request Broker Architecture (CORBA). Java's platform independence is critical to realizing these dynamic capabilities. Compiled java code (class files) can move to a variety of platforms and be executed without loss of meaning. This powerful capability, which has not been realized to the same level and extent by any other language efforts, was first made generally available by Java implementations. Java's reflection capabilities, allow a program to discover and access the properties available in an object. This allows internet available objects to be manipulated in a manner not necessarily know by the program at the time it was compiled. In addition to facilitating distribution, Serialization, RMI, and Reflection are leading to a view of internet enabled software service objects. These provide the critical infrastructure for e-commerce services, such as financial, investment, and retail purchase. The current e-commerce solutions utilize the web and represent a composition of diverse technologies: 1. User interface through html, xml, Java, JavaScript, Flash and so on, 2. Server functions through dynamic html, JSP, ASP, J2EE, CGI or servlets, 3. Legacy data through RMI, ODBC or JDBC connectivity to a relational database. The challenge is to formulate a secure impenetrable application in light of the combination of a variety of technologies and capabilities. The Java model for securing the operations in an executing virtual machine has progressed significantly since the

FIGURE 3.

Snipet of Java to sign a file

and uses the private key to generate a digital signature for a data file. The signature is saved to file. This code represents simplistically what must be done by the sender. The public key, the signature file and the data file are all transmitted to the receiver, where a similar program verifies the signature using the data and public key. The primary vulnerability of this approach rests in communicating the public key. An attack may replace the data, signature file and public key if they are all three transmitted together. Certificates are the most common mechanism used to assure the public key authentically identifies the sender. Good practice dictates that the public key be transmitted separately in an assureable manner. The public key (certificate) is stored by the receiver for later use to authenticate multiple subsequent transmissions. This mechanism can be used to verify the authenticity and integrity of either data or program code that is transferred in distributed e-commerce applications. The approach verifies that information came from the purported sender and was not modified in transmission. Encryption is necessary to protect information from reading by others during transmission, as discussed below. Security within an executing Java application is based upon authenticity and integrity using digital signatures.

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

3

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

initial introduction of the sandbox model for Java applications. Initial versions of Java provided full trust to classes loaded locally and prohibited all sensitive operations from any code obtained dynamically. Java now supports a continuum of access control. Access to system resources (such as files, sockets, runtime, properties, security permissions, serializable, reflection, and window toolkit) is granted based on domains. A domain includes a set of permissions together with a codebase and an indication of who signed the code. The codebase indicates the file or URL from which the code is loaded. If signed, the alias of the public key can also be used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which defines the access it has to resources.

{

permission java.net.SocketPermission "*.GSE.com:2575-", "accept, connect, listen, resolve";

};

A policy may consist of one or more grants each defining different domains. Each domain may have one or more associated permissions. When a protected operation is attempted, the virtual machine's security manager performs a security check. It looks at the classes of all methods currently on the runtime call stack. Each associated protection domain is queried to determine whether the operation is allowed. An operation is performed only if all methods on the runtime stack have the appropriate permission. Signing executable is an important application of authentication and integrity technology. As the number of distributed applications grows and those applications increasingly rely upon migration of code, we need assurance that we are granting permissions to trustworthy code. Today, code signing is largely platform dependent. For example, applets executed with the Netscape or Internet Explorer Java virtual machines require use of Netscape or Microsoft tools to sign the code. Applets designed to run with the SUN plug-in virtual machine must be signed with the SUN tools. This lack of consistency only accentuates the problems arising from utilizing multiple technologies to realize an e-commerce application.

5. Cryptography
FIGURE 4. Controlling Access to Java Resources

Figure 4, is taken from the On-line Java Tutorial, http://java.sun.com/docs/books/tutorial/ and shows the interaction between the security domains defined in Java2 and the original sandbox model. In the Java 2 SDK version 1.4, the standard platform has been further augmented to integrate the Java authentication and authorization service (JAAS). Doing so takes a step closer to integrating user login services with authentication mechanisms. See: http://java.sun.com/products/jaas/ In Java 2, security domains are defined by a policy granting permissions to the domain. For example, suppose the company GrowthStocksExpress publishes an applet on their hypothetical web site at the URL: http://GSE.com/applets Assuming the applet needs connections to one or more hosts having a domain address ending with GSE.com on ports beginning at 2575, a policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase "http://GSE.com/applets"

The SUN implementation of the Java Cryptography Architecture (Java Crypto Extensions) is freely available for developing applications that rely on encryption. Similarly, if the application requires an encrypting web server, Apache-SSL is one of the freely available web servers based on OpenSSL. It can be freely obtained and used commercially. See: http://www.apache-ssl.org/ In addition to authentication and integrity, distributed ecommerce applications require cryptographic services. Authentication and integrity assure the identity of the sender and that information was not changed in transmission, but they do not protect against reading during transmission. Encryption is a concern for financial transactions or other communication where personal identification information must be transmitted. To guard against this type of intrusion, many encryption / decryption algorithms and implementations exist. Encryption is the process of taking clear text and converting it into cypher data that is unreadable to anyone who does not know the key. Decryption reconstructs clear text from the cypher data, using the key. The sender performs encryption before transmission and the receiver decrypts to

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

4

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

reconstruct the information. Several implementations of various strength exist. Primary issues relate to strength and performance - time and space to encrypt and decrypt. Figure 5 is taken from the Java Tutorial and shows the service provider architecture that is used in the security frameworks provided by SUN with Java. The API (application program interface) provides a common interface for developing e-commerce or other distributed applications. To the extent possible, various alternative approaches to security are cast into a single interface. Engine classes abstractly define cryptographic services. Providers (implementing security services) write to the lower level SPI (service providers interface). For an example implementation see JCSI [5]. SUN also provides a default implementation which is distributed with the downloadable extensions. Where multiple implementations exist, initialization methods select the appropriate implementation based on parameters. This same approach is used, for example in Java's database connectivity, JDBC. Where multiple drivers exist, selection is wired-into the API through initialization methods. Although this architecture is a powerful approach that adds considerable value to the Java framework, in practice it is very difficult to achieve a single common interface that works equally well for all implementations.

Authenticity and integrity are just that and no more. Signed Java can be relied upon regarding who signed it and that it has not been disturbed in transmission. The fact that a digital signature has been verified tells the user nothing about the goodness of the code or the security of the system that is delivered in signed form. These are elements of trust in the individual or company that signed the code. To further the problem, security problems do and will continue to result from problems in the infrastructure upon which the Java implementation is built. For example, denial of service attacks, file access, host system intrusion and underlying problems with TCP/IP all arise to the applications built on these technologies. Nevertheless, e-commerce applications must be secure and the best way to build in security is to use best software practices and processes for their development. Specification and design of a secure distributed Java application should include security risks, requirements and underlying constraints. Development should proceed with a security risk assessment, followed by design and reviews from risk perspective. Security testing, which is necessarily different from specification testing, should consider likely avenues of problems and exercise documented successful attacks on similar systems. For further reading on security problems with Java and related technology, see: http://www.cigital.com/javasecurity/articles-1.html http://www.w3.org/pub/Conferences/WWW4/Papers/ 197/40.html and the Java security website: http://www.rstcorp.com/java-security.html For further reading in Security and Encryption, see Peter Guttmann's web site [4], which contains references to various research publications as well as software and other internet resources related to security and encryption.

7. References
IGURE 5. Service Provider Architecture

6. Closing Remarks
For a language that has developed and whose use has spread so rapidly, Java's features are remarkably complete and consistent. Nevertheless, security in Java applications is a difficult task. Java security mechanisms are complex and as such are likely to be inappropriately used by developers. The Java security model, together with the Java cryptography architecture are powerful tools that are integrated well into the language both in terms of controlling applications and in terms of defining security frameworks that are amenable to realization by multiple implementations.

[1.] McGraw, Gary and Felton, Ed; Securing Java, John Wiley and Sons Inc., 1999, see: http://www.securingjava.com/ [2.] Griscom, Daniel; Code Signing for Java Applets; see: http://www.suitable.com/Doc_CodeSigning.shtml [3.] Campione, Mary, et al., The Java Tutorial, SUN, Addison Wesley, December 2000, http://java.sun.com/docs/books/tutorial/ [4.] Guttmann, Peter; Security and Encryption-Related Resources and Links, http://www.cs.auckland.ac.nz/~pgut001/links.html [5.] Sun Microsystems Java Security and Crypto Implementation, http://www.cs.wustl.edu/~luther/Classes/Cs502/ WHITE-PAPERS/jcsi.html

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35'02) 0-7695-1435-9/02 $17.00 © 2002 IEEE

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

5

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION OF TIME SERIES*
´ , AND ANDERS LINDQUIST JORGE MARI, ANDERS DAHLEN

Abstract. In this paper we consider a three-step procedure for identification of time series, based on covariance extension and model reduction, and we present a complete analysis of its statistical convergence properties. A partial covariance sequence is estimated from statistical data. Then a high-order maximum-entropy model is determined, which is finally approximated by a lower-order model by stochastically balanced model reduction. Such procedures have been studied before, in various combinations, but an overall convergence analysis comprising all three steps has been lacking. Supposing the data is generated from a true finitedimensional system which is minimum phase, it is shown that the transfer function of the estimated system tends in H to the true transfer function as the data length tends to infinity, if the covariance extension and the model reduction is done properly. The proposed identification procedure, and some variations of it, are evaluated by simulations.

1. Introduction In recent years there has been quite some interest in a certain type of procedures for identification of time series known as subspace methods [1, 42, 41, 28, 29]. These identification procedures are based on geometric projection methods, and they could be understood in the context of splitting geometry and partial stochastic realization theory [30, 31]. However, as pointed out in [32] and further elaborated upon in [9], these procedures are algebraically equivalent to minimal factorization of a Hankel matrix of covariance estimates, and they make no distinction between stochastic and deterministic partial realizations. Therefore they may fail because of loss of positive realness in the spectral estimation phase. In an attempt to overcome these problems we analyze an alternative approach to time series identification proposed in [32], namely a three-step procedure consisting of estimation of a partial covariance sequence, covariance extension by the maximum-entropy method, leading to a high order autoregressive (AR) process, and finally stochastically balanced truncation. This method shares certain features with stochastic subspace identification methods, the most obvious one being that it is based on partial stochastic realization theory, but, unlike stochastic subspace methods, it guarantees positive realness. Moreover, our procedure only involves linear
 This research was supported by a grant from the Swedish Research Council for Engineering Sciences (TFR).  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden 1

2

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

algebra operations, and no iterations or optimization of nonconvex functions, as for maximum-likelihood (ML) methods, are needed. The idea of approximating an autoregressive moving-average (ARMA) process by an AR process is by no means new. Its origins can be traced back to the Wold decomposition [55] where L2 -convergence of high-order AR models to general analytic models is shown. Pioneers in the use of this concept for systems identification are Durbin [12, 13] and Whittle [54]. The convergence properties of such approximations were studied by Berk [2] and later refined in [36, 34, 33, 7]. The interesting paper [7] contains nice proofs of some of the convergence results needed in this paper, but, for the sake of completeness and insight, we provide new proofs based on some properties of fast filtering algorithms [5] and simple methods of complex analysis and Szeg o polynomials. The power of the theory of Szeg o polynomials and Toeplitz matrices in analyzing stochastic processes is reported in [24], but, except for elementary theory, it has not been much used in systems identification [39]. This is even more true for the newer results [16, 40, 37, 27] on orthogonal polynomials. The idea of using model reduction for systems identification appears in the thesis by Wahlberg [50] and the subsequent paper [51], where the emphasis is on frequency weighted reduction. Instead, we use stochastically balanced truncation, for which we develop a simple computational procedure, exploiting the special structure of the AR model. We also show the advantage of this reduction procedure by theoretical analysis and simulations. In fact, a comprehensive study comprising all the steps mentioned above together with a qualitative and quantitative analysis of the entire identification strategy has been lacking, and that is what we offer in this paper. The paper is outlined as follows. In Section 2 we formulate the problem and motivate our measure of approximation. Each of the three steps in the overall identification procedure contributes to the estimation error. In Section 3 we show that the transfer function of the maximum-entropy filter, constructed from true covariances, tends to that of the true filter in H norm at a geometric rate determined by the largest modulus of the zeros of the true filter as the order of the maximum-entropy filter becomes large. However the order of the approximation is too high, and therefore model reduction is performed. This is studied in Section 4. A stochastic balancing procedure, based only on linear-algebra operations so that no Riccati equations need to be solved, is provided together with the analysis of the model-reduction error. Both deterministically and stochastically balanced truncation lead to good results. However, when the covariances are estimated from statistical data, stochastic model reduction is found to be superior. In particular, variances are considerably closer to the Cram´ er-Rao bounds. In Section 5 we state our statistical convergence theorems, proving that the total error tends to zero as the length of the data string tends to infinity, provided the degree of the AR model tends to infinity in the proper manner. In Section 6 some simulations are presented. For comparison, a simulation using stochastic subspace identification [43] is included. For clarity of exposition, all the proofs have been deferred to two appendices, Appendix A dealing with the asymptotic properties of the maximum-entropy filter, and Appendix B devoted to the statistical error analysis. Finally, in Section 7 conclusions and open questions are discussed.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

3

2. A covariance extension strategy for time series identification Time series identification in the form studied here amounts to estimating the matrices (A, B, C, D) in some n-dimensional linear stochastic system x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) driven by normalized white noise {w(t)}, from a data string of observations {y0 , y1 , y2 , . . . , yN } (2.2) of the output process {y (t)}, which here will be taken to be scalar. The basic idea behind our approach is very simple: given estimates of a partial sequence c 0 , c 1 , c 2 , . . . , c (2.3) of the covariances ck = E{y (t + k )y (t)}, which satisfies the condition that the Toeplitz matrix   c2 · · · c c0 c1  c1 c0 c 1 · · · c  -1     c c c 0 · · · c  -2  1 T +1 :=  2 (2.4)  . . . ... .  . . . . . . . . c  c  -1 c  -2 · · · c0 is positive definite, first construct a high-order model continuing (2.3) by covariance extension. This model has all the required positivity properties, but the order is too high. Then reduce the order by means of a positivity-preserving model reduction procedure to be specified below. That this simple recipe will in fact provide a good identification method is by no means a trivial matter but is based on some rather deep results, which will be presented here. More specifically, the approach consists of three steps, for which there are several possible variants that will be discussed below. The rigorous mathematical analysis, however, will be carried out for the following procedure, for which we shall give theoretical bounds. (i) Estimate a partial covariance sequence ^1 , c ^2 , . . . , c ^ c ^0 , c from the time-series data (2.2) via the ergodic estimate 1 c ^k = N +1
N -k

(2.1)

(2.5)

yt+k yt
t=0

k = 0, 1, . . . , .

(2.6)

(ii) Use the maximum entropy extension to construct an AR model with transfer function  ^  (z ) = z , (2.7) W ^ (z )  ^ (z ) is the normalized Szeg¨ where  o polynomial of degree  , to be introduced in Section 3, computed from the estimated covariance data (2.5).

4

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

^ (z ) of W ^  (z ) via a stochastic (iii) Determine a reduced-degree approximation W model reduction procedure [11] to be described in more detail in Section 4. In this procedure, the idea is that  >> n, the order of the system to be identified, ^ equals the degree n of the true system (2.1). However, the and ideally n ^ := deg W method will produce a valid model even if this is not the case or even if there is no "true" underlying model. This is in contrast to stochastic subspace identification models, which may fail to produce any model at all [9]. There are possibilities for variations of the procedure described above. In Step (i) we could use alternative covariance estimates or Burg's estimation of Schur parameters ^ +1 of (2.5) is [3], the only requirements being that the estimated Toeplitz matrix T positive definite and that c ^k  ck a.s. as N  . In Step (ii) we could instead use approximate covariance extension or covariance extension with prescribed zeros, for which there is now a complete parameterization [5] and an algorithm [4]. (In the latter case a zero estimator is needed; see, e.g., [15, 35].) In Step (iii) other model reduction methods could be used. For example, an important model reduction paradigm is the one based on optimal Hankel norm approximation [21]. Before proceeding along these lines we need to decide what measure of approximation to use. Suppose that there is a true underlying system (2.1) with a stable transfer function W (z ) = C (zI - A)-1 B + D, (2.8)

L

of McMillan degree n. We also assume that W (z ) is minimum-phase so that both zeros and poles are located in the open unit disc. Then, we need to be able to measure ^ (z ), converges to the true one as how the estimated model, with transfer function W ^ (z ) in N  . In this paper we have chosen to use distance between W (z ) and W  norm as a measure of proximity between the true and estimated model. From an engineering point of view this could be called worst case identification. The modern literature in robust control makes extensive use of the worst case philosophy; see for example [20, 52]. There are also other reasons for using the  , as discussed in [35]. Returning, then, to the identification approach outlined above, the estimation error can be decomposed into three parts, one corresponding to each of the steps (i), (ii) and (iii). Hence we have the error bound

L

^ W -W



 W - W



^ + W - W



^ - W ^ + W

,

(2.9)

^  is where W is the AR model corresponding to the true covariances (2.3) and W the one determined from the estimated covariances (2.6). To prove convergence to zero of the estimation error (2.9), we shall need to assume that W is minimum-phase, ^ should have the same property, which moreover is desirable in many and hence W applications. Our procedure insures this. Estimating the first term in (2.9) is a problem in stochastic partial realization theory and function theory and will be dealt with in the next section. The third term concerns model reduction which will be studied, in the particular setting required here, in Sections 4 and 5. In Section 5, finally, we consider the second term together with the overall statistical analysis.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

5

3. Rational modeling from a long covariance sequence Step (ii) in the identification procedure outlined in Section 2 is based on rational covariance extension. To understand this, let us consider the covariance extension problem from a more general point of view. Given a partial covariance sequence c 0 , c 1 , c 2 , . . . , c , (3.1)

covariance extension amounts to finding an infinite extension c +1 , c +2 , c +3 , . . . of this sequence such that the function is strictly positive real, i.e., it is an analytic function in the complement Dc of the open unit disc D, which maps Dc to the open right complex half-plane. Then (z ) := V (z ) + V (z -1 ) is a spectral density for a process having c0 , c1 , . . . , c as its first  covariances and which is coercive in the sense that (ei ) > 0 for all . Spectral factorization is then to find a stable transfer function W (z ) such that |W (ei )|2 = (ei ). In particular, we are interested in finding covariance extensions for which V (z ), and hence W (z ), have at most degree  . For the moment disregarding this degree condition and allowing it to be meromorphic, there is a complete parameterization of all covariance extensions, which is classical and due to Schur [45]: modulo a normalization of c0 , there is a one-one correspondence between infinite covariance sequences c0 , c1 , c2 , c3 , . . . and a sequence of Schur parameters, or reflection coefficients, 0 , 1 , 2 , 3 , . . . , (3.3) with the property |t | < 1 for all t. In fact, fixing the value of c0 to one, there is a oneone correspondence between the partial sequences 1, c1 , . . . , cm and 0 , 1 , . . . , m-1 for each m. The Schur parameters can be determined from the covariances via the Szeg¨ o polynomials t (z ) = z t + t1 z t-1 + · · · + tt t = 0, 1, 2 . . . , computed by means of the Szeg¨ o-Levinson recursion z -t t+1 (z ) = ( z ) - z 1  t t+1 where t (z ) ;  t (z ) 0 (z ) 1 = , ( z ) 1  0 (3.4) (3.2) c + c 1 z -1 + c 2 z -2 + . . . V (z ) := 1 2 0

t -1  t (z ) := z t (z ) is the reciprocal polynomial of t (z ), and the Schur parameters are computed via

= r1t t t j =0 t,t-j cj +1 rt+1 = rt (1 - |t |2 ), r0 = c0 .

(3.5)

6

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Hence t = -t+1 (0), a fact that we shall use below. In the problem to find a covariance extension for (3.1), therefore, 0 , 1 , . . . ,  -1 are fixed and the infinite continuation  ,  +1 , . . . can be chosen freely. In particular, if we take t = 0 for t = ,  + 1,  + 2, . . . . We obtain the maximum entropy solution W (z ) = z ,  (z ) (3.6)

where  (z ) is the normalized Szeg¨ o polynomial 1  (z ) :=   (z ). r (3.7)

Thus, in this particular case, the solution to the covariance extension problem turns out to be rational of degree at most  as required. In general, the Schur parameterization is not suitable for characterizing such rationality, but other parameterizations are needed. In fact, it has recently been shown [5] that there is exactly one such solution for each choice of zeros of W (z ), thus proving a long-standing conjecture by Georgiou [18], who had established existence. Nevertheless, as we shall see next, rationality implies that the Schur parameters tend geometrically to zero, provided W (z ) has no zeros on the unit circle. In this section we shall demonstrate that the rational transfer function (2.8) can be approximated arbitrarily closely in L by the transfer function W (z ) of a maximum entropy filter for sufficiently large  and that this  depends on the maximum modulus of the zeros of W (z ). We shall first present a heuristic argument in support of this conclusion. To this end, let (3.2) be the infinite covariance sequence of the output process y in (2.1), and let (3.3) be the corresponding sequence of Schur parameters determined via the Szeg¨ o-Levinson algorithm presented above. Then we have the following special case of Corollary 2.1 in [5]. Lemma 3.1. Let the spectral density (ei ) = |W (ei )|2 (3.8)

be coercive in the sense that it is positive for all  and let (3.3) be the corresponding infinite sequence of Schur parameters. Moreover, let   (0, 1) be greater than the maximum of the moduli of the zeros of W (z ). Then |t | = O( t ), i.e., |t |  M  t for some M  R and for sufficiently large t. Remark 3.2. Since (3.9) holds for all  greater than the the maximum of the moduli of the zeros of W (z ), we have in fact that |t | = o( t ), i.e., limt |t | -t = 0. For ease of reference, we shall henceforth refer to this property as geometric convergence rate of the Schur parameters. The proof of Lemma 3.1 is based on the analysis of certain fast algorithms for Kalman filtering [6]. (3.9)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

7

Remark 3.3. Coercivity is essential for the validity of Lemma 3.1. For example, the spectral density z (z - 1)2 (z ) = - 2 (z + z + 2)(2z 2 + z + 1) is rational but not coercive, since it has a double zero at z = 1. Its Schur parameters are seen to be -1/2, -2/3, -2/5, -2/7, -2/9, -2/11, . . . , which tend to zero but not geometrically. On the other hand, there are coercive, analytic but nonrational models which also exhibit geometric convergence rate. A classical example [23] is obtained 2 when ck = k for some   (-1, 1). The Schur parameters in this case form an exact geometric sequence, k = (-)k+1 , k  0. Lemma 3.1 implies that, for a sufficiently large  which depends on  , the Schur parameters t are close to zero for t = ,  + 1,  + 2, . . . . But, the Schur parameters of W are exactly zero for t = ,  + 1,  + 2, . . . , and hence geometric convergence would insure that W is a good approximation of W (z ) for sufficiently large  . We shall prove that this is indeed the case. Theorem 3.4. Suppose W (z ) is the minimum-phase spectral factor of a coercive spectral density (3.8), and let   (0, 1) be greater than the maximum of the moduli of the zeros of W (z ). Then
 

lim W - W



= 0,

(3.10)

and the convergence is geometric. More precisely, there is a constant M such that W - W


 M  .

(3.11)

The proof of Theorem 3.4, which is given in Appendix A, amounts to first showing that
  -1 - W -1 lim W 

= 0.

(3.12)

A very nice result of this type has already been given in [7]. In Appendix A we give an alternative proof of this fact based on Szeg¨ o theory, and also show that the convergence is geometric. In fact, we can choose  arbitrarily close to the maximum modulus of the zeros of W . However, as we shall see next, we can actually prove more. To this end, let us first -1 and W -1 have their poles in the open unit disc D and thus observe that, since W are bounded and analytic in the complement Dc of D, they belong to the Hardy space  of functions which are analytic and bounded in {z  C | |z | > 1}. Hence the H-  , and convergence (3.12) is in H- z -  (z )  W -1 (z ) (3.13) uniformly in each compact subset of Dc . Now, W -1 is analytic in {z  C | |z |   }, a region that is strictly larger than Dc . This in itself of course does not insure that the convergence (3.13) extends to this larger region. In fact, even if z -  (z ) did converge in {z  C |   |z |  1}, it could fail to converge to W -1 (z ) there. The fact that it really does converge uniformly to this limit is another consequence of Lemma 3.1. We state this as a separate result, to be proven in Appendix A. A method for

8

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

computing the maximum of the modulus of the zeros of W from statistical data, and hence an estimate of the convergence rate  , is given in [35]. Theorem 3.5. Suppose W (z ) is a minimum-phase rational function having all its poles in the open unit disc D and all its zeros in

D := {z  C | |z |  }  D

where 0 <  < 1,

and let { (z )} o polynomial (3.7) determined from the co0 be the normalized Szeg¨ variances in the spectral density


|W (e )| = c0 + 2
i 2 k=1

ck cos k.

Then, as   , z -  (z )  W -1 (z ) uniformly in every compact subset of {z  C | |z | > }, the complement of D .

Dc  :=

Lemma 3.1 and Theorem 3.5 give us some interesting information about the asymptotic distribution of the roots of  (z ) and hence of the poles of the high-order AR model with transfer function W (z ). It is known that, if the Toeplitz matrix T +1 is positive definite, all roots of  (z ) are located in the open unit disc D, but little has been reported in the literature on their behavior as   . This behavior is illustrated in Figure 3.1.
Original system 1 1 Original system

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0

0.5

1

-1 -1

-0.5

0

0.5

1

Original and AR(24) 1 1

Original and AR(24)

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0

0.5

1

-1 -1

-0.5

0

0.5

1

Figure 3.1: Distribution of zeros of  (z ).

The top two diagrams show the zero-pole positions, within the boundaries of the unit circle, of two minimum phase spectral factors W , both of degree five. Also indicated is a circle of radius equal to the maximum modulus of the zeros of these spectral factors. The little circles "" represent zeros and the "+" sign represent poles. The lower two figures show the position of zeros and poles of the original

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

9

system superposed with those obtained from an AR(24) model constructed from the exact covariance sequence. The poles of the latter models are indicated with "×". The left part of Figure 3.1 illustrates what may happen if all the poles of W (z ) are located in {z  C | |z | < }, where  is chosen to be the maximum of the moduli of the zeros of W (z ). The roots of  (z ) tend to cluster inside a circle of radius  as   . This phenomenon is in a sense predictable, since the constant term of the Szeg¨ o polynomials is n+1 (0) = -n , which equals the product of the roots and, by Lemma 3.1, decays at a geometric rate, which can be chosen arbitrarily close to . This does not preclude that other types of crowns may occur, because subsequences of {n } could decay faster than the overall rate  , as follows from [5]. Very general statements about the distribution of zeros of orthogonal polynomials, derived with the help of potential-theoretic methods, can be found in [37, 27]. To the right in Figure 3.1 we see what happens in the case that W has poles with moduli larger than . Then, for  sufficiently large, the normalized Szeg¨ o polynomial  (z ) has roots in {z  C |   |z | < 1}, but exactly as many as the poles of W in this region and approximately at the same place as these. This is of course due to the uniform convergence of z -  (z ) to W -1 (z ) in every compact subset of Dc  . The other roots of  (z ) behave exactly as in the previous case and tend to accumulate in a crown inside and very close to the circle {z  C | |z | = }.  approximation W of W which can be made We have thus constructed an H- arbitrarily good by choosing  sufficiently large. However, W will have much larger degree and, except for the poles outside the circle {z  C | |z | = }, a completely different zero-pole pattern. We shall rectify this situation by model reduction. In fact, for the moment considering the perfect modeling problem to identify the rational transfer function (2.8) given an exact partial covariance sequence (3.1), the last step in our procedure consists in approximating W by a rational function Wred of smaller degree, ideally of the same degree as W . The simplest model reduction procedure is deterministically balanced truncation (DBT), first introduced by Moore [38]. Though easy to implement, it may fail to yield a minimum-phase approximation, a requirement which is important in certain contexts. For this and, more importantly, for statistical reasons to be reported in Section 5, we prefer another model reduction procedure, namely stochastically balanced truncation (SBT), first introduced by Desai and Pal [10], which is based on a different balancing strategy to be explained in detail in Section 4.
Original system 1 0.5 0 -0.5 -1 -1 1 0.5 0 -0.5 -1 -1 Reduction by DBT 1 0.5 0 -0.5 -1 -1 Reduction by SBT

0

1

0

1

0

1

Figure 3.2: Zero-pole pattern of W (z ) and Wred (z ) for different model reduction methods.

Let us now return to the example depicted to the right in Figure 3.1. This fifth-order

10

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

model has first been approximated by W of degree  = 24, producing the pole-zero pattern in the lower right corner of Figure 3.1. Figure 3.2 illustrates what happens when the model is reduced back to order five by either deterministically balanced truncation or stochastically balanced truncation. The zeros are denoted by "" and the poles by "+". Both reduction procedures give good approximations when applied to exact covariance data. However, as we shall see in Section 5, the advantages of SBT becomes apparent when applied to statistical data. Also, as explained in Remark 4.5, there are theoretical reasons to prefer stochastic model reduction. 4. Model reduction In the present setting, model reduction amounts to replacing a stochastic system (2.1) of dimension  by one of some dimension r <  in such a way that most of its statistical features are retained. In particular, we want to remove the part of the system which corresponds to the weakest correlation between past and future. This idea can be formalized in the following way. Basic concepts. In the Hilbert space generated by the random variables {y (t) | - < t < } in the inner product u, v = E{uv }, let H - be the subspace generated by the past, i.e., {y (t) | t < 0}, and H + that generated by the future {y (t) | t  0}. Consider the Hankel operator H : H +  H - and its adjoint H : H -  H + defined as

H = EH
-

-

|H +

and

H = E H

+

|H - ,

(4.1)

where E H denotes orthogonal projection onto the past space H - . More precisely, H sends   H + to E H -   H - and H sends   H - to E H +   H +. Since the process y is the output of a minimal stochastic system of dimension  , rank H =  by Kronecker's Theorem [56], and hence H has exactly  singular values, 1 , 2 , . . . ,  , which are positive, as usually listed so that 1  2  · · ·   . These singular values are the canonical correlation coefficients and hence the cosines of the angles between the principal directions of the past space H - and the future space H + . They are therefore less than one, and the part of the stochastic system corresponding to singular values which are close to zero have a weak coupling between past and future, i.e., the corresponding subspaces are almost orthogonal. The basic idea of stochastic model reduction is to truncate the system so that this part is removed. To each singular value k there is an associated Schmidt pair (k , k ) with k  H + and k  H - such that

Hk = k k ,

Hk = k k ,

and such that the sequences 1 , 2 , 3 , . . . and 1 , 2 , 3 , . . . of singular vectors are orthonormal. The singular vectors corresponding to nonzero singular values span the predictor spaces X- := span{1 , 2 , . . . ,  }, X+ := span{1 , 2 , . . . ,  }. Clearly, X-  H - and X+  H + . The process y has one representation (2.1) for each minimal spectral factor W , having W as its transfer function. Such representations are called minimal stochastic realizations and the corresponding subspaces X := {a x(0) | a  R } are called

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

11

splitting subspaces [30, 31]. In particular, X- is the splitting subspace of the stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) (4.2)

with the transfer function W- (z ), the minimum-phase spectral factor; and X+ is the splitting subspace of x+ (t + 1) = Ax+ (t) + B+ w+ (t) y (t) = Cx+ (t) + D+ w+ (t) (4.3)

with transfer function W+ (z ), the maximum-phase spectral factor, having all its zeros in Dc . Note that A and C are the same in both realizations (uniform choice of bases). Each realization has a counterpart which evolves backwards in time and has the same splitting subspace. For example, the backward realization of X+ , ¯+ w ¯+ (t) + B ¯+ (t) x ¯+ (t - 1) = A x , ¯ ¯ ¯+ (t) y (t) = Cx ¯+ (t) + D+ w (4.4)

¯ + (z ), the coanalytic minimum-phase spectral factor, having all has transfer function W ¯ + (z ) = W- (z -1 ). its poles and zeros in Dc . In the present case with scalar y , we have W Now, in order to identify the part of the system which has the weakest coupling between past and future, and hence will be removed in the model reduction, we need to balance the system in the sense of Desai and Pal, as we shall explain next. To this end, we make a coordinate transformation ¯ ), ¯ )  (SAS -1 , CS -1 , CS (A, C, C in the minimal realization of ¯ + 1 c0 , (4.6) V (z ) = C (zI - A)-1 C 2 the strictly positive real part of the spectral density of y , so that the state covariances ¯+ = E{x ¯+ (t)¯ x+ (t) } coincide with the diagonal  ×  P- := E{x- (t)x- (t) } and P matrix  of nonzero canonical correlation coefficients, i.e., ¯+ =  := diag(1 , 2 , . . . ,  ). P- = P
1

(4.5)

(4.7)

This is done by choosing S so that Sx- (0) =  2  , where  = (1 , 2 , . . . ,  ) , and 1 (S )-1 x ¯+ (0) =  2  , where  := (1 , 2 , . . . ,  ) . To compute the canonical correlation coefficients, we first observe that the eigen¯+ are precisely the squares of the canonical correlation values of the product P- P coefficients, i.e.,
-1 2 2 2 ¯+ ) = (P- P+ ) = {1 , 2 , . . . ,  }, (P- P

(4.8)

-1 ¯+ where we have used the fact that the state covariance of (4.3) is P+ = P . Therefore the canonical correlation coefficients can then be determined via (4.8) by solving the Lyapunov equations

P- = AP- A + B- B-

and P+ = AP+ A + B+ B+ .

(4.9)

12

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

The point is now to identify the canonical correlation coefficients 1 , 2 , . . . , r corresponding to the part of the system one wants to keep. The part corresponding to r+1 , r+2 , . . . ,  will be disposed of. This amounts to partitioning  as = 1 2 , (4.10)

where 1 is r × r. In order to reduce model (2.1) we make the coordinate transformation (A, B, C )  (SAS -1 , SB, CS -1 ), with the same balancing transformation S . Then, partition the new triplet (A, B, C ) conformally with (4.10) as A= A11 A12 , A21 A22 B= B1 , B2 C = C1 C2 , (4.11)

and perform a principal subsystem truncation to obtain the transfer function of a reduced-order system Wred (z ) = C1 (zI - A11 )-1 B1 + D (4.12) of degree r. If 2 is close to zero, while 1 is not, the rank of H is close to r, and the discarded part of the system gives a negligible contribution to y . Stochastically balanced truncation of AR models. We now consider the problem of stochastically balanced truncation of the maximum entropy filter   r z (4.13) W- (z ) := W (z ) =  (z ) of order  , which, for the moment we denote W- (z ) to emphasize its character as the minimum-phase spectral factor of the spectral density r .  (z ) (z -1 ) Remark 4.1. Without loss of generality we assume that  (0) = 0 so that no cancellations occur; otherwise, we may choose a smaller  for which this condition holds. In fact,  (0) =  -1 , and if  -p =  -p+1 = · · · =  -1 = 0 and  -p-1 = 0 for some p = 1, 2, . . . ,  , then  (z ) = z  -p  -p (z ) by (3.4), and hence (3.6) can be replaced by W (z ) = W -p (z ), and for W -p (z ) the required condition holds. The maximum-phase spectral factor W+ (z ) has all its zeros at infinity, and hence  r -1 W+ (z ) = h (zI - F ) b = , (4.14)  (z ) where (F, b, g ) is the (observable) canonical form     0 0 1 ··· 0 . . ...  .  . . . . . .  .  . , b =  .  F =  0 ,  0 0 ··· 1   - -, -1 · · · - 1 r   1 0 , h= . . . 0

(4.15)

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

13

 1 ,  2 , . . . ,  being the coefficients of the Szeg¨ o polynomial  (z ). In this basis, it follows from (4.9) that [P+ ]jk = 1 2
 - 

(ei I - A)-1 bb (e-i I - A )-1 d
jk

1 = 2
   .  .  . =   2    1 1

r e-(j -k)i d = cj -k ,  (ei ) (e-i ) -
···  1        r -1   and R =    r -2 .. . r0    , (4.16) 

and hence P+ = T . It is well-known and easy to prove that  T  = R , where
 -1, -1 . . .  -1,1 1  -2, -2 . . . 1

 +1

and consequently
-1 -1 ¯+ = T P =  R  .

(4.17)

It remains to determine P- . From (4.13) is easy to see that  W- (z ) = - (zI - F )-1 b + r , where  :=  , -1 · · ·  1 ,

(4.18)

(4.19)

but, in order to determine P- , this realization needs to be transformed so that the A and C matrices are the same as in (4.14) (uniform choice of bases). More precisely, we need to perform a transformation (F, b, - )  (QF Q-1 , Qb, - Q-1 ) =: (F, Qb, h ). Then P- is the solution of the Lyapunov equation P- = F P- F + Qbb Q , and therefore, since T = F T F + bb and QF = F Q and consequently QT Q = F QT Q F + Qbb Q , we have P- = QT Q . To determine Q, notice that - = h Q and QF    h -  - F   h F = .  .   .  . . . - F  -1 Next, define the symmetric matrix
- 1/2 - 1/ 2  QT Q  R . M := R

(4.20) = F Q to form    Q = Q.  (4.21)

h F  -1

(4.22)

14

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

¯+ ), and hence, by (4.8), M In view of (4.20) and (4.17), det(zI - M ) = det(zI - P- P 2 2 2 has the eigenvalues 1 , 2 , . . . ,  , and the singular-value decomposition M = U 2 U , where U U = U U = I . It is then well-known and simple to check that
- 1/2  S := -1/2 U R

(4.23) (4.24)

¯+ S -1 = . is the required balancing transformation (4.5) such that SP- S = (S )-1 P Proposition 4.2. Given the partial covariance sequence ck = E{y (t + k )y (t)}, k = 0, 1, . . . , , o polynomials let 1 (z ), 2 (z ), . . . ,  (z ) and r0 , r1 , . . . , r be the corresponding Szeg¨ and error variances. Supposing that  -1 = - (0) = 0, let (F, b, h) be given by (4.15), R and  by (4.16) and Q by (4.21). Moreover, let U and  be defined by the singular value decomposition (4.23) of (4.22). Then, the canonical correlation coefficients 1 , 2 , . . . ,  are the diagonal elements of , as described in (4.7), and the stochastically balanced realization of W is given by  (4.25) (A, B, C, D) = (SF S -1 , SQb, h S -1 , r ), where S is defined by (4.24). Stochastic balanced truncation (SBT) is then performed as described above. Principal subsystem truncation (4.11) is executed on the balanced realization (4.25) to yield a transfer function (4.12) of degree r. Bounds can be derived for the approximation error, and the procedure can be designed so that it preserves the minimum-phase property. In fact, we have the following result, the proof of which is given in Section A. Theorem 4.3. Let Wred be the SBT approximation of degree r of W , and set k := 2 1 - k k=r+1
  -1

and

 :=



c0
k=0

1 + |k | , 1 - |k |

(4.26)

where 0 , 1 , . . . ,  -1 are the Schur parameters of c0 , c1 , c2 , . . . , c . Then c0 (1 - )-1  |Wred (ei )|  (1 + ) for all , W - Wred  . (4.27) (4.28) and, if < 1, Wred is minimum phase. Finally, the approximation error has the bound


A properly executed SBT procedure should imply that the canonical correlation coefficients r+1 , . . . ,  , and hence , are close to zero, insuring the minimum-phase condition. Remark 4.4. Stochastic model reduction can also be carried out by instead per¯ + 1 c0 , ¯ ) in V (z ) = C (zI - A)-1 C forming principal subsystem truncation on (A, C, C 2 ¯ = S (c1 , c2 , . . . , cn ). It was shown in where A and C are given by (4.25) and C [32] that this preserves the necessary positivity, i.e., Vred is positive real. Finally, the spectral density red (z ) := Vred (z ) + Vred (z -1 ) is factorized to yield a minimum~ . This is in a sense a more natural procedure, but we do phase spectral factor W not know of any error bound for it. Statistically it behaves essentially as SBT,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

15

and for small 2 it yields almost the same result. In fact, it is shown in [53], that ~ (ei )|2 = |Wred (ei )|2 + H (ei )2 H (e-i ), where H (z ) = C1 (zI - A11 )-1 A12 . |W Remark 4.5. There are good reasons to prefer stochastic over deterministic model reduction, as seen from the following heuristics. In fact, it can be seen that V (z ) = c0  (z ) , 2  (z ) (4.29)

where  (z ) is the Szeg¨ o polynomial of the second kind (obtained by exchanging -t for t in the recursion (3.4)). Now, the matrix representation of the Hankel operator ¯+ respecH in the innovation bases of the past and the future, provided by w- and w is the infinite Hankel matrix of the sequence tively, is given by L-1 (L-1 ) , where c1 , c2 , c3 , . . . and L is the lower triangular Cholesky factor of the Toeplitz matrix T ; see, e.g., [32, p. 714]. It is easy to see that  (z ) has the same asymptotic behavior as  (z ), i.e., the roots tend to cluster uniformly inside the circle z =  as   , and hence these roots are close to canceling in (4.29). Consequently, the corresponding Hankel matrix is close to having low rank. This massive "almost cancellation" does not occur in W (z ), and hence the corresponding infinite Hankel matrix, constructed from the Laurent coefficients of W (z ), may have a less distinct separation between 1 and 2 . On the other hand, since the Schur parameters tend geometrically to zero, the lower part of L tends to the identity, and hence the asymptotic behavior of the canonical correlation coefficients is very much like that of the singular values of . Therefore we may expect SBT to have better statistical behavior than DBT. In Section 6 we shall see that this is the case.

H

H

H

H

5. Identification from statistical data We now return to our original problem of time series identification: Given a data string (2.2) of observations of the output process y of some n-dimensional linear stochastic system (2.1) with minimum-phase transfer function W (z ), given by (2.8), ^ B, ^ C, ^ D ^ ) of the matrices (A, B, C, D). find an estimate (A, The identification method proceeds as follows. Given the covariance estimates (2.5), we compute the corresponding maximum entropy filter (2.7), a balanced realization (4.25), and the canonical correlation coefficients ^2 ,  ^3 , . . . ,  ^ ,  ^1 ,  (5.1)

^1 , . . . , c ^ . determined as in Proposition 4.2 from the covariance estimates c ^0 , c Based on (5.1), choose an integer n ^ such that  ^n ^n ^ are close to zero or ^ +1 ,  ^ +2 , . . . ,  ^2 , . . . ,  ^n . Then, the balanced realization (4.25) at least distinctively smaller than  ^1 ,  ^ is truncated accordingly as in (4.11) to yield a n ^ -dimensional triplet (A11 , B1 , C1 ) and a transfer function ^ (z ) = C1 (zI - A11 )-1 B1 + D. W ^ B, ^ C, ^ D ^ ). Then, (A11 , B1 , C1 , D) is the required estimate (A, As pointed out in Section 2, we have a bound ^ W -W


(5.2)

 W - W



^ + W - W



^ - W ^ + W

,

(5.3)

16

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

for the estimation error. As seen from Theorem 3.4, the first term W - W  , which does not depend on the statistical data (2.2) but only on the underlying system (2.1), tends to zero geometrically with a rate   (0, 1) as   . The other two terms depend on the data (2.2), and here N must grow at a faster rate than  . In fact, we shall assume that  =  (N ) = O(log N ), (5.4)

 = 0. We also need to assume that the which in particular requires that limN  N white noise process in (2.1) satisfies a mild technical condition, namely

E{w(t)4 } < . This condition is, of course, satisfied if w is Gaussian. Next, we present our main convergence theorem.

(5.5)

Theorem 5.1. Suppose y is the output process of a system (2.1), having a minimumphase transfer function W , and driven by a white noise with the property (5.5). Then, to each length N of the data string (2.2), there is a  (N ), tending to infinity with N ^ of fixed at the rate (5.4), such that any sequence of estimated transfer functions W degree n ^  n, determined, for each N and corresponding  =  (N ), by the procedure described above, satisfies ^ 0 W -W ^ has almost surely as  (N )  . For sufficiently large  (N ), the transfer function W minimum phase. We have already proven that the first term in (5.3) tends to zero, so Theorem 5.1 follows from the next two theorems, each corresponding to one of the remaining terms in (5.3). As for the second term, we have the following result, the proof of which is deferred to Appendix B. Theorem 5.2. Suppose the system (2.1) satisfies the conditions of Theorem 5.1. Let W be the maximum-entropy filter (3.6) determined from the partial covariance se^  be the corresponding function determined from the ergodic quence (3.1) of y and let W estimates (2.5). Then, if  (N ) is defined as in Theorem 5.1, ^  (N ) W  (N ) - W almost surely as  (N )  . There are several results of this type in the literature [2, 36, 7, 33]. In particular, 3  0 as N   and  is coercive (i.e. positive Berk [2] proved that, provided  N on the unit circle), the estimated AR spectral density (ei )  (ei ) in probability. Under the same hypotheses, Caines and Baykal-G¨ ursoy [7] showed that if N   5+ -1 ^ for some  > 0, then W - W -1   0 almost surely as   . However, in both cases, ergodic estimates are used which are not quite the same as (2.5). Finally, we consider the last term in (5.3). The proof of the following theorem is given in Appendix B.


0

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

17

Theorem 5.3. Suppose the system (2.1) and the function  (N ) are defined as in ^ ^  (N ) be defined as in Theorem 5.2 and W Theorem 5.1. Moreover, for each N , let W ^ has minimum phase, and as in Theorem 5.1. Then, for sufficiently large  (N ), W ^ ^  (N ) - W W almost surely as  (N )  . 6. Simulations ^  , rather than on the maximum-entropy filter of Performing model reduction on W exact covariance data as in Section 3, the advantage of stochastically balanced truncation becomes apparent. First stochastically balanced truncation allows for easier and more accurate order determination, as the heuristics of Remark 4.5 suggest. There are also alternative order determination statistical tests based on the canonical correlation coefficients [17, 26, 46]. But, even more importantly, there is less bias, and the error variances are closer to the Cram´ er-Rao bound. Since we are approximating rational models with AR models the method will be biased for finite amount of data, unless the model generating the data really is an AR model. The consistency result given in Theorem 5.1 implies that the method is asymptotically unbiased and therefore we consider the Cram´ er-Rao bound for unbiased methods; see [44, pp. 137­138]. The Cram´ er-Rao bound for biased estimation requires knowledge about the bias as a function of the parameter to be estimated. As already mentioned, the method will be unbiased and even statistically efficient for Gaussian AR processes if the model reduction step is omitted. Despite the fact that an algorithm based on covariance estimates (2.6) is not asymptotically efficient for general ARMA models [44, p. 144], our method can be used to provide a starting guess for other algorithms, for example the maximum likelihood method.
6 SBT dashed line, DBT dotted line. 5



0

4

3

2

1

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.1: Biases of the identification estimates.

To illustrate our procedure, let us consider data generated by passing white noise through a "true system" with transfer function W (z ) = z 5 - 0.0550z 4 - 0.1497z 3 - 0.2159z 2 + 0.1717z - 0.0495 . z 5 - 0.7031z 4 + 0.3029z 3 + 0.1103z 2 - 0.1461z + 0.2845

18

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Model reduction is performed, both with DBT and SBT, on a maximum-entropy ^  of degree  = 24 determined from estimated covariances. Based on 100 model W test runs, the empirical means and standard deviations are determined. Figure 6.1 illustrates the statistical bias as a function of the length N of the data string when using stochastic (dashed curve) and deterministic (dotted curve) model reduction respectively. For the same test runs, Figure 6.2 illustrates the corresponding standard deviations together with the Cram´ er-Rao bound (solid curve). More precisely, the figures depict the sums of the moduli of the biases and standard deviations respectively for the ^ (z ). coefficients of the numerator and denominator polynomials of W
5 CRB solid line, SBT dashed line, DBT dotted line. 4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Figure 6.2: Standard deviations of estimates and the Cram´ er-Rao bound.

The same pattern can be observed in the next experiment, where a model W (z ) with poles and zeros closer to the unit circle is considered. The poles and zeros of ^ (z ) are determined for 100 runs and a data length N = 500. As before,  = 24. W Figure 6.3 depicts these poles and zeros in the case that SBT is used, together with the poles and zeros of W (z ), which are denoted by "".

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.3: SBT estimates of zeros (left) and poles (right) for N = 500 and  = 24.

The approximation obtained from the alternative stochastic model reduction pro-

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

19

cedure discussed in Remark 4.4 shows the same pattern as SBT. To further motivate the reason for preferring stochastic model reduction, the corresponding experiment with deterministically balanced truncation is illustrated in Figure 6.4. As expected, the spread is greater, and some of the solutions are nonminimum phase.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.4: DBT estimates of zeros (left) and poles (right) for N = 500 and  = 24.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.5: Estimates of zeros (left) and poles (right) when using subspace identification.

Figure 6.5 describes the result obtained when applying stochastic subspace identification to the same data. More precisely, Algorithm # 2 in [43] is used. In order to make the experiments comparable, we have chosen a Hankel matrix of dimension 13 × 13, which corresponds to  = 25 in our procedure. Note that the estimates are much less focused, and many zeros tend to cluster on the unit circle, implying that coercivity becomes critical. This is related to the positivity issues discussed in [9]. Also for the model illustrated in Figure 6.1 and Figure 6.2 the subspace identification method performs worse than our SBT identification method, yielding larger biases and standard deviations, but performs better than when DBT is used.

20

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Although the method considered here yields very focused pole-zero estimates, as illustrated in Figure 6.3, there is a noticeable bias in the zero estimates. It will disappear as  and N are increased. In Figure 6.6 we show the same experiment for  = 64 and N = 2000.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.6: SBT estimates of zeros (left) and poles (right) for N = 2000 and  = 64.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1 -1

-0.5

0 zeros

0.5

1

-1 -1

-0.5

0 poles

0.5

1

Figure 6.7: SBT estimates of zeros (left) and poles (right) for N = 500 and  = 40 using Burg's method.

In practice, there is a trade-off between the quality of the ergodic estimates, which roughly speaking depend on |max (A)|, the  -error tolerance, which is a function of |max (A - BD-1 C )|, and the numerical accuracy of the computations. For example, if the zeros of W (z ) are far from the unit circle and  is chosen very large, the error may increase. In the present example, it turns out that using Burg's method [3] in lieu of the ergodic estimate (2.6) yields better estimates for smaller  and N , as illustrated in Figure 6.7 which shows the case N = 500 and  = 40. A more detailed picture of the same experiment is given In Table 6.1 and 6.2. There we give the empirical bias and standard deviation for the coefficients of the numerator and the denominator, respectively, of the estimated transfer functions together with the Cram´ er-Rao bound. It is the authors experience that Burg's method gives at least as good results as when using the ergodic covariance estimate (2.6), unless the intermediate AR model used has a very high model order.

L

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

21

Parameter True value Bias: CE: Burg: Std.dev.: CE: Burg: CRB:

2 w b1 b2 b3 b4 b5 1.0000 -0.8762 0.0184 0.0197 0.8591 -0.7491 0.5458 0.1434 0.0100 0.0573 -0.2193 0.2895 0.0971 0.0383 -0.0147 -0.0117 -0.0544 0.0734 0.2332 0.1314 0.0508 0.0611 0.0722 0.0802 0.0712 0.0411 0.0381 0.0339 0.0339 0.0356 0.0632 0.0313 0.0312 0.0313 0.0312 0.0309

Table 6.1. Bias and standard deviation of estimated numerator polynomials for
N = 500 and  = 40 using covariance estimation (CE) or Burg estimation and , in both cases, followed by SBT.

Parameter a1 a2 a3 a4 a5 True value -0.6281 0.3597 0.2634 -0.5322 0.7900 Bias: CE: 0.0087 -0.0044 -0.0003 0.0066 -0.0152 Burg: -0.0293 -0.0014 -0.0047 -0.0138 0.0125 Std.dev.: CE: 0.0274 0.0304 0.0371 0.0305 0.0304 Burg: 0.0336 0.0307 0.0358 0.0324 0.0306 0.0293 0.0321 0.0342 0.0322 0.0290 CRB: Table 6.2. Bias and standard deviation of estimated denominator polynomials
for N = 500 and  = 40 using covariance estimation (CE) or Burg estimation and, in both cases, followed by SBT.

7. Conclusions We have presented a three-step procedure for identification of time series, which is easy to understand and implement. Just like for subspace identification methods, robust linear-algebra algorithms can be used and no nonconvex optimization computations are required. Moreover, it has a sound theoretical basis and is computationally competitive to stochastic subspace identification, as our extensive simulations indicate. In particular, its good performance has been confirmed by Monte Carlo simulations. The paper only covers the scalar case, but the multivariate case is presently being worked out. The three steps, covariance estimation, covariance extension and model reduction have each been studied separately before. This is an advantage which should make the method easy to grasp. However, a comprehensive study of the entire identification strategy, giving appropriate bounds, has been missing and this is what we offered here. The observation that the Schur parameters converge geometrically simplifies our application of Szeg¨ o theory and allows us to give a complete account of the asymptotic behavior of maximum entropy models of growing order. This analysis provides us with a clear indication as to when the identification strategy is good and when it might face difficulties, based purely on the closeness of the maximum modulus zero to the unit circle. The parsimony permeating other system identification methods should not be a reason for refraining from high-order modeling as an intermediate step. In fact, such a strategy might be desirable, since we have shown that the poles of the "true" system which lie outside a circle in the complex plane containing all of its zeros are

22

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

directly inherited by the high order models. The rest of the poles cluster inside the perimeter of this circle, providing a justification for choosing stochastically balanced model reduction, rather than deterministically balanced truncation, in the last step. With this reduction procedure, we have confirmed better statistical properties with variances closer to the Cram´ er Rao bound. The procedure could also be modified by exchanging exact covariance extension for approximate one, as outlined in [35]. Even though, in general, stochastic balancing would require the solution of a pair of Riccati equations, this is not the case for the particular maximum entropy models used here. In fact, the balancing procedure only requires linear algebra, and hence an intelligent use of the Levinson algorithm may substantially reduce the number of arithmetic operations. Finally, by decomposing the total error as a sum of three terms, each one independently adjustable by choosing the integers N ,  and n ^ , we gave worst-case guaranteed bounds, which complement the nice statistical properties of the method. The error analysis is based on the assumption that the data comes from a rational coercive stochastic system, but the method returns a valid model also for generic data. In fact, in contrast to many stochastic subspace identification [9], all steps of the procedure preserve the positive real property. Appendix A. Asymptotic behavior of the maximum entropy filter Theorem 3.4 is actually a modification to the rational setting of a theorem due to Szeg¨ o [47], and the proof is modeled after [19], which in turn includes aspects already present in the work of Schur [45]. See also [48], [49] and [16] for more facts on orthogonal polynomials. However, rationality and coercivity allows us to present a simplified and self-contained proof of a version of Szeg¨ o's classical theorem, to which we also are able to add geometric convergence. The derivation of Caines and BaykalG¨ ursoy [7] is shorter, but we feel that our approach is more systematic and gives additional insight into the mechanism of identification. To prove Theorems 3.4, 3.5 and 4.3 we need the following lemmas. o polynomials (3.7). Then |z -  (z )| Lemma A.1. Let { (z )} 0 be the normalized Szeg¨ is uniformly bounded from above and away from zero in the complement Dc of the open unit disc, i.e., there are positive numbers ,   R such that   |z -  (z )|   for all  and all z  Dc . Proof. In view of the Szeg¨ o-Levinson recursion (3.4), t+1 (z ) = t (z ) z -  ¯t and hence z -  (z ) =
 -1

 t (z ) , t (z )  k (z ) . k (z )

¯k 1 - z -1 

k=0

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

23

Now, if z1 , z2 , . . . , z are the roots of  (z ), it is immediately seen that 1 - zz ¯k   (z ) , =  (z ) k=1 z - zk which is a Blaschke product, analytic in Dc and having modulus one on the unit circle, and thus modulus less than or equal to one in Dc . Hence, since |z -1 |  1 in Dc ,
 -1 

(1 - |k |)  |z
k=0

-

 -1

 (z )| 
k=0

(1 + |k |)

(A.1)

for all z  Dc . But, these products converge to positive numbers as   . This follows from the absolute convergence of the infinite sum  k=0 |k |, a fact that, in the present context, stems from Lemma 3.1. From (3.5) we also have 0 < r  r  r0 , and consequently the lemma follows. Remark A.2. An equivalent statement of this lemma is that the maximum entropy solution W (z ), defined by (3.6), is uniformly bounded from above and away from zero for all  and z  Dc . Lemma A.3. Suppose W is rational and minimum-phase. Then, the sequence of functions f (z ) := z -  (z ) converges uniformly to an analytic function f in statement of Theorem 3.5.

Dc ,

where

Dc 

is defined in the

Proof. Recall from the theory of polynomials orthogonal on the unit circle [49] the purely algebraic relation


k (z )k (w) =
k=0

  ¯  (z ) (w)  (z ) (w ) - z w , 1 - zw ¯

(A.2)

which is called the Christoffel-Darboux-Szeg¨ o formula. In particular, setting w = 0 -1 and exchanging z for z in (A.2), (3.5) and (3.7) yield f (z ) 1 k-1 - k (z -1 )  .  = r c0 k=1 rk


(A.3)

c Observe that k (z -1 ) is analytic and bounded in Dc  , and hence in D , and therefore it  belongs to - . Moreover, by the maximum modulus principle, it attains its maximum value in Dc on the unit circle where, by Lemma A.1, it is bounded by  . Hence

H

|k (z -1 )|  

for z  Dc and for all k.

(A.4)

Therefore, in view of (A.3) and the fact that rk  r , we have f (z ) fµ (z )    -  r rµ r


|k-1 |,
k=µ+1

(A.5)

24

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

which, by Lemma 3.1, can be made arbitrarily small for sufficiently large  and µ.  . The same holds for f (z ). In This establishes (A.3) as a Cauchy sequence in - c fact, since r  c0 , for all z  D

H

|f (z ) - fµ (z )| 

f (z ) fµ (z ) c0  -  r r  f (z ) fµ (z ) 1 1  c0  -  + |fµ (z )|  -  . r rµ r rµ



(A.6)

But, by Lemma A.1, |fµ (z )|   for all  and z  Dc , and therefore, in view of (A.5), we obtain |f (z ) - fµ (z )|   c0 r 1 1 |k-1 | +   -  r rµ k=µ+1


for all z  Dc . (A.7)

Since r  r as   , we see that, for each > 0, |f (z ) - fµ (z )| < for sufficiently large  and µ. Consequently, f tends uniformly in Dc to a function  . f  H- The uniform convergence and the analyticity can be extended to any compact -1  Dc . Therefore, by subset of Dc  . To see this, first note that z  D if and only if z Lemma A.1, |k (z -1 )|   |z |-k for z  D, and consequently, since r  rk , (A.3) yields 1 |f (z )|   +  |z |-1 |k ||z |-k . c0 k=0 Similarly, instead of (A.5) we have f (z ) fµ (z )  |k ||z |-k .   |z |-1  -  r rµ r k =µ
 -1  -1

(A.8)

(A.9)

> 0 such Now, for any compact subset K  Dc  , there is a   (, 1) and an -k ^ k where that |z | >  + for all z  K . Hence, by Lemma 3.1, |k ||z |  M   ^ :=  ( + )-1 < 1. Consequently, by (A.8), f (z ) is uniformly bounded in K , and (A.9) can be made arbitrarily small for sufficiently large  and µ. Therefore, by (A.6), f tends uniformly in K to the analytic function f . Lemma A.4. Let  be a real number such that  <  < 1. Then f -f Proof. It follows from (A.7) that   c0 |k-1 | + 1 - |f (z ) - f (z )|   r k= +1
 

= O(  ).

r r

for all z  Dc . (A.10)

By Lemma 3.1, the first term is O(  ). It remains to show that the same holds for the second term. To this end, first note that, by (3.5), 1- r =1- r k =
 2 1 - k .

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

25

But, by Lemma 3.1, |k |  M  k for some M . Therefore, since each x  [0, 1],  r  1 - (1 - M  k ) = O( t ) 1- rt k =t for t large enough. This concludes the proof.



1 - x2  1 - x for

Recalling the definition (3.6) of W , we note that Lemma A.4 may be written
-1 W - f 

= O(  ).

-1 in the same manner. As it turns out, by coercivity, this implies that W  f

Lemma A.5. Let W be the transfer function (3.6) of the maximum entropy filter. Then -1  W - f  = O ( ), where f is the limit function of Lemma A.3. Proof. Note that the limit function f has the same uniform bounds as f in Lemma A.1. In particular, |f (z )|  , |f (z )|-1  -1 , and |W (z )|  -1 for all z  Dc . Consequently,
-1 W - f 

 W



-1 f



-1 W - f



-1   -2 W - f

,

so the required result follows from Lemma A.4. Lemma A.6. Let W be the rational minimum-phase function defined above, and let f be the limit function in Lemma A.3. Then W (z ) = f (z )-1 for all z  Dc . Proof. Let  (ei ) := |W (ei )|2 be the spectral density of the maximum entropy process. Then, in view of the interpolation condition, 1 2


e
-

ik

1 (e )d = ck = 2
i

 -

eik  (ei )d

for k = 0, 1, . . . , , (A.11)

from which we have pointwise convergence of the Fourier coefficients of  (ei ) to those of (ei ) as   , and hence  (ei )  (ei ) in the 2 sense. However, by Lemma A.5,  (ei )  |f (ei )|-2 in  norm, and hence a fortiori in 2 norm, as   . Since, in addition, not only (ei ) but also f is analytic in a neighborhood of the unit circle (Lemma A.3), we have

L

L

L

(ei ) = |f (ei )|-2 .

(A.12)

In the language of Hardy space theory [14], a minimum-phase spectral factor is outer. In particular, W is an outer spectral factor of  (ei ) satisfying W (z ) = exp 1 4
 -  -

eit + z log |W (eit )|2 dt . eit - z

But Lemma A.5, Equation (A.12) and the fact that (ei ) = |W (ei )|2 , W (z )  exp 1 4 eit + z log |W (eit )|2 dt = W (z ), it e -z

the outer spectral factor of . But, by Lemma A.3, W (z )  f (z )-1 in therefore f (z ) = W -1 (z ) as claimed.

Dc  , and

26

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

Proof of Theorem 3.4. The theorem is a direct consequence of Lemmas A.5 and A.6. Proof of Theorem 3.5. The theorem follows from Lemmas A.3 and A.6. Proof of Theorem 4.3. Following [53] we see that
-1 (W - Wred ) W 

 ,

(A.13)

and consequently |W (ei ) - Wred (ei )|  |W (ei )| holds for all , from which we have (1 - )|W (ei )|  |Wred (ei )|  (1 + )|W (ei )|. However, in view of (3.6) and (3.7), it follows from (A.1) that   r r i  |W (e )|   -1 ,  -1 k=0 (1 + |k |) k=0 (1 - |k |) which together with (3.5) yields c0  |W (ei )|    (A.14)

for all . This establishes (4.27). To see that Wred is minimum phase if < 1, note e's that, by (4.27), Wred cannot have a zero on the unit circle. Moreover, by Rouch´ c Theorem, Wred has the same number of zeros in D (including ) as W . Hence, since W is minimum phase, so is Wred . To establish the bound (4.28) note that W - Wred From (A.14) we have W
 

 W



-1 W (W - Wred )

.

 , and hence (4.28) follows from (A.13).

Appendix B. Statistical convergence proofs Proof of Theorem 5.2. Given the covariance estimates (2.6) we determine the corre^ from (3.4) and (3.5), sponding Szeg¨ o polynomial  ^ (z ) and predictor error variance r and form the maximum-entropy filter   r ^ z ^ . W (z ) =  ^ (z ) ^ To determine W - W let z  Dc and form     r z r ^ z ^ W (z ) - W (z ) = -  (z )  ^ (z )    ^ )z -  (z ) - r z - ( (z ) -  ^ (z )) ( r - r . = z -  (z )z -  ^ (z )


Since r > 0, by (3.7) and Lemma A.1,   0 < µ := r   |z -  (z )|  c0  =: M,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

27

and, by (A.1), |z
-  -1

 ^ (z )|  µ ^ :=
k=0

(1 - | ^k |),

^1 , . . . ,  ^ -1 are the Schur parameters corresponding to the estimated cowhere  ^0 ,  variances (2.6). Therefore, by the maximum-modulus principle, ^  (z )|  max 1 {M |r - |W (z ) - W |z |=1 µµ ^ r ^ | +  c0 | (z ) -  ^ (z )|},

where we have also used the fact that r  c0 . But, for |z | = 1, ^  1, ^ (z )|   -  | (z ) -  ^  are the  -vectors formed as in (4.19) and · where  and  that  is the unique solution of the normal equations T  = -c where c := c c -1 . . .
1

is the

1

norm. Recall (B.1)

c1 ,

where T is the Toeplitz matrix defined by (2.4), and that r  = c 0 + c   . ^ . Then, Also, the analogous relations hold for  ^ and r ^  )  + c ^ ( -  ^ ) ^ = (c0 - c ^0 ) + (c - c r - r and hence ^ ^ |  |c0 - c ^0 | + c - c |r - r Finally,  | r -
1 1

(B.2)





^ + c



^  1.  - 

^ | ^ | |r - r |r - r  r ^ |     , r r + r ^


and consequently, since x ^ W - W


 x M 

for any x  R ,

^ {|c0 - c ^0 | +    c - c µµ ^  r ^  M c 1  ^  . c0 +  +   -  µµ ^ r



}

(B.3)

^  are each solutions of a normal equation (B.1). More Recall now that  and  ¯ for k > 0, where all ^  ^  = -c ^ . Since ck = CAk-1 C precisely, T  = -c and T eigenvalues of A are less than one in modulus, ck  0 exponentially, we have
 -1

c



 K1

and

T



 c0 + 2
k=1

|ck |  K2

for some constants K1 and K2 . Moreover, from [8] we have
-1 T 



1 c0

 -1

1 + |k |  K3 1 - |  k| k=0

28

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

for some constant K3 ; see the proof of Lemma A.1. Hence  and the condition number (T ) := T
 -1 T   -1  T 

c



 K 1 K3

 K := K2 K3

is bounded for all  . Now, it is known [25] that for each data length N in (2.2), there is a  (N ) of order O(log N ) such that ^k | = O max |ck - c log log N N , (B.4)

0 k   (N )

and therefore, for any a  R, ^ ^0 |  0 and  a c - c  a |c0 - c


 0 as  =  (N )  .

(B.5)

Consequently the first term in the bound (B.3) tends to zero as N   and  (N )   provided it is done at the specified relative rates and provided µ ^ is bounded away from zero. However, the estimate (2.6) has the property that the ^ is positive definite for each finite  , and this in turn corresponding Toeplitz matrix T ^ > 0. Since, in addition is equivalent to | ^k | < 1 for k = 0, 1, . . . ,  - 1 so that µ µ ^  µ > 0 as  (N )   by (B.4) and continuity, the second requirement is also fulfilled. To simplify notations, we have suppressed the index N in the quantities marked with a hat, which of course depend on the data (2.2) and hence also on N . ^   Next we show that also the second term in (B.3) tends to zero. Since c ^  is bounded, it thus remains to demonstrate that c   + c - c ^  (N )    (N ) - 


 0 as  (N )  .

This follows from the more general fact, needed for the proof of Corollary B.1, that ^  (N )  a  (N ) - 


 0 as  (N )  

(B.6)

for any a  R. To prove this, first note that ^ T - T


^  |c0 - c ^0 | + 2 c - c

,

-1 ^   0. Therefore  := T - T ^  T and hence T - T  < 1 for  :=  (N ) sufficiently large, and, provided c = 0, the standard perturbation estimate [22] yields

^  -   





1 (T ) 1 - 

^ T - T T 



+

^ c - c c 



,

(B.7)

and consequently, since T   c0 > 0, it follows from (B.5) that (B.6) tends to zero in the required manner. If c = 0,  = 0, and hence ^  - 


^ = 



-1 ^  T



^ c



-1 ^ = T



^ c - c

,

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

29

which shows that (B.6) tends to zero also in the case c = 0. In fact, since µ ^ is bounded away from zero, by continuity, for each > 0, there is a N0 such that ^ -1 T  for   N0 .


1  c ^0

 -1

1 1 + | ^ k|  1 - | ^ c0 k| k=0

 -1

1 + |k | +  K3 + 1 - |k | k=0

Corollary B.1. If  (N ) is defined as in Theorem 5.1, then, for any a  R, ^  a W - W


0

almost surely as  :=  (N )  .

To prove Theorem 5.3, we first note that the Hankel operator H, defined by (4.1), has a nice representation in the space 2 of square-integrable functions. In fact, let 2 2 of functions with vanishing negative Fourier coefficients, + be the subspace in hence being analytic in the unit disc D. In this setting, H has the representation 2 2  2 H : + + given by

H

L

L

H

L H

H f = P  f, where P  is the orthogonal projection onto the orthogonal complement 2 2 , and where  is the  -function + in

H

L

L

L H
2

(B.8)
2 +

of

¯ + (z )-1 . (z ) = W- (z )W

(B.9)

¯ + (z ) are the analytic and coanalytic minimum-phase spectral facHere W- (z ) and W ¯ + (z ) = tors defined in Section 4. (See, e.g., [30, 31].) In the present scalar case, W -1 W- (z ). In fact, the phase function  is the transfer function of an all-pass filter transforming the white noise w- in (4.2) to the white noise w+ in (4.3) [30, p. 834]. ^ ¯ + be the stochastic measures such that Let dw ^- and dw
 

w- (t) = Then

-

eit dw ^-


and w ¯+ (t) =


-

^ eit dw ¯+

H+ = H- = and consequently H := E f  f dw ^- .
H- -  -

H

2 ^ ¯+ + dw

=
-

H

2 it ^- + (e )dw

L H
2

2 ^- + dw

|H + corresponds to H under the isomorphism defined by

^  (ei )| - |W (ei )|  0 Proof of Theorem 5.3. It follows from Theorem 5.2 that |W uniformly in  as   , and hence, by Lemma A.1, there are positive real numbers µ1 and µ2 such that ^  (ei )|  µ2 µ1  |W for all  and sufficiently large  . Therefore, since -1 ^ ^ - W ^  W ^  W ^ ^) W (W - W
,

(B.10)

30

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

(A.13) and (4.26) imply that ^ - W ^ W


 2µ2

 ^k , 1- ^k k=^ n+1



(B.11)

^2 , . . . ,  ^ are the singular values (5.1) determined for sufficiently large  , where  ^1 ,  from the covariance estimates (2.6). It is well-known (see, e.g., [56, p. 204]) that the singular value k of the Hankel operator H , defined by (B.8) equals the infimum of H - K over all operators 2 2 -1  2 K : + + of finite rank at most k . Recall that (z ) = W (z )/W (z ). ^ ^ ^ -1 The singular value  ^k of H ^ , where (z ) = W (z )/W (z ), is described analogously. Therefore, since

H

L H

^ H ^ - K  H ^ - H  + H - K   - 



+ H - K ,
.

^ -   + k . But, for k > n, k = 0, and hence  ^ - we have  ^k   ^k   Consequently, (B.11) yields ^ ^ - W W


^ -  M1  

,

(B.12)

-1 ^n where M1 := 2µ2 (1 -  ^ +1 ) . However,

^  (z -1 ) - W (z -1 )] , ^  (z ) - W (z ) - (z )[W ^ z ) - (z ) = W ^  (z -1 )-1 W ( ^  (z -1 ) so, since W


is uniformly bounded by (B.10), and  ^ - 




is constant,

^  M2 W - W

,

which together with (B.12) yields ^ ^ - W W


 M1 M 2  W - W 



^ + M1 M2  W  - W



for sufficiently large  . Consequently the theorem follows from Theorem 3.4 and Corollary B.1. Acknowledgment. We would like to thank Gy. Michaletzky, L. Baratchart, A. Gombani, W. B. Gragg, G. Picci and T. S¨ oderstr¨ om for stimulating discussions and for providing us with appropriate references. We are also indebted to the anonymous referees for several useful suggestions. References
1. M. Aoki. State Space Modeling of Time Series. Springer Verlag, 1987. 2. K. N. Berk. Consistent autoregressive spectral estimates. Annals Statistics, 2:489­502, 1974. 3. J. P. Burg. Maximum entropy spectral analysis. PhD thesis, Stanford University, Dept. Geophysics, Stanford CA., 1975. 4. C. I. Byrnes, S. V. Gusev, and A. Lindquist. A convex optimization approach to the rational covariance extension problem. SIAM Journal on Control and Optimization, 37:211­229, 1999. 5. C. I. Byrnes, A. Lindquist, S. V. Gusev, and A. V. Matveev. A complete parametrization of all positive rational extensions of a covariance sequence. IEEE Trans. Automatic Control, AC40:1841­1857, 1995. 6. C. I. Byrnes, A. Lindquist, and Y. Zhou. On the nonlinear dynamics of fast filtering algorithms. SIAM Journal on Control and Optimization, 32:744­789, 1994.

A COVARIANCE EXTENSION APPROACH TO IDENTIFICATION

31

7. P.E. Caines and M. Baykal-G¨ ursoy. On the L consistency of L2 estimators. Systems & Control Letters, 12:71­76, 1989. 8. G. Cybenko. Error Analysis of Some Signal Processing Algorithms. PhD thesis, Princeton University, 1978. 9. A. Dahl´ en, A. Lindquist, and J. Mari. Experimental evidence showing that stochastic subspace identification methods may fail. Systems and Control Letters, 34:303­312, 1998. 10. U. B. Desai and D. Pal. A realization approach to stochastic model reduction and balanced stochatic realizations. In Proc. 21st IEEE CDC, pages 1105­1112, 1983. 11. U. B. Desai and D. Pal. A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29:1097­1100, 1984. 12. J. Durbin. Efficient estimation of parameters in moving average models. Biometrika, 46:306­316, 1959. 13. J. Durbin. The fitting of time-series models. Rev. Inst. Int. Stat., pages 223­243, 1960. 14. P. Duren. Theory of Hp spaces. Academic Press, 1970. 15. P. Enqvist. PhD thesis, Royal Institute of Technology, to appear. 16. G. Freud. Orthogonale Polynome. Birkh¨ auser Verlag, 1969. 17. J. J. Fuchs. ARMA order estimation via matrix perturbation theory. IEEE Trans. Automatic Control, AC-32:358­361, 1987. 18. T. Georgiou. Realization of power spectra from partial covariance sequences. IEEE Trans. Ac., Speech and Signal Processing, ASSP-35:438­449, 1987. 19. L. Geronimus. Orthogonal Polynomials. Consultant Bureau, 1961. 20. M. Gevers. Towards a joint design of identification and control. In J. Willems and H. Trentelman, editors, Essays on Control: Perspectives in the Theory and its Applications, 1993. 21. K. Glover. All optimal Hankel-norm approximations of linear multivariable systems and their l -error bounds. Int. J. Contr., 39:1115­1193, 1984. 22. G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins, 1989. 23. W. B. Gragg. Positive definite Toeplitz matrices, the Arnoldi process for isometric operators, and Gaussian quadrature on the unit circle. In E. S. Nikolaev, editor, Numerical Methods in Linear Algebra, pages 16­32. Moscow U. P., 1982. 24. U. Grenander and G. Szeg¨ o. Toeplitz forms and their applications. Univ. California Press, 1958. 25. E. J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. John Wiley & Sons, 1988. 26. C. Hurvich and C.L. Tsai. Regression and time series model selection in small samples. Biometrika, pages 297­307, 1989. 27. W. Jones and E. Saff. Szeg¨ o polynomials and frequency analysis. In Approximation Theory, pages 341­352. Dekker Inc., 1992. 28. S. Y. Kung. A new identification method and model reduction algorithm via singular value decomposition. In 12th Asilomar Conf. on Circuits, Systems and Comp., pages 705­714, 1978. 29. W. E. Larimore. System identification, reduced ordered filtering and modeling via canonical variate analysis. In Proc. of the American Control Conference, 1983. 30. A. Lindquist and G Picci. Realization theory for multivariate stationary gaussian processes. SIAM J. Control and Optimization, 23:809­857, 1985. 31. A. Lindquist and G. Picci. A geometric approach to modelling and estimation of linear stochastic systems. J. of Math. Systems, Estimation and Control, 1:241­333, 1991. 32. A. Lindquist and G. Picci. Canonical correlation analysis, approximate covariance extension, and identification of stationary time series. Automatica, 32(5):709­733, 1996. 33. L. Ljung and B. Wahlberg. Asymptotic properties of the least-squares method for estimating transfer functions and disturbance spectra. Adv. Appl. Prob., 24:412­440, 1992. 34. L. Ljung and Z. Yuan. Asymptotic properties of black box identification of transfer functions. IEEE Trans. Automatic Control, AC-26:514­530, 1985. 35. J. Mari. Rational Modeling of Time Series and Applications to Geometric Control. PhD thesis, Royal Instiute of Technology, 1998. 36. D. Q. Mayne and F. Firoozan. Linear identification of ARMA processes. Automatica, 18:461­466, 1982.

32

´ J. MARI, A. DAHLEN, AND A. LINDQUIST

37. H. Mhaskar and E. Saff. The distribution of zeros of asymptotically extremal polynomials. J. Approx. Theory, 3:279­300, 1991. 38. B. C. Moore. Singular value analysis of linear systems. In Proc. IEEE CDC, pages 66­73, 1978. 39. P. Nevai. Research problems in orthogonal polynomials. In C. Chui, L. Schumaker, and J. Ward, editors, Approximation Theory VI, 1989. 40. E. Nikishin and V. Sorokin. Rational approximations and orthogonality. In Translations of Mathematical Monographs, volume 92, 1991. 41. P. Van Overschee. Subspace Identification, Theory - Implementation - Application. PhD thesis, Katholieke Universiteit Leuven, 1995. Kluwer book with same title by Van Overschee and De Moor. 42. P. Van Overschee and B. De Moor. Subspace algorithms for the stochastic identification problem. In Proc. 30th Conference on Decision and Control, Brighton, 1991. 43. P. Van Overschee and B. De Moor. Subspace Identification for Linear Systems - Theory Implementation Applications. Kluwer Academic Publishers, 1996. 44. B. Porat. Digital Processing of Random Signals, Theory & Methods. Prentice Hall, 1994. ¨ 45. I. Schur. Uber Potenzreihen, die im Innern des Einheitskreises beschr¨ ankt sind. J. f¨ ur die Reine und Angewandte Mathematik, 147:205­232, 1917. 46. J. Sorelius, T. S¨ oderstr¨ om, P. Stoica, and M. Cedervall. Order estimation method for subspacebased system identification. In Proc. SYSID '97, 1997. 47. G. Szeg¨ o. Beitr¨ age zur Theorie der Toeplitzschen Formen, I, II. Mathematische Zeitschrift, 6:167­202, 1920. ¨ 48. G. Szeg¨ o. Uber die Randwerte analytischer Funktionen. Mat. Annalen, 84:232­244, 1921. 49. G. Szeg¨ o. Orthogonal Polynomials. American Mathematical Society, Colloqium Publications, 1939 (4th edition 1975). 50. B. Wahlberg. On the Identification and Approximation of Linear Systems. PhD thesis, Link¨ oping University, 1987. Link¨ oping Studies in Science and technology. Dissertations No. 163. 51. B. Wahlberg. Estimation of autoregressive moving-average models via high-order autoregressive approximations. Journal of Time Series Analysis, 10:283­299, 1989. 52. B. Wahlberg and L. Ljung. Hard frequency-domain model error bounds from least-squares like identification techniques. IEEE Trans. Automatic Control, 37:900­912, 1992. 53. W. Wang and M. G. Safonov. A tighter relative error bound for balanced stochastic truncation. Systems and Control Letters, 14:307­317, 1990. 54. P. Whittle. Estimation and information in stationary time series. Ark. Mat. Astr. Fys., 2:423­ 434, 1953. 55. H. Wold. A study in the analysis of Stationary Time Series. Almqvist and Wiksell, 1938. 56. N. Young. A Introduction to Hilbert Space. Cambridge University Press, 1988.

1688

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Universal Regulators for Optimal Tracking in Discrete-Time Systems Affected by Harmonic Disturbances
Anders Lindquist, Fellow, IEEE, and Vladimir A. Yakubovich, Member, IEEE
Abstract-- The authors consider the problem of controlling a discrete-time linear system by output feedback so as to have a second output z t track an observed reference signal r t . First, as a preliminary, we consider the problem of asymptotic tracking, i.e., to design a regulator such that jz t 0 rt j ! 0. This problem has been studied intensely in the literature, mainly in the continuoustime case. It is known that only under very special conditions does there exist a linear regulator which achieves this design goal and which is universal in the sense that it works for all reference signals and does not depend on them. On the other hand, if rt is a harmonic signal with known frequencies but with unknown amplitudes and phases, there exist such regulators under mild conditions, provided the dimension of rt is no larger than the number of controls. This is true even if the plant itself is corrupted by an unobserved additive harmonic disturbance wt of the same type as rt , if the dimension of wt is no larger than the number of outputs available for feedback control. However, if the first dimensionality condition is not satisfied, asymptotic tracking is not possible, but a steady-state tracking error remains. Therefore, the authors turn to another approach to the tracking problem, which also allows for damping of other system and control variables, and this is our main result. The measure of performance is given by a natural quadratic cost function. The object is to design an optimal regulator which is universal in the sense that it does not depend on the unknown amplitudes and phases of rt and wt and is optimal for all choices of rt and wt . The authors prove that an optimal universal regulator exists in a wide class of stabilizing and possibly nonlinear regulators under natural technical conditions and that this regulator is in fact linear, provided that the second dimensionality condition above is satisfied. On the other hand, if it is not satisfied, the existence of an optimal universal regulator is not a generic property, so as a rule no optimal universal regulator exists. The authors provide complete solutions of all the problems described above. Index Terms-- Internal model principle, optimal tracking, optimal universal regulators, sinusoidal disturbance.

Fig. 1. Feedback configuration.

(1b) (1c) , two vector outputs and , with a state and an and two vector inputs, namely a control which we shall take to be unobserved disturbance harmonic with known frequencies but unknown amplitudes and phases. More precisely (2) where the frequencies (3) , , are known, but the complex vector amplitudes , , in which the phases have been absorbed, are either completely unknown or zero. Consequently, some frequencies and have been included for (3) may not be represented in notational purposes to be explained shortly. In this paper we consider the problem to control the system (1) by feedback from the output so as to have the output track an observed -dimensional real reference signal (4)

I. INTRODUCTION

C

ONSIDER a discrete-time linear control system (1a)

Manuscript received January 4, 1996; revised February 1, 1997 and April 6, 1998. Recommended by Associate Editor, S. Weiland. This work was supported in part by grants from NFR, INTAS, and NUTEK. A. Lindquist is with the Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden. V. A. Yakubovich is with the Department of Mathematics and Mechanics, St. Petersburg University, St. Petersburg 198904, Russia. Publisher Item Identifier S 0018-9286(99)07132-9.

which is harmonic with the known frequencies (3) but with , , , which are complex vector amplitudes either completely unknown or zero so that certain frequencies (3) may not occur in . The feedback configuration of this problem is described in the flow diagram as shown in Fig. 1. Many important engineering problems could be modeled in this way. Some examples are connected to industrial machines and helicopters [2], [9]­[12], [27], [28], control of aircraft in

0018­9286/99$10.00 © 1999 IEEE

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1689

the presence of wind shear [19], [23], [31], and control of the roll motion of a ship [14]. For notational convenience we use a common set of freand , forcing us to set certain complex quencies (3) for vector amplitudes equal to zero. To formalize this we introduce , of for which the index sets and , respectively, are nonzero and arbitrary. Then and Without loss of generality we assume that (5)

satisfies the weak stability condition as (11)

Accordingly, we define the class of disturbances and the and , class of reference signals consisting of all signals and respectively, obtained by letting vary arbitrarily subject to the constraint that the signals (5) are real. , and are constant We assume that , , , , real matrices of appropriate dimensions such that is stabilizable and is detectable. Without loss of generality we may also assume that and (6)

In fact, if the first condition is not satisfied, some components of could be eliminated. Moreover, if has linearly dependent columns, these could be combined without restriction. and . Clearly, (6) implies that Now, a possible criterion of performance for the tracking problem described above is given by (7) but, to allow for damping of internal system variables and the energy of control, we shall also consider a more general criterion of the type (8) where is a real quadratic form (9) with properties to be specified in Section V. [To ensure that the , we must of course introduce some infimum of is not condition on the quadratic form (9).] We note that the second functional (8) becomes a measure not only of the tracking accuracy but also of the forced oscillations in the closed-loop system. For the classes of admissible regulators to be defined next, these cost functions do not depend on initial conditions. , a regulator The object is to find, for suitable , (10) which is: sat1) stabilizing in the sense that any process isfying the closed-loop system equations (1), (10) also

2) optimal in the sense that the cost function (8) is minimized; 3) universal in the sense that it simultaneously solves the complete family of optimization problems corresponding to different values of the complex vector amplitudes and and thus does not depend on these amplitudes. Such a regulator will be referred to as an optimal universal regulator (OUR), and the class of regulators (10) satisfying conditions 1) and 2) will be denoted . The stability condition (11) may at first sight seem somewhat unnatural, but, as we shall see in Section VI, it is the natural mathematical condition for which statements of necessity defining the largest class and sufficiency can be made. Removing the last term of (8) related to tracking we obtain some special cases of this problem which were studied in [21] and in [22] for the cases of complete and incomplete state information, respectively. In this paper we show that, under suitable technical con, the problem stated above has ditions and provided a solution in , and this solution happens to be a linear stabilizing regulator of type (12) is the backward shift and , where , and are real matrix polynomials, of dimensions , , and , respectively, with the property that and and are proper rational functions so that the regulator is nonanticipatory in the sense does not depend on future values of and , in that the subclass of harmony with (10). We shall denote by such linear regulators. Existence of an OUR in the subclass itself can be established under somewhat milder technical is important. conditions. The dimensionality condition As in [22], it can be shown that if it fails, then the existence of an optimal universal regulator becomes a nongeneric property. It means that no optimal universal regulator exists from a . practical point of view if The cost function (7) would of course be minimized if we could control (1a) so that as (13) In fact, it would be zero. Therefore, asymptotic tracking appears as a special case in our analysis. This problem has been studied intensely in the literature, at least in the continuoustime case; see, e.g., [1], [4]­[8], [13], [16], and the references therein. The connection to this earlier work, developed in continuous time, is made evident by noting that the disturbance and reference signals (5) can be modeled as the output of a critically stable system

with

having all its eigenvalues on the unit circle.

1690

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Therefore, we begin by developing our optimization procedure in this well-known setting of asymptotic tracking, thereby obtaining alternative formulations in the discrete-time case. Using a very short and simple proof, we are able to give a complete solution to the problem of finding all universal tracking regulators, i.e., all regulators which achieve asymptotic tracking (13) for all values of the complex vector and and which do not amplitudes depend on these amplitudes. This will be done in Section IV. As a preliminary for this, and to set up notations, in Section III ), and we we first consider an undisturbed system ( characterize all regulators (12) achieving the design objective (13) for all reference signals , not only harmonic ones, and all initial conditions; we shall refer to this property as Tuniversal. The solution of this problem is certainly known, but we include it for conceptual reasons. , i.e., the dimension of is larger than However, if the number of outputs available for feedback, no universal tracking regulator exists, so a nonzero tracking error remains. To damp this error we turn to our main problem, namely to characterize all optimal universal regulators, as defined above. Also, we may want to use a criterion (8) even if asymptotic tracing is possible, if it is desirable to damp the control energy and/or some particular internal system variables. This is the topic of Section V, where optimality in the linear class is studied. In Section VI we show that these linear universal regulators are optimal also in the wider class of nonlinear regulators satisfying (11), provided slightly stronger technical conditions are satisfied. The complete solution is given. We note that a similar but different optimization problem, over a finite horizon, is considered in [26]. Obviously, there is no a priori guarantee that a regulator which minimizes (8) will also satisfy other design specifications, and hence we look for complete solutions with many free parameters which then can be tuned by loop shaping. In fact, all our results are based on a parameterization derived in Section II, which is akin to that of Youla and Ku cera and which generalizes some parameterizations previously presented in [21] and [22]. Finally, in Section VII, we give some simple numerical examples. II. LINEAR STABILIZING AND REALIZABLE REGULATORS In order to design universal regulators we need a parameterization of all linear regulators (14) which stabilize the control system (1) and which are realizable in a sense to be defined shortly. As before, is the backward , and , , and are real matrix shift , , and , respectively. polynomials of dimensions Let us consider a bit closer the meaning of (14) being , , stabilizing. To this end, note that the transfer functions from to , , and , respectively, in the closed-loop system (1), (14) satisfy (15a)

(15b) (15c) so, in particular, (16) where is the matrix polynomial (17) Similarly, the transfer functions respectively, are given by , from to and ,

(18) which together with (16) yields (19) We shall say that the regulator (14) is stabilizing if the matrix is stable, i.e., for . polynomial Next we consider the condition that the regulator be realizable. Clearly (14) must be nonanticipatory in the sense that does not depend on future values of and . To ensure this, we must assume that and are proper (20)

. requiring in particular that Let us investigate what properties must have for (20) to be satisfied. To this end, let us introduce the rational transfer functions

(21) from the control signal to the outputs Then it is easy to see that and , respectively.

(22) and that

(23) Writing (22) in the alternative form

we see that (20) implies that is strictly proper and is proper. In fact, is strictly proper, making as well as its inverse proper. Then, it follows from and are strictly proper also. Consequently (23) that where is finite (24)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1691

so that and depend on for only and on for only. We shall say that the regulator (14) is realizable if condition (24) is satisfied. At the end of this section we shall demonstrate that any stabilizing and realizable regulator satisfies (20) so that the nonanticipatory property is implied (Corollary 2.3). We say that two regulators

is stabilizing and realizable, and for this regulator (30) and (31) is given by (17). Conversely, any stabilizing and where realizable regulator (28) is equivalent to one constructed in this way. Before turning to the proof of this parameterization, let us is a briefly explain the nature of relation (31). Although for the regulator defined via (29), this is in factor in general not the case for an arbitrary regulator belonging the same equivalence class. In fact, while the closed-loop transfer and the regulator transfer functions and function are invariant under the equivalence (25), is not. Taking the Schur complement, it immediately follows from (17) that (32) is given by (21). Since, in general, the second factor where in is not a polynomial, is of course not a factor in general. Nevertheless, it will turn out to be useful to represent each equivalence class by a regulator that has this property. Proof of Theorem 2.1: In view of (29), we have (33) and consequently (30) follows from (22) and (31) follows from is a stable matrix poly(32). By construction, therefore, nomial, establishing that the regulator is stabilizing. Moreover, is strictly proper and is proper, i.e., in view of (27), and is finite. It then follows from (23) and are strictly proper, and hence the regulator that is realizable. , , To prove the converse statement, suppose that is an arbitrary stabilizing and realizable regulator. Then (32) may be written

and

are equivalent if there are stable and such that

matrix polynomials (25)

Hence we allow the systems matrices , , and to have stable common factors, as coprimeness is not required. Clearly, , , , and as can be seen from (22) and (23), are invariant under this equivalence and so are the regulator transfer functions (20). is a stable matrix, i.e., From now on, we assume that for all . Since is stabilizable is detectable, this is no restriction. In fact, it is and well-known that the system (1) can be replaced by a similar system having a stable -matrix but, in general, a larger dimension. (See any standard text, such as [1] and [18].) Only under special conditions [15], including the case of complete state observation, is it possible to do this by constant feedback, but the system can always be stabilized by a dynamic observer. Then, extending the state space by including this observer, a system with stable -matrix is obtained. For these reasons we shall from now on, without loss of generality, assume that in (1) is a stable matrix. The following theorem, generalizing a similar result in [22], provides a parameterization akin to the well-known Youla­Ku cera parameterization. (We note that if is not stable, also the latter parameterization requires an observer-based prestabilization, increasing the dimension of the regulator; see, e.g., [32, p. 226].) be a stable matrix with Theorem 2.1: Let being its characteristic polynomial, and let and be the matrix polynomials (26) be an arbitrary stable scalar polynomial Moreover, let and be arbitrary matrix polynomials of and let and , respectively, such that dimensions (27) Then the regulator (28) with

where

is the

matrix polynomial (34)

which is stable and full rank, since stable and nontrivial. It follows from (22) that

is (35)

and are the closed-loop transfer functions where , , . Therefore, setting corresponding to the regulator

and

(29)

where , (35) shows that

is the adjoint matrix polynomial of and are given by (30). Since ,

1692

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

, is a realizable regulator, it follows from (24) that the and degree conditions (27) hold. Consequently, defining via (29), it follows from the first part of the theorem that , , is a stabilizing and realizable regulator with the and as , , same closed-loop transfer functions . It remains to show that , , and , , are equivalent. To this end, note that

Also it follows from (34) that

Consequently

i.e.,

and

are equivalent as required.

If so that , the representation of stabilizing regulators can be simplified considerably, since and can be chosen so that cancellations occur. Since this formulation has a different form and, moreover, will be used later, we state it as a corollary. Note that, in view of the converse statement, this corollary is strictly speaking not a special case of Theorem 2.1. It is in fact a generalization of [21, Lemma 4.3], but the proof here is new. Corollary 2.2: Let be a stable matrix, and suppose that . Let be an arbitrary real scalar stable polynomial, and be arbitrary real matrix polynomials, and let and , respectively, such that of dimensions (36) Then the regulator (37) with (38) is stabilizing and realizable, and, for this regulator (39) satisfies (31). Conversely, any stabilizing and and realizable regulator (37) is equivalent to one constructed in this way. Proof: Let the polynomials and be chosen as in the and statement of the corollary, and take to be the corresponding polynomials and in Theorem 2.1. Then, since , the degree conditions (27) are satisfied for and . Moreover, the corresponding regulator polynomials matrices and , become (29), which we denote and , where and are given by (38). Then, , the regulator , , is stabilizing and setting realizable by Theorem 2.1. Thanks to cancellation, therefore, is a stabilizing and realizable regulator for the problem of Corollary 2.2, as claimed.

Conversely, by Theorem 2.1, any stabilizing and realizable regulator (37) is equivalent to some regulator of the type described in Theorem 2.1, where we set everywhere. It remains to show that is also a regulator of the type described in the corollary. To this end, . This implies that , define and hence the equations of Theorem 2.1 become those of the replaced by . Hence is also a corollary with regulator in the sense of the corollary. In the beginning of this section we demonstrated that the realizability condition (24) is a consequence of nonanticipatory condition (20). Next we show that the converse is also true, has full rank as assumed in (6). provided . Then, for any Corollary 2.3: Suppose that stabilizing regulator (28), the realizability condition (24) and the nonanticipatory condition (20) are equivalent. Proof: The proof is immediate in the special case . In fact, for a regulator (37) with and given by (38), condition (20) is a direct consequence of the degree condition (36). For any other stabilizing regulator (37), it follows from the definition of equivalence. The general case follows from the fact that (28) is a subclass of (37). In fact, writing (28) as

it follows from what has already been proved that is proper. Since has full rank, this implies that is proper follows directly. proper. That III. -UNIVERSAL REGULATORS

is

As a preliminary for the analysis in Sections IV and V, in this section we consider the problem of controlling the undisturbed system (40a) (40b) (40c) so that it tracks a given by feedback from the output in the sense that reference signal as (41)

As explained in Section II it is no restriction to assume that is stable if it is assumed that is stabilizable and is detectable. The solution of this problem is simple and certainly known, but we include it for completeness and for conceptual reasons. More precisely, we want to find a stabilizing and realizable regulator of the form (42) which is universal for the asymptotic tracking problem in the sense that (41) holds for all solutions of (40), (42), and all reference signals . More specifically we shall refer to this property as T-universal. Clearly, for (42) to be stabilizing and realizable, the matrix , , and must satisfy the specifipolynomials cations of Theorem 2.1. It remains to investigate under what

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1693

conditions the tracking criterion (41) is satisfied and under what conditions this regulator is T-universal. We begin by deriving a necessary condition for Tuniversality. Consider a reference signal of the type (43) and are fixed but arbitrary. Then the where closed-loop system (40), (42) has solutions

delays between and . Indeed, the condition (48) for Tuniversality imposes some rather stringent conditions on the is and is , system (40). In particular, since , and must have full rank. (48) implies that Theorem 3.2: Suppose that is stable. Then there exists a T-universal regulator for the tracking problem if and only if matrix function with no there is a proper rational which satisfies the equation poles in the region (49)

(44) with (45) , where are defined by (21). Moreover, , and and (46) But the tracking condition (41) requires that as and, since is arbitrary, this implies that follows from (44) and (46) that . Therefore, it (47) Now, in order for the regulator (42) be T-universal, (47) must hold for all , that is, for all and . Consequently, we must have (48) on the unit circle and, by analytic continuation, in the rest of the complex plane. Lemma 3.1: A stabilizing and realizable regulator (42) is T-universal if and only if the identity (48) holds. Proof: We have already proved that (48) is a necessary condition for (42) to be T-universal, so it remains to prove that it is also sufficient. To this end, first assume that there are such that for all . Then positive numbers , has a -transform

which, in particular, implies that In this case, let be a stable scalar polynomial such that

. (50)

be a matrix is a matrix polynomial, and let polynomial satisfying the first degree constraint (27). Then, the and given by (29), is a T-universal regulator (28), with regulator for the tracking problem, and any other T-universal regulator is equivalent to one obtained in this way. Proof: First, suppose that there exists a T-universal regulator of the form (42). Then, according to Lemma 3.1, there to (49) with the prescribed properties, exists a solution . In fact, in view of (22), (32) and the fact that namely is stable, it follows that has no poles in the region . Moreover, since the regulator is realizable, is proper. which is proper Next, suppose that (49) has a solution , and let , , and be with no poles in the region defined as in the theorem. [Note that in order to satisfy the first of degree conditions (27) we may need to choose and which are not coprime.] Then, by Theorem 2.1, the regulator given by (29) is stabilizing and realizable and (28) with (51) . Consequently, it follows from i.e., in view of (50), (49) and Lemma 3.1 that the regulator is T-universal. It remains to prove the last statement of the theorem. To this end, suppose that the regulator (52) is T-universal. Then, in particular, it is stabilizing and realizable, and thus, by Theorem 2.1, there are some , , and with the properties specified in Theorem 2.1 such that the given by (29) is equivalent to (52). regulator (28) with is invariant under this equivalence. Therefore, since Now, (48) holds for the regulator (52) by Lemma 3.1, (48) also holds for (28). However, by Theorem 2.1, (51) holds, and hence there , satisfying (49) and (50). is an , namely In general, a solution to (49) cannot be expected to be , only one solution is possible, namely unique, but if

which converges for . It follows from (45) and is the transfer function from to , (46) that with a -transform and hence (40), (42) has a solution . But, if (48) holds, then and hence for all . Because of stability any other solution tends asymptotically to this solution, and therefore (41) holds. If increases so fast that it does not have a -transform, set for and for , and be the corresponding -solution. Then it is easy to see let for . Since is arbitrary, the that conclusion follows. As a corollary we see that must be full rank, or else (48) will be violated. This implies that there are no

and this would require that is a stable, proper rational must be minimum phase with function, implying that must be no zeros at infinity. In particular, nonsingular.

1694

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Corollary 3.3: Suppose that is stable and the transfer is square, i.e., . function Then there is a T-universal regulator for the tracking problem is proper with no poles in the region if and only if . In this case, let be a stable scalar polynomial is a matrix polynomial and is such that matrix polynomial satisfying the degree requirement a and are defined by (29) and by (27). Then, if (53) the regulator (28) is a T-universal regulator, and any other T-universal regulator is equivalent to one obtained in this way. A T-universal regulator exists only under rather special conditions. However, if we restrict our attention to harmonic reference signals (4), these conditions can be considerably relaxed and we may also allow for external harmonic disturbances. This is the topic of the next section. IV. UNIVERSAL TRACKING REGULATORS IN HARMONICALLY DISTURBED SYSTEMS We now return to the situation described in Section I, where the control system takes the form (1) with a harmonic disturbance (2) and where there is a harmonic reference signal to be empty, for (4). Although we may allow the index set . tracking we must take The first question to be answered is when it is possible to find a regulator (12) in such that as (54)

respectively, satisfying the degree requirements (27) and the interpolation conditions for for (59a) (59b)

and are given by (29), the regulator (28) is a Then, if universal tracking regulator, and any other universal tracking regulator (28) is equivalent to one obtained in this way. Proof: Whenever a linear stabilizing regulator is applied tends exponentially to the to system (1), the process harmonic steady-state solution (60) where (61a) (61b) , , , and being the closed-loop transfer functions , defined in Section II. In fact, for any regulator in , defined by (17), is a stable matrix polynomial. In the same tends exponentially to way, in view of (1c),

(62) Now, the basic idea is that the tracking condition (54) is achieved precisely when the cost function (7) is zero. It is easy to see that (63) To see this, observe that if sequences and are two harmonic vector

which is universal in the sense that (54) holds for all values and and does not depend on these of vector amplitudes. We shall refer to such a regulator as a universal tracking regulator. For convenience, in the sequel we use the notation (55) is stable, and let Theorem 4.1: Suppose that the matrix and be the matrix polynomials defined by (26). be the matrix function defined by Moreover, let the matrix polynomial (21) and (56) Then, for a universal tracking regulator to exist in necessary that the rank condition for all , it is (57)

and with distinct as in (3), and appropriate dimensions, then is an arbitrary matrix of

holds, and it is sufficient that both rank conditions (57) and for all (58) Moreover, in view of (61b) and (62)

(64)

, and hold. In particular, (57) requires that . More precisely, let be (58) that and an arbitrary stable scalar real polynomial, and let be matrix polynomials, of dimensions and ,

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1695

and consequently (63) equals zero for all values of and if and only if for for (65a) (65b)

Remark 4.3--Internal Model Principle: The situation most often studied in the literature is when , i.e., , , and , and when the regulator (28) takes the form

Theorem 2.1 states that the regulator (28) is stabilizing if and are defined by (29) for some stable scalar real and some real matrix polynomials polynomial and satisfying (27) and that any other stabilizing and realizable regulator (28) is equivalent to one obtained in this way. Moreover (66) which inserted into (65) yields precisely (59). If the rank conditions (57) and (58) hold, the interpolation conditions (59) have a solution, and the general solution is

obtained by setting . We assume that the rank . conditions (57) and (58) are satisfied so that For robustness it is desirable to include a model of the disturbance dynamics in the regulator. This is the internal model principle. Following [3], we replace the matrix fracby the (reachable) matrix fraction tion representation so that . The harmonic representation dynamics is then included in the regulator dynamics by setting , where and is a stable matrix polynomial. Then, by (29)

which, in view of the fact that for for where, for , and are arbitrary matrices and . Here such that the degree of the stable polynomial is chosen sufficiently high to satisfy the degree constraints (67). On the other hand, the rank condition (57) is also necessary for the existence of a is stable, (65b) universal tracking regulator. In fact, since for some . cannot hold if Remark 4.2: The two rank conditions (57) and (58) in Theorem 4.1, which of course can be stated in terms of zeros of certain transfer functions, have different status. If (57) is violated, the interpolation condition (59b) cannot hold, so there could be no universal tracking regulator. On the other hand, if (57) holds but (58) does not, interpolation condition (59a) could still be valid, as the rank of the right member could be less than . However, this is a nongeneric situation, and hence it cannot be expected to occur in practice. In fact, if and , the following equation must hold:

, yields

where we have assumed that has no zeros in the points . (Otherwise we include a simple feedback loop to clearly satisfy the interpolation move the zeros.) These , and conditions (59). In fact, since , by (29), these can be written for for Consequently, we see that the internal-model-principle regulators form a subclass of the ones considered above. , which The rank condition (58) becomes void if is equivalent to the case with complete state information, i.e., . Then the formulas for the regulator the case when also simplify considerably. so that . Theorem 4.4: Suppose that Moreover, suppose that is stable and that condition (6) holds. Then, there exists a universal tracking regulator (37) in if and only if the rank condition (57) holds. In fact, let be a stable scalar real polynomial, and let and be matrix polynomials satisfying the degree constraints (36) and the interpolation conditions for for (67a) (67b)

which will occur only on a lower-dimensional algebraic set in the parameter space. Theorem 4.1 provides a complete solution of a problem studied in various degrees of generality in [4]­[8], [13], [16] and of course is consistent with the solutions given there, although given in a different form and in continuous time. , rank condition (58) becomes void and only (57), a If considerably weaker version of condition (49) in Section III, remains. Hence, for universal tracking regulators to exist the is necessary, and if there are external condition , in practice, we must also have . disturbances Consequently, as also noted in [4], [7], [8], [13], and [16], asymptotic tracking is only possible under certain specific conditions.

and are given by (38), the regulator (37) is a Then, if universal tracking regulator, and any other universal tracking regulator (37) is equivalent to one obtained in this way.

1696

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof: The proof follows the same lines as that of Theorem 4.1, except that (39) from Corollary 2.2 is used in , exists and (67a) lieu of (66). Since can be solved. When , there are no universal tracking regulators, and in order to damp the steady-state tracking error we shall therefore next turn to an optimization procedure. This is the topic of the next section. V. LINEAR QUADRATIC OPTIMIZATION FOR TRACKING AND DAMPING We now return to the optimization problem stated in Section I. In this section we consider only linear regulators. Later, in Section VI, we demonstrate that under slightly stronger technical conditions the optimal universal regulators presented here are actually optimal in the much larger class , which includes nonlinear regulators. Let us recall that the problem under consideration is to control the disturbed system (1) by feedback from the output so as to minimize the cost function (68) is the quadratic form defined by (9). Hence, where we may not only want to damp the tracking error, but also some internal systems variables. As before, both the disturand the reference signal are harmonic and given by bance (5), where only the frequencies are known. The optimization is performed over the class of stabilizing and realizable linear regulators (12). The problem under consideration is: 1) to find the conditions under which there are optimal regulators which are universal in the sense that they are optimal for all choices of the amplitudes of (5) and independent of these and 2) to characterize the class of all such universal optimal regulators. To address this problem, let us first take a closer look at the cost function (68). A straightforward reformulation taking (1c) into consideration yields

for all

,

satisfying (73)

such that . It can be shown [21] for all that if this condition fails in a strong way, i.e., there are , , and , , such that , then there such that . In is an external disturbance this section, however, we shall only need the weak frequency , , , domain condition that (72) and (73) hold for , defined as in (55). Both of these conditions are invariant under the action of the feedback group

where is a nonsingular matrix and is an arbitrary matrix of appropriate dimensions. Moreover, since has no eigenvalues on the unit circle, the inverse (74) exists for all on the unit circle, and hence where is the Hermitian that matrix function so

(75) In this notation the strong frequency domain condition may be written for all and the weak one as for (77) on the unit circle (76)

We now state the main result of this section. It will be strengthened in Section VI, where we show that under mild is technical conditions the optimal universal regulator in also optimal in the wider class . , , and be the matrix Theorem 5.1: Let polynomials defined by (26) and (56). Suppose that the matrix is stable and that the weak frequency domain condition (77) holds, and suppose that for all (78)

(69) where is the real quadratic form (70) with the real matrices , , and given by

(71) The quadratic form (70) need not be nonnegative definite but must of course satisfy some condition ensuring that . As we shall see, a sufficient condition for this is the strong frequency domain condition, i.e., that there is a such that (72)

. Then, i.e., in particular that which is universal in there exists an optimal regulator in and the sense that it is optimal for all values of and does not depend on these vector amplitudes. be an arbitrary stable scalar real More precisely, let and be matrix polynomials of polynomial, and let and , respectively, satisfying the degree dimensions requirements (27) and the interpolation conditions for for with and given by (80) (79a) (79b)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1697

where . Then the regulator (28) is an and universal regulator, which is optimal in , provided are given by (29) and any other universal regulator (28), which is optimal in , is equivalent to one obtained in this way. is nonsingular for Since, by assumption, , (79a) has the solution

(60) of . In fact, we have the following lemma. The proof follows from a simple completion-of-squares argument and is deferred to Appendix A. be any solution to the closed-loop Lemma 5.5: Let system (1), (12), where (12) is a stabilizing and realizable regulator, and suppose that the weak frequency domain condition (77) holds. Then the cost function (68) exists as a usual limit, and it takes the value (82)

(81) , and these are precisely all solutions of (79a). for and Clearly, there are always matrix polynomials satisfying (81), (79b) and the degree constraints (27), provided is chosen the degree of the stable scalar polynomial sufficiently large. , there exist optimal regulators, but, Remark 5.2: If as explained in Remark 4.2, universality is not a generic property; therefore, for all practical purposes, there are no . optimal universal regulators if Remark 5.3: Before proceeding to the proof of Theorem 5.1, let us make certain that it is consistent with the results of Section IV. To this end, let us consider a cost function (7), . Then i.e., suppose that

where, for (83) with and given by (80) and by (84)

where where the matrix function is given by (21). If , the weak frequency domain condition cannot hold, so Theorem 5.1 does not apply. Instead, Theorem 4.1 should , the weak frequency domain condition is be used. If a consequence of condition (57), and it is easy to check that the optimal cost will be zero, as required by Theorem 4.1. Moreover, interpolation conditions (59) and (79) are identical. , no universal tracking regulator exists by Finally, if Theorem 4.1, and the optimal cost will be nonzero in general. Remark 5.4--Generalized Internal Model Principle: As in , so that Remark 4.3, let us consider the case when , , , and , and in the regulator (28). For simplicity, also assume that . and and , the interpolation If conditions (79) can be written

(85) , In the expression (82) for the cost function , only , , depend on the regulator to be chosen. They are defined by (61b), i.e., (86) of external disturbances Recall that we consider the class for and for with arbitrary and the class of reference signals with for and for . Consequently, if we could find a stabilizing and realizsatisfy the able regulator (12) such that optimality conditions (87)

for , as can be seen from (29), (80), and , , and . All of these the fact that interpolation conditions are satisfied if the second set is, and in this case (29) implies that

which, in view of (86), is the same as (88) then this regulator would be optimal. If, in addition, this regu, lator does not depend on the amplitudes and , and the conditions (88) hold for all and , i.e., all disturbances in and all reference signals in , then this optimal regulator is also universal. This condition holds if and only if for for (89a) (89b)

which could be interpreted as a generalized internal model principle for the optimization problem. The basic idea behind the proof of Theorem 5.1 is, as for Theorem 4.1, that whenever a linear stabilizing regulator is tends exponenapplied to the system (1), the process tially to the harmonic steady-state solution (60). Therefore, the cost function (68) depends only on the harmonic component

1698

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Proof of Theorem 5.1: Theorem 2.1 states that the reguand are defined by (29) for lator (28) is stabilizing if and some real matrix some stable scalar real polynomial satisfying (27), and that any other stabilizing polynomial and realizable regulator (28) is equivalent to one obtained in this way. Moreover (90) We have demonstrated above that (89) is a necessary condition for the regulator (28) to be an optimal universal regulator, and inserting (90) into (89) yields precisely (79). Clearly, as we have already discussed, there are always matrix polynomials and satisfying these conditions and the degree constraints (27), provided the degree of the stable scalar is chosen sufficiently large, and provided polynomial condition (78) is satisfied. It remains to prove the converse statement. For any optimal , the value of the cost function universal regulator , defined by (84). It follows from (82) and the (68) equals , for , that (87) holds for fact that , and . Therefore, (89) follows from all is equivalent to (88). By Theorem 2.1, the regulator given by (29) for some satisfying the (28) with requirements of Theorem 5.1. This regulator is also optimal since equivalent regulators have the same cost . It is also does not depend on universal because and . Corollary 5.6: The optimal value of the cost function (68) , defined by (82) and (83). in the class is Note that, although an optimal universal regulator will not and , the cost function (84) depend on will. In the special case of complete state information, i.e., , condition (78) is always satisfied. In view of Corollary 2.2, Theorem 5.1 can be considerably simplified in this case, so we state it separately. The proof is the same as for Theorem 5.1, except that we now use the equations of Corollary 2.2. so that . Theorem 5.7: Suppose that Moreover, suppose that is stable and that condition (6) holds. Then, if the weak frequency domain condition (77) holds, there exists a universal regulator (37), which is optimal in . In fact, let be a stable scalar real polynomial, and let and be matrix polynomials satisfying the degree constraints (36) and the interpolation conditions for for (91a) (91b)

Since, by assumption, is a nonsingular matrix of , (91a) has the solution dimension

(92) . There are always matrix polynomials and satisfying (92), (91b), and the degree constraints (36), is provided the degree of the stable scalar polynomial chosen sufficiently large. for VI. OPTIMALITY IN THE CLASS OF NONLINEAR REGULATORS In this section we show that the universal optimal linear regulators described in Theorems 5.1 and 5.7 are actually optimal in a wide class of nonlinear regulators. We now define this class. of Given the control system (1), consider the class nonlinear regulators (93) of which is stabilizing in the sense that any solution the closed-loop system consisting of (1) and (93) satisfies the condition as (94)

This stability condition is quite weak but will suffice for our purposes. Of course, a weaker condition has the advantage of allowing for a larger class of controls. We consider the same problem as in Section V, except that . we now optimize over all regulators in . Clearly, The only price we have to pay for this generalization is that the weak frequency domain condition needs to be replaced by the strong one. be stable, and suppose that the rank Theorem 6.1: Let condition (78) holds. Then, if the strong frequency domain condition (76) holds, the linear optimal universal regulators of Theorem 5.1 are optimal in the class . It turns out that Theorem 6.1 is a simple consequence of the corresponding result for complete state information. In fact, the class of stabilizing and realizable regulators with is a subclass of the class of stabilizing and realizable regulators

and are where and are defined as in (80). Then, if given by (38), the regulator (37) is a universal regulator, which is optimal in . Conversely, any other universal regulator (37), which is optimal in , is equivalent to one obtained in this way. Finally, the optimal value of the cost function (68) is given by (84).

in that only a special structure of is required. But, as seen in Section V, an optimal universal regulator in the former class is optimal also in the latter, since the same optimal value is achieved (Corollary 5.6 and Theorem 5.7). (The only difference between the cases of complete and incomplete state information is that a higher degree regulator may be required in the latter case to achieve the optimum.) Consequently, if we can prove the following theorem, we have also proved Theorem 6.1.

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1699

Theorem 6.2: Let be stable, and suppose that and that . Then, if the strong frequency domain condition (76) holds, the linear optimal universal regulators of Theorem 5.7 are optimal in the class . In order to prove this theorem we consider an optimization problem which unlike that in Section V does not require that a linear regulator has been applied. More precisely, let us first consider the problem of finding a process which minimizes the cost function (8), subject to the constraints (94) and (95) and are arbitrary bounded and where now complex-valued vector sequences. It is well known (see, e.g., [20], [21], [24], [25], and [29]) that if the strong frequency domain condition (76) holds and is stabilizable, then the algebraic Riccati equation

The optimal value of the cost function is (103) where

(104) exists, any optimal process If the limit is produced in this way. Note that the control (101) cannot in general be used in and . Even practice, since it depends on future values of in the harmonic case when this dependence can be resolved, this control law has serious disadvantages [21, Sec. III]. It is developed here as an instrument of proof. Next, let us return to our original problem and take and to be harmonic, given by (5). Then a simple calculation, using (99) and (100), yields the representation with where (105)

(96) has a unique symmetric solution matrix where (97) stable in the sense that all eigenvalues of lie strictly inside the unit circle. We shall refer to this solution as the stabilizing solution of (96). For this solution we also have that (98) is positive definite.1 Then we have the following result, which should be compared to [21, Th. 2.3], the proof of which we defer to Appendix B. be stabilizable and suppose that Lemma 6.3: Let the strong frequency domain condition (76) holds so that (96) has a stabilizing solution . Moreover, let (99) where (100) Then the problem to minimize the cost function (8) subject to constraints (94) and (95) is solved by a process such that (101) where is given by (97) and such that is any vector sequence is optimal in . Therefore, the optimal linear regulators of Theorem 2.1 must be optimal also in . of the closedSince (108) is stabilizing, the solution loop system (107), (108) tends exponentially to a harmonic solution We want to find a stabilizing and realizable regulator (108) so that the closed-loop system (106)­(108) has a solution satisfying (101) for some with the property (102). Then, by Lemma 6.3, the regulator (106), (108), i.e., (109) which renders the feedback

We are now in a position to prove Theorem 6.1. Proof of Theorem 6.1: Clearly, for any regulator in , (103) is a lower bound for the cost . Therefore, if we can demonstrate that there is a regulator in which achieves the same value (103) of the cost , this regulator must be optimal also in , and so must all regulators which are optimal in . so that To this end, let us introduce a new control (106) transforming the system (1a) to (107)

(102)
1 Note that there is a misprint in [21, p. 788]: In Theorem 2.1, replace "statements hold" for "statements are equivalent."

1700

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

which of course yields the same value to so that if we can choose for

as

. Now, (110)

The matrix polynomials (26) are and Let us first take a T-universal regulator and consider the problem to find (112) tends asymptotically to . By Corollary 3.3, a so that T-universal regulator exists if and only if and where stable (113)

, then has the property (102) and hence and (106) becomes (101) as required. such that (110) holds, we To show that there are first apply Corollary 2.2 to the system (107), where takes and that of . In fact, by Corollary 2.2, the place of there is a stable scalar polynomial and matrix polynomials such that and so that are given by and and

In fact, . In this case, (112) is a Tuniversal regulator if and only if However, tends exponentially to the harmonic solution Since therefore . (114) and such that is stable and for some polynomials or is equivalent to one obtained in this . Of course, way. This corresponds to the choice asymptotic tracking is achieved for all choices of reference signal . If, instead, we consider a reference signal (115) is full rank, in view of the discussion in Section V can be chosen to satisfy these interpolation conditions. VII. SOME SIMPLE NUMERICAL EXAMPLES To illustrate the results of this paper, let us consider the system (111) is the control, where characteristic polynomial and are outputs, and the Since where the frequencies , are given, but the amplitudes , and the phases , are unknown, the class of regulators (112) which achieve asymptotic tracking is much larger, and condition (113) need not be satisfied but can be exchanged for for (116)

and is given by (105), the optimality condition (110) will and if be satisfied for all

In fact, by Theorem 4.1, in this case we may choose any stabilizing regulator (117) provided is stable and the degree constraint (27) and the interpolation conditions for

is stable with

. Defining the state

the plant equations (111) can be written in state-space form (1), where

so that

is the characteristic polynomial of

, and

are satisfied. The same regulator is obtained by applying Theorem 5.1, now observing that (116) is the weak frequency domain condition; see Remark 5.3. This allows for more tuning parameters to satisfy other design specifications. Of course, if condition (113) is fulfilled, the T-universal regulator can still be used. , , and As a numerical example, suppose that , and let and . Then condition (113) is satisfied, so a T-universal regulator exists. Such a regulator and in is obtained by, for example, setting and and the initial conditions are (114). If , this yields the error depicted in Fig. 2. The

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1701

Fig. 2.

Fig. 3.

dashed line in the same figure is the tracking error obtained . by setting , while and remain the same. Next, let us take Then becomes unstable, so a T-universal regulator fails to exist. Although condition (113) fails, we could still obtain asymptotic tracking by using a universal tracking regulator, constructed as in Theorem 4.1, provided condition (116) holds, and we shall present a simulation for this case in the end of the section. We now add an harmonic disturbance (118) and , in the system (111), where , are given, but , are unknown. Suppose we want to determine an optimal universal regulator for the cost function (119)

straightforward calculation yields

for any on the unit circle. In order to construct an optimal universal regulator we need to choose a stable polynomial

of degree at least five. The parameters , , , , , as well as will be available for tuning in order to improve the overall design. Then, defining the real numbers , , , , , , , via for for it is easily seen that the polynomials

Since the matrices

,

, and

in (71) become will satisfy the interpolation conditions (79a) if and only if its coefficients satisfy the linear system of equations

a simple calculation yields

for (75), and therefore the strong frequency domain condition , so any optimal universal (76) is always satisfied if of possibly regulator (112) is optimal in the larger class , the nonlinear regulators described in Section VI. If strong frequency domain condition will fail if and only if the has a root on the unit circle, while the weak polynomial frequency condition (77) will still hold provided we avoid choosing any of the frequencies in (115) and (118) so that , , , or is such a root. Next, let us consider the interpolation condition (79). defined by (56) is identically one, and a Clearly,

Consequently, by Theorem 5.1, (117) is an optimal universal and are determined in this way. regulator if , , and For an example, take as before . Moreover, we choose a disturbance (118) with and , while the harmonic frequencies , reference signal (115) has the same frequencies as in the first simulation. In Fig. 3 we illustrate the tracking error of the optimal universal regulator corresponding to a polynomial with roots 0.3 0.3 , 0.3 0.2 , 0.5, and . The amplitudes in (115) and (118) have been taken

1702

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

Fig. 4.

to be , , and , and the initial . As before, the dashed line is conditions are . Remember the tracking error obtained by setting , the control energy is also damped, so that, since there is a certain tradeoff here. We remark that it is important to tune the free parameters to obtain good properties of the regulator. In particular, the transients, which do not affect the cost function, can change dramatically with different choices of free parameters. and instead, Now, setting while keeping all the other parameters the same, we obtain the errors in Fig. 4. As seen, the error goes asymptotically to zero, despite the fact that condition (113) is not fulfilled so that a T-universal regulator does not exist. In fact, by Theorem 4.1, this is a universal tracking regulator which exists since on the unit circle. In order to speed up the convergence, the roots of have been reset at 0.7 0.1 , 0.3 0.2 , and 0.8. Since now we do not have the disturbance and , we could choose another frequencies to possibly get a universal tracking regulator with a better transient. VIII. CONCLUSIONS In this paper we have given complete characterizations of regulators which satisfy certain tracking specifications and which are universal in the sense that they are independent of disturbances and tracking signals and apply regardless of the values of these. As a preliminary, we considered a problem of asymptotic tracking of an arbitrary signal , and we characterized all regulators which are universal with respect to the choice of . We showed that such universal regulators exist only under very special conditions. These conditions can be considerably relaxed if the reference signal is exchanged for a harmonic signal with known frequencies but unknown amplitudes and phases, and we want the regulator to be universal in the sense that it achieves asymptotic tracking for all choices of the of amplitudes and phases. Then, if the dimension

reference signal is no larger than the dimension of the control, such a regulator exists under mild conditions. This is in harmony with other results in the literature [4]­[8], [13], [16], where, however, the continuous-time case is considered. We provided complete solutions of these problems in discrete time, and our proof is considerably simpler. If the system is also corrupted by a harmonic disturbance , asymptotic tracking may still be possible provided the dimension of the disturbance is no larger than the dimension of the output available for feedback. However, if a certain , rank condition fails, which in particular is the case if asymptotic tracking is not possible, but a steady-state error will remain. Therefore, we considered next an optimal control problem to damp the steady-state tracking error, also giving the option to damp internal system variables. We characterized the class of all optimal regulators which are universal in the sense that they are optimal for all choices of the amplitudes of and . Such regulators were shown to exist if the weak . On the other frequency domain condition holds and , there are always algebraic conditions on the hand, if system parameters, implying that universality is not a generic property in this case. We have also shown that all optimal universal regulators can be chosen as linear even if the optimization is over a very large class of nonlinear regulators, provided the strong frequency domain condition holds. We have given complete characterizations of all linear optimal universal regulators in terms of parameterizations containing many free parameters. This allows for a considerable amount of design freedom, which can be used to satisfy other design specifications via loop shaping. Indeed, we stress that our solutions are optimal in the sense stated in this paper only, and that other desirable design specifications may not be satisfied for an arbitrary universal optimal regulator. APPENDIX A PROOF OF LEMMA 5.5 and tend exponentially to the harmonic comSince ponents (60), only these contribute to the cost function (70); consequently, the usual limit (rather than just limsup) does where exist in (69), and it is given by

(A1) . In fact, this follows from the argument for leading to (64). Now, in view of the constraint (1a) (A2) and therefore (A1) takes the form (A3) if the weak frequency domain condition (77) where is given by (85), and is fulfilled. Here (A4)

LINDQUIST AND YAKUBOVICH: UNIVERSAL REGULATORS FOR OPTIMAL TRACKING

1703

Therefore, assuming that the weak frequency domain condition for , we may (77) holds so that complete squares in (A3) to obtain (A5) where (A6) From this the equations of the lemma follow readily. APPENDIX B PROOF OF LEMMA 6.3 The proof is similar, mutatis mutandis, to the one given in [21, Sec. II]. Recall from (69) that the cost function can be written (B1) where

By virtue of condition (94) and the boundedness of

where of course the last term tends to zero as . , the cost function Consequently, for any admissible (B1) becomes

(B5) Therefore, since (B6) for any admissible control. Clearly, equality would be achieved to satisfy (101) since does not if we could take contribute to by virtue of (102). Hence it remains to prove that such a process satisfies the stability condition (94). To this end, insert (101) in (95) to obtain (B7) and are bounded, satisfies Since is a stability matrix, satisfies the (102) and weak stability condition (94). The last statement follows immediately from (B5) and (B6). ACKNOWLEDGMENT The authors would like to thank the anonymous referees and the associate editor for several useful suggestions. They would also like to thank X. Hu for technical advice and stimulating discussions. REFERENCES
[1] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic Methods. London, U.K.: Prentice-Hall, 1989. [2] S. Bittanti, F. Lorito, and S. Strada, "An LQ approach to active control of vibrations in helicopters," Trans. ASME, J. Dynamical Systems, Measurement and Contr., vol. 118, pp. 482­488, 1996. [3] C. T. Chen, Linear System Theory and Design. New York: Holt, Rinehart and Winston, 1984. [4] E. J. Davison and A. Goldenberg, "Robust control of a general servomechanism problem: The servo compensator," Automatica, vol. 11, pp. 461­471, 1975. [5] E. J. Davison and B. R. Copeland, "Gain margin and time lag tolerance constraints applied to the stabilization problem and robust servomechanism problem," IEEE Trans. Automat. Contr., vol. AC-30, pp. 229­239, 1985. [6] E. J. Davison and B. M. Scherzinger, "Perfect control of the robust servomechanism problem," IEEE Trans. Automat. Contr., vol. AC-32, pp. 689­702, 1987. [7] B. A. Francis, "The linear multivariable regulator problem," SIAM J. Contr. Optim., vol. 15, pp. 486­505, 1977. [8] B. A. Francis and W. M. Wonham, "The internal model principle of control theory," Automatica, vol. 12, pp. 457­465, 1977. [9] K. V. Frolov and F. A. Furman, Applied Theory of Vibration Protected Systems. Moscow, Russia: Mashinostroenie, 1980 (in Russian). [10] K. V. Frolov, Vibration in Engineering. Moscow, Russia: Mashinostroenie, 1981 (in Russian). [11] M. D. Genkin, V. G. Elezov, and V. D. Iablonski, Methods of Controlled Vibration Protection of Engines. Moscow, Russia: Nauka, 1985 (in Russian).

(B2) being the quadratic form (70). Next, introduce with the Lyapunov function (B3) is the unique stabilizing solution of (96), where is given by (100) and satisfies (104). Then, along the trajectory of (95)

(B4) is given by (99). where In fact, inserting (95) and completing squares in the left member of (B4) yields the right member of (B4) plus a number of terms which are either quadratic in , linear in , or constant with respect to . The quadratic terms cancel due to the fact that satisfies the algebraic Riccati equation (96), and the constant terms cancel due to (104). Finally, the linear terms cancel provided

which has the unique bounded solution (100), since is a stable matrix. and , where Now, set is an admissible process, and sum (B4) from to to obtain

1704

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 44, NO. 9, SEPTEMBER 1999

[12] D. Guicking, Active Noise and Vibration Control, reference bibliography, Third Physical Institute, Univ. Goettingen, Jan. 1990. [13] A. Isidori and C. I. Byrnes, "Output regulation of nonlinear systems," IEEE Trans. Automat. Contr., vol. 35, pp. 131­140, 1990. [14] C. G. K¨ allstr¨ om and P. Ottosson, "The generation and control of roll motion of ships in closed turns," in Proc. 4th Int. Symp. Ship Operation Automat., Geneva, Switzerland, 1982, pp. 1­12. [15] H. Kimura, "Pole assignment by output feedback: A longstanding open problem," in Proc. 33rd Conf. Decision and Control, Lake Buena Vista, FL, Dec. 1994. [16] A. Krener, "The construction of optimal linear and nonlinear regulators," in Systems, Models and Feedback: Theory and Applications, A. Isidori and T. J. Tarn, Eds. Boston, MA: Birkh¨ auser, 1992, pp. 301­322. [17] V. Ku cera, "The discrete Riccati equation of optimal control," Kybernetika, vol. 8, pp. 430­447, 1972. [18] H. Kwakernaak and R. Sivan, Modern Signals and Systems. Englewood Cliffs, NJ: Prentice-Hall, 1991. [19] G. Leitmann and S. Pandey, "Aircraft control under conditions of windshear," in Proc. 29th Conf. Decision and Control, Honolulu, HI, 1990, pp. 747­752. [20] P. Lancaster, A. C. M. Ran, and L. Rodman, "Hermitian solution of the discrete algebraic Riccati equation," Int. J. Contr., vol. 44, pp. 777­802, 1986. [21] A. Lindquist and V. A. Yakubovich, "Optimal damping of forced oscillations in discrete-time systems," IEEE Trans. Automat. Contr., vol. 42, pp. 786­802, 1997. , "Optimal damping of forced oscillations by output feedback," [22] in Stochastic Differential and Difference Equations, Progress in Systems and Control Theory, vol. 23, I. Csisz´ ar and G. Michaletzky, Eds. Boston, MA: Birkh¨ auser, 1997, pp. 203­231. [23] A. Miele, "Optimal trajectories and guidance trajectories for aircraft flight through windshears," in Proc. 29th Conf. Decision and Control, Honolulu, HI, 1990, pp. 737­746. [24] B. P. Molinari, "The stabilizing solution of the discrete algebraic Riccati equation," IEEE Trans. Automat. Contr., vol. AC-20, pp. 396­399, 1975. [25] V. M. Popov, Hyperstability of Control Systems. Berlin, Germany: Springer, 1973. [26] A. V. Savkin and I. R. Petersen, "Robust control with rejection of harmonic disturbances," IEEE Trans. Automat. Contr., vol. 40, pp. 1968­1971, 1995. [27] R. Shoureshi, L. Brackney, N. Kubota, and G. Batta, "A modern control approach to active noise control," Trans. ASME, J. Dynamical Syst.s, Measurement and Contr., vol. 115, pp. 673­678, 1993. [28] V. Z. Weytz, M. Z. Kolovski, and A. E. Koguza, Dynamics of Controlled Machine Units. Moscow, Russia: Nauka, 1984 (in Russian). [29] V. A. Yakubovich, "A frequency theorem in control theory," Sibirskij Mat. Zh., vol. 4, pp. 386­419, 1973 (in Russian); English translation in Sibirian Math. J. [30] , "Universal regulators in linear-quadratic optimization problems," in Trends in Control, A. Isidori, Ed. New York: Springer-Verlag, 1995, pp. 53­68. [31] Y. Zhao and A. E. Bryson, "Aircraft control in a downburst on takeoff and landing," in Proc. 29th Conf. Decision and Control, Honolulu, HI, 1990, pp. 753­757. [32] K. Zhou, Essentials of Robust Control. Englewood Cliffs, NJ: PrenticeHall, 1998.

Anders Lindquist (M'77­SM'86­F'89) received the M.S. and Ph.D. degrees from the Royal Institute of Technology, Stockholm, Sweden, and in 1972 he was appointed a Docent of Optimization and Systems Theory there. From 1972 to 1974, he held visiting positions at the University of Florida, Brown University, and State University of New York, Albany. In 1974, he became an Associate Professor and in 1980 a Professor of Mathematics at the University of Kentucky, where he remained until 1983. He is presently a Professor at the Royal Institute of Technology, where in 1982 he was appointed to the Chair of Optimization and Systems Theory, as well as an Affiliate Professor at Washington University, St. Louis. He has also held visiting positions at University of Padova, Italy, University of Arizona, USSR Academy of Sciences, Moscow, East China Normal University, Shanghai, and Technion, Haifa, Israel. He is the author of many papers in the area of systems and control, especially stochastic control, filtering, stochastic systems theory, realization theory, robust control, and applications of nonlinear dynamics in estimation and control. Dr. Lindquist is a Member of the Royal Swedish Academy of Engineering Sciences, a Foreign Member of the Russian Academy of Natural Sciences, and an Honorary Member of the Hungarian Operations Research Society. He has also served on many editorial and advisory boards.

Vladimir A. Yakubovich (M'97) was born in Novosibirsk, Russia, in 1926. He graduated from Moscow University in 1949. He received the Candidate of Science degree (Ph.D.) in 1953 and the Doctor of Science degree in 1959, both from Leningrad University. After having worked for some time in industry as an Engineer, he was admitted to the Leningrad University, where he has remained. He is the author of more than 270 papers and coauthor of seven books in different areas of applied mathematics and control theory. He has worked in parametric resonance theory, in the theory of stability of nonlinear systems, and in optimization theory. Dr. Yakubovich has served on many scientific committees and editorial boards. He is a member of several scientific societies in Russia. He was awarded the Norbert Wiener Prize in 1991, a prize from the international editorial company "Nauka" for best publication in its journals in 1995, and the IEEE Control Systems Award in 1996. Since 1991, he has been a Corresponding Member of the Russian Academy of Sciences and since 1994 a Member of the Russian Academy of Natural Sciences.

Tracking Personal Processes in Group Projects
Ly Danielle Sauer, Timothy E. Lindquist, Jeremy Cairney Computer Science and Engineering Arizona State University Tempe, Arizona 85287-5406 {sauer, lindquist, cairney}@asu.edu February 8, 1999 Abstract
Software engineering continues to develop methods for process improvement and quality. The Personal Software Process is one way to introduce software engineers to aspects of process tracking, assessment and improvement. In this paper, we describe the software tools that we've constructed to support the planning and postmortem of software activities. We describe an approach that allows the personal software process to be used in group projects, while still allowing the individual engineer to employ personal process quality and improvement techniques on their own activities. The tools supporting planning and postmortem are used in the context of a workflow system developed at ASU, called Open Process Components, whose aim is to componentize software services and provide interoperability among various approaches. These tools and approaches explore software development in the increasingly distributed environment in which the software engineer is responsible for their own assessment and improvement.

1.0 Introduction
Measuring, guiding and refining an organization's software process improves effectiveness of development resources and provides a level of control on software quality. The development of the SEI Capability Maturity Model[24] has raised awareness of the need for better software processes. Software processes are often discussed at the project management level, and its not uncommon for an organization to employ the services of a process engineer with the intent of wide-scale process improvement. Software processes describe the interaction among people and artifacts in carrying out the work involved in the software life-cycle. A software process encompasses the work that will be done (activities), what it will use and produce (input and output products), who will do it (agents), as well as, when and how it will be done (behavior). The past decade has seen increased demand for more powerful and robust automated software process systems. Tool vendors and the research community have responded with a variety of approaches. A review of the tool market place shows many groupware, process and workflow tools whose functionality ranges from graphical modeling or simple enactment to full support for defining, executing, analyzing, measuring, and tracking software processes. The Plethora of

tools, most of which have not been widely adopted, combines together with the increasingly distributed nature of software development today to form one of the challenges addressed by this paper. That is, the need to have interoperability among a heterogeneous set of process tools (which execute on distributed heterogeneous platforms.) The efforts of the Workflow Management Coalition (WfMC) [30] and the Object Management Group (OMG) are aimed at this challenge. Both organizations are identifying common interfaces that vendors can use for interoperability among their products. Other middleware efforts have identified process support services, for example, PCIS (Portable Common Interface Set) [9]. A follow-on project [18] integrates the Open Process Components of Gary [13] with other middleware components, such as version and configuration management. In this paper, we build on these efforts to show how processes can be distributed compositions of personal process components. Considerable research has addressed automating the software process. Some are addressing formalisms for expressing process [3]. Different formalisms such as Petri nets[12], rule-based formalisms[1,25], process programming languages[10], event-based representations [4,8,21], and object-oriented approaches[8,21] are

1 of 8

Overview of Personal Software Process

proposed for representing software processes. Other research includes comprehensive environments centered on process, such as ISTAR, in which all activities are modeled using a contractual model. In a process-centered environment, nearly all activity takes place within a defined process. Christie has elaborated several problems in the adoption of process automation [6]. Process-centered environments are typically all-or-nothing and difficult to adopt in steps. Management is justifiably reluctant to invest in dramatic change without a gradual migration path or concrete evidence of value-added. Benefits of enactment support or tracking require time consuming frontend resources for process definition. Some systems require definition of activities that don't have relevance to tracking and improvement. Adoption also places other stresses on an organization ranging from engineer's perception of excessive intrusion to the need for additional personnel who specialize in process engineering. In this paper, we present a process framework that shifts its approach toward composable process components. Project processes are created by brokering among the building blocks of engineer's defined personal processes. Software engineers are responsible for tracking measuring and analyzing their own processes distinct from organizational concerns.

managing, and improving their predictability, productivity, and quality. PSP consists of a family of seven personal processes that progressively introduce data and analysis techniques (Figure 1 on page 2) [16,28]. Engineers use these data and analysis outcomes to determine their performance and to measure the effectiveness of their methods. Humphrey's initial result (applied to 50 students and three industrial software organization) indicates an average test defects improvement of over ten times and productivity improvements of better than 25% [28].
PSP3 Cyclic Development

Cyclic Personal Process

Personal Quality Managem ent

PSP2 Code Reviews Design Reviews

PSP2.1 Design Templates

Personal Planning Process

PSP1 Size Estimating Test Report

PSP1.1 Task Planning Schedule Planning

PSP0 Baseline Personal Process Current Process Time Recording Defect Recording Defect Type Standard

PSP0.1 Coding Standard Size Measurement Process Improvement Proposal

2.0 Overview of Personal Software Process
Current software professionals utilize private techniques and practices that were learned from peers or through personal experiences. Few software engineers are aware of, or consistently use methods that lend themselves to personal process improvement. A personal software development process is a concept introduced to address improvement needs of an individual. Watts Humphrey of the Software Engineering Institute has formalized a personal software development process called Personal Software Process (PSP). Today, there are various realizations of PSP to aid software engineers in applying the process. The realizations range from case tools and web-based repository browsers to formal training classes.

FIGURE 1.

PSP Process Evolution

2.1

Personal Software Process

Figure 1 on page 2 shows the PSP progression in which each PSP step includes all the elements of prior steps together with additions. The PSP process steps are Baseline Personal Process (PSP0, PSP0.1), Personal Planning Process (PSP1, PSP1.1), Personal Quality Management (PSP2, PSP2.1), and Cyclic Personal Process (PSP3). Starting in The Baseline Personal Process, the software engineer creates the foundations for measurement and improvement. PSP0 is the software engineers current software development process extended to provide measurements (time and defect trackings). PSP0 covers three phases: planning, development (design, code, compile, and test), and postmortem. PSP0.1 includes coding standards, size measurements, and a Process Improvement Proposal (PIP). Personal Planning Process (PSP1, PSP1.1) adds planning to the baseline. Here, the software engineer pre-

Personal Software Process (PSP) [15,16,17,28] is designed to assist software engineers in controlling,

Tracking Personal Processes in Group Projects

2 of 8

Applying PSP to Group Projects

pares the basis for project tracking, which include software estimates and development plans. The goal is to learn the relationship between program size and resources, as well as how to make realistic schedules. PSP1 enhances PSP0 and PSP0.1 to include size and resource estimation and a test report using Proxy Based Estimation (PROBE) as a method to estimate sizes and development times. Personal Quality Management (PSP2, PSP2.1) provides defect management by tracking the relationship between time spent in reviews and the phases during which defects are injected and removed. Prior project defect data are used to realize review checklists and selfassessments. PSP2.1 extends PSP2 with design specifications and accompanying analyses. The goal is to provide the criteria for design completion. Cycle Personal Process (PSP3) introduces techniques for developing large-scale projects. The approach calls for sub-dividing into personal processes. Development is done in incremental steps starting with a base module.

Project Tables are automated forms, such as Logs, Summaries, and Templates. The log tables support tracking time, defects and issues. The Project Summary table records the estimated and actual totals for the project and for all projects to date. The Cycle Summary table supports the project summaries by capturing the planned and actual size, time, and defects for each cycle.

2.3

ECEN 4553 Database Browser

ECEN 4553 PSP Database (PSP Database Browser) [5] is a web-based database that also automates many of the PSP forms, scripts, calculations, and reports. The database is organized to capture a set of related data for an individual software engineer. The core of the database is the concept of a job, which is a software engineer's activity. Once the job is defined, the software engineer can log time against the job, log defects against the job, and specify a detailed project plan for the job. Although the PSP Database Browser does not strictly adhere to all of Watts Humphrey's Personal Software Process data, it collects planned and actual data for each job. ECEN 4553 PSP Database Browser automates a subset of Watts Humphrey's Personal Software Process with a web-based user interface.

2.2

Personal Software Process Studio

Personal Software Process Studio (PSP Studio or PSPS) [11] is a case tool developed at East Tennessee State University to assist in using the Personal Software Process. PSPS does so by automating the planning and postmortem artifacts. In particular, PSPS provides the following features: Data Measurement, Historical Database, Convenient Access to Tables, Statistical Calculations, and Guidance through the Process. The Data Measurement feature allows developers to accurately (similar to a stopwatch) measure development times, track defects, and measure program sizes. The Historical Database feature allows developers to store all of the developers historical PSP data in a reliable and secure database. Convenient Access to Tables provides a window with tab access to the forms. The Statistical Calculations feature automatically maintains totals and performs the statistical calculations. The Guidance through the Process feature provides on-line direction for using the PSP. PSP Studio groups all of the automated paper works, forms, and calculations into two categories: Process Tables and Project Tables. Process Tables guide or improve the individual software engineer process with an online process outline, access to standard tables for defect, LOC and coding standards, and access to a process improvement proposal.

3.0 Applying PSP to Group Projects
ISO 9000 [7,22] and the Capability Maturity Model (CMM) [23,24] assist organizations in improving their processes. Personal Software Process [15,16,17,28], on the other hand, provides an improvement technique for software engineers, in the context of individually developed software. Seamless integration of the PSP within a software organization cannot be achieved, since individuals rarely cycle through all phases of development on a software project. Engineers can, however, apply PSP analysis techniques to their individual activities on group projects. The resulting metrics can be the basis for personal process improvement, without having the "big brother is watching over me" complex that is common to organizationally imposed quality and improvement efforts. This section discusses our approach to providing well integrated organizational and personal process improvement. At ASU, we have been developing software to support the use of planning and postmortem phases of the PSP and to support their application to various life-cycle activities. For example, in an organizational setting, an individual may be assigned to testing. The test engineer

Tracking Personal Processes in Group Projects

3 of 8

Applying PSP to Group Projects

would develop their own test process that includes planning and postmortem. The resulting personal test process becomes part of an organizational or project process. The "integratable" personal processes (personal test process) collect product measures, use defect analysis and consider resource usage as a means of improving that process segment. The artifacts and the automation we have developed are discussed in Section 3.2.

3.1

Process Components

The Open Process Component Toolset (OPC) [14,19,20] is a set of tools developed at ASU to support process definitions and enactment. OPC's basic premise is that a process is a process component and may consist of one or more process components. Process components may be compositions of subcomponents whose underlying representations may differ. For example, a Process Weaver component, called create_design, may be composed with a TeamWare Flow component called review_design. Thus, the Integrated Process is defined as a process component consisting of three process components: the Planning Process Component (Figure 2 on page 4), the Personal Software Activities Component, and the Postmortem Component. The Software Activities component may be any process component such as, testing, design, coding, or review. Discussion of these components and the support we have implemented for Planning and Postmortem artifacts can be found in the following sections (Section 3.1.1, Section 3.1.2, and Section 3.1.3). 3.1.1 The Planning Process The Planning Process Component defines the individual engineer's plans for the software activity. The process is assigned to the project planner, takes as inputs the customer requirements (written or oral) and produces as output an initial version of the planning artifacts, a requirements specification, a cost estimate report, and a size estimate report. Additionally, an engineering notebook for the project is created and initialized based on the activity schedule. At this phase, the project activity schedule and the project plan summary forms only contain planning information such as estimated total size, the project development duration, and defects injected and removed. Further descriptions of the project activity schedule, the project plan summary, and the engineering notebook are discussed in Section 3.2.

FIGURE 2.

Planning Process Component

As shown in Figure 2 on page 4, the Planning Process Component is composed of its children process components: Identify Requirement, Perform Size Estimation, Perform Cost Estimation, and Construct Plan. Each child process component is defined to perform a specific task to help planning the software activity and laying the groundwork for analysis. For instance, the Identify Requirement Process Component generates the requirement specification (SRS) given the customer requirements (Cust Req). Figure 2 on page 4 shows the OPC definition tool's graphical depiction of the Planning Process. The model includes nodes for Processes (process components that have subcomponents), Activities (process components without subs), Roles and Products. Directed edges depict relationships such as can_perform, is_input_to, has_output and has_sub. For example, the Requirement Specification (Product) is_input_to Perform Cost Estimation (Activity), and the Identify Requirement (Activity) has_output which is the Requirement Specification. 3.1.2 Personal Software Activities In the PSP, the planning and postmortem activities depend on a personal software process that includes the phases, planning, design, code, code review, compile, test, and postmortem. In our application of the PSP to group projects, we provide the capability to replace design, code, code review, compile and test with other activities. Our approach is to provide the background for the planning and postmortem phases as applied to any software related activities. In a group project, an individual engineer may

4 of 8

Tracking Personal Processes in Group Projects

Applying PSP to Group Projects

not be involved in coding, compiling and testing, but may instead work on design and design reviews, or may instead be a test engineer whose involvement does not go beyond planning, developing, executing and reporting on tests. Our assumption is that the analysis techniques that consider resources (labor, primarily), product measures and quality assessment all equally apply to any other software related activities, whether directly developing code or not. Process improvement should be a center of focus for all participants in a software process. At ASU, we have been using this approach to Integrating Personal Processes for group software projects in a classroom setting and for group independent study projects. Thus far, uses are for small applications in which most project members get involved with all of the life-cycle activities. The primary challenge in generalizing the approach to large group efforts has to do with product and quality measures. PSP relies on Source Lines of Code as the basis for product measures. Software defect management is the basis for quality, planning and process improvement. Engineers using PSP, record defects by type, phase injected and phase removed. PSP uses yield (percentage of defects removed before compiling), appraisal cost of quality and failure cost of quality as the primary input for quality management and process improvement. These are a good starting point for the practicing software engineer, however, one must define product measures and defects in a manner appropriate to the activity. For example, a test engineer may use test cases generated as the primary product measure. For example, test cases may be defined to be triples (input condition, action, expected result) independent of how the test case is realized in performing tests. Defect types for a test engineer may include: unsatisfied test requirement, and resulting software defects for which there existed a test case.

We have used OPC to depict Postmortem. The process is assigned to the process engineer and accomplishes its objective of producing the project plan summary by using the initialized project activity schedule, the project defect log, and the initialized project plan summary as input products. Unlike the Planning Process Component, Postmortem does not use children process components to accomplish its goal.

3.2

Automated Support for Process Artifacts

The Integrated Personal Process uses four artifacts: the Engineering Notebook, the Project Activity Schedule & Log, the Project Defect Log, and the Project Plan Summary. We have implemented each artifact as a standalone application. When using the worklist handler of OPC, enacting one of the Integrated Personal Process Planning or Postmortem activities may cause the invocation of one or all of these applications according to the process input and output specifications. All four artifacts use a single repository interface to store and manipulate data. The interface is implemented in Java, using synchronization to support multiple concurrent access. Highlights of these artifacts are detailed in the following sections. 3.2.1 The Engineering Notebook The Engineering Notebook is an application which implements some concept of the Personal Software Process Engineering Notebook. The Engineering Notebook objective is to create an engineering notebook that tracks a software engineers daily time usage. More specifically, as shown in Figure 3 on page 6, the Engineering Notebook allows a software engineer to define and record, for a given project, its activities, the time spent on the activities, and product lists of the activities. An activity is a unit of work which takes an engineers time (e.g. interruptions, coding, breaks, lunch, designing, etc.); it is any work performed by a software engineer. The time spent on each activity is recorded in an increment of hours; for instance, a job that takes 15 minutes could be recorded as 0.25 hours, but our usage generally limits granularity to one tenth of an hour (6 minutes). The product list entry allows the engineer to list products produced by the activity. The initial engineering notebook is derived from infromation in the Project Activity Schedule & Log.

3.1.3 The Postmortem Process The Postmortem Process Component defines a process for analyzing the performance (postmortem analysis) of a completed project. Postmortem analysis gathers product measures, performs actual resource usage analysis, performs actual defect analysis, and performs summary quality analysis.

Tracking Personal Processes in Group Projects

5 of 8

Applying PSP to Group Projects

to allow add-on functions. For example, the user can add the tools for estimation or a LOC Counter. The add-on tools are specified using MIME types. 3.2.3 Defect Log The Defect Log (DL) is an application which automates Defect Recording [15,16] to aid in tracking defects injected and removed. The defect data are stored in the defect log, which are used as input to generating the plan summary (Section 3.2.4) in postmortem analysis. The Defect Log is realized as a tabular application where the rows represent the defects and their information and the columns are classifications of the defects. As shown in Figure 4 on page 6, the DL allows its user to specify the date, the defect type, the injected phase, the removed phase, the fixed time, and a description.

FIGURE 3.

Engineering Notebook Main Window

Modification to the times in the engineering notebook causes the transfer of the times spent and the product list to the Project Activity Schedule & Log. 3.2.2 Activity Schedule & Log The Activity Schedule & Log (ASL) is an application, which aids in developing project plans. The ASL application allows the project planner to identify the activities, phases, agents, and times for the activity. When the user completes the activity specifications, ASL places the schedule in a persistent repository. An activity is a task of the project. A project may have a set of activities (process components) representing the work of all group members assigned activities on the project. Similarly, all engineers working on a project will have their own activity schedules, which reflect the lowerlevel activities necessary to complete their input to the group. The lower-level activities are subject to analysis and improvement as defined above. A project planner may also have an Activity Schedule and Log to coordinate the activities and products of a group of engineers. We envision that the project planner can use the Process Broker [27] (which is currently under development), to determine the kind of activities that the project may need and to check for the availability of those process components. To do this, the project planner first specifies the characteristics of the current project to the Process Broker. The Process Broker uses its locating and matching semantic engine and its repository of process components to determine the projects that best fit the specified criteria. For each activity, the project planner estimates a begin and end time, the development duration, the project size, and the output products. These are estimated values, thus, the project planner can use experience to determine the values, some tools, or historical project data. OPC is designed

FIGURE 4.

Project Defect Log Main Window

The date that the defect was discovered and the defect description can be anything that the user enters. The DL default defect types are: Documentation, Syntax, Build/ Package, Assignment, Interface, Checking, Data, Function, System, and Environment. These can be modified to allow application of planning and postmortem to any software activity. Additionally, the DL also provides default phases including: Planning, Design, Code, Review, Compile, Test, and Postmortem. Analogous to the defect types, these can be changed to accommodate the activity. Finally, the time required to correct the defect is recorded in hours and tenths. 3.2.4 Project Plan Summary The Plan Summary (PS) [15,16] is an application, to aid in planning and tracking a software activity. The plan summary is initialized in the planning activity and is com-

6 of 8

Tracking Personal Processes in Group Projects

Current and Future Work

pleted in postmortem. In our implementation, information in the plan summary is derived from the Activity Schedule & Log. The plan summary can be saved and named so that an engineer who participates in several software activities (reviews, testing, and coding, for example) can track data specific to the activity.

www.eas.asu.edu/~yfppg/ [3.] Armenise, P., Bandinelli, S. Ghezzi, C., and Morzenti, A. A Survey and Assessment of Software Process Representation Formalisms. International Journal of Software Engineering and Knowledge Engineering, vol. 3, no. 3, pp. 401-426. 1993. Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS-034-95, Computer Science Department, Columbia U. 1995. L. Carter, ECEN 4553 PSP Database Tool, (University of Colorado at Boulder, Department of Electrical & Computer Engineering, ece-www.colorado.edu/~ecen4553/Reference/ psp/examples.html). A. Christie, et al. A Study into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. F. Coallier, "How ISO 9001 Fits into the Software World", (IEEE Software, January 1994, pp. 98-100). Conradi, R., et al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. J.C. Derniame, et al. "Life-Cycle Process Support in PCIS, Or It Is Time to Think about Software Process Formalisms Standardization", in Proc. of the PCTE'94 Conf. PCTE Technical Journal No.2, PIMB Assn, November 1994. J.C. Derniame, and Gruhn, V. Development of Process-Centered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127150. 1994. East Tennessee State University, Personal Software Process Studio, (East Tennessee State University, Computer and Information Science, www-cs.etsu.edu/softeng/psp/dlpsps.html). W. Emmerich, and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proceedings of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. K. Gary, "Process Interoperability with Open Process Components", Arizona State University, Computer Science and Engineering Department, Ph.D. Dissertation, expected August 1998.

4.0 Current and Future Work
OPC provides an initial set of tools for defining and enacting process components. The underlying implementation of OPC provides the framework for wrapping various process tools for interoperability. We have achieved initial wrappings of two products, and hope to soon demonstrate interoperability between these products in the near future. Thus, a process component can be defined in terms of sub components each under the direction of a different enactment engine. We have used our integrated personal software process approach in classroom projects and in group independent studies. The tools described in this paper will be introduced to these projects beginning in the Fall semester 1998. Enactment using OPC is controlled by a worklist handler tool, which connects to a repository of process components. Process components are all represented using Java objects. Until the tool wrappers are fully functional, enactment involves launching an application associated with the input and output products as specified with MIME types. A few important distinctions differentiate our approach to integrating personal processes. Engineers are not asked to carry out a defined process that they themselves have not developed. Engineers are motivated to use process improvement techniques, since they directly and solely apply to their own activities. Product and defect measures are defined by the engineer and thus problems of consistency do not arise. Engineer define their own personal process for the software activities they perform. These may defined or applied from definitions they obtain from other engineers.

[4.]

[5.]

[6.]

[7.]

[8.]

[9.]

[10.]

[11.]

5.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proceedings of the 2nd EWSPT `92, Trondheim, Norway. September, 1992. Arizona State University, Open Process Component Toolkit, (Computer Science and Engineering Department, YFPPG Group, http://

[12.]

[13.]

[2.]

Tracking Personal Processes in Group Projects

7 of 8

References

[14.]

K. Gary, T. Lindquist, L. Sauer, and H. Koehnemann, "Automated Process Support for Organizational and Personal Processes", (Proceedings of the International Conference on Supporting Group Work (GROUP `97), the Integration Challenge, Phoenix, Arizona, USA, 16-19 Nov 1997). W. S. Humphrey, Introduction to the Personal Software Process (Reading, MA: Addison-Wesley, 1997). W. S. Humphrey, A Discipline for Software Engineering (Reading, MA: Addison-Wesley, 1995). W. S. Humphrey, "The Personal Process in Software Engineering", (Software Process Newsletter, Technical Council on Software Engineering, IEEE Computer Society, Vol. 13, No. 1, September 1994, pp. 1-3, http://www.sei.cmu.edu/products/publications/95.reports/95.ar.psp.swe.html). The US-France Technology Research and Development Project, PCIS2 Architecture Specification Version 1.0, (Lindquist, TE editor) SPAWAR Systems Command, San Diego CA, January 1998. T. Lindquist, "A Toolset Supporting Distributed Process Components", (Arizona State University, Computer Science & Engineering Department, Technical Report, TR-97-034, 1997). T. Lindquist, and J. Derniame, "Towards Distributed and Composable Process Components", (Proceedings of the European Workshop on Software Process Technology, June 1997). Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Mark C. Paulk, "How ISO 9001 Compares with the CMM", (IEEE Software, January 1993, pp. 74-83). Mark C. Paulk, Bill Curtis, and Mary Beth Chrissis, "Capability Maturity Model, Version 1.1", (IEEE Software, July 1993, pp. 18-27). Mark C. Paulk et al., "Capability Maturity Model for Software, Version 1.1", (Technical Report, CMU/SEI-93-TR-24, Software Engineering Institute, 1993). Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. W. Royce, "Managing the Development of Large Software Systems: Concepts and Techniques", (WESCON Technical Papers, Vol. 14, Los Ana-

gles, WESCON, August 1970). [27.] L. Sauer, "Brokering of Process Components", (Arizona State University, Computer Science & Engineering Department, Ph.D. Dissertation Proposal, December 1997). Software Engineering Institute, Personal Software Process (PSP), (SEI Technology, http:// www.sei.cmu.edu/technology/psp/). Software Engineering Institute, A Specification for Automated Support for the PSP, (SEI Technology, http://www.sei.cmu.edu/technology/pspAuto/indexh.html). The Workflow Management Coalition. The Reference Model. WfMC Document Number TC001003, January 1995.

[15.]

[28.]

[16.] [17.]

[29.]

[30.]

[18.]

[19.]

[20.]

[21.]

[22.]

[23.]

[24.]

[25.]

[26.]

8 of 8

Tracking Personal Processes in Group Projects

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann Computer Science and Engineering Department Arizona State University Tempe, Arizona 85287-5406 Jean-Claude Derniame Laboratoire lorrain de Recherche en Informatique et Applications LORIA : Bd des Aiguillettes BP 239 54 506 Vandoeuvre Cedex Abstract
Only recently has the research community started to consider how to make software process models interoperable and reusable. The task is difficult. Software processes are inherently creative and dynamic, difficult to define and repeat at an enactable level of detail. Additionally, interoperability and reusability have not been considered important issues. Recent interoperability and reusability solutions advocate the development of standard process model representations based on common concepts or generic schemas, which are used as a basis for translating between heterogeneous process representations. In this paper we propose an alternative approach through the development of process-based components. We present the Open Process Components Framework, a componentbased framework for software process modeling. In this approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of process representations, an explicit representation of process state, and an extendable set of class relationships. their benefits, systems based on these formalisms create enactable process models which are not interoperable nor reusable with one another. The prevailing solution is to advocate an intermediary standard process representation and provide translations for interoperability and reuse. We do not believe this approach is scalable and defeats the purpose of using heterogeneous process representations. We advocate an object-oriented, component-based philosophy for providing software process interoperability and reuse. This paper presents Open Process Components, a component-based framework for software process definition and enactment. In this framework, components are well-encapsulated representations of process entities that interact in meaningful ways. The framework is solidly founded on mature concepts in the software process field, and yet is extendable so that process models may be customized in a particular domain. A componentized view of process representations results in easier process definition, modularized process enactment, natural interoperability, and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable. Instances vary according to constantly changing demands of specific projects. Fully elaborating a software process model to an enactable level of granularity is often too tedious, time-consuming, and costly[4]. Motivated by the need for interoperability and reuse, we advocate applying component-based techniques to software process modeling. Constructing software process models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined components.

1.0 Introduction
Since Osterweil's proposal[12] for automating the software process a decade ago, there has been significant debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets [9], rule-based formalisms [1,8,13], process programming languages [15], event-based representations [3,6,10], object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research under Award number N00014-97-1-0872

A component-based approach:

2.1.2 Process Component States
The elements of the meta-model appear in most process models. Each model requires a different enactment service to interpret the representation and execute the process. Regardless of the formalism employed and the interpreter used, all models define actions on the entities within the process domain, which effect the states of those entities. The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes finite state machines as part of its basic abstractions. Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for process modeling is not unique[7,11]. The OPC framework defines a basic set of states and transitions for Process and Activity components. These include states such as executing, suspended , and aborted, with corresponding transitions between states defined by actions such as startProcess, suspendProcess, and abortProcess. Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object's unique behavior when interacting with other components within the framework. The current class definition for state transition graphs include operations to add and remove states and allowable transitions between states, making a component's state and behaviors affecting state explicit and manipulable.

· · · ·

avoids deep integration of semantic models handles the natural complexity of software processes, responds to dynamic software processes, and facilitates reuse, minimizing one-shot process models. Component-based process modeling requires a framework for developing components. The framework must identify process entities, define meaningful interactions between entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides a foundation for developing, integrating, maintaining, and reusing a variety of process representations. The framework defines basic abstractions of the problem space that can be specialized. Yet, the framework must make some commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this meta-model as a basis for translation between process models, we use it as a foundation for identifying elements of the process space for componentization, and for defining meaningful ways in which process components interact.
has_sub

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a layered, three-tier software architecture (Figure 2). The Framework Layer defines classes and interfaces modeling process entities derived from the OPC meta-model. The Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component Layer extends representations to particular domains. It is from this layer that actual Process component objects are instantiated. A process model in the OPC framework is a set of components, realized as objects of Component Layer classes, and a set of relations between those components, created under the constraints of the Framework Layer, implemented using Representation Layer semantics. Figure 2 shows example classes at each layer of abstraction for the meta-model element Process. The Framework Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation Layer is comprised of class definitions for specific process representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets (PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any other representations we wish to encapsulate. The Compo-

role
assigned_to can_perform

activity

has_input has_output

product

consists_of

has_version has_variant
has_sub

agent

process

FIGURE 1. The Open Process Components Framework Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and Agent), and artifacts (Product). The meta-model defines the "rules of engagement" for components. It identifies what component types interact with what other component types under what relationships. These relationships are not static; process components and component relationships are highly dynamic during the course of the component's life cycle.

Framework Layer

Process

Process
Representation Layer ECAProcess OrderedProcess PetriNetProcess RuleProcess

PDLProcess

Component Layer Bug Fix Code Module Integration Test Design

Code Module

Peer Review Stress Test

FIGURE 2. Object-oriented class diagram for Process components

nent Layer contains type definitions for actual process types. The dashed lines between layers in Figure 2 denotes that the Representation and Component Layers in fact can have many levels. This allows for multiple ways in which to extend and specialize the framework. The first step identifies a base set of classes and interfaces at the Framework Layer. The next step is to construct encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are encapsulated behind the interfaces inherited from the Framework Layer. For example, the implementation may come from a COTS process tool. Finally, components defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used since inheritance would tie the component's type to its implementation. Component Layer objects are configurable. Component Layer classes represent generic process models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product, Role, or Agent) fully specified and bound is part of an instantiated process model[5]. Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given representations. Delegating Representation Layer classes provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi's levels of process specialization[5].

software development support. PCIS2 services include Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA. The process support services in PCIS2 are based on the OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification, known as jFlow[11], submitted to the OMG. The jFlow specification is largely an "object-ization" of existing WfMC interfaces[16]. This is not a drawback, but one of the strengths of the OMG's approach to adopting and adapting existing technology. The jFlow specification improves upon the original WAPI specifications by defining appropriate interactions between objects to gain interoperability and maintainability of workflow systems. The PCIS2 specification is object-oriented from the ground up, but has borrowed some of the jFlow concepts in order to maintain compliance with emerging standards. PCIS2 and the jFlow specification differ in three areas. First, PCIS2 supports dynamic processes through ad hoc process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2 incorporates support for the metaprocess, by defining views on its services for controlling, defining, performing, and monitoring processes. jFlow only defines interfaces for performing (enacting) and monitoring workflows. It should also be pointed out that jFlow identifies concepts not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these specifications, they are largely complementary and both provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The authors propose the construction of software process components for producing process artifacts. A "software process component" is essentially a process model fragment written in some Process Modeling Language (PML). Components are dynamically combined to construct complete process models through interface types and their respective "connectors ports". The authors correctly motivate the need to eliminate monolithic process systems and instead provide reuse and integration capabilities for process representations. However, the approach lacks adherence to foundational concepts, such as those used in OPC (see Section 2.1). The three-tier layering of the OPC framework provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite their differences, the Pynode component approach is simi-

3.0 PCIS2 Process Services
The Open Process Components framework is currently used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

lar in philosophy and motivation to the OPC framework, and appears to be at roughly the same level of maturity. Results of these two experiments will be very useful to the software process modeling community. A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14]. The authors view the object space and the process space at different levels. The object space is data-oriented, whereas the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that brings together the object level and the process level together. Shams-Aliee and Warboys[14] also advocate modeling a process as a collection of objects or components. However, we find the distinction between the object level and the process level unnecessary. In particular, we do not agree that the object level is a data-oriented model. In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of these objects as defined by their interfaces. OPC merges objects and processes into components through an explicit representation of process state contained in the component. We propose a full object-oriented framework that includes class definitions, inheritance, and rules for component interaction. This merging of objects and processes into a complete component-based model allows OPC the full potential to achieve interoperability and reuse by being independent of any process modeling formalism.

Software Process (ICSP4). December, 1996. [3.] Ben-Shaul, I. and Kaiser, G. An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995. Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R. Towards a Reference Framework for Process Concepts. Proc. of EWSPT'92, pp. 3-17, Trondheim, Norway. September 1992. Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling and Technology, A. Finklestein, J. Kramer, and B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994. Derniame, J.C. Life Cycle Process Support in PCIS. Proc. of the PCTE `94 Conference. 1994. Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994. Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net Based Software Process Modeling Language. Proc. of the 6th International Workshop on Software Specification and Design, Como, Italy. September 1991. Melo, W.L. and Belkhatir, N. TEMPO: A Support for the Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers, North Holland. 1994. Object Management Group. jFlow Joint Submission. OMG Document Number bom/98-06-07. July 4, 1998. Osterweil, L. Software Processes are Software Too. Proc. of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987. Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the 14th International Conference on Software Engineering, pp. 262-279. May, 1992. Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings of the First World Conference on Design and Process Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for Design and Process Science, Austin, TX. December 1995. Sutton, S., Heimbigner, D., and Osterweil, L. Language Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium on Software Development Environments, Irvine, CA. December 1990. Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003, January 1995.

[4.]

[5.]

[6.]

[7.] [8.]

[9.]

[10.]

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software processes. This framework identifies common concepts in the research community and defines an object-oriented framework for applying these concepts. This framework is currently employed in the construction of a software architecture for support distributed software development. This approach, together with related efforts in the field of workflow, makes the important contribution that the software process automation field is maturing to the point that efforts such as the one described herein can be attempted. Despite whether the reader agrees with the design of this framework, providing interoperability and reusability will overcome one of the serious hurdles preventing wide scale deployment of software process automation technology.

[11.] [12.]

[13.]

[14.]

6.0 References
[1.] Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G. PEACE: Describing and Managing Evolving Knowledge in Software Process. Proc. of EWSPT `92, Trondheim, Norway. September, 1992. Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

[15.]

[16.]

[2.]

Automated Process Support for Organizational and Personal Processes
Kevin Gary, Tim Lindquist, Harry Koehnemann. Ly Sauer Arizona State University Computer Science Department Mail Stop 5406 Tempe, AZ 85287-5406 yfppg@asu.edu
ABSTRACT We propose two views on process: an organizational view and a personal process view. Information technology applies Automated Workflow technology to define, execute, and track an organization's automated business processes. Calendaring tools provide a form of personal process view through scheduled work items. However, the personal, or individual, view of the process space has largely been ignored. We maintain that as organizations become increasingly decentralized, a single organization's process space is becoming difficult to recognize. Individuals of the organization are asked to do work that spans organizational, functional, and even geographic boundaries. An integrated view of organizational workflows and personal processes is needed to address these new demands. In this paper we argue for the need to integrate organizational and personal processes. We then propose a component-based process modeling approach and supporting process architecture that integrates these process spaces. Finally, we describe our recent efforts at developing Java prototype process tools that realize the proposed modeling technique and supporting architecture. Keywords: Workflow, Personal Process, Components units. Current technology is growing at a rate that can be difficult to track. Industry investments in desktop tools and groupware must be leveraged against growth in local and wide-area networks (LANs and WANs). In particular, the growth of the Internet makes it possible to envision computer support of global, decentralized, business processes. These changes in industry and technology escalate the pressures put on information technology research. Providing computer support for widely distributed organizations using new technologies such as the Internet, Groupware, Calendar Management, and Automated Workflow is at least an IT systems analyst's headache. Determining the best way to integrate these tools to ensure maximum productivity is at best an IT manager's nightmare. In our view, the ability to define, execute, and track business processes is central to the ability to integrate these technologies in a widely distributed setting and make their use productive. Therefore in our research we focus on automated process support. In the business domain, automating business processes is known as Automated Workflow. Workflow is the study of modeling and enacting business processes by human and computer agents. Automated Workflow adds an emphasis on applying current computer and information technology in a workflow environment, with the desire of automating parts of workflows, or supporting entire workflows. Automated Workflow has traditionally focused on defining and automating business processes from the organization's standpoint. Little regard is given to managing overlapping workflows in an individual workspace, or for even considering the personal processes of an individual when considering the productivity of the organization. The current solution is to drop a set of personal productivity tools, such as calendaring tools, in the lap of the individual and let her/ him work it out.

1.0 Introduction
Recent changes in industry and technology are imposing more demanding technical requirements on information technology. In industry, organizations are downsizing and becoming increasingly decentralized, often causing projects to be managed across multiple organizations or functional

In order to achieve greater productivity from both workflow and personal productivity tools, a more integrated view of organizational and personal processes must be considered. An integrated view allows individuals the ability to develop their own productive work practices in support of an organization's processes, and allows for a more natural handling of processes spanning multiple organizations and individuals. We are in the beginning stages of our research into the utility of providing such an integrated view. In this paper we propose an open architecture for integrating organizational workflows and personal productivity processes. We motivate the need for an integrated approach, and present a component-based approach to process modeling that provides the interoperability required to achieve the integration. We also present a suite of tools being developed at Arizona State University that realize this architecture. The rest of this paper is organized as follows. Section 2.0 discusses relevant issues in current workflow and calendaring technology. Section 3.0 argues for an integrated view of organizational and personal process spaces, presents a component-based approach to process modeling, and proposes a general process support architecture. Section 4.0 presents tool prototypes realizing this architecture that were recently developed at Arizona State University. We conclude in Section 5.0 with a summary and discuss future avenues for our research.

tectural elements of workflow systems and the interactions between those elements. The Process Interchange Format (PIF) Working Group was formed to explore the potential to provide automatic translations between process representation formalisms[15]. Finally, Microsoft is pushing their Messaging API (MAPI) as a defacto standard for implementing workflow systems. Microsoft has recently teamed with Wang to develop the MAPI-WF specification[17], an extension of MAPI for supporting workflow-specific services. The WfMC is presently the most significant of the efforts attempting to standardize workflow systems. The WfMC Reference Model (Figure 1) identifies the basic architectural components of a workflow environment. At the center of the model is a Workflow Enactment Service (WES), comprised of one or more Workflow Engines. A WES provides services through the WAPIs to workflow-related tools. These include Process Definition Tools for defining processes, Workflow Client Applications for handling user requests for work, Third-party Applications that need to communicate data and operations to the WES, other WESs for providing interoperability between enactment services, and Administration and Monitoring Tools for data gathering for process improvement activities. The WfMC Reference Model identifies common workflow system components and interfaces. The WAPI interface specifications define a set of low-level protocols for synchronously and asynchronously exchanging workflow data between the tools and the WES. Our basic problem with this approach is that these protocols are too low-level; they imply a restrictive workflow model. Workflow representations that cannot easily convert their process data to conform with this underlying model cannot obtain conformance with the model. This is one of the issues our research addresses. Recent standardization efforts also address the area of calendaring protocols. One popular calendaring protocol (adopted by Netscape's Calendar Server[18]) is the vCalendar protocol[12]. In the vCalendar protocol, calendaring and scheduling entities, called events, are transported between applications that can understand the protocol. This approach is similar to the effort of the WfMC protocols in that it defines a low-level data interchange format that tools must understand to conform to the protocol. Other, more industry-wide standardization efforts are being sponsored by the Internet Engineering Task Force (IETF) based in part on the vCalendar specification. The IETF has recently sponsored the development of three separate calendaring protocols, the Calendaring Interoperability Protocol (CIP), the Core Object Specification (COS), and the Internet Calendar 1. For Workflow API and Interchange

2.0 Background
Approaches to developing workflow systems have both commercial and academic origins. Commercial systems have evolved from work on forms-based image processing systems and groupware[14]. The line between workflow and other types of systems is often blurred, with groupware, scheduling, database, and email tools providing some workflow functionality. In addition, several commercial products that advertise workflow capabilities fall far short of providing full-fledged support for defining and enacting business processes. Academic research has focused mainly on process modeling and database transaction issues[9]. Process modeling research has led to the development of workflow representations based on a variety of formalisms. Database transaction research focuses on extending traditional transaction semantics to support long duration[2][9] and/or cooperative transaction[5][10] models. The result is a proliferation of approaches and issues relating to workflow. Current efforts are attempting to get researchers and vendors to converge on a common foundation for workflow. The Workflow Management Coalition (WfMC) was formed in August 1993 to promote workflow technology. The WfMC has proposed a reference model[22] and a set of interfaces, called WAPIs1 based on that model[24][25][26][27] as an attempt at standardizing archi-

3.1 Organization vs. Personal Process Space
Automated Workflow is the specification and execution of a business process of an organization[9]. Workflows are modeled as a collection of process steps, or tasks, assigned to individuals taking on particular roles. Many modern workflow systems work in this way; the process is considered from a single organization's viewpoint. This viewpoint is illustrated in Figure 2. Organization A Workflow 1 Workflow 2 FIGURE 1. WfMC Reference Model ([23]) Access Protocol (ICAP). These protocols specify interface and other requirements on calendaring systems exchanging calendaring data. The standardization efforts in both workflow and calendaring focus on low-level data interchange and protocols for exchanging such data in a client-server environment. While this is a widely accepted standardization approach, we fear that a stable data format is difficult to obtain due to the maturing of the underlying models in each domain. This is especially true in workflow. In the calendaring domain, a problematic issue is that calendaring formats and tools support only rudimentary dependencies between tasks. These issues are compounded when integrating workflow and calendaring systems. Workflow systems can write events to calendar tools, but are not aware of the personal views of the process of the participating individuals. Likewise, calendaring systems provide a personalized view of work, but do not possess sophisticated enough models to negotiate with workflow systems over the ability to do assigned work. Workflow 3 Workflow 4 Workflow 5

Organization B FIGURE 2. Organizational Process Perspective Figure 2 shows the process space of two organizations, generically labeled A and B. These organizations share two workflows: Workflow 3 and Workflow 4. Interoperability of the underlying process models and process support architecture is required to allow these organizations to share these workflows. The workflow systems we have experienced or seen in the literature take this organization-centered approach to automating business processes. For example, Action Workflow from Action Technologies[1] operates on a cyclical model where workflow units interoperate to produce customer satisfaction. Different participants are viewed as customers, performers, or observers at each workflow stage of the cycle. While Action Workflow provides client-side functionality to obtain task lists for individuals, it does not provide a structured way for individuals to define personal processes and integrate them into the scope of organizational processes. Another example is the application of groupware-oriented tools such as Lotus Notes to workflow[20]. Notes provides much of the needed infrastructure for managing data and transactions within a workflow. However, again there is no structured way to define personal processes and integrate them into organizational processes. Instead, the approach is again organization-centered, where workflows are defined at the organizational scope,

3.0 Integrated Process Support
We advocate an integrated view of an organization's process space and the personal process spaces of its individual workers. In this view, the organization's workflows are integrated with individual personal process spaces. Section 3.1 discusses this idea in more detail. To support this integrated view, we advocate a component-based approach to process modeling that avoids a reliance on low-level data interchange formats. This approach is called Open Process Components, and is described in Section 3.2. Finally, we propose a generic architecture in Section 3.3 that derives from our integrated view of process. In Section 4.0 we present some Java prototype tools based on our ideas.

and personal tasks derived from the workflow model. Still other workflow platforms, such as InConcert[16], emphasize collaborative aspects of workflow execution. Collaborative work is closer in spirit to the idea of integrated process spaces, but differs in that the emphasis is on mechanisms supporting shared access to data. Users still act on tasks delegated to them by the organizational workflow model. To keep pace with industry trends and technology impacts, this organization-centered viewpoint will have to change in at least the following ways:

· Interoperability between workflows developed across
business functional units and/or organizations must be supported.

Figure 3 shows an agent-centered viewpoint of the process space. Jill is an agent working for Organization A, Bob works for Organization B. Jill participates in Organization A's workflow 1 and 3. Bob participates in Organization B's workflows 3 and 5. In order to accomplish tasks in workflow 1, Jill employs her Personal Process 1. Likewise, Bob employs his Personal Process 3 in carrying out tasks relevant to Workflow 5. In addition, Bob employs Personal Process 3 to carry out similar tasks in the shared Workflow 3. Jill does not have a relevant personal process defined for her assigned tasks in Workflow 3. Finally, each individual may have personal processes defined that are outside the scope of an explicit workflow for either organization. These may be processes defined solely by the individual's personal productivity initiative.
Organization A's Space

· The potential for wide-area distributed participation
must be supported. Personal Process 1

Jill

Personal Space

· Individuals must have the ability to define, execute, and
track the personal processes they perform to be productive within the context of an organization's business processes and goals. The work of the Workflow Management Coalition as well as research efforts such as our Open Process Components Framework (see Section 3.2) address the first two issues directly. However, there has not been a lot of consideration for the last issue. At best, current workflow systems notify individuals of new work items through email or custom client applications. Some even have the ability to write to personal calendaring software through interfaces such as Microsoft and Wang's MAPI-WF[17]. But the viewpoint still originates with the organizational process. An agentcentered viewpoint, showing the distribution of workflows an individual participates in, and the set of personal processes an individual employs, is not considered. The need for supporting the personal process view is just beginning to be recognized in more dynamic process areas such as Software Engineering[11]. In the software process domain, the work of the software developer is considered dynamic in the sense that the developer must be creative in seeking the solutions to design, implementation, and maintenance dilemmas[4]. As workflow extends to more complex and skilled tasks, automated workflow systems will be required to encompass more than just the straightforward document-routing capabilities of image processing systems. Future demands will include the ability to support more of the skilled, or knowledge work, that people perform in the organization. In order to do this, workflow systems must relax the prescriptive constraints it places on performers of the workflow, and allow these workers to perform their own personal processes to carry out the work. Workflow 1 Workflow 3 Workflow 5

Personal Process 2

Personal Process 4 Personal Process 3
Organization B's Space

Bob

Personal Space

FIGURE 3. Personal Process Perspective There are several reasons for arguing for an integrated view of organizational and personal processes. Figure 3 shows the overlap of the personal and organizational process space. Defining and executing business processes is motivated in part by the need to ensure business goals are achieved. Workflows are largely assumed to be static, repetitive processes that involve rote decision-making in support of well-defined business goals[9] 1. To expand the scope of processes automated workflow systems can support, more dynamic workflows that include personal processes should be considered. Another motivating reason comes from the diverse set of relationships in which both organizations and individuals participate. Individual workers, particularly at 1. We refer to Georgakopoulos, Hornick, and Sheth's[9] trade press characterization of administrative and production workflows. Our research is closer to ad hoc workflows, though our point is they can be better understood through an integrated view of the process space.

highly skilled levels, perform in a wide variety of diverse business functions. Downsizing and decentralization of organizations coupled with increasing outsourcing of work makes it unrealistic to take the single organization approach. The business processes of multiple organizations must be integrated with the personal processes of the participants. In order to accomplish this integration, we propose a component-based approach to process modeling and an open architecture for supporting personal and organizational process spaces.

work artifact that is either consumed as input by an Activity or produced as output. A Role is a process-specific definition of the skill set required to perform an Activity. A Role is process-specific as opposed to organization-specific, meaning management must decide how to map organizational roles to process-specific roles. This mapping is the relationship between Roles and Agents. The meta-model described briefly here is adopted from the PCIS LCPS metamodel[7]. However, the concepts are similar in a variety of general descriptions of process in the literature[5][9][15][22]. In the OPC Framework, this set of process entities and relationships form the basis for meaningful component interactions. The second important aspect of the OPC Framework is a state-based encapsulation of execution interfaces. By this we mean each component in a process model possesses a process state, and this state is manipulable by a set of interfaces to the component that are available during various stages of executing the process model. Example interfaces include start, suspend, resume, abort, completeWithFailure and completeWithSuccess. Each component maintains an explicit, independent state during execution of the process model, and the state of process execution at any point in time is the combination of states of the components involved in the process. The final salient feature of the OPC Framework is a threetiered object-oriented class hierarchy for defining components. An object-oriented methodology provides several advantages: encapsulation of heterogeneous process representations, an economy of representation through inheritance, and the ability to specialize component definitions through subclassing. From a process modeling perspective, one major advantage of the hierarchy is its ability to be extended. New component definitions and abstractions can be added within the framework without modifying preexisting definitions. A second important advantage is that specialized component definitions allow heterogeneous process modeling formalisms to interoperate with one another. For example, a Petri-net based process model fragment can interoperate with a process model fragment developed in a scripting language by encapsulating each as a component under the framework. This is especially beneficial in the organizational/personal context of processes we consider in this paper since it should not be assumed that homogeneous process models are generated across these contexts. As a brief example, consider the ad hoc workflow depicted in Figure 4, taken from [9]. This workflow represents a paper review process. In a component-based process model, each task in the workflow is represented as an activity component. Interactions between the components is governed by the set of interfaces each component supports. The benefit is that the implementation of each component is separated from these interfaces. Different process modeling and

3.2 Component-based Process Modeling
Organizations developing standards in workflow and calendaring focus on low-level data interchange protocols to be applied in a client-server environment. The development of such protocols, particularly the protocols related to workflow definition interchange1, are too restrictive to ensure widespread adoption. Instead, we propose an object-oriented component-based approach to process modeling and execution. In our research we are developing a componentbased framework for process modeling called the Open Process Components (OPC) Framework. It is not the focus of this paper to delve into the details of the OPC Framework, but we do provide a brief discussion relevant to the process support architecture presented in Section 3.3. Further details may be found in [8]. There is a need for a unifying framework for representing and manipulating workflow abstractions. We take an objectoriented approach we call Open Process Components. Entities of the workflow domain are represented as objects, with manipulations of those objects defined as object behaviors. The approach is component-based, from the perspective that interfaces are well-defined so that components interact in meaningful ways. The OPC Framework provides a foundation for constructing component-based process models in an extendable fashion. There are three important aspects to the OPC Framework that allow it to support component-based process modeling. The first is a meta-model that identifies basic process entities and relationships between entities. Basic process entities include Process, Activity, Product, Role, and Agent. A Process is a decomposable entity into subprocesses and subactivities. This allows development of process models in a top-down fashion. An Activity is an executable fragment of a process model; it represents a refinement of a portion of a process model down to an executable state. A Product is a

1. More specifically, the Workflow Process Definition Language proposed in WAPI 1[24].

enactment services can be used to define and execute the details of each task. This differs from existing systems where homogeneous models and supporting services are employed. The workflow in Figure 4 is a relevant example of the utility of integrated organizational and personal process spaces. Consider for example the "Review" tasks in the workflow. These are assigned to separate persons fulfilling the role of Reviewer. However, there is not sufficient detail in this definition to automate the support of review activities for each reviewer. Furthermore, it is not appropriate to believe that this organizational workflow should provide such detail. Instead, it is more natural that each reviewer perform a personalized review process that meets the requirements of the organizational workflow. Therefore, if Jill and Bob were Reviewers in this workflow, each would carry out the review according to her/his own personal process for reviewing papers, employing familiar tools and methods for producing the needed results.

3.3 A Process Support Architecture
To support the integration of the organizational and personal process spaces, we propose an architecture that extends traditional workflow client-server architectures to include support for the personal process space. Figure 6 shows the proposed general architecture.

Process Definition Tools

Personal Process Servers

Workflow Servers

Calendar Manager

Select reviewers Review request 2 Distribute Papers Review 1 Review 2 Produce joint reviews Forward review Review 3

Review request 1

Review request n

Worklist Handler

Calendar Tool

FIGURE 6. Process Support Architecture The architecture in Figure 6 integrates organizations' workflow servers and personal process servers with calendaring technology to produce a time-oriented view of work for the end-user. Arcs indicate the bidirectional flow of components over the architecture. This architecture extends traditional workflow architectures, such as the Workflow Management Coalition's Reference Model[22], to include the end user's personal process space. The components of this architecture are:

· Process Definition Tools
FIGURE 4. Example Workflow (taken from [9) Component-based process modeling is at the heart of our research and relevant to the topics discussed in the rest of this paper. However, the elements of organizational versus personal process spaces and process architecture we discuss do not necessarily rely on a component-based approach. One can readily envision modifications to existing tools such as Action Workflow or Lotus Notes discussed earlier that would address process space integration. We encourage the reader to consider process modeling approaches and process space integration issues as independently as possible. Process Definition Tools are used to create componentbased process models. These tools may query Personal Process and Workflow servers in order to reuse existing process component definitions.

· Workflow Servers
One or more servers create the organizational process space(s). These servers manage component-based workflow models created for organizational units by Process Definition Tools.

· Personal Process Servers
Similar to a Workflow Server, a Personal Process Server manages process definitions for individuals, created from components by Process Definition Tools.

· Calendar Manager
The Calendar Manager is the organizer of an individual's process space. The Calendar Manager manages instances of process models from the individual's perspective.

Servers to communicate directly to negotiate over rights to assign work to an individual. The proposed architecture is process model independent. It does not favor any particular representation of process. However, we again advocate the use of component-based process models. Component-based process modeling allows for easier integration of organizational and personal process spaces in the Process Definition Tools and Calendar Managers. Without components, there would be a push on each tool to support low-level protocols allowing for heterogeneous process models to be integrated. This is just the type of interoperability that is deficient in current workflow systems, and a major motivating force behind the componentbased approach to process modeling described in Section 3.2. We have developed a set of Java tools realizing the proposed architecture. In the next section we present our progress with this project.

· Worklist Handler/Calendar Tool
This is a client-side tool that presents the individual with her/his work to do. This may be in the form of a task list, or may be a time-oriented view depending on process constraints and personal scheduling preferences. This general architecture clearly shows the separation and integration of organizational and personal process spaces. The distinct servers manage personal and organization processes. This distinction is a logical one; in practice a single implemented server may include the functionality to manage both process spaces. Integration of the spaces comes from the Process Definition Tools and the Calendar Manager. A Process Definition Tool creates component-based process models. By accessing the process definitions on both servers, the tool is able to create and reuse organizational process that utilizes process specifications of relevant individuals. The Calendar Manager integrates instances of organizational and personal processes from the individual's perspective. The Calendar Manager has the ability to accept or decline work requests from process servers, or manage changes to the individual's process space when forced to do so. This tool is the focal point of the individual's process space. Finally, the Worklist Handler/Calendar Tool is a combination of a workflow client and a personal calendaring tool. This client-side tool has the ability to host process components and support the enactment of such components in order to carry out the actual work. The architecture we propose is an integration of current workflow architectures such as the WfMC's Reference Model[22] and calendaring environments such as Netscape's Calendar Server[18]. However, current architectures do not take such an integrated view. We know of no tool that allows for process models to be created that integrate a workflow model and a personal process model. The proposed process definition tool allows for this integration. We know of no environment that provides a componentized personal view of process like the proposed Calendar Manager. One can envision workflow servers writing to an individual's calendar through an interface such as Microsoft and Wang's MAPI-WF interface[17]. However, this requires that the workflow server have explicit knowledge and access rights to individuals' calendars. The proposed Calendar Manager explicitly manages an individual's workspace, negotiating between servers and individual preferences to present the personal process space to the end user. The existence of such a tool enables a component-based architecture that does not require Personal Process and Workflow

4.0 The Current Prototype
The YFPPG Research Group at Arizona State University has sponsored a series of Master's projects during the Spring 1997 semester for developing a toolset in Java for component-based process modeling and enactment. This toolset conforms closely to the general architecture presented in Section 3.3. The specific architecture is shown in Figure 7.

Repository Browser Process Component Repository

Components Editor Worklist Handler

Calendar Manager Server

Calendar Client

FIGURE 7. OPC Support Architecture The components of this architecture are:

· Process Component Repository
This is implemented as a Java RMI[22] server that uses Java Serialization facilities to distribute process objects to client tools. The repository stores component-based process definitions and distributed components for

enactment. Multiple named repositories, each storing multiple process models, can be managed by a single server.

· Calendar Manager Server
Java RMI and CORBA1 versions of this server exist. This server stores time-oriented appointments as well as task lists for individuals.

· Repository Browser
The Repository Browser is a process administration and management tool that allows users to browse through the current objects in a repository. This is implemented as a Java RMI client.

· Components Editor
The Components Editor is another Java RMI client. It allows users to graphically create component-based process models through component creation and reuse. Figure 8 shows the Components Editor GUI with our example process definition from Figure 4. FIGURE 8. Components Editor

· Worklist Handler
This client-side tool obtains work items for a user from a repository. The work items are actually Java objects that are serialized and obtained through Java RMI calls. Once the Worklist Handler obtains these objects, it can execute them, changing the state of the process model and invoking tools on work products. Figure 9 shows a Worklist Handler for Bob.

· Calendar Client
The Calendar Client obtains the appointments and task lists for an individual from a Calendar Manager Server. In addition, the Calendar Client can bring up a Worklist Handler to access the Process Component Repository. Java RMI and CORBA versions of this tool exist.

FIGURE 9. Worklist Handler At this point in the development of our toolset we have yet to implement the full envisioned functionality of the Calendar Manager Server. The overlap of the organizational and personal process space occurs in the Calendar Client, which is responsible for providing the integrated view of the two spaces. The next step is to implement the full negotiation between the two servers, as we discuss in the next section. We have already learned several lessons during the development and use of this toolset. On the plus side, these tools successfully demonstrate the integration of organizational and personal process spaces. These tools are also demonstrations of forward-looking component distribution technologies such as Java RMI[22] and CORBA[19]. Finally, these tools demonstrate the utility of component-based process modeling. There have been some hiccups however. Managing migrating components in a distributed environment is a difficult configuration management problem. It has proven troublesome to track distributed process components' states and synchronize updates to process models stored in the repository. Despite these problems, we are excited by the possibilities of distributed, component-based process modeling, and are initiating a new set of projects to

1. Iona Technologies' OrbixWeb[13] was used to implement the CORBA-enabled calendar server and client.

update the current environment. Readers interested in obtaining the prototypes or tracking progress of this project may visit the YFPPG website at http://www.eas.asu.edu/ ~yfppg.

6.0 References
[1.] Action Technologies, Inc. Coordination Software: Enabling the Horizontal Corporation. Action Technologies, Inc. White Paper. July, 1994. Alonso, G. and Schek, H. Research Issues in Large Workflow Management Systems. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996. Armitage, J. and Kellner, M. A Conceptual Schema for Process Definitions and Models. Proceedings of the Third International Conference on the Software Process (ICSP3), pp. 153-165, Reston, VA. October, 1994. Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems, Athens, GA, May, 1996. Conradi, R., Liu, C., and Hagaseth, M. Planning Support for Cooperating Transactions in EPOS. Information Sciences, vol. 20, no. 4, pp. 317-336. 1995. Curtis, B., Kellner, M., and Over, J. Process Modeling. Communications of the ACM, vol. 35, no. 9, pp. 75-90, September, 1992. Derniame, J.C. Life Cycle Process Support in PCIS. Proceedings of the PCTE `94 Conference. 1994. Gary, K., Lindquist, T., and Koehnemann, H. Component-based Process Modeling. Technical Report TR97-022, Computer Science Department, Arizona State University. May, 1997. Georgakopoulos, D., Hornick, M., and Sheth, A. An Overview of Workflow Management: From Process Modeling to Workflow Automation Infrastructure. Distributed and Parallel Databases, vol. 3, pp. 119153. 1995.

5.0 Summary and Future Work
In this paper we have advocated an integrated view of organizational workflows and personal process spaces. In this view, both the perspective of the organization and the perspective of the individual are considered when integrating process spaces. This view allows organizational goals to be pursued while allowing individual workers the flexibility to define how to accomplish such goals. Such flexibility will be required in the not-too-distant future due to the increasing demands on current workflow systems and the current pace of technology. In this paper we proposed a generic architecture for process support that logically integrates functionality needed for both perspectives. We suggest a component-based process modeling approach to further reduce the dependencies between workflow and calendaring systems by avoiding the need for low-level, brittle data interchange protocols. Finally, we described a set of prototype tools based on component-based process modeling that realizes the generic architecture. Despite the success or failure of our efforts, we hope that the argument for integrated organizational and personal process spaces will have an effect on future considerations in the converging areas of workflow and groupware research. Given the relatively early stage of this research, there are several avenues we intend to pursue in this area. First, further research is needed to fully understand the nature of the negotiation between organizational and personal process spaces that takes place in the Calendar Manager. We are pursuing research in this area under the topic Process Component Brokering, where such negotiation is carried out by having the Calendar Manager provide a brokering service that identifies personal process components that meet organizational process requirements. Second, we are looking at ways to integrate automated planning and scheduling techniques for workflow and personal processes. The result will be enhanced Calendar Managers that negotiate with organizations Workflow Servers to optimize the overlap between organizational and personal process execution. Finally, we plan to validate the proposed architecture by employing our tools in real workflow settings, and extending our work into more dynamic process areas. Specifically, we are looking at ways to support Personal Software Processes and Distributed Learning processes between mentors and students.

[2.]

[3.]

[4.]

[5.]

[6.]

[7.] [8.]

[9.]

[10.] Godart, C., Canals, G., Charoy, F., and Molli, P. An Introduction to Cooperative Software Development in COO. International Conference on System Integration, 1994. [11.] Humphrey, W. The Personal Process in Software Engineering. Proceedings of the Third International Conference on the Software Process (ICSP-3). IEEE Press. October, 1994. [12.] Internet Mail Consortium. vCalendar V1.0 Specification. Available at http://www.imc.org/pdi/pdiproddev.html [13.] Iona Technologies. OrbixWeb 2.0 Programming Guide. November 1995. [14.] Khoshafian, S., and Buckiewicz, M. Introduction to Groupware, Workflow, and Workgroup Computing.

J. Wiley and Sons, New York. 1995. [15.] Lee, J. Gruniger, M., Jin, Y., Malone, T., and Yost, G. The PIF Process Interchange Format and Framework. Available at http://www.aiai.ed.ac.uk/pif/index.html. May 24, 1996. [16.] McCarthy, D. and Sarin, S. Workflow and Transactions in InConcert. IEEE Bulletin of the Technical Committee on Data Engineering, vol. 16 no. 2. June 1993. [17.] Microsoft Corporation and Wang Laboratories, Inc. Microsoft MAPI Workflow Framework Concepts and Facilities (White Paper). Available at http:// www.wang.com/sbu/w9602210.htm. February 21, 1996. [18.] Netscape Communications Corporation. Netscape Calendar Server 1.0 and 2.0. Available at http:// home.netscape.com/comprod/server_central/product/calendar/calendar2_data.html. [19.] Object Management Group. Corba 2.0 Specification. Available at http://www.omg.org/corbask.htm. July 1995. [20.] Reinwald, B and Mohan, C. Structured Workflow Management with Lotus Notes release 4. Proceedings of the 41st IEEE CompCon digest of papers, pp.451457, Santa Clara, CA. February, 1996. [21.] Riddle, W. E. Fundamental Process Modeling Con-

cepts. Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996. [22.] Sun Microsystems, Inc. Remote Method Invocation Specification. Available at http://www.javasoft.com:80/products/jdk/1.1/docs/guide/rmi/spec/ rmiTOC.doc.html [23.] The Workflow Management Coalition. The Reference Model. WfMC Document Number TC00-1003, January 1995. [24.] The Workflow Management Coalition. Interface 1: Process Definition Interchange. WfMC Document Number TC-1016, Version 1.0 Beta. May 29, 1996. [25.] The Workflow Management Coalition. Interface 2 Specification. WfMC Document Number TC-1009, Version 1.0. November 20, 1995. [26.] The Workflow Management Coalition. Interoperability Abstract Specification. WfMC Document Number TC-1012, Version 1.0. October 20, 1996. [27.] The Workflow Management Coalition. Draft Audit Specification. WfMC Document Number TC-1015. August 14, 1996.

1

On time-reversibility of linear stochastic models
Tryphon T. Georgiou and Anders Lindquist

Abstract Reversal of the time direction in stochastic systems driven by white noise has been central throughout the development of stochastic realization theory, filtering and smoothing. Similar ideas were developed in connection with certain problems in the theory of moments, where a duality induced by time reversal was introduced to parametrize solutions. In this latter work it was shown that stochastic systems driven by arbitrary second-order stationary processes can be similarly time-reversed. By combining these two sets of ideas we present herein a generalization of time-reversal in stochastic realization theory.

arXiv:1309.0165v1 [cs.SY] 31 Aug 2013

I. I NTRODUCTION Time reversal of stochastic systems is central in stochastic realization theory (see, e.g., [1], [2], [3], [4], [5], [6], [7], [8]), filtering (see [9]), smoothing (see [10], [11], [12]) and system identification. The principal construction is to model a stochastic process as the output of a linear system driven by a noise process which is assumed to be white in discrete time, and orthogonal-increment in continuous time. In studying the dependence between past and future of the process, it is natural to decompose the interface between past and future in a time-symmetric manner. This gives rise to systems representations of the process running in either time direction, forward or backward in time. In a different context (see [13]) a certain duality between the two time-directions in modeling a stochastic process was introduced in order to characterize solutions to moment problems. In this new setting the noise-process was general (not necessarily white), and the correspondence between the driving inputs to the two time-opposite models was shown to be captured by suitable dual all-pass dynamics. In the present note, we combine these two sets of ideas to develop a general framework where two time-opposite stochastic systems model a given stochastic process. We study the relationship between these systems and the corresponding processes. In particular, we recover as a special case certain results of stochastic realization theory ([1], [5], [10]) from the 1970's using a novel procedure. In Section II we explain how a lifting of state-dynamics into an all-pass system allows direct correspondence between sample-paths of driving generating processes, in opposite time-directions, via causal and anti-causal mappings, respectively. In Section III we utilize this mechanism in the context of general output processes and, similarly, introduce a pair of time-opposite models. Finally, in Section IV, we draw connection to literature on time reversibility and related issues in physics, and we indicate directions for future research. II. S TATE
DYNAMICS AND ALL - PASS EXTENSION

In this paper we consider discrete-time as well as continuous-time stochastic linear state-dynamics. As usual, in discrete-time these take the form of a set of difference equations x(t + 1) = Ax(t) + Bu(t) (1)

This research was supported by grants from AFOSR, NSF, VR, and the SSF. Department of Electrical & Computer Engineering, University of Minnesota, Minneapolis, Minnesota, tryphon@umn.edu Department of Automation, Shanghai Jiao Tong University, Shanghai, China, and Center for Industrial and Applied Mathematics and ACCESS Linnaeus Center, KTH Royal Institute of Technology, Stockholm, Sweden, alq@kth.se

2

where t  Z, A  Rn×n , B  Rn×p , n, p  N, A has all eigenvalues in the open unit disc D = {z | |z | < 1}, and u(t), x(t) are stationary vector-valued stochastic processes. The system of equations is assumed to be reachable, i.e., rank B, AB, . . . An-1 B = n, and non-trivial in the sense that B is full rank. In continuous-time, state-dynamics take the form of a system of stochastic differential equations dx(t) = Ax(t)dt + Bdu(t) (3) (2)

where, here, u(t), x(t) are stationary continuous-time vector-valued stochastic processes. Reachability (which in this case, is equivalent to controllability) of the pair (A, B ) is also assumed throughout and the condition for this is identical to the one for discrete-time given above (as is well known). In continuous time, stability of the system of equations is equivalent to A having only eigenvalues with negative real part, and will be assumed throughout along with the condition that B has full rank. In either case, discrete-time or continuous-time, it is possible to define an output equation so that the overall system is all-pass. This is done next. The assumptions of stationarity and constant parameter matrices is made for simplicity of notation and brevity and can be easily removed. A. All-pass extension in discrete-time Consider the discrete-time Lyapunov equation P = AP A + BB  . (4)

Since A has all eigenvalues inside the unit disc of the complex plane and (2) holds, (4) has as solution a matrix P which is positive definite. The state transformation  = P - 2 x, and F = P - 2 AP 2 , G = P - 2 B, brings (1) into  (t + 1) = F  (t) + Gu(t). (7)
1 1 1 1

(5)

(6)

For this new system, the corresponding Lyapunov equation X = F XF  + GG has In as solution, where In denotes the (n × n) identity matrix. This fact, namely, that In = F F  + GG implies that this [F, G] can be embedded as part of an orthogonal matrix U= i.e., such that UU  = U  U = In+p . Define the transfer function U(z ) := H (zIn - F )-1 G + J (10) F G H J , (9) (8)

3

corresponding to  (t + 1) = F  (t) + Gu(t) u ¯(t) = H (t) + Ju(t). This is also the transfer function of x(t + 1) = Ax(t) + Bu(t) ¯  x(t) + Ju(t), u ¯(t) = B
1 ¯ := P - 2 where B H , since the two systems are related by a similarity transformation. Hence,

(11a) (11b)

(12a) (12b)

¯  (zIn - A)-1 B + J. U(z ) = B

(13)

We claim that U(z ) is an all-pass transfer function (with respect to the unit disc), i.e., that U(z ) is a transfer function of a stable system (obvious) and that U(z )U(z -1 ) = U(z -1 ) U(z ) = Ip . The latter claim is immediate after we observe that, since U  U = In+p , U and hence,  (t) = F   (t + 1) + H  u ¯(t)   u(t) = G  (t + 1) + J u ¯(t) or, equivalently, x(t) = P AP -1 x(t + 1) + P 2 H  u(t) u(t) = B  P -1 x(t + 1) + J  u ¯(t). Setting x ¯(t) := P -1x(t + 1), (16) can be written ¯u x ¯(t - 1) = A x ¯(t) + B ¯(t)   u(t) = B x ¯(t) + J u ¯(t) with transfer function ¯ + J . U(z ) = B  (z -1 In - A )-1 B Either of the above systems inverts the dynamical relation u  u ¯ (in (12) or (11)). (19) (18a) (18b) (17)
1

(14)

 (t + 1) u ¯(t)

=

 (t) u(t)

,

(15a) (15b)

(16a) (16b)

u(t )

U

u ¯(t) 

Fig. 1.

Realization (12) in the forward time-direction.

4



u(t)

U



u ¯(t)

Fig. 2.

Realization (18) in the backward time-direction.

B. All-pass extension in continuous-time Consider the continuous-time Lyapunov equation AP + P A + BB  = 0. (20)

Since A has all its eigenvalues in the left half of the complex plane and since (2) holds, (20) has as solution a positive definite matrix P . Once again, applying (5-6), the system in (3) becomes d (t) = F  (t)dt + Gdu(t). We now seek a completion by adding an output equation du ¯(t) = H (t)dt + Jdu(t) so that the transfer function U(s) := H (sIn - F )-1 G + J is all-pass (with respect to the imaginary axis), i.e., U(s)U(-s) = U(-s) U(s) = Ip . (23) (22) (21b) (21a)

For this new system, the corresponding Lyapunov equation has as solution the identity matrix and hence, F + F  + GG = 0. Utilizing this relationship we note that (sIn - F )-1 GG (-sIn - F  )-1 = (sIn - F )-1 (sIn - F - sIn - F  )(-sIn - F  )-1 = (sIn - F )-1 + (-sIn - F  )-1 , and we calculate that U(s)U(-s) = (H (sIn - F )-1 G + J )(G (-sIn - F  )-1 H  + J  ) = JJ  + H (sIn - F )-1 (GJ  + H  ) (JG + H )(-sIn - F  )-1 H  . For the product to equal the identity, JJ  = Ip H = -JG . Thus, we may take J = Ip H = -G , (24)

5

and the forward dynamics d (t) = F  (t)dt + Gdu(t) du ¯(t) = -G  (t)dt + du(t). Substituting F = -F  - GG from (24) into (25a) we obtain the reverse-time dynamics d (t) = -F   (t)dt + Gdu ¯(t)  du(t) = G  (t)dt + du ¯(t). Now defining x ¯(t) := P -1x(t) and using (5) and (6), (26) becomes ¯ u dx ¯(t) = -A x ¯(t)dt + Bd ¯(t)  du(t) = B x ¯(t)dt + du ¯(t), with transfer function ¯ + Ip , U(s) = B  (sIn + A )-1 B where ¯ := P -1 B. B (29) (30) (28a) (28b) (27) (26a) (26b) (25a) (25b)

Furthermore, the forward dynamics (25) can be expressed in the form dx(t) = Ax(t)dt + Bdu(t) ¯  x(t)dt + du(t) du ¯(t) = B with transfer function ¯  (sIn - A )-1 B + Ip . U(s) = B III. T IME - REVERSAL
OF LINEAR STOCHASTIC SYSTEMS

(31a) (31b)

(32)

The development so far allows us to draw a connection between two linear stochastic systems having the same output and driven by a pair of arbitrary, but dual, stationary processes u(t) and u ¯(t), one evolving forward in time and one evolving backward in time. When one of these two processes is white noise (or, orthogonal increment process, in continuous-time), then so is the other. For this special case we recover results of [1] and [5] in stochastic realization theory. A. Time-reversal of discrete-time stochastic systems Consider a stochastic linear system x(t + 1) = Ax(t) + Bu(t) y (t) = Cx(t) + Du(t) (33a) (33b)

with an m-dimensional output process y , and x, u, A, B are defined as in Section II-A. All processes are stationary and the system can be thought as evolving forward in time from the remote past (t = -). In particular, x(t + 1) is Ftu -measurable y (t)

6

for all t  Z, where Ftu is the  -algebra generated by {u(s) | s  t}. Next we construct a stochastic system ¯u x ¯(t - 1) = A x ¯(t) + B ¯(t) ¯x ¯u y (t) = C ¯(t) + D ¯(t), which evolves backward in time from the remote future (t = ), and for which x ¯(t - 1) y (t)
¯ ¯u is F t -measurable

(34a) (34b)

¯ ¯u for all t  Z, where F ¯(s) | s  t}. The processes x ¯, x, u ¯, u relate as in t is the  -algebra generated by {u the previous section. More specifically, as shown in Section II-A,

u ¯(t) is Ftu -measurable while
¯ ¯u u(t) is F t -measurable

for all t, as examplified in Figures 1 and 2. In fact, the all-pass extension (12) of (33a) yields ¯  x(t) + Ju(t) u ¯(t) = B It follows from (18b) that (35) can be inverted to yield u(t) = B  x ¯(t) + J  u ¯(t), where x ¯(t) = P -1 x(t + 1), and that we have the reverse-time recursion ¯u x ¯(t - 1) = A x ¯(t) + B ¯(t). Then inserting (36) and into (33b), we obtain ¯ := DJ  and where D ¯u x(t) = P x ¯(t - 1) = P A x ¯(t) + P B ¯(t) ¯x ¯u y (t) = C ¯(t) + D ¯(t), ¯ := CP A + DB  . C (37b) (38) (37a) (36) (35)

Then, (37) is precisely what we wanted to establish. Moreover, the transfer functions W(z ) = C (zIn - A)-1 B + D of (33) and ¯ (z ) = C ¯ (z -1 In - A )-1 B ¯ +D ¯ W of (34) satisfy ¯ (z )U(z ). W (z ) = W (41) (40) (39)

In the context of stochastic realization theory, discussed next, U(z ) is called structural function ([3], [4]).

7

u(t )

W

y (t) 

Fig. 3.

The forward stochastic system (33).



y (t)

¯ W



u ¯(t)

Fig. 4.

The backward stochastic system (34)

1) Time-reversal of stochastic realizations.: Given an m-dimensional stationary process y , consider a minimal stochastic realization (33), evolving forward in time, where now u is a normalized white noise process, i.e., E{u(t)u(s)} = Ip t-s . Since U, given by (13), is all-pass, u ¯ is also a normalized white noise process, i.e., E{u ¯(t)¯ u(s) } = Ip t-s . From the reverse-time recursion (34a)


x ¯(t) =

¯u (A )k-(t+1) B ¯ (k ).
k =t+1

Since, u ¯ is a white noise process, E{x ¯(t)¯ u(s) } = 0 for all s  t. Consequently, (34) is a backward stochastic realization in the sense of stochastic realization theory. B. Time-reversal of continuous-time stochastic systems We now turn to the continuous-time case. Let dx = Axdt + Bdu dy = Cxdt + Ddu (42a) (42b)

be a stochastic system with x, u, A, B as in Section II-B, evolving forward in time from the remote past (t = -). All processes have stationary increments and x(t) y (t) is Ftu -measurable

for all t  R, where Ftu is the  -algebra generated by {u(s) | s  t}. The all-pass extension of Section II-B yields ¯  xdt du ¯ = du - B as well as the reverse-time relation ¯ u dx ¯ = -A x ¯dt + Bd ¯  du = B x ¯dt + du ¯, (44a) (44b) (43)

8

where x ¯(t) = P -1 x(t). Inserting (44b) into dy = CP x ¯dt + Ddu yields ¯x dy = C ¯dt + Ddu ¯, where ¯ = CP + DB  . C Thus, the reverse-time system is ¯ u dx ¯ = -A x ¯dt + Bd ¯ ¯x dy = C ¯dt + Ddu ¯. From this, we deduce that x ¯(t) y (t)
¯ ¯u is F t -measurable

(45)

(46a) (46b)

for all t  R. We also note that the transfer function W(s) = C (sIn - A)-1 B + D of (42) and the transfer function ¯ ( s) = C ¯ (sIn + A )-1 B ¯ +D W of (46) also satisfy ¯ (s)U(s) W ( s) = W as in discrete-time. 1) Time-reversal of stochastic realizations.: In continuous-time stochastic realization theory, (42) is a forward minimal stochastic realization of an m-dimensional process y with stationary increments provided u is a normalized orthogonal-increment process satisfying E{du(t)du(t)} = Ip dt. Since U(s) is all-pass, ¯  xdt du ¯ = du - B also defines a stationary orthogonal-increment process u ¯ such that E{du ¯(t)du ¯(t) } = Ip dt. It remains to show that (46) is a backward stochastic realization, that is, at each time t the past increments of u ¯ are orthogonal to x ¯(t). But this follows from the fact that


(47)

x ¯(t) =
t

¯ u e-A (t-s) Bd ¯ ( s)


and u ¯ has orthogonal increments.

9

IV. C ONCLUDING

REMARKS

The direction of time in physical laws and the fact that physical laws are symmetric with respect to time have occupied some of the most prominent minds in science and mathematics ([14], [15], [16]). These early consideration were motivated by no less an issue than that of the very nature of the quantum. Indeed, Erwin Schr¨ odinger's aim appears to have been to draw a classical analog to his famous equation. A large body of work followed. In particular, closer to our immediate interests, dual time-reversed models have been employed to model, in different time-directions, Brownian or Schr¨ odinger bridges (see [17], [18]), a subject which is related to reciprocal processes ([19], [20], [21], [22]). The topic of time reversibility has also been central to thermodynamics, and in recent years studies have sought to elucidate its relation to systems theory (see [23], [24]). Possible connections between this body of work and our present paper will be the subject of future work. The thesis of the present work is that under mild assumptions on a stochastic process, any model that consists of a linear stable dynamical system driven by an appropriate input process can be reversed in time. In fact, a reverse-time dual system along with the corresponding input process can be obtained via an all-pass extension of the state equation. The correspondence between the two input processes can be expressed in terms of each other by a causal and an anti-causal map, respectively. The formalism of our paper can easily be extended to a non-stationary setting at a price of increased notational, but not conceptual, complexity. Informally, and in order to underscore the point, if u(t) is a non-stationary process and the linear system is time-varying, under suitable conditions, a reverse-time system and a process u ¯(t) can be similarly constructed via a time-varying orthogonal transformation. R EFERENCES
[1] A. Lindquist and G. Picci, "On the stochastic realization problem," SIAM J. Control Optim., vol. 17, no. 3, pp. 365­389, 1979. [2] ----, "Forward and backward semimartingale models for Gaussian processes with stationary increments," Stochastics, vol. 15, no. 1, pp. 1­50, 1985. [3] ----, "Realization theory for multivariate stationary Gaussian processes," SIAM J. Control Optim., vol. 23, no. 6, pp. 809­857, 1985. [4] ----, "A geometric approach to modelling and estimation of linear stochastic systems," J. Math. Systems Estim. Control, vol. 1, no. 3, pp. 241­333, 1991. [5] M. Pavon, "Stochastic realization and invariant directions of the matrix Riccati equation," SIAM Journal on Control and Optimization, vol. 18, no. 2, pp. 155­180, 1980. [6] A. Lindquist and M. Pavon, "On the structure of state-space models for discrete-time stochastic vector processes," IEEE Trans. Automat. Control, vol. 29, no. 5, pp. 418­432, 1984. [7] G. Michaletzky, J. Bokor, and P. V´ arlaki, Representability of stochastic systems. Budapest: Akad´ emiai Kiad´ o, 1998. [8] G. Michaletzky and A. Ferrante, "Splitting subspaces and acausal spectral factors," J. Math. Systems Estim. Control, vol. 5, no. 3, pp. 1­26, 1995. [9] A. Lindquist, "A new algorithm for optimal filtering of discrete-time stationary processes," SIAM J. Control, vol. 12, pp. 736­746, 1974. [10] F. Badawi, A. Lindquist, and M. Pavon, "On the Mayne-Fraser smoothing formula and stochastic realization theory for nonstationary linear stochastic systems," in Decision and Control including the Symposium on Adaptive Processes, 1979 18th IEEE Conference on, vol. 18. IEEE, 1979, pp. 505­510. [11] F. A. Badawi, A. Lindquist, and M. Pavon, "A stochastic realization approach to the smoothing problem," IEEE Trans. Automat. Control, vol. 24, no. 6, pp. 878­888, 1979. [12] A. Ferrante and G. Picci, "Minimal realization and dynamic properties of optimal smoothers," Automatic Control, IEEE Transactions on, vol. 45, no. 11, pp. 2028­2046, 2000. [13] T. T. Georgiou, "The Carath´ eodory­Fej´ er­Pisarenko decomposition and its multivariable counterpart," Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 212­228, 2007. ¨ [14] E. Schr¨ odinger, Uber die Umkehrung der Naturgesetze. Akad. d. Wissenschaften, 1931. [15] A. Kolmogorov, Selected Works of AN Kolmogorov: Probability theory and mathematical statistics. Springer, 1992, vol. 26. [16] A. Shiryayev, "On the reversibility of the statistical laws of nature," in Selected Works of AN Kolmogorov. Springer, 1992, pp. 209­215. [17] M. Pavon and A. Wakolbinger, "On free energy, stochastic control, and Schr¨ odinger processes," in Modeling, Estimation and Control of Systems with Uncertainty. Springer, 1991, pp. 334­348. [18] P. Dai Pra and M. Pavon, "On the Markov processes of Schr¨ odinger, the Feynman-Kac formula and stochastic control," in Realization and Modelling in System Theory. Springer, 1990, pp. 497­504.

10

[19] B. Jamison, "Reciprocal processes," Probability Theory and Related Fields, vol. 30, no. 1, pp. 65­86, 1974. [20] A. Krener, "Reciprocal processes and the stochastic realization problem for acausal systems," in Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, Eds. Amsterdam: North-Holland, 1986, pp. 197­211. [21] B. C. Levy, R. Frezza, and A. J. Krener, "Modeling and estimation of discrete-time gaussian reciprocal processes," Automatic Control, IEEE Transactions on, vol. 35, no. 9, pp. 1013­1023, 1990. [22] P. Dai Pra, "A stochastic control approach to reciprocal diffusion processes," Applied mathematics and Optimization, vol. 23, no. 1, pp. 313­329, 1991. [23] W. M. Haddad, V. Chellaboina, and S. G. Nersesov, "Time-reversal symmetry, poincar´ e recurrence, irreversibility, and the entropic arrow of time: From mechanics to system thermodynamics," Nonlinear Analysis: Real World Applications, vol. 9, no. 2, pp. 250­271, 2008. [24] ----, Thermodynamics: A dynamical systems approach. Princeton University Press, 2009.

CANONICAL CORRELATION ANALYSIS, APPROXIMATE COVARIANCE EXTENSION, AND IDENTIFICATION OF STATIONARY TIME SERIES*
ANDERS LINDQUIST AND GIORGIO PICCI

Abstract. In this paper we analyze a class of state-space identification algorithms for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called "subspace methods" can be described as covariance estimation followed by stochastic realization. The methods offer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identification into the solution of a Riccati equation but introduce at the same time some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identification procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not explicitly reported. In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We review the basic ideas of stochastic realization theory in the context of identification, discuss the concept of stochastic balancing and of stochastic model reduction by principal subsystem truncation. The model reduction method of Desai and Pal, based on truncated balanced stochastic realizations, is partially justified, showing that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing conditions under which the "subspace identification" methods produce bona fide stochastic systems.

1. Introduction Recently there has been a renewed interest in state-space identification algorithms for time series based on a two steps procedure which in principle can be described as estimation of a rational covariance model from observed data followed by stochastic realization. The method offers the major advantage of converting the nonlinear parameter estimation phase which is necessary in traditional ARMA models identification into a partial realization problem, involving a Hankel matrix of estimated
 This research was supported in part by grants from TFR, the G¨ oran Gustafsson Foundation, the SCIENCE project "System Identification" and LADSEB-CNR.  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden  Dipartimento di Elettronica e Informatica, Universita' di Padova, 35131 Padova, Italy 1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which efficient numerical solution techniques are available. In this framework we can naturally accommodate multivariate processes and there are indications that the algorithms may work also with data containing purely deterministic components (van Overschee and De Moor, 1993). A drawback, however, to be emphasized in this paper, is that, unlike, say, least-squares identification of ARMA models, these methods do not work for arbitrary data. This type of procedure was apparently first advocated by Faurre (1969); see also Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work, based on canonical correlation analysis (Akaike, 1975) (or some other singular-value decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki (1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor, 1993). Numerical experience shows that the computation time needed to get the final model parameters estimates compares very favorably with traditional iterative prediction error methods for ARMA models. On the other hand there is a price to be paid for this simplification. These methods introduce some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic realization arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. Note that positivity is the natural condition insuring solvability of the Riccati equation required to compute state-space models of the signal from the covariance estimates. Central in the procedures described above is the following classical problem of identification of a covariance sequence. Let {0 , 1 , . . . ,  } (1.1)

be a finite set of sample m × m covariance matrices estimated in some unspecified way from a certain m-dimensional sequence of observations {y0 , y1 , y2 , . . . yT }, ¯ = k CAk-1 C and such that the infinite sequence {0 , 1 , 2 , . . . }, (1.4) (1.2)

¯ ) such that and consider the problem of finding a minimal1 triplet of matrices (A, C, C k = 1, 2, . . . ,  (1.3)

¯ for k =  + 1,  + 2, . . . , is a bona fide obtained from (1.1) by setting k := CAk-1 C covariance sequence. In the literature the last condition is generally ignored. The remaining problem of ¯ ) satisfying (1.3) is called the minimal partial realfinding a minimal triplet (A, C, C ¯ ) is usually computed by minimal factorization ization problem. The triplet (A, C, C
1

¯ ) is minimal if (A, C ) is completely observable and (A, C ¯ ) is completely reachable. Here (A, C, C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

3

of a block Hankel matrix corresponding to the data (1.1) as follows:      ¯ C 1 2 C 3 · · · j ¯ 2 3  4 · · · j +1   CA   CA      , H= = . . . . . .  .      . .. . . . . . . . . . . . ¯ (A )j -1 i i+1 i+2 · · · i+j -1 CAi-1 C

(1.5)

where i + j - 1 =  and the Hankel matrix H is chosen as close to square as possible by taking |i - j |  1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j ) such that i + j - 1 =  , and hence the minimal factorization must be made for a choice of (i, j ) in which the Hankel matrix (1.5) has maximal rank. The infinite sequence ¯ for k =  +1,  +2, . . . {0 , 1 , 2 , . . . } obtained in this way by setting k := CAk-1 C is called a minimal rational extension of the finite sequence (1.1) and is in general not a covariance sequence. The dimension r of a minimal rational extension is called the (algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to the McMillan degree of the m × m rational matrix ¯ + 1 0 , (1.6) Z (z ) = C (zI - A)-1 C 2 and the elements of the infinite sequence (1.4) are the coefficients of the Laurent expansion 1 Z (z ) = 0 + 1 z -1 + 2 z -2 + . . . 2 (1.7)

about z = . The underlying identification problem is however a great deal more complicated than the classical partial realization problem. In fact, the requirement that (1.4) be a bona fide covariance sequence amounts to (1.4) being a positive sequence in the sense that, for every t  Z+ , the block Toeplitz matrices Tt ,   2 · · · t 0 1 1 0 1 · · · t-1  , Tt =  (1.8) . . . . ...  . . . . . .  . . t t-1 t-2 · · · 0 formed from the infinite sequence (1.4), be positive definite or, equivalently, that the matrix function (z ) := Z (z ) + Z (1/z ) be positive semidefinite on the unit circle, i.e. (ei )  0   [0, 2 ). (1.10) (1.9)

This property is equivalent to  being a spectral density matrix. In fact, it will be the spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial covariance sequence unless T > 0, but this is not enough. From the point of view of identification there seem to be two possible routes to ¯ ) from the finite covariance sequence (1.1). One that determine a model (A, C, C has been proposed in the literature is do minimal factorization (1.5) of a finite block Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown in this paper, there is no a priori guarantee that this method will yield a positive extension. This fact has nothing to do with sample variability (random fluctuations) of the covariance estimates (1.1), and to emphasize this point we initially assume that all strings of data (1.2) are infinitely long. A theoretically sounder identification method, which will not be considered in this paper, could instead be to do positive extension first and then to use a stochastic model reduction procedure on the triplet ¯ ) of the positive extended sequence. (A, C, C The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this difficulty into consideration have been pointed out by the authors of this paper at many scientific meetings in the last ten years. This has had no apparent effect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identification procedure of Aoki (1990) and demonstrate that there is a hidden, and not easily tested, assumption without which the procedure will not be guaranteed to succeed. The punch line is that none of the subspace identification methods under consideration can be expected to always work for generic data but that some not entirely natural conditions on the data are needed. The analysis of the basic theoretical issues behind subspace identification is carried out in the geometric framework of stochastic realization theory; see, e.g., Lindquist and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this theory and adapt them to the problem of identification. To this end, we first discuss an idealized situation in which the time series (1.2) is infinitely long i.e. T = , and the available covariance data are given by the ergodic limit 1 T  T + 1 lim
T

yt+k yt+j = k-j
t=0

(1.11)

for all k and j . Then the sample estimates in the sequence (1.1) are bona fide covariance matrices and the Toeplitz matrix T formed from the data will be positive definite and symmetric. We introduce a Hilbert space of observed (infinite) strings of data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we establish a correspondence which turns operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. Canonical correlations and balanced stochastic realizations are then analyzed in this setting in Section 4, and the basic concepts and principles used in the subspace identification methods, as well as in the model reduction procedures of Desai and Pal, are translated into the more natural context of geometric stochastic realization theory. Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact infinite covariance sequence (1.4). Consequently, the difficult question of positivity is not an issue here. Nor is it the finite sample size per se which is the problem, but the fact that only a finite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is finite. Therefore we separate these issues by discussing stochastic realization theory from finite covariance data in Section 5 and subspace identification in Section 6. In this framework we show that the method of van Overschee and De Moor (1993) is valid under some rather stringent assumptions. We stress that we are only concerned with identification procedures for state space modeling of time series. "Subspace identification" methods for deterministic systems with measurable inputs or for spectral factors do not involve positivity, but stability may still be a problem. However, the algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part, so the problem of positivity arises here too. Another idea behind the subspace identification methods considered in this paper is to disregard modes corresponding to "small" canonical correlation coefficients. This is called balanced truncation and is in fact a stochastic model reduction procedure. In all such procedures there must be a guarantee that the reduced-degree matrix function (1.6) is positive real, and therefore the preservation of positivity in such reductions is a main concern of this paper. Section 7 is devoted to such issues. The model reduction procedure of Desai and Pal (1982) was never theoretically justified in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 . Here we shall demonstrate that this reduction procedure produces a positive real, but not in general balanced, reduced model structure. In fact, the singular values of the truncated system are usually not equal to the r first singular values of the original system. It is an interesting fact that the procedure of Desai and Pal does produce balanced truncations for continuous-time stochastic systems. A partial result in this direction was given by Harshavardana, Jonckheere and Silverman (1984), who showed that the truncated function is positive real and conjectured that it is balanced. We shall demonstrate that it is indeed balanced, a result that is actually already contained in the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time depends on the fact that the spectral factors of the truncated approximate spectrum behave differently than in continuous time. While in continuous time the realizations of the reduced spectral factors are proper subsystems, obtained by partitioning the matrices of the realizations of the factors of , this is not the case in discrete time, contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced truncation procedure is available in discrete time, but the systems matrices are no longer submatrices of those of the original system, and therefore it is not equivalent to the truncation procedure used in subspace identification. Several of the results of this paper have previously been announced in Lindquist and Picci (1994a)3 and in Lindquist and Picci (1994b).

In Desai et al. (1985) a different model reduction procedure, which is not relevant to subspace identification, is considered, namely "deterministic" model reduction of the minimum phase spectral factors. 3 We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some erroneous statements, was accidentally published in place of the paper finally submitted for publication. The correct version can be obtained from the authors.

2

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances The solution to the minimal partial realization problem , i.e., the problem to find ¯ ) satisfying (1.1) is in general not unique. This lack of uniquethe triplet (A, C, C ness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1) equals that of {0 , 1 , . . . ,  -1 } so that we can use a Hankel matrix (1.5) based allowing us to define the shifted Hankel matrix  3 4 2  3 4 5  (H ) =  . . .  . . . . . . i+1 i+2 i+3 (2.1)

on this data, i.e., with i + j =  ,  · · · j +1 · · · j +2   . ... . .  · · · 

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) pro¯ ) which is unique up to a similarity transformation. duces a minimal solution (A, C, C As first pointed out by Zeiger and McEwen (1974), the minimal factorization on which the Ho-Kalman procedure is based may be performed by singular-value decom¯ ) uniquely; see also Kung (1978). In fact, the Hankel position, thereby fixing (A, C, C matrix H may be factored as H = U V U U = I = V V, (2.3) where  is the square n × n diagonal matrix of the nonzero singular values taken in ¯ := V 1/2 this leads to a factorization decreasing order. Setting  := U 1/2 and  ¯ H =  ¯ ¯ == (2.4)

¯ ) is obtained by solving of the type (1.5). Then a minimal realization (A, C, C ¯ =  (H ), A ¯ = 1 (H ) and C ¯  = 1 (H ), C

where  (H ) is the shifted Hankel matrix (2.2) and 1 (H ) is the first block row of H . ¯ ) must be given by It follows that the triplet (A, C, C A = -1/2 U  (H )V -1/2 , C = 1 (H )V -1/2 , ¯ = 1 (H )U -1/2 , C (2.5a) (2.5b) (2.5c)

a form to which we refer as finite-interval balanced, since it is balanced in the sense ¯ are both equal to , and that ¯ that   and      ¯ C C ¯  CA   CA  ¯   .  = (2.6) = . .    . . .  . ¯ (A )j -1 CAi-1 C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identification of time series. The problem with such a strategy is that this algorithm is a deterministic realization procedure and hence does not a priori insure that (1.6) is positive real, or even stable for that matter, even if the Toeplitz matrix T is positive definite. In fact, it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space of covariance data (1.1) for which A is not stable, and a fortiori the same holds for positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in Aoki (1990) is based on the following hidden assumption which is not entirely natural. Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H . Therefore, not only must we know that there exists an underlying finite-dimensional system, but we must also have some upper bound for its dimension. A conservative ]. upper bound which will always suffice is [  2 Is this assumption natural? If the covariance data are really generated exactly from a "true" stochastic system and there is a reliable estimate of its order which is no more than half of the length of the covariance sequence, then the assumption will hold. However, and this is an important point of this paper, one cannot expect Assumption 2.1 to hold for an arbitrary covariance sequence (1.1). To clarify this point, let us agree to call {0 , 1 , 2 , . . . } a minimal rational extension of {0 , 1 , . . . ,  } if the rational function (1.7) has minimal degree. By definition this is the algebraic degree of {0 , 1 , . . . ,  }. A rational extension is called positive if, for every µ >  , the block Toeplitz matrices Tµ formed from the corresponding infinite sequence (1.4) are positive definite. An extension with this property is called a positive rational extension. It is well known that the extension {0 , 1 , 2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function Z (z ) is analytic in the closed unit disc and the matrix function (z ) = Z (z ) + Z (1/z ) (2.7)

is nonnegative definite on the unit circle, making  a spectral density matrix. A minimal positive rational extension of the finite sequence (1.1) is one for which the ¯ ) in (1.6) is as small as possible. dimension of the triplet (A, C, C Definition 2.2. The positive degree p of the finite covariance sequence {0 , 1 , . . . ,  } is the dimension of any minimal positive extension. A well-known example of a positive extension is the maximum entropy extension (Whittle, 1963) corresponding to the spectral density (z ) := W (z )W (1/z ) , where the spectral factor W (z ) is (modulo a multiplicative constant matrix) the inverse of the Levinson-Szeg¨ o matrix polynomial of order  corresponding to the finite covariance sequence (1.1). Since the rational function W (z ) generically has the McMillan degree equal to m , it follows from spectral factorization theory (Anderson, 1958) that Z (z ) has also degree m . Consequently, the positive degree p is bounded from below by the algebraic degree r and from above by m . As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use algebraic rather than positive extensions, usually computed by minimal factorization a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1 may also be formulated in the following way. Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree. This assumption prescribes a property of the covariance sequence (1.1) which is not generic. We can illustrate this point by considering the rational extension problem for a finite scalar covariance sequence (1.1). The positive degree p lies between the algebraic degree r and  . Note that neither the case p =  nor the case p <  are "rare events", because there are open sets of covariance sequences (1.1) of both categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each µ  µ   there is an open set of covariance data in R for which p = µ. such that  2 If the upper limit p =  is attained there are infinitely many nonequivalent minimal ¯ ) providing a positive extension, one of which is the maximum entropy triplets (A, C, C extension. In fact, it can be shown that these  -dimensional extensions form an Euclidean space (Byrnes and Lindquist, 1989). This shows that the finite data (1.1) never contains enough information to establish a "true" underlying system. A similar statement can be made in the case when p <  . Example 2.3. Consider the case m = 1 and  = 2, i.e., consider a scalar partial covariance sequence {0 , 1 , 2 }. If 1 = 2 = 0, we have r = p = 0. Otherwise, we always have r = 1, whereas the positive degree can be either one or two. In fact, 2 setting 0 := 1 /0 and 1 := (2 1 + 2 )/(1 - 1 ), it can be shown (Georgiou, 1987; also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1 if and only if |0 | |1 | < 1 + |0 | and p = 2 otherwise. In fact, it is not hard to construct examples for which the gap between algebraic and positive rank is arbitrarily large, as the following theorem shows. Theorem 2.4. Let n  Z+ be fixed. Then for an arbitrarily large  there is a stable rational function Z (z ) of degree n, such that the Toeplitz matrix T formed as in ( 1.8) from the coefficients of the Laurent expansion ( 1.7), is positive definite while T +1 is indefinite. Consequently, you cannot test the positivity of a rational extension of (1.1) by checking a finite Toeplitz matrix, however large is its dimension. The proof of Theorem 2.4 is given in Appendix A. Let us now return to the identification procedure of Aoki (1990). In practice the rank of H will always be full, and to compute a partial realization of reasonable dimension the basic idea is to partition  as = 1 0 , 0 2 (2.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in 2 are smaller than those in 1 , perhaps close to zero, and then take 2 = 0 so that H is approximated by H1 = U 1 0 V = U1 1 V1 . 0 0 (2.9)

The matrix H1 is a best approximation (given the rank) of H in (the induced) 2 ­ norm, but it is in general not Hankel and hence can not be used to determine a reduced order system. Of course, one may instead use Hankel-norm approximation (Adamjan, Arov and Krein, 1971), which produces another best approximation of H in 2 -norm that is Hankel and has the same rank as H1 . However, if 2 is "very small" compared to 1 , then H1 is close to H and hence approximately Hankel. For this reason, Aoki's procedure (Aoki, 1990) is based on the original data H and  (H ). Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I , the ¯r ) given by same type of calculation as above yields the reduced triplet (Ar , Cr , C Ar = 1
- 1/ 2

U1  (H )V1 1
- 1/2

- 1/ 2

,

(2.10a) (2.10b) (2.10c)

Cr = 1 (H )V1 1 , 1 /2 ¯r = 1 (H )U1 - C . 1

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal subsystem truncation in the sense that, if H is produced by a finite-dimensional system ¯ ) having finite-interval balanced form (2.5), we have with (A, C, C Ar = A11 , where A= A11 A12 A21 A22 C = C1 C2 ¯1 C ¯2 . ¯= C C (2.12) Cr = C1 , ¯r = C ¯1 , C (2.11)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for  (H ), 1 (H ) and 1 (H ) and inserting in (2.10). However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that ¯1 + 1 0 (2.13) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real, provided of course that Z , defined by (1.6), is positive real. The question of stability was answered in the affirmative in Pernebo and Silverman (1982) and is addressed in Aoki (1990). The crucial question of positivity, however, is not discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be proven for a somewhat modified procedure described below. In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall consider a normalized Hankel matrix
1 -T ^ = L- H + HL- ,

(2.14)

where L- and L+ are lower triangular Cholesky factors of the Toeplitz matrices T- and T+ of (1.1) and the corresponding sequence of transposed covariances respectively; see Section 4 below. This is also the Hankel matrix considered in van Overschee and ^ instead of H , the De Moor (1993). Taking the singular value decomposition of H

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coefficients, i.e., the cosines of the angles between the past and the future of the process y . The systems matrices can be determined in a manner analogous to (2.5), but now
-1 -1 ¯ ^ = ¯ T-  =  T+

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as ^ =U ^ ^U ^ so that H = above. To see this, consider the singular value decomposition H ¯ ^ ^ ^ (L+ U )(L- V ) . Since H =  and this factorization is unique modulo coordinate ¯ = L- V ^ ^ 1/2 and  ^ ^ 1/2 . Then transformation in state space, we may take  = L+ U ^ ^ ^ ^ (2.15) follows from U U = I = V V . As we shall see next, (2.15) corresponds to a more natural type of balancing corresponding to a Hankel operator describing the interface between the past and the future of the time series y . 3. Stochastic realization theory in the Hilbert space of a sample function In this section we introduce a mathematical framework which is suitable for the identification problem described above. We define a Hilbert space of observed (infinite) strings of data {yt }. This framework turns out to be isomorphic to that of geometric stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we also establish a correspondence which converts operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. In identification we have access only to a finite string of data {y0 , y1 , y2 , . . . , yT }. (3.1)

Here T may be quite large but, of course, always finite. To begin with, we shall, however, consider the idealized situation that we are given a doubly infinite sequence of m-dimensional data {. . . , y-3 , y-2 , y-1 , y0 , y1 , y2 , y3 . . . } (3.2)

together with a corresponding covariance sequence {k }k0 , each matrix k of the sequence being computed from the data (3.2) by an ergodic limit of the type (1.11). In Section 5 we then modify the theory to handle the situation of finite data (3.1). For each k  Z define the m ×  matrix y (t) := [yt , yt+1 , yt+2 , . . . ] (3.3)

and consider the sequence y := {y (t)}tZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of all finite linear combinations ak y (tk ); ak  Rm , tk  Z

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product defined by linear extension of the bilinear form 1 a y (k ), b y (j ) := lim T  T + 1
t0 +T

a yt+k yt+j b = a k-j b,
t=t0

(3.4)

which clearly does not depend on t0 . This inner product is nondegenerate if the Toeplitz matrix Tk , constructed from the covariance data {0 , 1 , . . . , k }, is a positive definite symmetric matrix for all k . Here we shall assume that the sequence {Tk }k0 is actually coercive, i.e., Tk > cI for some c > 0 and all k  0. (See Assumption 3.2 below for an alternative characterization.) We also define a shift operator U on the family of semi-infinite matrices (3.3), by setting Ua y (t) = a y (t + 1) t  Z, a  Rm ,

defining a linear map which is isometric with respect to the inner product (3.4) and extendable by linearity to all of Y . In particular the sequence of matrices {y (k )} corresponding to the time series y is propagated in time by the action of the operator U, i.e., yi (t) = Ut yi (0), i = 1, 2, . . . , m, t  Z, (3.5)

where yi denotes the i:th row component of y . Then, closing the vector space Y in the inner product (3.4), we obtain a Hilbert space H (y ) := cl Y . The shift operator U is extended by continuity to all of H (y ) and is a unitary operator there. As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results in the geometric theory of stochastic realization can be carried over to the present framework by merely identifying the time series y with a stationary stochastic process y. In particular, the subspaces H - and H + of H (y ) generated by the elements (3.3) for t < 0 and t  0 respectively can be regarded as the past and future subspaces of the stationary process y. For reasons of uniformity of notation the inner product (3.4) will also be denoted ,  = E { }, (3.6)

as the frameworks are completely equivalent. Here we allow E {·} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity condition introduced above insures that tZ Ut H - = 0 and tZ Ut H + = 0, i.e., y is a purely nondeterministic sequence. As we have pointed out above, the subspace identification methods of Aoki (1990) and van Overschee and De Moor (1993) are based on the assumption that the available data is generated by an underlying stochastic system of finite dimension. More specifically, using the notations introduced above, we assume that the data are generated by a linear system of the type x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (3.7)

12

ANDERS LINDQUIST AND GIORGIO PICCI

defined for all t  Z, where w is some vector-valued normalized white noise time series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability matrix. Throughout this paper we shall assume (without restriction) that (A, B, C ) B is a minimal triplet and that the matrix has linearly independent columns. D The system is assumed to be in statistical steady state so that the n-dimensional state x and the m-dimensional output y are uniquely defined by (3.7) as linear causal functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and y (s) will depend only on the difference t - s. We shall think of the system (3.7) as a representation of the output time series y . The state and input variables x and w are introduced in order to display the special structure of the dynamic model of y and are by no means unique. Such a representation is called a state-space realization of y . Remark 3.1. Despite the fact that the model (3.7) is defined in terms of sample sequences, all equalities must be understood in the sense of Hilbert space metric, just as in the case of models based on random variables. The number of state variables n is called the dimension of the realization. A realization is minimal if there is no other realization of y of smaller dimension. In this case the covariance matrix of the state vector, P = E {x(t)x(t) } is positive definite. Moreover as the matrix (3.8)

B is taken with linearly independent D columns, the number of (scalar) white noise inputs p is also as small as possible. Clearly, the covariance sequence {0 , 1 , 2 , . . . } of the output {y (t)} of a minimal model (3.7) is a rational sequence of degree n, i.e., represented as ¯ = AP C + BD ¯ k = 0, 1, 2, . . . where C k = CAk-1 C 0 = CP C + DD . (3.9)

In the following we shall need to assume that the corresponding spectral density (z ) satisfies the following condition. Assumption 3.2. The spectral density  of the output process of the underlying system (3.7) is coercive in the sense that (ei ) > 0 for all   [0, 2 ]. (3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent sequences. Recall that a positive real function Z such that (z ) := Z (z ) + Z (z -1 ) satisfies (3.10) is called strictly positive real. Let H (w) be the Hilbert space generated by w, i.e. the closure of the linear space spanned by the family {wi (t), i = 1 . . . p, t  Z} with respect to the metric induced by the inner product ,  = E {  } where E {·} is defined by (3.6). Let H + and H - be the subspaces of H (w) generated by the components of future {y (0), y (1), y (2) . . . } and past outputs {y (-1), y (-2), y (-3) . . . }, respectively.
4

This means that E {w(t)w(s) } = Its where ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace X := {a x(0) | a  Rn } (3.11)

is invariant under coordinate changes of the type (A, B, C )  (T AT -1 , T B, CT -1 ) and is a coordinate-free representation of the realization (3.7). Such an object is called a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next define the stationary Hankel operator of y , H : H +  H - as H := E H |H +
- -

(3.12)

where E H  is the orthogonal projection of  onto H - . The splitting subspace property of X is equivalent to the commutativity of the diagram H+ O i.e. to the factorization H = CO ,
+ -

- H - C X

H

(3.13)

where the operators O := E H |X and C := E H |X are the observability respectively constructibility operators relative to the splitting subspace X . It can be shown that the splitting subspace X is minimal if and only if O and C are both injective. (See, e.g., Lindquist and Picci (1991).) The system (3.7) is a forward or causal realization of y in the sense that the subspace + H (w), generated by the future of w, is orthogonal to X and H - , i.e. to the present state and past output. Corresponding to (3.7) there is another realization ¯w ¯(t) + B ¯ (t - 1) x ¯(t - 1) = A x ¯x ¯w y (t - 1) = C ¯(t) + D ¯ (t - 1) (3.14)

¯ ), generated by which is backward or anticausal in the sense that the subspace H - (w + ¯(0) is a basis in X , i.e. the past of w ¯ , is orthogonal to X and H . Like x(0), x X := {a x ¯(0) | a  Rn }. ¯ = P -1 P x ¯(0) = P -1 x(0). (3.15)

In fact, x ¯(0) is the dual basis of x(0) in the sense that E {x(0)¯ x(0) } = I . Hence (3.16)

The particular notations used in (3.7) and (3.14) reflect the special meaning of the ¯ ). Computing the covariance matrix of the output using the dual parameters (A, C, C ¯ ) is precisely a realizations (3.7) and (3.14), it is in fact readily seen that (A, C, C triplet realizing the positive real part (1.6) of the spectral density matrix (z ) of the time series y . There are infinitely many minimal factorizations (3.13), one for each Markovian splitting subspace, but the basis in each state space X can be chosen so ¯ ) are the same for each minimal X . This is called a uniform that the triplets (A, C, C choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward predictor spaces X- = E H H +
-

X+ = E H H - ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12) and of its adjoint, respectively. ¯ ), the splitting Fixing a uniform choice of bases, and thus the triplets (A, C, C subspace X- has the forward stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) with state covariance P- , and X+ has the backward realization ¯+ w x ¯+ (t - 1) = A x ¯+ (t) + B ¯+ (t - 1) ¯ +w ¯x ¯+ (t - 1) y (t - 1) = C ¯+ (t) + D (3.19) (3.18)

¯+ . with state covariance P These two stochastic realizations will play an important role in what follows. In fact, an important interpretation of these realizations is that
-1 [y (t) - Cx- (t)] x- (t + 1) = Ax- (t) + B- D-

is the unique steady-state Kalman filter of any minimal realization (3.7) of y in the fixed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8) ¯+ )-1 , then corresponding to the forward counterpart of (3.19), i.e., P+ = (P P -  P  P+ for the state covariance of any minimal realization (3.7). In the same way
-1 ¯+ ¯+ D ¯+ (t) + B [y (t - 1) - C x ¯+ (t)] x ¯+ (t - 1) = A x

(3.20)

is the backward steady-state Kalman filter of all minimal backward realizations (3.14), and ¯+  P ¯P ¯- P ¯- is the backward counfor an arbitrary backward minimal realization (3.14), where P terpart of P- . 4. Canonical correlations and balanced stochastic realization In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a finite-dimensional realization of the type (3.7). Equivalently, we study certain factorizations of the infinite Hankel matrix of the corresponding infinite covariance sequence {0 , 1 , 2 , . . . }. Some portions of this section can be found in an equivalent but somewhat different setting in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and set notations. This will be done in the geometric framework of Section 3, thereby providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall introduce orthonormal bases in H - and H + . To this end it will be useful to represent past and future outputs as infinite vectors in the form,     y (-1) y (0) y (-2) y (1)    y = (4.1) y- =  + y (-3) y (2) . . . . . . Let L- and L+ be the lower triangular Cholesky factors of the infinite block Toeplitz matrices T+ := E {y+ y+ } = L+ L+ T- := E {y- y- } = L- L- and let
1  := L- - y- 1  ¯ := L- + y+

(4.2)

be the corresponding orthonormal implies that  1 2 H := E {y+ y- } =  3 . . .

bases in H - and H + respectively. Now, (3.9) 2 3 3 4 4 5 . . . . . .    ¯  C C ... ¯  . . .  CA   CA = 2  ¯ 2 , . . . CA  C (A )  . . ... . . . .

(4.3)

and therefore we have the following representation result, which can be found in Desai et al. (1985). Proposition 4.1. Let y be realized by a finite dimensional model of the form (3.7). Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator H is
1 -T -1 ¯ -T ^  = L- H + E {y+ y- }L- = L+  L- ,

(4.4)

where

 C  CA   = CA2  . . . 

and

¯  C ¯  CA  ¯   = C ¯ (A )2  . . . .



(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization (4.3) for H , irrespective of which X (i.e. which minimal realization of y ) is chosen. Recall that the adjoint O of the observability operator O is defined as the unique linear operator H +  X such that O,  = , O  for all   X and   H + . Orthogonality implies that E H ,  = ,  = , E X  , and therefore O = E X |H + . In the same way, we see that C  = E X |H - . The finiterank linear operators O O and C  C are defined on X and are the coordinate-free representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O O is full rank and constructible if and only if C  C is full rank. The following representations show that these gramians are related
+

16

ANDERS LINDQUIST AND GIORGIO PICCI

¯+ , the state covariances of the forward and backward steady-state Kalman to P- and P filters (Picci and Pinzoni, 1994). Proposition 4.2. Let x(0) and x ¯(0) be the conjugate basis vectors in a minimal splitting subspace X as defined above. Then, in a uniform choice of bases, ¯+ x(0) ¯(0) = a P O O a x and C  C a x(0) = a P- x ¯(0), (4.7) ¯+ , respectively, independently i.e., C  C and O O have matrix representations P- and P of X . Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal, E H a x(0) = a x- (0), C  C a x(0) = E X a x- (0) = E X a P- x ¯- (0). But, since the bases x ¯(0) and x ¯- (0) are chosen uniformly, EX a x ¯- (0) = a x ¯(0) a  Rn , and consequently (4.7) follows. The proof of (4.6) is analogous. The factorization (4.4) can also be derived from (3.13) and the following useful matrix representations of the observability and constructibility operators. Proposition 4.3. Let x(0) and x ¯(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
T ¯(0) = a  L- ¯ Oax +  1 O b  ¯ = b L- + x(0)
-

(4.6)

and therefore

(4.8)

and
T ¯ L- C a x(0) = a  -  1¯ x(0), C  b  = b L- - ¯

(4.9)

¯ are given by (4.5). where  and  Proof. Since, in view of (3.7), y+ = x(0) + terms which are orthogonal to X,
1 and  ¯ = L- + y+ , we have 1 E { ¯x(0) } = L- + P.

(4.10)

Consequently, for any a  Rn , the usual projection formula5 yields O a x(0) = E H a x(0) = a E {x(0)¯  } ¯ and O b  ¯ = EX b  ¯ = b E { ¯x(0) }P -1 x(0), from which (4.8) follows. A symmetric argument yields (4.9).
If   H (w) and the subspace Z  H (w) is spanned by the components of the full-rank random vector z , then E Z  = E {z }(E {zz })-1 z .
5
+

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should note that the matrix representations of O and C  are the transposes of those of O ¯ = I . Moreover, it follows and C if and only if x(0) is an orthogonal basis, i.e., P = P from (4.8) that -1 ¯(0) = a  T+ x(0), O Oa x
-1 showing that  T+  is a matrix representation of O O, in harmony with the analysis at the end of Section 2. In the same way, (4.9) yields -1 ¯ ¯ T- ¯ x(0), C  C a x(0) = a  -1 ¯ ¯ T-  is a matrix representation of C  C . Together with Proposition 4.2 and hence  ¯+ : this yields the following explicit formulas for P- and P -1 ¯+ =P  T+ -1 ¯ ¯ T-   = P- .

(4.11)

Now, let {1 , 2 , 3 , . . . } be the singular values of the Hankel operator H. Since rank H = n, i = 0 for i > n. The nonzero singular values 1  1  2  3 . . .  n > 0 (4.12)

are the cosines of the angles between the subspaces H- and H+ ; they are known as the canonical correlation coefficients of y (Hotelling, 1936, Anderson, 1958). Obviously 1 < 1 if and and only if H-  H+ = 0. The squares of the canonical correlation coefficients are the eigenvalues of H H, i.e.,
2 i , H H i = i

which, in view of (3.13) may be written
2 (O i ), O OC  C (O i ) = i

and therefore, as was also demonstrated in Picci and Pinzoni (1994),
2 2 2 , 2 , . . . , n }, {O OC  C} = {1

(4.13)

2 2 2 , 2 , . . . , n are the eigenvalues of O OC  C . But, in view of Proposition 4.2, i.e., 1 this is precisely the coordinate-free version of the invariance condition 2 2 2 ¯+ } , 2 , . . . , n } = {P- P {1

(4.14)

of Desai and Pal (1984). This suggests that an appropriate uniform choice of bases would be the one that ¯+ equal and equal to the diagonal matrix of nonzero canonical corremakes P- and P lation coefficients. ^  is the In fact, in view of Proposition 4.1, the infinite normalized Hankel matrix H matrix representation of the operator H in the orthonormal bases (4.2). Therefore ^  has the singular-value decomposition H ^  = U   V = U  V , H   = diag{1 , 2 , 3 , . . . , n }, (4.15) where  is the diagonal n × n matrix consisting of the canonical correlation coefficients (4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and  is the infinite matrix  =  0 . 0 0

Moreover U and V are infinite orthogonal matrices, and U and V are  × n submatrices of U and V with the the property that U U = I = V V. (4.17) We now rotate the the orthonormal bases (4.2) in H + and H - to obtain u := U  ¯ and v := V  respectively. Note that E {uv } =  . What makes these orthonormal bases useful is that they are adapted to the orthogonal decompositions6 H -  H + = [H -  (H + ) ]  H  [H +  (H - ) ], (4.18)

where H := X-  X+ is the so-called frame space (Lindquist and Picci (1985, 1991), in the sense that X- = span{v1 , v2 , . . . , vn } X+ = span{u1 , u2 , . . . , un }. This is true since X- is precisely the subspace of random variables in H - having nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H - . Since therefore {vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H -  (H + ) and H +  (H - ) , respectively, these spaces will play no role in what follows. Now define the n-dimensional vectors  1/2   1/2  1 u1 1 v1   1/2 u    1/2 v  2 2   1 1 z ¯ =  2 .  = 1/2 U L- (4.19) z =  2 .  = 1/2 V L- - y- + y+ . .  .   .  n vn
1/2

n un

1/ 2

¯ is a basis in X+ , and they From what we have seen before, z is a basis in X- and z have the properties ¯z ¯ }. E {zz } =  = E {z (4.20)

In fact, we even have more as seen from the following amplification7 of a theorem by Desai and Pal (1984) (Theorem 1). Theorem 4.4. The basis vectors x- (0) = z x ¯+ (0) = z ¯ (4.21)

in X- and X+ respectively belong to the same uniform choice of basis, i.e. to the ¯ ), and in this uniform choice same choice of triplets (A, C, C ¯+ . P- =  = P
6 7

(4.22)

The symbols  and  denote vector sum and orthogonal vector sum of subspaces. ¯ ). A priori there is no reason why choosing bases in X- and X+ would lead to the same (A, C, C This important property is explicitly mentioned in Theorem 4.4.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coefficients {1 , 2 , 3 , . . . , n } are distinct, this is, modulo multiplication with a signature matrix 8 , the only uniform choice of bases for which ( 4.22) holds. ¯ ) is know as stochastically balanced, and, in the case of Such a choice of (A, C, C distinct canonical correlation coefficients, it defines a canonical form with respect to state space isomorphism in (1.6) by fixing the sign in, say, the first element in each row of C . Such canonical forms have also been studied by Ober (1991). Proof. It follows from (4.4) and (4.15) that E {z ¯z } = 2 . (4.23) ¯ ) so that x ¯, and let the bases in the other splitting Now, choose (A, C, C ¯+ (0) = z subspaces be chosen accordingly so that the choice of bases is uniform. We want ¯+ (0) and that to show that x- (0) = z . To this end, first note that x+ (0) = -1 x x- (0) = E X- x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula and the fact that z is a basis in X- , ¯z }-1 z, x- (0) = -1 E {z which, in view of (4.23), yields x- (0) = z as claimed. Hence (4.22) follows from (4.20). ¯ ) is another uniform choice of bases which Next, suppose that (QAQ-1 , CQ-1 , CQ is also stochastically balanced. Since then x- (0) = Qz and, as is readily seen from ¯+ = Q-T Q-1 , ¯ so that P- = QQ and P the backward system (3.14), x ¯+ (0) = Q-T z (4.22) yields QQ =  and Q-T Q-1 = , from which we have Q2 = 2 Q. Since  has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959) that there is a scalar polynomial (z ) such that Q = (2 ). Hence Q is diagonal and commutes with  so that, by QQ = , we have QQ = I. Consequently, since Q is diagonal, it must be a signature matrix. In view of (4.21) and (3.16), the first of relations (4.9) and (4.8) respectively yield -1 -1 ¯ T- y- z ¯ =  T+ y+ . (4.24) z= Consequently, in view of (4.20), (2.15) holds also for the case of an infinite Hankel matrix. This can of course also be seen from (4.11). Note that the normalization of the block Hankel matrix H is necessary in order for the singular values to become the canonical correlation coefficients, i.e., the singular values of H. In fact, if we were to use the unnormalized matrix representation (4.3) of H instead, as may seem simpler and more natural, the transpose of (4.3) would not be the matrix representation of H in the same bases, a property which is crucial in the singular value decomposition above. This is because (4.3) corresponds to the bases y- in H - and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the finite-dimensional case studied in ^ , defined in Section 2, is Section 2, and therefore the normalized Hankel matrix H preferable to the unnormalized H . ¯ in terms of the Hankel matrix H , can Formulas, such as (2.5), expressing A, C, C be easily derived from basic principles. In fact, standard calculations based on the forward model (3.7) and the backward model (3.14) yield A = E {x(1)x(0) }P -1 C = E {y (0)x(0) }P -1 , ¯ -1 = E {y (-1)x(0) } ¯ = E {y (-1)¯ C x(0) }P for any dual pair of bases x(0) and x ¯(0). Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases (4.19) can be computed by means of the formulas
1 -T - 1/ 2 , A = -1/2 U L- +  (H )L- V  T - 1/2 C = 1 (H )L- , - V - T - 1 / 2 ¯ = 1 (H )L+ U  C ,

(4.25a) (4.25b) (4.25c)

(4.26a) (4.26b) (4.26c)

where H is the unnormalized Hankel matrix (4.3),  (H ) is obtained from H by deleting the first block row, and 1 (H ) is the first block row. Proof. First, in (4.25a) and (4.25b), we take x(0) to be x- (0). By the Kalman filter representation a [x+ (1) - x- (1)]  UH -  H - for all a  Rn ,
-1 ¯+ E {x ¯+ (1)x- (0) }. E {x- (1)x- (0) } = E {x+ (1)x- (0) } = P

¯ ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19), But (A, C, C 1 1 ¯+ , x- (0) = 1/2 V L- ¯+ (1) = 1/2 U L- P- =  = P - y- and x +  (y+ ), where  (y+ ) is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),
1 -T - 1/ 2 , A = -1/2 U L- + E { (y+ )y- }L- V 

which is identical to (4.26a). Likewise, from (4.26b),
T - 1/ 2 , C = E {y (0)y- }L- - V

which yields (4.26b). Finally, taking x ¯(0) to be x ¯+ (0) in (4.25c), a symmetric argument yields (4.26c). Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from finite covariance data In this section we modify the realization theory of Section 4 to the case that only a finite segment {y (0), y (1), y (2), . . . , y ( )}, (5.1) of the time series {y (t)} is available. We still define each y (t) as the semi-infinite string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact partial covariance sequence {0 , 1 , 2 . . . ,  }. (5.2)

The corresponding realization problem, which is purely theoretical and is intended to prepare for the more realistic identification situation with finite strings of observed data (Section 6), is therefore the partial stochastic realization problem mentioned in Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the output of some minimal "true" system (3.7) of dimension n and that  is large enough for n to equal the positive degree of the partial sequence (5.2). Now, suppose that  = 2 - 1, and partition the data into two matrices     y (0) y ( )  y (1)   y ( + 1)  - +   , = y = (5.3) y . .      . . . . y ( - 1) y (2 - 1) representing the past and the future respectively, and define the corresponding (finite- + and y respectively as dimensional) subspaces Y- and Y+ spanned by the rows of y explained in Section 3. Since the data size  will be important in the considerations that will follow, we denote the finite block Hankel matrix H of Section 2, relative to the data (5.3), by H , i.e.,
+ - H = E {y (y ) }.

(5.4)

Let 0 be the smallest integer  such that rank H = n. It is well-known that 0 is ¯ ), so n is an the maximum of the observability and constructibility indicies of (A, C, C upper bound for 0 . As pointed out in the beginning of Section 2, we need  > 0 to ¯ ). be certain that the factorization of H yields a unique (A, C, C Next we shall consider the class of minimal splitting subspaces for Y- and Y+ , i.e., the subspaces X admitting a canonical factorization Y+  O of the finite-interval Hankel operator H := E Y |Y+ .
-  - Y- C X

H

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces, ^  - = E Y- Y+ X ^  + = E Y+ Y- , and X

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred to Appendix D. Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary time series {y (t)}. Then, if  > 0 , X := U X is a minimal splitting subspace for Y- and Y+ , and ^  - = E Y- X , X ^  + = E Y+ X . X (5.7) (5.6)

^  - has a unique representation9 Conversely, any basis x ^( ) in X x ^( ) = E Y x( ),
-

(5.8)

^  + has a unique representation ^ where x( ) is a basis in X , and any basis x ¯( ) in X ^ x ¯( ) = E Y x ¯( ),
+

with x ¯( ) a basis in X . As X varies over the family of all minimal Markovian splitting subspaces, the corresponding x(0) [¯ x(0)] constitute a uniform choice of bases. ^ - The stochastic realizations corresponding to the finite-interval predictor spaces X ^  + are nonstationary. However, taking advantage of the representations (5.8) and X and (5.9), we shall be able to express these realizations in such a way that they can ¯ ) corresponding to one uniform be parameterized by the stationary triplet (A, C, C choice of bases, both for the forward and the backward settings. In fact, if the bases ^ x ^( ) and x ¯( ) are chosen so that x( ) and x ¯( ) in representations (5.8) and (5.9) are ¯ ) is used for x( )} = I , then the same choice of (A, C, C dual bases in X , i.e., E {x( )¯ ^  + is called coherent. ^  - and X all X  . Such a choice of bases in X The realizations generated by these coherent bases are precisely the (transient) forward and backward Kalman filters. In fact, the vector x ^( ) is the one-step predictor of x( ) based on Y- and, as shown in Appendix C, it evolves in time as the Kalman filter

X

(5.9)

X

x ^(t + 1) = Ax ^(t) + K (t)[y (t) - C x ^(t)]; where the gain K (t) is given by

x ^(0) = 0,

(5.10)

¯ - AP- (t)C )(0 - CP- (t)C )-1 K (t) = (C and the filter estimate covariance ^(t)^ x(t) } P- (t) = E {x is the solution of the matrix Riccati equation

(5.11)

(5.12)

¯ - AP- (t)C )(0 - CP- (t)C )-1 (C ¯ - AP- (t)C ) P- (t + 1) = AP- (t)A + (C P- (0)) = 0. (5.13)
With slight misuse of notations, the orthogonal projection operator applied to a vector will denote the vector of the projections of the components.
9

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the components of ^ ¯( ) x ¯( ) = E Y x ^  + and are generated by the backward Kalman filter form a basis in X ¯ (t)[y (t - 1) - C ¯x ^ ^ ^ x ¯(t - 1) = A x ¯(t) + K ¯(t)]; with ¯+ (t)C ¯ )(0 - CP ¯ - (t)C ¯ )-1 , ¯ (t) = (C - A P K where ¯+ (t) = E {x ^ ^ P ¯(t)x ¯(t) } is obtained by solving the matrix Riccati equation ¯+ (t)A + (C - A P ¯+ (t)C ¯+ (t)C ¯ )(0 - C ¯P ¯+ (t)C ¯ )-1 (C - A P ¯) ¯+ (t - 1) = A P P ¯+ (2 - 1) = 0. P (5.18) Now, it is well-known that both ^(t)]  (t) = (0 - CP- (t)C )-1/2 [y (t) - C x and ¯P ¯+ (t)C ¯ )-1/2 [y (t - 1) - C ¯x ^  ¯(t) = (0 - C ¯(t)] (5.20) (5.19) (5.17) (5.16) ^ x ¯(2 - 1) = 0, (5.15)
+

(5.14)

are normalized white noises, called the forward respectively the backward (transient) innovation processes. Consequently, we may write the Kalman filter (5.10) as x ^(t + 1) = Ax ^(t) + B- (t) (t) y (t) = C x ^(t) + D- (t) (t) (5.21)

where D- (t) := (0 - CP- (t)C )1/2 and B- (t) := K (t)D- (t). Likewise, the backward Kalman filter (5.10) may be written ¯+ (t)¯ ^ ^ ¯(t) + B x ¯(t - 1) = A x  (t - 1) ¯ ¯ ^ y (t - 1) = C x ¯(t) + D+ (t)¯  (t - 1) (5.22)

¯ + (t) := (0 - C ¯P ¯+ (t)C ¯ )1/2 and B ¯+ (t) := K ¯ (t)D ¯ + (t). Comparing with (3.7) where D and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7) and (3.14) are time-varying and describe the output y only on the interval [0, 2 - 1]. In fact, since ^(t)][x(t) - x ^(t)] }  0, P - P- (t) = E {[x(t) - x ¯ ¯ and, for the same reason, P - P+ (t)  0, we have ¯+ (t)-1 , P- (t)  P  P+ (t) := P (5.23)

^  - and X ^  + are extremal splitting subspaces, so we see that the predictor spaces X just as X- and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the finite-interval counterparts of equations (4.25) are given by A = E {x ^( + 1)^ x( ) }P- ( )-1 C = E {y ( )^ x( ) }P- ( )-1 , ¯+ ( )-1 = E {y ( - 1)^ ¯ = E {y ( - 1)x ^ x( ) } C ¯( ) }P (5.24a) (5.24b) (5.24c)

In complete analogy with the stationary framework in Section 4, the canonical correlation coefficients 1  1 ( )  2 ( )  · · ·  n ( ) > 0 (5.25) between the finite past Y- and the finite future Y+ are now defined as the singular values of the operator H given by (5.5). To determine these we need a matrix representation of H in some orthonormal bases. Using the pair (5.19)­(5.20) of transient innovation processes for this purpose, we obtain the normalized matrix (2.14), which ^  . Singular value decomposition yields we shall here denote H ^  = U  V , H (5.26) where U U = I = V V , and  is the diagonal matrix of canonical correlation coefficients. As in Section 4 it is seen that
-1 - z ( ) =  V (L-  ) y 1/2 -1 + z ¯( ) =  U (L+  ) y 1/2

(5.27)

^  - and X ^  + respectively and that are bases in X E {z ( )z ( ) } =  = E {z ¯ z ¯ }. (5.28)
+ Here L-  and L are the finite-interval counterparts of L- and L+ respectively, and they are of course submatrices of these. Note that H , as defined by (5.4), is now given by - ^ H  = L+  H (L ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z ( ) and z ¯( ) are coherent bases, and the ¯ corresponding triplet (A, C, C ) is a finite-interval stochastically balanced realization, i.e., ¯+ ( ). P- ( ) =  = P (5.30)

The following finite-interval modification of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the finite block hankel matrix H , and the proof is analogous. ¯ ), obProposition 5.2. The finite-interval stochastically balanced triplet (A , C , C ^ tained from (5.24) by choosing the bases x ^( ) = z ( ) and x ¯( ) = z ¯( ), is given by
1 /2 -1 - -T 1/ 2 U (L+ V - , A = -   )  (H )(L )  -T 1/ 2 V - , C = 1 (H )(L-  )  + -T - 1 /2 ¯ C = 1 (H )(L ) U  ,

(5.31a) (5.31b) (5.31c)

where the operators  (·) and 1 (·) are defined as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

¯ ) actually varies with  , but that, for each  , it Note that the triplet (A , C , C ¯ ) of Section 4, i.e., there is a is similar to the stochastically balanced triplet (A, C, C nonsingular matrix Q so that ¯ ) = (Q AQ-1 , CQ-1 , CQ ¯ ). (A , C , C    (5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the stationary predictor spaces X- and X+ will have the state covariances P- = Q Q
T -1 ¯+ = Q- and P  Q ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state covariances are not diagonal and equal is a manifestation of the fact that the triplet ¯ ) is not stochastically balanced in the sense of Section 4. It is well known (A , C , C ¯+ , respectively, as t  , and ¯+ (t) tend monotonically to P- and P that P- (t) and P therefore we have the following ordering ¯+ )-1  (P ¯+ ( ))-1 := -1 . P- ( ) :=   P-  (P


Since the number n of nonzero singular values (5.25) is in general too large too yield a reasonable model, we must consider what happens when some of the smallest singular values are set equal to zero. The truncation procedure employed by van Overschee and De Moor (1993) is equivalent to the principal subsystem truncation presented in Section 2, except that, and this is very important, the singular-value ^  , which is the decomposition is performed on the normalized block Hankel matrix H natural matrix representation of the operator  . It will be shown in Section 7 that such a truncation will preserve positivity in the stationary case (Theorem 7.3). In order to carry this result over to the case of finite  , we need to assume that the spectral density  of the time series {y (t)} is coercive so that Assumption 3.2 is fulfilled, i.e., that the function Z is strictly positive real. The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D, shows that principal subsystem truncation preserves positivity provided  is chosen large enough.

H

Theorem 5.3. Suppose that the spectral density  of the time series {y (t)} is coercive. Then, there is an integer 1 > 0 such that, for   1 , the principal subsystem ¯ )1 ) of (A , C , C ¯ ) is a minimal realization of a strictly truncation ((A )11 , (C )1 , (C positive real function (2.13). 6. Subspace identification The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have access to an infinite sequence (3.2) of data. In reality we will have a finite string of observed data {y0 , y1 , y2 , . . . , yN }, (6.1)

where, however, N may be quite large. More specifically, we assume that N is sufficiently large that replacing the ergodic limits (1.11) by truncated sums yields good approximations of {0 , 1 , 2 . . . ,  }, (6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course,  << N . This is equivalent to saying that T := N -  is sufficiently large for 1 T +1
T

a yt+k yt+j b
t=0

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we shall use the finite-interval realization theory of Section 5 as if we had a finite time series {y (0), y (1), y (2), . . . , y ( )}, while substituting the semi-infinite string (3.3) of data by y (t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . ,  . (6.5) (6.4)

In particular, in this case the inner product becomes merely that of a finite-dimensional Euclidean space so that the block Hankel matrix H can be written H = where   y  -1 y  . . . yT + -1 y -2 y -1 . . . yT + -2  -  y = . . ..  . . . . . . .  . y0 y1 . . . yT 1 y + (y - ) T +1    y +1 . . . yT + y  y +1 y +2 . . . yT + +1  + . = and y . . ..  . . . . . . .  . y2 -1 y2 . . . yT +2 -1 

Consequently, the identification of a minimal stationary state-space innovation model describing the data (6.1) can be performed in the following steps. - + , y to obtain, from (1) Perform canonical correlation analysis on the data y ^ ¯+ ( ) = z ¯( ) and, from (5.26), the (5.27), the state vectors x ^- ( ) = z ( ) and x corresponding common state covariance matrix  , i.e., the diagonal matrix of the (finite interval) canonical correlation coefficients (5.25). (2) Given the singular value decomposition (5.26), compute via (5.31) a minimal ¯ ). This realization will be in finite-interval balanced form, realization (A, C, C i.e., (5.30) will hold instead of (4.22). (3) To obtain a state space model (3.7) for y we need to compute the matrices B ¯ 0 ) defines and D. Note that such matrices will exist if and only if (A, C, C, a positive real function (1.6), or, in other words, if and only if there is a symmetric positive definite P = P such that M (P ) := P - AP A ¯ - CP A C ¯ - AP C C 0 - CP C  0. (6.6)

[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6), B and D can be determined (in a nonunique way) by a full rank factorization of M (P ), i.e., B D B D = M (P ). (6.7)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P- = E {x- (t)x- (t) } has been determined. Obtaining P- amounts to finding the minimal solution of the algebraic Riccati equation ¯ - AP C )(0 - CP C )-1 (C ¯ - AP C ) P = AP A + (C (6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t   with initial condition P- ( ) =  . (The corresponding dual procedures yield ¯+ .) Again, in both cases, a positive definite P- can be found if and only P ¯ 0 ) defines a positive real function (1.6). In fact, in general, if (A, C, C, {P- (t)}t0 may not even converge unless this positivity condition is fulfilled and may in fact exhibit dynamical behavior with several of the characteristics of chaotic dynamics (Byrnes et al., 1991, 1994). Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that, for  fixed but sufficiently large (see Section 2), we will have rank H = n as T  , ¯ ) will be uniquely determined from the data and similar to the and the triplet (A, C, C ¯ triplet (A, C, C ) of the "true" generating system. Hence, in particular, in the limit as T  , at least in theory positivity will be guaranteed. If n ^ is an upper bound for the order of the "true" system, we may choose  to be any integer larger than n ^. In practice, however, T is finite, and even if we had a true system generating exact data, the spectral estimate T , although converging to the true spectrum  as T   may in principle fail to be positive for any finite T if there are frequencies  for which (ei ) = 0. Positivity for a suitably large T can however be guaranteed if the "true" spectrum is coercive. The following proposition, which also applies to Aoki's method discussed in Section 2, is proved in Appendix D. Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are ful¯ ) defined by filled. Then, there is a T0  Z+ such that, for T  T0 , the triplet (A, C, C (5.31) yields a function (1.6) which is strictly positive real. However, in practice, rank H normally will keep increasing with  , even for very large T , so that one must resort to some kind of truncation of the Hankel singular values. As we have pointed out in Section 5, setting all canonical correlation coefficients r+1 ( ), r+2 ( ), . . . equal to zero for some suitable r, as is done in, for example, van Overschee and De Moor (1993), is equivalent to principal subsystem truncation. An important issue is therefore under what conditions such a procedure will insure positivity. Here we must distinguish between problems generated by the sample fluctuations of the data due to finite sample size T , as considered in Proposition 6.1, and the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an infinite string of data generated by a "true" high-dimensional system, such a truncation procedure may fail if  is smaller than that dimension. Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following result, which justifies this approximation procedure, provided the rather stringent Assumption 2.1 holds and we have coercivity, and provided T and  are sufficiently large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulfilled. Then, there are positive integers T0 and 1 > 0 such that, for T  T0 and   1 , the ¯1 ), obtained from (2.12) by taking H := H in (2.10), is a minimal triplet (A11 , C1 , C realization of a strictly positive real function (2.13). We note that, in van Overschee and De Moor (1993), the large Hankel matrix
+ + + + - - - - ~  = (y ) (E {y (y ) })-1 E {y (y ) }(E {y (y ) })-1 y H

^  . This leads to a procedure which is equivalent to the one is used in place of H described above. Moreover, the computation of a second singular-value decomposition + - in van Overschee and De Moor (1993), based on H +1 := E {y +1 (y +1 ) }, together with a subsequent change of bases, is actually redundant, as can be deduced from the following proposition. In fact, a considerable amount of computation is needed in van Overschee and De Moor (1993) to compensate for the fact that taking z ( + 1), ^ ( +1)- would computed from a second singular-value decomposition, as a basis in X lead to a Kalman filter model with time-varying parameters. ^ Proposition 6.3. To each coherent pair of bases x ^( ) and x ¯( ) in the finite-interval ^ ^ predictor spaces X - and X + , there corresponds a minimal factorization ¯ H =   of the block Hankel matrix H . Here
+  x ^( ) = E Y y
- + - ¯x ^ ¯( ) = E Y y  .

(6.9)

and

(6.10)

Conversely, given a minimal factorization (6.9), ¯ (T - )-1 y - x ^( ) =     and
+ ^ x ¯( ) =  (T+ )-1 y

(6.11)

^ +. ^  - and X is a coherent pair of bases in X ^  - and X ^  + . Then, for ^ Proof. Let x ^( ) and x ¯( ) be a coherent choice of bases in X ¯( )) of dual bases any X as defined in Theorem 5.1, there is a unique pair (x( ), x ¯  be the matrices defined via such that (5.8) and (5.9) hold. Let  and 
+ - ¯x E X y =  x( ) and E X y = ¯( ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of X with respect to Y- and Y+ yields + - + - (y ) } = E {E X y (E X y )) }, E {y which, in view of (6.12), is the same as (6.9). Applying E Y and E Y to respectively the first and second equations of (6.12), the splitting property yields (6.10). As for the converse statement, equations (6.11) follow from the construction in the proof of Theorem 5.1, from which it also follows that the resulting bases x ^( ) and ¯ ^ x ¯( ) are constructed from the same (A, C, C ) and therefore coherent. ¯ ) have been fixed by a particular choice of x( ) As soon as the parameters (A, C, C in the representation (5.8) in Theorem 5.1, we must choose x ^( + 1) as x ^( + 1) = E Y +1 Ux( ) (6.13)
- +

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More specifically Proposition 6.3 ¯  are uniquely determined once x( ) has been selected. Hence implies that  and  ¯ ) is uniquely determined by the Ho-Kalman algorithm so that (A, C, C ¯ ¯  +1 = C  ¯  A is prescribed, as is
-1 - ¯  (T- x ^( + 1) =  +1 ) y +1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining x ^( + 1) by an extra singular-value decomposition, as in van Overschee and De Moor (1993), is actually redundant. If we actually were to determine x ^( + 1) as described -L ¯ ¯ above, we would better compute  +1 from  +1 =  H +1 , where the left inverse is very easily obtained from the singular-value decomposition of H . We stress that Assumption 2.1, although quite limiting, is absolutely crucial in insuring that the subspace identification algorithms mentioned above will actually work. Note that for generic data these algorithms may break down for any fixed  . The same is true for all other subspace methods which deal with identification of covariance models (or equivalent) involving stochastic signals. On the other hand, Assumption 2.1 introduces a quite unrealistic condition which, as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure to estimate T0 and 1 in Proposition 6.2, as the proof is based only on continuity arguments. 7. Stochastic model reduction As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step in order to keep the dimension of the model at a reasonable level. To justify any such procedure one must either assume that there is an underlying "true" system of sufficiently low order, i.e., invoke Assumption 2.1, or to perform rational covariance extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995), Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an infinite one. The latter can be done in many ways, one of which is the maximum entropy extension. In either case, the truncation problem is equivalent to approximating a positive real matrix function ¯ + 1 0 , Z (z ) = C (zI - A)-1 C 2 (7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of lower degree. In this section we shall investigate how this can be done and also how such an approximation affects the canonical correlation structure. One main question to be addressed is whether the principal subsystem truncation (2.11) preserves positive realness and balancing, and hence the leading canonical correlation coefficients, as originally claimed by Desai and Pal (1982). As it turns out, the answer is affirmative to the first but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identification approximation obtained by setting some canonical correlation coefficients equal to zero. It is instructive to first consider the continuous-time counterpart of this problem since the latter is simpler and exhibits more desirable properties. Also, it has been widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true. It is well-known [see, e.g., Faurre et al. (1979)] that an m × m matrix function Z with minimal realization ¯ + 1 R, (7.2) Z (s) = C (sI - A)-1 C 2 is positive real with respect to the right half plane if and only if there is a symmetric matrix P > 0 such that ¯ - PC -AP - P A C  0, (7.3) M (P ) := ¯ C - CP R where here we assume that R is positive definite and symmetric. In this case there are two solutions of (7.3), P- and P+ , with the property that any other solution of (7.3) satisfies P -  P  P+ . (7.4)

These extremal solutions play the same role as P- and P+ in the discrete-time setting, and rank M (P- ) = m = rank M (P+ ). (7.5)

-1 ¯+ := P+ If the state-space coordinates are chosen so that both P- and P are diagonal and equal, and thus, by (4.14), equal to the diagonal matrix  of canonical correlation ¯ ) is stochastically balanced. coefficients, we say that (A, C, C Now, suppose that  is partitioned as in (2.8) with r+1 < r , and consider the corresponding principal subsystem truncation (2.12). Using the stochastic realization framework, Harshavaradana, Jonckheere and Silverman (1984) showed that

¯1 + 1 R, Z1 (s) = C1 (sI - A11 )-1 C 2

(7.6)

¯1 ) is a minimal realization of a positive real function and conjectured that (A11 , C1 , C is stochastically balanced. We shall next show that this conjecture is true, as has already been done by Ober (1991) in a framework of canonical forms. First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield   ¯1 - 1 C1 -A11 1 - 1 A11  C   0,     (7.7) ¯  R C1 - C1 1 where blocks which play no role in the analysis are marked by an asterisk. Consequently, M1 (1 ) = ¯1 - 1 C1 -A11 1 - 1 A11 C  0. ¯ C1 - C1 1 R (7.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982), Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane, ¯1 ) is a minimal realization. (7.6) is positive real, but it remains to prove that (A11 , C1 , C This was done in Harshavaradhana et al. (1984). It is important to observe here that, contrary to the situation in the discrete-time setting, rank M1 (1 ) = rank M () = m 1 -1 and rank M1 (- 1 ) = rank M ( ) = m, important facts that will be seen to imply that the reduced system is stochastically balanced. Recall that in the continuous-time setting the spectral density (s) = Z (s)+ Z (-s) is coercive if, for some > 0, we have (s)  I for all s on the imaginary axis. This is equivalent to the condition that R > 0 and  has no zeros on the imaginary axis (Faurre et al., 1979, Theorem 4.17). Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with (s) := ¯ ) be in stochastically balanced form. Then, Z (s) + Z (-s) coercive, and let (A, C, C ¯1 ) defines a positive real function (7.6) if r+1 < r , the reduced system (A11 , C1 , C for which it is a minimal realization in stochastically balanced form, and 1 (s) := Z1 (s) + Z1 (-s) is coercive. Proof. We have already shown that Z1 is positive real, and we refer the reader to ¯1 ) is a minimal realization Harshavaradhana et al. (1984) for the proof that (A11 , C1 , C ¯1 ) is stochastically of Z1 . It remains to show that 1 is coercive and that (A11 , C1 , C -1 balanced, i.e., that P1- = 1 = P1+ , where P1- and P1+ are solutions to the algebraic Riccati equation ¯ - P1 C1 )R-1 (C ¯ - P1 C1 ) = 0 A11 P1 + P1 A11 + (C (7.9) such that any other solution P1 of (7.9) satisfies P1-  P1  P1+ . To this end, 1 -1 note that since M1 (1 ) and M1 (- 1 ) have rank m, both 1 and 1 satisfy (7.9). 1 Therefore, as is well-known (Molinari, 1977) and easy to show, Q := - 1 - 1 satisfies 1 Q + Q1 + QC1 R-1 C1 Q = 0, where ¯ - 1 C )R-1 C1 . 1 = A11 - (C 1 (7.11) Since  is coercive, -1 -  = P+ - P- > 0 (Faurre et al., 1979, Theorem 4.17) so that 1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to 1 Q-1 + Q-1 1 + C1 R-1 C1 = 0. (7.12) (7.10)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , 1 ). Since, in addition, the Lyapunov equation (7.12) has a positive definite solution Q-1 , 1 must be a stability matrix. Therefore 1 is the minimal (stabilizing) solution P1- of -1 ¯1+ := P1+ = 1 . (7.9). In the same way, using the backward setting, we show that P ¯1 ) is stochastically balanced. Since P1+ - P1- > 0, 1 is Consequently, (A11 , C1 , C coercive. ¯ 1 0 ) Let us now return to the discrete-time setting. Let us recall that, if (A, C, C, 2 is a minimal realization of (7.1), the matrix function Z is positive real if and only if the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given the positive real rational function (7.1) with the property that (z ) = Z (z ) + Z (z -1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y , the state covariance P of any minimal stochastic realization (3.7) of y satisfies (6.6) and the matrices B, D in (3.7) satisfy (6.7). Consequently, as pointed out in Section 5, the matrices B and D can be determined via a matrix factorization of M (P ) once P has been determined. ¯ ) is in stochastically balanced form, Theorem 4.4 implies that Now, if (A, C, C M ()  0. In view of (4.16) and (2.12), M () may be written   ¯1 - A11 1 C1 - A12 2 C2 1 - A11 1 A11 - A12 2 A12  C ,     ¯ C1 - C1 1 A11 - C2 2 A12  0 - C1 1 C1 - C2 2 C2 where, as before, the blocks which do not enter the analysis are marked with an asterisk. Since M ()  0, this implies that M1 (1 ) - where M1 (1 ) = ¯1 - A11 1 C1 1 - A11 1 A11 C ¯ C1 - C1 1 A11 0 - C1 1 C1 (7.14) A A12 2 12 C2 C2  0, (7.13)

¯1 ). Thereis the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , C fore, M (1 )  0, so if we can show that A11 is stable, i.e., has all its eigenvalues strictly inside the unit circle, it follows that ¯1 + 1 0 , (7.15) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real. As we shall see below this is true without the requirement needed in continuous time that r+1 < r . ¯1 ) also to be balanced, 1 would have to be the minimal solution P1- For (A11 , C1 , C of M1 (P1 )  0, which in turn would require that rank M1 (1 ) = rank M () = m. Due to the extra positive semidefinite term in (7.13), however, this will in general not be the case and therefore 1  P1- will correspond to an external realization, as will 1 - 1  P1+ ; see Lindquist and Picci (1991). ¯1 ) is minimal we need to assume that  is coercive, or, To show that (A11 , C1 , C equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979, Theorem A4.4) that this implies that P+ - P- > 0. (7.16)

In fact, if 0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to coercivity. Coercivity also implies that 0 - CP- C > 0. (7.17)

¯ ) in balanced form, P- =  = P ¯+ and, in view of (3.16), Remark 7.2. With (A, C, C -1 -1 P+ =  . Hence (7.16) becomes  > , which obviously holds if and only if 1 < 1, which in turn is equivalent to H -  H + = 0. Consequently, given the full rank condition 0 > 0, coercivity is equivalent to the past and the future spaces of y having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

¯ ) be in stochastically balTheorem 7.3. Let (7.1) be positive real, and let (A, C, C anced form. Then the reduced-degree function (7.15) obtained via principal subsystem decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so ¯1 , 1 0 ) is a minimal realization of Z1 . is Z1 , and (A11 , C1 , C 2 For the proof we need the following lemma, the proof of which is given in Appendix D. Lemma 7.4. Let the matrix function Z be given by (7.1), where 0 > 0, but where ¯ A ) are not necessarily observable, and suppose that (6.6) has two (C, A) and (C, positive definite symmetric solutions, P1 and P2 , such that P2 - P1 > 0. Then Z is strictly positive real. Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is stable. To this end, we note that P is the reachability gramian of (3.7). In particular, ¯ ) is stochastically balanced, the reachability gramian of the system (3.18) if (A, C, C equals  so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable. By Remark 7.2, coercivity of  implies that -1 -  > 0, from which it follows 1 that - 1 - 1 > 0 and that 0 > 0. Moreover, By construction, M1 (1 )  0 and -1 M1 (1 )  0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is. To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows by symmetry. By regularity condition (7.17), 0 - C1 1 C1  0 - C C > 0, and consequently, since M1 (1 )  0, 1 satisfies the algebraic Riccati inequality ¯1 - A11 P1 C1 )(0 - C1 P1 C1 )-1 (C ¯1 - A11 P1 C1 )  0, (7.19) A11 P1 A11 - P1 + (C but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable. Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density 1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a unique symmetric P1- > 0 which satisfies (7.19) with equality and for which ¯1 - A11 P1- C1 )(0 - C1 P1- C1 )-1 C1 1- := A11 - (C is stable. It is well-known (Faurre et al., 1979) that P1- is the minimal symmetric solution of the linear matrix inequality M1 (P1 )  0, i.e., that any other symmetric 1 -1 solution P1 satisfies P1  P1- . We also know that M1 (- 1 )  0. Next, since 1 - 1 1 > 0, a fortiori it holds that Q := - 1 - P1- > 0. A tedious but straight-forward calculation shows that Q satisfies 1- (Q-1 - C1 R-1 C1 )-1 1- - Q  0, from which it follows that Q-1 - C1 R-1 C1 - 1- Q-1 1-  0. Cf. Faurre et al. (1979), pp. 85 and 95. (7.20) (7.18)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a  and a   C such that [C1 , I - A11 ]a = 0. and therefore, in view of (7.20), (1 - ||2 )a Q-1 a  0.

Cr

But  is an eigenvalue of the stable matrix A11 , implying that || < 1, so we must have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable. A remaining question is whether there is some balanced order-reduction procedure in discrete time which preserves both positivity and balancing. That this is the case in continuous time implies that the answer is affirmative, but the reduced system cannot be a simple principal subsystem truncation. ¯ ) be in stochastically Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, C balanced form. Moreover, given a decomposition ( 2.12) such that r+1 < r , let Ar Cr ¯r C r0 = = = = A11 - A12 (I + A22 )-1 A21 C1 - C2 (I + A22 )-1 A21 ¯1 - C ¯2 (I + A22 )-1 A12 C ¯2 - C ¯2 (I + A22 )-1 C2 0 - C2 (I + A22 )-1 C

¯r , r0 ) is a minimal realization of a strictly positive real function Then (Ar , Cr , C ¯r + 1 r0 . Zr (z ) = Cr (zI - Ar )-1 C 2 (7.21)

¯r , r0 ) is stochastically balanced with canonical correlation coeffiMoreover, (Ar , Cr , C cients 1 , 2 , . . . , r . To understand why this reduced-order system does preserve both positivity and balancing, note that for   I -A12 (I + A22 )-1 0 I 0 T = 0 -1 I 0 -C2 (I + A22 ) we obtain   ¯r - Ar 1 Cr 1 - Ar 1 Ar  C ,    T M ()T =  ¯r - Cr 1 A  r0 - Cr 1 C C r r

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the reduced-order system, Mr (1 )  0 and rank Mr (1 )  rank M (). ¯r , r0 ) is precisely what one obtains To prove Theorem 7.5 we observe that (Ar , Cr , C ¯ 0 ) by the appropriate linear fractional transform to the if one transforms (A, C, C, continuous-time setting and then, after reduction, back to discrete time again as suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions The purpose of this paper is to analyze a class of popular subspace identification procedures for state space models in the theoretical framework of rational covariance extension, balanced model reduction, and geometric theory for splitting subspaces. We have pointed out that these methods are based on the hidden Assumption 2.1 which is not entirely natural and which is in general untestable. The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially equivalent to the Ho-Kalman algorithm in which the basic factorization is performed by singular-value decomposition of a block Hankel matrix of finite covariance data, as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in orthonormal coordinates and allows for theoretical verification of the truncation step. A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of the data equals the algebraic degree. To achieve this, one must assume that the data are generated exactly by an underlying system and that the amount of data is sufficient for constructing an accurate partial covariance sequence the length of which is sufficient in relation to the dimension of the underlying system. Hence it is absolutely crucial that a reliable upper bound of the dimension of the "true" underlying system is available. We stress that these stringent assumptions are not satisfied for generic data, as was pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that the positive degree has no generic value. In fact, just for the moment considering the single-output case, for each p such that r  p   there is a nonempty open set of partial covariance sequences having positive degree p in the space of sequences of length  . Secondly, for any r, it is possible to construct examples of long partial covariance sequences having algebraic degree r but having arbitrarily large positive degree (Theorem 2.4). In Section 7 we proved an open question concerning the preservation of positivity in the original (discrete-time) model reduction procedure of Desai and Pal (1984). Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to the principal subsystem truncation used in van Overschee and De Moor (1993), but not to the one in Aoki (1990). We prove that positivity is preserved provided that the original data satisfies Assumption 2.1, justifying setting the smaller canonical correlation coefficients equal to zero. Unlike the situation in continuous time, this truncation does not preserve balancing. The validity of the corresponding procedure of Aoki (1990) has not been settled. The contribution of this paper is to provide theoretical understanding of these identification algorithms and to point out possible pitfalls of such procedures. Hence the primary purpose is not to suggest alternative procedures. Nevertheless, we would like to point out that a two-stage procedure equivalent to covariance extension followed by model reduction would work on any finite string of data, thus elimination the need for Assumptions 2.1. However, we leave open the question of how such a procedure should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension or some other. Acknowledgment. We would like to thank the referees and the associate editor for the careful review of our paper and for many useful suggestions, which have led to considerable improvements of this paper. References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31­73. 2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner. 3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley. 4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM J. Control, 13, 162­173. 5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag. 6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems and Control Letters, 2, 2301­2312. 7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization of modelling filter. Inter. J. of Control, 50, 2301­2321. 8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted for publication. 9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40. 10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in Kalman filtering. IEEE Trans. Autom. Control, 36, 563­579. 11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast filtering algorithms. SIAM J. Control and Optimization, 32, 744­789. 12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105­1112. 13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29, 1097­1100. 14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821­839. 15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer. 16. Faurre, P. (1969). Identification par minimisation d'une representation Markovienne de processus aleatoires. Symposium on Optimization, Nice. 17. Faurre, P. and Chataigner (1971). Identification en temp reel et en temp differee par factorisation de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt. 18. Faurre, P., M. Clerget, and F. Germain (1979). Op´ erateurs Rationnels Positifs, Dunod. 19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de r´ ealisation stochastique. C. R. Academie Sciences Paris 268. 20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York. 21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438­449. 22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and their L error bounds. Intern. J. Control, 39, 1115­1193. 23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and its Applications, 50, 277­319. 24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York. 25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling of time series. Econometric Reviews 11, 379­396. 26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321­377. 27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744­746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference, Tel Aviv, Israel. 29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory, McGraw-Hill. 30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta Polytech. Scand., MA31, 9­39. 31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499­513. 32. Kung, S. Y. (1978). A new identification and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705­714. 33. Larimore, W. E. (1990). System identification, reduced-order filtering and modeling via canonical variate analysis. Proc. 29th Conf. Decison and Control, pp. 445­451. 34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809­857. 35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241­333. 36. Lindquist, A. and G. Picci (1994a). On "subspace methods" identification. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds., Akademie Verlag, 315­320. 37. Lindquist, A. and G. Picci (1994b). On "subspace methods" identification and stochastic model reduction. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 397­403. 38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE Trans. Automatic Control, 20, 396­399. 39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica, 13, 347­357. 40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and Optimization, 29, 1251­1287. 41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identification problem. Automatica, 29 , 649-660. 42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica, 30, 75­93. 43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identification algorithms and its interpretation. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 145­156. 44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382­387. 45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes. Linear Algebra and its Applications, 205-206, 957-1003. 46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day. 47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II. Journal fur die reine und angewandte Mathematik, 148, 122­145. 48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401­ 421. 49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation. IEEE Trans. Automatic Control, AC-16, 621­634. 50. Whittle, P. (1963). On the fitting of multivariate autoregressions and the approximate canonical factorization of a spectral density matrix. Biometrica, 50, 129­134. 51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its Applications, Cambridge U.P. 52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via Ho's algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4. We first give a proof for the special case n = 1. Consider a scalar function 1z+b (A.1) 2z +a with a scalar sequence (1.4) such that 0 = 1. Now it is well-known [see, e.g., Schur (1918), Akhiezer (1965)] that T is positive definite if and only if Z (z ) = |t | < 1 t = 0, 1, 2, . . . ,  - 1 (A.2) where {0 , 1 , 2 , . . . } are the so called Schur parameters. There is a bijective relation between partial sequences (1.1) and partial sequences {0 , 1 , . . . ,  -1 } of the same length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the Schur parameters of (A.1) are generated by the nonlinear dynamical system t+1 = t+1 =
t 2 1 - t - t  t 2 1 - t

0 = 1 (a + b) 2 0 = 1 (b - a) 2

(A.3)

and that Tt becomes singular precisely when there is finite escape. It was also shown in Byrnes et al. (1991) that {t } is generated by a linear system 2/ -1 ut+1 = vt+1 1 0 ut , vt (A.4)

where t = vt /ut and  := (a + b)(1 + ab)-1 . If  is greater than one in modulus, the coefficient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant scalar factor, similar to cos  sin  , - sin  cos   where  := arctan 2 - 1. Hence t is the slope of a line through the origin in R2 which rotates counter-clockwise with the constant angle  in each time step. Consequently, arctan t+1 = arctan t + . Moreover, assuming that 0 > 0, the Schur condition t < 1 will fail as soon as t+1 negative or infinite, as can be seen from the first of recursions (A.3). Hence (A.2) holds if and only if  (A.5) arctan  < . 2 Therefore for a small > 0, take a = 1 - and b = 1 + , yielding a stable Z . Then  2 4 - 2 . We may choose so that  = 2- 2 > 1 and  = arctan 2- 2   << , +1  - arctan 0 . Then (A.5) holds so that T > 0, but we also have where  :=  2  arctan  +1 > 2 so that T +1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function (a + b)n-1 (z ) 1 n (z ) + 1 2 Z (z ) = 1 2 n (z ) + 2 (a + b)n-1 (z ) o polynomials of the first and second kind rewhere {t } and {t } are the Szeg¨ spectively (Akhiezer, 1965). The function Z has the property that its first n Schur parameters, {0 , 1 , . . . , n-1 }, are precisely the data which uniquely determines n , n-1 , n and n-1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated by t 0 = 1 (a + b) t+1 = 1- 2 2 t+1 =
- t  t 2 1- t +n-1
t+n-1

Hence, we have reduced the problem to the case n = 1. If we choose the initial Schur parameters sufficiently small so that n (z ) and n-1 (z ) are approximately z n and z n-1 , n (z ) + 0 n-1 (z ) is stable if we choose a := 1 - 2 and b := 1 + for some small > 0. Then  > 1 and the proof for the case n = 1 carries through with a trivial modification. Appendix B. The Hilbert space of a sample function Let y = {y(t)}t0 be a zero-mean wide-sense-stationary stochastic process defined on a probability space {, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t,  ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit exists, the m × m matrix function k  k obtained from a particular trajectory is then a bona-fide covariance function. [The continuous-time analog of this property was observed already by Wiener (1933)]. If moreover the sample limit is (almost surely) independent of the particular trajectory and hence necessarily coincides with the "ensemble" covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in the book of Hannan (1970). It is obvious from Birkhoff's ergodic theorem that any (zero-mean) strictly stationary ergodic process is also second-order ergodic. In this Appendix we shall show that the properties of the Hilbert space structure associated to a stationary time series y , defined on page 10, are identical to those of the Hilbert space induced by a second-order ergodic process.10 The two frameworks, i.e., the statistical "time-series" structure and the "probabilistic" structure, are in fact isomorphic. To see this, pick a "representative" trajectory of y, i.e. one in the subset of  (of probability one) for which the limit (1.11) exists. Clearly there will be no loss of generality in assuming that the probability space  of y is the "sample space", of all possible trajectories of y, i.e. the set of all semi-infinite sequences  = {0 , 1 , 2 , . . . }, t  Rm . With this choice, A will be the usual  algebra of cylinder subsets of  and the t:th random variable of the process, y(t), is just the canonical projection function y(t,  ) :   t .
For a process of this kind the Hilbert space H (y) is the closure in L2 (, A, P ) of the linear vector space generated by the scalar random variables   yi (t,  ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence of m ×  matrices y := {y (k )}k0 as in (3.3). For  in the subset of  where the time averages converge, define the map T , T : a y(t)  a y (t) t  0 a  Rm associating the i:th scalar components of each m-dimensional random vector y(t) of the process to the corresponding i:th (infinite) row of the m ×  matrix y (t) constructed from the corresponding sample path {y(t,  ); t  Z}. By second-order ergodicity, the set of all such    will have probability measure one and the map T will in fact be norm preserving, since by construction we have t-s = E y(t)y(s) = Ey (t)y (s) , where t is the covariance matrix of y. The map T can then be extended by linearity and continuity to a unitary linear operator T : H (y)  H (y ) which commutes with the action of the natural shift operators (both of which we denote U), in these two Hilbert spaces: H (y) -H (y)  T T  H (y ) -H (y ) This isomorphism allows us to employ exactly the same formalism and notations used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991) in the present statistical setup, where we build estimates of the parameters of models describing the data in terms of an observed time series instead of stochastic processes. This provides a remarkable conceptual unity and admits a straightforward derivation in the style of stochastic realization theory of the formulas in the paper van Overschee and De Moor (1993), there obtained with considerable effort through lengthy and formal manipulations. Appendix C. The invariant form of the Kalman filter Given a stationary stochastic system (3.7), the Kalman filter is usually determined via the matrix Riccati equation Q(t + 1) = AQ(t)A - [AQ(t)C + BD ][CQ(t)C + DD ]-1 [AQ(t)C + BD ] + BB (C.1) where Q(0) = P := E{x(0)x(0) }. Here Q(t) = E{[x(t) - x ^(t)][x(t) - x ^(t)] }, and the Kalman gain is given by K (t) = [AQ(t)C + BD ][CQ(t)C + DD ]-1 . (C.3) (C.2)
U U

These equations of course depend on P , B and D, which vary as the splitting subspace ¯ ) is invariant if a uniform choice of bases is made. X varies over , whereas (A, C, C ¯ ) and hence However, as shall see, the gain K depends only on the triplet (A, C, C one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

¯ ), and hence are invariant over . Clearly, in view of Theorem 5.1, P- (t), on (A, C, C as defined by (5.12), has this property. Moreover, Q(t) = P - P- (t), and, consequently, in view of (3.9), and the Lyapunov equation P = AP A + BB , P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11). A symmetric argument yields the backward equations. It is easy to see that as Q(t)  Q monotonously, P- (t)  P- , and hence P  P- , as should be. Appendix D. Some deferred proofs Proof of Theorem 5.1. Since X is a splitting subspace for the infinite past H - and the - + := U  H - and H := U  H + . But infinite future H + , by stationarity, X splits H - - + + - + Y  H and Y  H , and hence X splits Y and Y also. (See, e.g., Lindquist and Picci (1985, 1991).) Now, using the projection formula in the footnote of page +  Y+ 16, we have for any b y   -1  1 2 . . . 0 1 . . .  2 3 . . .  +1  1 0 . . .  -1  - + -  .  y E Y b y =b  . . . . .. ..  .    . . . . . . . . . . . . . .   +1 · · · 2 -1   -1 · · · 0 - ¯  (T- )-1 y = b   = b   ¯  are appropriate finite-dimensional observability and constructibility where  and  ¯  such matrices (2.6) of full rank. If  > 0 , there is a minimal factorization H =   - -1 - ¯ that  :=  (T ) y has n components, and ¯  > 0. ¯  (T- )-1  E { } =  ^  - , dim X ^  -  n = dim X so, since Therefore, since the components of  belong to X ^ ^ X - is minimal, X must also be minimal and X - be spanned by the components of  . Next, from the backward system (3.14) we see that - ¯x = ¯( ) + terms ortogonal to X , y and therefore, by the same projection formula,
- ¯ (T - )-1 y - = a . E Y a x( ) = a E {x( )¯ x( ) }    - ^  - , establishing the first of identities Consequently, E Y X = {a  | a  Rn } = X (5.7). The second follows from a symmetric argument. The representation formula (5.8) follows from the minimality of X as a splitting subspace for Y+ and Y- , which, in particular, implies that the constructibility operator, -  ^ Ct := E|Y X : X  X -

X

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n, ^k ( ). there is a unique random variable xk ( )  X whose projection onto Y- is x To show that x(0) form a uniform choice of bases as X varies over , first take X to be the stationary backward predictor space X+ and let x+ ( ) be the unique basis - ^( ) = E Y x+ ( ). Now, let X  be arbitrary. Then, since in U X+ such that x -   + X is a splitting subspace for Y and U X+  U H (Lindquist and Picci, 1991, Proposition 2.1(vi)), we have

X

X

x ^( ) = E Y x+ ( ) = E Y E X x+ ( ), and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all X  , which is a well-known characterization of uniform choice of bases; see Section 6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields the corresponding statement for (5.9).

-

-

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a corresponding partial covariance sequence , where  is large enough for the Hankel ¯ ) be the triplet determined matrix H , defined by (1.5), to have rank n. Let (A, C, C from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the covariance data by estimates {0T , 1T , . . . , T } ¯T ) be the corresponding triplet obtained via (2.5). of type (6.3), and let (AT , CT , C We want to prove that ¯T + 1 0T ZT (z ) := CT (zI - AT )-1 C 2 is strictly positive real for a sufficiently large T . Now, if deg ZT = deg Z , replace  by -1 0  0 in (2.5) in the appropriate , U by U 0 , V by V 0 , and -1 by 0 0 0 0 ¯ ) and (AT , CT , C ¯T ) have the same dimensions. This will calculation so that (A, C, C ¯T , 0T ) can be made arbitrarily close not affect Z and ZT . By continuity, (AT , CT , C ¯ to (A, C, C, 0 ) in any norm by choosing T sufficiently large. Thus the same holds for max Z (ei ) - ZT (ei )
[0,2 ]

and hence, since (z ) := Z (z ) + Z (z -1 ) satisfies (3.10), so will T (z ) := ZT (z ) + ZT (z -1 ) for sufficiently large T . Moreover, since |(A)| < 1, we have |(AT )| < 1 by continuity for sufficiently large T . Consequently, there is a T0 such that ZT is strictly positive real for T  T0 . ¯) Proof of Theorem 5.3. Let Z , defined by (1.6), be strictly positive real, and let (A, C, C be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , defined by ¯1 ), is also strictly (7.15) in terms of the principal subsystem truncation (A11 , C1 , C positive real. We want to prove that this property is carried over to rational matrix function ¯ )1 ) + 1 0 Z 1 (z ) = (C )1 (zI - (A )11 )-1 (C 2 for  sufficiently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let Q be defined by (5.32). Since the canonical correlation coefficients (5.25) tend to the canonical correlation coefficients (4.12) as   ,   . Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P- (t) tends to Q Q as t   if the initial condition is taken to be P- ( ) =  . Consequently, for any > 0, there is a sufficiently large  such that  -  < 2 and  - Q Q < 2 so that  - Q Q < . Hence Q tends to a limit Q with the property  = Q Q . Using the same argument in the backward direction, the T -1 second of relations (5.33) shows that Q also satisfies  = Q-  Q . Consequently, by the same argument as in the proof of Theorem 4.4, Q is a signature matrix, and hence in particular diagonal. Therefore,
1 -1 ¯ ¯ )1 )  ((Q )11 A(Q )- ((A )11 , (C )1 , (C 11 , C (Q )11 , C (Q )11 ) as   ,

where (Q )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, Z 1  Z1 . Hence, since Z1 is positive real, then so is Z 1 for  sufficiently large. ¯ ) is a minimal triplet. Proof of Lemma 7.4. Let us first consider the case when (A, C, C Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P- and P+ respectively, which, in particular, have the property that P-  P1 and P2  P+ . Then, in view of (7.18), P+ - P- > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem A4.4). Next, let us reduce the general case to the case considered above. If (C, A) is not observable, change the coordinates in state space, through a transformation ¯ ), so that ¯ )  (QAQ-1 , CQ-1 , QC (A, C, C ^ 0 C= C A= ^ 0 A   ^ ¯= C ¯  , C

^ A ^) is observable. Then, if P1 and P2 have the corresponding representations where (C, P1 = ^1  P   P2 = ^2  P ,  

^2 satisfy the reduced version of the linear matrix ^1 and P it is easy to see that P ^) and that, in this new ¯ ) for (A, ^ C, ^ C ¯ inequality (6.6) obtained by exchanging (A, C, C ^ ^2 - P ^1 > 0. If (C, ¯ A ^ ) is not observable, we proceed setting, (7.18) holds, i.e., P -1 -1 ^2 ^1 and P satisfy the by removing these unobservable modes. First note that P ^ ^ ^ C, ^ C ¯ ) by (A ^ , C, ¯ C ^ ). Then, dual linear matrix inequality obtained by exchanging (A, changing coordinates in state space so that ^ ~ ¯= C ¯  C ~ ^ = A A  0  ^= C ~ 0 , C

^ ¯ A ~ ) observable, and defining with (C, ~ -1  P -1 ^1 P = 1   ~ -1  ^ - 1 = P2 P , 2  

44

ANDERS LINDQUIST AND GIORGIO PICCI

~ ~ C, ~ C, ¯ 1 0 ) is a minimal realization of Z . Moreover, P ~1 and P ~2 satisfy we see that (A, 2 the corresponding linear matrix inequality (6.6) and have the property (7.18) in this setting. Hence the problem is reduced to the case already studied above. Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans-1 , mapping formed to the continuous-time setting via a bilinear transformation s = z z +1 the unit disc onto the left half plane so that Zc (s) = Zd 1+s 1-s (D.1)

is positive real in the continuous-time sense if and only if Zd is positive real in the discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al. ¯d , 1 0 ) and (Ac , Cc , C ¯c , 1 R) are realizations of Zd and Zc (1979)] that, if (Ad , Cd , C 2 2 respectively, we have  Ac = (Ad + I )-1 (Ad - I )    C = 2C (A + I )-1 c  d d (D.2) ¯ ¯d (A + I )-1  Cc = 2C  d   ¯ -C ¯d (A + I )-1 C R = 0 - Cd (Ad + I )-1 C d d d and inversely  Ad = (I - Ac )-1 (I + Ac )    C = 2C (I - A )-1 d c  c -1 ¯ ¯  = 2 C ( I - A C d c  c)   ¯ +C ¯c (I - Ac )-1 Cc 0 = R + Cc (I - Ac )-1 C c

(D.3)

Under this transformation the observability gramian and the constructibility gramian ¯d , 1 0 ) is ¯ A )) are preserved so that (Ad , Cd , C (i.e., the observability gramian of (C, 2 ¯c , 1 R) is; see, e.g., p. 1119 in Glover a minimal realization if and only if (Ac , Cc , C 2 (1984). Moreover, coercivity is preserved, and the solution sets of the corresponding linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the reachability gramian of a spectral factor and this gramian is also preserved.) Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact, transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting, all the requirements of Theorem 7.1 are satisfied. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order positive real function thus obtained via (D.3) back to discrete time, the desired result is obtained.

Towards Target-Level Testing and Debugging Tools for Embedded Software
Harry Koehnemann, Arizona State University Dr. Timothy Lindquist, Arizona State University

Abstract

The current process for testing and debugging embedded sojware is ine~ective at revealing errors. There are currently huge costs associated with the validation of embedded applications. Despite the huge costs, the most dl~cult errors to reveal and locate are found extremely late in the testing process, making them even more costly to repm"r. This paper first presents a discussion of embedded testing research andpractice. This discussion raises a need to improve the existing process and tools for embe&@i testing as well as enable better processes and tools for the jWure. To fmilitate this improvement, architectural and software capabilities which support testing and &bugging with minimal intrusion on the executing system must be developed. Execution visibility and control must come @om the underlying system, which should ofJer interjbces to testing and debugging tools in the same numner it offers them to a compiler. Finally we propose txtenswns to the underlying system, which consists of adiiitions to both the architecture and run-time system that will help reulize target-level tools. 1. Introduction Software validation involves many activities that take place throughout the lifecycle of soft w are development. A substantial portion of the validation process is software testing, which is the development of test procedures and the generation and execution of test eases. Notice we are not only concerned with the generation of a test case, but are also concerned with how that test is executed. Therefore, a test case is not simply composed of inputs to a system, but rdso includes any environmental factors. Other research has examined the issues behind test case selection, but few are addressing the problems that surround the execution of those test cases. The goal of this paper is to identify the problems associated with test case execution for embedded systems and to propose solutions for making embedded testing more effective at revealing errors.
Testing and Debugging Process Many of the activities, tools, and methods used during software testing are shared by software debugging. Software testing is concerned with executing a piece of software in order to reveal errors, while software debugging is concerned with locating and correcting the cause of an error once it has been revealed. Though these two activities are often referenced separately, their ac$ivi$ies are tightly coupled and share many common features.
Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise or republish, requires a fee and/or specific permiss~m.

During debugging, a developer must recreate the exact execution scenario that revealed the fault during testing. Not only must the code execute the same instruction sequences, but all environmental variants must be accounted for during the debugging session. In addition, the tools assisting in the debugging process must providing a &veloper with a certain degree of execution visibility and control while not impacting the execution behavior of the program. Embedded Systems The testing and debugging process is greatly restricted by embedded systems. Embedded applications are among the most complex software systems being developed today. Such software is often constrained by q Concaumnt designs q RcaI-time constraints q l%lbedded target Imvilrmments q Distributed hardware ambitectures q Device control dependencies Each of these properties of embedded software severely restrict execution visibility and control, which conseqmdy restricts the testing and debugging process. Our current methods and tools for software testing and debugging require a great deal of computing resources. Such resources are not available on the target environment. Therefore, a large gap exists between the methods and tools used during evaluation on the host and those used on the target. Unfortunately, mauy errors are only revealed during testing in the target environment. Because of the above issues, concerns are raised over the effectiveness of software validation for embedded systems. Embedded applications are responsible for controlling physical devices and their correct execution is critical in avoiding and/or recovetig from device failure. Often these physical devices control life-critical processes, making the embedded software a key element of a lifecritical system. Software failure can lead to system failure, which in turn could lead to loss of life. In addition, designers are increasing their use of embedded software to conuol the physical elements of large systems. This rate of increase is likely to increase as the cost for embedded controllers becomes cheaper and more attractive when compared with other mechanical techniques. Computer networks are fast replacing point-to-point wiring, due to the networks light weight, easy cotilgurability and expansibility, and lower design complexity. The advancement in the complexity of problems addressed by software in these types of 1.2

1.1

01993

ACM

0-89791-621

-2/93/0009--0288

1.50

applications may soon be limited satisfy reliability needs and concerns.

by our inability

to

2. Software Testing The software testing phase is concerned with executing a software program in order to reveal errors. Software testing for embedded systems takes place in four basic stages: 1) Module Level Testing 2) Integration Testing 3) System Testing 4) Hardware/Software Integmtion Testing The first three testing stages are typical of any software product. Testing begins with exercising each soft ware module and concludes when the software is shown to meet system specifications by passing some rigorous set of system tests. The fourth phase is unique to embedded systems. The software must not only be correct, but must also interface properly with the devices it is controlling. Testing literature contains countless methodologies, techniques, and tools that support the software testing process. They range from software verification and program proving to random test case selection. All testing methods indirectly apply to embedded systems, as they do all software. Of particular interest to this paper are those techniques that address the problems identified for embedded software - concurrency, real-time constraints, embedded environment, etc. Unfortunately, there exists little research into the unique with testing embedded software. problems SSSOCilltd
Testing Concurrent Systems Concurrency increases the difficulty of software testing, Given a concurrent program and some set of input, there exists an unmanageably large set of legal execution sequences the program will take. Furthermore, subsequent execution with the same input may yield different, yet correct results due to differences in the operating environment. This is all complicated by Ada's nondetermins tic select construct. Therefore, when testing concurrent software, we are not only concerned with a valid resuk but must also be concerned with how the program arrived at that result. Since multiple executions of a concurrent program may yield different results, it is not enough to ensure that the system produces the correct output for a given input. One must also ensure that the system always produces an acceptable output for each execution sequence that is legal under the language definition. Without sufficient control over program execution, there is no way of ensuring a given test is exerasing the code it was intended to test. Taylor and Osterweil ~ay180] examined static analysis of concument programs, However, this research considered processes in isolation and does not consider interprocess cmrummication. Taylor later extended this work to Ada and a subset of the Ada rendezvous mechanism ~ay183]. Through this static aualysis technique, one could determine aIl parallel actions and states that could block a task from executing. This method, as with most static 2.1

techniques, must examine a large set of statw and therefore must constrain itself to small, simple programs. Research in dynamic testing of concurrent Ada programs has largely focused on the detection of deadlocks ~emb85], the saving of event histories KeDo85, Maug85], and other tec.huiques that passively watch a program execute then allow the execution sequences to be replayed after a failure has been detected. Hanson Eans78] was among the first to discuss run-time control of concment programs. In order to regulate the sequences of events, he assigned each concurrent event in the test program a unique time value. He then introduced a test clock that regulated the system during execution. A given event could only execute if it's time was greater than that of the clock. Tai ~ai86, Tai91] extended Hanson's work to the Ada programnn "ng language. His method takes an Ada program P and a rendezvous ordering R and produces a new Ada program P' such that the intertask communication in P' is always R. A similar approach was used in Koeh89] to apply these techniques to testing and debugging tools. This work addressed the facl that in order to, test a specific program state, values in a program may need to be modified during run-time. Modification o~f the program state is a capability provided by any debugging tool and is a required property of a tool debugging tasked programs. It is important to note that both techniques explicitly perform rendezvous scheduling, removing those decisions from the run-time system and placing control in the hands of the tool. Non-intrusive testing Intrusion plays a significant role in the testing and debugging of embedded software. Any technique used to raise execution visibility or provide for program control must not interfere with the behavior of the teat program. Embedded applications have strict timing requirements and any intrusion on a test execution will likely make that test void. Intrusion is typical for host-based testing, but becomes a large problem for target-level testing and debugging activities. The above approaches address the need for visibility, control, and predictability for testing concurrent software. However, they are all intrusive and use instrumentation (inserting probes into a usem program and rewriting certain constructs before submission to the compiler) to gather run-time information and to control ~gram exmtion. After the probes are added, the user's object code is linked with the rest of the debugging system and then executed under test. This additional code has a serious impact on the execution behavior of the program. Instrumentation is not appropriate for testing real-time, embedded applications. A non-intrusive debugger for Ada is proposed in [Gil188]. A separate processor executes the testing system and communicates with the target processor through some special purpose hardware. Lyttle and Ford ~ytt90] have also implemented a non-intrusive embedded debugger for Ada. Their tool provides monitoring, breakpoints, and

2.2

289

display facilities for executing embedded applications. While these efforts provide an excellent start towards targetlevel tools, they do have severe limitations. These implementation do not deal with high level activities such as task interactions and are only concerned with items that can be translated from monitoring system bus activity. As discussed later in Chapter 5, techniques dependent on bus activity will likely fail for future architecture designs. In addition, many of the error detwted in the target environment are indeed concerned with high-level activities (process scheduling and interactions, fault handling, interrupt response, etc.). Other real-time, embedded tools have been proposed for crossdevelopment environments. They can typically be classified into one of the following three categories: 1) ROM monitors, 2) Emulators, and 3) Bus monitors. These types of tools will be further discussed later in this paper. of the Underlying System One of the large problems with testing concurrent systems is dealing with abstraction. The Ada programming language abstracts concurrent activities through task objects ~D83]. Tasks allow a developer to abstract the concepts of concurrency and interprocess cxmmmnication and discuss them at a high level. The burden of implementation is then placed on the compiler, and typically the run-time system. While abstraction is a powerful design tool, it leads to significant prthlems during the testing phase of software development. Implementation details become buried in the underlying system. At the development level, this high degree of abstraction is appropriate. However, abstraction complicates the testing process. Not only are we concerned with implementation details, but we must aIso control them to demonstrate that certain properties about a program will hold for every legal execution scenario. Without sufficient control over program execution, there is no way of ensuring that a spedc test is exerasing the code it was intended to evaluate. In addition, cmmxt operation in one environment (host) does not necessarily imply comet operation in another (target) due to implementation difference in the underlying system. The underlying system is composed of two parts, the features of the hardware architecture and the operations provided by the run-time system. As language constructs become more abstract, compilers are required to generate more code to implement them. There is no longer a trivial mapping from language construct to machine instruction. Rather, the compiler must provide an algorithmic solution in order to implement these high level constructs. Those solutions exist as operations in the run-time system. Rather than generate code for these constructs, the compiler generates a call to a run-time system operation or servim. As the constmcts become more abstract, compilers develop an increasing dependency on the underlying system. This increase in shown in figure 2.1. As new constructs are introduced to programming languages, their increase in abstraction is greater than that

of hardware and the run-time system is called upon to bridge the impending gap. No argument is made as to the rate of increase identified by the line slopes; nor is an argument ma& that these increases are even linear.

c
o
m P 1 e x

i~ t

L=
Time Figure 2.1

Language

constructs

Hardware

Y

2.3 Impact

Embedded systems raise many problems for software testing and debugging. Such software typically must deal with concurrency, real-time constraints, au embedded target environment, distributed hardware architectures, and a great deal of hardware-software interfaces for controlling externaI devices. These issues tdone do not provide a complete view of the problems SyStetUS are extcotttttered by embedded testing. bbedded typically developed on custom hardware configurations m@ng that each system introduces it's own unique problems. Tools and techniques that apply to one are not generally applicable on another, which leads to ad hoc approaches to integration and system testing of embedded software. The program is executed for some length of time and continual y bombarded with inputs in an attempt to show it adheres to some speeifkation, Current state of embedded testing As described earlier, the testing process for embedded systems consists of 4 phases that conclude with Hardware/Software (H/S) Integration During H/S integration testing, device and timing related errors are reveakd. These errors eneompass problems such as: q incorrect handling of interrupts q distributed communication problems q incorrect ordering of concumen t events q resource contention q incorrect use of device protocols and timing " incomect response to failures or transients These errors are often extremely difficult problems to fix and often require significant modifications to the software system. In addition, software is for~d to conform to custom hardware that may itself have errors. As stated above, H/S integration is the last phase of testing for an embedded system. Since errors are much cheaper to fix the earlier they are revealed,. why would one wait until the last phase of product development to reveal the most diftlcult to 3.1

290

locate, costly errors to fix? Our goal should be to reveal these errors as early as possible. Unfortunately, target level testing tools have yet to become a reality. The target processor of au embedded computer is typically minimal in function and size. It is only a small portion of a larger system, whose goals are to minimize cost and space. Therefore, target hardware of au embedded systems will not support software development nor any development tools. To resolve this problem, the source is developed on a larger host platform and cross axnpilers and linkers are used to generate code and download it to the target processor. Consequently, two environments exist in our development process, the host environment and the target environment, each having completely different functionality and interface to a user. Tools that run on the host provide a high level interface and give users detailed information on and control over their program execution. However, little is provided on the target. Typically, the best information obtainable is a low-level execution trace provided by an in-circuit emulator. Current Solutions Approaches to dealing with the above problems can be divided into hardware solution and software solutions. The hardware solutions are attempts at gaining execution visibility and program control and include the bus monitors, ROM monitors, and in-circuit emulators. A bus monitor gains visibility of an executing program by observing data and instructions transferred across the system bus. With a ROM monitor, debugger code is placed into ROM on the target board. When a break point is encountered, control is transfered to the debug code which can accept commands from the user to examine and change the program's state. Finally, an in-circuit emulator connects with a host system across an ethernet connection. At the other end, a probe replaces the processor on the target board. The emulator then simulates the behavior of the processor in (ideally) real-time, which allows the emulator to tell the outaide world what it's doing while it's doing it. The hardware solutions have mini m al effectiveness for software development. They can only gather information based on low-level machine data. The developer must then create the mapping between low-level system eventa and the entities defiied in the program. That -ping is the implementation strategy chosen by a given compilation system and becomes severely complicated for abstractions such as tasks Maintaining an understanding of the mapping is extremely difficult and cumbersome. The software solutions cart be viewed as attempts to reduce the tremendous costs of testing on the target. Several factors determine how a pitxe of software is tested 1) Level of criticality of software module Each software module is assigned a different level of criticality based on it's importance to the overall operation of the system. 2) Test platform availability

3.2

Typically, there will exist several test environments available to test a piece of soft ware, each providing a closer approximation to the actual target environment: q Host-baaed sours level debugger q Host-based instruction set simulator q Target emulator q Integrated validation faality 3) Test Classification The tests to be performed can be categorized to determine what they are attempting to demonstrate. The goal of a test plays a large role in determining the platform on which it will execute. Some examples are shown below. q Algorithmic q Inter-module q Intra-module q Performance q HE integration q InttX-cabinet Each of these factors play a role in assigning program modules to the various test platforms based on some criteria that might contain the following: q Type of software q Hardware requirements q Test chssifkation q Platform availability q Coverage requirements q Test support software availability (drivers, stubs) q Certification Requirements q Level of effort required for test This criteria takes into account the 3 factors discussed above as well as additional ones. The software solutions are an attempt to minimize the time spent testing in the target environment. Validation facilities are expensive to build and time utilimd for testing is expensive. This is due to the f;act that target level testing occurs extremely late in the development lifecycle and only a small window is allocated for HAS integration testing. However, the target is the only location that can reveal tin errors. It is ironic that our current solutions attempt to reduce the anmunt of target testing, but will likely lead to extensive modifications and thercfom extensive retesting.

4.

Problems with Embedded Irestinq The solutions proposedaboveare not effective at

revealing errors. Effective implies that a technique reveals a high percentage of the errors and that it does so in a costef!iaent manner. Instead, what the above tools provide is a minimal, low-level view of the execution of a program and those tools become available at a very late stage in development. Below is a list of problems associated with current approaches to embedded testing Expense of Testing Process Target testing requires expensive, custom validation facilities. The expense of these target facilities is incurred for every project, since little reuse across projects is ever realized. The effort required to build these validation facilities means that every test execution is expensive, making retests extremely costly. Yet, hardware often arrives late and full of errors, forcing software to be 4.1

291

modified and subsequently retested. This late arrival of hardware also impacts the cost of an error, since certain errom are only revealed during I-IN integrations testing. Perhaps the largest factor associated with the high costs of testing will be the questions and concerns that certification processes are beginning to raise about sofhvare tools. Typically, development tools have not been required to meet any validation criteria and certairdy not the strict criteria imposed on the development system. This luxury may soon disappear as the role tools play in the development process comes under tighter scrutiny. The huge expense of validation facilities will increase dmmaticauy. of Functionality on Target The level of functionality found on a target machine is minimal and does not support tools. This lack of functionality greatly limits the effectiveness of testing, since more time and effort is required to locate an error. While a host system provides a high-level interface and discussed software in terms of the high-level language, the target typically deals in machine instructions and physical addresses. Translating these low-level entities requires time and a great deal of tedious, error-prone activities. Errora revealed late in development lifecycle Embedded system designs often incorporate custom ASIC parta that are typicaIly not available until very late in the development process, delaying the availability of any target validation facility. In addition, errors designed into the ASICa are extremely expensive to fix, requiring new masks be created and complete refabrication. InsteaA errors in ASICs and other hardware problems are resolved by modifying the software. As stated before, this greatly delays the time which errors are revealed, which in turn increasing the cost of software testing. 4.4 Poor teat selection criteria AIl to often, tool availability diclates the quality of a testing process. Tests cases and scenarios are determined by what will work on available platforms and which test are achedulable rather than being determined by some theoretical test cxitcria. A prime example is the FAA's requirements ~AAS51 that 1) all testing be done in the target environment and 2) testing include statement coverage. Of course, test coverage is not currently measured on the target. Unfortunately, it is cheaper for a company to spend it's resources preparing an argument to obtain some form of `waiver" than to actually perform a test. In time, the argument approach will no longer be accepted and the solutions for embedded testing must be in place to accommodate this change. It will only take one implementation that performs statement coverage on the target to force every embedded, real-time software developer to perform statement coverage on the target to meet such a certification requirement.

4.S Potential use in advancing architectures Perhaps the largest problem facing embedded testing is that the current solutions cannot be applied to future h~dware architectures. Future architectures are Proposing q wider addreas Spaces q higher -Sor speeds q huge numbers of pins q internal pipes q multiple execution units q large internal caches q multi-dip modules Such complexities cast a dark shadow over the hardware solutions previously discussed. With internal caching and parallel activity being done on the chip, one will no longer be able to gain processor state information from simply monitoring the system bus. And as on-chip functions become more complex, emulator vendors will no longer be able to see into the chip through the pins making them obsolete as well. In [Chi191] an even stronger claim is made that the debugging capabilities provided by the chip will need to become more sophisticated. In future architectures, perhaps the only possibility to view and control the execution of hardware is to gain that information from the hardware itself.

4.2 Level

4.3

The previous sections raised issues about the effectiveness of our testing process and claimed that tcating is currently being limited by tool functionality. l%e goal of this paper is to identify shortcomings in the embedded testing proccas and propose a solution to those problems. The view taken by the authors is that tool support for embedded systems is lacking. Further, those approaches currently used for gaining execution visibility and control will soon be obsolete for future architectures. We propose adding facilities to the underlying system to better support testing and debugging tools for embedded software. As stated previously, the underlying system is composed of the hardware architecture and the run-time system (RTS). Both are composed of data structures and operations that implement common system abstractions such as processes, semaphores, ports, timers, memory heaps, and faultdexceptions. It should be noted that there is no distinct line between features of hardware and features of the RTS. In fact, as these features and abstractions become more standardized, newer architectures are attempting to incorporate them into their instruction sets ~Nl%92]. In addition, the implementation,of a feature may span parts of the architecture, RTS, and compiler generated code (i.e. fauhdexceptions). 5.1 Model Debugging System Below is an illustration of a debugging system (Figure 5.1). The data path from the debugging/testing tool represents symbol table information that allows the tool to map machine level information to source level

292

Test/Debug Compiler Generated Code \ Tool 1

A
e x
t e

Ada Compilation

3

"snext two and RTS

Figure 5.1 constmcts. The ASIS toolkit provides easy required for this physical connection. The implementation for this facility. ASIS is a proposed sw-tions describe `tie architecture additions standard interface between an Ada library and any tool interfaces in more detail. requiring compilation information. Of more interest is the communication path between the target processor and the testing tool. A tool sits external to the rest of the embedded system, while the RTS resides internally on the target board. At frost glance, this conceptual path seems rather difficult to realize. However, the implementation becomes easier if thought about as a typical host debugging system. Any debugging system has a least two processes executing, one running the test program and one running the &bugga. These two p= Sa common physical machine, which allows one process to gain information about the other. The debugger procem simple requires data and computation time, which it shares with the test program. This same scenario is rcqnired for embedded debugging, except that the debugger process is split. Part of the debugger process runs on the target machine and part runs on the host. The goal is to minimize the portion that must be run on the target so that it does not intrude on execution of the test program. To realize this nonintrusive execution of the debug software, the target 1) Execute debug code only at a break poinL 2) Run the debugger as a separate process, or 3) Provide a separate execution unit to execute the debugger. The details of these options are explored in depth later in this paper. The problem now lies with the interfa= between the embedded part of the debugger (intermddebuggcr) and the portion that lies on the host (external-debugger). The solution requires hardware additions that will be discussed later in this paper. A high level view is given in figure 5.2. In this figure, the tool makes logical calls to services provided by the RTS. These calls are actually implemented by the debugging system through data passed between the internal and external debuggers. Hardware additions arc -=

1

I

I

Figure 5.2

The past decade has seen hug{: advances in microprocessor designs. Several of these advancements were listed previously and include pipelining and separate functional units. The concept of partitioning a microprocessor in order to perform parallel activities is of great interest to this work. It was noted earlier that these parallel amputations severely restrict current methods for testing and debugging embedded systems, since on{e must simulate a great amount of computations. However, debugging tools can also use architectural parallelism to their advantage. If a hardware design is partitioned successfully to allow certain activities to occur concurrently, then the testing and debugging methodologies might wish to add

293

their own computational requirements to the list of parallel activities. This section will explore additions to hardware architectures. No claim is made as to the costs associated with these features. They assured y will require space (transistors) and possibly even add to the execution cycles required to implement certain instructions. 6.1 Hardware Partitioning of Memory One primary concern for industry is reducing the huge volume of retests associated with development. The current testing process ensures that errors are revealed late, which forces retesting large portions of the system. Despite correcting these problems, industry will still be faced with software that is constantly changing. Software is deceivingly easy to change and often the element of a system assigned to unknown or "risky" aspects during design. Changing software is extremely expensive late in the development for critical systems. Such systems typically have requirement that an error raised in one portion of the system won't interfere with the correct operation of the rest of the system. Current software certification agencies ~AA85] have several software restricdons including q Any modikation made to a software module forces the retesting of all other modules operating on that same physical device. . All software on a device must be developed under the highest level of criticality of any module that will execute on the same device. Without the ability of hardware to guarantee software boundaries, such requirements must be enforced. However, these requirements add a great deal of costs to software development. Consequently, software is often physically partitioned based on critical level, rather than design factors. Partitioning software modules based on critical levels greatly interferes with the design process. One would rather partition modules based on factors such as processor utilization and inter-module exmnmnication requirements. In fact, load balancing and p17XXsSmigration are techniques that would not be usable by embedded system developers unless all software is developed at the highest critical level. The solution to these issues is hardware partitioning. Each process should have it's own protected address space that is not accessible by any other process. In addition, sets of processes may wish to share memory. The processor should tdso provide the capability to restrict access to segments of memory based on some criteria. 6.2 Computational Facilities for Debugger. The debugging system is partitioned into an internal debugger and au external debugger. The internal debugger must physically exist on the target board and communicate with the external debugger through some dedicated medium. The internal debugger will also require execution from the target without interfering with the

operation of the application program. There are two possible scenarios q The internal debugger runs as a regular process on the -Or The architecture provides separate facilities to execute the internal debugger code In either case, control is transfered to the debugger when a breakpoint is encountered In the fiit scemuio, the debugger is executed by the processor as any other process. If the debugger executes as a low-level process, it would not interfere with the operation of the rest of the system. However, this is not a feasible approach. Most intern-sting errors occur during peak system loads, which would mean that the debugger could only execute when the probability of an error occurring was low. Another approach wouId be to execute the internal debugger as a periodic process of high priority and design the entire system to take this process into account when determining issues such as scheduling. The second scenario requires the target processor to provide some form of computational facilities. This extra execution will certainly require some amount of utilization of architecture resources such as internal registers and bus accesses. The simplest example of architecture facilities would be a machine that contirtuaU y dumps some representation of the instruction it is currently executing. This would require a dedicated bus to the external world (proposed later in this section) and that additional circuitry be attached to the computation units to gain access to the current instruction. The problem with fis approach is that the processor is not aware of what data is required by the tools at the other end. Therefore, it must dump everything. At high processor speeds, the amount of information being sent could become overwhelming. However, the data could be faltered and then captured so that a tool could parse it later and recreate an execution history of the program. The hardware required for filtering is not trivial and requires great speed and storage capaaty to maintain pace with the target processor. lle next step is to allow software to dictate the information sent by the processor. The functional unit of the hardware sed.ing messagea could be implemented as a state machine, emitting different messages based on its current state. The default state would be all processor transactions. Basically, in this eontiguration, the processor is performing the filtering rather than the external debugger. This addition should not add much in complexity to the hardware architecture and would greatly reduee the wmplexity of the external debugging hardware. The final step is to take the (now stateful) functional unit and make it programmable. Instead of a state machine, it now becomes a complete functional unit within the processor itself. The internal debugger code would then be loaded into this portion of the prowssor at boot time and reside there for the entire execution, transmitting and receiving messages to and from the external debugger.
q

294

6.3 Hardware Break Points Software break points are intrusive and require instructions be inserted into the code of the test program. Conditioned break points present a more significant problem, since they require a computation every time they are encountered to determine if the proper conditions are met to halt execution. Such breakpoints are unacceptable for d-time programs. To resolve this issue, architectures need to provide the capability to set breakpointa in hardware. A set of registers would be classified as BreakPoint Registers (BPR), which the processor would check against the operands for each instruction. Two types of breakpoints are required, data and instruction. Each data BPRs inside the processor would be compared with the address of every data operand for each instruction. Instruction BPRs would be compared with instruction addresses or type. When a match occurs, a breakpoint fault would be raised and control trsnafered to the internal debugger Upon returning from a break, the processor is required to restart execution precisely where it had terminated. The state of the processor consists of all it's internal registers, including any pipeline information and cache memory. These values must be saved automatically when a break is encountered. Another issues is that of conditional breakpoints. Such breakpoints require computations by the processor that run in the background behind the program under test. The evaluation of the conditional expression must begin far enough in advance so that it may complete before the processor has passed the breakpoint location. This evaluation will require memory accesses, raising additional problems. The current value of operands in the expressions must be available to the processor, which might involve accessing it from memory or cache. Any accesses to memory must be scheduled in such a manner that they do not block any resources required by the program under test. F@dly, the value used must be valid and not in danger of before the breakpoint. While the problems raised above seem difficult, they are not insurmountable. The extend debugger must compile the conditional expression and download the code. At that point it can determine the scheduktbili~ of this evaluation by comparing the @e for the conditional to the other code that will occur in parallel. The user could then be notified of problems with their additional breakpoint. The hardware is responsible for detecting any collisions in parallel activity and must not assume the debugger is always accurate. Any debugger activity intruding on the behavior of the test program is important information and must be flagged by the processor. The primary additions required for hardware break points are additional registers from the architecture and the logic necessary to compare them with the operands of the current instruction. To support conditional breakpoints, the processor must provide background computational support. This support could come from a portion of the processor dedicated to conditional breakpoints, or the code -g

could be downloaded to the internal debugger, given the internal debugger support described previously. 6.4 Architectural Support for Abstractions As common programming paradigms become more refined, architectures will begin to inax-porate them into their instruction sets. It would be unlikely that the only abstractions supported by architectures would remain simple data types (integer, real) and their associated operations (add, subtract, convert). Other abstractions such as processes, semaphores, ports, timers, memory management, and faults that are found in typical applications should be supported as well, along with associated operations on those abstractions. M&ing hardware to another level of abstraction provides huge advantages for testing tools. As stated earlier, the architecture must be the basis for emulation capabilities and providing execution visibility. As the hardware becomes more aware of programming elements, it gains the abdity to send more meaningful messages to the external world. A context switch between processes could be sent with a single message, rather than the hundreds of machine instructions it takes to implement the switch. As the processor becom-es the single point of visibility, awareness of the progrdng environment becomes important. A processor with a high-level understanding of program ,entities can emit fewer, more meaningful - messag-es than a processor that only comprehends low-level instructions. 6.5 Dedicated Bus Embedded testing and debugging require an interface that aliows the processor to communicate with the external world without interfering with the behavior of the system under test. This physical connection should reside on the target and interface extemall y tlhrough some detachable mechanism. The separation technique is important, since the external debugging system will be detached from this connection once the system is placed into operation. The execution behavior alf the program should be independent of whether or not any external tool is attached. Assuming an adequate physical connection, the next detmminah "on is the protocol across it. `Ile following issues must be addressed 1) At what rate will messrwes need to be sent? `Processor speed raises i~teresting problems, since future speeds might be too quick for external processing techniques. A solution to this problem was discussed previously where the processor became aware of highlevel program elements. The goal is to decrease the number of messages required relative to the number of machine cycles. 2) How much data is associated with a message? If an architecture is required to emit large volumes of data for messages, there may be instanms where the processor must be suspended to allow the internal debugging hardware to catch up to the current processor state. Higher level messages may compound the problem, since more maningful messages might require more

295

information. There is likely a tradeoff between message level and data volume. 3) Is the connection bidirectional? Visibility concerns dictate that state information travel However, methods requiring out of the processor. control of the executing program require that state information travel the other direction. Protoczds must be in place to handle contention across the bus and those must be extremely well defined, due to the extreme data rate that could will be emmuntered across the bus. 4) Who is the active element in sending message9? Either the processor or the RTS must determine the information sent from the processor. The processor cannot provide all the state information needed, while the RTS will likely not be able to maintain adequate speeds for sending messages. These questions play a role in determining the interface between the internal and external debuggers. A likely solution would be a master-slave relation, where either the internal or external debugger regulated the other. This scenario does not seem likely, since each has such critical processing concerns. Therefore, each will likely execute independently, while communication is handled via some bus and protocol. There does exist a master-slave relationship in respect to the bus, howevex. During program executiw, the internal debugger must `ownn the bus, since it's processing concerns are the greatest. It must meet the message sending deadlines without altering computations in other parts of the system. There are points dting execution where the extend debugger must aeiz cmtrol. If the internal debugger cannot allocate the bus to meet the demands of the extcmal debuggm, the user must be notifkd that their requested operation cannot be accomplished during a real-time execution. The final determination is that of the active element within the processor. There are two basic approaches to detemnining control of the internal debugging activities. In the fiit the processor is active and becomes responsible for sending messages to the extend debugger. `fhesecondappmachuse aaspecialdebugge rportionofthe RTS to emit messagea, which is loaded into a dedicated functional unit within the architecture. Tools require information maintained by both the architecture and the RTS. Perhaps the solution lies between the two where both the RTS and architecture have the ability to dump messages, depending on the cmrcnt mquiremcnts dictated by the external tool. 7. Run-Time Svs tern Additions The RTS requirements deseribe an interface between a tool and the underlying system. This is a logical interface requiring substantial hardware support as outlined above. An obvious goal is to minimize the required data and computational requhements of the internal debugger as well as the required communications between the internal and external debuggers.

This paper does not address the question of how these interfaces should be UtdiZSd. Such SllSWerS should be given by methodologies and techniques for detecting and locating errors in embedded, real-time systems. As discussed earlier, the lack of these methods has led to difficulties for determining adequate RTS services for testing and debugging tools, which has forced a different approach to determine the required operations. Since the RTS is in essence offering au implementation of high-level abstractions, services that provide visibility into the implementation of RTS abstractions should adequately fidfdl the needs of most testing and debugging techniques. A standard currently exists for implementing these abstractions in the MRTSI [ARTE89] and CIFO [ARTE91]. In addition, most of the needs for testing and debugging can be fulfiiled by these standards. This is not surprising, since our solution is based on implementation visibility, and the MRTSI and CEO are providing an implementation interface. However, it is important to note that this approach also indicates that implementations that support these staudards should require minimal additions to and debugging tools as prOpOSed by akw SUppCWt testing this paper. Below is a small discussion surrounding each of these abstractions and a list of shortcomings in the MRTSI and CIFO for testing and debugging. Processes Concurrency is a common abstraction used in embedded systems. A design can be decomposed without concern for computational resources, which can then be determined by a scheduler during run-time. A& irqplements concurren cy through tasks and task types. The CIFO and MRTSI provide extensive tasking support includlng identifieation, creation and activation, communication through rendezvous, concurmat access to shared entities, and support for scheduling control. Elements of interest that are not provided by the CIFO or MRTSI include q Task State - A developer must have the ability to query and modify the task state for each task in their system. However, a modification could leave the RTS in an inanaistent state. For example, changing a task's state from "delaying" to %unning" without removing it from the &lay queue would place the RTS into a state that could not be achieved through normal execution. However, the same modification ability is available on typical debugging systems and should be offered by em beddcddebwrgera as Wd. q Commm&ation and Synchronization - A developer must have the ability tb view and modify eaeh entry queue to determine the concurrent state of the system. Again, modifications could leave the RTS in an unobtainable state. q Scheduling Control - In addition to the extensive operations provided by the CIFO for concurrency control, a developer must have awess to the dispatch port (or ports for muhiprqxssor systems). 7.2 Interrupt Management 7.1

296

One of our criticism of the current approach to embedded testing is that timing errors are revealed late in Interrupts are very related to the development process. timing issues and their correctness is an important element in embedded testing. Therefore, support for interrupts is extremely important to target testing and debugging. Faalities provided through the CIFO and MRTSI would allow developers to bind various interrupt handling routines, enable and disable certain interrupts, mask and unmask interrupts, and generate software interrupts all controlled dynamically duting program a program test. Time Management As stated earlier, important" to target testing target tools require sfilaertt time. Tools must be allowed (although such modifications results) and the delay Iist of by the RTS. 7.3

timing issues are extremely and debugging. Therefore, control over issues relating to to view and modify the clock might produce undefined waiting processes maintained

Management Dynamic memory is not typically used by due to diffldtk% in dcmonstradng embedded ti@iC4itiOliS reliability. However, future systems will likely incorporate algorithms that requite dynamic storage. In addition, memory ~agemcnt for dynamic allocations is part of a RTS and should therefore be included in RTS visibility and control discussions. A tool will likely require that ability to demonstrate an application programs behavior when memory is exhausted. The MRTSI would need to be extended to provide operations that mim a collection making it smaller to show execution behavior when memory is exhausted or larger to demonstrate correct execution should a collection be expanded by the developer. Resizing is not cheap and could require a gnat deal of computation and data transfers, depending on an implementation. Exception/Fault Handling Proper handling of exceptional events is evaluated during hardwaresoftware integration testing. Therefore, tools require a great deal of cattrol over exceptions and One must be able to raise an recovery mechanisms. exception or fault during program execution and also modify handler binding during execution. Another question of interest might be to locate the handler for a given fault or exception at a given program location. Such information is not easily gained from the underlying system. The compiler is responsible for handling exception propagation [ARTE89], so &k " " g the handler from only RTS information might be an impossibility and is at best resolved uniquely for each compilation system. 7.5

7.4 Memory

architectural and RTS additions. The architectural additions will certainly be costly in both time and space, requiring space (transistors) on the chip and access to internal registers and busses that could cause contention and slow the execution of other instructions provided by the architecture. However, the RTS additions are minimal. We defined the needs of testing as making the implementation details of common system abstractions visible and then determined the functionality required to view and control them. The ARTEWG'S MRTSI and CIFO provided an outstanding basis for this approach. The RTS additions are admittedly weak. Our initial goal was to have the methodologies and techniques used for testing embedded, real-time systems drive the operations required by the RTS. Unfort.tmatcl y, such methods do not yet exist. As stated earlier, testing and debugging of embedded, real-time software remains a black art, with ad hoc methods and techniques. While there has been much research into the concurrency and distribution issues, none has examined real-time constraints, embedded environments, and other issues relating to embedded systems Perhaps the MRTSI and CIFO are sufficient for implementing target level testing and debugging tools. However, this question cannot fully be resolved until more formal methods exist. Our next step is to evaluate the additions and determine their feasibility. Questions relating the cost of these additions to au architecture and RTS in terms of time and space must be answered. Also, a more complete mapping should exist between the added feattues and the impact they have on the desired features. One can then make a valid comparison between a feature and the costs associated with it. `l'he embedded contmllermark~ is currently huge, but has only begun to require the computational powers ~SOCiKltti With lUiCrOpKXXWOrS. Embedded i@k.i3tiOttS have traditional been event driven rather than computation dependent. Due to their light weigh~ easy con@mbility and expansibility, and lower design complexity, computers are quickly being chosen over mechanical techniques for controlling devices. As this transition continues, the size and complexity of embedded programs will grow. Controllers will not only have strict timing requirements, but also have significant computational needs as well. This combination requires new approaches to our current testing process for embedded systems and therefore, more effective tools to aid in testing and debugging embedded applications. References

[ARTES9]

The first goal is to identify defkienaes in embedded system testing and raise questions about the future of current tools. The second is to propose a solution to these problems through

co nclusions The goal of this paper is two fold.

s

Ada Run-time Environment Working Oroup, "A Model Run-Time System Interface for A&m Ada Letters, January, 1989. Ada Run-time Environment Working Group, "Catslogue of Interface Features

[ARTE91]

297

and Options for the Ada Runtime Environment," Special Edition of Ada Letters, Fall 1991 (fI). [CHIL91] Child, Jeffrey, "32-bit Emulators Struggle with Processor Complexities,n Computer Design, May 1,1991. Department of Defense, Reference Manual for the Ada Programming Language, ANSI/MIL-STD1815a, United States DoD, 1983. Federal Aviation Association, Software Consideration in Airlx)me Systems and Equipment Certification, RTCA/DO178A, 1985. [GILL88] Gilles, Jeff aud Ford, Ray, "A Guided Tour Through a Window Oriented Debugging Environment for Embedded Real Time Ada Systems," IEEE Transactions on Software Engineering, 1988. Hansen, B., ~eproduable Testing of Monitors," Software-practice and Experience, Volume 8,1978. Hembold, D. and Luckham, D., "Debugging Ada Tasking Rograms," IEEE software, March, 1985.

lyxn6J

Tai, K.C., "&producing Testing of Ada Tasking Programs," IEEB Transactions on Software Engineering, 1986. Tai, K.C., Carver, R.H., and Obaid, E.E., "Debugging Concurrent Ada Programs by Deterministic Execution," IEEE Transactions on Software Engineering, January, 1991. Taylor, R.N. and Osterweil, L. J., "Anomaly Detection in Concurrent Software by Static Data Flow Analysis," IEEE Transactions on Software Engineering, May, 1980. Taylor, R.N., "A General Purpose Algorithm for Analyzing Concurrent Programs," Communications of the ACM, ~y, 1983.

~A191]

DD83]

~AYL80]

~AYIJ33]

&IAm78]

Iw'fJ=l

Intel Corporation, i960 Architecture programmer's Manual, 1993.

Extended Reference

KOEH91]

Koehnemann, H.E. and LindquisL T.E., "Runtime Control of Ada Rendezvous for Testing and ~U@llg," Procedm - gs of the 24th Hawaii International Conference on System Sciences, Volume II, 1991. LeDoux, C, and Parker, D.S., "Saving Traces for Ada Debugging," Ada in Use Proceedings of the Paris Conference, 1985.

Km]

Lyttle, D. and Ford, R., "A Symbolic Debugger for Red-Time Embedded Ada Software," Software - Practice and Experience, May 1990. Mauger, C. and Pammett K., "An EventDriven Debugger for Ati" Ada in Use: Proceedings of the Paris Conference, 1985.

@fAUG851

298

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

FRAMEWORKS FOR SECURING LIMITED-DEVICE APPLICATIONS
Timothy Lindquist Aarthi Ramamurthy Ramon Anguamea Timothy.Lindquist@asu.edu Aarthi.Ramamurthy@asu.edu Ramon.Anguamea@asu.edu Division of Computing Studies Arizona State University Abstract
realized by the IBM J9 runtime and SUN Wireless toolkit pre-defined classes, is the security environment we evaluated for this paper. Various development environments are available depending upon platform and language. For Microsoft Windows Mobile 6 the application development environment for the .NET languages, such as C#, is the .NET Framework together with Visual Studio 2005 with the Compact Framework 2.0. Several alternatives are available for configurations utilizing Java, in part depending on the Java runtime environment being used. Sun's CLDC HotSpot and IBM's J9 are two popular Java runtime environment choices. Add-on packages and various configurations are available to support different security approaches, device capabilities and networking needs.

In this paper, we compare the features available for developing secure distributed applications for limited devices, such as smart phones. We limit our scope to examine frameworks for Java. This work is part of a continuing project which is considering capabilities and performance for application development on these platforms. The paper considers performance as it relates to various approaches to securing applications. The paper addresses two separate concerns. First is protecting access to resources by an executing application. The facilities for defining, limiting and controlling applications during their development, installation and execution are described. Second, we discuss approaches available for securing communication among application components running on servers or limited devices.

2. Background
The connectivity of computing devices to the Internet, has enabled malicious attacks. The motivation for attacks varies from willful espionage to experimentation. Equally important to protection from attack is the ability to prevent harm from mistakes in coding, configuration or user operations. Protection and detection are difficult in handheld devices because of limited capability. Trust is confidence in expected functionality. When running an application the user must trust that it produces valid information and that privacy, integrity, or confidentiality will not be compromised. There are several security policies, protocols and mechanisms that are of particular interest to the limited devices. Languages such as Java, C#, and other scripting languages are widely used in distributed applications and provide varying degrees security support. Java and C# both permit examination of compiled intermediate code for unsafe actions. Both the Java CLDC/MIDP and Mobile 6 execution environments support an array of cryptographic functions that can be used

1. Introduction
In this paper we consider limited devices that are connected to the Internet and other communication media, for example, handheld devices such as intelligent cell phones and PDAs with Internet connections or platforms which combine these functionalities. These devices have limited memory, limited processing power, no hard disk storage, small display screens, and limited human input capability. We consider only those having communication facilities (WiFi or EV-DO).. The operating environments for these devices are comprised of three base components: local operating system, network operating system and language runtime environment. The leading operating systems for these devices are Symbian, Palm and Windows Mobile 6. Connected limited-device configuration and mobile information device profile (CLDC1.1/MIDP2) are the Java frameworks designed for resource constrained devices, such as phones and PDA's. CLDC1.1/MIDP2 as

1530-1605/08 $25.00 © 2008 IEEE

1

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

by communication protocols and by the system elements to aid in access control at the device and application level.

· · ·

3. Java CLDC/MIDP
Three different limited device configurations exist for the Java 2 Micro Edition (J2ME). For more capable devices such as set-top boxes and high-end wireless devices the Connected Device Configuration (CDC) defines an API whose functionality is close to J2SE, but is reduced as appropriate for the limited hardware and applications. At the lowest level of functionality is the JavaCard API (for Smart-cards/Sim-cards). JavaCard as can include functionality for asynchronous security operations, such as encryption, decryption, digital signature, verification and others for limited devices whose computing capacity is unable to perform such operations without disrupting user-functionality. The Connected Limited Device Configuration (CLDC) is defined for PDA and wireless phone devices. A device such as a PDA or smart-phone running Java applications would include a virtual software stack with the following components: · Mobile Information Device Profile (MIDP2) that supports the application life-time model, persistent storage, network resources and the user-interface. · CLDC1.1 that supports the core Java language, IO and networking classes, security features and internationalization facilities. · The selected Java runtime environment · The device operating system and related services

a set of operations associated with each permission, codebase indicating the code origin, a digital signature of the code which allows identification of the signer and verification that the code has not been modified.

The codebase indicates the file or URL from which the code is loaded. If signed, the alias of the public key can also be used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which defines the access it has to resources. When execution encounters an operation that requires a system resource, all classes representing the currently executing methods (contents of the runtime stack) are checked to assure all have access to the resource. The Figure below is taken from the On-line Java Tutorial and shows how an execution can include a range of protection domains ranging from no access to resources to full access.

3.1 Application Security Model
The J2SE model for securing the operations in an executing virtual machine changed dramatically as Java evolved. Java originally, used the sandbox model for application security. Initial versions of Java provided full trust to classes loaded locally and prohibited all sensitive operations from any code obtained dynamically. Java1.2 introduced support for a continuum of access control. Access to system resources (such as files, sockets, runtime, properties, security permissions, serializable, reflection, and window toolkit) is granted based on domains. A domain is defined to include: · a set of permissions (resources),

Figure 1.

Controlling Access to Java Resources

In J2SE (Java2), security domains are defined by a policy file granting permissions to the domain. For example, suppose the company GrowthStocksExpress publishes an applet on their (hypothetical) web site at the URL: http://GSE.com/applets Assuming the applet needs connections to one or more hosts having a domain address ending with GSE.com on ports beginning at 2575, a policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase "http://GSE.com/applets" { permission java.net.SocketPermission

2

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

"*.GSE.com:2575-", "accept, connect, listen, resolve"; };

A policy may consist of one or more grants each defining different domains. Each domain may have one or more associated permissions. CLDC/MIDP2 security. The application security model for CLDC/MIDP2 draws on the model for J2SE, in that it includes domains and signed code. CLDC/MIDP2, however has a simpler model, in part because of the constraints imposed by the configuration and profile. The following CLDC/MIDP2 constraints are most significant · Java Native Interface (JNI). JNI provides J2SE applications access to native code running on the platform. CLDC provides similar capabilities in Kilo Native Interface (KNI), but prohibits dynamically loading and calling arbitrary native functions. · No reflection, remote method invocation or serialization. In J2SE, an RMI server or client can cause remote code to be automatically downloaded and executed to satisfy argument or return (sub)classes. When a serializable RMI parameter is provided an argument of an extended type, the RMI system will attempt to load (if necessary from an http codebase) the needed class. · No user-defined class loaders. Related to the constraint above, the developer cannot define a class loader in CLDC. The classloader in CLDC cannot be extended or replaced by the developer. A CLDC/MIDP2 application can only load classes from its own (signed) Java archive. As a result, the developer cannot extend or modify any classes in the CLDC configuration, MIDP2 profile, or which are provided by the runtime environment vendor. · CLDC supports multi-threading, but it does not provide facilities to build daemons or thread-groups. MIDP2 security protects access to sensitive API's by permissions. Protecting resources includes the concept of a domain, which is conceptually similar to J2SE. The full scope of protection includes the following elements: · Protection domains (4) that are statically defined in a policy file (by

·

·

the vendor) and associated to resource permissions; for example, socket, http, https, PushRegistry. The protection domains are Minimum, Maximum (or Trusted) and Untrusted. Certificate and archive signature. The jar file containing application class files and other resources can be digitally signed. Level of access ­ either Allowed or User.

Unlike J2SE, the 4 protection domains are device-specific and defined by the runtime vendor. They can be modified only as provided by the vendor. Each of the four domains is associated with a set of permissions together with a level of access. The 4 protection domains are defined by the runtime vendor. · Minimum. None of the permissions are allowed. · Maximum. All of the permissions are allowed. · Trusted. All of the permissions are allowed. · Untrusted. To be allowed, the user must provide consent. The permissions defined by the MIDP2 specification include: http, socket, https, ssl, datagram, serversocket, datagramreceiver, and PushRegistry (invoke other applications). These permissions may be grouped together by the vendor into meaningful subsets and assigned to domains based on the subsets; for example, NetworkAccess. Within a domain, the level of access may be different for different permission (sets). The accesses are: · Allowed. The permission (set) is allowed without involving the device user. · User level. The application's access to the permission(set) depends on explicit authorization from the device user. With user level of access, a dialog box is presented to the user indicating information about the permission and asking the user

3

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

whether access should be granted. User level access can be specified on one of 3 modes: · Oneshot. The user must be prompted for each operation on the protected resource. · Session. When the user grants access, it applies to all operations on the resource during a single execution of the MIDlet. · Blanket. When the user grants access, it applies to all operations on the resource during any execution of the MIDlet. CLDC/MIDP2 provides MIDlet access to the Record Management System (RMS), which provides persistent storage for application data via a record store. MIDP2 provides shared access to the record store of other MIDlet suites, and provides that access should be provided as readwrite or read-only. Low-level security is provided by the J2ME Java virtual machine. A virtual machine supporting CLDC must reject invalid class files. This is accomplished by a two-step process. At development time, classes are pre-verified by a tool which adds special attributes to class files to facilitate runtime class verification on the device. Much of the verification process can be handled statically by the pre-verifier. At runtime, the virtual machine rejects classes that have not been pre-verified.

3.2 Security and Trust API
MIDP2 provides HTTPS and SSL for secure communications with other devices. But, runtime environment providers are increasingly providing additional options. For example, IBM's J9 version 5.7 provides web service security package which allows web method calls using encrypted SOAP envelopes or digitally signed method calls for authentication and information integrity. Security and Trust Services API (SATSA) provides access to more comprehensive hash code, digital signature/verification, key/certificate management, as well as encryption and decryption. SATSA is designed as 4 optional components. The primary purpose is to provide access to a SmartCard Java device, which provides security functionality in an asynchronous manner that does not disrupt applications supporting the device user.

SmartCard includes the Java Card Protection Profile. The protection profile supports both open and closed cards. Open cards provide the end-user with the ability to install or activate new applications on the card. Closed cards have applications set by the vendor at the time the card is personalized for the end-user. A good example of a closed card may be a banking card that supports personal electronic purchases and bank account functions. Open cards that allow new applications to be downloaded and installed on the card present special security risk that would exclude open cards that include banking applications. Nevertheless, applications for open cards that support other aspects of security may become increasingly important. An example may be securely communicating information outside of direct e-commerce applications. Data integrity and authentication are becoming increasingly important as electronic communication proliferates. The Java Card Protection Profile defines four different configurations for a Java Card based on open and closed cards. The minimum configuration corresponds to a closed card in which no applications can be installed on the card after it's been issued to an end-user. The three remaining configurations provide additional functionality that's available through the evolution of the Java Card specification, such as RMI (a limited version), logical channels, applet deletion, object deletion, external memory, biometry, and contactless interface. The Java virtual machine for the device includes an API (RTE API) that may contain classes for performing security operations on information and for certificate and key management. SATSA runs on the limited-device, not on the Java Card. SATSA provides an interface to card security functionality, or when there is no associated smart card, provides security operations for the limited-device. SATSA has four optional packages. · SATSA-APDU provides low-level stream/socket-based protocol for communicating between the limited-device and the card. · SATSA-JCRMI provides an RMI interface that allows an application running on the limited-device to call methods running in applications on the Card. This interface would be used to instead of SATSA-APDU to avoid the overhead of programming with a low-level socket data protocol. · SATSA-PKI allows limited device applications to use the smart card to digitally sign information or to verify digital signatures.

4

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

PKI also provides for key and certificate management. · SATSA-CRYPTO. When a Java Card is not available, the CRYPTO API is used to compute security operations directly on the limited-device. Use of APDU, JCRMI, and/or PKI is accomplished using threading on the limiteddevice. Threading allows security operations to take place on the card while other applications continue to run on the device supporting the enduser. In this scenario, SATSA is appropriate for limited-devices with constrained processing power. Independent of processing capability, using a Java Card may be necessary to provide assurance level that is appropriate to the application. The open-device nature of cell phones and PDA's make it difficult to certify trustworthiness of applications on the device. Instead, we can isolate all high-risk user-specific information and computations to a certified secure Java Card.

HTML page, it returns an XML message in Simple Object Application Protocol (SOAP) format. The service description - specified in Web Services Description Language (WSDL) - this description defines the web methods (functions) that a service will accept - the inputs that go into these methods, and the format of the output that can be expected in return. This is used in generating a web service client proxy class for the limited device. The web service registry - is a directory of web services. The directory is optional because a web service need not be listed in a registry to be used. The registry provides a catalogue of available services - similar to Java Naming and Directory Service (JNDI). The web service client proxy ­ The proxy negotiates the communication between a limited device client and the web service. It marshals arguments, signs or encrypts as appropriate, posts the message and interprets the result.

4. Secure Web Services
Web services provide an XML-based service protocol for communicating among components of a distributed application. Web services differ from prior similar technologies, such as Microsoft DCOM, Object Management Group CORBA and Java Remote Method Invocation through reliance on http protocol and XML.

4.1 Types of Security Services
The following are the security services that may be required by a distributed limited device application. Authentication: Ensures that the sender and receiver are who they claim to be. Mechanisms such as username/password, smart cards, and Public Key Infrastructure (PKI) can be used to assure authentication. Authorization or Access Control: Ensures that an authenticated entity can access only those services they are allowed to access. Access control lists are used to implement this. Confidentiality: This assures that information in storage and in-transit are accessible only for reading by authorized parties. Encryption is used to assure message confidentiality. Integrity: Ensures that information, either in storage or in-transit cannot be modified intentionally unintentionally. Digital signatures are used to assure message integrity. Non-repudiation: Requires that neither the sender nor the receiver of a message be able to legitimately claim they didn't send/receive the message.

Figure 2. Web Services Architecture [9] In Figure 2, the service, is performed by a web server acting as a container for executing the service code. This is generally just a web like page that gets posted similar to the way other http web requests are done. Instead of returning a

4.2 Transport Level Security
The most popular security scheme for web services is SSL (Secure Socket Layer), which is typically used with http, and is supported by

5

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

CLDC/MIDP2. However using SSL for securing web services has a number of limitations. The inadequacy of SSL can be easily explained by a simple example. Consider a Web Service that can be provided indirectly to a user. A user accesses a website which indirectly invokes another remote web service. In this case, we have two security contexts: 1. Between the user and the website 2. Between the user and the web service The second security context requires the security of SOAP request/reply message (between the web site and the web service) to be assured over more than one client-server connection. SSL is inadequate to provide this type of security mainly because of the fact that while it encrypts the data stream, it does not support end-to-end confidentiality. The shortcomings of SSL (https) should be considered when being used for a distributed application to reside on a limited-device. SSL is designed to provide point-to-point security. Often, Web services require end-to-end security, where multiple intermediary nodes could exist between the two endpoints. In a typical Web services environment XML-based business documents route through multiple intermediary nodes. Https in its current form does not support non-repudiation well. Non-repudiation is critical for business Web services and, for that matter, any business transaction. Finally, SSL does not provide element-wise signing and encryption. For example, if there is a large purchase order XML document, yet only a single element, say, a credit card element needs to be encrypted. Signing or encrypting a single element is difficult with transport level security.

Another important area that XML digital signature addresses is the canonicalization of XML documents. Canonicalization enables the generation of the identical message digest and thus identical digital signatures for XML documents that are syntactically equivalent but different in appearance due to, for example, a different number of white spaces present in the documents. The advantages of using XML digital signature can be summarized as below. · · It accounts for and takes advantages of two existing and popular technologies, viz., the Internet and XML. XML digital signature provides a flexible means of signing. For example, individual item or multiple items of an XML document can be signed. This becomes extremely useful in a scenario where each person in a workflow is responsible ONLY for certain work. It supports diverse sets of Internet transaction models. For instance, the document signed can be local or even a remote object, as long as those objects can be referenced through a URI (Uniform Resource Identifier). A signature can be either enveloped or enveloping, which means the signature can be either embedded in a document being signed or reside outside the document. It provides important security features like authentication, data integrity (tamperproofing), and non-repudiation. XML digital signature also allows multiple signing levels for the same content, thus allowing flexible signing semantics. For example, the same content can be semantically signed, cosigned, witnessed, and notarized by different people.

·

· ·

4.3 XML Signature
XML based security schemes, provide unified and comprehensive security functionalities for Web Services. The important ones being, XML Signature, XML Encryption, WS-Security (Web ServicesSecurity). The W3C (World Wide Web Consortium) and the IETF (Internet Engineering Task Force) jointly coordinated to generate the XML digital signature technology. The XML digital signature specification [10] defines XML syntax for representing digital signatures over any data type. It also specifies the procedures for computing and verifying such signatures.

Figure 3. WSE ­ Input/Output filters [1]

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

Web Service Deployment Descriptor (WSDD) and Handlers play the pivotal role in the implementation of digital signature in Java. A deployment descriptor specifies aspects such as handlers and communication protocol. The Handler is a java class (implementing the Input and output filters of Figure 3) that provides a MessageContext through which access is provided to the input/output stream of XML \In the Apache AXIS framework, MessageContext is a structure, that contains: 1) a request message, 2) a response message, and 3) a number of properties. All the SOAP message manipulation is done within the handler class.

6. Conclusions and Issues
The authors have continuing efforts in this area which include obtaining devices, software development environments, and simulators / emulators related to securing limited-devices. Our approach considers the operating environment on the device as well as their applications. We are in the midst of consolidation of small hand-held devices to provide integrated functionality. Common applications including personal organizers, cell-phones, and multimedia players can effectively be placed on a single platform. While users who desire more than one of these functionalities are exploring integrated solutions, the industry is pushing separation (partly for financial reasons.) Consolidated functionality brings a higher diversity of applications onto limited devices, as does special purpose applications (for example autonomous vehicle control). Either way, security concerns increase. The use of smart cards in the United States is just beginning after lagging behind use in some other regions. The integration of smart cards (SIM-Cards) on cell phones is an indication of this trend. Enabling high-risk applications, such as banking and purchasing, by leveraging smart cards integrated with other devices presents an attractive alternative. Of course the concern for security places new demands on platforms in which security has not historically been a high priority. Performance has been the primary impediment to the use of more strongly objectoriented languages such as Java for limited device applications. Securing a distributed

application complicates the issue. Important considerations include: · Underlying architecture processor performance, ancillary processing capability such as SmartCard, · Frameworks supporting securing the application, as well as communications, · Use of security mechanisms appropriate to application needs (authentication, integrity, confidentiality), · Proper use of available frameworks including proper handling of passwords, certificates, keys, digital signatures, and encrypted information. Frameworks discussed in the paper are an important enabler to developing more secure distributed limited-device applications. Further usage reports and benchmarking for security mechanisms would better support developers.

References
[1.] Tim Ewald, "Programming with Web Services Enhancements 1.0 for Microsoft.NET", Available, see: http://msdn.microsoft.com/webservices/buil ding/wse/default.aspx?pull=/library/enus/dnwse/html/progwse.asp [2.] Sun Microsystems Java Security and Crypto Implementation, http://www.cs.wustl.edu/~luther/Classes/Cs 502/WHITE-PAPERS/jcsi.html [3.] Knudsen, Jonathan; Understanding MIDP 2.0's Security Architecture. http://developers.sun.com/techtopics/mobilit y/midp/articles/permissions/ [4.] WebSphere Everyplace Micro Environment v5.7; MIDP Installation guide for J9 Palm runtime environment. Available online from IBM. [5.] Mourad Debbabi, Mohamed Saleh, Chmseddine Talhi and Sami Zhioua: "Security Evaluation of J2ME CLDC Embedded Java Platform", in Journal of Object Technology, (5,2) Mar-Apr 2006. pp. 125-54. [6.] Security and Trust Service APIs for Java Platform Micro Edition Developers Guide. Available from http://www.java.sun.com/ [7.] Pannu, K.; Lindquist, TE; Whitehouse, RO; and Li, YH; "Java Performance on Limited Devices"; Proc The 2005 International Conference on Embedded Systems and Applications, CSREA Press, Las Vegas, June, 2005.

7

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

[8.] Lindquist, TE, Diarra, M, and Millard, BR; "A Java Cryptography Service Provider Implementing One-Time Pad"; Proc. 37th Annual Hawaii Int'l Conf on Systems Sciences, ACM, IEEE Computer Society, January 2004. [9.] Online Documentation on Web Services,

Available from: http://www.servicearchitecture.com/webservices/articles/web_services_explained.ht ml [10.] XML Digital Signature Specification, W3C Recommendations, Available from: http://www.w3.org/TR/xmldsig-core

8

ARTICLE IN PRESS

The Journal of Systems and Software xxx (2004) xxx­xxx www.elsevier.com/locate/jss

Automated support for service-based software development and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a Department of Computer Science and Engineering, Arizona State University­­Main, P.O. Box 875406, Tempe, AZ 85287-5406, USA Department of Electronics and Computer Engineering Technology, Arizona State University­­East 7001 E, Williams Field Road, Building 50, Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003

Abstract A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. Ó 2003 Published by Elsevier Inc.

1. Introduction A service-based development paradigm, or services model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can interact with one another and be providers or consumers of data and behavior. Some of the defining characteristics of service-based technologies include modularity, availability, description, implementation-independence, and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon the definition of the interface needed to access a service (description) while hiding the details of its implementation (implementation-independence). Since the client and service are decoupled, other concerns such as side effects become non-factors (modularity). One of the potential benefits of using a service-based approach for developing software is that at any given time, a wide variety of alternatives may be available that meet the needs of a given client (availability). As a result, any or all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. The technique utilizes an architecture description language to describe services and achieves run-time integration using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and involves the automatic generation of ``glue'' code for both services and applications. The Jini interconnection technology (Edwards, 1999) is used as a broker for facilitating service registration, lookup, and integration at runtime. The remainder of this paper is organized as follows. Section 2 describes background material in the areas of software architecture and the middleware technology we are using to enable dynamic integration (i.e. Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work, and Section 5 draws conclusions and suggests further investigations.

2. Background
Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751. E-mail address: gannod@asu.edu (G.C. Gannod). 0164-1212/$ - see front matter Ó 2003 Published by Elsevier Inc. doi:10.1016/j.jss.2003.05.002
*

This section describes background material on software architecture and Jini.

ARTICLE IN PRESS
2 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx

2.1. Software architecture A software architecture describes the overall organization of a software system in terms of its constituent elements, including computational units and their interrelationships (Shaw and Garlan, 1996). In general, an architecture is defined as a configuration of components and connectors. A component is an encapsulation of a computational unit and has an interface (e.g. port) that specifies the capabilities that the component can provide. Connectors encapsulate the ways that components interact. A connector is specified by the type of the connector, the roles defined by the connector type, and the constraints imposed on the roles of the connector. A connector defines a set of roles for the participants of the interaction specified by the connector. Components are connected by attaching their ports to the roles of connectors. Another important concept is an architectural style. An architectural style defines patterns and semantic constraints on a configuration of components and connectors. As such, a style can define a set or family of systems that share common architectural semantics (Medvidovic and Taylor, 1997).

3. Approach This section describes the service-based development approach including the techniques used for defining services, specifying client applications, realizing integration, and generating glue code. 3.1. Example Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on the health of systems in a network. This application utilizes a network sniffer service and a port monitoring service. The network sniffer service gives an administrator information about traffic on the network. The port monitoring service provides information about the open ports on the various machines on a network. Together, these services facilitate determining whether certain kinds of attacks (such as ping storms) are being directed to a machine or machines. The client application supports analysis of several networks, each of which is accessed using the buttons shown on the top portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services that utilize different models of execution (strict call return and data streams). The remainder of this section refers to architectural specifications that were used in the construction of this example. 3.2. Overview The methodology that we have developed follows closely the model suggested by Stal (2002) for web ser-

2.2. Jini The primary enabling feature of the work described in this paper is the existence of Jini (Edwards, 1999) for the delivery and management of services. In a typical Jini network, services are provided by devices that are connected to the network. A Jini technology layer provides distributed system services for activities such as discovery, lookup, remote event management, transaction management, service registration, and service leasing. When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is registered, a proxy (Gamma et al., 1995) is stored by the lookup service. The proxy can later be transported to the clients of the service. Other network members can discover the availability of the service via the lookup service. When a client application finds an appropriate device, the lookup service sets up the connection. In our approach to component integration, we use Jini to provide a standard method for registering and connecting a client to corresponding software components that are acting as services. One of the advantages of using this Jini-based integration technique is that it facilitates construction of applications ``on-the-fly'' whereby components can be used on an as-needed basis. One of the disadvantages is that clients of services must have some prior knowledge about how to use each respective service.

Fig. 1. Running example.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx 3

vices, although the technology that we are using to realize our approach is Jini. The approach itself focuses on two concerns with respect to software reuse. That is, it addresses both for reuse and with reuse concerns. With respect to for reuse, the approach involves the construction of services via the use of adapter and proxy synthesis. Specifically, the methodology involves two steps for creating services as follows: (1) specification of components as services, and (2) generation of services using proxies via the construction of appropriate adapters and glue code. These services are consequently registered and made available on a network. With respect to with reuse concerns, the approach involves the construction of applications using services as follows: (1) specification of a client to make use of services from a repository or network, (2) generation of the client (both manual construction of client application specific code and automated generation of glue code), and (3) execution of the client, including integration of the specified services at runtime. Within our approach, a user (e.g. developer) is responsible for writing the source code for the client application along with the specification of the architecture for a client. Among other things, the client specification contains a description of the basic services that the client application will need in order to be a complete system. All other source code, including code necessary to realize the connections between the client and employed services, is generated based on the specifications describing clients, services, and connectors. 3.3. Service generation In this section we describe some of the issues related to automating the creation of service wrappers. To support these activities, we have developed an automated tool that takes as input a software architecture and produces glue code. A primary source of reusable components that we employ in our approach are legacy command-line applications (Gannod et al., 2000). In order to generate services from legacy components, we take the approach of wrapping the components by utilizing the interface provided by the component. Since command-line applications have a well-defined input and output interface, the interface of the application as a service can be based entirely upon the knowledge of what the application intends to provide. 3.3.1. Specification and synthesis The concept of using an adapter for wrapping legacy software is not a new one (Gamma et al., 1995). As a migration strategy, component wrapping has many benefits in terms of re-engineering including a reduction in the amount of new code that must be created and a reduction in the amount of existing code that must be rewritten.

In regards to wrapping components, our approach uses two steps. First, a specification of the legacy software as an architectural component is created. These specifications provide vital information that is required to define the interface to the legacy software. Second, the appropriate adapter source code is synthesized based on the specification. 3.3.2. Specification requirements To aid in the development of an appropriate scheme for the wrapping activity, we defined the following requirements upon specifications. These requirements are as follows: (S1) a sufficient amount of information should be captured in the interface specification in order to minimize the amount of source code that must be manually constructed, (S2) a specification of the interface of the adapted component should be as loosely coupled as possible from the target implementation language, and (S3) the specification of the adapted component should be usable within a more general architectural context. The requirement S1 addresses the fact that we are interested in gaining a benefit from reusing legacy software. As a consequence, we must avoid modifying the source code of the legacy software. At the same time, we must provide an interface that is sufficient for use by a target application. To provide that interface, a sufficient amount of information is needed in order to automatically construct the adapter. Our selection of command-line applications addresses the modification concern of requirement S1 since source code is not available. As such, we are required to provide an interface that is based solely on the knowledge of how the application is used rather than how it works. Table 1 shows the properties used in the specification of services, clients and connectors. A service component specification consists of two parts: properties and ports. The properties section describes style of the service, while the ports section describes functions provided by the service. In addition, the service specifications indicate style-based information as well as conditions or commands that need to be true or executed, respectively, in order to establish an environment necessary to use the service. Finally, a key in terms of a ``service type'' (e.g. interface property) is used to support a service lookup, which is later utilized during application integration. The requirement S2 (i.e. the decoupling of a specification from a target implementation language) is based on the desire to apply the synthesis approach to a variety of target languages and implementations. In addition, this requirement facilitates enforcement of requirement S1 by ensuring that new source code is not artificially embedded in the specification. While satisfying this requirement is ideal, we found in our strategy that a certain amount of implementation dependence was

ARTICLE IN PRESS
4 Table 1 Properties Group Service properties Service port properties Attribute Component-Type Signature Return Cmd Pre Post Interface Path Port-Type Shared-GUI Part-of-client GUI-CodeFile Component-Type Shared-GUI Port-Type Interface Connector-Type Prop-type Description Architectural style this component adheres to The port's signature The port's return type The command-line program being wrapped Pre-processing command Post-processing command The generic interface implemented by this port Path to the wrapped command-line program The port's type based on the Component-Type Boolean indicating shared (true) or exclusive (false) GUI Identifies inclusion in client application The filename for client's GUI code Architectural style this component adheres to Boolean indicating shared (true) or exclusive (false) GUI The port's type based on the Component-Type The generic interface that this port can bind with Architectural style this connector adheres to The connectors role based on the Connector-Type G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx

Client properties

Client port properties Connector properties Connector role

necessary due to the fact that our implementation would make use of Jini. When a component has been wrapped using our technique, an interface is defined that facilitates the use of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also desirable to be able to use the specification of the adapted component within a more general architectural context. That is, it is advantageous to be able to use the specification as part of the software architecture specification for new systems. In using a content-rich specification, where interfaces are defined explicitly, the added benefit of providing information that can be integrated into an architectural specification of a target application is gained. In order to realize the requirements placed upon desired interface specifications for legacy software wrappers, we used the ACME (Garlan et al., 1997) architecture description language (ADL). Specifically, we used the properties section of the ACME ADL to specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an ADL that has been used for high-level architectural specification and interchange (Garlan et al., 1997). 3.3.3. Synthesis As stated earlier, the class of legacy systems that we are considering are command-line applications (Gannod et al., 2000). Given this constraint, we make the assumption that any client applications utilizing the wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We find this assumption to be reasonable due to the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage profiles. In our approach, the specification that is needed to generate wrappers contains properties associated with the ports as shown in Fig. 2. These properties include Signature, Command, Pre, Post, Path, Interface, and Return. In this case, the specification describes the NetworkSniffing and PortMonitor services, which are services created by wrapping tcpdump, and nmap, respectively. In the synthesis process, ACME specifica-

Fig. 2. ACME services section.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx 5

tions are combined with a standard template that implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we have constructed to automate this process generates the appropriate source code for facilitating interaction between a potential client and the wrapped component. At present, this is an automated tool that generates fully executable code for the wrapped application and does not require the user to modify or write any new code outside of option GUI code. Both the service and client synthesis steps utilize a template-based approach to synthesize code. That is, a standard file has been created that has stubs containing place holders that must be instantiated with either service or client specific parameters. Fig. 3 contains a portion of the ServiceTemplate file which contains all of the application and service independent source code and provides the routines necessary to integrate the legacy code into a Jini network. Specifically, the ServiceTemplate contains functions that implement the discover and join protocol for registering a service with the lookup service. The ServiceTemplate also contains tags that are place-holders for the automatically generated functions. For instance, in Fig. 3 the tag <put-ServerName> is a place-holder for the final name of the adapter component. In addition to the ServiceTemplate, there is also a reusable set of functions that can be utilized in an interface specification and consequently in the generated wrappers. For instance, the getOutputStream( ) routine (shown in Fig. 4) is available as a function for use within the Java code to provide standard stream input support.

The amount of automation that has been achieved through the approach described above is dependent on the degree of graphical user interface (GUI) support that is desired. For a service, the code synthesis step can be fully automated if no GUI support is desired. Otherwise, the amount of manual code construction is limited to GUI support. 3.4. Client generation Once the services are generated and stored in a repository, a client application can be architected. First we need to specify the client application taking into account the architectural style of each of the services. Once a client is specified, it can be verified and generated. In this subsection we look at the requirements for specifying the client and then describe synthesis of the client. 3.4.1. Specification Refer again to Table 1 which, in addition to the properties for service specifications, contains the properties of client application components and connectors. When dealing with integration at the component level, two issues arise (among others) that are of interest. First, the problem of architectural style mismatch (Shaw and Garlan, 1996) occurs when the underlying assumptions made by components conflict. Second, most modern applications provide a graphical user interface (GUI). As a result, integration of off-the-shelf components can leverage these user interfaces in order to take advantage of previously built technology. To cope with these issues we impose two requirements on the specification of client applications as follows: (C1) the specification of the components should capture the notion of architectural style so that the high-level interaction between clients and services can be verified, and (C2) the specification must facilitate the use of shared and exclusive GUI components. The requirement C1 addresses the fact that a component must provide a notion of architectural style. A component's style plays a very important role when it interacts with other components by imposing interaction constraints. Using a basic style attribute (by name) architectural mismatches can be determined by simple keyword matching. Requirement C2 addresses the fact that a service may provide a GUI that allows a user to access and control the service. In this context, there may be GUI components provided by services that are either sharable by other services or exclusive to the service. A sharable GUI component can be used by both the client as well as other integrated services while an exclusive GUI component can only be used by the service that provides the interface.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

ARTICLE IN PRESS
6 G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx

3.4.2. Synthesis The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample specification of a client. The information contained within client specifications are used to support the synthesis of client code. This synthesis step utilizes two features; first, the information regarding connectors and attachments, such as those shown in Fig. 5 are used to determine the relationships between client applications and desired services. Second, information regarding GUIs provided by services is used to determine how to realize the GUI in a client application. In our framework, the wrappers for the various services can implement a common interface that allows the client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially used across multiple services and are identified using a name taken from a standard GUI vocabulary (for example ``ResultsWindow''). The name is then used to identify which GUI components can be shared across services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive component is independent and cannot be shared between services. The exclusive GUI components of the wrappers are used as is but may interact with one or more of the shared components. For both shared and exclusive components, the interaction with the client GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers. 3.5. Discussion As stated in Section 1, the service-oriented domain are characterized by modularity, availability, description, implementation-independence, and publication. As a result, services and service-based approaches are more coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a software architecture to specify applications that operate under these characteristics. As such, a software architecture in this context defines components, their interfaces, and the mechanisms by which services (as components) can be joined in order to fulfill needed software behavior. Consequently, services enable the use of a software architecture as an integration vehicle in which the architecture facilitates generation of glue code. It is the very fact that services adhere to the characteristics described above that the integration and code generation become possible at this level. However, the approach does lack in its ability to address needs that are more specific than what individual services provide. To cope with this, we are developing an approach that allows for the creation of federated services, where services are combined to meet some higher-level objective.

4. Related work Recently, the use of web services has gained attention with vendors releasing webservices toolkits that allow for building and using webservices. Webservices and .NET (Meyer, 2001) are based on the SOAP and XML (Seely and Sharkey, 2001) protocols. The Jini approach to service integration goes beyond what the webservices paradigm provides by defining how services can be used within a larger application context and providing support for code transportation. FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server that distributed messages to other tools that were interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this approach is a precursor to service-based development. Urnes and Graham (1999) describe an approach to facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the component space across a network. In our approach, services are potentially developed by different organizations and thus the choice of what to distribute is not

Fig. 5. Portion of ACME client specification.

ARTICLE IN PRESS
G.C. Gannod et al. / The Journal of Systems and Software xxx (2004) xxx­xxx 7

available. The component model being addressed by Urnes and Graham, as such, is finer-grained and violates implementation-independence, a tenet of servicebased development. Grundy et al. (2000) discuss issues and experiences in constructing component-based software engineering environments. They created a variety of useful software engineering tools using their tool set (JViews, JComposer, etc.). They use ``plug and play'' and an event-based composition approach to achieve component integration. In this framework, components are more tightly coupled and their granularity is finegrained. In contrast, our approach is based on dynamic integration of coarse-grained services that are loosely coupled. Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and component gluing. This creates a clean separation of customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work provides a potential strategy for dealing with component mismatches, which is currently ignored in our approach.

Acknowledgements G. Gannod is supported in part by NSF CAREER grant CCR-0133956. References
Edwards, W.K., 1999. Core Jini. Prentice-Hall. Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services. Commun. ACM 45 (10), 77­80. Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison Wesley Longman. Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for legacy software. In: Proc. 7th Working Conf. Reverse Eng., IEEE, pp. 128­137. Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture description interchange language. In: Proc. CASCON'97, pp. 69­ 183. Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2). Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to develop a family of applications. IEE Proc. Software Eng. 144 (5­ 6), 237­248. Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92­97. Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration with pluggable composite adapters. Software Archit. Comp. Technol.. Reiss, S.P., 1990. Connecting tools using message passing in field environment. IEEE Software 7 (7), 57­66. Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services Development Using XML. Prentice-Hall. Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on an Emerging Discipline. Prentice-Hall. Stal, M., 2002. Web services: beyond component-based computing. Commun. ACM 45 (10), 71­76. Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In Proc. of Design, Specification and Verification of Interactive Systems. Gerald C. Gannod is an Assistant Professor in the Department of Computer Science and Engineering at Arizona State University and is a recipient of a 2002 NSF CAREER Award. He received the M.S. (1994) and Ph.D. (1998) degrees in Computer Science from Michigan State University. His research interests include software product lines, software reverse engineering, formal methods for software development, software architecture, and software for embedded systems. Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras (Chennai), India. His research interests include software engineering, distributed and object-oriented systems, software design, software architecture, service-oriented software engineering, and Wireless Application platforms. Timothy E. Lindquist is Professor and Chair in the Department of Electronics and Computer Engineering Technology at Arizona State University East Campus in Mesa, Arizona. He received the Ph.D. (1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

5. Conclusions The web-based services paradigm has gained attention recently with the development of technologies such as SOAP (Seely and Sharkey, 2001). The benefits of such technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these benefits by providing facilities for on-the-fly integration and component introspection. In this paper, we described an approach for addressing component integration via the use of services in the context of Jini interconnection technology. Specifically, the approach utilizes synthesis to generate code necessary to realize component integration. To facilitate integration, the ACME ADL is used to specify both services and target applications, and is used a medium for performing service compatibility checking. We are currently developing an environment that will assist in the creation of applications within the servicebased paradigm and will support service browsing to facilitate application design. In addition, we are investigating approaches for allowing services to collaborate beyond the scope of a client application in order to create federated groups of services. Furthermore, we are developing technologies similar to the ones described in this paper in order to support service-based application within the .NET and web service frameworks.

INDUSTRY TRENDS

Moving Java into Mobile Phones
George Lawton

Telecom are now selling Java phones. And, said Ben Wang, manager of systems development for Sprint PCS, 80 percent of the new phones the company sells will be Java enabled after the big rollout next month. Nokia alone plans to ship 50 million Java phones this year and 100 million next year. In fact, 15 handset makers either are or soon will be selling 50 models of Java phones.

Advantages

A

s mobile technology matures, handheld-device vendors are looking for ways to make their products more functional, and Java is one approach they are turning to. This is particularly the case with smart cellular phones, which are using Java to help add new capabilities. In smart phones, Java functions as a layer between the operating system and the hardware, or runs parallel to the OS within a separate chip. In the past, the key constraint to running Java on mobile devices has been their processing, memory, and powerconsumption limitations. However, new mobile hardware and software developments are reducing these limitations. Thus, industry observers expect Java use in mobile devices, which is already supported by many vendors, to explode during the coming years. Nick Jones, a fellow at Gartner Inc., a market research firm, said Java will become a de facto standard on midrange and high-end cellular phones. He predicted that at least 80 percent of mobile phones will support Java by 2006, although some may also run on other technologies, such as Microsoft's Pocket PC operating system. According to Jones, mobile-device manufacturers' desire for an aftermarket is driving interest in Java as a mechanism for easily adding software to devices. Java also permits applications to work across platforms. This is important in the mobile-phone market,

which features many platforms. However, questions about Java's performance and a dearth of Java-based applications for cellular phones, particularly in Europe and the US, remain as obstacles to the technology's widespread adoption in mobile devices.

DRIVING JAVA USE IN HANDHELDS
Work on Java-enabled handheld devices began several years ago, but completion of the Java 2 Platform Mobile Edition (J2ME) and support from device vendors and cellularphone-service providers have driven the recent level of interest, explained Eric Chu, Sun Microsystems' group product manager for industry marketing.

Adoption levels
Korea's LG Telecom in became the first service provider to deploy Java in September 2000. Since then, users have deployed between 18 million and 20 million Java-enabled telephones, said Sun spokesperson Marie Domingo. Companies such as Nextel in the US, NTT DoCoMo in Japan, and British

According to Sun's Chu, one of Java's major benefits for cellular phones is support for packet-based networks running TCP/IP. Using TCP/IP makes it easier to write applications that communicate directly with the phone, rather than relying on an intermediate technology such as the wireless application protocol (WAP). Also, Chu said, Java, unlike WAP, supports pictures and colors. In addition, he explained, the Java environment provides good security because it includes a sandbox that limits downloaded code's access to the rest of a host system. Moreover, Java's ability to work with different platforms is important in the fragmented cellular-phone market. This capability lets a Java-enabled phone run applications and services written for other mobile platforms and also lets software vendors save time and money by writing a single, Java-based version of an application to run on multiple platforms. And Java-enabled phones and servers could communicate directly with each other, thereby enhancing interactive applications. Java enables smart-phone users to download applications directly from the Internet. Similarly, Java lets users download Java applets that customize their devices in various ways, such as with special ring tones or improved caller ID. This lets users get new features more easily. In the past, users had to buy new phones, run new applications remotely using WAP, or download programs first downloaded to a PC. Meanwhile, there are many Java developers, which makes it easier for
June 2002

17

I n d u s t r y Tr e n d s

Java 2 Platform Micro Edition (J2ME)

Optional packages Optional packages Java 2 Platform Enterprise Edition (J2EE) Java 2 Platform Standard Edition (J2SE) Personal basis profile Personal profile MIDP CLDC KVM
Source: Sun Microsystems

Foundation profile CDC JVM

Figure 1. Sun's three primary Java platforms are each designed primarily to run on a different type of machine. The Java 2 Platform Enterprise Edition is designed for servers; the Java 2 Platform Standard Edition for workstations, PCs, and laptops; and the Java 2 Platform Micro Edition for PDAs, smart cellular phones, and other smaller systems. J2EE and J2SE use the full Java virtual machine (JVM). J2ME also works with the slimmed-down K virtual machine (KVM), the connected limited device configuration (CLDC), and the mobile information device profile (MIDP).

Other approaches help Java technologies designed for larger computers work on mobile devices. For example, SavaJe developed the SavaJe OS, which supports Java applications in a mobile environment by optimizing J2SE libraries for common mobile CPUs. Mathew Catino, SavaJe's cofounder and vice president of marketing, said Java applications typically spend 80 to 90 percent of their time executing the libraries. Therefore, he explained, optimizing the libraries enables applications to run 10 to 20 times faster. Zeosoft has developed ZeoSphere Developer, which permits the creation of mobile applications that support Enterprise Java Beans, Sun's Java-based software-component architecture. This could simplify the development of complex enterprise applications that communicate and run across servers (via J2EE), PCs (via J2SE), and mobile devices (via J2ME).

Software development tools
Application developers can use existing tools to create Java programs for handheld devices by limiting their code to libraries and APIs supported by J2ME. However, J2ME includes only a limited number of development libraries, noted Jacob Christfort, chief technology officer of Oracle's Mobile Division. Also, said Gartner's Jones, enterprises might shy away from J2ME because of the poor user interface designed for small device screens, the primitive threading model, and minimal native data-handling facilities. In essence, he explained, the design approach that lets J2ME work on small devices sometimes makes it inappropriate for large-scale enterprise uses. To address these concerns, several vendors have released or will soon release development toolkits or toolkit extensions to help developers more easily meet enterprise applications' needs. The new approaches include Sun's Forte for Java Programming Tools, the Oracle 9i Application Server

vendors of Java-enabled mobile devices to find people to write their software.

MAKING JAVA WORK IN HANDHELDS
Sun, which designed and manages development of Java, is in the forefront of making the technology work in handheld devices. However, other vendors have also become active in this area.

Sun Microsystems
Sun and a group of partners created J2ME to make Java work on smaller devices. J2ME includes some core Java instructions and APIs but runs more easily on small devices because it has a smaller footprint than the Java 2 Platform Standard Edition (J2SE) or Enterprise Edition (J2EE), shown in Figure 1, and has only those features relevant for the targeted devices. For example, J2ME's graphics and database-access capabilities are less sophisticated.
18
Computer

J2ME generally incorporates the connected limited device configuration (CLDC), which is implemented on top of operating systems and serves as an interface between the OS and Javabased applications. The CLDC generally uses the K virtual machine (KVM), a slimmed-down, less-functional version of the Java virtual machine (JVM) for small devices. The J2ME mobile information device profile (MIDP) sits on top of the CLDC and provides a set of APIs that define how mobile phones will interface with applications.

Other vendors
Several vendors besides Sun are creating Java-based technologies for handheld devices. Hewlett-Packard makes the MicroChaiVM (http://www.hp. com/products1/embedded/products/dev tools/microchai_vm.html), a cloned JVM that doesn't have Sun's licensing fees and usage restrictions. Several vendors, including Ericsson and HP, plan to use MicroChaiVM-based phones.

Wireless architecture toolkit, and the Sprint PCS Wireless Toolkit. Because of J2ME's shortcomings, Jones said, corporate applications will probably be based on the larger-footprint J2SE as mobile devices get more processing power. Regardless, said John Montgomery, product manager with Microsoft's .NET Development Group, current Java tools are too primitive and difficult to use for most developers.

ETM9 interface Instruction TCM interface Instruction cache Memory management unit ARM9EJ-S core Data TCM interface Data cache Memory management unit Write buffer Control logic and bus interface unit

Server-side handheld Java
Another Java-enabling approach would link handheld devices to Java applications and services on servers. AT&T Wireless, BEA Systems, IBM, Nokia, NTT DoCoMo, Sun, and other companies have created the Java-based Open Mobile Architecture for linking cellular phones and servers. The project would augment J2EE, designed primarily for servers, so that it would support standards that mobile devices can use with Internet-based information. The standards include XHTML (for displaying Web pages on mobile devices), SyncML (for synchronizing data between mobile devices and other machines), WAP 2.0 (to access Internet content and services), and the multimedia messaging service (for handheld messaging).

A R M96EJ- S

Coprocessor interface

AHB interface Instruction Data

Source: ARM Ltd.

Figure 2. ARM Ltd.'s ARM926EJ-S chip includes the company's Jazelle technology in its ARM9EJ-S Java-enabled processor core. In addition, the chip includes separate ETM (embedded trace macrocell), data TCM (tightly coupled memory), and AHB (advanced high-performance bus) interfaces.

IMPLEMENTATION IN HARDWARE AND SOFTWARE
Java technology can be implemented in software or in hardware on either a specialized Java acceleration chip or a core within the main processor. Software implementations tend to run less efficiently because systems must translate each Java instruction into native instructions that the CPU can run. Separate hardware chips are more efficient but represent additional device components and cost. Java cores integrate some of both approaches.

Components Group, said his company has developed techniques for speeding up the software process, which used to bog down when the CPU switched from instructions it could accelerate to instructions it couldn't. In addition, Intel and other software-based Java proponents say the latest mobile processors can run Java fast enough to compete with hardware-based approaches. Analyst Markus Levy with MicroDesign Resources, a semiconductorindustry research firm, disagreed. He said, "People are spending a lot of energy fine-tuning the software-based approaches. For some people that may be good enough, but if you really want the most efficient implementation you need a hardware-based approach."

Software approach
In the software approach, a device's CPU runs the Java code. David Rogers, marketing manager for Intel's PCA

Java hardware
Companies such as ARC Cores, ARM Ltd., Aurora VLSI, Digital Communications Technologies, inSili-

con, and Zucotto Wireless are developing hardware that runs Java, either as Java coprocessing cores for integration into CPUs or as stand-alone Java chips. Both hardware-based approaches promise to increase Java-based application performance and, by running more efficiently, reduce power demands on battery-dependent cellular phones. Different companies' chips execute different subsets of the Java instructions. For example, ARM's Jazelle chip, shown in Figure 2, executes about 68.2 percent of all possible Java instructions, while Aurora's DeCaf runs about 95 percent. Running a bigger set of Java instructions provides more functionality but makes a chip cost more and consume more power. Joan Pendleton, Aurora's cofounder and chief architect, said there are two classes of acceleration. The first, used by most vendors, translates Java byteJune 2002

19

I n d u s t r y Tr e n d s

code into native processor instructions. The second directly executes Java bytecode, which offers better performance but requires a larger footprint because of the additional circuitry necessary to run the software in hardware. Levy predicted that Java cores will be more popular than stand-alone Java processors. This approach's primary constraint is that developers must use a system-on-chip approach to create their products. Putting multiple functions on a chip is more expensive to develop, but the elimination of additional chips reduces device costs. Standalone Java chips are less expensive to design but lead to higher device costs.

performance across platforms. Levy has thus launched a Java-processor group within the Embedded Microprocessor Benchmark Consortium (http://www.eembc.org/). The group expects to release its first benchmark by next month.

"We are still in a phase of market confusion and have not yet gotten to a state of market consolidation," Jones explained.

Not enough applications
There are currently some mobileJava applications, including games and weather and traffic maps. However, Jones said, there are not enough desirable mobile-Java applications yet. The reason is not the technology, he said, but instead the lack of an effective business model and a commercial infrastructure that would enable developers to profit from their work.

A

CONCERNS AND CHALLENGES
Mobile Java is still a relatively new technology. Many industry watchers say the technology has kinks that still need to be worked out. For example, Gartner's Jones expressed concern about vendors' differing Java implementations. He said some developers are complaining about having to manually optimize their Java games for different cellular phones. And although there are many Java developers, there are fewer who have experience working with J2ME and writing code for small, resource-constrained devices. Overall, said Microsoft's Montgomery, "J2ME is an interesting set of engineering compromises, but I would argue exactly the wrong set of compromises. It is too big for the smallest devices but too small to have the features you want on the smartest devices."

Industry observers say mobile Java still has kinks that must be worked out.
The growth of publishing intermediaries that would certify and sell mobile-Java software may eliminate this problem.

HANDHELDS AND THE FUTURE OF JAVA
Jones said Java is doing well on back-end servers because Java-based applications can easily be redeployed as companies buy new servers. However, he noted, client-side Java use has faded considerably because many enterprise-application developers turned to Visual Basic to work within the corporate environment, which is typically Microsoft-based. Thus, the battle for the mobile platform is important to Sun. However, Sun's Java initiatives for cellular phones are facing stiff competition from various sources, including Microsoft's wireless efforts, the Symbian operating system, Linux, and Qualcomm's binary runtime environment for wireless (http://www.qualcomm.com/ brew/).

ccording to Jones, J2ME will attract more application developers as it becomes a richer and less constrained environment. A survey by Evans Data, a market research firm, found that wireless developers who have used Java expect to use the technology a bit more in 2003 than they will this year. Java will also become even more attractive as smart phones get more processing power and vendors design better APIs for color screens, higher quality sound, intellectual-property protection, and user-location capabilities, he added. However, he cautioned, these extra features would give vendors more opportunity to create their own Java implementations, which could fragment the application-development environment. Sprint PCS's Wang said the initial focus of mobile Java will be on games, multimedia, and ring tones. Over time, Levy added, Java will become a de facto standard built into smart phones. SavaJe's Catino predicted that Microsoft and Java-based technologies are likely to coexist in phones during the coming years. Third-party vendors could help this process by developing software-integration techniques that would combine the two environments in devices. I

Performance
Jones said that mobile Java can be somewhat slow because the KVM is not particularly fast. However, he added, the KVM should become faster in the future, particularly as phones with more memory can run just-in-time compiler technology, which enhances performance. "In five years," he said, "[performance] will be a nonissue." Another problem, said Levy, is a lack of standards to objectively measure
20
Computer

George Lawton is a freelance technology writer based in Brisbane, California. Contact him at glawton@ glawton.com.

Editor: Lee Garber, Computer, 10662 Los Vaqueros Circle, PO Box 3014, Los Alamitos, CA 90720-1314; l.garber@computer.org

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

A Java Cryptography Service Provider Implementing One-Time Pad
Timothy E. Lindquist, Mohamed Diarra, and Bruce R. Millard Electronics and Computer Engineering Technology Arizona State University East http://www.east.asu.edu/ctas/ecet mailto:Tim@asu.edu

Abstract
Security is a challenging aspect of communications today that touches many areas including memory space, processing speed, code development and maintenance issues. When it comes to dealing with lightweight computing devices, each of these problems is amplified. In an attempt to address some of these problems, SUN's Java 2 Standard Edition version 1.4 includes the Java Cryptography Architecture (JCA). The JCA provides a single encryption API for application developers within a framework where multiple service providers may implement different algorithms. To the extent possible application developers have available multiple encryption technologies through a framework of common classes, interfaces and methods. The One Time Pad encryption method is a simple and reliable cryptographic algorithm whose characteristics make it attractive for communication with limited computing devices. The major difficulty of the One-Time pad is key distribution.In this paper, we present an implementation of One-Time Pad as a JCA service provider, and demonstrate its usefulness on Palm devices.

Java continues to enjoy dominance in server-side technologies, however, a small but growing number of limited device applications are developed in Java. Nevertheless, Sun Microsystems Inc., added Java Cryptography Extension (JCE) and JCA (to the Java T M 2 Development Kit Standard Edition v1.4 (J2SDK), and has created a substantial market for applications running on J2ME (Java 2 Micro Edition). Other vendors are offering Java runtimes for limited devices. These versions bring Java to client application developers [9], [11], and raise the issue of appropriate Java-based security mechanisms. J2ME does not include JCE and JCA, however The Legion Of The Bouncy Castle has developed a lightweight Cryptography API and a Provider for JCE and JCA [14]. Neither provider offers implementation of the One-Time Pad cryptography service [14]. The simplicity of the One-Time Pad method and the fact that it does not require high processor speed, make it ideal for lightweight computing devices.

1.1

Context

1. Problem
Dependence on the communications infrastructure continues to grow as the size of computing devices decreases. The growing dependence on Internet accessibility to services that do not reside in a local machine brings with it the need for secure communications. The target of this work are relatively small devices and their related systems, such as Windows CE, PalmTE, Handspring and cell phones used to access Internet services. While several large computer service organizations have spent millions of dollars recovering from cyber attacks, the potential economic impact of insecure e-commerce communications on limited devices is huge[1], [3].

This paper focuses on integrating the JCA cryptography service provider, starting by defining the engine classes and then implementing the One-Time Pad method. We include simple evaluation programs to test the provider. The problem of pad distribution is one of the tasks taken-on in order to have successful deployment. Implementations of the one-time pad encryption0.9.4 are readily available. For example, one product is available for Windows command line launching. The source code written in ANSI-C and DOS executable are available for download at http://www.vidwest.com/otp/ [1]. The Security documentation provided with J2SDK includes detailed information on the implementation of

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

1

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

the Provider for the JCA [12]. The documentation for "a Provider for JCE and JCA" of The Legion Of The Bouncy Castle is also available [14]. The possibility of using the One-Time pad for data encryption and decryption for security purposes on lightweight computing devices was covered at the 35th Hawaii International Conference on System Science 2002 [2].

Sender

2. One Time Pad
The one-time pad algorithm is among the simplest in the world of cryptography and is considered by some to be unbreakable. It is nothing more than an exclusive OR between the message (to be encrypted) and the pad (a random key - sequence of bits). The principles that govern the encryption technique are not that simple to apply. First, the key must be random, which by itself is a big challenge. Second, parts of the key that have already been used to do encryption must not be available for other encryption. The key (Pad) must be a sequence of random bits as long as the message to be encrypted. The sender exclusive-OR's the message with the pad and sends the result through a communication channel. The one time requirement that makes it unbreakable and difficult at the same time is that after use, the sender must get rid of part of the pad, and not use it again. At the other end of the communication, the receiver must have an identical copy of the pad. The receiver decrypts the cipher text to obtain the original message by doing an exclusive-OR of the incoming cipher with its copy of the pad [1], [3], [4]. The receiver should also destroy the pad after use. See Figure 1.

Message

XOR

Encrypted message transmitted normally

OTP
Receiver

Encrypted message transmitted normally

XOR

Original Message

OTP

Figure 1. One-time pad (OTP) cryptography.

2.2

Disadvantages of the One Time Pad

2.1

Advantages of the One Time Pad

If the pad is actually random and has been distributed securely to the receiver, then no third party can decrypt the message. Even guessing part of the key will not allow a third party to determine the remainder. This is why some people claim that one-time pad is unbreakable. While there are a number of very good pseudo-random number generators, so far any attempt to generate a truly random key with computers appears to generate the same sequence after a certain point. Several approaches avoid this problem by personalizing the key. Another advantage of this technique resides in the simplicity of its algorithm. It does not involve complex operations that challenge the computational speed of some relatively small processors.

The key must be as large as the message being encrypted; this fact is sometimes inconvenient especially in the case of large messages. The principle of the one-time pad is to have a unique key for each communication, which makes the generation and management of keys problematic as the number of recipients and frequency of use escalates. The last challenging aspect of the One Time Pad is the Key distribution. In fact the key should remain undisclosed (secret) to any other party besides the communicating parties. Extensions to the One-Time Pad provider discussed in this paper center on portable memory devices [15][16] that are frequently synchronized with more capable machine (laptops or desktops) for key exchange. However, our implementation is aimed at handheld devices in general. General purpose (non-proprietary) portable memory interfaces for handheld devices don't exist yet, so another approach is necessary. Some possibilities are discussed in the Key Management section below. Each application must deal with this issue in its own effective way(s).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

2

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3. Java Cryptography Architecture
The Java security model has been evolving to adjust to new security issues. The JCA is a framework providing cryptography functionality development capabilities for a Java platform. It was introduced early in Java's evolution as an add-on package. The first release of the Security API was an extension of JCA including API's for encryption, key exchange, and coding message authentication. Prior to J2SDK 1.4, JCE was optional, in part due to export restrictions. The "Java Secure Socket Extension" (JSSE) and "Java Authentication and Authorization Service" (JAAS) security features have also been integrated into the J2SDK, version 1.4. Two new security features have been introduced: "Java GSS-API" (Java Generic Security Services Application Program Interface) that can be used for securely exchanging messages between communicating applications using the Kerberos V5 mechanism and "Java Certification Path API" that includes classes and methods in the java.security.cert package. These classes allow the developer to build and validate certificate chains. The java cryptography architecture includes a provider architecture [2], [5], [6], [7], [10], [12]. The notion of Cryptography Service Provider (CSP), or just provider, has been introduced in JCA. The provider archit e ct u r e a l l o w s f o r m u l t i p l e an d i n t er o p er ab l e cryptography implementations. An application developer can create or specify his/her own cryptography service provider. The service provider interface (SPI) presents a single interface for implementors. Classes, methods and properties are accessible to applications through the JCA application program interface (API). The SPI allows a cryptography service provider to plugin implementations for java applications. A provider can be used to implement any security service. Several providers can be available and they may or may not provide similar cryptography services and algorithms. Figure 2 depicts the layers of Java Cryptography Architecture, and is taken from Sun Java documentation [6]. A given installation of J2SDK may have several cryptography service providers installed, which may provide implementations of different algorithms and/or may provide multiple implementations of a single cryptography algorithm. Each provider has a name that is used by application programmers to specify the desired provider. It is also possible to specify the order of preference of providers. The default provider that comes with the J2SDK is the Sun provider, which includes a

wide variety of cryptographic algorithms and tools [2], [5], [6], [7], [10], [12].

Figure 2. Java cryptography architecture

3.1

Java Cryptographic Service Providers

The Java Cryptography Architecture, which includes the provider(s), has two main design principles, First, is independence from implementation and interoperability: This derives from using the services without knowing their implementation details [12]. Second, is algorithm independence and extensibility; meaning that new service providers and/or algorithms can be added without effecting existing providers. Together these provide a modular architecture that allows for encryption to be done by an implementation of a specific algorithm and subsequent decryption to be done by another implementation.

3.2

Provider Implementing One Time Pad

The primary question when building such a provider is: does the nature of the one-time pad allow it to be implemented in a pluggable architecture? Figure 1 shows the interoperability between Sender and Receiver as independent systems in communication. The element they are required to have in common is the key. The implementation of the algorithm does not matter. As far as extensibility is concerned, it is up to the provider programmer to remain independent of other cryptographic services. A Cryptographic Service provider is a package or set of packages providing concrete implementations of a subset of the cryptography portion of the Java security SPI. The Java Security Guide in J2SDK, v1.4 documentation lists a series of nine steps to follow for implementing a Provider. This paper follows those steps as guidelines for its development. Two aspects in the structures of cryptographic service were needed to write the implementation code: the Engine Class and the Service Provider Interface (SPI).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

3

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3.3

Engine Class

An Engine Class is an abstraction of a cryptographic service. It defines the service without a concrete implementation of the particular associated algorithms. Applications access instances of the engine class through the API, to carry out available operations. Every engine class has a corresponding service provider interface, which provides abstract classes accessing the engine class features. The service provider interface indicates all the methods that the actual cryptographic service provider should implement for a particular cryptographic algorithm or type. A service provider interface is named with its engine class name followed by "Spi" [12]. For each service that a provider implements, we must define the engine class, and then write its service provider interface. For the One Time Pad technique, the service provider interface's abstract class is called OneTimePadSpi. The engine class for OneTimePadSpi in compliance to the nomenclature of JCA is called OneTimePad. The engine class is a concrete subclass of the service provider interface, implementing all the abstract methods. The provider class is a final subclass of java.security.provider. Our provider is named ASUEcetProvider. The provider name is used by applications to access our one-time pad service [12].

Java. The file can be bundled with a particular application (with a manifest indicating relative URLs), or it can be installed in the Java Runtime Environment to be shared by all running applications.

3.6

Registering the Service

Configuring the service provider enables client access to the service(s) by registering the provider and defining default preferences where more than one provider is registered for the same service algorithm. Static Registration consists of editing the java.security file (located in " lib\security " subdirectory of the Java Runtime Environment) to add the provider name to the list of approved providers. For each available provider for a given algorithm, there is a corresponding line in the java.security file with the form: security.provider.<n>=<providerClassName> Where "n " is the preference number for the provider. For example the line: security.provider.2=asue.provider.ASUEcetProvider registers our provider with an order of preference 2. Dynamic Registration can be done by a client application upon requesting service(s) from a provider. The client application calls a class method, such as: Security.addProvider (Provider providerName).

3.7

Test Programs and Documentation

3.4

Provider's Information

The provider class provides access to various properties of the service, including the version, and other information about the service(s) it provides such as algorithm, type, and techniques [2], [12]. The value provided for this argument in this project is: "ASUEcetProvider v1.0, implementing One Time Pad (OTP) cryptographic technique, Arizona State University East, Electronic and Computer Engineering Technology. May 2003"

3.5

Install and Configure the Provider

The provider needs to be correctly installed and configured for the application program to utilize its cryptographic service(s). There are two different ways to install a provider. The first method consists of creating a JAR file (Java Archive File) or ZIP file containing all the class files belonging to the Cryptography Service Provider. The JAR file is added to the CLASSPATH environment variable. The exact steps of doing this last action, depends upon the local operating system [2], [12]. The second approach deploys the provider's JAR file of classes as an extension (optional package) to

Several test programs were written to exercise three aspects of the service provider. For client applications to be able to request service(s) provided by a specific provider, the provider should be successfully registered with the security API. A simple test program can verify registration by creating an instance of the provider and accessing its name, version, and info (getName (), getVersion (), and getInfo () methods. After making sure that the provider is accessible from the security API, we need to retrieve the provided service(s) by calling its "getAlgorithm ()" method. Finally, we check the functionality provided by the service by writing sender and receiver applications that use the service. In addition to providing sample service test programs, our implementation provides documentation. Our documentation is generated by the Javadoc tool from the source code and it targets application programmers.

4. Key Management
The generation and distribution of the random keys for this method is of primary concern. Since the size of the key must match the size of the message being

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

4

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

encrypted, we limit our application to transmission of relatively small messages. The key can be any random array of bits, the key type used in JCA has not been used. The OneTimePad class has been designed, so that when a new random key is generated, it is stored in an external file that could be later sent to the receivers. There are many approaches to providing random keys for the One-Time Pad. The primary problem with using the One-Time Pad is the amount of random bits needed. Every message sent needs a key of the same length. If every message is sent encrypted, the required key space becomes large very quickly. The alternative of only encrypting sensitive data suffers from forcing the application to make these choices, and only delays the issue of when and how to replenish the key once all the bits have been used on earlier messages. Handheld devices, the target for this work, generally only send small messages requiring encryption. This allows for many messages using a small key of say 1 MB. A nightly synchronization using a recharging cradle can be used to also replenish the One-Time Pad key. For devices that don't have a connection to a host while charging, another approach is portable memory devices [15][16]. With a 2 GB pad, the replenishment cycle would be much longer. Generally long enough to add a few jpeg or gif pictures a day. The primary problem with portable memory and handheld devices is that of interfacing requirements. Currently, no general-purpose interfaces (e.g., USB) are available for handhelds. Portable memory devices, to be useful, must come with general purpose interfaces, such as USB. An alternative to the cradle and portable memory approach is to update or replenish the pad on-line. In this approach, a One-Time Pad is used until almost exhausted. Then the remaining pad is used to exchange a block-cipher key for a more computationally complex cryptography algorithm, such as RC4 or AES. The One-Time Pad can then be generated by the server and sent to the handheld using the complex cipher. The encryption process would then return to the One-Time Pad methodology. This approach is expensive on both network and CPU utilization. It may only be acceptable on larger handhelds such as Windows CE (Pocket PC) machines. A final alternative would be to use PRNG (pseudo random number generator) that is cryptographically appropriate. An example PRNG is ISAAC [17]. ISAAC is a relatively fast random number generator with a very long cycle (i.e., guaranteed 240 with an average 28295). Generating random pads then requires knowing the seed. Using the prior approach of a trailing seed from the last pad as a seed for the new pad would permit, possibly, near realtime generation of One-Time Pads.

5. Running on Limited Devices
The fact that JCA and JCE are not part of J2ME, limits applicability to our intended application space. One approach is to configure limited client applications by embedding the provider directly in the deployed J2ME application. Another approach is to use the lightweight cryptography API defined by The Legion Of The Bouncy Castle to develop a provider based on their design principle of A provider for the JCE and JCA [14]. This solution results with a provider not fitting exactly the Java Cryptography Architecture, but which is usable on J2ME devices, such as PalmOS [14].

6. Conclusions and Enhancements
As long as electronic communication continues to expand, security will remain an issue. Cryptography is one of the most effective tools available to address these issues. One-time pad is among the most powerful existing cryptographic techniques, providing it is used within the constraints of its applicability. Primarily the constraints are key management, compute limited devices and encryption tasks that do not require encrypting large files. The Java Cryptography Architecture offers the potential of a single interface for applications that allows plug-in of any number of participating service providers. The approach allows evolution of security approaches with the promise of minimal impact on applications. Security remains an open field on every computing platform. As platforms continue to evolve to smaller and better connected devices, they will meet the information needs of a broader range of consumers. Its importance to provide frameworks for developing secure distributed and web-based applications on such mobile devices.

7. References
[1.] d@vidwest.net (2000, November 17). "One Time Pad Encryption v0.9.4" Retrieved January 25, 2002 from the World Wide Web: http://www.vidwest.com/otp/ Gong, L. (1998). "JavaTM 2 Platform Security Architecture Version 1.1". Sun Microsystems, Inc. Retrieved February 20, 2003 from the World Wide Web: http://java.sun.com/j2se/1.4/docs/guide/security/spec/securityspec.doc.html Jenkin M. & Dymond P. (2002) "Secure communication between lightweight computing devices over the Internet". HICSS 35 January 2002. Kahn D. (1967) The Codebreakers, New York, NY, MacMillan. Knudsen J. (1998) Java Cryptography, Sebastopol, CA,

[2.]

[3.]

[4.] [5.]

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

5

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

O'Reilly. [6.] Lindquist T. (2003, January 14). "Cet427/598 Distributed Object Systems". Retrieved February 02, 2003 from the World Wide Web: http://pooh.east.asu.edu/Cet427/ClassNotes/Security/ cnSecurity.html McGraw, G. and Felten, E. (1996) Java Security: Hostile Applets, Holes, and Antidotes. New York, NY. John Wiley & Sons. McGraw, G. (1998) "Testing for security during development: why we should scrap penetrate and patch". IEEE Aerospace and Electronic Systems, 13(4):13-15, April 1998. Muchow J. (2001) Core J2METM Technology and MIDP, Upper Saddle River, NJ, Prentice Hall. Oaks, S. (1998) Java Security, Sebastopol, CA, O'Reilly & Associates. Rubin, A, Geer, D. and Ranum, M. (1997) The Web Security Sourcebook. New York, NY, John Wiley & Sons.

[12.]

Sun Microsystems, Inc (2002). "Java 2 Platform, Standard Edition, v 1.4.0 API Specification". Retried January 12, 2002 from World Wide Web: http:// java.sun.com/docs/ Sundsted T. (2001). "Java, J2ME, and Cryptography" Retrieved March 20, 2003 from the World Wide Web: http://www.itworld.com/nl/java_sec/10262001/ The Legion Of The Bouncy Castle (2000). "The Bouncy Castle Crypto APIs" Retrieved March 20, 2003 from the World Wide Web: http://www.bouncycastle.org/index.html Zbar, Jeff, "Portable Memory Gets Small," retrieved Sept. 2003; http://www.beststuff.com/article.php3?story_id=4395. Lexar Media, "Samsung Sampling 2Gb NAND Flash Memory Devices to Lexar Media," retrieved Sept. 2003; http://www.digitalfilm.com/newsroom/press/ press_02_25_02a.html. Jenkins, Bob, "ISAAC: a fast cryptographic random number generator," retrieved Sept. 2003; http:// www.burtleburtle.net/bob/rand/isaacafa.html.

[13.]

[7.]

[14.]

[8.]

[15.]

[9.] [10.] [11.]

[16.]

[17.]

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

6

