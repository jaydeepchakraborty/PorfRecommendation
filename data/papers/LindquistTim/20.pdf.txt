CANONICAL CORRELATION ANALYSIS, APPROXIMATE
COVARIANCE EXTENSION, AND IDENTIFICATION OF
STATIONARY TIME SERIES*
ANDERS LINDQUIST‚Ä† AND GIORGIO PICCI‚Ä°

Abstract. In this paper we analyze a class of state-space identiÔ¨Åcation algorithms
for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called ‚Äúsubspace methods‚Äù
can be described as covariance estimation followed by stochastic realization. The
methods oÔ¨Äer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identiÔ¨Åcation into the solution of a Riccati
equation but introduce at the same time some nontrivial mathematical problems
related to positivity. The reason for this is that an essential part of the problem
is equivalent to the well-known rational covariance extension problem. Therefore
the usual deterministic arguments based on factorization of a Hankel matrix are
not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identiÔ¨Åcation
procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not
explicitly reported.
In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We
review the basic ideas of stochastic realization theory in the context of identiÔ¨Åcation, discuss the concept of stochastic balancing and of stochastic model reduction
by principal subsystem truncation. The model reduction method of Desai and Pal,
based on truncated balanced stochastic realizations, is partially justiÔ¨Åed, showing
that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing
conditions under which the ‚Äùsubspace identiÔ¨Åcation‚Äù methods produce bona Ô¨Åde
stochastic systems.

1. Introduction
Recently there has been a renewed interest in state-space identiÔ¨Åcation algorithms
for time series based on a two steps procedure which in principle can be described
as estimation of a rational covariance model from observed data followed by stochastic realization. The method oÔ¨Äers the major advantage of converting the nonlinear
parameter estimation phase which is necessary in traditional ARMA models identiÔ¨Åcation into a partial realization problem, involving a Hankel matrix of estimated
‚àó This research was supported in part by grants from TFR, the GoÃàran Gustafsson Foundation,
the SCIENCE project ‚ÄúSystem IdentiÔ¨Åcation‚Äù and LADSEB-CNR.
‚Ä† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
‚Ä° Dipartimento di Elettronica e Informatica, Universita‚Äô di Padova, 35131 Padova, Italy
1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which eÔ¨Écient numerical solution techniques are available. In this framework
we can naturally accommodate multivariate processes and there are indications that
the algorithms may work also with data containing purely deterministic components
(van Overschee and De Moor, 1993). A drawback, however, to be emphasized in
this paper, is that, unlike, say, least-squares identiÔ¨Åcation of ARMA models, these
methods do not work for arbitrary data.
This type of procedure was apparently Ô¨Årst advocated by Faurre (1969); see also
Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work,
based on canonical correlation analysis (Akaike, 1975) (or some other singular-value
decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki
(1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor,
1993). Numerical experience shows that the computation time needed to get the Ô¨Ånal model parameters estimates compares very favorably with traditional iterative
prediction error methods for ARMA models.
On the other hand there is a price to be paid for this simpliÔ¨Åcation. These methods
introduce some nontrivial mathematical problems related to positivity. The reason
for this is that an essential part of the problem is equivalent to the well-known rational
covariance extension problem. Therefore the usual deterministic realization arguments
based on factorization of a Hankel matrix are not valid for generic data, something
that is habitually overlooked in the literature. Note that positivity is the natural
condition insuring solvability of the Riccati equation required to compute state-space
models of the signal from the covariance estimates.
Central in the procedures described above is the following classical problem of
identiÔ¨Åcation of a covariance sequence. Let
{Œõ0 , Œõ1 , . . . , ŒõŒΩ }

(1.1)

be a Ô¨Ånite set of sample m √ó m covariance matrices estimated in some unspeciÔ¨Åed
way from a certain m-dimensional sequence of observations
{y0 , y1 , y2 , . . . yT },

(1.2)

and consider the problem of Ô¨Ånding a minimal1 triplet of matrices (A, C, CÃÑ) such that
CAk‚àí1 CÃÑ  = Œõk

k = 1, 2, . . . , ŒΩ

(1.3)

and such that the inÔ¨Ånite sequence
{Œõ0 , Œõ1 , Œõ2 , . . . },

(1.4)

obtained from (1.1) by setting Œõk := CAk‚àí1 CÃÑ  for k = ŒΩ + 1, ŒΩ + 2, . . . , is a bona Ô¨Åde
covariance sequence.
In the literature the last condition is generally ignored. The remaining problem of
Ô¨Ånding a minimal triplet (A, C, CÃÑ) satisfying (1.3) is called the minimal partial realization problem. The triplet (A, C, CÃÑ) is usually computed by minimal factorization
1

Here (A, C, CÃÑ) is minimal if (A, C) is completely observable and (A, CÃÑ  ) is completely reachable.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

of a block Hankel matrix corresponding to the data (1.1) as follows:
Ô£Æ
Ô£π Ô£Æ
Ô£πÔ£Æ
Ô£π
CÃÑ
Œõ1 Œõ2
C
Œõ3 ¬∑ ¬∑ ¬∑
Œõj
Ô£ØŒõ2 Œõ3
Œõ4 ¬∑ ¬∑ ¬∑ Œõj+1 Ô£∫ Ô£Ø CA Ô£∫ Ô£Ø CÃÑA Ô£∫
Ô£∫ ,
H=Ô£Ø
= Ô£Ø . Ô£∫Ô£Ø
..
.. Ô£∫
..
..
..
Ô£∞ ...
Ô£ª
.
.
. Ô£ª Ô£∞ .. Ô£ª Ô£∞
.
.
Œõi Œõi+1 Œõi+2 ¬∑ ¬∑ ¬∑ Œõi+j‚àí1
CAi‚àí1
CÃÑ(A )j‚àí1

3

(1.5)

where i + j ‚àí 1 = ŒΩ and the Hankel matrix H is chosen as close to square as possible
by taking |i ‚àí j| ‚â§ 1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j) such
that i + j ‚àí 1 = ŒΩ, and hence the minimal factorization must be made for a choice
of (i, j) in which the Hankel matrix (1.5) has maximal rank. The inÔ¨Ånite sequence
{Œõ0 , Œõ1 , Œõ2 , . . . } obtained in this way by setting Œõk := CAk‚àí1 CÃÑ  for k = ŒΩ +1, ŒΩ +2, . . .
is called a minimal rational extension of the Ô¨Ånite sequence (1.1) and is in general not
a covariance sequence. The dimension r of a minimal rational extension is called the
(algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to
the McMillan degree of the m √ó m rational matrix
1
(1.6)
Z(z) = C(zI ‚àí A)‚àí1 CÃÑ  + Œõ0 ,
2
and the elements of the inÔ¨Ånite sequence (1.4) are the coeÔ¨Écients of the Laurent
expansion
1
Z(z) = Œõ0 + Œõ1 z ‚àí1 + Œõ2 z ‚àí2 + . . .
2

(1.7)

about z = ‚àû.
The underlying identiÔ¨Åcation problem is however a great deal more complicated
than the classical partial realization problem. In fact, the requirement that (1.4) be a
bona Ô¨Åde covariance sequence amounts to (1.4) being a positive sequence in the sense
that, for every t ‚àà Z+ , the block Toeplitz matrices Tt ,
Ô£Æ
Ô£π
Œõ2 ¬∑ ¬∑ ¬∑ Œõt
Œõ0 Œõ1
Ô£ØŒõ1 Œõ0
Œõ1 ¬∑ ¬∑ ¬∑ Œõt‚àí1 Ô£∫
Tt = Ô£Ø
,
(1.8)
..
.. Ô£∫
.
.
...
Ô£∞ ..
..
.
. Ô£ª
Œõt Œõt‚àí1 Œõt‚àí2 ¬∑ ¬∑ ¬∑

Œõ0

formed from the inÔ¨Ånite sequence (1.4), be positive deÔ¨Ånite or, equivalently, that the
matrix function
Œ¶(z) := Z(z) + Z(1/z)

(1.9)

be positive semideÔ¨Ånite on the unit circle, i.e.
Œ¶(eiŒ∏ ) ‚â• 0 Œ∏ ‚àà [0, 2œÄ).

(1.10)

This property is equivalent to Œ¶ being a spectral density matrix. In fact, it will be the
spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial
covariance sequence unless TŒΩ > 0, but this is not enough.
From the point of view of identiÔ¨Åcation there seem to be two possible routes to
determine a model (A, C, CÃÑ) from the Ô¨Ånite covariance sequence (1.1). One that
has been proposed in the literature is do minimal factorization (1.5) of a Ô¨Ånite block
Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown
in this paper, there is no a priori guarantee that this method will yield a positive
extension. This fact has nothing to do with sample variability (random Ô¨Çuctuations)
of the covariance estimates (1.1), and to emphasize this point we initially assume
that all strings of data (1.2) are inÔ¨Ånitely long. A theoretically sounder identiÔ¨Åcation
method, which will not be considered in this paper, could instead be to do positive
extension Ô¨Årst and then to use a stochastic model reduction procedure on the triplet
(A, C, CÃÑ) of the positive extended sequence.
The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this
diÔ¨Éculty into consideration have been pointed out by the authors of this paper at
many scientiÔ¨Åc meetings in the last ten years. This has had no apparent eÔ¨Äect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in
which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identiÔ¨Åcation procedure of Aoki (1990) and
demonstrate that there is a hidden, and not easily tested, assumption without which
the procedure will not be guaranteed to succeed. The punch line is that none of the
subspace identiÔ¨Åcation methods under consideration can be expected to always work
for generic data but that some not entirely natural conditions on the data are needed.
The analysis of the basic theoretical issues behind subspace identiÔ¨Åcation is carried
out in the geometric framework of stochastic realization theory; see, e.g., Lindquist
and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this
theory and adapt them to the problem of identiÔ¨Åcation. To this end, we Ô¨Årst discuss
an idealized situation in which the time series (1.2) is inÔ¨Ånitely long i.e. T = ‚àû, and
the available covariance data are given by the ergodic limit
1 

yt+k yt+j
= Œõk‚àíj
T ‚Üí‚àû T + 1
t=0
T

lim

(1.11)

for all k and j. Then the sample estimates in the sequence (1.1) are bona Ô¨Åde covariance matrices and the Toeplitz matrix TŒΩ formed from the data will be positive
deÔ¨Ånite and symmetric. We introduce a Hilbert space of observed (inÔ¨Ånite) strings of
data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic
system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical
problem of identiÔ¨Åcation. In this way we establish a correspondence which turns operations on random quantities deÔ¨Åned on abstract probability spaces into prototypes of
statistical algorithms involving computations based on the observed data. Canonical
correlations and balanced stochastic realizations are then analyzed in this setting in
Section 4, and the basic concepts and principles used in the subspace identiÔ¨Åcation
methods, as well as in the model reduction procedures of Desai and Pal, are translated
into the more natural context of geometric stochastic realization theory.
Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such
objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact inÔ¨Ånite covariance sequence
(1.4). Consequently, the diÔ¨Écult question of positivity is not an issue here. Nor is
it the Ô¨Ånite sample size per se which is the problem, but the fact that only a Ô¨Ånite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is Ô¨Ånite.
Therefore we separate these issues by discussing stochastic realization theory from
Ô¨Ånite covariance data in Section 5 and subspace identiÔ¨Åcation in Section 6. In this
framework we show that the method of van Overschee and De Moor (1993) is valid
under some rather stringent assumptions. We stress that we are only concerned with
identiÔ¨Åcation procedures for state space modeling of time series. ‚ÄúSubspace identiÔ¨Åcation‚Äù methods for deterministic systems with measurable inputs or for spectral
factors do not involve positivity, but stability may still be a problem. However, the
algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part,
so the problem of positivity arises here too.
Another idea behind the subspace identiÔ¨Åcation methods considered in this paper is
to disregard modes corresponding to ‚Äúsmall‚Äù canonical correlation coeÔ¨Écients. This
is called balanced truncation and is in fact a stochastic model reduction procedure.
In all such procedures there must be a guarantee that the reduced-degree matrix
function (1.6) is positive real, and therefore the preservation of positivity in such
reductions is a main concern of this paper. Section 7 is devoted to such issues. The
model reduction procedure of Desai and Pal (1982) was never theoretically justiÔ¨Åed
in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 .
Here we shall demonstrate that this reduction procedure produces a positive real, but
not in general balanced, reduced model structure. In fact, the singular values of the
truncated system are usually not equal to the r Ô¨Årst singular values of the original
system.
It is an interesting fact that the procedure of Desai and Pal does produce balanced
truncations for continuous-time stochastic systems. A partial result in this direction
was given by Harshavardana, Jonckheere and Silverman (1984), who showed that
the truncated function is positive real and conjectured that it is balanced. We shall
demonstrate that it is indeed balanced, a result that is actually already contained in
the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time
depends on the fact that the spectral factors of the truncated approximate spectrum
behave diÔ¨Äerently than in continuous time. While in continuous time the realizations
of the reduced spectral factors are proper subsystems, obtained by partitioning the
matrices of the realizations of the factors of Œ¶, this is not the case in discrete time,
contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced
truncation procedure is available in discrete time, but the systems matrices are no
longer submatrices of those of the original system, and therefore it is not equivalent
to the truncation procedure used in subspace identiÔ¨Åcation.
Several of the results of this paper have previously been announced in Lindquist
and Picci (1994a)3 and in Lindquist and Picci (1994b).

2

In Desai et al. (1985) a diÔ¨Äerent model reduction procedure, which is not relevant to subspace
identiÔ¨Åcation, is considered, namely ‚Äúdeterministic‚Äù model reduction of the minimum phase spectral
factors.
3
We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some
erroneous statements, was accidentally published in place of the paper Ô¨Ånally submitted for publication. The correct version can be obtained from the authors.

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances
The solution to the minimal partial realization problem , i.e., the problem to Ô¨Ånd
the triplet (A, C, CÃÑ) satisfying (1.1) is in general not unique. This lack of uniqueness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and
Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1)
equals that of
{Œõ0 , Œõ1 , . . . , ŒõŒΩ‚àí1 }
so that we can use a Hankel matrix (1.5) based
allowing us to deÔ¨Åne the shifted Hankel matrix
Ô£Æ
Œõ3
Œõ4
Œõ2
Ô£Ø Œõ3
Œõ4
Œõ5
œÉ(H) = Ô£Ø
..
.
.
Ô£∞ ..
..
.
Œõi+1 Œõi+2 Œõi+3

(2.1)

on this data, i.e., with i + j = ŒΩ,
Ô£π
¬∑ ¬∑ ¬∑ Œõj+1
¬∑ ¬∑ ¬∑ Œõj+2 Ô£∫
.. Ô£∫
...
. Ô£ª
¬∑ ¬∑ ¬∑ ŒõŒΩ

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) produces a minimal solution (A, C, CÃÑ) which is unique up to a similarity transformation.
As Ô¨Årst pointed out by Zeiger and McEwen (1974), the minimal factorization on
which the Ho-Kalman procedure is based may be performed by singular-value decomposition, thereby Ô¨Åxing (A, C, CÃÑ) uniquely; see also Kung (1978). In fact, the Hankel
matrix H may be factored as
H = U Œ£V 

U  U = I = V  V,

(2.3)

where Œ£ is the square n √ó n diagonal matrix of the nonzero singular values taken in
decreasing order. Setting ‚Ñ¶ := U Œ£1/2 and ‚Ñ¶ÃÑ := V Œ£1/2 this leads to a factorization
H = ‚Ñ¶‚Ñ¶ÃÑ

‚Ñ¶ ‚Ñ¶ = Œ£ = ‚Ñ¶ÃÑ ‚Ñ¶ÃÑ

(2.4)

of the type (1.5). Then a minimal realization (A, C, CÃÑ) is obtained by solving
‚Ñ¶A‚Ñ¶ÃÑ = œÉ(H),

C ‚Ñ¶ÃÑ = œÅ1 (H) and CÃÑ‚Ñ¶ = œÅ1 (H  ),

where œÉ(H) is the shifted Hankel matrix (2.2) and œÅ1 (H) is the Ô¨Årst block row of H.
It follows that the triplet (A, C, CÃÑ) must be given by
A = Œ£‚àí1/2 U  œÉ(H)V Œ£‚àí1/2 ,
C = œÅ1 (H)V Œ£‚àí1/2 ,
CÃÑ = œÅ1 (H  )U Œ£‚àí1/2 ,

(2.5a)
(2.5b)
(2.5c)

a form to which we refer as Ô¨Ånite-interval balanced, since it is balanced in the sense
that ‚Ñ¶ ‚Ñ¶ and ‚Ñ¶ÃÑ ‚Ñ¶ÃÑ are both equal to Œ£, and that
Ô£π
Ô£Æ
Ô£π
Ô£Æ
CÃÑ
C
Ô£Ø CÃÑA Ô£∫
Ô£Ø CA Ô£∫
Ô£∫
Ô£Ø
Ô£∫.
‚Ñ¶ÃÑ
=
(2.6)
‚Ñ¶=Ô£Ø
..
Ô£ª
Ô£∞
Ô£∞ ... Ô£ª
.
CAi‚àí1

CÃÑ(A )j‚àí1

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identiÔ¨Åcation of time
series. The problem with such a strategy is that this algorithm is a deterministic
realization procedure and hence does not a priori insure that (1.6) is positive real, or
even stable for that matter, even if the Toeplitz matrix TŒΩ is positive deÔ¨Ånite. In fact,
it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space
of covariance data (1.1) for which A is not stable, and a fortiori the same holds for
positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in
Aoki (1990) is based on the following hidden assumption which is not entirely natural.
Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H.
Therefore, not only must we know that there exists an underlying Ô¨Ånite-dimensional
system, but we must also have some upper bound for its dimension. A conservative
upper bound which will always suÔ¨Éce is [ ŒΩ2 ].
Is this assumption natural? If the covariance data are really generated exactly from
a ‚Äútrue‚Äù stochastic system and there is a reliable estimate of its order which is no
more than half of the length of the covariance sequence, then the assumption will hold.
However, and this is an important point of this paper, one cannot expect Assumption
2.1 to hold for an arbitrary covariance sequence (1.1).
To clarify this point, let us agree to call {Œõ0 , Œõ1 , Œõ2 , . . . } a minimal rational extension of {Œõ0 , Œõ1 , . . . , ŒõŒΩ } if the rational function (1.7) has minimal degree. By
deÔ¨Ånition this is the algebraic degree of {Œõ0 , Œõ1 , . . . , ŒõŒΩ }. A rational extension is
called positive if, for every ¬µ > ŒΩ, the block Toeplitz matrices T¬µ formed from the
corresponding inÔ¨Ånite sequence (1.4) are positive deÔ¨Ånite. An extension with this
property is called a positive rational extension. It is well known that the extension
{Œõ0 , Œõ1 , Œõ2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function
Z(z) is analytic in the closed unit disc and the matrix function
Œ¶(z) = Z(z) + Z(1/z)

(2.7)

is nonnegative deÔ¨Ånite on the unit circle, making Œ¶ a spectral density matrix. A
minimal positive rational extension of the Ô¨Ånite sequence (1.1) is one for which the
dimension of the triplet (A, C, CÃÑ) in (1.6) is as small as possible.
DeÔ¨Ånition 2.2. The positive degree p of the Ô¨Ånite covariance sequence {Œõ0 , Œõ1 , . . . , ŒõŒΩ }
is the dimension of any minimal positive extension.
A well-known example of a positive extension is the maximum entropy extension
(Whittle, 1963) corresponding to the spectral density Œ¶(z) := W (z)W (1/z) , where
the spectral factor W (z) is (modulo a multiplicative constant matrix) the inverse of the
Levinson-SzegoÃà matrix polynomial of order ŒΩ corresponding to the Ô¨Ånite covariance
sequence (1.1). Since the rational function W (z) generically has the McMillan degree
equal to mŒΩ, it follows from spectral factorization theory (Anderson, 1958) that Z(z)
has also degree mŒΩ. Consequently, the positive degree p is bounded from below by
the algebraic degree r and from above by mŒΩ.
As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use
algebraic rather than positive extensions, usually computed by minimal factorization
a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1
may also be formulated in the following way.


Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree.
This assumption prescribes a property of the covariance sequence (1.1) which is not
generic. We can illustrate this point by considering the rational extension problem
for a Ô¨Ånite scalar covariance sequence (1.1). The positive degree p lies between the
algebraic degree r and ŒΩ. Note that neither the case p = ŒΩ nor the case p < ŒΩ
are ‚Äùrare events‚Äù, because there are open sets of covariance sequences (1.1) of both
categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each ¬µ
such that ŒΩ2 ‚â§ ¬µ ‚â§ ŒΩ there is an open set of covariance data in RŒΩ for which p = ¬µ.
If the upper limit p = ŒΩ is attained there are inÔ¨Ånitely many nonequivalent minimal
triplets (A, C, CÃÑ) providing a positive extension, one of which is the maximum entropy
extension. In fact, it can be shown that these ŒΩ-dimensional extensions form an
Euclidean space (Byrnes and Lindquist, 1989). This shows that the Ô¨Ånite data (1.1)
never contains enough information to establish a ‚Äùtrue‚Äù underlying system. A similar
statement can be made in the case when p < ŒΩ.
Example 2.3. Consider the case m = 1 and ŒΩ = 2, i.e., consider a scalar partial
covariance sequence {Œõ0 , Œõ1 , Œõ2 }. If Œõ1 = Œõ2 = 0, we have r = p = 0. Otherwise,
we always have r = 1, whereas the positive degree can be either one or two. In fact,
setting Œ≥0 := Œõ1 /Œõ0 and Œ≥1 := (Œõ21 + Œõ2 )/(1 ‚àí Œõ21 ), it can be shown (Georgiou, 1987;
also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1
if and only if
|Œ≥0 |
|Œ≥1 | <
1 + |Œ≥0 |
and p = 2 otherwise.
In fact, it is not hard to construct examples for which the gap between algebraic
and positive rank is arbitrarily large, as the following theorem shows.
Theorem 2.4. Let n ‚àà Z+ be Ô¨Åxed. Then for an arbitrarily large ŒΩ there is a stable
rational function Z(z) of degree n, such that the Toeplitz matrix TŒΩ formed as in ( 1.8)
from the coeÔ¨Écients of the Laurent expansion ( 1.7), is positive deÔ¨Ånite while TŒΩ+1 is
indeÔ¨Ånite.
Consequently, you cannot test the positivity of a rational extension of (1.1) by
checking a Ô¨Ånite Toeplitz matrix, however large is its dimension. The proof of Theorem
2.4 is given in Appendix A.
Let us now return to the identiÔ¨Åcation procedure of Aoki (1990). In practice the
rank of H will always be full, and to compute a partial realization of reasonable
dimension the basic idea is to partition Œ£ as


	
Œ£1 0
,
(2.8)
Œ£=
0 Œ£2

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in Œ£2 are smaller than those in Œ£1 , perhaps close to zero,
and then take Œ£2 = 0 so that H is approximated by
	


Œ£1 0 
(2.9)
V = U1 Œ£1 V1 .
H1 = U
0 0
The matrix H1 is a best approximation (given the rank) of H in (the induced) !2 ‚Äì
norm, but it is in general not Hankel and hence can not be used to determine a
reduced order system. Of course, one may instead use Hankel-norm approximation
(Adamjan, Arov and Krein, 1971), which produces another best approximation of
H in !2 -norm that is Hankel and has the same rank as H1 . However, if Œ£2 is ‚Äúvery
small‚Äù compared to Œ£1 , then H1 is close to H and hence approximately Hankel. For
this reason, Aoki‚Äôs procedure (Aoki, 1990) is based on the original data H and œÉ(H).
Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I, the
same type of calculation as above yields the reduced triplet (Ar , Cr , CÃÑr ) given by
‚àí1/2

Ar = Œ£1

‚àí1/2

U1 œÉ(H)V1 Œ£1
‚àí1/2

Cr = œÅ1 (H)V1 Œ£1

,

,

‚àí1/2

CÃÑr = œÅ1 (H  )U1 Œ£1

(2.10a)
(2.10b)

.

(2.10c)

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal
subsystem truncation in the sense that, if H is produced by a Ô¨Ånite-dimensional system
with (A, C, CÃÑ) having Ô¨Ånite-interval balanced form (2.5), we have
Ar = A11 ,
where



	
A11 A12
A=
A21 A22

Cr = C1 ,

CÃÑr = CÃÑ1 ,



C = C1 C2



CÃÑ = CÃÑ1 CÃÑ2 .

(2.11)

(2.12)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for œÉ(H),
œÅ1 (H) and œÅ1 (H  ) and inserting in (2.10).
However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that
1
(2.13)
Z1 (z) = C1 (zI ‚àí A11 )‚àí1 CÃÑ1 + Œõ0
2
is positive real, provided of course that Z, deÔ¨Åned by (1.6), is positive real. The
question of stability was answered in the aÔ¨Érmative in Pernebo and Silverman (1982)
and is addressed in Aoki (1990). The crucial question of positivity, however, is not
discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be
proven for a somewhat modiÔ¨Åed procedure described below.
In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall
consider a normalized Hankel matrix
‚àíT
HÃÇ = L‚àí1
+ HL‚àí ,

(2.14)

where L‚àí and L+ are lower triangular Cholesky factors of the Toeplitz matrices T‚àí
and T+ of (1.1) and the corresponding sequence of transposed covariances respectively;
see Section 4 below. This is also the Hankel matrix considered in van Overschee and
De Moor (1993). Taking the singular value decomposition of HÃÇ instead of H, the

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coeÔ¨Écients, i.e., the cosines of the
angles between the past and the future of the process y. The systems matrices can
be determined in a manner analogous to (2.5), but now
‚Ñ¶ T+‚àí1 ‚Ñ¶ = Œ£ÃÇ = ‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑ

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as
above. To see this, consider the singular value decomposition HÃÇ = UÃÇ Œ£ÃÇUÃÇ  so that H =
(L+ UÃÇ )Œ£ÃÇ(L‚àí VÃÇ ) . Since H = ‚Ñ¶‚Ñ¶ÃÑ and this factorization is unique modulo coordinate
transformation in state space, we may take ‚Ñ¶ = L+ UÃÇ Œ£ÃÇ1/2 and ‚Ñ¶ÃÑ = L‚àí VÃÇ Œ£ÃÇ1/2 . Then
(2.15) follows from UÃÇ  UÃÇ = I = VÃÇ  VÃÇ . As we shall see next, (2.15) corresponds to a
more natural type of balancing corresponding to a Hankel operator describing the
interface between the past and the future of the time series y.
3. Stochastic realization theory in the Hilbert space of a sample function
In this section we introduce a mathematical framework which is suitable for the identiÔ¨Åcation problem described above. We deÔ¨Åne a Hilbert space of observed (inÔ¨Ånite)
strings of data {yt }. This framework turns out to be isomorphic to that of geometric
stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985,
1991) also for the statistical problem of identiÔ¨Åcation. In this way we also establish a
correspondence which converts operations on random quantities deÔ¨Åned on abstract
probability spaces into prototypes of statistical algorithms involving computations
based on the observed data.
In identiÔ¨Åcation we have access only to a Ô¨Ånite string of data
{y0 , y1 , y2 , . . . , yT }.

(3.1)

Here T may be quite large but, of course, always Ô¨Ånite. To begin with, we shall,
however, consider the idealized situation that we are given a doubly inÔ¨Ånite sequence
of m-dimensional data
{. . . , y‚àí3 , y‚àí2 , y‚àí1 , y0 , y1 , y2 , y3 . . . }

(3.2)

together with a corresponding covariance sequence {Œõk }k‚â•0 , each matrix Œõk of the
sequence being computed from the data (3.2) by an ergodic limit of the type (1.11).
In Section 5 we then modify the theory to handle the situation of Ô¨Ånite data (3.1).
For each k ‚àà Z deÔ¨Åne the m √ó ‚àû matrix
y(t) := [yt , yt+1 , yt+2 , . . . ]

(3.3)

and consider the sequence y := {y(t)}t‚ààZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of
all Ô¨Ånite linear combinations

ak ‚àà Rm , tk ‚àà Z
ak y(tk );

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product deÔ¨Åned by linear
extension of the bilinear form
t0 +T
1 

a yt+k yt+j
b = a Œõk‚àíj b,
a y(k), b y(j) := lim
T ‚Üí‚àû T + 1
t=t




(3.4)

0

which clearly does not depend on t0 . This inner product is nondegenerate if the
Toeplitz matrix Tk , constructed from the covariance data {Œõ0 , Œõ1 , . . . , Œõk }, is a positive deÔ¨Ånite symmetric matrix for all k. Here we shall assume that the sequence
{Tk }k‚â•0 is actually coercive, i.e., Tk > cI for some c > 0 and all k ‚â• 0. (See Assumption 3.2 below for an alternative characterization.) We also deÔ¨Åne a shift operator U
on the family of semi-inÔ¨Ånite matrices (3.3), by setting
Ua y(t) = a y(t + 1)

t ‚àà Z,

a ‚àà Rm ,

deÔ¨Åning a linear map which is isometric with respect to the inner product (3.4) and
extendable by linearity to all of Y . In particular the sequence of matrices {y(k)}
corresponding to the time series y is propagated in time by the action of the operator
U, i.e.,
yi (t) = Ut yi (0),

i = 1, 2, . . . , m,

t ‚àà Z,

(3.5)

where yi denotes the i:th row component of y. Then, closing the vector space Y in
the inner product (3.4), we obtain a Hilbert space H(y) := cl Y . The shift operator
U is extended by continuity to all of H(y) and is a unitary operator there.
As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results
in the geometric theory of stochastic realization can be carried over to the present
framework by merely identifying the time series y with a stationary stochastic process
y. In particular, the subspaces H ‚àí and H + of H(y) generated by the elements (3.3)
for t < 0 and t ‚â• 0 respectively can be regarded as the past and future subspaces
of the stationary process y. For reasons of uniformity of notation the inner product
(3.4) will also be denoted
Œæ, Œ∑ = E{ŒæŒ∑},

(3.6)

as the frameworks are completely equivalent. Here we allow E{¬∑} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity
condition introduced above insures that ‚à©t‚ààZ Ut H ‚àí = 0 and ‚à©t‚ààZ Ut H + = 0, i.e., y is
a purely nondeterministic sequence.
As we have pointed out above, the subspace identiÔ¨Åcation methods of Aoki (1990)
and van Overschee and De Moor (1993) are based on the assumption that the available
data is generated by an underlying stochastic system of Ô¨Ånite dimension. More specifically, using the notations introduced above, we assume that the data are generated
by a linear system of the type

x(t + 1) = Ax(t) + Bw(t)
(3.7)
y(t) = Cx(t) + Dw(t)

12

ANDERS LINDQUIST AND GIORGIO PICCI

deÔ¨Åned for all t ‚àà Z, where w is some vector-valued normalized white noise time
series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability
matrix. Throughout this paper we shall 	assume
(without restriction) that (A, B, C)


B
is a minimal triplet and that the matrix
has linearly independent columns.
D
The system is assumed to be in statistical steady state so that the n-dimensional
state x and the m-dimensional output y are uniquely deÔ¨Åned by (3.7) as linear causal
functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and
y(s) will depend only on the diÔ¨Äerence t ‚àí s. We shall think of the system (3.7) as a
representation of the output time series y. The state and input variables x and w are
introduced in order to display the special structure of the dynamic model of y and
are by no means unique. Such a representation is called a state-space realization of y.
Remark 3.1. Despite the fact that the model (3.7) is deÔ¨Åned in terms of sample
sequences, all equalities must be understood in the sense of Hilbert space metric, just
as in the case of models based on random variables.
The number of state variables n is called the dimension of the realization. A
realization is minimal if there is no other realization of y of smaller dimension. In
this case the covariance matrix of the state vector,
(3.8)
P = E{x(t)x(t) }
	 

B
is positive deÔ¨Ånite. Moreover as the matrix
is taken with linearly independent
D
columns, the number of (scalar) white noise inputs p is also as small as possible.
Clearly, the covariance sequence {Œõ0 , Œõ1 , Œõ2 , . . . } of the output {y(t)} of a minimal
model (3.7) is a rational sequence of degree n, i.e., represented as

Œõk = CAk‚àí1 CÃÑ  k = 0, 1, 2, . . . where CÃÑ  = AP C  + BD
.
(3.9)
Œõ0 = CP C  + DD
In the following we shall need to assume that the corresponding spectral density Œ¶(z)
satisÔ¨Åes the following condition.
Assumption 3.2. The spectral density Œ¶ of the output process of the underlying
system (3.7) is coercive in the sense that
Œ¶(eiŒ∏ ) > 0 for all Œ∏ ‚àà [0, 2œÄ].

(3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent
sequences. Recall that a positive real function Z such that Œ¶(z) := Z(z) + Z(z ‚àí1 )
satisÔ¨Åes (3.10) is called strictly positive real.
Let H(w) be the Hilbert space generated by w, i.e. the closure of the linear space
spanned by the family {wi (t), i = 1 . . . p, t ‚àà Z} with respect to the metric induced
by the inner product Œæ, Œ∑ = E{Œæ Œ∑} where E{¬∑} is deÔ¨Åned by (3.6). Let H + and H ‚àí
be the subspaces of H(w) generated by the components of future {y(0), y(1), y(2) . . . }
and past outputs {y(‚àí1), y(‚àí2), y(‚àí3) . . . }, respectively.
4

This means that E{w(t)w(s) } = IŒ¥ts where Œ¥ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace
X := {a x(0) | a ‚àà Rn }

(3.11)

is invariant under coordinate changes of the type (A, B, C) ‚Üí (T AT ‚àí1 , T B, CT ‚àí1 )
and is a coordinate-free representation of the realization (3.7). Such an object is called
a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next deÔ¨Åne the
stationary Hankel operator of y, H : H + ‚Üí H ‚àí as
‚àí

H := E H |H +

(3.12)

‚àí

where E H Œª is the orthogonal projection of Œª onto H ‚àí . The splitting subspace
property of X is equivalent to the commutativity of the diagram
H

H + ‚àí‚Üí H ‚àí
O‚àó 
C
X
i.e. to the factorization
H = CO‚àó ,
+

(3.13)
‚àí

where the operators O := E H |X and C := E H |X are the observability respectively
constructibility operators relative to the splitting subspace X. It can be shown that
the splitting subspace X is minimal if and only if O and C are both injective. (See,
e.g., Lindquist and Picci (1991).)
The system (3.7) is a forward or causal realization of y in the sense that the subspace
+
H (w), generated by the future of w, is orthogonal to X and H ‚àí , i.e. to the present
state and past output. Corresponding to (3.7) there is another realization

xÃÑ(t ‚àí 1) = A xÃÑ(t) + BÃÑ wÃÑ(t ‚àí 1)
(3.14)
y(t ‚àí 1) = CÃÑ xÃÑ(t) + DÃÑwÃÑ(t ‚àí 1)
which is backward or anticausal in the sense that the subspace H ‚àí (wÃÑ), generated by
the past of wÃÑ, is orthogonal to X and H + . Like x(0), xÃÑ(0) is a basis in X, i.e.
X := {a xÃÑ(0) | a ‚àà Rn }.

(3.15)

In fact, xÃÑ(0) is the dual basis of x(0) in the sense that E{x(0)xÃÑ(0) } = I. Hence
PÃÑ = P ‚àí1

xÃÑ(0) = P ‚àí1 x(0).

(3.16)

The particular notations used in (3.7) and (3.14) reÔ¨Çect the special meaning of the
parameters (A, C, CÃÑ). Computing the covariance matrix of the output using the dual
realizations (3.7) and (3.14), it is in fact readily seen that (A, C, CÃÑ) is precisely a
triplet realizing the positive real part (1.6) of the spectral density matrix Œ¶(z) of the
time series y. There are inÔ¨Ånitely many minimal factorizations (3.13), one for each
Markovian splitting subspace, but the basis in each state space X can be chosen so
that the triplets (A, C, CÃÑ) are the same for each minimal X. This is called a uniform
choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward
predictor spaces
‚àí

X‚àí = E H H +

X+ = E H H ‚àí ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12)
and of its adjoint, respectively.
Fixing a uniform choice of bases, and thus the triplets (A, C, CÃÑ), the splitting
subspace X‚àí has the forward stochastic realization

x‚àí (t + 1) = Ax‚àí (t) + B‚àí w‚àí (t)
(3.18)
y(t) = Cx‚àí (t) + D‚àí w‚àí (t)
with state covariance P‚àí , and X+ has the backward realization

xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ wÃÑ+ (t ‚àí 1)
y(t ‚àí 1) = CÃÑ xÃÑ+ (t) + DÃÑ+ wÃÑ+ (t ‚àí 1)

(3.19)

with state covariance PÃÑ+ .
These two stochastic realizations will play an important role in what follows. In
fact, an important interpretation of these realizations is that
‚àí1
[y(t) ‚àí Cx‚àí (t)]
x‚àí (t + 1) = Ax‚àí (t) + B‚àí D‚àí

is the unique steady-state Kalman Ô¨Ålter of any minimal realization (3.7) of y in the
Ô¨Åxed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8)
corresponding to the forward counterpart of (3.19), i.e., P+ = (PÃÑ+ )‚àí1 , then
P ‚àí ‚â§ P ‚â§ P+

(3.20)

for the state covariance of any minimal realization (3.7).
In the same way
‚àí1
[y(t ‚àí 1) ‚àí C xÃÑ+ (t)]
xÃÑ+ (t ‚àí 1) = A xÃÑ+ (t) + BÃÑ+ DÃÑ+

is the backward steady-state Kalman Ô¨Ålter of all minimal backward realizations (3.14),
and
PÃÑ+ ‚â§ PÃÑ ‚â§ PÃÑ‚àí
for an arbitrary backward minimal realization (3.14), where PÃÑ‚àí is the backward counterpart of P‚àí .
4. Canonical correlations and balanced stochastic realization
In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a Ô¨Ånite-dimensional realization
of the type (3.7). Equivalently, we study certain factorizations of the inÔ¨Ånite Hankel matrix of the corresponding inÔ¨Ånite covariance sequence {Œõ0 , Œõ1 , Œõ2 , . . . }. Some
portions of this section can be found in an equivalent but somewhat diÔ¨Äerent setting
in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and
set notations. This will be done in the geometric framework of Section 3, thereby
providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall
introduce orthonormal bases in H ‚àí and H + . To this end it will be useful to represent
past and future outputs as inÔ¨Ånite vectors in the form,
Ô£Æ
Ô£π
Ô£Æ
Ô£π
y(‚àí1)
y(0)
Ô£Øy(‚àí2)Ô£∫
Ô£Øy(1)Ô£∫
Ô£∫
Ô£Ø
Ô£∫
y
=
(4.1)
y‚àí = Ô£Ø
+
Ô£∞y(‚àí3)Ô£ª
Ô£∞y(2)Ô£ª
..
..
.
.
Let L‚àí and L+ be the lower triangular Cholesky factors of the inÔ¨Ånite block Toeplitz
matrices


} = L‚àí L‚àí
T+ := E{y+ y+
} = L+ L+
T‚àí := E{y‚àí y‚àí
and let
ŒΩ := L‚àí1
‚àí y‚àí
be the corresponding orthonormal
implies that
Ô£Æ
Œõ1
Ô£Ø
Œõ2

}=Ô£Ø
H‚àû := E{y+ y‚àí
Ô£∞Œõ3
..
.

ŒΩÃÑ := L‚àí1
+ y+

(4.2)

bases in H ‚àí and H + respectively. Now, (3.9)
Œõ2 Œõ3
Œõ3 Œõ4
Œõ4 Œõ5
..
..
.
.

Ô£π Ô£Æ
Ô£πÔ£Æ
Ô£π
CÃÑ
C
...
. . .Ô£∫ Ô£Ø CA Ô£∫ Ô£Ø CÃÑA Ô£∫
Ô£∫ Ô£Ø
Ô£∫Ô£Ø
Ô£∫
. . .Ô£ª = Ô£∞CA2 Ô£ª Ô£∞CÃÑ(A )2 Ô£ª ,
..
..
...
.
.

(4.3)

and therefore we have the following representation result, which can be found in Desai
et al. (1985).
Proposition 4.1. Let y be realized by a Ô¨Ånite dimensional model of the form (3.7).
Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator
H is

‚àíT
‚àí1
 ‚àíT
HÃÇ‚àû = L‚àí1
+ E{y+ y‚àí }L‚àí = L+ ‚Ñ¶‚Ñ¶ÃÑ L‚àí ,

where

Ô£π
C
Ô£Ø CA Ô£∫
Ô£∫
‚Ñ¶=Ô£Ø
Ô£∞CA2 Ô£ª
..
.

(4.4)

Ô£Æ

Ô£Æ

and

Ô£π
CÃÑ
Ô£Ø CÃÑA Ô£∫
Ô£∫
‚Ñ¶ÃÑ = Ô£Ø
Ô£∞CÃÑ(A )2 Ô£ª .
..
.

(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization
(4.3) for H‚àû , irrespective of which X (i.e. which minimal realization of y) is chosen.
Recall that the adjoint O‚àó of the observability operator O is deÔ¨Åned as the unique
linear operator H + ‚Üí X such that OŒæ, Œª = Œæ, O‚àó Œª for all Œæ ‚àà X and Œª ‚àà H + .
Orthogonality implies that
+

E H Œæ, Œª = Œæ, Œª = Œæ, E X Œª,
and therefore O‚àó = E X |H + . In the same way, we see that C ‚àó = E X |H ‚àí . The Ô¨Åniterank linear operators O‚àó O and C ‚àó C are deÔ¨Åned on X and are the coordinate-free
representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O‚àó O is full rank and constructible if and only if
C ‚àó C is full rank. The following representations show that these gramians are related

16

ANDERS LINDQUIST AND GIORGIO PICCI

to P‚àí and PÃÑ+ , the state covariances of the forward and backward steady-state Kalman
Ô¨Ålters (Picci and Pinzoni, 1994).
Proposition 4.2. Let x(0) and xÃÑ(0) be the conjugate basis vectors in a minimal splitting subspace X as deÔ¨Åned above. Then, in a uniform choice of bases,
O‚àó O a xÃÑ(0) = a PÃÑ+ x(0)

(4.6)

C ‚àó C a x(0) = a P‚àí xÃÑ(0),

(4.7)

and
i.e., C ‚àó C and O‚àó O have matrix representations P‚àí and PÃÑ+ , respectively, independently
of X.
Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal,
‚àí

E H a x(0) = a x‚àí (0),
and therefore

C ‚àó C a x(0) = E X a x‚àí (0) = E X a P‚àí xÃÑ‚àí (0).
But, since the bases xÃÑ(0) and xÃÑ‚àí (0) are chosen uniformly,
E X a xÃÑ‚àí (0) = a xÃÑ(0) a ‚àà Rn ,

and consequently (4.7) follows. The proof of (4.6) is analogous.
The factorization (4.4) can also be derived from (3.13) and the following useful
matrix representations of the observability and constructibility operators.
Proposition 4.3. Let x(0) and xÃÑ(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
O a xÃÑ(0) = a ‚Ñ¶ L‚àíT
+ ŒΩÃÑ

O‚àó b ŒΩÃÑ = b L‚àí1
+ ‚Ñ¶x(0)

(4.8)

C a x(0) = a ‚Ñ¶ÃÑ L‚àíT
‚àí ŒΩ

C ‚àó b ŒΩ = b L‚àí1
‚àí ‚Ñ¶ÃÑxÃÑ(0),

(4.9)

and

where ‚Ñ¶ and ‚Ñ¶ÃÑ are given by (4.5).
Proof. Since, in view of (3.7),
y+ = ‚Ñ¶x(0) + terms which are orthogonal to X,
and ŒΩÃÑ = L‚àí1
+ y+ , we have
E{ŒΩÃÑx(0) } = L‚àí1
+ ‚Ñ¶P.

(4.10)

Consequently, for any a ‚àà Rn , the usual projection formula5 yields
O a x(0) = E H a x(0) = a E{x(0)ŒΩÃÑ  }ŒΩÃÑ
+

and

O‚àó b ŒΩÃÑ = E X b ŒΩÃÑ = b E{ŒΩÃÑx(0) }P ‚àí1 x(0),
from which (4.8) follows. A symmetric argument yields (4.9).
If Œæ ‚àà H(w) and the subspace Z ‚äÇ H(w) is spanned by the components of the full-rank random
vector z, then E Z Œæ = E{Œæz  }(E{zz  })‚àí1 z.
5

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should
note that the matrix representations of O‚àó and C ‚àó are the transposes of those of O
and C if and only if x(0) is an orthogonal basis, i.e., P = PÃÑ = I. Moreover, it follows
from (4.8) that
O‚àó Oa xÃÑ(0) = a ‚Ñ¶ T+‚àí1 ‚Ñ¶x(0),
showing that ‚Ñ¶ T+‚àí1 ‚Ñ¶ is a matrix representation of O‚àó O, in harmony with the analysis
at the end of Section 2. In the same way, (4.9) yields
C ‚àó Ca x(0) = a ‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑxÃÑ(0),
and hence ‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑ is a matrix representation of C ‚àó C. Together with Proposition 4.2
this yields the following explicit formulas for P‚àí and PÃÑ+ :
‚Ñ¶ T+‚àí1 ‚Ñ¶ = PÃÑ+

‚Ñ¶ÃÑ T‚àí‚àí1 ‚Ñ¶ÃÑ = P‚àí .

(4.11)

Now, let {œÉ1 , œÉ2 , œÉ3 , . . . } be the singular values of the Hankel operator H. Since
rank H = n, œÉi = 0 for i > n. The nonzero singular values
1 ‚â• œÉ1 ‚â• œÉ2 ‚â• œÉ3 . . . ‚â• œÉn > 0

(4.12)

are the cosines of the angles between the subspaces H‚àí and H+ ; they are known as the
canonical correlation coeÔ¨Écients of y (Hotelling, 1936, Anderson, 1958). Obviously
œÉ1 < 1 if and and only if H‚àí ‚à© H+ = 0. The squares of the canonical correlation
coeÔ¨Écients are the eigenvalues of H‚àó H, i.e.,
H‚àó H Œæi = œÉi2 Œæi ,
which, in view of (3.13) may be written
O‚àó OC ‚àó C(O‚àó Œæi ) = œÉi2 (O‚àó Œæi ),
and therefore, as was also demonstrated in Picci and Pinzoni (1994),
Œª{O‚àó OC ‚àó C} = {œÉ12 , œÉ22 , . . . , œÉn2 },

(4.13)

i.e., œÉ12 , œÉ22 , . . . , œÉn2 are the eigenvalues of O‚àó OC ‚àó C. But, in view of Proposition 4.2,
this is precisely the coordinate-free version of the invariance condition
{œÉ12 , œÉ22 , . . . , œÉn2 } = Œª{P‚àí PÃÑ+ }

(4.14)

of Desai and Pal (1984).
This suggests that an appropriate uniform choice of bases would be the one that
makes P‚àí and PÃÑ+ equal and equal to the diagonal matrix of nonzero canonical correlation coeÔ¨Écients.
In fact, in view of Proposition 4.1, the inÔ¨Ånite normalized Hankel matrix HÃÇ‚àû is the
matrix representation of the operator H in the orthonormal bases (4.2). Therefore
HÃÇ‚àû has the singular-value decomposition
HÃÇ‚àû = U‚àû Œ£‚àû V‚àû = U Œ£V  ,

(4.15)

where Œ£ is the diagonal n√ón matrix consisting of the canonical correlation coeÔ¨Écients
Œ£ = diag{œÉ1 , œÉ2 , œÉ3 , . . . , œÉn },

(4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and Œ£‚àû is the inÔ¨Ånite matrix
Œ£‚àû

	


Œ£ 0
=
.
0 0

Moreover U‚àû and V‚àû are inÔ¨Ånite orthogonal matrices, and U and V are ‚àû √ó n
submatrices of U‚àû and V‚àû with the the property that
U  U = I = V  V.

(4.17)


We now rotate the the orthonormal bases (4.2) in H + and H ‚àí to obtain u := U‚àû
ŒΩÃÑ


and v := V‚àû ŒΩ respectively. Note that E{uv } = Œ£‚àû . What makes these orthonormal
bases useful is that they are adapted to the orthogonal decompositions6

H ‚àí ‚à® H + = [H ‚àí ‚à© (H + )‚ä• ] ‚äï H  ‚äï [H + ‚à© (H ‚àí )‚ä• ],

(4.18)

where H  := X‚àí ‚à® X+ is the so-called frame space (Lindquist and Picci (1985, 1991),
in the sense that
X‚àí = span{v1 , v2 , . . . , vn }

X+ = span{u1 , u2 , . . . , un }.

This is true since X‚àí is precisely the subspace of random variables in H ‚àí having
nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H ‚àí . Since therefore
{vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H ‚àí ‚à© (H + )‚ä• and H + ‚à© (H ‚àí )‚ä• ,
respectively, these spaces will play no role in what follows.
Now deÔ¨Åne the n-dimensional vectors
Ô£Æ 1/2 Ô£π
Ô£Æ 1/2 Ô£π
œÉ1 u1
œÉ1 v1
Ô£Ø œÉ 1/2 u Ô£∫
Ô£Ø œÉ 1/2 v Ô£∫
2Ô£∫
2Ô£∫
Ô£Ø
Ô£Ø
zÃÑ = Ô£Ø 2 . Ô£∫ = Œ£1/2 U  L‚àí1
(4.19)
z = Ô£Ø 2 . Ô£∫ = Œ£1/2 V  L‚àí1
‚àí y‚àí
+ y+
.
.
Ô£∞ . Ô£ª
Ô£∞ . Ô£ª
1/2

1/2

œÉn vn

œÉn un

From what we have seen before, z is a basis in X‚àí and zÃÑ is a basis in X+ , and they
have the properties
E{zz  } = Œ£ = E{zÃÑ zÃÑ  }.

(4.20)

In fact, we even have more as seen from the following ampliÔ¨Åcation7 of a theorem by
Desai and Pal (1984) (Theorem 1).
Theorem 4.4. The basis vectors
x‚àí (0) = z

xÃÑ+ (0) = zÃÑ

(4.21)

in X‚àí and X+ respectively belong to the same uniform choice of basis, i.e. to the
same choice of triplets (A, C, CÃÑ), and in this uniform choice
P‚àí = Œ£ = PÃÑ+ .

(4.22)

The symbols ‚à® and ‚äï denote vector sum and orthogonal vector sum of subspaces.
A priori there is no reason why choosing bases in X‚àí and X+ would lead to the same (A, C, CÃÑ).
This important property is explicitly mentioned in Theorem 4.4.
6
7

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coeÔ¨Écients {œÉ1 , œÉ2 , œÉ3 , . . . , œÉn } are distinct, this is, modulo
multiplication with a signature matrix 8 , the only uniform choice of bases for which
( 4.22) holds.
Such a choice of (A, C, CÃÑ) is know as stochastically balanced, and, in the case of
distinct canonical correlation coeÔ¨Écients, it deÔ¨Ånes a canonical form with respect to
state space isomorphism in (1.6) by Ô¨Åxing the sign in, say, the Ô¨Årst element in each
row of C. Such canonical forms have also been studied by Ober (1991).
Proof. It follows from (4.4) and (4.15) that
E{zÃÑz  } = Œ£2 .

(4.23)

Now, choose (A, C, CÃÑ) so that xÃÑ+ (0) = zÃÑ, and let the bases in the other splitting
subspaces be chosen accordingly so that the choice of bases is uniform. We want
to show that x‚àí (0) = z. To this end, Ô¨Årst note that x+ (0) = Œ£‚àí1 xÃÑ+ (0) and that
x‚àí (0) = E X‚àí x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula
and the fact that z is a basis in X‚àí ,
x‚àí (0) = Œ£‚àí1 E{zÃÑz  }Œ£‚àí1 z,
which, in view of (4.23), yields x‚àí (0) = z as claimed. Hence (4.22) follows from
(4.20).
Next, suppose that (QAQ‚àí1 , CQ‚àí1 , CÃÑQ ) is another uniform choice of bases which
is also stochastically balanced. Since then x‚àí (0) = Qz and, as is readily seen from
the backward system (3.14), xÃÑ+ (0) = Q‚àíT zÃÑ so that P‚àí = QŒ£Q and PÃÑ+ = Q‚àíT Œ£Q‚àí1 ,
(4.22) yields
QŒ£Q = Œ£ and Q‚àíT Œ£Q‚àí1 = Œ£,
from which we have
QŒ£2 = Œ£2 Q.
Since Œ£ has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959)
that there is a scalar polynomial œï(z) such that Q = œï(Œ£2 ). Hence Q is diagonal and
commutes with Œ£ so that, by QŒ£Q = Œ£, we have
QQ = I.
Consequently, since Q is diagonal, it must be a signature matrix.
In view of (4.21) and (3.16), the Ô¨Årst of relations (4.9) and (4.8) respectively yield
z = ‚Ñ¶ÃÑ T‚àí‚àí1 y‚àí

zÃÑ = ‚Ñ¶ T+‚àí1 y+ .

(4.24)

Consequently, in view of (4.20), (2.15) holds also for the case of an inÔ¨Ånite Hankel
matrix. This can of course also be seen from (4.11).
Note that the normalization of the block Hankel matrix H‚àû is necessary in order for
the singular values to become the canonical correlation coeÔ¨Écients, i.e., the singular
values of H. In fact, if we were to use the unnormalized matrix representation (4.3)
of H instead, as may seem simpler and more natural, the transpose of (4.3) would
not be the matrix representation of H‚àó in the same bases, a property which is crucial
in the singular value decomposition above. This is because (4.3) corresponds to the
bases y‚àí in H ‚àí and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ¬±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the Ô¨Ånite-dimensional case studied in
Section 2, and therefore the normalized Hankel matrix HÃÇ, deÔ¨Åned in Section 2, is
preferable to the unnormalized H.
Formulas, such as (2.5), expressing A, C, CÃÑ in terms of the Hankel matrix H‚àû , can
be easily derived from basic principles. In fact, standard calculations based on the
forward model (3.7) and the backward model (3.14) yield
A = E{x(1)x(0) }P ‚àí1
C = E{y(0)x(0) }P ‚àí1 ,
CÃÑ = E{y(‚àí1)xÃÑ(0) }PÃÑ ‚àí1 = E{y(‚àí1)x(0) }

(4.25a)
(4.25b)
(4.25c)

for any dual pair of bases x(0) and xÃÑ(0).
Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases
(4.19) can be computed by means of the formulas
‚àíT
‚àí1/2
,
A = Œ£‚àí1/2 U  L‚àí1
+ œÉ(H‚àû )L‚àí V Œ£

(4.26a)

‚àí1/2
C = œÅ1 (H‚àû )L‚àíT
,
‚àí VŒ£

(4.26b)


‚àí1/2
œÅ1 (H‚àû
)L‚àíT
,
+ UŒ£

(4.26c)

CÃÑ =

where H‚àû is the unnormalized Hankel matrix (4.3), œÉ(H‚àû ) is obtained from H‚àû by
deleting the Ô¨Årst block row, and œÅ1 (H‚àû ) is the Ô¨Årst block row.
Proof. First, in (4.25a) and (4.25b), we take x(0) to be x‚àí (0). By the Kalman Ô¨Ålter
representation a [x+ (1) ‚àí x‚àí (1)] ‚ä• UH ‚àí ‚äÉ H ‚àí for all a ‚àà Rn ,
E{x‚àí (1)x‚àí (0) } = E{x+ (1)x‚àí (0) } = PÃÑ+‚àí1 E{xÃÑ+ (1)x‚àí (0) }.
But (A, C, CÃÑ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19),
1/2  ‚àí1
U L+ œÉ(y+ ), where œÉ(y+ )
P‚àí = Œ£ = PÃÑ+ , x‚àí (0) = Œ£1/2 V  L‚àí1
‚àí y‚àí and xÃÑ+ (1) = Œ£
is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),

‚àíT
‚àí1/2
,
A = Œ£‚àí1/2 U  L‚àí1
+ E{œÉ(y+ )y‚àí }L‚àí V Œ£

which is identical to (4.26a). Likewise, from (4.26b),
‚àí1/2
,
C = E{y(0)y‚àí }L‚àíT
‚àí VŒ£

which yields (4.26b). Finally, taking xÃÑ(0) to be xÃÑ+ (0) in (4.25c), a symmetric argument yields (4.26c).
Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H‚àû factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from Ô¨Ånite covariance data
In this section we modify the realization theory of Section 4 to the case that only a
Ô¨Ånite segment
{y(0), y(1), y(2), . . . , y(ŒΩ)},

(5.1)

of the time series {y(t)} is available. We still deÔ¨Åne each y(t) as the semi-inÔ¨Ånite
string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact
partial covariance sequence
{Œõ0 , Œõ1 , Œõ2 . . . , ŒõŒΩ }.

(5.2)

The corresponding realization problem, which is purely theoretical and is intended to
prepare for the more realistic identiÔ¨Åcation situation with Ô¨Ånite strings of observed
data (Section 6), is therefore the partial stochastic realization problem mentioned in
Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the
output of some minimal ‚Äútrue‚Äù system (3.7) of dimension n and that ŒΩ is large enough
for n to equal the positive degree of the partial sequence (5.2).
Now, suppose that ŒΩ = 2œÑ ‚àí 1, and partition the data into two matrices
Ô£Æ
Ô£Æ
Ô£π
Ô£π
y(0)
y(œÑ )
Ô£Ø y(1) Ô£∫
Ô£Ø y(œÑ + 1) Ô£∫
+
Ô£∫
Ô£Ø
Ô£∫,
y
=
(5.3)
yœÑ‚àí = Ô£Ø
.
..
œÑ
Ô£∞
Ô£ª
Ô£∞
Ô£ª
..
.
y(œÑ ‚àí 1)

y(2œÑ ‚àí 1)

representing the past and the future respectively, and deÔ¨Åne the corresponding (Ô¨Ånitedimensional) subspaces YœÑ‚àí and YœÑ+ spanned by the rows of yœÑ‚àí and yœÑ+ respectively as
explained in Section 3. Since the data size œÑ will be important in the considerations
that will follow, we denote the Ô¨Ånite block Hankel matrix H of Section 2, relative to
the data (5.3), by HœÑ , i.e.,
HœÑ = E{yœÑ+ (yœÑ‚àí ) }.

(5.4)

Let œÑ0 be the smallest integer œÑ such that rank HœÑ = n. It is well-known that œÑ0 is
the maximum of the observability and constructibility indicies of (A, C, CÃÑ), so n is an
upper bound for œÑ0 . As pointed out in the beginning of Section 2, we need œÑ > œÑ0 to
be certain that the factorization of HœÑ yields a unique (A, C, CÃÑ).
Next we shall consider the class of minimal splitting subspaces for YœÑ‚àí and YœÑ+ , i.e.,
the subspaces XœÑ admitting a canonical factorization
H

œÑ
YœÑ‚àí
YœÑ+ ‚àí‚Üí
OœÑ‚àó 
 CœÑ
XœÑ

of the Ô¨Ånite-interval Hankel operator
‚àí

HœÑ := E YœÑ |YœÑ+ .

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces,
‚àí

XÃÇœÑ ‚àí = E YœÑ YœÑ+

+

and XÃÇœÑ + = E YœÑ YœÑ‚àí ,

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred
to Appendix D.
Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary
time series {y(t)}. Then, if œÑ > œÑ0 ,
XœÑ := UœÑ X

(5.6)

is a minimal splitting subspace for YœÑ‚àí and YœÑ+ , and
‚àí

XÃÇœÑ ‚àí = E YœÑ XœÑ ,

+

XÃÇœÑ + = E YœÑ XœÑ .

(5.7)

Conversely, any basis xÃÇ(œÑ ) in XÃÇœÑ ‚àí has a unique representation9
‚àí

xÃÇ(œÑ ) = E YœÑ x(œÑ ),

(5.8)

ÀÜ (œÑ ) in XÃÇœÑ + has a unique representation
where x(œÑ ) is a basis in XœÑ , and any basis xÃÑ
+

ÀÜ (œÑ ) = E YœÑ xÃÑ(œÑ ),
xÃÑ

(5.9)

X

with xÃÑ(œÑ ) a basis in XœÑ . As X varies over the family
of all minimal Markovian
splitting subspaces, the corresponding x(0) [xÃÑ(0)] constitute a uniform choice of bases.
The stochastic realizations corresponding to the Ô¨Ånite-interval predictor spaces XÃÇœÑ ‚àí
and XÃÇœÑ + are nonstationary. However, taking advantage of the representations (5.8)
and (5.9), we shall be able to express these realizations in such a way that they can
be parameterized by the stationary triplet (A, C, CÃÑ) corresponding to one uniform
choice of bases, both for the forward and the backward settings. In fact, if the bases
ÀÜ (œÑ ) are chosen so that x(œÑ ) and xÃÑ(œÑ ) in representations (5.8) and (5.9) are
xÃÇ(œÑ ) and xÃÑ
dual bases in XœÑ , i.e., E{x(œÑ )xÃÑ(œÑ )} = I, then the same choice of (A, C, CÃÑ) is used for
all X ‚àà . Such a choice of bases in XÃÇœÑ ‚àí and XÃÇœÑ + is called coherent.
The realizations generated by these coherent bases are precisely the (transient)
forward and backward Kalman Ô¨Ålters. In fact, the vector xÃÇ(œÑ ) is the one-step predictor
of x(œÑ ) based on YœÑ‚àí and, as shown in Appendix C, it evolves in time as the Kalman
Ô¨Ålter

X

xÃÇ(t + 1) = AxÃÇ(t) + K(t)[y(t) ‚àí C xÃÇ(t)];

xÃÇ(0) = 0,

(5.10)

where the gain K(t) is given by
K(t) = (CÃÑ  ‚àí AP‚àí (t)C  )(Œõ0 ‚àí CP‚àí (t)C  )‚àí1

(5.11)

and the Ô¨Ålter estimate covariance
P‚àí (t) = E{xÃÇ(t)xÃÇ(t) }

(5.12)

is the solution of the matrix Riccati equation

P‚àí (t + 1) = AP‚àí (t)A + (CÃÑ  ‚àí AP‚àí (t)C  )(Œõ0 ‚àí CP‚àí (t)C  )‚àí1 (CÃÑ  ‚àí AP‚àí (t)C  )
P‚àí (0)) = 0.
(5.13)
9

With slight misuse of notations, the orthogonal projection operator applied to a vector will
denote the vector of the projections of the components.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the
components of
+

ÀÜ (œÑ ) = E YœÑ xÃÑ(œÑ )
xÃÑ

(5.14)

form a basis in XÃÇœÑ + and are generated by the backward Kalman Ô¨Ålter
ÀÜ (t ‚àí 1) = A xÃÑ
ÀÜ (t) + KÃÑ(t)[y(t ‚àí 1) ‚àí CÃÑ xÃÑ
ÀÜ (t)];
xÃÑ

ÀÜ (2œÑ ‚àí 1) = 0,
xÃÑ

(5.15)

with
KÃÑ(t) = (C  ‚àí A PÃÑ+ (t)CÃÑ  )(Œõ0 ‚àí CÃÑP‚àí (t)CÃÑ  )‚àí1 ,

(5.16)

ÀÜ (t)xÃÑ
ÀÜ (t) }
PÃÑ+ (t) = E{xÃÑ

(5.17)

where

is obtained by solving the matrix Riccati equation

PÃÑ+ (t ‚àí 1) = A PÃÑ+ (t)A + (C  ‚àí A PÃÑ+ (t)CÃÑ  )(Œõ0 ‚àí CÃÑ PÃÑ+ (t)CÃÑ  )‚àí1 (C  ‚àí A PÃÑ+ (t)CÃÑ  )
PÃÑ+ (2œÑ ‚àí 1) = 0.
(5.18)
Now, it is well-known that both
ŒΩ(t) = (Œõ0 ‚àí CP‚àí (t)C  )‚àí1/2 [y(t) ‚àí C xÃÇ(t)]

(5.19)

ÀÜ (t)]
ŒΩÃÑ(t) = (Œõ0 ‚àí CÃÑ PÃÑ+ (t)CÃÑ  )‚àí1/2 [y(t ‚àí 1) ‚àí CÃÑ xÃÑ

(5.20)

and

are normalized white noises, called the forward respectively the backward (transient)
innovation processes. Consequently, we may write the Kalman Ô¨Ålter (5.10) as

xÃÇ(t + 1) = AxÃÇ(t) + B‚àí (t)ŒΩ(t)
(5.21)
y(t) = C xÃÇ(t) + D‚àí (t)ŒΩ(t)
where D‚àí (t) := (Œõ0 ‚àí CP‚àí (t)C  )1/2 and B‚àí (t) := K(t)D‚àí (t). Likewise, the backward
Kalman Ô¨Ålter (5.10) may be written

ÀÜ (t) + BÃÑ+ (t)ŒΩÃÑ(t ‚àí 1)
ÀÜ (t ‚àí 1) = A xÃÑ
xÃÑ
(5.22)
ÀÜ (t) + DÃÑ+ (t)ŒΩÃÑ(t ‚àí 1)
y(t ‚àí 1) = CÃÑ xÃÑ
where DÃÑ+ (t) := (Œõ0 ‚àí CÃÑ PÃÑ+ (t)CÃÑ  )1/2 and BÃÑ+ (t) := KÃÑ(t)DÃÑ+ (t). Comparing with (3.7)
and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7)
and (3.14) are time-varying and describe the output y only on the interval [0, 2œÑ ‚àí 1].
In fact, since
P ‚àí P‚àí (t) = E{[x(t) ‚àí xÃÇ(t)][x(t) ‚àí xÃÇ(t)] } ‚â• 0,
and, for the same reason, PÃÑ ‚àí PÃÑ+ (t) ‚â• 0, we have
P‚àí (t) ‚â§ P ‚â§ P+ (t) := PÃÑ+ (t)‚àí1 ,

(5.23)

so we see that the predictor spaces XÃÇœÑ ‚àí and XÃÇœÑ + are extremal splitting subspaces,
just as X‚àí and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the Ô¨Ånite-interval counterparts of equations (4.25)
are given by
A = E{xÃÇ(œÑ + 1)xÃÇ(œÑ ) }P‚àí (œÑ )‚àí1
C = E{y(œÑ )xÃÇ(œÑ ) }P‚àí (œÑ )‚àí1 ,
ÀÜ (œÑ ) }PÃÑ+ (œÑ )‚àí1 = E{y(œÑ ‚àí 1)xÃÇ(œÑ ) }
CÃÑ = E{y(œÑ ‚àí 1)xÃÑ

(5.24a)
(5.24b)
(5.24c)

In complete analogy with the stationary framework in Section 4, the canonical
correlation coeÔ¨Écients
1 ‚â• œÉ1 (œÑ ) ‚â• œÉ2 (œÑ ) ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉn (œÑ ) > 0

(5.25)

between the Ô¨Ånite past YœÑ‚àí and the Ô¨Ånite future YœÑ+ are now deÔ¨Åned as the singular
values of the operator HœÑ given by (5.5). To determine these we need a matrix representation of HœÑ in some orthonormal bases. Using the pair (5.19)‚Äì(5.20) of transient
innovation processes for this purpose, we obtain the normalized matrix (2.14), which
we shall here denote HÃÇœÑ . Singular value decomposition yields
HÃÇœÑ = UœÑ Œ£œÑ VœÑ ,

(5.26)

where UœÑ UœÑ = I = VœÑ VœÑ , and Œ£œÑ is the diagonal matrix of canonical correlation
coeÔ¨Écients. As in Section 4 it is seen that

1/2
‚àí1 ‚àí
z(œÑ ) = Œ£œÑ VœÑ (L‚àí
œÑ ) yœÑ
(5.27)
1/2
‚àí1 +
zÃÑ(œÑ ) = Œ£œÑ UœÑ (L+
œÑ ) yœÑ
are bases in XÃÇœÑ ‚àí and XÃÇœÑ + respectively and that
E{z(œÑ )z(œÑ ) } = Œ£œÑ = E{zÃÑœÑ zÃÑœÑ }.

(5.28)

+
Here L‚àí
œÑ and LœÑ are the Ô¨Ånite-interval counterparts of L‚àí and L+ respectively, and
they are of course submatrices of these. Note that HœÑ , as deÔ¨Åned by (5.4), is now
given by
‚àí 
H œÑ = L+
œÑ HÃÇœÑ (LœÑ ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z(œÑ ) and zÃÑ(œÑ ) are coherent bases, and the
corresponding triplet (A, C, CÃÑ) is a Ô¨Ånite-interval stochastically balanced realization,
i.e.,
P‚àí (œÑ ) = Œ£œÑ = PÃÑ+ (œÑ ).

(5.30)

The following Ô¨Ånite-interval modiÔ¨Åcation of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the
Ô¨Ånite block hankel matrix HœÑ , and the proof is analogous.
Proposition 5.2. The Ô¨Ånite-interval stochastically balanced triplet (AœÑ , CœÑ , CÃÑœÑ ), obÀÜ (œÑ ) = zÃÑ(œÑ ), is given by
tained from (5.24) by choosing the bases xÃÇ(œÑ ) = z(œÑ ) and xÃÑ
‚àí1
‚àí ‚àíT
UœÑ (L+
VœÑ Œ£‚àí1/2
,
AœÑ = Œ£‚àí1/2
œÑ
œÑ ) œÉ(HœÑ )(LœÑ )
œÑ
‚àíT
VœÑ Œ£‚àí1/2
,
CœÑ = œÅ1 (HœÑ )(L‚àí
œÑ )
œÑ

+ ‚àíT
‚àí1/2
CÃÑœÑ = œÅ1 (HœÑ )(LœÑ ) UœÑ Œ£œÑ ,

(5.31a)
(5.31b)
(5.31c)

where the operators œÉ(¬∑) and œÅ1 (¬∑) are deÔ¨Åned as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

Note that the triplet (AœÑ , CœÑ , CÃÑœÑ ) actually varies with œÑ , but that, for each œÑ , it
is similar to the stochastically balanced triplet (A, C, CÃÑ) of Section 4, i.e., there is a
nonsingular matrix QœÑ so that
‚àí1

(AœÑ , CœÑ , CÃÑœÑ ) = (QœÑ AQ‚àí1
œÑ , CQœÑ , CÃÑQœÑ ).

(5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the
stationary predictor spaces X‚àí and X+ will have the state covariances
P‚àí = QœÑ Œ£QœÑ

‚àí1
and PÃÑ+ = Q‚àíT
œÑ Œ£QœÑ ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state
covariances are not diagonal and equal is a manifestation of the fact that the triplet
(AœÑ , CœÑ , CÃÑœÑ ) is not stochastically balanced in the sense of Section 4. It is well known
that P‚àí (t) and PÃÑ+ (t) tend monotonically to P‚àí and PÃÑ+ , respectively, as t ‚Üí ‚àû, and
therefore we have the following ordering
P‚àí (œÑ ) := Œ£œÑ ‚â§ P‚àí ‚â§ (PÃÑ+ )‚àí1 ‚â§ (PÃÑ+ (œÑ ))‚àí1 := Œ£‚àí1
œÑ .
Since the number n of nonzero singular values (5.25) is in general too large too
yield a reasonable model, we must consider what happens when some of the smallest
singular values are set equal to zero. The truncation procedure employed by van
Overschee and De Moor (1993) is equivalent to the principal subsystem truncation
presented in Section 2, except that, and this is very important, the singular-value
decomposition is performed on the normalized block Hankel matrix HÃÇœÑ , which is the
natural matrix representation of the operator œÑ . It will be shown in Section 7 that
such a truncation will preserve positivity in the stationary case (Theorem 7.3). In
order to carry this result over to the case of Ô¨Ånite œÑ , we need to assume that the
spectral density Œ¶ of the time series {y(t)} is coercive so that Assumption 3.2 is
fulÔ¨Ålled, i.e., that the function Z is strictly positive real.
The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D,
shows that principal subsystem truncation preserves positivity provided œÑ is chosen
large enough.

H

Theorem 5.3. Suppose that the spectral density Œ¶ of the time series {y(t)} is coercive. Then, there is an integer œÑ1 > œÑ0 such that, for œÑ ‚â• œÑ1 , the principal subsystem
truncation ((AœÑ )11 , (CœÑ )1 , (CÃÑœÑ )1 ) of (AœÑ , CœÑ , CÃÑœÑ ) is a minimal realization of a strictly
positive real function (2.13).
6. Subspace identiÔ¨Åcation
The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have
access to an inÔ¨Ånite sequence (3.2) of data. In reality we will have a Ô¨Ånite string of
observed data
{y0 , y1 , y2 , . . . , yN },

(6.1)

where, however, N may be quite large. More speciÔ¨Åcally, we assume that N is sufÔ¨Åciently large that replacing the ergodic limits (1.11) by truncated sums yields good
approximations of
{Œõ0 , Œõ1 , Œõ2 . . . , ŒõŒΩ },

(6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course, ŒΩ << N . This is equivalent to saying that T := N ‚àí ŒΩ is suÔ¨Éciently
large for
1  

a yt+k yt+j
b
T + 1 t=0
T

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we
shall use the Ô¨Ånite-interval realization theory of Section 5 as if we had a Ô¨Ånite time
series
{y(0), y(1), y(2), . . . , y(ŒΩ)},

(6.4)

while substituting the semi-inÔ¨Ånite string (3.3) of data by
y(t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . , ŒΩ.

(6.5)

In particular, in this case the inner product becomes merely that of a Ô¨Ånite-dimensional
Euclidean space so that the block Hankel matrix HœÑ can be written
HœÑ =
where

Ô£Æ
Ô£π
yœÑ ‚àí1 yœÑ . . . yT +œÑ ‚àí1
Ô£ØyœÑ ‚àí2 yœÑ ‚àí1 . . . yT +œÑ ‚àí2 Ô£∫
yœÑ‚àí = Ô£Ø
..
.. Ô£∫
..
Ô£∞ ...
.
.
. Ô£ª
y0
y1 . . .
yT

1
y + (y ‚àí )
T +1 œÑ œÑ
Ô£Æ

Ô£π
yœÑ +1 . . .
yT +œÑ
yœÑ
Ô£Ø yœÑ +1 yœÑ +2 . . . yT +œÑ +1 Ô£∫
and yœÑ+ = Ô£Ø
.
..
.. Ô£∫
..
Ô£∞ ...
.
.
. Ô£ª
y2œÑ ‚àí1 y2œÑ . . . yT +2œÑ ‚àí1

Consequently, the identiÔ¨Åcation of a minimal stationary state-space innovation
model describing the data (6.1) can be performed in the following steps.
(1) Perform canonical correlation analysis on the data yœÑ‚àí , yœÑ+ to obtain, from
ÀÜ + (œÑ ) = zÃÑ(œÑ ) and, from (5.26), the
(5.27), the state vectors xÃÇ‚àí (œÑ ) = z(œÑ ) and xÃÑ
corresponding common state covariance matrix Œ£œÑ , i.e., the diagonal matrix of
the (Ô¨Ånite interval) canonical correlation coeÔ¨Écients (5.25).
(2) Given the singular value decomposition (5.26), compute via (5.31) a minimal
realization (A, C, CÃÑ). This realization will be in Ô¨Ånite-interval balanced form,
i.e., (5.30) will hold instead of (4.22).
(3) To obtain a state space model (3.7) for y we need to compute the matrices B
and D. Note that such matrices will exist if and only if (A, C, CÃÑ, Œõ0 ) deÔ¨Ånes
a positive real function (1.6), or, in other words, if and only if there is a
symmetric positive deÔ¨Ånite P = P  such that


	
P ‚àí AP A CÃÑ  ‚àí AP C 
‚â• 0.
(6.6)
M (P ) :=
CÃÑ ‚àí CP A Œõ0 ‚àí CP C 
[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6),
B and D can be determined (in a nonunique way) by a full rank factorization
of M (P ), i.e.,
	 


B  
B D = M (P ).
(6.7)
D

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P‚àí = E{x‚àí (t)x‚àí (t) } has been
determined. Obtaining P‚àí amounts to Ô¨Ånding the minimal solution of the
algebraic Riccati equation
P = AP A + (CÃÑ  ‚àí AP C  )(Œõ0 ‚àí CP C  )‚àí1 (CÃÑ  ‚àí AP C  )

(6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t ‚Üí ‚àû
with initial condition P‚àí (œÑ ) = Œ£œÑ . (The corresponding dual procedures yield
PÃÑ+ .) Again, in both cases, a positive deÔ¨Ånite P‚àí can be found if and only
if (A, C, CÃÑ, Œõ0 ) deÔ¨Ånes a positive real function (1.6). In fact, in general,
{P‚àí (t)}t‚â•0 may not even converge unless this positivity condition is fulÔ¨Ålled
and may in fact exhibit dynamical behavior with several of the characteristics
of chaotic dynamics (Byrnes et al., 1991, 1994).
Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that,
for œÑ Ô¨Åxed but suÔ¨Éciently large (see Section 2), we will have rank HœÑ = n as T ‚Üí ‚àû,
and the triplet (A, C, CÃÑ) will be uniquely determined from the data and similar to the
triplet (A, C, CÃÑ) of the ‚Äútrue‚Äù generating system. Hence, in particular, in the limit as
T ‚Üí ‚àû, at least in theory positivity will be guaranteed. If nÃÇ is an upper bound for
the order of the ‚Äútrue‚Äù system, we may choose œÑ to be any integer larger than nÃÇ.
In practice, however, T is Ô¨Ånite, and even if we had a true system generating exact
data, the spectral estimate Œ¶T , although converging to the true spectrum Œ¶ as T ‚Üí ‚àû
may in principle fail to be positive for any Ô¨Ånite T if there are frequencies œâ for which
Œ¶(eiœâ ) = 0. Positivity for a suitably large T can however be guaranteed if the ‚Äútrue‚Äù
spectrum is coercive. The following proposition, which also applies to Aoki‚Äôs method
discussed in Section 2, is proved in Appendix D.
Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulÔ¨Ålled. Then, there is a T0 ‚àà Z+ such that, for T ‚â• T0 , the triplet (A, C, CÃÑ) deÔ¨Åned by
(5.31) yields a function (1.6) which is strictly positive real.
However, in practice, rank HœÑ normally will keep increasing with œÑ , even for very
large T , so that one must resort to some kind of truncation of the Hankel singular
values. As we have pointed out in Section 5, setting all canonical correlation coeÔ¨Écients œÉr+1 (œÑ ), œÉr+2 (œÑ ), . . . equal to zero for some suitable r, as is done in, for example,
van Overschee and De Moor (1993), is equivalent to principal subsystem truncation.
An important issue is therefore under what conditions such a procedure will insure
positivity. Here we must distinguish between problems generated by the sample Ô¨Çuctuations of the data due to Ô¨Ånite sample size T , as considered in Proposition 6.1, and
the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an inÔ¨Ånite string of data generated by a ‚Äútrue‚Äù
high-dimensional system, such a truncation procedure may fail if œÑ is smaller than
that dimension.
Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following
result, which justiÔ¨Åes this approximation procedure, provided the rather stringent
Assumption 2.1 holds and we have coercivity, and provided T and œÑ are suÔ¨Éciently
large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulÔ¨Ålled.
Then, there are positive integers T0 and œÑ1 > œÑ0 such that, for T ‚â• T0 and œÑ ‚â• œÑ1 , the
triplet (A11 , C1 , CÃÑ1 ), obtained from (2.12) by taking H := HœÑ in (2.10), is a minimal
realization of a strictly positive real function (2.13).
We note that, in van Overschee and De Moor (1993), the large Hankel matrix
HÃÉœÑ = (yœÑ+ ) (E{yœÑ+ (yœÑ+ ) })‚àí1 E{yœÑ+ (yœÑ‚àí ) }(E{yœÑ‚àí (yœÑ‚àí ) })‚àí1 yœÑ‚àí
is used in place of HÃÇœÑ . This leads to a procedure which is equivalent to the one
described above. Moreover, the computation of a second singular-value decomposition
in van Overschee and De Moor (1993), based on HœÑ +1 := E{yœÑ++1 (yœÑ‚àí+1 ) }, together
with a subsequent change of bases, is actually redundant, as can be deduced from
the following proposition. In fact, a considerable amount of computation is needed in
van Overschee and De Moor (1993) to compensate for the fact that taking z(œÑ + 1),
computed from a second singular-value decomposition, as a basis in XÃÇ(œÑ +1)‚àí would
lead to a Kalman Ô¨Ålter model with time-varying parameters.
ÀÜ (œÑ ) in the Ô¨Ånite-interval
Proposition 6.3. To each coherent pair of bases xÃÇ(œÑ ) and xÃÑ
predictor spaces XÃÇœÑ ‚àí and XÃÇœÑ + , there corresponds a minimal factorization
HœÑ = ‚Ñ¶œÑ ‚Ñ¶ÃÑœÑ

(6.9)

of the block Hankel matrix HœÑ . Here
‚àí

‚Ñ¶œÑ xÃÇ(œÑ ) = E YœÑ yœÑ+

and

+

ÀÜ (œÑ ) = E YœÑ yœÑ‚àí .
‚Ñ¶ÃÑœÑ xÃÑ

(6.10)

Conversely, given a minimal factorization (6.9),
xÃÇ(œÑ ) = ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí

and

ÀÜ (œÑ ) = ‚Ñ¶œÑ (TœÑ+ )‚àí1 yœÑ+
xÃÑ

(6.11)

is a coherent pair of bases in XÃÇœÑ ‚àí and XÃÇœÑ + .
ÀÜ (œÑ ) be a coherent choice of bases in XÃÇœÑ ‚àí and XÃÇœÑ + . Then, for
Proof. Let xÃÇ(œÑ ) and xÃÑ
any XœÑ as deÔ¨Åned in Theorem 5.1, there is a unique pair (x(œÑ ), xÃÑ(œÑ )) of dual bases
such that (5.8) and (5.9) hold. Let ‚Ñ¶œÑ and ‚Ñ¶ÃÑœÑ be the matrices deÔ¨Åned via
E XœÑ yœÑ+ = ‚Ñ¶œÑ x(œÑ ) and E XœÑ yœÑ‚àí = ‚Ñ¶ÃÑœÑ xÃÑ(œÑ ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of XœÑ with respect to
YœÑ‚àí and YœÑ+ yields
E{yœÑ+ (yœÑ‚àí ) } = E{E XœÑ yœÑ+ (E XœÑ yœÑ‚àí )) },
‚àí

+

which, in view of (6.12), is the same as (6.9). Applying E YœÑ and E YœÑ to respectively
the Ô¨Årst and second equations of (6.12), the splitting property yields (6.10).
As for the converse statement, equations (6.11) follow from the construction in the
proof of Theorem 5.1, from which it also follows that the resulting bases xÃÇ(œÑ ) and
ÀÜ (œÑ ) are constructed from the same (A, C, CÃÑ) and therefore coherent.
xÃÑ
As soon as the parameters (A, C, CÃÑ) have been Ô¨Åxed by a particular choice of x(œÑ )
in the representation (5.8) in Theorem 5.1, we must choose xÃÇ(œÑ + 1) as
xÃÇ(œÑ + 1) = E YœÑ +1 Ux(œÑ )

(6.13)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More speciÔ¨Åcally Proposition 6.3
implies that ‚Ñ¶œÑ and ‚Ñ¶ÃÑœÑ are uniquely determined once x(œÑ ) has been selected. Hence
(A, C, CÃÑ) is uniquely determined by the Ho-Kalman algorithm so that
	


CÃÑ
‚Ñ¶ÃÑœÑ +1 =
‚Ñ¶ÃÑœÑ A
is prescribed, as is
xÃÇ(œÑ + 1) = ‚Ñ¶ÃÑœÑ (TœÑ‚àí+1 )‚àí1 yœÑ‚àí+1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining
xÃÇ(œÑ + 1) by an extra singular-value decomposition, as in van Overschee and De Moor
(1993), is actually redundant. If we actually were to determine xÃÇ(œÑ + 1) as described
above, we would better compute ‚Ñ¶ÃÑœÑ +1 from ‚Ñ¶ÃÑœÑ +1 = ‚Ñ¶‚àíL
œÑ HœÑ +1 , where the left inverse
is very easily obtained from the singular-value decomposition of HœÑ .
We stress that Assumption 2.1, although quite limiting, is absolutely crucial in
insuring that the subspace identiÔ¨Åcation algorithms mentioned above will actually
work. Note that for generic data these algorithms may break down for any Ô¨Åxed œÑ .
The same is true for all other subspace methods which deal with identiÔ¨Åcation of
covariance models (or equivalent) involving stochastic signals.
On the other hand, Assumption 2.1 introduces a quite unrealistic condition which,
as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure
to estimate T0 and œÑ1 in Proposition 6.2, as the proof is based only on continuity
arguments.
7. Stochastic model reduction
As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step
in order to keep the dimension of the model at a reasonable level. To justify any
such procedure one must either assume that there is an underlying ‚Äútrue‚Äù system of
suÔ¨Éciently low order, i.e., invoke Assumption 2.1, or to perform rational covariance
extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995),
Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an inÔ¨Ånite
one. The latter can be done in many ways, one of which is the maximum entropy
extension.
In either case, the truncation problem is equivalent to approximating a positive
real matrix function
1
Z(z) = C(zI ‚àí A)‚àí1 CÃÑ  + Œõ0 ,
2

(7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of
lower degree. In this section we shall investigate how this can be done and also how
such an approximation aÔ¨Äects the canonical correlation structure.
One main question to be addressed is whether the principal subsystem truncation
(2.11) preserves positive realness and balancing, and hence the leading canonical
correlation coeÔ¨Écients, as originally claimed by Desai and Pal (1982). As it turns
out, the answer is aÔ¨Érmative to the Ô¨Årst but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identiÔ¨Åcation approximation obtained
by setting some canonical correlation coeÔ¨Écients equal to zero.
It is instructive to Ô¨Årst consider the continuous-time counterpart of this problem
since the latter is simpler and exhibits more desirable properties. Also, it has been
widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true.
It is well-known [see, e.g., Faurre et al. (1979)] that an m √ó m matrix function Z
with minimal realization
1
(7.2)
Z(s) = C(sI ‚àí A)‚àí1 CÃÑ  + R,
2
is positive real with respect to the right half plane if and only if there is a symmetric
matrix P > 0 such that


	
‚àíAP ‚àí P A CÃÑ  ‚àí P C 
‚â• 0,
(7.3)
M (P ) :=
CÃÑ ‚àí CP
R
where here we assume that R is positive deÔ¨Ånite and symmetric. In this case there
are two solutions of (7.3), P‚àí and P+ , with the property that any other solution of
(7.3) satisÔ¨Åes
P ‚àí ‚â§ P ‚â§ P+ .

(7.4)

These extremal solutions play the same role as P‚àí and P+ in the discrete-time setting,
and
rank M (P‚àí ) = m = rank M (P+ ).

(7.5)

If the state-space coordinates are chosen so that both P‚àí and PÃÑ+ := P+‚àí1 are diagonal
and equal, and thus, by (4.14), equal to the diagonal matrix Œ£ of canonical correlation
coeÔ¨Écients, we say that (A, C, CÃÑ) is stochastically balanced.
Now, suppose that Œ£ is partitioned as in (2.8) with œÉr+1 < œÉr , and consider the
corresponding principal subsystem truncation (2.12). Using the stochastic realization
framework, Harshavaradana, Jonckheere and Silverman (1984) showed that
1
Z1 (s) = C1 (sI ‚àí A11 )‚àí1 CÃÑ1 + R,
2

(7.6)

is a minimal realization of a positive real function and conjectured that (A11 , C1 , CÃÑ1 )
is stochastically balanced. We shall next show that this conjecture is true, as has
already been done by Ober (1991) in a framework of canonical forms.
First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield
Ô£π
Ô£Æ
‚àíA11 Œ£1 ‚àí Œ£1 A11 ‚àó CÃÑ1 ‚àí Œ£1 C1
Ô£ª ‚â• 0,
Ô£∞
‚àó
‚àó
‚àó
(7.7)
‚àó
R
CÃÑ1 ‚àí C1 Œ£1
where blocks which play no role in the analysis are marked by an asterisk. Consequently,


	
‚àíA11 Œ£1 ‚àí Œ£1 A11 CÃÑ1 ‚àí Œ£1 C1
‚â• 0.
(7.8)
M1 (Œ£1 ) =
CÃÑ1 ‚àí C1 Œ£1
R

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982),
Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane,
(7.6) is positive real, but it remains to prove that (A11 , C1 , CÃÑ1 ) is a minimal realization.
This was done in Harshavaradhana et al. (1984). It is important to observe here that,
contrary to the situation in the discrete-time setting, rank M1 (Œ£1 ) = rank M (Œ£) = m
‚àí1
and rank M1 (Œ£‚àí1
1 ) = rank M (Œ£ ) = m, important facts that will be seen to imply
that the reduced system is stochastically balanced.
Recall that in the continuous-time setting the spectral density Œ¶(s) = Z(s)+Z(‚àís)
is coercive if, for some < > 0, we have Œ¶(s) ‚â• <I for all s on the imaginary axis. This
is equivalent to the condition that R > 0 and Œ¶ has no zeros on the imaginary axis
(Faurre et al., 1979, Theorem 4.17).
Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with Œ¶(s) :=
Z(s) + Z(‚àís) coercive, and let (A, C, CÃÑ) be in stochastically balanced form. Then,
if œÉr+1 < œÉr , the reduced system (A11 , C1 , CÃÑ1 ) deÔ¨Ånes a positive real function (7.6)
for which it is a minimal realization in stochastically balanced form, and Œ¶1 (s) :=
Z1 (s) + Z1 (‚àís) is coercive.
Proof. We have already shown that Z1 is positive real, and we refer the reader to
Harshavaradhana et al. (1984) for the proof that (A11 , C1 , CÃÑ1 ) is a minimal realization
of Z1 . It remains to show that Œ¶1 is coercive and that (A11 , C1 , CÃÑ1 ) is stochastically
‚àí1
, where P1‚àí and P1+ are solutions to the algebraic
balanced, i.e., that P1‚àí = Œ£1 = P1+
Riccati equation
A11 P1 + P1 A11 + (CÃÑ  ‚àí P1 C1 )R‚àí1 (CÃÑ  ‚àí P1 C1 ) = 0

(7.9)

such that any other solution P1 of (7.9) satisÔ¨Åes P1‚àí ‚â§ P1 ‚â§ P1+ . To this end,
‚àí1
note that since M1 (Œ£1 ) and M1 (Œ£‚àí1
1 ) have rank m, both Œ£1 and Œ£1 satisfy (7.9).
Therefore, as is well-known (Molinari, 1977) and easy to show, Q := Œ£‚àí1
1 ‚àíŒ£1 satisÔ¨Åes
Œì1 Q + QŒì1 + QC1 R‚àí1 C1 Q = 0,

(7.10)

Œì1 = A11 ‚àí (CÃÑ  ‚àí Œ£1 C1 )R‚àí1 C1 .

(7.11)

where
Since Œ¶ is coercive, Œ£‚àí1 ‚àí Œ£ = P+ ‚àí P‚àí > 0 (Faurre et al., 1979, Theorem 4.17) so
that œÉ1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to
Œì1 Q‚àí1 + Q‚àí1 Œì1 + C1 R‚àí1 C1 = 0.

(7.12)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , Œì1 ). Since,
in addition, the Lyapunov equation (7.12) has a positive deÔ¨Ånite solution Q‚àí1 , Œì1
must be a stability matrix. Therefore Œ£1 is the minimal (stabilizing) solution P1‚àí of
‚àí1
= Œ£1 .
(7.9). In the same way, using the backward setting, we show that PÃÑ1+ := P1+
Consequently, (A11 , C1 , CÃÑ1 ) is stochastically balanced. Since P1+ ‚àí P1‚àí > 0, Œ¶1 is
coercive.
Let us now return to the discrete-time setting. Let us recall that, if (A, C, CÃÑ, 12 Œõ0 )
is a minimal realization of (7.1), the matrix function Z is positive real if and only if
the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given
the positive real rational function (7.1) with the property that Œ¶(z) = Z(z) + Z(z ‚àí1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y, the state covariance P of any minimal
stochastic realization (3.7) of y satisÔ¨Åes (6.6) and the matrices B, D in (3.7) satisfy
(6.7). Consequently, as pointed out in Section 5, the matrices B and D can be
determined via a matrix factorization of M (P ) once P has been determined.
Now, if (A, C, CÃÑ) is in stochastically balanced form, Theorem 4.4 implies that
M (Œ£) ‚â• 0. In view of (4.16) and (2.12), M (Œ£) may be written
Ô£π
Ô£Æ
Œ£1 ‚àí A11 Œ£1 A11 ‚àí A12 Œ£2 A12 ‚àó CÃÑ1 ‚àí A11 Œ£1 C1 ‚àí A12 Œ£2 C2
Ô£ª,
Ô£∞
‚àó
‚àó
‚àó




CÃÑ1 ‚àí C1 Œ£1 A11 ‚àí C2 Œ£2 A12 ‚àó Œõ0 ‚àí C1 Œ£1 C1 ‚àí C2 Œ£2 C2
where, as before, the blocks which do not enter the analysis are marked with an
asterisk. Since M (Œ£) ‚â• 0, this implies that
	 
 	 

A
A
(7.13)
M1 (Œ£1 ) ‚àí 12 Œ£2 12 ‚â• 0,
C2
C2
where

	


Œ£1 ‚àí A11 Œ£1 A11 CÃÑ1 ‚àí A11 Œ£1 C1
M1 (Œ£1 ) =
CÃÑ1 ‚àí C1 Œ£1 A11 Œõ0 ‚àí C1 Œ£1 C1

(7.14)

is the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , CÃÑ1 ). Therefore, M (Œ£1 ) ‚â• 0, so if we can show that A11 is stable, i.e., has all its eigenvalues
strictly inside the unit circle, it follows that
1
(7.15)
Z1 (z) = C1 (zI ‚àí A11 )‚àí1 CÃÑ1 + Œõ0 ,
2
is positive real. As we shall see below this is true without the requirement needed in
continuous time that œÉr+1 < œÉr .
For (A11 , C1 , CÃÑ1 ) also to be balanced, Œ£1 would have to be the minimal solution P1‚àí
of M1 (P1 ) ‚â• 0, which in turn would require that rank M1 (Œ£1 ) = rank M (Œ£) = m.
Due to the extra positive semideÔ¨Ånite term in (7.13), however, this will in general not
be the case and therefore Œ£1 ‚â• P1‚àí will correspond to an external realization, as will
Œ£‚àí1
1 ‚â§ P1+ ; see Lindquist and Picci (1991).
To show that (A11 , C1 , CÃÑ1 ) is minimal we need to assume that Œ¶ is coercive, or,
equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979,
Theorem A4.4) that this implies that
P+ ‚àí P‚àí > 0.

(7.16)

In fact, if Œõ0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to
coercivity. Coercivity also implies that
Œõ0 ‚àí CP‚àí C  > 0.

(7.17)

Remark 7.2. With (A, C, CÃÑ) in balanced form, P‚àí = Œ£ = PÃÑ+ and, in view of (3.16),
P+ = Œ£‚àí1 . Hence (7.16) becomes Œ£‚àí1 > Œ£, which obviously holds if and only if
œÉ1 < 1, which in turn is equivalent to H ‚àí ‚à© H + = 0. Consequently, given the full
rank condition Œõ0 > 0, coercivity is equivalent to the past and the future spaces of y
having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

Theorem 7.3. Let (7.1) be positive real, and let (A, C, CÃÑ) be in stochastically balanced form. Then the reduced-degree function (7.15) obtained via principal subsystem
decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so
is Z1 , and (A11 , C1 , CÃÑ1 , 12 Œõ0 ) is a minimal realization of Z1 .
For the proof we need the following lemma, the proof of which is given in Appendix
D.
Lemma 7.4. Let the matrix function Z be given by (7.1), where Œõ0 > 0, but where
(C, A) and (CÃÑ, A ) are not necessarily observable, and suppose that (6.6) has two
positive deÔ¨Ånite symmetric solutions, P1 and P2 , such that
P2 ‚àí P1 > 0.

(7.18)

Then Z is strictly positive real.
Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is
stable. To this end, we note that P is the reachability gramian of (3.7). In particular,
if (A, C, CÃÑ) is stochastically balanced, the reachability gramian of the system (3.18)
equals Œ£ so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable.
By Remark 7.2, coercivity of Œ¶ implies that Œ£‚àí1 ‚àí Œ£ > 0, from which it follows
that Œ£‚àí1
1 ‚àí Œ£1 > 0 and that Œõ0 > 0. Moreover, By construction, M1 (Œ£1 ) ‚â• 0 and
‚àí1
M1 (Œ£1 ) ‚â• 0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is.
To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows
by symmetry. By regularity condition (7.17),
Œõ0 ‚àí C1 Œ£1 C1 ‚â• Œõ0 ‚àí CŒ£C  > 0,
and consequently, since M1 (Œ£1 ) ‚â• 0, Œ£1 satisÔ¨Åes the algebraic Riccati inequality
A11 P1 A11 ‚àí P1 + (CÃÑ1 ‚àí A11 P1 C1 )(Œõ0 ‚àí C1 P1 C1 )‚àí1 (CÃÑ1 ‚àí A11 P1 C1 ) ‚â• 0, (7.19)
but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable.
Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density Œ¶1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a
unique symmetric P1‚àí > 0 which satisÔ¨Åes (7.19) with equality and for which
Œì1‚àí := A11 ‚àí (CÃÑ1 ‚àí A11 P1‚àí C1 )(Œõ0 ‚àí C1 P1‚àí C1 )‚àí1 C1
is stable. It is well-known (Faurre et al., 1979) that P1‚àí is the minimal symmetric
solution of the linear matrix inequality M1 (P1 ) ‚â• 0, i.e., that any other symmetric
‚àí1
solution P1 satisÔ¨Åes P1 ‚â• P1‚àí . We also know that M1 (Œ£‚àí1
1 ) ‚â• 0. Next, since Œ£1 ‚àí
Œ£1 > 0, a fortiori it holds that Q := Œ£‚àí1
1 ‚àí P1‚àí > 0. A tedious but straight-forward
calculation shows that Q satisÔ¨Åes
Œì1‚àí (Q‚àí1 ‚àí C1 R‚àí1 C1 )‚àí1 Œì1‚àí ‚àí Q ‚â• 0,
from which it follows that
Q‚àí1 ‚àí C1 R‚àí1 C1 ‚àí Œì1‚àí Q‚àí1 Œì1‚àí ‚â§ 0.
Cf. Faurre et al. (1979), pp. 85 and 95.

(7.20)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a ‚àà
and a Œª ‚àà C such that [C1 , ŒªI ‚àí A11 ]a = 0. and therefore, in view of (7.20),

Cr

(1 ‚àí |Œª|2 )a‚àó Q‚àí1 a ‚â§ 0.
But Œª is an eigenvalue of the stable matrix A11 , implying that |Œª| < 1, so we must
have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable.
A remaining question is whether there is some balanced order-reduction procedure
in discrete time which preserves both positivity and balancing. That this is the case
in continuous time implies that the answer is aÔ¨Érmative, but the reduced system
cannot be a simple principal subsystem truncation.
Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, CÃÑ) be in stochastically
balanced form. Moreover, given a decomposition ( 2.12) such that œÉr+1 < œÉr , let
Ar
Cr
CÃÑr
Œõr0

=
=
=
=

A11 ‚àí A12 (I + A22 )‚àí1 A21
C1 ‚àí C2 (I + A22 )‚àí1 A21
CÃÑ1 ‚àí CÃÑ2 (I + A22 )‚àí1 A12
Œõ0 ‚àí C2 (I + A22 )‚àí1 CÃÑ2 ‚àí CÃÑ2 (I + A22 )‚àí1 C2

Then (Ar , Cr , CÃÑr , Œõr0 ) is a minimal realization of a strictly positive real function
1
Zr (z) = Cr (zI ‚àí Ar )‚àí1 CÃÑr + Œõr0 .
2

(7.21)

Moreover, (Ar , Cr , CÃÑr , Œõr0 ) is stochastically balanced with canonical correlation coeÔ¨Écients œÉ1 , œÉ2 , . . . , œÉr .
To understand why this reduced-order system does preserve both positivity and
balancing, note that for
Ô£π
Ô£Æ
I ‚àíA12 (I + A22 )‚àí1 0
I
0Ô£ª
T = Ô£∞0
‚àí1
I
0 ‚àíC2 (I + A22 )
we obtain

Ô£Æ
Ô£π
Œ£1 ‚àí Ar Œ£1 Ar ‚àó CÃÑr ‚àí Ar Œ£1 Cr
Ô£ª,
‚àó
‚àó
‚àó
T M (Œ£)T  = Ô£∞


CÃÑr ‚àí Cr Œ£1 Ar ‚àó Œõr0 ‚àí Cr Œ£1 Cr

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the
reduced-order system, Mr (œÉ1 ) ‚â• 0 and rank Mr (Œ£1 ) ‚â§ rank M (Œ£).
To prove Theorem 7.5 we observe that (Ar , Cr , CÃÑr , Œõr0 ) is precisely what one obtains
if one transforms (A, C, CÃÑ, Œõ0 ) by the appropriate linear fractional transform to the
continuous-time setting and then, after reduction, back to discrete time again as
suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions
The purpose of this paper is to analyze a class of popular subspace identiÔ¨Åcation
procedures for state space models in the theoretical framework of rational covariance
extension, balanced model reduction, and geometric theory for splitting subspaces.
We have pointed out that these methods are based on the hidden Assumption 2.1
which is not entirely natural and which is in general untestable.
The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially
equivalent to the Ho-Kalman algorithm in which the basic factorization is performed
by singular-value decomposition of a block Hankel matrix of Ô¨Ånite covariance data,
as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and
De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in
orthonormal coordinates and allows for theoretical veriÔ¨Åcation of the truncation step.
A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of
the data equals the algebraic degree. To achieve this, one must assume that the data
are generated exactly by an underlying system and that the amount of data is suÔ¨Écient for constructing an accurate partial covariance sequence the length of which is
suÔ¨Écient in relation to the dimension of the underlying system. Hence it is absolutely
crucial that a reliable upper bound of the dimension of the ‚Äútrue‚Äù underlying system
is available.
We stress that these stringent assumptions are not satisÔ¨Åed for generic data, as was
pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that
the positive degree has no generic value. In fact, just for the moment considering
the single-output case, for each p such that r ‚â§ p ‚â§ ŒΩ there is a nonempty open
set of partial covariance sequences having positive degree p in the space of sequences
of length ŒΩ. Secondly, for any r, it is possible to construct examples of long partial
covariance sequences having algebraic degree r but having arbitrarily large positive
degree (Theorem 2.4).
In Section 7 we proved an open question concerning the preservation of positivity
in the original (discrete-time) model reduction procedure of Desai and Pal (1984).
Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to
the principal subsystem truncation used in van Overschee and De Moor (1993), but
not to the one in Aoki (1990). We prove that positivity is preserved provided that
the original data satisÔ¨Åes Assumption 2.1, justifying setting the smaller canonical
correlation coeÔ¨Écients equal to zero. Unlike the situation in continuous time, this
truncation does not preserve balancing. The validity of the corresponding procedure
of Aoki (1990) has not been settled.
The contribution of this paper is to provide theoretical understanding of these
identiÔ¨Åcation algorithms and to point out possible pitfalls of such procedures. Hence
the primary purpose is not to suggest alternative procedures. Nevertheless, we would
like to point out that a two-stage procedure equivalent to covariance extension followed
by model reduction would work on any Ô¨Ånite string of data, thus elimination the need
for Assumptions 2.1. However, we leave open the question of how such a procedure
should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension
or some other.
Acknowledgment. We would like to thank the referees and the associate editor for
the careful review of our paper and for many useful suggestions, which have led to
considerable improvements of this paper.
References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel
operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31‚Äì73.
2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner.
3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley.
4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM
J. Control, 13, 162‚Äì173.
5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag.
6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems
and Control Letters, 2, 2301‚Äì2312.
7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization
of modelling Ô¨Ålter. Inter. J. of Control, 50, 2301‚Äì2321.
8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted
for publication.
9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization
of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40.
10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in
Kalman Ô¨Åltering. IEEE Trans. Autom. Control, 36, 563‚Äì579.
11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast Ô¨Åltering
algorithms. SIAM J. Control and Optimization, 32, 744‚Äì789.
12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and
balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105‚Äì1112.
13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE
Trans. Automatic Control, AC-29, 1097‚Äì1100.
14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821‚Äì839.
15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer.
16. Faurre, P. (1969). IdentiÔ¨Åcation par minimisation d‚Äôune representation Markovienne de processus
aleatoires. Symposium on Optimization, Nice.
17. Faurre, P. and Chataigner (1971). IdentiÔ¨Åcation en temp reel et en temp diÔ¨Äeree par factorisation
de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt.
18. Faurre, P., M. Clerget, and F. Germain (1979). OpeÃÅrateurs Rationnels Positifs, Dunod.
19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de reÃÅalisation stochastique. C. R. Academie
Sciences Paris 268.
20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York.
21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE
Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438‚Äì449.
22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and
their L‚àû error bounds. Intern. J. Control, 39, 1115‚Äì1193.
23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and
its Applications, 50, 277‚Äì319.
24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York.
25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling
of time series. Econometric Reviews 11, 379‚Äì396.
26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321‚Äì377.
27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and
approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744‚Äì746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference,
Tel Aviv, Israel.
29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory,
McGraw-Hill.
30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta
Polytech. Scand., MA31, 9‚Äì39.
31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, IdentiÔ¨Åcation
and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499‚Äì513.
32. Kung, S. Y. (1978). A new identiÔ¨Åcation and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705‚Äì714.
33. Larimore, W. E. (1990). System identiÔ¨Åcation, reduced-order Ô¨Åltering and modeling via canonical
variate analysis. Proc. 29th Conf. Decison and Control, pp. 445‚Äì451.
34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809‚Äì857.
35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear
stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241‚Äì333.
36. Lindquist, A. and G. Picci (1994a). On ‚Äúsubspace methods‚Äù identiÔ¨Åcation. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds.,
Akademie Verlag, 315‚Äì320.
37. Lindquist, A. and G. Picci (1994b). On ‚Äúsubspace methods‚Äù identiÔ¨Åcation and stochastic model
reduction. Proceedings 10th IFAC Symposium on System IdentiÔ¨Åcation, Copenhagen, June 1994,
Volume 2, 397‚Äì403.
38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE
Trans. Automatic Control, 20, 396‚Äì399.
39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica,
13, 347‚Äì357.
40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and
Optimization, 29, 1251‚Äì1287.
41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identiÔ¨Åcation
problem. Automatica, 29 , 649-660.
42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identiÔ¨Åcation of
combined deterministic-stochastic systems. Automatica, 30, 75‚Äì93.
43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identiÔ¨Åcation
algorithms and its interpretation. Proceedings 10th IFAC Symposium on System IdentiÔ¨Åcation,
Copenhagen, June 1994, Volume 2, 145‚Äì156.
44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382‚Äì387.
45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes.
Linear Algebra and its Applications, 205-206, 957-1003.
46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day.
47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II.
Journal fur die reine und angewandte Mathematik, 148, 122‚Äì145.
48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space
approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401‚Äì
421.
49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation.
IEEE Trans. Automatic Control, AC-16, 621‚Äì634.
50. Whittle, P. (1963). On the Ô¨Åtting of multivariate autoregressions and the approximate canonical
factorization of a spectral density matrix. Biometrica, 50, 129‚Äì134.
51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its
Applications, Cambridge U.P.
52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via
Ho‚Äôs algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4.
We Ô¨Årst give a proof for the special case n = 1. Consider a scalar function
1z+b
(A.1)
2z +a
with a scalar sequence (1.4) such that Œõ0 = 1. Now it is well-known [see, e.g., Schur
(1918), Akhiezer (1965)] that TŒΩ is positive deÔ¨Ånite if and only if
Z(z) =

|Œ≥t | < 1 t = 0, 1, 2, . . . , ŒΩ ‚àí 1

(A.2)

where {Œ≥0 , Œ≥1 , Œ≥2 , . . . } are the so called Schur parameters. There is a bijective relation
between partial sequences (1.1) and partial sequences {Œ≥0 , Œ≥1 , . . . , Œ≥ŒΩ‚àí1 } of the same
length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the
Schur parameters of (A.1) are generated by the nonlinear dynamical system

Œ±t
Œ±0 = 12 (a + b)
Œ±t+1 = 1‚àíŒ≥
2
t
(A.3)
t Œ±t
Œ≥0 = 12 (b ‚àí a)
Œ≥t+1 = ‚àíŒ≥
1‚àíŒ≥ 2
t

and that Tt becomes singular precisely when there is Ô¨Ånite escape. It was also shown
in Byrnes et al. (1991) that {Œ±t } is generated by a linear system

 	

	 

	
2/Œ∫ ‚àí1 ut
ut+1
=
,
(A.4)
vt+1
vt
1
0
where Œ±t = vt /ut and Œ∫ := (a + b)(1 + ab)‚àí1 . If Œ∫ is greater than one in modulus, the
coeÔ¨Écient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant
scalar factor, similar to
	


cos Œ∏ sin Œ∏
,
‚àí sin Œ∏ cos Œ∏
‚àö
where Œ∏ := arctan Œ∫2 ‚àí 1. Hence Œ±t is the slope of a line through the origin in R2
which rotates counter-clockwise with the constant angle Œ∏ in each time step. Consequently,
arctan Œ±t+1 = arctan Œ±t + Œ∏.
Moreover, assuming that Œ±0 > 0, the Schur condition Œ≥t < 1 will fail as soon as Œ±t+1
negative or inÔ¨Ånite, as can be seen from the Ô¨Årst of recursions (A.3). Hence (A.2)
holds if and only if
œÄ
(A.5)
arctan Œ±ŒΩ < .
2
Therefore for a small < > 0, take‚àöa = 1 ‚àí < and b = 1 + <, yielding a stable Z. Then
2

4 ‚àí <2 . We may choose < so that
Œ∫ = 2‚àí
2 > 1 and Œ∏ = arctan 2‚àí2
œë
œë
<Œ∏< ,
ŒΩ+1
ŒΩ
where œë := œÄ2 ‚àí arctan Œ±0 . Then (A.5) holds so that TŒΩ > 0, but we also have
œÄ
arctan Œ±ŒΩ+1 >
2
so that TŒΩ+1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function
1 œàn (z) + 12 (a + b)œàn‚àí1 (z)
Z(z) =
2 œïn (z) + 12 (a + b)œïn‚àí1 (z)
where {œït } and {œàt } are the SzegoÃà polynomials of the Ô¨Årst and second kind respectively (Akhiezer, 1965). The function Z has the property that its Ô¨Årst n Schur
parameters, {Œ≥0 , Œ≥1 , . . . , Œ≥n‚àí1 }, are precisely the data which uniquely determines œïn ,
œïn‚àí1 , œàn and œàn‚àí1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in
Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated
by

Œ±0 = 12 (a + b)
Œ±t+1 = 1‚àíŒ≥Œ±2 t
t+n‚àí1

Œ≥t+1 =

‚àíŒ≥t Œ±t
2
1‚àíŒ≥t+n‚àí1

Hence, we have reduced the problem to the case n = 1. If we choose the initial
Schur parameters suÔ¨Éciently small so that œïn (z) and œïn‚àí1 (z) are approximately z n
and z n‚àí1 ,
œïn (z) + Œ±0 œïn‚àí1 (z)
is stable if we choose a := 1 ‚àí 2< and b := 1 + < for some small < > 0. Then Œ∫ > 1
and the proof for the case n = 1 carries through with a trivial modiÔ¨Åcation.
Appendix B. The Hilbert space of a sample function
Let y = {y(t)}t‚â•0 be a zero-mean wide-sense-stationary stochastic process deÔ¨Åned on
a probability space {‚Ñ¶, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t, œâ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit
exists, the m √ó m matrix function k ‚Üí Œõk obtained from a particular trajectory is
then a bona-Ô¨Åde covariance function. [The continuous-time analog of this property
was observed already by Wiener (1933)]. If moreover the sample limit is (almost
surely) independent of the particular trajectory and hence necessarily coincides with
the ‚Äùensemble‚Äù covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in
the book of Hannan (1970). It is obvious from BirkhoÔ¨Ä‚Äôs ergodic theorem that any
(zero-mean) strictly stationary ergodic process is also second-order ergodic.
In this Appendix we shall show that the properties of the Hilbert space structure
associated to a stationary time series y, deÔ¨Åned on page 10, are identical to those of
the Hilbert space induced by a second-order ergodic process.10
The two frameworks, i.e., the statistical ‚Äútime-series‚Äù structure and the ‚Äúprobabilistic‚Äù structure, are in fact isomorphic. To see this, pick a ‚Äúrepresentative‚Äù trajectory
of y, i.e. one in the subset of ‚Ñ¶ (of probability one) for which the limit (1.11) exists.
Clearly there will be no loss of generality in assuming that the probability space ‚Ñ¶ of
y is the ‚Äúsample space‚Äù, of all possible trajectories of y, i.e. the set of all semi-inÔ¨Ånite
sequences œâ = {œâ0 , œâ1 , œâ2 , . . . }, œât ‚àà Rm . With this choice, A will be the usual œÉalgebra of cylinder subsets of ‚Ñ¶ and the t:th random variable of the process, y(t), is
just the canonical projection function
y(t, œâ) : œâ ‚Üí œât .
For a process of this kind the Hilbert space H(y) is the closure in L2 (‚Ñ¶, A, P ) of the linear
vector space generated by the scalar random variables œâ ‚Üí yi (t, œâ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence
of m √ó ‚àû matrices y := {y(k)}k‚â•0 as in (3.3). For œâ in the subset of ‚Ñ¶ where the
time averages converge, deÔ¨Åne the map Tœâ ,
Tœâ : a y(t) ‚Üí a y(t) t ‚â• 0 a ‚àà Rm
associating the i:th scalar components of each m-dimensional random vector y(t)
of the process to the corresponding i:th (inÔ¨Ånite) row of the m √ó ‚àû matrix y(t)
constructed from the corresponding sample path {y(t, œâ); t ‚àà Z}. By second-order
ergodicity, the set of all such œâ ‚àà ‚Ñ¶ will have probability measure one and the map
Tœâ will in fact be norm preserving, since by construction we have
Œõt‚àís = Ey(t)y(s) = Ey(t)y(s) ,
where Œõt is the covariance matrix of y. The map Tœâ can then be extended by linearity
and continuity to a unitary linear operator Tœâ : H(y) ‚Üí H(y) which commutes with
the action of the natural shift operators (both of which we denote U), in these two
Hilbert spaces:
U

H(y) ‚àí‚ÜíH(y)
‚Üì Tœâ
Tœâ ‚Üì
U

H(y) ‚àí‚ÜíH(y)
This isomorphism allows us to employ exactly the same formalism and notations
used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991)
in the present statistical setup, where we build estimates of the parameters of models
describing the data in terms of an observed time series instead of stochastic processes.
This provides a remarkable conceptual unity and admits a straightforward derivation
in the style of stochastic realization theory of the formulas in the paper van Overschee
and De Moor (1993), there obtained with considerable eÔ¨Äort through lengthy and
formal manipulations.
Appendix C. The invariant form of the Kalman Ô¨Ålter
Given a stationary stochastic system (3.7), the Kalman Ô¨Ålter is usually determined
via the matrix Riccati equation
Q(t + 1) = AQ(t)A ‚àí [AQ(t)C  + BD ][CQ(t)C  + DD ]‚àí1 [AQ(t)C  + BD ] + BB 
(C.1)
where Q(0) = P := E{x(0)x(0) }. Here
Q(t) = E{[x(t) ‚àí xÃÇ(t)][x(t) ‚àí xÃÇ(t)] },

(C.2)

and the Kalman gain is given by
K(t) = [AQ(t)C  + BD ][CQ(t)C  + DD ]‚àí1 .

(C.3)

These equations of course depend on P , B and D, which vary as the splitting subspace
X varies over , whereas (A, C, CÃÑ) is invariant if a uniform choice of bases is made.
However, as shall see, the gain K depends only on the triplet (A, C, CÃÑ) and hence
one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

X

on (A, C, CÃÑ), and hence are invariant over . Clearly, in view of Theorem 5.1, P‚àí (t),
as deÔ¨Åned by (5.12), has this property. Moreover,
Q(t) = P ‚àí P‚àí (t),
and, consequently, in view of (3.9), and the Lyapunov equation
P = AP A + BB  ,
P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11).
A symmetric argument yields the backward equations.
It is easy to see that as Q(t) ‚Üí Q‚àû monotonously, P‚àí (t) ‚Üí P‚àí , and hence P ‚â• P‚àí ,
as should be.
Appendix D. Some deferred proofs
Proof of Theorem 5.1. Since X is a splitting subspace for the inÔ¨Ånite past H ‚àí and the
inÔ¨Ånite future H + , by stationarity, XœÑ splits HœÑ‚àí := U œÑ H ‚àí and HœÑ+ := U œÑ H + . But
YœÑ‚àí ‚äÇ HœÑ‚àí and YœÑ+ ‚äÇ HœÑ+ , and hence XœÑ splits YœÑ‚àí and YœÑ+ also. (See, e.g., Lindquist
and Picci (1985, 1991).) Now, using the projection formula in the footnote of page
16, we have for any b yœÑ+ ‚àà YœÑ+
Ô£Æ
Ô£πÔ£Æ
Ô£π‚àí1
ŒõœÑ
Œõ1 Œõ2 . . .
Œõ0 Œõ1 . . . ŒõœÑ
Ô£ØŒõ2 Œõ3 . . . ŒõœÑ +1 Ô£∫ Ô£ØŒõ1 Œõ0 . . . ŒõœÑ ‚àí1 Ô£∫
‚àí
Ô£Ø .
E YœÑ b yœÑ+ = b Ô£Ø
yœÑ‚àí
..
.. Ô£∫
..
.. Ô£∫
..
..
Ô£∞ ...
Ô£ª
Ô£∞
Ô£ª
.
.
.
.
.
.
.
.
ŒõœÑ ŒõœÑ +1 ¬∑ ¬∑ ¬∑ Œõ2œÑ ‚àí1
ŒõœÑ ŒõœÑ ‚àí1 ¬∑ ¬∑ ¬∑ Œõ0
= b ‚Ñ¶œÑ ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí
= b  ‚Ñ¶œÑ Œæ
where ‚Ñ¶œÑ and ‚Ñ¶ÃÑœÑ are appropriate Ô¨Ånite-dimensional observability and constructibility
matrices (2.6) of full rank. If œÑ > œÑ0 , there is a minimal factorization H = ‚Ñ¶œÑ ‚Ñ¶ÃÑœÑ such
that Œæ := ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí has n components, and
E{ŒæŒæ  } = ‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 ‚Ñ¶ÃÑœÑ > 0.
Therefore, since the components of Œæ belong to XÃÇœÑ ‚àí , dim XÃÇœÑ ‚àí ‚â• n = dim XœÑ so, since
XÃÇœÑ ‚àí is minimal, XœÑ must also be minimal and XÃÇœÑ ‚àí be spanned by the components
of Œæ.
Next, from the backward system (3.14) we see that
yœÑ‚àí = ‚Ñ¶ÃÑœÑ xÃÑ(œÑ ) + terms ortogonal to XœÑ ,
and therefore, by the same projection formula,
‚àí

E YœÑ a x(œÑ ) = a E{x(œÑ )xÃÑ(œÑ ) }‚Ñ¶ÃÑœÑ (TœÑ‚àí )‚àí1 yœÑ‚àí = a Œæ.
Consequently, E YœÑ XœÑ = {a Œæ | a ‚àà Rn } = XÃÇœÑ ‚àí , establishing the Ô¨Årst of identities
(5.7). The second follows from a symmetric argument.
The representation formula (5.8) follows from the minimality of XœÑ as a splitting
subspace for YœÑ+ and YœÑ‚àí , which, in particular, implies that the constructibility operator,
YœÑ‚àí
: XœÑ ‚Üí XÃÇœÑ ‚àí
Ct := E|X
œÑ
‚àí

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n,
there is a unique random variable xk (œÑ ) ‚àà XœÑ whose projection onto YœÑ‚àí is xÃÇk (œÑ ).
To show that x(0) form a uniform choice of bases as X varies over , Ô¨Årst take X
to be the stationary backward predictor space X+ and let x+ (œÑ ) be the unique basis
‚àí
be arbitrary. Then, since
in UœÑ X+ such that xÃÇ(œÑ ) = E YœÑ x+ (œÑ ). Now, let X ‚àà
‚àí
œÑ
œÑ
+
XœÑ is a splitting subspace for YœÑ and U X+ ‚äÇ U H (Lindquist and Picci, 1991,
Proposition 2.1(vi)), we have

X

X

‚àí

‚àí

xÃÇ(œÑ ) = E YœÑ x+ (œÑ ) = E YœÑ E XœÑ x+ (œÑ ),
and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all
X ‚àà , which is a well-known characterization of uniform choice of bases; see Section
6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields
the corresponding statement for (5.9).

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a
corresponding partial covariance sequence , where ŒΩ is large enough for the Hankel
matrix H, deÔ¨Åned by (1.5), to have rank n. Let (A, C, CÃÑ) be the triplet determined
from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the
covariance data by estimates
{Œõ0T , Œõ1T , . . . , ŒõŒΩT }
of type (6.3), and let (AT , CT , CÃÑT ) be the corresponding triplet obtained via (2.5).
We want to prove that
1
ZT (z) := CT (zI ‚àí AT )‚àí1 CÃÑT + Œõ0T
2
is
strictly
positive
real
for
a
suÔ¨Éciently
large
T
.
Now,
if
	 ‚àí1 deg

 ZT = deg Z, replace Œ£ by
	






Œ£
0
Œ£ 0
in (2.5) in the appropriate
, U by U 0 , V by V 0 , and Œ£‚àí1 by
0 0
0 0
calculation so that (A, C, CÃÑ) and (AT , CT , CÃÑT ) have the same dimensions. This will
not aÔ¨Äect Z and ZT . By continuity, (AT , CT , CÃÑT , Œõ0T ) can be made arbitrarily close
to (A, C, CÃÑ, Œõ0 ) in any norm by choosing T suÔ¨Éciently large. Thus the same holds
for
max !Z(eiŒ∏ ) ‚àí ZT (eiŒ∏ )!
Œ∏‚àà[0,2œÄ]

and hence, since Œ¶(z) := Z(z) + Z(z ‚àí1 ) satisÔ¨Åes (3.10), so will Œ¶T (z) := ZT (z) +
ZT (z ‚àí1 ) for suÔ¨Éciently large T . Moreover, since |Œª(A)| < 1, we have |Œª(AT )| < 1 by
continuity for suÔ¨Éciently large T . Consequently, there is a T0 such that ZT is strictly
positive real for T ‚â• T0 .
Proof of Theorem 5.3. Let Z, deÔ¨Åned by (1.6), be strictly positive real, and let (A, C, CÃÑ)
be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , deÔ¨Åned by
(7.15) in terms of the principal subsystem truncation (A11 , C1 , CÃÑ1 ), is also strictly
positive real. We want to prove that this property is carried over to rational matrix
function
1
ZœÑ 1 (z) = (CœÑ )1 (zI ‚àí (AœÑ )11 )‚àí1 (CÃÑœÑ )1 ) + Œõ0
2
for œÑ suÔ¨Éciently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let QœÑ be deÔ¨Åned by (5.32). Since the canonical correlation coeÔ¨Écients (5.25) tend to the canonical correlation coeÔ¨Écients (4.12) as œÑ ‚Üí ‚àû, Œ£œÑ ‚Üí Œ£.
Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P‚àí (t)
tends to QœÑ Œ£QœÑ as t ‚Üí ‚àû if the initial condition is taken to be P‚àí (œÑ ) = Œ£œÑ . Consequently, for any < > 0, there is a suÔ¨Éciently large œÑ such that !Œ£œÑ ‚àí Œ£! < 2 and
!Œ£œÑ ‚àí QœÑ Œ£QœÑ ! < 2 so that !Œ£ ‚àí QœÑ Œ£QœÑ ! < <. Hence QœÑ tends to a limit Q‚àû with
the property Œ£ = Q‚àû Œ£Q‚àû . Using the same argument in the backward direction, the
‚àí1
second of relations (5.33) shows that Q‚àû also satisÔ¨Åes Œ£ = Q‚àíT
‚àû Œ£Q‚àû . Consequently,
by the same argument as in the proof of Theorem 4.4, Q‚àû is a signature matrix, and
hence in particular diagonal. Therefore,
‚àí1

((AœÑ )11 , (CœÑ )1 , (CÃÑœÑ )1 ) ‚Üí ((Q‚àû )11 A(Q‚àû )‚àí1
11 , C(Q‚àû )11 , CÃÑ(Q‚àû )11 ) as œÑ ‚Üí ‚àû,

where (Q‚àû )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, ZœÑ 1 ‚Üí Z1 . Hence, since Z1 is positive real, then so is ZœÑ 1 for
œÑ suÔ¨Éciently large.
Proof of Lemma 7.4. Let us Ô¨Årst consider the case when (A, C, CÃÑ) is a minimal triplet.
Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P‚àí and P+ respectively, which, in
particular, have the property that P‚àí ‚â§ P1 and P2 ‚â§ P+ . Then, in view of (7.18),
P+ ‚àí P‚àí > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem
A4.4).
Next, let us reduce the general case to the case considered above. If (C, A) is
not observable, change the coordinates in state space, through a transformation
(A, C, CÃÑ) ‚Üí (QAQ‚àí1 , CQ‚àí1 , QCÃÑ  ), so that
	






AÃÇ 0
ÀÜ ‚àó ,
A=
C = CÃÇ 0
CÃÑ = CÃÑ
‚àó ‚àó
where (CÃÇ, AÃÇ) is observable. Then, if P1 and P2 have the corresponding representations
	
	




PÃÇ1 ‚àó
PÃÇ2 ‚àó
P2 =
,
P1 =
‚àó ‚àó
‚àó ‚àó
it is easy to see that PÃÇ1 and PÃÇ2 satisfy the reduced version of the linear matrix
ÀÜ and that, in this new
inequality (6.6) obtained by exchanging (A, C, CÃÑ) for (AÃÇ, CÃÇ, CÃÑ)
ÀÜ , AÃÇ ) is not observable, we proceed
setting, (7.18) holds, i.e., PÃÇ2 ‚àí PÃÇ1 > 0. If (CÃÑ
by removing these unobservable modes. First note that PÃÇ1‚àí1 and PÃÇ2‚àí1 satisfy the
ÀÜ by (AÃÇ , CÃÑ
ÀÜ , CÃÇ). Then,
dual linear matrix inequality obtained by exchanging (AÃÇ, CÃÇ, CÃÑ)
changing coordinates in state space so that
	  





AÃÉ 0

ÀÜ
Àú
AÃÇ =
CÃÑ = CÃÑ ‚àó
CÃÇ = CÃÉ 0 ,
‚àó ‚àó
ÀÜ , AÃÉ ) observable, and deÔ¨Åning
with (CÃÑ
	 ‚àí1 

PÃÉ
‚àó
‚àí1
PÃÇ1 = 1
‚àó ‚àó

PÃÇ2‚àí1

	 ‚àí1 

PÃÉ
‚àó
= 2
,
‚àó ‚àó

44

ANDERS LINDQUIST AND GIORGIO PICCI

Àú , 1 Œõ ) is a minimal realization of Z. Moreover, PÃÉ and PÃÉ satisfy
we see that (AÃÉ, CÃÉ, CÃÑ
1
2
2 0
the corresponding linear matrix inequality (6.6) and have the property (7.18) in this
setting. Hence the problem is reduced to the case already studied above.
Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans, mapping
formed to the continuous-time setting via a bilinear transformation s = z‚àí1
z+1
the unit disc onto the left half plane so that


1+s
(D.1)
Zc (s) = Zd
1‚àís
is positive real in the continuous-time sense if and only if Zd is positive real in the
discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al.
(1979)] that, if (Ad , Cd , CÃÑd , 12 Œõ0 ) and (Ac , Cc , CÃÑc , 12 R) are realizations of Zd and Zc
respectively, we have
Ô£±
Ac = (Ad + I)‚àí1 (Ad ‚àí I)
Ô£¥
Ô£¥
Ô£¥
Ô£≤C = ‚àö2C (A + I)‚àí1
c
‚àö d d
(D.2)
Ô£¥
CÃÑc = 2CÃÑd (Ad + I)‚àí1
Ô£¥
Ô£¥
Ô£≥
R = Œõ0 ‚àí Cd (Ad + I)‚àí1 CÃÑd ‚àí CÃÑd (Ad + I)‚àí1 Cd
and inversely

Ô£±
Ad = (I ‚àí Ac )‚àí1 (I + Ac )
Ô£¥
Ô£¥
Ô£¥
Ô£≤C = ‚àö2C (I ‚àí A )‚àí1
d
c
‚àö c
 ‚àí1
Ô£¥
=
2
CÃÑ
(I
‚àí
A
CÃÑ
d
c
Ô£¥
c)
Ô£¥
Ô£≥
Œõ0 = R + Cc (I ‚àí Ac )‚àí1 CÃÑc + CÃÑc (I ‚àí Ac )‚àí1 Cc

(D.3)

Under this transformation the observability gramian and the constructibility gramian
(i.e., the observability gramian of (CÃÑ, A )) are preserved so that (Ad , Cd , CÃÑd , 12 Œõ0 ) is
a minimal realization if and only if (Ac , Cc , CÃÑc , 12 R) is; see, e.g., p. 1119 in Glover
(1984). Moreover, coercivity is preserved, and the solution sets of the corresponding
linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the
reachability gramian of a spectral factor and this gramian is also preserved.)
Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact,
transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting,
all the requirements of Theorem 7.1 are satisÔ¨Åed. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order
positive real function thus obtained via (D.3) back to discrete time, the desired result
is obtained.

