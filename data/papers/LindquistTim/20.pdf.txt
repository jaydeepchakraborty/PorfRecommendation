CANONICAL CORRELATION ANALYSIS, APPROXIMATE COVARIANCE EXTENSION, AND IDENTIFICATION OF STATIONARY TIME SERIES*
ANDERS LINDQUIST AND GIORGIO PICCI

Abstract. In this paper we analyze a class of state-space identification algorithms for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called "subspace methods" can be described as covariance estimation followed by stochastic realization. The methods offer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identification into the solution of a Riccati equation but introduce at the same time some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identification procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not explicitly reported. In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We review the basic ideas of stochastic realization theory in the context of identification, discuss the concept of stochastic balancing and of stochastic model reduction by principal subsystem truncation. The model reduction method of Desai and Pal, based on truncated balanced stochastic realizations, is partially justified, showing that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing conditions under which the "subspace identification" methods produce bona fide stochastic systems.

1. Introduction Recently there has been a renewed interest in state-space identification algorithms for time series based on a two steps procedure which in principle can be described as estimation of a rational covariance model from observed data followed by stochastic realization. The method offers the major advantage of converting the nonlinear parameter estimation phase which is necessary in traditional ARMA models identification into a partial realization problem, involving a Hankel matrix of estimated
 This research was supported in part by grants from TFR, the G¨ oran Gustafsson Foundation, the SCIENCE project "System Identification" and LADSEB-CNR.  Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm, Sweden  Dipartimento di Elettronica e Informatica, Universita' di Padova, 35131 Padova, Italy 1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which efficient numerical solution techniques are available. In this framework we can naturally accommodate multivariate processes and there are indications that the algorithms may work also with data containing purely deterministic components (van Overschee and De Moor, 1993). A drawback, however, to be emphasized in this paper, is that, unlike, say, least-squares identification of ARMA models, these methods do not work for arbitrary data. This type of procedure was apparently first advocated by Faurre (1969); see also Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work, based on canonical correlation analysis (Akaike, 1975) (or some other singular-value decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki (1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor, 1993). Numerical experience shows that the computation time needed to get the final model parameters estimates compares very favorably with traditional iterative prediction error methods for ARMA models. On the other hand there is a price to be paid for this simplification. These methods introduce some nontrivial mathematical problems related to positivity. The reason for this is that an essential part of the problem is equivalent to the well-known rational covariance extension problem. Therefore the usual deterministic realization arguments based on factorization of a Hankel matrix are not valid for generic data, something that is habitually overlooked in the literature. Note that positivity is the natural condition insuring solvability of the Riccati equation required to compute state-space models of the signal from the covariance estimates. Central in the procedures described above is the following classical problem of identification of a covariance sequence. Let {0 , 1 , . . . ,  } (1.1)

be a finite set of sample m × m covariance matrices estimated in some unspecified way from a certain m-dimensional sequence of observations {y0 , y1 , y2 , . . . yT }, ¯ = k CAk-1 C and such that the infinite sequence {0 , 1 , 2 , . . . }, (1.4) (1.2)

¯ ) such that and consider the problem of finding a minimal1 triplet of matrices (A, C, C k = 1, 2, . . . ,  (1.3)

¯ for k =  + 1,  + 2, . . . , is a bona fide obtained from (1.1) by setting k := CAk-1 C covariance sequence. In the literature the last condition is generally ignored. The remaining problem of ¯ ) satisfying (1.3) is called the minimal partial realfinding a minimal triplet (A, C, C ¯ ) is usually computed by minimal factorization ization problem. The triplet (A, C, C
1

¯ ) is minimal if (A, C ) is completely observable and (A, C ¯ ) is completely reachable. Here (A, C, C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

3

of a block Hankel matrix corresponding to the data (1.1) as follows:      ¯ C 1 2 C 3 · · · j ¯ 2 3  4 · · · j +1   CA   CA      , H= = . . . . . .  .      . .. . . . . . . . . . . . ¯ (A )j -1 i i+1 i+2 · · · i+j -1 CAi-1 C

(1.5)

where i + j - 1 =  and the Hankel matrix H is chosen as close to square as possible by taking |i - j |  1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j ) such that i + j - 1 =  , and hence the minimal factorization must be made for a choice of (i, j ) in which the Hankel matrix (1.5) has maximal rank. The infinite sequence ¯ for k =  +1,  +2, . . . {0 , 1 , 2 , . . . } obtained in this way by setting k := CAk-1 C is called a minimal rational extension of the finite sequence (1.1) and is in general not a covariance sequence. The dimension r of a minimal rational extension is called the (algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to the McMillan degree of the m × m rational matrix ¯ + 1 0 , (1.6) Z (z ) = C (zI - A)-1 C 2 and the elements of the infinite sequence (1.4) are the coefficients of the Laurent expansion 1 Z (z ) = 0 + 1 z -1 + 2 z -2 + . . . 2 (1.7)

about z = . The underlying identification problem is however a great deal more complicated than the classical partial realization problem. In fact, the requirement that (1.4) be a bona fide covariance sequence amounts to (1.4) being a positive sequence in the sense that, for every t  Z+ , the block Toeplitz matrices Tt ,   2 · · · t 0 1 1 0 1 · · · t-1  , Tt =  (1.8) . . . . ...  . . . . . .  . . t t-1 t-2 · · · 0 formed from the infinite sequence (1.4), be positive definite or, equivalently, that the matrix function (z ) := Z (z ) + Z (1/z ) be positive semidefinite on the unit circle, i.e. (ei )  0   [0, 2 ). (1.10) (1.9)

This property is equivalent to  being a spectral density matrix. In fact, it will be the spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial covariance sequence unless T > 0, but this is not enough. From the point of view of identification there seem to be two possible routes to ¯ ) from the finite covariance sequence (1.1). One that determine a model (A, C, C has been proposed in the literature is do minimal factorization (1.5) of a finite block Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown in this paper, there is no a priori guarantee that this method will yield a positive extension. This fact has nothing to do with sample variability (random fluctuations) of the covariance estimates (1.1), and to emphasize this point we initially assume that all strings of data (1.2) are infinitely long. A theoretically sounder identification method, which will not be considered in this paper, could instead be to do positive extension first and then to use a stochastic model reduction procedure on the triplet ¯ ) of the positive extended sequence. (A, C, C The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this difficulty into consideration have been pointed out by the authors of this paper at many scientific meetings in the last ten years. This has had no apparent effect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identification procedure of Aoki (1990) and demonstrate that there is a hidden, and not easily tested, assumption without which the procedure will not be guaranteed to succeed. The punch line is that none of the subspace identification methods under consideration can be expected to always work for generic data but that some not entirely natural conditions on the data are needed. The analysis of the basic theoretical issues behind subspace identification is carried out in the geometric framework of stochastic realization theory; see, e.g., Lindquist and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this theory and adapt them to the problem of identification. To this end, we first discuss an idealized situation in which the time series (1.2) is infinitely long i.e. T = , and the available covariance data are given by the ergodic limit 1 T  T + 1 lim
T

yt+k yt+j = k-j
t=0

(1.11)

for all k and j . Then the sample estimates in the sequence (1.1) are bona fide covariance matrices and the Toeplitz matrix T formed from the data will be positive definite and symmetric. We introduce a Hilbert space of observed (infinite) strings of data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we establish a correspondence which turns operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. Canonical correlations and balanced stochastic realizations are then analyzed in this setting in Section 4, and the basic concepts and principles used in the subspace identification methods, as well as in the model reduction procedures of Desai and Pal, are translated into the more natural context of geometric stochastic realization theory. Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact infinite covariance sequence (1.4). Consequently, the difficult question of positivity is not an issue here. Nor is it the finite sample size per se which is the problem, but the fact that only a finite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is finite. Therefore we separate these issues by discussing stochastic realization theory from finite covariance data in Section 5 and subspace identification in Section 6. In this framework we show that the method of van Overschee and De Moor (1993) is valid under some rather stringent assumptions. We stress that we are only concerned with identification procedures for state space modeling of time series. "Subspace identification" methods for deterministic systems with measurable inputs or for spectral factors do not involve positivity, but stability may still be a problem. However, the algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part, so the problem of positivity arises here too. Another idea behind the subspace identification methods considered in this paper is to disregard modes corresponding to "small" canonical correlation coefficients. This is called balanced truncation and is in fact a stochastic model reduction procedure. In all such procedures there must be a guarantee that the reduced-degree matrix function (1.6) is positive real, and therefore the preservation of positivity in such reductions is a main concern of this paper. Section 7 is devoted to such issues. The model reduction procedure of Desai and Pal (1982) was never theoretically justified in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 . Here we shall demonstrate that this reduction procedure produces a positive real, but not in general balanced, reduced model structure. In fact, the singular values of the truncated system are usually not equal to the r first singular values of the original system. It is an interesting fact that the procedure of Desai and Pal does produce balanced truncations for continuous-time stochastic systems. A partial result in this direction was given by Harshavardana, Jonckheere and Silverman (1984), who showed that the truncated function is positive real and conjectured that it is balanced. We shall demonstrate that it is indeed balanced, a result that is actually already contained in the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time depends on the fact that the spectral factors of the truncated approximate spectrum behave differently than in continuous time. While in continuous time the realizations of the reduced spectral factors are proper subsystems, obtained by partitioning the matrices of the realizations of the factors of , this is not the case in discrete time, contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced truncation procedure is available in discrete time, but the systems matrices are no longer submatrices of those of the original system, and therefore it is not equivalent to the truncation procedure used in subspace identification. Several of the results of this paper have previously been announced in Lindquist and Picci (1994a)3 and in Lindquist and Picci (1994b).

In Desai et al. (1985) a different model reduction procedure, which is not relevant to subspace identification, is considered, namely "deterministic" model reduction of the minimum phase spectral factors. 3 We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some erroneous statements, was accidentally published in place of the paper finally submitted for publication. The correct version can be obtained from the authors.

2

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances The solution to the minimal partial realization problem , i.e., the problem to find ¯ ) satisfying (1.1) is in general not unique. This lack of uniquethe triplet (A, C, C ness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1) equals that of {0 , 1 , . . . ,  -1 } so that we can use a Hankel matrix (1.5) based allowing us to define the shifted Hankel matrix  3 4 2  3 4 5  (H ) =  . . .  . . . . . . i+1 i+2 i+3 (2.1)

on this data, i.e., with i + j =  ,  · · · j +1 · · · j +2   . ... . .  · · · 

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) pro¯ ) which is unique up to a similarity transformation. duces a minimal solution (A, C, C As first pointed out by Zeiger and McEwen (1974), the minimal factorization on which the Ho-Kalman procedure is based may be performed by singular-value decom¯ ) uniquely; see also Kung (1978). In fact, the Hankel position, thereby fixing (A, C, C matrix H may be factored as H = U V U U = I = V V, (2.3) where  is the square n × n diagonal matrix of the nonzero singular values taken in ¯ := V 1/2 this leads to a factorization decreasing order. Setting  := U 1/2 and  ¯ H =  ¯ ¯ == (2.4)

¯ ) is obtained by solving of the type (1.5). Then a minimal realization (A, C, C ¯ =  (H ), A ¯ = 1 (H ) and C ¯  = 1 (H ), C

where  (H ) is the shifted Hankel matrix (2.2) and 1 (H ) is the first block row of H . ¯ ) must be given by It follows that the triplet (A, C, C A = -1/2 U  (H )V -1/2 , C = 1 (H )V -1/2 , ¯ = 1 (H )U -1/2 , C (2.5a) (2.5b) (2.5c)

a form to which we refer as finite-interval balanced, since it is balanced in the sense ¯ are both equal to , and that ¯ that   and      ¯ C C ¯  CA   CA  ¯   .  = (2.6) = . .    . . .  . ¯ (A )j -1 CAi-1 C

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identification of time series. The problem with such a strategy is that this algorithm is a deterministic realization procedure and hence does not a priori insure that (1.6) is positive real, or even stable for that matter, even if the Toeplitz matrix T is positive definite. In fact, it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space of covariance data (1.1) for which A is not stable, and a fortiori the same holds for positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in Aoki (1990) is based on the following hidden assumption which is not entirely natural. Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H . Therefore, not only must we know that there exists an underlying finite-dimensional system, but we must also have some upper bound for its dimension. A conservative ]. upper bound which will always suffice is [  2 Is this assumption natural? If the covariance data are really generated exactly from a "true" stochastic system and there is a reliable estimate of its order which is no more than half of the length of the covariance sequence, then the assumption will hold. However, and this is an important point of this paper, one cannot expect Assumption 2.1 to hold for an arbitrary covariance sequence (1.1). To clarify this point, let us agree to call {0 , 1 , 2 , . . . } a minimal rational extension of {0 , 1 , . . . ,  } if the rational function (1.7) has minimal degree. By definition this is the algebraic degree of {0 , 1 , . . . ,  }. A rational extension is called positive if, for every µ >  , the block Toeplitz matrices Tµ formed from the corresponding infinite sequence (1.4) are positive definite. An extension with this property is called a positive rational extension. It is well known that the extension {0 , 1 , 2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function Z (z ) is analytic in the closed unit disc and the matrix function (z ) = Z (z ) + Z (1/z ) (2.7)

is nonnegative definite on the unit circle, making  a spectral density matrix. A minimal positive rational extension of the finite sequence (1.1) is one for which the ¯ ) in (1.6) is as small as possible. dimension of the triplet (A, C, C Definition 2.2. The positive degree p of the finite covariance sequence {0 , 1 , . . . ,  } is the dimension of any minimal positive extension. A well-known example of a positive extension is the maximum entropy extension (Whittle, 1963) corresponding to the spectral density (z ) := W (z )W (1/z ) , where the spectral factor W (z ) is (modulo a multiplicative constant matrix) the inverse of the Levinson-Szeg¨ o matrix polynomial of order  corresponding to the finite covariance sequence (1.1). Since the rational function W (z ) generically has the McMillan degree equal to m , it follows from spectral factorization theory (Anderson, 1958) that Z (z ) has also degree m . Consequently, the positive degree p is bounded from below by the algebraic degree r and from above by m . As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use algebraic rather than positive extensions, usually computed by minimal factorization a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1 may also be formulated in the following way. Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree. This assumption prescribes a property of the covariance sequence (1.1) which is not generic. We can illustrate this point by considering the rational extension problem for a finite scalar covariance sequence (1.1). The positive degree p lies between the algebraic degree r and  . Note that neither the case p =  nor the case p <  are "rare events", because there are open sets of covariance sequences (1.1) of both categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each µ  µ   there is an open set of covariance data in R for which p = µ. such that  2 If the upper limit p =  is attained there are infinitely many nonequivalent minimal ¯ ) providing a positive extension, one of which is the maximum entropy triplets (A, C, C extension. In fact, it can be shown that these  -dimensional extensions form an Euclidean space (Byrnes and Lindquist, 1989). This shows that the finite data (1.1) never contains enough information to establish a "true" underlying system. A similar statement can be made in the case when p <  . Example 2.3. Consider the case m = 1 and  = 2, i.e., consider a scalar partial covariance sequence {0 , 1 , 2 }. If 1 = 2 = 0, we have r = p = 0. Otherwise, we always have r = 1, whereas the positive degree can be either one or two. In fact, 2 setting 0 := 1 /0 and 1 := (2 1 + 2 )/(1 - 1 ), it can be shown (Georgiou, 1987; also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1 if and only if |0 | |1 | < 1 + |0 | and p = 2 otherwise. In fact, it is not hard to construct examples for which the gap between algebraic and positive rank is arbitrarily large, as the following theorem shows. Theorem 2.4. Let n  Z+ be fixed. Then for an arbitrarily large  there is a stable rational function Z (z ) of degree n, such that the Toeplitz matrix T formed as in ( 1.8) from the coefficients of the Laurent expansion ( 1.7), is positive definite while T +1 is indefinite. Consequently, you cannot test the positivity of a rational extension of (1.1) by checking a finite Toeplitz matrix, however large is its dimension. The proof of Theorem 2.4 is given in Appendix A. Let us now return to the identification procedure of Aoki (1990). In practice the rank of H will always be full, and to compute a partial realization of reasonable dimension the basic idea is to partition  as = 1 0 , 0 2 (2.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in 2 are smaller than those in 1 , perhaps close to zero, and then take 2 = 0 so that H is approximated by H1 = U 1 0 V = U1 1 V1 . 0 0 (2.9)

The matrix H1 is a best approximation (given the rank) of H in (the induced) 2 ­ norm, but it is in general not Hankel and hence can not be used to determine a reduced order system. Of course, one may instead use Hankel-norm approximation (Adamjan, Arov and Krein, 1971), which produces another best approximation of H in 2 -norm that is Hankel and has the same rank as H1 . However, if 2 is "very small" compared to 1 , then H1 is close to H and hence approximately Hankel. For this reason, Aoki's procedure (Aoki, 1990) is based on the original data H and  (H ). Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I , the ¯r ) given by same type of calculation as above yields the reduced triplet (Ar , Cr , C Ar = 1
- 1/ 2

U1  (H )V1 1
- 1/2

- 1/ 2

,

(2.10a) (2.10b) (2.10c)

Cr = 1 (H )V1 1 , 1 /2 ¯r = 1 (H )U1 - C . 1

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal subsystem truncation in the sense that, if H is produced by a finite-dimensional system ¯ ) having finite-interval balanced form (2.5), we have with (A, C, C Ar = A11 , where A= A11 A12 A21 A22 C = C1 C2 ¯1 C ¯2 . ¯= C C (2.12) Cr = C1 , ¯r = C ¯1 , C (2.11)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for  (H ), 1 (H ) and 1 (H ) and inserting in (2.10). However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that ¯1 + 1 0 (2.13) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real, provided of course that Z , defined by (1.6), is positive real. The question of stability was answered in the affirmative in Pernebo and Silverman (1982) and is addressed in Aoki (1990). The crucial question of positivity, however, is not discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be proven for a somewhat modified procedure described below. In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall consider a normalized Hankel matrix
1 -T ^ = L- H + HL- ,

(2.14)

where L- and L+ are lower triangular Cholesky factors of the Toeplitz matrices T- and T+ of (1.1) and the corresponding sequence of transposed covariances respectively; see Section 4 below. This is also the Hankel matrix considered in van Overschee and ^ instead of H , the De Moor (1993). Taking the singular value decomposition of H

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coefficients, i.e., the cosines of the angles between the past and the future of the process y . The systems matrices can be determined in a manner analogous to (2.5), but now
-1 -1 ¯ ^ = ¯ T-  =  T+

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as ^ =U ^ ^U ^ so that H = above. To see this, consider the singular value decomposition H ¯ ^ ^ ^ (L+ U )(L- V ) . Since H =  and this factorization is unique modulo coordinate ¯ = L- V ^ ^ 1/2 and  ^ ^ 1/2 . Then transformation in state space, we may take  = L+ U ^ ^ ^ ^ (2.15) follows from U U = I = V V . As we shall see next, (2.15) corresponds to a more natural type of balancing corresponding to a Hankel operator describing the interface between the past and the future of the time series y . 3. Stochastic realization theory in the Hilbert space of a sample function In this section we introduce a mathematical framework which is suitable for the identification problem described above. We define a Hilbert space of observed (infinite) strings of data {yt }. This framework turns out to be isomorphic to that of geometric stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical problem of identification. In this way we also establish a correspondence which converts operations on random quantities defined on abstract probability spaces into prototypes of statistical algorithms involving computations based on the observed data. In identification we have access only to a finite string of data {y0 , y1 , y2 , . . . , yT }. (3.1)

Here T may be quite large but, of course, always finite. To begin with, we shall, however, consider the idealized situation that we are given a doubly infinite sequence of m-dimensional data {. . . , y-3 , y-2 , y-1 , y0 , y1 , y2 , y3 . . . } (3.2)

together with a corresponding covariance sequence {k }k0 , each matrix k of the sequence being computed from the data (3.2) by an ergodic limit of the type (1.11). In Section 5 we then modify the theory to handle the situation of finite data (3.1). For each k  Z define the m ×  matrix y (t) := [yt , yt+1 , yt+2 , . . . ] (3.3)

and consider the sequence y := {y (t)}tZ . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of all finite linear combinations ak y (tk ); ak  Rm , tk  Z

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product defined by linear extension of the bilinear form 1 a y (k ), b y (j ) := lim T  T + 1
t0 +T

a yt+k yt+j b = a k-j b,
t=t0

(3.4)

which clearly does not depend on t0 . This inner product is nondegenerate if the Toeplitz matrix Tk , constructed from the covariance data {0 , 1 , . . . , k }, is a positive definite symmetric matrix for all k . Here we shall assume that the sequence {Tk }k0 is actually coercive, i.e., Tk > cI for some c > 0 and all k  0. (See Assumption 3.2 below for an alternative characterization.) We also define a shift operator U on the family of semi-infinite matrices (3.3), by setting Ua y (t) = a y (t + 1) t  Z, a  Rm ,

defining a linear map which is isometric with respect to the inner product (3.4) and extendable by linearity to all of Y . In particular the sequence of matrices {y (k )} corresponding to the time series y is propagated in time by the action of the operator U, i.e., yi (t) = Ut yi (0), i = 1, 2, . . . , m, t  Z, (3.5)

where yi denotes the i:th row component of y . Then, closing the vector space Y in the inner product (3.4), we obtain a Hilbert space H (y ) := cl Y . The shift operator U is extended by continuity to all of H (y ) and is a unitary operator there. As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results in the geometric theory of stochastic realization can be carried over to the present framework by merely identifying the time series y with a stationary stochastic process y. In particular, the subspaces H - and H + of H (y ) generated by the elements (3.3) for t < 0 and t  0 respectively can be regarded as the past and future subspaces of the stationary process y. For reasons of uniformity of notation the inner product (3.4) will also be denoted ,  = E { }, (3.6)

as the frameworks are completely equivalent. Here we allow E {·} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity condition introduced above insures that tZ Ut H - = 0 and tZ Ut H + = 0, i.e., y is a purely nondeterministic sequence. As we have pointed out above, the subspace identification methods of Aoki (1990) and van Overschee and De Moor (1993) are based on the assumption that the available data is generated by an underlying stochastic system of finite dimension. More specifically, using the notations introduced above, we assume that the data are generated by a linear system of the type x(t + 1) = Ax(t) + Bw(t) y (t) = Cx(t) + Dw(t) (3.7)

12

ANDERS LINDQUIST AND GIORGIO PICCI

defined for all t  Z, where w is some vector-valued normalized white noise time series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability matrix. Throughout this paper we shall assume (without restriction) that (A, B, C ) B is a minimal triplet and that the matrix has linearly independent columns. D The system is assumed to be in statistical steady state so that the n-dimensional state x and the m-dimensional output y are uniquely defined by (3.7) as linear causal functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and y (s) will depend only on the difference t - s. We shall think of the system (3.7) as a representation of the output time series y . The state and input variables x and w are introduced in order to display the special structure of the dynamic model of y and are by no means unique. Such a representation is called a state-space realization of y . Remark 3.1. Despite the fact that the model (3.7) is defined in terms of sample sequences, all equalities must be understood in the sense of Hilbert space metric, just as in the case of models based on random variables. The number of state variables n is called the dimension of the realization. A realization is minimal if there is no other realization of y of smaller dimension. In this case the covariance matrix of the state vector, P = E {x(t)x(t) } is positive definite. Moreover as the matrix (3.8)

B is taken with linearly independent D columns, the number of (scalar) white noise inputs p is also as small as possible. Clearly, the covariance sequence {0 , 1 , 2 , . . . } of the output {y (t)} of a minimal model (3.7) is a rational sequence of degree n, i.e., represented as ¯ = AP C + BD ¯ k = 0, 1, 2, . . . where C k = CAk-1 C 0 = CP C + DD . (3.9)

In the following we shall need to assume that the corresponding spectral density (z ) satisfies the following condition. Assumption 3.2. The spectral density  of the output process of the underlying system (3.7) is coercive in the sense that (ei ) > 0 for all   [0, 2 ]. (3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent sequences. Recall that a positive real function Z such that (z ) := Z (z ) + Z (z -1 ) satisfies (3.10) is called strictly positive real. Let H (w) be the Hilbert space generated by w, i.e. the closure of the linear space spanned by the family {wi (t), i = 1 . . . p, t  Z} with respect to the metric induced by the inner product ,  = E {  } where E {·} is defined by (3.6). Let H + and H - be the subspaces of H (w) generated by the components of future {y (0), y (1), y (2) . . . } and past outputs {y (-1), y (-2), y (-3) . . . }, respectively.
4

This means that E {w(t)w(s) } = Its where ts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace X := {a x(0) | a  Rn } (3.11)

is invariant under coordinate changes of the type (A, B, C )  (T AT -1 , T B, CT -1 ) and is a coordinate-free representation of the realization (3.7). Such an object is called a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next define the stationary Hankel operator of y , H : H +  H - as H := E H |H +
- -

(3.12)

where E H  is the orthogonal projection of  onto H - . The splitting subspace property of X is equivalent to the commutativity of the diagram H+ O i.e. to the factorization H = CO ,
+ -

- H - C X

H

(3.13)

where the operators O := E H |X and C := E H |X are the observability respectively constructibility operators relative to the splitting subspace X . It can be shown that the splitting subspace X is minimal if and only if O and C are both injective. (See, e.g., Lindquist and Picci (1991).) The system (3.7) is a forward or causal realization of y in the sense that the subspace + H (w), generated by the future of w, is orthogonal to X and H - , i.e. to the present state and past output. Corresponding to (3.7) there is another realization ¯w ¯(t) + B ¯ (t - 1) x ¯(t - 1) = A x ¯x ¯w y (t - 1) = C ¯(t) + D ¯ (t - 1) (3.14)

¯ ), generated by which is backward or anticausal in the sense that the subspace H - (w + ¯(0) is a basis in X , i.e. the past of w ¯ , is orthogonal to X and H . Like x(0), x X := {a x ¯(0) | a  Rn }. ¯ = P -1 P x ¯(0) = P -1 x(0). (3.15)

In fact, x ¯(0) is the dual basis of x(0) in the sense that E {x(0)¯ x(0) } = I . Hence (3.16)

The particular notations used in (3.7) and (3.14) reflect the special meaning of the ¯ ). Computing the covariance matrix of the output using the dual parameters (A, C, C ¯ ) is precisely a realizations (3.7) and (3.14), it is in fact readily seen that (A, C, C triplet realizing the positive real part (1.6) of the spectral density matrix (z ) of the time series y . There are infinitely many minimal factorizations (3.13), one for each Markovian splitting subspace, but the basis in each state space X can be chosen so ¯ ) are the same for each minimal X . This is called a uniform that the triplets (A, C, C choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward predictor spaces X- = E H H +
-

X+ = E H H - ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12) and of its adjoint, respectively. ¯ ), the splitting Fixing a uniform choice of bases, and thus the triplets (A, C, C subspace X- has the forward stochastic realization x- (t + 1) = Ax- (t) + B- w- (t) y (t) = Cx- (t) + D- w- (t) with state covariance P- , and X+ has the backward realization ¯+ w x ¯+ (t - 1) = A x ¯+ (t) + B ¯+ (t - 1) ¯ +w ¯x ¯+ (t - 1) y (t - 1) = C ¯+ (t) + D (3.19) (3.18)

¯+ . with state covariance P These two stochastic realizations will play an important role in what follows. In fact, an important interpretation of these realizations is that
-1 [y (t) - Cx- (t)] x- (t + 1) = Ax- (t) + B- D-

is the unique steady-state Kalman filter of any minimal realization (3.7) of y in the fixed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8) ¯+ )-1 , then corresponding to the forward counterpart of (3.19), i.e., P+ = (P P -  P  P+ for the state covariance of any minimal realization (3.7). In the same way
-1 ¯+ ¯+ D ¯+ (t) + B [y (t - 1) - C x ¯+ (t)] x ¯+ (t - 1) = A x

(3.20)

is the backward steady-state Kalman filter of all minimal backward realizations (3.14), and ¯+  P ¯P ¯- P ¯- is the backward counfor an arbitrary backward minimal realization (3.14), where P terpart of P- . 4. Canonical correlations and balanced stochastic realization In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a finite-dimensional realization of the type (3.7). Equivalently, we study certain factorizations of the infinite Hankel matrix of the corresponding infinite covariance sequence {0 , 1 , 2 , . . . }. Some portions of this section can be found in an equivalent but somewhat different setting in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and set notations. This will be done in the geometric framework of Section 3, thereby providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall introduce orthonormal bases in H - and H + . To this end it will be useful to represent past and future outputs as infinite vectors in the form,     y (-1) y (0) y (-2) y (1)    y = (4.1) y- =  + y (-3) y (2) . . . . . . Let L- and L+ be the lower triangular Cholesky factors of the infinite block Toeplitz matrices T+ := E {y+ y+ } = L+ L+ T- := E {y- y- } = L- L- and let
1  := L- - y- 1  ¯ := L- + y+

(4.2)

be the corresponding orthonormal implies that  1 2 H := E {y+ y- } =  3 . . .

bases in H - and H + respectively. Now, (3.9) 2 3 3 4 4 5 . . . . . .    ¯  C C ... ¯  . . .  CA   CA = 2  ¯ 2 , . . . CA  C (A )  . . ... . . . .

(4.3)

and therefore we have the following representation result, which can be found in Desai et al. (1985). Proposition 4.1. Let y be realized by a finite dimensional model of the form (3.7). Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator H is
1 -T -1 ¯ -T ^  = L- H + E {y+ y- }L- = L+  L- ,

(4.4)

where

 C  CA   = CA2  . . . 

and

¯  C ¯  CA  ¯   = C ¯ (A )2  . . . .



(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization (4.3) for H , irrespective of which X (i.e. which minimal realization of y ) is chosen. Recall that the adjoint O of the observability operator O is defined as the unique linear operator H +  X such that O,  = , O  for all   X and   H + . Orthogonality implies that E H ,  = ,  = , E X  , and therefore O = E X |H + . In the same way, we see that C  = E X |H - . The finiterank linear operators O O and C  C are defined on X and are the coordinate-free representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O O is full rank and constructible if and only if C  C is full rank. The following representations show that these gramians are related
+

16

ANDERS LINDQUIST AND GIORGIO PICCI

¯+ , the state covariances of the forward and backward steady-state Kalman to P- and P filters (Picci and Pinzoni, 1994). Proposition 4.2. Let x(0) and x ¯(0) be the conjugate basis vectors in a minimal splitting subspace X as defined above. Then, in a uniform choice of bases, ¯+ x(0) ¯(0) = a P O O a x and C  C a x(0) = a P- x ¯(0), (4.7) ¯+ , respectively, independently i.e., C  C and O O have matrix representations P- and P of X . Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal, E H a x(0) = a x- (0), C  C a x(0) = E X a x- (0) = E X a P- x ¯- (0). But, since the bases x ¯(0) and x ¯- (0) are chosen uniformly, EX a x ¯- (0) = a x ¯(0) a  Rn , and consequently (4.7) follows. The proof of (4.6) is analogous. The factorization (4.4) can also be derived from (3.13) and the following useful matrix representations of the observability and constructibility operators. Proposition 4.3. Let x(0) and x ¯(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
T ¯(0) = a  L- ¯ Oax +  1 O b  ¯ = b L- + x(0)
-

(4.6)

and therefore

(4.8)

and
T ¯ L- C a x(0) = a  -  1¯ x(0), C  b  = b L- - ¯

(4.9)

¯ are given by (4.5). where  and  Proof. Since, in view of (3.7), y+ = x(0) + terms which are orthogonal to X,
1 and  ¯ = L- + y+ , we have 1 E { ¯x(0) } = L- + P.

(4.10)

Consequently, for any a  Rn , the usual projection formula5 yields O a x(0) = E H a x(0) = a E {x(0)¯  } ¯ and O b  ¯ = EX b  ¯ = b E { ¯x(0) }P -1 x(0), from which (4.8) follows. A symmetric argument yields (4.9).
If   H (w) and the subspace Z  H (w) is spanned by the components of the full-rank random vector z , then E Z  = E {z }(E {zz })-1 z .
5
+

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should note that the matrix representations of O and C  are the transposes of those of O ¯ = I . Moreover, it follows and C if and only if x(0) is an orthogonal basis, i.e., P = P from (4.8) that -1 ¯(0) = a  T+ x(0), O Oa x
-1 showing that  T+  is a matrix representation of O O, in harmony with the analysis at the end of Section 2. In the same way, (4.9) yields -1 ¯ ¯ T- ¯ x(0), C  C a x(0) = a  -1 ¯ ¯ T-  is a matrix representation of C  C . Together with Proposition 4.2 and hence  ¯+ : this yields the following explicit formulas for P- and P -1 ¯+ =P  T+ -1 ¯ ¯ T-   = P- .

(4.11)

Now, let {1 , 2 , 3 , . . . } be the singular values of the Hankel operator H. Since rank H = n, i = 0 for i > n. The nonzero singular values 1  1  2  3 . . .  n > 0 (4.12)

are the cosines of the angles between the subspaces H- and H+ ; they are known as the canonical correlation coefficients of y (Hotelling, 1936, Anderson, 1958). Obviously 1 < 1 if and and only if H-  H+ = 0. The squares of the canonical correlation coefficients are the eigenvalues of H H, i.e.,
2 i , H H i = i

which, in view of (3.13) may be written
2 (O i ), O OC  C (O i ) = i

and therefore, as was also demonstrated in Picci and Pinzoni (1994),
2 2 2 , 2 , . . . , n }, {O OC  C} = {1

(4.13)

2 2 2 , 2 , . . . , n are the eigenvalues of O OC  C . But, in view of Proposition 4.2, i.e., 1 this is precisely the coordinate-free version of the invariance condition 2 2 2 ¯+ } , 2 , . . . , n } = {P- P {1

(4.14)

of Desai and Pal (1984). This suggests that an appropriate uniform choice of bases would be the one that ¯+ equal and equal to the diagonal matrix of nonzero canonical corremakes P- and P lation coefficients. ^  is the In fact, in view of Proposition 4.1, the infinite normalized Hankel matrix H matrix representation of the operator H in the orthonormal bases (4.2). Therefore ^  has the singular-value decomposition H ^  = U   V = U  V , H   = diag{1 , 2 , 3 , . . . , n }, (4.15) where  is the diagonal n × n matrix consisting of the canonical correlation coefficients (4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and  is the infinite matrix  =  0 . 0 0

Moreover U and V are infinite orthogonal matrices, and U and V are  × n submatrices of U and V with the the property that U U = I = V V. (4.17) We now rotate the the orthonormal bases (4.2) in H + and H - to obtain u := U  ¯ and v := V  respectively. Note that E {uv } =  . What makes these orthonormal bases useful is that they are adapted to the orthogonal decompositions6 H -  H + = [H -  (H + ) ]  H  [H +  (H - ) ], (4.18)

where H := X-  X+ is the so-called frame space (Lindquist and Picci (1985, 1991), in the sense that X- = span{v1 , v2 , . . . , vn } X+ = span{u1 , u2 , . . . , un }. This is true since X- is precisely the subspace of random variables in H - having nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H - . Since therefore {vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H -  (H + ) and H +  (H - ) , respectively, these spaces will play no role in what follows. Now define the n-dimensional vectors  1/2   1/2  1 u1 1 v1   1/2 u    1/2 v  2 2   1 1 z ¯ =  2 .  = 1/2 U L- (4.19) z =  2 .  = 1/2 V L- - y- + y+ . .  .   .  n vn
1/2

n un

1/ 2

¯ is a basis in X+ , and they From what we have seen before, z is a basis in X- and z have the properties ¯z ¯ }. E {zz } =  = E {z (4.20)

In fact, we even have more as seen from the following amplification7 of a theorem by Desai and Pal (1984) (Theorem 1). Theorem 4.4. The basis vectors x- (0) = z x ¯+ (0) = z ¯ (4.21)

in X- and X+ respectively belong to the same uniform choice of basis, i.e. to the ¯ ), and in this uniform choice same choice of triplets (A, C, C ¯+ . P- =  = P
6 7

(4.22)

The symbols  and  denote vector sum and orthogonal vector sum of subspaces. ¯ ). A priori there is no reason why choosing bases in X- and X+ would lead to the same (A, C, C This important property is explicitly mentioned in Theorem 4.4.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coefficients {1 , 2 , 3 , . . . , n } are distinct, this is, modulo multiplication with a signature matrix 8 , the only uniform choice of bases for which ( 4.22) holds. ¯ ) is know as stochastically balanced, and, in the case of Such a choice of (A, C, C distinct canonical correlation coefficients, it defines a canonical form with respect to state space isomorphism in (1.6) by fixing the sign in, say, the first element in each row of C . Such canonical forms have also been studied by Ober (1991). Proof. It follows from (4.4) and (4.15) that E {z ¯z } = 2 . (4.23) ¯ ) so that x ¯, and let the bases in the other splitting Now, choose (A, C, C ¯+ (0) = z subspaces be chosen accordingly so that the choice of bases is uniform. We want ¯+ (0) and that to show that x- (0) = z . To this end, first note that x+ (0) = -1 x x- (0) = E X- x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula and the fact that z is a basis in X- , ¯z }-1 z, x- (0) = -1 E {z which, in view of (4.23), yields x- (0) = z as claimed. Hence (4.22) follows from (4.20). ¯ ) is another uniform choice of bases which Next, suppose that (QAQ-1 , CQ-1 , CQ is also stochastically balanced. Since then x- (0) = Qz and, as is readily seen from ¯+ = Q-T Q-1 , ¯ so that P- = QQ and P the backward system (3.14), x ¯+ (0) = Q-T z (4.22) yields QQ =  and Q-T Q-1 = , from which we have Q2 = 2 Q. Since  has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959) that there is a scalar polynomial (z ) such that Q = (2 ). Hence Q is diagonal and commutes with  so that, by QQ = , we have QQ = I. Consequently, since Q is diagonal, it must be a signature matrix. In view of (4.21) and (3.16), the first of relations (4.9) and (4.8) respectively yield -1 -1 ¯ T- y- z ¯ =  T+ y+ . (4.24) z= Consequently, in view of (4.20), (2.15) holds also for the case of an infinite Hankel matrix. This can of course also be seen from (4.11). Note that the normalization of the block Hankel matrix H is necessary in order for the singular values to become the canonical correlation coefficients, i.e., the singular values of H. In fact, if we were to use the unnormalized matrix representation (4.3) of H instead, as may seem simpler and more natural, the transpose of (4.3) would not be the matrix representation of H in the same bases, a property which is crucial in the singular value decomposition above. This is because (4.3) corresponds to the bases y- in H - and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the finite-dimensional case studied in ^ , defined in Section 2, is Section 2, and therefore the normalized Hankel matrix H preferable to the unnormalized H . ¯ in terms of the Hankel matrix H , can Formulas, such as (2.5), expressing A, C, C be easily derived from basic principles. In fact, standard calculations based on the forward model (3.7) and the backward model (3.14) yield A = E {x(1)x(0) }P -1 C = E {y (0)x(0) }P -1 , ¯ -1 = E {y (-1)x(0) } ¯ = E {y (-1)¯ C x(0) }P for any dual pair of bases x(0) and x ¯(0). Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases (4.19) can be computed by means of the formulas
1 -T - 1/ 2 , A = -1/2 U L- +  (H )L- V  T - 1/2 C = 1 (H )L- , - V - T - 1 / 2 ¯ = 1 (H )L+ U  C ,

(4.25a) (4.25b) (4.25c)

(4.26a) (4.26b) (4.26c)

where H is the unnormalized Hankel matrix (4.3),  (H ) is obtained from H by deleting the first block row, and 1 (H ) is the first block row. Proof. First, in (4.25a) and (4.25b), we take x(0) to be x- (0). By the Kalman filter representation a [x+ (1) - x- (1)]  UH -  H - for all a  Rn ,
-1 ¯+ E {x ¯+ (1)x- (0) }. E {x- (1)x- (0) } = E {x+ (1)x- (0) } = P

¯ ) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19), But (A, C, C 1 1 ¯+ , x- (0) = 1/2 V L- ¯+ (1) = 1/2 U L- P- =  = P - y- and x +  (y+ ), where  (y+ ) is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),
1 -T - 1/ 2 , A = -1/2 U L- + E { (y+ )y- }L- V 

which is identical to (4.26a). Likewise, from (4.26b),
T - 1/ 2 , C = E {y (0)y- }L- - V

which yields (4.26b). Finally, taking x ¯(0) to be x ¯+ (0) in (4.25c), a symmetric argument yields (4.26c). Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from finite covariance data In this section we modify the realization theory of Section 4 to the case that only a finite segment {y (0), y (1), y (2), . . . , y ( )}, (5.1) of the time series {y (t)} is available. We still define each y (t) as the semi-infinite string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact partial covariance sequence {0 , 1 , 2 . . . ,  }. (5.2)

The corresponding realization problem, which is purely theoretical and is intended to prepare for the more realistic identification situation with finite strings of observed data (Section 6), is therefore the partial stochastic realization problem mentioned in Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the output of some minimal "true" system (3.7) of dimension n and that  is large enough for n to equal the positive degree of the partial sequence (5.2). Now, suppose that  = 2 - 1, and partition the data into two matrices     y (0) y ( )  y (1)   y ( + 1)  - +   , = y = (5.3) y . .      . . . . y ( - 1) y (2 - 1) representing the past and the future respectively, and define the corresponding (finite- + and y respectively as dimensional) subspaces Y- and Y+ spanned by the rows of y explained in Section 3. Since the data size  will be important in the considerations that will follow, we denote the finite block Hankel matrix H of Section 2, relative to the data (5.3), by H , i.e.,
+ - H = E {y (y ) }.

(5.4)

Let 0 be the smallest integer  such that rank H = n. It is well-known that 0 is ¯ ), so n is an the maximum of the observability and constructibility indicies of (A, C, C upper bound for 0 . As pointed out in the beginning of Section 2, we need  > 0 to ¯ ). be certain that the factorization of H yields a unique (A, C, C Next we shall consider the class of minimal splitting subspaces for Y- and Y+ , i.e., the subspaces X admitting a canonical factorization Y+  O of the finite-interval Hankel operator H := E Y |Y+ .
-  - Y- C X

H

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces, ^  - = E Y- Y+ X ^  + = E Y+ Y- , and X

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred to Appendix D. Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary time series {y (t)}. Then, if  > 0 , X := U X is a minimal splitting subspace for Y- and Y+ , and ^  - = E Y- X , X ^  + = E Y+ X . X (5.7) (5.6)

^  - has a unique representation9 Conversely, any basis x ^( ) in X x ^( ) = E Y x( ),
-

(5.8)

^  + has a unique representation ^ where x( ) is a basis in X , and any basis x ¯( ) in X ^ x ¯( ) = E Y x ¯( ),
+

with x ¯( ) a basis in X . As X varies over the family of all minimal Markovian splitting subspaces, the corresponding x(0) [¯ x(0)] constitute a uniform choice of bases. ^ - The stochastic realizations corresponding to the finite-interval predictor spaces X ^  + are nonstationary. However, taking advantage of the representations (5.8) and X and (5.9), we shall be able to express these realizations in such a way that they can ¯ ) corresponding to one uniform be parameterized by the stationary triplet (A, C, C choice of bases, both for the forward and the backward settings. In fact, if the bases ^ x ^( ) and x ¯( ) are chosen so that x( ) and x ¯( ) in representations (5.8) and (5.9) are ¯ ) is used for x( )} = I , then the same choice of (A, C, C dual bases in X , i.e., E {x( )¯ ^  + is called coherent. ^  - and X all X  . Such a choice of bases in X The realizations generated by these coherent bases are precisely the (transient) forward and backward Kalman filters. In fact, the vector x ^( ) is the one-step predictor of x( ) based on Y- and, as shown in Appendix C, it evolves in time as the Kalman filter

X

(5.9)

X

x ^(t + 1) = Ax ^(t) + K (t)[y (t) - C x ^(t)]; where the gain K (t) is given by

x ^(0) = 0,

(5.10)

¯ - AP- (t)C )(0 - CP- (t)C )-1 K (t) = (C and the filter estimate covariance ^(t)^ x(t) } P- (t) = E {x is the solution of the matrix Riccati equation

(5.11)

(5.12)

¯ - AP- (t)C )(0 - CP- (t)C )-1 (C ¯ - AP- (t)C ) P- (t + 1) = AP- (t)A + (C P- (0)) = 0. (5.13)
With slight misuse of notations, the orthogonal projection operator applied to a vector will denote the vector of the projections of the components.
9

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the components of ^ ¯( ) x ¯( ) = E Y x ^  + and are generated by the backward Kalman filter form a basis in X ¯ (t)[y (t - 1) - C ¯x ^ ^ ^ x ¯(t - 1) = A x ¯(t) + K ¯(t)]; with ¯+ (t)C ¯ )(0 - CP ¯ - (t)C ¯ )-1 , ¯ (t) = (C - A P K where ¯+ (t) = E {x ^ ^ P ¯(t)x ¯(t) } is obtained by solving the matrix Riccati equation ¯+ (t)A + (C - A P ¯+ (t)C ¯+ (t)C ¯ )(0 - C ¯P ¯+ (t)C ¯ )-1 (C - A P ¯) ¯+ (t - 1) = A P P ¯+ (2 - 1) = 0. P (5.18) Now, it is well-known that both ^(t)]  (t) = (0 - CP- (t)C )-1/2 [y (t) - C x and ¯P ¯+ (t)C ¯ )-1/2 [y (t - 1) - C ¯x ^  ¯(t) = (0 - C ¯(t)] (5.20) (5.19) (5.17) (5.16) ^ x ¯(2 - 1) = 0, (5.15)
+

(5.14)

are normalized white noises, called the forward respectively the backward (transient) innovation processes. Consequently, we may write the Kalman filter (5.10) as x ^(t + 1) = Ax ^(t) + B- (t) (t) y (t) = C x ^(t) + D- (t) (t) (5.21)

where D- (t) := (0 - CP- (t)C )1/2 and B- (t) := K (t)D- (t). Likewise, the backward Kalman filter (5.10) may be written ¯+ (t)¯ ^ ^ ¯(t) + B x ¯(t - 1) = A x  (t - 1) ¯ ¯ ^ y (t - 1) = C x ¯(t) + D+ (t)¯  (t - 1) (5.22)

¯ + (t) := (0 - C ¯P ¯+ (t)C ¯ )1/2 and B ¯+ (t) := K ¯ (t)D ¯ + (t). Comparing with (3.7) where D and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7) and (3.14) are time-varying and describe the output y only on the interval [0, 2 - 1]. In fact, since ^(t)][x(t) - x ^(t)] }  0, P - P- (t) = E {[x(t) - x ¯ ¯ and, for the same reason, P - P+ (t)  0, we have ¯+ (t)-1 , P- (t)  P  P+ (t) := P (5.23)

^  - and X ^  + are extremal splitting subspaces, so we see that the predictor spaces X just as X- and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the finite-interval counterparts of equations (4.25) are given by A = E {x ^( + 1)^ x( ) }P- ( )-1 C = E {y ( )^ x( ) }P- ( )-1 , ¯+ ( )-1 = E {y ( - 1)^ ¯ = E {y ( - 1)x ^ x( ) } C ¯( ) }P (5.24a) (5.24b) (5.24c)

In complete analogy with the stationary framework in Section 4, the canonical correlation coefficients 1  1 ( )  2 ( )  · · ·  n ( ) > 0 (5.25) between the finite past Y- and the finite future Y+ are now defined as the singular values of the operator H given by (5.5). To determine these we need a matrix representation of H in some orthonormal bases. Using the pair (5.19)­(5.20) of transient innovation processes for this purpose, we obtain the normalized matrix (2.14), which ^  . Singular value decomposition yields we shall here denote H ^  = U  V , H (5.26) where U U = I = V V , and  is the diagonal matrix of canonical correlation coefficients. As in Section 4 it is seen that
-1 - z ( ) =  V (L-  ) y 1/2 -1 + z ¯( ) =  U (L+  ) y 1/2

(5.27)

^  - and X ^  + respectively and that are bases in X E {z ( )z ( ) } =  = E {z ¯ z ¯ }. (5.28)
+ Here L-  and L are the finite-interval counterparts of L- and L+ respectively, and they are of course submatrices of these. Note that H , as defined by (5.4), is now given by - ^ H  = L+  H (L ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z ( ) and z ¯( ) are coherent bases, and the ¯ corresponding triplet (A, C, C ) is a finite-interval stochastically balanced realization, i.e., ¯+ ( ). P- ( ) =  = P (5.30)

The following finite-interval modification of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the finite block hankel matrix H , and the proof is analogous. ¯ ), obProposition 5.2. The finite-interval stochastically balanced triplet (A , C , C ^ tained from (5.24) by choosing the bases x ^( ) = z ( ) and x ¯( ) = z ¯( ), is given by
1 /2 -1 - -T 1/ 2 U (L+ V - , A = -   )  (H )(L )  -T 1/ 2 V - , C = 1 (H )(L-  )  + -T - 1 /2 ¯ C = 1 (H )(L ) U  ,

(5.31a) (5.31b) (5.31c)

where the operators  (·) and 1 (·) are defined as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

¯ ) actually varies with  , but that, for each  , it Note that the triplet (A , C , C ¯ ) of Section 4, i.e., there is a is similar to the stochastically balanced triplet (A, C, C nonsingular matrix Q so that ¯ ) = (Q AQ-1 , CQ-1 , CQ ¯ ). (A , C , C    (5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the stationary predictor spaces X- and X+ will have the state covariances P- = Q Q
T -1 ¯+ = Q- and P  Q ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state covariances are not diagonal and equal is a manifestation of the fact that the triplet ¯ ) is not stochastically balanced in the sense of Section 4. It is well known (A , C , C ¯+ , respectively, as t  , and ¯+ (t) tend monotonically to P- and P that P- (t) and P therefore we have the following ordering ¯+ )-1  (P ¯+ ( ))-1 := -1 . P- ( ) :=   P-  (P


Since the number n of nonzero singular values (5.25) is in general too large too yield a reasonable model, we must consider what happens when some of the smallest singular values are set equal to zero. The truncation procedure employed by van Overschee and De Moor (1993) is equivalent to the principal subsystem truncation presented in Section 2, except that, and this is very important, the singular-value ^  , which is the decomposition is performed on the normalized block Hankel matrix H natural matrix representation of the operator  . It will be shown in Section 7 that such a truncation will preserve positivity in the stationary case (Theorem 7.3). In order to carry this result over to the case of finite  , we need to assume that the spectral density  of the time series {y (t)} is coercive so that Assumption 3.2 is fulfilled, i.e., that the function Z is strictly positive real. The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D, shows that principal subsystem truncation preserves positivity provided  is chosen large enough.

H

Theorem 5.3. Suppose that the spectral density  of the time series {y (t)} is coercive. Then, there is an integer 1 > 0 such that, for   1 , the principal subsystem ¯ )1 ) of (A , C , C ¯ ) is a minimal realization of a strictly truncation ((A )11 , (C )1 , (C positive real function (2.13). 6. Subspace identification The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have access to an infinite sequence (3.2) of data. In reality we will have a finite string of observed data {y0 , y1 , y2 , . . . , yN }, (6.1)

where, however, N may be quite large. More specifically, we assume that N is sufficiently large that replacing the ergodic limits (1.11) by truncated sums yields good approximations of {0 , 1 , 2 . . . ,  }, (6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course,  << N . This is equivalent to saying that T := N -  is sufficiently large for 1 T +1
T

a yt+k yt+j b
t=0

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we shall use the finite-interval realization theory of Section 5 as if we had a finite time series {y (0), y (1), y (2), . . . , y ( )}, while substituting the semi-infinite string (3.3) of data by y (t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . ,  . (6.5) (6.4)

In particular, in this case the inner product becomes merely that of a finite-dimensional Euclidean space so that the block Hankel matrix H can be written H = where   y  -1 y  . . . yT + -1 y -2 y -1 . . . yT + -2  -  y = . . ..  . . . . . . .  . y0 y1 . . . yT 1 y + (y - ) T +1    y +1 . . . yT + y  y +1 y +2 . . . yT + +1  + . = and y . . ..  . . . . . . .  . y2 -1 y2 . . . yT +2 -1 

Consequently, the identification of a minimal stationary state-space innovation model describing the data (6.1) can be performed in the following steps. - + , y to obtain, from (1) Perform canonical correlation analysis on the data y ^ ¯+ ( ) = z ¯( ) and, from (5.26), the (5.27), the state vectors x ^- ( ) = z ( ) and x corresponding common state covariance matrix  , i.e., the diagonal matrix of the (finite interval) canonical correlation coefficients (5.25). (2) Given the singular value decomposition (5.26), compute via (5.31) a minimal ¯ ). This realization will be in finite-interval balanced form, realization (A, C, C i.e., (5.30) will hold instead of (4.22). (3) To obtain a state space model (3.7) for y we need to compute the matrices B ¯ 0 ) defines and D. Note that such matrices will exist if and only if (A, C, C, a positive real function (1.6), or, in other words, if and only if there is a symmetric positive definite P = P such that M (P ) := P - AP A ¯ - CP A C ¯ - AP C C 0 - CP C  0. (6.6)

[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6), B and D can be determined (in a nonunique way) by a full rank factorization of M (P ), i.e., B D B D = M (P ). (6.7)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P- = E {x- (t)x- (t) } has been determined. Obtaining P- amounts to finding the minimal solution of the algebraic Riccati equation ¯ - AP C )(0 - CP C )-1 (C ¯ - AP C ) P = AP A + (C (6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t   with initial condition P- ( ) =  . (The corresponding dual procedures yield ¯+ .) Again, in both cases, a positive definite P- can be found if and only P ¯ 0 ) defines a positive real function (1.6). In fact, in general, if (A, C, C, {P- (t)}t0 may not even converge unless this positivity condition is fulfilled and may in fact exhibit dynamical behavior with several of the characteristics of chaotic dynamics (Byrnes et al., 1991, 1994). Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that, for  fixed but sufficiently large (see Section 2), we will have rank H = n as T  , ¯ ) will be uniquely determined from the data and similar to the and the triplet (A, C, C ¯ triplet (A, C, C ) of the "true" generating system. Hence, in particular, in the limit as T  , at least in theory positivity will be guaranteed. If n ^ is an upper bound for the order of the "true" system, we may choose  to be any integer larger than n ^. In practice, however, T is finite, and even if we had a true system generating exact data, the spectral estimate T , although converging to the true spectrum  as T   may in principle fail to be positive for any finite T if there are frequencies  for which (ei ) = 0. Positivity for a suitably large T can however be guaranteed if the "true" spectrum is coercive. The following proposition, which also applies to Aoki's method discussed in Section 2, is proved in Appendix D. Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are ful¯ ) defined by filled. Then, there is a T0  Z+ such that, for T  T0 , the triplet (A, C, C (5.31) yields a function (1.6) which is strictly positive real. However, in practice, rank H normally will keep increasing with  , even for very large T , so that one must resort to some kind of truncation of the Hankel singular values. As we have pointed out in Section 5, setting all canonical correlation coefficients r+1 ( ), r+2 ( ), . . . equal to zero for some suitable r, as is done in, for example, van Overschee and De Moor (1993), is equivalent to principal subsystem truncation. An important issue is therefore under what conditions such a procedure will insure positivity. Here we must distinguish between problems generated by the sample fluctuations of the data due to finite sample size T , as considered in Proposition 6.1, and the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an infinite string of data generated by a "true" high-dimensional system, such a truncation procedure may fail if  is smaller than that dimension. Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following result, which justifies this approximation procedure, provided the rather stringent Assumption 2.1 holds and we have coercivity, and provided T and  are sufficiently large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulfilled. Then, there are positive integers T0 and 1 > 0 such that, for T  T0 and   1 , the ¯1 ), obtained from (2.12) by taking H := H in (2.10), is a minimal triplet (A11 , C1 , C realization of a strictly positive real function (2.13). We note that, in van Overschee and De Moor (1993), the large Hankel matrix
+ + + + - - - - ~  = (y ) (E {y (y ) })-1 E {y (y ) }(E {y (y ) })-1 y H

^  . This leads to a procedure which is equivalent to the one is used in place of H described above. Moreover, the computation of a second singular-value decomposition + - in van Overschee and De Moor (1993), based on H +1 := E {y +1 (y +1 ) }, together with a subsequent change of bases, is actually redundant, as can be deduced from the following proposition. In fact, a considerable amount of computation is needed in van Overschee and De Moor (1993) to compensate for the fact that taking z ( + 1), ^ ( +1)- would computed from a second singular-value decomposition, as a basis in X lead to a Kalman filter model with time-varying parameters. ^ Proposition 6.3. To each coherent pair of bases x ^( ) and x ¯( ) in the finite-interval ^ ^ predictor spaces X - and X + , there corresponds a minimal factorization ¯ H =   of the block Hankel matrix H . Here
+  x ^( ) = E Y y
- + - ¯x ^ ¯( ) = E Y y  .

(6.9)

and

(6.10)

Conversely, given a minimal factorization (6.9), ¯ (T - )-1 y - x ^( ) =     and
+ ^ x ¯( ) =  (T+ )-1 y

(6.11)

^ +. ^  - and X is a coherent pair of bases in X ^  - and X ^  + . Then, for ^ Proof. Let x ^( ) and x ¯( ) be a coherent choice of bases in X ¯( )) of dual bases any X as defined in Theorem 5.1, there is a unique pair (x( ), x ¯  be the matrices defined via such that (5.8) and (5.9) hold. Let  and 
+ - ¯x E X y =  x( ) and E X y = ¯( ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of X with respect to Y- and Y+ yields + - + - (y ) } = E {E X y (E X y )) }, E {y which, in view of (6.12), is the same as (6.9). Applying E Y and E Y to respectively the first and second equations of (6.12), the splitting property yields (6.10). As for the converse statement, equations (6.11) follow from the construction in the proof of Theorem 5.1, from which it also follows that the resulting bases x ^( ) and ¯ ^ x ¯( ) are constructed from the same (A, C, C ) and therefore coherent. ¯ ) have been fixed by a particular choice of x( ) As soon as the parameters (A, C, C in the representation (5.8) in Theorem 5.1, we must choose x ^( + 1) as x ^( + 1) = E Y +1 Ux( ) (6.13)
- +

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More specifically Proposition 6.3 ¯  are uniquely determined once x( ) has been selected. Hence implies that  and  ¯ ) is uniquely determined by the Ho-Kalman algorithm so that (A, C, C ¯ ¯  +1 = C  ¯  A is prescribed, as is
-1 - ¯  (T- x ^( + 1) =  +1 ) y +1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining x ^( + 1) by an extra singular-value decomposition, as in van Overschee and De Moor (1993), is actually redundant. If we actually were to determine x ^( + 1) as described -L ¯ ¯ above, we would better compute  +1 from  +1 =  H +1 , where the left inverse is very easily obtained from the singular-value decomposition of H . We stress that Assumption 2.1, although quite limiting, is absolutely crucial in insuring that the subspace identification algorithms mentioned above will actually work. Note that for generic data these algorithms may break down for any fixed  . The same is true for all other subspace methods which deal with identification of covariance models (or equivalent) involving stochastic signals. On the other hand, Assumption 2.1 introduces a quite unrealistic condition which, as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure to estimate T0 and 1 in Proposition 6.2, as the proof is based only on continuity arguments. 7. Stochastic model reduction As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step in order to keep the dimension of the model at a reasonable level. To justify any such procedure one must either assume that there is an underlying "true" system of sufficiently low order, i.e., invoke Assumption 2.1, or to perform rational covariance extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995), Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an infinite one. The latter can be done in many ways, one of which is the maximum entropy extension. In either case, the truncation problem is equivalent to approximating a positive real matrix function ¯ + 1 0 , Z (z ) = C (zI - A)-1 C 2 (7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of lower degree. In this section we shall investigate how this can be done and also how such an approximation affects the canonical correlation structure. One main question to be addressed is whether the principal subsystem truncation (2.11) preserves positive realness and balancing, and hence the leading canonical correlation coefficients, as originally claimed by Desai and Pal (1982). As it turns out, the answer is affirmative to the first but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identification approximation obtained by setting some canonical correlation coefficients equal to zero. It is instructive to first consider the continuous-time counterpart of this problem since the latter is simpler and exhibits more desirable properties. Also, it has been widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true. It is well-known [see, e.g., Faurre et al. (1979)] that an m × m matrix function Z with minimal realization ¯ + 1 R, (7.2) Z (s) = C (sI - A)-1 C 2 is positive real with respect to the right half plane if and only if there is a symmetric matrix P > 0 such that ¯ - PC -AP - P A C  0, (7.3) M (P ) := ¯ C - CP R where here we assume that R is positive definite and symmetric. In this case there are two solutions of (7.3), P- and P+ , with the property that any other solution of (7.3) satisfies P -  P  P+ . (7.4)

These extremal solutions play the same role as P- and P+ in the discrete-time setting, and rank M (P- ) = m = rank M (P+ ). (7.5)

-1 ¯+ := P+ If the state-space coordinates are chosen so that both P- and P are diagonal and equal, and thus, by (4.14), equal to the diagonal matrix  of canonical correlation ¯ ) is stochastically balanced. coefficients, we say that (A, C, C Now, suppose that  is partitioned as in (2.8) with r+1 < r , and consider the corresponding principal subsystem truncation (2.12). Using the stochastic realization framework, Harshavaradana, Jonckheere and Silverman (1984) showed that

¯1 + 1 R, Z1 (s) = C1 (sI - A11 )-1 C 2

(7.6)

¯1 ) is a minimal realization of a positive real function and conjectured that (A11 , C1 , C is stochastically balanced. We shall next show that this conjecture is true, as has already been done by Ober (1991) in a framework of canonical forms. First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield   ¯1 - 1 C1 -A11 1 - 1 A11  C   0,     (7.7) ¯  R C1 - C1 1 where blocks which play no role in the analysis are marked by an asterisk. Consequently, M1 (1 ) = ¯1 - 1 C1 -A11 1 - 1 A11 C  0. ¯ C1 - C1 1 R (7.8)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982), Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane, ¯1 ) is a minimal realization. (7.6) is positive real, but it remains to prove that (A11 , C1 , C This was done in Harshavaradhana et al. (1984). It is important to observe here that, contrary to the situation in the discrete-time setting, rank M1 (1 ) = rank M () = m 1 -1 and rank M1 (- 1 ) = rank M ( ) = m, important facts that will be seen to imply that the reduced system is stochastically balanced. Recall that in the continuous-time setting the spectral density (s) = Z (s)+ Z (-s) is coercive if, for some > 0, we have (s)  I for all s on the imaginary axis. This is equivalent to the condition that R > 0 and  has no zeros on the imaginary axis (Faurre et al., 1979, Theorem 4.17). Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with (s) := ¯ ) be in stochastically balanced form. Then, Z (s) + Z (-s) coercive, and let (A, C, C ¯1 ) defines a positive real function (7.6) if r+1 < r , the reduced system (A11 , C1 , C for which it is a minimal realization in stochastically balanced form, and 1 (s) := Z1 (s) + Z1 (-s) is coercive. Proof. We have already shown that Z1 is positive real, and we refer the reader to ¯1 ) is a minimal realization Harshavaradhana et al. (1984) for the proof that (A11 , C1 , C ¯1 ) is stochastically of Z1 . It remains to show that 1 is coercive and that (A11 , C1 , C -1 balanced, i.e., that P1- = 1 = P1+ , where P1- and P1+ are solutions to the algebraic Riccati equation ¯ - P1 C1 )R-1 (C ¯ - P1 C1 ) = 0 A11 P1 + P1 A11 + (C (7.9) such that any other solution P1 of (7.9) satisfies P1-  P1  P1+ . To this end, 1 -1 note that since M1 (1 ) and M1 (- 1 ) have rank m, both 1 and 1 satisfy (7.9). 1 Therefore, as is well-known (Molinari, 1977) and easy to show, Q := - 1 - 1 satisfies 1 Q + Q1 + QC1 R-1 C1 Q = 0, where ¯ - 1 C )R-1 C1 . 1 = A11 - (C 1 (7.11) Since  is coercive, -1 -  = P+ - P- > 0 (Faurre et al., 1979, Theorem 4.17) so that 1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to 1 Q-1 + Q-1 1 + C1 R-1 C1 = 0. (7.12) (7.10)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , 1 ). Since, in addition, the Lyapunov equation (7.12) has a positive definite solution Q-1 , 1 must be a stability matrix. Therefore 1 is the minimal (stabilizing) solution P1- of -1 ¯1+ := P1+ = 1 . (7.9). In the same way, using the backward setting, we show that P ¯1 ) is stochastically balanced. Since P1+ - P1- > 0, 1 is Consequently, (A11 , C1 , C coercive. ¯ 1 0 ) Let us now return to the discrete-time setting. Let us recall that, if (A, C, C, 2 is a minimal realization of (7.1), the matrix function Z is positive real if and only if the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given the positive real rational function (7.1) with the property that (z ) = Z (z ) + Z (z -1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y , the state covariance P of any minimal stochastic realization (3.7) of y satisfies (6.6) and the matrices B, D in (3.7) satisfy (6.7). Consequently, as pointed out in Section 5, the matrices B and D can be determined via a matrix factorization of M (P ) once P has been determined. ¯ ) is in stochastically balanced form, Theorem 4.4 implies that Now, if (A, C, C M ()  0. In view of (4.16) and (2.12), M () may be written   ¯1 - A11 1 C1 - A12 2 C2 1 - A11 1 A11 - A12 2 A12  C ,     ¯ C1 - C1 1 A11 - C2 2 A12  0 - C1 1 C1 - C2 2 C2 where, as before, the blocks which do not enter the analysis are marked with an asterisk. Since M ()  0, this implies that M1 (1 ) - where M1 (1 ) = ¯1 - A11 1 C1 1 - A11 1 A11 C ¯ C1 - C1 1 A11 0 - C1 1 C1 (7.14) A A12 2 12 C2 C2  0, (7.13)

¯1 ). Thereis the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , C fore, M (1 )  0, so if we can show that A11 is stable, i.e., has all its eigenvalues strictly inside the unit circle, it follows that ¯1 + 1 0 , (7.15) Z1 (z ) = C1 (zI - A11 )-1 C 2 is positive real. As we shall see below this is true without the requirement needed in continuous time that r+1 < r . ¯1 ) also to be balanced, 1 would have to be the minimal solution P1- For (A11 , C1 , C of M1 (P1 )  0, which in turn would require that rank M1 (1 ) = rank M () = m. Due to the extra positive semidefinite term in (7.13), however, this will in general not be the case and therefore 1  P1- will correspond to an external realization, as will 1 - 1  P1+ ; see Lindquist and Picci (1991). ¯1 ) is minimal we need to assume that  is coercive, or, To show that (A11 , C1 , C equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979, Theorem A4.4) that this implies that P+ - P- > 0. (7.16)

In fact, if 0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to coercivity. Coercivity also implies that 0 - CP- C > 0. (7.17)

¯ ) in balanced form, P- =  = P ¯+ and, in view of (3.16), Remark 7.2. With (A, C, C -1 -1 P+ =  . Hence (7.16) becomes  > , which obviously holds if and only if 1 < 1, which in turn is equivalent to H -  H + = 0. Consequently, given the full rank condition 0 > 0, coercivity is equivalent to the past and the future spaces of y having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

¯ ) be in stochastically balTheorem 7.3. Let (7.1) be positive real, and let (A, C, C anced form. Then the reduced-degree function (7.15) obtained via principal subsystem decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so ¯1 , 1 0 ) is a minimal realization of Z1 . is Z1 , and (A11 , C1 , C 2 For the proof we need the following lemma, the proof of which is given in Appendix D. Lemma 7.4. Let the matrix function Z be given by (7.1), where 0 > 0, but where ¯ A ) are not necessarily observable, and suppose that (6.6) has two (C, A) and (C, positive definite symmetric solutions, P1 and P2 , such that P2 - P1 > 0. Then Z is strictly positive real. Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is stable. To this end, we note that P is the reachability gramian of (3.7). In particular, ¯ ) is stochastically balanced, the reachability gramian of the system (3.18) if (A, C, C equals  so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable. By Remark 7.2, coercivity of  implies that -1 -  > 0, from which it follows 1 that - 1 - 1 > 0 and that 0 > 0. Moreover, By construction, M1 (1 )  0 and -1 M1 (1 )  0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is. To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows by symmetry. By regularity condition (7.17), 0 - C1 1 C1  0 - C C > 0, and consequently, since M1 (1 )  0, 1 satisfies the algebraic Riccati inequality ¯1 - A11 P1 C1 )(0 - C1 P1 C1 )-1 (C ¯1 - A11 P1 C1 )  0, (7.19) A11 P1 A11 - P1 + (C but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable. Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density 1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a unique symmetric P1- > 0 which satisfies (7.19) with equality and for which ¯1 - A11 P1- C1 )(0 - C1 P1- C1 )-1 C1 1- := A11 - (C is stable. It is well-known (Faurre et al., 1979) that P1- is the minimal symmetric solution of the linear matrix inequality M1 (P1 )  0, i.e., that any other symmetric 1 -1 solution P1 satisfies P1  P1- . We also know that M1 (- 1 )  0. Next, since 1 - 1 1 > 0, a fortiori it holds that Q := - 1 - P1- > 0. A tedious but straight-forward calculation shows that Q satisfies 1- (Q-1 - C1 R-1 C1 )-1 1- - Q  0, from which it follows that Q-1 - C1 R-1 C1 - 1- Q-1 1-  0. Cf. Faurre et al. (1979), pp. 85 and 95. (7.20) (7.18)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a  and a   C such that [C1 , I - A11 ]a = 0. and therefore, in view of (7.20), (1 - ||2 )a Q-1 a  0.

Cr

But  is an eigenvalue of the stable matrix A11 , implying that || < 1, so we must have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable. A remaining question is whether there is some balanced order-reduction procedure in discrete time which preserves both positivity and balancing. That this is the case in continuous time implies that the answer is affirmative, but the reduced system cannot be a simple principal subsystem truncation. ¯ ) be in stochastically Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, C balanced form. Moreover, given a decomposition ( 2.12) such that r+1 < r , let Ar Cr ¯r C r0 = = = = A11 - A12 (I + A22 )-1 A21 C1 - C2 (I + A22 )-1 A21 ¯1 - C ¯2 (I + A22 )-1 A12 C ¯2 - C ¯2 (I + A22 )-1 C2 0 - C2 (I + A22 )-1 C

¯r , r0 ) is a minimal realization of a strictly positive real function Then (Ar , Cr , C ¯r + 1 r0 . Zr (z ) = Cr (zI - Ar )-1 C 2 (7.21)

¯r , r0 ) is stochastically balanced with canonical correlation coeffiMoreover, (Ar , Cr , C cients 1 , 2 , . . . , r . To understand why this reduced-order system does preserve both positivity and balancing, note that for   I -A12 (I + A22 )-1 0 I 0 T = 0 -1 I 0 -C2 (I + A22 ) we obtain   ¯r - Ar 1 Cr 1 - Ar 1 Ar  C ,    T M ()T =  ¯r - Cr 1 A  r0 - Cr 1 C C r r

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the reduced-order system, Mr (1 )  0 and rank Mr (1 )  rank M (). ¯r , r0 ) is precisely what one obtains To prove Theorem 7.5 we observe that (Ar , Cr , C ¯ 0 ) by the appropriate linear fractional transform to the if one transforms (A, C, C, continuous-time setting and then, after reduction, back to discrete time again as suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions The purpose of this paper is to analyze a class of popular subspace identification procedures for state space models in the theoretical framework of rational covariance extension, balanced model reduction, and geometric theory for splitting subspaces. We have pointed out that these methods are based on the hidden Assumption 2.1 which is not entirely natural and which is in general untestable. The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially equivalent to the Ho-Kalman algorithm in which the basic factorization is performed by singular-value decomposition of a block Hankel matrix of finite covariance data, as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in orthonormal coordinates and allows for theoretical verification of the truncation step. A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of the data equals the algebraic degree. To achieve this, one must assume that the data are generated exactly by an underlying system and that the amount of data is sufficient for constructing an accurate partial covariance sequence the length of which is sufficient in relation to the dimension of the underlying system. Hence it is absolutely crucial that a reliable upper bound of the dimension of the "true" underlying system is available. We stress that these stringent assumptions are not satisfied for generic data, as was pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that the positive degree has no generic value. In fact, just for the moment considering the single-output case, for each p such that r  p   there is a nonempty open set of partial covariance sequences having positive degree p in the space of sequences of length  . Secondly, for any r, it is possible to construct examples of long partial covariance sequences having algebraic degree r but having arbitrarily large positive degree (Theorem 2.4). In Section 7 we proved an open question concerning the preservation of positivity in the original (discrete-time) model reduction procedure of Desai and Pal (1984). Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to the principal subsystem truncation used in van Overschee and De Moor (1993), but not to the one in Aoki (1990). We prove that positivity is preserved provided that the original data satisfies Assumption 2.1, justifying setting the smaller canonical correlation coefficients equal to zero. Unlike the situation in continuous time, this truncation does not preserve balancing. The validity of the corresponding procedure of Aoki (1990) has not been settled. The contribution of this paper is to provide theoretical understanding of these identification algorithms and to point out possible pitfalls of such procedures. Hence the primary purpose is not to suggest alternative procedures. Nevertheless, we would like to point out that a two-stage procedure equivalent to covariance extension followed by model reduction would work on any finite string of data, thus elimination the need for Assumptions 2.1. However, we leave open the question of how such a procedure should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension or some other. Acknowledgment. We would like to thank the referees and the associate editor for the careful review of our paper and for many useful suggestions, which have led to considerable improvements of this paper. References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31­73. 2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner. 3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley. 4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM J. Control, 13, 162­173. 5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag. 6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems and Control Letters, 2, 2301­2312. 7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization of modelling filter. Inter. J. of Control, 50, 2301­2321. 8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted for publication. 9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40. 10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in Kalman filtering. IEEE Trans. Autom. Control, 36, 563­579. 11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast filtering algorithms. SIAM J. Control and Optimization, 32, 744­789. 12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105­1112. 13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE Trans. Automatic Control, AC-29, 1097­1100. 14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821­839. 15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer. 16. Faurre, P. (1969). Identification par minimisation d'une representation Markovienne de processus aleatoires. Symposium on Optimization, Nice. 17. Faurre, P. and Chataigner (1971). Identification en temp reel et en temp differee par factorisation de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt. 18. Faurre, P., M. Clerget, and F. Germain (1979). Op´ erateurs Rationnels Positifs, Dunod. 19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de r´ ealisation stochastique. C. R. Academie Sciences Paris 268. 20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York. 21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438­449. 22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and their L error bounds. Intern. J. Control, 39, 1115­1193. 23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and its Applications, 50, 277­319. 24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York. 25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling of time series. Econometric Reviews 11, 379­396. 26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321­377. 27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744­746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference, Tel Aviv, Israel. 29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory, McGraw-Hill. 30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta Polytech. Scand., MA31, 9­39. 31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, Identification and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499­513. 32. Kung, S. Y. (1978). A new identification and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705­714. 33. Larimore, W. E. (1990). System identification, reduced-order filtering and modeling via canonical variate analysis. Proc. 29th Conf. Decison and Control, pp. 445­451. 34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809­857. 35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241­333. 36. Lindquist, A. and G. Picci (1994a). On "subspace methods" identification. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds., Akademie Verlag, 315­320. 37. Lindquist, A. and G. Picci (1994b). On "subspace methods" identification and stochastic model reduction. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 397­403. 38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE Trans. Automatic Control, 20, 396­399. 39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica, 13, 347­357. 40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and Optimization, 29, 1251­1287. 41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identification problem. Automatica, 29 , 649-660. 42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica, 30, 75­93. 43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identification algorithms and its interpretation. Proceedings 10th IFAC Symposium on System Identification, Copenhagen, June 1994, Volume 2, 145­156. 44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382­387. 45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes. Linear Algebra and its Applications, 205-206, 957-1003. 46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day. 47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II. Journal fur die reine und angewandte Mathematik, 148, 122­145. 48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401­ 421. 49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation. IEEE Trans. Automatic Control, AC-16, 621­634. 50. Whittle, P. (1963). On the fitting of multivariate autoregressions and the approximate canonical factorization of a spectral density matrix. Biometrica, 50, 129­134. 51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its Applications, Cambridge U.P. 52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via Ho's algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4. We first give a proof for the special case n = 1. Consider a scalar function 1z+b (A.1) 2z +a with a scalar sequence (1.4) such that 0 = 1. Now it is well-known [see, e.g., Schur (1918), Akhiezer (1965)] that T is positive definite if and only if Z (z ) = |t | < 1 t = 0, 1, 2, . . . ,  - 1 (A.2) where {0 , 1 , 2 , . . . } are the so called Schur parameters. There is a bijective relation between partial sequences (1.1) and partial sequences {0 , 1 , . . . ,  -1 } of the same length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the Schur parameters of (A.1) are generated by the nonlinear dynamical system t+1 = t+1 =
t 2 1 - t - t  t 2 1 - t

0 = 1 (a + b) 2 0 = 1 (b - a) 2

(A.3)

and that Tt becomes singular precisely when there is finite escape. It was also shown in Byrnes et al. (1991) that {t } is generated by a linear system 2/ -1 ut+1 = vt+1 1 0 ut , vt (A.4)

where t = vt /ut and  := (a + b)(1 + ab)-1 . If  is greater than one in modulus, the coefficient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant scalar factor, similar to cos  sin  , - sin  cos   where  := arctan 2 - 1. Hence t is the slope of a line through the origin in R2 which rotates counter-clockwise with the constant angle  in each time step. Consequently, arctan t+1 = arctan t + . Moreover, assuming that 0 > 0, the Schur condition t < 1 will fail as soon as t+1 negative or infinite, as can be seen from the first of recursions (A.3). Hence (A.2) holds if and only if  (A.5) arctan  < . 2 Therefore for a small > 0, take a = 1 - and b = 1 + , yielding a stable Z . Then  2 4 - 2 . We may choose so that  = 2- 2 > 1 and  = arctan 2- 2   << , +1  - arctan 0 . Then (A.5) holds so that T > 0, but we also have where  :=  2  arctan  +1 > 2 so that T +1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function (a + b)n-1 (z ) 1 n (z ) + 1 2 Z (z ) = 1 2 n (z ) + 2 (a + b)n-1 (z ) o polynomials of the first and second kind rewhere {t } and {t } are the Szeg¨ spectively (Akhiezer, 1965). The function Z has the property that its first n Schur parameters, {0 , 1 , . . . , n-1 }, are precisely the data which uniquely determines n , n-1 , n and n-1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated by t 0 = 1 (a + b) t+1 = 1- 2 2 t+1 =
- t  t 2 1- t +n-1
t+n-1

Hence, we have reduced the problem to the case n = 1. If we choose the initial Schur parameters sufficiently small so that n (z ) and n-1 (z ) are approximately z n and z n-1 , n (z ) + 0 n-1 (z ) is stable if we choose a := 1 - 2 and b := 1 + for some small > 0. Then  > 1 and the proof for the case n = 1 carries through with a trivial modification. Appendix B. The Hilbert space of a sample function Let y = {y(t)}t0 be a zero-mean wide-sense-stationary stochastic process defined on a probability space {, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t,  ); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit exists, the m × m matrix function k  k obtained from a particular trajectory is then a bona-fide covariance function. [The continuous-time analog of this property was observed already by Wiener (1933)]. If moreover the sample limit is (almost surely) independent of the particular trajectory and hence necessarily coincides with the "ensemble" covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in the book of Hannan (1970). It is obvious from Birkhoff's ergodic theorem that any (zero-mean) strictly stationary ergodic process is also second-order ergodic. In this Appendix we shall show that the properties of the Hilbert space structure associated to a stationary time series y , defined on page 10, are identical to those of the Hilbert space induced by a second-order ergodic process.10 The two frameworks, i.e., the statistical "time-series" structure and the "probabilistic" structure, are in fact isomorphic. To see this, pick a "representative" trajectory of y, i.e. one in the subset of  (of probability one) for which the limit (1.11) exists. Clearly there will be no loss of generality in assuming that the probability space  of y is the "sample space", of all possible trajectories of y, i.e. the set of all semi-infinite sequences  = {0 , 1 , 2 , . . . }, t  Rm . With this choice, A will be the usual  algebra of cylinder subsets of  and the t:th random variable of the process, y(t), is just the canonical projection function y(t,  ) :   t .
For a process of this kind the Hilbert space H (y) is the closure in L2 (, A, P ) of the linear vector space generated by the scalar random variables   yi (t,  ) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence of m ×  matrices y := {y (k )}k0 as in (3.3). For  in the subset of  where the time averages converge, define the map T , T : a y(t)  a y (t) t  0 a  Rm associating the i:th scalar components of each m-dimensional random vector y(t) of the process to the corresponding i:th (infinite) row of the m ×  matrix y (t) constructed from the corresponding sample path {y(t,  ); t  Z}. By second-order ergodicity, the set of all such    will have probability measure one and the map T will in fact be norm preserving, since by construction we have t-s = E y(t)y(s) = Ey (t)y (s) , where t is the covariance matrix of y. The map T can then be extended by linearity and continuity to a unitary linear operator T : H (y)  H (y ) which commutes with the action of the natural shift operators (both of which we denote U), in these two Hilbert spaces: H (y) -H (y)  T T  H (y ) -H (y ) This isomorphism allows us to employ exactly the same formalism and notations used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991) in the present statistical setup, where we build estimates of the parameters of models describing the data in terms of an observed time series instead of stochastic processes. This provides a remarkable conceptual unity and admits a straightforward derivation in the style of stochastic realization theory of the formulas in the paper van Overschee and De Moor (1993), there obtained with considerable effort through lengthy and formal manipulations. Appendix C. The invariant form of the Kalman filter Given a stationary stochastic system (3.7), the Kalman filter is usually determined via the matrix Riccati equation Q(t + 1) = AQ(t)A - [AQ(t)C + BD ][CQ(t)C + DD ]-1 [AQ(t)C + BD ] + BB (C.1) where Q(0) = P := E{x(0)x(0) }. Here Q(t) = E{[x(t) - x ^(t)][x(t) - x ^(t)] }, and the Kalman gain is given by K (t) = [AQ(t)C + BD ][CQ(t)C + DD ]-1 . (C.3) (C.2)
U U

These equations of course depend on P , B and D, which vary as the splitting subspace ¯ ) is invariant if a uniform choice of bases is made. X varies over , whereas (A, C, C ¯ ) and hence However, as shall see, the gain K depends only on the triplet (A, C, C one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

¯ ), and hence are invariant over . Clearly, in view of Theorem 5.1, P- (t), on (A, C, C as defined by (5.12), has this property. Moreover, Q(t) = P - P- (t), and, consequently, in view of (3.9), and the Lyapunov equation P = AP A + BB , P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11). A symmetric argument yields the backward equations. It is easy to see that as Q(t)  Q monotonously, P- (t)  P- , and hence P  P- , as should be. Appendix D. Some deferred proofs Proof of Theorem 5.1. Since X is a splitting subspace for the infinite past H - and the - + := U  H - and H := U  H + . But infinite future H + , by stationarity, X splits H - - + + - + Y  H and Y  H , and hence X splits Y and Y also. (See, e.g., Lindquist and Picci (1985, 1991).) Now, using the projection formula in the footnote of page +  Y+ 16, we have for any b y   -1  1 2 . . . 0 1 . . .  2 3 . . .  +1  1 0 . . .  -1  - + -  .  y E Y b y =b  . . . . .. ..  .    . . . . . . . . . . . . . .   +1 · · · 2 -1   -1 · · · 0 - ¯  (T- )-1 y = b   = b   ¯  are appropriate finite-dimensional observability and constructibility where  and  ¯  such matrices (2.6) of full rank. If  > 0 , there is a minimal factorization H =   - -1 - ¯ that  :=  (T ) y has n components, and ¯  > 0. ¯  (T- )-1  E { } =  ^  - , dim X ^  -  n = dim X so, since Therefore, since the components of  belong to X ^ ^ X - is minimal, X must also be minimal and X - be spanned by the components of  . Next, from the backward system (3.14) we see that - ¯x = ¯( ) + terms ortogonal to X , y and therefore, by the same projection formula,
- ¯ (T - )-1 y - = a . E Y a x( ) = a E {x( )¯ x( ) }    - ^  - , establishing the first of identities Consequently, E Y X = {a  | a  Rn } = X (5.7). The second follows from a symmetric argument. The representation formula (5.8) follows from the minimality of X as a splitting subspace for Y+ and Y- , which, in particular, implies that the constructibility operator, -  ^ Ct := E|Y X : X  X -

X

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n, ^k ( ). there is a unique random variable xk ( )  X whose projection onto Y- is x To show that x(0) form a uniform choice of bases as X varies over , first take X to be the stationary backward predictor space X+ and let x+ ( ) be the unique basis - ^( ) = E Y x+ ( ). Now, let X  be arbitrary. Then, since in U X+ such that x -   + X is a splitting subspace for Y and U X+  U H (Lindquist and Picci, 1991, Proposition 2.1(vi)), we have

X

X

x ^( ) = E Y x+ ( ) = E Y E X x+ ( ), and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all X  , which is a well-known characterization of uniform choice of bases; see Section 6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields the corresponding statement for (5.9).

-

-

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a corresponding partial covariance sequence , where  is large enough for the Hankel ¯ ) be the triplet determined matrix H , defined by (1.5), to have rank n. Let (A, C, C from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the covariance data by estimates {0T , 1T , . . . , T } ¯T ) be the corresponding triplet obtained via (2.5). of type (6.3), and let (AT , CT , C We want to prove that ¯T + 1 0T ZT (z ) := CT (zI - AT )-1 C 2 is strictly positive real for a sufficiently large T . Now, if deg ZT = deg Z , replace  by -1 0  0 in (2.5) in the appropriate , U by U 0 , V by V 0 , and -1 by 0 0 0 0 ¯ ) and (AT , CT , C ¯T ) have the same dimensions. This will calculation so that (A, C, C ¯T , 0T ) can be made arbitrarily close not affect Z and ZT . By continuity, (AT , CT , C ¯ to (A, C, C, 0 ) in any norm by choosing T sufficiently large. Thus the same holds for max Z (ei ) - ZT (ei )
[0,2 ]

and hence, since (z ) := Z (z ) + Z (z -1 ) satisfies (3.10), so will T (z ) := ZT (z ) + ZT (z -1 ) for sufficiently large T . Moreover, since |(A)| < 1, we have |(AT )| < 1 by continuity for sufficiently large T . Consequently, there is a T0 such that ZT is strictly positive real for T  T0 . ¯) Proof of Theorem 5.3. Let Z , defined by (1.6), be strictly positive real, and let (A, C, C be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , defined by ¯1 ), is also strictly (7.15) in terms of the principal subsystem truncation (A11 , C1 , C positive real. We want to prove that this property is carried over to rational matrix function ¯ )1 ) + 1 0 Z 1 (z ) = (C )1 (zI - (A )11 )-1 (C 2 for  sufficiently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let Q be defined by (5.32). Since the canonical correlation coefficients (5.25) tend to the canonical correlation coefficients (4.12) as   ,   . Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P- (t) tends to Q Q as t   if the initial condition is taken to be P- ( ) =  . Consequently, for any > 0, there is a sufficiently large  such that  -  < 2 and  - Q Q < 2 so that  - Q Q < . Hence Q tends to a limit Q with the property  = Q Q . Using the same argument in the backward direction, the T -1 second of relations (5.33) shows that Q also satisfies  = Q-  Q . Consequently, by the same argument as in the proof of Theorem 4.4, Q is a signature matrix, and hence in particular diagonal. Therefore,
1 -1 ¯ ¯ )1 )  ((Q )11 A(Q )- ((A )11 , (C )1 , (C 11 , C (Q )11 , C (Q )11 ) as   ,

where (Q )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, Z 1  Z1 . Hence, since Z1 is positive real, then so is Z 1 for  sufficiently large. ¯ ) is a minimal triplet. Proof of Lemma 7.4. Let us first consider the case when (A, C, C Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P- and P+ respectively, which, in particular, have the property that P-  P1 and P2  P+ . Then, in view of (7.18), P+ - P- > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem A4.4). Next, let us reduce the general case to the case considered above. If (C, A) is not observable, change the coordinates in state space, through a transformation ¯ ), so that ¯ )  (QAQ-1 , CQ-1 , QC (A, C, C ^ 0 C= C A= ^ 0 A   ^ ¯= C ¯  , C

^ A ^) is observable. Then, if P1 and P2 have the corresponding representations where (C, P1 = ^1  P   P2 = ^2  P ,  

^2 satisfy the reduced version of the linear matrix ^1 and P it is easy to see that P ^) and that, in this new ¯ ) for (A, ^ C, ^ C ¯ inequality (6.6) obtained by exchanging (A, C, C ^ ^2 - P ^1 > 0. If (C, ¯ A ^ ) is not observable, we proceed setting, (7.18) holds, i.e., P -1 -1 ^2 ^1 and P satisfy the by removing these unobservable modes. First note that P ^ ^ ^ C, ^ C ¯ ) by (A ^ , C, ¯ C ^ ). Then, dual linear matrix inequality obtained by exchanging (A, changing coordinates in state space so that ^ ~ ¯= C ¯  C ~ ^ = A A  0  ^= C ~ 0 , C

^ ¯ A ~ ) observable, and defining with (C, ~ -1  P -1 ^1 P = 1   ~ -1  ^ - 1 = P2 P , 2  

44

ANDERS LINDQUIST AND GIORGIO PICCI

~ ~ C, ~ C, ¯ 1 0 ) is a minimal realization of Z . Moreover, P ~1 and P ~2 satisfy we see that (A, 2 the corresponding linear matrix inequality (6.6) and have the property (7.18) in this setting. Hence the problem is reduced to the case already studied above. Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans-1 , mapping formed to the continuous-time setting via a bilinear transformation s = z z +1 the unit disc onto the left half plane so that Zc (s) = Zd 1+s 1-s (D.1)

is positive real in the continuous-time sense if and only if Zd is positive real in the discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al. ¯d , 1 0 ) and (Ac , Cc , C ¯c , 1 R) are realizations of Zd and Zc (1979)] that, if (Ad , Cd , C 2 2 respectively, we have  Ac = (Ad + I )-1 (Ad - I )    C = 2C (A + I )-1 c  d d (D.2) ¯ ¯d (A + I )-1  Cc = 2C  d   ¯ -C ¯d (A + I )-1 C R = 0 - Cd (Ad + I )-1 C d d d and inversely  Ad = (I - Ac )-1 (I + Ac )    C = 2C (I - A )-1 d c  c -1 ¯ ¯  = 2 C ( I - A C d c  c)   ¯ +C ¯c (I - Ac )-1 Cc 0 = R + Cc (I - Ac )-1 C c

(D.3)

Under this transformation the observability gramian and the constructibility gramian ¯d , 1 0 ) is ¯ A )) are preserved so that (Ad , Cd , C (i.e., the observability gramian of (C, 2 ¯c , 1 R) is; see, e.g., p. 1119 in Glover a minimal realization if and only if (Ac , Cc , C 2 (1984). Moreover, coercivity is preserved, and the solution sets of the corresponding linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the reachability gramian of a spectral factor and this gramian is also preserved.) Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact, transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting, all the requirements of Theorem 7.1 are satisfied. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order positive real function thus obtained via (D.3) back to discrete time, the desired result is obtained.

