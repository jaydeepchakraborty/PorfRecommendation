CANONICAL CORRELATION ANALYSIS, APPROXIMATE
COVARIANCE EXTENSION, AND IDENTIFICATION OF
STATIONARY TIME SERIES*
ANDERS LINDQUIST† AND GIORGIO PICCI‡

Abstract. In this paper we analyze a class of state-space identiﬁcation algorithms
for time-series, based on canonical correlation analysis, in the light of recent results on stochastic systems theory. In principle, these so called “subspace methods”
can be described as covariance estimation followed by stochastic realization. The
methods oﬀer the major advantage of converting the nonlinear parameter estimation phase in traditional ARMA models identiﬁcation into the solution of a Riccati
equation but introduce at the same time some nontrivial mathematical problems
related to positivity. The reason for this is that an essential part of the problem
is equivalent to the well-known rational covariance extension problem. Therefore
the usual deterministic arguments based on factorization of a Hankel matrix are
not valid for generic data, something that is habitually overlooked in the literature. We demonstrate that there is no guarantee that several popular identiﬁcation
procedures based on the same principle will not fail to produce a positive extension, unless some rather stringent assumptions are made which, in general, are not
explicitly reported.
In this paper the statistical problem of stochastic modeling from estimated covariances is phrased in the geometric language of stochastic realization theory. We
review the basic ideas of stochastic realization theory in the context of identiﬁcation, discuss the concept of stochastic balancing and of stochastic model reduction
by principal subsystem truncation. The model reduction method of Desai and Pal,
based on truncated balanced stochastic realizations, is partially justiﬁed, showing
that the reduced system structure has a positive covariance sequence but is in general not balanced. As a byproduct of this analysis we obtain a theorem prescribing
conditions under which the ”subspace identiﬁcation” methods produce bona ﬁde
stochastic systems.

1. Introduction
Recently there has been a renewed interest in state-space identiﬁcation algorithms
for time series based on a two steps procedure which in principle can be described
as estimation of a rational covariance model from observed data followed by stochastic realization. The method oﬀers the major advantage of converting the nonlinear
parameter estimation phase which is necessary in traditional ARMA models identiﬁcation into a partial realization problem, involving a Hankel matrix of estimated
∗ This research was supported in part by grants from TFR, the Göran Gustafsson Foundation,
the SCIENCE project “System Identiﬁcation” and LADSEB-CNR.
† Division of Optimization and Systems Theory, Royal Institute of Technology, 100 44 Stockholm,
Sweden
‡ Dipartimento di Elettronica e Informatica, Universita’ di Padova, 35131 Padova, Italy
1

2

ANDERS LINDQUIST AND GIORGIO PICCI

covariances, and the solution of a Riccati equation, both much better understood problems for which eﬃcient numerical solution techniques are available. In this framework
we can naturally accommodate multivariate processes and there are indications that
the algorithms may work also with data containing purely deterministic components
(van Overschee and De Moor, 1993). A drawback, however, to be emphasized in
this paper, is that, unlike, say, least-squares identiﬁcation of ARMA models, these
methods do not work for arbitrary data.
This type of procedure was apparently ﬁrst advocated by Faurre (1969); see also
Faurre and Chataigner (1971) and Faurre and Marmorat (1969). More recent work,
based on canonical correlation analysis (Akaike, 1975) (or some other singular-value
decomposition) and the Ho-Kalman algorithm (Kalman et al.,1969), is due to Aoki
(1990), Larimore (1990), and van Overschee and De Moor (1993). In the modern versions of the algorithm canonical correlation analysis is performed directly on the observed data without computing the covariance estimates (van Overschee and De Moor,
1993). Numerical experience shows that the computation time needed to get the ﬁnal model parameters estimates compares very favorably with traditional iterative
prediction error methods for ARMA models.
On the other hand there is a price to be paid for this simpliﬁcation. These methods
introduce some nontrivial mathematical problems related to positivity. The reason
for this is that an essential part of the problem is equivalent to the well-known rational
covariance extension problem. Therefore the usual deterministic realization arguments
based on factorization of a Hankel matrix are not valid for generic data, something
that is habitually overlooked in the literature. Note that positivity is the natural
condition insuring solvability of the Riccati equation required to compute state-space
models of the signal from the covariance estimates.
Central in the procedures described above is the following classical problem of
identiﬁcation of a covariance sequence. Let
{Λ0 , Λ1 , . . . , Λν }

(1.1)

be a ﬁnite set of sample m × m covariance matrices estimated in some unspeciﬁed
way from a certain m-dimensional sequence of observations
{y0 , y1 , y2 , . . . yT },

(1.2)

and consider the problem of ﬁnding a minimal1 triplet of matrices (A, C, C̄) such that
CAk−1 C̄  = Λk

k = 1, 2, . . . , ν

(1.3)

and such that the inﬁnite sequence
{Λ0 , Λ1 , Λ2 , . . . },

(1.4)

obtained from (1.1) by setting Λk := CAk−1 C̄  for k = ν + 1, ν + 2, . . . , is a bona ﬁde
covariance sequence.
In the literature the last condition is generally ignored. The remaining problem of
ﬁnding a minimal triplet (A, C, C̄) satisfying (1.3) is called the minimal partial realization problem. The triplet (A, C, C̄) is usually computed by minimal factorization
1

Here (A, C, C̄) is minimal if (A, C) is completely observable and (A, C̄  ) is completely reachable.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

of a block Hankel matrix corresponding to the data (1.1) as follows:

 


C̄
Λ1 Λ2
C
Λ3 · · ·
Λj
Λ2 Λ3
Λ4 · · · Λj+1   CA   C̄A 
 ,
H=
=  . 
..
.. 
..
..
..
 ...

.
.
.   ..  
.
.
Λi Λi+1 Λi+2 · · · Λi+j−1
CAi−1
C̄(A )j−1

3

(1.5)

where i + j − 1 = ν and the Hankel matrix H is chosen as close to square as possible
by taking |i − j| ≤ 1. In fact, (1.3) holds if and only if (1.5) holds for all (i, j) such
that i + j − 1 = ν, and hence the minimal factorization must be made for a choice
of (i, j) in which the Hankel matrix (1.5) has maximal rank. The inﬁnite sequence
{Λ0 , Λ1 , Λ2 , . . . } obtained in this way by setting Λk := CAk−1 C̄  for k = ν +1, ν +2, . . .
is called a minimal rational extension of the ﬁnite sequence (1.1) and is in general not
a covariance sequence. The dimension r of a minimal rational extension is called the
(algebraic) degree of the partial sequence (1.1). Clearly the degree r is also equal to
the McMillan degree of the m × m rational matrix
1
(1.6)
Z(z) = C(zI − A)−1 C̄  + Λ0 ,
2
and the elements of the inﬁnite sequence (1.4) are the coeﬃcients of the Laurent
expansion
1
Z(z) = Λ0 + Λ1 z −1 + Λ2 z −2 + . . .
2

(1.7)

about z = ∞.
The underlying identiﬁcation problem is however a great deal more complicated
than the classical partial realization problem. In fact, the requirement that (1.4) be a
bona ﬁde covariance sequence amounts to (1.4) being a positive sequence in the sense
that, for every t ∈ Z+ , the block Toeplitz matrices Tt ,


Λ2 · · · Λt
Λ0 Λ1
Λ1 Λ0
Λ1 · · · Λt−1 
Tt = 
,
(1.8)
..
.. 
.
.
...
 ..
..
.
. 
Λt Λt−1 Λt−2 · · ·

Λ0

formed from the inﬁnite sequence (1.4), be positive deﬁnite or, equivalently, that the
matrix function
Φ(z) := Z(z) + Z(1/z)

(1.9)

be positive semideﬁnite on the unit circle, i.e.
Φ(eiθ ) ≥ 0 θ ∈ [0, 2π).

(1.10)

This property is equivalent to Φ being a spectral density matrix. In fact, it will be the
spectral density of the covariance sequence (1.4). Clearly (1.1) cannot be a partial
covariance sequence unless Tν > 0, but this is not enough.
From the point of view of identiﬁcation there seem to be two possible routes to
determine a model (A, C, C̄) from the ﬁnite covariance sequence (1.1). One that
has been proposed in the literature is do minimal factorization (1.5) of a ﬁnite block
Hankel matrix in balanced form (Aoki, 1990, van Overschee and De Moor, 1993). This

4

ANDERS LINDQUIST AND GIORGIO PICCI

yields a solution to the minimal partial realization problem, and, as will be shown
in this paper, there is no a priori guarantee that this method will yield a positive
extension. This fact has nothing to do with sample variability (random ﬂuctuations)
of the covariance estimates (1.1), and to emphasize this point we initially assume
that all strings of data (1.2) are inﬁnitely long. A theoretically sounder identiﬁcation
method, which will not be considered in this paper, could instead be to do positive
extension ﬁrst and then to use a stochastic model reduction procedure on the triplet
(A, C, C̄) of the positive extended sequence.
The issues regarding positive extension are discussed in Section 2, where the nontrivial nature of the positivity constraints are explained. The failure to take this
diﬃculty into consideration have been pointed out by the authors of this paper at
many scientiﬁc meetings in the last ten years. This has had no apparent eﬀect, except for two recent papers, Heij et al. (1992) and Vaccaro and Vukina (1993), in
which these problems are mentioned. Consequently this point will be strongly emphasized. We illustrate our point on the identiﬁcation procedure of Aoki (1990) and
demonstrate that there is a hidden, and not easily tested, assumption without which
the procedure will not be guaranteed to succeed. The punch line is that none of the
subspace identiﬁcation methods under consideration can be expected to always work
for generic data but that some not entirely natural conditions on the data are needed.
The analysis of the basic theoretical issues behind subspace identiﬁcation is carried
out in the geometric framework of stochastic realization theory; see, e.g., Lindquist
and Picci (1985, 1991). In Section 3 we introduce some basic concepts from this
theory and adapt them to the problem of identiﬁcation. To this end, we ﬁrst discuss
an idealized situation in which the time series (1.2) is inﬁnitely long i.e. T = ∞, and
the available covariance data are given by the ergodic limit
1 

yt+k yt+j
= Λk−j
T →∞ T + 1
t=0
T

lim

(1.11)

for all k and j. Then the sample estimates in the sequence (1.1) are bona ﬁde covariance matrices and the Toeplitz matrix Tν formed from the data will be positive
deﬁnite and symmetric. We introduce a Hilbert space of observed (inﬁnite) strings of
data {yt }, allowing us to use the geometric concepts and machinery of linear stochastic
system theory as developed in Lindquist and Picci (1985, 1991) also for the statistical
problem of identiﬁcation. In this way we establish a correspondence which turns operations on random quantities deﬁned on abstract probability spaces into prototypes of
statistical algorithms involving computations based on the observed data. Canonical
correlations and balanced stochastic realizations are then analyzed in this setting in
Section 4, and the basic concepts and principles used in the subspace identiﬁcation
methods, as well as in the model reduction procedures of Desai and Pal, are translated
into the more natural context of geometric stochastic realization theory.
Although the explicit computation of covariance sequences can be avoided completely in the methods discussed in this paper, it is useful to think in terms of such
objects. The realization theory developed in Sections 3 and 4 deals with an idealized situation which admits the construction of an exact inﬁnite covariance sequence
(1.4). Consequently, the diﬃcult question of positivity is not an issue here. Nor is
it the ﬁnite sample size per se which is the problem, but the fact that only a ﬁnite

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

5

covariance sequence (1.1) could be constructed from the data (1.2) when T is ﬁnite.
Therefore we separate these issues by discussing stochastic realization theory from
ﬁnite covariance data in Section 5 and subspace identiﬁcation in Section 6. In this
framework we show that the method of van Overschee and De Moor (1993) is valid
under some rather stringent assumptions. We stress that we are only concerned with
identiﬁcation procedures for state space modeling of time series. “Subspace identiﬁcation” methods for deterministic systems with measurable inputs or for spectral
factors do not involve positivity, but stability may still be a problem. However, the
algorithms of van Overschee and De Moor (1994a, 1994b) also have a stochastic part,
so the problem of positivity arises here too.
Another idea behind the subspace identiﬁcation methods considered in this paper is
to disregard modes corresponding to “small” canonical correlation coeﬃcients. This
is called balanced truncation and is in fact a stochastic model reduction procedure.
In all such procedures there must be a guarantee that the reduced-degree matrix
function (1.6) is positive real, and therefore the preservation of positivity in such
reductions is a main concern of this paper. Section 7 is devoted to such issues. The
model reduction procedure of Desai and Pal (1982) was never theoretically justiﬁed
in their work or in their subsequent work Desai et al. (1985) and Desai (1986)2 .
Here we shall demonstrate that this reduction procedure produces a positive real, but
not in general balanced, reduced model structure. In fact, the singular values of the
truncated system are usually not equal to the r ﬁrst singular values of the original
system.
It is an interesting fact that the procedure of Desai and Pal does produce balanced
truncations for continuous-time stochastic systems. A partial result in this direction
was given by Harshavardana, Jonckheere and Silverman (1984), who showed that
the truncated function is positive real and conjectured that it is balanced. We shall
demonstrate that it is indeed balanced, a result that is actually already contained in
the work of Ober (1991). The problem with the Desai-Pal procedure in discrete time
depends on the fact that the spectral factors of the truncated approximate spectrum
behave diﬀerently than in continuous time. While in continuous time the realizations
of the reduced spectral factors are proper subsystems, obtained by partitioning the
matrices of the realizations of the factors of Φ, this is not the case in discrete time,
contrary to early claims of Desai and Pal. As indicated in Ober (1991), a balanced
truncation procedure is available in discrete time, but the systems matrices are no
longer submatrices of those of the original system, and therefore it is not equivalent
to the truncation procedure used in subspace identiﬁcation.
Several of the results of this paper have previously been announced in Lindquist
and Picci (1994a)3 and in Lindquist and Picci (1994b).

2

In Desai et al. (1985) a diﬀerent model reduction procedure, which is not relevant to subspace
identiﬁcation, is considered, namely “deterministic” model reduction of the minimum phase spectral
factors.
3
We warn the reader that a preliminary version of Lindquist and Picci (1994a), containing some
erroneous statements, was accidentally published in place of the paper ﬁnally submitted for publication. The correct version can be obtained from the authors.

6

ANDERS LINDQUIST AND GIORGIO PICCI

2. Positive, nonpositive and approximate factorizations of the Hankel matrix of covariances
The solution to the minimal partial realization problem , i.e., the problem to ﬁnd
the triplet (A, C, C̄) satisfying (1.1) is in general not unique. This lack of uniqueness, studied in, for example, Kalman et al. (1969), Kalman (1979) and Gragg and
Lindquist (1983), is not an issue in this paper. Therefore, to avoid this question altogether, we shall make the standard assumption that the algebraic degree of (1.1)
equals that of
{Λ0 , Λ1 , . . . , Λν−1 }
so that we can use a Hankel matrix (1.5) based
allowing us to deﬁne the shifted Hankel matrix

Λ3
Λ4
Λ2
 Λ3
Λ4
Λ5
σ(H) = 
..
.
.
 ..
..
.
Λi+1 Λi+2 Λi+3

(2.1)

on this data, i.e., with i + j = ν,

· · · Λj+1
· · · Λj+2 
.. 
...
. 
· · · Λν

(2.2)

uniquely. In this case the classical Ho-Kalman algorithm (Kalman et al. 1969) produces a minimal solution (A, C, C̄) which is unique up to a similarity transformation.
As ﬁrst pointed out by Zeiger and McEwen (1974), the minimal factorization on
which the Ho-Kalman procedure is based may be performed by singular-value decomposition, thereby ﬁxing (A, C, C̄) uniquely; see also Kung (1978). In fact, the Hankel
matrix H may be factored as
H = U ΣV 

U  U = I = V  V,

(2.3)

where Σ is the square n × n diagonal matrix of the nonzero singular values taken in
decreasing order. Setting Ω := U Σ1/2 and Ω̄ := V Σ1/2 this leads to a factorization
H = ΩΩ̄

Ω Ω = Σ = Ω̄ Ω̄

(2.4)

of the type (1.5). Then a minimal realization (A, C, C̄) is obtained by solving
ΩAΩ̄ = σ(H),

C Ω̄ = ρ1 (H) and C̄Ω = ρ1 (H  ),

where σ(H) is the shifted Hankel matrix (2.2) and ρ1 (H) is the ﬁrst block row of H.
It follows that the triplet (A, C, C̄) must be given by
A = Σ−1/2 U  σ(H)V Σ−1/2 ,
C = ρ1 (H)V Σ−1/2 ,
C̄ = ρ1 (H  )U Σ−1/2 ,

(2.5a)
(2.5b)
(2.5c)

a form to which we refer as ﬁnite-interval balanced, since it is balanced in the sense
that Ω Ω and Ω̄ Ω̄ are both equal to Σ, and that




C̄
C
 C̄A 
 CA 


.
Ω̄
=
(2.6)
Ω=
..


 ... 
.
CAi−1

C̄(A )j−1

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

7

Aoki (1990) has proposed that this procedure be used also for identiﬁcation of time
series. The problem with such a strategy is that this algorithm is a deterministic
realization procedure and hence does not a priori insure that (1.6) is positive real, or
even stable for that matter, even if the Toeplitz matrix Tν is positive deﬁnite. In fact,
it is shown in Byrnes and Lindquist (1982) that there are open subsets of the space
of covariance data (1.1) for which A is not stable, and a fortiori the same holds for
positivity. In fact, like that in van Overschee and De Moor (1993), the procedure in
Aoki (1990) is based on the following hidden assumption which is not entirely natural.
Assumption 2.1. The covariance data (1.1) can be generated exactly by some (unknown) stochastic system of dimension equal to rank H.
Therefore, not only must we know that there exists an underlying ﬁnite-dimensional
system, but we must also have some upper bound for its dimension. A conservative
upper bound which will always suﬃce is [ ν2 ].
Is this assumption natural? If the covariance data are really generated exactly from
a “true” stochastic system and there is a reliable estimate of its order which is no
more than half of the length of the covariance sequence, then the assumption will hold.
However, and this is an important point of this paper, one cannot expect Assumption
2.1 to hold for an arbitrary covariance sequence (1.1).
To clarify this point, let us agree to call {Λ0 , Λ1 , Λ2 , . . . } a minimal rational extension of {Λ0 , Λ1 , . . . , Λν } if the rational function (1.7) has minimal degree. By
deﬁnition this is the algebraic degree of {Λ0 , Λ1 , . . . , Λν }. A rational extension is
called positive if, for every µ > ν, the block Toeplitz matrices Tµ formed from the
corresponding inﬁnite sequence (1.4) are positive deﬁnite. An extension with this
property is called a positive rational extension. It is well known that the extension
{Λ0 , Λ1 , Λ2 , . . . } is positive if and only if (1.7) is positive real, i.e. the rational function
Z(z) is analytic in the closed unit disc and the matrix function
Φ(z) = Z(z) + Z(1/z)

(2.7)

is nonnegative deﬁnite on the unit circle, making Φ a spectral density matrix. A
minimal positive rational extension of the ﬁnite sequence (1.1) is one for which the
dimension of the triplet (A, C, C̄) in (1.6) is as small as possible.
Deﬁnition 2.2. The positive degree p of the ﬁnite covariance sequence {Λ0 , Λ1 , . . . , Λν }
is the dimension of any minimal positive extension.
A well-known example of a positive extension is the maximum entropy extension
(Whittle, 1963) corresponding to the spectral density Φ(z) := W (z)W (1/z) , where
the spectral factor W (z) is (modulo a multiplicative constant matrix) the inverse of the
Levinson-Szegö matrix polynomial of order ν corresponding to the ﬁnite covariance
sequence (1.1). Since the rational function W (z) generically has the McMillan degree
equal to mν, it follows from spectral factorization theory (Anderson, 1958) that Z(z)
has also degree mν. Consequently, the positive degree p is bounded from below by
the algebraic degree r and from above by mν.
As already pointed out, it is very common in the literature (Aoki, 1990, van Overschee and De Moor, 1993 and others) to disregard the positivity constraint and to use
algebraic rather than positive extensions, usually computed by minimal factorization
a block Hankel matrix such as (1.5), or by methods which in principle are equivalent

8

ANDERS LINDQUIST AND GIORGIO PICCI

to this, even if the Hankel matrix is not explicitly computed. In fact, Assumption 2.1
may also be formulated in the following way.


Assumption 2.1 . The positive degree of (1.1) equals the algebraic degree.
This assumption prescribes a property of the covariance sequence (1.1) which is not
generic. We can illustrate this point by considering the rational extension problem
for a ﬁnite scalar covariance sequence (1.1). The positive degree p lies between the
algebraic degree r and ν. Note that neither the case p = ν nor the case p < ν
are ”rare events”, because there are open sets of covariance sequences (1.1) of both
categories. In fact, it was shown in Byrnes and Lindquist (1996) that for each µ
such that ν2 ≤ µ ≤ ν there is an open set of covariance data in Rν for which p = µ.
If the upper limit p = ν is attained there are inﬁnitely many nonequivalent minimal
triplets (A, C, C̄) providing a positive extension, one of which is the maximum entropy
extension. In fact, it can be shown that these ν-dimensional extensions form an
Euclidean space (Byrnes and Lindquist, 1989). This shows that the ﬁnite data (1.1)
never contains enough information to establish a ”true” underlying system. A similar
statement can be made in the case when p < ν.
Example 2.3. Consider the case m = 1 and ν = 2, i.e., consider a scalar partial
covariance sequence {Λ0 , Λ1 , Λ2 }. If Λ1 = Λ2 = 0, we have r = p = 0. Otherwise,
we always have r = 1, whereas the positive degree can be either one or two. In fact,
setting γ0 := Λ1 /Λ0 and γ1 := (Λ21 + Λ2 )/(1 − Λ21 ), it can be shown (Georgiou, 1987;
also see Byrnes and Lindquist, 1996, where other examples are also given) that p = 1
if and only if
|γ0 |
|γ1 | <
1 + |γ0 |
and p = 2 otherwise.
In fact, it is not hard to construct examples for which the gap between algebraic
and positive rank is arbitrarily large, as the following theorem shows.
Theorem 2.4. Let n ∈ Z+ be ﬁxed. Then for an arbitrarily large ν there is a stable
rational function Z(z) of degree n, such that the Toeplitz matrix Tν formed as in ( 1.8)
from the coeﬃcients of the Laurent expansion ( 1.7), is positive deﬁnite while Tν+1 is
indeﬁnite.
Consequently, you cannot test the positivity of a rational extension of (1.1) by
checking a ﬁnite Toeplitz matrix, however large is its dimension. The proof of Theorem
2.4 is given in Appendix A.
Let us now return to the identiﬁcation procedure of Aoki (1990). In practice the
rank of H will always be full, and to compute a partial realization of reasonable
dimension the basic idea is to partition Σ as


	
Σ1 0
,
(2.8)
Σ=
0 Σ2

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

9

where the singular values in Σ2 are smaller than those in Σ1 , perhaps close to zero,
and then take Σ2 = 0 so that H is approximated by
	


Σ1 0 
(2.9)
V = U1 Σ1 V1 .
H1 = U
0 0
The matrix H1 is a best approximation (given the rank) of H in (the induced) !2 –
norm, but it is in general not Hankel and hence can not be used to determine a
reduced order system. Of course, one may instead use Hankel-norm approximation
(Adamjan, Arov and Krein, 1971), which produces another best approximation of
H in !2 -norm that is Hankel and has the same rank as H1 . However, if Σ2 is “very
small” compared to Σ1 , then H1 is close to H and hence approximately Hankel. For
this reason, Aoki’s procedure (Aoki, 1990) is based on the original data H and σ(H).
Thus identifying H1 with H in (2.9) and noting that U1 U1 = I and V1 V1 = I, the
same type of calculation as above yields the reduced triplet (Ar , Cr , C̄r ) given by
−1/2

Ar = Σ1

−1/2

U1 σ(H)V1 Σ1
−1/2

Cr = ρ1 (H)V1 Σ1

,

,

−1/2

C̄r = ρ1 (H  )U1 Σ1

(2.10a)
(2.10b)

.

(2.10c)

It is not hard to see, and it is shown in Aoki (1990), that (2.10) is a principal
subsystem truncation in the sense that, if H is produced by a ﬁnite-dimensional system
with (A, C, C̄) having ﬁnite-interval balanced form (2.5), we have
Ar = A11 ,
where



	
A11 A12
A=
A21 A22

Cr = C1 ,

C̄r = C̄1 ,



C = C1 C2



C̄ = C̄1 C̄2 .

(2.11)

(2.12)

In fact, since U1 U1 = V1 V1 = [I, 0], this is seen by merely solving (2.5) for σ(H),
ρ1 (H) and ρ1 (H  ) and inserting in (2.10).
However, it must be shown that (2.11) corresponds to a stochastic system, i.e., that
1
(2.13)
Z1 (z) = C1 (zI − A11 )−1 C̄1 + Λ0
2
is positive real, provided of course that Z, deﬁned by (1.6), is positive real. The
question of stability was answered in the aﬃrmative in Pernebo and Silverman (1982)
and is addressed in Aoki (1990). The crucial question of positivity, however, is not
discussed in Aoki (1990) and its validity is in doubt. Positivity will, however, be
proven for a somewhat modiﬁed procedure described below.
In fact, following Akaike (1975) and Desai et al. (1984, 1985), instead of H we shall
consider a normalized Hankel matrix
−T
Ĥ = L−1
+ HL− ,

(2.14)

where L− and L+ are lower triangular Cholesky factors of the Toeplitz matrices T−
and T+ of (1.1) and the corresponding sequence of transposed covariances respectively;
see Section 4 below. This is also the Hankel matrix considered in van Overschee and
De Moor (1993). Taking the singular value decomposition of Ĥ instead of H, the

10

ANDERS LINDQUIST AND GIORGIO PICCI

singular values become the canonical correlation coeﬃcients, i.e., the cosines of the
angles between the past and the future of the process y. The systems matrices can
be determined in a manner analogous to (2.5), but now
Ω T+−1 Ω = Σ̂ = Ω̄ T−−1 Ω̄

(2.15)

instead of (2.4) so the realization is not balanced in the same (deterministic) way as
above. To see this, consider the singular value decomposition Ĥ = Û Σ̂Û  so that H =
(L+ Û )Σ̂(L− V̂ ) . Since H = ΩΩ̄ and this factorization is unique modulo coordinate
transformation in state space, we may take Ω = L+ Û Σ̂1/2 and Ω̄ = L− V̂ Σ̂1/2 . Then
(2.15) follows from Û  Û = I = V̂  V̂ . As we shall see next, (2.15) corresponds to a
more natural type of balancing corresponding to a Hankel operator describing the
interface between the past and the future of the time series y.
3. Stochastic realization theory in the Hilbert space of a sample function
In this section we introduce a mathematical framework which is suitable for the identiﬁcation problem described above. We deﬁne a Hilbert space of observed (inﬁnite)
strings of data {yt }. This framework turns out to be isomorphic to that of geometric
stochastic realization theory, thus allowing us to use the geometric concepts and machinery of linear stochastic system theory as developed in Lindquist and Picci (1985,
1991) also for the statistical problem of identiﬁcation. In this way we also establish a
correspondence which converts operations on random quantities deﬁned on abstract
probability spaces into prototypes of statistical algorithms involving computations
based on the observed data.
In identiﬁcation we have access only to a ﬁnite string of data
{y0 , y1 , y2 , . . . , yT }.

(3.1)

Here T may be quite large but, of course, always ﬁnite. To begin with, we shall,
however, consider the idealized situation that we are given a doubly inﬁnite sequence
of m-dimensional data
{. . . , y−3 , y−2 , y−1 , y0 , y1 , y2 , y3 . . . }

(3.2)

together with a corresponding covariance sequence {Λk }k≥0 , each matrix Λk of the
sequence being computed from the data (3.2) by an ergodic limit of the type (1.11).
In Section 5 we then modify the theory to handle the situation of ﬁnite data (3.1).
For each k ∈ Z deﬁne the m × ∞ matrix
y(t) := [yt , yt+1 , yt+2 , . . . ]

(3.3)

and consider the sequence y := {y(t)}t∈Z . This object will be referred to as the mdimensional stationary time series constructed from the data (3.2). The space Y of
all ﬁnite linear combinations

ak ∈ Rm , tk ∈ Z
ak y(tk );

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

11

is a real vector space and can be equipped with an inner product deﬁned by linear
extension of the bilinear form
t0 +T
1 

a yt+k yt+j
b = a Λk−j b,
a y(k), b y(j) := lim
T →∞ T + 1
t=t




(3.4)

0

which clearly does not depend on t0 . This inner product is nondegenerate if the
Toeplitz matrix Tk , constructed from the covariance data {Λ0 , Λ1 , . . . , Λk }, is a positive deﬁnite symmetric matrix for all k. Here we shall assume that the sequence
{Tk }k≥0 is actually coercive, i.e., Tk > cI for some c > 0 and all k ≥ 0. (See Assumption 3.2 below for an alternative characterization.) We also deﬁne a shift operator U
on the family of semi-inﬁnite matrices (3.3), by setting
Ua y(t) = a y(t + 1)

t ∈ Z,

a ∈ Rm ,

deﬁning a linear map which is isometric with respect to the inner product (3.4) and
extendable by linearity to all of Y . In particular the sequence of matrices {y(k)}
corresponding to the time series y is propagated in time by the action of the operator
U, i.e.,
yi (t) = Ut yi (0),

i = 1, 2, . . . , m,

t ∈ Z,

(3.5)

where yi denotes the i:th row component of y. Then, closing the vector space Y in
the inner product (3.4), we obtain a Hilbert space H(y) := cl Y . The shift operator
U is extended by continuity to all of H(y) and is a unitary operator there.
As explained in more detail in Appendix B, this Hilbert space framework is isomorphic to the one described in Lindquist and Picci (1985, 1991), and hence all results
in the geometric theory of stochastic realization can be carried over to the present
framework by merely identifying the time series y with a stationary stochastic process
y. In particular, the subspaces H − and H + of H(y) generated by the elements (3.3)
for t < 0 and t ≥ 0 respectively can be regarded as the past and future subspaces
of the stationary process y. For reasons of uniformity of notation the inner product
(3.4) will also be denoted
ξ, η = E{ξη},

(3.6)

as the frameworks are completely equivalent. Here we allow E{·} to operate on matrices of time series, taking inner products component-wise. Moreover, the coercivity
condition introduced above insures that ∩t∈Z Ut H − = 0 and ∩t∈Z Ut H + = 0, i.e., y is
a purely nondeterministic sequence.
As we have pointed out above, the subspace identiﬁcation methods of Aoki (1990)
and van Overschee and De Moor (1993) are based on the assumption that the available
data is generated by an underlying stochastic system of ﬁnite dimension. More specifically, using the notations introduced above, we assume that the data are generated
by a linear system of the type

x(t + 1) = Ax(t) + Bw(t)
(3.7)
y(t) = Cx(t) + Dw(t)

12

ANDERS LINDQUIST AND GIORGIO PICCI

deﬁned for all t ∈ Z, where w is some vector-valued normalized white noise time
series4 (say, of dimension p), and (A, B, C, D) are constant matrices with A a stability
matrix. Throughout this paper we shall 	assume
(without restriction) that (A, B, C)


B
is a minimal triplet and that the matrix
has linearly independent columns.
D
The system is assumed to be in statistical steady state so that the n-dimensional
state x and the m-dimensional output y are uniquely deﬁned by (3.7) as linear causal
functionals of the past input w. This clearly implies that x and y are jointly stationary time series so that in particular, the cross covariance matrices of x(t) and
y(s) will depend only on the diﬀerence t − s. We shall think of the system (3.7) as a
representation of the output time series y. The state and input variables x and w are
introduced in order to display the special structure of the dynamic model of y and
are by no means unique. Such a representation is called a state-space realization of y.
Remark 3.1. Despite the fact that the model (3.7) is deﬁned in terms of sample
sequences, all equalities must be understood in the sense of Hilbert space metric, just
as in the case of models based on random variables.
The number of state variables n is called the dimension of the realization. A
realization is minimal if there is no other realization of y of smaller dimension. In
this case the covariance matrix of the state vector,
(3.8)
P = E{x(t)x(t) }
	 

B
is positive deﬁnite. Moreover as the matrix
is taken with linearly independent
D
columns, the number of (scalar) white noise inputs p is also as small as possible.
Clearly, the covariance sequence {Λ0 , Λ1 , Λ2 , . . . } of the output {y(t)} of a minimal
model (3.7) is a rational sequence of degree n, i.e., represented as

Λk = CAk−1 C̄  k = 0, 1, 2, . . . where C̄  = AP C  + BD
.
(3.9)
Λ0 = CP C  + DD
In the following we shall need to assume that the corresponding spectral density Φ(z)
satisﬁes the following condition.
Assumption 3.2. The spectral density Φ of the output process of the underlying
system (3.7) is coercive in the sense that
Φ(eiθ ) > 0 for all θ ∈ [0, 2π].

(3.10)

In particular, y is a full-rank process, i.e. its components are linearly independent
sequences. Recall that a positive real function Z such that Φ(z) := Z(z) + Z(z −1 )
satisﬁes (3.10) is called strictly positive real.
Let H(w) be the Hilbert space generated by w, i.e. the closure of the linear space
spanned by the family {wi (t), i = 1 . . . p, t ∈ Z} with respect to the metric induced
by the inner product ξ, η = E{ξ η} where E{·} is deﬁned by (3.6). Let H + and H −
be the subspaces of H(w) generated by the components of future {y(0), y(1), y(2) . . . }
and past outputs {y(−1), y(−2), y(−3) . . . }, respectively.
4

This means that E{w(t)w(s) } = Iδts where δts is the Kronecker delta.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

13

The subspace
X := {a x(0) | a ∈ Rn }

(3.11)

is invariant under coordinate changes of the type (A, B, C) → (T AT −1 , T B, CT −1 )
and is a coordinate-free representation of the realization (3.7). Such an object is called
a Markovian splitting subspace in Lindquist and Picci (1985, 1991). Next deﬁne the
stationary Hankel operator of y, H : H + → H − as
−

H := E H |H +

(3.12)

−

where E H λ is the orthogonal projection of λ onto H − . The splitting subspace
property of X is equivalent to the commutativity of the diagram
H

H + −→ H −
O∗ 
C
X
i.e. to the factorization
H = CO∗ ,
+

(3.13)
−

where the operators O := E H |X and C := E H |X are the observability respectively
constructibility operators relative to the splitting subspace X. It can be shown that
the splitting subspace X is minimal if and only if O and C are both injective. (See,
e.g., Lindquist and Picci (1991).)
The system (3.7) is a forward or causal realization of y in the sense that the subspace
+
H (w), generated by the future of w, is orthogonal to X and H − , i.e. to the present
state and past output. Corresponding to (3.7) there is another realization

x̄(t − 1) = A x̄(t) + B̄ w̄(t − 1)
(3.14)
y(t − 1) = C̄ x̄(t) + D̄w̄(t − 1)
which is backward or anticausal in the sense that the subspace H − (w̄), generated by
the past of w̄, is orthogonal to X and H + . Like x(0), x̄(0) is a basis in X, i.e.
X := {a x̄(0) | a ∈ Rn }.

(3.15)

In fact, x̄(0) is the dual basis of x(0) in the sense that E{x(0)x̄(0) } = I. Hence
P̄ = P −1

x̄(0) = P −1 x(0).

(3.16)

The particular notations used in (3.7) and (3.14) reﬂect the special meaning of the
parameters (A, C, C̄). Computing the covariance matrix of the output using the dual
realizations (3.7) and (3.14), it is in fact readily seen that (A, C, C̄) is precisely a
triplet realizing the positive real part (1.6) of the spectral density matrix Φ(z) of the
time series y. There are inﬁnitely many minimal factorizations (3.13), one for each
Markovian splitting subspace, but the basis in each state space X can be chosen so
that the triplets (A, C, C̄) are the same for each minimal X. This is called a uniform
choice of bases (Lindquist and Picci, 1991).

14

ANDERS LINDQUIST AND GIORGIO PICCI

Important examples of minimal splitting subspaces are the forward and backward
predictor spaces
−

X− = E H H +

X+ = E H H − ,
+

(3.17)

which are the orthogonal complements of the null space of the Hankel operator (3.12)
and of its adjoint, respectively.
Fixing a uniform choice of bases, and thus the triplets (A, C, C̄), the splitting
subspace X− has the forward stochastic realization

x− (t + 1) = Ax− (t) + B− w− (t)
(3.18)
y(t) = Cx− (t) + D− w− (t)
with state covariance P− , and X+ has the backward realization

x̄+ (t − 1) = A x̄+ (t) + B̄+ w̄+ (t − 1)
y(t − 1) = C̄ x̄+ (t) + D̄+ w̄+ (t − 1)

(3.19)

with state covariance P̄+ .
These two stochastic realizations will play an important role in what follows. In
fact, an important interpretation of these realizations is that
−1
[y(t) − Cx− (t)]
x− (t + 1) = Ax− (t) + B− D−

is the unique steady-state Kalman ﬁlter of any minimal realization (3.7) of y in the
ﬁxed uniform choice of bases. Moreover, if P+ is the state covariance matrix (3.8)
corresponding to the forward counterpart of (3.19), i.e., P+ = (P̄+ )−1 , then
P − ≤ P ≤ P+

(3.20)

for the state covariance of any minimal realization (3.7).
In the same way
−1
[y(t − 1) − C x̄+ (t)]
x̄+ (t − 1) = A x̄+ (t) + B̄+ D̄+

is the backward steady-state Kalman ﬁlter of all minimal backward realizations (3.14),
and
P̄+ ≤ P̄ ≤ P̄−
for an arbitrary backward minimal realization (3.14), where P̄− is the backward counterpart of P− .
4. Canonical correlations and balanced stochastic realization
In this section we characterize the properties of minimal factorizations of the (stationary) Hankel operator (3.12) of a time series admitting a ﬁnite-dimensional realization
of the type (3.7). Equivalently, we study certain factorizations of the inﬁnite Hankel matrix of the corresponding inﬁnite covariance sequence {Λ0 , Λ1 , Λ2 , . . . }. Some
portions of this section can be found in an equivalent but somewhat diﬀerent setting
in Section 2 of Desai et al. (1985). Here we need to recall the basic concepts and
set notations. This will be done in the geometric framework of Section 3, thereby
providing several new insights.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

15

To obtain a convenient matrix representation of the Hankel operator H we shall
introduce orthonormal bases in H − and H + . To this end it will be useful to represent
past and future outputs as inﬁnite vectors in the form,




y(−1)
y(0)
y(−2)
y(1)



y
=
(4.1)
y− = 
+
y(−3)
y(2)
..
..
.
.
Let L− and L+ be the lower triangular Cholesky factors of the inﬁnite block Toeplitz
matrices


} = L− L−
T+ := E{y+ y+
} = L+ L+
T− := E{y− y−
and let
ν := L−1
− y−
be the corresponding orthonormal
implies that

Λ1

Λ2

}=
H∞ := E{y+ y−
Λ3
..
.

ν̄ := L−1
+ y+

(4.2)

bases in H − and H + respectively. Now, (3.9)
Λ2 Λ3
Λ3 Λ4
Λ4 Λ5
..
..
.
.

 


C̄
C
...
. . .  CA   C̄A 
 


. . . = CA2  C̄(A )2  ,
..
..
...
.
.

(4.3)

and therefore we have the following representation result, which can be found in Desai
et al. (1985).
Proposition 4.1. Let y be realized by a ﬁnite dimensional model of the form (3.7).
Then in the orthonormal basis (4.2) the matrix representation of the Hankel operator
H is

−T
−1
 −T
Ĥ∞ = L−1
+ E{y+ y− }L− = L+ ΩΩ̄ L− ,

where


C
 CA 

Ω=
CA2 
..
.

(4.4)





and


C̄
 C̄A 

Ω̄ = 
C̄(A )2  .
..
.

(4.5)

Note that, with a uniform choice of bases, we obtain the same matrix factorization
(4.3) for H∞ , irrespective of which X (i.e. which minimal realization of y) is chosen.
Recall that the adjoint O∗ of the observability operator O is deﬁned as the unique
linear operator H + → X such that Oξ, λ = ξ, O∗ λ for all ξ ∈ X and λ ∈ H + .
Orthogonality implies that
+

E H ξ, λ = ξ, λ = ξ, E X λ,
and therefore O∗ = E X |H + . In the same way, we see that C ∗ = E X |H − . The ﬁniterank linear operators O∗ O and C ∗ C are deﬁned on X and are the coordinate-free
representations of the observability and constructibility gramians. The splitting subspace X is observable if and only if O∗ O is full rank and constructible if and only if
C ∗ C is full rank. The following representations show that these gramians are related

16

ANDERS LINDQUIST AND GIORGIO PICCI

to P− and P̄+ , the state covariances of the forward and backward steady-state Kalman
ﬁlters (Picci and Pinzoni, 1994).
Proposition 4.2. Let x(0) and x̄(0) be the conjugate basis vectors in a minimal splitting subspace X as deﬁned above. Then, in a uniform choice of bases,
O∗ O a x̄(0) = a P̄+ x(0)

(4.6)

C ∗ C a x(0) = a P− x̄(0),

(4.7)

and
i.e., C ∗ C and O∗ O have matrix representations P− and P̄+ , respectively, independently
of X.
Proof. It is shown in Lindquist and Picci (1991) that, since X is minimal,
−

E H a x(0) = a x− (0),
and therefore

C ∗ C a x(0) = E X a x− (0) = E X a P− x̄− (0).
But, since the bases x̄(0) and x̄− (0) are chosen uniformly,
E X a x̄− (0) = a x̄(0) a ∈ Rn ,

and consequently (4.7) follows. The proof of (4.6) is analogous.
The factorization (4.4) can also be derived from (3.13) and the following useful
matrix representations of the observability and constructibility operators.
Proposition 4.3. Let x(0) and x̄(0) be basis vectors for the minimal splitting subspace X given by (3.11) and (3.15).Then
O a x̄(0) = a Ω L−T
+ ν̄

O∗ b ν̄ = b L−1
+ Ωx(0)

(4.8)

C a x(0) = a Ω̄ L−T
− ν

C ∗ b ν = b L−1
− Ω̄x̄(0),

(4.9)

and

where Ω and Ω̄ are given by (4.5).
Proof. Since, in view of (3.7),
y+ = Ωx(0) + terms which are orthogonal to X,
and ν̄ = L−1
+ y+ , we have
E{ν̄x(0) } = L−1
+ ΩP.

(4.10)

Consequently, for any a ∈ Rn , the usual projection formula5 yields
O a x(0) = E H a x(0) = a E{x(0)ν̄  }ν̄
+

and

O∗ b ν̄ = E X b ν̄ = b E{ν̄x(0) }P −1 x(0),
from which (4.8) follows. A symmetric argument yields (4.9).
If ξ ∈ H(w) and the subspace Z ⊂ H(w) is spanned by the components of the full-rank random
vector z, then E Z ξ = E{ξz  }(E{zz  })−1 z.
5

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

17

To interpret this result in the context of balanced realization theory one should
note that the matrix representations of O∗ and C ∗ are the transposes of those of O
and C if and only if x(0) is an orthogonal basis, i.e., P = P̄ = I. Moreover, it follows
from (4.8) that
O∗ Oa x̄(0) = a Ω T+−1 Ωx(0),
showing that Ω T+−1 Ω is a matrix representation of O∗ O, in harmony with the analysis
at the end of Section 2. In the same way, (4.9) yields
C ∗ Ca x(0) = a Ω̄ T−−1 Ω̄x̄(0),
and hence Ω̄ T−−1 Ω̄ is a matrix representation of C ∗ C. Together with Proposition 4.2
this yields the following explicit formulas for P− and P̄+ :
Ω T+−1 Ω = P̄+

Ω̄ T−−1 Ω̄ = P− .

(4.11)

Now, let {σ1 , σ2 , σ3 , . . . } be the singular values of the Hankel operator H. Since
rank H = n, σi = 0 for i > n. The nonzero singular values
1 ≥ σ1 ≥ σ2 ≥ σ3 . . . ≥ σn > 0

(4.12)

are the cosines of the angles between the subspaces H− and H+ ; they are known as the
canonical correlation coeﬃcients of y (Hotelling, 1936, Anderson, 1958). Obviously
σ1 < 1 if and and only if H− ∩ H+ = 0. The squares of the canonical correlation
coeﬃcients are the eigenvalues of H∗ H, i.e.,
H∗ H ξi = σi2 ξi ,
which, in view of (3.13) may be written
O∗ OC ∗ C(O∗ ξi ) = σi2 (O∗ ξi ),
and therefore, as was also demonstrated in Picci and Pinzoni (1994),
λ{O∗ OC ∗ C} = {σ12 , σ22 , . . . , σn2 },

(4.13)

i.e., σ12 , σ22 , . . . , σn2 are the eigenvalues of O∗ OC ∗ C. But, in view of Proposition 4.2,
this is precisely the coordinate-free version of the invariance condition
{σ12 , σ22 , . . . , σn2 } = λ{P− P̄+ }

(4.14)

of Desai and Pal (1984).
This suggests that an appropriate uniform choice of bases would be the one that
makes P− and P̄+ equal and equal to the diagonal matrix of nonzero canonical correlation coeﬃcients.
In fact, in view of Proposition 4.1, the inﬁnite normalized Hankel matrix Ĥ∞ is the
matrix representation of the operator H in the orthonormal bases (4.2). Therefore
Ĥ∞ has the singular-value decomposition
Ĥ∞ = U∞ Σ∞ V∞ = U ΣV  ,

(4.15)

where Σ is the diagonal n×n matrix consisting of the canonical correlation coeﬃcients
Σ = diag{σ1 , σ2 , σ3 , . . . , σn },

(4.16)

18

ANDERS LINDQUIST AND GIORGIO PICCI

and Σ∞ is the inﬁnite matrix
Σ∞

	


Σ 0
=
.
0 0

Moreover U∞ and V∞ are inﬁnite orthogonal matrices, and U and V are ∞ × n
submatrices of U∞ and V∞ with the the property that
U  U = I = V  V.

(4.17)


We now rotate the the orthonormal bases (4.2) in H + and H − to obtain u := U∞
ν̄


and v := V∞ ν respectively. Note that E{uv } = Σ∞ . What makes these orthonormal
bases useful is that they are adapted to the orthogonal decompositions6

H − ∨ H + = [H − ∩ (H + )⊥ ] ⊕ H  ⊕ [H + ∩ (H − )⊥ ],

(4.18)

where H  := X− ∨ X+ is the so-called frame space (Lindquist and Picci (1985, 1991),
in the sense that
X− = span{v1 , v2 , . . . , vn }

X+ = span{u1 , u2 , . . . , un }.

This is true since X− is precisely the subspace of random variables in H − having
nonzero correlation with the future H + and, dually, X+ is the subspace of random variables in H + having nonzero correlation with the past H − . Since therefore
{vn+1 , vn+2 , vn+3 , . . . } and {un+1 , un+2 , un+3 , . . . } span H − ∩ (H + )⊥ and H + ∩ (H − )⊥ ,
respectively, these spaces will play no role in what follows.
Now deﬁne the n-dimensional vectors
 1/2 
 1/2 
σ1 u1
σ1 v1
 σ 1/2 u 
 σ 1/2 v 
2
2


z̄ =  2 .  = Σ1/2 U  L−1
(4.19)
z =  2 .  = Σ1/2 V  L−1
− y−
+ y+
.
.
 . 
 . 
1/2

1/2

σn vn

σn un

From what we have seen before, z is a basis in X− and z̄ is a basis in X+ , and they
have the properties
E{zz  } = Σ = E{z̄ z̄  }.

(4.20)

In fact, we even have more as seen from the following ampliﬁcation7 of a theorem by
Desai and Pal (1984) (Theorem 1).
Theorem 4.4. The basis vectors
x− (0) = z

x̄+ (0) = z̄

(4.21)

in X− and X+ respectively belong to the same uniform choice of basis, i.e. to the
same choice of triplets (A, C, C̄), and in this uniform choice
P− = Σ = P̄+ .

(4.22)

The symbols ∨ and ⊕ denote vector sum and orthogonal vector sum of subspaces.
A priori there is no reason why choosing bases in X− and X+ would lead to the same (A, C, C̄).
This important property is explicitly mentioned in Theorem 4.4.
6
7

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

19

If the canonical correlation coeﬃcients {σ1 , σ2 , σ3 , . . . , σn } are distinct, this is, modulo
multiplication with a signature matrix 8 , the only uniform choice of bases for which
( 4.22) holds.
Such a choice of (A, C, C̄) is know as stochastically balanced, and, in the case of
distinct canonical correlation coeﬃcients, it deﬁnes a canonical form with respect to
state space isomorphism in (1.6) by ﬁxing the sign in, say, the ﬁrst element in each
row of C. Such canonical forms have also been studied by Ober (1991).
Proof. It follows from (4.4) and (4.15) that
E{z̄z  } = Σ2 .

(4.23)

Now, choose (A, C, C̄) so that x̄+ (0) = z̄, and let the bases in the other splitting
subspaces be chosen accordingly so that the choice of bases is uniform. We want
to show that x− (0) = z. To this end, ﬁrst note that x+ (0) = Σ−1 x̄+ (0) and that
x− (0) = E X− x+ (0); see Lindquist and Picci (1991). Then, by usual projection formula
and the fact that z is a basis in X− ,
x− (0) = Σ−1 E{z̄z  }Σ−1 z,
which, in view of (4.23), yields x− (0) = z as claimed. Hence (4.22) follows from
(4.20).
Next, suppose that (QAQ−1 , CQ−1 , C̄Q ) is another uniform choice of bases which
is also stochastically balanced. Since then x− (0) = Qz and, as is readily seen from
the backward system (3.14), x̄+ (0) = Q−T z̄ so that P− = QΣQ and P̄+ = Q−T ΣQ−1 ,
(4.22) yields
QΣQ = Σ and Q−T ΣQ−1 = Σ,
from which we have
QΣ2 = Σ2 Q.
Since Σ has distinct entries, it follows from Corollary 2, p.223 in Gantmacher (1959)
that there is a scalar polynomial ϕ(z) such that Q = ϕ(Σ2 ). Hence Q is diagonal and
commutes with Σ so that, by QΣQ = Σ, we have
QQ = I.
Consequently, since Q is diagonal, it must be a signature matrix.
In view of (4.21) and (3.16), the ﬁrst of relations (4.9) and (4.8) respectively yield
z = Ω̄ T−−1 y−

z̄ = Ω T+−1 y+ .

(4.24)

Consequently, in view of (4.20), (2.15) holds also for the case of an inﬁnite Hankel
matrix. This can of course also be seen from (4.11).
Note that the normalization of the block Hankel matrix H∞ is necessary in order for
the singular values to become the canonical correlation coeﬃcients, i.e., the singular
values of H. In fact, if we were to use the unnormalized matrix representation (4.3)
of H instead, as may seem simpler and more natural, the transpose of (4.3) would
not be the matrix representation of H∗ in the same bases, a property which is crucial
in the singular value decomposition above. This is because (4.3) corresponds to the
bases y− in H − and y+ in H + , which are not orthogonal. As we shall see in the next
8

A signature matrix is a diagonal matrix of ±1.

20

ANDERS LINDQUIST AND GIORGIO PICCI

section, this holds also in applicable parts for the ﬁnite-dimensional case studied in
Section 2, and therefore the normalized Hankel matrix Ĥ, deﬁned in Section 2, is
preferable to the unnormalized H.
Formulas, such as (2.5), expressing A, C, C̄ in terms of the Hankel matrix H∞ , can
be easily derived from basic principles. In fact, standard calculations based on the
forward model (3.7) and the backward model (3.14) yield
A = E{x(1)x(0) }P −1
C = E{y(0)x(0) }P −1 ,
C̄ = E{y(−1)x̄(0) }P̄ −1 = E{y(−1)x(0) }

(4.25a)
(4.25b)
(4.25c)

for any dual pair of bases x(0) and x̄(0).
Proposition 4.5. The triplet (4.25) corresponding to the stochastically balanced bases
(4.19) can be computed by means of the formulas
−T
−1/2
,
A = Σ−1/2 U  L−1
+ σ(H∞ )L− V Σ

(4.26a)

−1/2
C = ρ1 (H∞ )L−T
,
− VΣ

(4.26b)


−1/2
ρ1 (H∞
)L−T
,
+ UΣ

(4.26c)

C̄ =

where H∞ is the unnormalized Hankel matrix (4.3), σ(H∞ ) is obtained from H∞ by
deleting the ﬁrst block row, and ρ1 (H∞ ) is the ﬁrst block row.
Proof. First, in (4.25a) and (4.25b), we take x(0) to be x− (0). By the Kalman ﬁlter
representation a [x+ (1) − x− (1)] ⊥ UH − ⊃ H − for all a ∈ Rn ,
E{x− (1)x− (0) } = E{x+ (1)x− (0) } = P̄+−1 E{x̄+ (1)x− (0) }.
But (A, C, C̄) is stochastically balanced, and therefore, by Theorem 4.4 and (4.19),
1/2  −1
U L+ σ(y+ ), where σ(y+ )
P− = Σ = P̄+ , x− (0) = Σ1/2 V  L−1
− y− and x̄+ (1) = Σ
is obtained from y+ by deleting the subvector corresponding to time t = 0. Consequently, in view of (4.25a),

−T
−1/2
,
A = Σ−1/2 U  L−1
+ E{σ(y+ )y− }L− V Σ

which is identical to (4.26a). Likewise, from (4.26b),
−1/2
,
C = E{y(0)y− }L−T
− VΣ

which yields (4.26b). Finally, taking x̄(0) to be x̄+ (0) in (4.25c), a symmetric argument yields (4.26c).
Note that (4.26) are obtained by applying the Ho-Kalman algorithm to H∞ factorized corresponding to the singular-value decomposition (4.15).

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

21

5. Stochastic realization from ﬁnite covariance data
In this section we modify the realization theory of Section 4 to the case that only a
ﬁnite segment
{y(0), y(1), y(2), . . . , y(ν)},

(5.1)

of the time series {y(t)} is available. We still deﬁne each y(t) as the semi-inﬁnite
string (3.3) of data, and therefore we can form, via the ergodic limit (1.11), an exact
partial covariance sequence
{Λ0 , Λ1 , Λ2 . . . , Λν }.

(5.2)

The corresponding realization problem, which is purely theoretical and is intended to
prepare for the more realistic identiﬁcation situation with ﬁnite strings of observed
data (Section 6), is therefore the partial stochastic realization problem mentioned in
Section 2. We retain the crucial Assumption 2.1, implying that the data (5.1) is the
output of some minimal “true” system (3.7) of dimension n and that ν is large enough
for n to equal the positive degree of the partial sequence (5.2).
Now, suppose that ν = 2τ − 1, and partition the data into two matrices




y(0)
y(τ )
 y(1) 
 y(τ + 1) 
+


,
y
=
(5.3)
yτ− = 
.
..
τ




..
.
y(τ − 1)

y(2τ − 1)

representing the past and the future respectively, and deﬁne the corresponding (ﬁnitedimensional) subspaces Yτ− and Yτ+ spanned by the rows of yτ− and yτ+ respectively as
explained in Section 3. Since the data size τ will be important in the considerations
that will follow, we denote the ﬁnite block Hankel matrix H of Section 2, relative to
the data (5.3), by Hτ , i.e.,
Hτ = E{yτ+ (yτ− ) }.

(5.4)

Let τ0 be the smallest integer τ such that rank Hτ = n. It is well-known that τ0 is
the maximum of the observability and constructibility indicies of (A, C, C̄), so n is an
upper bound for τ0 . As pointed out in the beginning of Section 2, we need τ > τ0 to
be certain that the factorization of Hτ yields a unique (A, C, C̄).
Next we shall consider the class of minimal splitting subspaces for Yτ− and Yτ+ , i.e.,
the subspaces Xτ admitting a canonical factorization
H

τ
Yτ−
Yτ+ −→
Oτ∗ 
 Cτ
Xτ

of the ﬁnite-interval Hankel operator
−

Hτ := E Yτ |Yτ+ .

(5.5)

It is standard (Lindquist and Picci, 1985, 1991) to show that the forward and backward predictor spaces,
−

X̂τ − = E Yτ Yτ+

+

and X̂τ + = E Yτ Yτ− ,

22

ANDERS LINDQUIST AND GIORGIO PICCI

are such minimal splitting subspaces. The proof of the following theorem is deferred
to Appendix D.
Theorem 5.1. Let X be a minimal Markovian splitting subspace for the stationary
time series {y(t)}. Then, if τ > τ0 ,
Xτ := Uτ X

(5.6)

is a minimal splitting subspace for Yτ− and Yτ+ , and
−

X̂τ − = E Yτ Xτ ,

+

X̂τ + = E Yτ Xτ .

(5.7)

Conversely, any basis x̂(τ ) in X̂τ − has a unique representation9
−

x̂(τ ) = E Yτ x(τ ),

(5.8)

ˆ (τ ) in X̂τ + has a unique representation
where x(τ ) is a basis in Xτ , and any basis x̄
+

ˆ (τ ) = E Yτ x̄(τ ),
x̄

(5.9)

X

with x̄(τ ) a basis in Xτ . As X varies over the family
of all minimal Markovian
splitting subspaces, the corresponding x(0) [x̄(0)] constitute a uniform choice of bases.
The stochastic realizations corresponding to the ﬁnite-interval predictor spaces X̂τ −
and X̂τ + are nonstationary. However, taking advantage of the representations (5.8)
and (5.9), we shall be able to express these realizations in such a way that they can
be parameterized by the stationary triplet (A, C, C̄) corresponding to one uniform
choice of bases, both for the forward and the backward settings. In fact, if the bases
ˆ (τ ) are chosen so that x(τ ) and x̄(τ ) in representations (5.8) and (5.9) are
x̂(τ ) and x̄
dual bases in Xτ , i.e., E{x(τ )x̄(τ )} = I, then the same choice of (A, C, C̄) is used for
all X ∈ . Such a choice of bases in X̂τ − and X̂τ + is called coherent.
The realizations generated by these coherent bases are precisely the (transient)
forward and backward Kalman ﬁlters. In fact, the vector x̂(τ ) is the one-step predictor
of x(τ ) based on Yτ− and, as shown in Appendix C, it evolves in time as the Kalman
ﬁlter

X

x̂(t + 1) = Ax̂(t) + K(t)[y(t) − C x̂(t)];

x̂(0) = 0,

(5.10)

where the gain K(t) is given by
K(t) = (C̄  − AP− (t)C  )(Λ0 − CP− (t)C  )−1

(5.11)

and the ﬁlter estimate covariance
P− (t) = E{x̂(t)x̂(t) }

(5.12)

is the solution of the matrix Riccati equation

P− (t + 1) = AP− (t)A + (C̄  − AP− (t)C  )(Λ0 − CP− (t)C  )−1 (C̄  − AP− (t)C  )
P− (0)) = 0.
(5.13)
9

With slight misuse of notations, the orthogonal projection operator applied to a vector will
denote the vector of the projections of the components.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

23

Symmetrically, in terms of the backward system (3.14) corresponding to (3.7), the
components of
+

ˆ (τ ) = E Yτ x̄(τ )
x̄

(5.14)

form a basis in X̂τ + and are generated by the backward Kalman ﬁlter
ˆ (t − 1) = A x̄
ˆ (t) + K̄(t)[y(t − 1) − C̄ x̄
ˆ (t)];
x̄

ˆ (2τ − 1) = 0,
x̄

(5.15)

with
K̄(t) = (C  − A P̄+ (t)C̄  )(Λ0 − C̄P− (t)C̄  )−1 ,

(5.16)

ˆ (t)x̄
ˆ (t) }
P̄+ (t) = E{x̄

(5.17)

where

is obtained by solving the matrix Riccati equation

P̄+ (t − 1) = A P̄+ (t)A + (C  − A P̄+ (t)C̄  )(Λ0 − C̄ P̄+ (t)C̄  )−1 (C  − A P̄+ (t)C̄  )
P̄+ (2τ − 1) = 0.
(5.18)
Now, it is well-known that both
ν(t) = (Λ0 − CP− (t)C  )−1/2 [y(t) − C x̂(t)]

(5.19)

ˆ (t)]
ν̄(t) = (Λ0 − C̄ P̄+ (t)C̄  )−1/2 [y(t − 1) − C̄ x̄

(5.20)

and

are normalized white noises, called the forward respectively the backward (transient)
innovation processes. Consequently, we may write the Kalman ﬁlter (5.10) as

x̂(t + 1) = Ax̂(t) + B− (t)ν(t)
(5.21)
y(t) = C x̂(t) + D− (t)ν(t)
where D− (t) := (Λ0 − CP− (t)C  )1/2 and B− (t) := K(t)D− (t). Likewise, the backward
Kalman ﬁlter (5.10) may be written

ˆ (t) + B̄+ (t)ν̄(t − 1)
ˆ (t − 1) = A x̄
x̄
(5.22)
ˆ (t) + D̄+ (t)ν̄(t − 1)
y(t − 1) = C̄ x̄
where D̄+ (t) := (Λ0 − C̄ P̄+ (t)C̄  )1/2 and B̄+ (t) := K̄(t)D̄+ (t). Comparing with (3.7)
and (3.14), we see that (5.21) and (5.22) are stochastic realizations, which unlike (3.7)
and (3.14) are time-varying and describe the output y only on the interval [0, 2τ − 1].
In fact, since
P − P− (t) = E{[x(t) − x̂(t)][x(t) − x̂(t)] } ≥ 0,
and, for the same reason, P̄ − P̄+ (t) ≥ 0, we have
P− (t) ≤ P ≤ P+ (t) := P̄+ (t)−1 ,

(5.23)

so we see that the predictor spaces X̂τ − and X̂τ + are extremal splitting subspaces,
just as X− and X+ in (3.20).

24

ANDERS LINDQUIST AND GIORGIO PICCI

It is now immediately seen that the ﬁnite-interval counterparts of equations (4.25)
are given by
A = E{x̂(τ + 1)x̂(τ ) }P− (τ )−1
C = E{y(τ )x̂(τ ) }P− (τ )−1 ,
ˆ (τ ) }P̄+ (τ )−1 = E{y(τ − 1)x̂(τ ) }
C̄ = E{y(τ − 1)x̄

(5.24a)
(5.24b)
(5.24c)

In complete analogy with the stationary framework in Section 4, the canonical
correlation coeﬃcients
1 ≥ σ1 (τ ) ≥ σ2 (τ ) ≥ · · · ≥ σn (τ ) > 0

(5.25)

between the ﬁnite past Yτ− and the ﬁnite future Yτ+ are now deﬁned as the singular
values of the operator Hτ given by (5.5). To determine these we need a matrix representation of Hτ in some orthonormal bases. Using the pair (5.19)–(5.20) of transient
innovation processes for this purpose, we obtain the normalized matrix (2.14), which
we shall here denote Ĥτ . Singular value decomposition yields
Ĥτ = Uτ Στ Vτ ,

(5.26)

where Uτ Uτ = I = Vτ Vτ , and Στ is the diagonal matrix of canonical correlation
coeﬃcients. As in Section 4 it is seen that

1/2
−1 −
z(τ ) = Στ Vτ (L−
τ ) yτ
(5.27)
1/2
−1 +
z̄(τ ) = Στ Uτ (L+
τ ) yτ
are bases in X̂τ − and X̂τ + respectively and that
E{z(τ )z(τ ) } = Στ = E{z̄τ z̄τ }.

(5.28)

+
Here L−
τ and Lτ are the ﬁnite-interval counterparts of L− and L+ respectively, and
they are of course submatrices of these. Note that Hτ , as deﬁned by (5.4), is now
given by
− 
H τ = L+
τ Ĥτ (Lτ ) .

(5.29)

We observe that, in analogy to Theorem 4.4, z(τ ) and z̄(τ ) are coherent bases, and the
corresponding triplet (A, C, C̄) is a ﬁnite-interval stochastically balanced realization,
i.e.,
P− (τ ) = Στ = P̄+ (τ ).

(5.30)

The following ﬁnite-interval modiﬁcation of Proposition 4.5 is essentially the canonical singular-value decomposition version of the Ho-Kalman algorithm applied to the
ﬁnite block hankel matrix Hτ , and the proof is analogous.
Proposition 5.2. The ﬁnite-interval stochastically balanced triplet (Aτ , Cτ , C̄τ ), obˆ (τ ) = z̄(τ ), is given by
tained from (5.24) by choosing the bases x̂(τ ) = z(τ ) and x̄
−1
− −T
Uτ (L+
Vτ Σ−1/2
,
Aτ = Σ−1/2
τ
τ ) σ(Hτ )(Lτ )
τ
−T
Vτ Σ−1/2
,
Cτ = ρ1 (Hτ )(L−
τ )
τ

+ −T
−1/2
C̄τ = ρ1 (Hτ )(Lτ ) Uτ Στ ,

(5.31a)
(5.31b)
(5.31c)

where the operators σ(·) and ρ1 (·) are deﬁned as in Section 2 and in Proposition 4.5.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

25

Note that the triplet (Aτ , Cτ , C̄τ ) actually varies with τ , but that, for each τ , it
is similar to the stochastically balanced triplet (A, C, C̄) of Section 4, i.e., there is a
nonsingular matrix Qτ so that
−1

(Aτ , Cτ , C̄τ ) = (Qτ AQ−1
τ , CQτ , C̄Qτ ).

(5.32)

It is easy to check that, in the uniform choice of bases corresponding (5.32), the
stationary predictor spaces X− and X+ will have the state covariances
P− = Qτ ΣQτ

−1
and P̄+ = Q−T
τ ΣQτ ,

(5.33)

analogously to the situation in the proof of Theorem 4.4. The fact that these state
covariances are not diagonal and equal is a manifestation of the fact that the triplet
(Aτ , Cτ , C̄τ ) is not stochastically balanced in the sense of Section 4. It is well known
that P− (t) and P̄+ (t) tend monotonically to P− and P̄+ , respectively, as t → ∞, and
therefore we have the following ordering
P− (τ ) := Στ ≤ P− ≤ (P̄+ )−1 ≤ (P̄+ (τ ))−1 := Σ−1
τ .
Since the number n of nonzero singular values (5.25) is in general too large too
yield a reasonable model, we must consider what happens when some of the smallest
singular values are set equal to zero. The truncation procedure employed by van
Overschee and De Moor (1993) is equivalent to the principal subsystem truncation
presented in Section 2, except that, and this is very important, the singular-value
decomposition is performed on the normalized block Hankel matrix Ĥτ , which is the
natural matrix representation of the operator τ . It will be shown in Section 7 that
such a truncation will preserve positivity in the stationary case (Theorem 7.3). In
order to carry this result over to the case of ﬁnite τ , we need to assume that the
spectral density Φ of the time series {y(t)} is coercive so that Assumption 3.2 is
fulﬁlled, i.e., that the function Z is strictly positive real.
The following theorem is a corollary of Theorem 7.3, to be proved in Appendix D,
shows that principal subsystem truncation preserves positivity provided τ is chosen
large enough.

H

Theorem 5.3. Suppose that the spectral density Φ of the time series {y(t)} is coercive. Then, there is an integer τ1 > τ0 such that, for τ ≥ τ1 , the principal subsystem
truncation ((Aτ )11 , (Cτ )1 , (C̄τ )1 ) of (Aτ , Cτ , C̄τ ) is a minimal realization of a strictly
positive real function (2.13).
6. Subspace identiﬁcation
The analysis in Sections 3, 4 and 5 is based on the idealized assumption that we have
access to an inﬁnite sequence (3.2) of data. In reality we will have a ﬁnite string of
observed data
{y0 , y1 , y2 , . . . , yN },

(6.1)

where, however, N may be quite large. More speciﬁcally, we assume that N is sufﬁciently large that replacing the ergodic limits (1.11) by truncated sums yields good
approximations of
{Λ0 , Λ1 , Λ2 . . . , Λν },

(6.2)

26

ANDERS LINDQUIST AND GIORGIO PICCI

where, of course, ν << N . This is equivalent to saying that T := N − ν is suﬃciently
large for
1  

a yt+k yt+j
b
T + 1 t=0
T

(6.3)

to be essentially the same as the inner product (3.4). In this section, therefore, we
shall use the ﬁnite-interval realization theory of Section 5 as if we had a ﬁnite time
series
{y(0), y(1), y(2), . . . , y(ν)},

(6.4)

while substituting the semi-inﬁnite string (3.3) of data by
y(t) = [yt , yt+1 , . . . , yT +t ] for t = 0, 1, . . . , ν.

(6.5)

In particular, in this case the inner product becomes merely that of a ﬁnite-dimensional
Euclidean space so that the block Hankel matrix Hτ can be written
Hτ =
where



yτ −1 yτ . . . yT +τ −1
yτ −2 yτ −1 . . . yT +τ −2 
yτ− = 
..
.. 
..
 ...
.
.
. 
y0
y1 . . .
yT

1
y + (y − )
T +1 τ τ



yτ +1 . . .
yT +τ
yτ
 yτ +1 yτ +2 . . . yT +τ +1 
and yτ+ = 
.
..
.. 
..
 ...
.
.
. 
y2τ −1 y2τ . . . yT +2τ −1

Consequently, the identiﬁcation of a minimal stationary state-space innovation
model describing the data (6.1) can be performed in the following steps.
(1) Perform canonical correlation analysis on the data yτ− , yτ+ to obtain, from
ˆ + (τ ) = z̄(τ ) and, from (5.26), the
(5.27), the state vectors x̂− (τ ) = z(τ ) and x̄
corresponding common state covariance matrix Στ , i.e., the diagonal matrix of
the (ﬁnite interval) canonical correlation coeﬃcients (5.25).
(2) Given the singular value decomposition (5.26), compute via (5.31) a minimal
realization (A, C, C̄). This realization will be in ﬁnite-interval balanced form,
i.e., (5.30) will hold instead of (4.22).
(3) To obtain a state space model (3.7) for y we need to compute the matrices B
and D. Note that such matrices will exist if and only if (A, C, C̄, Λ0 ) deﬁnes
a positive real function (1.6), or, in other words, if and only if there is a
symmetric positive deﬁnite P = P  such that


	
P − AP A C̄  − AP C 
≥ 0.
(6.6)
M (P ) :=
C̄ − CP A Λ0 − CP C 
[See, e.g., Faurre et al. (1979) or Willems (1971).] For each P satisfying (6.6),
B and D can be determined (in a nonunique way) by a full rank factorization
of M (P ), i.e.,
	 


B  
B D = M (P ).
(6.7)
D

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

27

(4) In particular, the (stationary) forward innovation model (3.18) can be determined in this way once the state covariance P− = E{x− (t)x− (t) } has been
determined. Obtaining P− amounts to ﬁnding the minimal solution of the
algebraic Riccati equation
P = AP A + (C̄  − AP C  )(Λ0 − CP C  )−1 (C̄  − AP C  )

(6.8)

or, alternatively, taking the limit in the Riccati equation (5.13) as t → ∞
with initial condition P− (τ ) = Στ . (The corresponding dual procedures yield
P̄+ .) Again, in both cases, a positive deﬁnite P− can be found if and only
if (A, C, C̄, Λ0 ) deﬁnes a positive real function (1.6). In fact, in general,
{P− (t)}t≥0 may not even converge unless this positivity condition is fulﬁlled
and may in fact exhibit dynamical behavior with several of the characteristics
of chaotic dynamics (Byrnes et al., 1991, 1994).
Assuming that Assumption 2.1 holds, this procedure is consistent in the sense that,
for τ ﬁxed but suﬃciently large (see Section 2), we will have rank Hτ = n as T → ∞,
and the triplet (A, C, C̄) will be uniquely determined from the data and similar to the
triplet (A, C, C̄) of the “true” generating system. Hence, in particular, in the limit as
T → ∞, at least in theory positivity will be guaranteed. If n̂ is an upper bound for
the order of the “true” system, we may choose τ to be any integer larger than n̂.
In practice, however, T is ﬁnite, and even if we had a true system generating exact
data, the spectral estimate ΦT , although converging to the true spectrum Φ as T → ∞
may in principle fail to be positive for any ﬁnite T if there are frequencies ω for which
Φ(eiω ) = 0. Positivity for a suitably large T can however be guaranteed if the “true”
spectrum is coercive. The following proposition, which also applies to Aoki’s method
discussed in Section 2, is proved in Appendix D.
Proposition 6.1. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulﬁlled. Then, there is a T0 ∈ Z+ such that, for T ≥ T0 , the triplet (A, C, C̄) deﬁned by
(5.31) yields a function (1.6) which is strictly positive real.
However, in practice, rank Hτ normally will keep increasing with τ , even for very
large T , so that one must resort to some kind of truncation of the Hankel singular
values. As we have pointed out in Section 5, setting all canonical correlation coeﬃcients σr+1 (τ ), σr+2 (τ ), . . . equal to zero for some suitable r, as is done in, for example,
van Overschee and De Moor (1993), is equivalent to principal subsystem truncation.
An important issue is therefore under what conditions such a procedure will insure
positivity. Here we must distinguish between problems generated by the sample ﬂuctuations of the data due to ﬁnite sample size T , as considered in Proposition 6.1, and
the system theoretical question of preserving positivity under truncation, as considered in Theorem 5.3. Even if we had an inﬁnite string of data generated by a “true”
high-dimensional system, such a truncation procedure may fail if τ is smaller than
that dimension.
Combining Theorem 5.3 with Proposition 6.1, we immediately obtain the following
result, which justiﬁes this approximation procedure, provided the rather stringent
Assumption 2.1 holds and we have coercivity, and provided T and τ are suﬃciently
large.

28

ANDERS LINDQUIST AND GIORGIO PICCI

Theorem 6.2. Suppose that the conditions of Assumptions 2.1 and 3.2 are fulﬁlled.
Then, there are positive integers T0 and τ1 > τ0 such that, for T ≥ T0 and τ ≥ τ1 , the
triplet (A11 , C1 , C̄1 ), obtained from (2.12) by taking H := Hτ in (2.10), is a minimal
realization of a strictly positive real function (2.13).
We note that, in van Overschee and De Moor (1993), the large Hankel matrix
H̃τ = (yτ+ ) (E{yτ+ (yτ+ ) })−1 E{yτ+ (yτ− ) }(E{yτ− (yτ− ) })−1 yτ−
is used in place of Ĥτ . This leads to a procedure which is equivalent to the one
described above. Moreover, the computation of a second singular-value decomposition
in van Overschee and De Moor (1993), based on Hτ +1 := E{yτ++1 (yτ−+1 ) }, together
with a subsequent change of bases, is actually redundant, as can be deduced from
the following proposition. In fact, a considerable amount of computation is needed in
van Overschee and De Moor (1993) to compensate for the fact that taking z(τ + 1),
computed from a second singular-value decomposition, as a basis in X̂(τ +1)− would
lead to a Kalman ﬁlter model with time-varying parameters.
ˆ (τ ) in the ﬁnite-interval
Proposition 6.3. To each coherent pair of bases x̂(τ ) and x̄
predictor spaces X̂τ − and X̂τ + , there corresponds a minimal factorization
Hτ = Ωτ Ω̄τ

(6.9)

of the block Hankel matrix Hτ . Here
−

Ωτ x̂(τ ) = E Yτ yτ+

and

+

ˆ (τ ) = E Yτ yτ− .
Ω̄τ x̄

(6.10)

Conversely, given a minimal factorization (6.9),
x̂(τ ) = Ω̄τ (Tτ− )−1 yτ−

and

ˆ (τ ) = Ωτ (Tτ+ )−1 yτ+
x̄

(6.11)

is a coherent pair of bases in X̂τ − and X̂τ + .
ˆ (τ ) be a coherent choice of bases in X̂τ − and X̂τ + . Then, for
Proof. Let x̂(τ ) and x̄
any Xτ as deﬁned in Theorem 5.1, there is a unique pair (x(τ ), x̄(τ )) of dual bases
such that (5.8) and (5.9) hold. Let Ωτ and Ω̄τ be the matrices deﬁned via
E Xτ yτ+ = Ωτ x(τ ) and E Xτ yτ− = Ω̄τ x̄(τ ).

(6.12)

Then, the splitting property (Lindquist and Picci, 1985, 1991) of Xτ with respect to
Yτ− and Yτ+ yields
E{yτ+ (yτ− ) } = E{E Xτ yτ+ (E Xτ yτ− )) },
−

+

which, in view of (6.12), is the same as (6.9). Applying E Yτ and E Yτ to respectively
the ﬁrst and second equations of (6.12), the splitting property yields (6.10).
As for the converse statement, equations (6.11) follow from the construction in the
proof of Theorem 5.1, from which it also follows that the resulting bases x̂(τ ) and
ˆ (τ ) are constructed from the same (A, C, C̄) and therefore coherent.
x̄
As soon as the parameters (A, C, C̄) have been ﬁxed by a particular choice of x(τ )
in the representation (5.8) in Theorem 5.1, we must choose x̂(τ + 1) as
x̂(τ + 1) = E Yτ +1 Ux(τ )

(6.13)

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

29

to stay within the same uniform choice of bases. More speciﬁcally Proposition 6.3
implies that Ωτ and Ω̄τ are uniquely determined once x(τ ) has been selected. Hence
(A, C, C̄) is uniquely determined by the Ho-Kalman algorithm so that
	


C̄
Ω̄τ +1 =
Ω̄τ A
is prescribed, as is
x̂(τ + 1) = Ω̄τ (Tτ−+1 )−1 yτ−+1 .

(6.14)

Of course, this analysis is purely conceptual, demonstrating that the step determining
x̂(τ + 1) by an extra singular-value decomposition, as in van Overschee and De Moor
(1993), is actually redundant. If we actually were to determine x̂(τ + 1) as described
above, we would better compute Ω̄τ +1 from Ω̄τ +1 = Ω−L
τ Hτ +1 , where the left inverse
is very easily obtained from the singular-value decomposition of Hτ .
We stress that Assumption 2.1, although quite limiting, is absolutely crucial in
insuring that the subspace identiﬁcation algorithms mentioned above will actually
work. Note that for generic data these algorithms may break down for any ﬁxed τ .
The same is true for all other subspace methods which deal with identiﬁcation of
covariance models (or equivalent) involving stochastic signals.
On the other hand, Assumption 2.1 introduces a quite unrealistic condition which,
as we have seen in Section 2, is untestable. Moreover, we have absolutely no procedure
to estimate T0 and τ1 in Proposition 6.2, as the proof is based only on continuity
arguments.
7. Stochastic model reduction
As we have already pointed out, some truncation procedure or stochastic model reduction technique may have to be employed in the partial stochastic realization step
in order to keep the dimension of the model at a reasonable level. To justify any
such procedure one must either assume that there is an underlying “true” system of
suﬃciently low order, i.e., invoke Assumption 2.1, or to perform rational covariance
extension [Kalman (1981), Georgiou (1987), Kimura (1987), Byrnes et al. (1995),
Byrnes and Lindquist (1996)] to extend the covariance sequence (5.2) to an inﬁnite
one. The latter can be done in many ways, one of which is the maximum entropy
extension.
In either case, the truncation problem is equivalent to approximating a positive
real matrix function
1
Z(z) = C(zI − A)−1 C̄  + Λ0 ,
2

(7.1)

of a degree n which is often too large, by another positive real matrix function Z1 of
lower degree. In this section we shall investigate how this can be done and also how
such an approximation aﬀects the canonical correlation structure.
One main question to be addressed is whether the principal subsystem truncation
(2.11) preserves positive realness and balancing, and hence the leading canonical
correlation coeﬃcients, as originally claimed by Desai and Pal (1982). As it turns
out, the answer is aﬃrmative to the ﬁrst but not to the second of these questions.

30

ANDERS LINDQUIST AND GIORGIO PICCI

This also explains the nature of the subspace-identiﬁcation approximation obtained
by setting some canonical correlation coeﬃcients equal to zero.
It is instructive to ﬁrst consider the continuous-time counterpart of this problem
since the latter is simpler and exhibits more desirable properties. Also, it has been
widely believed that the continuous-time results are valid also in the present discretetime setting, which in general is not true.
It is well-known [see, e.g., Faurre et al. (1979)] that an m × m matrix function Z
with minimal realization
1
(7.2)
Z(s) = C(sI − A)−1 C̄  + R,
2
is positive real with respect to the right half plane if and only if there is a symmetric
matrix P > 0 such that


	
−AP − P A C̄  − P C 
≥ 0,
(7.3)
M (P ) :=
C̄ − CP
R
where here we assume that R is positive deﬁnite and symmetric. In this case there
are two solutions of (7.3), P− and P+ , with the property that any other solution of
(7.3) satisﬁes
P − ≤ P ≤ P+ .

(7.4)

These extremal solutions play the same role as P− and P+ in the discrete-time setting,
and
rank M (P− ) = m = rank M (P+ ).

(7.5)

If the state-space coordinates are chosen so that both P− and P̄+ := P+−1 are diagonal
and equal, and thus, by (4.14), equal to the diagonal matrix Σ of canonical correlation
coeﬃcients, we say that (A, C, C̄) is stochastically balanced.
Now, suppose that Σ is partitioned as in (2.8) with σr+1 < σr , and consider the
corresponding principal subsystem truncation (2.12). Using the stochastic realization
framework, Harshavaradana, Jonckheere and Silverman (1984) showed that
1
Z1 (s) = C1 (sI − A11 )−1 C̄1 + R,
2

(7.6)

is a minimal realization of a positive real function and conjectured that (A11 , C1 , C̄1 )
is stochastically balanced. We shall next show that this conjecture is true, as has
already been done by Ober (1991) in a framework of canonical forms.
First, note that positivity is easily proved by inserting (2.8) into (7.3) to yield


−A11 Σ1 − Σ1 A11 ∗ C̄1 − Σ1 C1
 ≥ 0,

∗
∗
∗
(7.7)
∗
R
C̄1 − C1 Σ1
where blocks which play no role in the analysis are marked by an asterisk. Consequently,


	
−A11 Σ1 − Σ1 A11 C̄1 − Σ1 C1
≥ 0.
(7.8)
M1 (Σ1 ) =
C̄1 − C1 Σ1
R

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

31

Since, in addition, it can be shown that A11 is stable [Pernebo and Silverman (1982),
Harshavaradhana et al. (1984)], i.e., has all its eigenvalues in the open left half plane,
(7.6) is positive real, but it remains to prove that (A11 , C1 , C̄1 ) is a minimal realization.
This was done in Harshavaradhana et al. (1984). It is important to observe here that,
contrary to the situation in the discrete-time setting, rank M1 (Σ1 ) = rank M (Σ) = m
−1
and rank M1 (Σ−1
1 ) = rank M (Σ ) = m, important facts that will be seen to imply
that the reduced system is stochastically balanced.
Recall that in the continuous-time setting the spectral density Φ(s) = Z(s)+Z(−s)
is coercive if, for some < > 0, we have Φ(s) ≥ <I for all s on the imaginary axis. This
is equivalent to the condition that R > 0 and Φ has no zeros on the imaginary axis
(Faurre et al., 1979, Theorem 4.17).
Theorem 7.1. Let (7.2) be positive real (in the continuous-time sense) with Φ(s) :=
Z(s) + Z(−s) coercive, and let (A, C, C̄) be in stochastically balanced form. Then,
if σr+1 < σr , the reduced system (A11 , C1 , C̄1 ) deﬁnes a positive real function (7.6)
for which it is a minimal realization in stochastically balanced form, and Φ1 (s) :=
Z1 (s) + Z1 (−s) is coercive.
Proof. We have already shown that Z1 is positive real, and we refer the reader to
Harshavaradhana et al. (1984) for the proof that (A11 , C1 , C̄1 ) is a minimal realization
of Z1 . It remains to show that Φ1 is coercive and that (A11 , C1 , C̄1 ) is stochastically
−1
, where P1− and P1+ are solutions to the algebraic
balanced, i.e., that P1− = Σ1 = P1+
Riccati equation
A11 P1 + P1 A11 + (C̄  − P1 C1 )R−1 (C̄  − P1 C1 ) = 0

(7.9)

such that any other solution P1 of (7.9) satisﬁes P1− ≤ P1 ≤ P1+ . To this end,
−1
note that since M1 (Σ1 ) and M1 (Σ−1
1 ) have rank m, both Σ1 and Σ1 satisfy (7.9).
Therefore, as is well-known (Molinari, 1977) and easy to show, Q := Σ−1
1 −Σ1 satisﬁes
Γ1 Q + QΓ1 + QC1 R−1 C1 Q = 0,

(7.10)

Γ1 = A11 − (C̄  − Σ1 C1 )R−1 C1 .

(7.11)

where
Since Φ is coercive, Σ−1 − Σ = P+ − P− > 0 (Faurre et al., 1979, Theorem 4.17) so
that σ1 < 1. Hence Q > 0, and therefore (7.10) is equivalent to
Γ1 Q−1 + Q−1 Γ1 + C1 R−1 C1 = 0.

(7.12)

Now, since (C1 , A11 ) is observable, then, in view of (7.11), so is (C1 , Γ1 ). Since,
in addition, the Lyapunov equation (7.12) has a positive deﬁnite solution Q−1 , Γ1
must be a stability matrix. Therefore Σ1 is the minimal (stabilizing) solution P1− of
−1
= Σ1 .
(7.9). In the same way, using the backward setting, we show that P̄1+ := P1+
Consequently, (A11 , C1 , C̄1 ) is stochastically balanced. Since P1+ − P1− > 0, Φ1 is
coercive.
Let us now return to the discrete-time setting. Let us recall that, if (A, C, C̄, 12 Λ0 )
is a minimal realization of (7.1), the matrix function Z is positive real if and only if
the linear matrix inequality (6.6) has a symmetric solution P > 0. Conversely, given
the positive real rational function (7.1) with the property that Φ(z) = Z(z) + Z(z −1 )

32

ANDERS LINDQUIST AND GIORGIO PICCI

is the spectral density of the time series y, the state covariance P of any minimal
stochastic realization (3.7) of y satisﬁes (6.6) and the matrices B, D in (3.7) satisfy
(6.7). Consequently, as pointed out in Section 5, the matrices B and D can be
determined via a matrix factorization of M (P ) once P has been determined.
Now, if (A, C, C̄) is in stochastically balanced form, Theorem 4.4 implies that
M (Σ) ≥ 0. In view of (4.16) and (2.12), M (Σ) may be written


Σ1 − A11 Σ1 A11 − A12 Σ2 A12 ∗ C̄1 − A11 Σ1 C1 − A12 Σ2 C2
,

∗
∗
∗




C̄1 − C1 Σ1 A11 − C2 Σ2 A12 ∗ Λ0 − C1 Σ1 C1 − C2 Σ2 C2
where, as before, the blocks which do not enter the analysis are marked with an
asterisk. Since M (Σ) ≥ 0, this implies that
	 
 	 

A
A
(7.13)
M1 (Σ1 ) − 12 Σ2 12 ≥ 0,
C2
C2
where

	


Σ1 − A11 Σ1 A11 C̄1 − A11 Σ1 C1
M1 (Σ1 ) =
C̄1 − C1 Σ1 A11 Λ0 − C1 Σ1 C1

(7.14)

is the matrix function (6.6) corresponding to the reduced triplet (A11 , C1 , C̄1 ). Therefore, M (Σ1 ) ≥ 0, so if we can show that A11 is stable, i.e., has all its eigenvalues
strictly inside the unit circle, it follows that
1
(7.15)
Z1 (z) = C1 (zI − A11 )−1 C̄1 + Λ0 ,
2
is positive real. As we shall see below this is true without the requirement needed in
continuous time that σr+1 < σr .
For (A11 , C1 , C̄1 ) also to be balanced, Σ1 would have to be the minimal solution P1−
of M1 (P1 ) ≥ 0, which in turn would require that rank M1 (Σ1 ) = rank M (Σ) = m.
Due to the extra positive semideﬁnite term in (7.13), however, this will in general not
be the case and therefore Σ1 ≥ P1− will correspond to an external realization, as will
Σ−1
1 ≤ P1+ ; see Lindquist and Picci (1991).
To show that (A11 , C1 , C̄1 ) is minimal we need to assume that Φ is coercive, or,
equivalently, that Z is strictly positive real. It is well-known (Faurre et al., 1979,
Theorem A4.4) that this implies that
P+ − P− > 0.

(7.16)

In fact, if Λ0 > 0, which in particular holds if y is full rank, (7.16) is equivalent to
coercivity. Coercivity also implies that
Λ0 − CP− C  > 0.

(7.17)

Remark 7.2. With (A, C, C̄) in balanced form, P− = Σ = P̄+ and, in view of (3.16),
P+ = Σ−1 . Hence (7.16) becomes Σ−1 > Σ, which obviously holds if and only if
σ1 < 1, which in turn is equivalent to H − ∩ H + = 0. Consequently, given the full
rank condition Λ0 > 0, coercivity is equivalent to the past and the future spaces of y
having a trivial intersection.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

33

Theorem 7.3. Let (7.1) be positive real, and let (A, C, C̄) be in stochastically balanced form. Then the reduced-degree function (7.15) obtained via principal subsystem
decomposition (2.13) is positive real. Moreover, if Z is strictly positive real, then so
is Z1 , and (A11 , C1 , C̄1 , 12 Λ0 ) is a minimal realization of Z1 .
For the proof we need the following lemma, the proof of which is given in Appendix
D.
Lemma 7.4. Let the matrix function Z be given by (7.1), where Λ0 > 0, but where
(C, A) and (C̄, A ) are not necessarily observable, and suppose that (6.6) has two
positive deﬁnite symmetric solutions, P1 and P2 , such that
P2 − P1 > 0.

(7.18)

Then Z is strictly positive real.
Proof of Theorem 7.3. To prove that Z1 is positive real it remains to show that A11 is
stable. To this end, we note that P is the reachability gramian of (3.7). In particular,
if (A, C, C̄) is stochastically balanced, the reachability gramian of the system (3.18)
equals Σ so, in view of Theorem 4.2 in Pernebo and Silverman (1982), A11 is stable.
By Remark 7.2, coercivity of Φ implies that Σ−1 − Σ > 0, from which it follows
that Σ−1
1 − Σ1 > 0 and that Λ0 > 0. Moreover, By construction, M1 (Σ1 ) ≥ 0 and
−1
M1 (Σ1 ) ≥ 0. Therefore, by Lemma 7.4, Z1 is strictly positive real if Z is.
To prove minimality, we prove that (C1 , A11 ) is observable. Then the rest follows
by symmetry. By regularity condition (7.17),
Λ0 − C1 Σ1 C1 ≥ Λ0 − CΣC  > 0,
and consequently, since M1 (Σ1 ) ≥ 0, Σ1 satisﬁes the algebraic Riccati inequality
A11 P1 A11 − P1 + (C̄1 − A11 P1 C1 )(Λ0 − C1 P1 C1 )−1 (C̄1 − A11 P1 C1 ) ≥ 0, (7.19)
but in general not with equality. Now, since A11 is stable, (A11 , C1 ) is stabilizable.
Moreover, given condition (3.10), we have proved above that the reduced-degree spectral density Φ1 is coercive. Therefore, by Theorem 2 in Molinari (1975), there is a
unique symmetric P1− > 0 which satisﬁes (7.19) with equality and for which
Γ1− := A11 − (C̄1 − A11 P1− C1 )(Λ0 − C1 P1− C1 )−1 C1
is stable. It is well-known (Faurre et al., 1979) that P1− is the minimal symmetric
solution of the linear matrix inequality M1 (P1 ) ≥ 0, i.e., that any other symmetric
−1
solution P1 satisﬁes P1 ≥ P1− . We also know that M1 (Σ−1
1 ) ≥ 0. Next, since Σ1 −
Σ1 > 0, a fortiori it holds that Q := Σ−1
1 − P1− > 0. A tedious but straight-forward
calculation shows that Q satisﬁes
Γ1− (Q−1 − C1 R−1 C1 )−1 Γ1− − Q ≥ 0,
from which it follows that
Q−1 − C1 R−1 C1 − Γ1− Q−1 Γ1− ≤ 0.
Cf. Faurre et al. (1979), pp. 85 and 95.

(7.20)

34

ANDERS LINDQUIST AND GIORGIO PICCI

Now, suppose that (C1 , A11 ) is not observable. Then, there is a nonzero a ∈
and a λ ∈ C such that [C1 , λI − A11 ]a = 0. and therefore, in view of (7.20),

Cr

(1 − |λ|2 )a∗ Q−1 a ≤ 0.
But λ is an eigenvalue of the stable matrix A11 , implying that |λ| < 1, so we must
have a = 0 contrary to assumption. Consequently, (C1 , A11 ) is observable.
A remaining question is whether there is some balanced order-reduction procedure
in discrete time which preserves both positivity and balancing. That this is the case
in continuous time implies that the answer is aﬃrmative, but the reduced system
cannot be a simple principal subsystem truncation.
Theorem 7.5. Let ( 1.6) be strictly positive real and let (A, C, C̄) be in stochastically
balanced form. Moreover, given a decomposition ( 2.12) such that σr+1 < σr , let
Ar
Cr
C̄r
Λr0

=
=
=
=

A11 − A12 (I + A22 )−1 A21
C1 − C2 (I + A22 )−1 A21
C̄1 − C̄2 (I + A22 )−1 A12
Λ0 − C2 (I + A22 )−1 C̄2 − C̄2 (I + A22 )−1 C2

Then (Ar , Cr , C̄r , Λr0 ) is a minimal realization of a strictly positive real function
1
Zr (z) = Cr (zI − Ar )−1 C̄r + Λr0 .
2

(7.21)

Moreover, (Ar , Cr , C̄r , Λr0 ) is stochastically balanced with canonical correlation coeﬃcients σ1 , σ2 , . . . , σr .
To understand why this reduced-order system does preserve both positivity and
balancing, note that for


I −A12 (I + A22 )−1 0
I
0
T = 0
−1
I
0 −C2 (I + A22 )
we obtain



Σ1 − Ar Σ1 Ar ∗ C̄r − Ar Σ1 Cr
,
∗
∗
∗
T M (Σ)T  = 


C̄r − Cr Σ1 Ar ∗ Λr0 − Cr Σ1 Cr

and consequently, if Mr (P ) is the the matrix function (6.6) corresponding to the
reduced-order system, Mr (σ1 ) ≥ 0 and rank Mr (Σ1 ) ≤ rank M (Σ).
To prove Theorem 7.5 we observe that (Ar , Cr , C̄r , Λr0 ) is precisely what one obtains
if one transforms (A, C, C̄, Λ0 ) by the appropriate linear fractional transform to the
continuous-time setting and then, after reduction, back to discrete time again as
suggested in Ober (1991). The proof is deferred to Appendix D.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

35

8. Conclusions
The purpose of this paper is to analyze a class of popular subspace identiﬁcation
procedures for state space models in the theoretical framework of rational covariance
extension, balanced model reduction, and geometric theory for splitting subspaces.
We have pointed out that these methods are based on the hidden Assumption 2.1
which is not entirely natural and which is in general untestable.
The procedures of Aoki (1990) and van Overschee and De Moor (1993) can be regarded as prototypes for this class of algorithms. We point out that they are essentially
equivalent to the Ho-Kalman algorithm in which the basic factorization is performed
by singular-value decomposition of a block Hankel matrix of ﬁnite covariance data,
as in Aoki (1990), or of a normalized version of this matrix, as in van Overschee and
De Moor (1993). The latter normalization is natural in that it yields a matrix representation of the abstract Hankel operator of geometric stochastic systems theory in
orthonormal coordinates and allows for theoretical veriﬁcation of the truncation step.
A major problem with these algorithms is that they are based on realization algorithms for deterministic systems. Therefore they require that the positive degree of
the data equals the algebraic degree. To achieve this, one must assume that the data
are generated exactly by an underlying system and that the amount of data is suﬃcient for constructing an accurate partial covariance sequence the length of which is
suﬃcient in relation to the dimension of the underlying system. Hence it is absolutely
crucial that a reliable upper bound of the dimension of the “true” underlying system
is available.
We stress that these stringent assumptions are not satisﬁed for generic data, as was
pointed out in Section 2. In fact, in Byrnes and Lindquist (1996) it is shown that
the positive degree has no generic value. In fact, just for the moment considering
the single-output case, for each p such that r ≤ p ≤ ν there is a nonempty open
set of partial covariance sequences having positive degree p in the space of sequences
of length ν. Secondly, for any r, it is possible to construct examples of long partial
covariance sequences having algebraic degree r but having arbitrarily large positive
degree (Theorem 2.4).
In Section 7 we proved an open question concerning the preservation of positivity
in the original (discrete-time) model reduction procedure of Desai and Pal (1984).
Unlike that of the later paper Desai et al. (1985), this procedure is equivalent to
the principal subsystem truncation used in van Overschee and De Moor (1993), but
not to the one in Aoki (1990). We prove that positivity is preserved provided that
the original data satisﬁes Assumption 2.1, justifying setting the smaller canonical
correlation coeﬃcients equal to zero. Unlike the situation in continuous time, this
truncation does not preserve balancing. The validity of the corresponding procedure
of Aoki (1990) has not been settled.
The contribution of this paper is to provide theoretical understanding of these
identiﬁcation algorithms and to point out possible pitfalls of such procedures. Hence
the primary purpose is not to suggest alternative procedures. Nevertheless, we would
like to point out that a two-stage procedure equivalent to covariance extension followed
by model reduction would work on any ﬁnite string of data, thus elimination the need
for Assumptions 2.1. However, we leave open the question of how such a procedure
should be implemented with respect to the data. The approximation would then of

36

ANDERS LINDQUIST AND GIORGIO PICCI

course depend on which covariance extension is used, a maximum-entropy extension
or some other.
Acknowledgment. We would like to thank the referees and the associate editor for
the careful review of our paper and for many useful suggestions, which have led to
considerable improvements of this paper.
References
1. Adamjan, D. Z., Arov and M. G. Krein (1971). Analytic properties of Schmidt pairs for a Hankel
operator and the generalized Schur-Takagi problem. Math. USSR Sbornik, 15, 31–73.
2. Akhiezer, N. I. (1965). The Classical Moment Problem, Hafner.
3. Anderson, T. W. (1958). Introduction to Multivariate Statistical Analysis, John Wiley.
4. Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. SIAM
J. Control, 13, 162–173.
5. Aoki, M. (1990). State Space Modeling of Time Series (second ed.), Springer-Verlag.
6. Byrnes, C. I. and A. Lindquist (1982). The stability and instability of partial realizations. Systems
and Control Letters, 2, 2301–2312.
7. Byrnes, C. I. and A. Lindquist (1989). On the geometry of the Kimura-Georgiou parameterization
of modelling ﬁlter. Inter. J. of Control, 50, 2301–2321.
8. Byrnes, C. I. and A. Lindquist (1996), On the partial stochasic realization problem, submitted
for publication.
9. Byrnes, C. I., A. Lindquist, S. V. Gusev and A. S. Matveev (1995). A complete parameterization
of all positive rational extensions of a covariance sequence. IEEE Trans. Autom. Control, AC-40.
10. Byrnes, C. I., A. Lindquist, and T. McGregor (1991). Predictability and unpredictability in
Kalman ﬁltering. IEEE Trans. Autom. Control, 36, 563–579.
11. Byrnes, C. I., A. Lindquist, and Y. Zhou (1994). On the nonlinear dynamics of fast ﬁltering
algorithms. SIAM J. Control and Optimization, 32, 744–789.
12. Desai, U. B. and D. Pal (1982). A realization approach to stochastic model reduction and
balanced stochastic realization. Proc. 21st Decision and Control Conference, 1105–1112.
13. Desai, U. B. and D. Pal (1984). A transformation approach to stochastic model reduction. IEEE
Trans. Automatic Control, AC-29, 1097–1100.
14. Desai, U. B., D. Pal and Kirkpatrick (1985). A realization approach to stochastic model reduction. Intern. J. Control, 42, 821–839.
15. Desai, U. B. (1986) Modeling and Application of Stochastic Processes, Kluwer.
16. Faurre, P. (1969). Identiﬁcation par minimisation d’une representation Markovienne de processus
aleatoires. Symposium on Optimization, Nice.
17. Faurre, P. and Chataigner (1971). Identiﬁcation en temp reel et en temp diﬀeree par factorisation
de matrices de Hankel. French-Swedish colloquium on process control, IRIA Roquencourt.
18. Faurre, P., M. Clerget, and F. Germain (1979). Opérateurs Rationnels Positifs, Dunod.
19. Faurre, P. and J. P. Marmorat (1969). Un algorithme de réalisation stochastique. C. R. Academie
Sciences Paris 268.
20. Gantmacher, F. R. (1959). The Theory of Matrix, Vol. I, Chelsea, New York.
21. Georgiou, T. T. (1987). Realization of power spectra from partial covariance sequences. IEEE
Transactions Acoustics, Speech and Signal Processing, ASSP-35, 438–449.
22. Glover, K. (1984). All optimal Hankel norm approximations of linear multivariable systems and
their L∞ error bounds. Intern. J. Control, 39, 1115–1193.
23. Gragg, W. B. and A. Lindquist (1983). On the partial Realization problem. Linear Algebra and
its Applications, 50, 277–319.
24. Hannan, E. J. (1970). Multiple Time Series, Wiley, New York.
25. Heij, Ch., T. Kloek and A. Lucas (1992). Positivity conditions for stochastic state space modelling
of time series. Econometric Reviews 11, 379–396.
26. Hotelling, H. (1936). Relations between two sets of variables. Biometrica, 28, 321–377.
27. Harshavaradhana, P., E. A. Jonckheere and L. M. Silverman (1984). Stochastic balancing and
approximation-stability and minimality. IEEE Trans. Autom. Control, AC-29, 744–746.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

37

28. Kalman, R. E. (1981). Realization of covariance sequences. Proc. Toeplitz Memorial Conference,
Tel Aviv, Israel.
29. Kalman, R.E., P.L.Falb, and M.A.Arbib (1969). Topics in Mathematical Systems Theory,
McGraw-Hill.
30. Kalman, R. E. (1979). On partial realizations, transfer functions and canonical forms. Acta
Polytech. Scand., MA31, 9–39.
31. Kimura, H. (1987). Positive partial realization of covariance sequences. Modelling, Identiﬁcation
and Robust Control, C. I. Byrnes and A. Lindquist, eds., North-Holland, 499–513.
32. Kung, S. Y. (1978). A new identiﬁcation and model reduction algoritm via singular value decomposition. Proc. 12th Asilomar Conf. Circuit, Systems and Computers, 705–714.
33. Larimore, W. E. (1990). System identiﬁcation, reduced-order ﬁltering and modeling via canonical
variate analysis. Proc. 29th Conf. Decison and Control, pp. 445–451.
34. Lindquist, A. and G. Picci (1985). Realization theory for multivariate stationary Gaussian processes. SIAM J. Control and Optimization, 23, 809–857.
35. Lindquist, A. and G. Picci (1991). A geometric approach to modelling and estimation of linear
stochastic systems. Journal of Mathematical Systems, Estimation and Control, 1, 241–333.
36. Lindquist, A. and G. Picci (1994a). On “subspace methods” identiﬁcation. in Systems and Networks: Mathematical Theory and Applications, II, U. Hemke, R. Mennicken and J Saurer, eds.,
Akademie Verlag, 315–320.
37. Lindquist, A. and G. Picci (1994b). On “subspace methods” identiﬁcation and stochastic model
reduction. Proceedings 10th IFAC Symposium on System Identiﬁcation, Copenhagen, June 1994,
Volume 2, 397–403.
38. Molinari, B.P. (1975). The stabilizing solution of the discrete algebraic Riccati equation. IEEE
Trans. Automatic Control, 20, 396–399.
39. Molinari, B.P. (1977). The time-invariant linear-quadratic optimal-control problem. Automatica,
13, 347–357.
40. Ober, R. (1991). Balanced parametrization of classes of linear systems. SIAM J. Control and
Optimization, 29, 1251–1287.
41. van Overschee, P., and B. De Moor (1993). Subspace algorithms for stochastic identiﬁcation
problem. Automatica, 29 , 649-660.
42. van Overschee, P., and B. De Moor (1994a). Two subspace algorithms for the identiﬁcation of
combined deterministic-stochastic systems. Automatica, 30, 75–93.
43. van Overschee, P., and B. De Moor (1994b). A unifying theorem for subspace identiﬁcation
algorithms and its interpretation. Proceedings 10th IFAC Symposium on System Identiﬁcation,
Copenhagen, June 1994, Volume 2, 145–156.
44. Pernebo, L. and L. M. Silverman (1982). Model reduction via balanced state space representations. IEEE Trans. Automatic Control, AC-27, 382–387.
45. Picci, G. and S. Pinzoni (1994). Acausal models and balanced realizations of stationary processes.
Linear Algebra and its Applications, 205-206, 957-1003.
46. Rozanov, N. I. (1963). Stationary Random Processes, Holden Day.
47. Schur, I. (1918). On power series which are bounded in the interior of the unit circle I and II.
Journal fur die reine und angewandte Mathematik, 148, 122–145.
48. Vaccaro, R. J. and T. Vukina (1993). A solution to the positivity problem in the state-space
approach to modeling vector-valued time series. J. Economic Dynamics and Control, 17, 401–
421.
49. Willems, J. C. (1971) Least squares stationary optimal control and the algebraic Riccati equation.
IEEE Trans. Automatic Control, AC-16, 621–634.
50. Whittle, P. (1963). On the ﬁtting of multivariate autoregressions and the approximate canonical
factorization of a spectral density matrix. Biometrica, 50, 129–134.
51. Wiener, N. (1933). Generalized Harmonic Analysis, in The Fourier Integral and Certain of its
Applications, Cambridge U.P.
52. Zeiger, H. P. and A. J. McEwen (1974). Approximate linear realization of given dimension via
Ho’s algorithm. IEEE Trans. Automatic Control. AC-19, 153.

38

ANDERS LINDQUIST AND GIORGIO PICCI

Appendix A. Proof of Theorem 2.4.
We ﬁrst give a proof for the special case n = 1. Consider a scalar function
1z+b
(A.1)
2z +a
with a scalar sequence (1.4) such that Λ0 = 1. Now it is well-known [see, e.g., Schur
(1918), Akhiezer (1965)] that Tν is positive deﬁnite if and only if
Z(z) =

|γt | < 1 t = 0, 1, 2, . . . , ν − 1

(A.2)

where {γ0 , γ1 , γ2 , . . . } are the so called Schur parameters. There is a bijective relation
between partial sequences (1.1) and partial sequences {γ0 , γ1 , . . . , γν−1 } of the same
length; Schur (1918), Akhiezer (1965). In Byrnes et al. (1991) it was shown that the
Schur parameters of (A.1) are generated by the nonlinear dynamical system

αt
α0 = 12 (a + b)
αt+1 = 1−γ
2
t
(A.3)
t αt
γ0 = 12 (b − a)
γt+1 = −γ
1−γ 2
t

and that Tt becomes singular precisely when there is ﬁnite escape. It was also shown
in Byrnes et al. (1991) that {αt } is generated by a linear system

 	

	 

	
2/κ −1 ut
ut+1
=
,
(A.4)
vt+1
vt
1
0
where αt = vt /ut and κ := (a + b)(1 + ab)−1 . If κ is greater than one in modulus, the
coeﬃcient matrix of (A.4) has complex eigenvalues and is thus, modulo a constant
scalar factor, similar to
	


cos θ sin θ
,
− sin θ cos θ
√
where θ := arctan κ2 − 1. Hence αt is the slope of a line through the origin in R2
which rotates counter-clockwise with the constant angle θ in each time step. Consequently,
arctan αt+1 = arctan αt + θ.
Moreover, assuming that α0 > 0, the Schur condition γt < 1 will fail as soon as αt+1
negative or inﬁnite, as can be seen from the ﬁrst of recursions (A.3). Hence (A.2)
holds if and only if
π
(A.5)
arctan αν < .
2
Therefore for a small < > 0, take√a = 1 − < and b = 1 + <, yielding a stable Z. Then
2

4 − <2 . We may choose < so that
κ = 2−
2 > 1 and θ = arctan 2−2
ϑ
ϑ
<θ< ,
ν+1
ν
where ϑ := π2 − arctan α0 . Then (A.5) holds so that Tν > 0, but we also have
π
arctan αν+1 >
2
so that Tν+1 > 0.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

39

Next, let n be arbitrary. Consider the scalar function
1 ψn (z) + 12 (a + b)ψn−1 (z)
Z(z) =
2 ϕn (z) + 12 (a + b)ϕn−1 (z)
where {ϕt } and {ψt } are the Szegö polynomials of the ﬁrst and second kind respectively (Akhiezer, 1965). The function Z has the property that its ﬁrst n Schur
parameters, {γ0 , γ1 , . . . , γn−1 }, are precisely the data which uniquely determines ϕn ,
ϕn−1 , ψn and ψn−1 ; Georgiou (1987), Kimura (1987), Byrnes at al. (1994). Now, in
Byrnes at al. (1994) it is shown that the remaining Schur parameters are generated
by

α0 = 12 (a + b)
αt+1 = 1−γα2 t
t+n−1

γt+1 =

−γt αt
2
1−γt+n−1

Hence, we have reduced the problem to the case n = 1. If we choose the initial
Schur parameters suﬃciently small so that ϕn (z) and ϕn−1 (z) are approximately z n
and z n−1 ,
ϕn (z) + α0 ϕn−1 (z)
is stable if we choose a := 1 − 2< and b := 1 + < for some small < > 0. Then κ > 1
and the proof for the case n = 1 carries through with a trivial modiﬁcation.
Appendix B. The Hilbert space of a sample function
Let y = {y(t)}t≥0 be a zero-mean wide-sense-stationary stochastic process deﬁned on
a probability space {Ω, A, P } such that the limit (1.11) exists for almost all trajectories {yt = y(t, ω); t = 0, 1, . . . }. It is relatively easy to show that whenever the limit
exists, the m × m matrix function k → Λk obtained from a particular trajectory is
then a bona-ﬁde covariance function. [The continuous-time analog of this property
was observed already by Wiener (1933)]. If moreover the sample limit is (almost
surely) independent of the particular trajectory and hence necessarily coincides with
the ”ensemble” covariance function, we shall call such a process second-order stationary. Conditions for second order stationarity are given, for example, on page 210 in
the book of Hannan (1970). It is obvious from Birkhoﬀ’s ergodic theorem that any
(zero-mean) strictly stationary ergodic process is also second-order ergodic.
In this Appendix we shall show that the properties of the Hilbert space structure
associated to a stationary time series y, deﬁned on page 10, are identical to those of
the Hilbert space induced by a second-order ergodic process.10
The two frameworks, i.e., the statistical “time-series” structure and the “probabilistic” structure, are in fact isomorphic. To see this, pick a “representative” trajectory
of y, i.e. one in the subset of Ω (of probability one) for which the limit (1.11) exists.
Clearly there will be no loss of generality in assuming that the probability space Ω of
y is the “sample space”, of all possible trajectories of y, i.e. the set of all semi-inﬁnite
sequences ω = {ω0 , ω1 , ω2 , . . . }, ωt ∈ Rm . With this choice, A will be the usual σalgebra of cylinder subsets of Ω and the t:th random variable of the process, y(t), is
just the canonical projection function
y(t, ω) : ω → ωt .
For a process of this kind the Hilbert space H(y) is the closure in L2 (Ω, A, P ) of the linear
vector space generated by the scalar random variables ω → yi (t, ω) (Rozanov, 1963).
10

40

ANDERS LINDQUIST AND GIORGIO PICCI

Let us arrange the tails of the observed sample trajectory of the process in a sequence
of m × ∞ matrices y := {y(k)}k≥0 as in (3.3). For ω in the subset of Ω where the
time averages converge, deﬁne the map Tω ,
Tω : a y(t) → a y(t) t ≥ 0 a ∈ Rm
associating the i:th scalar components of each m-dimensional random vector y(t)
of the process to the corresponding i:th (inﬁnite) row of the m × ∞ matrix y(t)
constructed from the corresponding sample path {y(t, ω); t ∈ Z}. By second-order
ergodicity, the set of all such ω ∈ Ω will have probability measure one and the map
Tω will in fact be norm preserving, since by construction we have
Λt−s = Ey(t)y(s) = Ey(t)y(s) ,
where Λt is the covariance matrix of y. The map Tω can then be extended by linearity
and continuity to a unitary linear operator Tω : H(y) → H(y) which commutes with
the action of the natural shift operators (both of which we denote U), in these two
Hilbert spaces:
U

H(y) −→H(y)
↓ Tω
Tω ↓
U

H(y) −→H(y)
This isomorphism allows us to employ exactly the same formalism and notations
used in the geometric theory of stochastic systems (Lindquist and Picci, 1985, 1991)
in the present statistical setup, where we build estimates of the parameters of models
describing the data in terms of an observed time series instead of stochastic processes.
This provides a remarkable conceptual unity and admits a straightforward derivation
in the style of stochastic realization theory of the formulas in the paper van Overschee
and De Moor (1993), there obtained with considerable eﬀort through lengthy and
formal manipulations.
Appendix C. The invariant form of the Kalman ﬁlter
Given a stationary stochastic system (3.7), the Kalman ﬁlter is usually determined
via the matrix Riccati equation
Q(t + 1) = AQ(t)A − [AQ(t)C  + BD ][CQ(t)C  + DD ]−1 [AQ(t)C  + BD ] + BB 
(C.1)
where Q(0) = P := E{x(0)x(0) }. Here
Q(t) = E{[x(t) − x̂(t)][x(t) − x̂(t)] },

(C.2)

and the Kalman gain is given by
K(t) = [AQ(t)C  + BD ][CQ(t)C  + DD ]−1 .

(C.3)

These equations of course depend on P , B and D, which vary as the splitting subspace
X varies over , whereas (A, C, C̄) is invariant if a uniform choice of bases is made.
However, as shall see, the gain K depends only on the triplet (A, C, C̄) and hence
one should be able to replace (C.1) and (C.3) with equations which also only depend

X

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

41

X

on (A, C, C̄), and hence are invariant over . Clearly, in view of Theorem 5.1, P− (t),
as deﬁned by (5.12), has this property. Moreover,
Q(t) = P − P− (t),
and, consequently, in view of (3.9), and the Lyapunov equation
P = AP A + BB  ,
P , B and D in (C.1) and (C.3) can be eliminated to yield precisely (5.13) and (5.11).
A symmetric argument yields the backward equations.
It is easy to see that as Q(t) → Q∞ monotonously, P− (t) → P− , and hence P ≥ P− ,
as should be.
Appendix D. Some deferred proofs
Proof of Theorem 5.1. Since X is a splitting subspace for the inﬁnite past H − and the
inﬁnite future H + , by stationarity, Xτ splits Hτ− := U τ H − and Hτ+ := U τ H + . But
Yτ− ⊂ Hτ− and Yτ+ ⊂ Hτ+ , and hence Xτ splits Yτ− and Yτ+ also. (See, e.g., Lindquist
and Picci (1985, 1991).) Now, using the projection formula in the footnote of page
16, we have for any b yτ+ ∈ Yτ+


−1
Λτ
Λ1 Λ2 . . .
Λ0 Λ1 . . . Λτ
Λ2 Λ3 . . . Λτ +1  Λ1 Λ0 . . . Λτ −1 
−
 .
E Yτ b yτ+ = b 
yτ−
..
.. 
..
.. 
..
..
 ...



.
.
.
.
.
.
.
.
Λτ Λτ +1 · · · Λ2τ −1
Λτ Λτ −1 · · · Λ0
= b Ωτ Ω̄τ (Tτ− )−1 yτ−
= b  Ωτ ξ
where Ωτ and Ω̄τ are appropriate ﬁnite-dimensional observability and constructibility
matrices (2.6) of full rank. If τ > τ0 , there is a minimal factorization H = Ωτ Ω̄τ such
that ξ := Ω̄τ (Tτ− )−1 yτ− has n components, and
E{ξξ  } = Ω̄τ (Tτ− )−1 Ω̄τ > 0.
Therefore, since the components of ξ belong to X̂τ − , dim X̂τ − ≥ n = dim Xτ so, since
X̂τ − is minimal, Xτ must also be minimal and X̂τ − be spanned by the components
of ξ.
Next, from the backward system (3.14) we see that
yτ− = Ω̄τ x̄(τ ) + terms ortogonal to Xτ ,
and therefore, by the same projection formula,
−

E Yτ a x(τ ) = a E{x(τ )x̄(τ ) }Ω̄τ (Tτ− )−1 yτ− = a ξ.
Consequently, E Yτ Xτ = {a ξ | a ∈ Rn } = X̂τ − , establishing the ﬁrst of identities
(5.7). The second follows from a symmetric argument.
The representation formula (5.8) follows from the minimality of Xτ as a splitting
subspace for Yτ+ and Yτ− , which, in particular, implies that the constructibility operator,
Yτ−
: Xτ → X̂τ −
Ct := E|X
τ
−

42

ANDERS LINDQUIST AND GIORGIO PICCI

is injective (Lindquist and Picci, 1985, 1991). In other words, for each k = 1, 2, . . . , n,
there is a unique random variable xk (τ ) ∈ Xτ whose projection onto Yτ− is x̂k (τ ).
To show that x(0) form a uniform choice of bases as X varies over , ﬁrst take X
to be the stationary backward predictor space X+ and let x+ (τ ) be the unique basis
−
be arbitrary. Then, since
in Uτ X+ such that x̂(τ ) = E Yτ x+ (τ ). Now, let X ∈
−
τ
τ
+
Xτ is a splitting subspace for Yτ and U X+ ⊂ U H (Lindquist and Picci, 1991,
Proposition 2.1(vi)), we have

X

X

−

−

x̂(τ ) = E Yτ x+ (τ ) = E Yτ E Xτ x+ (τ ),
and therefore, by the uniqueness of the representation (5.8), x(0) = E X x+ (0) for all
X ∈ , which is a well-known characterization of uniform choice of bases; see Section
6 in Lindquist and Picci (1991). A symmetric argument in the backward setting yields
the corresponding statement for (5.9).

X

Proof of Proposition 6.1. Suppose that the underlying system prescribed by Assumption 2.1 has a positive real function Z of MacMillan degree n, and let (1.1) be a
corresponding partial covariance sequence , where ν is large enough for the Hankel
matrix H, deﬁned by (1.5), to have rank n. Let (A, C, C̄) be the triplet determined
from H via (2.5). Likewise, let HT be the Hankel matrix obtained by exchanging the
covariance data by estimates
{Λ0T , Λ1T , . . . , ΛνT }
of type (6.3), and let (AT , CT , C̄T ) be the corresponding triplet obtained via (2.5).
We want to prove that
1
ZT (z) := CT (zI − AT )−1 C̄T + Λ0T
2
is
strictly
positive
real
for
a
suﬃciently
large
T
.
Now,
if
	 −1 deg

 ZT = deg Z, replace Σ by
	






Σ
0
Σ 0
in (2.5) in the appropriate
, U by U 0 , V by V 0 , and Σ−1 by
0 0
0 0
calculation so that (A, C, C̄) and (AT , CT , C̄T ) have the same dimensions. This will
not aﬀect Z and ZT . By continuity, (AT , CT , C̄T , Λ0T ) can be made arbitrarily close
to (A, C, C̄, Λ0 ) in any norm by choosing T suﬃciently large. Thus the same holds
for
max !Z(eiθ ) − ZT (eiθ )!
θ∈[0,2π]

and hence, since Φ(z) := Z(z) + Z(z −1 ) satisﬁes (3.10), so will ΦT (z) := ZT (z) +
ZT (z −1 ) for suﬃciently large T . Moreover, since |λ(A)| < 1, we have |λ(AT )| < 1 by
continuity for suﬃciently large T . Consequently, there is a T0 such that ZT is strictly
positive real for T ≥ T0 .
Proof of Theorem 5.3. Let Z, deﬁned by (1.6), be strictly positive real, and let (A, C, C̄)
be chosen in stochastically balanced form. Then, by Theorem 7.3, Z1 , deﬁned by
(7.15) in terms of the principal subsystem truncation (A11 , C1 , C̄1 ), is also strictly
positive real. We want to prove that this property is carried over to rational matrix
function
1
Zτ 1 (z) = (Cτ )1 (zI − (Aτ )11 )−1 (C̄τ )1 ) + Λ0
2
for τ suﬃciently large.

APPROXIMATE COVARIANCE EXTENSION AND IDENTIFICATION

43

To this end, let Qτ be deﬁned by (5.32). Since the canonical correlation coeﬃcients (5.25) tend to the canonical correlation coeﬃcients (4.12) as τ → ∞, Στ → Σ.
Moreover, as explained in the text preceding Theorem 5.3, the Riccati solution P− (t)
tends to Qτ ΣQτ as t → ∞ if the initial condition is taken to be P− (τ ) = Στ . Consequently, for any < > 0, there is a suﬃciently large τ such that !Στ − Σ! < 2 and
!Στ − Qτ ΣQτ ! < 2 so that !Σ − Qτ ΣQτ ! < <. Hence Qτ tends to a limit Q∞ with
the property Σ = Q∞ ΣQ∞ . Using the same argument in the backward direction, the
−1
second of relations (5.33) shows that Q∞ also satisﬁes Σ = Q−T
∞ ΣQ∞ . Consequently,
by the same argument as in the proof of Theorem 4.4, Q∞ is a signature matrix, and
hence in particular diagonal. Therefore,
−1

((Aτ )11 , (Cτ )1 , (C̄τ )1 ) → ((Q∞ )11 A(Q∞ )−1
11 , C(Q∞ )11 , C̄(Q∞ )11 ) as τ → ∞,

where (Q∞ )11 is the corresponding truncation of the signature matrix, and consequently, by continuity, Zτ 1 → Z1 . Hence, since Z1 is positive real, then so is Zτ 1 for
τ suﬃciently large.
Proof of Lemma 7.4. Let us ﬁrst consider the case when (A, C, C̄) is a minimal triplet.
Then Z is positive real by the Positive Real Lemma, and the linear matrix inequality (6.6) has a minimal and a maximal solution, P− and P+ respectively, which, in
particular, have the property that P− ≤ P1 and P2 ≤ P+ . Then, in view of (7.18),
P+ − P− > 0, and therefore Z is strictly positive real (Faurre et al., 1967, Theorem
A4.4).
Next, let us reduce the general case to the case considered above. If (C, A) is
not observable, change the coordinates in state space, through a transformation
(A, C, C̄) → (QAQ−1 , CQ−1 , QC̄  ), so that
	






Â 0
ˆ ∗ ,
A=
C = Ĉ 0
C̄ = C̄
∗ ∗
where (Ĉ, Â) is observable. Then, if P1 and P2 have the corresponding representations
	
	




P̂1 ∗
P̂2 ∗
P2 =
,
P1 =
∗ ∗
∗ ∗
it is easy to see that P̂1 and P̂2 satisfy the reduced version of the linear matrix
ˆ and that, in this new
inequality (6.6) obtained by exchanging (A, C, C̄) for (Â, Ĉ, C̄)
ˆ , Â ) is not observable, we proceed
setting, (7.18) holds, i.e., P̂2 − P̂1 > 0. If (C̄
by removing these unobservable modes. First note that P̂1−1 and P̂2−1 satisfy the
ˆ by (Â , C̄
ˆ , Ĉ). Then,
dual linear matrix inequality obtained by exchanging (Â, Ĉ, C̄)
changing coordinates in state space so that
	  





Ã 0

ˆ
˜
Â =
C̄ = C̄ ∗
Ĉ = C̃ 0 ,
∗ ∗
ˆ , Ã ) observable, and deﬁning
with (C̄
	 −1 

P̃
∗
−1
P̂1 = 1
∗ ∗

P̂2−1

	 −1 

P̃
∗
= 2
,
∗ ∗

44

ANDERS LINDQUIST AND GIORGIO PICCI

˜ , 1 Λ ) is a minimal realization of Z. Moreover, P̃ and P̃ satisfy
we see that (Ã, C̃, C̄
1
2
2 0
the corresponding linear matrix inequality (6.6) and have the property (7.18) in this
setting. Hence the problem is reduced to the case already studied above.
Proof of Theorem 7.5. It is well-known that the discrete-time setting can be trans, mapping
formed to the continuous-time setting via a bilinear transformation s = z−1
z+1
the unit disc onto the left half plane so that


1+s
(D.1)
Zc (s) = Zd
1−s
is positive real in the continuous-time sense if and only if Zd is positive real in the
discrete-time sense. It is not hard to show [see, e.g., Glover (1984), Faurre et al.
(1979)] that, if (Ad , Cd , C̄d , 12 Λ0 ) and (Ac , Cc , C̄c , 12 R) are realizations of Zd and Zc
respectively, we have

Ac = (Ad + I)−1 (Ad − I)



C = √2C (A + I)−1
c
√ d d
(D.2)

C̄c = 2C̄d (Ad + I)−1



R = Λ0 − Cd (Ad + I)−1 C̄d − C̄d (Ad + I)−1 Cd
and inversely


Ad = (I − Ac )−1 (I + Ac )



C = √2C (I − A )−1
d
c
√ c
 −1

=
2
C̄
(I
−
A
C̄
d
c

c)


Λ0 = R + Cc (I − Ac )−1 C̄c + C̄c (I − Ac )−1 Cc

(D.3)

Under this transformation the observability gramian and the constructibility gramian
(i.e., the observability gramian of (C̄, A )) are preserved so that (Ad , Cd , C̄d , 12 Λ0 ) is
a minimal realization if and only if (Ac , Cc , C̄c , 12 R) is; see, e.g., p. 1119 in Glover
(1984). Moreover, coercivity is preserved, and the solution sets of the corresponding
linear matrix inequalities (7.3) and (6.6) are identical. (This is because P is the
reachability gramian of a spectral factor and this gramian is also preserved.)
Therefore, Theorem 7.5 is a straight-forward consequence of Theorem 7.1. In fact,
transforming the problem of Theorem 7.5 via (D.2) to the continuous-time setting,
all the requirements of Theorem 7.1 are satisﬁed. Then, performing principal subsystem decomposition in the continuous-time setting and transforming the reduced-order
positive real function thus obtained via (D.3) back to discrete time, the desired result
is obtained.

