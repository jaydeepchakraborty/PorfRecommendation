A Universal Service-Semantics Description Language
Ajay Bansal, Srividya Kona, Luke Simon,
Ajay Mallya, and Gopal Gupt.a
Department of Computer Science
University of Texas at Dallas
Richardson, T X 75083

Abstract
For web-services to become pructdcal, an infrustructure needs to be supported that allo~ususers and applications to discover, deploy, compose, and synthesize
services automatically. This automation can take place
only if a formal description of the web-services is available. In this paper we present an infrastructure using
USDL (Universal Service-Semantics Description Language), a language for formally describing the semantics of web-services. USDL is based on the Web Ontology Language ( O W L ) and employs WordNet as a common basis for understanding the meaning of services.
USDL can be regarded as formal service documentation that ?udl allow sophisticated conceptual modeling
and searching of available web-services, automated service composition, and other forms of automated service integration. A theory of safe service substitution
for USDL is presented and proved sound and complete.
The rationale behind the design of USDL along with
its formal specification in O W L is presented with examples. We also compare USDL with other approaches
like 0 WL-S and WSML and show that USDL is complementary to these approaches.

1. Introduction
A web-service is a program available on a website
that "effects some action or change" in the world (i.e.,
causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane
reservation made over the Internet, a device being controlled, etc. The next milestone in the Web's evohition is making services nbiquitousiy available. As automation increases, these web-services will be accessed
directly by the applications themselves rather than by
humans. In this context, a wcb-service can be regarded
as a "programmatic interface" that makes application

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Thomas D. Hite
Met,allect Corp
2400 Dallas Parkway
Plano, TX 75093

to application comml~nicationpossible. An infrastrncture that allows users to discover, deploy, synthesize
and compose services automatically needs t o be s u p
ported in order t o make web-services more practical.
T o make services ubiquitously available we need a
semantics-based approach such that applications can
reason about a service's capability t o a level of detail that permits their discovery, deployment, composition and synthesis. Several efforts are underway to
build such an infrastructure. These efforts include a p
proaches based on the semantic web (such as OWL-S
[ 5 ] ) as well as those based on XML, such as Web Setvices Description Language (WSDL [7]). Approaches
such as WSDL are purely syntactic in nature, that is,
they merely specify the format of the service. In this
paper we present an approach that is based on semantics. Our approach can be regarded as providing semantics t o WSDL statements. We present the design
of a language called Universal Service-Semantics Description Language (USDL) which service developers
can use t o specify formal semantics of web-services [14].
Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can
be regarded as a language for formally specifying their
semantics. USDL can be thought of as formal servzce
documentatzon that will allow sophisticated conceptual
modeling and searching of available web-services, automated composition, and other forms of automated
service integration. For example, the WSDL syntax
and USDL semantics of web services can be published
in a directory which applications can access to automatically discover services. T h a t is, given a formal
description of the context in which a service is needed,
the service(s) that will precisely fulfill that need can
be determined. The directory can then be searched for
the exact service, or two or more services that can be
composed to synthesize the required service, etc.
To provide formal semantics, a common denominator must be agreed upon that everybody can use as a

basis of understanding the meaning of services. This
common conceptual ground must also bc somewhat
coarse-grained so as to be tractable for use by both engineers and compl~tcrs.That is, semantics of services
should not be given in terms of low-level concepts such
as Tnring machines, first-order logic and their variants, since service description, discovery, and synthesis
then become tasks that are practically intractable and
theoretically undecidable. Additionally, the semantics
should be given at a concept~iallevel that captures
common real world concepts. Furthermore, it is too impractical to expect disparate companies t o standardize
on application (or domain) specific ontologies to formally define semantics of web-services, and instead a
common nniversal ontology must be agreed npon with
additional constrlictors. Also, application specific ontologies will be an impediment t o automatic discovery
of services since the application developer will have to
be aware of the specific ontology that has been used to
describe the semantics of the service in order to frame
the query that will search for the service. The danger
is that the service may not be defined wing the particnlar domain specific ontology that the application
developer uses to frame the clnery, however, it may be
defined using some other domain specific ontology, and
so the application developer will be prevented from discovering the service even though it exists. These reasons make an ontology based on OWL WordNet [2, 81
a suitable candidate for a universal ontology of atomic'
concepts upon which arbitrary meets and joins can be
added in order to gain tractable flexibility.
We describe the meaning of conceptual modeling
and how it could be obtained via a common universal
ontology based on WordNet in the next section. Section 3, gives a brief overview of how USDL attempts
to semantically describe web-services. In section 4, we
discuss precisely how a WSDL document can be prescribed meaning in terms of WordNet ontology. Section 5 gives a complete USDL annotation for a BookBuying service. Alltomatic discovery and composition
of web-services using USDL is disclissed in section 6.
In section 7 we explore some of the theoretical aspects
of service description in USDL. Comparison of USDL
with other approaches like OWL-S and WSML is discllssed in section 8. Finally, conclusions and fiiture
work are addressed in the last section.

2. A Universal Ontology
To describe service semantics, we should agree on a
common ground to model our concepts. We can describe what any given web-service does from first principles using approaches based on logic. This is the

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

approach taken by frameworks such as dependent type
systcms and programming logics prevalent in the field
of software verification where a "formal understanding" of the service/software is needed in order to verify
it. However, such sollitions are both low-level, tedious,
and nndecidable to be of practical use. Instead, we are
interested in modeling higher-level concepts. That is,
we are more interested in answering questions such as,
what does a service do from the end user's or service
integrator's perspective, as opposed to the far more
difficult questions, such as, what does the service do
from a computational view? We care more about real
world concepts such as "customer" , "bank account",
and "flight itinerary" as opposed to the data strlictiires and algorithms used by a service t o model these
concepts. The distinction is subtle, but is a distinct>ion
of granularity as well as a distinction of scope.
In order to allow interoperability and machinereadability of our documents, a common conceptual
ground must be agreed upon. The first step towards
this common ground are standard languages such as
WSDL and OWL. However, these do not go far enough,
as for any given type of service there are numerous distinct representations in WSDL and for high-level concepts (e.g., a ternary predicate), there are nnmerous
disparate representations in terms of OWL, representations that are distinct in terms of OWL'S formal semantics, yet equal in the actual concepts they model. This
is known as the semantic aliasing problem: distinct
syntactic representations with distinct formal semantics yet equal conceptual semantics. For the semantics t o equate things that are conceptually equal, we
need to standardize a sufficiently comprehensive set of
atomic concepts, i.e., a universal ontology, along with
a restricted set of connectives.
Industry specific ontologies along with OWL can
also be used to formally describe web-services. This
is the approach taken by the OWL-S language 151. The
problem with this approach is that it requires standardization and undne foresight. Standardization is a slow,
bitter process, and industry specific ontologies would
require this process to be iterated for each specific industry. Furthermore, reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing is even more difficult. Undue foresight
is required because many useful web services will address innovative applications and industries that don't
currently exist. Standardizing an ontology for travel
and finances is easy, as these industries are well established, but new innovative services in new upcoming
industries also need be ascribed formal meaning. A
universal ontology will have no difficlilty in describing
such new services.

\Ye need an ontology that is somewhat coarsegrained yet, ~~niversal,
and at a similar conceptual
level to common rcal world concepts. Currently there
is only one sufficiently comprehensive ontology that
meets these criteria: WordNet [8]. As stated, part of
the common ground involves standardized languages
snch as OWL. For this reason, WordNet cannot be used
directly, and instead we make use of an encoding of
IVordNct as an OWI, base ontology [2]. Using an OWL
WordNet ontology allows for our solution to use a universal, complct,e, and tractable framework, which lacks
the semantic aliasing problem, to which we map web
service mcssages and operations. As long as this m a p
ping is precise and sn ficiently expressive, reasoning can
be done within the realm of OWL by using automated
inference systems (s~lcllas, one based on description
logic), and we antomatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological concepts, cspeciall y subsumption (hyponym) and
ecluivalencc (synonym) relationships.

3. USDL: An Overview
As mentioned earlier, USDL can be regarded as
a langnagc to formally specify the semantics of webservices. It is perhaps the first attempt, to capture the
semantics of web-services in a universal, yet decidable
manner. It, is quite distinct from previous approaches
such as WSDI, and OW1,-S [5]. As mentioned earlier,
WSDT, only defines syntax of the service; USDT, provides the missing semantic component. USDL can be
thought of as a formal langnage for service documentation. Thus, instead of docnmenting the function of
a service as comments in English, one can write USDT,
statements that describe the function of that service.
USDL is quite distinct from OWL-S, which is designed
for a similar purpose, and as we shall see the two are
in fact complement,ary. OWL-S primarily describes the
states that exist before and after the service and how
a service is composed of other smaller s~ihservices(if
any). Description of atomic services is left undcrspecified in OWJ,-S. They have to be specified using domain
specific ontologies; in contrast, atomic services are complet,ely specified in XiSDT,, and USDT, relics on a nniversa1 ontology (OWL FVordNet Ontology) to specify
the semantics of atomic services. USDT, and OW1,-S
are complementary in that OWL-S's strength lies in describing the structure of composite services, i.e., how
various atomic services are algorithmically combined
to prodnce a new service, while USDL is good for fully
describing atomic services. Thus, OWL-S can be used
for describing the s t r ~ ~ c t of
~ ~composite
re
services that,

combine atomic services described using USDI,.
USDL describes a service in terms of portType and
messages, similar to WSDT,. The semantics of a service
is given wing the OWL WordiXet ontology: portType
(operations provided by the service) and messages ( o p
eration parameters) are mapped to disjnnctions of conjunctions of (possibly negated) concepts in thc OWL
IVordNet ontology. The semantics is given in terms of
how a service affects the external world. The present
design of USDL asslimes that each side-effect is one
of following four operations: create, update, delete, or
find. A generic affects side-effect is nsed when none of
the four apply. An application that wishes to make use
l d able to reason with
of a service antomatically s h o ~ ~be
WordNet atoms using the OWL IVordNet ontology.
We also define the formal semantics of USDT,. As
stated earlier, the syntactic terms describing portType
and messages are mapped to disjunctions of conjunctions of (possibly negated) OWL WordNet ontological
terms which can be represented by points in the lattice
obtained from the WordNet ontology with regards to
the OWT, subsumption relation. A service is then formally defined as a function, labeled with zero or more
side-effects, between points in this lattice. The main
contribution of our work is the design of a llnivcrsal
service-semantics description language (USDL), along
with its formal semantics, and soundness and completeness proofs for a theory of safe service substitution.

4. Design of USDL
The design of USDI, rests on two formal languages:
Web Services Description Language (WSDL) [7] and
Web Ontology Language (OWL) [6]. The Web Services Description Language (WSDL) [7], is used to give
a syntactic description of the name and parameters of
a service. The description is syntactic in the sense that
it describes the formatting of services on a syntactic
level of method signatures, but is incapable of describing what concepts are involved in a service and what
a service actlially does, i t . , the conceptl~alsemantics
of the service. T,ikewise, the Web Ontology T,anguage
(OWL) [6], was developed as an extension to the Resource Description Framework (RDF) [3], both standards are designed to allow formal conceptlial modeling
via logical ontologies, and these languages also allow for
the markup of existing web resources with semantic
information from the conceptual models. USDL employs WSDI, and OWL in order to describe the syntax
and semantics of web-services. WSDL is used to describe message formats, types, and method prototypes,
while a specialized ~lniversalOWT, ontology is used to
formally describe what these messages and methods

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

,

mean, on a conccpt,~lallcvcl .
USDI, can be regarded as the semantic counterpart,
t,o the syntactic WSDI, description. WSDT, documents
contain two main c o n s t n ~ c t to
, ~ which we want to ascribe conccpt,l~al meaning: messages and portType.
Thcsc constructs arc aggregates of scrvlcc components
whlch w ~ lbe
l directly ascrlhed meanlng Messages cons ~ s of
t typed parts and portType consists of operations
parameterized on messages USDI, defines OWL surrogates or proxles of thtsc constrl~ctsIn the form of
clnsscs, wh~chhave properties wlth values In thc OIVT,
IVordNct ontology

4.1

4.1.1

Atomic Concept

AtomicConcept is the act,llal contact point bctu~ecn
USDT, and IVordNet. This class acts as proxy for
WordKet lexical entities.

Concept

USDL defines a generic class called Conxept which
is used to define thc semantics of parts of messages. Semantically, instances of Con,cept form a complete lattice, which will be covered in section 7.
<owl:Class rdf:ID="Concept">
<rdfs:comment>
Generic class o f USDL Concept
</rdfs:comment>
<oul:unionOf rdf:parseType="Collection">
< o w l ~ C l a s srdf:about="#AtomicConcept"/>
<owl:Class rdf:about="#InvertedConcept"/)
<oal:Class rdf:about="#ConjunctiveConcept"/)
<owl:Class rdf:about="1DisjunctiveConcept"/>
</owl:unionOf>

<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#ofKind"/>
<owl:mincardinality
rdf :datatype="&xsd;nonBegativeInteger">
0
</owl:mincardinality>
</oul:Restriction>
</rdfs:subClassOf>

The property cardinality restrictions require all
USDL AtomicCoricepts t o have exactly one defining
value for the is,4 property, and zero or more vallies for
the ofKind property. An instance of iltomicConcept
is considered t o be equated with a WordNet lexical
concept given by the i s 4 property and classified by a
lexical concept given by the optional ofliind property.

4.1.2

The USDL Concept class denotes t,hc top element, of
the lattice of concept~ialobjects constn~ctedfrom the
OWT, IVordNct ontology. For most pnrpo.ws, message
parts and other WSDT, constrlicts will he mapped t o a
sl~bclassof USDT, Concept so that useft11 concepts can
be modeled as set thcorct,ic formlllas of llnion, intersection, and negation of atomic concepts. These snbclasses of Concept are iltomicConccpt, InuertcdConcept, C,'or~junctivcConccpt,and DisjunctducConcept.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Inverted Concept

In case of InvertedConcept the corresponding semantics
arc the complement of WordNct lexical entities.

4.1.3

C o n j u n c t i v e and D i s j u n c t i v e C o n c e p t

The Con.junctiveConcept and Disjunctit~cConcept respectively denote the intersection and union of cSDI,
Concepts.
Note that each of these specializations inherits the
domain and range of the affects property. Most services can be described as a conjl~nctionof these types of
effects. For those services that cannot be described in
terms of a combination of these specializations, the parent affects property can be used instead, or the p r o p
erty can be omitted entirely when the meaning of the
operation parameter messages are enough for concep
tnal reasoning. The purpose of limiting the types of
services as opposed t o allowing the creation of new arbitrary side-effect types, for cxample via OWL-DL, is
to: (i) make USDI, morc structured and therefore rasier t o create docliments in, (ii) make USDI, compiltationally more tractable for programs that process large
volumes of USDI, documents, and (iii) help prcvcnt the
semantic aliasing problem mentioned in section 2.

4.3 Conditions and Constraints

The property cardinality restrictions on Conjunctit:cConccpt and DisjtinctiveConcept allow for n,-ary
intersections and iinions (where n
2) of USDI, concepts. For generality, these concepts are either AtomicConcepts, Conj~~nctiveConccpts,
DisjurictiveConcepts,
or InvcrtcdConccpts .

>

Services may have some extmnal conditions (preconditions and post-conditions) specified on the input or outpnt parameters. Condition class is used
to describe all such constraints. Conditions are r e p
resented as conjunction or disjlinction of binary predicates. Predicate is a trait or aspect of the resource
being described.

4.2 Affects
Thc r~ffectsproperty is specialized into fonr types
of actions common to enterprise services: creates, updates, delctes, and finds.
<owl:ObjectProperty rdf:ID="affectsW>
<rdfs:comment>
Generic class of USDL Affects
</rdfs:comment>
<rdfs:domain rdf:resource="#Operation"/>

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

<owl:Class rdf:ID="Condition">
<rdfs:comment>
Generic class of USDL Condition
</rdfs:comment>
<owl:unionOf rdf:parseType="Collection">
<awl:Class
rdf:about="#AtomicCondition"/)
<owl :Class
rdf:about="#ConjunctiveCondition"/)
<owl:Class
rdf:about="#DisjunctiveCondition"/>
</owl:unionOf>

<onl:ObjectProperty rdf:ID="hasCondition">
<rdfs :domain rdf :resource="#Concept"/>
<rdfs:range rdf:resource="#Condition"/)
</owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveConditioiz and DisjfinctiveCondition allow for n,ary conjllnctions and disjllnctions (where n
2) of
USDT, conditions. In general any n,-ary condition can
be written as a combination of conjl~nctionsand disjunctions of binary conditions.

>

4.4

A condition has exactly one value for the onPart
property and at most one valllc for the ha3 Valur property, each of which is of type USDL Concept
<owl:ObjectProperty rdf:ID="onPart">
<rdfs:domain rdf:resource="#AtornicCondition"/>
<rdf s :range rdf :resource="#Concept"/>
</owl:ObjectProperty>
<owl : ObjectProperty rdf :ID="hasValue0'>
<rdfs:domain rdf:resource="#AtomicCondition"/)
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

4.3.1

Conjlinctive a n d Disjunctive Conditions

The Con,.i~~n,ctiireCondition
and Disjt~nctiveCondition
respectively denote the conjilnct,ion and disjunction of
USDL Conditions.

Messages

Services commnnicat,c by exchanging messages. As
mentioned, messages are simple tuples of actual data,
called parts. Take for example, a flight reservation service similar to thc SAP ABAP Workbench Interface
Repository for flight reservations [4], which makes use
of the following message.
(message name="&flight;ReserveFlight-Request?
<part name="&flight;CustomerUame" type="xsd:stringU>
<part name="&flight;FlightBumber" type="xsd:string">
<part nme="&flight;DepartureDate" type="xsd:date">

T h e USDL surrogate for a WSDL message is the
Message class, which is a composite entity with zero
or more parts. Note that for generality, messages are
allowed t o contain zero parts.
<owl:Class rdf:abaut="#lessageU>
<rdfs:subClassOf>
<onl:Restriction>
<owl :onproperty rdf :resource="PhasPart"/>
<owl:minCardinality
rdf :datatype="&xsd;nonlegativeZnteger">
0

</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</oal:Class>

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Each part of a nicssagc 1s simply a VSDT, Conccpf,
as defined by t,hc hasPrart propcrty. Scmantically mcssages are trcatcd as t,nplcs of concepts.

Continning onr example flight, reservation service,
the Itincrarv message is given scniantics using USDT, as
follows, where &wn ;customer and &wn;name are valid
XMT, rcfercnccs to IVordNet lexical concepts.

<owl:ObjectProperty rdf:ID="hasOperation">
<rdfs :domain rdf :resource="#portType"/>
<rdfs:range rdf:resource="#Operation"/>
</owl:ObjectProperty>

As with the case of messages, portTypes are not directly assigned meaning via t'he OWL IliordIL'et, ontology. Instead tho individual Operations are described by
their side-effects via an r~flectsproperty. Note that the
parameters of an operation are already given meaning
by ascribing meaning t o the messages that constitute
the parameters.

A scrvicc consists of portTypc, which is a collecor operations that arc parametric
tion of proced~~rcs
on messages. Our example flight rcscrvation scrvice
might contain a p o r t T w c definition for a flight reservation scrvicc that takes as input an itinerary and outputs a rcscrvation receipt.

An operation can have mnltiplc or no values for the
a f i c t s property, all of which are of type USDL Concept,
which is the target, of the effect.

Finishing our flight reservation scrvice cxamplc, we
can describe the main side-effect of invoking the ReserucFlzght operation in USDL as follows.
The USDL s ~ ~ r r o g ais
t cdcfincd as the class portType
which contains zero or more O p c m t l o n s as valllcsof the
hasOpcratzon property.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Again, it, is not necessary t o annotate the operation
with regards to its input and out,put parameters, a..
t,hcse arc already annot,atcd by ~brcssc~gc
surrogates.

5. Semantic Description of a Service
This scction shows an cxample syntactic description
of a wcb-scrvicc nsing IVSDT, and its corresponding
scmantic dpscription Iislng ITSDT,.
A simple Book-Buying Service: T h e service dcscribed here is a simplifietl book-buying service published in a wehscrvic-c registry. This service can be
treated as atomic: i.c., no interactlions between buying
and selling agents are rcqiiired, apart from invocation
of t,hc service and rcceipt of it,s olitp~ltsby the buyer.
Given certhin inp~ltsand pre-conditions, the servicc
provides certain o ~ i t p u t sand has spccific effects.
This service takcs in a BooklSR,V, U~serldentifier,
and Poss,tuord as input parameters. It has an input,
prc-condition that a Ltscrldentificr for the buyer must
cxist before invoking the service. It also has a global
e r the buyer
constraint that, a valid credit card n ~ ~ m bfor
mlist exist. This service o11tp11tsan OrderNumber if the
In case the book is not
order was placed s~lcccssfi~lly.
available, the o i ~ t p ~isi t an "Out of Stock" mcssagc.

5.1 WSDL definition
The following is WSDT, definition of t,hc service.
This service provides a single operation called RookBuying. The inpnt and o u t p ~ i tmessagcs arc defincd
below. The conditions on the service cannot be described using WSDL.

<portType name="BookBuying-Serviceu>
(operation name="BookBuying">
<input message="BuyBook-Request"/>
<output message="BuyBook-Response"/>

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

5.2 USDL annotation
The following is the completc USDL annotation corresponding t o the above mentioned WSDL description.
The input pre-condition and the global constraint on
the service are also described semantically.
<definitions>

...

<portType rdf:about="#BookBuying_Service">
<hasoperation>
<operation name="BuyBook">
<input message="BuyBook-Request"/)
<output message="BuyBook-Response"/>
<creates>
<Atomicconcept rdf:about="#BookOrder">
<isA rdf:resource="&wn;order"/>
<ofKind>
<Atomicconcept>
<isA rdf:resource="&an;book"/>
</AtomicConcept>
</of Kind>
<hascondition>
(Condition rdf:about="#exists">
<Atomicconcept>
<isA rdf:resource="&wn;exists"/>
</AtomicConcept>
<onpart rdf :about="#CreditCard">
<Atomicconcept>
<isA rdf:resource="&nn;card"/>
<ofKind>
<Atomicconcept>
<isA rdf:resource="&wn;credit"/>
</AtornicConcept>
</ofKind>
</Atomicconcept>
</onpart>
</Condition>
</hascondition>
</Atomicconcept>
</creates>
</operation>
</hasoperation>
</portType>

6. Service Discovery and Composition
<isA rdf:resource="&an;identifier"/>
<of Kind>
<Atomicconcept>

<haspart>
<Atomicconcept rdf:about="#UserIdentifierU>
<isA rdf:resource="&wn;identifier"/>
<ofKind>
<AtomicConcept>

</haspart>

<DisjunctiveConcept

rdf:about="#OrderEurnber/Availability">
<hasconcept>
<Atomicconcept rdf:about="#Orderkunber">
<isA rdf:resource="&an;number"/>
<of Kind>

Note that, given a directory of services, a USDT, description conld bc inclllded for each service, making
the servicc directly "semantically" searchable. However, we still need a query language t o scarch this directory, i . ~ . .wc
, need a langnage t o frame the recll~iremcnts on the service t h a t an application developer is
seeking. Note that USDT, itself collld be such a query
language. A USDI, description of the dcsired service
can be writken, a clllcry processor can then search the
service directory to look for a "matching" service. For
matching we could treat USDT, descriptions as well
the USDI, query as terms, and perhaps nse some kind
of cxtendcd nnification t o chtck for a match. This work
is current.ly in progress [lo].
With tjhe USDT, descriptions and query language
in place, nllmerous applications become possible ranging from querying a database of services t o rapid a p
plication dcvclopmcnt via automated integration tools
and even real-time servicc c ~ m p o s i t ~ i o[In21. Take our
flight reservation service example. Assume that somebody wants to find a travcl reservation service and
that they qllcry a USDT, database containing gcneral
purpose fl~ghtrescrvation servlccs, bns reservation services, etc One could then form a USDL query conslstIng of a descrtpt~onof a travel reservat~onservice and
ld
with a sct of travel reserthe databasc c o ~ ~ respond
vation services whether it be via flight, bus, or some
other means of travcl. This flexibility of generalization
and specialization is gained from USDL's sllbsnmption
relation covered in section 7.
Furthermore, with a pool of USDI, services a t one's
disposal, rapid application development (RAD) tools
could be used to aid a systems integrator with the task
of creating composite services, i.c., services consisting
of the composition of already existing services. T h e
servicc designer collld 11sesuch a RAD tool by descrih
ing the desired service via a USDI, document,, and then
the tool would cluery the pool of services for composable sets of services that, can be 11scd t o accomplish the
task as well as automatically generate boilerplate code
for managing the composite service, as well as menial
inter-service d a t a format conversions and other glue.
Of course these additional RAD steps wo111d require
[13].
other technologies already being iii~estigat~ed
At present, we prcsnme only four specific opcrations (find, create, delete, update); this set of basic operations may be extended as more experience is
gained with VSDI,. In practice, most services, however, deal with manipnlating databases and for such
services these four operations s~~ffice.
As stated, one of
the reasons for limiting the side-effect types is t o safe-

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

gl~ardagainst, t,hc semantic aliasing problem described
in scct,ion 2. This is also one of t,hc main reasons for rest,ricting the combining forms in 'CTSDT, t,o conjunction,
disjnnction, and negated atoms. As disc~lsscd nest,,
this allows UST)T, descriptions to be put into a type of
disjnnctivc normal form (DNF), from which a sollnrl
and complet,~notion of snbsnmption is created.

Now that the formal definitions of concept and scrvice de~cript~ions
are given, we wonld likc t o define a
snbsumption relat,ion SZ:over C so that we can r c a s o ~ ~
,
this will, in turn,
about s~~hstit~lt,ability
of S C ~ V ~ C C Sbut
recluirc a sl~bsnmptionrelation
over O. The proof
of correctness of <c is covered by the principle of safc
sl~bst~itll
tion bclow.

7. Theory of Substitution of Services

Definition 5. As.su,rning ,tuithout loss of generality that
011 concepts are e.rpressed i n DNF, let
be the ordcring relation defined on concepts such that:

Next, we will in~cstigat~c
the theorct,ical aspects of
USDT,. This involves conccpts from set theory, lat,t,ice
theory, and typc thcory. From a systems integration
perspective, an engineer is intcrestcd in finding (discovering) a service that accomplishes some necessary
task. Of conrsc, snch ;I service may not be present
in any service directory. Tn such a casc the discovery soft,warc should return a sct of services that can
be used in a contcxt cspcct,ing a service that, meets
that description (of c o ~ ~ r sthis
e , set may be empty). To
find scrviccs that can be snbstit,~~ted
for a given service
that is not present in the directory, we need to develop
a theory of scrvice .substitutability. \Vc develop stlch a
theory in this section, and show its sonndness and completeness. Onr thcory relates service substitntabilit,y to
\?VordNct,'s snbsumption relation.
Tn order t,o develop this thcory, we must first formally define const,n~ctssuch as USDL-described concepts and scrviccs, which we will also call concepts and
servicc.~for short. While it is possiblc t o work dircctly
wit,h the SMT, USDI, syntax, doing so is cumbcrsomc
and so we will instead opt for sct theoret,ic notation.

Definition 1. Let 0 be the set of VVordNet entities
and <a
- be the OM'I, .stihsnrnption relation o n 0.
Definition 2. Let O be the least set of concepts s.t.:
,
:E implies E , 7.1: E O
2. z , y E O implies n: U y, z n y E O
Hence 0 is simply the set of USDT, concepts.
1.

2

Definition 3. Lrf r = { ( L , E ) I L E Q, F: E
O} be thc .set of USDL sidc-cfJccts, ~uhere Q =
{ c r c n t c s ,,trpdntrs, delctcs, finds), L is the affect type
and E is thc affected object.
Definition 4. For any sct S , lct

1. x i e y f o r x , y E R and x < n y
2. l x So ~y if and only if y Ln I for

Uvi u ~ i

2,

Uvj x"jf

and only if for all wj =
zl such that for
every q there exists some yk Ina.
Item 3 essentially extends the subsl~mptionrclation to
conccpts expressed in DNF.

3.

nVkyk there exists some xj = nvl
<@
-

Definition 6. I,et ( A , I , 0)5 ~(A',
: I', 0')if and only
if V ( L , E ) E A . 3 ( L , E') E A ' . E
E' and
I' So* I and 0 l o * 0',where <o.
is
the
elementwise e.tten.sion of So t o lists of concepts.

sO)

Note that (0,
and ( 3 ,Lc) respectively form
a complete lattice. Given a description of a service
cr E C, we can now define the set of s ~ ~ b s t i t l ~ t a b l e s
C'(rz),which in practice corresponds t o a query against
a database of services C, for services that satisfy dcscription a. Notice that t,he definition is contravariant
~ t s contravari.
for inputs and covariant, for o ~ ~ t p ~This
ance is also seen in the field of typc theory with regards
g , will be covered in the
to polymorphic s ~ ~ b t y p i n and
proof of thc principle of safe substitution below.

Definition 7. Let C be a function parametric over
services, 7ohich maps X t o a sribset of services such
that C ( a ) = {a'] a' Sc a).
s comTn order t o be able to prove the s o ~ ~ n d n e sand
~ l c t ~ e n e sofs C', we need a lemma known as the "Principle of Safe Snbstitt~t,ion."

Lemma 1. For a n y services a,a' E X, if a <_r, a' then
a can sc~felybe used i n a context expecting scrvicc a'.
Theorem 1. (Soundness of Sub.stztutab1e.s) For any
services (T and a', z f servzce a' can not be safelv 11sed
in o context expecting service a then a' # C(a), that is
the set of .silbstitt~tahlesfor a service does not contain
a n y incompatible scraiccs,
"

S* = { c l c $ S ) ~ S ~ { ( x , Iyx), : S S , y ~ S * }
he the set of 1i.st.s over S . Iiet C = ((4,I)O)I A E
2r, I E 0 " , 0 E O* } be the set of USDI, serr~iccdcscriptions, ~i:hcreA is the set of side-effects, I is the

E0

"

Proof. Assume the existence of services a and a' SIICII
list of input pnramctcrs, ond 0 is the list of o t ~ t p t ~ t that a' can not, be safely used in a contcxt expecting
service a. By the principle of safe s n b s t i t ~ ~ t i oitn is not,
por(~m.ctersof (I particular .seruice.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

true that a' S s a , a n t i hcncc by t,hc principle of ext,ensionality, a' # C((a). Th(v-cforc the set of snbstitutablcs
only cont,ains correct,, i.c., compatible scrvices.
Theorem 2. (C0~m~1ctcnc.s.s
of Substitutables) For any
service a , if thcrc c.rists a scrziicr (TI
that can safely be
ilsed in a coiite.rt c.rpectiny scrvicc a then cr' E C(cr),
that is thc set of suh.stitutab1e.s contains all ser1:iccs
compotiblc ~r:itlto yi11cn dc.scription.
Proof. Assnmc thcrc cxist,s snch a service n', then by
(T' S c a . So by the
the principle of safe s~lbst,it,~~t,ion
, therefore the set of
definition of Gq(a),n' E C ( ( T )and
0
s~~bstitntablcs
is complct,e for arbitrary (T E 3 .

The sollndncss and complet,cness theorems essentially state that, the WorclNet snbs~lmption relation
provides a strong enollgh fo~lndationfor safe servicc
snbstitt~tability.Note that this theory is quite general,
and is cven applicable to scrvicas described wing domain spccific ontologies (as in OW1,-S and WSML).

8. Comparison with OWL-S and WSML
In this section we present, a comparison of USDL
with othcr popnlar approaches such as OWL-S [5] and
WSMT, [I]. Our goal is t,o identify the similarities and
differences of LTSDI, with t,hese approaches. OWT,-S is
a scrvicc dcscription l a n g ~ ~ a gwhich
c
attempts to address the problcm of semantic description via a highly
detailed scrvicc ontology. Rnt OW1,-S also allows for
complicated combining forms, which seem to defeat, the
tractability and pract,icalit,y of O\VT,-S. The focns in
the design of O\VT,-S is to describe t,he structure of a
scrvicc in terms of how it combincs othcr slihservices
(if any nscd). The dcscription of atomic services in
OWTI-S is left nnderspccificd [9]. OW1,-S inclndes the
tags presents to dcscribc the scrvice profile, and the tag
describedRy t,o dcscribc the scrcicc m.odcl. The profile
describes the (possibly conditional) states that exist
before and after the scrvicc is esccnted. The service
model describes how the scrvicc is (algorithmically)
constructed from ot,her simpler scrvices. What the serly
has to be inferred from these
vice a c t ~ ~ a laccomplishes
two descriptions in OWT,-S. Given that OWL-S uses
complicated combining forms, inferring thc task that
a servicc actnally performs is, in general, nndecidable.
In contrast, in USDT,, what, the scrvice actually docs is
directly described (via the verb affects and its refinements create, update, dcletc, and find).
OW[,-S recommends that atomic services be defined
using domain specific ont,ologics. Thus, OWL-S needs
~lscrsdescribing the scrvices and ~lscrsusing the services to know, ~~ndcrst,and
and agree on domain specific

ontologies in which the scrvices arc described. Hence,
annot2ating scrvices with OW1,-S is a very timc consnming, cumbersome, and invasive process. The complicated natnrc of OWL-S's combining forms, cspccially conditions and control constructs, seems to alsemantic aliasing problem
low for the aforem~nt~ioned
[9]. Other recent approxhes such as WSMO, WSMT,,
etc., snffer from the same limitation [I]. In contrast,
USDL nscs the nnivcrsal IVordNet ontology t,o solve
this problem.
Note that USDT, and OW1,-S can be used together.
A USDT, description can be placed under the dcscribcdBy tag for atomic processes, while OWL-S can
be nsed to compose atomic USDI, services. Thus,
USDL along with JVordNet can be treated as the universal ontology that can make an OWL-S dcscrip
tion complete. USDT, documents can be nsed to describe the semantics of atomic services that Owl,-S
assumes will be described by domain specific ontologies and pointed to by the OWL-S describedRy tag. In
this respect, USDL and OWL-S are complementary:
USDL can be treated as an extension to OWL-S which
makes OW1,-S description easy to write and scmantically more complete.
OWL-S can also be regarded as the composition langnage for USDL. If a new service can be built, by composing a few already existing services, then this new
servicc can be described in OWL-S nsing the USDI,
descriptions of the existing services. Next, this new
scrvice can be al~tomaticallygenerated from its Owl,S dcscription. The control constructs like Scyucncc and
If-Then-Else of OWL-S allows us t o achieve this. Note
once a composite service has been defined using OWLS that uses atomic services described in USDT,, a new
USDL dcscription mnst be written for this composite
servicc (automatic generation of this description is currently being investigated [lo]). This USDL description
is the formal documentation of the new composite service and will make it antomatically searchable oncc the
new service is placed in the directory service. It also
allows this composite service to be treated as an atomic
service by some other application.
For example, the aforementioned ReserueFlight scrvice which creates a flight reservation can be viewed as
a composite process of first getting the flight details,
then checking the flight availability and then booking
the flight (creating the reservation). If we have these
three atomic services namely GetFlightDetails, CheckFlight.4 uailability and RookFlight then we can create
our Reserl~cFlightservice by composing these three services in sequence 11singthe OWL-S Scgr~cnceconstruct.
The following is the OWL-S description of t,hc composed Reser~teFltghtservicc.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

<rdf :RDF
xmlns : rdf="http: //www.w3. org/1999
/02/22-rdf-syntax-ns#"
xmlns:process="http://nww.daml.org/services
/owl-s/l.O/Process.owl#">
<process:CompositeProcess rdf:ID="ReserveFlightU>
<process:composedOf>
<process:Sequence>
<process:components
rdf:parseType="Collection">
<process:AtomicProcess
rdf:about="#GetFlightDetails"/>
<process:AtomicProcess
rdf:about="#CheckFlightAvailability"/>
<process:AtomicProcess
rdf:about="#BookFlight"/>
</process:components>
</process:Sequence>

\Vc can generate this composed Rescr7icFlight service ant,omat,ically by irsing the USDT, descriptions of
the component scrviccs for discovering them from the
misting scrviccs. Once we have the component services, t11c OWTI-S description can be used to generate
the new composcd service.
Another rclatcrl area of research involves message
conversation constraints, also known as behavioral signatures [I:3]. Behavior signature models d o not stray
far from the explicit description of the lexical form
of mcssagcs, they cxpect, the mcssagcs t o be Iexically and semant,ically correct, prior to verification
via model checking. Hcncc behavior signatures deal
wit>h low-lcvcl fi~nctionalimplementation constraints,
while USDL deals with higher-level real world concepts. However, USDT, and behavioral signatnres can
be regarded as complemcnt,ary concepts when takcn in
the context, of real world service composition and both
technologies arc currently bcing nscd i n the devclopmcnt, of a commercial scrviccs intcgrat,ion tool [12].

9. Coriclusions and Future Work
To reliably catalognc, scardi and compose scrviccs
in a scrni-a~~tornatic
to fi~lly-al~toniatic
manner we need
standards to publish and docllmcnt services. This rcqnires langllagc standards for specifying not just the
syntax, i.e., protot,ypcs of service procedur~sand messages, but, it also ncccssitates a standard formal, yet
higll-level means for specifying the semantics of servicc
proccdrlrcs and nicssages. We have addressed thesc issnes by dcfining a nnivcrsal service-scmant,ics descript,ion language, its semantics, and we have proved sonic
nscfnl properties abont t,his langnage. The current version of USDI, incorporates crlrrcnt standards in a way

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

t,o fnrther aid marknp of I T scrviccs by allowing constnlcts t,o be given meaning in terms of an OWT, based
WordNet ontology. This approach is more practical
and tractable than other approaches becansc dcscrip
tion docl~mentsare more casily created by humans and
more casily processed by complltcrs. USDT, is currently
being nscd t,o formally describe web-services related to
emergency response fi~nctions[Ill. Flltllrc work involves tho application of USDI, t o formally describing
commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well
as to scrvice discovery and rapid application develop
ment (RAD) in commercial environments [12]. Flitnre
work also inclndes developing tools that will allow alltomatic generation of new services based on combining
USDL descriptions of existing atomic services. The
interesting problem t o be addressed is: can USDT, description of such automatically generated services be
also alltomatically generated?

References
[I] A c o n c e p t ~ ~ acomparison
l
bet,ween wsmo and owl-s.
http://www.wsmo.org/TR/d4/d4.l/v0.1.
[2] Ont,ology-based information management syst,em,
wordnet owl-ontology.
http: //taurus.unine .ch/
knowler/wordnet.html.
[.?I Resoiirce description framework. http: / / w w .w3.
org/RDF.
[-11 Sap interface repository.
http: //ifr .sap.com/
catalog/query.asp.
[j] Semantic markup for web services. http: //wvw .daml.
org/services/ovl-s/l.O/owl-s.htm1.
[6] Web ontology language referenre. http :/ / w w .v3.
org/TR/owl-ref.
[7] Web services description I a ~ ~ g i a g ehttp:
.
/ / w w .w3.
org/TR/wsdl.
[8] Wordnet: a lexical database for t,he english langllage.
http://wuw.cogsci.princeton.edu/-vn.
[9] S. Balzer, T. T,iebig, and M. Wagner. Pitfalls of owl-s
- a practical semant,ic web use case. In 1(7SOC, 2004.
[I01 A. Bansal, S. Kona, I,. Simon, A. Mallya, G. Glrpt,a,
and T. Hite. Automatic querying and composite services generation with usdl. Working paper, 2005.
[I I] A. Ransal, K . Patel, G. Gupta, R. Raghavachari, E. D.
Harris, and J . C. Staves. Towartls intelligent services:
A case study in chemical emergency response. In
ICkV.7, pages 751-758, 2005.
[I?] T. Hite. Service composil.ion and ranking: A strategic
overview. Internal Report, Met,allect Inc., 2005.
[13] R. H~illand .l. Su. Tools for design of composit,~web
services. In SJGMOD,2004.
[II] I,. Simon, A. Ransal, A. Tvlallya, G. Cup(.a, and
T. Hite. Towards a universal service descript.ion Ianp a g e . I11 hrW'eSP, 2005.

Decision Support Systems 53 (2012) 234–244

Contents lists available at SciVerse ScienceDirect

Decision Support Systems
journal homepage: www.elsevier.com/locate/dss

Workﬂow composition of service level agreements for web services☆
M. Brian Blake a,⁎, David J. Cummings b, Ajay Bansal c, Srividya Kona Bansal c
a
b
c

Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, United States
Department of Computer Science, Stanford University, United States
Department of Engineering, Arizona State University, United States

a r t i c l e

i n f o

Article history:
Received 19 July 2010
Received in revised form 22 December 2011
Accepted 30 January 2012
Available online 8 February 2012
Keywords:
Service level agreements
Quality of service
Web services
Service-oriented computing

a b s t r a c t
Service-oriented architecture enables an environment where businesses can expose services for use by their
collaborators and their peer organizations. In this dynamic environment, organizations require the use of service level agreements (SLAs) to assure the quality of service (QoS) standards of services provided by their collaborators. In an ad-hoc workﬂow scenario, a business may need to perform real-time composition of existing
services in response to consumer requests. In this work, we suggest that, in parallel to traditional web service
composition, the business must also compose the existing SLAs in order to ensure the service levels that must
be guaranteed to new consumers. Ultimately, this approach to SLA composition must align with the overarching principles of the provider and the priorities of the consumer. In this paper, we introduce a model
and representations of service level agreement attributes appropriate for managing a service provider's expectations when adding new partners. Our evaluations suggest that the SLA composition can efﬁciently
run concurrently with traditional service composition.
© 2012 Elsevier B.V. All rights reserved.

1. Introduction
A service level agreement, SLA, is a technical contract between two
types of businesses, producers and consumers. A SLA captures the
agreed-upon terms between organizations with respect to quality of
service (QoS) and other related concerns. In simple cases, one consumer forms a SLA with a producer. In more complex cases, a consumer may form a SLA that deﬁnes a set of producer businesses.
Considering a service-oriented computing environment, capabilities
are shared via the implementation of web services exposed by a producer organization. The ultimate goal of service-oriented computing
is for consumers to access these shared capabilities on-demand. As
such, in cases where businesses have longstanding relationships,
such as workﬂow and supply chain environments, peer companies
that share services must be able to assure a level of service to their
underlying customers [4,8].
New speciﬁcations, such as the Web Service Level Agreement (WSLA)
and Web Service Agreement (WS-Agreement) [2] enable SLAs to be associated with an individual web service or even groups of web services.
These speciﬁcations deﬁne an eXtensible Markup Language (XML)based data model that can be used along with the Web Service Description Language (WSDL) documents that traditionally describe the
web services. These speciﬁcations provide a signiﬁcant opportunity.
☆ This paper is a substantial extension of earlier work presented in [7] and [5].
⁎ Corresponding author. Tel.: + 1 574 631 1625; fax: + 1 574 631 8007.
E-mail addresses: m.brian.blake@nd.edu (M.B. Blake),
david.cummings@stanford.edu (D.J. Cummings), srividya.bansal@asu.edu,
ajay.bansal@asu.edu (A. Bansal).
0167-9236/$ – see front matter © 2012 Elsevier B.V. All rights reserved.
doi:10.1016/j.dss.2012.01.017

Organizations can specify QoS-related concerns in concert with the functionality concerns already captured in the WSDL ﬁles. As a result, when a
new organization searches for a pertinent web service, the SLA-enhanced
WSDL ﬁle can be used to determine the appropriateness of the service to
meet the required business need. Furthermore organizations can use the
SLA-enhanced WSDL ﬁle to negotiate the QoS terms.
Although these SLA technologies and speciﬁcations present new
opportunities for service-oriented business processes, there are a
number of signiﬁcant barriers. When a consumer organization must
create a new business capability that requires the workﬂow composition of multiple web services, then that organization will also need to
understand the composite impact of the underlying SLAs. Consequently, in addition to composing web services that are functionally
compatible, the organization will need to ensure that the web services are compatible with regard to their service levels. Also, the
product of all the SLAs for a composition of web services must be
within the required threshold of feasibility as deﬁned by the end
users. As the service-oriented computing paradigm increases in popularity, the consumer will have the option of many similar services
that may meet a particular requirement. As such, the composition of
web services that is most efﬁcient for a particular business purpose
will rest on the organization's ability to understand and optimize
the corresponding composition of SLAs.
To deal with the aforementioned issues, we introduce the phrase,
workﬂow composition of SLAs. Our approach suggests the multidimensional evaluation of existing agreed-upon QoS standards in
order to predict the standards possible for the introduction of new
agreements. While the notions of multi-dimensional analysis, optimization, dynamic programming are not new [9,10,12,17,35] in this

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

work, we identify the speciﬁc SLA-based attributes that allows for the
introduction of new partners. Furthermore we develop a set of principles and the associated process that utilize the SLA information to estimate service levels. This approach favors services with clean
request/response (RPC-type) communication, generally known as
WSDL-based web services. Further investigation would be required
to assess this approach as it relates to REST-based services [37].
In this work, we investigate several research issues relevant to the
integration of web services-based workﬂow:
1. What SLA measures and principles are appropriate to support QoSbased assessment of existing service level guarantees?
2. Given a group of SLAs and knowledge about current consumer service
level needs, can an on-demand request be analyzed against existing
SLAs to guarantee a certain service level for a new consumer?
The paper proceeds in the following section with a discussion of
related work. In Section 3, we discuss how the SLA-based QoS assessment values are derived from higher-level organizational principles.
The formal details of the attributes are deﬁned in Section 4, and
how the attributes are physically captured in markup languages in
shown in Section 5. Finally, in Section 6 we evaluate the performance
SLA composition as it runs parallel with traditional service composition routines.
2. Related work
There are many related projects that investigate the general use of
SLAs for web services [18]. Some projects characterize SLA approaches to speciﬁc domains, such as military, database management,
or information systems [13,20,26,29]. There is also a large body of
work that attempts to automate the management and negotiation of
SLAs [11,16,23,27,33]. Other work attempts to use semantics to automate the negotiation of SLAs [15,25].
Our work leverages markup language (i.e. WS-Agreements) for
providing SLA measures as in other studies [1,28,30]. All related
work describes the importance of composing SLAs. In [28], their emphasis is on compatibility between user requirements and provider
constraints. Their approach suggests a promising model-based approach to assuring the compatibility.
Our work is closely related to the comprehensive work performed
by [9,10,35]. Each of these approaches investigates the QoS-based and
constrained composition of web services. Although Canfora et al. [9]
has an elegant approach that allows for the insertion and aggregation
of any user-deﬁned QoS attribute, our approach identiﬁes the speciﬁc
SLA measures that support the user-driven assessment of an environment where their SLAs dictate current system state. Table 1 shows a
survey of SLA attributes and how they are exploited in related projects speciﬁcally in the service-oriented computing domain.
Our work can be loosely classiﬁed in the body of work that looks
to automate the aggregation of QoS attributes [14,21,24,32]. The

235

uniqueness of our approach is that we consider the impacts when
new web service workﬂows must be added as they affect the existing
operational SLAs. More speciﬁcally, if a composite capability overlaps
multiple SLAs, then the characteristics of an early SLA can impact a
later SLA in the composition routine. Canfora et al., Cardoso et al.,
Zeng et al., [9,10,35] concentrate on deriving a speciﬁc composition
routine as constrained by QoS values. Canfora et al., Zhang et al.,
and Yu et al. focus on iterative multiattribute utility approaches [12]
where to focus is on the overall optimization function and less on
the details of each of the attribute. Our work attempts to consider
both consumer and producer concerns when assessing the entry of
a new workﬂow. As such, our work contains low-level details for
each service level objective such that subsequent optimization approaches use them as a model for optimization that targets each attribute at a low-level. Although [22] has a similar approach where SLAs
are aggregated formally, they do not consider consumer and producer
services independently as in our work.
In summary, we deﬁne speciﬁc SLA measures and formally integrate measures across multiple SLAs. We also deﬁne a principled process for the composition of SLAs. This work extends related work [6]
by concentrating on SLA measures in markup language ﬁles as opposed to Uniﬁed Modeling Language (UML) models. Unlike other
work in QoS-based web service composition, we attempt to classify
QoS attributes by those associated with the provider and those associated with the consumer. Another variation here is the introduction
of several high-level criteria that can be used to characterize organizations. We believe that by aggregating all lower-level attributes
into a smaller set of higher-level criteria then organizations can be
quantitatively evaluated or scored. Further evaluation in this paper
justiﬁes that such on-demand assessment of SLAs performs feasibly
in an operational environment where very large numbers of web service workﬂows exist.
3. Assessing an enterprise based on its SLAs
The typical SLA has a large number of measures and criteria. However, in this work, we attempt to choose the measures that are most
closely aligned to aggregation of a group of SLAs and ultimately
their assessment. In the operational notion of web service composition, a basic web services workﬂow system must ensure that the
input information supplied by the consumer ultimately leads to the
required actions and outputs required by that consumer. In parallel,
the workﬂow management system must ensure that the predicates
and requisites match (either by syntactical or semantic techniques)
in each step of the workﬂow. It is the operational composition routines that motivate the set of SLA attributes relevant to our work.
In order to designate which attributes that are most relevant to
our proposed innovation, we developed a set of principles important
to managing the quality of an enterprise with many business processes. The three relevant principles are Compliance (Suitability),

Table 1
Survey of research projects that consider SLA attributes for web services.
Author names

Run
time

(Blake et al.) [7] and this paper
✓
(Canfora et al.) [9]
✓
(Yu et al.) [34] Zhang et al.) [36] **
(Zeng et al.) [35]
(Cardoso et al.) [10]
(Jin et al.)[18] *
✓
(Mohabey et al.) [22]

Reputation Uptime
(Avail)

Resp time
***

Negotiation
(rebinding)

Cost
(price)

Success rate/
reliability

Problem
resolution

Maintenance

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓

✓
✓
✓
✓

✓
✓
✓
✓

✓
✓
✓
✓

✓
✓
✓
✓

✓
✓

* [18] list attributes, but do not develop formal approaches for composition.
** Although Canfora et al., Yu et al., and Zhang et al. do not formally deﬁne each attribute, their work focuses on an approach that allows any attribute to be aggregated within the
composition routine.
*** Response time and run time (and in other places Service Rate) have the same or different deﬁnitions. This table places those attributes into separate columns to show the
different meanings across the survey of related literature.

236

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

in this paper. As such, we assume the validity of this model as the applicability of the work to other domains, in some sense, relies on
these underlying elements. The paper proceeds with a formal deﬁnition of all attributes relevant to aggregating and assessing a group of
SLAs, and subsequently a description of how the information can be
stored physically. The ﬁnal sections discuss the approach and performance for aggregating SLAs of many business processes.

Sustainability, and Resiliency. By correlating the QoS attributes to
these principles, we have developed a list that is suitable for assessing
an organization based on existing SLAs. The three principles (illustrated in Fig. 1 as a Uniﬁed Modeling Language class diagram) are deﬁned
below in addition to their underlying SLA measures.
3.1. Compliance (suitability)

4. Aggregating and assessing service level agreements

Compliance is the principle that ensures that the consumer receives the requested composite capability at the service level that is
required. The functional notion of web service composition (as illustrated in Fig. 1) ﬁts within this principle, since the consumer speciﬁes
their required outputs of the composition. Considering SLA terms, the
composition process must assure that the aggregate cost, uptime, and
run time are compliant with the user requirements. Cost is the sum
total price of all services participating in the solution process. Uptime
is a guarantee by the service providers that their services will be
available a speciﬁed percentage of the time per day or month. Finally,
run time is the time it takes to complete the process by adding the response times of each service in the composition.

When assessing a group of web service workﬂows, the resulting
assessment must be a result of the aggregated SLA terms of the underlying services. In some cases, aggregating the SLA terms is as
straightforward as adding the measures of each of the dependent services, but in other cases the aggregate measure must be created based
on consumer requirements. Since there may be subjectivity with respect to how the aforementioned SLA measures can be aggregated
and calculated, we introduce the following formal concepts for composing the SLA measures in the context of web service composition.
4.1. Deﬁning the physical entities for assessing SLAs

3.2. Sustainability
Sustainability is the ability to maintain the underlying services in a
timely fashion. Success rate, Negotiation/renegotiation, and problem resolution are related to ensuring the process continues to execute effectively. Success rate is deﬁned by the historical rate at which a provider
successfully completes a request. A consumer will require assurance
that a particular business is capable of agreeing on contract terms (i.e.
negotiation/renegotiation) in a timely manner. Moreover, the result of
a negotiation scheme might be the development of new agreement. In
addition, the service providers must be capable of resolving highimpact problems (perhaps identiﬁed by the consumer) in a timely manner. Success rate, negotiation, and problem resolution times ensure that
a consumer can meet the demands of their end users.

Deﬁnition 1. Server
Composition in a service-oriented architecture involves a consumer collaborating with a producer. The mapping from service to resources (servers and computational units) must consider system
boundaries (i.e. which servers are dedicated, which are not), the
size of the messages and transactions, and initialization/set up costs.
Here, we illustrate an over-simpliﬁed correlation of the services to
the server. The capabilities of the producers and consumers are
hosted on servers. Each server can be characterized by its performance, uptime percentage, throughput, and the pre-notiﬁcation
time, i.e., the server informing its constituents prior to any downtime
associated with server enhancements and repairs. A server, v, can be
formally deﬁned as a tuple of these SLA measures as shown below:

3.3. Resiliency



v ¼ Perf v ; Upv ; Tiv; Tov ; PreNT v

Resiliency is the principle of a service to perform at high service
levels over an extended period of time. This principle was derived
from our work with data-centric government transactional systems.
The resiliency principle specializes many of the principles that
underly the general notion of mean time between failures (MTBF)
[3,31]. Low resiliency is represented by a service that is frequently
taken off-line for maintenance. Additional, the frequency of updates
may impede the predictability of its operation. Consumers will need
adequate notice prior to maintenance downtimes. In addition, resiliency dictates a low frequency of maintenance downtime. Peer consumers may also add comments about a particular provider and
thus create a quantiﬁed reputation. The reputation rating also inﬂuences the resiliency of a service.
The model elements of our work were derived from our literature
review and from work with collaborating stakeholders acknowledge

*

ð1Þ

where Perfv is the performance of the server measured loosely in
computations/second, Upv is the uptime percentage of the server,
Tiv is the throughput of input messages measured in bytes/second,
Tov is the throughput of output messages measured in bytes/second,
and PreNTv is the pre-notiﬁcation time measured in seconds.
Deﬁnition 2. Set of servers
Let Ω be the set of servers of all producer and consumers involved
in the collaboration.
Ω ¼ fv1 ; v2 ; v3 ;…; vn g:

1 1 1

<<has criteria>>

*

<<is a>>
<<is a>>

*

<<is a>>

Fig. 1. A taxonomy of SLA measures for web services workﬂow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

where v1 , v2 , v3 , …, vn are the producer and consumer servers. Each
vi Ω (for all i = 1 to n), is a tuple of SLA measures as described above.
Deﬁnition 3. Service
A service can be deﬁned by some function (in some cases the average) of computations (i.e. processor cycles or relevant measure for
the speciﬁc domain) it requires based on the nature of the processing
domain. This computation must incorporate length of the message it
receives when realizing its capabilities. A service, s, can be deﬁned
as the following pair:
s ¼ ðComps ; MLs Þ

ð2Þ

where Comps is the number of required computations of the service
and MLs is the message length measured in bytes. The intention of
this formalization is to illuminate conceptually the connection between the services and the servers that host them.
Deﬁnition 4. Set of services

237

consumer concerns. The authors work closely with a federal government organization that leverages data-centric web services to develop the list of attributes that are most closely related to services
concerned with sharing data from distributed information stores.
Hence, the SLAs here consists of uptime of the composite service,
the allowable run time, the service response time, the subscription
cost or price, the maintenance pre-notiﬁcation time, and the renegotiation time of the agreement.
Let AWf represent the composite SLA that is calculated by composing the set of all agreements of all relevant services, i.e., the services
on the producer servers and any other services on the consumer's
server that might impact the overall system-wide assessment.
Let APS represent the set of agreements of the services on the producer's servers that are involved in the composition.
APS ¼ fAPS1 ; APS2 ; APS3 ;…; APSm g
Let ACS represent the set of agreements of the services on the consumer's servers that may impact the composite agreement AWf.
ACS ¼ fACS1 ; ACS2 ; ACS3 ;…; ACSn g

Let Γ be the set of all services involved in a composition.
Γ ¼ fs1 ; s2 ; s3 ;…; sm g
where s1 , s2 , s3 , …, sm are the services involved in the composition.
Each si Γ (for all i = 1 to m), is a pair of the number of required computations and the message length (in bytes) as described above.
Deﬁnition 5. SLA of a service
SLA of a service consists of speciﬁc web service-oriented measures, as deﬁned in the lower elements in the taxonomy in Fig. 2.
Let Asi be the SLA of a service si. Asi can be represented as a tuple
shown below:


Asi ¼ Upsi ; RTimesi ; SRespsi ; Cost si ; PreNT si ; RenegotT si; Repsi; Relsi
ð3Þ
where Upsi is the uptime of the service speciﬁed by the agreement,
RTimesi is the allowable run time, SRespsi is the allowable service response time, Costsi is the subscription cost or price of the service,
PreNTsi is the maintenance pre-notiﬁcation time, RenegotTsi is the renegotiation (expiration) time of the agreement, Repsi is the reputation
of the service and Relsi is the reliability rating of the service.
4.2. Generating aggregate SLAs across workﬂows
In this context, we deﬁne a web service workﬂow as a preestablished process (as deﬁned by an SLA) between a consumer and
provider (consisting of a group of web services). At composition
time, this web service workﬂow is not yet an active process or instance, but the speciﬁcation of all pre-established partnerships or
agreements of organizations to share web services. The SLA for a
body of many web service workﬂows is obtained by composing the
set of SLAs of all the services participating in each composition. This
process is similar to the traditional QoS-based service composition
process although QoS measures must be separated by provider and

4.2.1. Composite service uptime (UpWf)
The uptime for the composite SLA must be less than the minimum of:
• The uptime Upvi for consumer's server vi Ω and
• The agreed uptimes of the producer's services in APS = {APS1, A
…, A PSm}
The relationship can be shown as:
UpWf bminðUpvi ; mini¼1tom ðUpPSi ÞÞ

4.2.2. Composite run time (RTimeWf)
The time required for an individual service to complete its execution
can be considered the task time. The repeatability of the task time while
the service is in commission translates to the run time. For a web service,
the run time is a function of the internet connection, the internal network connection, the hosting hardware, and the software service. Run
time is deﬁned by the provider or consumer which is captured within
the SLA. The combined run time is illustrated in Fig. 2.
The run time for the composite SLA must be less than the minimum of:
• The producer server throughput divided by the message lengths of
service input and output
• The consumer server throughput divided by the message lengths of
service input and output
Let RTimeC represent the run time of the consumer. Including the
sum of run times already guaranteed to others is important to avoid
the impact of a voluminous load from external operations. The run
time of the consumer server is obtained by dividing the server
throughput TC by the message length of the consumer services MLCS.
RTimeC ¼ T C =MLCS

RTimeP ¼ ∑i¼1tom RTimePSi −∑i¼1ton RTimeCSi
Network

System

ð5Þ

ð6Þ

Let RTimeP represent the run time of all the stakeholders, i.e., the
producers, p. It is the difference of the run times of the sum, m, of all services on the provider-side and the sum, n, of all client-side services.

SLA Task Time (RunTime)

Internet

PS2,

ð7Þ

Service

The run time for the composite SLA is represented as:
Fig. 2. Task time and run time.

RTimeWf bmin ðRTimeP ; RTimeC Þ

ð8Þ

238

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

4.2.3. Composite service cost (CostWf)
The cost of using a web service can be aggregated to understand the
price of an entire process. The cost of an individual service is the sum of:
• The bandwidth used (i.e. the product of the usage frequency and
the message length) multiplied by the cost per byte/sec of bandwidth, which is a predeﬁned constant BC. BC could be zero if bandwidth costs are negligible to the service provider.
• The computations used multiplied by the cost of computation/sec,
which is some predeﬁned constant CC. As above, CC could be zero
if computation costs are negligible to the provider.
• The sum of the costs of all dependent services.
This results in the following equation:

 

Cost Wf ≥ RTimeWf  MLCS  BC þ RTimeWf  C S  CC
ð9Þ

þ ∑i¼1tom Cost PSi

• The reputation for the consumer server vi
• The average reputation for all the dependent services
This relationship can be illustrated as:
RepWf ¼ averageðRepvi þ ð∑i¼1tom RepPsi Þ=mÞ

ð12Þ

4.2.7. Composite service reliability (RelWf)
Reliability in traditional QoS-based web service composition has
been calculated using several approaches in related work [19]. Unlike
other approaches, this approach deals with SLA speciﬁcations which
tend to be the worst case agreement between consumer and provider. As such, we believe closest estimate is to aggregate the values by
calculating the minimum of:
• The reliability for the consumer server vi
• The minimum of reliabilities for all the dependent services
This relationship can be illustrated as:

4.2.4. Composite service pre-notiﬁcation time (PreNTWf)
When a service must be disabled for maintenance, organizations
must inform their collaborators. The minimum time for notiﬁcation
before maintenance begins is calculated as the minimum of:
• The time of notiﬁcation for the consumer server vi
• The minimum of all notiﬁcation times agreed upon for all dependent services

Deﬁnition 6. Composite SLA
The composite SLA, AWf is a tuple of all the aggregated SLA measures as shown:

ð10Þ

4.2.5. Composite service renegotiation time (RenegotTWf)
Negotiation/renegotional time is the guarantee giving by the provider and consumer that deﬁnes how quickly a request for new QoS
attributes will be acknowledged the other party. The renegotiation
date for this agreement must be after the predeﬁned constant waiting
time, WT, with respect to each other's agreement to which this server
is a party. This notion of renegotiation time is illustrated in Fig. 3. The
equation below must hold for each agreement ACSi ACS to which the
service provider is a party. This test can be shown as:
f or all i ¼ 1 to n; Renegot T Wf ¼ maxðRenegotT CSi þ WT Þ

ð13Þ



AWf ¼ UpWf ; RTimeWf ; SRespWf ; Cost Wf ; PreNT Wf ; Renegot T Wf ; RepWf ; RelWf

This relationship can be illustrated as:
PreNT Wf ≤minðPreNT vi ; mini¼1tom ðPreNT PSi ÞÞ

RelWf ≤minðRelvi ; mini¼1tom ðRelPSi ÞÞ

ð11Þ

Problem resolution is not formally deﬁned because the calculation
is a similar formula as renegotiation. As a deﬁnition (shown in Fig. 4),
problem resolution can be deﬁned as the sum of the time for recognizing the error, the time that it takes for a provider to acknowledge
the error, and the actual time for resolving the problem.
4.2.6. Composite service reputation (RepWf)
In a service composition scenario, reputation is deﬁned as a numeric score on a relative scale for a web service as captured by consumers and providers in a web service repository. The Reputation
for the composite service is calculated as the average of:

where the aggregated SLA measures UpWf, RTimeWf, SRespWf, CostWf,
PreNTWf, RenegTWf , RepWf , RelWf are obtained from the relations
shown in Eqs. (5), (8), (9), (10), (11), (12), and (13) respectively.
5. Representing SLA attributes as WS-Agreements (WSAG)
In order to store and manage SLAs, the attributes must be represented in a format conducive for distributed data management. XML
is a language that allows complex information to be represented
with embedded metadata. An XML-based approach to representing
SLA information facilitates quick interpretation of data and using
translation techniques, such as the eXtensible Stylesheet Language,
XSL, allows the comparison and aggregation of the underlying information. WSAG is an XML-based language that is deﬁned with SLA attributes. A brief background is discussed here, but more information
can be found at [21]. In a WSAG document, the data are represented
hierarchically underneath the notion of an agreement. An agreement
can be further speciﬁed with name or identifying string. An agreement can also be described by its context. Context information includes the name of the consumer and producer, the timeframe by
which the agreement is valid, and other related template information.
Each agreement encapsulates a list of terms. Terms describe the information of the services that are included.
Of most importance to this work are the guarantee terms. Guarantee terms consist of a service scope which contains the service names
of the speciﬁc service relevant to the guarantee. The service level objective contains a predicate for the metrics that quantitatively deﬁne

SLA Renegotiation Time
SLA Problem Resolution Time

Request

Negotiation
(Wait Time)

Fig. 3. Aspects of renegotiation time.

Error
Acknowledge
Recognition
ment

Resolution Time by
Service

Fig. 4. Aspects of problem resolution.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

the guarantee. The service level objective contains the parameter
name, value, and unit of measure. As an example, the SLA attributes
of uptime and maintenance are shown in Table 2. Uptime is a common attribute for SLAs. As described earlier, an uptime SLA guarantees the availability of a service by percentage over a designated
period of time. The other SLA metric, maintenance notiﬁcation time,
is not as universally used as uptime. The example in Table 2 shows
that maintenance notiﬁcation time is also straightforward with regard to representation in the WS-Agreement notations.
Capturing service level agreements in XML-based notations allows
the SLA attributes to be represented in a format similar to the WSDL
ﬁles that represent the operational speciﬁcations of the service.
WSAG can be transported and negotiated along with WSDL ﬁles.
This represents a beneﬁt if a service stakeholder wants to evaluate
multiple services, side-by-side. Another beneﬁt of capturing SLA attributes in XML-based ﬁles is the ability of enhancing attributes with semantics. It is possible that organizations will name attributes with
their own specialized naming schemes. Semantics would allow disparate organizations to mediate SLA attributes that are the same but
may be named differently. This approach leverages the numerous
projects that use semantics to highlight web services for mediation.
However, a detriment of capturing these attributes in XML-based notations occurs when organizations have pre-established agreements. It is
likely that, in a service-oriented computing environment, coalitions of
businesses will form similar to partnerships that occur in traditional businesses. In such cases, the overhead of descriptive tags may be unnecessary. In addition, SLA constraints may become more comprehensive as
the partnerships enhance their coordination and negotiation. These ﬁles
may become too cumbersome for semi-automated manipulation where
human inspection may be a required step in overall process.
6. Creating aggregated SLAs on-demand
The formalization in previous sections shows the necessary measures for creating SLAs for new business workﬂows that reﬂect the effects of other workﬂows existing at the same organization. The
authors collaborated with The MITRE Corporation for the development
of a framework for defense information systems organizations shown in
Table 2
Sample SLA attributes shown in WS-Agreement representations.
WS-Agreement for uptime
b wsag:GuaranteeTerm wsag:Name = "uptimePref" wsag:Obligated = "ACME">
bwsag:ServiceScope >
b wsag:ServiceName > AcmeService1b/wsag:ServiceName >
b/wsag:ServiceScope >
bwsag:ServiceLevelObjective >
b wsag:predicate type = "greater">
bwsag:parameter>job:uptimePercentageb/wsag:parameter >
bwsag:value>5b/wsag:value >
bwsag:unit>time:secondsb/wsag:unit >
b/wsag:predicate >
b/wsag:ServiceLevelObjective >
b/wsag:GuaranteeTerm >
WS-Agreement for Maintenance
b wsag:GuaranteeTerm
wsag:Name="maintenanceNotiﬁcationPref"wsag:
Obligated="ACME">
bwsag:ServiceScope >
b wsag:ServiceName>AcmeService1b/wsag:ServiceName >
b/wsag:ServiceScope >
bwsag:ServiceLevelObjective >
b wsag:predicate type="greater">
bwsag:parameter>job:maintPreNotiﬁcationb/wsag:parameter >
bwsag:value>7b/wsag:value >
bwsag:unit>time:daysb/wsag:unit >
b/wsag:predicate >
b/wsag:ServiceLevelObjective >
b/wsag:GuaranteeTerm >

239

Fig. 5. These organizations host large numbers of web services that provide battleﬁeld information such as weather information, force location
and tracking information, and satellite telemetry. Each of the various
defense forces (Army, Navy, Air Force, etc.) provides access their web
services via this shared portal. The sources vary in their guaranteed service level objectives. The defense information systems organizations are
devising portals that manage SLAs in governance database while recording historical service level information. The approaches devise in
this paper are incorporated within the portal logic. As such, when producers provide new services and when clients gain access to existing
services, SLAs are veriﬁed for validity.
In such an ad-hoc environment, it is important to understand if
the SLA composition process can be performed efﬁciently enough to
support the real-time insertion of new partners looking to exploit
existing services. In this work, we deﬁne an integrated process for
workﬂow-based SLA composition when suggesting new SLAs. This
process can be decomposed into two steps, SLA composition and evaluation. These two steps are illustrated in Fig. 6.
The SLA composition step is similar to traditional QoS-based web service composition approaches. In the evaluation step, user preferences are
used to prioritize the list of candidate service chains. Numerous dynamic
programming techniques can be used to achieve this step. We present a
unique approach where instead of acquiring speciﬁc QoS value expectation from the user, instead user's priorities are collected. A quality score
is generated based on the user's preferences. Our evaluation shows that,
in real-time operations, the composition and evaluation steps are feasible
considering a large number of existing business processes.
6.1. Composition: building candidate workﬂows
In order for SLA measures to be useful for both real-time operations and decision support, the web services workﬂow generation
must integrate traditional web service composition with the SLA
composition procedures. The SLA composition procedure must be integrated at each step and also applied to the workﬂow as a whole
once the full process is generated. We deﬁned the information provided by the user to be the user.predicate and the desired outcome
to be the user.reqresults. The step.predicate is the set of information
used to select subsequent services in the composition. At the initiation of a composition routine, the user.predicate is equivalent to the
step.predicate. These relationships are illustrated in Fig. 7.
We introduce an integrated procedure that combines standard web
service composition with SLA composition. Table 3 shows the pseudocode of the integrated process. The composition process has a main integrated process, IntegratedComp(). The StepCompose() process occurs
at each step and WorkﬂowChk() process occurs once the required information and actions are realized with the execution of the sequence of
web services. The ComposeSLA() process is the computation mechanisms that implement the SLA aggregation procedures.
6.2. Evaluation: prioritizing SLA compositions
In the prior section, candidate service chains are generated that
meet the functional and SLA requirements of the user. Nevertheless,
at this point, there are still multiple chains that can fulﬁll a capability.
As such, there remains an open requirement to sort the chains based
on quality and choose the best chain in the group.
In order to prioritize service chains, users are asked to provide a priority, Pr, for each attribute with respect to their environment, where
Rr = {Pr1, Pr2, Pr3,…, Prn}. The priorities represent the rank ordered
SLA measures from greatest to least importance. The priorities relate
to the corresponding set of SLA measures, where SLA = {SLA1, SLA2,
SLA3,…, SLAn}. A SLA attribute has the best possible value, B. Our approach will strongly consider chains that perform favorably with respect to the user's preferences. After evaluation, GS and GW represent
the quality score for the service and workﬂow, respectively.

240

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

Certification
Module

WS
Interface

Governance
Governance
DB

Concerns ....
Module
WS interface

Web
Service
DB

WS
Interface

WS interface

QoS-Repos/Gov
Manager

WS-Repos
Manager

Historical
QoS
DB
WS interface

QoS
Manager

Portal User Interface

Introduce new web
service that
require new SLA
generation in
Governance DB

Service Provider Service Consumer

Many different
providers/clients
on the Portal

Require regular
access to existing
service that affects
existing SLAs

Fig. 5. A practical operational scenario.

As such, the quality score for an individual service can be deﬁned
by weighing the user's priority with respect to the ratio of the SLA
measure to the best possible measure for that attribute. The calculation is deﬁned as:

n 
X
SLAn −BðSLAn Þ
GS ¼
ð14Þ
Prn •
BðSLAn Þ
n¼0
The corresponding quality score for the service chain is:
GW ¼

X

ðGS Þm



ð15Þ

m¼0

This approach relies on user-supplied priority weights and perhaps lacks the precision of standard multiple criteria decision analysis

[12] which focuses on decision optimization. However, in the next
section, we demonstrate that this approach performs favorably as a
real-time assessment during the dynamic composition process.
7. Performance evaluation of the integrated composition process
In real-time operations, SLA composition will be required to occur
within a reasonable response time. In this work, we evaluate
expected response time for the composition and prioritization of
SLA measures. Our experiments are performed on a Mobile Intel Pentium 4, 2.4 GHz, 1 GB RAM, running Windows XP and the Java Runtime Environment (JRE) 6 Update 1. An initial experiment was
performed that evaluates the response when prioritizing SLA-

SLA COMPOSITION

Assess existing and new workflow chains

EVALUATION

2. Prioritize predicted SLAs based on
existing services with respect to user
preferences
Chain1.effectiveness=
WSa WSb + WSc + WSd

Chain 1

Chain2.effectiveness=
WSa WSb + WSc + WSd

WSc

WSa

WSb

WSd

ChainN.effectiveness=
WSa WSb + WSc + WSd

WSc

Chain 2
WSa

WSb

WSd
WSc

Chain n
WSa

WSb

WSd
WSc

Fig. 6. Two-step process for integrated SLA workﬂow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

step.predicate
user.predicate

241

WS

WS

WS

WS

user.ReqResult

WS

Fig. 7. Data ﬂow in a composition routine.

annotated workﬂows of web services. Based on a set of random
WSAG ﬁles (with uniformly distributed randomly generated SLA
terms and their values), we created software that generates web service objects. In this context, using a uniformly distributed, randomly
generated set of attributes for values is appropriate. This experimentation is a proof of concept that the SLA composition can execute fast
enough to appropriately be included into the real-time composition
scenarios. This is mostly an information management procedure.
The authors understand that a non-uniform list of attributes may
incur additional overhead. Here, the argument is made that datacentric organizations must keep their list of guarantees (as represented by attributes) uniform, such that their operations are consistent across each of their consumers. It is also not implausible that
the provider use such policy to ensure a predictable processing time.
Each service objects has a unique identiﬁcation codes that associates
it with the corresponding WSAG ﬁles. We duplicated identiﬁcation
codes (i.e. ids) such that increasing numbers of services have the
same ids. The process of aggregating ids was done in an attempt to

Table 3
Integrated composition pseudo-code.
IntegratedComp: Main integrated composition function
StepCompose: Function that occurs at each step
WorkﬂowChk : Process-Level Functional/SLA check
composeSLA: Function that aggregates SLA measures
PR,RenegotT,Cost : Problem Resolution, Renegotiation, Cost
RTime,Up,PreNT: Run time, uptime, maintenance
Rel,Rep: Reliability, reputation
step.predicate: Message information required to execute service
ws WSDL Object with SLA measures
user.reqresults: Message information required as output of service
service_chain: Candidate workﬂow of web services
IntegratedComp
{
step.predicate = user.predicate
WHILE (step.predicate ! = user.reqresults)
{ StepCompose() }
WorkﬂowChk()
}
StepCompose
{
FOR EACH candidate ws where
ws. message step. predicate
THEN ADD (ws) to List b ws >
FOR EACH candidate ws IN List b ws>
IF ((ws.PR b = step.PR) || (ws.RenegotT b = step.RenegotT) ||
(ws.Cost > step.Cost) || (ws.RTime b = step.RTime) ||
(ws.Up b = step.Up) || (ws.PreNT b = step.PreNT) ||
(ws.Rel b = step.Rel) || (ws.Rep b = step.Rep))
THEN REMOVE (ws) from List b ws>
ELSE {
ADD ws to List b service_chain >
ADD ws.outputs to List b step.predicate >
}
}
WorkﬂowChk
{
FOR EACH service_chain
IF (chain. outputs user.reqresults ) &&
(ComposeSLA (ws.PR, ws.RenegotT ,ws.Cost, ws.Up,
ws.PreNT, ws.RTime, ws.Rep, ws.Rel ) b user.SLA))
THEN ADD (service_chain) to Candidate List
}

simulate composition as a prerequisite step to the actual SLA composition. Each web service object was populated with six SLA measures. Although there are more attributes, we decided to experiment with 6
with the expectation that more attributes would scale consistently.
Our experimentation searches the repository of web service objects,
composes services of the same id, and then prioritizes the resulting
chain. This experiment was executed on a repository with varying
sizes from 100 to 100,000 services. The workﬂow size (number of services in a chain) was also varied. The number of services per chain is
varied from 10 to 100,000 services within a repository of 1,000,000 services. Fig. 8 shows the results of this ﬁrst experiment. Response time
represents the average time required for the proposed algorithm to correlate the SLA attributes to generate a composite measure for a particular web service composition routine. Considering a reasonable
workﬂow of web services of 100 interconnected services or less, a response time of 1.6 ms per service chain is promising. As a variation of
the response time experimentation, we also investigated how the size
of the repository affects the performance of our algorithm. Fig. 9
shows the response time of our algorithm as the repository increases.
The workﬂow chain of the composition request is held constant at 10
services but the repository increases from 100 to 100,000. In the results
of this experiment, search time is also considered. Given the largest repository of 100,000 services, the SLA composition time (including the
processing time for discovering the relevant services (i.e. 10 services)
is 30.2 s. Although the results are favorable in a simulated environment,
the reader should understand that many other conditions with regard
to performance variations and real-time factors on open systems reduce
the conﬁdence of these results.
In a second experiment, we evaluated the performance by varying
the number of SLA attributes that are used for calculation. In this
paper, we experiment with up to six attributes, but we expect that
other attributes will be required to extend this approach in the future.
As such, it is important to understand the overhead associated with
adding a new SLA attribute for real-time operations. Although it is understood that the performance increases as the computing hardware
is improved, we are generally interested in how the approach scales
as the number of attributes increases in a ﬁxed-size repository.
Fig. 10 shows the search and calculation time when varying the
numbers of attributes (i.e. the number of attributes tested for each).
Although this graph has a high concentration of search time (~ 90%),
it is clear that the performance degrades favorably (linearly) at less
than 12 ms per attribute (i.e. the calculation time increases less than
1 millisecond for the addition of each new attribute). Another variation of this experiment, also shown in Fig. 10, considers the impact
of SLAs that process fewer attributes. In some cases, service agreements may only consider a subset of the total possible guarantee conditions. As such, less attributes may be stored per service (i.e.
Attributes Stored). The Attributes Stored measure in Fig. 10 shows
that there is only a slight advantage for service-oriented providers
to be less stringent. Another variation considers increasing performance of SLA composition by limiting the number of attributes that
a service composition considers. The major question is “Can runtime performance increase if the business management system only
considers a subset of possible SLA attributes?”. Experimentation
shows that even at the most extreme case, if a service provider only
allows services to be constrained by one attribute, the performance
is only improved by approximately 30%. Predictably, as shown in

242

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

Performance Evaluation by
Number of Services per ID
(constant 100000 services with search time subtracted)

SLA Composition Time
(Milliseconds)

1000

100

10

1
10

100

1000

10000

0.1

Services per ID
Fig. 8. The performance of the SLA prioritization function as the number of services per workﬂow increase (service discovery time excluded).

Fig. 10, the improvement in performance decreases as more attributes
are considered.
8. Conclusion
When provider organizations expose their services for consumption by their peers, it is important for them to understand what
they are guaranteeing. Moreover, consumers that receive such commitments must abide by their own commitments such that the providers can meet their guarantees to all consumers. In this work, we
introduce a method of assessing an organization based on both pending and existing SLAs. Since the estimation of QoS measures for web
services has been investigated in great detail, in this paper, we
make a varied contribution. Our work builds on the existing studies
by considering the QoS guarantees, as captured in SLAs, such that provider and consumer concerns can be modeled independently. This
work also considers that SLA assessment occurs in a separate process
than standard QoS estimation at service composition time. Here, we
introduce high-level criteria that can be created from the aggregation
of a comprehensive list of lower-level QoS-based attributes. This variation to related work facilitates automated cross-enterprise SLA negotiation. In this paper, we deﬁne a two-step process for composing
SLAs and evaluating their efﬁciency. Consistent with results in related

work that estimate QoS on fully operational web services, we have
found through simulated experimentation that the management of
third-party SLA information can be composed efﬁciently in parallel
with the actual operational service composition. In future work, we
plan to implement our approach within a real operational setting as
opposed to the simulation setting that is represented in this paper.
In the real operational setting, data will be produce using a variety
of distributions. As such, WS-Agreement ﬁles will be appended to
WSDL ﬁles and evaluated in a network environment for performance
and feasibility. In addition, we plan to extend the SLA composition
paradigm to protocols that mandate the discovery process within
web service registries. In this way, it may be possible for adaptive
software (or intelligent agents) to negotiate SLAs, in real time, while
they perform on-demand, service discovery.
Acknowledgment
This work was beneﬁted by the participation of Dr. M. Brian Blake
in the Service Level Agreement Technical Exchange Meeting held at
The MITRE Corporation on July 2006 in McLean, Virginia. In addition,
the service discovery approach/software used in this work was partially funded by the National Science Foundation under award number 0548514.

Performance Evaluation by Repository Size
(constant 10 services per workflow chain)

SLA Composition Time
(Seconds)

100

10

1
100

1000

10000

100000

0.1

Number of Services
Fig. 9. The performance of the SLA prioritization function as the repository increases (service discovery time included).

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

243

Performance Evaluation by
Attributes Stored and Attributes Tested
(100,000 services, 10 services per chain)
SLA Composition Time
(Milliseconds)

80
70
60
50
40
30
20
10
0

1

2

3

4

5

6

Attributes
Attributes Stored

Attributes Tested

Fig. 10. The performance overhead associated with the addition of new attributes.

References
[1] P. Alipio, S. Lima, P. Carvalho, XML service level speciﬁcation and validation, Proc.
of the 10th IEEE Symposium on Computers and Communications (ISCC), June
2005, pp. 975–980.
[2] A. Andrieux, K. Czajkowski, A. Dan, K. Keahe, H. Ludwig, T. Nakata, J. Pruyne, J. Rofrano,
S. Tuecke, M. Xu, “Web Service Agreements Speciﬁcation (WS-Agreement),” Proposed
Recommendation, Open Grid Forum (OGF) Document Number GFD-R-P.107, Mar 2007
OGF Grid Resource Allocation Agreement Protocol Working Group (GRAAP-WG)Accessible at, http://www.gridforum.org/Public_Comment_Docs/Documents/Oct-2005/WSAgreementSpeciﬁcationDraft050920.pdf.
[3] J.E. Angus, On computing MTBF for a k-out-of-n: G repairable system, IEEE Transactions on Reliability 37 (3) (1988) 312–313.
[4] M.B. Blake, B2B electronic commerce: where do agents ﬁt in? Proceedings at the
AAAI-2002 Workshop on Agent Technologies for B2B E-Commerce/AAAI Press,
Edmonton, Alberta, Canada, July 2002.
[5] M.B. Blake, Decomposing composition: service-oriented software engineers, IEEE
Software 24 (6) (Nov/Dec 2007) 68–77.
[6] M.B. Blake, A lightweight software design process for web services workﬂows,
Proc. of the 4th IEEE International Conference on Web Services (ICWS), Sept.
2006, pp. 411–418.
[7] M.B. Blake, D.J. Cummings, Workﬂow composition of service level agreements,
Proc. of the IEEE International Conference on Services Computing (SCC 2007),
July 2007, pp. 138–145.
[8] M.B. Blake, M. Gini, Guest editorial: agent-based approaches to B2B electronic
commerce, International Journal of Electronic Commerce 7 (1) (2002) 113–114.
[9] G. Canfora, G. M Di Penta, R. Esposito, F. Perfetto, M.L. Villani, Service composition
(re)binding driven by application-speciﬁc QoS, Proc of International Conference
on Service-Oriented Computing (ICSOC), 2006, pp. 141–152.
[10] A.J. Cardoso, “Quality of Service and Semantic Composition of Workﬂows,” Ph.D.
Dissertation, University of Georgia, 2002.
[11] G. Di Modica, V. Regalbuto, O. Tomarchio, L. Vita, Dynamic re-negotiations of SLA
in service composition scenarios, Proc. of the 33rd EUROMICRO Conference on
Software Engineeringand Advanced Applications, August 2007, pp. 359–366.
[12] J. Dyer, Maut — multiattribute utility theory in multiple criteria decision analysis:
state of the art surveys, International Series in Operations Research, Management
Science, vol. 78, Springer, 2005, pp. 265–292.
[13] T. Falkowski, S. Vob, Application service providing as part of intelligent decision
support for supply-chain management, Proc. of the 36th Annual Hawaii International Conference on System Sciences (HICSS), vol. 3, 2003, p. 80.
[14] M. Gillman, G. Weikum, W. Wonner, Workﬂow management with service quality
guarantees, Proc. of ACM SIGMOD International Conference on Management of
Data, 2002, pp. 223–239.
[15] L. Green, Service level agreements: an ontological approach, Proc. of the 8th ACM
International Conference on Electronic Commerce (ICEC), August 2006,
pp. 185–194, Fredericton, Canada.
[16] D. Greenwood, G. Vitaglione, L. Keller, M. Calisti, Service level agreement management with adaptive coordination, Proc. of the International Conference on Networking and Services (ICNS), July 2006, p. 45–50, Silicon Valley, USA.
[17] C.-W. Hang, M.P. Singh, Trustworthy service selection and composition, ACM
Transactions on Autonomous and Adaptive Systems 6 (1) (Feb 2011).
[18] L. Jin, V. Machiraju, A. Sahai, Analysis on service level agreement of web services,
HP Technical Report, HPL-2002-180, June 2002, accessible at (2008), http://www.
hpl.hp.com/techreports/2002/HPL-2002-180.pdf.
[19] J. Ko, C.O. Kim, I. Kwon, Quality-of-service oriented web service composition algorithm and planning architecture, Journal of Systems and Software 81 (11) (November 2008) 2079–2090.
[20] H. Ludwig, A. Keller, A. Dan, R.P. King, R. Franck, Web Service Level Agreement (WSLA)
Language SpeciﬁcationAccessible at, http://www.research.ibm.com/wsla2007.

[21] M. Mecella, M. Scannapieco, A. Virgillito, R. Baldoni, T. Catarci, C. Batini, Managing
data quality in cooperative information systems, Lecture Notes in Comptuer Science 2512 (2002) 486–502.
[22] M. Mohabey, Y. Narahari, S. Mallick, P. Suresh, S.V. Subrahmanya, An intelligent
procurement marketplace for web services composition, Proc. of the IEEE/WIC/ACM International Conference on Web Intelligence, Nov. 2007, pp. 551–554.
[23] N.J. Muller, Managing service level agreements, International Journal of Network
Management 9 (3) (May 1999).
[24] F. Naurmann, U. Leser, J.C. Freytag, Quality-driven integration of heterogeneous
information systems, Proc. of the 25th International Conference on Very Large
Databases, September 1999, pp. 447–458, Edinburgh, Scotland, UK.
[25] N. Oldham, K. Verma, A.P. Sheth, F. Hakimpour, Semantic WS-Agreement partner
selection, Proc. of the 15th International World Wide Web Conference (WWW),
2006.
[26] F.R. Reiss, T. Kanungo, Satisfying database service level agreements while minimizing cost through storage QoS, Proc. of the IEEE International Conference on
Services Computing (SCC), July 2005, pp. 13–21.
[27] A. Sahai, V. Machiraju, M. Sayal, A. Moorsel, F. Casati, Automated SLA monitoring
for web services, Proc. of the IEEE/IFIP International Workshop on Distributed
Systems: Operation and Management (DSOM), October 2002, pp. 28–41, Montreal,
Canada.
[28] D. Skene, Lamanna, W. Emmerich, Precise service level agreements, Proc. of the 26th
International Conference on Software Engineering (ICSE), 2004, pp. 179–188,
Edinburgh, UK.
[29] I. Sorteberg, O. Kure, The use of service level agreements in tactical military coalition
force networks, IEEE Communications Magazine 43 (11) (November 2005) 107–114.
[30] W. Sun, Y. Xu, F. Liu, The role of XML in service level agreements management,
Proc. of the International Conference on Services Systems and Services Management, June 2005, pp. 1118–1120.
[31] K.M. van Hee, L.J. Somers, M. Voorhoeve, A modeling environment for decision
support systems, Decision Support Systems 7 (1) (1991) 241–251.
[32] G. Xiaohui, K. Nahrstedt, A scalable QoS-aware service aggregation model for
peer-to-peer computing grids, Proc. of the 11th IEEE International Symposium
on Higher Performance Distributed Computing (HPDC), 2002, pp. 73–82.
[33] J. Yan, R. Kowalczyk, J. Lin, M.B. Chhetri, S.K. Goh, J. Zhang, Autonomous service
level agreement negotiation for service composition provision, Future Generation
Computer Systems 23 (6) (July 2007) 748–759.
[34] T. Yu, K.J. Lin, Service selection algorithms for composing complex services with
end-to-end QoS constraints. Proc. 3rd International Conference on Service Oriented
Computing (ICSOC2005), The Netherlands Amsterdam, 2005.
[35] L. Zeng, B. Benatallah, A.H.H. Ngu, M. Dumas, J. Kalagnanam, H. Chang, QoS-aware
middleware for web services composition, IEEE Transactions on Software Engineering 30 (5) (May 2004) 311–327.
[36] Y. Zhang, K.J. Lin, J.Y.J. Hsu, Accountability monitoring and reasoning in service oriented architectures, Service-Oriented Computing and Applications 1 (2007) 35–50.
[37] M. zur Muehlen, J. Nickerson, K.D. Swenson, Developing web services choreography
standards — the case of REST vs. SOAP, Decision Support Systems 40 (1) (2005).

M. Brian Blake received the BS degree in electrical engineering from the Georgia Institute of Technology, Atlanta and the PhD degree in information technology with a concentration in information and software engineering from George Mason University,
Fairfax, Virginia. He is currently Professor of Computer Science and Associate Dean of
Engineering at the University of Notre Dame, Indiana. As a professor, he has published
more than 120 journal and refereed conference papers in the domains of workﬂow and
agent-based systems, service-oriented computing, distributed data management, and
software engineering education. His investigations cover the spectrum of software engineering: design, speciﬁcation, proof of correctness, implementation/experimentation,
performance evaluation, and application.

244

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

Ajay Bansal received his B. Tech. degree in Computer Science from NIT (previously
known as REC), Warangal, India, M.S. in Computer Science from Texas Tech Univ.,
Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas. He is
currently a lecturer in the College of Technology and Innovation at Arizona State
University. He has over 3 years of industry experience. His research interests include
Programming Languages, Logic Programming, Declarative Programming, Automated
Reasoning, Service-Oriented Computing, Semantic Web Services, and Language-based
Security.

Srividya Kona Bansal received her B. Tech. degree in Computer Science from NIT (previously known as REC), Warangal, India, M.S. in Computer Science from Texas Tech
Univ., Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas.
She is currently an assistant professor in the College of Technology and Innovation at
Arizona State University. She has over 5 years of industry experience. Her research interests include Service-Oriented Computing, Semantic Web, Software Engineering, Engineering Education, and Bioinformatics.

2008 IEEE International Conference on Web Services

Generalized Semantics-based Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Abstract

cation possible. Informally, a service is characterized by its
input parameters, the outputs it produces, and the actions
that it initiates. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order
to make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize, and
compose services automatically. To make services ubiquitously available we need a semantics-based approach such
that applications can reason about a service’s capability to a
level of detail that permits their discovery, composition, deployment, and synthesis [6]. Several efforts are underway
to build such an infrastructure [15, 17, 20].

Service-oriented computing (SOC) has emerged as
the eminent market environment for sharing and reusing
service-centric capabilities. The underpinning for an organization’s use of SOC techniques is the ability to discover
and compose Web services. Although industry approaches
to composition have a strong notion of business processes,
these approaches largely use syntactic descriptions. As
such composition is limited since the true functionality of
ambiguous service operations cannot be inferred. Alternatively, academia uses semantic approaches to disambiguate
services, but, at the same time, most of these approaches neglect the process rigor needed for complex compositions. In
this paper we present a generalized semantics-based technique for automatic service composition that combines the
rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the
common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG)
where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented. Furthermore, the composition can be represented
semantically as OWL-S documents. Our contributions are
applied for automatic workflow generation in context of the
currently important bioinformatics domain.

With regards to service composition, a composite service
is a collection of services combined together in some way
to achieve a desired effect. Traditionally, the task of automatic service composition has been split into four phases:
(i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [26]. Most efforts reported in the literature focus on
one or more of these four phases. The first phase involves
generating a plan, i.e., all the services and the order in which
they are to be composed in order to obtain the composition.
The plan may be generated manually, semi-automatically,
or automatically. The second phase involves discovering
services as per the plan. Depending on the approach, often
planning and discovery are combined into one step. After all the appropriate services are discovered, the selection
phase involves selecting the optimal solution from the available potential solutions based on non-functional properties
like QoS properties. The last phase involves executing the
services as per the plan and in case any of them are not
available, an alternate solution has to be used.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered, and consumed [1]. A Web service is an autonomous,
platform-independent program accessible over the web that
may affect some action or change in the world. Sample of
Web services include common plane, hotel, rental car reservation services or device controls like sensors or satellites.
As automation increases, these services will be accessed directly by applications rather than by humans [11]. In this
context, a Web service can be regarded as a “programmatic
interface” that makes application to application communi-

978-0-7695-3310-0/08 $25.00 © 2008 IEEE
DOI 10.1109/ICWS.2008.118

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

In this paper we present a general approach for automatic
service composition. Our composition algorithm performs
planning, discovery, and selection automatically, all at once,
in one single process. This is in contrast to most methods
in the literature where one of the phases (most frequently
planning) is performed manually. Additionally, our method
generates most general compositions based on (conditional)
directed acyclic graphs (DAG). Note that service discovery

219

is a special case of composition of n services, i.e., when
n=1. Thus, we mainly study the general problem of automatically composing n services to satisfy the demand for
a particular service, posed as a query by the user. In our
framework, the DAG representation of the composite service is reified as an OWL-S description. This description
document can be registered in a repository and is thus available for future searches. The composite service can now
be discovered as a direct match instead of having to look
through the entire repository and build the composition solution again. We show how service composition can be applied to a Bioinformatics analysis application, for automatic
workflow generation in the field of Phylogenetics [4].
Our research makes the following novel contributions:
(i) We formalize the generalized composition problem
based on our conditional directed acyclic graph representation; (ii) we present an efficient and scalable algorithm
for solving the composition problem that takes semantics of
services into account; our algorithm automatically discovers and selects the individual services involved in composition for a given query, without the need for manual intervention; (iii) we automatically generate OWL-S descriptions of
the new composite service obtained; and, (iv) we apply our
generalized composition engine to automatically generate
workflows in the field of Bioinformatics.
The rest of the paper is organized as follows. In Section 2
we present the related work in the area of service composition and discuss their limitations. In Section 3, we formalize the generalized service composition problem followed
by a discussion of our technique for automatic Web service
composition and automatic generation of OWL-S service
descriptions in Section 4. Section 5 presents an application of our generalized composition engine to automatically
generate workflows for Bioinformatics analysis tasks. The
last section presents the conclusions and future work.

knowledge, most of these approaches that use planning are
restricted to sequential compositions, rather than a directed
acyclic graph. In this paper we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential but also nonsequential that can be represented in the form of a directed
acyclic graph. The authors in [18] present a composition
technique by applying logical inferencing on pre-defined
plan templates. Given a goal description, they use the logic
programming language Golog to instantiate the appropriate
plan for composing Web services. This approach also relies
on a user-defined plan template which is created manually.
One of the main objective of our work is to come up with
a technique that can automatically produce the composition
without the need for any manual intervention.
There are industry solutions based on WSDL and
BPEL4WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service
by composing a set of existing ones. It does not assemble
complex flows of atomic services based on a search process. They select appropriate services using a planner when
an explicit flow is provided. In contrast, our technique automatically determines these complex flows using semantic
descriptions of atomic services.
A process-level composition solution based on OWL-S
is proposed in [19]. In this work the authors assume that
they already have the appropriate individual services involved in the composition, i.e., they are not automatically
discovered. They use the descriptions of these individual
services to produce a process-level description of the composite service. They do not automatically discover/select
the services involved in the composition, but instead assume that they already have the list of atomic services. In
contrast, we present a technique that automatically finds the
services that are suitable for composition based on the query
requirements for the new composed service.
There are solutions such as [23] that solve the selection
phase of composition. This work uses pre-defined plans
and discovered services provided in a matrix representation.
Then the best composition plans are selected and ranked
based on QoS parameters like cost, time, and reputation.
These criterion are measured using fuzzy numbers.
There has been a lot of work on composition languages
such as WS-BPEL, FuseJ, AO4BPEL, etc. which are useful
only during the execution phase. FuseJ is a description language for unifying aspects and components [22]. Though
this language was not designed for Web services, the authors contend that it can be used for service composition as
well. It uses connectors to interconnect services. We believe
that there is no centralized process description, but instead
information about services is spread across the connectors.
With FuseJ, the planning phase has to be performed manually, that is the connectors have to be written by the de-

2. Related Work
Composition of Web services has been active area of
research recently [25, 24, 26]. Most of these approaches
present techniques to solve one or more phases listed in
Section 1. There are many approaches [18, 19, 20] that
solve the first two phases of composition namely planning
and discovery. These are based on capturing the formal semantics of the service using action description languages
or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and
the overall service desired is represented by the goal to be
achieved using some combination of atomic actions. A
planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available. To the best of our

220

I is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the list of
post-conditions. The pre- and post-conditions are ground
logical predicates.

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Definition (Query): The query service is defined as Q
= (CI  , I  , A , AO , O , CO ) where CI  is the list of preconditions, I  is the input list, A is the service affect, AO
is the affected object, O is the output list, and CO is the
list of post-conditions. These are all the parameters of the
requested service.

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

veloper. Similarly, OWL-S also describes a composite service but does not automatically find the services involved
in the composition. So these languages are only useful for
execution which happens after the planning, discovery, and
selection of services is done. Service grounding of OWL-S
maps the described abstract service to the concrete WSDL
specification which helps in executing the service. In contrast, our approach automatically generates the composite
service. This new composite service generated can then be
described using one of these languages.
In this paper we present a technique for automatically
planning, discovering, and selecting services that are suitable for obtaining a composite service, based on the user
query requirements. As far as we know, all the related approaches to this problem assume that they either already
have information about the services involved or use human
input on what services would be suitable for composition.

Definition (Generalized Composition): The generalized
Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI  , I  , A , AO , O , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming
edge of a node represents the inputs and pre-conditions of
the service. The following conditions should hold on the
nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming edge
that 
represents the query inputs and pre-conditions,
I   i I i , CI  ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing edge
that represents
the query outputs and post-conditions,

O  i Oi , CO ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has at
least one incoming edge, let Si1 , Si2 , ..., Sim be the
nodes such that there is a directed
 edge from each
of these nodes to Si . Then Ii  k Oik ∪ I  , CI i ⇐
(COi1 ∧COi2 ... ∧ COim ∧ CI  ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming
edge, let Sj be its immediate predecessor node such
that there is a directed edge from Sj to Si . Then the
inputs and pre-conditions at node Si are Ii = Oj ∪ I  ,
CI i = COj . The outgoing edges from Si represent
the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time.
The meaning of the  is the subsumption (subsumes) relation and ⇒ is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage

3. Web service Composition
Informally, the Web service Composition problem can
be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested
service, in case a matching service is not found, the composition problem involves automatically finding a directed
acyclic graph of services that can be composed to obtain
the desired service. Figure 1 shows an example composite
service made up of five services S1 to S5 . In the figure, I 
and CI  are the query input parameters and pre-conditions
respectively. O and CO are the query output parameters
and post-conditions respectively. Informally, the directed
arc between nodes Si and Sj indicates that outputs of Si
constitute (some of) the inputs of Sj . We next formalize
the generalized composition problem. In this generalization, we extend our previous notion of composition [10] to
handle non-sequential conditional composition (which we
believe is the most general case of composition).
Definition (Repository of Services): Repository (R) is a
set of Web services.
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions,

221

in composition should imply the pre-conditions of services
in the next stage. When it cannot be determined at compile
time whether the post-conditions imply the pre-conditions
or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible
conditions which will be evaluated at run-time. Depending
on the condition that holds, the corresponding services are
executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply
the preconditions CI 2 of S2 . The following conditions are
evaluated at run-time:
if (CO1 ⇒ CI 2 ) then execute S1 ;
else if (CO1 ⇒ ¬ CI 2 ) then no-op;
else if (CI 2 ) then execute S1 ;
With the above formalism in place, we next present our
generalized composition algorithm.

can see that every composite service generated by our algorithm is indeed a service that meets all the query requirements and hence is sound. Also our algorithm generates
every composite service possible for the given query and
hence is complete. Intuitively, we can see from the algorithm that these theorems hold and can be shown via proofs
by contradiction. These soundness and completeness proofs
are not included here due to lack of space.

Algorithm: GenerateServiceDescription
(Input: G - Solution Graph)
1. Generate generic header constructs
2. Start Composite Service element
3. Start SequenceConstruct
4.
If Number(SourceVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each starting/source Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct
End If
5.
If Number(SinkVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each ending/sink Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct
End If
6.
For Each remaining vertex V in G
If V is non-conditional vertex
with one outgoing edge
GenerateAtomicService
If V is non-conditional vertex
with more than one outgoing edge
GenerateSplitJoinConstruct
If V is Conditional vertex
with one outgoing edge
GenerateAtomicService
If V is Conditional vertex with
more than one outgoing edge
GenerateConditionalConstruct
End For
7. End SequenceConstruct
8. End Composite Service element
9. Generate generic footer constructs

4. Automatic Generation of Composite Services
In this section we present our algorithm for Automatic
Composition that produces a general directed acyclic graph.
The composition solution produced is for the generalized
composition problem presented in Section 3. We also
present our algorithm for automatic generation of OWL-S
descriptions for the new composite service produced.

4.1. Generalized Composition Algorithm
In order to produce the composite service which is the
graph, as shown in the example Figure 1, we filter out services that are not useful for the composition at multiple
stages. Figure 2 shows the filtering technique for the particular instance shown in Figure 1. The composition routine starts with the query input parameters. It finds all those
services from the repository which require a subset of the
query input parameters. In Figure 2, CI, I are the preconditions and the input parameters provided by the query.
S1 and S2 are the services found after step 1. O1 is the
union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous stage, i.e., I2 = O1 ∪ I. I2 is used to find services at
the next stage, i.e., all those services that require a subset
of I2 . In order to make sure we do not end up in cycles,
we get only those services which require at least one parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.
The Web service Composition algorithm presented here
has been proven to be sound and complete. Intuitively, we

Table 1. Automatic Generation of OWL-S description of Composite Service

4.2. Automatic Generation of OWL-S Description
After we have obtained a composition solution (sequential, non-sequential, or conditional), the next step is to produce a semantic description document for this new composite service. Then this document can be used for execution of the service and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of performing the composition process
all over again. We used the existing language OWL-S [7]

222

I=I
CI, I

1

S1
S2
.
.

O1

I=IUO
2

1

1

O

S

2

I=IUO
3

3

2

2

O

3

S

I=IUO
4

4

.
.

.
.

3

3

O

S

4

O

5

.
.

Figure 2. Generalized Composition Technique
to describe composite services. OWL-S models services as
processes and when used to describe composite services, it
maintains the state throughout the process. It provides control constructs such as Sequence, Split-Join, If-Then-Else
and many more to describe composite services. These control constructs can be used to describe the kind of composition. OWL-S also provides a property called composedBy
using which the services involved in the composition can be
specified. Table 1 presents the algorithm for automatic generation of the OWL-S description of the composite service.
A sequential composition can be described using the Sequence construct which indicates that all the services inside
this construct have to invoked one after the other in the same
order. The non-sequential composition can be described in
OWL-S using the Split-Join construct which indicates that
all the services inside this construct can be invoked concurrently. The process completes execution only when all
the services in this construct have completed their execution. The non-sequential conditional composition can be
described in OWL-S using the If-Then-Else construct which
specifies the condition and the services that should be executed if the condition holds and also specifies what happens
when the condition does not hold. Conditions in OWL-S
are described using SWRL.
There are other constructs such as looping constructs in
OWL-S which can be used to describe composite services
with complex looping process flows. We are currently investigating other kinds of compositions with iterations and
repeat-until loops and their OWL-S document generation.
We are exploring the possibility of unfolding a loop into a
linear chain of services that are repeatedly executed.

composition engine consists of modules: (i) Tuple Generator; (ii) Query Reader; (iii) SemanticRelations Generator;
(iv) Composition Query Processor; (v) OWL-S Description
Generator;
TupleGenerator converts each service in the repository
into the tuple format. The SemanticRelationsGenerator
module extracts all the semantic relations and creates a list
of Prolog facts. The CompositionQueryProcessor module
uses the repository of facts, which contains all the services,
their input and output parameters and the semantic relations
between the parameters. The output of the query processor
is the composition solution which is directed acyclic graph
of all the services involved in the composition. Our algorithm selects the optimal solution with least composition
length (i.e., the number of stages involved in the composition). The next step is to produce a description of the
new composite service solution found. OWL-S DescriptionGenerator automatically generates the OWL-S description
of the composite service using constructs depending on the
type of composition.

5. Application to Bioinformatics
We illustrate the practicality of our general framework
for automatically composing services by applying it to phylogenetics, a subfield of bioinformatics, for automatic generation of workflows. In this section, we present a brief
description of the field of Phylogenetics [4] followed by
an example of a workflow generation problem that can be
mapped to a non-sequential conditional composition problem (the most general case of the composition problem) and
can be solved using our generalized composition engine.

4.3. Implementation

Phylogenetics

We implemented a prototype composition engine using
Prolog with Constraint Logic Programming over finite domain [16], referred to as CLP(FD) hereafter. In our current
implementation, we used semantic descriptions of web services written in the language called USDL [9]. The repository of services contains one description document for each
service. USDL itself is used to specify the requirements of
the service that an application developer is seeking. The

Phylogenetic inferencing involves an attempt to estimate
the evolutionary history of a collection of organisms (taxa)
or a family of genes [5]. The two major components of this
task are the estimation of the evolutionary tree (branching
order), then using the estimated trees (phylogenies) as analytical framework for further evolutionary study and finally
performing the traditional role of systematics and classification. Using this study a number of interesting facts can

223

MIPSBlastBetterE13
Query
Inputs

CreateMobyData

MOBYSHoundGetGen
BankWhateverSequence

ExtractBestHit
Query
Outputs
ExtractAccession

Figure 3. Example of Non-Sequential Composition as a Directed Acyclic Graph
Service
Query
Create
MobyData
MOBYSHound
GetGenBank
WhateverSeq
MIPSBlast
BetterE13
Extract
Accession
Extract
BestHit

Pre-Conditions

Format(MobyData)
= NCBI

Input Parameters
GeneInput
GeneInput

Output Parameters
AccessionNumbers,
AGI, GeneIdentity
MobyData

MobyData

GeneSequence

GeneSequence

WUGeneSequence
AccessionNumbers
AGI,
GeneIdentity

GeneSequence
WUGeneSequence

Post-Conditions

Format(MobyData)
= NCBI

Table 2. Example Scenario for Non-Sequential Composition
be discovered, for example, who are the closest living relatives of humans, who are whales related to, etc. Different
studies can be conducted, for example, studying dynamics
of microbial communities, predicting evolution of influenza
viruses and other applications such as Drug Discovery, and
vaccine development, etc.

format translators and put them in the correct order to produce a workflow. We show how Web service composition
can be directly applied to automate this task of producing
a workflow. MyGrid [8] has a wealth of bioinformatics resources and data that provides opportunities for research. It
has hundreds of biological Web services and their WSDL
descriptions, provided by various research groups around
the world. We illustrate our generalized framework for Web
service composition by applying it to these services to generate workflows automatically that are practically useful for
this field.

In order to perform these tasks scientists use a number
of software tools and programs already available. They use
them by putting them together in a particular order (i.e., a
workflow) to get their desired results. That is, it involves
execution of a sequence of steps using various programs or
software tools. The software tools and programs created
for specific phylogenetic tasks use different data formats for
their input and output parameters. The data description language Nexus [12, 13] is used as a universal language for
representation of these bioinformatics related data. There
are translator programs that convert different formats into
Nexus and vice versa. For example, one could use the
BLAST program to get a sequence set of genes. Once the
sequence set is obtained, the sequences can be aligned using the CLUSTAL program. But the output from BLAST
cannot be directly fed to CLUSTAL as their data formats are
different. The translator can be used to convert the BLAST
format to Nexus and then the Nexus format to CLUSTAL.

Example 1: Workflow Generation (Non-sequential
Composition)
Suppose we are looking for a service that takes a GeneInput
and produces its corresponding AccessionNumbers, AGI,
and GeneIdentifier as output. The directory of services contains CreateMobyData, MOBYSHoundGetGenBankWhateverSequence, MIPSBlastBetterE13, ExtractAccession, and
ExtractBestHit services. In this scenario, the GeneInput
first needs to be converted to NCBI data format and then
its corresponding GeneSequence is further passed to ExtractAccession and ExtractBestHit to obtain the AccessionNumbers, AGI, and GeneIdentity respectively. Figure 3
shows this non-sequential composition example as a directed acyclic graph. In this example:

In order to perform an inferencing task, one has to manually pick all the appropriate programs and corresponding

• Service CreateMobyData has a post-condition on its

224

MEGA

Paup

PAUP
BLAST

Format(AlignedSequenceSet)
?

CLUSTAL
Query
Inputs

Query
Outputs

BlastNexus
Phylip
NexusClustal
Phylip

Figure 4. Example of Non-Sequential Conditional Composition as a Directed Acyclic Graph
output parameter MobyData that the format is NCBI
and the service MOBYSHoundGetGenBankWhateverSequence has a pre-condition that the its input parameter MobyData has to be in NCBI format for service execution. The post-condition of CreateMobyData should imply the pre-condition of the service
MOBYSHoundGetGenBankWhateverSequence.
• Both services ExtractAccession and ExtractBestHit
have to be executed to obtain the query outputs.
• The semantic descriptions of the service input/output
parameters should be the same as the query parameters
or have the subsumption relation. This can be inferred
using semantics from the ontology provided.

vice should imply the pre-conditions of the following service. The post-condition of the service CLUSTAL is that
the output parameter AlignedSequenceSet has either Paup or
Phylip format. Depending on which one of these two conditions hold, the next service for the composition is chosen.
In this case, one cannot determine if the post-conditions of
the service CLUSTAL imply the pre-conditions of PAUP or
PHYLIP until the services are actually executed. In such a
case, a condition can be generated which will be evaluated
at runtime and depending on the outcome of the condition,
corresponding services will be executed. The vertex for service CLUSTAL in the Figure 4 has an outgoing edge to a
conditional node. The outgoing edge represents the outputs
and post-conditions of the service. The conditional node
has multiple outgoing edges which represent the generated
conditions that are evaluated at run-time. In this case the
following conditions are generated:
• (Format(AlignedSequenceSet) = Paup ∨ Format(AlignedSequenceSet) = Phylip) ⇒ (Format(AlignedSequenceSet) = Paup
• (Format(AlignedSequenceSet) = Paup ∨ Format(AlignedSequenceSet) = Phylip) ⇒ (Format(AlignedSequenceSet) = Phylip
Depending on the condition that holds, the corresponding
services PAUP and MEGA or PHYLIP are executed respectively. The outputs EvolutionTree and EvolutionDistance
are produced in both the cases along with the post-condition
that the format of the evolution tree is Newick. Figure 4
shows this non-sequential conditional composition example
as a conditional directed acyclic graph.

Example 2: Workflow Generation (Non-sequential Conditional Composition)
Suppose we are looking for a service that takes a GeneSequence and produces an EvolutionTree and EvolutionDistance after performing a phylogenetic analysis. Also the
service should satisfy the post-condition that the EvolutionTree produced is in the Newick format. This involves producing a sequence set first, followed by aligning the sequence set and then producing the evolution tree and evolution distance. Also any necessary intermediate data format
translations have to be performed. Table 3 shows the services in the repository and their corresponding input/output
parameters and user-query. For the sake of simplicity, the
query and services have fewer input/output parameters than
the real-world services.
In this example, service BLAST has to be executed first
so that its output BLASTSequenceSet can be used as input
by CLUSTAL after the data format has been translated using BLASTNexus and NexusCLUSTAL. The service BLASTNexus has a post-condition that the format of the output
parameter NexusSequenceSet is Nexus which is the precondition of the next service NexusCLUSTAL. Similarly the
service NexusCLUSTAL has a post-condition that the format of the output parameter ClustalSequenceSet is Clustal
which is the pre-condition of the next service CLUSTAL.
At every step of composition, the post-conditions of a ser-

6. Performance
We tested our composition engine using repositories
from WS-Challenge website [14], slightly modified to fit
into our framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The queries and solutions are provided in an XML format. The semantic
relations between various parameters are provided in an
XML Schema file. We evaluated our approach on three

225

Service
Query

Pre-Conditions

Input Parameters
Sequence,OrganismType,
WordSize,DatabaseName
Sequence,OrganismType,
DatabaseName
ClustalSequenceSet

Output Parameters
EvolutionTree,
EvolutionDistance
BlastSequenceSet

Post-Conditions
Format(EvolutionTree)=Newick

AlignedSequenceSet

BLASTNexus

BlastSequenceSet

NexusSequenceSet

NexusCLUSTAL Format(NexusSequenceSet)=Nexus
PAUP
Format(AlignedSequenceSet)=Paup
PHYLIP
Format(AlignedSequenceSet)=Phylip
MEGA
Format(AlignedSequenceSet)=Paup

NexusSequenceSet

ClustalSequenceSet

AlignedSequenceSet,
WordSize
AlignedSequenceSet

EvolutionTree

Format(AlignedSequenceSet)=Paup ∨
Format(AlignedSequenceSet)=Phylip
Format(NexusSequenceSet)=Nexus
Format(ClustalSequenceSet)=Clustal
Format(EvolutionTree)=Newick
Format(EvolutionTree)=Newick

AlignedSequenceSet

EvolutionDistance

BLAST
CLUSTAL

Format(ClustalSequenceSet)=Clustal

EvolutionTree

Table 3. Example Scenario for Non-Sequential Conditional Composition
different kind of queries that produce sequential compositions, non-sequential compositions, and non-sequential
conditional compositions. We tested the engine on different
size repositories and tabulated Pre-processing and Query
Execution time. Table 4 shows the performance results for
these three kind of compositions. We noticed that there was
a significant difference in pre-processing time and query execution time. Also the pre-processing performed on the
repositories remains the same irrespective of the kind of
queries. Also the query execution time is slightly higher for
the case of non-sequential and non-sequential conditional
composition. The results are consistent with our expectations: for a fixed repository size, the preprocessing time
increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, the efficiency of service query processing is
negligible (just 1 to 3 msecs) even for complex queries with
large repositories.

tion, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential, non-sequential or non-sequential
conditional composition that is possible for a given query
and also automatically generates OWL-S description of the
composite service. This OWL-S description can be used
during the execution phase and subsequent searches for this
composite service will yield a direct match. We are able to
apply many optimization techniques to our system so that
it works efficiently even on large repositories. Use of Constraint Logic Programming helped greatly in obtaining an
efficient implementation of this system.
Our future work includes investigating other kinds of
compositions with loops such as repeat-until and iterations
and their OWL-S description generation. Analyzing the
choice of the composition language (e.g., BioPerl [3] for
phylogenetic workflows) and exploring other language possibilities is also part of our future work. We are also exploring combining technologies of automated service composition and domain specific languages to develop a framework
for problem solving and software engineering [2].

7. Conclusions and Future Work
To make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize and compose services automatically. Our semanticsbased approach uses semantic description of Web services
to find substitutable and composite services that best match
the desired service. Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services, number of services in a composi-

References
[1] G. Castagna, N. Gesbert, L. Padovani, et al. A Theory of Contracts for Web Services. In Symposium on
Principles of Programming Languages, January 2008
[2] G. Gupta, S. Kona, B. Devries, et al. Problem
Solving in Evolutionary Analysis with Web Ser-

226

Repository
Size
(num of
services)
2000
2000
2000
2500
2500
2500
3000
3000
3000

Number
of I/O
param-eters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.5
45.8
57.8
47.7
58.7
71.6
56.8
77.1
88.2

QueryExec Time (msecs)
Sequential
Composition
1
1
2
1
1
2
1
1
3

NonSequential
Composition
1
1
2
1
2
2
1
2
3

Conditional
Composition
1
2
2
1
2
3
1
3
4

Table 4. Performance of Generalized Composition Engine
vices and DSLs. https://www.nescent.org/
wg_evoinfo/Announcements.

[14] WS Challenge 2006. http://insel.flp.cs.
tu-berlin.de/wsc06.
[15] D. Mandell, S. McIlraith Adapting BPEL4WS for the
Semantic Web: The Bottom-Up Approach to Web Service Interoperation. In ISWC, 2003.
[16] K. Marriott and P. Stuckey. Prog. with Constraints:
An Introduction. MIT Press, 1998.
[17] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara
Semantic Matching of Web Service Capabilities. In
ISWC, pages 333-347, 2002.
[18] S. McIlraith, T. Son Adapting golog for composition
of semantic Web services. In KRR, pp.482–493, 2002.
[19] M. Pistore, P. Roberti, and P. Traverso Process-Level
Composition of Executable Web Services In European
Semantic Web Conference, pages 62-77, 2005.
[20] J. Rao, D. Dimitrov, P. Hofmann, and N. Sadeh A
Mixed-Initiative Approach to Semantic Web Service
Discovery and Composition In International Conference on Web Services, 2006.
[21] P. Hofmann. SAP AG. Personal Communication.
[22] D. Suvee, B. Fraine, and M. Cibran Evaluating FuseJ
as a Web Service Composition Language In European
Conference on Web Services, 2005.
[23] D. Claro, P. Albers, and J. Hao Selecting Web services
for Optimal Compositions In Workshop on Semantic
Web Services and Web Service Composition, 2004.
[24] B. Srivastava, J. Koehler. Web Services Composition Current Solutions and Open Problems In ICAPS, 2003.
[25] J. Rao, X. Su. A Survey of Automated Web Service
Composition Methods In Workshop on Semantic Web
Services and Web Process Composition, 2004.
[26] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

[3] BioPerl. http://www.bioperl.org.
[4] A.W.F. Edwards, L.L. Cavalli-Sforza (1964). Systematics Assoc. Publ. No. 6: Phenetic and Phylogenetic Classification: Reconstruction of evolutionary trees, 67-76.
[5] J. Felsenstein. Inferring Phylogenies Sinauer; 2 ed.,
2003
[6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp.
46-53, March 2001.
[7] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[8] MyGrid. http://www.mygrid.org.uk/.
[9] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005.
[10] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007.
[11] A. Bansal, K. Patel, G. Gupta, B. Raghavachari,
E. Harris, and J. Staves. Towards Intelligent Services:
A case study in chemical emergency response. In
ICWS, pp.751-758, 2005.
[12] J. Iglesias, G. Gupta, E. Pontelli, D. Ranjan, and
B. Milligan. Interoperability between Bioinformatics
Tools: A Logic Programming Approach. In Practical Aspects of Declarative Languages (PADL), Lecture
Notes in Computer Science. Springer Heidelberg, 2001.
[13] G. Gupta, et al. Semantics-Based Filtering: Logic Programming’s Killer App In International Symposium on
Practical Aspects of Declarative Languages (PADL),
pages 82–100. Springer-Verlag, 2002.

227

Towards Intelligent Services: A Case Study in Chemical Emergency
Response∗
Ajay Bansal, Kunal Patel, Gopal Gupta,
B. Raghavachari, E. D. Harris
University of Texas at Dallas
Richardson, TX 75080.

James C. Staves
University of Texas at Dallas†
& Environmental Protection Agency
Dallas, TX 75201.

Abstract

Examples of such side-effects include a web-base
being updated because of a plane reservation made
over the web, a devise being controlled, etc. As
automation increases, these web-services will be
accessed directly by the applications themselves
rather than by humans. In this context, a webservice can be regarded as the “programmatic interface” that makes application to application communication possible.

In a short period the Web has become an important
part of our lives. However, the full potential of the web
is still not realized. Two recent developments—web services and the semantic web—are steps in the direction
of utilizing the full potential of the Web. Web services
allow applications to utilize the web for automatically
extracting (and updating) information while the semantic web enterprise promises to provide the infrastructure that allows intelligent web services to be rapidly
created and deployed. However, with this comes the
task of transforming the traditional web-based systems
to web-services over the semantic web. In this paper, we
demonstrate how an existing successful web-based system for providing help to ﬁrst responders of chemically
hazardous emergencies (called E-plan) can be converted
into a web-services based model using the semantic web
and intelligent reasoning technologies. Our efforts can
be regarded as a case study in converting monolithic
web-based applications to a more agile, rapidly deployable intelligent web-services model.

1

• The Semantic Web: The vision of the semantic web
is to tag web-content in a way that it becomes automatically processable by machines. Once this
vision is realized, many sophisticated applications
will become possible, ranging from e-commerce to
knowledge discovery to more intelligent search capabilities. Given the present organization of the
Web, a dedicated software tool is needed for any
desired web-service. With the semantic web, a particular service can be obtained by merely posing an
appropriate query on the semantic web.
While a service based view increases Web’s utility, relying on a semantic web infrastructure increases the
ease and rapidity with which highly sophisticated webservices can be developed and deployed. Given the current structure of the Web, providing sophisticated services requires enormous effort. This is indeed borne
from our experience developing the E-plan system, a
web-based system for hazardous emergency response
that is currently in service in various cities and states
of the US. The E-plan system has been developed by
the authors atop the traditional Web and required many
man years of work for development and deployment.
Because of the monolithic nature of the system, adding
new capabilities (services) to the E-plan system entails
considerable amount of work. With the advent of the
semantic web, an obvious step is to migrate the E-plan
system to it, so that more sophisticated services can be
rapidly provided.

Introduction

The web is becoming increasingly important to our
daily lives. However, the full potential of the web is
not yet fully realized. Web services and the Semantic
Web enterprise are two important steps in realizing full
potential of the Web.
• Web Services: A web service is a program available on a web-site that “effects some action or
change” [3] in the world (i.e., causes a side-effect).
∗ Authors are partially supported by the National Science Foundation, Environmental Protection Agency and the Dept. of Education.
† On IPA assignment to direct the Center for Emergency Preparedness at U.T. Dallas.

1

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

In this paper, we present the architecture of the semantic web based E-plan system. We explore the application of semantic web to developing automated emergency response services. Speciﬁcally, we are interested
in services for aiding in emergency response in case of
a ﬁre at a facility that stores or manufactures hazardous
chemicals. Once such a ﬁre has been reported, ﬁre ﬁghters and emergency responders would like to use these
services to know what chemicals are stored in the facility, how a ﬁre involving them should be handled, what
other actions must be taken, etc. They will also need
services that look up the present weather conditions (for
example, wind speed and wind direction) in order to
project which way the chemical plume will go, as well as
services that provide locations of nearby public facilities
(e.g., schools), that may have to be evacuated because
they are in the path of the plume. Of course, all this has
to be done quickly as the ﬁre ﬁghters and emergency responders need to get to the scene of the emergency as
soon as possible.
Much of the information/knowledge that ﬁre-ﬁghters
need for effective emergency response is resident on the
Web, however, getting to this knowledge, composing
it, and querying it in a way that it becomes useful is a
daunting task given the current state of the Web. The
authors of this paper, have developed the E-plan system
[1], a web-based emergency response system, to aid ﬁreﬁghters in handling chemically hazardous emergencies.
The E-plan system is in operation in several states and
counties in the United States and has proven instrumental in saving lives and making emergency response to
chemically hazardous emergencies more efﬁcient. Plans
are afoot (by the EPA) to roll out the system nationally.
The E-plan system is based on the traditional notion
of the web, and took several man years to develop. Even
a small change in the design or the desire to add more
functions to the system requires considerable programming effort. In this paper, we show that the utility of
the E-plan system would be considerably increased if a
service-oriented model is adopted that is based on the
semantic web. We report on our efforts on building a
prototype of E-plan based on the semantic web. Our
experience indicates that the semantic web will considerably facilitate the development of E-plan like system.
It will also result in such systems becoming more useful, as more sophisticated services can be realized with
considerably more ease.

2

volving chemicals (e.g., ﬁre at a chemical plant). The
system is accessed via the traditional World Wide Web.
E-Plan also serves as a conduit between regulatory agencies and the regulated chemical industry. The system has
been developed by the authors [1]. Our goal was to develop a system that can be deployed in the real-world
and that is usable by all the stake holders (ﬁre ﬁghters,
emergency responders, EPA regulators, chemical industry representatives). Thus input from these stake-holders
was sought throughout the design process. The system
is currently operational in Plano (Texas), Corpus Christi
(Texas), Iberville (Louisiana), Cincinnati (Ohio), and in
the whole states of Arkansas and New Mexico. It will be
soon deployed in the entire state of Louisiana. Fire ﬁghters and emergency responders at these sites have been
trained to use the E-Plan system. Plans are in progress
to deploy the E-Plan system nationwide in the US for
handling chemicals-related emergencies and for providing information to regulatory agencies (e.g., EPA, Department of Transportation, Coast Guard).
The E-Plan system consists of two subsystems:
RegReS (Regulatory Response System) and Hazmat
(Hazardous Materials Repository and Responders’ Resource). RegReS provides a communication channel between companies that have to report information about
the chemicals they store/manufacture and regulatory
state and federal government agencies which approve
the information ﬁled by these companies. The information ﬁled by companies includes a list of chemicals stored on-site, their locations, approximate quantities, and the temperature and pressure at which they are
stored. The E-Plan system further adds value to this information by searching the various chemical databases
and other electronic resources on the web that contain
ﬁre-ﬁghting information and recording this information
in a structured way for use by ﬁre-ﬁghters and emergency responders. Latitude and longitude information
for the facility’s location is also added, and can be used
for ﬁnding current weather at the facility location. A
GIS map is also linked to ﬁnd out the entities surrounding the facilities. The Hazmat component deals with the
primary functions of information storage and dissemination to emergency responders during accidents involving
chemicals.
Additional sophisticated services can be extracted
from the E-plan system, for example, extending it to
automatically generate advice for ﬁre-ﬁghters. For instance, given the the current weather information, the
system can compute the direction of the chemical plume
and after consulting a GIS system it can advise the ﬁreﬁghters about the public places which should be evacuated ﬁrst. Extending the current E-plan system with
these extra services requires considerable effort. At the

The E-plan System

E-Plan is primarily an information repository for hazardous chemical data that is needed by ﬁrst responders
and incident commanders in case of an emergency in2

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

very least, an inferencing capability is needed. Inferring
new knowledge will become considerably easier if the
semantic web is used as the basis of the E-plan system,
and is the focus of the rest of the paper.

3

but also because a tool for semantically mapping RDF
into logical predicates has already been developed by us
[13]. It should be noted that in order to make sophisticated intelligent services related to chemical emergency
response available on E-plan, the semantic web is essential. Thus, approaches based on making data available
in XML format [10] are not enough. This is because for
building intelligent services, we should be able to reason with the data we have in our information resources.
An RDF statement can be mapped to a triple of Subject, Predicate and Object. This type of structure is not
implicit in data coded in XML/XML Schema. RDF syntax allows us to distinguish between the subject, predicate and object resources of a statement, which can then
be semantically used for reasoning and inferencing for
building intelligent services.
Our approach is based on converting the RDF coded
data to logic programs so that intelligent inferencing can
be done. An XML document can also be converted to a
logic program using the same technique, but since XML
does not have any semantics associated with it, we cannot use the generated logic program to reason with the
data. The RDF syntax allows us to convert RDF statements to logical predicates of the form:
property(subject, predicate, object)
and RDF semantics then allows us to reason with this set
of property predicates to make consistent inferences.
This idea of converting data in RDF format to a set of
logical predicates can be extended for data coded in any
web-based formal language like RDFS, OWL, DAML
etc. So when the ontologies are developed for various
domains using OWL, DAML+OIL or any other ontology development language, we would still be able to use
the same approach for developing Intelligent Web Services.

E-plan on the Semantic Web

Development of the E-plan system, described in the
previous section, required considerable effort. The
whole system was developed over a period of several
years and involved more than a dozen people. Much
of the effort was spent in acquiring the data and transforming it in an appropriate form for use by the E-plan
system and then programming the various functions. In
contrast, if the data was available in a machine processable format through web-services, then an E-plan like
system can be developed in a fraction of the time. Additionally, extensions of E-plan for providing services
such as automatically generating advise for evacuation,
environment protection, and ﬁre-ﬁghting can be realized
almost effortlessly in a semantic-web based system.
There are two main aspects to automatically providing such services:
1. Machine Readable Formats: The information
that is needed for providing these services should
be available in an unambiguous machine understandable format. This format can be based on Semantic Web languages such as RDF [5], RDFS [6],
OWL [2], etc. Various annotation tools have been
developed to help annotate the information present
on the web with machine understandable data. Examples of several such tools can be found in [8].
2. Inferring New Information: Once the information has been obtained in a machine understandable format, we should be able to deduce information implied by this data. That is, inference tools
should be available that allow services that reason
with this data to be rapidly developed. This entails
mapping the information that is in machine understandable format into some kind of an “executable
format” that can be directly fed to a knowledge inference engine. We adopt the semantic-mapping
based approach developed by us [13] in which the
machine understandable information is mapped to
Horn clause logic via semantics-preserving transformations. The resulting Horn clause logic code
can be directly executed on a traditional logic programming system [15].

4

Information Resources
Emergency Response

for

Development of an E-plan like system that is based
on the semantic-web requires a number of information
resources. For the prototype system, this data was taken
from the traditional E-plan system, and converted to
RDF. In particular, the following machine processable
information is needed:
1. Information about a chemical facility and the
chemicals that are stored in it.
2. Information on the properties of chemicals, e.g., reactivity of the chemical with water, etc.

We have adopted the Resource Description Framework
(RDF) notation [5] as the basis of E-plan system on the
semantic web, not merely because of RDF’s popularity

3. Information on important places (e.g., schools) situated near a chemical facility.
3

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

Needed

4. Real-time Information on local weather.

<fac:maxqty>99999</fac:maxqty>
...
</rdf:Description>

Currently, much of this information is available on
the web, albeit not in RDF format. As the semantic web
gains popularity, and the world-at-large sees its utility,
this information will be maintained in RDF (or similar)
formats. Until such time, of course, we can use existing
tools or build our own for translating various formats
into RDF. We have already developed many such tools
[11, 12].

<rdf:Description rdf:nodeID="chem2">
<fac:name>Chlorine</fac:name>
<fac:maxqty>9999</fac:maxqty>
...
</rdf:Description>
...
</rdf:RDF>

From the E-plan database we can get information
regarding various facilities and the chemicals that are
stored in those facilities. We converted this information in a machine processable form using RDF. This
data comes from the regulatory agencies of the state, and
once these organizations have adopted the semantic web,
we will be able to obtain this data in the appropriate format directly from them.
An example RDF document describing a facility is shown below. This information involves the
name of the facility (fac:name), the phone number (fac:company-phone) and addr (fac:addr)
of the facility, the emergency contact personnel
(fac:contact) of the company, and various chemicals (fac:chem) that are present in the facility.

Information regarding various chemicals is also
stored in the E-plan database. This information has been
extracted from various chemical databases maintained
by various professional organizations (e.g., National Library of Medicine, National Institutes of Health). Once
the semantic web becomes a standard, these organizations will directly publish their data in RDF formats. For the prototype, however, we converted E-plan
chemical property database (currently it has more than
20,000 entries for hazardous chemicals) into RDF. Following is the RDF rendition of such data for one of
the chemicals. This information includes name of the
chemical (chem:name), CAS number1 of the chemical
(chem:cas), the level of hazard to human health posed
by that chemical (chem:h), ﬂamability of the chemical (chem:f), reactivity of the chemical with water
(chem:r) and other physical properties of the chemical.

<rdf:RDF xmlns:rdf="http://www.w3.org/1999
/02/22-rdf-syntax-ns#"
xmlns:fac="http://www.eplan.org/fac/">

<rdf:RDF xmlns:rdf="http://www.w3.org/1999
/02/22-rdf-syntax-ns#"
xmlns:chem="http://www.eplan.org/chem/">

<rdf:Description rdf:about="http://www.
eplan.org/fac/IGC">
<fac:name>Igno Gas Company</fac:name>
<fac:phone>555-000-3456</fac:phone>
<fac:addr>1234 main street</fac:addr>
<fac:city>oakville</fac:city>
<fac:eplan-id>DF1002</fac:eplan-id>
<fac:contact rdf:nodeID="id1"/>
<fac:chem rdf:nodeID="chem1"/>
<fac:chem rdf:nodeID="chem2"/>
...
</rdf:Description>

<rdf:Description rdf:about="http://www.
eplan.org/chem/HCL">
<chem:name>HCL</chem:name>
<chem:cas>7647010</chem:cas>
<chem:h>2</chem:h>
<chem:r>2</chem:r>
<chem:ai-temp>102</chem:ai-temp>
...
</rdf:Description>
...
</rdf:RDF>

<rdf:Description rdf:nodeID="id1">
<fac:name>Ash Bell</fac:name>
<fac:title>Manager</fac:title>
<fac:phone>555-001-3456</fac:phone>
<fac:24hr>555-001-9872</fac:24hr>
...
</rdf:Description>

GIS information helps a ﬁre ﬁghter in locating nearby
public places or other places of interest. This informa-

<rdf:Description rdf:nodeID="chem1">
<fac:name>HCL</fac:name>

1 The CAS number is a unique universal id assigned to each chemical by the Chemical Abstract Service; there are more than 33 million
assigned CAS numbers.

4

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

tion can then be used to make important decisions. Suppose for example there is a ﬁre in a big chemical facility
and there is a school close to this facility. It might be
necessary to evacuate the school. A service can be implemented in the E-plan system which advises the ﬁreﬁghter of the places that should be evacuated based on
the proximity to the accident site.
GIS information can be also used for automatically
generating advise on taking actions to lessen the environmental impact of a chemical ﬁre/spill. For example,
if the chemical spill may run off into a river, then the
system can advise that appropriate booms be set up ﬁrst.
Following is an example of GIS information that can
be obtained from the web. The information is translated
into RDF format, and includes the addr (gis:addr) and
the exact location (gis:lat and gis:long) of the facility
and information about the nearby public places (we only
abstract the GIS layer that shows locations of prominent
public structures).

We can easily obtain weather information from
weather sites and convert this information to RDF
data. Following is an example of the weather information in RDF format. It includes the current dewpoint (wt:dewpoint), current humidity (wt:humidity),
current temperature (wt:temperature), current wind-dir
(wt:wind-dir), etc.
<rdf:RDF xmlns:rdf="http://www.w3.org/1999
/02/22-rdf-syntax-ns#"
xmlns:wt="http://www.weather.com/wt/">
<rdf:Description rdf:about="http://www.
weather.com/today">
<wt:uv-index>2</wt:uv-index>
<wt:dewpoint>61</wt:dewpoint>
<wt:humidity>83</wt:humidity>
<wt:temperature>72</wt:temperature>
...
</rdf:Description>
...
</rdf:RDF>

<rdf:RDF xmlns:rdf="http://www.w3.org/1999
/02/22-rdf-syntax-ns#"
xmlns:gis="http://www.gis.org/gis/">

5
<rdf:Description rdf:about="http://www.
eplan.org/fac/IGC">
<gis:name>Igno Gas Company</gis:name>
<gis:lat>36.25</gis:lat>
<gis:long>84.76</gis:long>
...
</rdf:Description>
...

Intelligent Services for Emergency Response

In this section we describe our approach to develop
intelligent services using the query languages for RDF
data. Once all the information resources discussed in
the previous section are translated to RDF format, it becomes possible to extract information from them. Moreover, it also becomes possible to reason with this information and provide intelligent services. These services
provide relevant information as well as crucial advise
to the ﬁre ﬁghters and emergency responders. As discussed earlier, we use the semantics-based approach described in [13] for making inferences over the semantic
web. This approach provides a framework for making
semantic web documents “executable”. In this framework, documents that are coded in a formal language
(RDF, RDFS, DAML, etc.) are denotationally mapped
to logical predicates(property relations). Thus, RDF information is denotationally mapped to triple statements:
property(S, P, O), which says that subject S has
property P with value O. These triple statements are interpreted as ground facts in a logic programming system
(e.g. Prolog). The query languages developed to query
RDF data are also denotationally mapped to such triple
statements. The query triples can then be executed w.r.t.
the data triples and the answers to the query can be obtained using a Prolog interpreter.
In our prototype semantic-web based E-plan system, the data is coded in RDF and converted to logical
predicates using the semantics based approach outlined

<rdf:Description rdf:about="http://www.
gis.org/publicplace1">
<rdf:type rdf:resource="http://www.
gis.org/school"/>
<gis:name>Plano School</gis:name>
<gis:lat>40.4</gis:lat>
<gis:long>81.9</gis:long>
...
</rdf:Description>
...
</rdf:RDF>

In emergency responses, such as a ﬁre hazard or other
natural calamity, weather information can play a very
critical role in determining the appropriate emergency
response. For example, in case of a chemical spill in a
plant near a river, a boom needs to be deployed to protect the run off from getting into the river only if it is
currently raining. Likewise, certain chemicals are hazardous at a lower temperatures and not at higher temperatures, etc.
5

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

above. The following are some of the property triples
that are generated by our semantics based system for the
RDF data described in the previous section:

property(’http://www.weather.com/today’,
’http://www.weather.com/
wt/uv-index’, 2).
property(’http://www.weather.com/today’,
’http://www.weather.com/
wt/dewpoint’, 61).
property(’http://www.weather.com/today’,
’http://www.weather.com/
wt/humidity’, 83).
property(’http://www.weather.com/today’,
’http://www.weather.com/
wt/temperature’, 72).
...

Facility Information:
property(’http://www.eplan.org/fac/IGC’,
’http://www.eplan.org/fac/name’,
’Igno Gas Company’).
property(’http://www.eplan.org/fac/IGC’,
’http://www.eplan.org/fac/phone’,
’555-000-3456’).
property(’http://www.eplan.org/fac/IGC’,
’http://www.eplan.org/fac/addr’,
’1234 main street’).
property(’http://www.eplan.org/fac/IGC’,
’http://www.eplan.org/fac/city’,
’oakville’).
...

Once the above RDF information is converted to
property predicates (ternary relations), intelligent services can be readily built. This can be achieved by
posing intelligent queries to extract meaningful results
from the data. To execute these queries on the RDF information, the queries should also be converted to such
property relations. The approach described in [13] is
general and can be applied to any query language. For
the semantic web based E-plan system, the desired services are coded as RDQL queries [7] (of course, other
query languages for RDF such as Versa, Notation3 [4],
SiLRI [9], etc. can also be used). RDQL treats RDF as
a database and presents query in terms of triple patterns.
We transform an RDQL query to a logic based query
[13].
Next, we show examples of some services with their
corresponding RDQL query code and their denotations
in terms of property relations. These property relations
constitute a query that is executed w.r.t. the property relations obtained from the RDF documents.

Chemical Information:
property(’http://www.eplan.org/chem/HCL’,
’http://www.eplan.org/chem/name’,
’HCL’).
property(’http://www.eplan.org/chem/HCL’,
’http://www.eplan.org/chem/cas’,
’7647010’).
property(’http://www.eplan.org/chem/HCL’,
’http://www.eplan.org/chem/h’,
2).
property(’http://www.eplan.org/chem/HCL’,
’http://www.eplan.org/chem/r’,
2).
property(’http://www.eplan.org/chem/HCL’,
’http://www.eplan.org/chem
/ai-temp’, 102).
...

A Web Service to ﬁnd out the chemicals stored at a
facility: In the case of an emergency situation at a facility, one of the most important thing to know is the
chemicals that are present in the facility. Given the facility name, a service to ﬁnd the name of all the chemicals that are present in a facility can be coded using the
following RDQL query.

GIS Information:
property(’http://www.gis.org/publicplace1’,
’http://www.w3.org/1999/02
/22-rdf-syntax-ns#type’,
’http://www.gis.org/school’).
property(’http://www.gis.org/publicplace1’,
’http://www.gis.org/gis/name’,
’Plano School’).
property(’http://www.gis.org/publicplace1’,
’http://www.gis.org/gis/lat’,
40.4).
property(’http://www.gis.org/publicplace1’,
’http://www.gis.org/gis/long’,
81.9).
...

SELECT ?namechem
WHERE (?resource,fac:name,Igno Gas Company)
(?resource,fac:chem,?chemid)
(?chemid, fac:name, ?namechem)
USING fac FOR <http://www.eplan.org/fac/>

The above RDQL query is denotationally mapped by our
semantics based system to the property relation shown
below:
:- property(_RESOURCE,
’http://www.eplan.org/fac/name’,
’Igno Gas Company’),
property(_RESOURCE,

Weather Information:
6

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

(?places, gis:long, ?endlong)
(?startlat - ?endlat < k ||
?startlong - ?endlong < m)
USING fac FOR <http://www.eplan.org/fac/>
gis FOR <http://www.gis.org/gis/>

’http://www.eplan.org/fac/chem’,
_CHEMID).
property(_CHEMID,
’http://www.eplan.org/fac/name’,
NAMECHEM)

AND

The corresponding query in terms of property triple generated by our system is shown below:

Executing the above query (i.e., treating it as a top-level
goal) on the property triples for facility information, we
get the following result:

:- property(_RESOURCE,
’http://www.eplan.org/fac/name’,
’Igno Gas Company’),
property(_RESOURCE,
’http://www.gis.org/gis/lat’,
_STARTLAT),
property(_RESOURCE,
’http://www.gis.org/gis/long’,
_STARTLONG),
property(_PLACES,
’http://www.gis.org/gis/name’,
NAMEPLACE),
property(_PLACES,
’http://www.gis.org/gis/lat’,
_ENDLAT),
property(_PLACES,
’http://www.gis.org/gis/long’,
_ENDLONG),
_STARTLAT - _ENDLAT < k;
_STARTLONG - _ENDLONG < m.

NAMECHEM = HCL;
NAMECHEM = Chlorine;
...

The property relations this RDQL query is translated
into is shown below:
:- property(_RESOURCE,
’http://www.eplan.org/fac/name’,
’Igno Gas Company’),
property(_RESOURCE,
’http://www.eplan.org/fac/chem’,
_CHEMID),
property(_CHEMID,
’http://www.eplan.org/fac/name’,
NAMECHEM),
property(_NOWATERCHEM,
’http://www.eplan.org/chem/name’,
NAMECHEM),
property(_NOWATERCHEM,
’http://www.eplan.org/chem/r’,
_REACTIVITY),
_REACTIVITY >= 1.

Note that this service can be further improved to take
wind direction into account. With help of the wind direction, we can write a query to determine only those
public facilities that are down-wind and within 2 miles
of the chemical facility that has an emergency.

A Web Service to determine public places near a facility: In certain kind of emergency situations like leakage of toxic gas from a facility or a huge ﬁre at a facility,
etc., it becomes necessary to evacuate the nearby public places for safety reasons. The ﬁre ﬁghters need to
know the public places that are located within a certain
distance from the facility. The following RDQL query
can be used to code a service to ﬁnd all the public places
that are located at a distance of less than 2 miles from
the facility.
In the following query it is assumed that if the difference of the values of the latitude of two places is less
than k and the difference of the values of the longitude
of two places is less than m then the distance between
the two places is less than 2 miles (the exact values of
constants k and m are not important to our discussion).

A Web Service to determine if a chemical spill is
harmless based on weather conditions: In this service,
we want to ﬁnd out if a response is urgently needed or
not (to prioritize the response assuming multiple facilities are on ﬁre). The chemical that leaked in this particular facility is not hazardous to health (health hazard
index is 0), and will ignite only if the current temperature is lower than the autoignition temperature of the
chemical. The query to determine which chemicals are
not hazardous due to the current air temperature can be
posed as shown below:
SELECT ?namechem
WHERE (?resource,fac:name,Igno Gas Company)
(?resource, fac:chem, ?chemid)
(?chemid, fac:name, ?namechem)
(?nohazchem, chem:name, ?namechem)
(?nohazchem, chem:r, ?reactivity)
(?nohazchem, chem:h, ?hazindex)
(?nohazchem, chem:ai-temp, ?airtemp)
(<http://www.weather.com/today>,
wt:temperature,?curtemp)

SELECT ?nameplace
WHERE (?resource,fac:name,Igno Gas Company)
(?resource, gis:lat, ?startlat)
(?resource, gis:long, ?startlong)
(?places, gis:name, ?nameplace)
(?places, gis:lat, ?endlat)

7

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

7

AND

(?hazindex==0 && ?reactivity==0
&& ?airtemp > ?curtemp)
USING fac FOR <http://www.eplan.org/fac/>
chem FOR <http://www.eplan.org/chem/>
wt
FOR <http://www.weather.com>

In this paper we discussed the development of a prototype system for providing intelligent emergency response services atop the semantic web. The use of the
semantic-web allows us to rapidly develop services that
will require considerable time to develop on the conventional web.
Our goal is to switch completely to a semantic-web
based solution for E-plan. We are considering high
level service description languages like USDL [14] to
describe our services so that they can be automatically
discovered and composed to build new services. We are
also building ﬁlters for data sources that are not available in RDF. Our future work includes designing userfriendly query languages and query interfaces that can
be used by non-experts for obtaining services from the
semantic-web based E-plan system.

It is translated into the following query over property
relations by our semantics-based system:
:- property(_RESOURCE,
’http://www.eplan.org/fac/name’,
’Igno Gas Company’),
property(_RESOURCE,
’http://www.eplan.org/fac/chem’,
_CHEMID),
property(_CHEMID,
’http://www.eplan.org/fac/name’,
NAMECHEM),
property(NOHAZCHEM,
’http://www.eplan.org/chem/name’,
NAMECHEM),
property(NOHAZCHEM,
’http://www.eplan.org/chem/r’,
_REACTIVITY),
property(NOHAZCHEM,
’http://www.eplan.org/chem/h’,
_HAZINDEX),
property(NOHAZCHEM,
’http://www.eplan.org/chem
/ai-temp’,_AIRTEMP),
property(’http://www.weather.com/today’,
’http://www.weather.com/wt/
temperature’, _CURTEMP),
_HAZINDEX = 0, _REACTIVITY = 0,
_AIRTEMP > _CURTEMP.

References
[1] Eplan: The e-plan emergency response system. http:
//eplannews.utdallas.edu.
[2] Owl: Web ontology language reference. http://
www.w3.org/TR/owl-ref.
[3] Owl(s):
Semantic markup for web services.
http://www.daml.org/services/owl-s/1.
0/owl-s.html.
[4] Rdf: Notation3. http://www.gingerall.com/
charlie/ga/html/rdf-notation3.html.
[5] Rdf: Rdf model theory. http://www.w3.org/TR/
rdf-mt.
[6] Rdf schema: Rdf vocabulary description language 1.0.
http://www.w3.org/TR/rdf-schema.
[7] Rdql: Jena toolkit - rdql query language. http://
www.hpl.hp.com/semweb/rdql.html.
[8] Semantic web: Annotation and authoring. http://
annotation.semanticweb.org/tools.
[9] S. Decker, D. Brickley, J. Sarrela, and J. Angele. Silri:
Simple logic based rdf interpreter. http://www.
aifb.uni-karlsruhe.de/˜sde/rdf.
[10] C. Goldfarb and P. Prescod. The XML Handbook. Prentice Hall, 1998.
[11] G. Gupta, H.-F. Guo, A. Karshmer, and E. Pontelli.
Semantic-based ﬁltering: Logic programming’s killer
app. In PADL ’02, pages 82–100. LNCS 2257.
[12] G. Gupta and X. Zhou. Automatic generation of validating parsers for xml. Technical report, Univ. of Texas at
Dallas, 2001.
[13] K. Patel and G.Gupta. Semantically processing the semantic web. In ISWC ’03, pages 80–95. LNCS 2870.
[14] L. Simon, A. Mallya, A. Bansal, G. Gupta, and T. Hite.
A universal service description language. Technical Report UTDCS-09-05, Univ. of Texas at Dallas, 2005.
[15] L. Sterling and S. Shapiro. The Art of Prolog. MIT Press,
1994.

As one can notice, once the conventions of the semantic
web are adopted, it is relatively easy to provide sophisticated services merely by coding them in RDQL. Our
semantics based system provides a framework for processing these queries, and thus make the semantic web
“executable”.

6

Conclusions & Future Work

Plans for Deployment

A prototype semantic-web based E-plan system has
been developed as described above. The prototype was
developed to study the feasibility of a semantic-web
based E-plan system. Our prototype requires that all
data sources be in RDF format. As semantic web gains
acceptance, we anticipate that all information will be indeed available in RDF. However, until such time, translaters that convert database formats into RDF can be either bought off-the-shelf or easily crafted. As the semantic web technology becomes more widely adopted,
we hope to move the E-plan system completely on top
of the semantic web.
8

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

SOCA (2016) 10:111–133
DOI 10.1007/s11761-014-0167-5

ORIGINAL RESEARCH PAPER

Generalized semantic Web service composition
Srividya Bansal · Ajay Bansal · Gopal Gupta ·
M. Brian Blake

Received: 29 January 2014 / Revised: 31 August 2014 / Accepted: 23 October 2014 / Published online: 8 November 2014
© Springer-Verlag London 2014

Abstract With the increasing popularity of Web Services
and Service-Oriented Architecture, we need infrastructure
to discover and compose Web services. In this paper, we
present a generalized semantics-based technique for automatic service composition that combines the rigor of processoriented composition with the descriptiveness of semantics.
Our generalized approach presented in this paper introduces
the use of a conditional directed acyclic graph where complex
interactions, containing control flow, information flow, and
pre-/post-conditions are effectively represented. Composition solution obtained is represented semantically as OWL-S
documents. Web service composition will gain wider acceptance only when users know that the solutions obtained are
comprised of trustworthy services. We present a framework
that not only uses functional and non-functional attributes
provided by the Web service description document but also
filters and ranks solutions based on their trust rating that is
computed using Centrality Measure of Social Networks. Our
contributions are applied for automatic workflow generation
in context of the currently important bioinformatics domain.
We evaluate our engine for automatic workflow generation
of a phylogenetic inference task. We also evaluate our engine
S. Bansal (B) · A. Bansal
Arizona State University, Mesa, AZ, USA
e-mail: srividya.bansal@asu.edu
A. Bansal
e-mail: ajay.bansal@asu.edu
G. Gupta
The University of Texas at Dallas, Richardson, TX, USA
e-mail: gupta@utdallas.edu
M. B. Blake
University of Miami, Miami, FL, USA
e-mail: m.brian.blake@miami.edu

for automated discovery and composition on repositories of
different sizes and present the results.
Keywords Service composition · Service discovery ·
Semantic Web · Ontology · Workflow generation

1 Introduction
The next milestone in the evolution of the World Wide Web
is making services ubiquitously available. As automation
increases, Web services will be accessed directly by the applications themselves rather than by humans [1,2]. In this context, a Web service can be regarded as a “programmatic interface” that makes application-to-application communication
possible. To make services ubiquitously available, we need
infrastructure that applications can use to automatically discover, deploy, compose, and synthesize services. A Web service is an autonomous, platform-independent program accessible over the web that may affect some action or change in
the world. Sample of Web services include common plane,
hotel, rental car reservation services or device controls like
sensors or satellites. A Web service can be regarded as a “programmatic interface” that makes application-to-application
communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the
actions that it initiates. The input parameter may be further
subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order
to make Web services more practical, we need an infrastructure that allows users to discover, deploy, synthesize, and
compose services automatically. To make services ubiquitously available, we need a semantics-based approach such
that applications can reason about a service’s capability to
a level of detail that permits their discovery, composition,

123

112

deployment, and synthesis [3]. Several efforts are underway
to build such an infrastructure [4–6].
With regard to service composition, a composite service
is a collection of services combined together in some way to
achieve a desired effect. Traditionally, the task of automatic
service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [7].
Most efforts reported in the literature focus on one or more
of these four phases. The first phase involves generating a
plan, i.e., all the services and the order in which they are to
be composed in order to obtain the composition. The plan
may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per
the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate
services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions
based on non-functional properties like QoS properties. The
last phase involves executing the services as per the plan and
in case any of them are not available, an alternate solution
has to be used.
In this paper, we present a general approach for automatic
service composition. Our composition algorithm performs
planning, discovery, and selection automatically, all at once,
in one single process. This is in contrast to most methods
in the literature where one of the phases (most frequently
planning) is performed manually. Additionally, our method
generates most general compositions based on (conditional)
directed acyclic graphs (DAG). Note that service discovery is
a special case of composition of n services, i.e., when n = 1.
Thus, we mainly study the general problem of automatically
composing n services to satisfy the demand for a particular
service, posed as a query by the user. In our framework,
the DAG representation of the composite service is reified
as an OWL-S description. This description document can
be registered in a repository and is thus available for future
searches. The composite service can now be discovered as
a direct match instead of having to look through the entire
repository and build the composition solution again. We show
how service composition can be applied to a Bioinformatics
analysis application, for automatic workflow generation in
the field of Phylogenetics [8].
One of the current challenges in automatic composition
of Web services also includes finding a composite Web service that can be trusted by consumers before using it. Our
approach uses analysis of Social Networks to calculate a
trust rating for each Web service involved in the composition
and further prune results based on this rating. Web-based
Social Networks have become increasingly popular these
days. Social Network Analysis is the process of mapping
and measuring the relationships between connected nodes.
These nodes could represent people, groups, organizations,
computers, or any knowledge entity. We propose to measure

123

SOCA (2016) 10:111–133

the trust factor of a service by measuring the centrality of
a service provider and/or a service provider organization in
a well-known Social Network. The three level indices that
can be applied to measure centrality are degree, betweenness, and closeness [9]. We adopt our idea of computing
trust using centrality measure based on the notion of centrality and prestige being key in the study of social networks
[9,10]. The role of central people (nodes with high centrality) in a network seems to be fundamental as they adopt the
innovation and help in transportation and diffusion of information throughout the rest of the network. So our rationale
is that these central figures who play a fundamental role in
the network are trusted by others in the network who are
connected (directly or indirectly) to them.
A simple use case scenario Jane is a researcher in the field
of Evolutionary Genetics. One evening she is examining the
evolution of crab species and needs to build a Phylogenetic
tree for various crab species using protein sequence data. In
order to complete this task, she will have to go to her lab and
access the computer with necessary software and perform
multiple computations using various algorithms. She uses
the well-known Molecular Evolutionary Genetics Analysis
software program (MEGA5) [11] that is an integrated tool
for conducting automatic and manual sequence alignment,
inferring phylogenetic trees, mining web-based databases,
estimating rates of molecular evolution, inferring ancestral
sequences, and testing evolutionary hypotheses. Jane has to
use this software to first align the sequence data using one
of several algorithms (such as Clustal W [12], MUSCLE
[13], etc.) provided by MEGA5 for this purpose. Next, she
wants to compute and compare the evolutionary distances
for sequences from crab species using various algorithms. In
order to do this she must first compute relevant models for
crab species. The next step is to compute evolutionary distances using Jukes–Cantor model followed by Tamura–Nei
model and compare them. She is also interested in the evolutionary distance computed based on proportion of amino acid
differences. Finally, she is interested in building a Phylogenetic tree from the aligned sequence data. She will have to
pick one of the algorithms/methods provided by MEGA5 that
include Maximum Likelihood, Minimum Evolution, Maximum Parsimony, Neighbor-Joining, etc. Currently, there is
no easy way to perform this analysis and she will have to use
the software tools manually and go through this step-by-step
process and wait while computation for each of the steps is
being performed. Jane will have to go through this laborintensive process in order to find an answer to her research
question.
Imagine a Software-as-a-Service (SaaS) platform [14]
available on the cloud and Jane has access to it from any
computer or mobile device. With a few simple clicks, Jane
provides a query request that includes input parameters to this
workflow process and expected final outputs. The software

SOCA (2016) 10:111–133

platform built upon our composition engine produces multiple possible workflows using different combinations of algorithms/methods (available as Web services) for each of the
tasks such as sequence alignment, model computation, distance computation, and generation of phylogeny. Jane picks a
workflow that is most suited for her research analysis and possibly even edits the workflow by adding a service to compute
the diversity in the subpopulation of crabs. She saves off this
workflow to her profile for future use. She initiates the workflow execution and on the following day, analyses the output
results that were produced and saved off in her account. This
software platform is able to save Jane a significant amount of
time—not only in performing the computations for analysis,
but also with configuring workflows and using interesting
workflows already created by her colleagues. This is just one
simple example of the potential of Web service discovery
and composition in various disciplines. This paper presents
the underlying composition engine that is needed in order to
build such a software platform.
This paper extends our previous work in the area of Web
service composition [15] by conducting a case study on automatic workflow generation for Phylogenetic Inference tasks
in Bioinformatics using our composition engine and introducing the computation of a trust rating of each Web service,
based on Centrality measure in Social Network analysis, and
using this trust rating in filtering and ranking services. This
work would support the development of a SaaS platform that
supports domain-specific workflow generation. Our research
makes the following novel contributions:
(i) Formalization of the generalized composition problem based on our conditional directed acyclic graph representation; (ii) Computation of trust rating of composition
solutions based on individual ratings of service providers
obtained using the Centrality measure of Social Networks;
(iii) Efficient and scalable algorithm for solving the composition problem that takes semantics of services into account;
our algorithm automatically discovers and selects the individual services involved in composition for a given query,
without the need for manual intervention; (iv) Automatic
generation of OWL-S descriptions of the new composite service obtained; (v) Case study of our generalized composition
engine to automatically generate workflows in the field of
Bioinformatics for Phylogenetic Inference tasks.
The rest of the paper is organized as follows. In Sect. 2, we
present the related work in the area of Web service discovery
and composition and discuss their limitations. In Sect. 3, we
formalize the generalized Web service composition problem.
We present our multi-step narrowing technique for automatic
Web service composition and automatic generation of OWLS service description in Sect. 4. We present the implementation and experimental results in Sect. 5. Section 6 presents
an application of our generalized composition engine to
automatically generate workflows for Bioinformatics analy-

113

sis tasks. The last section presents conclusions and future
work.

2 Related work
Composition of Web services has been active area of research
[7,16,17]. Most of these approaches present techniques to
solve one or more phases of composition as listed in Sect. 1.
There are many approaches [6,18,19] that solve the first two
phases of composition namely planning and discovery. These
are based on capturing the formal semantics of the service
using action description languages or some kind of logic
(e.g., description logic). The service composition problem
is reduced to a planning problem where the sub-services
constitute atomic actions and the overall service desired is
represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal.
With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available.
To the best of our knowledge, most of these approaches that
use planning are restricted to sequential compositions, rather
than a directed acyclic graph. In this paper, we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential
but also non-sequential that can be represented in the form of
a directed acyclic graph. The authors in [18] present a composition technique by applying logical inferencing on predefined plan templates. Given a goal description, they use the
logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also
relies on a user-defined plan template, which is created manually. One of the main objectives of our work is to come up
with a technique that can automatically produce composition
without the need for any manual intervention. Boustil et al.
[20] present an approach that uses an intermediate ontology
built using OWL-DL and SWRL rules to define the affected
object and their relationships. Their selection strategy considers relationships between services by looking at object
values of affected objects. They use a custom intermediate
ontology that is built within their framework using OWL-DL.
Our approach focuses on the semantics of the parameters as
well as constraints represented as pre- and post-conditions.
Also our approach is generic and can be used with any domain
ontology to provide semantics.
There are industry solutions based on WSDL and BPEL4
WS where the composition flow is obtained manually.
BPEL4WS can be used to define a new Web service by
composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process.
They select appropriate services using a planner when an
explicit flow is provided. In contrast, our technique auto-

123

114

matically determines these complex flows using semantic
descriptions of atomic services. A process-level composition
solution based on OWL-S is proposed in [19]. In this work,
the authors assume that they already have the appropriate
individual services involved in the composition, i.e., they are
not automatically discovered. They use the descriptions of
these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but
instead assume that they already have the list of atomic services. In contrast, we present a technique that automatically
finds the services that are suitable for composition based on
the query requirements for the new composed service. There
are solutions such as [21] that solve the selection phase of
composition. This work uses pre-defined plans and discovered services provided in a matrix representation. Then, the
best composition plans are selected and ranked based on QoS
parameters like cost, time, and reputation. These criterions
are measured using fuzzy numbers.
There has been a lot of work on composition languages
such as WS-BPEL, FuseJ, AO4BPEL, etc. which are useful only during the execution phase. FuseJ is a description
language for unifying aspects and components [22]. Though
this language was not designed for Web services, the authors
contend that it can be used for service composition as well.
It uses connectors to interconnect services. We believe that
there is no centralized process description, but instead information about services is spread across the connectors. With
FuseJ, the planning phase has to be performed manually that
is the connectors have to be written by the developer. Similarly, OWL-S also describes a composite service but does not
automatically find the services involved in the composition.
So these languages are only useful for execution which happens after the planning, discovery, and selection of services
is done. Service grounding of OWL-S maps that describe
abstract services to the concrete WSDL specification helps
in executing the service. In contrast, our approach automatically generates the composite service. This new composite
service generated can then be described using one of these
composition languages.
QoS-aware composition has also been active area of
research [6,21]. Research on a QoS-aware composition [23–
25] consider applying SLA’s to workflow compositions or
Web service compositions, although they do not perform
dynamic composition. They use one of the existing composition languages to create the composite service manually or create a template that is later used to select appropriate services for each stage of composition. After obtaining composition solutions manually or semi-automatically,
these approaches present a QoS model and apply the nonfunctional attributes on the potential solutions to confirm
that they comply with the pre-defined agreements. Thus,
the solutions are pruned based on SLA compliance. Work

123

SOCA (2016) 10:111–133

on workflow Composition of service-level agreements [26]
presents a set of SLA measures and principles that best support QoS-based Composition. A model and representation of
SLA attributes were introduced and an approach to compose
SLA’s associated with a workflow of Web services was presented. The research on creating a QoS-Aware middleware
for Web service Composition in [27] is similar to our work
as they identify services that can fit into a useful composition
based on QoS measures. They use two approaches for selection: one based on local (task-level) selection of services and
the second is based on a global allocation of tasks to services.
They also use a template for composition; in this case, a state
chart that has the generic service tasks defined. Finding a
composite service involves finding concrete services that fit
into the template. In contrast, we do not use any template
but instead find the composition solution automatically. The
work presented in [28] combine semantic annotations and
SLA’s thereby providing better approach to specification of
SLA’s.
Researchers have looked into a fuzzy linguistic preference model to provide preference relations on various QoS
dimensions [29]. They use a specific weighting procedure to
provide numeric weights to preference relations, and then
use a hybrid evolutionary algorithm to find skyline solutions efficiently. Their algorithm is designed on the basis
of Pareto-dominance and weighted Tchebycheff distance. In
this approach, the authors assume that they have candidate
services for composition. Their algorithm helps identify best
solution based on their SLA’s.
Feng’s research group proposed an approach to composition that associated QoS attributes to service dependencies
and showed their approach could model real-life services and
perform effective QoS constraint satisfaction and optimization [30]. The attributes taken into consideration by this study
are Response time, Cost, Reliability, Availability, and Reputation. They consider that QoS values of a service may be
dependent not only on the service itself but also some other
services in the workflow. They propose 3 types of QoS for
each attribute namely: default QoS, partially dependent QoS,
and totally dependent QoS. Default QoS applies no matter
what the preceding service is in a workflow, just like the conventional QoS. Partially dependent QoS applies if and only if
some of the inputs of a service are provided by the outputs of
another service. Totally dependent QoS applies if and only if
all inputs of a service are provided by the outputs of another
service. Formal modeling of QoS attributes is provided in
OWL-S [27]. Work by Wen et al. [32] presents an approach
to obtaining probabilistic top-K dominating services with
uncertain QoS. QoS values tend to fluctuate at run-time and
hence this approach uses probabilistic characteristics of service instances to identify dominating service abilities for better selection. A detailed survey of approaches for a reliable
dynamic Web service composition is presented by Immonen

SOCA (2016) 10:111–133

115

and Pakkala [33]. They discuss various approaches that use
Reliability ontology to manage and achieve reliable composition. They address the lack of formalization to handle reliability of composition, whereas the focus of our approach is the
formalization of a generalized composition that uses functional attributes to compose a solution and non-functional
attributes help in further filtering and ranking solutions.
A number approaches focus on trust and reputation QoS
criteria for service selection. Mehdi et al.’s [34] approach
assigns trust scores to Web services and only services
with highest scores are selected for composition. They use
Bayesian networks to learn the structure of composition. The
approach presented by Kutler et al. [35] considers social trust
in Web service composition. They compute trust based on
similarity measures over ratings of users added into a system.
They use the correlation between trust and overall similarity
measures in online communities. On the contrary, we use the
centrality measure in a Web-based Social Network.
In this paper, we present a technique for automatically
planning, discovering, and selecting services that are suitable for obtaining a composite service based on user-query
requirements. As far as we know, all the related approaches
to this problem assume that they either already have information about services involved or use human input on what
services would be suitable for composition. This work is an
extension of our earlier work [15] that introduced a generalized Web service composition engine. In this paper, we
use the trust rating of a Web service in addition to the functional and non-functional attributes of a service in filtering
and ranking solutions. In this paper, we evaluate our composition the engine using a case study from the bioinformatics
domain for Phylogenetic inference tasks to show that this
engine can be used for automatic workflow generation. The
case study uses example workflows that would be generated
as a sequential composition, non-sequential composition as
well as non-sequential conditional composition.

3 Automated Web service discovery and composition
Discovery and composition are two important tasks related
to Web services. In this section, we formally describe these

tasks and develop the requirements of an ideal discovery/composition engine.
3.1 The discovery problem
Given a repository of Web services, and a query requesting
a service (hereafter query service), automatically finding a
service from the repository that matches these requirements
is the Web service discovery problem. Only those services
that produce at least the requested output parameters that
satisfy the post-conditions and use only from the provided
input parameters that satisfy the pre-conditions and produce
the same side effects can be valid solutions to the query. Some
of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre-/post-conditions, and side effect requirements. This
activity is best illustrated using an example:
Example (Discovery) A buyer is looking for a service to buy
a book and the directory of services contains services S1 and
S2 . Table 1 shows the input/output parameters of the query
and services S1 and S2 . In this example service S2 satisfies
the query; however, S1 does not as it requires BookISBN
as an input and it is not provided by the query. Our query
requires ConfirmationNumber as the output and S2 produces
ConfirmationNumber and TrackingNumber. The extra output
produced can be ignored. Also the semantic descriptions of
service input/output parameters should be same as the query
parameters or satisfy the subsumption relation. The discovery engine should be able to infer that the query parameter
BookTitle and input parameter BookName of service S2 are
semantically the same concepts. This can be inferred using
semantics from the annotation of the service and the ontology (e.g., OWL WordNet ontology) provided. The query also
has a pre-condition that the CreditCardNumber is numeric,
which should logically imply pre-conditions of the discovered service.
Definition (Service) A service is a 6-tuple of its preconditions, inputs, side effect, affected object, outputs and
post-conditions.
S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions, I is the input list,

Table 1 Discovery—example
Service

Input parameters

Pre-conditions

Output parameters

Query

BookTitle, CreditCardNumber,
AuthorName, CreditCardType

lsNumeric(CreditCard Number)

ConfirmationNumber

S1

BookName, AuthorName,
BooklSBN, CreditCardNumber

S2

BookName, CreditCardNumber

Post-conditions

ConfirmationNumber
lsNumeric(CreditCard Number)

ConfirmationNumber,
TrackingNumber

123

116

SOCA (2016) 10:111–133

A is the service’s side effect, AO is the affected object, O is
the output list, and CO is the list of post-conditions. The preand post-conditions are ground logical predicates.
Definition (Repository of Services) Repository (R) is a set
of Web services.
Definition (Query) The query service is defined as Q =
(CI , I  , A , AO , O  , CO ) where CI is the list of preconditions, I  is the input list, A is the service affect, AO is
the affected object, O  is the output list, and CO is the list of
post-conditions. These are all the parameters of the requested
service.
Definition (Discovery) Given a repository R and a query
Q, the discovery problem can be defined as automatically
finding a set S of services from R such that S = {s | s =
(CI, I, A, AO, O, CO), s  R, CI ⇒ CI, I  I  , A =
A , AO = AO  , CO ⇒ CO , O ⊇ O  }. The meaning
of  is the subsumption (subsumes) relation and ⇒ is the
implication relation. For example, say x and y are input and
output parameters, respectively, of a service. If a query has
(x > 5) as a pre-condition and (y > −x) as post-condition,
then a service with pre-condition (x > 0) and post-condition
(y > x) can satisfy the query as (x > 5) ⇒ (x > 0) and

Fig. 1 Substitutable service

Fig. 2 Composite service represented as a directed acyclic graph

(y > x) ⇒ (y > −x) since (x > 0). Figure 1 shows the
substitution rules for the discovery problem.
3.2 The composition problem
Given a repository of service descriptions, and a query with
the requirements of the requested service, in case a matching
service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can
be composed to obtain the desired service. Figure 2 shows
an example composite service made up of five services S1 to
S5 . In figure, I  and CI are the query input parameters and
pre-conditions, respectively. O  and CO are the query output
parameters and post-conditions respectively. Informally, the
directed arc between nodes Si and S j indicates that outputs
of Si constitute (some of) the inputs of S j .
Example (Sequential Composition) Suppose we are looking for a service to make travel arrangements, i.e., flight,
hotel, and rental car reservations. The directory of services
contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 2 shows the input/output parameters of the user
query and the three services ReserveFlight, ReserveHotel,
and ReserveCar. For the sake of simplicity, the query and
services have fewer input/output parameters than the realworld services. In this example, service ReserveFlight has
to be executed first so that its output ArrivalFlightNum can
be used as input by ReserveHotel followed by the service
ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service
input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred
using semantics from the ontology provided. Figure 3 shows
this example sequential composition as a directed acyclic
graph.
Definition (Sequential Composition) The sequential Composition problem can be defined as automatically finding a
directed acyclic graph G = (V, E) of services from repository R, given query Q = (CI , I  , A , AO , O  , CO ),
where V is the set of vertices and E is the set of edges

Table 2 Sequential composition example
Service

Input parameters

Query

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

HotelConfirmationNum,
CarConfirmationNum

ReserveFlight

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

FlightConfirmationNum,
ArrivalFlightNum

ReserveHotel

PassengerName, ArrivalFlightNum,
StartDate, ReturnDate

HotelConfirmationNum,
HotelAddress

ReserveCar

PassengerName, ArrivalDate,
ArrivalFlightNum, HotelAddress

CarConfirmationNum

123

Pre-conditions

Output parameters

Post-conditions

SOCA (2016) 10:111–133

117

Fig. 3 Sequential composition example

Fig. 5 Non-sequential composition example

Fig. 4 Sequential composition

of the graph. Each vertex in the graph represents a service
in the composition. Each outgoing edge of a node (service)
represents the outputs and post-conditions produced by the
service. Each incoming edge of a node represents the inputs
and pre-conditions of the service. The following conditions
should hold on the nodes of the graph: ∀i Si V, Si R, Si =
(CIi , Ii , Ai , AOi , Oi , COi )
1. I   I1 , O1  I2 , . . . , On  O 
2. CI ⇒ CI1 , CO1 ⇒ CI2 , . . . , COn ⇒ CO
The meaning of the  is the subsumption (subsumes) relation, and ⇒ is the implication relation. In other words, we
are deriving a possible sequence of services where only the
provided input parameters are used for the services and at
least the required output parameters are provided as an output by the chain of services. The goal is to derive a solution
with minimal number of services. Also, the post-conditions
of a service in the chain should imply the pre-conditions of
the next service in the chain. Figure 4 depicts an instance of
sequential composition.
Example (Non-sequential composition) Suppose we are
looking for a service to buy a book and the directory of
services contains services GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. Table 3 shows the
input/output parameters of the query and the four services
in the repository. Suppose a single matching service is not

found in the repository, a solution is synthesized from among
the set of services available in the repository. Figure 5 shows
this composite service. The post-conditions of the service
GetAvailability should logically imply the pre-conditions of
service PurchaseBook.
Definition (Non-sequential composition) More generally,
the Composition problem can be defined as automatically
finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI , I  , A , AO , O  , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph represents a service in the
composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs
and pre-conditions of the service. The following conditions
should hold on the nodes of the graph:
1. ∀i Si  V where Si has exactly one incoming edge that
represents the query inputs and pre-conditions, I  
∪i Ii , CI ⇒ ∧i CIi .
2. ∀i Si  V where Si has exactly one outgoing edge that
represents the query outputs and post-conditions, O  
∪i Oi , CO ⇐ ∧i COi .
3. ∀i Si  V where Si has at least one incoming edge,
let Si1 , Si2 , . . . , Sim be the nodes such that there is a
directed edge from each of these nodes to Si . Then, Ii 
∪k Oik ∪ I  , CIi ⇐ (COi1 ∧ COi2 . . . ∧ COim ∧ CI ).
Figure 6 depicts an instance of non sequential composition.

Table 3 Non-sequential composition example
Service

Input parameters

Query

BookTitle, CreditCardNum,
AuthorName, CardType

Pre-conditions

Output parameters

Post-conditions

ConfNumber

GetlSBN

BookName, AuthorName

ConfNumber

GetAvailability

BooklSBN

NumAvailable

NumAvailable > 0

Authorize CreditCard

CreditCardNum

AuthCode

AuthCode > 99 ∧
AuthCode < 1000

PurchaseBook

BooklSBN,
NumAvailable, AuthCode

NumAvailable > 0

ConfNumber

123

118

SOCA (2016) 10:111–133

Fig. 6 Non-sequential composition

Example (Non-sequential conditional composition) Nonsequential conditional composition consists of if-then-else
conditions, i.e., the composition flow varies depending on
the result of the post-conditions of a service. Suppose we are
looking for a service to make international travel arrangements. We first need to make a tentative flight and hotel
reservation and then apply for a visa. If the visa is approved,
we can buy the flight ticket and confirm the hotel reservation, else we will have to cancel both the reservations. Also,
if the visa is approved, we need to make a car reservation.
The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table 4 shows the
input/output parameters of the user query and services. In this
example, service ProcessVisa produces the post-condition
VisaApproved ∨ VisaDenied. The services ConfirmFlight and
ConfirmHotel have the pre-condition VisaApproved. In this
case, one cannot determine whether the post-conditions of

service ProcessVisa implies the pre-conditions of services
ConfirmFlight and ConfirmHotel until the services are actually executed. In such a case, a condition can be generated
which will be evaluated at runtime and depending on the
outcome of the condition, the corresponding services will be
executed. The vertex for service ProcessVisa in the graph
is followed by a condition node which represents the postcondition of service ProcessVisa. This node has two outgoing
edges one representing the case if the condition is satisfied at
run-time and other edge for the case where the condition is not
satisfied. In other words, these edges represent the generated
conditions which in this case are, (VisaApproved ∨ VisaDenied) ⇒ VisaApproved and VisaApproved ∨ VisaDenied) ⇒
VisaDenied. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed. Figure 7 shows this conditional composition example
as a directed acyclic graph.

Definition (Generalized Composition) The generalized
Composition problem can be defined as automatically finding
a directed acyclic graph G = (V, E) of services from repository R, given query Q = (CI , I  , A , AO , O  , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming edge
of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:

Table 4 Non sequential conditional composition example
Service

Pre-conditions

Input parameters

Output parameter

Query

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

FlightConfirmationNum,
HotelConfirmationNum,
CarConfirmationNum

ReserveFlight

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

FlightConfirmationNum,
ArrivalFlightNum

ReserveHotel

PassengerName, ArrivalFlightNum,
StartDate, ReturnDate

HotelConfirmationNum,
HotelAddress

ProcessVisa

PassengerName, VisaType,
FlightConfirmationNum,
HotelConfirmationNum

ConfirmationNum

ConfirmFlight

VisaApproved

FlightConfirmationNum, CreditCardNum

FlightConfirmationNum

ConfirmHotel

VisaApproved

HotelConfirmationNum, CreditCardNum

HotelConfirmationNum

CancelFlight

VisaDenied

FlightConfirmationNum, PassengerName

CancelCode

Cancel Hotel

VisaDenied

HotelConfirmationNum, PassengerName

CancelCode

PassengerName, ArrivalDate,
HotelAddress, ArrivalFlightNum

CarConfirmationNum

ReserveCar

123

Post-conditions

VisaApproved
V VisaDenied

SOCA (2016) 10:111–133

119

Fig. 7 Non-sequential conditional composition

1. ∀i Si  V where Si has exactly one incoming edge that
represents the query inputs and pre-conditions, I  
∪i Ii, CI ⇒ ∧i CIi .
2. ∀i Si  V where Si has exactly one outgoing edge that
represents the query outputs and post-conditions, O  
∪i Oi, CO ⇐ ∧i COi .
3. ∀i Si  V where Si represents a service and has at least
one incoming edge, let Si1 , Si2 , ..., Sim be the nodes such
that there is a directed edge from each of these nodes to
Si . Then, Ii  ∪k Oik ∪ I  , C Ii ⇐ (COi1 ∧ COi2 . . . ∧
COim ∧ CI ).
4. ∀i Si  V where Si represents a condition that is evaluated
at run-time and has exactly one incoming edge, let S j
be its immediate predecessor node such that there is a
directed edge from S j to Si . Then, the inputs and preconditions at node Si are Ii = O j ∪ I  ; CIi = CO j .
The outgoing edges from Si represent the outputs that
are same as the inputs Ii and the post-conditions that are
the result of the condition evaluation at run-time.

The meaning of the  is the subsumption (subsumes) relation
and ⇒ is the implication relation. In other words, a service
at any stage in the composition can potentially have as its
inputs all the outputs from its predecessors as well as the
query inputs. The services in the first stage of composition
can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should
contain all the outputs that the query requires to be produced.
Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next
stage. When it cannot be determined at compile time whether
the post-conditions imply the pre-conditions or not, a conditional node is created in the graph. The outgoing edges of
the conditional node represent the possible conditions that
will be evaluated at run-time. Depending on the condition
that holds, the corresponding services are executed. That is,
if a subservice S1 is composed with subservice S2 , then the

post-conditions CO1 of S1 must imply the preconditions CI2
of S2 . The following conditions are evaluated at run-time:
if (CO1 ⇒ CI2 ) then execute S1;
else if (CO1 ⇒ ¬ CI2 then no-op};
else if (CI2 then execute S1 };
When the number of nodes in the graph is equal to one,
the composition problem reduces to the discovery problem.
When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem
reduces to a sequential composition problem. Further details
and examples are available in our prior work [15].
3.3 Requirements of an ideal engine
The features of an ideal discovery/composition engine are as
follows:
Correctness One of the most important requirements for
an ideal engine is to produce correct results, i.e., the services
discovered and composed by it should satisfy all the requirements of the query. Also, the engine should be able to find
all services that satisfy the query requirements.
Minimal query execution time Querying a repository of
services for a requested service should take a reasonable
amount of (minimal) time, i.e., a few milliseconds. Here, we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental updates Adding or updating a service to an
existing repository of services should take minimal time.
An ideal discovery and composition engine should not preprocess the entire repository again; rather incrementally
update pre-processed data (indexes, etc.) with data for the
new service.

123

120

Cost function If there are costs associated with every service in the repository, then an ideal discovery and composition engine should be able to provide results based on requirements (minimize, maximize, etc.) over the costs. We can
extend this to services having an associated attribute vector, and the engine should be able to provide results based on
maximizing or minimizing functions over the attribute vector.
These requirements have driven the design of our semanticsbased discovery and composition engine described in this
paper.
3.4 Centrality measure in social networks
Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds.
It involves measuring the formal and informal relationships to
understand information/knowledge flow that binds the interacting units that could be a person, group, organization, or any
knowledge entity. Social Network Analysis has an increasing application in social sciences that has been applied to
diverse areas such as psychology, health, electronic communications, and business organization. In order to understand
social networks and their participants, the location of an actor
in a network is evaluated. The network location is measured
in terms of centrality of a node that gives an insight into the
various roles and groupings in a network. Centrality gives
a rough indication of the social power of a node based on
how well they “connect” the network. There has been extensive discussion in the Social Network community regarding the meaning of the term centrality when it is applied to
Social Networks. One view stems directly from graph theory [9]. The graph-theoretic conception of compactness has
been extended to the study of Social Networks and simply
renamed “graph centrality”. Their measures are all based
upon distances between points, and all define graphs as centralized to the degree that their points are all close together.
The alternative view emerged from substantive research on
communication in Social Networks. From this perspective,
the centrality of an entire network should index the tendency
of a single point to be more central than all other points in
the network. Measures of a graph centrality of this type are
based on differences between the centrality of the most central point and that of all others. Thus, they are indexes of the
centralization of the network [36]. The three most popular
individual centrality measures are Degree, Betweenness, and
Closeness Centrality.
• Degree centrality The network activity of a node can be
measured using the concept of degrees, i.e., the number
of direct connections a node has. In the example, network
shown in Fig. 10 and Table 5, Provider D has the most
direct connections in the network, making it the most

123

SOCA (2016) 10:111–133
Table 5 Degree centrality of nodes in Fig. 10
Service provider

Degree

Provider A

2

Provider B

3

Provider C

1

Provider D

8

Provider E

3

Provider F

4

Provider G

5

Provider H

3

Provider I

1

Provider J

2

Provider K

4

Provider L

1

Provider M

active node in the network. In personal Social Networks,
the common thought is that “the more connections, the
better”.
• Betweenness centrality Though Provider D has many
direct ties, Provider H has fewer direct connections (close
to the average in the network). Yet, in many ways,
Provider H has one of the best locations in the network
by playing the role of a “broker” between two important
components. A node with high betweenness has greater
influence over what flows and does not in the network.
• Closeness centrality Provider F and G have fewer connections than Provider D, yet the pattern of their direct
and indirect ties allow them to access all the nodes in the
network more quickly than anyone else. They have the
shortest paths to all others, i.e., they are close to everyone
else. They are in an excellent position and have the best
visibility into what is happening in the network.

Individual network centralities provide insight into the individual’s location in the network. The relationship between
the centralities of all nodes can reveal much about the overall network structure.
3.5 Trust rating of a service and trust threshold
The trust rating of each service in the repository is computed
as a measure of the degree centrality (CD) of the social network to which the service provider belongs. It is calculated
as the degree or count of the number of adjacencies for a
node, sk :
C D (sk ) =

n
i−0

a (si , sk )

SOCA (2016) 10:111–133

where
a(si sk ) = 1 iff si and sk are connected by a line
0 otherwise
As such it is a straightforward index of the extent to which
sk is a focus of activity [9]. C D (sk ) is large if service provider
sk is adjacent to, or in direct contact with, a large number of
other service providers, and small if sk tends to be cut off
from such direct contact. C D (sk ) = 0 for a service provider
that is totally isolated from any other point. Our algorithm
filters out any services whose provider has a zero degree
centrality in a social network, i.e., such services will not
be used in building composition solutions. Trust rating of
the entire composite service is computed as an average of
the individual trust ratings of the services involved in the
composition. We also need to set a Trust Threshold and any
service with a Trust rating that is below this threshold is not
used while generating composition solutions. In our initial
prototype implementation, we set the Trust threshold to zero,
i.e., degree centrality of the service provider in the network
is zero. A service provider or service provider organization
that is not connected to any other nodes in the Social network
is not known to anyone else and is an immediate reason to be
pruned out from composition solutions, as the service cannot
be trusted. Composition solutions can be ranked such that
solutions with highest trust rating appear on top of the list.
4 Dynamic Web service composition: methodology
In this section, we describe our methodology for automatic
Web service composition that produces a general directed
acyclic graph. The composition solution produced is for the
generalized composition problem presented in Sect. 3. We
also present our algorithm for automatic generation of OWLS descriptions for the new composite service produced.
4.1 Algorithms for Web service discovery and composition
Our approach is based on a multi-step narrowing of the list of
candidate services using various constraints at each step. As
mentioned earlier, discovery is a simple case of Composition.
When the number of services involved in the composition
is exactly equal to one, the problem reduces to a discovery
problem. Hence, we use the same engine for both discovery and composition. We assume that a directory of services
has already been compiled, and that this directory includes
semantic descriptions for each service. In our implementation, we use semantic descriptions written in USDL [37],
although the algorithms are general enough that they will
work with any semantic annotation language. The repository of services contains one USDL description document
for each service. However, we still need a query language to

121

search this directory, i.e., we need a language to frame the
requirements of the service that an application developer is
seeking. USDL itself can be used as such as a query language. A USDL description of the desired service can be
written (with tool assistance), a query processor can then
search the service directory for a “matching” service. For
service composition, the first step is finding the set of composable services. USDL itself is used to specify the requirements of the composed service that an application developer
is seeking. Using the discovery engine, individual services
that make up the composed service can be selected. Part substitution techniques [38] can be used to find the different parts
of a whole task and the selected services can be composed
into one by applying the correct sequence of their execution. The correct sequence of execution can be determined
by the pre-conditions and post-conditions of the individual
services. That is, if a subservice S1 is composed with subservice S2 , then the post-conditions of S1 must imply the
pre-conditions of S2 . The goal is to derive a single solution,
which is a directed acyclic graph of services that can be composed together to produce the requested service in the query.
Figure 8 shows a pictorial representation of our composition
engine.
4.2 Multi-step narrowing solution
To produce a composite service, as shown in the example
Fig. 2, our algorithm filters services that are not useful for
the composition at multiple stages. Figure 9 shows the filtering technique for the particular instance graph represented in
Fig. 2. The composition routine begins with the query input
parameters and finds all those services from the repository
that require a subset of the query input parameters. In Fig. 9,
CI, I are the pre-conditions and the input parameters provided
by the query. S1 and S2 are the services found after step 1. O1
is the union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous
stage, i.e., I2 = O1 ∪ I. I2 is used to find services at the next
stage, i.e., all those services that require a subset of I2 . To
make sure we do not end up in cycles, we get only those services that require at least one parameter from the outputs produced in the previous stage. This filtering continues until all
the query output parameters are produced. At this point, we
make another pass in the reverse direction to remove redundant services that do not directly or indirectly contribute to
the query output parameters. This is done starting with the
output parameters and working our way backwards. Next,
another level of filtering is performed using the trust ratings
of services. Table 6 shows the algorithm and we have prototype implementation of this algorithm implemented using
Prolog [39] with Constraint Logic Programming over finite
domain (CLP(FD)) [40]. The rationale behind the choice of

123

122

SOCA (2016) 10:111–133

Fig. 8 Composition
engine—design

Fig. 9 Multi-step narrowing solution
Table 6 Algorithm for multi-step narrowing
Multi-step Narrowing Algorithm
Algorithm: Composition
(Input QI - QueryInputs, QO - QueryOutputs, QCI - Pre-Cond, QCO - Post-Cond, T - TrustThreshold)
(Output: Result - ListOfServices)
1. L ⇐ NarrowServiceList(QI, QCI);
2. O ⇐ GetAllOutputParameters(L);
3. CO ⇐ GetAllPostConditions(L);
4. While Not (O  QO)
5. I = QI ∪ O; CI ⇐ QCI ∧ CO;
6. L ⇐ NarrowServiceList(I, CI);
7. End While;
8. IntResult ⇐ RemoveRedundantServices(QO, QCO);
9. Result ⇐ RemoveRedundantServices(T, IntResult);
10. Return Result;

CLP(FD), implementation details, and experimental results
are available [15].
4.3 Automatic generation of OWL-S descriptions
After obtaining a composition solution (sequential, nonsequential, or conditional), the next step is to produce a
semantic description document for this new composite service. This document can be used for execution of the service

123

and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of
performing the composition process all over again. We used
the existing language OWL-S [31] to describe composite services. OWL-S models services as processes and when used to
describe composite services, it maintains the state throughout
the process. It provides control constructs such as Sequence,
Split-Join, If-Then-Else and many more to describe composite services. These control constructs can be used to describe

SOCA (2016) 10:111–133
Table 7 Generation of composite service description
Generation of Composite Service Description
Algorithm: GenerateCompositeServiceDescription
(Input: G - CompositionSolutionGraph)
(Output: D - CompositeServiceDescription)
1. Generate generic header constructs
2. Start Composite Service element
3. Start SequenceConstruct
4. If Number(SourceVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each starting/source Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct

123

rently. The process completes execution only when all the
services in this construct have completed their execution.
The non-sequential conditional composition can be described
in OWL-S using the If-Then-Else construct which specifies
the condition and the services that should be executed if the
condition holds and also specifies what happens when the
condition does not hold. Conditions in OWL-S are described
using SWRL. There are other constructs such as looping constructs in OWL-S that can be used to describe composite services with complex looping process flows. We are currently
investigating other kinds of compositions with iterations and
repeat-until loops and their OWL-S document generation.
We are exploring the possibility of unfolding a loop into a
linear chain of services that are repeatedly executed. We are
also analyzing our choice of the composition language and
looking at other possibilities as part of our future work.

End If
5. If Number(SinkVertices) = 1

5 Implementation and experimental results

GenerateAtomicService
Else StartSplitJoinConstruct
For Each ending/sink Vertex V
GenerateAtomicService

This section presents implementation details of the composition engine. We also analyze the performance and present
experimental results.

End For
EndSplitJoinConstruct
End If
6. For Each remaining vertex V in G
If V is AND vertex with one outgoing edge
GenerateAtomicService
If V is AND vertex with > 1 outgoing edge
GenerateSplitJoinConstruct
If V is OR vertex with one outgoing edge
GenerateAtomicService
qquad If V is OR vertex with > 1 outgoing edge
GenerateConditionalConstruct
End For
7. End SequenceConstruct
8. End Composite Service element
9. Generate generic footer constructs

the kind of composition. OWL-S also provides a property
called composedBy using which the services involved in the
composition can be specified. Table 7 shows the algorithm for
generation of the OWL-S document when the composition
solution in the form of a graph is provided as the input.
A sequential composition can be described using the
Sequence construct that indicates that all the services inside
this construct have to be invoked one after the other in the
same order. The non-sequential composition can be described
in OWL-S using the Split-Join construct which indicates that
all the services inside this construct can be invoked concur-

5.1 Implementation
Our discovery and composition engine is implemented using
Prolog [39] with Constraint Logic Programming over finite
domain [40], referred to as CLP(FD) hereafter. In our current
implementation, we used semantic descriptions written in
the language called Universal Semantics-Service Description
Language (USDL) [37]. The repository of services contains
one USDL description document for each service. USDL
itself is used to specify the requirements of the service that an
application developer is seeking. USDL is a language that service developers can use to specify formal semantics of Web
services. In order to provide semantic descriptions of services, we need an ontology that is somewhat coarse-grained
yet universal, and at a similar conceptual level to common
real-world concepts. USDL uses WordNet [41] which is a
sufficiently comprehensive ontology that meets these criteria. Thus, the “meaning” of input parameters, outputs, and
the side effect induced by the service is given by mapping
these syntactic terms to concepts in WordNet [38] for details
of the representation. Inclusion of USDL descriptions thus
makes services directly “semantically” searchable. However,
we still need a query language to search this directory, i.e., we
need a language to frame the requirements on the service that
an application developer is seeking. USDL itself can be used
as such a query language. A USDL description of the desired
service can be written, a query processor can then search the
service directory for a “matching” service. These algorithms
can be used with any other Semantic Web service descrip-

123

124

SOCA (2016) 10:111–133

A USDL description of the desired service can be written,
which is read by the query reader and converted to a triple.
This module can be easily extended to read descriptions written in other languages.
(iii) Semantic relations generator

Fig. 10 A social network of Web service providers

tion language as well. It will involve extending our implementation to work for other description formats, and we are
looking into that as part of our future work. The parsing of
all the USDL description documents and the universal ontology is written in Java. The parsing is done via SAXReader
library of Java and after the parsing, prolog engine is instantiated to run the Composition query processor. The complete
discovery and composition engine is implemented as a Web
service in Java using Apache Tomcat. The Web service in
turn invokes Prolog to do all the processing [42–44]. The
high-level design of the Discovery and Composition engines
is shown in Fig. 10. The software system is made up of the
following components:
(i) Triple generator
The triple generator module converts each service description
into a triple. In this case, USDL descriptions are converted
to triples like:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions)
The function symbol affect-type is the side effect of the
service and affected object} is the object that changed due
to the side effect. I is the list of inputs, and O is the list
of outputs. Pre-Conditions are the conditions on the input
parameters, and Post-Conditions are the conditions on the
output parameters. Services are converted to triples so that
they can be treated as terms in first-order logic and specialized
unification algorithms can be applied to obtain exact, generic,
specific, part and whole substitutions [38]. In case conditions
on a service are not provided, the Pre-Conditions and PostConditions in the triple will be null. Similarly, if the affecttype is not available, this module assigns a generic affect to
the service.

We obtain the semantic relations from the OWL WordNet
ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms,
hypernyms, meronyms, holonyms and many more. USDL
descriptions point to OWL WordNet for the meanings of concepts. A theory of service substitution is described in detail
in [38] which uses the semantic relations between basic concepts of WordNet to derive the semantic relations between
services. This module extracts all the semantic relations and
creates a list of Prolog facts. We can also use any other
domain-specific ontology to obtain semantic relations of concepts. We are currently looking into making the parser in this
module more generic to handle any other ontology written in
OWL.
(iv) Discovery query processor
This module compares the discovery query with all the services in the repository. The processor works as follows:
1. On the output parameters of a service, the processor
first looks for an exact substitutable. If it does not find
one, then it looks for a parameter with hyponym relation
[38], i.e., a specific substitutable.
2. On the input parameters of a service, the processor first
looks for an exact substitutable. If it does not find one,
then it looks for a parameter with hypernym relation
[38], i.e., a generic substitutable.
The discovery engine, written using Prolog with CLP(FD)
library, uses a repository of facts, which contains a list of
all services, their input and output parameters and semantic
relations between parameters. The code snippet of our engine
is shown in Table 8. The query is parsed and converted into
a Prolog query that looks as follows:
discovery(sol(queryService, ListOfSolutionServices).
The engine will try to find a list of SolutionServices that
match the queryService.
(v) Composition engine

(ii) Query reader
This module reads the query file and passes it on to the
Triple Generator. We use USDL itself as the query language.

123

The composition engine is written using Prolog with CLP(FD)
library. It uses a repository of facts, which contains all the
services, their input and output parameters and the semantic

SOCA (2016) 10:111–133

125

Discovery Algorithm

step narrowing-based approach to solve these problems and
implemented it using constraint logic programming.

discovery(sol(Qname,A)) :-

(i) Correctness

Table 8 Discovery Algorithm—Code Snippet

dQuery(Qname,I,O), encodeParam(O,OL),
/* Narrow candidate services(S) using output list(OL) */
narrowO(OL,S), fdset(S,FDs), fdsettolist(FDs,SL),
/* Expand InputList(I) using semantic relations */
getExtInpList(I, ExtInpList), encodeParam(ExtInpList,IL),
/* Narrow candidate services(SL) using input list (IL) */
narrowI(IL,SL,SA), decodeS(SA,A).

Our system takes into account all the services that can be
satisfied by the provided input parameters and pre-conditions
at every step of our narrowing algorithm. So our search space
has all the possible solutions. Our backward narrowing step,
which removes the redundant services, does so taking into
account the output parameters and post-conditions. So our
algorithm will always find a correct solution (if one exists)
in the minimum possible number of steps.

Table 9 Composition Algorithm—Code Snippet
Composition Algorithm
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs),
encodeParam(QueryOutputs, QO),
getExtInpList(QueryInputs, InpList),
encodeParam(InpList, QI),
performForwardTask(QI, QO, LF),
performBackwardTask(LF, QO, LR),
getMinSolution(LR, QI, QO, A), reverse(A, RevA),
confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

relations between the parameters. A code snippet of our composition engine is shown in Table 9. The query is converted
into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate “bagof” to return all possible ListOfServices that can be composed to get the requested
queryService.
(vi) Output generator
After the Composition engine finds a matching service, or
the list of atomic services for a composed service, the results
are sent to the output generator in the form of triples. This
module generates the output files in any desired XML format.
5.2 Efficiency and scalability issues
In this section, we discuss the salient features of our system
with respect to the efficiency and scalability issues related
to Web service discovery and composition problem. It is
because of these features that we decided on the multi-

(ii) Pre-processing
Our system initially pre-processes the repository and converts all service descriptions into Prolog terms. The semantic relations are also processed and loaded as Prolog terms
in memory. Once the pre-processing is done, then discovery
or composition queries are run against all these Prolog terms
and, hence, we obtain results quickly and efficiently. The
built-in indexing scheme and constraints in CLP (FD) facilitate the fast execution of queries. During the pre-processing
phase, we use the term representations of services to set up
constraints on services and the individual input and output
parameters. This further helped us in getting optimal results.
(iii) Execution efficiency
The use of CLP (FD) helped significantly in rapidly obtaining
answers to the discovery and composition queries. We tabulated processing times for different size repositories, and
the results are shown in the next section. As one can see,
after pre-processing the repository, our system is quite efficient in processing the query. The query execution time is
insignificant.
(iv) Programming efficiency
The use of Constraint Logic Programming helped us in coming up with a simple and elegant code. We used a number of
built-in features such as indexing, set operations, and constraints and, hence, did not have to spend time coding these
ourselves. This made our approach efficient in terms of programming time as well. Not only the whole system is about
200 lines of code, but we also managed to develop it in less
than 2 weeks.
(v) Scalability
Our system allows for incremental updates on the repository,
i.e., once the pre-processing of a repository is done, adding

123

126

a new service or updating an existing one will not need reexecution of the entire pre-processing phase. Instead, we can
easily update the existing list of CLP (FD) terms loaded in
the memory and run discovery and composition queries. Our
estimate is that this update time will be negligible, perhaps
a few milliseconds. With real-world services, it is likely that
new services will get added often or updates might be made
on existing services. In such a case, avoiding repeated preprocessing of the entire repository will definitely be needed
and incremental updates will be of great practical use. The
efficiency of the incremental update operation makes our system highly scalable.

SOCA (2016) 10:111–133
Table 10 Sample service interface description
Sample WSDL description
<message name =“InputName”>
<part name = “part0” type = “Name”/>
</message >
<message name =“OutputAddress”>
<part name = “part0” type = “US-Address” />
</message>
<portType name = “AdressConverter”>
<operation name =“Convert” >
<input message =“InputName” />
<output message = “OutputAddress”/>

(vi) Use of external database
In case the repository grows extremely large in size, then saving off results from the pre-processing phase into some external database might be useful. This is part of our future work.
With extremely large repositories, holding all the results of
pre-processing in the main memory may not be feasible. In
such a case, we can query a database where all the information is stored. Applying incremental updates to the database is easily possible, thus avoiding recomputation of preprocessed data.
(vii) Searching for optimal solution
If there are any properties with respect to which the solutions
can be ranked, then setting up global constraints to get the
optimal solution is relatively easy with the constraint-based
approach. For example, if each service has an associated cost,
then the discovery and the composition problem can be redefined to find the solutions with the minimal cost. Our system
can be easily extended to take these global constraints into
account.
5.3 Performance and experimental results
To conduct our experiments, we looked at various benchmarks and Web services challenge (WSC) datasets [42,43]
best suited our needs and fit well into the overall architecture with minimal changes. They provided semantics through
XML schema, provided queries and corresponding solutions.
We used repositories from WSC website [42,43], slightly
modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The input
and output messages of the services may contain multiple
parameters. Each parameter is annotated with a semantic
concept stored in the attribute type. Table 10 shows a service AddressConverter with one operation named Convert.
It can be invoked with an input message (InputName) and
produces a response message (OutputAddress). The value of

123

</operation >
</ portType >

the attribute message represents a reference to a message element. Each message has a set of part elements as children,
which represent the service parameters, annotated with concepts referenced by the type-attribute. The Convert operation
in this example requires a parameter of the type Name and
returns an instance of US-Address.
The queries and solutions are provided in an XML format. The semantic relations between various parameters are
provided in an XML Schema format. Concepts were treated
as data types and taxonomies encoded as hierarchies of such
data types in XSD schemas. The subsumes relation between
two semantic concepts can be compared to the subclass relationship in Object-oriented programming. Table 11 shows
a sample XSD schema defining the data types Address and
US-Address inheriting from Address. In the context of the
WSC, this schema would be interpreted as a taxonomy introducing the concepts Address and US-Address with subsumes(Address, US-Address).
We evaluated our approach on different size repositories
and tabulated Pre-processing, Query Execution, and Incremental update time. We noticed that there was a significant
difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data)
on the same repository. What we found is that the repository
was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 12
shows performance results of our Composition algorithm on
discovery queries, and Table 13 shows the results of our algorithm on composition queries. The times shown in tables are
the wall clock times. The actual CPU time to pre-process
the repository and execute the query should be less than or
equal to the wall clock time. The experiments were conducted on different repository sizes as well as varying number of input and output parameters for each service in the
repository. The results are plotted in Figs. 11 and 12 where

SOCA (2016) 10:111–133

127

Table 11 Sample XSD Schema providing semantics
Sample XSD Schema
<complexType name =“ Address ” >
<sequence >
<element name = “name” type = “string” minOccurs = “0”/>
<element name = “street” type = “string” />
<element name = “city” type = “string” />
</ sequence>
</complexType>
<complexType name = “US - Address”>
<complexContent >
<extension base = “Address” >
<sequence>
<element name = “state” type = “US - State”/>
<element name = “zip” type = “positiveInteger”/>
</sequence>
</extension>
</complexContent>

Fig. 11 High-level design on composition engine

</complexType>

Table 12 Performance on discovery queries
Pre-processing
time (ms)

Incremental
Query
execution update (ms)
time (ms)

Repository
size

Number
of I/O
parameters

2,000

4–8

36.5

1

18

2,000

16–20

45.8

1

23

2,000

32–36

57.8

2

28

2,500

4–8

47.7

1

19

2,500

16–20

58.7

1

23

2,500

32–36

71.6

2

29

3,000

4–8

56.8

1

19

3,000

16–20

77.1

1

26

3,000

32–36

88.2

3

29

Table 13 Performance on composition queries
Pre-processing
I/O time (ms)

Incremental
Query
execution update (ms)
time (ms)

Repository
size

Number
of I/O
parameters

2,000

4–8

36.1

1

18

2,000

16–20

47.1

1

23

2,000

32–36

60.2

1

30

3,000

4–8

58.4

1

19

3,000

16–20

60.1

1

20

3,000

32–36

102.1

1

34

4,000

4–8

71.2

1

18

4,000

16–20

87.9

1

22

4,000

32–36

129.2

1

32

the numbers of I/O parameters were 4–8, 16–20, and 32–36,
respectively. The graphs exhibit behavior consistent with our
expectations: for a fixed repository size, the pre-processing
time increases with the increase in number of input/output
parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the
repository. However, what is surprising is the efficiency of
service query processing, which is negligible (just 1–3 ms)
even for complex queries with large repositories.
Discussion
We evaluated our approach for correctness, efficiency, and
scalability of the software. The correctness of the algorithm
has been described in Sect. 4. In the experiments conducted,
discovery and composition solutions obtained were checked
against a pre-defined set of solutions produced for the WSChallenge datasets [43]. This was the first and most important
criterion for evaluation of the engine. Our second criterion
was query efficiency, i.e., rapidly obtaining answers to discovery and composition queries. The results show that irrespective of the repository size the query execution time was
always 1–2 ms. This was because the repositories were preprocessed, and the queries were run against pre-processed
data. Different repository sizes were used along with varying the number of input and output parameters of the Web
services. The software also scaled well with varying the size
of the repositories. These three criterion are important for
the successful adoption of the composition engine to produce a Software-as-a-Service (SaaS) platform for automatic
workflow generation in a specific domain as described in the
sample scenario in Sect. 1. An important part of scalability
is the ability to sustained performance even if pre-processed

123

128

SOCA (2016) 10:111–133

Fig. 12 Performance on discovery queries

repositories change in size, new services are added, existing
services are removed, etc. To evaluate this aspect, we studied the time taken for incremental updates to the repository.
The results obtained were all less than one second for different repository sizes and well as varying number of input
and output parameters. A network of service providers was
introduced synthetically into the experimental datasets. Trust
rating of services was computed based on this network. Trust
ratings generator helps with filtering services based on the
trust threshold and helps in ranking. The performance of the
engine on discovery and composition queries still remains
the same.

6 Application to bioinformatics
We illustrate the practicality of our general framework for
automatically composing services by applying it to phylogenetics, a subfield of bioinformatics, for automatic generation
of workflows. In this section, we present a brief description
of the field of Phylogenetics [8] followed by an example
of a workflow generation problem that can be mapped to a
non-sequential conditional composition problem (the most
general case of the composition problem) and can be solved
using our generalized composition engine.
6.1 Phylogenetics
Phylogenetic inferencing involves an attempt to estimate the
evolutionary history of a collection of organisms (taxa) or
a family of genes [45]. The two major components of this
task are the estimation of the evolutionary tree (branching
order), then using the estimated trees (phylogenies) as analytical framework for further evolutionary study and finally
performing the traditional role of systematics and classification. Using this study, a number of interesting facts can be
discovered, for example, who are the closest living relatives
of humans, who are whales related to, etc. Different studies
can be conducted, for example, studying dynamics of microbial communities, predicting evolution of influenza viruses
and other applications such as Drug Discovery, and vaccine

123

development, etc. In order to perform these tasks, scientists
use a number of software tools and programs already available. They use them by putting them together in a particular
order (i.e., a workflow) to get their desired results. That is, it
involves execution of a sequence of steps using various programs or software tools. These tools use different data formats, and hence translating from one data format to another
becomes necessary.
6.2 Automatic workflow generation
The software tools and programs created for specific phylogenetic tasks use different data formats for their input and output parameters. The data description language Nexus [46,47]
is used as a universal language for representation of these
bioinformatics related data. There are translator programs
that convert different formats into Nexus and vice versa. For
example, one could use the BLAST program to get a sequence
set of genes. Once the sequence set is obtained, the sequences
can be aligned using the CLUSTAL program. But the output
from BLAST cannot be directly fed to CLUSTAL, as their
data formats are different. The translator can be used to convert the BLAST format to Nexus and then the Nexus format
to CLUSTAL. In order to perform an inferencing task, one
has to manually pick all the appropriate programs and corresponding format translators and put them in the correct order
to produce a workflow. We show how Web service composition can be directly applied to automate this task of producing a workflow. MyGrid [48] has a wealth of bioinformatics
resources and data that provides opportunities for research.
It has hundreds of biological Web services and their WSDL
descriptions, provided by various research groups around the
world. We illustrate our generalized framework for Web service composition by applying it to these services to generate
workflows automatically that are practically useful for this
field.
Example Workflow Generation (Non-sequential Composition) Suppose we are looking for a service that takes a
GeneInput and produces its corresponding AccessionNumbers, AGI, and GeneIdentifier as output. The directory of
services contains CreateMobyData, MOBYSHoundGetGen-

SOCA (2016) 10:111–133

129

Table 14 Bioinformatics application—non-sequential composition example
Service

Input parameters

Output parameters

Query

Genelnput

AccessionN umbers,
AGIj Geneldentity

CreateMoby Data

Genelnput

MobyData

MobyData

GeneSequence

MOBYSHoundGet
GenBankWbatev
erSequence

Pre-conditions

Format (MobyData) =
NCBI

MlPSBlastBetterE13

GeneSequence

WUGene Sequence

Extract Accession

GeneSequence

AccessionNumbers

ExtractBestHit

WUGeneSequence

AGI, Geneldentity

Post-conditions

Format (MobyData) = NCBI

Fig. 13 Performance on composition queries

Bank WhateverSequence, ExtractAccession, ExtractBestHit,
MIPSBlastBetterE13 services. In this scenario, the GeneInput first needs to be converted to NCBI data format and then
its corresponding GeneSequence is further passed to ExtractAccession and ExtractBestHit to obtain the AccessionNumbers, AGI, and GeneIdentity respectively. Table 14 shows the
input/output parameters of the user query and the services.
Figure 13 shows this non-sequential composition example as
a directed acyclic graph. In this example:
• Service CreateMobyData has a post-condition on its output parameter MobyData that the format is NCBI and service MOBYSHoundGetGenBankWhateverSequence has
a pre-condition that its input parameter MobyData has
to be in NCBI format for service execution. The postcondition of CreateMobyData must imply pre-condition
of MOBYSHoundGetGenBankWhateverSequence service.
• Both services ExtractAccession and ExtractBestHit have
to be executed to obtain the query outputs.
• The semantic descriptions of the service input/output
parameters should be the same as the query parameters
or have the subsumption relation. This can be inferred
using semantics from the ontology provided.
Example Workflow Generation (Non-sequential Conditional
Composition) Suppose we are looking for a service that takes

a GeneSequence and produces an EvolutionTree and EvolutionDistance after performing a phylogenetic analysis. Also
the service should satisfy the post-condition that the EvolutionTree produced is in the Newick format. This involves producing a sequence set first, followed by aligning the sequence
set and then producing the evolution tree and evolution distance. Also any necessary intermediate data format translations have to be performed. Table 15 lists the services in
the repository and their corresponding input/output parameters and user query. For the sake of simplicity, the query and
services have fewer input/output parameters than the realworld services. In this example, service BLAST has to be
executed first so that its output BLASTSequenceSet can be
used as input by CLUSTAL after the data format has been
translated using BLASTNexus and NexusCLUSTAL. The service BLASTNexus has a post-condition that the format of
the output parameter NexusSequenceSet is Nexus which is
the pre-condition of the next service NexusCLUSTAL. Similarly the service NexusCLUSTAL has a post-condition that the
format of the output parameter ClustalSequenceSet is Clustal
that is the pre-condition of the next service CLUSTAL. At
every step of composition, the post-conditions of a service
should imply the pre-conditions of the following service. The
post-condition of the service CLUSTAL is that the output
parameter AlignedSequenceSet has either Paup or Phylip format. Depending on which one of these two conditions hold,

123

130

SOCA (2016) 10:111–133

Table 15 Bioinformatics application—non-sequential conditional composition example
Service

Input parameter

Outpur parameter

Post-conditions

Query

Sequence,
OrganismType, Word
Size, DatabaseName

EvolutionTree,
EvolutionDistance

Format (EvolutionTree)
= Newick

BLAST

Sequence, Organism
Type, DatabaseName

BlastSequenceSet

Clustal SequenceSet

AlignedSequenceSet

Format (AlignedSequence
Set) = Paup ∨
Forrmat(AlignedSequence
Set) = Phylip

BlastSequenceSet

NexusSequenceSet

Format (NexusSequenceSet)
= Nexus

CLUSTAL

Pre-conditions

Format (Clustal
SequenceSet) = Clustal

BLASTNexus
NexusCLUSTAL

Format (Nexus
SequenceSet) = Nexus

NexusSeqtienceSet

ClustalSequenceSet

Forma(ClustalSequenceSet)
= Clustal

PAUP

Format (Aligned
SequenceSet) = Paup

AlignedSequenceSet,
Word Size

EvolutionTree

Format (EvolutionTree)
= Newick

PHYLIP

Format (Aligned
SequenceSet) = Phylip

AlignetBeciuenceSet

EvokiSonTree

Format (EvolutionTree)
= Newick

MEGA

Format (Aligned
Sequenced) = Paup

AlignedSequenceSet

EvaluationDistance

Fig. 14 Bioinformatics
application—non sequential
composition as a directed
acyclic graph

the next service for composition is chosen. In this case, one
cannot determine whether the post-conditions of the service
CLUSTAL imply pre-conditions of PAUP or PHYLIP until
the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and
depending on the outcome of the condition, corresponding
services will be executed.
The vertex for service CLUSTAL in Fig. 14 has an outgoing
edge to a conditional node. The outgoing edge represents the
outputs and post-conditions of the service. The conditional
node has multiple outgoing edges that represent the generated
conditions that are evaluated at run-time. In this case, the
following conditions are generated:
• (Format(AlignedSequenceSet) = Paup ∨
Format(AlignedSequenceSet) = Phylip) ⇒
(Format(AlignedSequenceSet) = Paup)
• (Format(AlignedSequenceSet) = Paup ∨
Format(AlignedSequenceSet) = Phylip) ⇒
(Format(AlignedSequenceSet) = Phylip)

123

Depending on the condition that holds, the corresponding
services PAUP and MEGA or PHYLIP are executed respectively. The outputs EvolutionTree and EvolutionDistance are
produced in both the cases along with the post-condition
that the format of the evolution tree is Newick. Figure 14
shows this non-sequential conditional composition example
as a conditional directed acyclic graph.
6.3 Implementation
In order to apply service composition to obtain the sequence
of tasks automatically, these programs have to be made available as Web services and their descriptions should be provided using one of the Web services description languages
like WSDL (Web Services Description Language) or USDL
(Universal Service-Semantics Description Language). The
translator programs also have to be available as Web services. Then, we can write a query specifying the input parameters provided and the output parameters that have to be
obtained. The Composition engine then looks up the repository of available services and finds the solution, i.e., a set

SOCA (2016) 10:111–133

131

Fig. 15 Bioinformatics
application—non sequential
conditional composition as a
directed acyclic graph

Fig. 16 Composition Solution
1 for query in Table 15

of services that can be executed to obtain the requested output parameters. These set of services obtained will need only
those input parameters that are provided by the query and
their execution produces the output parameters specified by
the query. We tested our Composition engine on a repository
of services that had descriptions of Web services corresponding to the following programs:

1. BLAST: This service compares protein sequences to
sequence databases and calculates the statistical significance of matches. It query’s a public database of generic
information like GenBank and GSDB and produces a
molecular sequence. It takes in Sequence, DatabaseName, OrganismType, SelectionOptions, MaxTargetSequences, ExpectedThreshold, WordSize as input parameters and produces BlastSequenceSet as the
output.
2. CLUSTAL: This service produces multiple sequence
alignment for DNA or proteins. It takes in ClustalSequenceSet as input parameter and produces ClustalAlignedSequenceSet as the output.
3. PHYLIP: This service is used for inferring phylogenies. It analyzes molecular sequences and infers phylogenetic information. It takes in PhylipAlignedSequenceSet, UseThresholdParsimony, UseTransversionParsimony as input parameters and produces NewickEvolutionTree as the output.
4. PAUP: This service is used for inferring phylogentic
trees. It analyzes molecular sequences and infers phylogenetic information. It takes in PaupAlignedSequenceSet as input parameter and produces NewickEvolutionTree as the output.
5. BLASTNexus: This service takes input in BLAST format and converts it into Nexus format. It takes in BLAST-

Table 16 Bioinformatics application—workflow query
Service

Input parameters

Output parameters

Query1

Sequence, DatabaseName,
OrganismType,
SelectionOptions,
MaxTargetSequences,
ExpectedThreshold,
UserTransversion Parsimony,
UseThresholdParsimony,
WordSize

NewickEvoIutionTree

SequenceSet as input and produces NexusSequence
Set.
6. NexusCLUSTAL: This service takes input in Nexus
format and converts it into CLUSTAL format. It takes
in NexusSequenceSet as input parameter and produces
CLUSTALSequenceSet as the output.
7. CLUSTALNexus: This service takes input in CLUSTAL
format and converts it into Nexus format. It takes in
CLUSTALAlignedSequenceSet as input parameter and
produces NexusAlignedSequenceSet as the output.
8. NexusPAUP: This service takes input in Nexus format and converts it into PAUP format. It takes in
NexusAlignedSequenceSet as input parameter and produces PaupAlignedSequenceSet as the output.
9. NexusPHYLIP: This service takes input in Nexus format and converts it into PHYLIP format. It takes in
NexusAlignedSequenceSet as input parameter and produces PhylipAlignedSequenceSet} as the output.
10. MEGA: This service is used for Molecular Evolutionary Genetics Analysis. It takes in MEGAInp as
input parameter and produces MEGASequence as the
output.

123

132

SOCA (2016) 10:111–133

Fig. 17 Composition Solution
2 for query in Table 15

11. KEPLER: This service provides scientific workflows. It
takes in KEPLERData as input parameter and produces
KEPLERSequence as the output.
The composition engine can automatically discover a complex workflow based on the query requirements, from a large
repository without having to analyze all the programs manually. Figure 15 shows the non-sequential conditional composition for query specified in Table 16 as a directed cyclic
graph. Figures 16 and 17 show the solutions obtained for the
query specified in Table 16.
A task that had to be performed manually by biologists
whenever they had to make phylogenetic inferences can now
be done automatically with Web services Composition. The
programs have to be made available as Web services and
their descriptions provided. Once we have a repository of
such services, the composition engine can be used as shown
above to automatically generate workflows.
7 Conclusions and future work
Due to the growing number of services on the Web, we need
automatic and dynamic Web service composition in order to
utilize and reuse existing services effectively. It is also important that the composition solutions obtained can be trusted.
Our semantics-based approach uses semantic description of
Web services to find substitutable and composite services that
best match the desired service. Given semantic description
of Web services, our engine produces optimal results (based
on number of services in the composition). The composition
flow is determined automatically without the need for any
manual intervention. Our engine finds any sequential, nonsequential or non-sequential conditional composition that is
possible for a given query and also automatically generates
OWL-S description of the composite service. This OWLS description can be used during the execution phase and
subsequent searches for this composite service will yield a
direct match. A trust rating is computed for every service in
the repository based on the degree centrality of the service
provider in a known social network. Currently, we are in
the process of testing the trust-based dynamic Web service
composition engine in a complete operational setting and
running experiments to measure the quality of composition
results obtained. We will also explore the other measures of
centrality such as betweenness centrality and closeness centrality and analyze the possibility of using a combination of

123

all three measures of centrality to compute trust rating of
a service provider. We are able to apply many optimization
techniques to our system so that it works efficiently even on
large repositories. The strengths of this engine is the minimal
query execution time that is achieved through pre-processing
of repositories and incremental updates to the pre-processed
data whenever a service is added, removed, or modified. Use
of Constraint Logic Programming helped greatly in obtaining
an efficient implementation of this system and made it easy to
incorporate non-functional parameters for ranking of results.
The limitations of the engine include trust aspect of the Web
services involved in a composition solution. A model that
provides a trust rating to services or service providers would
improve confidence in the generated solutions. Also, when
working with domains such as Bioinformatics where software systems involved in a workflow need to be converted
into services, generation of semantics of the inputs and outputs of the Web services is a challenge. They have to be
manually assigned semantics by a domain expert.
Our future work includes investigating other kinds of compositions with loops such as repeat-until and iterations and
their OWL-S description generation. Analyzing the choice
of the composition language (e.g., BioPerl [49] for phylogenetic workflows) and exploring other language possibilities
is also part of our future work. We are also exploring combining technologies of automated service composition and
domain-specific languages to develop a framework for problem solving and software engineering.

References
1. Castagna G, Gesbert N, Padovani L (2008) A theory of contracts
for web services. ACM SIGPLAN Not 43:261–272
2. Bansal A, Patel K, Gupta G, Raghavachari B, Harris ED, Staves
JC (2005) Towards intelligent services: a case study in chemical
emergency response. In: IEEE International conference on web
services (ICWS)
3. McIlraith SA, Son TC, Zeng H (2001) Semantic web services.
IEEE Intell Syst 16(2):46–53
4. Mandell DJ, McIlraith SA (2003) Adapting BPEL4WS for the
semantic web: the bottom-up approach to web service interoperation. In: The semantic web-ISWC. Springer 2003, pp 227–241
5. Paolucci M, Kawamura T, Payne TR, Sycara K (2002) Semantic matching of web services capabilities. In: The semantic Web–
ISWC. Springer 2002, pp 333–347
6. Rao J, Dimitrov D, Hofmann P, Sadeh N (2006) A mixed initiative approach to semantic web service discovery and composition:
SAP’s guided procedures framework. In: International conference
on web services. ICWS’06, 2006, pp 401–410

SOCA (2016) 10:111–133
7. Cardoso J, Sheth AP (2006) Semantic web services, processes and
applications. Springer, Berlin
8. Edwards AWF, Cavalli-Sforza LL (1964) Reconstruction of evolutionary trees, systematics association publication number 6, No.
Phenetic and Phylogenetic Classification, pp 67–76
9. Freeman LC (1979) Centrality in social networks conceptual clarification. Social Netw 1(3):215–239
10. Wasserman SF (1994) Social network analysis: methods and applications. Cambridge University Press, Cambridge
11. Tamura K, Peterson D, Peterson N, Stecher G, Nei M, Kumar S
(2011) MEGA5: molecular evolutionary genetics analysis using
maximum likelihood, evolutionary distance, and maximum parsimony methods. Mol Biol Evol 28(10):2731–2739
12. Thompson JD, Higgins DG, Gibson TJ (1994) CLUSTAL W:
improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties
and weight matrix choice. Nucleic Acids Res 22(22):4673–4680
13. Edgar RC (2004) MUSCLE: multiple sequence alignment with
high accuracy and high throughput. Nucleic Acids Res 32(5):1792–
1797
14. Badr Y, Caplat G (2010) Software-as-a-service and versionology:
towards innovative service differentiation. In: 24th IEEE international conference on advanced information networking and applications (AINA), 2010, pp 237–243
15. Kona S, Bansal A, Blake MB, Gupta G (2008) Generalized
semantics-based service composition. In: IEEE international conference on web services (ICWS), pp 219–227
16. Rao J, Su X (2005) A survey of automated web service composition
methods. In: Cardoso J, Sheth A (eds) Semantic web services and
web process composition. Springer, Berlin, pp 43–54
17. Srivastava B, Koehler J (2003) Web service composition-current
solutions and open problems. In: ICAPS 2003 workshop on planning for web services, vol 35, pp 28–35
18. McIlraith S, Son TC (2002) Adapting golog for composition of
semantic web services. KR 2:482–493
19. Pistore M, Roberti P, Traverso P (2005) Process-level composition
of executable web services: on-the-fly versus once-for-all composition. In: Gomez-Perez A, Euzenat J (eds) The semantic web:
research and applications. Springer, Berlin, pp 62–77
20. Boustil A, Maamri R, Sahnoun Z (2013) A semantic selection
approach for composite web services using OWL-DL and rules.
Serv Oriented Comput Appl 8:1–18
21. Claro DB, Albers P, Hao JK (2005) Selecting web services for
optimal composition. In ICWS international workshop on semantic
and dynamic web processes, Orlando-USA
22. Suvee D, De Fraine B, Cibrán MA, Verheecke B, Joncheere N,
Vanderperren W (2005) Evaluating FuseJ as a web service composition language. In: Third IEEE European conference on web
services (ECOWS)
23. Dong W, Jiao L (2008) QoS-aware Web service composition based
on SLA. In: Fourth international conference on natural computation
(ICNC) vol 5, pp 247–251
24. Yan J, Kowalczyk R, Lin J, Chhetri MB, Goh SK, Zhang J (2007)
Autonomous service level agreement negotiation for service composition provision. Future Gener Comput Syst 23(6):748–759
25. Wada H, Champrasert P, Suzuki J, Oba K (2008) Multiobjective
optimization of SLA-aware service composition. In: IEEE congress
on services-Part I, pp 368–375
26. Blake MB (2007) Decomposing composition: service-oriented
software engineers. IEEE Softw 24(6):68–77
27. Zeng L, Benatallah B, Ngu AH, Dumas M, Kalagnanam J, Chang
H (2004) QoS-aware middleware for web services composition.
IEEE Trans Softw Eng 30(5):311–327

133
28. Cardoso J, Sheth A, Miller J, Arnold J, Kochut K (2004) Quality
of service for workflows and web service processes. Web Semant
1(3):281–308
29. Zhao X, Shen LW, Peng X, Zhao W (2013) Finding preferred
skyline solutions for SLA-constrained service composition. In:
2013 IEEE 20th international conference on web services (ICWS),
pp 195–202
30. Feng Y, Ngan LD, Kanagasabai R (2013) Dynamic service composition with service-dependent QoS attributes. In: 2013 IEEE 20th
international conference on web services (ICWS), pp 10–17
31. “OWL-S”. [Online]. http://www.w3.org/Submission/OWL-S/.
(Accessed: 22-Jan-2014)
32. Wen S, Tang C, Li Q, Chiu DK, Liu A, Han X (2014) Probabilistic
top-K dominating services composition with uncertain QoS. Serv
Oriented Comput Appl 8(1):91–103
33. Immonen A, Pakkala D (2014) A survey of methods and approaches
for reliable dynamic service compositions. Serv Oriented Comput
Appl 8(2):129–158
34. Mehdi M, Bouguila N, Bentahar J (2013) A QoS-based trust
approach for service selection and composition via Bayesian networks. In: 2013 IEEE 20th international conference on web services (ICWS), pp 211–218
35. Kuter U, Golbeck J (2009) Semantic web service composition in
social environments. Springer, Berlin
36. Leavitt HJ (1951) Some effects of certain communication patterns
on group performance. J Abnorm Soc Psychol 46(1):38
37. Bansal A, Kona S, Simon L, Mallya A, Gupta G, Hite TD (2005) A
universal service-semantics description language. In: Third IEEE
European conference on web services (ECOWS), pp 214–225
38. Kona S, Bansal A, Simon L, Mallya A, Gupta G (2009) USDL: a
service-semantics description language for automatic service discovery and composition. Int J Web Serv Res 6(1):20–48
39. Sterling L, Shapiro EY, Warren DH (1986) The art of Prolog:
advanced programming techniques. MIT Press, Cambridge
40. Marriott K, Stuckey PJ (1998) Programming with constraints: an
introduction. MIT Press, Cambridge
41. “RDF/OWL Representation of WordNet”. [Online]. http://www.
w3.org/TR/wordnet-rdf/. (Accessed: 23-Jan-2014)
42. Blake MB, Cheung W, Jaeger MC, Wombacher A (2006) WSC-06:
the web service challenge. In: The 3rd IEEE international conference on E-commerce technology, 2006. The 8th IEEE international
conference on enterprise computing, E-commerce, and E-services,
pp 62–62
43. Blake MB, Cheung WKW, Jaeger MC, Wombacher A (2007)
WSC-07: Evolving the web services challenge. In: The 9th
IEEE international conference on E-commerce technology and
the 4th IEEE international conference on enterprise computing,
E-commerce, and E-services, 2007. CEC/EEE 2007, pp 505–508
44. Kona S, Bansal A, Gupta G, Hite D (2007) Automatic composition of semantic web services. In: International conference on web
services (ICWS), vol 7, pp 150–158
45. Felsenstein J (2004) Inferring phylogenies, vol 2. Sinauer Associates, Sunderland
46. Iglesias JR, Gupta G, Pontelli E, Ranjan D, Milligan B (2001) Interoperability between bioinformatics tools: a logic programming
approach. In: Practical aspects of declarative languages. Springer,
Berlin, pp 153–168
47. Maddison DR, Swofford DL, Maddison WP (1997) NEXUS:
an extensible file format for systematic information. Syst Biol
46(4):590–621
48. “myGrid”. [Online]. http://www.mygrid.org.uk/
49. “BioPerl”. [Online]. http://www.bioperl.org/wiki/Main_Page

123

2017 IEEE 11th International Conference on Semantic Computing

Information Retrieval from a Structured KnowledgeBase
Avani Chandurkar

Ajay Bansal

Arizona State University
Mesa, Arizona, USA
ajchandu@asu.edu

Arizona State University
Mesa, Arizona, USA
ajay.bansal@asu.edu

Abstract—With the inception of World Wide Web, the
amount of data present on the internet is tremendous. This
makes the task of navigating through this enormous amount of
data quite difficult for the user. As users struggle to navigate
through this wealth of information, the need for the development
of an automated system that can extract the required
information becomes urgent. This paper presents a Question
Answering system to ease the process of information retrieval.
Question Answering systems have been around for quite some
time and are a sub-field of information retrieval and natural
language processing. The task of any Question Answering system
is to seek an answer to a free form factual question. The difficulty
of pinpointing and verifying the precise answer makes question
answering more challenging than simple information retrieval
done by search engines. The research objective of this paper is to
develop a novel approach to Question Answering based on a
composition of conventional approaches of Information Retrieval
(IR) and Natural Language processing (NLP). The focus is also
on exploring the use of a structured and annotated knowledge
base as opposed to an unstructured knowledge base. The
knowledge base used here is DBpedia and the final system is
evaluated on the Text REtrieval Conference (TREC) 2004
questions dataset.

the users have to go through all the questions to determine the
one which matches their query. Hence, overall it is a very time
consuming and unintuitive process. Web search engines are
another popular way of searching for information on the
internet but the onus is still on the users to find the exact
information that they need from returned results. Another form
of searching information on internet is querying the
information present in online databases. This requires users to
be familiar with a formal query language like SQL. So, for a
normal user the most natural way to query is using human
language. It also has the added advantage of specifying exactly
what user is expecting. Question Answering systems are one
such effort to present the user with a direct answer to the
question, as opposed to a bunch of ranked documents
containing the answer. There are many question answering
systems ranging from LUNAR [10], which answered questions
about the geological analysis of rocks returned from the Apollo
missions to Apple’s Siri [2], which is a computer program that
acts as an intelligent personal assistant. All these question
answering systems aim to present the users with the required
information by searching the resources they have access to in
their collections of databases. The difficulty of verifying and
pinpointing the correct answer makes the task of question
answering a hard problem. The huge amount of data currently
present on the internet increases the difficulty of the task at
hand. The format of the knowledge base used is pivotal in the
efficiency of the question answering system.
From an IR perspective, the question answering task can be
viewed as a task of returning specific pieces of information as
answer. This approach works well for giving long answers to
the asked questions. But, it is not sufficient for extracting
specific fact-based answers. NLP techniques analyze the
syntactic and semantic structures of the sentence and makes an
attempt to “understand” the sentence. Named Entity
Recognition is a very useful NLP technique which helps in
identifying specific entities in a given sentence. Hence, the
combination of powerful retrieval mechanisms and
understanding of natural language present a robust approach to
question answering. Structured knowledgebase does not have
just raw text and documents. It has some additional mark up or
information about the data present. This additional information
helps in locating the required information easily.

Keywords— Structured Semantic Data, Information Retrieval,
Natural Language Processing, Question Answering System.

I.

INTRODUCTION

Question Answering (QA) is a discipline of natural language
processing and information retrieval, which involves
developing a system that provides an answer to a question
asked by the user in natural language. The primary function of
QA system is to generate the answer to the posed question by
querying a knowledgebase which contains information
pertaining to the user’s question. The database can be either
unstructured, consisting of a collection of documents in
English or it can be a more structured form of database known
as knowledgebase. Question answering systems can be broadly
classified into two domains: open domain QA systems and
closed domain QA systems.
A. Motivation
As the amount of data available on the internet is vast, it
becomes difficult for the user to navigate through all this
information to retrieve the desired knowledge. As a result, a lot
of research is focused on improving the ease of retrieval of the
data. Frequently Asked Questions (FAQ’s) are the most
traditional form of question answering on the internet. But,
there are two big disadvantages of using this approach. Firstly,
the users do not get to ask the question explicitly and secondly,
978-1-5090-4284-5/17 $31.00 © 2017 IEEE
DOI 10.1109/ICSC.2017.95

B. Problem Statement
The objective of this research is to develop a novel and robust
approach to question answering based on composition of
conventional approaches of Information Retrieval and Natural

407

Language Processing. The system will be evaluated using a
structured knowledgebase, as opposed to using textual data as a
knowledge resource. The specific research questions are (i)
Does having a structured knowledgebase aid in developing a
question answering system? (ii) Does the format of annotations
matter while using a structured knowledgebase?
The rest of the paper is organized as follows: Section II
presents background and related work. Section III presents the
proposed QA System. Section IV describes the implementation
of a web application and a running example. Section V
describes the experimental results and analysis. Finally, we
present conclusions and future work.
II.

with when, where, who, whom and why the class of question
is easily determined by direct mapping. For more ambiguous
questions, like those beginning with what, which etc. focus of
the question is given as input to WordNet [8]. The additional
context provided by WordNet is then used as a pointer to
determine the question type. Li and Roth describe a machine
learning approach used for question classification. They have
defined their goal as, “to categorize questions into different
semantic classes based on the possible semantic types of the
answers”. In their work, they have developed a two-layered
semantic hierarchy of answer types [9]. BASEBALL [1] and
LUNAR [10], were two of the earliest question answering
systems which queried a structural database using natural
language questions. The questions were processed and
translated into a formal query to extract answers from the
databases. ELIZA [11] was the first program to make a natural
language conversation with computers possible. Input
statements in natural language were analyzed based on a set of
decomposition rules. The responses were triggered based on
some of the keywords detected in the input statement. Katz,
Borchardt and Felshin employ a technique called Natural
Language annotations to match questions to candidate answers
[3]. These natural language annotations serve to add some
structure to the underlying knowledgebase. The information
resources added to knowledgebase is mostly done manually
i.e. whenever new information source has to be incorporated
to the existing knowledgebase, the natural language
annotations are most often composed manually and then
linked to various other parallel information content.
Natural language annotations are a new and innovative
technique used here which makes it possible to index the
information resources. McGowan discussed his interpretation
of the question answering as an information retrieval problem
that transforms user’s questions into formal search queries
[12]. Barskar et al. discuss their approach for extraction based
on pattern learning. Their work is focused on finding patterns
to formulate “complete and natural” answers to questions,
given the short answers [13].
In conclusion, the field of Question Answering has seen
immense growth and interest in the previous decade. It can be
concluded from this study of approaches and existing QA
systems that most researches in the field were heterogeneous
in terms of their system architecture and/or approaches. In this
paper we present a QA system that focuses on combination of
techniques of Information Retrieval and Natural Language
Processing used against a Structured Knowledgebase.

BACKGROUND AND RELATED WORK

A. Question Answering Systems
Question Answering (QA) systems have been around for quite
some time and have gained widespread use due to its
applications and promising results. All these systems aim to
present the user with a direct and precise answer to their
question. QA systems are a combination of various fields like
Natural Language Processing, Information Retrieval and
Information Extraction. There has been great number of
breakthroughs in the field beginning from BASEBALL [1] to
the most advanced systems that we use today like Siri [2].
This combination of user demand and promising results has
encouraged work and research in the field of question
answering. QA systems can be broadly classified into two
main categories, namely open domain QA and closed domain
QA. The medium of communication is also a factor in
categorizing the QA systems. The system can be text based,
where the user has to type in a question and get a written
textual answer or it can be voiced controlled, where the user
can speak the question into it and have the system read back
the answer in natural language. START [3], Wolfram Alpha
[4] etc. are examples of text based QA systems, and Siri [2],
Google Now are both voice based QA systems. Text REtrieval
Conference (TREC) offers a question-answering track since
1999 [5], which tests system’s ability to answer short factoid
questions. The type of questions that a typical QA system can
answer is factual (for example, Where was Barack Obama
born?) or list (for example, List the countries in the continent
of Asia.) questions. But recently research trend is going in the
direction of addressing more complex questions. Factual or
list questions are pretty straightforward as the system has to
find the appropriate answer in the information resources
present. A more complex type of questions beginning with
‘why’ (for example, Why do rainbows form?), which involves
building a reasoning model.

III.

PROPOSED QUESTION ANSWERING SYSTEM

The QA system consists of three modules, namely question
processing, information retrieval or document processing and
answer extraction that are described in detail in this section.
Figure 1 shows the high-level architecture.

B. Related work in QA systems
QA systems typically involve 3 major steps: Question
Classification, Information Retrieval and Answer Extraction
[6]. Cooper and Ruger [7] use a pattern mapping technique to
determine the focus and class of the question. Their approach
involves classifying the question based on the interrogative
word in the question. For straightforward questions beginning

A. KnowledgeBase
Typically any QA system consults a “resource” to search and
extract answers. There are many resources used by the QA

408

systems like the World Wide Web (WWW), document corpus,
or databases, etc. QA systems can be broadly classified into
two categories i.e. Open domain question answering (ODQA)
and restricted domain question answering (RDQA). Being
domain-specific, RDQA systems typically use knowledge
encoded in databases as the knowledge resource. And these
systems can provide answers concerning the knowledge
previously added in the database. Hence, research in RDQA
mainly focuses on incorporation of domain-specific
information into databases for the QA systems to query [14].
For RDQA, the knowledge resources are specifically
constructed by the domain experts and then incorporated into
the QA system. The advent of question answering track in
TREC [5] led to advances in open domain question answering.
ODQA systems generally use a large text database like the
World Wide Web as their knowledge resource.
Knowledgebase can also be classified into unstructured, semistructured and structured.

It is used to determine the “focus” of the question. Along with
POS tags, the Stanford universal dependencies between each
word in the question are also retrieved. Hence, the combined
knowledge from the classifier and parser is used to make sense
of the question.
Question Classification: A question, when understood correctly
places some constraints on a possible answer and determining
those constraints becomes an important task for any QA
system. Hence, to enhance accuracy of the system, filtering out
unsuitable candidate answers plays an important part. This is
done by Question Classification (QC). QC tasks determines a
type of the question which narrows down the type of the
answer the question is looking for. For example, consider the
question from the TREC 2004 dataset “Where was James Dean
born?” For this particular question, the question classification
task involves classifying this question into a category of
“location”. This eliminates all the possible candidate answers
which are not locations. Hence, the question classification does
two important things. Firstly, it places constraints on the
answer type. And secondly, it provides additional information
about the question which can be used further in answer
selection strategies.

Figure 1 High-level System Overview

Table 1 Example questions and corresponding class types

Python-based Factoid Question Classifier: An open source
Python factoid question classifier that uses machine learning
approach for question classification. It is a hierarchical
classifier which semantically parses the question and classifies
it into different semantic classes based on the possible semantic
type of answers. This classifier does not classify questions
which calls for an action. It only addresses questions like
‘What’, ‘Which’, ‘Who’, ‘When’, ‘Where’ and ‘Why’ questions which ask for a simple fact. It consists of 6 coarse
classes (ABBREVIATION, DESCRIPTION, ENTITY,
HUMAN, LOCATION and NUMERIC VALUE) and they are
further classified into a set of 50 non-overlapping fine classes
[9]. Table 1 shows some example questions and their
corresponding coarse class type retrieved from the Python
Factoid Question classifier.
Stanford Dependency Parser: Stanford CoreNLP toolkit [16]
is used for parsing the question. It is an open source Java
implementation that provides Part-of-speech tagging and
Syntactic Parsing functionalities. The main noun and verb
provide pointers to what the question is asking for. Consider
the previous example, “When was James Dean born?” The
main noun is “James Dean” and the verb is “born”. This gives
the system an idea that the question is pertaining to the “birth
of James Dean”. Hence, syntactic parsing of the question is
the second important step in processing of the question. Figure
3 shows the POS tags and parse tree returned by the Stanford
for the above-mentioned question. The detected noun phrase
and verb is used to determine the “focus” of the question.

Figure 2 Question processing

B. Question Processing
Question can be defined as, “a natural language sentence
which usually starts with an interrogative word and expresses
some information need of the user” [15]. To answer the
question, firstly the system needs to understand what the
question is asking for. Hence processing, tagging and parsing
the question to make sense out of it is the first step towards
development of a good question answering system. The flow of
question processing in this system is shown in Figure 2. Firstly,
the question is given to a Python factoid question classifier
developed by Li and Roth [9] that determines the type of
question, as well as type of the answer that is expected. After
the class of the question is determined, the question is parsed
using the Stanford Dependency parser and the part-of-speech
(POS) tagging is retrieved. The main noun and verb in the
question gives helpful pointers to what the question is asking.

409

sentence that may contain the answer. After choosing the
sentence, a combination of Named Entity Recognition and
simple heuristic rules are used to extract the answer.

Figure 3: Stanford Dependency
Parsing

Figure 4: Stanford Universal
Dependencies

The parser also returns a set of Universal dependencies, which
are nothing but grammatical relations between the words in a
sentence. Figure 4 shows the returned Universal Dependencies
for our running example. It gives specific grammatical
relations between almost every word in the question. The
dependency “advmod” namely adverb modifier associates
main verb “born” with interrogative word “when”.
“compound” dependency forms “James Dean” as one
compound phrase, and “nsubjpass” further associates the noun
phrase “James Dean” with main verb “born”. Hence, by
connecting “when”, “James Dean” and “born” dependencies
provide indication towards what the question is asking for.
The combination of information received from the Question
Classifier and the Stanford Dependency parser gives us the
overall analysis of the question that is used in subsequent
processing modules.

Figure 5: Overview of KnowledgeBase Processing Module

D. Answer Extraction
There are two different modules within the Answer Extraction
module: SPARQL query generation and Named Entity
Recognition. Initially, tags are processed and if the highest
scored tag does not have a score of zero, this highest ranked tag
is passed over to the SPARQL query generation sub-module.
Whereas, if the highest score is zero then the control passes
over to the Abstract parsing module. This module selects one
sentence from the abstract which has the maximum probability
of containing the answer, again based on a rank assigned to
each sentence. If the highest rank is not zero, then this sentence
passes over to the Named Entity Recognition sub-module.
Answer Extraction is a sub-area of Question Answering which
specifically aims at accurately pinpointing the exact answer in
the retrieved information [18]. From the previous module, we
have received relevant information in the form of a most
probable tag or a sentence to work with. The task of the answer
extraction module is to extract the correct answer from this
information. SPARQL query generation involves generating a
query in a formal language and using it to retrieve the answer,
follows the IR paradigm. Whereas, parsing a sentence written
in natural language and performing operations to extract the
exact answer from it follows the NLP paradigm.

C. KnowledgeBase Processing
We use DBpedia [17] as our knowledgebase to retrieve data
relevant to the information received from the questionprocessing module. The two main tasks in this module are
Tags Processing and Abstract Parsing. Figure 5 shows the
overview of the knowledgebase query. DBPedia has data in
the form of key-value pairs, where the key is called label or a
tag. The tag is manually created and assigned to the particular
value and can be seen as a simple manual annotation to the
data. Hence, the first step in this module is processing these
tags to find the tag representing information that the question
is demanding. So, in tag processing all the tags corresponding
to a DBpedia page are retrieved, classified and ranked. A
ranking algorithm is used to rank the tags based on feature
matching. Basically, the ranking algorithm assigns score to
each tag based on number of features that match between the
tag and the question. The highest ranked tag is selected and
the answer is extracted from DBpedia by the means of a
SPARQL query. If there are no relevant tags on the DBpedia
page, or none of the features match between the tags and the
question, then the abstract parsing module is invoked.
DBpedia also has a long and short abstract associated with
each page. The system retrieves the corresponding short
abstract and it is parsed. The Stanford Universal Dependencies
and pattern matching techniques are used to pick a probable

IV.

WEB APPLICATION IMPLEMENTATION

The system is developed as a stand-alone web application that
is hosted on the Apache Tomcat Server local server. This web
application has a Java backend. The User Interface is designed
using HTML and JavaScript. The Factoid Question classifier
[9] is developed in Python. The Java backend communicates
with the question classifier giving it arguments and executing
it on the command line. Apache Jena framework is used to
query the DBpedia from the Java backend. Figure 6 shows the
screenshot for the example question “Where is its
headquarters?” Here, the target subject in question is “AARP”.
The user selects the subject and the question from the User
Interface (UI). The information is passed over to the first

410

questions provide a context to the current question. The dataset
was designed with the view of the questioner being an English
speaking adult, and an average reader trying to find more
information about a term he/she encountered while reading.
The final TREC 2004 questions dataset consists of 65 targets,
out of which 23 are people, 25 are organizations and 17 are
things. The TREC 2004 series contains a total of 286 factoid
questions. Figure 8 depicts sample questions series from the
TREC 2004 dataset in which series 3 has a thing for a target,
series 21 has an organization and series 22 has a person for a
target. All these questions and the associated target are encoded
in a XML document.

module which is, question processing. Here, the question is
parsed and its POS tags and Stanford Universal dependencies
are retrieved. The question is classified and its corresponding
question class type is retrieved. This information is passed
over to the knowledgebase processing module that first
transfers control to tags processing submodule. Here, the
subject “AARP” is used to access the particular DBpedia
page, and then all corresponding tags are retrieved by the
means of the JSON interface. Once all tags are retrieved, they
are classified and the tags matching the class of question are
taken ahead for further processing. These tags are then ranked
using the ranking algorithm. In this case, the highest ranked
tag is dbp:headquarters, with common noun and the stem of
common noun matching with the tag. As the highest ranked
tag is greater than 0, the tag is passed over to the SPARQL
query generation sub-module.

Figure 8: Sample Question series from TREC 2004 dataset

The system was tested against all the questions present in the
TREC 2004 questions dataset. The results in terms of accuracy
is presented in Table 2 and 3. Figure 9 depicts the overall
accuracy and the results of the system evaluated on the entire
TREC 2004 dataset. It depicts the breakdown of the questions
answered correctly and incorrectly by each module of the
system. Table 2 provides a breakdown of the questions
answered incorrectly by the system. Table 3 depicts the
number of questions answered correctly or incorrectly by the
two tracks used in the development of the system viz. tags
processing and abstract parsing.

Figure 6: Example screenshot of Web Application

Figure 7: Control flow for Processing

V.

RESULTS AND ANALYSIS

A. Results
Our Question Answering system was evaluated on the TREC
2004 dataset [5]. The objective of the TREC question
answering track is to encourage research in developing
question answering systems. TREC question datasets usually
contain fact-based, short-answer questions. The TREC 2004
dataset consists of a series of questions based on one particular
target, where the target can be a person, an organization or a
thing. Each question in the series asks for more information
about the target. The order in which the questions are asked to
the system is very important, as the target and the previous

Figure 9: Experimental Results
Table 2: Breakdown of questions not handled by the system

Table 3: Accuracy of tags processing v/s abstract parsing

411

B. Analysis
In order to analyze the results obtained, we revisit the research
questions. The first research question was, “Does having a
structured knowledgebase aid in developing a question
answering system?” From this prototype system, it can be
concluded that having a structured knowledgebase definitely
aids in the development of any question answering system.
Having a structured or even a semi-structured knowledgebase
eases the process of retrieval of data. It provides cues or hints
for the possible location of the answer, and hence saves a lot of
processing time which would otherwise take a long time. It
eliminates the process of finding the area or location where the
answer might be present, and narrows down the search field
just to the particular location or paragraph. The second research
question focusses on the method used for structuring the
knowledge resource. There are various ways in which the
knowledgebase can be structured and ‘annotations’ is one of
the most popular technique. The research question states,
“Does the format of the annotations matter while using a
structured knowledge resource?” After this research, it can be
stated that the format of the annotations does matter while
structuring the knowledge resource. In DBpedia, the tags or the
annotations are manual and done by different individuals. As
every individual has a different way of making up annotations,
they are not very uniform and are sometimes ambiguous.
Hence, the annotations which follow a particular format or
structure and are standard across the entire knowledgebase will
be more helpful than manually written annotations.
VI.

then assigning them specific roles. Semantic role labelling aims
at reducing ambiguity and giving a formal and standard
representation of different sentences which mean the same
thing. Another reason for some of the incorrect answers is that
DBpedia is not a complete knowledge resource. This can be
handled by incorporating or querying other structured
knowledge resources like YAGO or NELL. The system can
query these knowledgebases for those questions whose
answers are not present in DBpedia. Also, incorporation of
semantic parsing tools like word2vec will help in the abstract
parsing module, because the semantic parsing gives a better
understanding of the language than syntactic parsing.
VII. REFERENCES
[1] Green, B., Wolf, A., et al. “Baseball: An automatic Question
Answerer” in Proceedings of Western Computing Conference,
Vol. 19, pp. 219–224, 1961.
[2] Roush, W. “Siri, Apple’s New Old Personal Assistant App,
Points Toward A Voice -Activated Future”, April, 2010–2013.
[3] Katz, B. “Annotating the World Wide Web Using Natural
Language” in Proceedings of the 5th RIAO Conference on
Computer Assisted Information Searching on the Internet, 1997.
[4] Wolfram, S. (2007). Today, Mathematica Is Reinvented.
http://blog.stephenwolfram.com/2007/05/today-mathematica-isreinvented/ [Retrieved May 2016]
[5] Voorhees, E. M. “Overview of the TREC 2004 question
answering track” In Proceedings of TREC, 2004.
http://doi.org/10.1016/S0306-4573(99)00043-6
[6] Allam, A. M. N., & Haggag, M. H. “The question answering
systems: A survey” in Intl. Journal of Research and Reviews in
Information Sciences (IJRRIS), 2(3), 211–220, 2002.
[7] Cooper, R., & Ruger, S. “A simple question answering system”,
In Voorhees and Harman, 2000.
[8] Miller, G. WordNet: a lexical database for English.
Communications of the ACM, 38(11), 39–41, 1995.
[9] Li, X., & Roth, D. “Learning question classifiers: the role of
semantic information”. In Natural Language Engineering,
12(03), 229, 2006.
[10] Woods, W., Kaplan, R., & Nash-Webber, B. “The Lunar
Sciences Natural Language Information System: Final Report”,
BBN Report 2378, January 1972.
[11] Weizenbaum, J. “ELIZA — A Computer Program For the Study
of Natural Language Communication Between Man And
Machine” in Communications of the ACM, 9(1), 36–45, 1966.
[12] Mcgowan, K. (n.d.). “Emma : A Natural Language Question
Answering System” from www.umich. edu.
[13] Barskar, R., et al. “An approach for extracting exact answers to
QA system for english sentences” in Procedia Engineering, 30,
1187–1194, 2012.
[14] Mollá, D., & Vicedo, J. L. “Question Answering in Restricted
Domains: An Overview” in Computational Linguistics, 33(1),
41–61, 2007.
[15] Kolomiyets, O., & Moens, F. “A survey on question answering
technology from an information retrieval perspective” in
Information Sciences, 2011, 181(24), 5412–5434.
[16] Manning, C. D., et al. “The Stanford CoreNLP Natural
Language Processing Toolkit”.
[17] Bizer, C. , et al. “DBpedia - A crystallization point for the Web
of Data”, in Journal of Web Semantics, 2009, 7(3), 154–165.
[18] Wang, M. “A Survey of Answer Extraction Techniques in
Factoid
Question
Answering”,
2006.

CONCLUSION & FUTURE SCOPE

In this paper we presented a QA system that combines work
from fields of IR as well as NLP. The success of any QA
system is dependent on the underlying knowledge resource and
hence, having a structured and annotated knowledge resource
greatly aids in the process of development, as well as enhances
the performance of the system. There are various
enhancements that can be made to each of the modules and
techniques. Currently, our QA system depends on getting the
target subject and the question from the User Interface. The UI
is populated from the encoded XML document of the TREC
2004 dataset. The system can be made more generalized by
adding a text box which will enable the user to input their own
question. As the approach depends on using the ‘target’ subject
to access the particular DBpedia page, another routine can be
incorporated by using Stanford Dependency parser, to extract
the ‘focus’ or the ‘target’ subject in the question. A minor
limitation of the system is its dependency on the python factoid
Question Classifier [9]. Among the questions not being
answered correctly by the system, around 20% are because of
incorrect classification by the factoid question classifier.
Further enhancement could be a better trained question
classifier with more fine grained classification. Also, adding
semantic role labelling module while parsing the question can
be more helpful. Semantic role labelling, also known as
shallow semantic parsing, is a task which involves detecting
semantic arguments associated with a verb or a predicate and

412

2008 IEEE International Conference on Services Computing

Towards a General Framework for Web Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

Abstract

S2
S5
CI’,I’

Service-oriented computing (SOC) has emerged as
the eminent market environment for sharing and reusing
service-centric capabilities. The underpinning for an organization’s use of SOC techniques is the ability to discover
and compose Web services. In this paper we present a generalized semantics-based technique for automatic service
composition that combines the rigor of process-oriented
composition with the descriptiveness of semantics. Our
generalized approach extends the common practice of linearly linked services by introducing the use of a conditional
directed acyclic graph (DAG) where complex interactions,
containing control flow, information flow and pre/post conditions, are effectively represented.

S1

S4

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

representation. We present our approach that generates
most general compositions based on (conditional) directed
acyclic graphs (DAG). In our framework, the DAG representation of the composite service is reified as an OWL-S
description. This description document can be registered in
a repository and is thus available for future searches. The
composite service can now be discovered as a direct match
instead of having to look through the entire repository and
build the composition solution again.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered,
and consumed. A composite service is a collection of services combined together in some way to achieve a desired
effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [2]. Most efforts
reported in the literature focus on one or more of these four
phases. The first phase involves generating a plan, i.e., all
the services and the order in which they are to be composed
in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The
second phase involves discovering services as per the plan.
Depending on the approach, often planning and discovery
are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting
the optimal solution from the available potential solutions
based on non-functional properties like QoS properties. The
last phase involves executing the services as per the plan and
in case any of them are not available, an alternate solution
has to be used.
In this paper we formalize the generalized composition
problem based on our conditional directed acyclic graph

978-0-7695-3283-7/08 $25.00 © 2008 IEEE
DOI 10.1109/SCC.2008.134

CO’,O’

S3

2. Web service Composition
In this section we formalize the generalized composition
problem. In this generalization, we extend our previous
notion of composition [1] to handle non-sequential conditional composition (which we believe is the most general
case of composition). Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service
is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be
composed to obtain the desired service. Figure 1 shows an
example composite service made up of five services S1 to
S5 . In the figure, I  and CI  are the query input parameters
and pre-conditions respectively. O and CO are the query
output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and Sj indicates
that outputs of Si constitute (some of) the inputs of Sj .
Definition (Repository of Services): Repository (R) is a
set of Web services.

497

Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions,
I is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the list of
post-conditions. The pre- and post-conditions are ground
logical predicates.

tion can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services
in the next stage. When it cannot be determined at compile
time whether the post-conditions imply the pre-conditions
or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible
conditions which will be evaluated at run-time. Depending
on the condition that holds, the corresponding services are
executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply
the preconditions CI 2 of S2 . The following conditions are
evaluated at run-time:
if (CO1 ⇒ CI 2 ) then execute S1 ;
else if (CO1 ⇒ ¬ CI 2 ) then no-op;
else if (CI 2 ) then execute S1 ;

Definition (Query): The query service is defined as Q
= (CI  , I  , A , AO , O , CO ) where CI  is the list of preconditions, I  is the input list, A is the service affect, AO
is the affected object, O is the output list, and CO is the
list of post-conditions. These are all the parameters of the
requested service.
Definition (Generalized Composition): The generalized
Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI  , I  , A , AO , O , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming
edge of a node represents the inputs and pre-conditions of
the service. The following conditions should hold on the
nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming edge
that 
represents the query inputs and pre-conditions,
I   i I i , CI  ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing edge
that represents
the query outputs and post-conditions,

O  i Oi , CO ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has at
least one incoming edge, let Si1 , Si2 , ..., Sim be the
nodes such that there is a directed
 edge from each
of these nodes to Si . Then Ii  k Oik ∪ I  , CI i ⇐
(COi1 ∧COi2 ... ∧ COim ∧ CI  ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming
edge, let Sj be its immediate predecessor node such
that there is a directed edge from Sj to Si . Then the
inputs and pre-conditions at node Si are Ii = Oj ∪ I  ,
CI i = COj . The outgoing edges from Si represent
the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time.
The meaning of the  is the subsumption (subsumes) relation and ⇒ is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composi-

3. Automatic Generation of Composite Services
In order to produce the composite service which is the
graph, we filter out services that are not useful for the composition at multiple stages. The composition routine starts
with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. For the next stage, the inputs available are
the query input parameters and all the outputs produced by
the previous stage, i.e., I2 = O1 ∪ I. I2 is used to find services at the next stage, i.e., all those services that require
a subset of I2 . In order to make sure we do not end up in
cycles, we get only those services which require at least one
parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.

4. Conclusions and Future Work
To make Web services more practical we need a general framework for composition of Web services. The generalized approach presented in this paper can handle nonsequential conditional composition that can be used in automatic workflow generation in a number of applications.

References
[1] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007.
[2] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

498

Proceedings of the Twenty-Third International Florida Artificial Intelligence Research Society Conference (FLAIRS 2010)

Timed Planning
Ajay Bansal

Neda Saeedloei and Gopal Gupta

Department of Computer Science,
Georgetown University,
Washington, DC 20057.
Email: bansal@cs.georgetown.edu

Department of Computer Science,
The University of Texas at Dallas,
Richardson, TX 75080.
Email: {nxs048000, gupta}@utdallas.edu

2. A Real-time Action Description Lang.: AT

Abstract

The action description language A was proposed by Gelfond
and Lifschitz to represent actions and change using logic
programs (Gelfond and Lifschitz 1993). Real-time systems
are computing systems in domains where response within
a hard time bound is critical for success. The real-time action description language AT , which extends language A by
augmenting action with real-time constraints, is presented in
(Simon, Mallya, and Gupta 2005). AT deﬁnes a complete
real-time action α by pairing its name with a list of clock
constraints associated with it. In AT , this is written as
A at T1 , . . . , Tn
where T1 . . . , Tn (n ≥ 0) are clock constraints of the form
C ≤ E, C ≥ E, C < E, and C > E, where C and E
are clock names or a clock name plus or minus a real valued
constant. When n = 0 the at clause can be dropped.
With the ability to explicitly state when an action occurs, value propositions can be easily extended to include it.
Given ﬂuent expressions F1 , . . . , Fm (m > 0) and real-time
actions α1 , . . . , αn (n ≥ 0), a real-time value proposition
can be written as:

Planning has been at the forefront of research in the
areas of Artiﬁcial Intelligence and cognitive science.
High-level action description languages (e.g., language
A) have been used to specify, verify and diagnose plans.
Timed Planning is planning under real-time constraints.
To specify timed planning problems, an extension of
the action description language A with real-time stopwatches, called AT has been used. In this paper, we
show how timed planning domains (described in AT )
can be easily and elegantly encoded as answer set programs extended with constraints over reals.

1. Introduction
Planning has been an active area of research since the early
days of AI and cognitive science. In planning, a domain description D is given along with a set of observations about
the initial state O and a collection of ﬂuent literals G =
{g1 , . . . , gl }, which is referred to as a goal. The problem is to ﬁnd a sequence of actions a1 , . . . , an such that
∀i, 1 ≤ i ≤ l, D entails gi from initial state O, after actions
a1 , . . . , an . The sequence of actions a1 , . . . , an is called a
plan for goal G w.r.t. (D,O) (Baral 2003).
Timed Planning is planning under real-time constraints.
For real-time domains, the occurrence of an action is as important as the time at which the action occurs. One needs to
be able to reason about time in a quantitative manner, as the
system may have real-time constraints that must be satisﬁed
for an action to occur. In the ﬁeld of logic programming,
action description languages (e.g., language A - realized via
Answer Set Programming (ASP)) have been used to represent and reason about actions and change (Gelfond and Lifschitz 1993). These are (high-level) languages that can be
used to specify, verify and diagnose plans.They are widely
used for planning with domain speciﬁc constraints To encode timed planning problems, an extension of the action
description language A with real-time stop-watches, called
AT has been proposed (Simon, Mallya, and Gupta 2005).
However, the implementation of AT is quite ad hoc. In this
paper, we show how timed planning domains (described in
AT ) can be elegantly encoded as answer set programs extended with constraints over reals.

F1 , . . . , Fm after α1 ; . . . ; αn
When the sequence of actions is empty (n = 0), a real-time
value proposition is written as
initially F1 , . . . , Fm
The real-time effect propositions (sometimes referred to as
action rules) in AT are written as
A causes F1 , . . . , Fm resets C1 , . . . Cn
when T1 , . . . , Tk if P1 , . . . , Pi
for action name A, ﬂuent expressions F1 . . . , Fm ,
P1 , . . . , Pi (m, i ≥ 0), clock names C1 , . . . , Cn (n ≥ 0),
and clock constraints T1 , . . . , Tk (k ≥ 0), where m + n +
k + i > 0. As usual, when m, n, k, or i is zero the keywords causes, resets, when, or if respectively, can be
dropped. The resets clause speciﬁes clocks that are to be reset, assuming the when clause and ﬂuent preconditions are
satisﬁed. Clocks continue to advance, if they are not reset.
A special action wait that denotes the action of waiting for
time to elapse is also provided in AT . This action acts as a
wild-card that matches all other action names, and thus provides the ability to encode the passing of time in the current
state of the system.

c 2010, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

136

3. AT = ASP + CLP(R)

Table 1: Fragile Object Domain in language AT

AT programs can easily and elegantly be encoded as answer
set programs extended with constraints over reals. Given an
answer set program, constraints over reals can be embedded
within goals in the body of the rules, similar to how Horn
clause logic programs are extended with constraints. As the
body of these rules are evaluated in a goal-directed manner,
these constraints are posted in the constraint store as they
are encountered. Posting a constraint that is not entailed by
the constraint store will result in a failure which will cause
backtracking. Constraints over reals in logic programming
are realized in the CLP(R) system (Jaffar and Lassez 1987),
which we have incorporated in our goal-directed implementation of Answer Set Programming (ASP).

Drop causes ¬Holding, F alling
resets Clock if Holding, ¬F alling
Catch causes Holding, ¬F alling, ¬Broken
when Clock ≤ 1 if ¬Holding, F alling
wait causes Broken, ¬F alling
when Clock > 1 if ¬Holding, F alling

A causes F1 , . . . , Fl , ¬Fl+1 , . . . , ¬Fm
resets C1 , . . . , Cn when T1 , . . . , Tk
if P1 , . . . , Pu , ¬Pu+1 , . . . , ¬Pv

3.1 Example: Fragile Object Domain
Now we present a domain with real-time constraints on its
actions (Gelfond and Lifschitz 1993). The real-time Fragile
Object domain, extends the original example with the notion
that a dropped object can be caught before it hits the ground.
We assume the object takes 1 second to hit the ground. The
assumption that units are in seconds is merely a convention
we use in this example. In the language AT all clocks are
variables in the real number domain, i.e., they can take any
arbitrary real values. Figure 1 depicts the real-time Fragile
Object Domain.

is encoded in our framework as:
for each Fi (i = 1 . . . l), we deﬁne
holds(Fi , res(A, S)) :holds(P1 , S), . . ., holds(Pu , S),
not holds(Pu+1 , S), . . ., not holds(Pv , S),
T1 , . . ., Tn , C1 > 0, . . ., Cn > 0,
N ewC1 > C1 , . . ., N ewCn > Cn .
for each Fj (j = l + 1 . . . m), we deﬁne
not holds(Fj , res(A, S)) :holds(P1 , S), . . ., holds(Pu , S),
not holds(Pu+1 , S), . . ., not holds(Pv , S),
T1 , . . ., Tn , C1 > 0, . . ., Cn > 0,
N ewC1 > C1 , . . ., N ewCn > Cn .

drop
clock : 0
holding
falling
broken

holding
falling
broken

wait
clock > 1

holding
falling
broken

catch
clock < 1

For the ﬂuents that are true initially, we deﬁne the holds and
not holds clauses with the second argument as s0.

Figure 1: Real-time Fragile Object Domain

4. Conclusions

The language AT describes many possible worlds. In one
of these worlds initially Holding, ¬F alling, ¬Broken is
true, and therefore Broken after Drop; wait at Clock =
2 also holds as the object is dropped and then allowed to fall
to the ground. In that same world, if one takes too long to
catch the object, then the object still shatters on the ground.
Hence in the aforementioned world Broken after Drop;
Catch at Clock = 2 is also true. However, if the object is
dropped and then is successfully caught, say at half a second
after dropping (i.e., before it hits the ground), then the object is not broken by the sequence of events, i.e., ¬Broken
after Drop; Catch at Clock = 0.5 is true. Other possible worlds include the object starting out already in a falling
state, while another world could even have the object already
broken. Table 1 shows the encoding of the real-time Fragile
Object Domain in language AT .

We presented how real-time domains and thus timed planning problems can easily and elegantly be encoded in our
integrated framework, that combines the power of CLP and
ASP in one system. The ability to do non-monotonic reasoning (ASP) in presence of time constraints (CLP) in a single
system, is needed to realize Timed Planning.

References
Baral, C. 2003. Knowledge Representation - Reasoning and
Declarative Problem Solving. Cambridge Univ. Press.
Gelfond, M., and Lifschitz, V. 1993. Representing action
and change by logic programs. Journal of Logic Programming 17(2/3&4):301–321.
Simon, L.; Mallya, A.; and Gupta, G. 2005. Design and
implementation of AT : A real-time action description language. In International Workshop on Logic-based Program
Synthesis and Transformation. Springer Verlag.
Jaffar, J., and Lassez, J. L. 1987. Constraint Logic Programming. In POPL, 111–119.

3.2 General Procedure
Now we brieﬂy present the general procedure for encoding
an AT program in our integrated framework. The general
AT command:

137

Automatic Composition of Semantic Web Services
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
The University of Texas at Dallas
Richardson, TX 75083

Abstract
Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services
automatically. For this automation to be effective, formal
semantic descriptions of Web services should be available.
In this paper we formally define the Web service discovery
and composition problem and present an approach for automatic service discovery and composition based on semantic
description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This
engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are
its scalability, i.e., its ability to handle very large service
repositories, and its extremely efficient processing times for
discovery and composition queries. We evaluate our engine
for automated discovery and composition on repositories of
different sizes and present the results.

1

Introduction

A Web service is a program accessible over the web that
may effect some action or change in the world (i.e., causes
a side-effect). Examples of such side-effects include a webbase being updated because of a plane reservation made
over the Internet, a device being controlled, etc. An important future milestone in the Web’s evolution is making services ubiquitously available. As automation increases, these
Web services will be accessed directly by the applications
rather than by humans [8]. In this context, a Web service
can be regarded as a “programmatic interface” that makes
application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize
and compose services automatically is needed in order to
make Web services more practical.
To make services ubiquitously available we need a

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis [3]. Informally, a service is characterized by its input
parameters, the outputs it produces, and the side-effect(s)
it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced
may have to satisfy certain post-conditions. For discovery,
composition, etc., one could take the syntactic approach in
which the services being sought in response to a query simply have their inputs syntactically match those of the query,
or, alternatively, one could take the semantic approach in
which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the
matching process. Several efforts are underway to build an
infrastructure [17, 23, 15] for service discovery, composition, etc. These efforts include approaches based on the
semantic web (such as USDL [1], OWL-S [4], WSML [5],
WSDL-S [6]) as well as those based on XML, such as Web
Services Description Language (WSDL [7]). Approaches
such as WSDL are purely syntactic in nature, that is, they
only address the syntactical aspects of a Web service [14].
Given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that
need can be automatically determined. This task is called
discovery. If the service is not found, the directory can be
searched for two or more services that can be composed to
synthesize the required service. This task is called composition. In this paper we present an approach for automatic
discovery and composition of Web services using their semantic descriptions.
Our research makes the following novel contributions:
(i) We formally define the discovery and composition problems; to the best of our knowledge, the formal description of
the generalized composition problem has been given for the
first time; (ii) We present efficient and scalable algorithms
for solving the discovery and composition problem that take
semantics of services into account; our algorithm automatically selects the individual services involved in composition
for a given query, without the need for manual intervention;

Q

and, (iii) we present a prototype implementation based on
constraint logic programming that works efficiently on large
repositories.
The rest of the paper is organized as follows. Section
2 describes the two major Web services tasks, namely, discovery and composition with their formal definitions. In
section 3 and 4, we present our multi-step narrowing solution and implementation for automatic service discovery
and composition. Finally we present our performance results, related work and conclusions.

2

Automated Web service Discovery and
Composition

Discovery and Composition are two important tasks related to Web services. In this section we formally describe
these tasks. We also develop the requirements of an ideal
Discovery/Composition engine.

2.1

The Discovery Problem

Given a repository of Web services, and a query requesting a service (we refer to it as the query service in
the rest of the text), automatically finding a service from
the repository that matches the query requirements is the
Web service Discovery problem. Valid solutions to the
query satisfy the following conditions: (i) they produce
at least the query output parameters and satisfy the query
post-conditions; (ii) they use only from the provided input
parameters and satisfy the query pre-conditions; (iii) they
produce the query side-effects. Some of the solutions may
be over-qualified, but they are still considered valid as
long as they fulfill input and output parameters, pre/post
conditions, and side-effects requirements.
Example 1: Say we are looking for a service to buy a book
and the directory of services contains services S1 and S2 .
The table 1 shows the input/output parameters of the query
and services S1 and S2.
In this example service S2 satisfies the query, but
S1 does not as it requires BookISBN as an input but
that is not provided by the query. Our query requires
ConfirmationNumber as the output and S2 produces ConfirmationNumber and TrackingNumber. The extra output
produced can be ignored. Also the semantic descriptions
of the service input/output parameters should be the same
as the query parameters or have the subsumption relation.
The discovery engine should be able to infer that the query
parameter BookTitle and input parameter BookName of
service S2 are semantically the same concepts. This can be
inferred using semantics from the ontology provided. The
query also has a pre-condition that the CreditCardNumber
is numeric which should logically imply the pre-condition
of the discovered service.

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

CI’,I’

CO,O

CI,I

S

where CI’ ==> CI,
I’
I,

CO’,O’

CO ==> CO’,
O
O’

Figure 1. Substitutable Service
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI ; I ; A; AO; O; CO) is the representation of a service where CI is the pre-conditions, I
is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the postconditions.
Definition (Repository of Services): Repository is a set of
Web services.
Definition (Query): The query service is defined as
Q = (CI 0; I 0; A0; AO0 ; O0; CO0 ) where CI 0 is the preconditions, I 0 is the input list, A0 is the service affect,
AO0 is the affected object, O0 is the output list, and CO0
is the post-conditions. These are all the parameters of the
requested service.
Definition (Discovery): Given a repository R and a query
Q, the Discovery problem can be defined as automatically
finding a set S of services from R such that S = fs j s =
(CI ; I ; A; AO ; O ; CO ), s 2 R, CI 0 ) CI , I v I 0 , A =
A0 , AO = AO0 , CO ) CO0 , O w O0 g. The meaning
of v is the subsumption (subsumes) relation and ) is the
implication relation. For example, say x and y are input
and output parameters respectively of a service. If a query
has (x > 5) as a pre-condition and (y > x) as postcondition, then a service with pre-condition (x > 0) and
post-condition (y > x) can satisfy the query as (x > 5) )
(x > 0) and (y > x) ) (y > x) since (x > 0).
In other words, the discovery problem involves finding
suitable services from the repository that match the query
requirements. Valid solutions have to produce at least those
output parameters specified in the query, satisfy the query
pre and post-conditions, use at most those input parameters that are provided by the query, and produce the same
side-effect as the query requirement. Figure 1 explains the
discovery problem pictorially.

2.2

The Composition Problem

Given a repository of service descriptions, and a query
with the requirements of the requested service, in case a
matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service.
Figure 2 shows an example composite service made up of

Service
Query

S1
S2

Input Parameters
BookTitle,CreditCardNumber,
AuthorName,CreditCardType
BookName,AuthorName
BookISBN,CreditCardNumber
BookName,
CreditCardNumber

Pre-conditions
IsNumeric(Credit
CardNumber)

Output Parameters
Post-Cond
ConfirmationNumber
ConfirmationNumber

IsNumeric(Credit
CardNumber)

ConfirmationNumber,
TrackingNumber

Table 1. Example Scenario for Discovery problem

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Figure 2. Example of a Composite Service as
a Directed Acyclic Graph

BookName,
AuthorName
GetISBN

NumAvailable

BookISBN

ConfNumber
PurchaseBook

AuthCode
CreditCardNum

AuthorizeCreditCard

Figure 3. Example of a Composite Service
five services S1 to S5 . In the figure, I 0 and C I 0 are the query
input parameters and pre-conditions respectively. O0 and
0
C O are the query output parameters and post-conditions
respectively. Informally, the directed arc between nodes Si
and Sj indicates that outputs of Si constitute (some of) the
inputs of Sj .
Example 2: Suppose we are looking for a service to
buy a book and the directory of services contains services
GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. The table 2 shows the input/output parameters
of the query and these services.
Suppose the repository does not find a single service that
matches these criteria, then it synthesizes a composite service from among the set of services available in the repository. Figure 3 shows this composite service. The postconditions of the service GetAvailability should logically
imply the pre-conditions of service PurchaseBook.
Definition (Composition): The Composition problem can
be defined as automatically finding a directed acyclic graph
G = (V ; E ) of services from repository R, given query Q =
(CI 0; I 0; A0; AO0 ; O0; CO0 ), where V is the set of vertices
and E is the set of edges of the graph. Each vertex in the

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

S

S

GetAvailability
BookISBN

graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of
a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:
1. 8i Si 2 V where Si has exactly one incoming edge
that represents the query inputs and pre-conditions,
I 0 w i I i , CI 0 )^i CI i .
2. 8i Si 2 V where Si has exactly one outgoing edge
that represents the query outputs and post-conditions,
O0 v i Oi , CO0 (^iCO i .
3. 8i Si 2 V where Si has at least one incoming edge,
let Si1, Si2 , ..., Sim be the nodes such that there is
a directed edge from each of these nodes to Si . Then
Ii v
O [ I 0 , C I i ( (COi1^CO i2::: ^COim ^ C I 0).
k ik
The meaning of the v is the subsumption (subsumes) relation and ) is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services
in the next stage.
Figure 4 explains one instance of the composition problem pictorially. When the number of nodes in the graph
is equal to one, the composition problem reduces to the
discovery problem. When all nodes in the graph have not
more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition
problem (i.e., the graph is a linear chain of services).

S

2.3

Requirements of an ideal Engine

Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition
where the number of services involved in composition is
exactly equal to one. The features of an ideal Discovery/Composition engine are:
Correctness: One of the most important requirements for

Service

Input Parameters

Pre-Conditions

Query

Output
Params
ConfNumber

PurchaseBook

BookISBN
NumAvailable NumAvailable > 0
AuthCode
AuthCode > 99^
AuthCode < 1000
ConfNumber

BookTitle,CreditCardNum,
AuthorName,CardType
GetISBN
BookName,AuthorName
GetAvailability
BookISBN
AuthorizeCreditCard CreditCardNum
BookISBN, NumAvailable, AuthCode

NumAvailable > 0

Post-Conditions

Table 2. Example Scenario for Composition problem
These requirements have driven the design of our
semantics-based Discovery and Composition engine described in the following sections.

2.4

Figure 4. Composite Service
an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the
requirements of the query. Also, the engine should be able
to find all services that satisfy the query requirements.
Small Query Execution Time: Querying a repository of
services for a requested service should take a reasonable
amount of (small) time, i.e., a few milliseconds. Here we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental Updates: Adding or updating a service to an
existing repository of services should take a small amount
of time. A good Discovery/Composition engine should not
pre-process the entire repository again, rather incrementally
update the pre-processed data (indexes, etc.) of the repository for this new service added.
Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition
engine should be able to give results based on requirements
(minimize, maximize, etc.) over the costs. We can extend
this to services having an attribute vector associated with
them and the engine should be able to give results based on
maximizing or minimizing functions over this vector.

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

Semantic Description of Web Services

A Web service is a software system designed to support
interoperable machine-to-machine interaction over a network. It has an interface that is described in a machineprocessible format so that other systems can interact with
the Web service through its interface using messages. The
automation of Web service tasks (discovery, composition,
etc.) can take place effectively only if formal semantic descriptions of Web services are available. Currently, there
are a number of approaches for describing the semantics of
Web services such as OWL-S [4], WSML [5], WSDL-S [6],
and USDL [1].

3

A Multi-step Narrowing Solution

With the formal definition of the Discovery and Composition problem, presented in the previous section, one can
see that there can be many approaches to solving the problem. Our approach is based on a multi-step narrowing of the
list of candidate services using various constraints at each
step. In this section we discuss our Composition algorithm
in detail. As mentioned earlier, discovery is a simple case of
Composition. When the number of services involved in the
composition is exactly equal to one, the problem reduces
to a discovery problem. Hence we use the same engine for
both discovery and composition. We assume that a directory of services has already been compiled, and that this
directory includes semantic descriptions for each service.

3.1

The Service Composition Algorithm

For service composition, the first step is finding the set
of composable services. Using the discovery engine, individual services that make up the composed service can be
selected. Part substitution techniques [2] can be used to find
the different parts of a whole task and the selected services
can be composed into one by applying the correct sequence
of their execution. The correct sequence of execution can

be determined by the inputs, outputs, pre-conditions and
post-conditions of the individual services. That is, if a subservice S1 is composed with subservice S2 , then the postconditions of S1 must imply the pre-conditions of S2 . The
goal is to derive a single solution, which is a directed acyclic
graph of services that can be composed together to produce
the requested service in the query. Figure 6 shows a pictorial representation of our composition engine.
In order to produce the composite service which is the
graph, as shown in the example figure 2, we filter out services that are not useful for the composition at multiple
stages. Figure 5 shows the filtering technique for the particular instance shown in figure 2. The composition routine starts with the query input parameters. It finds all those
services from the repository which require a subset of the
query input parameters. In figure 5, C I ; I are the preconditions and the input parameters provided by the query.
S1 and S2 are the services found after step 1. O1 is the
union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous stage, i.e., I2 = O1 [ I . I2 is used to find services at
the next stage, i.e., all those services that require a subset
of I2. In order to make sure we do not end up in cycles,
we get only those services which require at least one parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.

I=I
CI, I

1

S1
S2
.
.

O1

I=IUO
2

1

1

S
.
.

O

2

3

I=IUO
3

2

2

O
S

3

I=IUO
4

3

4

.
.

3

S

O

4

O

5

.
.

Figure 5. Composite Service
Algorithm: Composition
Input: QI - QueryInputs, QO - QueryOutputs, QCI - PreCond, QCO - Post-Cond
Output: Result - ListOfServices
1. L NarrowServiceList(QI, QCI);
2. O GetAllOutputParameters(L);
3. CO GetAllPostConditions(L);
4. While Not (O w QO)
5.
I = QI [ O; CI QCI ^ CO;
6.
L’ NarrowServiceList(I, CI);
7. End While;
RemoveRedundantServices(QO, QCO);
8. Result
9.Return Result;

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

S1
Query
described
using
USDL
(S)

Infer
Sub-queries

.
.
.
Sn

Discovery Module
(Discovery Engine + Service
Directory + Term Generator)
S1

Composed Service

Pre-Cond(S)
S1
Pre-Cond( S)1

Post-Cond( S)1
Pre-Cond( S)2

S2

..........................

Sn

Composition Engine
(implemented using
Constraint Logic
Programming

................................. S n

Post-Cond( S)n
Post-Cond(S)

Figure 6. Composition Engine

4

Implementation

Our discovery and composition engine is implemented
using Prolog [11] with Constraint Logic Programming over
finite domain [10], referred to as CLP(FD) hereafter. In
our current implementation, we used semantic descriptions
written in the language called USDL [1]. The repository of
services contains one USDL description document for each
service. USDL itself is used to specify the requirements of
the service that an application developer is seeking.
USDL is a language that service developers can use to
specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology
that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. USDL
uses WordNet [9] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the “meaning” of
input parameters, outputs, and the side-effect induced by
the service is given by mapping these syntactic terms to
concepts in WordNet (see [1] for details of the representation). Inclusion of USDL descriptions, thus makes services
directly “semantically” searchable. However, we still need
a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as
such a query language. A USDL description of the desired
service can be written, a query processor can then search
the service directory for a “matching” service. Due to lack
of space, we do not go into the details of the language in
this paper. They are available in our previous work [2].
These algorithms can be used with any other Semantic
Web service description language as well. It will involve
extending our implementation to work for other description
formats, and we are looking into that as part of our future
work.
The software system is made up of the following components.
Triple Generator: The triple generator module converts

each service description into a triple. In this case, USDL
descriptions are converted to triples like:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the
side-effect. I is the list of inputs and O is the list of outputs.
Pre-Conditions are the conditions on the input parameters
and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can
be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic,
specific, part and whole substitutions [2]. In case conditions on a service are not provided, the Pre-Conditions and
Post-Conditions in the triple will be null. Similarly if the
affect-type is not available, this module assigns a generic
affect to the service.
Query Reader: This module reads the query file and passes
it on to the Triple Generator. We use USDL itself as the
query language. A USDL description of the desired service
can be written, which is read by the query reader and converted to a triple. This module can be easily extended to
read descriptions written in other languages.
Semantic Relations Generator: We obtain the semantic
relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms,
meronyms, holonyms and many more. USDL descriptions
point to OWL WordNet for the meanings of concepts. A
theory of service substitution is described in detail in [2]
which uses the semantic relations between basic concepts of
WordNet, to derive the semantic relations between services.
This module extracts all the semantic relations and creates
a list of Prolog facts. We can also use any other domainspecific ontology to obtain semantic relations of concepts.
We are currently looking into making the parser in this module more generic to handle any other ontology written in
OWL.
The query is parsed and converted into a Prolog query that
looks as follows:
discovery(sol(queryService, ListOfSolutionServices).
The engine will try to find a list of SolutionServices that
match the queryService.
Composition Engine: The composition engine is written
using Prolog with CLP(FD) library. It uses a repository of
facts, which contains all the services, their input and output
parameters and the semantic relations between the parameters. The following is the code snippet of our composition
engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs),
encodeParam(QueryOutputs, QO),
getExtInpList(QueryInputs, InpList),
encodeParam(InpList, QI),
performForwardTask(QI, QO, LF),

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

performBackwardTask(LF, QO, LR),
getMinSolution(LR, QI, QO, A), reverse(A, RevA),
confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The query is converted into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate ’bagof’ to return all possible
ListOfServices that can be composed to get the requested
queryService.
Output Generator: After the Composition engine finds a
matching service, or the list of atomic services for a composed service, the results are sent to the output generator in
the form of triples. This module generates the output files
in any desired XML format.

5

Efficiency and Scalability Issues

In this section we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It
is because of these features, we decided on the multi-step
narrowing based approach to solve these problems and implemented it using constraint logic programming.
Correctness: Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions.
Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always
find a correct solution (if one exists) in the minimum possible steps. The formal proof of correctness and minimality
is beyond the scope of this paper.
Pre-processing: Our system initially pre-processes the
repository and converts all service descriptions into Prolog
terms. The semantic relations are also processed and loaded
as Prolog terms in memory. Once the pre-processing is
done, then discovery or composition queries are run against
all these Prolog terms and hence we obtain results quickly
and efficiently. The built-in indexing scheme and constraints in CLP(FD) facilitate the fast execution of queries.
During the pre-processing phase, we use the term representations of services to set up constraints on services and the
individual input and output parameters. This further helped
us in getting optimal results.
Execution Efficiency: The use of CLP(FD) helped significantly in rapidly obtaining answers to the discovery and
composition queries. We tabulated processing times for different size repositories and the results are shown in Section
6. As one can see, after pre-processing the repository, our
system is quite efficient in processing the query. The query
execution time is insignificant.

Programming Efficiency: The use of Constraint Logic
Programming helped us in coming up with a simple and
elegant code. We used a number of built-in features such as
indexing, set operations, and constraints and hence did not
have to spend time coding these ourselves. This made our
approach efficient in terms of programming time as well.
Not only the whole system is about 200 lines of code, but
we also managed to develop it in less than 2 weeks.
Scalability: Our system allows for incremental updates on
the repository, i.e., once the pre-processing of a repository is
done, adding a new service or updating an existing one will
not need re-execution of the entire pre-processing phase.
Instead we can easily update the existing list of CLP(FD)
terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will
be negligible, perhaps a few milliseconds. With real-world
services, it is likely that new services will get added often
or updates might be made on existing services. In such a
case, avoiding repeated pre-processing of the entire repository will definitely be needed and incremental update will
be of great practical use. The efficiency of the incremental
update operation makes our system highly scalable.
Use of external Database: In case the repository grows
extremely large in size, then saving off results from the preprocessing phase into some external database might be useful. This is part of our future work. With extremely large
repositories, holding all the results of pre-processing in the
main memory may not be feasible. In such a case we can
query a database where all the information is stored. Applying incremental updates to the database is easily possible
thus avoiding recomputation of pre-processed data .
Searching for Optimal Solution: If there are any properties with respect to which the solutions can be ranked,
then setting up global constraints to get the optimal solution is relatively easy with the constraint based approach.
For example, if each service has an associated cost, then the
discovery and the composition problem can be redefined to
find the solutions with the minimal cost. Our system can be
easily extended to take these global constraints into account.

6

Repository Size
(num of
services)
2000
2000
2000
2500
2500
2500
3000
3000
3000

Number
of I/O
parameters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.5
45.8
57.8
47.7
58.7
71.6
56.8
77.1
88.2

Query
Exec
Time
(msecs)
1
1
2
1
1
2
1
1
3

Incremental
update
(msecs)
18
23
28
19
23
29
19
26
29

Table 3. Performance on Discovery Queries

found is that the repository was cached after the first run
and that explained the difference in the pre-processing time
for subsequent runs. Table 3 shows performance results of
our Composition algorithm on discovery queries and table 4
shows results of our algorithm on composition queries. The
times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the
query should be less than or equal to the wall clock time.
The results are plotted in figure 8. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase
in number of input/output parameters. Similarly, for fixed
input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which
is negligible (just 1 to 3 msecs) even for complex queries
with large repositories.

Performance

We used repositories from WS-Challenge website[13],
slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services).
These repositories contain WSDL descriptions of services.
The queries and solutions are provided in an XML format. The semantic relations between various parameters
are provided in an XML Schema file. We evaluated our
approach on different size repositories and tabulated Preprocessing and Query Execution time. We noticed that there
was a significant difference in pre-processing time between
the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

Figure 7. Performance on Discovery Queries

Figure 8.
Queries
Repository Size
(num of
services)
2000
2000
2000
3000
3000
3000
4000
4000
4000
Table 4.
Queries

7

Performance

Number
of I/O
parameters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.1
47.1
60.2
58.4
60.1
102.1
71.2
87.9
129.2

Performance

on

Composition

Query
Exec
Time
(msecs)
1
1
1
1
1
1
1
1
1
on

Incremental
update
(msecs)
18
23
30
19
20
34
18
22
32

Composition

Related Work

Composition of Web services has been active area of research recently [14, 15, 23, 18]. Most of these approaches
are based on capturing the formal semantics of the service
using an action description languages or some kind of logic
(e.g., description logic). The service composition problem
is reduced to a planning problem where the sub-services
constitute atomic actions and the overall service desired is
represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine
the combination of actions needed to reach the goal. With
this approach an explicit goal definition has to be provided,
whereas such explicit goals are usually not available [17].
To the best of our knowledge, most of these approaches that
use planning are restricted to sequential compositions (i.e.,

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

a linear chain of services), rather than a directed acyclic
graph. Our approach automatically selects atomic services
from a repository and produces the composition flow in the
form of a directed acyclic graph.
The authors in [19, 20] present a composition technique
by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for
composing Web services. This approach also relies on a
user-defined plan template which is created manually.
There are industry solutions based on WSDL and
BPEL4WS where the composition of the flow is obtained
manually. A comparison of the approaches based on AI
planning techniques and approach based on BPEL4WS is
presented in [17]. This work shows that in both these approaches, the flow of the composition is determined manually. They do not assemble complex flows of atomic services based on a search process. They select appropriate
services using a planner when an explicit flow is provided.
In contrast, we have shown a technique to automatically determine these complex flows using semantic descriptions of
atomic services.
A process-level composition solution based on OWL-S
is proposed in [21]. In this work the authors assume that
they already have the appropriate individual services involved in the composition, i.e., they are not automatically
discovered. They use the descriptions of these individual
services to produce a process-level description of the composite service. They do not automatically discover/select
the services involved in the composition, but instead assume
that they already have the list of atomic services. In contrast, we automatically find the services that are suitable for
composition based on the query requirements for the new
composed service.
In [22], a semi-automatic composition technique is presented in which atomic services are selected for each stage
of composition. This selection process involves decision
making by a human controller at each stage, i.e., the selection process requires some manual intervention.
Another related area of research involves message conversation constraints, also known as behavioral signatures
[16]. Behavior signature models do not stray far from the
explicit description of the lexical form of messages, they
expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while our approach deals with higher-level
real world concepts. However, both these approaches can
be regarded as complementary concepts when taken in the
context of real world service composition, and both technologies are currently being used in the development of a
commercial services integration tool.
Our most important, novel contribution in this paper is

our technique for automatically selecting the services that
are suitable for obtaining a composite service, based on the
user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use
human input on what services would be suitable for composition. Our technique also handles non-sequential compositions (i.e., composition where there can be more than
one service involved at any stage, represented as a directed
acyclic graph of services) rather than sequential composition (i.e, a linear chain of services) which is the case with
most of the existing approaches.

8

Conclusions and Future Work

To catalogue, search and compose Web services in a
semi-automatic to fully-automatic manner we need infrastructure to publish Web services, document them, and query
them for matching services. Our semantics-based approach
uses semantic description of Web services (example USDL
descriptions). Our composition engines find substitutable
and composite services that best match the desired service.
Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services,
number of services in a composition, etc.). The composition flow is determined automatically without the need for
any manual intervention. Our engine finds any sequential
or non-sequential composition that is possible for a given
query. We are able to apply many optimization techniques
to our system so that it works efficiently even on large
repositories. Use of Constraint Logic Programming helped
greatly in obtaining an efficient implementation of this system.
Our future work includes extending our engine to work
with other web services description languages like OWLS, WSML, WSDL-S, etc. This should be possible as long
as semantic relations between concepts are provided. It
will involve extending the TripleGenerator, QueryReader,
and SemanticRelationsGenerator modules. We would also
like to extend our engine to support an external database to
save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size
which can easily be the case in future. Future work also
includes developing an industrial-strength system based on
the research reported in this paper, in conjunction with a
system that allows (semi-) automatic generation of USDL
descriptions from code and documentation of a service [24].

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005.
[2] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and
T. Hite. USDL: A Service-Semantics Description Lan-

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

guage for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas.
edu/˜sxk038200/USDL.pdf.
[3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems, pp. 46-53, Mar. ’01.
[4] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[5] WSML: Web Service Modeling Language. www.
wsmo.org/wsml/.
[6] WSDL-S: Web Service Semantics. http://www.
w3.org/Submission/WSDL-S.
[7] WSDL: Web Services Description Language. http:
//www.w3.org/TR/wsdl.
[8] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services. In
ICWS, pp 751-758, ’05.
[9] OWL WordNet http://taurus.unine.ch/
knowler/wordnet.html.
[10] K. Marriott, P. Stuckey. Prog. with Constraints: An
Introduction. MIT Press, ’98.
[11] L. Sterling, S. Shapiro. The Art of Prolog. MIT Press.
[12] OWL: Web Ontology Language Reference. http:
//www.w3.org/TR/owl-ref.
[13] WS Challenge 2006. http://insel.flp.cs.
tu-berlin.de/wsc06.
[14] U. Keller, R. Lara, H. Lausen, A. Polleres, D. Fensel.
Automatic Location of Services. In ESWC, ’05.
[15] S. Grimm, B. Motik, and C. Preist Variance in eBusiness Service Discovery. In Workshop at ISWC, ’04.
[16] R. Hull and J. Su. Tools for design of composite Web
services. In SIGMOD, ’04.
[17] B. Srivastava, J. Koehler. Web Services Composition
- Current Solutions and Open Problems. In ICAPS, ’03.
[18] B. Srivastava. Automatic Web Services Composition
using planning. In KBCS, pp 467-477.
[19] S. McIlraith, T.C. Son Adapting golog for composition of Web services. In KRR, pp 482-493, ’02.
[20] S. McIlraith, S. Narayanan Simulation, verification
and automated composition of services. In WWW, ’02.
[21] M. Pistore, P. Roberti, P. Traverso Process-Level
Composition of Executable Services In ESWC, pp 6277, ’05.
[22] E. Sirin, J. Hendler, and B. Parsia Semi-automatic
Composition of Web Services using Semantic Descriptions In Workshop at ICEIS, ’02 .
[23] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara
Semantic Matching of Web Service Capabilities. In
ISWC, pp 333-347, ’02.
[24] Metallect IQ Server. http://metallect.com/
downloads/Metallect_Product_Brief_
IQServer.pdf

A Universal Service Description Language
Luke Simon, Ajay Mallya, Ajay Bansal, Gopal Gupta
Department of Computer Science
University of Texas at Dallas
Richardson, TX 75083

Abstract

(USDL) which service developers can use to specify formal semantics of web-services. Thus, if WSDL can be
regarded as a language for formally specifying the syntax of web services, USDL can be regarded as a language for formally specifying their semantics. USDL
can be thought of as formal program documentation
that will allow sophisticated conceptual modeling and
searching of available web services, automated composition, and other forms of automated service integration.
For example, the WSDL syntax and USDL semantics
of web services can be published in a directory which
applications can access to automatically discover services. That is, given a formal description of the context in which a service is needed, the service(s) that will
precisely fulﬁll that need can be determined. The directory can then be searched to check if this exact service
is available, and if not available, then whether it can be
synthesized by composing two or more services listed in
this (or another) directory.

To fully utilize web-services, users and applications
should be able to discover, deploy, compose and synthesize services automatically. This automation can
take place only if a formal semantic description of the
web-services is available. In this paper we present a
markup language called USDL (Universal Service Description Language), for formally describing the semantics of web-services.

1

Introduction

The next milestone in the Web’s evolution is making services ubiquitously available. A web service is a
program available on a web-site that “effects some action or change" in the world (i.e., causes a side-effect).
Examples of such side-effects include a web-base being
updated because of a plane reservation made over the
Internet, a device being controlled, etc. As automation
increases, these web-services will be accessed directly
by the applications themselves rather than by humans.
In this context, a web-service can be regarded as a “programmatic interface" that makes application to application communication possible. For web-services to become practical, an infrastructure needs to be supported
that allows users to discover, deploy, compose, and synthesize services automatically. Such an infrastructure
must be semantics based so that applications can reason
about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis.
Approaches such as WSDL are purely syntactic in
nature, that is, they merely specify the format of the
service. Our approach can be regarded as providing
semantics to WSDL statements. We present a language called Universal Services Description Language

The design of USDL rests on two formal languages:
Web Services Description Language (WSDL) [3] and
Web Ontology Language (OWL) [2]. The Web Services
Description Language (WSDL) [3], is used to give a
syntactic description of the name and parameters of a
service. The description is syntactic in the sense that
it describes the formatting of services on a syntactic
level of method signatures, but is incapable of describing what concepts are involved in a service and what a
service actually does, i.e. the conceptual semantics of
the service.
USDL can be regarded as as a merger of WSDL and
OWL that yields a language capable of describing the
syntax and semantics of web services. WSDL will be
used to describe message formats, types, and method
prototypes, while a specialized universal OWL ontology
will be used to formally describe what these messages
and methods mean, on a conceptual level.
1

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

Thomas D. Hite
Metallect Corp
2400 Dallas Parkway
Plano, TX 75093

2

USDL

3

Applications

With a pool of USDL services at one’s disposal, rapid
application development (RAD) tools could be used to
aid a systems integrator with the task of creating composite services, i.e., services consisting of the composition of already existing services. The service designer
could use such a RAD tool by describing the desired
service via a USDL document, and then the tool would
query the pool of services for composable sets of services that can be used to accomplish the task as well
as automatically generate boilerplate code for managing the composite service, as well as menial inter-service
data format conversions and other glue. Of course these
additional RAD steps would require other technologies
already being researched and developed [4, 5, 7].

As mentioned earlier, USDL can be regarded as the
semantic counterpart to the syntactic WSDL description.
WSDL documents contain two main constructs to which
we want to ascribe conceptual meaning: messages and
ports. These constructs are actually aggregates of service components which will actually be directly ascribed
meaning. Messages consist of typed parts and ports consist of operations parameterized on messages. USDL
deﬁnes OWL surrogates or proxies of these constructs
in the form of classes, which have properties with values in the OWL WordNet ontology. Furthermore, the
USDL surrogates for WSDL operations allow for the
description of the side-effect of executing the operation,
in addition to the semantics of the input and output parameters of the operation, which are also mapped to the
WordNet.
Using an OWL WordNet ontology allows for our solution to use a universal, complete, and tractable framework, which lacks the semantic aliasing problem, to
which we map web service messages and operations.
The semantic aliasing problem occurs when the formal
semantics distinguishes two identical concepts. Other
approaches such as OWL-S suffer from this problem
[1, 8]. As long as this mapping into the WordNet ontology is precise and sufﬁciently expressive, reasoning can
be done within the realm of OWL by using an automated
inference systems (such as, one based on description
logic), and we automatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological
concepts, especially subsumption (hyponym) and equivalence (synonym) relationships. Finally, USDL restricts
its conceptual constructors, in order to address the problems of semantic aliasing and tractability.
At present, we presume only four side-effect speciﬁc
operations (ﬁnd, create, delete, update) which have an
ontology member as their "target"; this set of basic
operations may be extended as more experience is
gained with USDL. In practice, most services, however,
deal with manipulating databases and for such services
these four operations are sufﬁcient. As stated, one of the
reasons for limiting the side-effect types is to safe-guard
against the semantic aliasing problem. This is also one
of the main reasons for restricting the combining forms
in USDL to conjunction, disjunction, and negated atoms
[8].

4

Conclusion

The current version of USDL incorporates current
standards in a way to further aid markup of IT services
by allowing constructs to be given meaning in terms
of an OWL based WordNet ontology. Future work involves the application of USDL to formally describe
commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well as
to service discovery and rapid application development
(RAD) in commercial environments [6].

References
[1] Semantic markup for web services.
http:
//www.daml.org/services/owl-s/1.0/
owl-s.html.
[2] Web ontology language reference. http://www.w3.
org/TR/owl-ref.
[3] Web services description language. http://www.w3.
org/TR/wsdl.
[4] P. A. Bernstein. Applying model management to classical
meta data problems. In CIDR, pages 209–220, 2003.
[5] C. E. Gerede, R. Hull, O. H. Ibarra, and J. Su. Automated
composition of e-services:lookaheads. In ICSOC, 2004.
[6] T. Hite. Service composition and ranking: A strategic
overview. Metallect Inc., 2005.
[7] R. Hull and J. Su. Tools for design of composite web
services. In SIGMOD, 2004.
[8] L. Simon, A. Mallya, A. Bansal, G. Gupta, and T. Hite. A
universal service description language. Technical Report
UTDCS-09-05, University of Texas at Dallas, 2005.

2

Proceedings of the IEEE International Conference on Web Services (ICWS’05)
0-7695-2409-5/05 $20.00 IEEE

10th IEEE Conference on E-Commerce Technology and the Fifth IEEE Conference on Enterprise Computing, E-Commerce and E-Services

WSC-08: Continuing the Web Services Challenge
Ajay Bansal, M. Brian Blake, and
Srividya Kona

Steffen Bleul and
Thomas Weise

Georgetown University
Washington, DC 20057

University of Kassel
D-34121, Kassel Germany

{bansal,blake,kona}@cs.georgetown.edu

{bleul|weise}@vs.uni-kassel.de

Berlin University of
Technology
D-10587 Berlin Germany
mcj@cs.tu-berlin.de

and automated composition. This forum provides
quantitative and qualitative evaluation results on the
performance of participating matchmaking and
automated composition software and facilitates the
dissemination of results that advance this field.
This fourth year extends the original criteria of the
first two competitions which focused on service
discovery and service composition based on the
syntactic matching of WSDL part names. WSC-08 is a
continuation of the evolution of the challenge
considering the adoption of ontologies written in OWL
to provide semantics. It has been the tradition of the
WSC events to adhere to technology-independent
approaches to semantics. In this way, the competition
attempts to circumvent debates on representations,
such as differences between approaches such as OWLS [5] [4] and WSDL-S [10]. In 2006, the use of pure
XML-based semantics allowed for a bi-partisan
approach. In 2007, we have evolved the challenge by
mirroring the XML-based semantics with equivalent
representations using an OWL ontology. During the
previous three years, web service challenge focused on
optimizing the discovery and composition process
solely using abstractions from real-world situations.
The taxonomies of semantic concepts as well as the
involved data formats were purely artificial. In 2008,
we have further evolved the challenge by using data
formats and contest data based on OWL, WSDL, and
WSBPEL schemas for ontologies, services, and service
orchestrations.
In the earlier competitions, web service composition
applications (i.e. participant entries) were constructed
as stand-alone applications. In 2007, participating
software had to be implemented as web services. As
such, requests and results of composition routines are
transmitted as SOAP messages to and from the
participating software applications.
This natural
progression in the WSCs allowed the competition itself
to take place within a SOA environment. In 2008 also,
the participating software will have to be implemented
as web services.

Abstract
The capabilities of organizations can be openly
exposed, easily searched and discovered, and made
readily-accessible to humans and particularly to
machines,
using
service-oriented
computing
approaches. Artificial intelligence and software
engineering researchers alike are tantalized by the
promise
of
ubiquitously
discovering
and
incorporating services into their own business
processes (i.e. composition and orchestration). With
growing acceptance of service-oriented computing,
an emerging area of research is the investigation of
technologies that will enable the discovery and
composition of web services. The Web Services
Challenge (WSC) is a forum where academic and
industry researchers can share experiences of
developing tools that automate the integration of Web
services. In the fourth year (i.e. WSC-08) of the Web
Services Challenge, software platforms will address
several new composition challenges. Requests and
results will be transmitted within SOAP messages. In
addition, semantics will be represented as ontologies
written in OWL, services will be represented in WSDL,
and service orchestrations will be represented in WSBPEL.

1. Introduction
Succeeding the EEE-05 [3] [9], WSC-06 and WSC07 Challenges [2] [8], the 2008 Web Services
Challenge (WSC-08) [8] is the fourth year of this SOA
venue [1] that looks to benchmark software
applications that automatically manage web services.
WSC-08, held at the IEEE Joint Conference on
Electronic Commerce Technology (CEC 2008) and
Enterprise Computing, E-Commerce, and E-Services
(EEE 2008) continues to provide a forum where
researchers can collaborate on approaches, methods
and algorithms in the domain of web service discovery

1530-1354/08 $25.00 © 2008 IEEE
DOI 10.1109/CEC/EEE.2008.67

Michael C. Jaeger

351

Although in 2005, composition problems included
the requirement to handle concurrent threads for
composition routines, in 2006, the problem sets did not
contain multiple branches. In WSC-07, concurrency
was re-introduced into the composition problem sets.
The combination of complex, workflow-based
heuristics with semantically-rich representations
required participants to create new software that is
both robust and efficient. The problem sets in WSC-08
will also have concurrency in them.

2. Related Challenges/Competitions
Although WSC is perhaps the first competition,
other unique venues have been established to
investigate the need for solutions to the service
composition problem. The Semantic Web Services
(SWS) Challenge [6] is less of a competition and more
of a challenge. Both business cases and solution
applications are the focus of SWS. Participants are
placed in a forum where they can incrementally and
collaboratively learn from each other. While WSC is
more driven by application, SWS concentrates more on
the environment. As such, the SWS places more focus
on semantics where the WSC favors applied, shortterm solutions with semantic and syntactical
approaches.
Alternatively, the SOA Contest [7] held at the
International Conference on Services Computing
(SCC2006, SCC2007, and SCC2008) allows
participants to openly choose the problems that best
demonstrate their approach. The benefit of SOA
contest is that participants can show the best approach
for a particular domain-specific problem. In contrast,
WSC attempts to set a common problem where
approaches can be evaluated side-by-side.
There is a unique niche for each forum and the
composition of the results from all the venues will
undoubtedly advance the state-of-the-art in serviceoriented computing.

Figure 1. The 2008 Web Service Challenge procedure.
The composer software of the contestants is placed
on the server side and started with a bootstrap
procedure. First of all, the system is provided with a
path to a WSDL file. The WSDL file contains a set of
services along with annotations of their input- and
output parameters. The number of services will change
from challenge to challenge. Every service will have
an arbitrary number of parameters. In addition to the
WSDL file, we also provide the address of the OWL
file during the bootstrapping process. This file contains
the taxonomy of concepts used in this challenge in
OWL format. The bootstrapping process includes
loading the relevant information from these files. The
challenge task will then be sent to the composer via a
client-side GUI very similar to last year’s challenge.
After the bootstrapping on the server side is finished,
the GUI queries the composition system with the
challenge problem definition. The contestant’s
software must now compute a solution – one or more
service compositions – and answer in the solution
format which is a subset of the WSBPEL schema.
When the WSBPEL document is received by the GUI,
we will stop a time measurement and afterwards
evaluate the compositions themselves.

3. The Challenge
In the competition, we adopt the idea of so-called
Semantic Web Services that represent Web Services
with a semantic description of the interface and its
characteristics. The task is to find a composition of
services that produces a set of queried output
parameters from a set of given input parameters. The
overall challenge procedure is shown in Figure 1.

352

3.1. New Technical Innovations
x
x

x

x

Document Formats: OWL Ontologies,
WSDL-Queries,
WSBPEL
solutions
schemas.
XSD-Types: The challenge will include
matching XSD-Type definitions like
arrays, simple types, complex types with
substructures and enumerations.
Parallel Execution: The WSBPEL schema
supports the specification of parallel
execution of services. Valid parallel
execution will positively influence the
systems challenge score.
No results: The challenge will include sets
of services that actually deliver no
solution.
The system should act
accordingly and cancel the discovery in
time.

1.

Time
Measurement:
The
time
measurement is done by the interface
package. We take the time after submitting
the query and the time when the
composition result is fully received. The
bootstrap mechanism is excluded from the
assessment. There will be a time limit for
bootstrapping after which a challenge is
considered as failure.

2.

Composition Evaluation:
x Completeness: The amount of
compositions discovered by the
system.
x Composition Length: The shortest
composition.
x Composition Efficiency: Parallel
versus sequential execution of
services.

3.2. Semantics
Ontologies are usually expressed with OWL [12],
an XML format [13]. We use the OWL format in the
2008 challenge, but like in the previous years, we limit
semantic evaluation strictly to taxonomies consisting
of sub- and super-class relationship between semantic
concepts only. In addition to semantic concepts
(OWL-Classes), OWL allows to specify instances of
classes called individuals. While we also distinguish
between individuals and classes in the competition, the
possibility to express equivalence relations between
concepts is not used. In OWL, the semantics is defined
with statements consisting of subject, predicate, and
object, e.g. ISBN-10 is a ISBN (ISBN subsumes
ISBN-10). Such statements can be specified with
simple triplets but also with XML-Hierarchies and
XML-References. The implementation of an OWLParser is hence not trivial. In order to ease the
development of the competition contributions, we will
stick to a fixed but valid OWL-Schema.

Figure 2. Evaluation for WSC-08

4. Logistics of the Web Services Challenge
3.3. Evaluation
WSC-08 has attracted international teams which
come from universities and research organizations in
countries including Canada, China, Germany,
Netherlands, and the United States. The organization
of this event is divided into two phases: The first phase
focuses on evaluating the technical viability of the
methodologies proposed by the participating teams. A
four-page technical description submitted from each
team is peer-reviewed and included in the proceedings

The Web Service Challenge awards the most
efficient system and also the best architectural
solution. The best architectural effort will be awarded
according to the contestant’s presentation and system
features. The evaluation of efficiency consists of two
parts as described below.

353

of the joint conferences (CEC/EEE2008). Once the
reviewing and acceptance notification have completed,
teams that successfully complete this first phase are
asked to submit a version of their software for
evaluation. Preliminary tests are conducted using the
evaluation version to ensure the compatibility and
applicability during the final competition. The main
objective is to avoid in advance the potential format
related problems that may otherwise occur when the
competition takes place.
The second phase is the final competition which is
scheduled for two days at CEC/EEE-08. On the first
day, all the participating teams will present their
approaches in a specialized session of the conference.
On the same day, the participants are schedule times to
install the latest version of their software on the
evaluations services located onsite at the conference.
On the second day, the teams must execute their
software using a customized data set prepared
specifically for the competition.
Participating software is measured for performance
during any indexing phases and during the actual
composition routine.
Composition results are
evaluated against known solutions for correctness and
completeness. In 2008 there will be multiple sets of
correct answers with variable length chains.
Applications will be judged with weighted scores
based on the best solutions that they present.
The solution application with the best qualitative
and quantitative scores when run against several
datasets is awarded first place. The competition
typically has a winner and several runner-ups.

References
[1] Blake, M.B., Cheung, W., and Wombacher, A. “Web
Services Discovery and Composition Systems”
International Journal of Web Services Research, Vol. 4,
No. 1, pp iii – viii, January 2007
[2] Blake, M.B., Cheung, W., Jaeger, M.C., and
Wombacher, A., “WSC-06: The Web Service
Challenge”, Joint Proceedings of the CEC/EEE 2006,
San Francisco, California, USA, June 2006.
[3] Blake, M.B., Tsui, K.C., Cheung, W., “The EEE-05
Challenge: A New Web Service Discovery and
Composition Competition” Proceedings of the IEEE
International Conference on E-Technology, ECommerce, and E-Services (EEE-05), Hong Kong,
March 2005.
[4] Fensel, D. and Bussler, C. “The Web Service Modeling
Framework”, Electronic Commerce: Research and
Applications, 1(2): 113-137, 2002
[5] Martin, D. et al. “Bringing Semantics to Web Services:
The OWL-S Approach”, Proceedings of the First
International Workshop on Semantic Web Services and
Web Process Composition (SWSWPC-04), San Diego,
CA, USA, July 2004.
[6] The Semantic Web Services Challenge (2007):
http://sws-challenge.org/wiki/index.php/Main_Page
[7] The
Services
Computing
Contest
(2007):
http://iscc.servicescomputing.org/2007/
[8] The
Web
Services
Challenge
(2007).
http://www.wschallenge.org/wsc07/
[9] The Web Services Challenge at the IEEE Conference on
e-Business
Engineering
(ICEBE-05)
(2007):
http://www.comp.hkbu.edu.hk/simctr/wschallenge/
[10] WSDL-S
(2007):
http://www.w3.org/Submission/WSDL-S/
[11] The
Web
Services
Challenge
(2008).
http://cec2008.cs.georgetown.edu/wsc08/
[12] Bechhofer, S., Harmelen, F., Hendler, J., Horrocks, I.,
McGuinness, D., Patel-Schneider, P., and Stein L.
OWL Web Ontology Language Reference. World Wide
Web Consortium (W3C), February 10, 2004. Online
available at http://www.w3.org/TR/2004/REC-owl-ref20040210/
[13] Bray, T., Paoli, J., Sperberg-McQueen, C., Maler, E.,
and Yergeau, F. Extensible Markup Language (XML)
1.0 (Fourth Edition). World Wide Web Consortium
(W3C), September 29, 2007. Online available at
http://www.w3.org/TR/2006/REC-xml-20060816

5. Acknowledgements
The authors would like to acknowledge the efforts of
Georgetown students Brian Miller, David Cummings
and Michael Fitzgerald Nowlan who facilitated the
advertisement, review, and evaluation of the
participating technical reports and software.
Georgetown graduate student, John Adams, organized
the travel logistics of the web service challenge. The
authors also acknowledge William K. W. Cheung and
Andreas Wombacher for their efforts and contributions
to the web services challenge. The authors also
acknowledge Hong Kong Baptist graduate student,
Kai-Kin Chan, for preparing the OWL representations.
The Web Service Challenge has been extensively
funded by the National Science Foundation under
award number 0548514 and 0723990. The Hewlett
Packard Corporation and Springer-Verlag have also
supported an award to the winners of the competition.

354

2009 IEEE Conference on Commerce and Enterprise Computing

WSC-2009: A Quality of Service-Oriented Web Services Challenge
Srividya Kona

Ajay Bansal

M. Brian Blake

Georgetown University
Washington, DC 20057

Georgetown University
Washington, DC 20057

Georgetown University
Washington, DC 20057

kona@cs.georgetown.edu

bansal@cs.georgetown.edu

blakeb@cs.georgetown.edu

Steffen Bleul

Thomas Weise

Universityof Kassel
D-34121,Kassel Germany

Universityof Kassel
D-34121,Kassel Germany

bleul@vs.uni-kassel.de

weise@vs.uni-kassel.de

automated composition software and facilitates the
dissemination of results that advance this field.
The WSC-09 represents the fifth event of the
matchmaking and composition challenge series. It
extends the original criteria of the first two
competitions which focused on service discovery and
service composition based on the syntactic matching of
WSDL part names. It also further extends the third and
fourth competition in 2006 and 2007 which provided
taxonomy of parameter types represented using the
natural hierarchies that are captured using simple and
complex types within XML documents. WSC-08
further evolved with the adoption of ontologies written
in OWL to provide semantics.
The 2009 competition is a continuation of the
evolution of the challenge considering non-functional
properties of a web service. The Quality of Service of a
web service is expressed by its response time and
throughput. It has been the tradition of the WSC events
to adhere to technology-independent approaches to
semantics. In this way, the competition attempts to
circumvent debates on representations, such as
differences between approaches like OWL-S [8],
WSML [7], and WSDL-S [12]. In 2006, the use of
pure XML-based semantics allowed for a bi-partisan
approach. In 2007, we have evolved the challenge by
mirroring the XML-based semantics with equivalent
representations using OWL ontology. During the
previous three years, web service challenge focused on
optimizing the discovery and composition process
solely using abstractions from real-world situations.
The taxonomies of semantic concepts as well as the
involved data formats were purely artificial. In 2008,
we have further evolved the challenge by using data
formats and contest data based on OWL, WSDL, and
WSBPEL schemas for ontologies, services, and service
orchestrations.

Abstract
With the growing acceptance of service-oriented
computing, an emerging area of research is the
investigation of technologies that will enable the
discovery and composition of web services. The Web
Services Challenge (WSC) is a forum where academic
and industry researchers can share experiences of
developing tools that automate the integration of web
services. In the fifth year (i.e. WSC-09) of the Web
Services Challenge, software platforms will address
several new composition challenges. Requests and
results will be transmitted within SOAP messages.
Semantics will be represented as ontologies written in
OWL, services will be represented in WSDL, and
service orchestrations will be represented in WSBPEL.
In addition, non-functional properties (Quality of
Service) of a service will be represented using WSLA
format.

1. Introduction
The annual Web Services Challenge has provided a
platform for researchers in the area of Web Service
Composition since 2005. Succeeding the EEE-05
[2][11], WSC-06, WSC-07, and WSC-08 Challenges
[3][4][5], the 2009 Web Services Challenge (WSC-09)
[6] is the fifth year of this SOA venue [1] that looks to
benchmark software applications that automatically
manage web services. WSC-09, held at the 11th IEEE
Conference on Commerce and Enterprise Computing
(CEC 2009) continues to provide a forum where
researchers can collaborate on approaches, methods
and algorithms in the domain of web service discovery
and automated composition. This forum provides
quantitative and qualitative evaluation results on the
performance of participating matchmaking and

978-0-7695-3755-9/09 $25.00 © 2009 IEEE
DOI 10.1109/CEC.2009.80

487

file during the bootstrapping process. This file contains
the taxonomy of concepts used in this challenge in
OWL format. The bootstrapping process includes
loading the relevant information from these files. The
challenge task will then be sent to the composer via a
client-side GUI very similar to last year’s challenge.
After the bootstrapping on the server side is finished,
the GUI queries the composition system with the
challenge problem definition. The contestant’s
software must now compute a solution – one or more
service compositions – and answer in the solution
format which is a subset of the WSBPEL schema.
When the WSBPEL document is received by the GUI,
we will stop a time measurement and afterwards
evaluate the compositions themselves.

In the WSC-08, concurrency was introduced into
the composition problem sets. The combination of
complex, workflow-based heuristics with semanticallyrich representations required participants to create new
software that is both robust and efficient. The problem
sets in WSC-09 also are inherently concurrent.

2. Related Venues
Although WSC is perhaps the first venue, other
unique venues have been established to investigate the
need for solutions to the Service Composition Problem.
The Semantic Web Services Challenge [9] is less of a
competition and more of a challenge. Both business
cases and solution applications are the focus of the
venue. Participants are placed in a forum where they
can incrementally and collaboratively learn from each
other. While WSC venues are more driven by
application, the SWS venues concentrate more on the
environment. As such, the SWS venues place more
focus on semantics where the WSC favors applied,
short-term solutions.
Alternatively, the SOA Contest [10] held at the
International Conference on Services Computing
(SCC2006, SCC2007, and SCC2008) allows
participants to openly choose the problems that best
demonstrate their approach. The benefit of this venue
is that participants can show the best approach for a
particular domain-specific problem. In contrast, the
WSC venue attempts to set a common problem where
approaches can be evaluated side-by-side.
There is a unique niche for each forum and the
composition of the results from all the venues will
undoubtedly advance the state-of-the-art in serviceoriented computing.

Bootstrap Phase

Challenge Server Side
WSDL
File

OWL
File

Challenge Client Side

WSLA
Files

Evaluation

WSDL of
Required
Service
Parse WSDL
Service Descriptions

Parse OWL
Ontology

Parse WSLA
QoS Description

Compute Service
Composition

WSDL of
Required
Service

Generate
WSBPEL

WSBPEL
File

Interface
Package

WSBPEL
File

Figure 1: The procedure of the Web Service
Challenge 2009.

3. The Challenge

3.1. What’s New





In the competition, we adopt the idea of so-called
Semantic Web Services that represent Web Services
with a semantic description of the interface and its
characteristics. The task is to find a composition of
services that produces a set of queried output
parameters from a set of given input parameters. The
overall challenge procedure is shown in Figure 1. The
composer software of the contestants is placed on the
server side and started with a bootstrap procedure. First
of all, the system is provided with a path to a WSDL
file. The WSDL file contains a set of service
descriptions along with annotations of input- and
output parameters. The number of services will change
from challenge to challenge. Every service will have an
arbitrary number of parameters. Additional to the
WSDL file, we also provide the address of the OWL

•
•

Document Formats: WSLA format
Quality of Service (QoS): Each service
will be annotated with its non-functional
properties on response time and
throughput. The contestants do not have to
find the shortest or minimal composition
considering the amount of services. The
contestants should, instead, find the
composition with the least response time
and the highest possible throughput.

3.2. Semantics
Ontologies are usually expressed with OWL’s XML
format [13][14]. We use OWL format in the 2009
challenge, but like in the previous years, we limit

488

milliseconds and the throughput (invocations per
minute) of each service in a challenge set. Metrics can
be omitted as they do not contain relevant information
for the service composition. They are interesting
nonetheless as they present the capabilities of WSLA.

semantic evaluation strictly to taxonomies consisting
of sub and super class relationship between semantic
concepts only. OWL is quite powerful. In addition to
semantic concepts (OWL-Classes), OWL allows to
specify instances of classes called individuals. While
we also distinguish between individuals and classes in
the competition, the possibility to express equivalence
relations between concepts is not used. In OWL, the
semantics are defined with statements consisting of
subject, predicate, and object, e.g. ISBN-10 is_a
ISBN (ISBN subsumes ISBN-10). Such statements
can be specified with simple triplets but also with
XML-Hierarchies
and
XML-References.
The
implementation of an OWL-Parser is hence not trivial.
In order to ease the development of the competition
contributions, we will stick to a fixed but valid OWLSchema.

3.4. Evaluation
The Web Service Challenge awards the most
efficient system and also the best architectural solution.
The best architectural effort will be awarded according
to the contestant’s presentation and system features.
The evaluation of efficiency consists of two parts as
shown in Figure 2.
The BPEL checking software evaluates the results
of the participant’s composition system. The BPEL file
is examined for a solution path and its correctness with
respect to the challenge task.
1. Three challenge sets are provided and each
composition system can achieve up to 18 points.
2. The time limit for solving a challenge is five
minutes. Every composition system replying with a
solution later than five minutes will receive 0 points for
the challenge set.
3. The task is to find the service composition
solution with the lowest response time. Additionally
the composition system that finds the service
composition with the highest throughput in the fastest
time will be rewarded.

3.3. Quality of Service
The Quality of Service for a service can be
specified using the Web Service Level Agreements
(WSLA) [15], language from IBM. In contrast to the
Web Service Agreements (WS-A) language, WSLA is
in its final version. Furthermore, WSLA offers more
specific information than WS-A. We can not only
specify the Service Level Objectives (SLO) of a
service and its service operations, but also the
measurement directives and measurement endpoints
for each quality dimension. WSLA represents a
configuration for a SLA management and monitoring
system. In contrast to WS-A, WSLA enables the
automated discovery and deployment of service
contracts inside SOAs. In the WSC-09, we define the
following quality dimensions for a Web Service. They
can be accessed in this document format and must be
calculated for a whole BPEL process.
• Response Time: In a data system, the
system response time is the interval between the
receipt of the end of transmission of an inquiry
message and the beginning of the transmission of a
response message to the station where the inquiry
originated.1 Example: 200 milliseconds.
• Throughput: In communication networks
such as Ethernet or packet radio, throughput is the
average rate of successful message delivery over a
communication channel.2 Example: 10.000 (successful)
invocations per minute.
We define one WSLA document containing
the specification of the average response time in

WSDL of
available
Services

Compute Service
Composition
Interface
Package

Time
Measurement

WSBPEL
file

Composition
Evaluation

Generate
WSBPEL

Challenge Score

Figure 2: Evaluation in the WS-Challenge 2009.

4. Logistics of the Web Services Challenge
WSC-09 has attracted international teams which
come from universities and research organizations in
countries including the Slovak Republic, China,
Vienna, Netherlands, South Korea, and the United
States. The organization of this event is divided into
two phases: The first phase focuses on evaluating the

1

http://en.wikipedia.org/wiki/Response_time_(technology)
[accessed on 2009-05-12]

2

http://en.wikipedia.org/wiki/Throughput [accessed on 200905-12]

489

technical viability of the methodologies proposed by
the participating teams. A four-page technical
description submitted from each team is peer-reviewed
and included in the proceedings of the conference CEC
2009. Once the reviewing and acceptance notification
phases have completed, teams that successfully
complete this first step are asked to submit a version of
their software for evaluation. Preliminary tests are
conducted using this evaluation version to ensure the
compatibility and applicability during the final
competition. The main objective is to avoid, in
advance, potential format related problems that may
otherwise occur when the competition takes place.
The second phase is the final competition which is
scheduled for two days at CEC-09. On the first day,
all the participating teams will present their approaches
in a specialized session of the conference. On the same
day, the participants are allotted times to install the
latest version of their software on the evaluations
stations located onsite at the conference. On the second
day, the teams must execute their software using a
customized data set prepared specifically for the
competition. Participating software is measured for
performance during any indexing phases and during
the actual composition routine. Composition results
are evaluated against known solutions for correctness
and completeness. In 2009 there will be multiple sets
of correct answers with variable length chains.
Applications will be judged with weighted scores
based on the best solutions that they present.
The solution application with the best qualitative
and quantitative scores when run against several
datasets is awarded first place. The competition
typically has a winner and several runner-ups.

[2]

[3]

[4]
[5]
[6]
[7]
[8]

[9]
[10]
[11]
[12]
[13]

5. Acknowledgements

[14]

The authors would like to acknowledge the efforts of
Georgetown student Brian Miller who facilitated the
web site development for the challenge. Georgetown
graduate student, John Adams, organized the travel
logistics of the web service challenge. The authors also
acknowledge Hong Kong Baptist graduate student,
Kai-Kin Chan, for preparing the OWL representations.
The Web Service Challenge has been extensively
funded by the National Science Foundation under
award number 0548514 and 0723990. The Hewlett
Packard Corporation and Springer-Verlag have also
supported an award to the winners of the competition.

[15]

References
[1] Blake, M.B., Cheung, W., and Wombacher, A. “Web
Services Discovery and Composition Systems”

490

International Journal of Web Services Research, Vol. 4,
No. 1, pp iii – viii, January 2007
Blake, M.B., Tsui, K.C., Cheung, W., “The EEE-05
Challenge: A New Web Service Discovery and
Composition Competition” Proc. of the IEEE Intl.
Conference on E-Technology, E-Commerce, and EServices (EEE-05), Hong Kong, March 2005.
Blake, M.B., Cheung, W., Jaeger, M.C., and
Wombacher, A., “WSC-06: The Web Service
Challenge”, Joint Proceedings of the CEC/EEE 2006,
San Francisco, California, USA, June 2006.
The
Web
Services
Challenge
(2007).
http://www.wschallenge.org/wsc07/
The
Web
Services
Challenge
(2008).
http://cec2008.cs.georgetown.edu/wsc08/
The Web Services Challenge (2009). http://www.wschallenge.org/wsc09
Fensel, D. and Bussler, C. “The Web Service Modeling
Framework”, Electronic Commerce: Research and
Applications, 1(2): 113-137, 2002
Martin, D. et al. “Bringing Semantics to Web Services:
The OWL-S Approach”, Proc. of the First Intl.
Workshop on Semantic Web Services and Web Process
Composition (SWSWPC-04), San Diego, USA, July ‘04.
The Semantic Web Services Challenge (2007):
http://sws-challenge.org/wiki/index.php/Main_Page
The
Services
Computing
Contest
(2007):
http://iscc.servicescomputing.org/2007/
The Web Services Challenge at the IEEE Conference on
e-Business
Engineering
(ICEBE-05)
(2007):
http://www.comp.hkbu.edu.hk/simctr/wschallenge/
WSDL-S
(2007):
http://www.w3.org/Submission/WSDL-S/
Bechhofer, S., Harmelen, F., Hendler, J., Horrocks, I.,
McGuinness, D., Patel-Schneider, P., and Stein L.
OWL Web Ontology Language Reference. World Wide
Web Consortium (W3C), February 10, 2004. Online
available at http://www.w3.org/TR/2004/REC-owl-ref20040210/
Bray, T., Paoli, J., Sperberg-McQueen, C., Maler, E.,
and Yergeau, F. Extensible Markup Language (XML)
1.0 (Fourth Edition). World Wide Web Consortium
(W3C), September 29, 2007. Online available at
http://www.w3.org/TR/2006/REC-xml-20060816
Keller, A., Ludwig, H. The WSLA Framework:
Specifying and monitoring service level agreements for
Web services. Journal of Network System Management,
11(1), 2003.

2011 IEEE World Congress on Services

Reputation-based Web service selection for Composition
Srividya K Bansal
Department of Engineering
Arizona State University
Mesa, Arizona 85212, USA
Email: srividya.bansal@asu.edu

Ajay Bansal
Department of Computer Science
Georgetown University
Washington, DC 20057, USA
Email: bansal@cs.georgetown.edu

rest of the network. So our rationale is that these central
ﬁgures who play a fundamental role in the network are
trusted by others in the network who are connected (directly
or indirectly) to them.
Our work investigates the following research issues: (i)
compute the reputation score of composition solutions based
on individual scores of service providers obtained using the
centrality measure of social networks (ii) set a threshold for
reputation that each and every Web service involved in the
composition has to satisfy. Failure to meet the threshold will
result in ﬁltering out the Web service and it will not be used
in any composition solution.

Abstract—The success and acceptance of Web service composition depends on computing solutions comprised of trustworthy services. In this paper, we extend our Web service Composition framework to include selection and ranking of services
based on their reputation score. With the increasing popularity
of Web-based Social Networks like Linkedin, Facebook, and
Twitter, there is great potential in determining the reputation
score of a particular service provider using Social Network
Analysis. We present a technique to calculate a reputation score
per service using centrality measure of Social Networks. We
use this score to produce composition solutions that consist of
services provided by trust-worthy and reputed providers.
Keywords-service composition; reputation; social networks;

I. I NTRODUCTION

II. C ENTRALITY M EASURE IN S OCIAL N ETWORKS :

The next milestone in the evolution of the World Wide
Web is making services ubiquitously available. We need
infrastructure that applications can use to automatically
discover, deploy, compose, and synthesize services. Along
with the functional attributes there is a need to consider
non-functional attributes (Quality of Service parameters) of
Web services in the process of building composite Web
services. The current challenge in automatic composition of
Web services also includes ﬁnding a composite Web service
that can be trusted by consumers before using it. In this
paper, we present our approach that uses analysis of Social
Networks to calculate a reputation score for each service
involved in the composition and further prune results based
on this score.
Web-based Social Networks have become increasingly
popular these days. Social Network Analysis is the process
of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups,
organizations, computers, or any knowledge entity. We propose to measure the reputation of a service by measuring
the centrality of a service provider and/or a service provider
organization in a well-known Social Network. We adopt our
idea of computing a reputation score using centrality measure based on the notion of centrality and prestige being key
in the study of social networks [1], [2]. The role of central
people (nodes with high centrality) in a network seems to
be fundamental as they adopt the innovation and help in
transportation and diffusion of information throughout the

Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds.
It involves measuring the formal and informal relationships
to understand information/knowldege ﬂow that binds the
interacting units that could be a person, group, organization,
or any knowledge entity. In order to understand social
networks and their participants, the location of an actor in a
network is evaluated. The network location is measured in
terms of centrality of a node that gives an insight into the
various roles and groupings in a network.
Centrality gives a rough indication of the social power
of a node based on how well they “connect” the network.
The graph-theoretic conception of compactness has been
extended to the study of Social Networks and simply renamed “graph centrality” [1]. Their measures are all based
upon distances between points, and all deﬁne graphs as
centralized to the degree that their points are all close
together. Based on research on communication in Social
Networks, the centrality of an entire network should index
the tendency of a single point to be more central than all
other points in the network. Measures of a graph centrality
are based on differences between the centrality of the most
central point and that of all others. Thus, they are indexes
of the centralization of the network [4]. The three most
popular individual centrality measures are Degree Centrality,
Betweenness Centrality, and Closeness Centrality.
Degree Centrality: The network activity of a node can
be measured using the concept of degrees, i.e., the number

978-0-7695-4461-8/11 $26.00 © 2011 IEEE
DOI 10.1109/SERVICES.2011.96

95

ServiceProvider
Provider A
Provider B
Provider C
Provider D
Provider E
Provider F
Provider G

Degree
2
3
1
8
3
4
5

ServiceProvider
Provider H
Provider I
Provider J
Provider K
Provider L
Provider M

Degree
3
1
2
4
1
1

CD (sk ) =

a(si , sk )

i=0

where a(si , sk ) = 1 iff si and sk are connected
0 otherwise
As such it is a straightforward index of the extent to which
sk is a focus of activity. CD (sk ) is large if service provider
sk is adjacent to, or in direct contact with, a large number of
other service providers, and small if sk tends to be cut off
from such direct contact. CD (sk ) = 0 for a service provider
that is totally isolated from any other point. Our algorithm
ﬁlters out any services whose provider has a zero degree
centrality in a social network, i.e., such services will not be
used in building composition solutions. Reputation of the
entire composite service is computed as an average of the
individual reputation score of the services involved in the
composition.
We also need to set a reputation threshold and any service
with a reputation score that is below this threshold is not
used while generating composition solutions. In our initial
prototype implementation we set the reputation threshold
to zero, i.e., degree centrality of the service provider in
the network is zero. A service provider or service provider
organization that is not connected to any other nodes in
the Social network is not known to anyone else and is
an immediate reason to be pruned out from composition
solutions as the service cannot be trusted. Composition
solutions can be ranked such that solutions with highest
reputation score appear on top of the list.

Table I
D EGREE C ENTRALITY OF N ODES IN F IGURE 1

Figure 1.

n


A Social Network of Web service Providers

of direct connections a node has. In the example network
shown in ﬁgure 1 and table I, Provider D has the most direct
connections in the network, making it the most active node
in the network. In personal Social Networks, the common
thought is that “the more connections, the better”.
Betweenness Centrality: Though Provider D has many
direct ties, Provider H has fewer direct connections (close
to the average in the network). Yet, in many ways, Provider
H has one of the best locations in the network by playing
the role of a “broker” between two important components.
Closeness Centrality: Provider F and G have fewer connections than Provider D, yet the pattern of their direct and
indirect ties allow them to access all the nodes in the network
more quickly than anyone else. They have the shortest paths
to all other and hence are in an excellent position to have
the best visibility into what is happening in the network.
Individual network centralities provide insight into the individual’s location in the network. The relationship between
the centralities of all nodes can reveal much about the overall
network structure.

IV. C ONCLUSIONS AND F UTURE W ORK
In this paper, we presented our approach to compute
reputation of services and use this score to select services
for composition. A reputation score is computed for every
service in the repository based on degree centrality of
the service provider in a well-known Web-based Social
Network. Our future work includes exploring other measures
of centrality such as betweenness centrality and closeness
centrality and analyzing the possibility of using a combination of all three measures of centrality to compute reputation
of a service and/or provider.
R EFERENCES

III. R EPUTATION - BASED W EB SERVICE SELECTION FOR

[1] L. C. Freeman, Centrality in Social Networks Conceptual
Clariﬁcation, in Social Networks, Vol. 1, No. 3. (1979), pp.
215-239.

COMPOSITION

We extend our previous work on Web service composition
[3] (that uses both functional and non-functional parameters
to compute composition solutions) by using reputation
to ﬁlter services. The reputation score of each service
in a Web service repository is computed as a measure
of the degree centrality (CD ) of the social network to
which the service provider belongs. It is calculated as the
degree or count of the number of adjacencies for a node, sk :

[2] S. Wasserman, and K. Faust, Social Network Analysis: Methods
and Applications, Cambridge: Cambridge Univ. Press, 1994.
[3] S. Kona, A. Bansal, M. Blake, and G. Gupta, Generalized
Semantics-based Service Composition in Proceedings of IEEE
Intl. Conference on Web Services (ICWS), September 2008.
[4] H. J. Leavitt, Some effects of communication patterns on group
performance in Journal of Abnormal and Social Psychology,
pp. 46:38-50, 1951.

96

Goal-Directed Execution of Answer Set Programs
Kyle Marple

Ajay Bansal

Richard Min

University of Texas at Dallas
800 W. Campbell Road
Richardson, TX, USA
kbm072000@utdallas.edu

Arizona State University
7231 E. Sonoran Arroyo Mall
Mesa, Arizona, USA
Ajay.Bansal@asu.edu

University of Texas at Dallas
800 W. Campbell Road
Richardson, TX, USA
min75243@hotmail.com

Gopal Gupta
University of Texas at Dallas
800 W. Campbell Road
Richardson, TX, USA
gupta@utdallas.edu

Abstract

enumerate—via SLD style call expansions and backtracking—all
answer sets that contain the propositions/predicates in Q. Other
efforts have been made to realize goal-directed implementations
(e.g., [5]), however, these approaches can handle only a limited
class of programs and/or queries.
In this paper we describe a goal-directed execution method that
works for any answer set program as well as for any query. The
method relies on coinductive logic programming (co-LP) [14]. CoLP can be regarded as providing an operational semantics, termed
co-SLD resolution, for computing greatest fixed points (gfp) of
logic programs. Co-SLD resolution systematically computes elements of the gfp of a program via backtracking [14, 27]. Additionally, calls are allowed to coinductively succeed if they unify with
one of their ancestor calls [27]. To permit this, each call is stored
in the coinductive hypothesis set (CHS) as the call is made. A more
detailed introduction to co-LP and co-SLD resolution can be found
in Appendix A.
A goal-directed method for executing answer set programs is
analogous to top-down, SLD style resolution for Prolog, while current popular methods for ASP are analogous to bottom-up methods
that have been used for evaluating Prolog (and Datalog) programs
[25]. A goal-directed execution method for answering queries for
an answer set program has several advantages. The main advantage
is that it paves the way to lifting the restriction to finitely groundable programs, and allows realization of ASP with full first-order
predicates [20].
In the rest of the paper we develop a goal-directed strategy
for executing answer set programs, and prove that it is sound and
complete with respect to the method of Gelfond and Lifschitz.
We restrict ourselves to only propositional (grounded) answer set
programs in this paper; work is in progress to extend our goaldirected method to predicate answer set programs [20, 21]. Note
that the design of a top-down goal-directed execution strategy for
answer set programs has been regarded as quite a challenging
problem [3]. As pointed out in [6], the difficulty in designing a
goal-directed method for ASP comes about due to the absence of a
relevance property in stable model semantics, on which answer set
programming is based [6, 23, 24]. We will introduce a modified
relevance property that holds for our goal-directed method and
guarantees that partial answer sets computed by our method can
be extended to complete answer sets.

Answer Set Programming (ASP) represents an elegant way of introducing non-monotonic reasoning into logic programming. ASP
has gained popularity due to its applications to planning, default
reasoning and other areas of AI. However, none of the approaches
and current implementations for ASP are goal-directed. In this paper we present a technique based on coinduction that can be employed to design SLD resolution-style, goal-directed methods for
executing answer set programs. We also discuss advantages and applications of such goal-directed execution of answer set programs,
and report results from our implementation.
Categories and Subject Descriptors
niques]: Logic Programming

D.1.6 [Programming Tech-

General Terms Algorithms
Keywords Answer set programming, goal-directed execution,
coinduction

1.

Introduction

Answer Set Programming (ASP) is an elegant way of developing non-monotonic reasoning applications. ASP has gained wide
acceptance, and considerable research has been done in developing the paradigm as well as its implementations and applications.
ASP has been applied to important areas such as planning, scheduling, default reasoning, reasoning about actions [3], etc. Numerous
implementations of ASP have been developed, ranging from DLV
([17]) and smodels [22] to SAT-based solvers such as cmodels [13]
and the conflict-driven solver clasp [10]. However, these implementations compute the whole answer set: i.e., they are not goaldirected in the fashion of Prolog. Given an answer set program
and a query goal Q, a goal-directed execution will systematically

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
PPDP’12, September 19–21, 2012, Leuven, Belgium.
c 2012 ACM 978-1-4503-1522-7/12/09. . . $10.00
Copyright 

35

2.

Answer Set Programming (ASP)

2. Cyclical rules which when used to expand a call to subgoal G
lead to a recursive call to G that is in the scope of an odd number
of negations. Such recursive calls are known as odd loops over
negation (OLONs). For example, given the program P2 below:

Answer Set Programming (ASP) [12] (A-Prolog [11] or AnsProlog
[3]) is a declarative logic programming paradigm which encapsulates non-monotonic or common sense reasoning. The rules in an
ASP program are of the form:
p :- q1 , ..., qm , not r1 , ..., not rn .
where m ≥ 0 and n ≥ 0. Each of p and qi (∀i ≤ m) is a literal,
each not rj (∀j ≤ n) is a naf-literal (not is a logical connective
called negation as failure (naf) or default negation). The semantics
of an Answer Set program P is given via the Gelfond-Lifschitz
method [3] in terms of the answer sets of the program ground(P),
obtained by grounding the variables in the program P.

p :- q, not p, r. . . . Rule P2.a
a call to p using Rule P2.a will eventually lead to a call to
not p. Under ordinary logic programming execution, this will
lead to non-termination. Under ASP, however, the program
consisting of Rule P2.a has {not p, not q, not r} as its
answer set. For brevity, we refer to rules containing OLONs as
OLON rules.

Gelfond-Lifschitz Transform (GLT) Given a grounded Answer
Set program P and a candidate answer set A, a residual program R
is obtained by applying the following transformation rules: for all
literals L ∈ A,

Note that a rule can be both an ordinary rule and an OLON rule,
since given a subgoal G, its expansion can lead to a recursive call to
G through both even and odd numbers of negations along different
expansion paths. For example in program P3 below, Rule P3.a is
both an ordinary rule and an OLON rule.

1. Delete all rules in P which have not L in their body.
2. Delete all the remaining naf-literals (of the form not M) from
the bodies of the remaining rules.

p :- q, not r.
r :- not p.
q :- t, not p.

The least fixed-point (say, F) of the residual program R is next
computed. If F = A, then A is a stable model or an answer set
of P.
ASP can also have rules of the form:
:- q1 , ..., qm , not r1 , ..., not rn .
p :- q1 , ..., qm , not r1 , ..., not rn , not p.
These rules capture the non-monotonic aspect of Answer Set Programming. Consider an example rule:
p :- q, not p.
Following the Gelfond-Lifschitz method (GL method) outlined
above, this rule restricts q (and p) to not be in the answer set (unless p happens to be in the answer set via other rules, in which case
due to presence of not p this rule will be removed while generating the residual program). Note that even though an answer set
program can have other rules to establish that q is in the answer set,
adding the rule above forces q to not be in the answer set unless p
succeeds through another rule, thus making ASP non-monotonic.

3.

Our top-down method requires that we properly identify and
handle both ordinary and OLON rules. We will look at each type of
rule in turn, followed by the steps taken to ensure that our method
remains faithful to the GL method.
3.1

Ordinary Rules

Ordinary rules such as rules P1.a and P1.b in program P1 above
exemplify the cyclical reasoning in ASP. The rules in the example
force p and q to be mutually exclusive, i.e., either p is true or q is
true, but not both. One can argue the reasoning presented in such
rules is cyclical: If p is in the answer set, then q cannot be in the
answer set, and if q is not in the answer set, then p must be in the
answer set.
Given a goal, G, and an answer set program comprised of only
ordinary rules, G can be executed in a top-down manner using
coinduction, through the following steps:

Goal-directed ASP Issues

• Record each call in the CHS. The recorded calls constitute the

Any normal logic program can also be viewed as an answer set program. However, ASP adds complexity to a normal logic program in
two ways. In addition to the standard Prolog rules, it allows:

coinductive hypothesis set, which is the potential answer set.
• If at the time of the call, the call is already found in the CHS, it

succeeds coinductively and finishes.

1. Cyclical rules which when used to expand a call to a subgoal
G lead to a recursive call to G through an even (but non-zero)
number of negations. For example, given the program P1 below:
p :- not q.
q :- not p.

. . . Rule P3.a
. . . Rule P3.b
. . . Rule P3.c

• If the current call is not in the CHS, then expand it in the style

of ordinary SLD resolution (recording the call in the CHS prior
to expansion).
• Simplify not not p to p, whenever possible, where p is a

. . . Rule P1.a
. . . Rule P1.b

proposition occurring in the program.
• If success is achieved with no goals left to expand, then the

Ordinary logic programming execution for the query ?- p.
(or ?- q.) will lead to non-termination. However, ASP will
produce two answer sets: {p, not q} and {q, not p}1 . Expanding the call p using Rule P1.a in the style of SLD resolution
will lead to a recursive call to p that is in scope of two negations
(p → not q → not not p). Such rules are termed ordinary
rules. Rule P1.b is also an ordinary rule, since if used for expanding the call to q, it will lead to a recursive call to q through
two negations. For simplicity of presentation, all non-cyclical
rules will also be classified as ordinary rules.

coinductive hypothesis set contains the (partial) answer set.
The top-down resolution of query p with program P1 will proceed
as follows.
CHS = {}
(expand p by Rule P1.a)
:- not q
CHS = {p}
(expand q by Rule P1.b)
:- not not p CHS = {p, not q}
(simplify not not p → p )
:- p
CHS = {p, not q}
(coinductive success: p ∈ CHS)
:- 
success: answer set is {p, not q}
:- p

1 Note

that we will list all literals that are true in a given answer set.
Conventionally, an answer set is specified by listing only the positive literals
that are true; those not listed in the set are assumed to be false.

36

Note that the maintenance of the coinductive hypothesis set (CHS)
is critical. If a call is encountered that is already in the CHS, it
should not be expanded, it should simply (coinductively) succeed.
Note that the query q will produce the other answer set {q, not
p} in a symmetrical manner. Note also that the query not q will
also produce the answer set {p, not q} as shown below. Thus,
answers to negated queries can also be computed, if we apply the
coinductive hypothesis rule to negated goals also, i.e., a call to not
p succeeds, if an ancestor call to not p is present:
:- not q
:- not not p
:- p
:- not q
:- 
3.2

expanded by rule P4.1; if the call to q fails, then the goal p also
fails. Alternatively, if q succeeds due to other rules in the program,
then upon arriving at the call not p, failure will ensue, since not
p is inconsistent with the current CHS (which equals {p, q} prior
to the call not p).
Thus, OLON rules do not pose any problems in top-down execution based on coinduction, however, given an OLON rule with
p as its head, if p can be inferred by other means (i.e., through
ordinary rules) then the query p should succeed. Likewise, if q succeeds by other means and p does not, then we should report a failure (rather, report the absence of an answer set; note that given our
conventions, CHS = {} denotes no answer set). We discuss how
top-down execution of OLON rules is handled in Section 3.4.

CHS = {}
(expand q by Rule P1.b)
CHS = {not q}
(not not p → p)
CHS = {p, not q}
(expand p by Rule P1.a)
CHS = {p, not q}
(coinductive success for not q)
success: answer set is {p, not q}

3.3

OLON Rules

Our goal-directed procedure based on coinduction must also work
with OLON rules. OLON rules are problematic because their influence on answer sets is indirect. Under ASP, rules of the form
p :- q1 , q2 , ..., qk , not p.
hold only for those (stable) models in which p succeeds through
other rules in the program or at least one of the qi ’s is false. Note
that a headless rule of the form:
:- q1 , q2 , ..., qn .
is another manifestation of an OLON rule, as it is equivalent to the
rule:
p :- q1 , q2 , ..., qn , not p.
where p is a literal that does not occur anywhere else in the program, in the sense that the stable models for the two rules are identical.
Without loss of generality, consider the simpler rule:
p :- q, not p.
For an interpretation to be a (stable) model for this rule, either p
must succeed through other rules in the program or q must be false.
Two interesting cases arise: (i). p is true through other rules in the
program. (ii) q is true through other rules in the program.
For case (i), if p is true through other means in the program,
then according to the Gelfond-Lifschitz method, it is in the answer
set, and the OLON rule is taken out of consideration due to the
occurrence of not p in its body. For case (ii), if q is true through
other means and the rule is still in consideration due to p not being
true through other rules in the program, then there are no answer
sets, as q is both true and false. Thus, the answer set of the program
P4 below is: {p, not q}.
p :- q, not p.
p.

p :- q.
q :- p.

. . . Rule P6.1
. . . Rule P6.2

Our method based on coinduction will succeed for queries :p and :- q producing the answer set {p, q} while under the GL
method, the answer set for this program is {not p, not q}. Our
top-down method based on coinduction really computes the gfp of
the original program. The GL method computes a fixed point of the
original program (via the GL transformation and then computation
of the lfp of the residual program) that is in between the gfp and
the lfp of the original program. In the GL method, direct cyclical
reasoning is not allowed, however, cyclical reasoning that goes
through at least one negated literal is allowed. Thus, under the GL
method, the answer set of program P6 does not contain a single
positive literal, while there are two answer sets for the program P1
given earlier, each with exactly one positive literal, even though
both programs P1 and P6 have only cyclical rules.
Our top-down method can be modified so that it produces answer sets consistent with the GL method: a coinductive recursive
call can succeed only if it is in the scope of at least one negation.
In other words, the path from a successful coinductive call to its
ancestor call must include a call to not.
This restriction disallows inferring p from rules such as
p :- p.
With this operational restriction in place, the CHS will never contain a positive literal that is in the gfp of the residual program obtained after the GLT, but not in its lfp. To show this, let us assume
that, for some ASP program, a call to p will always encounter at
least one recursive call to p with no intervening negation. In such a
case, p will never be part of any answer set:

. . . Rule P4.1
. . . Rule P4.2

• Under our goal-directed method, any call to p will fail when a

while there is no answer set for program P5 below:
p :- q, not p.
q.

Coinductive Success Under ASP

While our technique’s use of co-SLD resolution has been outlined
above, it requires some additional modification to be faithful to
the Gelfond-Lifschitz method. Using normal coinductive success,
our method will compute the gfp of the residual program after the
GL transform, while the GL method computes the lfp. Consider
Program P6 below:

recursive call is encountered with no intervening negation.

. . . Rule P5.1
. . . Rule P5.2

• Under the GL method, p will never be in the lfp of the residual.

Even if a rule for p is present in the residual and all other
dependencies are satisfied, the rule will still depend on the
recursive call to p.

Given an OLON rule with p as its head and the query p, execution based on co-SLD resolution will fail, if we require that the
coinductive hypothesis set (CHS) remains consistent at all times.
That is, if we encounter the goal g (resp. not g) during execution
and not g ∈ CHS (resp. g ∈ CHS), then the computation fails and
backtracking ensues.
As another example, consider the program containing rule P4.1
(which has p in its head), but not rule P4.2, and the query :p. When execution starts, p will be added to the CHS and then

3.4

NMR Consistency Check

To summarize, the workings of our goal-directed strategy are as
follows: given a goal G, perform co-SLD resolution while restricting coinductive success as outlined in Section 3.3. The CHS serves
as the potential answer set. A successful answer will be computed
only through ordinary rules, as all OLON rules will lead to fail-

37

ure due to the fact that not h will be encountered with proposition
h present in the CHS while expanding with an OLON rule whose
head is h. Once success is achieved, the answer set is the CHS. As
discussed later, this answer set may be partial.
The answer set produced by the process above is only a potential
answer set. Once a candidate answer set has been generated by coSLD resolution as outlined above, the set has to be checked to see
that it will not be rejected by an OLON rule. Suppose there are n
OLON rules in the program of the form:
qi :- Bi .
where 1 ≤ i ≤ n and each Bi is a conjunction of goals. Each Bi
must contain a direct or indirect call to the respective qi which is
in the scope of odd number of negations in order for qi :- Bi . to
qualify as an OLON rule.
If a candidate answer set contains qj (1 ≤ j ≤ n), then
each OLON rule whose head matches qj must be taken out of
consideration (this is because Bj leads to not(qj ) which will be
false for this candidate answer set). For all the other OLON rules
whose head proposition qk (1 ≤ j ≤ n) is not in the candidate
answer set, their bodies must evaluate to false w.r.t. the candidate
answer set, i.e., for each such rule, Bk must evaluate to false w.r.t.
the candidate answer set.
The above restrictions can be restated as follows: a candidate
answer set must satisfy the formula qi ∨ not Bi (1 ≤ i ≤ n) for
each OLON rule qi :- Bi . (1 ≤ i ≤ n) in order to be reported as
the final answer set. Thus, for each OLON rule, the check
chk qi :- qi .
chk qi :- not Bi .
is constructed by our method. Furthermore, not Bi will be expanded to produce a chk qi clause for each literal in Bi . For example, if Bi represented the conjunction of literals s, not r, t
in the above example, the check created would be:
chk qi :- qi .
chk qi :- not s.
chk qi :- r.
chk qi :- not t.
A candidate answer set must satisfy each of these checks in
order to be reported as a solution. This is enforced by rolling the
checks into a single call, termed nmr chk:
nmr chk :- chk q1 , chk q2 , ...chk qn .
Now each query Q is transformed to Q, nmr chk. before it is posed
to our goal-directed system. One can think of Q as the generator
of candidate answer sets and nmr chk as the filter. If nmr chk
fails, then backtracking will take place and Q will produce another
candidate answer set, and so on. Backtracking can also take place
within Q itself when a call to p (resp. not p) is encountered and
not p (resp. p) is present in the CHS. Note that the CHS must be
a part of the execution state, and be restored upon backtracking.

not H :- not B1 , not B2 , ..., not Bn .
If a proposition q appears in the body of a rule but not in any of the
rule heads, then the fact
not q.
is added. Note that adding the dual rules is not necessary; it only
makes the exposition of our goal-directed method easier to present
and understand.
4.2

Goal-directed Method for Computing Answer Sets

Given a propositional query :- Q and a propositional answer set
program P, the goal-directed procedure works as described below.
Note that the execution state is a pair (G, S), where G is the current
goal list, and S the current CHS.
1. Identify the set of ordinary rules and OLON rules in the program.
2. Assert a chk qi rule for every OLON rule with qi as its head
and build the nmr check as described in Section 3.4.
3. For each ordinary rule and chk qi rule, construct its dual version.
4. Append the nmr check to the query.
5. Set the initial execution state to: (:- G1 , ..., Gn , {}).
6. Non-deterministically reduce the execution state using the following rules:
(a) Call Expansion:
(:- G1 , .., Gi , .., Gn , S)
→ (:- G1 , .., B1 , .., Bm , .., Gn , S ∪ {Gi })
where Gi matches the rule Gi :- B1 , ..., Bm . in P, Gi
∈
/ S and not Gi ∈
/ S.
(b) Coinductive Success:
(:- G1 , .., Gi−1 , Gi , Gi+1 , .., Gn , S)
→ (:- G1 , .., Gi−1 , Gi+1 , .., Gn , S)
if Gi ∈ S and either:
i. Gi is not a recursive call or
ii. Gi is a recursive call in the scope of a non-zero number
of intervening negations.
(c) Inductive Success:
(:- G1 , .., Gi−1 , Gi , Gi+1 , .., Gn , S)
→ (:- G1 , .., Gi−1 , Gi+1 , .., Gn , S ∪ {Gi })
if Gi matches a fact.
(d) Coinductive Failure:
(:- G1 , .., Gi , .., Gn , S) → (fail, S)
if either:
i. not Gi ∈ S or

4.

Goal-directed Execution of Answer Set
Programs

ii. Gi ∈ S and Gi is a recursive call without any intervening
negations.

We next describe our general goal-directed procedure for computing answer sets.

(e) Inductive Failure:
(:- G1 , .., Gi , .., Gn , S) → (fail, S)
if Gi has no matching rule in P.

4.1

(f) Print Answer:
(:- true, S) → success: S is the answer set
where ‘:- true’ ≡ empty goal list

Dual Rules

For simplicity, we add one more step to the process. Similarly to
Alferes et al [1], for each rule in the program, we introduce its dual.
That is, given a proposition H’s definition (Bi ’s are conjunction of
literals):
H :- B1 .
H :- B2 .
...
H :- Bn .
we add the dual rule

Note that when all the goals in the query are exhausted, execution of nmr chk begins. Upon failure, backtracking ensues, the
state is restored and another rule tried. Note that negated calls are
expanded using dual rules as in [1], so it is not necessary to check
whether the number of intervening negations between a recursive
call and its ancestor is even or odd. (See the call expansion rule

38

relevant rules [6, 23]. For instance, an irrelevant rule of the form p
:- not p. when added to an answer set program P, where P has
one or more stable models and p does not occur in P, results in a
program that has no stable models.
Approaches such as [23] have addressed the lack of a relevance
property by modifying stable model semantics to restore relevance.
However, our implementation can be viewed as restoring relevance
by expanding the definition of relevant rules to include all OLON
rules in a program. Because the NMR check processes every OLON
rule, it has the effect of making the truth value of every literal in a
program dependent on such rules. That is,

above). A detailed example of goal-directed execution can be found
in Appendix B. Next we discuss a few important issues:
Identifying OLON and Ordinary Rules Given a propositional
answer set program P, OLON rules and ordinary rules can be identified by constructing and traversing the call graph. The complexity
of this traversal is O(|P| ∗ n), where n is the number of propositional symbols occurring in the head of clauses in P and |P| is a
measure of the program size. Note also that during the execution
of a query Q, we need not make a distinction between ordinary and
OLON rules; knowledge of OLON rules is needed only for creating
the nmr chk.

nmr rel rul(P, L) = rel rul(P, L) ∪ O,
O = { R: R is an OLON rule in P }

Partial Answer Set Our top-down procedure might not generate
the entire answer set. It may generate only the part of the answer
set that is needed to evaluate the query. Consider program P7:
p
q
r
s

::::-

not
not
not
not

(2)

Using nmr rel rul(P,L) in place of rel rul(P,L), a modified version of equation 1 above holds for our semantics:

q.
p.
s.
r.

SEM (P )(L) = SEM (nmr rel rule(P, L))(L)

(3)

As a result, any partial model returned by our semantics is guaranteed to be a subset of one or more complete models.

Under goal-directed execution, the query :- q. for program P7
will produce only {q, not p} as the answer since the rules defining r and s are completely independent of rules for p and q. One
could argue that this is an advantage of a goal-directed execution
strategy rather than a disadvantage, as only the relevant part of the
program will be explored. In contrast, if the query is :- q, s, then
the right answer {q, not p, s, not r} will be produced by the
goal-directed execution method. Thus, the part of the answer set
that gets computed depends on the query. Correct maintenance of
the CHS throughout the execution is important as it ensures that
only consistent and correct answer sets are produced.

5.2

Soundness

Theorem 1. For the non-empty set X returned by successful topdown execution of some program P, the set of positive literals in X
will be an answer set of R, the set of rules of P used during topdown execution.
Proof. Let us assume that top-down execution of a program P
has succeeded for some query Q consisting of a set of literals in
P, returning
S a non-empty set of literals X. We can observe that
R ⊆
nmr rel rul(P, L): for each positive literal in Q, one
L∈Q

5.

rule with the literal in its head will need to succeed, for each
negative literal in Q all rules with the the positive form of the literal
in their head will need to fail, and the resulting set must satisfy the
NMR check. We will show that X is a valid answer set of R using
the GL method. First, because X may contain negative literals and
the residual program produced by the GL method is a positive one,
let us remove any rules in R containing the positive version of such
literals as a goal, and then remove the negated literals from X to
obtain X’. Because our algorithm allows negative literals to succeed
if and only if all rules for the positive form fail or no such rules
exist, only rules which failed during execution will be removed
by this step. Next, let us apply the GL transformation using X’ as
the candidate answer set to obtain the residual program R’. This
will remove rules containing the negation of any literal in X’ and
remove any negated goals from the remaining rules.
We know that X’ will be an answer set of R if and only if X’
= LFP(R’). Now let us examine the properties of R’. As positive
literals, we know that each literal in X’ must occur as the head of
a rule in R which succeeded during execution. Because such rules
would have failed if the negation of any goal was present in the
CHS, we know that such rules would not have been eliminated from
the residual program by the GL transformation, and are thus still
present in R’ save for the removal of any negated goals. Because
any rules containing the negation of a literal in X had to fail during
execution, at least one goal in each of these rules must have failed,
resulting in the negation of the goal being added to the CHS.
Furthermore, because the NMR check applies the negation of each
OLON rule, again the negation of some goal in each such rule
must have been added to the CHS. Thus any rule which failed
during execution and yet was included in R will have been removed
from R’. Finally, because our algorithm allows coinductive success
to occur only in the scope of at least one negation, the removal
of negated goals from the residual program will ensure that R’

Soundness and Correctness of the
Goal-directed Method

We will now show the correctness of our goal-directed execution
method by showing it to be sound and complete with respect to the
GL method. First, we will examine the modified relevance property
which holds for our method.
5.1

Relevance

As we stated in the introduction, one of the primary problems
with developing a goal-directed ASP implementation is the lack
of a relevance property in stable model semantics. Dix introduces
relevance by stating that, “given any semantics SEM and a program
P, it is perfectly reasonable that the truth-value of a literal L, with
respect to SEM(P), only depends on the subprogram formed from
the relevant rules of P with respect to L” [6]. He formalizes this
using the dependency-graph of P, first establishing that
• “dependencies of (X) := {A : X depends on A}, and
• rel rul(P,X) is the set of relevant rules of P with respect to X,

i.e. the set of rules that contain an A ∈ dependencies of (X)
in their heads” [6]
and noting that the dependencies and relevant rules of ¬X are the
same as those of X [6]. He then defines relevance as, for all literals
L:
SEM (P )(L) = SEM (rel rul(P, L))(L)
(1)
The relevance property is desirable because it would ensure that
a partial answer set computed using only relevant rules for each
literal could be extended into a complete answer set. However,
stable model semantics do not satisfy the definition as given. This
is because OLON rules can alter the meaning of a program and
the truth values of individual literals without occurring in the set of

39

contains no loops. Because the remaining rules in R’ must have
succeeded during execution, their goals must have been added to
the CHS, and therefore those goals consisting of positive literals
form X’. Thus R’ is a positive program with no loops, and each
literal in X’ must appear as the head of some rule in R’ which is
either a fact or whose goals consist only of other elements in X’.
Therefore the least fixed point of R’ must be equal to X’, and X’
must be an answer set of R.

Table 1. N-Queens Problem; Times in Seconds
Problem
Galliwasp clasp cmodels smodels
queens-12
0.033
0.019
0.055
0.112
queens-13
0.034
0.022
0.071
0.132
queens-14
0.076
0.029
0.098
0.362
queens-15
0.071
0.034
0.119
0.592
queens-16
0.293
0.043
0.138
1.356
queens-17
0.198
0.049
0.176
4.293
queens-18
1.239
0.059
0.224
8.653
queens-19
0.148
0.070
0.272
3.288
queens-20
6.744
0.084
0.316
47.782
queens-21
0.420
0.104
0.398
95.710
queens-22
69.224
0.112
0.472
N/A
queens-23
1.282
0.132
0.582
N/A
queens-24
19.916
0.152
0.602
N/A

Theorem 2. Our top-down execution algorithm is sound with
respect to the GL method. That is, for the non-empty set of literals
X returned by successful execution of some program P, the set of
positive literals in X is a subset of one or more answer sets of P.
Proof. As shown above, the positive literals in the set returned
by
S successful execution of P will be an answer set of R ⊆
nmr rel rul(P, L). Because R will always contain all OLON
L∈Q

rules in P, no unused rules in P are capable of affecting the truth
values of the literals in X. Thus the modified definition of relevance
holds for all literals in X under our semantics and the partial answer set returned by our algorithm is guaranteed to be extensible
to a complete one. Thus our algorithm for top-down execution is
sound with respect to the GL method.
5.3

Table 2. MxN-Pigeons Problem (No Solution for M>N)
Problem
Galliwasp
clasp
cmodels smodels
pigeon-10x10
0.020
0.009
0.020
0.025
pigeon-20x20
0.050
0.048
0.163
0.517
pigeon-30x30
0.132
0.178
0.691
4.985
pigeon-8x7
0.123
0.072
0.089
0.535
pigeon-9x8
0.888
0.528
0.569
4.713
pigeon-10x9
8.339
4.590
2.417
46.208
pigeon-11x10
90.082
40.182 102.694
N/A

Completeness

Theorem 3. Our top-down execution algorithm is complete with
respect to the GL method. That is, for a program P, any answer
set valid under the GL method will succeed if used as a query
for top-down execution. In addition, the set returned by successful
execution will contain no additional positive literals.

Table 3. MxN-Coloring problem (No Solution for M=3)
Problem
Galliwasp clasp cmodels smodels
mapclr-4x20
0.018
0.006
0.011
0.013
mapclr-4x25
0.021
0.007
0.014
0.016
mapclr-4x29
0.023
0.008
0.016
0.018
mapclr-4x30
0.026
0.008
0.016
0.019
mapclr-3x20
0.022
0.005
0.009
0.008
mapclr-3x25
0.065
0.006
0.011
0.010
mapclr-3x29
0.394
0.006
0.012
0.011
mapclr-3x30
0.342
0.007
0.012
0.011

Proof. Let X be a valid answer set of P obtained via the GL method.
Then there exists a resultant program P’ obtained by removing
those rules in P containing the negation of any literal in X and
removing any additional negated literals from the goals of the
remaining rules. Furthermore, because X is a valid answer set of
P, X = LFP(P’). This tells us that for every literal L ∈ X there is a
rule in P’ with L as its head, which is either a fact or whose goals
consist only of other literals in X.
Let us assume that X is posed as a query for top-down execution
of P. As we know that each L ∈ X has a rule in P’ with L as its
head and whose positive goals are other literals in X, we know that
such a rule also exists in P, with the possible addition of negated
literals as goals. However, we know that these negated literals must
succeed, that is, all rules with the positive form of such literals in
their heads must fail, either by calling the negation of some literal
in the answer set or by calling their heads recursively without an
intervening negation. Were this not the case, these rules would
remain in P’, their heads would be included in LFP(P’) and X
would not be a valid answer set of P. Therefore, a combination of
rules may be found such that each literal in X appears as the head
of at least one rule which will succeed under top-down execution,
and whose positive goals are all other literals in X. Furthermore,
because each literal in the query must be satisfied and added to the
CHS, and any rule with a goal whose negation is present in the CHS
will fail, such a combination of rules will eventually be executed by
our algorithm. Because such rules would also be present in P’, we
know that they cannot add additional positive literals to the CHS,
as these would be part of LFP(P’), again rendering X invalid.
This leaves the NMR check, which ensures the set returned by
our algorithm satisfies all OLON rules in P. However, we know this
is the case, as the subset of positive literals in the CHS is equal to
X. Because X is a valid answer set of P, there cannot be any rule
in P which renders X invalid, and thus the NMR check must be
satisfiable by a set of literals containing X. We also know that the

NMR check will not add additional positive literals to the CHS, as
any rules able to succeed would be present in P’ and thus present
in LFP(P’).
Therefore any valid answer set X of a program P must succeed
if posed as a query for top-down execution of P. Thus our top-down
algorithm is complete with respect to the GL method.

6.

Performance Results

The goal-directed method described in this paper has been implemented in our system Galliwasp. In addition to the goal-directed
method presented here, Galliwasp incorporates various other techniques to improve performance, including incremental enforcement
of the NMR check [19]. Tables 1, 2 and 3 give performance results
for some example programs. For the purpose of comparison, results
for clasp, cmodels and smodels are also given.
The Galliwasp system consists of two programs, a compiler
and an interpreter. The times given are for our interpreter using
a compiled program and for the other solvers reading a program
grounded by lparse. Neither compilation nor grounding times are
factored into the results. A timeout of 600 seconds was enforced,
with the instances which timed out listed as N/A in the tables.
As these results demonstrate, our goal-directed method is practical and can be efficiently implemented. While additional perfor-

40

8.

mance increases are possible, the Galliwasp interpreter is already
significantly faster than smodels in almost every case and comparable to clasp and cmodels in most cases.

7.

Conclusions

The main contribution of our paper is to present a practical, topdown method for goal-directed execution of Answer Set programs
along with proofs of soundness and completeness. Our method
stays faithful to ASP, and works for arbitrary answer set programs
as well as arbitrary queries. Other methods in the literature either change the semantics, or work for only restricted programs
or queries. Our method achieves this by relying on the coinductive logic programming paradigm. Details of our method were presented, along with proofs of soundness and correctness, and some
preliminary performance results. A goal-directed procedure has
many advantages, the main one being that execution of answer set
programs does not have to be restricted to only finitely groundable
ones. Our work thus paves the way for developing execution procedures for ASP over predicates. A goal-directed strategy permits
an easier integration with other extensions of logic programming,
which in turn makes it possible to develop more interesting applications of ASP and non-monotonic reasoning. Our current work is
focused on refining our implementation to improve efficiency and
add support for features such as constraints and predicates.

Discussion and Related Work

There are many advantages of top-down goal-directed execution of
answer set programs, the main one being that it paves the way to
answer set programming with predicates. The first step is to extend
our method to datalog answer set programs, i.e., programs that
allow only constants and variables as arguments in the predicates
they contain [20].
Another advantage of goal-directed execution is that answer set
programming can be made to work more naturally with other extensions that have been developed within logic programming, such as
constraint programming, abduction, parallelism, probabilistic reasoning, etc. This leads to more sophisticated applications. Timed
planning, i.e., planning in the presence of real-time constraints, is
one such example [2].
With respect to related work, a top-down, goal-directed execution strategy for ASP has been the aim of many researchers in
the past. Descriptions of some of these efforts can be found in
[1, 4, 5, 8, 9, 15, 23, 24, 26]. The strategy presented in this paper is based on one presented by several of this paper’s authors in
previous work [14, 21]. However, the strategy presented in those
works was limited to call-consistent or order-consistent programs.
While the possibility of expansion to arbitrary ASP programs was
mentioned, it was not expanded upon, and the proofs of soundness
and completeness covered only the restricted cases [21].
A query-driven procedure for computing answer sets via an
abductive proof procedure has been explored [7, 16]: a consistency
check via integrity constraints is done before a negated literal is
added to the answer set. However, “this procedure is not always
sound with respect to the above abductive semantics of NAF” [16].
Alferes et al [1] have worked in a similar direction, though this is
done in the context of abduction and again goal-directedness of
ASP is not the main focus. Gebser and Schaub have developed
a tableau based method which can be regarded as a step in this
direction, however, the motivation for their work is completely
different [9].
Bonatti, Pontelli and Tran [5] have proposed credulous resolution, an extension of earlier work of Bonatti [4], that extends SLD
resolution for ASP. However, they place restrictions on the type of
programs allowed and the type of queries allowed. Their method
can be regarded as allowing coinductive success to be inferred only
for negated goals. Thus, given query :- p and program P1, the
execution will look as follows: p → not q → not not p →
not q → success. Compared to our method, their method performs extra work. For example, if rule P1.1 is changed to p :big goal, not q, then big goal will be executed twice. The
main problem in their method is that since it does not take coinduction for positive goals into account, knowing when to succeed
inductively and when to succeed coinductively is undecidable. For
this reason, their method works correctly only for a limited class of
answer set programs (for example, answers to negated queries such
?-not p cannot be computed in a top-down manner). In contrast,
our goal-directed method works correctly for all types of answer
set programs and all types of queries.
Pereira’s group has done significant work on defining semantics for normal logic programs and implementing them, including
implementation in a top-down fashion [1, 23, 24]. However, their
approach is to modify stable model semantics so that the property
of relevance is restored [23]. For this modified semantics, goaldirected procedures have been designed [24]. In contrast, our goal
is to stay faithful to stable model semantics and answer set programming.

9.

Acknowledgments

Thanks to Michael Gelfond, Vladimir Lifschitz, Enrico Pontelli
and Feliks Kluźniak for discussions and feedback.

References
[1] J. J. Alferes, L. M. Pereira, and T. Swift. Abduction in Well-Founded
Semantics and Generalized Stable Models via Tabled Dual Programs.
Theory and Practice of Logic Programming, 4:383–428, July 2004.
[2] A. Bansal. Towards Next Generation Logic Programming Systems.
PhD thesis, University of Texas at Dallas, 2007.
[3] C. Baral. Knowledge Representation, Reasoning and Declarative
Problem Solving. Cambridge University Press, 2003.
[4] P. A. Bonatti. Resolution for Skeptical Stable Model Semantics.
Journal of Automated Reasoning, 27:391–421, November 2001.
[5] P. A. Bonatti, E. Pontelli, and T. C. Son. Credulous Resolution
for Answer Set Programming. In Proceedings of the 23rd national
conference on Artificial Intelligence - Volume 1, AAAI’08, pages 418–
423. AAAI Press, 2008.
[6] J. Dix. A Classification Theory of Semantics of Normal Logic Programs: II. Weak Properties. Fundamenta Informaticae, 22:257–288,
1995.
[7] K. Eshghi and R. A. Kowalski. Abduction compared with negation by
failure. In ICLP, pages 234–254, 1989.
[8] J. Fernández and J. Lobo. A Proof Procedure for Stable Theories. In
CS-TR-3034, Computer Science Technical Report Series. University
of Maryland, 1993.
[9] M. Gebser and T. Schaub. Tableau Calculi for Answer Set Programming. In Proceedings of the 22nd international conference on Logic
Programming, ICLP’06, pages 11–25. Springer-Verlag, 2006.
[10] M. Gebser, B. Kaufmann, A. Neumann, and T. Schaub. Clasp: A
Conflict-Driven Answer Set Solver. In Proceedings of the 9th international conference on Logic Programming and Nonmonotonic Reasoning, LPNMR’07, pages 260–265. Springer-Verlag, 2007.
[11] M. Gelfond. Representing Knowledge in A-Prolog. In Computational
Logic: Logic Programming and Beyond, Essays in Honour of Robert
A. Kowalski, Part II, pages 413–451. Springer-Verlag, 2002.
[12] M. Gelfond and V. Lifschitz. The Stable Model Semantics for Logic
Programming. In Proceedings of the Fifth international conference on
Logic Programming, pages 1070–1080. MIT Press, 1988.
[13] E. Giunchiglia, Y. Lierler, and M. Maratea. SAT-Based Answer Set
Programming. In Proceedings of the 19th national conference on
Artifical Intelligence, AAAI’04, pages 61–66. AAAI Press, 2004.

41

maximal models (computed using greatest fixed-points) [27]. The
operational semantics under coinduction is as follows [27]: a predicate call p(t̄) succeeds if it unifies with one of its ancestor calls.
Thus, every time a call is made, it has to be remembered. This set
of ancestor calls constitutes the coinductive hypothesis set (CHS).
Under co-LP, infinite rational answers can be computed, and infinite rational terms are allowed as arguments of predicates. Infinite
terms are represented as solutions to unification equations and the
occurs check is omitted during the unification process: for example, X = [1 | X] represents the binding of X to an infinite list of
1’s. Thus, in co-SLD resolution, given a single clause
p([ 1 | X ]) :- p(X).
The query ?- p(A) will succeed in two resolution steps with the
answer A = [1 | A], which is a finite representation of the infinite answer A = [1, 1, 1, ....]. Under coinductive interpretation of R2, the query ?- stream(X) produces all infinite sized
streams as answers, e.g., X = [1 | X], X = [1, 2 | X ], etc.
Thus, the semantics of R2 is not null, but proofs may be of infinite length. If we take a coinductive interpretation of program R1,
then we get all finite and infinite streams as answers to the query
?- stream(X).

[14] G. Gupta, A. Bansal, R. Min, L. Simon, and A. Mallya. Coinductive
Logic Programming and Its Applications. In Proceedings of the 23rd
international conference on Logic Programming, ICLP’07, pages 27–
44. Springer-Verlag, 2007.
[15] A. Kakas and F. Toni. Computing Argumentation in Logic Programming. Journal of Logic and Computation, 9(4):515–562, 1999.
[16] A. C. Kakas, R. A. Kowalski, and F. Toni. Abductive Logic Programming. Journal of Logic and Computation, 2(6):719–770, 1992.
[17] P. G. Leone, N. and W. Faber. DLV. http://www.dbai.tuwien.
ac.at/proj/dlv.
[18] J. Lloyd. Foundations of Logic Programming. Symbolic Computation:
Artificial Intelligence. Springer-Verlag, 1987.
[19] K. Marple and G. Gupta. Galliwasp: A Goal-Directed Answer
Set Solver.
Technical report, University of Texas at Dallas,
2012.
http://www.utdallas.edu/~kbm072000/galliwasp/
publications/galliwasp.pdf.
[20] R. Min. Predicate Answer Set Programming with Coinduction. PhD
thesis, University of Texas at Dallas, 2010.
[21] R. Min, A. Bansal, and G. Gupta. Towards Predicate Answer Set
Programming via Coinductive Logic Programming. In AIAI, pages
499–508. Springer, 2009.
[22] I. Niemelä and P. Simons. Smodels - An Implementation of the Stable
Model and Well-Founded Semantics for Normal Logic Programs. In
Logic Programming And Nonmonotonic Reasoning, volume 1265 of
Lecture Notes in Computer Science, pages 420–429. Springer-Verlag,
1997.

B.

We now present a larger, more complex example of execution using
our goal-directed method. Consider program A1:

[23] L. Pereira and A. Pinto. Revised Stable Models - A Semantics for
Logic Programs. In Progress in Artificial Intelligence, volume 3808
of Lecture Notes in Computer Science, pages 29–42. Springer-Verlag,
2005.

p
q
r
q

[24] L. Pereira and A. Pinto. Layered Models Top-Down Querying of Normal Logic Programs. In Practical Aspects of Declarative Languages,
volume 5418 of Lecture Notes in Computer Science, pages 254–268.
Springer-Verlag, 2009.

::::-

not
not
not
not

q.
r.
p.
p.

...
...
...
...

Rule A1.1
Rule A1.2
Rule A1.3
Rule A1.4

Rules A1.1, A1.2 and A1.3 are OLON rules, as calls to propositions
p, q, and r in the heads of these rules lead to recursive calls to
p, q and r respectively that are in the scope of odd numbers of
negations. A1.1 is also an ordinary rule, since in conjunction with
rule A1.4, a call to p resolved via rule A1.1 will lead to a call to
p in rule A1.4 that is in the scope of an even number of negations.
Thus, the nmr check rule can be defined as:
nmr check :- not chk p, not chk q, not chk r.
chk p :- not p, not q.
chk q :- not q, not r.
chk r :- not r, not p.
The duals of the above rules are as follows:

[25] K. Sagonas, T. Swift, and D. Warren. XSB as an Efficient Deductive
Database Engine. In ACM SIGMOD Record, volume 23, pages 442–
453. ACM, 1994.
[26] Y. Shen, J. You, and L. Yuan. Enhancing Global SLS-Resolution
with Loop Cutting and Tabling Mechanisms. Theoretical Computer
Science, 328(3):271–287, 2004.
[27] L. Simon. Extending Logic Programming with Coinduction. PhD
thesis, University of Texas at Dallas, 2006.

A.

Detailed Execution Example

Co-SLD Resolution

not
not
not
not
not
not

As mentioned in the introduction, our goal-directed method relies
on coinductive logic programming (co-LP) [14]. Co-SLD resolution, the operational semantics of coinduction, is briefly described
below. The semantics is limited to regular proofs, i.e., those cases
where the infinite behavior is obtained by infinite repetition of a
finite number of finite behaviors.
Consider the logic programming definition of a stream (list) of
numbers as in program R1 below:
stream([]).
stream([H|T]) :- number(H), stream(T).
Under SLD resolution, the query ?- stream(X) will systematically produce all finite streams one by one starting from the []
stream. Suppose now we remove the base case and obtain the program R2:
stream([H|T]) :- number(H), stream(T).
In the program R2, the meaning of the query ?- stream(X) is
semantically null under standard logic programming. In the coLP paradigm the declarative semantics of the predicate stream/1
above is given in terms of infinitary Herbrand (or co-Herbrand)
universe, infinitary Herbrand (or co-Herbrand) base [18], and

p :- q.
...
q :- r, p.
...
r :- p.
...
chk p :- p; q.. . .
chk q :- q; r.. . .
chk r :- r; p.. . .

Rule A1.6
Rule A1.7
Rule A1.8
Rule A1.9
Rule A1.10
Rule A1.11

Negated calls are resolved using these dual rules. Now the query q
will be extended to q, nmr chk and executed as follows:
:::::::::-

42

chk. CHS = {}; Rule A1.2
nmr chk.
CHS = {q}; Rule A1.8
chk. CHS = {q, not r}; Rule A1.1
nmr chk.
CHS = {q, not r}
fail: backtrack to step 1
q, nmr chk. CHS = {}; Rule A1.4
not p, nmr chk.
CHS = {q, not p}; Rule A1.6
q, nmr chk. CHS = {q, not p}
coinductive success
nmr chk.
CHS = {q, not p}
execution of q finished
not chk p, not chk q, not chk r.
q, nmr
not r,
p, nmr
not q,

:::::-

CHS = {q, not p}
nmr chk rule
(p ; q), not chk q, not chk r.
CHS = {q, not p}
not p is in CHS
q, not chk q, not chk r.
CHS = {q, not p}
coinductive success for q
not chk q, not chk r.
CHS = {q, not p}; Rule A1.10
(q ; r), not chk r.
CHS = {q, not p}
coinductive success for q
not chk r.
CHS = {q, not p}; Rule A1.11

:- r ; p.

CHS = {q, not p, r}; Rule A1.3

:- not p ; p.

CHS = {q, not p, r}
coinductive success for not p
success.
answer set is {q, not p, r}

:- .

43

Web Service Discovery and Composition using USDL
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
University of Texas at Dallas
Richardson, TX 75083
Abstract
For web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize
services automatically. In this paper, we present the
design of an automatic service discovery and composition engine using USDL (Universal Service-Semantics
Description Language) [1, 2], a language for formally
describing the semantics of web-services. The implementation will be used for the WS-Challenge 2006 [3].

1. Introduction
A web-service is a program available on a website that “eﬀects some action or change” in the world
(i.e., causes a side-eﬀect). The next milestone in the
Web’s evolution is making services ubiquitously available. As automation increases, these web-services will
be accessed directly by the applications themselves
rather than by humans. To achieve this, we need a
semantics-based approach such that applications can
reason about a service’s capability to a level of detail
that permits their discovery and composition.
In this paper we present the design of a software system for automatic service discovery and composition.
This system uses web service descriptions written in
USDL [1, 2]. In section 2 we present a brief overview
of USDL. Section 3 shows the design of our software
with brief descriptions of the diﬀerent components of
the system followed by conclusions and references.

2. Overview of USDL
USDL is a language that provides formal semantics of web-services thus allowing sophisticated conceptual modeling and searching of available services, automated composition, and automated service integration.

This is an ongoing project with Metallect Corp. The
design and formal speciﬁcation in OWL was published
in European Conference On Web Services, 2005 [1].
The WSDL [4] syntax and USDL semantics of web services can be published in a directory which applications
can access to discover services. To provide formal semantics, a common denominator must be agreed upon
that everybody can use as a basis of understanding the
meaning of services. Additionally, the semantics should
be given at a conceptual level that captures common
real world concepts. USDL uses an ontology based on
OWL WordNet [5] for a universal ontology of basic
concepts upon which arbitrary meets and joins can be
added in order to gain tractable ﬂexibility.
USDL describes a service in terms of portType and
messages, similar to WSDL. The semantics of a service
is given using the OWL WordNet ontology: portType
(operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly negated) concepts in the OWL
WordNet ontology. Additional semantics is given in
terms of how a service aﬀects the external world. Formal semantic description of any external conditions
(pre-conditions or post-conditions) acting on the service can also be provided using USDL.

3. Design
Our discovery and composition engine is written
using Prolog [6]. Our software system for the WSChallenge [3] comprises of the following components
shown in Figure 1.

3.1. USDL Generator
This module parses all WSDL descriptions from the
given repository and converts them into corresponding
USDL descriptions. For the WS-Challenge, this module maps part names of a service to their corresponding
type deﬁnitions from the XML Schema ﬁle, as opposed

Proceedings of the 8th IEEE International Conference on E-Commerce Technology and the 3rd IEEE
International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE’06)
0-7695-2511-3/06 $20.00 © 2006

IEEE

Thomas D. Hite
Metallect Corp.
2400 Dallas Parkway
Plano, TX 75093

3.3. Semantic Relations Generator
For the semantic part of the challenge, we have to
match the parmater types. The XML Schema will provide type hierarchy. We will obtain the semantic relations from the XML Schema ﬁle provided instead of
OWL WordNet ontology. Complex types will be described in the schema ﬁle using simple data types, thus
providing a type hierarchy. A supertype will subsume
its subtype which is nothing but the hyponym and hypernym relation. A hyponym is a word that is more
speciﬁc than a given word, also called the sub-ordinate.
A hypernym is a word that is more generic than a given
word, also called the super-ordinate.
This module will extract the type deﬁnitions from
the XML Schema ﬁle and create the semantic relations
(hypernym, hyponym, etc.) [7] between the diﬀerent
types in a format that the discovery and composition
query processors will understand. For example, if the
XML Schema has a type p66a9128258 which is a supertype of p56a4809967, then this module will add the
prolog fact
hyponym(p56a4809967,p66a9128258).
to the list of facts.
to pointing them to concepts in OWL WordNet ontology. Our approach does not need a separate query language. It parses the query ﬁle and converts each query
for the desired service also into a USDL description.

3.2. Triple Generator
The triple generator module converts each service
description into a triple as follows:
(Pre-Conditions, aﬀect-type(aﬀected-object, I, O),
Post-Conditions).
The function symbol aﬀect-type is the side-eﬀect of the
service and aﬀected object is the object that changed
due to the side-eﬀect. I is the list of inputs and O
is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions
are the conditions on the output parameters of the
service. Services are converted to triples so that they
can be treated as terms in ﬁrst-order logic and specialized uniﬁcation algorithms can be applied to obtain exact, generic, speciﬁc, part and whole substitutions [7].
For the WS-Challenge, conditions on a service will not
be provided and hence the Pre-Conditions and PostConditions in the triple will be null. The aﬀect-type
will also not be available and so this module will assign
a generic aﬀect to all services.

3.4. Discovery Query Processor
This module compares the discovery query with
all the services in the repository. It uses an extended/special uniﬁcation algorithm to ﬁnd a matching
term. The uniﬁcation mechanism is diﬀerent depending on the type of match (Exact, Speciﬁc, Generic,
Part or Whole) required. The processor works as follows:
1. On the input parts of a service, the processor ﬁrst
looks for an exact substitutable. If it does not
ﬁnd one, then it looks for a type with hypernym
relation [7], i.e., a generic substitutable.
2. On the output parts of a service, the processor ﬁrst
looks for an exact substitutable. If it does not ﬁnd
one, then it looks for a type with hyponym relation
[7], i.e., a speciﬁc substitutable.
The discovery engine is written using prolog (a logic
programming language) [6]. It uses a repository of
facts, which contains a list of all the services and the
semantic relations. The query is converted into a prolog query that looks as follows:
discoverServices(queryService, solutionService).
The engine will try to ﬁnd a list of solutionServices that
match the queryService. Our code for the engine will
have various rules to solve the discoverServices query.

Proceedings of the 8th IEEE International Conference on E-Commerce Technology and the 3rd IEEE
International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE’06)
0-7695-2511-3/06 $20.00 © 2006

IEEE

3.5. Composition Query Processor
For service composition, the ﬁrst step is ﬁnding the
set of composable services. If a subservice S1 is composed with subservice S2 , then the output parts of S1
must be the input parts of S2 . Thus the processor has
to ﬁnd a set of services such that the outputs of the
ﬁrst service are inputs to the next service and so on.
These services are then stitched together to produce
the desired service.
If the complete semantics of a web service are described using USDL and OWL WordNet ontology, Part
substitution technique [7] can be used to ﬁnd the different parts of a whole task and the selected services
can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and
post-conditions of the individual services. For the challenge, we do not have with mappings to OWL WordNet
Ontology and description of conditions. So we will be
using only Exact, Generic, and Speciﬁc substitution [7]
techniques to ﬁnd the list of composable services. Similar to the discovery engine, composition engine is also
written using prolog. The query is converted into a
prolog query that looks as follows:
composeServices(queryService, listOfServices).
The engine will try to ﬁnd a listOfServices that can be
composed into the requested queryService. Our code
for the engine will have various rules to solve the composeServices query. Our prolog code will be setup in
such a way that all possible listofServices that can be
used for composition will be returned.

3.6. Output Generator
After the Discovery/Composition Query processor
ﬁnds a matching service, or the list of atomic services
for a composed service, the results are sent to the output generator in the form of triples. This module generates the output ﬁles using the speciﬁed XML format
for the WS-Challenge[3].

4. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure to publish services, document services and
query repositories for matching services. Our approach
uses USDL to formally document the semantics of services and our discovery and composition engines ﬁnd

substitutable services that best match the desired service.
Our solution will produce very good results when semantic descriptions of web services are provided using
USDL. While matching real-world services, our discovery and composition engine will look at OWL WordNet
ontology for the meanings. For the challenge, type hierarchy is the only semantics provided and hence we will
not be able to use all the semantic relations available in
WordNet. We apply some optimization techniques to
our system so that it is eﬃcient on the WS-Challenge
repositories. We use constraint-logic programming [8]
for better pruning of the search space. We have put
in eﬀorts for testing the eﬃciency of our system and
identifying the correct optimization strategies.

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In European Conference On
Web Services, pp. 214-225, 2005.
[2] S. Kona, A. Bansal, G. Gupta, and T. Hite.
USDL - Formal Deﬁnitions in OWL. Internal Report, University of Texas, Dallas, 2006. Available
at http://www.utdallas.edu/~srividya.kona/
USDLFormalDefinitions.pdf.
[3] WS Challenge 2006.
tu-berlin.de/wsc06.

[4] Web Services Description Language. http://www.
w3.org/TR/wsdl.
[5] Ontology-based information management system,
wordnet OWL-Ontology. http://taurus.unine.
ch/knowler/wordnet.html.
[6] L. Sterling and S. Shapiro. The Art of Prolog. MIT
Press, 1994.
[7] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta,
and T. Hite. USDL: A Service-Semantics Description Language for Automatic Service Discovery
and Composition. Technical Report UTDCS-1806, University of Texas, Dallas, 2006. Submitted to
JWSR. Available at http://www.utdallas.edu/
~srividya.kona/USDL.pdf.
[8] K. Marriott and P. J. Stuckey. Programming with
Constraints: An Introduction. MIT Press, 1998.

Proceedings of the 8th IEEE International Conference on E-Commerce Technology and the 3rd IEEE
International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE’06)
0-7695-2511-3/06 $20.00 © 2006

IEEE

http://insel.flp.cs.

Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises

An Agent-based Approach for Composition of Semantic Web Services
Ajay Bansal, Srividya Kona, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Abstract

Approaches leveraging semantic annotations and information management techniques are necessary to enable reasoning about service-oriented capabilities to a
level of detail that permits discovery, composition, deployment, and synthesis [3]. This paper emphasizes
composition. Service composition [6, 7, 10] is the integration of distributed coarse-grained applications.using
four phases: (i) Planning, (ii) Discovery, (iii) Selection,
and (iv) Execution [12]. These four phases incorporate
the aggregation of services into a plan, the discovery of
candidate services, the selection of the most relevant
services, and the enactment of the process based on
service invocations.
In this paper, we extend our earlier work [4] towards
service composition with the introduction of intelligent
agents that manage automatic Web service composition across decentralized repositories. This approach
performs planning, discovery, and selection without
manual intervention. Note that service discovery is
a special case of composition of n services, i.e., when
n=1. Thus, we mainly study the general problem of
automatically composing n services to satisfy the demand for a particular service, posed as a query by the
user.
The rest of the paper is organized as follows. In
Section 2 we present the related work in the area of
service composition and agent-based composition and
discuss their limitations. In Section 3, we present the
Web service Composition problem with an example of
a travel reservation composite service. Next we present
a discussion of our agent-based technique for automatic
Web service composition in Section 4. Finally, we
present our conclusions and future work.

The paradigm of Service-oriented computing (SOC)
introduces emerging concepts for distributed- and ebusiness processing enabling the sharing and reuse of
service-centric capabilities. The underpinning for an
organization’s use of SOC techniques is the ability to
discover and compose Web services. Leading industry
approaches rely heavily on syntactical approaches for
managing service-based business processes. As such,
these approaches are limited since the true functionality
of ambiguous capabilities (i.e. web service operations)
cannot be inferred. We introduce approaches that disambiguate services by interleaving process-based control
with semantic annotations. In this paper, we introduce a generalized architecture where intelligent software agents control process-oriented composition that
leverages the descriptiveness of semantics. An outcome
of this work is the specification of a multiple agent system where a query agent interacts with multiple repository agents to perform business-oriented service composition.
Keywords: Semantic Web Services, Agents, Web
Service Composition, Service-Oriented Architecture.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered, and consumed [13]. A Web service is an
autonomous, platform-independent computational element that can be described, published, discovered and
accessed over the web using standard web protocols.
Sample of Web services include flight, hotel, rental car
reservation services or device controls like sensors or
satellites. As automation increases, these services will
be accessed directly by applications rather than by humans [5].
978-0-7695-3315-5/08 $25.00 © 2008 IEEE
DOI 10.1109/WETICE.2008.19

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

2. Related Work
Composition of Web services has been active area
of research recently [11, 12]. Most of these approaches
present techniques to solve one or more phases listed
in Section 1. There are many approaches [8, 9, 10] that
12

3. Web Service Composition

solve the first two phases of composition namely planning and discovery. These are based on capturing the
formal semantics of the service using action description languages or some kind of logic (e.g., description
logic). The service composition problem is reduced to
a planning problem where the sub-services constitute
atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to
determine the combination of actions needed to reach
the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals
are usually not available. The authors in [8] present
a composition technique by applying logical inferencing on pre-defined plan templates. There are industry
solutions based on WSDL and BPEL4WS where the
composition flow is obtained manually. BPEL4WS can
be used to define a new Web service by composing a set
of existing ones. It does not assemble complex flows of
atomic services based on a search process. They select
appropriate services using a planner when an explicit
flow is provided. In contrast, our agent-based technique automatically determines these complex flows using semantic descriptions of atomic services distributed
across various repositories.

A Web service can be regarded as a “programmatic
interface” that makes application to application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces,
and the actions that it initiates. The input parameter may be further subject to some pre-conditions, and
likewise, the outputs produced may have to satisfy certain post-conditions. Given a repository (or a set of
repositories) of service descriptions, and a query with
the requirements of the requested service, the composition problem involves, in case a matching service is not
found, automatically finding a directed acyclic graph of
services that can be composed in correct order of execution to obtain the desired service. Figure 1 shows an example composite service made up of five services S1 to
S5 . In the figure, I 0 and CI 0 are the query input parameters and pre-conditions respectively. O0 and CO0 are
the query output parameters and post-conditions respectively. Informally, the directed arc between nodes
Si and Sj indicates that outputs of Si constitute (some
of) the inputs of Sj .

Shenghua et. al. [2] propose an agent-based solution
to web service composition problem using JADE and
JXTA. The approach presented achieves automation in
composing services but it is shown to handle only the
sequential case of composition problem. It falls short
of handling the general case of non-sequential composition with conditions. Our approach produces compositions that are not only sequential but also nonsequential that can be represented in the form of a
directed acyclic graph (DAG). Buhler et. al. [1] use
multi-agents to solve the more general case of web service composition problem. The approach presented by
them works in the distributed environment as well. But
their approach is syntactic in nature and does not make
use of the semantics provided in the descriptions of
web services. So their approach is limited to syntactic
composition whereas our approach works for the composition of Semantic Web services as well. A processlevel composition solution based on OWL-S is proposed
in [9]. In this work the authors assume that they already have the appropriate individual services involved
in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual
services to produce a process-level description of the
composite service. In contrast, we present a technique
that automatically finds the services that are suitable
for composition based on the query requirements for
the new composed service.

Figure 1. Composite Service as a Directed
Acyclic Graph
Next we illustrate the composition problem by presenting an example of a non-sequential composition
with if-then-else conditions i.e., the composition flow
varies depending on the result of the post-conditions
of a service. We believe that the non-sequential conditional composition is the most general case of the
composition problem.
Example: Suppose we are looking for a service to
make international travel arrangements. We first need
to make a tentative flight and hotel reservation and
then apply for a visa. If the visa is approved, we can
buy the flight ticket and confirm the hotel reservation,
else we will have to cancel both the reservations. Also
if the visa is approved, we need to make a car reservation. The repository contains services ReserveFlight,
ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table
1 shows the input/output parameters of the user-query
and the services.

13

Service

PreConditions

Query
Reserve
Flight
Reserve
Hotel
Process
Visa
ConfirmFlight
ConfirmHotel
CancelFlight
CancelHotel
Reserve
Car

VisaApproved
VisaApproved
VisaDenied
VisaDenied

Input Parameters

Output Parameters

PasngrName,OriginArprt,DestArpt,StartDate,ReturnDate
PasngrName,OriginArprt,DestArpt,StartDate,ReturnDate
PasngrName, StartDate,
ReturnDate
PasngrName,VisaType,
FlightConfNum,HotelConfNum
FlightConfNum,CreditCardNum
HotelConfNum,CreditCardNum
PasngrName,FlightConfNum
PasngrName,HotelConfNum
PasngrName,ArrivalDate
ArrivalFlightNum

FlightConfNum,HotelConfNum,CarConfNum
FlightConfNum

PostConditions

HotelConfNum
ConfirmationNum

VisaApproved
∨ VisaDenied

ArrivalFlightNum
ConfirmationNum
CancelCode
CancelCode
CarConfirmNum

Table 1. Example Scenario of (Non-Sequential Conditional) Web Service Composition
effect, AO is the affected object, O is the output list,
and CO is the list of post-conditions. The pre- and
post-conditions are ground logical predicates.

In this example, service ProcessVisa produces the
post-condition VisaApproved ∨ VisaDenied. The services ConfirmFlight and ConfirmHotel have the precondition VisaApproved. In this case, one cannot determine if the post-conditions of service ProcessVisa implies the pre-conditions of services ConfirmFlight and
ConfirmHotel until the services are actually executed.
In such a case, a condition can be generated which will
be evaluated at runtime and depending on the outcome of the condition, the corresponding services will
be executed. The vertex VisaApproved in the graph
is a condtional node with the outgoing edges representing the generated conditions and the outputs. In
this case conditions (VisaApproved ∨ VisaDenied) ⇒
VisaApproved and (VisaApproved ∨ VisaDenied) ⇒
VisaDenied are generated. Depending on which condition holds, the corresponding services ConfirmFlight or
CancelFlight are executed. Figure 2 shows this conditional composition example as an conditional directed
acyclic graph.
Next, we present our formalization of the generalized
composition problem. This generalization is an extension of our previous notion of composition [4] which can
handle non-sequential conditional composition (which
we believe is the most general case of composition).

Definition (Query): The query service is defined as
Q = (CI 0 , I 0 , A0 , AO0 , O0 , CO0 ) where CI 0 is the list of
pre-conditions, I 0 is the input list, A0 is the service
affect, AO0 is the affected object, O0 is the output list,
and CO0 is the list of post-conditions. These are all the
parameters of the requested service.
Definition (Generalized Composition): The generalized Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E)
of services from repository R, given query Q =
(CI 0 , I 0 , A0 , AO0 , O0 , CO0 ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph either represents a service involved
in the composition or post-condition of the immediate
predecessor service in the graph, whose outcome can
be determined only after the execution of the service.
Each outgoing edge of a node (service) represents the
outputs and post-conditions produced by the service.
Each incoming edge of a node represents the inputs and
pre-conditions of the service. The following conditions
should hold on the nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming
edge that represents
the query inputs and preS
conditions, I 0 w i I i , CI 0 ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing
edge that represents
S the query outputs and postconditions, O0 v i Oi , CO0 ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has
at least one incoming edge, let Si1 , Si2 , ..., Sim be

Definition (Repository of Services): Repository
(R) is a set of Web services.
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs
and post-conditions. S = (CI, I, A, AO, O, CO) is the
representation of a service where CI is the list of preconditions, I is the input list, A is the service’s side-

14

Figure 2. Example Web Service Composition
the nodes such that there is a directed
Sedge from
each of these nodes to Si . Then Ii v k Oik ∪ I 0 ,
CI i ⇐ (COi1 ∧COi2 ... ∧ COim ∧ CI 0 ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming edge, let Sj be its immediate predecessor node
such that there is a directed edge from Sj to Si .
Then the inputs and pre-conditions at node Si are
Ii = Oj ∪I 0 , CI i = COj . The outgoing edges from
Si represent the outputs that are same as the inputs Ii and the post-conditions that are the result
of the condition evaluation at run-time.
The meaning of the v is the subsumption (subsumes)
relation and ⇒ is the implication relation. In other
words, a service at any stage in the composition can
potentially have as its inputs all the outputs from its
predecessors as well as the query inputs. The services
in the first stage of composition can only use the query
inputs. The union of the outputs produced by the services in the last stage of composition should contain all
the outputs required by the query.
Next, we present our agent-based approach to solve
the above described generalized composition problem.

agents to realize a solution for the generalized composition problem, that works in presence of decentralized
repositories over a distributed environment.
We
introduce two types of agents to perform composition.
• Repository Agent: Each repository will be
accompanied by a Repository agent that has
knowledge of all the services in its respective
repository and their semantic descriptions. This
agent perceives its environment for two kinds of
requests: (i) any updates to the services in the
repository (any new service being added or an
old service being deleted), (ii) a request for all
services that can be invoked by a set of inputs and
pre-conditions. The Repository agent may have
the ability to discover services (based on exact
match) but does not have an ability to perform
composition of services.
• Query Agent: The Query agent performs the
composition task by communicating with various
repository agents for the services involved in the
composition. This agent perceives its environment
for composition query requests. Once it receives
a query request, it requests the various repository
agents for the services that can be invoked using
the available query inputs. Upon receiving the information of the services, the query agent computes the set of outputs of all these services and
checks if the query outputs are satisfied by this
set. If yes, then it filters out the redundant services that do not contribute to the query outputs.
If no, then it adds this set of outputs to the set

4. Agent-based Service Composition
Our previous work presented in [4] solves the
automatic Web service composition problem using a
centralized repository. With the service domain being
inherently distributed in nature, in the real-world it
would be a wishful thinking to imagine a centralized
repository for a particular or all the domains. In this
section we extend our previous work with the use of

15

repositories at multiple stages. Figure 4 shows the composition technique for the particular instance shown in
Figure 1. The query agent performs the composition
task upon receiving a composite service request from
the user. The query agent starts with the query input
parameters. It contacts the repository agents to find all
those services from their respective repositories which
can be satisfied with a subset of the query input parameters and whose pre-conditions can be implied by the
query pre-conditions. In Figure 4, CI, I are the preconditions and the input parameters provided by the
query. S1 and S2 are the services found after step 1.
O1 is the union of all outputs produced by the services
at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs
produced by all the services in the previous stage, i.e.,
I2 = O1 ∪ I. I2 is used by the query agent to find
services at the next stage, i.e., it queries the repository
agents again for all those services that require a subset
of I2 . In order to make sure we do not end up in cycles,
the query agent keeps only those services which require
at least one parameter from the outputs produced in
the previous stage. This filtering continues until all
the query output parameters are produced. At this
point the query agent makes another pass in the reverse direction to remove redundant services which do
not directly or indirectly contribute to the query output parameters. Once the query agent has obtained
all the services required to produce the solution for the
query, it puts them in the correct order of execution to
produce the composite service. The generalized composition algorithm performed by the query agent is shown
in table 2. The repository agent, on the other hand,
performs nothing more than a service matching task.
It provides the query agent all the services that can be
satisfied with the input parameters and pre-conditions
provided by the query agent.

Figure 3. Agent-based Web Service Composition

of query inputs and sends out new requests to the
repository agents with this new query input set.
It keeps on repeating this until the set of outputs
of the services returned by the repository agent
subsumes the query output set. At this point the
query agent performs a backward pass through all
the services it has collected so far at various stages
to filter out redundant services that do not contribute directly or indirectly to the query outputs.
The query agent itself does not have any information about the services but it does have the ability
to solve the composition problem by collecting services from the repositories (via repository agents)
and putting them together in the correct order of
execution to provide a composite service.
Next, we present our agent-based generalized composition algorithm.

5. Conclusions and Future Work
In this paper we present an agent-based approach for
automatic service composition that extends the common practice of using centralized repositories for Web
service composition by introducing the use of agents
for multiple repositories in a distributed environment.
Furthermore, we introduce a query agent that interacts
with the repository agents to obtain services that can
be composed to produce the requested composite service. Our current implementation of this approach is at
a prototype stage. Our future work includes building
a full-fledged agent-based generalized composition engine and evaluating it using decentralized repositories
in a distributed enviroment.

4.1. Agent-based Generalized Composition
Algorithm
In order to produce the composite service which is
the graph, as shown in the example Figure 1, our agentbased approach selects relevant services from multiple

16

Figure 4. Multi-step Narrowing Technique for Web Service Composition
[3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web
Services. In IEEE Intelligent Systems Vol. 16, Issue
2, pp. 46-53, March 2001.
[4] S. Kona, A. Bansal, and G. Gupta. Automatic
Composition of Semantic Web Services. In Intl.
Conf. on Web Services (ICWS) pp 150-158, 2007.
[5] A. Bansal, K. Patel, G. Gupta, B. Raghavachari,
E. Harris, and J. Staves. Towards Intelligent Services: A case study in chemical emergency response. In ICWS, pp.751-758, 2005.
[6] D. Mandell, S. McIlraith Adapting BPEL4WS for
the Semantic Web: The Bottom-Up Approach to
Web Service Interoperation. In ISWC, 2003.
[7] M. Paolucci, T. Kawamura, T. Payne, and
K. Sycara Semantic Matching of Web Service Capabilities. In ISWC, pages 333-347, 2002.
[8] S. McIlraith, T. Son Adapting golog for composition of semantic Web services. In KRR, pp.482–
493, 2002.
[9] M. Pistore, P. Roberti, and P. Traverso ProcessLevel Composition of Executable Web Services In
European Semantic Web Conf., pp 62-77, 2005.
[10] J. Rao, D. Dimitrov, P. Hofmann, and N. Sadeh
A Mixed-Initiative Approach to Semantic Web Service Discovery and Composition In International
Conference on Web Services, 2006.
[11] J. Rao, X. Su. A Survey of Automated Web Service Composition Methods In Workshop on Semantic Web Services and Web Process Composition, 2004.
[12] J. Cardoso, A. Sheth. Semantic Web Services,
Processes and Applications. Springer, 2006.
[13] G. Castagna, N. Gesbert, L. Padovani, et al. A
Theory of Contracts for Web Services. In Symposium on Principles of Programming Languages,
January 2008

Algorithm: Query Agent
Input: QI - QueryInputs, QO - QueryOutputs,
QCI - Pre-Cond, QCO - Post-Cond
Output: Result - ListOfServices

1. L ← CollectServicesFromRepositoryAgents(QI,
QCI);
2. O ← GetAllOutputParameters(L);
3. CO ← GetAllPostConditions(L);
4. While Not (O w QO)
5.
I = QI ∪ O; CI ← QCI ∧ CO;
6.
L’ ← CollectServicesFromRepositoryAgents(I,
CI);
7. End While;
8. Result ← RemoveRedundantServices(QO, QCO);
9. Return Result;

Table 2. Agent-based Generalized Composition Algorithm

References
[1] P. Buhler, D. Greenwood, A. Reitbauer. A Multiagent Web Service Composition Engine. In Proceedings of First International Workshop on Engineering Service Compositions (WESC), 2005.
[2] S. Liu, P. Kungas, M. Matskin. Agent-Based Web
Service Composition with JADE and JXTA. In
Proceedings of the 2006 International Conference
on Semantic Web and Web Services, SWWS 2006,
Las Vegas, Nevada, USA, June 26-29, 2006.

17

Annotating UDDI Registries to Support the Management of
Composite Services
M. Brian Blake, Michael F. Nowlan, Ajay Bansal, and Srividya Kona
Department of Computer Science
Georgetown University
Washington, DC
202 687-3084

{mb7,mfn3,ab683,sk558}@georgetown.edu
Moreover, the syntactic and semantic metadata that accompanies
these services [9][11] enable the discovery of these capabilities,
on-demand. Discovery, in this environment, largely depends on
the accessibility and capabilities of the repositories for which
these services are stored. Universal Description, Discovery, and
Integration (UDDI) is the leading specification for the
development of service-based repositories or registries. UDDI
registries of the future should facilitate fast search and discovery
of relevant web services akin to the performance currently
associated with resolving a domain name. Although, performance
and federation are two important aspects of UDDI, an additional
requirement for UDDI should be effective process-oriented
storage and retrieval. Currently, the abilities to browse and
discover independent services as characterized by their
overarching business name and/or their capability name are
important fundamental operations. However, as service-oriented
architectures mature, composite services (i.e. capabilities based
on the workflow composition of multiple atomic services) will
also be important to persist and manage. UDDI currently has
limited support for managing business processes [6]. Although
not all federated service registries will need business process
annotations, we suggest that a subset of registries frequently used
by partnering organizations would benefit from maintaining
historical process information.

ABSTRACT
The future of service-centric environments suggests that
organizations will dynamically discover and utilize web services
for new business processes particularly those that span multiple
organizations. However, as service-oriented architectures mature,
it may be impractical for organizations to discover services and
orchestrate new business processes on a daily, case-by-case basis.
It is more likely that organizations will naturally aggregate
themselves into groups of collaborating partners that routinely
share services. In such cases, there is a requirement to maintain
an organizational memory with regards to the capabilities offered
by other enterprises and how they fit within relevant business
processes. As a result, registries must maintain information about
past business processes (i.e. relevant web services and their
performance, availability, and reliability). This paper discusses
and evaluates several hybrid approaches for incorporating
business process information into standards-based service
registries.

Categories and Subject Descriptors
D.2.11 [Software]: Software Engineering: Software Architectures
– domain specific architectures.

General Terms
Performance,
Design,
Standardization

Experimentation,

Security,

Currently, there are numerous languages and protocols
that support the specification and execution [3][4][5][9] of
composite web services.
Unfortunately, techniques for
incorporating the underlying process information into UDDI
registries are limited. In this work, we address several questions
as listed below.

and

Keywords
Keywords are your own designated keywords.

x

1. INTRODUCTION

x

Service-oriented computing [10] promotes the development
of modular domain-specific capabilities that can be advertised to
and shared among collaborating organizations.

x
x

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SAC’09, March 8-12, 2009, Honolulu, Hawaii, U.S.A.
Copyright 2009 ACM 978-1-60558-166-8/09/03…$5.00.

What are the relevant descriptive attributes of composite web
services that must be represented in web service registries?
What are the relevant use cases for process-oriented service
registries?
What are the state-of-the-art methods for incorporating
process information into registries and the corresponding
challenges?
What are the most efficient and effective approaches, both
qualitatively and quantitatively, for process management of
services in UDDI registries?

This paper proceeds in the next section with a survey of related
work and a discussion of the state of the practice. The next
section formalizes a composite service by detailing the most
relevant descriptive attributes.
Next, we describe the

2146

the process information can be valuable. Luo et al. [8] and
Srinivasan et al. [14] use semantic notations embedded in UDDI
tModels to associate services. This approach allows for more rich
definitions, but the underlying limitations caused by distribution
also apply. Other approaches look at the development of external,
integrated software mechanisms that run parallel with the UDDI
registry [18] [19]. These approaches tend to depart from the
essence of the SOA paradigm as they promote proprietary, less
standardized solutions.

requirements of a process-oriented registry. We next introduce
several hybrid approaches for adding process information to web
service repositories. Finally, we describe a case study and
experimental evaluation of both approaches.

2. RELATED WORK
Universal Description, Discovery and Integration (UDDI)
describes the data model associated with a web-accessible registry
for the storage and management of web services. Three core
hierarchical objects specify the service provider (businessEntity),
the web service (businessService), and information about how the
service is binded (bindingTemplate). These foundational objects
can be extended with the use of technical models or tModels.
TModels facilitate the further description of businessEntities,
businessServices, and bindingTemplates through classification
based on metadata. Each tModel of a businessService represents
a certain behavior or classification system that the service must
implement. An example would be a web service that takes a state
abbreviation as input. This web service would probably choose to
reference the tModel that represents the US-State abbreviation
classification system.
A person looking at the service’s
bindingTemplate would then be able to see a reference to the USState System and know that a state should be entered with its
abbreviation. UDDI also supports other structures called
keyedReferences that allow previously mentioned core objects to
be associated to tModels. KeyedReferences consist of a
tModelKey, keyValue, and keyName. The tModelKey identifies
the referenced tModel. The keyValue allows a categorization of
the link between the core object, and the tModel and the keyName
is a text string readable for humans. In UDDI v3,
keyedReferences can be aggregated with keyedReferenceGroups.

In this paper, we experiment with a combination of external
process documentation and annotations that are embedded
directly in the UDDI registry. Prerequisite to any solution, it is
important to understand the required aspects of the web servicesbased business process

3. ANATOMY OF WEB SERVICES-BASED
PROCESSES
Web-services based business processes also referred to as web
service workflows are similar to traditional processes that are
established between human stakeholders. We propose a model
that intersects business process semantics with web service-based
data management techniques. There are few approaches that
formally describe this intersection. Figure 1 illustrates the
metamodel of a web services-based business process using a
Unified Modeling Language (UML) class diagram. A difference
is, as opposed to human-managed tasks or steps, web services
enact the underlying steps. A web services-based business
process, BP, contains user data endpoints, DE , defined below.

The strength of the tModels and keyedReferences is that
further information about the main UDDI objects (i.e.
businessEntities, businessServices, and bindingTemplates) are not
populated in the repository. Tmodels merely point to webaccessible documents. This paradigm both reduces maintenance
of the registry and promotes overall robustness. However, this
paradigm also makes it difficult to associate web services that are
stored in the registry, which is a necessary requirement for
describing business processes within the registry.
The most common research projects tend to address the
problem of federating UDDI registries [1] [2][12], although, of
most relevance to our work, are the projects that directly address
the problem of business process annotations that associate
services. Perhaps the leading approach to inserting business
processes is the construction of a tModel classification system
that mirrors a particular taxonomy of business processes. These
tModels can then be used as pointers to the corresponding
business process description documents. In industry, several
OASIS technical reports [15] [17] describe high-level approaches
to integrating tModel classifications with ebXML and BPEL4WS
descriptions. Other research projects detail specialized domainspecific methods that leverage the same basic approach [6] [13]
These are reasonable approaches, but all business descriptions are
defined by external business process documents that are
decoupled from the individual services. In fact, since services
may be captured at individual locations, replacing services or
even discontinuing the offering of a particular business process
becomes difficult. In these cases, centralizing some vital part of

Figure 1. Metamodel of a Web Service-Based Business Process.
Definition (UserData EndPoint):
The user data end point is defined as a pair DE = (ID, OD) where
ID represents the input information of the business process
provided by the user and OD represents the output information,
ultimately generated by the completion of the business process.
The business process also has a sequence of tasks (realization of
the steps) that are implemented by a set of services,  = {S1, S2,
S3, ….Sn}. Each service Si has its own input, ISi, and output, OSi,
information; however the set of all input/output information of a
service is less relevant than the subset of inputs and outputs that

2147

are relevant to the business process. In addition, a service can also
be defined with its quality of service information and its URI
location.

4. BUSINESS PROCESS AND SERVICE
REGISTRIES

Definition (Service):
A service is a tuple of its inputs, outputs, QoS parameters, and its
URI location. S = (IS , OS , QS , US) is the representation of a
service where IS is the input list, OS is the output list, QS is the list
of quality of service parameters and US represents the URI
location. Each step in the business process is defined by a
transition, T, that defines the shared information between the
output, OT, of the preceding step that connects to the input, IT , of
the subsequent step.

4.1 Potential Use Cases

In order for organizations to understand their business processes
defined with web services, it is important that their process
databases include relevant process information as defined in the
previous section. Organizations should be able to generally access
process information in addition to the service-specific details.
There are several functions required by a registry that supports
composite services as business processes. Figure 3 illustrates the
functions of such a repository depicted as a UML use case
diagram. The basic registry actors follow the SOA paradigm (i.e.
service providers and consumers). Incorporating business process
metadata into the registry also supports the interaction of
intelligent software components or agents to autonomously
maintain the integrity of the information. Service providers should
be able to insert one or more services into the registry. Service
consumers should be able to either browse or explicitly search the
repository based on several attributes such as the name/type of
business, service, or process. Although only available using
specialized approaches, consumers may also want to search by
service/process message names. We focus on three major features
of such a repository.

Definition (Transition):
A transition is represented as a tuple of its inputs, outputs, flowtype, pre-conditions, and post-conditions. T = (IT , OT ,FT , CPre ,
CPost) is the representation of the transition where IT is the input
list, OT is the output list, FT is the flow type, CPre represents the
pre-conditions of the transition and CPost represent the postconditions of the transition. Ultimately, the business process can
be formally defined as follows:
Definition (BusinessProcess):
A BusinessProcess is defined as a tuple BP = (DE, , ) where DE
represents the user data endpoint,  is the set of services involved
in the business process, and  is the set of transitions in the
workflow .

DE = (ID, OD);
 = {S1, S2, ..., Sn} where Si = (ISi , OSi , QSi , USi),
for all i = 1 to n;
 = {T1, T2, .…, Tn} where Ti = (ITi , OTi ,FTi , CPrei ,
CPosti), for all i = 1 to n.

x

Advertising a set of services aggregated as a process

When partnering organizations decide to share services, there
may be a predefined understanding for orchestration. As such,
these organizations must insert their relevant services into shared
UDDI registries annotated by process-based information (i.e. the
underlying control flow and data flow).
x

The following conditions should hold for a valid business
process:
For all Si   and Ti  , the inputs of Si are
1.
subsumed by the inputs of Ti , i.e., ISi C ITi
For all Si   and Ti  , the outputs of Si
2.
subsumes the outputs of Ti , i.e., OTi C OSi
For all Ti , Ti+1  , the post-conditions of Ti imply
3.
the pre-conditions of Ti+1 , i.e., CPosti => CPre i+ 1
For all Ti , Ti+1  , the outputs of Ti along with
4.
the user data inputs of the business process
subsume the inputs of Ti+1 , i.e., (OTi U ID) C IT i+ 1
The inputs of the business process subsume the
5.
inputs of the first transition, T1 (where T1   ),
i.e., IT1 C ID
The outputs of the business process are subsumed
6.
by the outputs of the last transition Tn (where Tn 
), i.e., OD C OTn

Discovering services associated with processes
discovering processes associated with services

and

Current UDDI registries facilitate browsing of services by
business name and by service name. A process-oriented registry
will also support browsing by process name or type. Consumers
should also be able to find all services associated with a particular
process.
x Managing process information by software agents
Once process information is annotated into a registry, intelligent
agents can regularly check the health of the underlying services.
Agents can look for indexing configurations that best support the
storage of the process information. In addition, agents can record
QoS information supplied by service consumers. This QoS
information can represent individual services or the process as a
whole.

4.2 Alternative Hybrid Approaches
By extending and leveraging the UDDI specification, we have
identified several approaches for annotating business processes
within service registries. Every service in a UDDI registry has a
bindingTemplate structure, which stores references to tModels.
TModels are “sources for determining compatibility of Web
services and keyed namespace references” [16]. This means that
tModels identify how to interact with a web service by describing
the technologies it implements. Although, the UDDI registry

The cardinality of data endpoints, services, and transitions vary
for each step, such that is necessary to develop containers to
aggregate the information into sets. The notion of containers is
central to business process languages, such as BPEL4WS and
BPML [3] [4], for aggregating information related to
subprocesses.

2148

usually implements default tModels, such as the State
Abbreviation System, it is also possible for an administrator of the
registry to create tModels, on-demand. There are two approaches
for using tModels to describe web services-based business
processes.

<tModel tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14">
<description>This tModel represents the process starting with
firstService and ending with thirdService. </description>
<categoryBag>
<keyedReference keyName="uddi:Process-Representing"
keyValue="categorization"
tModelKey="uddi:uddi.org:categorization:types"/>
</categoryBag>
</tModel>

x Annotating business process information directly into the
UDDI registry
A new tModel is created for every business process that is
identified in the registry. In addition, a parent tModel is created
that simply classifies any tModel that annotates a business process
as such. In this way, when a new process chain is added or
identified in the registry, a tModel that points to the parent tModel
is created to represent the process. Furthermore, the categoryBag
element is used to store references to all the processes of which
the service is a part. Figure 2 shows an example tModel. Notice
that the keyName of the keyedReference contains the service
name and additional control flow information.
Also, the
keyValue maintains the sequence number of the service in the
process. Figure 4 demonstrates the actions taken by the registry
to identify and store the existence of a composite web service.
x

<businessService>
<name>firstService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV=firstService, TRAN = Sequence"
keyValue="1"/>
</categoryBag>
</businessService>
<businessService>
<name>secondService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV = secondService, TRAN = Sequence"
keyValue="2"/>
</categoryBag>
</businessService>

Defining business process information using external
markup documents

Figure 5 details the steps for annotating a business process within
the registry using the UDDI data structures. Perhaps the leading
approach in related work is the use of an external document (e.g.
BPEL4WS and ebXML) to store the process information. This
method involves simply adding an entry to each of the process
documents associated with the relevant services. The document
could exist centrally on a main server, or locally with each service
provider.

<businessService>
<name>thirdService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV=thirdService, TRAN = Sequence"
keyValue="3"/>
</categoryBag>
</businessService>

Figure 2. Sample Process-Oriented TModel.

Figure 3. Process-Oriented Service Repository Use Cases
(** Shaded areas are not currently well supported in the state-of-the-art)

2149

Annotate( VS): Annotating Function
Vector of serviceKeys for services in chain
VS
The first service in the vector
GSFIRST
The last service in the vector
GSLAST
TM
The newly created process-tModel
The newly created tModel’s key
TMKEY
PM
The tModel Process-Classification System
CB
The CategoryBag of an individual structure
S
An individual service
KR
KeyedReference – A pointer to a tModel
Function to add a KR to CB of Services
A(VS, TM)
MakeTM(GSFIRST , GSLAST ) Function to make a new TM

cause the process workflow chain to be incomplete. In this case,
all the services in the chain are affected and they must modify
their process annotation method (either XML document or
CategoryBag) by removing the process that is broken. When a
service is deleted, all the tModels that it points that represent
composite web services (excluding simple classifications
represented in the bindingTemplate) must be deleted. Any other
services in the registry that reference these tModels must remove
the reference from their categoryBags. In the future, registries
may be able to search for replacements services as opposed to
deleting the process.

A(VS , TM)
KR = TMKEY
forAll S in VS
S.CB += KR
return VS
MakeTM(GSFIRST , GSLAST )
TM.CB += PM
TM.description = GSFIRST , GSLAST
return TM

x Service Deletions or Changes when Using External Process
Documentation
External process documents contain one entry for each process.
Irrespective of the process document notations, each entry will
define the serviceKeys of the services in the particular process. In
this approach, these serviceKeys are used to find all services that
share a process with the deleted service. The registry then edits
the relevant XML documents by removing the entry that
corresponds to the broken process.

Annotate(VS)
TM = MakeTM(GSFIRST , GSLAST )
return A(VS , TM)

x Query for Processes by Service Name using UDDI Data
Structure Method

Figure 4. Annotating a Business Process of Web Services within
the UDDI Data Model.

The last action that can be taken on a registry, and perhaps the
most desired ability, is the ability to aggregate the information
and list of services for every process in which a particular service
plays a role. The serviceKey is used to get the service’s
categoryBag. The categoryBag contains references to all the
tModels, that represent all relevant processes. The registry is then
queried to find all services which point to any one of the tModels
in their categoryBags. The chain of services is then sorted by the
tModels and is returned as process.

Annotate(VS):
Annotating Function
MakeEntry(VS ) Function to make a new TM
Function to add entry to XML Doc.
A(VS , E)
VS
Vector of serviceKeys for services in chain
E
An entry containing a list of services
S
An individual service
S.KEY
Service key
D
A service’s description element
A service’s XML document file
FS
A(VS , E)
forAll S in VS
S. FS += E
return VS

x Query for Processes by Service Name using External Process
Documentation
The external document method is quicker because it does not
require querying the registry. The XML document for a service is
retrieved. In this document is a list of entries, each pertaining to
one composite web service. Each entry contains the serviceKey
for each service in that particular chain. These services can be
retrieved with a single call to the registry by compiling a vector of
the serviceKeys. The retrieved service information is then sorted
by the order of the entries in the initial service’s process
document and displayed.

MakeEntry(VS)
forAll S in VS
E += S.KEY
return E
Annotate(VS)
forAll S in VS
S.D = FS
E = MakeEntry(VS )
return A(VS , E)

Figure 5. Leveraging External Process Documents.

5. CASE STUDY: A GENERAL UDDI
BUSINESS EXPLORER

4.3 Alternative Hybrid Approaches
Both of the hybrid approaches also have other functions relevant
to a business-oriented registry.

Once process information is associated with or incorporated into a
UDDI registry, new graphical user interfaces can be created to
support enhanced service discovery and manipulation. As a part
of this work, we designed and experimented with a new user
interface front-end (i.e. UDDI-P) to the jUDDI registry Error!
Reference source not found.. A screenshot of the design of the
interface is shown in Figure 6. Using this new interface, a user

x Service Deletions or Changes when using UDDI Structure
Method
As the registry is updated when a process is identified, it will also
change when a service is removed or replaced. These actions

2150

has the capability of browsing known process names as shown on
the left side of Figure 6. Once a process is identified, the
consumer can further decide to analyze the process to see if it
meets the organization need.
The interface can display
parameters such as estimated delivery date and price range based
on service level objectives associated with the process stored in
the registry. By embedding process information into the registry,
classification of processes can occur to the point where they
themselves function as individual services. On the right side of
the interface, the user has the ability to browse each process
meeting the search criteria by price and delivery. This sort of
interface would enable a separate interface that allows the
services to be executed.

6. EXPERIMENTATION & EVALUATION
In previous sections, two distinct hybrid approaches for
aggregating UDDI services into business processes were
introduced. The leading approaches in published works suggest
using external mark-up documents to describe business processes.
An alternative approach is incorporating specific business process
information directly into the registry. We experimented to
characterize the performance of these approaches on our
prototype repository. The performance is illustrated in Table 1
and Figure 7 based on the use cases illustrated in Figure 3.
In general, the difference in performance between adding
individual services, adding a chain, and annotating services with
business process annotations were only negligibly different
between the two approaches. However, deleting a service was
approximately three times faster when external business process
documents were used. Another variation in performance is
associated with retrieving all service IDs associated with a
particular process (or aggregating the services). The aggregation
time increased linearly with the increase in size of the UDDI
registry embedded process information. The main reason for
disparity between the approaches is due to the fact that the latter
approach requires a significant query within the repository. This
approach must retrieve the categoryBag for all services in the
repository and search those structures for a particular
keyedReference. The external business process document does
not require this step since the process information for a service is
stored in one location: the XML-based document. The retrieval
of this document is much faster when compared to the query that
must take place within the registry. Although the performance for
an external file is more favorable, having external BPEL4WS
files causes the duplication of process information (i.e. the same
process entry could appear in multiple files that may be attached
to the underlying services). Depending on the management of the
BPEL4WS files, centralizing the process within the repository
may be more advantageous. In such cases, embedding processes
with the registry represents an effective solution.

Figure 6. UDDI-P: A Prototype User Interface for Process-Based
Service Management.
Table 1. Performance of External Document and Annotated UDDI Repository (milliseconds)

Repo
Size

Add
Serv.
(ms)

Add
Composite
(ms)

Annotate
(Note)
(ms)

Collect
Service IDs
by Process
(Aggregate)
(ms)

Ext.
Process
Doc

150
330
600
850

1573
1573
1573
1573

2500
2700
2600
2600

1500
1700
1600
1600

1500
1500
1500
1500

1900
1900
1900
1900

Internal
UDDI
Data
Structure

150
330
600
850

1573
1573
1573
1573

2800
3000
3000
3000

2100
2100
2100
2100

2600
4000
5700
7450

7790
7790
7790
7790

2151

Del. Serv.
(ms)

9000
8000
7000
6000
5000
4000
3000
2000
1000
0

Add Service
Add Composite
Note
Aggregate

External XML Document

Repo=850

Repo=600

Repo=330

Repo=150

Repo=850

Repo=600

Repo=330

Repo=150

Delete Service

Annotated UDDI

Figure 7. Comparison of the Performance (ms) for Storing Process Information in External Document versus Annotating the UDDI
Repository (by number of services).
[3] WS-BPEL(2008 ):
http://www.ibm.com/developerworks/library/specification/w
s-bpel/

7. CONCLUSIONS
An innovation in this paper is the formalized model for web
services-based business process and the relevant use cases for
using this information. In addition, we introduce the design of a
new interface for business-based UDDI interactions.
Our
experimentation evaluates the two leading approaches for
capturing process information in UDDI registries. Overall
performance information does not suggest a quantitative
advantage for embedding process information directly into the
repository. However, qualitatively, maintenance is less extensive
since process information is centralized in a potentially federated
registry. As future work, we plan to continue developing a
process-oriented UDDI explorer and experiment on new
approaches for interface design.

[4] BPML (2008): http://www.ebpml.org/bpml.htm (currently
moved to OMG)
[5] BPMN (2008): http://www.bpmn.org/
[6] Dogac, A., Tambag, Y., Pembecioglu, P, Pektas, S., Laleci,
G., Gokhan, K., Toprak, S., and Kabak, Y. “An ebXML
infrastructure implementation through UDDI registries and
RosettaNet PIPs” Proceedings of the 2002 ACM SIGMOD
Conference (SIGMOD 2002), Madison, Wisconsin, June
2002
[7] jUDDI (2008): http://ws.apache.org/juddi/
[8] Luo, J., Montrose, B., Kim, A., Khashnobish, A., Kang, M.
“Adding OWL-S Support to. the Existing UDDI
Infrastructure” Proceedings of the 4th International
Conference on Web Services (ICWS2006), Chicago, Ill,
November 2006.

8. ACKNOWLEDGMENTS
We acknowledge fruitful conversations with Brian Schott and
Robert Graybill of the University of Southern California, ISI-East
and Suzy Tichenor of the Council of Competitiveness. This
material is based on research sponsored by DARPA under
agreement number FA8750-06-1-0240. This U.S. Government is
authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or implied, of
DARPA or the U.S. Government.

[9] OWL-S(2008): http://www.daml.org/services/owl-s/
[10] Papazoglou, M. “Service-oriented computing: Concepts,
characteristics and directions. In Proc. of WISE ‘03
[11] RDF (2008): http://www.w3.org/RDF/
[12] Sivashanmugam, K., Verma, K., and Sheth, A. Discovery of
Web Services in a Federated Registry Environment,
Proceedings of 4th IEEE International Conference on Web
Services (ICWS), pp. 270-278, 2004.
[13] Spies, M., Schoning, H., and Swenson, K. “Publishing
Interoperable Services and Processes in UDDI” The 11th
Enterprise Computing Conference (EDOC 2007), Annapolis,
MD, October 2007

9. REFERENCES
[1] Al-Masri, E. and Mahmoud, Q.H., “Crawling Multiple UDDI
Business Registries”, Proceedings of the 16th International
Conference on the World Wide Web, Banff, Alberta, Canada,
2007

[14] Srinivasan, N., Paolucci, M. and Sycara, K. "Adding OWL-S
to UDDI, implementation and throughput," Proceedings of
First International Workshop on Semantic Web Services and
Web Process Composition (SWSWPC 2004), San Diego,
California, USA, 2004

[2] Blake, M.B., Sliva, A.L., zur Muehlen, M., and Nickerson, J.
“Binding Now or Binding Later: The Performance of UDDI
Registries”, IEEE Hawaii International Conference of
System Sciences (HICSS-2007), Track on Technology and
Strategies for Realizing Service-oriented Architectures with
Web services, January 2007

[15] UDDI as the registry for ebXML Components, OASIS
Technical Note, February 2004, Accessed (2008):
http://www.oasis-open.org/committees/uddispec/doc/tn/uddi-spec-tc-tn-uddi-ebxml-20040219.htm

2152

Representation Chain”, In Proceedings of the International
Conference on Web Services (ICWS 2004), 2004

[16] Universal Description, Discovery, and Integration (UDDI)
(2008): http://www.uddi.org/pubs/uddi_v3.htm

[19] Zhang, M., Cheng, Z., Zhao, Y. Huang, J.Z. Yinsheng,
L., Zang, B. “ADDI: an agent-based extension to UDDI for
supply chain management” Proceedings of the Ninth Int
Conference on CSCW in Design, Shanghai, China, May 2005

[17] Using BPEL4WS in UDDI Registry, OASIS Technical Note,
July 2005 Accessed (2008): http://www.oasisopen.org/committees/uddi-spec/doc/tn/uddi-spec-tc-tn-bpel20040725.htm
[18] Zhang, L-J., Zhou, Q., and Chao, T., “A Dynamic Services
Discovery Framework for Traversing Web Services

2153

Semantics-based Web Service Composition engine
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
The University of Texas at Dallas
Richardson, TX 75083

Abstract
Service-oriented computing is gaining wider acceptance. We need an infrastructure that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper we present an approach
for automatic service discovery and composition based on
semantic description of Web services. The implementation
will be used for the WS-Challenge 2007 [1].

1. Introduction
In order to make services ubiquitously available, we need
a semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis [6]. Informally, a service is characterized by its input
parameters, the outputs it produces, and the side-effect(s)
it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced
may have to satisfy certain post-conditions. For discovery and composition, one could take the syntactic approach
in which the services being sought in response to a query
simply have their inputs syntactically match those of the
query. Alternatively, one could take the semantic approach
in which the semantics of inputs and outputs, as well as a
semantic description of the side-effect is considered in the
matching process. Several efforts are underway to build an
infrastructure for service discovery, composition, etc. These
efforts include approaches based on the semantic web (such
as USDL [4], OWL-S [7], WSML [8], WSDL-S [9]) as well
as those based on XML, such as Web Services Description Language (WSDL [5]). Approaches such as WSDL are
purely syntactic in nature, that is, they only address the syntactical aspects of a Web service. In this paper we present
our approach for automatic service composition which is
an extension of our implementation that we used at WSChallenge 2006 [3].
In section 2 we present the formal definition of the ComThe 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

Thomas D. Hite
Metallect Corp.
2400 Dallas Parkway
Plano, TX 75093

position problem. We describe our Service Composition algorithm in section 3. Section 4 presents the design of our
software with brief descriptions of the different components
of the system followed by conclusions and references.

2. Automated Web service Discovery and Composition
Discovery and Composition are two important tasks related to Web services. In this section we formally describe
these tasks. We also develop the requirements of an ideal
Discovery/Composition engine.

2.1. The Discovery Problem
Given a repository of Web services, and a query
requesting a service (we refer to it as the query service
in the rest of the text), automatically finding a service
from the repository that matches these requirements is the
Web service Discovery problem. Valid solutions to the
query satisfy the following conditions: (i) they produce
at least the query output parameters and satisfy the query
post-conditions; (ii) they use only from the provided input
parameters and satisfy the query pre-conditions; (iii) they
produce the query side-effects. Some of the solutions may
be over-qualified, but they are still considered valid as
long as they fulfill input and output parameters, pre/post
conditions, and side-effects requirements.

2.2. The Composition Problem
Given a repository of service descriptions, and a query
with the requirements of the requested service, if a matching service is not found, then the composition task can be
performed. The composition problem involves automatically finding a directed acyclic graph of services that can be
composed to obtain the desired service. Figure 1 shows an
example composite service made up of five services 1 to
0
0
5 . In the figure, I and C I are the query input parameters
and pre-conditions respectively. O0 and C O0 are the query

S

S

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

S

S

output parameters and post-conditions respectively. Informally, the directed arc between nodes i and j indicates
that outputs of i constitute (some of) the inputs of j .
Discovery and composition can be viewed as a single
problem. Discovery is a simple case of composition where
the number of services involved in composition is exactly
equal to one.
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions.
=( ; ; ;
;
;
) is the representation of a service where
is the pre-conditions,
is the input list,
is the service’s side-effect,
is the
affected object,
is the output list, and
is the postconditions.
Definition (Repository of Services): Repository is a set of
Web services.
Definition (Query): The query service is defined as
0
0
0
0
= ( 0; 0; 0;
;
;
) where
is the pre0
0
is the input list,
is the service affect,
conditions,
0
0
is the affected object, 0 is the output list, and
is the post-conditions. These are all the parameters of the
requested service.
Definition (Composition): The Composition problem can
be defined as automatically finding a directed acyclic graph
= ( ; ) of services from repository , given query =
0
0
0
( 0; 0; 0;
;
;
), where is the set of vertices
and is the set of edges of the graph. Each vertex in the
graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of
a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:
where i has zero incoming edges,
1. i i
0
0
,
i
i.
i i
2. i i
where i has zero outgoing edges,
0
0
,
i
i.
i
i
3. i i
where i has at least one incoming edge,
let i1, i2 , ..., im be the nodes such that there is
a directed edge from each of these nodes to i . Then
0
0
Ii
I , CI i
( i1
C I ).
i2 :::
im
k ik
The meaning of the is the subsumption (subsumes) reis the implication relation. Figure 2 explains
lation and
one instance of the composition problem pictorially. When

S

S

S CI I A AO O CO
CI
A
AO
O
CO

Q CI I A AO O CO
I
A
AO
O
G VE
CI I A AO O CO
E

CI

V

R

I

CO
Q

8 SS 2 V
S
I w I CI )^ CI
8 SS 2 V
S
O v O CO (^ CO
8 S 2V
S
S S S
S
vS O [ ( CO ^CO ^CO ^
v
)

The 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

Figure 2. Composite Service
the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all
nodes in the graph have not more than one incoming edge
and not more than one outgoing edge, the problem reduces
to a sequential composition problem.

2.3. Requirements of an ideal Engine
The features of an ideal Discovery/Composition engine
are:
Correctness: One of the most important requirement for
an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the
requirements of the query. Also, the engine should be able
to find all services that satisfy the query requirements.
Small Query Execution Time: Querying a repository of
services for a requested service should take a reasonable
amount of (small) time, i.e., a few milliseconds. Here we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental Updates: Adding or updating a service to an
existing repository of services should take a small amount
of time. A good Discovery/Composition engine should not
pre-process the entire repository again, rather incrementally
update the pre-processed data (indexes, etc.) of the repository for this new service added.
Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition
engine should be able to give results based on requirements
(minimize, maximize, etc.) over the costs. We can extend
this to services having an attribute vector associated with
them and the engine should be able to give results based
on maximizing or minimizing functions over this attribute
vector.
These requirements have driven the design of our

S1
I=I
CI, I

1

S1
S2

O1

I=IUO
2

1

1

S

O

2

I=IUO
3

2

2

3

.
.

.
.

O
S

3

I=IUO
4

3

3

4

S

O

4

O

5

.
.

.
.

Query
described
using
USDL
(S)

.
.
.
Sn

Infer
Sub-queries

Discovery Module
(Discovery Engine + Service
Directory + Term Generator)
S1

Figure 3. Composite Service

..........................

Sn

Composition Engine
(implemented using
Constraint Logic
Programming

Composed Service

semantics-based Composition engine described in the following sections.
Pre-Cond(S)
S1
Pre-Cond( S)1

3. A Multi-step Narrowing Solution
We assume that a directory of services has already been
compiled, and that this directory includes semantic descriptions for each service. In this section we describe our Service Composition algorithm.
Service Composition Algorithm: For service composition, the first step is finding the set of composable services.
The correct sequence of execution of these services can be
determined by the pre-conditions and post-conditions of the
individual services. That is, if a subservice 1 is composed
with subservice 2 , then the post-conditions of 1 must imply the pre-conditions of 2. The goal is to derive a single
solution, which is a directed acyclic graph of services that
can be composed together to produce the requested service
in the query. Figure 4 shows a pictorial representation of
our composition engine.
In order to produce the composite service which is represented by a graph as shown in figure 1, we filter out services that are not useful for the composition at multiple
stages. Figure 3 shows the filtering stages for the particular
instance shown in figure 1. The composition routine starts
with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. In figure 3, C I ; I are the pre-conditions and
the input parameters provided by the query. 1 and 2 are
the services found after step 1. 1 is the union of all outputs produced by the services at the first stage. For the next
stage, the inputs available are the query input parameters
and all the outputs produced by the previous stage, i.e., 2
= 1 I . 2 is used to find services at the next stage, i.e., all
those services that require a subset of 2. In order to make
sure we do not end up in cycles, we get only those services
which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all
the query output parameters are produced. At this point we
make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute
to the query output parameters. This is done starting with
the output parameters working our way backwards.
Algorithm: Composition
Input: QI - QueryInputs, QO - QueryOutputs, QCI -

S

S

S

S

O

O[ I

S

I

S

I

The 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

Post-Cond( S)1

S2

................................. S n

Post-Cond( S)n
Post-Cond(S)

Pre-Cond( S)2

Figure 4. Composition Engine
Pre-Cond, QCO - Post-Cond
Output: Result - ListOfServices
1. L NarrowServiceList(QI, QCI);
2. O GetAllOutputParameters(L);
3. CO GetAllPostConditions(L);
4. While Not (O QO)
5.
I = QI O; CI QCI CO;
6.
L’ NarrowServiceList(I, CI);
7. End While;
8. Result
RemoveRedundantServices(QO, QCO);
9.Return Result;

[

w

^

4. Implementation
Our composition engine is implemented using Prolog
[10] with Constraint Logic Programming over finite domain
[11], referred to as CLP(FD) hereafter. In this section we
briefly describe our software system and its modules. The
details of the implementation along with performance results are shown in [2].
Triple Generator: The triple generator module converts each service description into a triple as follow:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to
the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output
parameters. Services are converted to triples so that they
can be treated as terms in first-order logic. In case conditions on a service are not provided, the Pre-Conditions and
Post-Conditions in the triple will be null. Similarly if the
affect-type is not available, this module assigns a generic
affect to the service.
Query Reader: This module reads a query file (in XML
format, possibly different from the XML format used for a

service) and converts it into a triple used for querying in our
engine.
Semantic Relations Generator: We obtain the semantic
relations from the provided ontology. This module extracts
all the semantic relations and creates a list of Prolog facts.
Composition Query Processor: The composition engine is
written using Prolog with CLP(FD) library. It uses a repository of facts, which contains list of services, their input and
output parameters and the semantic relations between the
parameters. The following is the code snippet of our composition engine:
composition(sol(Qname,A)) :dQuery(Qname,_,_),
minimize(compTask(Qname,A,SeqLen),SeqLen).
compTask(Qname, A, SeqLen) :dQuery(Qname,QI,QO), encodeParam(QO,OL),
narrowO(OL,SL), fd_set(SL,Sset),
fdset_member(S_Index,Sset),
getExtInpList(QI,InpList),
encodeParam(InpList,IL), list_to_fdset(IL,QIset),
serv(S_Index,SI,_), list_to_fdset(SI,SIset),
fdset_subtract(SIset,QIset,Iset),
comp(QIset,Iset,[S_Index],SA,CompLen),
SeqLen #= CompLen + 1, decodeS(SA,A).
comp(_, Iset, A, A, 0) :- empty_fdset(Iset),!.
comp(QIset, Iset, A, SA, SeqLen) :fdset_to_list(Iset,OL),
narrowO(OL,SL), fd_set(SL,Sset),
fdset_member(SO_Index,Sset), serv(SO_Index,SI,_),
list_to_fdset(SI,SIset),
fdset_subtract(SIset,QIset,DIset),
comp(QIset,DIset,[SO_Index|A],SA,CompLen),
SeqLen #= CompLen + 1.

The query is converted into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate “bagof” to return all possible ListOfServices that can be composed to get the requested
queryService.
Output Generator: After the Composition Query processor finds a matching service, or the graph of atomic
services for a composed service, the results are sent to
the output generator in the form of triples. This module
generates the output files in any desired XML format. For
the WS-Challenge, this module will produce output files in
the format provided [1].
For this year’s challenge, the software has to receive requests and return results via SOAP. Hence our software will
work as a Web service whose interface will accept the discovery/composition query.
The 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

5. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure
to publish services, document services and query repositories for matching services. We presented our approach for
Web service composition. Our composition engine can find
a graph of atomic services that can be composed to form the
desired service as opposed to simple sequential composition
in our previous work [3]. Given semantic description of
Web services, our solution produces accurate and quick results. We are able to apply many optimization techniques to
our system so that it works efficiently even on large repositories. The use of Constraint Logic Programming (CLP)
helped greatly in obtaining an efficient implementation of
this system. We used a number of built-in features such as
indexing, set operations, and constraints and hence did not
have to spend time coding these ourselves. These CLP(FD)
built-ins facilitated the fast execution of queries.

References
[1] WS Challenge 2007 http://ws-challenge.
org.
[2] S. Kona, A. Bansal, G. Gupta, and T. Hite. Efficient
Web Service Discovery and Composition using Constraint Logic Programming. In ALPSWS Workshop at
FLoC 2006.
[3] S. Kona, A. Bansal, G. Gupta, and T. Hite. Web
Service Discovery and Composition using USDL. In
CEC/EEE, June 2006.
[4] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005.
[5] Web Services Description Language. http://www.
w3.org/TR/wsdl.
[6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp.
46-53, March 2001.
[7] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[8] WSML: Web Service Modeling Language. www.
wsmo.org/wsml/.
[9] WSDL-S: Web Service Semantics. http://www.
w3.org/Submission/WSDL-S.
[10] L. Sterling and S. Shapiro. The Art of Prolog. MIT
Press, 1994.
[11] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

